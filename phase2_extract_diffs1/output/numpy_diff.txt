('', 'setup.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,37 +1,130 @@
-"""NumPy: array processing for numbers, strings, records, and objects.
-
-NumPy is a general-purpose array-processing package designed to
-efficiently manipulate large multi-dimensional arrays of arbitrary
-records without sacrificing too much speed for small multi-dimensional
-arrays.  NumPy is built on the Numeric code base and adds features
-introduced by numarray as well as an extended C-API and the ability to
-create arrays of arbitrary type.
-
-There are also basic facilities for discrete fourier transform,
-basic linear algebra and random number generation.
+#!/usr/bin/env python3
+""" NumPy is the fundamental package for array computing with Python.
+
+It provides:
+
+- a powerful N-dimensional array object
+- sophisticated (broadcasting) functions
+- tools for integrating C/C++ and Fortran code
+- useful linear algebra, Fourier transform, and random number capabilities
+- and much more
+
+Besides its obvious scientific uses, NumPy can also be used as an efficient
+multi-dimensional container of generic data. Arbitrary data-types can be
+defined. This allows NumPy to seamlessly and speedily integrate with a wide
+variety of databases.
+
+All NumPy wheels distributed on PyPI are BSD licensed.
+
+NumPy requires ``pytest`` and ``hypothesis``.  Tests can then be run after
+installation with::
+
+    python -c 'import numpy; numpy.test()'
+
 """
-
-DOCLINES = __doc__.split("\n")
+DOCLINES = (__doc__ or '').split("\n")
 
 import os
 import sys
+import subprocess
+import textwrap
+import warnings
+import builtins
+import re
+
+
+# Python supported version checks. Keep right after stdlib imports to ensure we
+# get a sensible error for older Python versions
+if sys.version_info[:2] < (3, 8):
+    raise RuntimeError("Python version >= 3.8 required.")
+
+
+import versioneer
+
+
+# This is a bit hackish: we are setting a global variable so that the main
+# numpy __init__ can detect if it is being loaded by the setup routine, to
+# avoid attempting to load components that aren't built yet.  While ugly, it's
+# a lot more robust than what was previously being used.
+builtins.__NUMPY_SETUP__ = True
+
+# Needed for backwards code compatibility below and in some CI scripts.
+# The version components are changed from ints to strings, but only VERSION
+# seems to matter outside of this module and it was already a str.
+FULLVERSION = versioneer.get_version()
+
+# Capture the version string:
+# 1.22.0.dev0+ ... -> ISRELEASED == False, VERSION == 1.22.0
+# 1.22.0rc1+ ... -> ISRELEASED == False, VERSION == 1.22.0
+# 1.22.0 ... -> ISRELEASED == True, VERSION == 1.22.0
+# 1.22.0rc1 ... -> ISRELEASED == True, VERSION == 1.22.0
+ISRELEASED = re.search(r'(dev|\+)', FULLVERSION) is None
+_V_MATCH = re.match(r'(\d+)\.(\d+)\.(\d+)', FULLVERSION)
+if _V_MATCH is None:
+    raise RuntimeError(f'Cannot parse version {FULLVERSION}')
+MAJOR, MINOR, MICRO = _V_MATCH.groups()
+VERSION = '{}.{}.{}'.format(MAJOR, MINOR, MICRO)
+
+# The first version not in the `Programming Language :: Python :: ...` classifiers above
+if sys.version_info >= (3, 11):
+    fmt = "NumPy {} may not yet support Python {}.{}."
+    warnings.warn(
+        fmt.format(VERSION, *sys.version_info[:2]),
+        RuntimeWarning)
+    del fmt
+
+# BEFORE importing setuptools, remove MANIFEST. Otherwise it may not be
+# properly updated when the contents of directories change (true for distutils,
+# not sure about setuptools).
+if os.path.exists('MANIFEST'):
+    os.remove('MANIFEST')
+
+# We need to import setuptools here in order for it to persist in sys.modules.
+# Its presence/absence is used in subclassing setup in numpy/distutils/core.py.
+# However, we need to run the distutils version of sdist, so import that first
+# so that it is in sys.modules
+import numpy.distutils.command.sdist
+import setuptools
+if int(setuptools.__version__.split('.')[0]) >= 60:
+    # setuptools >= 60 switches to vendored distutils by default; this
+    # may break the numpy build, so make sure the stdlib version is used
+    try:
+        setuptools_use_distutils = os.environ['SETUPTOOLS_USE_DISTUTILS']
+    except KeyError:
+        os.environ['SETUPTOOLS_USE_DISTUTILS'] = "stdlib"
+    else:
+        if setuptools_use_distutils != "stdlib":
+            raise RuntimeError("setuptools versions >= '60.0.0' require "
+                    "SETUPTOOLS_USE_DISTUTILS=stdlib in the environment")
+
+# Initialize cmdclass from versioneer
+from numpy.distutils.core import numpy_cmdclass
+cmdclass = versioneer.get_cmdclass(numpy_cmdclass)
 
 CLASSIFIERS = """\
-Development Status :: 4 - Beta
+Development Status :: 5 - Production/Stable
 Intended Audience :: Science/Research
 Intended Audience :: Developers
-License :: OSI Approved
+License :: OSI Approved :: BSD License
 Programming Language :: C
 Programming Language :: Python
+Programming Language :: Python :: 3
+Programming Language :: Python :: 3.8
+Programming Language :: Python :: 3.9
+Programming Language :: Python :: 3.10
+Programming Language :: Python :: 3 :: Only
+Programming Language :: Python :: Implementation :: CPython
 Topic :: Software Development
 Topic :: Scientific/Engineering
+Typing :: Typed
 Operating System :: Microsoft :: Windows
 Operating System :: POSIX
 Operating System :: Unix
 Operating System :: MacOS
 """
 
-def configuration(parent_package='',top_path=None):
+
+def configuration(parent_package='', top_path=None):
     from numpy.distutils.misc_util import Configuration
 
     config = Configuration(None, parent_package, top_path)
@@ -39,46 +132,367 @@
                        assume_default_configuration=True,
                        delegate_options_to_subpackages=True,
                        quiet=True)
-    
+
     config.add_subpackage('numpy')
-        
-    config.add_data_files(('numpy',['*.txt','COMPATIBILITY',
-                                    'scipy_compatibility']))
-
-    config.get_version('numpy/version.py') # sets config.version
-    
+    config.add_data_files(('numpy', 'LICENSE.txt'))
+    config.add_data_files(('numpy', 'numpy/*.pxd'))
+
+    config.get_version('numpy/version.py')  # sets config.version
+
     return config
 
+
+def check_submodules():
+    """ verify that the submodules are checked out and clean
+        use `git submodule update --init`; on failure
+    """
+    if not os.path.exists('.git'):
+        return
+    with open('.gitmodules') as f:
+        for line in f:
+            if 'path' in line:
+                p = line.split('=')[-1].strip()
+                if not os.path.exists(p):
+                    raise ValueError('Submodule {} missing'.format(p))
+
+    proc = subprocess.Popen(['git', 'submodule', 'status'],
+                            stdout=subprocess.PIPE)
+    status, _ = proc.communicate()
+    status = status.decode("ascii", "replace")
+    for line in status.splitlines():
+        if line.startswith('-') or line.startswith('+'):
+            raise ValueError('Submodule not clean: {}'.format(line))
+
+
+class concat_license_files():
+    """Merge LICENSE.txt and LICENSES_bundled.txt for sdist creation
+
+    Done this way to keep LICENSE.txt in repo as exact BSD 3-clause (see
+    gh-13447).  This makes GitHub state correctly how NumPy is licensed.
+    """
+    def __init__(self):
+        self.f1 = 'LICENSE.txt'
+        self.f2 = 'LICENSES_bundled.txt'
+
+    def __enter__(self):
+        """Concatenate files and remove LICENSES_bundled.txt"""
+        with open(self.f1, 'r') as f1:
+            self.bsd_text = f1.read()
+
+        with open(self.f1, 'a') as f1:
+            with open(self.f2, 'r') as f2:
+                self.bundled_text = f2.read()
+                f1.write('\n\n')
+                f1.write(self.bundled_text)
+
+    def __exit__(self, exception_type, exception_value, traceback):
+        """Restore content of both files"""
+        with open(self.f1, 'w') as f:
+            f.write(self.bsd_text)
+
+
+# Need to inherit from versioneer version of sdist to get the encoded
+# version information.
+class sdist_checked(cmdclass['sdist']):
+    """ check submodules on sdist to prevent incomplete tarballs """
+    def run(self):
+        check_submodules()
+        with concat_license_files():
+            super().run()
+
+
+def get_build_overrides():
+    """
+    Custom build commands to add `-std=c99` to compilation
+    """
+    from numpy.distutils.command.build_clib import build_clib
+    from numpy.distutils.command.build_ext import build_ext
+    from numpy.compat import _pep440
+
+    def _needs_gcc_c99_flag(obj):
+        if obj.compiler.compiler_type != 'unix':
+            return False
+
+        cc = obj.compiler.compiler[0]
+        if "gcc" not in cc:
+            return False
+
+        # will print something like '4.2.1\n'
+        out = subprocess.run([cc, '-dumpversion'], stdout=subprocess.PIPE,
+                             stderr=subprocess.PIPE, universal_newlines=True)
+        # -std=c99 is default from this version on
+        if _pep440.parse(out.stdout) >= _pep440.Version('5.0'):
+            return False
+        return True
+
+    class new_build_clib(build_clib):
+        def build_a_library(self, build_info, lib_name, libraries):
+            from numpy.distutils.ccompiler_opt import NPY_CXX_FLAGS
+            if _needs_gcc_c99_flag(self):
+                build_info['extra_cflags'] = ['-std=c99']
+            build_info['extra_cxxflags'] = NPY_CXX_FLAGS
+            build_clib.build_a_library(self, build_info, lib_name, libraries)
+
+    class new_build_ext(build_ext):
+        def build_extension(self, ext):
+            if _needs_gcc_c99_flag(self):
+                if '-std=c99' not in ext.extra_compile_args:
+                    ext.extra_compile_args.append('-std=c99')
+            build_ext.build_extension(self, ext)
+    return new_build_clib, new_build_ext
+
+
+def generate_cython():
+    # Check Cython version
+    from numpy.compat import _pep440
+    try:
+        # try the cython in the installed python first (somewhat related to
+        # scipy/scipy#2397)
+        import Cython
+        from Cython.Compiler.Version import version as cython_version
+    except ImportError as e:
+        # The `cython` command need not point to the version installed in the
+        # Python running this script, so raise an error to avoid the chance of
+        # using the wrong version of Cython.
+        msg = 'Cython needs to be installed in Python as a module'
+        raise OSError(msg) from e
+    else:
+        # Note: keep in sync with that in pyproject.toml
+        # Update for Python 3.11
+        required_version = '0.29.30'
+
+        if _pep440.parse(cython_version) < _pep440.Version(required_version):
+            cython_path = Cython.__file__
+            msg = 'Building NumPy requires Cython >= {}, found {} at {}'
+            msg = msg.format(required_version, cython_version, cython_path)
+            raise RuntimeError(msg)
+
+    # Process files
+    cwd = os.path.abspath(os.path.dirname(__file__))
+    print("Cythonizing sources")
+    for d in ('random',):
+        p = subprocess.call([sys.executable,
+                             os.path.join(cwd, 'tools', 'cythonize.py'),
+                             'numpy/{0}'.format(d)],
+                            cwd=cwd)
+        if p != 0:
+            raise RuntimeError("Running cythonize failed!")
+
+
+def parse_setuppy_commands():
+    """Check the commands and respond appropriately.  Disable broken commands.
+
+    Return a boolean value for whether or not to run the build or not (avoid
+    parsing Cython and template files if False).
+    """
+    args = sys.argv[1:]
+
+    if not args:
+        # User forgot to give an argument probably, let setuptools handle that.
+        return True
+
+    info_commands = ['--help-commands', '--name', '--version', '-V',
+                     '--fullname', '--author', '--author-email',
+                     '--maintainer', '--maintainer-email', '--contact',
+                     '--contact-email', '--url', '--license', '--description',
+                     '--long-description', '--platforms', '--classifiers',
+                     '--keywords', '--provides', '--requires', '--obsoletes',
+                     'version',]
+
+    for command in info_commands:
+        if command in args:
+            return False
+
+    # Note that 'alias', 'saveopts' and 'setopt' commands also seem to work
+    # fine as they are, but are usually used together with one of the commands
+    # below and not standalone.  Hence they're not added to good_commands.
+    good_commands = ('develop', 'sdist', 'build', 'build_ext', 'build_py',
+                     'build_clib', 'build_scripts', 'bdist_wheel', 'bdist_rpm',
+                     'bdist_wininst', 'bdist_msi', 'bdist_mpkg', 'build_src',
+                     'bdist_egg')
+
+    for command in good_commands:
+        if command in args:
+            return True
+
+    # The following commands are supported, but we need to show more
+    # useful messages to the user
+    if 'install' in args:
+        print(textwrap.dedent("""
+            Note: if you need reliable uninstall behavior, then install
+            with pip instead of using `setup.py install`:
+
+              - `pip install .`       (from a git repo or downloaded source
+                                       release)
+              - `pip install numpy`   (last NumPy release on PyPI)
+
+            """))
+        return True
+
+    if '--help' in args or '-h' in sys.argv[1]:
+        print(textwrap.dedent("""
+            NumPy-specific help
+            -------------------
+
+            To install NumPy from here with reliable uninstall, we recommend
+            that you use `pip install .`. To install the latest NumPy release
+            from PyPI, use `pip install numpy`.
+
+            For help with build/installation issues, please ask on the
+            numpy-discussion mailing list.  If you are sure that you have run
+            into a bug, please report it at https://github.com/numpy/numpy/issues.
+
+            Setuptools commands help
+            ------------------------
+            """))
+        return False
+
+    # The following commands aren't supported.  They can only be executed when
+    # the user explicitly adds a --force command-line argument.
+    bad_commands = dict(
+        test="""
+            `setup.py test` is not supported.  Use one of the following
+            instead:
+
+              - `python runtests.py`              (to build and test)
+              - `python runtests.py --no-build`   (to test installed numpy)
+              - `>>> numpy.test()`           (run tests for installed numpy
+                                              from within an interpreter)
+            """,
+        upload="""
+            `setup.py upload` is not supported, because it's insecure.
+            Instead, build what you want to upload and upload those files
+            with `twine upload -s <filenames>` instead.
+            """,
+        clean="""
+            `setup.py clean` is not supported, use one of the following instead:
+
+              - `git clean -xdf` (cleans all files)
+              - `git clean -Xdf` (cleans all versioned files, doesn't touch
+                                  files that aren't checked into the git repo)
+            """,
+        build_sphinx="""
+            `setup.py build_sphinx` is not supported, use the
+            Makefile under doc/""",
+        flake8="`setup.py flake8` is not supported, use flake8 standalone",
+        )
+    bad_commands['nosetests'] = bad_commands['test']
+    for command in ('upload_docs', 'easy_install', 'bdist', 'bdist_dumb',
+                    'register', 'check', 'install_data', 'install_headers',
+                    'install_lib', 'install_scripts', ):
+        bad_commands[command] = "`setup.py %s` is not supported" % command
+
+    for command in bad_commands.keys():
+        if command in args:
+            print(textwrap.dedent(bad_commands[command]) +
+                  "\nAdd `--force` to your command to use it anyway if you "
+                  "must (unsupported).\n")
+            sys.exit(1)
+
+    # Commands that do more than print info, but also don't need Cython and
+    # template parsing.
+    other_commands = ['egg_info', 'install_egg_info', 'rotate', 'dist_info']
+    for command in other_commands:
+        if command in args:
+            return False
+
+    # If we got here, we didn't detect what setup.py command was given
+    raise RuntimeError("Unrecognized setuptools command: {}".format(args))
+
+
+def get_docs_url():
+    if 'dev' in VERSION:
+        return "https://numpy.org/devdocs"
+    else:
+        # For releases, this URL ends up on PyPI.
+        # By pinning the version, users looking at old PyPI releases can get
+        # to the associated docs easily.
+        return "https://numpy.org/doc/{}.{}".format(MAJOR, MINOR)
+
+
 def setup_package():
-
-    from numpy.distutils.core import setup
-
+    src_path = os.path.dirname(os.path.abspath(__file__))
     old_path = os.getcwd()
-    local_path = os.path.dirname(os.path.abspath(sys.argv[0]))
-    os.chdir(local_path)
-    sys.path.insert(0,local_path)
+    os.chdir(src_path)
+    sys.path.insert(0, src_path)
+
+    # The f2py scripts that will be installed
+    if sys.platform == 'win32':
+        f2py_cmds = [
+            'f2py = numpy.f2py.f2py2e:main',
+            ]
+    else:
+        f2py_cmds = [
+            'f2py = numpy.f2py.f2py2e:main',
+            'f2py%s = numpy.f2py.f2py2e:main' % sys.version_info[:1],
+            'f2py%s.%s = numpy.f2py.f2py2e:main' % sys.version_info[:2],
+            ]
+
+    cmdclass["sdist"] = sdist_checked
+    metadata = dict(
+        name='numpy',
+        maintainer="NumPy Developers",
+        maintainer_email="numpy-discussion@python.org",
+        description=DOCLINES[0],
+        long_description="\n".join(DOCLINES[2:]),
+        url="https://www.numpy.org",
+        author="Travis E. Oliphant et al.",
+        download_url="https://pypi.python.org/pypi/numpy",
+        project_urls={
+            "Bug Tracker": "https://github.com/numpy/numpy/issues",
+            "Documentation": get_docs_url(),
+            "Source Code": "https://github.com/numpy/numpy",
+        },
+        license='BSD',
+        classifiers=[_f for _f in CLASSIFIERS.split('\n') if _f],
+        platforms=["Windows", "Linux", "Solaris", "Mac OS-X", "Unix"],
+        test_suite='pytest',
+        version=versioneer.get_version(),
+        cmdclass=cmdclass,
+        python_requires='>=3.8',
+        zip_safe=False,
+        entry_points={
+            'console_scripts': f2py_cmds,
+            'array_api': ['numpy = numpy.array_api'],
+            'pyinstaller40': ['hook-dirs = numpy:_pyinstaller_hooks_dir'],
+        },
+    )
+
+    if "--force" in sys.argv:
+        run_build = True
+        sys.argv.remove('--force')
+    else:
+        # Raise errors for unsupported commands, improve help output, etc.
+        run_build = parse_setuppy_commands()
+
+    if run_build:
+        # patches distutils, even though we don't use it
+        #from setuptools import setup
+        from numpy.distutils.core import setup
+
+        if 'sdist' not in sys.argv:
+            # Generate Cython sources, unless we're generating an sdist
+            generate_cython()
+
+        metadata['configuration'] = configuration
+        # Customize extension building
+        cmdclass['build_clib'], cmdclass['build_ext'] = get_build_overrides()
+    else:
+        #from numpy.distutils.core import setup
+        from setuptools import setup
 
     try:
-        from numpy.version import version
-        setup(
-            name = 'numpy',
-            version = version, # will be overwritten by configuration version
-            maintainer = "NumPy Developers",
-            maintainer_email = "numpy-discussion@lists.sourceforge.net",
-            description = DOCLINES[0],
-            long_description = "\n".join(DOCLINES[2:]),
-            url = "http://numeric.scipy.org",
-            download_url = "http://sourceforge.net/project/showfiles.php?group_id=1369&package_id=175103",
-            license = 'BSD',
-            classifiers=filter(None, CLASSIFIERS.split('\n')),
-            author = "Travis E. Oliphant, et.al.",
-            author_email = "oliphant@ee.byu.edu",
-            platforms = ["Windows", "Linux", "Solaris", "Mac OS-X", "Unix"],            
-            configuration=configuration )
+        setup(**metadata)
     finally:
         del sys.path[0]
         os.chdir(old_path)
     return
 
+
 if __name__ == '__main__':
     setup_package()
+    # This may avoid problems where numpy is installed via ``*_requires`` by
+    # setuptools, the global namespace isn't reset properly, and then numpy is
+    # imported later (which will then fail to load numpy extension modules).
+    # See gh-7956 for details
+    del builtins.__NUMPY_SETUP__
('numpy', 'version.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,12 +1,15 @@
-version='0.9.8'
+from __future__ import annotations
 
-#import os
-#svn_version_file = os.path.join(os.path.dirname(__file__),
-#                                'core','__svn_version__.py')
-#if os.path.isfile(svn_version_file):
-#    import imp
-#    svn = imp.load_module('numpy.core.__svn_version__',
-#                          open(svn_version_file),
-#                          svn_version_file,
-#                          ('.py','U',1))
-#    version += '.'+svn.version
+from ._version import get_versions
+
+__ALL__ = ['version', '__version__', 'full_version', 'git_revision', 'release']
+
+vinfo: dict[str, str] = get_versions()
+version = vinfo["version"]
+__version__ = vinfo.get("closest-tag", vinfo["version"])
+full_version = vinfo['version']
+git_revision = vinfo['full-revisionid']
+release = 'dev0' not in version and '+' not in version
+short_version = vinfo['version'].split("+")[0]
+
+del get_versions, vinfo
('numpy', '__init__.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,67 +1,419 @@
-"""\
+"""
 NumPy
-==========
-
-You can support the development of NumPy and SciPy by purchasing
-documentation at
-
-  http://www.trelgol.com
-
-It is being distributed for a fee for a limited time to try and raise
-money for development.
-
-Documentation is also available in the docstrings.
-
-"""
-
-try:
-    from __config__ import show as show_config
-except ImportError:
-    show_config = None
-
-if show_config is None:
-    import sys as _sys
-    print >> _sys.stderr, 'Running from numpy source directory.'
-    del _sys
-else:
-    from version import version as __version__
-
-    import os as _os
-    NUMPY_IMPORT_VERBOSE = int(_os.environ.get('NUMPY_IMPORT_VERBOSE','0'))
-    del _os
-    from _import_tools import PackageLoader
-    pkgload = PackageLoader()
-    pkgload('testing','core','lib','linalg','dft','random','f2py',
-            verbose=NUMPY_IMPORT_VERBOSE,postpone=False)
-
-    if __doc__ is not None:
-        __doc__ += """
+=====
+
+Provides
+  1. An array object of arbitrary homogeneous items
+  2. Fast mathematical operations over arrays
+  3. Linear Algebra, Fourier Transforms, Random Number Generation
+
+How to use the documentation
+----------------------------
+Documentation is available in two forms: docstrings provided
+with the code, and a loose standing reference guide, available from
+`the NumPy homepage <https://numpy.org>`_.
+
+We recommend exploring the docstrings using
+`IPython <https://ipython.org>`_, an advanced Python shell with
+TAB-completion and introspection capabilities.  See below for further
+instructions.
+
+The docstring examples assume that `numpy` has been imported as `np`::
+
+  >>> import numpy as np
+
+Code snippets are indicated by three greater-than signs::
+
+  >>> x = 42
+  >>> x = x + 1
+
+Use the built-in ``help`` function to view a function's docstring::
+
+  >>> help(np.sort)
+  ... # doctest: +SKIP
+
+For some objects, ``np.info(obj)`` may provide additional help.  This is
+particularly true if you see the line "Help on ufunc object:" at the top
+of the help() page.  Ufuncs are implemented in C, not Python, for speed.
+The native Python help() does not know how to view their help, but our
+np.info() function does.
+
+To search for documents containing a keyword, do::
+
+  >>> np.lookfor('keyword')
+  ... # doctest: +SKIP
+
+General-purpose documents like a glossary and help on the basic concepts
+of numpy are available under the ``doc`` sub-module::
+
+  >>> from numpy import doc
+  >>> help(doc)
+  ... # doctest: +SKIP
 
 Available subpackages
 ---------------------
+lib
+    Basic functions used by several sub-packages.
+random
+    Core Random Tools
+linalg
+    Core Linear Algebra Tools
+fft
+    Core FFT routines
+polynomial
+    Polynomial tools
+testing
+    NumPy testing tools
+distutils
+    Enhancements to distutils with support for
+    Fortran compilers support and more.
+
+Utilities
+---------
+test
+    Run numpy unittests
+show_config
+    Show numpy build configuration
+dual
+    Overwrite certain functions with high-performance SciPy tools.
+    Note: `numpy.dual` is deprecated.  Use the functions from NumPy or Scipy
+    directly instead of importing them from `numpy.dual`.
+matlib
+    Make everything matrices.
+__version__
+    NumPy version string
+
+Viewing documentation using IPython
+-----------------------------------
+Start IPython with the NumPy profile (``ipython -p numpy``), which will
+import `numpy` under the alias `np`.  Then, use the ``cpaste`` command to
+paste examples into the shell.  To see which functions are available in
+`numpy`, type ``np.<TAB>`` (where ``<TAB>`` refers to the TAB key), or use
+``np.*cos*?<ENTER>`` (where ``<ENTER>`` refers to the ENTER key) to narrow
+down the list.  To view the docstring for a function, use
+``np.cos?<ENTER>`` (to view the docstring) and ``np.cos??<ENTER>`` (to view
+the source code).
+
+Copies vs. in-place operation
+-----------------------------
+Most of the functions in `numpy` return a copy of the array argument
+(e.g., `np.sort`).  In-place versions of these functions are often
+available as array methods, i.e. ``x = np.array([1,2,3]); x.sort()``.
+Exceptions to this rule are documented.
+
 """
-    if __doc__ is not None:
-        __doc__ += pkgload.get_pkgdocs()
-
-    def test(level=1, verbosity=1):
-        return NumpyTest().test(level, verbosity)
-
-    import add_newdocs
-
-    if __doc__ is not None:
-        __doc__ += """
-
-Utility tools
--------------
-
-  test        --- Run numpy unittests
-  pkgload     --- Load numpy packages
-  show_config --- Show numpy build configuration
-  dual        --- Overwrite certain functions with high-performance Scipy tools
-  __version__ --- Numpy version string
-
-Environment variables
----------------------
-
-  NUMPY_IMPORT_VERBOSE --- pkgload verbose flag, default is 0.
-"""
+import sys
+import warnings
+
+from ._globals import (
+    ModuleDeprecationWarning, VisibleDeprecationWarning,
+    _NoValue, _CopyMode
+)
+
+# We first need to detect if we're being called as part of the numpy setup
+# procedure itself in a reliable manner.
+try:
+    __NUMPY_SETUP__
+except NameError:
+    __NUMPY_SETUP__ = False
+
+if __NUMPY_SETUP__:
+    sys.stderr.write('Running from numpy source directory.\n')
+else:
+    try:
+        from numpy.__config__ import show as show_config
+    except ImportError as e:
+        msg = """Error importing numpy: you should not try to import numpy from
+        its source directory; please exit the numpy source tree, and relaunch
+        your python interpreter from there."""
+        raise ImportError(msg) from e
+
+    __all__ = ['ModuleDeprecationWarning',
+               'VisibleDeprecationWarning']
+
+    # mapping of {name: (value, deprecation_msg)}
+    __deprecated_attrs__ = {}
+
+    # Allow distributors to run custom init code
+    from . import _distributor_init
+
+    from . import core
+    from .core import *
+    from . import compat
+    from . import lib
+    # NOTE: to be revisited following future namespace cleanup.
+    # See gh-14454 and gh-15672 for discussion.
+    from .lib import *
+
+    from . import linalg
+    from . import fft
+    from . import polynomial
+    from . import random
+    from . import ctypeslib
+    from . import ma
+    from . import matrixlib as _mat
+    from .matrixlib import *
+
+    # Deprecations introduced in NumPy 1.20.0, 2020-06-06
+    import builtins as _builtins
+
+    _msg = (
+        "`np.{n}` is a deprecated alias for the builtin `{n}`. "
+        "To silence this warning, use `{n}` by itself. Doing this will not "
+        "modify any behavior and is safe. {extended_msg}\n"
+        "Deprecated in NumPy 1.20; for more details and guidance: "
+        "https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations")
+
+    _specific_msg = (
+        "If you specifically wanted the numpy scalar type, use `np.{}` here.")
+
+    _int_extended_msg = (
+        "When replacing `np.{}`, you may wish to use e.g. `np.int64` "
+        "or `np.int32` to specify the precision. If you wish to review "
+        "your current use, check the release note link for "
+        "additional information.")
+
+    _type_info = [
+        ("object", ""),  # The NumPy scalar only exists by name.
+        ("bool", _specific_msg.format("bool_")),
+        ("float", _specific_msg.format("float64")),
+        ("complex", _specific_msg.format("complex128")),
+        ("str", _specific_msg.format("str_")),
+        ("int", _int_extended_msg.format("int"))]
+
+    __deprecated_attrs__.update({
+        n: (getattr(_builtins, n), _msg.format(n=n, extended_msg=extended_msg))
+        for n, extended_msg in _type_info
+    })
+
+    # Numpy 1.20.0, 2020-10-19
+    __deprecated_attrs__["typeDict"] = (
+        core.numerictypes.typeDict,
+        "`np.typeDict` is a deprecated alias for `np.sctypeDict`."
+    )
+
+    # NumPy 1.22, 2021-10-20
+    __deprecated_attrs__["MachAr"] = (
+        core._machar.MachAr,
+        "`np.MachAr` is deprecated (NumPy 1.22)."
+    )
+
+    _msg = (
+        "`np.{n}` is a deprecated alias for `np.compat.{n}`. "
+        "To silence this warning, use `np.compat.{n}` by itself. "
+        "In the likely event your code does not need to work on Python 2 "
+        "you can use the builtin `{n2}` for which `np.compat.{n}` is itself "
+        "an alias. Doing this will not modify any behaviour and is safe. "
+        "{extended_msg}\n"
+        "Deprecated in NumPy 1.20; for more details and guidance: "
+        "https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations")
+
+    __deprecated_attrs__["long"] = (
+        getattr(compat, "long"),
+        _msg.format(n="long", n2="int",
+                    extended_msg=_int_extended_msg.format("long")))
+
+    __deprecated_attrs__["unicode"] = (
+        getattr(compat, "unicode"),
+        _msg.format(n="unicode", n2="str",
+                    extended_msg=_specific_msg.format("str_")))
+
+    del _msg, _specific_msg, _int_extended_msg, _type_info, _builtins
+
+    from .core import round, abs, max, min
+    # now that numpy modules are imported, can initialize limits
+    core.getlimits._register_known_types()
+
+    __all__.extend(['__version__', 'show_config'])
+    __all__.extend(core.__all__)
+    __all__.extend(_mat.__all__)
+    __all__.extend(lib.__all__)
+    __all__.extend(['linalg', 'fft', 'random', 'ctypeslib', 'ma'])
+
+    # Remove one of the two occurrences of `issubdtype`, which is exposed as
+    # both `numpy.core.issubdtype` and `numpy.lib.issubdtype`.
+    __all__.remove('issubdtype')
+
+    # These are exported by np.core, but are replaced by the builtins below
+    # remove them to ensure that we don't end up with `np.long == np.int_`,
+    # which would be a breaking change.
+    del long, unicode
+    __all__.remove('long')
+    __all__.remove('unicode')
+
+    # Remove things that are in the numpy.lib but not in the numpy namespace
+    # Note that there is a test (numpy/tests/test_public_api.py:test_numpy_namespace)
+    # that prevents adding more things to the main namespace by accident.
+    # The list below will grow until the `from .lib import *` fixme above is
+    # taken care of
+    __all__.remove('Arrayterator')
+    del Arrayterator
+
+    # These names were removed in NumPy 1.20.  For at least one release,
+    # attempts to access these names in the numpy namespace will trigger
+    # a warning, and calling the function will raise an exception.
+    _financial_names = ['fv', 'ipmt', 'irr', 'mirr', 'nper', 'npv', 'pmt',
+                        'ppmt', 'pv', 'rate']
+    __expired_functions__ = {
+        name: (f'In accordance with NEP 32, the function {name} was removed '
+               'from NumPy version 1.20.  A replacement for this function '
+               'is available in the numpy_financial library: '
+               'https://pypi.org/project/numpy-financial')
+        for name in _financial_names}
+
+    # Filter out Cython harmless warnings
+    warnings.filterwarnings("ignore", message="numpy.dtype size changed")
+    warnings.filterwarnings("ignore", message="numpy.ufunc size changed")
+    warnings.filterwarnings("ignore", message="numpy.ndarray size changed")
+
+    # oldnumeric and numarray were removed in 1.9. In case some packages import
+    # but do not use them, we define them here for backward compatibility.
+    oldnumeric = 'removed'
+    numarray = 'removed'
+
+    def __getattr__(attr):
+        # Warn for expired attributes, and return a dummy function
+        # that always raises an exception.
+        try:
+            msg = __expired_functions__[attr]
+        except KeyError:
+            pass
+        else:
+            warnings.warn(msg, DeprecationWarning, stacklevel=2)
+
+            def _expired(*args, **kwds):
+                raise RuntimeError(msg)
+
+            return _expired
+
+        # Emit warnings for deprecated attributes
+        try:
+            val, msg = __deprecated_attrs__[attr]
+        except KeyError:
+            pass
+        else:
+            warnings.warn(msg, DeprecationWarning, stacklevel=2)
+            return val
+
+        # Importing Tester requires importing all of UnitTest which is not a
+        # cheap import Since it is mainly used in test suits, we lazy import it
+        # here to save on the order of 10 ms of import time for most users
+        #
+        # The previous way Tester was imported also had a side effect of adding
+        # the full `numpy.testing` namespace
+        if attr == 'testing':
+            import numpy.testing as testing
+            return testing
+        elif attr == 'Tester':
+            from .testing import Tester
+            return Tester
+
+        raise AttributeError("module {!r} has no attribute "
+                             "{!r}".format(__name__, attr))
+
+    def __dir__():
+        return list(globals().keys() | {'Tester', 'testing'})
+
+    # Pytest testing
+    from numpy._pytesttester import PytestTester
+    test = PytestTester(__name__)
+    del PytestTester
+
+    def _sanity_check():
+        """
+        Quick sanity checks for common bugs caused by environment.
+        There are some cases e.g. with wrong BLAS ABI that cause wrong
+        results under specific runtime conditions that are not necessarily
+        achieved during test suite runs, and it is useful to catch those early.
+
+        See https://github.com/numpy/numpy/issues/8577 and other
+        similar bug reports.
+
+        """
+        try:
+            x = ones(2, dtype=float32)
+            if not abs(x.dot(x) - 2.0) < 1e-5:
+                raise AssertionError()
+        except AssertionError:
+            msg = ("The current Numpy installation ({!r}) fails to "
+                   "pass simple sanity checks. This can be caused for example "
+                   "by incorrect BLAS library being linked in, or by mixing "
+                   "package managers (pip, conda, apt, ...). Search closed "
+                   "numpy issues for similar problems.")
+            raise RuntimeError(msg.format(__file__)) from None
+
+    _sanity_check()
+    del _sanity_check
+
+    def _mac_os_check():
+        """
+        Quick Sanity check for Mac OS look for accelerate build bugs.
+        Testing numpy polyfit calls init_dgelsd(LAPACK)
+        """
+        try:
+            c = array([3., 2., 1.])
+            x = linspace(0, 2, 5)
+            y = polyval(c, x)
+            _ = polyfit(x, y, 2, cov=True)
+        except ValueError:
+            pass
+
+    import sys
+    if sys.platform == "darwin":
+        with warnings.catch_warnings(record=True) as w:
+            _mac_os_check()
+            # Throw runtime error, if the test failed Check for warning and error_message
+            error_message = ""
+            if len(w) > 0:
+                error_message = "{}: {}".format(w[-1].category.__name__, str(w[-1].message))
+                msg = (
+                    "Polyfit sanity test emitted a warning, most likely due "
+                    "to using a buggy Accelerate backend."
+                    "\nIf you compiled yourself, more information is available at:"
+                    "\nhttps://numpy.org/doc/stable/user/building.html#accelerated-blas-lapack-libraries"
+                    "\nOtherwise report this to the vendor "
+                    "that provided NumPy.\n{}\n".format(error_message))
+                raise RuntimeError(msg)
+    del _mac_os_check
+
+    # We usually use madvise hugepages support, but on some old kernels it
+    # is slow and thus better avoided.
+    # Specifically kernel version 4.6 had a bug fix which probably fixed this:
+    # https://github.com/torvalds/linux/commit/7cf91a98e607c2f935dbcc177d70011e95b8faff
+    import os
+    use_hugepage = os.environ.get("NUMPY_MADVISE_HUGEPAGE", None)
+    if sys.platform == "linux" and use_hugepage is None:
+        # If there is an issue with parsing the kernel version,
+        # set use_hugepages to 0. Usage of LooseVersion will handle
+        # the kernel version parsing better, but avoided since it
+        # will increase the import time. See: #16679 for related discussion.
+        try:
+            use_hugepage = 1
+            kernel_version = os.uname().release.split(".")[:2]
+            kernel_version = tuple(int(v) for v in kernel_version)
+            if kernel_version < (4, 6):
+                use_hugepage = 0
+        except ValueError:
+            use_hugepages = 0
+    elif use_hugepage is None:
+        # This is not Linux, so it should not matter, just enable anyway
+        use_hugepage = 1
+    else:
+        use_hugepage = int(use_hugepage)
+
+    # Note that this will currently only make a difference on Linux
+    core.multiarray._set_madvise_hugepage(use_hugepage)
+
+    # Give a warning if NumPy is reloaded or imported on a sub-interpreter
+    # We do this from python, since the C-module may not be reloaded and
+    # it is tidier organized.
+    core.multiarray._multiarray_umath._reload_guard()
+
+    # Tell PyInstaller where to find hook-numpy.py
+    def _pyinstaller_hooks_dir():
+        from pathlib import Path
+        return [str(Path(__file__).with_name("_pyinstaller").resolve())]
+
+
+# get the version using versioneer
+from .version import __version__, git_revision as __git_version__
('numpy', 'dual.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,14 +1,40 @@
+"""
+.. deprecated:: 1.20
+
+*This module is deprecated.  Instead of importing functions from*
+``numpy.dual``, *the functions should be imported directly from NumPy
+or SciPy*.
+
+Aliases for functions which may be accelerated by SciPy.
+
+SciPy_ can be built to use accelerated or otherwise improved libraries
+for FFTs, linear algebra, and special functions. This module allows
+developers to transparently support these accelerated functions when
+SciPy is available but still support users who have only installed
+NumPy.
+
+.. _SciPy : https://www.scipy.org
+
+"""
+import warnings
+
+
+warnings.warn('The module numpy.dual is deprecated.  Instead of using dual, '
+              'use the functions directly from numpy or scipy.',
+              category=DeprecationWarning,
+              stacklevel=2)
+
 # This module should be used for functions both in numpy and scipy if
 #  you want to use the numpy version if available but the scipy version
 #  otherwise.
 #  Usage  --- from numpy.dual import fft, inv
 
-__all__ = ['fft','ifft','fftn','ifftn','fft2','ifft2',
-           'norm','inv','svd','solve','det','eig','eigvals',
-           'eigh','eigvalsh','lstsq', 'pinv','cholesky','i0']
+__all__ = ['fft', 'ifft', 'fftn', 'ifftn', 'fft2', 'ifft2',
+           'norm', 'inv', 'svd', 'solve', 'det', 'eig', 'eigvals',
+           'eigh', 'eigvalsh', 'lstsq', 'pinv', 'cholesky', 'i0']
 
 import numpy.linalg as linpkg
-import numpy.dft as fftpkg
+import numpy.fft as fftpkg
 from numpy.lib import i0
 import sys
 
@@ -37,14 +63,14 @@
 
 def register_func(name, func):
     if name not in __all__:
-        raise ValueError, "%s not a dual function." % name
+        raise ValueError("{} not a dual function.".format(name))
     f = sys._getframe(0).f_globals
     _restore_dict[name] = f[name]
     f[name] = func
 
 def restore_func(name):
     if name not in __all__:
-        raise ValueError, "%s not a dual function." % name
+        raise ValueError("{} not a dual function.".format(name))
     try:
         val = _restore_dict[name]
     except KeyError:
('numpy', 'setup.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,26 +1,32 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 
 def configuration(parent_package='',top_path=None):
     from numpy.distutils.misc_util import Configuration
-    config = Configuration('numpy',parent_package,top_path)
+    config = Configuration('numpy', parent_package, top_path)
+
+    config.add_subpackage('array_api')
+    config.add_subpackage('compat')
+    config.add_subpackage('core')
     config.add_subpackage('distutils')
+    config.add_subpackage('doc')
+    config.add_subpackage('f2py')
+    config.add_subpackage('fft')
+    config.add_subpackage('lib')
+    config.add_subpackage('linalg')
+    config.add_subpackage('ma')
+    config.add_subpackage('matrixlib')
+    config.add_subpackage('polynomial')
+    config.add_subpackage('random')
     config.add_subpackage('testing')
-    config.add_subpackage('f2py')
-    config.add_subpackage('core')
-    config.add_subpackage('lib')
-    config.add_subpackage('dft')
-    config.add_subpackage('linalg')
-    config.add_subpackage('random')
+    config.add_subpackage('typing')
+    config.add_subpackage('_typing')
     config.add_data_dir('doc')
+    config.add_data_files('py.typed')
+    config.add_data_files('*.pyi')
+    config.add_subpackage('tests')
+    config.add_subpackage('_pyinstaller')
     config.make_config_py() # installs __config__.py
     return config
 
 if __name__ == '__main__':
-    # Remove current working directory from sys.path
-    # to avoid importing numpy.distutils as Python std. distutils:
-    import os, sys
-    for cwd in ['','.',os.getcwd()]:
-        while cwd in sys.path: sys.path.remove(cwd)
-
-    from numpy.distutils.core import setup
-    setup(configuration=configuration)
+    print('This is the wrong setup.py file to run')
('numpy/distutils', 'unixccompiler.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,45 +1,121 @@
 """
 unixccompiler - can handle very long argument lists for ar.
+
 """
-
 import os
 import sys
-import new
+import subprocess
+import shlex
 
-from distutils.errors import DistutilsExecError, LinkError, CompileError
-from distutils.unixccompiler import *
-
-
-import log
+from distutils.errors import CompileError, DistutilsExecError, LibError
+from distutils.unixccompiler import UnixCCompiler
+from numpy.distutils.ccompiler import replace_method
+from numpy.distutils.misc_util import _commandline_dep_string
+from numpy.distutils import log
 
 # Note that UnixCCompiler._compile appeared in Python 2.3
 def UnixCCompiler__compile(self, obj, src, ext, cc_args, extra_postargs, pp_opts):
-    display = '%s: %s' % (os.path.basename(self.compiler_so[0]),src)
+    """Compile a single source files with a Unix-style compiler."""
+    # HP ad-hoc fix, see ticket 1383
+    ccomp = self.compiler_so
+    if ccomp[0] == 'aCC':
+        # remove flags that will trigger ANSI-C mode for aCC
+        if '-Ae' in ccomp:
+            ccomp.remove('-Ae')
+        if '-Aa' in ccomp:
+            ccomp.remove('-Aa')
+        # add flags for (almost) sane C++ handling
+        ccomp += ['-AA']
+        self.compiler_so = ccomp
+    # ensure OPT environment variable is read
+    if 'OPT' in os.environ:
+        # XXX who uses this?
+        from sysconfig import get_config_vars
+        opt = shlex.join(shlex.split(os.environ['OPT']))
+        gcv_opt = shlex.join(shlex.split(get_config_vars('OPT')[0]))
+        ccomp_s = shlex.join(self.compiler_so)
+        if opt not in ccomp_s:
+            ccomp_s = ccomp_s.replace(gcv_opt, opt)
+            self.compiler_so = shlex.split(ccomp_s)
+        llink_s = shlex.join(self.linker_so)
+        if opt not in llink_s:
+            self.linker_so = self.linker_so + shlex.split(opt)
+
+    display = '%s: %s' % (os.path.basename(self.compiler_so[0]), src)
+
+    # gcc style automatic dependencies, outputs a makefile (-MF) that lists
+    # all headers needed by a c file as a side effect of compilation (-MMD)
+    if getattr(self, '_auto_depends', False):
+        deps = ['-MMD', '-MF', obj + '.d']
+    else:
+        deps = []
+
     try:
-        self.spawn(self.compiler_so + cc_args + [src, '-o', obj] +
+        self.spawn(self.compiler_so + cc_args + [src, '-o', obj] + deps +
                    extra_postargs, display = display)
-    except DistutilsExecError, msg:
-        raise CompileError, msg
-UnixCCompiler._compile = new.instancemethod(UnixCCompiler__compile,
-                                            None,
-                                            UnixCCompiler)
+    except DistutilsExecError as e:
+        msg = str(e)
+        raise CompileError(msg) from None
+
+    # add commandline flags to dependency file
+    if deps:
+        # After running the compiler, the file created will be in EBCDIC
+        # but will not be tagged as such. This tags it so the file does not
+        # have multiple different encodings being written to it
+        if sys.platform == 'zos':
+            subprocess.check_output(['chtag', '-tc', 'IBM1047', obj + '.d'])
+        with open(obj + '.d', 'a') as f:
+            f.write(_commandline_dep_string(cc_args, extra_postargs, pp_opts))
+
+replace_method(UnixCCompiler, '_compile', UnixCCompiler__compile)
 
 
-def UnixCCompile_create_static_lib(self, objects, output_libname,
-                                   output_dir=None, debug=0, target_lang=None):
+def UnixCCompiler_create_static_lib(self, objects, output_libname,
+                                    output_dir=None, debug=0, target_lang=None):
+    """
+    Build a static library in a separate sub-process.
+
+    Parameters
+    ----------
+    objects : list or tuple of str
+        List of paths to object files used to build the static library.
+    output_libname : str
+        The library name as an absolute or relative (if `output_dir` is used)
+        path.
+    output_dir : str, optional
+        The path to the output directory. Default is None, in which case
+        the ``output_dir`` attribute of the UnixCCompiler instance.
+    debug : bool, optional
+        This parameter is not used.
+    target_lang : str, optional
+        This parameter is not used.
+
+    Returns
+    -------
+    None
+
+    """
     objects, output_dir = self._fix_object_args(objects, output_dir)
 
     output_filename = \
                     self.library_filename(output_libname, output_dir=output_dir)
 
     if self._need_link(objects, output_filename):
+        try:
+            # previous .a may be screwed up; best to remove it first
+            # and recreate.
+            # Also, ar on OS X doesn't handle updating universal archives
+            os.unlink(output_filename)
+        except OSError:
+            pass
         self.mkpath(os.path.dirname(output_filename))
         tmp_objects = objects + self.objects
         while tmp_objects:
             objects = tmp_objects[:50]
             tmp_objects = tmp_objects[50:]
-            display = '%s: adding %d object files to %s' % (os.path.basename(self.archiver[0]),
-                                               len(objects),output_filename)
+            display = '%s: adding %d object files to %s' % (
+                           os.path.basename(self.archiver[0]),
+                           len(objects), output_filename)
             self.spawn(self.archiver + [output_filename] + objects,
                        display = display)
 
@@ -54,12 +130,12 @@
             try:
                 self.spawn(self.ranlib + [output_filename],
                            display = display)
-            except DistutilsExecError, msg:
-                raise LibError, msg
+            except DistutilsExecError as e:
+                msg = str(e)
+                raise LibError(msg) from None
     else:
         log.debug("skipping %s (up-to-date)", output_filename)
     return
 
-UnixCCompiler.create_static_lib = \
-  new.instancemethod(UnixCCompile_create_static_lib,
-                     None,UnixCCompiler)
+replace_method(UnixCCompiler, 'create_static_lib',
+               UnixCCompiler_create_static_lib)
('numpy/distutils', 'conv_template.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,45 +1,129 @@
-#!/usr/bin/python
-
-# takes templated file .xxx.src and produces .xxx file  where .xxx is .i or .c or .h
-#  using the following template rules
-
-# /**begin repeat     on a line by itself marks the beginning of a segment of code to be repeated
-# /**end repeat**/    on a line by itself marks it's end
-
-# after the /**begin repeat and before the */
-#  all the named templates are placed
-#  these should all have the same number of replacements
-
-#  in the main body, the names are used.
-#  Each replace will use one entry from the list of named replacements
-
-#  Note that all #..# forms in a block must have the same number of
-#    comma-separated entries.
+#!/usr/bin/env python3
+"""
+takes templated file .xxx.src and produces .xxx file  where .xxx is
+.i or .c or .h, using the following template rules
+
+/**begin repeat  -- on a line by itself marks the start of a repeated code
+                    segment
+/**end repeat**/ -- on a line by itself marks it's end
+
+After the /**begin repeat and before the */, all the named templates are placed
+these should all have the same number of replacements
+
+Repeat blocks can be nested, with each nested block labeled with its depth,
+i.e.
+/**begin repeat1
+ *....
+ */
+/**end repeat1**/
+
+When using nested loops, you can optionally exclude particular
+combinations of the variables using (inside the comment portion of the inner loop):
+
+ :exclude: var1=value1, var2=value2, ...
+
+This will exclude the pattern where var1 is value1 and var2 is value2 when
+the result is being generated.
+
+
+In the main body each replace will use one entry from the list of named replacements
+
+ Note that all #..# forms in a block must have the same number of
+   comma-separated entries.
+
+Example:
+
+    An input file containing
+
+        /**begin repeat
+         * #a = 1,2,3#
+         * #b = 1,2,3#
+         */
+
+        /**begin repeat1
+         * #c = ted, jim#
+         */
+        @a@, @b@, @c@
+        /**end repeat1**/
+
+        /**end repeat**/
+
+    produces
+
+        line 1 "template.c.src"
+
+        /*
+         *********************************************************************
+         **       This file was autogenerated from a template  DO NOT EDIT!!**
+         **       Changes should be made to the original source (.src) file **
+         *********************************************************************
+         */
+
+        #line 9
+        1, 1, ted
+
+        #line 9
+        1, 1, jim
+
+        #line 9
+        2, 2, ted
+
+        #line 9
+        2, 2, jim
+
+        #line 9
+        3, 3, ted
+
+        #line 9
+        3, 3, jim
+
+"""
 
 __all__ = ['process_str', 'process_file']
 
-import string,os,sys
-if sys.version[:3]>='2.3':
-    import re
-else:
-    import pre as re
-    False = 0
-    True = 1
-
-
-def parse_structure(astr):
+import os
+import sys
+import re
+
+# names for replacement that are already global.
+global_names = {}
+
+# header placed at the front of head processed file
+header =\
+"""
+/*
+ *****************************************************************************
+ **       This file was autogenerated from a template  DO NOT EDIT!!!!      **
+ **       Changes should be made to the original source (.src) file         **
+ *****************************************************************************
+ */
+
+"""
+# Parse string for repeat loops
+def parse_structure(astr, level):
+    """
+    The returned line number is from the beginning of the string, starting
+    at zero. Returns an empty list if no loops found.
+
+    """
+    if level == 0 :
+        loopbeg = "/**begin repeat"
+        loopend = "/**end repeat**/"
+    else :
+        loopbeg = "/**begin repeat%d" % level
+        loopend = "/**end repeat%d**/" % level
+
+    ind = 0
+    line = 0
     spanlist = []
-    # subroutines
-    ind = 0
-    line = 1
-    while 1:
-        start = astr.find("/**begin repeat", ind)
+    while True:
+        start = astr.find(loopbeg, ind)
         if start == -1:
             break
-        start2 = astr.find("*/",start)
-        start2 = astr.find("\n",start2)
-        fini1 = astr.find("/**end repeat**/",start2)
-        fini2 = astr.find("\n",fini1)
+        start2 = astr.find("*/", start)
+        start2 = astr.find("\n", start2)
+        fini1 = astr.find(loopend, start2)
+        fini2 = astr.find("\n", fini1)
         line += astr.count("\n", ind, start2+1)
         spanlist.append((start, start2+1, fini1, fini2+1, line))
         line += astr.count("\n", start2+1, fini2)
@@ -47,34 +131,169 @@
     spanlist.sort()
     return spanlist
 
-# return n copies of substr with template replacement
-_special_names = {}
-
-template_re = re.compile(r"@([\w]+)@")
-named_re = re.compile(r"#([\w]*)=([^#]*?)#")
-
-parenrep = re.compile(r"[(]([^)]*?)[)]\*(\d+)")
+
 def paren_repl(obj):
     torep = obj.group(1)
     numrep = obj.group(2)
     return ','.join([torep]*int(numrep))
 
+parenrep = re.compile(r"\(([^)]*)\)\*(\d+)")
 plainrep = re.compile(r"([^*]+)\*(\d+)")
-
-def conv(astr):
+def parse_values(astr):
     # replaces all occurrences of '(a,b,c)*4' in astr
-    #  with 'a,b,c,a,b,c,a,b,c,a,b,c'
-    astr = parenrep.sub(paren_repl,astr)
-    # replaces occurences of xxx*3 with xxx, xxx, xxx
-    astr = ','.join([plainrep.sub(paren_repl,x.strip()) for x in astr.split(',')])
-    return astr
+    # with 'a,b,c,a,b,c,a,b,c,a,b,c'. Empty braces generate
+    # empty values, i.e., ()*4 yields ',,,'. The result is
+    # split at ',' and a list of values returned.
+    astr = parenrep.sub(paren_repl, astr)
+    # replaces occurrences of xxx*3 with xxx, xxx, xxx
+    astr = ','.join([plainrep.sub(paren_repl, x.strip())
+                     for x in astr.split(',')])
+    return astr.split(',')
+
+
+stripast = re.compile(r"\n\s*\*?")
+named_re = re.compile(r"#\s*(\w*)\s*=([^#]*)#")
+exclude_vars_re = re.compile(r"(\w*)=(\w*)")
+exclude_re = re.compile(":exclude:")
+def parse_loop_header(loophead) :
+    """Find all named replacements in the header
+
+    Returns a list of dictionaries, one for each loop iteration,
+    where each key is a name to be substituted and the corresponding
+    value is the replacement string.
+
+    Also return a list of exclusions.  The exclusions are dictionaries
+     of key value pairs. There can be more than one exclusion.
+     [{'var1':'value1', 'var2', 'value2'[,...]}, ...]
+
+    """
+    # Strip out '\n' and leading '*', if any, in continuation lines.
+    # This should not effect code previous to this change as
+    # continuation lines were not allowed.
+    loophead = stripast.sub("", loophead)
+    # parse out the names and lists of values
+    names = []
+    reps = named_re.findall(loophead)
+    nsub = None
+    for rep in reps:
+        name = rep[0]
+        vals = parse_values(rep[1])
+        size = len(vals)
+        if nsub is None :
+            nsub = size
+        elif nsub != size :
+            msg = "Mismatch in number of values, %d != %d\n%s = %s"
+            raise ValueError(msg % (nsub, size, name, vals))
+        names.append((name, vals))
+
+
+    # Find any exclude variables
+    excludes = []
+
+    for obj in exclude_re.finditer(loophead):
+        span = obj.span()
+        # find next newline
+        endline = loophead.find('\n', span[1])
+        substr = loophead[span[1]:endline]
+        ex_names = exclude_vars_re.findall(substr)
+        excludes.append(dict(ex_names))
+
+    # generate list of dictionaries, one for each template iteration
+    dlist = []
+    if nsub is None :
+        raise ValueError("No substitution variables found")
+    for i in range(nsub):
+        tmp = {name: vals[i] for name, vals in names}
+        dlist.append(tmp)
+    return dlist
+
+replace_re = re.compile(r"@(\w+)@")
+def parse_string(astr, env, level, line) :
+    lineno = "#line %d\n" % line
+
+    # local function for string replacement, uses env
+    def replace(match):
+        name = match.group(1)
+        try :
+            val = env[name]
+        except KeyError:
+            msg = 'line %d: no definition of key "%s"'%(line, name)
+            raise ValueError(msg) from None
+        return val
+
+    code = [lineno]
+    struct = parse_structure(astr, level)
+    if struct :
+        # recurse over inner loops
+        oldend = 0
+        newlevel = level + 1
+        for sub in struct:
+            pref = astr[oldend:sub[0]]
+            head = astr[sub[0]:sub[1]]
+            text = astr[sub[1]:sub[2]]
+            oldend = sub[3]
+            newline = line + sub[4]
+            code.append(replace_re.sub(replace, pref))
+            try :
+                envlist = parse_loop_header(head)
+            except ValueError as e:
+                msg = "line %d: %s" % (newline, e)
+                raise ValueError(msg)
+            for newenv in envlist :
+                newenv.update(env)
+                newcode = parse_string(text, newenv, newlevel, newline)
+                code.extend(newcode)
+        suff = astr[oldend:]
+        code.append(replace_re.sub(replace, suff))
+    else :
+        # replace keys
+        code.append(replace_re.sub(replace, astr))
+    code.append('\n')
+    return ''.join(code)
+
+def process_str(astr):
+    code = [header]
+    code.extend(parse_string(astr, global_names, 0, 1))
+    return ''.join(code)
+
+
+include_src_re = re.compile(r"(\n|\A)#include\s*['\"]"
+                            r"(?P<name>[\w\d./\\]+[.]src)['\"]", re.I)
+
+def resolve_includes(source):
+    d = os.path.dirname(source)
+    with open(source) as fid:
+        lines = []
+        for line in fid:
+            m = include_src_re.match(line)
+            if m:
+                fn = m.group('name')
+                if not os.path.isabs(fn):
+                    fn = os.path.join(d, fn)
+                if os.path.isfile(fn):
+                    lines.extend(resolve_includes(fn))
+                else:
+                    lines.append(line)
+            else:
+                lines.append(line)
+    return lines
+
+def process_file(source):
+    lines = resolve_includes(source)
+    sourcefile = os.path.normcase(source).replace("\\", "\\\\")
+    try:
+        code = process_str(''.join(lines))
+    except ValueError as e:
+        raise ValueError('In "%s" loop at %s' % (sourcefile, e)) from None
+    return '#line 1 "%s"\n%s' % (sourcefile, code)
+
 
 def unique_key(adict):
     # this obtains a unique key given a dictionary
     # currently it works by appending together n of the letters of the
     #   current keys and increasing n until a unique key is found
     # -- not particularly quick
-    allkeys = adict.keys()
+    allkeys = list(adict.keys())
     done = False
     n = 1
     while not done:
@@ -85,123 +304,26 @@
             done = True
     return newkey
 
-def namerepl(match):
-    global _names, _thissub
-    name = match.group(1)
-    return _names[name][_thissub]
-
-def expand_sub(substr, namestr, line):
-    global _names, _thissub
-    # find all named replacements
-    reps = named_re.findall(namestr)
-    _names = {}
-    _names.update(_special_names)
-    numsubs = None
-    for rep in reps:
-        name = rep[0].strip()
-        thelist = conv(rep[1])
-        _names[name] = thelist
-
-    # make lists out of string entries in name dictionary
-    for name in _names.keys():
-        entry = _names[name]
-        entrylist = entry.split(',')
-        _names[name] = entrylist
-        num = len(entrylist)
-        if numsubs is None:
-            numsubs = num
-        elif (numsubs != num):
-            print namestr
-            print substr
-            raise ValueError, "Mismatch in number to replace"
-
-    # now replace all keys for each of the lists
-    mystr = ''
-    for k in range(numsubs):
-        _thissub = k
-        mystr += ("#line %d\n%s\n\n"
-                  % (line, template_re.sub(namerepl, substr)))
-    return mystr
-
-
-_head = \
-"""/*  This file was autogenerated from a template  DO NOT EDIT!!!!
-       Changes should be made to the original source (.src) file
-*/
-
-"""
-
-def get_line_header(str,beg):
-    extra = []
-    ind = beg-1
-    char = str[ind]
-    while (ind > 0) and (char != '\n'):
-        extra.insert(0,char)
-        ind = ind - 1
-        char = str[ind]
-    return ''.join(extra)
-
-def process_str(allstr):
-    newstr = allstr
-    writestr = _head
-
-    struct = parse_structure(newstr)
-    #  return a (sorted) list of tuples for each begin repeat section
-    #  each tuple is the start and end of a region to be template repeated
-
-    oldend = 0
-    for sub in struct:
-        writestr += newstr[oldend:sub[0]]
-        expanded = expand_sub(newstr[sub[1]:sub[2]],
-                              newstr[sub[0]:sub[1]], sub[4])
-        writestr += expanded
-        oldend =  sub[3]
-
-
-    writestr += newstr[oldend:]
-    return writestr
-
-include_src_re = re.compile(r"(\n|\A)#include\s*['\"]"
-                            r"(?P<name>[\w\d./\\]+[.]src)['\"]", re.I)
-
-def resolve_includes(source):
-    d = os.path.dirname(source)
-    fid = open(source)
-    lines = []
-    for line in fid.readlines():
-        m = include_src_re.match(line)
-        if m:
-            fn = m.group('name')
-            if not os.path.isabs(fn):
-                fn = os.path.join(d,fn)
-            if os.path.isfile(fn):
-                print 'Including file',fn
-                lines.extend(resolve_includes(fn))
-            else:
-                lines.append(line)
-        else:
-            lines.append(line)
-    fid.close()
-    return lines
-
-def process_file(source):
-    lines = resolve_includes(source)
-    return ('#line 1 "%s"\n%s'
-            % (source, process_str(''.join(lines))))
-
-if __name__ == "__main__":
-
+
+def main():
     try:
         file = sys.argv[1]
     except IndexError:
         fid = sys.stdin
         outfile = sys.stdout
     else:
-        fid = open(file,'r')
+        fid = open(file, 'r')
         (base, ext) = os.path.splitext(file)
         newname = base
-        outfile = open(newname,'w')
+        outfile = open(newname, 'w')
 
     allstr = fid.read()
-    writestr = process_str(allstr)
+    try:
+        writestr = process_str(allstr)
+    except ValueError as e:
+        raise ValueError("In %s loop at %s" % (file, e)) from None
+
     outfile.write(writestr)
+
+if __name__ == "__main__":
+    main()
('numpy/distutils', 'cpuinfo.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,79 +1,126 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 """
 cpuinfo
 
 Copyright 2002 Pearu Peterson all rights reserved,
 Pearu Peterson <pearu@cens.ioc.ee>
 Permission to use, modify, and distribute this software is given under the
-terms of the SciPy (BSD style) license.  See LICENSE.txt that came with
+terms of the NumPy (BSD style) license.  See LICENSE.txt that came with
 this distribution for specifics.
 
-Note:  This should be merged into proc at some point.  Perhaps proc should
-be returning classes like this instead of using dictionaries.
-
 NO WARRANTY IS EXPRESSED OR IMPLIED.  USE AT YOUR OWN RISK.
-$Revision: 1.1 $
-$Date: 2005/04/09 19:29:34 $
 Pearu Peterson
+
 """
-
-__version__ = "$Id: cpuinfo.py,v 1.1 2005/04/09 19:29:34 pearu Exp $"
-
 __all__ = ['cpu']
 
-import sys,string,re,types
-
-class cpuinfo_base:
+import os
+import platform
+import re
+import sys
+import types
+import warnings
+
+from subprocess import getstatusoutput
+
+
+def getoutput(cmd, successful_status=(0,), stacklevel=1):
+    try:
+        status, output = getstatusoutput(cmd)
+    except OSError as e:
+        warnings.warn(str(e), UserWarning, stacklevel=stacklevel)
+        return False, ""
+    if os.WIFEXITED(status) and os.WEXITSTATUS(status) in successful_status:
+        return True, output
+    return False, output
+
+def command_info(successful_status=(0,), stacklevel=1, **kw):
+    info = {}
+    for key in kw:
+        ok, output = getoutput(kw[key], successful_status=successful_status,
+                               stacklevel=stacklevel+1)
+        if ok:
+            info[key] = output.strip()
+    return info
+
+def command_by_line(cmd, successful_status=(0,), stacklevel=1):
+    ok, output = getoutput(cmd, successful_status=successful_status,
+                           stacklevel=stacklevel+1)
+    if not ok:
+        return
+    for line in output.splitlines():
+        yield line.strip()
+
+def key_value_from_command(cmd, sep, successful_status=(0,),
+                           stacklevel=1):
+    d = {}
+    for line in command_by_line(cmd, successful_status=successful_status,
+                                stacklevel=stacklevel+1):
+        l = [s.strip() for s in line.split(sep, 1)]
+        if len(l) == 2:
+            d[l[0]] = l[1]
+    return d
+
+class CPUInfoBase:
     """Holds CPU information and provides methods for requiring
     the availability of various CPU features.
     """
 
-    def _try_call(self,func):
+    def _try_call(self, func):
         try:
             return func()
-        except:
+        except Exception:
             pass
 
-    def __getattr__(self,name):
-        if name[0]!='_':
-            if hasattr(self,'_'+name):
-                attr = getattr(self,'_'+name)
-                if type(attr) is types.MethodType:
+    def __getattr__(self, name):
+        if not name.startswith('_'):
+            if hasattr(self, '_'+name):
+                attr = getattr(self, '_'+name)
+                if isinstance(attr, types.MethodType):
                     return lambda func=self._try_call,attr=attr : func(attr)
             else:
                 return lambda : None
-        raise AttributeError,name
+        raise AttributeError(name)
 
     def _getNCPUs(self):
         return 1
 
+    def __get_nbits(self):
+        abits = platform.architecture()[0]
+        nbits = re.compile(r'(\d+)bit').search(abits).group(1)
+        return nbits
+
     def _is_32bit(self):
-        return not self.is_64bit()
-
-class linux_cpuinfo(cpuinfo_base):
+        return self.__get_nbits() == '32'
+
+    def _is_64bit(self):
+        return self.__get_nbits() == '64'
+
+class LinuxCPUInfo(CPUInfoBase):
 
     info = None
 
     def __init__(self):
         if self.info is not None:
             return
-        info = []
+        info = [ {} ]
+        ok, output = getoutput('uname -m')
+        if ok:
+            info[0]['uname_m'] = output.strip()
         try:
-            for line in open('/proc/cpuinfo').readlines():
-                name_value = map(string.strip,string.split(line,':',1))
-                if len(name_value)!=2:
+            fo = open('/proc/cpuinfo')
+        except OSError as e:
+            warnings.warn(str(e), UserWarning, stacklevel=2)
+        else:
+            for line in fo:
+                name_value = [s.strip() for s in line.split(':', 1)]
+                if len(name_value) != 2:
                     continue
-                name,value = name_value
-                if not info or info[-1].has_key(name): # next processor
+                name, value = name_value
+                if not info or name in info[-1]: # next processor
                     info.append({})
                 info[-1][name] = value
-            import commands
-            status,output = commands.getstatusoutput('uname -m')
-            if not status:
-                if not info: info.append({})
-                info[-1]['uname_m'] = string.strip(output)
-        except:
-            print sys.exc_value,'(ignoring)'
+            fo.close()
         self.__class__.info = info
 
     def _not_impl(self): pass
@@ -90,15 +137,18 @@
         return self._is_AMD() and self.info[0]['model'] == '3'
 
     def _is_AthlonK6(self):
-        return re.match(r'.*?AMD-K6',self.info[0]['model name']) is not None
+        return re.match(r'.*?AMD-K6', self.info[0]['model name']) is not None
 
     def _is_AthlonK7(self):
-        return re.match(r'.*?AMD-K7',self.info[0]['model name']) is not None
+        return re.match(r'.*?AMD-K7', self.info[0]['model name']) is not None
 
     def _is_AthlonMP(self):
         return re.match(r'.*?Athlon\(tm\) MP\b',
                         self.info[0]['model name']) is not None
 
+    def _is_AMD64(self):
+        return self.is_AMD() and self.info[0]['family'] == '15'
+
     def _is_Athlon64(self):
         return re.match(r'.*?Athlon\(tm\) 64\b',
                         self.info[0]['model name']) is not None
@@ -185,7 +235,16 @@
         return self.is_PentiumIV() and self.has_sse3()
 
     def _is_Nocona(self):
-        return self.is_PentiumIV() and self.is_64bit()
+        return (self.is_Intel()
+                and (self.info[0]['cpu family'] == '6'
+                     or self.info[0]['cpu family'] == '15')
+                and (self.has_sse3() and not self.has_ssse3())
+                and re.match(r'.*?\blm\b', self.info[0]['flags']) is not None)
+
+    def _is_Core2(self):
+        return (self.is_64bit() and self.is_Intel() and
+                re.match(r'.*?Core\(TM\)2\b',
+                         self.info[0]['model name']) is not None)
 
     def _is_Itanium(self):
         return re.match(r'.*?Itanium\b',
@@ -193,7 +252,7 @@
 
     def _is_XEON(self):
         return re.match(r'.*?XEON\b',
-                        self.info[0]['model name'],re.IGNORECASE) is not None
+                        self.info[0]['model name'], re.IGNORECASE) is not None
 
     _is_Xeon = _is_XEON
 
@@ -212,73 +271,46 @@
         return self.info[0]['f00f_bug']=='yes'
 
     def _has_mmx(self):
-        return re.match(r'.*?\bmmx\b',self.info[0]['flags']) is not None
+        return re.match(r'.*?\bmmx\b', self.info[0]['flags']) is not None
 
     def _has_sse(self):
-        return re.match(r'.*?\bsse\b',self.info[0]['flags']) is not None
+        return re.match(r'.*?\bsse\b', self.info[0]['flags']) is not None
 
     def _has_sse2(self):
-        return re.match(r'.*?\bsse2\b',self.info[0]['flags']) is not None
+        return re.match(r'.*?\bsse2\b', self.info[0]['flags']) is not None
 
     def _has_sse3(self):
-        return re.match(r'.*?\bsse3\b',self.info[0]['flags']) is not None
+        return re.match(r'.*?\bpni\b', self.info[0]['flags']) is not None
+
+    def _has_ssse3(self):
+        return re.match(r'.*?\bssse3\b', self.info[0]['flags']) is not None
 
     def _has_3dnow(self):
-        return re.match(r'.*?\b3dnow\b',self.info[0]['flags']) is not None
+        return re.match(r'.*?\b3dnow\b', self.info[0]['flags']) is not None
 
     def _has_3dnowext(self):
-        return re.match(r'.*?\b3dnowext\b',self.info[0]['flags']) is not None
-
-    def _is_64bit(self):
-        if self.is_Alpha():
-            return True
-        if self.info[0].get('clflush size','')=='64':
-            return True
-        if self.info[0].get('uname_m','')=='x86_64':
-            return True
-        if self.info[0].get('arch','')=='IA-64':
-            return True
-        return False
-
-    def _is_32bit(self):
-        return not self.is_64bit()
-
-class irix_cpuinfo(cpuinfo_base):
-
+        return re.match(r'.*?\b3dnowext\b', self.info[0]['flags']) is not None
+
+class IRIXCPUInfo(CPUInfoBase):
     info = None
 
     def __init__(self):
         if self.info is not None:
             return
-        info = []
-        try:
-            import commands
-            status,output = commands.getstatusoutput('sysconf')
-            if status not in [0,256]:
-                return
-            for line in output.split('\n'):
-                name_value = map(string.strip,string.split(line,' ',1))
-                if len(name_value)!=2:
-                    continue
-                name,value = name_value
-                if not info:
-                    info.append({})
-                info[-1][name] = value
-        except:
-            print sys.exc_value,'(ignoring)'
+        info = key_value_from_command('sysconf', sep=' ',
+                                      successful_status=(0, 1))
         self.__class__.info = info
 
-        #print info
     def _not_impl(self): pass
 
     def _is_singleCPU(self):
-        return self.info[0].get('NUM_PROCESSORS') == '1'
+        return self.info.get('NUM_PROCESSORS') == '1'
 
     def _getNCPUs(self):
-        return int(self.info[0].get('NUM_PROCESSORS'))
-
-    def __cputype(self,n):
-        return self.info[0].get('PROCESSORS').split()[0].lower() == 'r%s' % (n)
+        return int(self.info.get('NUM_PROCESSORS', 1))
+
+    def __cputype(self, n):
+        return self.info.get('PROCESSORS').split()[0].lower() == 'r%s' % (n)
     def _is_r2000(self): return self.__cputype(2000)
     def _is_r3000(self): return self.__cputype(3000)
     def _is_r3900(self): return self.__cputype(3900)
@@ -296,10 +328,10 @@
     def _is_rorion(self): return self.__cputype('orion')
 
     def get_ip(self):
-        try: return self.info[0].get('MACHINE')
-        except: pass
-    def __machine(self,n):
-        return self.info[0].get('MACHINE').lower() == 'ip%s' % (n)
+        try: return self.info.get('MACHINE')
+        except Exception: pass
+    def __machine(self, n):
+        return self.info.get('MACHINE').lower() == 'ip%s' % (n)
     def _is_IP19(self): return self.__machine(19)
     def _is_IP20(self): return self.__machine(20)
     def _is_IP21(self): return self.__machine(21)
@@ -316,53 +348,33 @@
     def _is_IP32_5k(self): return self.__machine(32) and self._is_r5000()
     def _is_IP32_10k(self): return self.__machine(32) and self._is_r10000()
 
-class darwin_cpuinfo(cpuinfo_base):
-
+
+class DarwinCPUInfo(CPUInfoBase):
     info = None
 
     def __init__(self):
         if self.info is not None:
             return
-        info = []
-        try:
-            import commands
-            status,output = commands.getstatusoutput('arch')
-            if not status:
-                if not info: info.append({})
-                info[-1]['arch'] = string.strip(output)
-            status,output = commands.getstatusoutput('machine')
-            if not status:
-                if not info: info.append({})
-                info[-1]['machine'] = string.strip(output)
-            status,output = commands.getstatusoutput('sysctl hw')
-            if not status:
-                if not info: info.append({})
-                d = {}
-                for l in string.split(output,'\n'):
-                    l = map(string.strip,string.split(l, '='))
-                    if len(l)==2:
-                        d[l[0]]=l[1]
-                info[-1]['sysctl_hw'] = d
-        except:
-            print sys.exc_value,'(ignoring)'
+        info = command_info(arch='arch',
+                            machine='machine')
+        info['sysctl_hw'] = key_value_from_command('sysctl hw', sep='=')
         self.__class__.info = info
 
     def _not_impl(self): pass
 
     def _getNCPUs(self):
-        try: return int(self.info[0]['sysctl_hw']['hw.ncpu'])
-        except: return 1
+        return int(self.info['sysctl_hw'].get('hw.ncpu', 1))
 
     def _is_Power_Macintosh(self):
-        return self.info[0]['sysctl_hw']['hw.machine']=='Power Macintosh'
+        return self.info['sysctl_hw']['hw.machine']=='Power Macintosh'
 
     def _is_i386(self):
-        return self.info[0]['arch']=='i386'
+        return self.info['arch']=='i386'
     def _is_ppc(self):
-        return self.info[0]['arch']=='ppc'
-
-    def __machine(self,n):
-        return self.info[0]['machine'] == 'ppc%s'%n
+        return self.info['arch']=='ppc'
+
+    def __machine(self, n):
+        return self.info['machine'] == 'ppc%s'%n
     def _is_ppc601(self): return self.__machine(601)
     def _is_ppc602(self): return self.__machine(602)
     def _is_ppc603(self): return self.__machine(603)
@@ -382,120 +394,85 @@
     def _is_ppc823(self): return self.__machine(823)
     def _is_ppc860(self): return self.__machine(860)
 
-class sunos_cpuinfo(cpuinfo_base):
+
+class SunOSCPUInfo(CPUInfoBase):
 
     info = None
 
     def __init__(self):
         if self.info is not None:
             return
-        info = []
-        try:
-            import commands
-            status,output = commands.getstatusoutput('arch')
-            if not status:
-                if not info: info.append({})
-                info[-1]['arch'] = string.strip(output)
-            status,output = commands.getstatusoutput('mach')
-            if not status:
-                if not info: info.append({})
-                info[-1]['mach'] = string.strip(output)
-            status,output = commands.getstatusoutput('uname -i')
-            if not status:
-                if not info: info.append({})
-                info[-1]['uname_i'] = string.strip(output)
-            status,output = commands.getstatusoutput('uname -X')
-            if not status:
-                if not info: info.append({})
-                d = {}
-                for l in string.split(output,'\n'):
-                    l = map(string.strip,string.split(l, '='))
-                    if len(l)==2:
-                        d[l[0]]=l[1]
-                info[-1]['uname_X'] = d
-            status,output = commands.getstatusoutput('isainfo -b')
-            if not status:
-                if not info: info.append({})
-                info[-1]['isainfo_b'] = string.strip(output)
-            status,output = commands.getstatusoutput('isainfo -n')
-            if not status:
-                if not info: info.append({})
-                info[-1]['isainfo_n'] = string.strip(output)
-            status,output = commands.getstatusoutput('psrinfo -v 0')
-            if not status:
-                if not info: info.append({})
-                for l in string.split(output,'\n'):
-                    m = re.match(r'\s*The (?P<p>[\w\d]+) processor operates at',l)
-                    if m:
-                        info[-1]['processor'] = m.group('p')
-                        break
-        except:
-            print sys.exc_value,'(ignoring)'
+        info = command_info(arch='arch',
+                            mach='mach',
+                            uname_i='uname_i',
+                            isainfo_b='isainfo -b',
+                            isainfo_n='isainfo -n',
+                            )
+        info['uname_X'] = key_value_from_command('uname -X', sep='=')
+        for line in command_by_line('psrinfo -v 0'):
+            m = re.match(r'\s*The (?P<p>[\w\d]+) processor operates at', line)
+            if m:
+                info['processor'] = m.group('p')
+                break
         self.__class__.info = info
 
     def _not_impl(self): pass
 
-    def _is_32bit(self):
-        return self.info[0]['isainfo_b']=='32'
-    def _is_64bit(self):
-        return self.info[0]['isainfo_b']=='64'
-
     def _is_i386(self):
-        return self.info[0]['isainfo_n']=='i386'
+        return self.info['isainfo_n']=='i386'
     def _is_sparc(self):
-        return self.info[0]['isainfo_n']=='sparc'
+        return self.info['isainfo_n']=='sparc'
     def _is_sparcv9(self):
-        return self.info[0]['isainfo_n']=='sparcv9'
+        return self.info['isainfo_n']=='sparcv9'
 
     def _getNCPUs(self):
-        try: return int(self.info[0]['uname_X']['NumCPU'])
-        except: return 1
+        return int(self.info['uname_X'].get('NumCPU', 1))
 
     def _is_sun4(self):
-        return self.info[0]['arch']=='sun4'
+        return self.info['arch']=='sun4'
 
     def _is_SUNW(self):
-        return re.match(r'SUNW',self.info[0]['uname_i']) is not None
+        return re.match(r'SUNW', self.info['uname_i']) is not None
     def _is_sparcstation5(self):
-        return re.match(r'.*SPARCstation-5',self.info[0]['uname_i']) is not None
+        return re.match(r'.*SPARCstation-5', self.info['uname_i']) is not None
     def _is_ultra1(self):
-        return re.match(r'.*Ultra-1',self.info[0]['uname_i']) is not None
+        return re.match(r'.*Ultra-1', self.info['uname_i']) is not None
     def _is_ultra250(self):
-        return re.match(r'.*Ultra-250',self.info[0]['uname_i']) is not None
+        return re.match(r'.*Ultra-250', self.info['uname_i']) is not None
     def _is_ultra2(self):
-        return re.match(r'.*Ultra-2',self.info[0]['uname_i']) is not None
+        return re.match(r'.*Ultra-2', self.info['uname_i']) is not None
     def _is_ultra30(self):
-        return re.match(r'.*Ultra-30',self.info[0]['uname_i']) is not None
+        return re.match(r'.*Ultra-30', self.info['uname_i']) is not None
     def _is_ultra4(self):
-        return re.match(r'.*Ultra-4',self.info[0]['uname_i']) is not None
+        return re.match(r'.*Ultra-4', self.info['uname_i']) is not None
     def _is_ultra5_10(self):
-        return re.match(r'.*Ultra-5_10',self.info[0]['uname_i']) is not None
+        return re.match(r'.*Ultra-5_10', self.info['uname_i']) is not None
     def _is_ultra5(self):
-        return re.match(r'.*Ultra-5',self.info[0]['uname_i']) is not None
+        return re.match(r'.*Ultra-5', self.info['uname_i']) is not None
     def _is_ultra60(self):
-        return re.match(r'.*Ultra-60',self.info[0]['uname_i']) is not None
+        return re.match(r'.*Ultra-60', self.info['uname_i']) is not None
     def _is_ultra80(self):
-        return re.match(r'.*Ultra-80',self.info[0]['uname_i']) is not None
+        return re.match(r'.*Ultra-80', self.info['uname_i']) is not None
     def _is_ultraenterprice(self):
-        return re.match(r'.*Ultra-Enterprise',self.info[0]['uname_i']) is not None
+        return re.match(r'.*Ultra-Enterprise', self.info['uname_i']) is not None
     def _is_ultraenterprice10k(self):
-        return re.match(r'.*Ultra-Enterprise-10000',self.info[0]['uname_i']) is not None
+        return re.match(r'.*Ultra-Enterprise-10000', self.info['uname_i']) is not None
     def _is_sunfire(self):
-        return re.match(r'.*Sun-Fire',self.info[0]['uname_i']) is not None
+        return re.match(r'.*Sun-Fire', self.info['uname_i']) is not None
     def _is_ultra(self):
-        return re.match(r'.*Ultra',self.info[0]['uname_i']) is not None
+        return re.match(r'.*Ultra', self.info['uname_i']) is not None
 
     def _is_cpusparcv7(self):
-        return self.info[0]['processor']=='sparcv7'
+        return self.info['processor']=='sparcv7'
     def _is_cpusparcv8(self):
-        return self.info[0]['processor']=='sparcv8'
+        return self.info['processor']=='sparcv8'
     def _is_cpusparcv9(self):
-        return self.info[0]['processor']=='sparcv9'
-
-class win32_cpuinfo(cpuinfo_base):
+        return self.info['processor']=='sparcv9'
+
+class Win32CPUInfo(CPUInfoBase):
 
     info = None
-    pkey = "HARDWARE\\DESCRIPTION\\System\\CentralProcessor"
+    pkey = r"HARDWARE\DESCRIPTION\System\CentralProcessor"
     # XXX: what does the value of
     #   HKEY_LOCAL_MACHINE\HARDWARE\DESCRIPTION\System\CentralProcessor\0
     # mean?
@@ -506,27 +483,26 @@
         info = []
         try:
             #XXX: Bad style to use so long `try:...except:...`. Fix it!
-            import _winreg
-            pkey = "HARDWARE\\DESCRIPTION\\System\\CentralProcessor"
-            prgx = re.compile(r"family\s+(?P<FML>\d+)\s+model\s+(?P<MDL>\d+)"\
-                              "\s+stepping\s+(?P<STP>\d+)",re.IGNORECASE)
-            chnd=_winreg.OpenKey(_winreg.HKEY_LOCAL_MACHINE,pkey)
+            import winreg
+
+            prgx = re.compile(r"family\s+(?P<FML>\d+)\s+model\s+(?P<MDL>\d+)"
+                              r"\s+stepping\s+(?P<STP>\d+)", re.IGNORECASE)
+            chnd=winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, self.pkey)
             pnum=0
-            while 1:
+            while True:
                 try:
-                    proc=_winreg.EnumKey(chnd,pnum)
-                except _winreg.error:
+                    proc=winreg.EnumKey(chnd, pnum)
+                except winreg.error:
                     break
                 else:
                     pnum+=1
-                    print proc
                     info.append({"Processor":proc})
-                    phnd=_winreg.OpenKey(chnd,proc)
+                    phnd=winreg.OpenKey(chnd, proc)
                     pidx=0
                     while True:
                         try:
-                            name,value,vtpe=_winreg.EnumValue(phnd,pidx)
-                        except _winreg.error:
+                            name, value, vtpe=winreg.EnumValue(phnd, pidx)
+                        except winreg.error:
                             break
                         else:
                             pidx=pidx+1
@@ -537,8 +513,8 @@
                                     info[-1]["Family"]=int(srch.group("FML"))
                                     info[-1]["Model"]=int(srch.group("MDL"))
                                     info[-1]["Stepping"]=int(srch.group("STP"))
-        except:
-            print sys.exc_value,'(ignoring)'
+        except Exception as e:
+            print(e, '(ignoring)')
         self.__class__.info = info
 
     def _not_impl(self): pass
@@ -556,11 +532,11 @@
 
     def _is_AMDK5(self):
         return self.is_AMD() and self.info[0]['Family']==5 \
-               and self.info[0]['Model'] in [0,1,2,3]
+               and self.info[0]['Model'] in [0, 1, 2, 3]
 
     def _is_AMDK6(self):
         return self.is_AMD() and self.info[0]['Family']==5 \
-               and self.info[0]['Model'] in [6,7]
+               and self.info[0]['Model'] in [6, 7]
 
     def _is_AMDK6_2(self):
         return self.is_AMD() and self.info[0]['Family']==5 \
@@ -570,16 +546,15 @@
         return self.is_AMD() and self.info[0]['Family']==5 \
                and self.info[0]['Model']==9
 
-    def _is_Athlon(self):
-        return self.is_AMD() and self.info[0]['Family']==6
-
-    def _is_Athlon64(self):
-        return self.is_AMD() and self.info[0]['Family']==15 \
-               and self.info[0]['Model']==4
-
-    def _is_Opteron(self):
-        return self.is_AMD() and self.info[0]['Family']==15 \
-               and self.info[0]['Model']==5
+    def _is_AMDK7(self):
+        return self.is_AMD() and self.info[0]['Family'] == 6
+
+    # To reliably distinguish between the different types of AMD64 chips
+    # (Athlon64, Operton, Athlon64 X2, Semperon, Turion 64, etc.) would
+    # require looking at the 'brand' from cpuid
+
+    def _is_AMD64(self):
+        return self.is_AMD() and self.info[0]['Family'] == 15
 
     # Intel
 
@@ -611,14 +586,22 @@
 
     def _is_PentiumII(self):
         return self.is_Intel() and self.info[0]['Family']==6 \
-               and self.info[0]['Model'] in [3,5,6]
+               and self.info[0]['Model'] in [3, 5, 6]
 
     def _is_PentiumIII(self):
         return self.is_Intel() and self.info[0]['Family']==6 \
-               and self.info[0]['Model'] in [7,8,9,10,11]
+               and self.info[0]['Model'] in [7, 8, 9, 10, 11]
 
     def _is_PentiumIV(self):
         return self.is_Intel() and self.info[0]['Family']==15
+
+    def _is_PentiumM(self):
+        return self.is_Intel() and self.info[0]['Family'] == 6 \
+               and self.info[0]['Model'] in [9, 13, 14]
+
+    def _is_Core2(self):
+        return self.is_Intel() and self.info[0]['Family'] == 6 \
+               and self.info[0]['Model'] in [15, 16, 17]
 
     # Varia
 
@@ -631,61 +614,70 @@
     def _has_mmx(self):
         if self.is_Intel():
             return (self.info[0]['Family']==5 and self.info[0]['Model']==4) \
-                   or (self.info[0]['Family'] in [6,15])
+                   or (self.info[0]['Family'] in [6, 15])
         elif self.is_AMD():
-            return self.info[0]['Family'] in [5,6,15]
+            return self.info[0]['Family'] in [5, 6, 15]
+        else:
+            return False
 
     def _has_sse(self):
         if self.is_Intel():
-            return (self.info[0]['Family']==6 and \
-                    self.info[0]['Model'] in [7,8,9,10,11]) \
-                    or self.info[0]['Family']==15
+            return ((self.info[0]['Family']==6 and
+                     self.info[0]['Model'] in [7, 8, 9, 10, 11])
+                     or self.info[0]['Family']==15)
         elif self.is_AMD():
-            return (self.info[0]['Family']==6 and \
-                    self.info[0]['Model'] in [6,7,8,10]) \
-                    or self.info[0]['Family']==15
+            return ((self.info[0]['Family']==6 and
+                     self.info[0]['Model'] in [6, 7, 8, 10])
+                     or self.info[0]['Family']==15)
+        else:
+            return False
 
     def _has_sse2(self):
-        return self.info[0]['Family']==15
+        if self.is_Intel():
+            return self.is_Pentium4() or self.is_PentiumM() \
+                   or self.is_Core2()
+        elif self.is_AMD():
+            return self.is_AMD64()
+        else:
+            return False
 
     def _has_3dnow(self):
-        # XXX: does only AMD have 3dnow??
-        return self.is_AMD() and self.info[0]['Family'] in [5,6,15]
+        return self.is_AMD() and self.info[0]['Family'] in [5, 6, 15]
 
     def _has_3dnowext(self):
-        return self.is_AMD() and self.info[0]['Family'] in [6,15]
-
-if sys.platform[:5] == 'linux': # variations: linux2,linux-i386 (any others?)
-    cpuinfo = linux_cpuinfo
-elif sys.platform[:4] == 'irix':
-    cpuinfo = irix_cpuinfo
+        return self.is_AMD() and self.info[0]['Family'] in [6, 15]
+
+if sys.platform.startswith('linux'): # variations: linux2,linux-i386 (any others?)
+    cpuinfo = LinuxCPUInfo
+elif sys.platform.startswith('irix'):
+    cpuinfo = IRIXCPUInfo
 elif sys.platform == 'darwin':
-    cpuinfo = darwin_cpuinfo
-elif sys.platform[:5] == 'sunos':
-    cpuinfo = sunos_cpuinfo
-elif sys.platform[:5] == 'win32':
-    cpuinfo = win32_cpuinfo
-elif sys.platform[:6] == 'cygwin':
-    cpuinfo = linux_cpuinfo
+    cpuinfo = DarwinCPUInfo
+elif sys.platform.startswith('sunos'):
+    cpuinfo = SunOSCPUInfo
+elif sys.platform.startswith('win32'):
+    cpuinfo = Win32CPUInfo
+elif sys.platform.startswith('cygwin'):
+    cpuinfo = LinuxCPUInfo
 #XXX: other OS's. Eg. use _winreg on Win32. Or os.uname on unices.
 else:
-    cpuinfo = cpuinfo_base
+    cpuinfo = CPUInfoBase
 
 cpu = cpuinfo()
 
-if __name__ == "__main__":
-
-    cpu.is_blaa()
-    cpu.is_Intel()
-    cpu.is_Alpha()
-
-    print 'CPU information:',
-    for name in dir(cpuinfo):
-        if name[0]=='_' and name[1]!='_':
-            r = getattr(cpu,name[1:])()
-            if r:
-                if r!=1:
-                    print '%s=%s' %(name[1:],r),
-                else:
-                    print name[1:],
-    print
+#if __name__ == "__main__":
+#
+#    cpu.is_blaa()
+#    cpu.is_Intel()
+#    cpu.is_Alpha()
+#
+#    print('CPU information:'),
+#    for name in dir(cpuinfo):
+#        if name[0]=='_' and name[1]!='_':
+#            r = getattr(cpu,name[1:])()
+#            if r:
+#                if r!=1:
+#                    print('%s=%s' %(name[1:],r))
+#                else:
+#                    print(name[1:]),
+#    print()
('numpy/distutils', 'ccompiler.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,45 +1,210 @@
+import os
 import re
-import os
 import sys
-import new
-
-from distutils.ccompiler import *
+import shlex
+import time
+import subprocess
+from copy import copy
 from distutils import ccompiler
+from distutils.ccompiler import (
+    compiler_class, gen_lib_options, get_default_compiler, new_compiler,
+    CCompiler
+)
+from distutils.errors import (
+    DistutilsExecError, DistutilsModuleError, DistutilsPlatformError,
+    CompileError, UnknownFileError
+)
 from distutils.sysconfig import customize_compiler
 from distutils.version import LooseVersion
 
-import log
-from exec_command import exec_command
-from misc_util import cyg2win32, is_sequence, mingw32
-from distutils.spawn import _nt_quote_args
-
-# hack to set compiler optimizing options. Needs to integrated with something.
-import distutils.sysconfig
-_old_init_posix = distutils.sysconfig._init_posix
-def _new_init_posix():
-    _old_init_posix()
-    distutils.sysconfig._config_vars['OPT'] = '-Wall -g -O2'
-#distutils.sysconfig._init_posix = _new_init_posix
+from numpy.distutils import log
+from numpy.distutils.exec_command import (
+    filepath_from_subprocess_output, forward_bytes_to_stdout
+)
+from numpy.distutils.misc_util import cyg2win32, is_sequence, mingw32, \
+                                      get_num_build_jobs, \
+                                      _commandline_dep_string, \
+                                      sanitize_cxx_flags
+
+# globals for parallel build management
+import threading
+
+_job_semaphore = None
+_global_lock = threading.Lock()
+_processing_files = set()
+
+
+def _needs_build(obj, cc_args, extra_postargs, pp_opts):
+    """
+    Check if an objects needs to be rebuild based on its dependencies
+
+    Parameters
+    ----------
+    obj : str
+        object file
+
+    Returns
+    -------
+    bool
+    """
+    # defined in unixcompiler.py
+    dep_file = obj + '.d'
+    if not os.path.exists(dep_file):
+        return True
+
+    # dep_file is a makefile containing 'object: dependencies'
+    # formatted like posix shell (spaces escaped, \ line continuations)
+    # the last line contains the compiler commandline arguments as some
+    # projects may compile an extension multiple times with different
+    # arguments
+    with open(dep_file, "r") as f:
+        lines = f.readlines()
+
+    cmdline =_commandline_dep_string(cc_args, extra_postargs, pp_opts)
+    last_cmdline = lines[-1]
+    if last_cmdline != cmdline:
+        return True
+
+    contents = ''.join(lines[:-1])
+    deps = [x for x in shlex.split(contents, posix=True)
+            if x != "\n" and not x.endswith(":")]
+
+    try:
+        t_obj = os.stat(obj).st_mtime
+
+        # check if any of the dependencies is newer than the object
+        # the dependencies includes the source used to create the object
+        for f in deps:
+            if os.stat(f).st_mtime > t_obj:
+                return True
+    except OSError:
+        # no object counts as newer (shouldn't happen if dep_file exists)
+        return True
+
+    return False
+
+
+def replace_method(klass, method_name, func):
+    # Py3k does not have unbound method anymore, MethodType does not work
+    m = lambda self, *args, **kw: func(self, *args, **kw)
+    setattr(klass, method_name, m)
+
+
+######################################################################
+## Method that subclasses may redefine. But don't call this method,
+## it i private to CCompiler class and may return unexpected
+## results if used elsewhere. So, you have been warned..
+
+def CCompiler_find_executables(self):
+    """
+    Does nothing here, but is called by the get_version method and can be
+    overridden by subclasses. In particular it is redefined in the `FCompiler`
+    class where more documentation can be found.
+
+    """
+    pass
+
+
+replace_method(CCompiler, 'find_executables', CCompiler_find_executables)
+
 
 # Using customized CCompiler.spawn.
-def CCompiler_spawn(self, cmd, display=None):
+def CCompiler_spawn(self, cmd, display=None, env=None):
+    """
+    Execute a command in a sub-process.
+
+    Parameters
+    ----------
+    cmd : str
+        The command to execute.
+    display : str or sequence of str, optional
+        The text to add to the log file kept by `numpy.distutils`.
+        If not given, `display` is equal to `cmd`.
+    env : a dictionary for environment variables, optional
+
+    Returns
+    -------
+    None
+
+    Raises
+    ------
+    DistutilsExecError
+        If the command failed, i.e. the exit status was not 0.
+
+    """
+    env = env if env is not None else dict(os.environ)
     if display is None:
         display = cmd
         if is_sequence(display):
             display = ' '.join(list(display))
     log.info(display)
-    if is_sequence(cmd) and os.name == 'nt':
-        cmd = _nt_quote_args(list(cmd))
-    s,o = exec_command(cmd)
-    if s:
-        if is_sequence(cmd):
-            cmd = ' '.join(list(cmd))
-        print o
-        raise DistutilsExecError,\
-              'Command "%s" failed with exit status %d' % (cmd, s)
-CCompiler.spawn = new.instancemethod(CCompiler_spawn,None,CCompiler)
+    try:
+        if self.verbose:
+            subprocess.check_output(cmd, env=env)
+        else:
+            subprocess.check_output(cmd, stderr=subprocess.STDOUT, env=env)
+    except subprocess.CalledProcessError as exc:
+        o = exc.output
+        s = exc.returncode
+    except OSError as e:
+        # OSError doesn't have the same hooks for the exception
+        # output, but exec_command() historically would use an
+        # empty string for EnvironmentError (base class for
+        # OSError)
+        # o = b''
+        # still that would make the end-user lost in translation!
+        o = f"\n\n{e}\n\n\n"
+        try:
+            o = o.encode(sys.stdout.encoding)
+        except AttributeError:
+            o = o.encode('utf8')
+        # status previously used by exec_command() for parent
+        # of OSError
+        s = 127
+    else:
+        # use a convenience return here so that any kind of
+        # caught exception will execute the default code after the
+        # try / except block, which handles various exceptions
+        return None
+
+    if is_sequence(cmd):
+        cmd = ' '.join(list(cmd))
+
+    if self.verbose:
+        forward_bytes_to_stdout(o)
+
+    if re.search(b'Too many open files', o):
+        msg = '\nTry rerunning setup command until build succeeds.'
+    else:
+        msg = ''
+    raise DistutilsExecError('Command "%s" failed with exit status %d%s' %
+                            (cmd, s, msg))
+
+replace_method(CCompiler, 'spawn', CCompiler_spawn)
 
 def CCompiler_object_filenames(self, source_filenames, strip_dir=0, output_dir=''):
+    """
+    Return the name of the object files for the given source files.
+
+    Parameters
+    ----------
+    source_filenames : list of str
+        The list of paths to source files. Paths can be either relative or
+        absolute, this is handled transparently.
+    strip_dir : bool, optional
+        Whether to strip the directory from the returned paths. If True,
+        the file name prepended by `output_dir` is returned. Default is False.
+    output_dir : str, optional
+        If given, this path is prepended to the returned paths to the
+        object files.
+
+    Returns
+    -------
+    obj_names : list of str
+        The list of paths to the object files corresponding to the source
+        files in `source_filenames`.
+
+    """
     if output_dir is None:
         output_dir = ''
     obj_names = []
@@ -55,40 +220,78 @@
             d = os.path.basename(os.path.abspath(d))
             base = d + base[i:]
         if ext not in self.src_extensions:
-            raise UnknownFileError, \
-                  "unknown file type '%s' (from '%s')" % (ext, src_name)
+            raise UnknownFileError("unknown file type '%s' (from '%s')" % (ext, src_name))
         if strip_dir:
             base = os.path.basename(base)
-        obj_name = os.path.join(output_dir,base + self.obj_extension)
+        obj_name = os.path.join(output_dir, base + self.obj_extension)
         obj_names.append(obj_name)
     return obj_names
 
-CCompiler.object_filenames = new.instancemethod(CCompiler_object_filenames,
-                                                None,CCompiler)
+replace_method(CCompiler, 'object_filenames', CCompiler_object_filenames)
 
 def CCompiler_compile(self, sources, output_dir=None, macros=None,
                       include_dirs=None, debug=0, extra_preargs=None,
                       extra_postargs=None, depends=None):
-    # This method is effective only with Python >=2.3 distutils.
-    # Any changes here should be applied also to fcompiler.compile
-    # method to support pre Python 2.3 distutils.
+    """
+    Compile one or more source files.
+
+    Please refer to the Python distutils API reference for more details.
+
+    Parameters
+    ----------
+    sources : list of str
+        A list of filenames
+    output_dir : str, optional
+        Path to the output directory.
+    macros : list of tuples
+        A list of macro definitions.
+    include_dirs : list of str, optional
+        The directories to add to the default include file search path for
+        this compilation only.
+    debug : bool, optional
+        Whether or not to output debug symbols in or alongside the object
+        file(s).
+    extra_preargs, extra_postargs : ?
+        Extra pre- and post-arguments.
+    depends : list of str, optional
+        A list of file names that all targets depend on.
+
+    Returns
+    -------
+    objects : list of str
+        A list of object file names, one per source file `sources`.
+
+    Raises
+    ------
+    CompileError
+        If compilation fails.
+
+    """
+    global _job_semaphore
+
+    jobs = get_num_build_jobs()
+
+    # setup semaphore to not exceed number of compile jobs when parallelized at
+    # extension level (python >= 3.5)
+    with _global_lock:
+        if _job_semaphore is None:
+            _job_semaphore = threading.Semaphore(jobs)
+
     if not sources:
         return []
-    from fcompiler import FCompiler
+    from numpy.distutils.fcompiler import (FCompiler, is_f_file,
+                                           has_f90_header)
     if isinstance(self, FCompiler):
         display = []
-        for fc in ['f77','f90','fix']:
-            fcomp = getattr(self,'compiler_'+fc)
+        for fc in ['f77', 'f90', 'fix']:
+            fcomp = getattr(self, 'compiler_'+fc)
             if fcomp is None:
                 continue
-            display.append("%s(%s) options: '%s'" % (os.path.basename(fcomp[0]),
-                                                     fc,
-                                                     ' '.join(fcomp[1:])))
+            display.append("Fortran %s compiler: %s" % (fc, ' '.join(fcomp)))
         display = '\n'.join(display)
     else:
         ccomp = self.compiler_so
-        display = "%s options: '%s'" % (os.path.basename(ccomp[0]),
-                                        ' '.join(ccomp[1:]))
+        display = "C compiler: %s\n" % (' '.join(ccomp),)
     log.info(display)
     macros, objects, extra_postargs, pp_opts, build = \
             self._setup_compile(output_dir, macros, include_dirs, sources,
@@ -99,122 +302,290 @@
         display += "\nextra options: '%s'" % (' '.join(extra_postargs))
     log.info(display)
 
-    # build any sources in same order as they were originally specified
-    #   especially important for fortran .f90 files using modules
+    def single_compile(args):
+        obj, (src, ext) = args
+        if not _needs_build(obj, cc_args, extra_postargs, pp_opts):
+            return
+
+        # check if we are currently already processing the same object
+        # happens when using the same source in multiple extensions
+        while True:
+            # need explicit lock as there is no atomic check and add with GIL
+            with _global_lock:
+                # file not being worked on, start working
+                if obj not in _processing_files:
+                    _processing_files.add(obj)
+                    break
+            # wait for the processing to end
+            time.sleep(0.1)
+
+        try:
+            # retrieve slot from our #job semaphore and build
+            with _job_semaphore:
+                self._compile(obj, src, ext, cc_args, extra_postargs, pp_opts)
+        finally:
+            # register being done processing
+            with _global_lock:
+                _processing_files.remove(obj)
+
+
     if isinstance(self, FCompiler):
-        objects_to_build = build.keys()
+        objects_to_build = list(build.keys())
+        f77_objects, other_objects = [], []
         for obj in objects:
             if obj in objects_to_build:
                 src, ext = build[obj]
                 if self.compiler_type=='absoft':
                     obj = cyg2win32(obj)
                     src = cyg2win32(src)
-                self._compile(obj, src, ext, cc_args, extra_postargs, pp_opts)
+                if is_f_file(src) and not has_f90_header(src):
+                    f77_objects.append((obj, (src, ext)))
+                else:
+                    other_objects.append((obj, (src, ext)))
+
+        # f77 objects can be built in parallel
+        build_items = f77_objects
+        # build f90 modules serial, module files are generated during
+        # compilation and may be used by files later in the list so the
+        # ordering is important
+        for o in other_objects:
+            single_compile(o)
     else:
-        for obj, (src, ext) in build.items():
-            self._compile(obj, src, ext, cc_args, extra_postargs, pp_opts)
+        build_items = build.items()
+
+    if len(build) > 1 and jobs > 1:
+        # build parallel
+        from concurrent.futures import ThreadPoolExecutor
+        with ThreadPoolExecutor(jobs) as pool:
+            res = pool.map(single_compile, build_items)
+        list(res)  # access result to raise errors
+    else:
+        # build serial
+        for o in build_items:
+            single_compile(o)
 
     # Return *all* object filenames, not just the ones we just built.
     return objects
 
-CCompiler.compile = new.instancemethod(CCompiler_compile,None,CCompiler)
-
-def CCompiler_customize_cmd(self, cmd):
-    """ Customize compiler using distutils command.
+replace_method(CCompiler, 'compile', CCompiler_compile)
+
+def CCompiler_customize_cmd(self, cmd, ignore=()):
+    """
+    Customize compiler using distutils command.
+
+    Parameters
+    ----------
+    cmd : class instance
+        An instance inheriting from `distutils.cmd.Command`.
+    ignore : sequence of str, optional
+        List of `CCompiler` commands (without ``'set_'``) that should not be
+        altered. Strings that are checked for are:
+        ``('include_dirs', 'define', 'undef', 'libraries', 'library_dirs',
+        'rpath', 'link_objects')``.
+
+    Returns
+    -------
+    None
+
     """
     log.info('customize %s using %s' % (self.__class__.__name__,
                                         cmd.__class__.__name__))
-    if getattr(cmd,'include_dirs',None) is not None:
+
+    if hasattr(self, 'compiler') and 'clang' in self.compiler[0]:
+        # clang defaults to a non-strict floating error point model.
+        # Since NumPy and most Python libs give warnings for these, override:
+        self.compiler.append('-ftrapping-math')
+        self.compiler_so.append('-ftrapping-math')
+
+    def allow(attr):
+        return getattr(cmd, attr, None) is not None and attr not in ignore
+
+    if allow('include_dirs'):
         self.set_include_dirs(cmd.include_dirs)
-    if getattr(cmd,'define',None) is not None:
-        for (name,value) in cmd.define:
+    if allow('define'):
+        for (name, value) in cmd.define:
             self.define_macro(name, value)
-    if getattr(cmd,'undef',None) is not None:
+    if allow('undef'):
         for macro in cmd.undef:
             self.undefine_macro(macro)
-    if getattr(cmd,'libraries',None) is not None:
+    if allow('libraries'):
         self.set_libraries(self.libraries + cmd.libraries)
-    if getattr(cmd,'library_dirs',None) is not None:
+    if allow('library_dirs'):
         self.set_library_dirs(self.library_dirs + cmd.library_dirs)
-    if getattr(cmd,'rpath',None) is not None:
+    if allow('rpath'):
         self.set_runtime_library_dirs(cmd.rpath)
-    if getattr(cmd,'link_objects',None) is not None:
+    if allow('link_objects'):
         self.set_link_objects(cmd.link_objects)
-    return
-
-CCompiler.customize_cmd = new.instancemethod(\
-    CCompiler_customize_cmd,None,CCompiler)
+
+replace_method(CCompiler, 'customize_cmd', CCompiler_customize_cmd)
 
 def _compiler_to_string(compiler):
     props = []
     mx = 0
-    keys = compiler.executables.keys()
-    for key in ['version','libraries','library_dirs',
-                'object_switch','compile_switch',
-                'include_dirs','define','undef','rpath','link_objects']:
+    keys = list(compiler.executables.keys())
+    for key in ['version', 'libraries', 'library_dirs',
+                'object_switch', 'compile_switch',
+                'include_dirs', 'define', 'undef', 'rpath', 'link_objects']:
         if key not in keys:
             keys.append(key)
     for key in keys:
-        if hasattr(compiler,key):
+        if hasattr(compiler, key):
             v = getattr(compiler, key)
-            mx = max(mx,len(key))
-            props.append((key,repr(v)))
-    lines = []
-    format = '%-' + repr(mx+1) + 's = %s'
-    for prop in props:
-        lines.append(format % prop)
+            mx = max(mx, len(key))
+            props.append((key, repr(v)))
+    fmt = '%-' + repr(mx+1) + 's = %s'
+    lines = [fmt % prop for prop in props]
     return '\n'.join(lines)
 
 def CCompiler_show_customization(self):
-    if 0:
-        for attrname in ['include_dirs','define','undef',
-                         'libraries','library_dirs',
-                         'rpath','link_objects']:
-            attr = getattr(self,attrname,None)
-            if not attr:
-                continue
-            log.info("compiler '%s' is set to %s" % (attrname,attr))
-    try: self.get_version()
-    except: pass
+    """
+    Print the compiler customizations to stdout.
+
+    Parameters
+    ----------
+    None
+
+    Returns
+    -------
+    None
+
+    Notes
+    -----
+    Printing is only done if the distutils log threshold is < 2.
+
+    """
+    try:
+        self.get_version()
+    except Exception:
+        pass
     if log._global_log.threshold<2:
-        print '*'*80
-        print self.__class__
-        print _compiler_to_string(self)
-        print '*'*80
-
-CCompiler.show_customization = new.instancemethod(\
-    CCompiler_show_customization,None,CCompiler)
-
+        print('*'*80)
+        print(self.__class__)
+        print(_compiler_to_string(self))
+        print('*'*80)
+
+replace_method(CCompiler, 'show_customization', CCompiler_show_customization)
 
 def CCompiler_customize(self, dist, need_cxx=0):
+    """
+    Do any platform-specific customization of a compiler instance.
+
+    This method calls `distutils.sysconfig.customize_compiler` for
+    platform-specific customization, as well as optionally remove a flag
+    to suppress spurious warnings in case C++ code is being compiled.
+
+    Parameters
+    ----------
+    dist : object
+        This parameter is not used for anything.
+    need_cxx : bool, optional
+        Whether or not C++ has to be compiled. If so (True), the
+        ``"-Wstrict-prototypes"`` option is removed to prevent spurious
+        warnings. Default is False.
+
+    Returns
+    -------
+    None
+
+    Notes
+    -----
+    All the default options used by distutils can be extracted with::
+
+      from distutils import sysconfig
+      sysconfig.get_config_vars('CC', 'CXX', 'OPT', 'BASECFLAGS',
+                                'CCSHARED', 'LDSHARED', 'SO')
+
+    """
     # See FCompiler.customize for suggested usage.
     log.info('customize %s' % (self.__class__.__name__))
     customize_compiler(self)
     if need_cxx:
-        if hasattr(self,'compiler') and self.compiler[0].find('gcc')>=0:
-            if sys.version[:3]>='2.3':
-                if not self.compiler_cxx:
-                    self.compiler_cxx = [self.compiler[0].replace('gcc','g++')]\
-                                        + self.compiler[1:]
-            else:
-                self.compiler_cxx = [self.compiler[0].replace('gcc','g++')]\
+        # In general, distutils uses -Wstrict-prototypes, but this option is
+        # not valid for C++ code, only for C.  Remove it if it's there to
+        # avoid a spurious warning on every compilation.
+        try:
+            self.compiler_so.remove('-Wstrict-prototypes')
+        except (AttributeError, ValueError):
+            pass
+
+        if hasattr(self, 'compiler') and 'cc' in self.compiler[0]:
+            if not self.compiler_cxx:
+                if self.compiler[0].startswith('gcc'):
+                    a, b = 'gcc', 'g++'
+                else:
+                    a, b = 'cc', 'c++'
+                self.compiler_cxx = [self.compiler[0].replace(a, b)]\
                                     + self.compiler[1:]
         else:
-            log.warn('Missing compiler_cxx fix for '+self.__class__.__name__)
+            if hasattr(self, 'compiler'):
+                log.warn("#### %s #######" % (self.compiler,))
+            if not hasattr(self, 'compiler_cxx'):
+                log.warn('Missing compiler_cxx fix for ' + self.__class__.__name__)
+
+
+    # check if compiler supports gcc style automatic dependencies
+    # run on every extension so skip for known good compilers
+    if hasattr(self, 'compiler') and ('gcc' in self.compiler[0] or
+                                      'g++' in self.compiler[0] or
+                                      'clang' in self.compiler[0]):
+        self._auto_depends = True
+    elif os.name == 'posix':
+        import tempfile
+        import shutil
+        tmpdir = tempfile.mkdtemp()
+        try:
+            fn = os.path.join(tmpdir, "file.c")
+            with open(fn, "w") as f:
+                f.write("int a;\n")
+            self.compile([fn], output_dir=tmpdir,
+                         extra_preargs=['-MMD', '-MF', fn + '.d'])
+            self._auto_depends = True
+        except CompileError:
+            self._auto_depends = False
+        finally:
+            shutil.rmtree(tmpdir)
+
     return
 
-CCompiler.customize = new.instancemethod(\
-    CCompiler_customize,None,CCompiler)
-
-def simple_version_match(pat=r'[-.\d]+', ignore=None, start=''):
+replace_method(CCompiler, 'customize', CCompiler_customize)
+
+def simple_version_match(pat=r'[-.\d]+', ignore='', start=''):
+    """
+    Simple matching of version numbers, for use in CCompiler and FCompiler.
+
+    Parameters
+    ----------
+    pat : str, optional
+        A regular expression matching version numbers.
+        Default is ``r'[-.\\d]+'``.
+    ignore : str, optional
+        A regular expression matching patterns to skip.
+        Default is ``''``, in which case nothing is skipped.
+    start : str, optional
+        A regular expression matching the start of where to start looking
+        for version numbers.
+        Default is ``''``, in which case searching is started at the
+        beginning of the version string given to `matcher`.
+
+    Returns
+    -------
+    matcher : callable
+        A function that is appropriate to use as the ``.version_match``
+        attribute of a `CCompiler` class. `matcher` takes a single parameter,
+        a version string.
+
+    """
     def matcher(self, version_string):
+        # version string may appear in the second line, so getting rid
+        # of new lines:
+        version_string = version_string.replace('\n', ' ')
         pos = 0
         if start:
             m = re.match(start, version_string)
             if not m:
                 return None
             pos = m.end()
-        while 1:
+        while True:
             m = re.search(pat, version_string[pos:])
             if not m:
                 return None
@@ -225,22 +596,42 @@
         return m.group(0)
     return matcher
 
-def CCompiler_get_version(self, force=0, ok_status=[0]):
-    """ Compiler version. Returns None if compiler is not available. """
-    if not force and hasattr(self,'version'):
+def CCompiler_get_version(self, force=False, ok_status=[0]):
+    """
+    Return compiler version, or None if compiler is not available.
+
+    Parameters
+    ----------
+    force : bool, optional
+        If True, force a new determination of the version, even if the
+        compiler already has a version attribute. Default is False.
+    ok_status : list of int, optional
+        The list of status values returned by the version look-up process
+        for which a version string is returned. If the status value is not
+        in `ok_status`, None is returned. Default is ``[0]``.
+
+    Returns
+    -------
+    version : str or None
+        Version string, in the format of `distutils.version.LooseVersion`.
+
+    """
+    if not force and hasattr(self, 'version'):
         return self.version
+    self.find_executables()
     try:
         version_cmd = self.version_cmd
     except AttributeError:
-        return
-    cmd = ' '.join(version_cmd)
+        return None
+    if not version_cmd or not version_cmd[0]:
+        return None
     try:
         matcher = self.version_match
     except AttributeError:
         try:
             pat = self.version_pattern
         except AttributeError:
-            return
+            return None
         def matcher(version_string):
             m = re.match(pat, version_string)
             if not m:
@@ -248,26 +639,84 @@
             version = m.group('version')
             return version
 
-    status, output = exec_command(cmd,use_tee=0)
+    try:
+        output = subprocess.check_output(version_cmd, stderr=subprocess.STDOUT)
+    except subprocess.CalledProcessError as exc:
+        output = exc.output
+        status = exc.returncode
+    except OSError:
+        # match the historical returns for a parent
+        # exception class caught by exec_command()
+        status = 127
+        output = b''
+    else:
+        # output isn't actually a filepath but we do this
+        # for now to match previous distutils behavior
+        output = filepath_from_subprocess_output(output)
+        status = 0
+
     version = None
     if status in ok_status:
         version = matcher(output)
-        if not version:
-            log.warn("Couldn't match compiler version for %r" % (output,))
-        else:
+        if version:
             version = LooseVersion(version)
     self.version = version
     return version
 
-CCompiler.get_version = new.instancemethod(\
-    CCompiler_get_version,None,CCompiler)
-
-compiler_class['intel'] = ('intelccompiler','IntelCCompiler',
+replace_method(CCompiler, 'get_version', CCompiler_get_version)
+
+def CCompiler_cxx_compiler(self):
+    """
+    Return the C++ compiler.
+
+    Parameters
+    ----------
+    None
+
+    Returns
+    -------
+    cxx : class instance
+        The C++ compiler, as a `CCompiler` instance.
+
+    """
+    if self.compiler_type in ('msvc', 'intelw', 'intelemw'):
+        return self
+
+    cxx = copy(self)
+    cxx.compiler_cxx = cxx.compiler_cxx
+    cxx.compiler_so = [cxx.compiler_cxx[0]] + \
+                      sanitize_cxx_flags(cxx.compiler_so[1:])
+    if sys.platform.startswith('aix') and 'ld_so_aix' in cxx.linker_so[0]:
+        # AIX needs the ld_so_aix script included with Python
+        cxx.linker_so = [cxx.linker_so[0], cxx.compiler_cxx[0]] \
+                        + cxx.linker_so[2:]
+    else:
+        cxx.linker_so = [cxx.compiler_cxx[0]] + cxx.linker_so[1:]
+    return cxx
+
+replace_method(CCompiler, 'cxx_compiler', CCompiler_cxx_compiler)
+
+compiler_class['intel'] = ('intelccompiler', 'IntelCCompiler',
                            "Intel C Compiler for 32-bit applications")
-compiler_class['intele'] = ('intelccompiler','IntelItaniumCCompiler',
-                           "Intel C Itanium Compiler for Itanium-based applications")
-ccompiler._default_compilers = ccompiler._default_compilers \
-                               + (('linux.*','intel'),('linux.*','intele'))
+compiler_class['intele'] = ('intelccompiler', 'IntelItaniumCCompiler',
+                            "Intel C Itanium Compiler for Itanium-based applications")
+compiler_class['intelem'] = ('intelccompiler', 'IntelEM64TCCompiler',
+                             "Intel C Compiler for 64-bit applications")
+compiler_class['intelw'] = ('intelccompiler', 'IntelCCompilerW',
+                            "Intel C Compiler for 32-bit applications on Windows")
+compiler_class['intelemw'] = ('intelccompiler', 'IntelEM64TCCompilerW',
+                              "Intel C Compiler for 64-bit applications on Windows")
+compiler_class['pathcc'] = ('pathccompiler', 'PathScaleCCompiler',
+                            "PathScale Compiler for SiCortex-based applications")
+compiler_class['arm'] = ('armccompiler', 'ArmCCompiler',
+                            "Arm C Compiler")
+
+ccompiler._default_compilers += (('linux.*', 'intel'),
+                                 ('linux.*', 'intele'),
+                                 ('linux.*', 'intelem'),
+                                 ('linux.*', 'pathcc'),
+                                 ('nt', 'intelw'),
+                                 ('nt', 'intelemw'))
 
 if sys.platform == 'win32':
     compiler_class['mingw32'] = ('mingw32ccompiler', 'Mingw32CCompiler',
@@ -284,10 +733,12 @@
 _distutils_new_compiler = new_compiler
 def new_compiler (plat=None,
                   compiler=None,
-                  verbose=0,
+                  verbose=None,
                   dry_run=0,
                   force=0):
     # Try first C compilers from numpy.distutils.
+    if verbose is None:
+        verbose = log.get_threshold() <= log.INFO
     if plat is None:
         plat = os.name
     try:
@@ -298,35 +749,43 @@
         msg = "don't know how to compile C/C++ code on platform '%s'" % plat
         if compiler is not None:
             msg = msg + " with '%s' compiler" % compiler
-        raise DistutilsPlatformError, msg
+        raise DistutilsPlatformError(msg)
     module_name = "numpy.distutils." + module_name
     try:
         __import__ (module_name)
-    except ImportError, msg:
-        print msg,'in numpy.distutils, trying from distutils..'
+    except ImportError as e:
+        msg = str(e)
+        log.info('%s in numpy.distutils; trying from distutils',
+                 str(msg))
         module_name = module_name[6:]
         try:
             __import__(module_name)
-        except ImportError, msg:
-            raise DistutilsModuleError, \
-                  "can't compile C/C++ code: unable to load module '%s'" % \
-                  module_name
+        except ImportError as e:
+            msg = str(e)
+            raise DistutilsModuleError("can't compile C/C++ code: unable to load module '%s'" % \
+                  module_name)
     try:
         module = sys.modules[module_name]
         klass = vars(module)[class_name]
     except KeyError:
-        raise DistutilsModuleError, \
-              ("can't compile C/C++ code: unable to find class '%s' " +
-               "in module '%s'") % (class_name, module_name)
+        raise DistutilsModuleError(("can't compile C/C++ code: unable to find class '%s' " +
+               "in module '%s'") % (class_name, module_name))
     compiler = klass(None, dry_run, force)
-    log.debug('new_fcompiler returns %s' % (klass))
+    compiler.verbose = verbose
+    log.debug('new_compiler returns %s' % (klass))
     return compiler
 
 ccompiler.new_compiler = new_compiler
-
 
 _distutils_gen_lib_options = gen_lib_options
 def gen_lib_options(compiler, library_dirs, runtime_library_dirs, libraries):
+    # the version of this function provided by CPython allows the following
+    # to return lists, which are unpacked automatically:
+    # - compiler.runtime_library_dir_option
+    # our version extends the behavior to:
+    # - compiler.library_dir_option
+    # - compiler.library_option
+    # - compiler.find_library_file
     r = _distutils_gen_lib_options(compiler, library_dirs,
                                    runtime_library_dirs, libraries)
     lib_opts = []
@@ -338,60 +797,11 @@
     return lib_opts
 ccompiler.gen_lib_options = gen_lib_options
 
-
-##Fix distutils.util.split_quoted:
-import re,string
-_wordchars_re = re.compile(r'[^\\\'\"%s ]*' % string.whitespace)
-_squote_re = re.compile(r"'(?:[^'\\]|\\.)*'")
-_dquote_re = re.compile(r'"(?:[^"\\]|\\.)*"')
-_has_white_re = re.compile(r'\s')
-def split_quoted(s):
-    s = string.strip(s)
-    words = []
-    pos = 0
-
-    while s:
-        m = _wordchars_re.match(s, pos)
-        end = m.end()
-        if end == len(s):
-            words.append(s[:end])
-            break
-
-        if s[end] in string.whitespace: # unescaped, unquoted whitespace: now
-            words.append(s[:end])       # we definitely have a word delimiter
-            s = string.lstrip(s[end:])
-            pos = 0
-
-        elif s[end] == '\\':            # preserve whatever is being escaped;
-                                        # will become part of the current word
-            s = s[:end] + s[end+1:]
-            pos = end+1
-
-        else:
-            if s[end] == "'":           # slurp singly-quoted string
-                m = _squote_re.match(s, end)
-            elif s[end] == '"':         # slurp doubly-quoted string
-                m = _dquote_re.match(s, end)
-            else:
-                raise RuntimeError, \
-                      "this can't happen (bad char '%c')" % s[end]
-
-            if m is None:
-                raise ValueError, \
-                      "bad string (mismatched %s quotes?)" % s[end]
-
-            (beg, end) = m.span()
-            if _has_white_re.search(s[beg+1:end-1]):
-                s = s[:beg] + s[beg+1:end-1] + s[end:]
-                pos = m.end() - 2
-            else:
-                # Keeping quotes when a quoted word does not contain
-                # white-space. XXX: send a patch to distutils
-                pos = m.end()
-
-        if pos >= len(s):
-            words.append(s)
-            break
-
-    return words
-ccompiler.split_quoted = split_quoted
+# Also fix up the various compiler modules, which do
+# from distutils.ccompiler import gen_lib_options
+# Don't bother with mwerks, as we don't support Classic Mac.
+for _cc in ['msvc9', 'msvc', '_msvc', 'bcpp', 'cygwinc', 'emxc', 'unixc']:
+    _m = sys.modules.get('distutils.' + _cc + 'compiler')
+    if _m is not None:
+        setattr(_m, 'gen_lib_options', gen_lib_options)
+
('numpy/distutils', 'misc_util.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,72 +1,182 @@
 import os
 import re
 import sys
-import imp
 import copy
 import glob
-
-try:
-    set
-except NameError:
-    from sets import Set as set
+import atexit
+import tempfile
+import subprocess
+import shutil
+import multiprocessing
+import textwrap
+import importlib.util
+from threading import local as tlocal
+from functools import reduce
+
+import distutils
+from distutils.errors import DistutilsError
+
+# stores temporary directory of each thread to only create one per thread
+_tdata = tlocal()
+
+# store all created temporary directories so they can be deleted on exit
+_tmpdirs = []
+def clean_up_temporary_directory():
+    if _tmpdirs is not None:
+        for d in _tmpdirs:
+            try:
+                shutil.rmtree(d)
+            except OSError:
+                pass
+
+atexit.register(clean_up_temporary_directory)
 
 __all__ = ['Configuration', 'get_numpy_include_dirs', 'default_config_dict',
            'dict_append', 'appendpath', 'generate_config_py',
            'get_cmd', 'allpath', 'get_mathlibs',
            'terminal_has_colors', 'red_text', 'green_text', 'yellow_text',
-           'blue_text', 'cyan_text', 'cyg2win32','mingw32','all_strings',
+           'blue_text', 'cyan_text', 'cyg2win32', 'mingw32', 'all_strings',
            'has_f_sources', 'has_cxx_sources', 'filter_sources',
            'get_dependencies', 'is_local_src_dir', 'get_ext_source_files',
            'get_script_files', 'get_lib_source_files', 'get_data_files',
-           'dot_join', 'get_frame', 'minrelpath','njoin',
-           'is_sequence', 'is_string', 'as_list', 'gpaths']
+           'dot_join', 'get_frame', 'minrelpath', 'njoin',
+           'is_sequence', 'is_string', 'as_list', 'gpaths', 'get_language',
+           'get_build_architecture', 'get_info', 'get_pkg_info',
+           'get_num_build_jobs', 'sanitize_cxx_flags',
+           'exec_mod_from_location']
+
+class InstallableLib:
+    """
+    Container to hold information on an installable library.
+
+    Parameters
+    ----------
+    name : str
+        Name of the installed library.
+    build_info : dict
+        Dictionary holding build information.
+    target_dir : str
+        Absolute path specifying where to install the library.
+
+    See Also
+    --------
+    Configuration.add_installed_library
+
+    Notes
+    -----
+    The three parameters are stored as attributes with the same names.
+
+    """
+    def __init__(self, name, build_info, target_dir):
+        self.name = name
+        self.build_info = build_info
+        self.target_dir = target_dir
+
+
+def get_num_build_jobs():
+    """
+    Get number of parallel build jobs set by the --parallel command line
+    argument of setup.py
+    If the command did not receive a setting the environment variable
+    NPY_NUM_BUILD_JOBS is checked. If that is unset, return the number of
+    processors on the system, with a maximum of 8 (to prevent
+    overloading the system if there a lot of CPUs).
+
+    Returns
+    -------
+    out : int
+        number of parallel jobs that can be run
+
+    """
+    from numpy.distutils.core import get_distribution
+    try:
+        cpu_count = len(os.sched_getaffinity(0))
+    except AttributeError:
+        cpu_count = multiprocessing.cpu_count()
+    cpu_count = min(cpu_count, 8)
+    envjobs = int(os.environ.get("NPY_NUM_BUILD_JOBS", cpu_count))
+    dist = get_distribution()
+    # may be None during configuration
+    if dist is None:
+        return envjobs
+
+    # any of these three may have the job set, take the largest
+    cmdattr = (getattr(dist.get_command_obj('build'), 'parallel', None),
+               getattr(dist.get_command_obj('build_ext'), 'parallel', None),
+               getattr(dist.get_command_obj('build_clib'), 'parallel', None))
+    if all(x is None for x in cmdattr):
+        return envjobs
+    else:
+        return max(x for x in cmdattr if x is not None)
+
+def quote_args(args):
+    """Quote list of arguments.
+
+    .. deprecated:: 1.22.
+    """
+    import warnings
+    warnings.warn('"quote_args" is deprecated.',
+                  DeprecationWarning, stacklevel=2)
+    # don't used _nt_quote_args as it does not check if
+    # args items already have quotes or not.
+    args = list(args)
+    for i in range(len(args)):
+        a = args[i]
+        if ' ' in a and a[0] not in '"\'':
+            args[i] = '"%s"' % (a)
+    return args
 
 def allpath(name):
     "Convert a /-separated pathname to one using the OS's path separator."
-    splitted = name.split('/')
-    return os.path.join(*splitted)
+    split = name.split('/')
+    return os.path.join(*split)
 
 def rel_path(path, parent_path):
-    """ Return path relative to parent_path.
-    """
-    pd = os.path.abspath(parent_path)
-    apath = os.path.abspath(path)
-    if len(apath)<len(pd):
+    """Return path relative to parent_path."""
+    # Use realpath to avoid issues with symlinked dirs (see gh-7707)
+    pd = os.path.realpath(os.path.abspath(parent_path))
+    apath = os.path.realpath(os.path.abspath(path))
+    if len(apath) < len(pd):
         return path
-    if apath==pd:
+    if apath == pd:
         return ''
     if pd == apath[:len(pd)]:
-        assert apath[len(pd)] in [os.sep],`path,apath[len(pd)]`
+        assert apath[len(pd)] in [os.sep], repr((path, apath[len(pd)]))
         path = apath[len(pd)+1:]
     return path
 
-def get_path(mod_name, parent_path=None):
-    """ Return path of the module.
+def get_path_from_frame(frame, parent_path=None):
+    """Return path of the module given a frame object from the call stack.
 
     Returned path is relative to parent_path when given,
     otherwise it is absolute path.
     """
-    if mod_name == '__builtin__':
-        #builtin if/then added by Pearu for use in core.run_setup.
-        d = os.path.dirname(os.path.abspath(sys.argv[0]))
-    else:
-        __import__(mod_name)
-        mod = sys.modules[mod_name]
-        if hasattr(mod,'__file__'):
-            filename = mod.__file__
+
+    # First, try to find if the file name is in the frame.
+    try:
+        caller_file = eval('__file__', frame.f_globals, frame.f_locals)
+        d = os.path.dirname(os.path.abspath(caller_file))
+    except NameError:
+        # __file__ is not defined, so let's try __name__. We try this second
+        # because setuptools spoofs __name__ to be '__main__' even though
+        # sys.modules['__main__'] might be something else, like easy_install(1).
+        caller_name = eval('__name__', frame.f_globals, frame.f_locals)
+        __import__(caller_name)
+        mod = sys.modules[caller_name]
+        if hasattr(mod, '__file__'):
             d = os.path.dirname(os.path.abspath(mod.__file__))
         else:
             # we're probably running setup.py as execfile("setup.py")
             # (likely we're building an egg)
             d = os.path.abspath('.')
-            # hmm, should we use sys.argv[0] like in __builtin__ case?
 
     if parent_path is not None:
         d = rel_path(d, parent_path)
+
     return d or '.'
 
 def njoin(*path):
-    """ Join two or more pathname components +
+    """Join two or more pathname components +
     - convert a /-separated pathname to one using the OS's path separator.
     - resolve `..` and `.` from path.
 
@@ -89,28 +199,38 @@
         # njoin('a', 'b')
         joined = os.path.join(*path)
     if os.path.sep != '/':
-        joined = joined.replace('/',os.path.sep)
+        joined = joined.replace('/', os.path.sep)
     return minrelpath(joined)
 
 def get_mathlibs(path=None):
-    """ Return the MATHLIB line from config.h
-    """
-    if path is None:
-        path = get_numpy_include_dirs()[0]
-    config_file = os.path.join(path,'config.h')
-    fid = open(config_file)
-    mathlibs = []
-    s = '#define MATHLIB'
-    for line in fid.readlines():
-        if line.startswith(s):
-            value = line[len(s):].strip()
-            if value:
-                mathlibs.extend(value.split(','))
-    fid.close()
+    """Return the MATHLIB line from numpyconfig.h
+    """
+    if path is not None:
+        config_file = os.path.join(path, '_numpyconfig.h')
+    else:
+        # Look for the file in each of the numpy include directories.
+        dirs = get_numpy_include_dirs()
+        for path in dirs:
+            fn = os.path.join(path, '_numpyconfig.h')
+            if os.path.exists(fn):
+                config_file = fn
+                break
+        else:
+            raise DistutilsError('_numpyconfig.h not found in numpy include '
+                'dirs %r' % (dirs,))
+
+    with open(config_file) as fid:
+        mathlibs = []
+        s = '#define MATHLIB'
+        for line in fid:
+            if line.startswith(s):
+                value = line[len(s):].strip()
+                if value:
+                    mathlibs.extend(value.split(','))
     return mathlibs
 
 def minrelpath(path):
-    """ Resolve `..` and '.' from path.
+    """Resolve `..` and '.' from path.
     """
     if not is_string(path):
         return path
@@ -119,34 +239,39 @@
     l = path.split(os.sep)
     while l:
         try:
-            i = l.index('.',1)
+            i = l.index('.', 1)
         except ValueError:
             break
         del l[i]
     j = 1
     while l:
         try:
-            i = l.index('..',j)
+            i = l.index('..', j)
         except ValueError:
             break
         if l[i-1]=='..':
             j += 1
         else:
-            del l[i],l[i-1]
+            del l[i], l[i-1]
             j = 1
     if not l:
         return ''
     return os.sep.join(l)
 
-def _fix_paths(paths,local_path,include_non_existing):
+def sorted_glob(fileglob):
+    """sorts output of python glob for https://bugs.python.org/issue30461
+    to allow extensions to have reproducible build results"""
+    return sorted(glob.glob(fileglob))
+
+def _fix_paths(paths, local_path, include_non_existing):
     assert is_sequence(paths), repr(type(paths))
     new_paths = []
-    assert not is_string(paths),`paths`
+    assert not is_string(paths), repr(paths)
     for n in paths:
         if is_string(n):
             if '*' in n or '?' in n:
-                p = glob.glob(n)
-                p2 = glob.glob(njoin(local_path,n))
+                p = sorted_glob(n)
+                p2 = sorted_glob(njoin(local_path, n))
                 if p2:
                     new_paths.extend(p2)
                 elif p:
@@ -154,10 +279,10 @@
                 else:
                     if include_non_existing:
                         new_paths.append(n)
-                    print 'could not resolve pattern in %r: %r' \
-                              % (local_path,n)
+                    print('could not resolve pattern in %r: %r' %
+                            (local_path, n))
             else:
-                n2 = njoin(local_path,n)
+                n2 = njoin(local_path, n)
                 if os.path.exists(n2):
                     new_paths.append(n2)
                 else:
@@ -166,27 +291,37 @@
                     elif include_non_existing:
                         new_paths.append(n)
                     if not os.path.exists(n):
-                        print 'non-existing path in %r: %r' \
-                              % (local_path,n)
+                        print('non-existing path in %r: %r' %
+                                (local_path, n))
 
         elif is_sequence(n):
-            new_paths.extend(_fix_paths(n,local_path,include_non_existing))
+            new_paths.extend(_fix_paths(n, local_path, include_non_existing))
         else:
             new_paths.append(n)
-    return map(minrelpath,new_paths)
+    return [minrelpath(p) for p in new_paths]
 
 def gpaths(paths, local_path='', include_non_existing=True):
-    """ Apply glob to paths and prepend local_path if needed.
+    """Apply glob to paths and prepend local_path if needed.
     """
     if is_string(paths):
         paths = (paths,)
-    return _fix_paths(paths,local_path, include_non_existing)
-
+    return _fix_paths(paths, local_path, include_non_existing)
+
+def make_temp_file(suffix='', prefix='', text=True):
+    if not hasattr(_tdata, 'tempdir'):
+        _tdata.tempdir = tempfile.mkdtemp()
+        _tmpdirs.append(_tdata.tempdir)
+    fid, name = tempfile.mkstemp(suffix=suffix,
+                                 prefix=prefix,
+                                 dir=_tdata.tempdir,
+                                 text=text)
+    fo = os.fdopen(fid, 'w')
+    return fo, name
 
 # Hooks for colored terminal output.
-# See also http://www.livinglogic.de/Python/ansistyle
+# See also https://web.archive.org/web/20100314204946/http://www.livinglogic.de/Python/ansistyle
 def terminal_has_colors():
-    if sys.platform=='cygwin' and not os.environ.has_key('USE_COLOR'):
+    if sys.platform=='cygwin' and 'USE_COLOR' not in os.environ:
         # Avoid importing curses that causes illegal operation
         # with a message:
         #  PYTHON2 caused an invalid page fault in
@@ -196,7 +331,7 @@
         #          curses.version is 2.2
         #          CYGWIN_98-4.10, release 1.5.7(0.109/3/2))
         return 0
-    if hasattr(sys.stdout,'isatty') and sys.stdout.isatty():
+    if hasattr(sys.stdout, 'isatty') and sys.stdout.isatty():
         try:
             import curses
             curses.setupterm()
@@ -208,70 +343,152 @@
                          and curses.tigetstr("setab") is not None)
                      or curses.tigetstr("scp") is not None)):
                 return 1
-        except Exception,msg:
+        except Exception:
             pass
     return 0
 
 if terminal_has_colors():
-    def red_text(s): return '\x1b[31m%s\x1b[0m'%s
-    def green_text(s): return '\x1b[32m%s\x1b[0m'%s
-    def yellow_text(s): return '\x1b[33m%s\x1b[0m'%s
-    def blue_text(s): return '\x1b[34m%s\x1b[0m'%s
-    def cyan_text(s): return '\x1b[35m%s\x1b[0m'%s
+    _colour_codes = dict(black=0, red=1, green=2, yellow=3,
+                         blue=4, magenta=5, cyan=6, white=7, default=9)
+    def colour_text(s, fg=None, bg=None, bold=False):
+        seq = []
+        if bold:
+            seq.append('1')
+        if fg:
+            fgcode = 30 + _colour_codes.get(fg.lower(), 0)
+            seq.append(str(fgcode))
+        if bg:
+            bgcode = 40 + _colour_codes.get(fg.lower(), 7)
+            seq.append(str(bgcode))
+        if seq:
+            return '\x1b[%sm%s\x1b[0m' % (';'.join(seq), s)
+        else:
+            return s
 else:
-    def red_text(s): return s
-    def green_text(s): return s
-    def yellow_text(s): return s
-    def cyan_text(s): return s
-    def blue_text(s): return s
+    def colour_text(s, fg=None, bg=None):
+        return s
+
+def default_text(s):
+    return colour_text(s, 'default')
+def red_text(s):
+    return colour_text(s, 'red')
+def green_text(s):
+    return colour_text(s, 'green')
+def yellow_text(s):
+    return colour_text(s, 'yellow')
+def cyan_text(s):
+    return colour_text(s, 'cyan')
+def blue_text(s):
+    return colour_text(s, 'blue')
 
 #########################
 
-def cyg2win32(path):
-    if sys.platform=='cygwin' and path.startswith('/cygdrive'):
-        path = path[10] + ':' + os.path.normcase(path[11:])
-    return path
+def cyg2win32(path: str) -> str:
+    """Convert a path from Cygwin-native to Windows-native.
+
+    Uses the cygpath utility (part of the Base install) to do the
+    actual conversion.  Falls back to returning the original path if
+    this fails.
+
+    Handles the default ``/cygdrive`` mount prefix as well as the
+    ``/proc/cygdrive`` portable prefix, custom cygdrive prefixes such
+    as ``/`` or ``/mnt``, and absolute paths such as ``/usr/src/`` or
+    ``/home/username``
+
+    Parameters
+    ----------
+    path : str
+       The path to convert
+
+    Returns
+    -------
+    converted_path : str
+        The converted path
+
+    Notes
+    -----
+    Documentation for cygpath utility:
+    https://cygwin.com/cygwin-ug-net/cygpath.html
+    Documentation for the C function it wraps:
+    https://cygwin.com/cygwin-api/func-cygwin-conv-path.html
+
+    """
+    if sys.platform != "cygwin":
+        return path
+    return subprocess.check_output(
+        ["/usr/bin/cygpath", "--windows", path], universal_newlines=True
+    )
+
 
 def mingw32():
-    """ Return true when using mingw32 environment.
+    """Return true when using mingw32 environment.
     """
     if sys.platform=='win32':
-        if os.environ.get('OSTYPE','')=='msys':
+        if os.environ.get('OSTYPE', '')=='msys':
             return True
-        if os.environ.get('MSYSTEM','')=='MINGW32':
+        if os.environ.get('MSYSTEM', '')=='MINGW32':
             return True
     return False
 
+def msvc_runtime_version():
+    "Return version of MSVC runtime library, as defined by __MSC_VER__ macro"
+    msc_pos = sys.version.find('MSC v.')
+    if msc_pos != -1:
+        msc_ver = int(sys.version[msc_pos+6:msc_pos+10])
+    else:
+        msc_ver = None
+    return msc_ver
+
+def msvc_runtime_library():
+    "Return name of MSVC runtime library if Python was built with MSVC >= 7"
+    ver = msvc_runtime_major ()
+    if ver:
+        if ver < 140:
+            return "msvcr%i" % ver
+        else:
+            return "vcruntime%i" % ver
+    else:
+        return None
+
+def msvc_runtime_major():
+    "Return major version of MSVC runtime coded like get_build_msvc_version"
+    major = {1300:  70,  # MSVC 7.0
+             1310:  71,  # MSVC 7.1
+             1400:  80,  # MSVC 8
+             1500:  90,  # MSVC 9  (aka 2008)
+             1600: 100,  # MSVC 10 (aka 2010)
+             1900: 140,  # MSVC 14 (aka 2015)
+    }.get(msvc_runtime_version(), None)
+    return major
+
 #########################
 
 #XXX need support for .C that is also C++
-cxx_ext_match = re.compile(r'.*[.](cpp|cxx|cc)\Z',re.I).match
-fortran_ext_match = re.compile(r'.*[.](f90|f95|f77|for|ftn|f)\Z',re.I).match
-f90_ext_match = re.compile(r'.*[.](f90|f95)\Z',re.I).match
-f90_module_name_match = re.compile(r'\s*module\s*(?P<name>[\w_]+)',re.I).match
+cxx_ext_match = re.compile(r'.*\.(cpp|cxx|cc)\Z', re.I).match
+fortran_ext_match = re.compile(r'.*\.(f90|f95|f77|for|ftn|f)\Z', re.I).match
+f90_ext_match = re.compile(r'.*\.(f90|f95)\Z', re.I).match
+f90_module_name_match = re.compile(r'\s*module\s*(?P<name>[\w_]+)', re.I).match
 def _get_f90_modules(source):
-    """ Return a list of Fortran f90 module names that
+    """Return a list of Fortran f90 module names that
     given source file defines.
     """
     if not f90_ext_match(source):
         return []
     modules = []
-    f = open(source,'r')
-    f_readlines = getattr(f,'xreadlines',f.readlines)
-    for line in f_readlines():
-        m = f90_module_name_match(line)
-        if m:
-            name = m.group('name')
-            modules.append(name)
-            # break  # XXX can we assume that there is one module per file?
-    f.close()
+    with open(source, 'r') as f:
+        for line in f:
+            m = f90_module_name_match(line)
+            if m:
+                name = m.group('name')
+                modules.append(name)
+                # break  # XXX can we assume that there is one module per file?
     return modules
 
 def is_string(s):
     return isinstance(s, str)
 
 def all_strings(lst):
-    """ Return True if all items in lst are string objects. """
+    """Return True if all items in lst are string objects. """
     for item in lst:
         if not is_string(item):
             return False
@@ -282,12 +499,12 @@
         return False
     try:
         len(seq)
-    except:
+    except Exception:
         return False
     return True
 
 def is_glob_pattern(s):
-    return is_string(s) and ('*' in s or '?' is s)
+    return is_string(s) and ('*' in s or '?' in s)
 
 def as_list(seq):
     if is_sequence(seq):
@@ -295,22 +512,35 @@
     else:
         return [seq]
 
+def get_language(sources):
+    # not used in numpy/scipy packages, use build_ext.detect_language instead
+    """Determine language value (c,f77,f90) from sources """
+    language = None
+    for source in sources:
+        if isinstance(source, str):
+            if f90_ext_match(source):
+                language = 'f90'
+                break
+            elif fortran_ext_match(source):
+                language = 'f77'
+    return language
+
 def has_f_sources(sources):
-    """ Return True if sources contains Fortran files """
+    """Return True if sources contains Fortran files """
     for source in sources:
         if fortran_ext_match(source):
             return True
     return False
 
 def has_cxx_sources(sources):
-    """ Return True if sources contains C++ files """
+    """Return True if sources contains C++ files """
     for source in sources:
         if cxx_ext_match(source):
             return True
     return False
 
 def filter_sources(sources):
-    """ Return four lists of filenames containing
+    """Return four lists of filenames containing
     C, C++, Fortran, and Fortran 90 module sources,
     respectively.
     """
@@ -336,7 +566,7 @@
     # get *.h files from list of directories
     headers = []
     for d in directory_list:
-        head = glob.glob(os.path.join(d,"*.h")) #XXX: *.hpp files??
+        head = sorted_glob(os.path.join(d, "*.h")) #XXX: *.hpp files??
         headers.extend(head)
     return headers
 
@@ -349,17 +579,29 @@
             direcs.append(d[0])
     return direcs
 
+def _commandline_dep_string(cc_args, extra_postargs, pp_opts):
+    """
+    Return commandline representation used to determine if a file needs
+    to be recompiled
+    """
+    cmdline = 'commandline: '
+    cmdline += ' '.join(cc_args)
+    cmdline += ' '.join(extra_postargs)
+    cmdline += ' '.join(pp_opts) + '\n'
+    return cmdline
+
+
 def get_dependencies(sources):
     #XXX scan sources for include statements
     return _get_headers(_get_directories(sources))
 
 def is_local_src_dir(directory):
-    """ Return true if directory is local directory.
+    """Return true if directory is local directory.
     """
     if not is_string(directory):
         return False
     abs_dir = os.path.abspath(directory)
-    c = os.path.commonprefix([os.getcwd(),abs_dir])
+    c = os.path.commonprefix([os.getcwd(), abs_dir])
     new_dir = abs_dir[len(c):].split(os.sep)
     if new_dir and not new_dir[0]:
         new_dir = new_dir[1:]
@@ -379,10 +621,10 @@
                 yield os.path.join(dirpath, f)
 
 def general_source_directories_files(top_path):
-    """ Return a directory name relative to top_path and
+    """Return a directory name relative to top_path and
     files contained.
     """
-    pruned_directories = ['CVS','.svn','build']
+    pruned_directories = ['CVS', '.svn', 'build']
     prune_file_pat = re.compile(r'(?:[~#]|\.py[co]|\.o)$')
     for dirpath, dirnames, filenames in os.walk(top_path, topdown=True):
         pruned = [ d for d in dirnames if d not in pruned_directories ]
@@ -392,13 +634,13 @@
             rpath = rel_path(dpath, top_path)
             files = []
             for f in os.listdir(dpath):
-                fn = os.path.join(dpath,f)
+                fn = os.path.join(dpath, f)
                 if os.path.isfile(fn) and not prune_file_pat.search(fn):
                     files.append(fn)
             yield rpath, files
     dpath = top_path
     rpath = rel_path(dpath, top_path)
-    filenames = [os.path.join(dpath,f) for f in os.listdir(dpath) \
+    filenames = [os.path.join(dpath, f) for f in os.listdir(dpath) \
                  if not prune_file_pat.search(f)]
     files = [f for f in filenames if os.path.isfile(f)]
     yield rpath, files
@@ -407,7 +649,7 @@
 def get_ext_source_files(ext):
     # Get sources and any include files in the same directory.
     filenames = []
-    sources = filter(is_string, ext.sources)
+    sources = [_m for _m in ext.sources if is_string(_m)]
     filenames.extend(sources)
     filenames.extend(get_dependencies(sources))
     for d in ext.depends:
@@ -418,16 +660,16 @@
     return filenames
 
 def get_script_files(scripts):
-    scripts = filter(is_string, scripts)
+    scripts = [_m for _m in scripts if is_string(_m)]
     return scripts
 
 def get_lib_source_files(lib):
     filenames = []
-    sources = lib[1].get('sources',[])
-    sources = filter(is_string, sources)
+    sources = lib[1].get('sources', [])
+    sources = [_m for _m in sources if is_string(_m)]
     filenames.extend(sources)
     filenames.extend(get_dependencies(sources))
-    depends = lib[1].get('depends',[])
+    depends = lib[1].get('depends', [])
     for d in depends:
         if is_local_src_dir(d):
             filenames.extend(list(general_source_files(d)))
@@ -435,13 +677,56 @@
             filenames.append(d)
     return filenames
 
+def get_shared_lib_extension(is_python_ext=False):
+    """Return the correct file extension for shared libraries.
+
+    Parameters
+    ----------
+    is_python_ext : bool, optional
+        Whether the shared library is a Python extension.  Default is False.
+
+    Returns
+    -------
+    so_ext : str
+        The shared library extension.
+
+    Notes
+    -----
+    For Python shared libs, `so_ext` will typically be '.so' on Linux and OS X,
+    and '.pyd' on Windows.  For Python >= 3.2 `so_ext` has a tag prepended on
+    POSIX systems according to PEP 3149.
+
+    """
+    confvars = distutils.sysconfig.get_config_vars()
+    so_ext = confvars.get('EXT_SUFFIX', '')
+
+    if not is_python_ext:
+        # hardcode known values, config vars (including SHLIB_SUFFIX) are
+        # unreliable (see #3182)
+        # darwin, windows and debug linux are wrong in 3.3.1 and older
+        if (sys.platform.startswith('linux') or
+            sys.platform.startswith('gnukfreebsd')):
+            so_ext = '.so'
+        elif sys.platform.startswith('darwin'):
+            so_ext = '.dylib'
+        elif sys.platform.startswith('win'):
+            so_ext = '.dll'
+        else:
+            # fall back to config vars for unknown platforms
+            # fix long extension for Python >=3.2, see PEP 3149.
+            if 'SOABI' in confvars:
+                # Does nothing unless SOABI config var exists
+                so_ext = so_ext.replace('.' + confvars.get('SOABI'), '', 1)
+
+    return so_ext
+
 def get_data_files(data):
     if is_string(data):
         return [data]
     sources = data[1]
     filenames = []
     for s in sources:
-        if callable(s):
+        if hasattr(s, '__call__'):
             continue
         if is_local_src_dir(s):
             filenames.extend(list(general_source_files(s)))
@@ -449,16 +734,16 @@
             if os.path.isfile(s):
                 filenames.append(s)
             else:
-                print 'Not existing data file:',s
-        else:
-            raise TypeError,repr(s)
+                print('Not existing data file:', s)
+        else:
+            raise TypeError(repr(s))
     return filenames
 
 def dot_join(*args):
     return '.'.join([a for a in args if a])
 
 def get_frame(level=0):
-    """ Return frame object from call stack with given level.
+    """Return frame object from call stack with given level.
     """
     try:
         return sys._getframe(level+1)
@@ -468,13 +753,15 @@
             frame = frame.f_back
         return frame
 
+
 ######################
 
-class Configuration(object):
+class Configuration:
 
     _list_keys = ['packages', 'ext_modules', 'data_files', 'include_dirs',
-                  'libraries', 'headers', 'scripts', 'py_modules']
-    _dict_keys = ['package_dir']
+                  'libraries', 'headers', 'scripts', 'py_modules',
+                  'installed_libraries', 'define_macros']
+    _dict_keys = ['package_dir', 'installed_pkg_config']
     _extra_keys = ['name', 'version']
 
     numpy_include_dirs = []
@@ -485,8 +772,9 @@
                  top_path=None,
                  package_path=None,
                  caller_level=1,
+                 setup_name='setup.py',
                  **attrs):
-        """ Construct configuration instance of a package.
+        """Construct configuration instance of a package.
 
         package_name -- name of the package
                         Ex.: 'distutils'
@@ -503,16 +791,19 @@
         self.version = None
 
         caller_frame = get_frame(caller_level)
-        caller_name = eval('__name__',caller_frame.f_globals,caller_frame.f_locals)
-        self.local_path = get_path(caller_name, top_path)
+        self.local_path = get_path_from_frame(caller_frame, top_path)
+        # local_path -- directory of a file (usually setup.py) that
+        #               defines a configuration() function.
+        # local_path -- directory of a file (usually setup.py) that
+        #               defines a configuration() function.
         if top_path is None:
             top_path = self.local_path
-            self.local_path = '.'
+            self.local_path = ''
         if package_path is None:
             package_path = self.local_path
-        elif os.path.isdir(njoin(self.local_path,package_path)):
-            package_path = njoin(self.local_path,package_path)
-        if not os.path.isdir(package_path):
+        elif os.path.isdir(njoin(self.local_path, package_path)):
+            package_path = njoin(self.local_path, package_path)
+        if not os.path.isdir(package_path or '.'):
             raise ValueError("%r is not a directory" % (package_path,))
         self.top_path = top_path
         self.package_path = package_path
@@ -536,7 +827,7 @@
             if n in known_keys:
                 continue
             a = attrs[n]
-            setattr(self,n,a)
+            setattr(self, n, a)
             if isinstance(a, list):
                 self.list_keys.append(n)
             elif isinstance(a, dict):
@@ -544,7 +835,7 @@
             else:
                 self.extra_keys.append(n)
 
-        if os.path.exists(njoin(package_path,'__init__.py')):
+        if os.path.exists(njoin(package_path, '__init__.py')):
             self.packages.append(self.name)
             self.package_dir[self.name] = package_path
 
@@ -556,13 +847,13 @@
             )
 
         caller_instance = None
-        for i in range(1,3):
+        for i in range(1, 3):
             try:
                 f = get_frame(i)
             except ValueError:
                 break
             try:
-                caller_instance = eval('self',f.f_globals,f.f_locals)
+                caller_instance = eval('self', f.f_globals, f.f_locals)
                 break
             except NameError:
                 pass
@@ -570,64 +861,65 @@
             if caller_instance.options['delegate_options_to_subpackages']:
                 self.set_options(**caller_instance.options)
 
+        self.setup_name = setup_name
+
     def todict(self):
-        """ Return configuration distionary suitable for passing
-        to distutils.core.setup() function.
-        """
+        """
+        Return a dictionary compatible with the keyword arguments of distutils
+        setup function.
+
+        Examples
+        --------
+        >>> setup(**config.todict())                           #doctest: +SKIP
+        """
+
         self._optimize_data_files()
         d = {}
         known_keys = self.list_keys + self.dict_keys + self.extra_keys
         for n in known_keys:
-            a = getattr(self,n)
+            a = getattr(self, n)
             if a:
                 d[n] = a
         return d
 
     def info(self, message):
         if not self.options['quiet']:
-            print message
+            print(message)
 
     def warn(self, message):
-        print>>sys.stderr, blue_text('Warning: %s' % (message,))
+        sys.stderr.write('Warning: %s\n' % (message,))
 
     def set_options(self, **options):
-        """ Configure Configuration instance.
+        """
+        Configure Configuration instance.
 
         The following options are available:
-        - ignore_setup_xxx_py
-        - assume_default_configuration
-        - delegate_options_to_subpackages
-        - quiet
+         - ignore_setup_xxx_py
+         - assume_default_configuration
+         - delegate_options_to_subpackages
+         - quiet
+
         """
         for key, value in options.items():
-            if self.options.has_key(key):
+            if key in self.options:
                 self.options[key] = value
             else:
-                raise ValueError,'Unknown option: '+key
+                raise ValueError('Unknown option: '+key)
 
     def get_distribution(self):
-        import distutils.core
-        dist = distutils.core._setup_distribution
-        # XXX Hack to get numpy installable with easy_install.
-        # The problem is easy_install runs it's own setup(), which
-        # sets up distutils.core._setup_distribution. However,
-        # when our setup() runs, that gets overwritten and lost.
-        # We can't use isinstance, as the DistributionWithoutHelpCommands
-        # class is local to a function in setuptools.command.easy_install
-        if dist is not None and \
-                repr(dist).find('DistributionWithoutHelpCommands') != -1:
-            return None
-        return dist
+        """Return the distutils distribution object for self."""
+        from numpy.distutils.core import get_distribution
+        return get_distribution()
 
     def _wildcard_get_subpackage(self, subpackage_name,
                                  parent_name,
                                  caller_level = 1):
         l = subpackage_name.split('.')
         subpackage_path = njoin([self.local_path]+l)
-        dirs = filter(os.path.isdir,glob.glob(subpackage_path))
+        dirs = [_m for _m in sorted_glob(subpackage_path) if os.path.isdir(_m)]
         config_list = []
         for d in dirs:
-            if not os.path.isfile(njoin(d,'__init__.py')):
+            if not os.path.isfile(njoin(d, '__init__.py')):
                 continue
             if 'build' in d.split(os.sep):
                 continue
@@ -644,17 +936,13 @@
                                          parent_name,
                                          caller_level = 1):
         # In case setup_py imports local modules:
-        sys.path.insert(0,os.path.dirname(setup_py))
+        sys.path.insert(0, os.path.dirname(setup_py))
         try:
-            fo_setup_py = open(setup_py, 'U')
             setup_name = os.path.splitext(os.path.basename(setup_py))[0]
-            n = dot_join(self.name,subpackage_name,setup_name)
-            setup_module = imp.load_module('_'.join(n.split('.')),
-                                           fo_setup_py,
-                                           setup_py,
-                                           ('.py', 'U', 1))
-            fo_setup_py.close()
-            if not hasattr(setup_module,'configuration'):
+            n = dot_join(self.name, subpackage_name, setup_name)
+            setup_module = exec_mod_from_location(
+                                '_'.join(n.split('.')), setup_py)
+            if not hasattr(setup_module, 'configuration'):
                 if not self.options['assume_default_configuration']:
                     self.warn('Assuming default configuration '\
                               '(%s does not define configuration())'\
@@ -665,12 +953,12 @@
             else:
                 pn = dot_join(*([parent_name] + subpackage_name.split('.')[:-1]))
                 args = (pn,)
-                if setup_module.configuration.func_code.co_argcount > 1:
+                if setup_module.configuration.__code__.co_argcount > 1:
                     args = args + (self.top_path,)
                 config = setup_module.configuration(*args)
-            if config.name!=dot_join(parent_name,subpackage_name):
+            if config.name!=dot_join(parent_name, subpackage_name):
                 self.warn('Subpackage %r configuration returned as %r' % \
-                          (dot_join(parent_name,subpackage_name), config.name))
+                          (dot_join(parent_name, subpackage_name), config.name))
         finally:
             del sys.path[0]
         return config
@@ -679,9 +967,19 @@
                        subpackage_path=None,
                        parent_name=None,
                        caller_level = 1):
-        """ Return list of subpackage configurations.
-
-        '*' in subpackage_name is handled as a wildcard.
+        """Return list of subpackage configurations.
+
+        Parameters
+        ----------
+        subpackage_name : str or None
+            Name of the subpackage to get the configuration. '*' in
+            subpackage_name is handled as a wildcard.
+        subpackage_path : str
+            If None, then the path is assumed to be the local path plus the
+            subpackage_name. If a setup.py file is not found in the
+            subpackage_path, then a default configuration is used.
+        parent_name : str
+            Parent name.
         """
         if subpackage_name is None:
             if subpackage_path is None:
@@ -695,13 +993,13 @@
             return self._wildcard_get_subpackage(subpackage_name,
                                                  parent_name,
                                                  caller_level = caller_level+1)
-        assert '*' not in subpackage_name,`subpackage_name, subpackage_path,parent_name`
+        assert '*' not in subpackage_name, repr((subpackage_name, subpackage_path, parent_name))
         if subpackage_path is None:
             subpackage_path = njoin([self.local_path] + l)
         else:
             subpackage_path = njoin([subpackage_path] + l[:-1])
             subpackage_path = self.paths([subpackage_path])[0]
-        setup_py = njoin(subpackage_path, 'setup.py')
+        setup_py = njoin(subpackage_path, self.setup_name)
         if not self.options['ignore_setup_xxx_py']:
             if not os.path.isfile(setup_py):
                 setup_py = njoin(subpackage_path,
@@ -729,13 +1027,27 @@
     def add_subpackage(self,subpackage_name,
                        subpackage_path=None,
                        standalone = False):
-        """ Add subpackage to configuration.
-        """
+        """Add a sub-package to the current Configuration instance.
+
+        This is useful in a setup.py script for adding sub-packages to a
+        package.
+
+        Parameters
+        ----------
+        subpackage_name : str
+            name of the subpackage
+        subpackage_path : str
+            if given, the subpackage path such as the subpackage is in
+            subpackage_path / subpackage_name. If None,the subpackage is
+            assumed to be located in the local path / subpackage_name.
+        standalone : bool
+        """
+
         if standalone:
             parent_name = None
         else:
             parent_name = self.name
-        config_list = self.get_subpackage(subpackage_name,subpackage_path,
+        config_list = self.get_subpackage(subpackage_name, subpackage_path,
                                           parent_name = parent_name,
                                           caller_level = 2)
         if not config_list:
@@ -744,7 +1056,7 @@
             d = config
             if isinstance(config, Configuration):
                 d = config.todict()
-            assert isinstance(d,dict),`type(d)`
+            assert isinstance(d, dict), repr(type(d))
 
             self.info('Appending %s configuration to %s' \
                       % (d.get('name'), self.name))
@@ -754,31 +1066,69 @@
         if dist is not None:
             self.warn('distutils distribution has been initialized,'\
                       ' it may be too late to add a subpackage '+ subpackage_name)
-        return
-
-    def add_data_dir(self,data_path):
-        """ Recursively add files under data_path to data_files list.
-        Argument can be either
-        - 2-sequence (<datadir suffix>,<path to data directory>)
-        - path to data directory where python datadir suffix defaults
-          to package dir.
-
-        Rules for installation paths:
-          foo/bar -> (foo/bar, foo/bar) -> parent/foo/bar
-          (gun, foo/bar) -> parent/gun
-          foo/* -> (foo/a, foo/a), (foo/b, foo/b) -> parent/foo/a, parent/foo/b
-          (gun, foo/*) -> (gun, foo/a), (gun, foo/b) -> gun
-          (gun/*, foo/*) -> parent/gun/a, parent/gun/b
-          /foo/bar -> (bar, /foo/bar) -> parent/bar
-          (gun, /foo/bar) -> parent/gun
-          (fun/*/gun/*, sun/foo/bar) -> parent/fun/foo/gun/bar
+
+    def add_data_dir(self, data_path):
+        """Recursively add files under data_path to data_files list.
+
+        Recursively add files under data_path to the list of data_files to be
+        installed (and distributed). The data_path can be either a relative
+        path-name, or an absolute path-name, or a 2-tuple where the first
+        argument shows where in the install directory the data directory
+        should be installed to.
+
+        Parameters
+        ----------
+        data_path : seq or str
+            Argument can be either
+
+                * 2-sequence (<datadir suffix>, <path to data directory>)
+                * path to data directory where python datadir suffix defaults
+                  to package dir.
+
+        Notes
+        -----
+        Rules for installation paths::
+
+            foo/bar -> (foo/bar, foo/bar) -> parent/foo/bar
+            (gun, foo/bar) -> parent/gun
+            foo/* -> (foo/a, foo/a), (foo/b, foo/b) -> parent/foo/a, parent/foo/b
+            (gun, foo/*) -> (gun, foo/a), (gun, foo/b) -> gun
+            (gun/*, foo/*) -> parent/gun/a, parent/gun/b
+            /foo/bar -> (bar, /foo/bar) -> parent/bar
+            (gun, /foo/bar) -> parent/gun
+            (fun/*/gun/*, sun/foo/bar) -> parent/fun/foo/gun/bar
+
+        Examples
+        --------
+        For example suppose the source directory contains fun/foo.dat and
+        fun/bar/car.dat:
+
+        >>> self.add_data_dir('fun')                       #doctest: +SKIP
+        >>> self.add_data_dir(('sun', 'fun'))              #doctest: +SKIP
+        >>> self.add_data_dir(('gun', '/full/path/to/fun'))#doctest: +SKIP
+
+        Will install data-files to the locations::
+
+            <package install directory>/
+              fun/
+                foo.dat
+                bar/
+                  car.dat
+              sun/
+                foo.dat
+                bar/
+                  car.dat
+              gun/
+                foo.dat
+                car.dat
+
         """
         if is_sequence(data_path):
             d, data_path = data_path
         else:
             d = None
         if is_sequence(data_path):
-            [self.add_data_dir((d,p)) for p in data_path]
+            [self.add_data_dir((d, p)) for p in data_path]
             return
         if not is_string(data_path):
             raise TypeError("not a string: %r" % (data_path,))
@@ -792,14 +1142,14 @@
                 pattern_list = allpath(d).split(os.sep)
                 pattern_list.reverse()
                 # /a/*//b/ -> /a/*/b
-                rl = range(len(pattern_list)-1); rl.reverse()
+                rl = list(range(len(pattern_list)-1)); rl.reverse()
                 for i in rl:
                     if not pattern_list[i]:
                         del pattern_list[i]
                 #
                 for path in paths:
                     if not os.path.isdir(path):
-                        print 'Not a directory, skipping',path
+                        print('Not a directory, skipping', path)
                         continue
                     rpath = rel_path(path, self.local_path)
                     path_list = rpath.split(os.sep)
@@ -809,73 +1159,139 @@
                     for s in pattern_list:
                         if is_glob_pattern(s):
                             if i>=len(path_list):
-                                raise ValueError,'cannot fill pattern %r with %r' \
-                                      % (d, path)
+                                raise ValueError('cannot fill pattern %r with %r' \
+                                      % (d, path))
                             target_list.append(path_list[i])
                         else:
-                            assert s==path_list[i],`s,path_list[i],data_path,d,path,rpath`
+                            assert s==path_list[i], repr((s, path_list[i], data_path, d, path, rpath))
                             target_list.append(s)
                         i += 1
                     if path_list[i:]:
                         self.warn('mismatch of pattern_list=%s and path_list=%s'\
-                                  % (pattern_list,path_list))
+                                  % (pattern_list, path_list))
                     target_list.reverse()
-                    self.add_data_dir((os.sep.join(target_list),path))
+                    self.add_data_dir((os.sep.join(target_list), path))
             else:
                 for path in paths:
-                    self.add_data_dir((d,path))
+                    self.add_data_dir((d, path))
             return
-        assert not is_glob_pattern(d),`d`
+        assert not is_glob_pattern(d), repr(d)
 
         dist = self.get_distribution()
-        if dist is not None:
+        if dist is not None and dist.data_files is not None:
             data_files = dist.data_files
         else:
             data_files = self.data_files
 
         for path in paths:
-            for d1,f in list(general_source_directories_files(path)):
-                target_path = os.path.join(self.path_in_package,d,d1)
+            for d1, f in list(general_source_directories_files(path)):
+                target_path = os.path.join(self.path_in_package, d, d1)
                 data_files.append((target_path, f))
-        return
 
     def _optimize_data_files(self):
         data_dict = {}
-        for p,files in self.data_files:
-            if not data_dict.has_key(p):
+        for p, files in self.data_files:
+            if p not in data_dict:
                 data_dict[p] = set()
-            map(data_dict[p].add,files)
-        self.data_files[:] = [(p,list(files)) for p,files in data_dict.items()]
-        return
+            for f in files:
+                data_dict[p].add(f)
+        self.data_files[:] = [(p, list(files)) for p, files in data_dict.items()]
 
     def add_data_files(self,*files):
-        """ Add data files to configuration data_files.
-        Argument(s) can be either
-        - 2-sequence (<datadir prefix>,<path to data file(s)>)
-        - paths to data files where python datadir prefix defaults
-          to package dir.
+        """Add data files to configuration data_files.
+
+        Parameters
+        ----------
+        files : sequence
+            Argument(s) can be either
+
+                * 2-sequence (<datadir prefix>,<path to data file(s)>)
+                * paths to data files where python datadir prefix defaults
+                  to package dir.
+
+        Notes
+        -----
+        The form of each element of the files sequence is very flexible
+        allowing many combinations of where to get the files from the package
+        and where they should ultimately be installed on the system. The most
+        basic usage is for an element of the files argument sequence to be a
+        simple filename. This will cause that file from the local path to be
+        installed to the installation path of the self.name package (package
+        path). The file argument can also be a relative path in which case the
+        entire relative path will be installed into the package directory.
+        Finally, the file can be an absolute path name in which case the file
+        will be found at the absolute path name but installed to the package
+        path.
+
+        This basic behavior can be augmented by passing a 2-tuple in as the
+        file argument. The first element of the tuple should specify the
+        relative path (under the package install directory) where the
+        remaining sequence of files should be installed to (it has nothing to
+        do with the file-names in the source distribution). The second element
+        of the tuple is the sequence of files that should be installed. The
+        files in this sequence can be filenames, relative paths, or absolute
+        paths. For absolute paths the file will be installed in the top-level
+        package installation directory (regardless of the first argument).
+        Filenames and relative path names will be installed in the package
+        install directory under the path name given as the first element of
+        the tuple.
 
         Rules for installation paths:
-          file.txt -> (., file.txt)-> parent/file.txt
-          foo/file.txt -> (foo, foo/file.txt) -> parent/foo/file.txt
-          /foo/bar/file.txt -> (., /foo/bar/file.txt) -> parent/file.txt
-          *.txt -> parent/a.txt, parent/b.txt
-          foo/*.txt -> parent/foo/a.txt, parent/foo/b.txt
-          */*.txt -> (*, */*.txt) -> parent/c/a.txt, parent/d/b.txt
-          (sun, file.txt) -> parent/sun/file.txt
-          (sun, bar/file.txt) -> parent/sun/file.txt
-          (sun, /foo/bar/file.txt) -> parent/sun/file.txt
-          (sun, *.txt) -> parent/sun/a.txt, parent/sun/b.txt
-          (sun, bar/*.txt) -> parent/sun/a.txt, parent/sun/b.txt
-          (sun/*, */*.txt) -> parent/sun/c/a.txt, parent/d/b.txt
+
+          #. file.txt -> (., file.txt)-> parent/file.txt
+          #. foo/file.txt -> (foo, foo/file.txt) -> parent/foo/file.txt
+          #. /foo/bar/file.txt -> (., /foo/bar/file.txt) -> parent/file.txt
+          #. ``*``.txt -> parent/a.txt, parent/b.txt
+          #. foo/``*``.txt`` -> parent/foo/a.txt, parent/foo/b.txt
+          #. ``*/*.txt`` -> (``*``, ``*``/``*``.txt) -> parent/c/a.txt, parent/d/b.txt
+          #. (sun, file.txt) -> parent/sun/file.txt
+          #. (sun, bar/file.txt) -> parent/sun/file.txt
+          #. (sun, /foo/bar/file.txt) -> parent/sun/file.txt
+          #. (sun, ``*``.txt) -> parent/sun/a.txt, parent/sun/b.txt
+          #. (sun, bar/``*``.txt) -> parent/sun/a.txt, parent/sun/b.txt
+          #. (sun/``*``, ``*``/``*``.txt) -> parent/sun/c/a.txt, parent/d/b.txt
+
+        An additional feature is that the path to a data-file can actually be
+        a function that takes no arguments and returns the actual path(s) to
+        the data-files. This is useful when the data files are generated while
+        building the package.
+
+        Examples
+        --------
+        Add files to the list of data_files to be included with the package.
+
+            >>> self.add_data_files('foo.dat',
+            ...     ('fun', ['gun.dat', 'nun/pun.dat', '/tmp/sun.dat']),
+            ...     'bar/cat.dat',
+            ...     '/full/path/to/can.dat')                   #doctest: +SKIP
+
+        will install these data files to::
+
+            <package install directory>/
+             foo.dat
+             fun/
+               gun.dat
+               nun/
+                 pun.dat
+             sun.dat
+             bar/
+               car.dat
+             can.dat
+
+        where <package install directory> is the package (or sub-package)
+        directory such as '/usr/lib/python2.4/site-packages/mypackage' ('C:
+        \\Python2.4 \\Lib \\site-packages \\mypackage') or
+        '/usr/lib/python2.4/site- packages/mypackage/mysubpackage' ('C:
+        \\Python2.4 \\Lib \\site-packages \\mypackage \\mysubpackage').
         """
 
         if len(files)>1:
-            map(self.add_data_files, files)
+            for f in files:
+                self.add_data_files(f)
             return
         assert len(files)==1
         if is_sequence(files[0]):
-            d,files = files[0]
+            d, files = files[0]
         else:
             d = None
         if is_string(files):
@@ -885,19 +1301,19 @@
                 filepat = files[0]
             else:
                 for f in files:
-                    self.add_data_files((d,f))
+                    self.add_data_files((d, f))
                 return
         else:
-            raise TypeError,`type(files)`
+            raise TypeError(repr(type(files)))
 
         if d is None:
-            if callable(filepat):
+            if hasattr(filepat, '__call__'):
                 d = ''
             elif os.path.isabs(filepat):
                 d = ''
             else:
                 d = os.path.dirname(filepat)
-            self.add_data_files((d,files))
+            self.add_data_files((d, files))
             return
 
         paths = self.paths(filepat, include_non_existing=False)
@@ -920,88 +1336,162 @@
                     target_list.reverse()
                     self.add_data_files((os.sep.join(target_list), path))
             else:
-                self.add_data_files((d,paths))
+                self.add_data_files((d, paths))
             return
-        assert not is_glob_pattern(d),`d,filepat`
-
+        assert not is_glob_pattern(d), repr((d, filepat))
+
+        dist = self.get_distribution()
+        if dist is not None and dist.data_files is not None:
+            data_files = dist.data_files
+        else:
+            data_files = self.data_files
+
+        data_files.append((os.path.join(self.path_in_package, d), paths))
+
+    ### XXX Implement add_py_modules
+
+    def add_define_macros(self, macros):
+        """Add define macros to configuration
+
+        Add the given sequence of macro name and value duples to the beginning
+        of the define_macros list This list will be visible to all extension
+        modules of the current package.
+        """
         dist = self.get_distribution()
         if dist is not None:
-            data_files = dist.data_files
-        else:
-            data_files = self.data_files
-
-        data_files.append((os.path.join(self.path_in_package,d),paths))
-        return
-
-    ### XXX Implement add_py_modules
+            if not hasattr(dist, 'define_macros'):
+                dist.define_macros = []
+            dist.define_macros.extend(macros)
+        else:
+            self.define_macros.extend(macros)
+
 
     def add_include_dirs(self,*paths):
-        """ Add paths to configuration include directories.
+        """Add paths to configuration include directories.
+
+        Add the given sequence of paths to the beginning of the include_dirs
+        list. This list will be visible to all extension modules of the
+        current package.
         """
         include_dirs = self.paths(paths)
         dist = self.get_distribution()
         if dist is not None:
+            if dist.include_dirs is None:
+                dist.include_dirs = []
             dist.include_dirs.extend(include_dirs)
         else:
             self.include_dirs.extend(include_dirs)
-        return
 
     def add_headers(self,*files):
-        """ Add installable headers to configuration.
-        Argument(s) can be either
-        - 2-sequence (<includedir suffix>,<path to header file(s)>)
-        - path(s) to header file(s) where python includedir suffix will default
-          to package name.
+        """Add installable headers to configuration.
+
+        Add the given sequence of files to the beginning of the headers list.
+        By default, headers will be installed under <python-
+        include>/<self.name.replace('.','/')>/ directory. If an item of files
+        is a tuple, then its first argument specifies the actual installation
+        location relative to the <python-include> path.
+
+        Parameters
+        ----------
+        files : str or seq
+            Argument(s) can be either:
+
+                * 2-sequence (<includedir suffix>,<path to header file(s)>)
+                * path(s) to header file(s) where python includedir suffix will
+                  default to package name.
         """
         headers = []
         for path in files:
             if is_string(path):
-                [headers.append((self.name,p)) for p in self.paths(path)]
+                [headers.append((self.name, p)) for p in self.paths(path)]
             else:
                 if not isinstance(path, (tuple, list)) or len(path) != 2:
                     raise TypeError(repr(path))
-                [headers.append((path[0],p)) for p in self.paths(path[1])]
+                [headers.append((path[0], p)) for p in self.paths(path[1])]
         dist = self.get_distribution()
         if dist is not None:
+            if dist.headers is None:
+                dist.headers = []
             dist.headers.extend(headers)
         else:
             self.headers.extend(headers)
-        return
 
     def paths(self,*paths,**kws):
-        """ Apply glob to paths and prepend local_path if needed.
-        """
-        include_non_existing = kws.get('include_non_existing',True)
+        """Apply glob to paths and prepend local_path if needed.
+
+        Applies glob.glob(...) to each path in the sequence (if needed) and
+        pre-pends the local_path if needed. Because this is called on all
+        source lists, this allows wildcard characters to be specified in lists
+        of sources for extension modules and libraries and scripts and allows
+        path-names be relative to the source directory.
+
+        """
+        include_non_existing = kws.get('include_non_existing', True)
         return gpaths(paths,
                       local_path = self.local_path,
                       include_non_existing=include_non_existing)
 
-    def _fix_paths_dict(self,kw):
+    def _fix_paths_dict(self, kw):
         for k in kw.keys():
             v = kw[k]
-            if k in ['sources','depends','include_dirs','library_dirs',
-                     'module_dirs','extra_objects']:
+            if k in ['sources', 'depends', 'include_dirs', 'library_dirs',
+                     'module_dirs', 'extra_objects']:
                 new_v = self.paths(v)
                 kw[k] = new_v
-        return
 
     def add_extension(self,name,sources,**kw):
-        """ Add extension to configuration.
-
-        Keywords:
-          include_dirs, define_macros, undef_macros,
-          library_dirs, libraries, runtime_library_dirs,
-          extra_objects, extra_compile_args, extra_link_args,
-          export_symbols, swig_opts, depends, language,
-          f2py_options, module_dirs
-          extra_info - dict or list of dict of keywords to be
-                       appended to keywords.
+        """Add extension to configuration.
+
+        Create and add an Extension instance to the ext_modules list. This
+        method also takes the following optional keyword arguments that are
+        passed on to the Extension constructor.
+
+        Parameters
+        ----------
+        name : str
+            name of the extension
+        sources : seq
+            list of the sources. The list of sources may contain functions
+            (called source generators) which must take an extension instance
+            and a build directory as inputs and return a source file or list of
+            source files or None. If None is returned then no sources are
+            generated. If the Extension instance has no sources after
+            processing all source generators, then no extension module is
+            built.
+        include_dirs :
+        define_macros :
+        undef_macros :
+        library_dirs :
+        libraries :
+        runtime_library_dirs :
+        extra_objects :
+        extra_compile_args :
+        extra_link_args :
+        extra_f77_compile_args :
+        extra_f90_compile_args :
+        export_symbols :
+        swig_opts :
+        depends :
+            The depends list contains paths to files or directories that the
+            sources of the extension module depend on. If any path in the
+            depends list is newer than the extension module, then the module
+            will be rebuilt.
+        language :
+        f2py_options :
+        module_dirs :
+        extra_info : dict or list
+            dict or list of dict of keywords to be appended to keywords.
+
+        Notes
+        -----
+        The self.paths(...) method is applied to all lists that may contain
+        paths.
         """
         ext_args = copy.copy(kw)
-        ext_args['name'] = dot_join(self.name,name)
+        ext_args['name'] = dot_join(self.name, name)
         ext_args['sources'] = sources
 
-        if ext_args.has_key('extra_info'):
+        if 'extra_info' in ext_args:
             extra_info = ext_args['extra_info']
             del ext_args['extra_info']
             if isinstance(extra_info, dict):
@@ -1013,32 +1503,34 @@
         self._fix_paths_dict(ext_args)
 
         # Resolve out-of-tree dependencies
-        libraries = ext_args.get('libraries',[])
+        libraries = ext_args.get('libraries', [])
         libnames = []
         ext_args['libraries'] = []
         for libname in libraries:
-            if isinstance(libname,tuple):
+            if isinstance(libname, tuple):
                 self._fix_paths_dict(libname[1])
 
             # Handle library names of the form libname@relative/path/to/library
             if '@' in libname:
-                lname,lpath = libname.split('@',1)
-                lpath = os.path.abspath(njoin(self.local_path,lpath))
+                lname, lpath = libname.split('@', 1)
+                lpath = os.path.abspath(njoin(self.local_path, lpath))
                 if os.path.isdir(lpath):
-                    c = self.get_subpackage(None,lpath,
+                    c = self.get_subpackage(None, lpath,
                                             caller_level = 2)
-                    if isinstance(c,Configuration):
+                    if isinstance(c, Configuration):
                         c = c.todict()
-                    for l in [l[0] for l in c.get('libraries',[])]:
-                        llname = l.split('__OF__',1)[0]
+                    for l in [l[0] for l in c.get('libraries', [])]:
+                        llname = l.split('__OF__', 1)[0]
                         if llname == lname:
-                            c.pop('name',None)
+                            c.pop('name', None)
                             dict_append(ext_args,**c)
                             break
                     continue
             libnames.append(libname)
 
         ext_args['libraries'] = libnames + ext_args['libraries']
+        ext_args['define_macros'] = \
+            self.define_macros + ext_args.get('define_macros', [])
 
         from numpy.distutils.core import Extension
         ext = Extension(**ext_args)
@@ -1051,65 +1543,255 @@
         return ext
 
     def add_library(self,name,sources,**build_info):
-        """ Add library to configuration.
-
-        Valid keywords for build_info:
-          depends
-          macros
-          include_dirs
-          extra_compiler_args
-          f2py_options
-        """
-        build_info = copy.copy(build_info)
-        name = name #+ '__OF__' + self.name
-        build_info['sources'] = sources
-
-        self._fix_paths_dict(build_info)
-
-        self.libraries.append((name,build_info))
+        """
+        Add library to configuration.
+
+        Parameters
+        ----------
+        name : str
+            Name of the extension.
+        sources : sequence
+            List of the sources. The list of sources may contain functions
+            (called source generators) which must take an extension instance
+            and a build directory as inputs and return a source file or list of
+            source files or None. If None is returned then no sources are
+            generated. If the Extension instance has no sources after
+            processing all source generators, then no extension module is
+            built.
+        build_info : dict, optional
+            The following keys are allowed:
+
+                * depends
+                * macros
+                * include_dirs
+                * extra_compiler_args
+                * extra_f77_compile_args
+                * extra_f90_compile_args
+                * f2py_options
+                * language
+
+        """
+        self._add_library(name, sources, None, build_info)
 
         dist = self.get_distribution()
         if dist is not None:
             self.warn('distutils distribution has been initialized,'\
                       ' it may be too late to add a library '+ name)
-        return
+
+    def _add_library(self, name, sources, install_dir, build_info):
+        """Common implementation for add_library and add_installed_library. Do
+        not use directly"""
+        build_info = copy.copy(build_info)
+        build_info['sources'] = sources
+
+        # Sometimes, depends is not set up to an empty list by default, and if
+        # depends is not given to add_library, distutils barfs (#1134)
+        if not 'depends' in build_info:
+            build_info['depends'] = []
+
+        self._fix_paths_dict(build_info)
+
+        # Add to libraries list so that it is build with build_clib
+        self.libraries.append((name, build_info))
+
+    def add_installed_library(self, name, sources, install_dir, build_info=None):
+        """
+        Similar to add_library, but the specified library is installed.
+
+        Most C libraries used with `distutils` are only used to build python
+        extensions, but libraries built through this method will be installed
+        so that they can be reused by third-party packages.
+
+        Parameters
+        ----------
+        name : str
+            Name of the installed library.
+        sources : sequence
+            List of the library's source files. See `add_library` for details.
+        install_dir : str
+            Path to install the library, relative to the current sub-package.
+        build_info : dict, optional
+            The following keys are allowed:
+
+                * depends
+                * macros
+                * include_dirs
+                * extra_compiler_args
+                * extra_f77_compile_args
+                * extra_f90_compile_args
+                * f2py_options
+                * language
+
+        Returns
+        -------
+        None
+
+        See Also
+        --------
+        add_library, add_npy_pkg_config, get_info
+
+        Notes
+        -----
+        The best way to encode the options required to link against the specified
+        C libraries is to use a "libname.ini" file, and use `get_info` to
+        retrieve the required options (see `add_npy_pkg_config` for more
+        information).
+
+        """
+        if not build_info:
+            build_info = {}
+
+        install_dir = os.path.join(self.package_path, install_dir)
+        self._add_library(name, sources, install_dir, build_info)
+        self.installed_libraries.append(InstallableLib(name, build_info, install_dir))
+
+    def add_npy_pkg_config(self, template, install_dir, subst_dict=None):
+        """
+        Generate and install a npy-pkg config file from a template.
+
+        The config file generated from `template` is installed in the
+        given install directory, using `subst_dict` for variable substitution.
+
+        Parameters
+        ----------
+        template : str
+            The path of the template, relatively to the current package path.
+        install_dir : str
+            Where to install the npy-pkg config file, relatively to the current
+            package path.
+        subst_dict : dict, optional
+            If given, any string of the form ``@key@`` will be replaced by
+            ``subst_dict[key]`` in the template file when installed. The install
+            prefix is always available through the variable ``@prefix@``, since the
+            install prefix is not easy to get reliably from setup.py.
+
+        See also
+        --------
+        add_installed_library, get_info
+
+        Notes
+        -----
+        This works for both standard installs and in-place builds, i.e. the
+        ``@prefix@`` refer to the source directory for in-place builds.
+
+        Examples
+        --------
+        ::
+
+            config.add_npy_pkg_config('foo.ini.in', 'lib', {'foo': bar})
+
+        Assuming the foo.ini.in file has the following content::
+
+            [meta]
+            Name=@foo@
+            Version=1.0
+            Description=dummy description
+
+            [default]
+            Cflags=-I@prefix@/include
+            Libs=
+
+        The generated file will have the following content::
+
+            [meta]
+            Name=bar
+            Version=1.0
+            Description=dummy description
+
+            [default]
+            Cflags=-Iprefix_dir/include
+            Libs=
+
+        and will be installed as foo.ini in the 'lib' subpath.
+
+        When cross-compiling with numpy distutils, it might be necessary to
+        use modified npy-pkg-config files.  Using the default/generated files
+        will link with the host libraries (i.e. libnpymath.a).  For
+        cross-compilation you of-course need to link with target libraries,
+        while using the host Python installation.
+
+        You can copy out the numpy/core/lib/npy-pkg-config directory, add a
+        pkgdir value to the .ini files and set NPY_PKG_CONFIG_PATH environment
+        variable to point to the directory with the modified npy-pkg-config
+        files.
+
+        Example npymath.ini modified for cross-compilation::
+
+            [meta]
+            Name=npymath
+            Description=Portable, core math library implementing C99 standard
+            Version=0.1
+
+            [variables]
+            pkgname=numpy.core
+            pkgdir=/build/arm-linux-gnueabi/sysroot/usr/lib/python3.7/site-packages/numpy/core
+            prefix=${pkgdir}
+            libdir=${prefix}/lib
+            includedir=${prefix}/include
+
+            [default]
+            Libs=-L${libdir} -lnpymath
+            Cflags=-I${includedir}
+            Requires=mlib
+
+            [msvc]
+            Libs=/LIBPATH:${libdir} npymath.lib
+            Cflags=/INCLUDE:${includedir}
+            Requires=mlib
+
+        """
+        if subst_dict is None:
+            subst_dict = {}
+        template = os.path.join(self.package_path, template)
+
+        if self.name in self.installed_pkg_config:
+            self.installed_pkg_config[self.name].append((template, install_dir,
+                subst_dict))
+        else:
+            self.installed_pkg_config[self.name] = [(template, install_dir,
+                subst_dict)]
+
 
     def add_scripts(self,*files):
-        """ Add scripts to configuration.
+        """Add scripts to configuration.
+
+        Add the sequence of files to the beginning of the scripts list.
+        Scripts will be installed under the <prefix>/bin/ directory.
+
         """
         scripts = self.paths(files)
         dist = self.get_distribution()
         if dist is not None:
+            if dist.scripts is None:
+                dist.scripts = []
             dist.scripts.extend(scripts)
         else:
             self.scripts.extend(scripts)
-        return
 
     def dict_append(self,**dict):
         for key in self.list_keys:
-            a = getattr(self,key)
-            a.extend(dict.get(key,[]))
+            a = getattr(self, key)
+            a.extend(dict.get(key, []))
         for key in self.dict_keys:
-            a = getattr(self,key)
-            a.update(dict.get(key,{}))
+            a = getattr(self, key)
+            a.update(dict.get(key, {}))
         known_keys = self.list_keys + self.dict_keys + self.extra_keys
         for key in dict.keys():
             if key not in known_keys:
                 a = getattr(self, key, None)
                 if a and a==dict[key]: continue
                 self.warn('Inheriting attribute %r=%r from %r' \
-                          % (key,dict[key],dict.get('name','?')))
-                setattr(self,key,dict[key])
+                          % (key, dict[key], dict.get('name', '?')))
+                setattr(self, key, dict[key])
                 self.extra_keys.append(key)
             elif key in self.extra_keys:
                 self.info('Ignoring attempt to set %r (from %r to %r)' \
-                          % (key, getattr(self,key), dict[key]))
+                          % (key, getattr(self, key), dict[key]))
             elif key in known_keys:
                 # key is already processed above
                 pass
             else:
-                raise ValueError, "Don't know about key=%r" % (key)
-        return
+                raise ValueError("Don't know about key=%r" % (key))
 
     def __str__(self):
         from pprint import pformat
@@ -1118,56 +1800,75 @@
         s += 'Configuration of '+self.name+':\n'
         known_keys.sort()
         for k in known_keys:
-            a = getattr(self,k,None)
+            a = getattr(self, k, None)
             if a:
-                s += '%s = %s\n' % (k,pformat(a))
+                s += '%s = %s\n' % (k, pformat(a))
         s += 5*'-' + '>'
         return s
 
     def get_config_cmd(self):
+        """
+        Returns the numpy.distutils config command instance.
+        """
         cmd = get_cmd('config')
         cmd.ensure_finalized()
         cmd.dump_source = 0
         cmd.noisy = 0
         old_path = os.environ.get('PATH')
         if old_path:
-            path = os.pathsep.join(['.',old_path])
+            path = os.pathsep.join(['.', old_path])
             os.environ['PATH'] = path
         return cmd
 
     def get_build_temp_dir(self):
+        """
+        Return a path to a temporary directory where temporary files should be
+        placed.
+        """
         cmd = get_cmd('build')
         cmd.ensure_finalized()
         return cmd.build_temp
 
     def have_f77c(self):
-        """ Check for availability of Fortran 77 compiler.
+        """Check for availability of Fortran 77 compiler.
+
         Use it inside source generating function to ensure that
         setup distribution instance has been initialized.
+
+        Notes
+        -----
+        True if a Fortran 77 compiler is available (because a simple Fortran 77
+        code was able to be compiled successfully).
         """
         simple_fortran_subroutine = '''
         subroutine simple
         end
         '''
         config_cmd = self.get_config_cmd()
-        flag = config_cmd.try_compile(simple_fortran_subroutine,lang='f77')
+        flag = config_cmd.try_compile(simple_fortran_subroutine, lang='f77')
         return flag
 
     def have_f90c(self):
-        """ Check for availability of Fortran 90 compiler.
+        """Check for availability of Fortran 90 compiler.
+
         Use it inside source generating function to ensure that
         setup distribution instance has been initialized.
+
+        Notes
+        -----
+        True if a Fortran 90 compiler is available (because a simple Fortran
+        90 code was able to be compiled successfully)
         """
         simple_fortran_subroutine = '''
         subroutine simple
         end
         '''
         config_cmd = self.get_config_cmd()
-        flag = config_cmd.try_compile(simple_fortran_subroutine,lang='f90')
+        flag = config_cmd.try_compile(simple_fortran_subroutine, lang='f90')
         return flag
 
     def append_to(self, extlib):
-        """ Append libraries, include_dirs to extension or library item.
+        """Append libraries, include_dirs to extension or library item.
         """
         if is_sequence(extlib):
             lib_name, build_info = extlib
@@ -1176,28 +1877,91 @@
                         include_dirs=self.include_dirs)
         else:
             from numpy.distutils.core import Extension
-            assert isinstance(extlib,Extension), repr(extlib)
+            assert isinstance(extlib, Extension), repr(extlib)
             extlib.libraries.extend(self.libraries)
             extlib.include_dirs.extend(self.include_dirs)
-        return
-
-    def _get_svn_revision(self,path):
-        """ Return path's SVN revision number.
-        """
-        entries = njoin(path,'.svn','entries')
-        revision = None
+
+    def _get_svn_revision(self, path):
+        """Return path's SVN revision number.
+        """
+        try:
+            output = subprocess.check_output(['svnversion'], cwd=path)
+        except (subprocess.CalledProcessError, OSError):
+            pass
+        else:
+            m = re.match(rb'(?P<revision>\d+)', output)
+            if m:
+                return int(m.group('revision'))
+
+        if sys.platform=='win32' and os.environ.get('SVN_ASP_DOT_NET_HACK', None):
+            entries = njoin(path, '_svn', 'entries')
+        else:
+            entries = njoin(path, '.svn', 'entries')
         if os.path.isfile(entries):
-            f = open(entries)
-            m = re.search(r'revision="(?P<revision>\d+)"',f.read())
-            f.close()
+            with open(entries) as f:
+                fstr = f.read()
+            if fstr[:5] == '<?xml':  # pre 1.4
+                m = re.search(r'revision="(?P<revision>\d+)"', fstr)
+                if m:
+                    return int(m.group('revision'))
+            else:  # non-xml entries file --- check to be sure that
+                m = re.search(r'dir[\n\r]+(?P<revision>\d+)', fstr)
+                if m:
+                    return int(m.group('revision'))
+        return None
+
+    def _get_hg_revision(self, path):
+        """Return path's Mercurial revision number.
+        """
+        try:
+            output = subprocess.check_output(
+                ['hg', 'identify', '--num'], cwd=path)
+        except (subprocess.CalledProcessError, OSError):
+            pass
+        else:
+            m = re.match(rb'(?P<revision>\d+)', output)
             if m:
-                revision = int(m.group('revision'))
-        return revision
+                return int(m.group('revision'))
+
+        branch_fn = njoin(path, '.hg', 'branch')
+        branch_cache_fn = njoin(path, '.hg', 'branch.cache')
+
+        if os.path.isfile(branch_fn):
+            branch0 = None
+            with open(branch_fn) as f:
+                revision0 = f.read().strip()
+
+            branch_map = {}
+            with open(branch_cache_fn, 'r') as f:
+                for line in f:
+                    branch1, revision1  = line.split()[:2]
+                    if revision1==revision0:
+                        branch0 = branch1
+                    try:
+                        revision1 = int(revision1)
+                    except ValueError:
+                        continue
+                    branch_map[branch1] = revision1
+
+            return branch_map.get(branch0)
+
+        return None
+
 
     def get_version(self, version_file=None, version_variable=None):
-        """ Try to get version string of a package.
-        """
-        version = getattr(self,'version',None)
+        """Try to get version string of a package.
+
+        Return a version string of the current package or None if the version
+        information could not be detected.
+
+        Notes
+        -----
+        This method scans files named
+        __version__.py, <packagename>_version.py, version.py, and
+        __svn_version__.py for string variables version, __version__, and
+        <packagename>_version, until a version number is found.
+        """
+        version = getattr(self, 'version', None)
         if version is not None:
             return version
 
@@ -1206,7 +1970,8 @@
             files = ['__version__.py',
                      self.name.split('.')[-1]+'_version.py',
                      'version.py',
-                     '__svn_version__.py']
+                     '__svn_version__.py',
+                     '__hg_version__.py']
         else:
             files = [version_file]
         if version_variable is None:
@@ -1216,23 +1981,31 @@
         else:
             version_vars = [version_variable]
         for f in files:
-            fn = njoin(self.local_path,f)
+            fn = njoin(self.local_path, f)
             if os.path.isfile(fn):
-                info = (open(fn),fn,('.py','U',1))
+                info = ('.py', 'U', 1)
                 name = os.path.splitext(os.path.basename(fn))[0]
-                n = dot_join(self.name,name)
+                n = dot_join(self.name, name)
                 try:
-                    version_module = imp.load_module('_'.join(n.split('.')),*info)
-                except ImportError,msg:
-                    self.warn(str(msg))
+                    version_module = exec_mod_from_location(
+                                        '_'.join(n.split('.')), fn)
+                except ImportError as e:
+                    self.warn(str(e))
                     version_module = None
                 if version_module is None:
                     continue
 
                 for a in version_vars:
-                    version = getattr(version_module,a,None)
+                    version = getattr(version_module, a, None)
                     if version is not None:
                         break
+
+                # Try if versioneer module
+                try:
+                    version = version_module.get_versions()['version']
+                except AttributeError:
+                    pass
+
                 if version is not None:
                     break
 
@@ -1240,57 +2013,115 @@
             self.version = version
             return version
 
-        # Get version as SVN revision number
+        # Get version as SVN or Mercurial revision number
         revision = self._get_svn_revision(self.local_path)
+        if revision is None:
+            revision = self._get_hg_revision(self.local_path)
+
         if revision is not None:
             version = str(revision)
             self.version = version
 
         return version
 
-    def make_svn_version_py(self):
-        """ Generate package __svn_version__.py file from SVN revision number,
+    def make_svn_version_py(self, delete=True):
+        """Appends a data function to the data_files list that will generate
+        __svn_version__.py file to the current package directory.
+
+        Generate package __svn_version__.py file from SVN revision number,
         it will be removed after python exits but will be available
         when sdist, etc commands are executed.
 
+        Notes
+        -----
         If __svn_version__.py existed before, nothing is done.
-        """
-        target = njoin(self.local_path,'__svn_version__.py')
-        if os.path.isfile(target):
+
+        This is
+        intended for working with source directories that are in an SVN
+        repository.
+        """
+        target = njoin(self.local_path, '__svn_version__.py')
+        revision = self._get_svn_revision(self.local_path)
+        if os.path.isfile(target) or revision is None:
             return
-        def generate_svn_version_py():
-            if not os.path.isfile(target):
-                revision = self._get_svn_revision(self.local_path)
-                assert revision is not None,'hmm, why I am not inside SVN tree???'
-                version = str(revision)
-                self.info('Creating %s (version=%r)' % (target,version))
-                f = open(target,'w')
-                f.write('version = %r\n' % (version))
-                f.close()
-
-            import atexit
-            def rm_file(f=target,p=self.info):
-                try: os.remove(f); p('removed '+f)
-                except OSError: pass
-                try: os.remove(f+'c'); p('removed '+f+'c')
-                except OSError: pass
-            atexit.register(rm_file)
-
-            return target
-
-        self.add_data_files(('', generate_svn_version_py()))
+        else:
+            def generate_svn_version_py():
+                if not os.path.isfile(target):
+                    version = str(revision)
+                    self.info('Creating %s (version=%r)' % (target, version))
+                    with open(target, 'w') as f:
+                        f.write('version = %r\n' % (version))
+
+                def rm_file(f=target,p=self.info):
+                    if delete:
+                        try: os.remove(f); p('removed '+f)
+                        except OSError: pass
+                        try: os.remove(f+'c'); p('removed '+f+'c')
+                        except OSError: pass
+
+                atexit.register(rm_file)
+
+                return target
+
+            self.add_data_files(('', generate_svn_version_py()))
+
+    def make_hg_version_py(self, delete=True):
+        """Appends a data function to the data_files list that will generate
+        __hg_version__.py file to the current package directory.
+
+        Generate package __hg_version__.py file from Mercurial revision,
+        it will be removed after python exits but will be available
+        when sdist, etc commands are executed.
+
+        Notes
+        -----
+        If __hg_version__.py existed before, nothing is done.
+
+        This is intended for working with source directories that are
+        in an Mercurial repository.
+        """
+        target = njoin(self.local_path, '__hg_version__.py')
+        revision = self._get_hg_revision(self.local_path)
+        if os.path.isfile(target) or revision is None:
+            return
+        else:
+            def generate_hg_version_py():
+                if not os.path.isfile(target):
+                    version = str(revision)
+                    self.info('Creating %s (version=%r)' % (target, version))
+                    with open(target, 'w') as f:
+                        f.write('version = %r\n' % (version))
+
+                def rm_file(f=target,p=self.info):
+                    if delete:
+                        try: os.remove(f); p('removed '+f)
+                        except OSError: pass
+                        try: os.remove(f+'c'); p('removed '+f+'c')
+                        except OSError: pass
+
+                atexit.register(rm_file)
+
+                return target
+
+            self.add_data_files(('', generate_hg_version_py()))
 
     def make_config_py(self,name='__config__'):
-        """ Generate package __config__.py file containing system_info
+        """Generate package __config__.py file containing system_info
         information used during building the package.
-        """
-        self.py_modules.append((self.name,name,generate_config_py))
-        return
+
+        This file is installed to the
+        package installation directory.
+
+        """
+        self.py_modules.append((self.name, name, generate_config_py))
 
     def get_info(self,*names):
-        """ Get resources information.
-        """
-        from system_info import get_info, dict_append
+        """Get resources information.
+
+        Return information (from system_info.get_info) for all of the names in
+        the argument list in a single dictionary.
+        """
+        from .system_info import get_info, dict_append
         info_dict = {}
         for a in names:
             dict_append(info_dict,**get_info(a))
@@ -1298,7 +2129,7 @@
 
 
 def get_cmd(cmdname, _cache={}):
-    if not _cache.has_key(cmdname):
+    if cmdname not in _cache:
         import distutils.core
         dist = distutils.core._setup_distribution
         if dist is None:
@@ -1314,21 +2145,142 @@
     include_dirs = Configuration.numpy_include_dirs[:]
     if not include_dirs:
         import numpy
-        if numpy.show_config is None:
-            # running from numpy_core source directory
-            include_dirs.append(njoin(os.path.dirname(numpy.__file__),
-                                             'core', 'include'))
-        else:
-            # using installed numpy core headers
-            import numpy.core as core
-            include_dirs.append(njoin(os.path.dirname(core.__file__), 'include'))
+        include_dirs = [ numpy.get_include() ]
     # else running numpy/core/setup.py
     return include_dirs
 
+def get_npy_pkg_dir():
+    """Return the path where to find the npy-pkg-config directory.
+
+    If the NPY_PKG_CONFIG_PATH environment variable is set, the value of that
+    is returned.  Otherwise, a path inside the location of the numpy module is
+    returned.
+
+    The NPY_PKG_CONFIG_PATH can be useful when cross-compiling, maintaining
+    customized npy-pkg-config .ini files for the cross-compilation
+    environment, and using them when cross-compiling.
+
+    """
+    d = os.environ.get('NPY_PKG_CONFIG_PATH')
+    if d is not None:
+        return d
+    spec = importlib.util.find_spec('numpy')
+    d = os.path.join(os.path.dirname(spec.origin),
+            'core', 'lib', 'npy-pkg-config')
+    return d
+
+def get_pkg_info(pkgname, dirs=None):
+    """
+    Return library info for the given package.
+
+    Parameters
+    ----------
+    pkgname : str
+        Name of the package (should match the name of the .ini file, without
+        the extension, e.g. foo for the file foo.ini).
+    dirs : sequence, optional
+        If given, should be a sequence of additional directories where to look
+        for npy-pkg-config files. Those directories are searched prior to the
+        NumPy directory.
+
+    Returns
+    -------
+    pkginfo : class instance
+        The `LibraryInfo` instance containing the build information.
+
+    Raises
+    ------
+    PkgNotFound
+        If the package is not found.
+
+    See Also
+    --------
+    Configuration.add_npy_pkg_config, Configuration.add_installed_library,
+    get_info
+
+    """
+    from numpy.distutils.npy_pkg_config import read_config
+
+    if dirs:
+        dirs.append(get_npy_pkg_dir())
+    else:
+        dirs = [get_npy_pkg_dir()]
+    return read_config(pkgname, dirs)
+
+def get_info(pkgname, dirs=None):
+    """
+    Return an info dict for a given C library.
+
+    The info dict contains the necessary options to use the C library.
+
+    Parameters
+    ----------
+    pkgname : str
+        Name of the package (should match the name of the .ini file, without
+        the extension, e.g. foo for the file foo.ini).
+    dirs : sequence, optional
+        If given, should be a sequence of additional directories where to look
+        for npy-pkg-config files. Those directories are searched prior to the
+        NumPy directory.
+
+    Returns
+    -------
+    info : dict
+        The dictionary with build information.
+
+    Raises
+    ------
+    PkgNotFound
+        If the package is not found.
+
+    See Also
+    --------
+    Configuration.add_npy_pkg_config, Configuration.add_installed_library,
+    get_pkg_info
+
+    Examples
+    --------
+    To get the necessary information for the npymath library from NumPy:
+
+    >>> npymath_info = np.distutils.misc_util.get_info('npymath')
+    >>> npymath_info                                    #doctest: +SKIP
+    {'define_macros': [], 'libraries': ['npymath'], 'library_dirs':
+    ['.../numpy/core/lib'], 'include_dirs': ['.../numpy/core/include']}
+
+    This info dict can then be used as input to a `Configuration` instance::
+
+      config.add_extension('foo', sources=['foo.c'], extra_info=npymath_info)
+
+    """
+    from numpy.distutils.npy_pkg_config import parse_flags
+    pkg_info = get_pkg_info(pkgname, dirs)
+
+    # Translate LibraryInfo instance into a build_info dict
+    info = parse_flags(pkg_info.cflags())
+    for k, v in parse_flags(pkg_info.libs()).items():
+        info[k].extend(v)
+
+    # add_extension extra_info argument is ANAL
+    info['define_macros'] = info['macros']
+    del info['macros']
+    del info['ignored']
+
+    return info
+
+def is_bootstrapping():
+    import builtins
+
+    try:
+        builtins.__NUMPY_SETUP__
+        return True
+    except AttributeError:
+        return False
+
+
 #########################
 
 def default_config_dict(name = None, parent_name = None, local_path=None):
-    """ Return a configuration dictionary for usage in
+    """Return a configuration dictionary for usage in
     configuration() function defined in file setup_<name>.py.
     """
     import warnings
@@ -1336,15 +2288,19 @@
                   'deprecated default_config_dict(%r,%r,%r)'
                   % (name, parent_name, local_path,
                      name, parent_name, local_path,
-                     ))
+                     ), stacklevel=2)
     c = Configuration(name, parent_name, local_path)
     return c.todict()
 
 
 def dict_append(d, **kws):
     for k, v in kws.items():
-        if d.has_key(k):
-            d[k].extend(v)
+        if k in d:
+            ov = d[k]
+            if isinstance(ov, str):
+                d[k] = v
+            else:
+                d[k].extend(v)
         else:
             d[k] = v
 
@@ -1370,36 +2326,168 @@
     return os.path.normpath(njoin(drive + prefix, subpath))
 
 def generate_config_py(target):
-    """ Generate config.py file containing system_info information
+    """Generate config.py file containing system_info information
     used during building the package.
 
-    Usage:\
+    Usage:
         config['py_modules'].append((packagename, '__config__',generate_config_py))
     """
     from numpy.distutils.system_info import system_info
     from distutils.dir_util import mkpath
     mkpath(os.path.dirname(target))
-    f = open(target, 'w')
-    f.write('# This file is generated by %s\n' % (os.path.abspath(sys.argv[0])))
-    f.write('# It contains system_info results at the time of building this package.\n')
-    f.write('__all__ = ["get_info","show"]\n\n')
-    for k, i in system_info.saved_results.items():
-        f.write('%s=%r\n' % (k, i))
-    f.write('\ndef get_info(name): g=globals(); return g.get(name,g.get(name+"_info",{}))\n')
-    f.write('''
-def show():
-    for name,info_dict in globals().items():
-        if name[0]=="_" or type(info_dict) is not type({}): continue
-        print name+":"
-        if not info_dict:
-            print "  NOT AVAILABLE"
-        for k,v in info_dict.items():
-            v = str(v)
-            if k==\'sources\' and len(v)>200: v = v[:60]+\' ...\\n... \'+v[-60:]
-            print \'    %s = %s\'%(k,v)
-        print
-    return
-    ''')
-
-    f.close()
+    with open(target, 'w') as f:
+        f.write('# This file is generated by numpy\'s %s\n' % (os.path.basename(sys.argv[0])))
+        f.write('# It contains system_info results at the time of building this package.\n')
+        f.write('__all__ = ["get_info","show"]\n\n')
+
+        # For gfortran+msvc combination, extra shared libraries may exist
+        f.write(textwrap.dedent("""
+            import os
+            import sys
+
+            extra_dll_dir = os.path.join(os.path.dirname(__file__), '.libs')
+
+            if sys.platform == 'win32' and os.path.isdir(extra_dll_dir):
+                os.add_dll_directory(extra_dll_dir)
+
+            """))
+
+        for k, i in system_info.saved_results.items():
+            f.write('%s=%r\n' % (k, i))
+        f.write(textwrap.dedent(r'''
+            def get_info(name):
+                g = globals()
+                return g.get(name, g.get(name + "_info", {}))
+
+            def show():
+                """
+                Show libraries in the system on which NumPy was built.
+
+                Print information about various resources (libraries, library
+                directories, include directories, etc.) in the system on which
+                NumPy was built.
+
+                See Also
+                --------
+                get_include : Returns the directory containing NumPy C
+                              header files.
+
+                Notes
+                -----
+                1. Classes specifying the information to be printed are defined
+                   in the `numpy.distutils.system_info` module.
+
+                   Information may include:
+
+                   * ``language``: language used to write the libraries (mostly
+                     C or f77)
+                   * ``libraries``: names of libraries found in the system
+                   * ``library_dirs``: directories containing the libraries
+                   * ``include_dirs``: directories containing library header files
+                   * ``src_dirs``: directories containing library source files
+                   * ``define_macros``: preprocessor macros used by
+                     ``distutils.setup``
+                   * ``baseline``: minimum CPU features required
+                   * ``found``: dispatched features supported in the system
+                   * ``not found``: dispatched features that are not supported
+                     in the system
+
+                2. NumPy BLAS/LAPACK Installation Notes
+
+                   Installing a numpy wheel (``pip install numpy`` or force it
+                   via ``pip install numpy --only-binary :numpy: numpy``) includes
+                   an OpenBLAS implementation of the BLAS and LAPACK linear algebra
+                   APIs. In this case, ``library_dirs`` reports the original build
+                   time configuration as compiled with gcc/gfortran; at run time
+                   the OpenBLAS library is in
+                   ``site-packages/numpy.libs/`` (linux), or
+                   ``site-packages/numpy/.dylibs/`` (macOS), or
+                   ``site-packages/numpy/.libs/`` (windows).
+
+                   Installing numpy from source
+                   (``pip install numpy --no-binary numpy``) searches for BLAS and
+                   LAPACK dynamic link libraries at build time as influenced by
+                   environment variables NPY_BLAS_LIBS, NPY_CBLAS_LIBS, and
+                   NPY_LAPACK_LIBS; or NPY_BLAS_ORDER and NPY_LAPACK_ORDER;
+                   or the optional file ``~/.numpy-site.cfg``.
+                   NumPy remembers those locations and expects to load the same
+                   libraries at run-time.
+                   In NumPy 1.21+ on macOS, 'accelerate' (Apple's Accelerate BLAS
+                   library) is in the default build-time search order after
+                   'openblas'.
+
+                Examples
+                --------
+                >>> import numpy as np
+                >>> np.show_config()
+                blas_opt_info:
+                    language = c
+                    define_macros = [('HAVE_CBLAS', None)]
+                    libraries = ['openblas', 'openblas']
+                    library_dirs = ['/usr/local/lib']
+                """
+                from numpy.core._multiarray_umath import (
+                    __cpu_features__, __cpu_baseline__, __cpu_dispatch__
+                )
+                for name,info_dict in globals().items():
+                    if name[0] == "_" or type(info_dict) is not type({}): continue
+                    print(name + ":")
+                    if not info_dict:
+                        print("  NOT AVAILABLE")
+                    for k,v in info_dict.items():
+                        v = str(v)
+                        if k == "sources" and len(v) > 200:
+                            v = v[:60] + " ...\n... " + v[-60:]
+                        print("    %s = %s" % (k,v))
+
+                features_found, features_not_found = [], []
+                for feature in __cpu_dispatch__:
+                    if __cpu_features__[feature]:
+                        features_found.append(feature)
+                    else:
+                        features_not_found.append(feature)
+
+                print("Supported SIMD extensions in this NumPy install:")
+                print("    baseline = %s" % (','.join(__cpu_baseline__)))
+                print("    found = %s" % (','.join(features_found)))
+                print("    not found = %s" % (','.join(features_not_found)))
+
+                    '''))
+
     return target
+
+def msvc_version(compiler):
+    """Return version major and minor of compiler instance if it is
+    MSVC, raise an exception otherwise."""
+    if not compiler.compiler_type == "msvc":
+        raise ValueError("Compiler instance is not msvc (%s)"\
+                         % compiler.compiler_type)
+    return compiler._MSVCCompiler__version
+
+def get_build_architecture():
+    # Importing distutils.msvccompiler triggers a warning on non-Windows
+    # systems, so delay the import to here.
+    from distutils.msvccompiler import get_build_architecture
+    return get_build_architecture()
+
+
+_cxx_ignore_flags = {'-Werror=implicit-function-declaration', '-std=c99'}
+
+
+def sanitize_cxx_flags(cxxflags):
+    '''
+    Some flags are valid for C but not C++. Prune them.
+    '''
+    return [flag for flag in cxxflags if flag not in _cxx_ignore_flags]
+
+
+def exec_mod_from_location(modname, modfile):
+    '''
+    Use importlib machinery to import a module `modname` from the file
+    `modfile`. Depending on the `spec.loader`, the module may not be
+    registered in sys.modules.
+    '''
+    spec = importlib.util.spec_from_file_location(modname, modfile)
+    foo = importlib.util.module_from_spec(spec)
+    spec.loader.exec_module(foo)
+    return foo
('numpy/distutils', 'log.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,47 +1,111 @@
-# Colored log, requires Python 2.3 or up.
-
+# Colored log
 import sys
-from distutils.log import *
+from distutils.log import *  # noqa: F403
 from distutils.log import Log as old_Log
 from distutils.log import _global_log
-from misc_util import red_text, yellow_text, cyan_text, is_sequence, is_string
+
+from numpy.distutils.misc_util import (red_text, default_text, cyan_text,
+        green_text, is_sequence, is_string)
 
 
 def _fix_args(args,flag=1):
     if is_string(args):
-        return args.replace('%','%%')
+        return args.replace('%', '%%')
     if flag and is_sequence(args):
-        return tuple([_fix_args(a,flag=0) for a in args])
+        return tuple([_fix_args(a, flag=0) for a in args])
     return args
+
 
 class Log(old_Log):
     def _log(self, level, msg, args):
-        if level>= self.threshold:
+        if level >= self.threshold:
             if args:
-                print _global_color_map[level](msg % _fix_args(args))
+                msg = msg % _fix_args(args)
+            if 0:
+                if msg.startswith('copying ') and msg.find(' -> ') != -1:
+                    return
+                if msg.startswith('byte-compiling '):
+                    return
+            print(_global_color_map[level](msg))
+            sys.stdout.flush()
+
+    def good(self, msg, *args):
+        """
+        If we log WARN messages, log this message as a 'nice' anti-warn
+        message.
+
+        """
+        if WARN >= self.threshold:
+            if args:
+                print(green_text(msg % _fix_args(args)))
             else:
-                print _global_color_map[level](msg)
+                print(green_text(msg))
             sys.stdout.flush()
+
+
 _global_log.__class__ = Log
 
-def set_verbosity(v):
+good = _global_log.good
+
+def set_threshold(level, force=False):
     prev_level = _global_log.threshold
-    if v<0:
-        set_threshold(ERROR)
+    if prev_level > DEBUG or force:
+        # If we're running at DEBUG, don't change the threshold, as there's
+        # likely a good reason why we're running at this level.
+        _global_log.threshold = level
+        if level <= DEBUG:
+            info('set_threshold: setting threshold to DEBUG level,'
+                    ' it can be changed only with force argument')
+    else:
+        info('set_threshold: not changing threshold from DEBUG level'
+                ' %s to %s' % (prev_level, level))
+    return prev_level
+
+def get_threshold():
+	return _global_log.threshold
+
+def set_verbosity(v, force=False):
+    prev_level = _global_log.threshold
+    if v < 0:
+        set_threshold(ERROR, force)
     elif v == 0:
-        set_threshold(WARN)
+        set_threshold(WARN, force)
     elif v == 1:
-        set_threshold(INFO)
+        set_threshold(INFO, force)
     elif v >= 2:
-        set_threshold(DEBUG)
-    return {FATAL:-2,ERROR:-1,WARN:0,INFO:1,DEBUG:2}.get(prev_level,1)
+        set_threshold(DEBUG, force)
+    return {FATAL:-2,ERROR:-1,WARN:0,INFO:1,DEBUG:2}.get(prev_level, 1)
+
 
 _global_color_map = {
     DEBUG:cyan_text,
-    INFO:yellow_text,
+    INFO:default_text,
     WARN:red_text,
     ERROR:red_text,
     FATAL:red_text
 }
 
-set_verbosity(1)
+# don't use INFO,.. flags in set_verbosity, these flags are for set_threshold.
+set_verbosity(0, force=True)
+
+
+_error = error
+_warn = warn
+_info = info
+_debug = debug
+
+
+def error(msg, *a, **kw):
+    _error(f"ERROR: {msg}", *a, **kw)
+
+
+def warn(msg, *a, **kw):
+    _warn(f"WARN: {msg}", *a, **kw)
+
+
+def info(msg, *a, **kw):
+    _info(f"INFO: {msg}", *a, **kw)
+
+
+def debug(msg, *a, **kw):
+    _debug(f"DEBUG: {msg}", *a, **kw)
('numpy/distutils', 'line_endings.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,75 +1,77 @@
 """ Functions for converting from DOS to UNIX line endings
+
 """
+import os
+import re
+import sys
 
-import sys, re, os
 
 def dos2unix(file):
     "Replace CRLF with LF in argument files.  Print names of changed files."
     if os.path.isdir(file):
-        print file, "Directory!"
+        print(file, "Directory!")
         return
 
-    data = open(file, "rb").read()
+    with open(file, "rb") as fp:
+        data = fp.read()
     if '\0' in data:
-        print file, "Binary!"
+        print(file, "Binary!")
         return
 
     newdata = re.sub("\r\n", "\n", data)
     if newdata != data:
-        print 'dos2unix:', file
-        f = open(file, "wb")
-        f.write(newdata)
-        f.close()
+        print('dos2unix:', file)
+        with open(file, "wb") as f:
+            f.write(newdata)
         return file
     else:
-        print file, 'ok'
+        print(file, 'ok')
 
-def dos2unix_one_dir(modified_files,dir_name,file_names):
+def dos2unix_one_dir(modified_files, dir_name, file_names):
     for file in file_names:
-        full_path = os.path.join(dir_name,file)
+        full_path = os.path.join(dir_name, file)
         file = dos2unix(full_path)
         if file is not None:
             modified_files.append(file)
 
 def dos2unix_dir(dir_name):
     modified_files = []
-    os.path.walk(dir_name,dos2unix_one_dir,modified_files)
+    os.path.walk(dir_name, dos2unix_one_dir, modified_files)
     return modified_files
 #----------------------------------
 
 def unix2dos(file):
     "Replace LF with CRLF in argument files.  Print names of changed files."
     if os.path.isdir(file):
-        print file, "Directory!"
+        print(file, "Directory!")
         return
 
-    data = open(file, "rb").read()
+    with open(file, "rb") as fp:
+        data = fp.read()
     if '\0' in data:
-        print file, "Binary!"
+        print(file, "Binary!")
         return
     newdata = re.sub("\r\n", "\n", data)
     newdata = re.sub("\n", "\r\n", newdata)
     if newdata != data:
-        print 'unix2dos:', file
-        f = open(file, "wb")
-        f.write(newdata)
-        f.close()
+        print('unix2dos:', file)
+        with open(file, "wb") as f:
+            f.write(newdata)
         return file
     else:
-        print file, 'ok'
+        print(file, 'ok')
 
-def unix2dos_one_dir(modified_files,dir_name,file_names):
+def unix2dos_one_dir(modified_files, dir_name, file_names):
     for file in file_names:
-        full_path = os.path.join(dir_name,file)
+        full_path = os.path.join(dir_name, file)
         unix2dos(full_path)
         if file is not None:
             modified_files.append(file)
 
 def unix2dos_dir(dir_name):
     modified_files = []
-    os.path.walk(dir_name,unix2dos_one_dir,modified_files)
+    os.path.walk(dir_name, unix2dos_one_dir, modified_files)
     return modified_files
 
 if __name__ == "__main__":
-    import sys
     dos2unix_dir(sys.argv[1])
('numpy/distutils', 'lib2def.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,7 +1,6 @@
 import re
 import sys
-import os
-import string
+import subprocess
 
 __doc__ = """This module generates a DEF file from the symbols in
 an MSVC-compiled DLL import library.  It correctly discriminates between
@@ -21,11 +20,9 @@
 
 __version__ = '0.1a'
 
-import sys
-
 py_ver = "%d%d" % tuple(sys.version_info[:2])
 
-DEFAULT_NM = 'nm -Cs'
+DEFAULT_NM = ['nm', '-Cs']
 
 DEF_HEADER = """LIBRARY         python%s.dll
 ;CODE           PRELOAD MOVEABLE DISCARDABLE
@@ -48,8 +45,8 @@
         elif sys.argv[1][-4:] == '.def' and sys.argv[2][-4:] == '.lib':
             deffile, libfile = sys.argv[1:]
         else:
-            print "I'm assuming that your first argument is the library"
-            print "and the second is the DEF file."
+            print("I'm assuming that your first argument is the library")
+            print("and the second is the DEF file.")
     elif len(sys.argv) == 2:
         if sys.argv[1][-4:] == '.def':
             deffile = sys.argv[1]
@@ -62,13 +59,16 @@
         deffile = None
     return libfile, deffile
 
-def getnm(nm_cmd = 'nm -Cs python%s.lib' % py_ver):
+def getnm(nm_cmd=['nm', '-Cs', 'python%s.lib' % py_ver], shell=True):
     """Returns the output of nm_cmd via a pipe.
 
-nm_output = getnam(nm_cmd = 'nm -Cs py_lib')"""
-    f = os.popen(nm_cmd)
-    nm_output = f.read()
-    f.close()
+nm_output = getnm(nm_cmd = 'nm -Cs py_lib')"""
+    p = subprocess.Popen(nm_cmd, shell=shell, stdout=subprocess.PIPE,
+                         stderr=subprocess.PIPE, universal_newlines=True)
+    nm_output, nm_err = p.communicate()
+    if p.returncode != 0:
+        raise RuntimeError('failed to run "%s": "%s"' % (
+                                     ' '.join(nm_cmd), nm_err))
     return nm_output
 
 def parse_nm(nm_output):
@@ -110,7 +110,7 @@
         deffile = sys.stdout
     else:
         deffile = open(deffile, 'w')
-    nm_cmd = '%s %s' % (DEFAULT_NM, libfile)
-    nm_output = getnm(nm_cmd)
+    nm_cmd = DEFAULT_NM + [str(libfile)]
+    nm_output = getnm(nm_cmd, shell=False)
     dlist, flist = parse_nm(nm_output)
     output_def(dlist, flist, DEF_HEADER, deffile)
('numpy/distutils', 'system_info.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,41 +1,8 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 """
 This file defines a set of system_info classes for getting
 information about various resources (libraries, library directories,
-include directories, etc.) in the system. Currently, the following
-classes are available:
-
-  atlas_info
-  atlas_threads_info
-  atlas_blas_info
-  atlas_blas_threads_info
-  lapack_atlas_info
-  blas_info
-  lapack_info
-  blas_opt_info       # usage recommended
-  lapack_opt_info     # usage recommended
-  fftw_info,dfftw_info,sfftw_info
-  fftw_threads_info,dfftw_threads_info,sfftw_threads_info
-  djbfft_info
-  x11_info
-  lapack_src_info
-  blas_src_info
-  numpy_info
-  numarray_info
-  numpy_info
-  boost_python_info
-  agg2_info
-  wx_info
-  gdk_pixbuf_xlib_2_info
-  gdk_pixbuf_2_info
-  gdk_x11_2_info
-  gtkp_x11_2_info
-  gtkp_2_info
-  xft_info
-  freetype2_info
-  umfpack_info
-
-Usage:
+include directories, etc.) in the system. Usage:
     info_dict = get_info(<name>)
   where <name> is a string 'atlas','x11','fftw','lapack','blas',
   'lapack_src', 'blas_src', etc. For a complete list of allowed names,
@@ -58,24 +25,112 @@
 The file 'site.cfg' is looked for in
 
 1) Directory of main setup.py file being run.
-2) Home directory of user running the setup.py file (Not implemented yet)
+2) Home directory of user running the setup.py file as ~/.numpy-site.cfg
 3) System wide directory (location of this file...)
 
 The first one found is used to get system configuration options The
 format is that used by ConfigParser (i.e., Windows .INI style). The
-section DEFAULT has options that are the default for each section. The
-available sections are fftw, atlas, and x11. Appropiate defaults are
-used if nothing is specified.
+section ALL is not intended for general use.
+
+Appropriate defaults are used if nothing is specified.
 
 The order of finding the locations of resources is the following:
  1. environment variable
  2. section in site.cfg
  3. DEFAULT section in site.cfg
+ 4. System default search paths (see ``default_*`` variables below).
 Only the first complete match is returned.
+
+Currently, the following classes are available, along with their section names:
+
+    Numeric_info:Numeric
+    _numpy_info:Numeric
+    _pkg_config_info:None
+    accelerate_info:accelerate
+    agg2_info:agg2
+    amd_info:amd
+    atlas_3_10_blas_info:atlas
+    atlas_3_10_blas_threads_info:atlas
+    atlas_3_10_info:atlas
+    atlas_3_10_threads_info:atlas
+    atlas_blas_info:atlas
+    atlas_blas_threads_info:atlas
+    atlas_info:atlas
+    atlas_threads_info:atlas
+    blas64__opt_info:ALL               # usage recommended (general ILP64 BLAS, 64_ symbol suffix)
+    blas_ilp64_opt_info:ALL            # usage recommended (general ILP64 BLAS)
+    blas_ilp64_plain_opt_info:ALL      # usage recommended (general ILP64 BLAS, no symbol suffix)
+    blas_info:blas
+    blas_mkl_info:mkl
+    blas_opt_info:ALL                  # usage recommended
+    blas_src_info:blas_src
+    blis_info:blis
+    boost_python_info:boost_python
+    dfftw_info:fftw
+    dfftw_threads_info:fftw
+    djbfft_info:djbfft
+    f2py_info:ALL
+    fft_opt_info:ALL
+    fftw2_info:fftw
+    fftw3_info:fftw3
+    fftw_info:fftw
+    fftw_threads_info:fftw
+    flame_info:flame
+    freetype2_info:freetype2
+    gdk_2_info:gdk_2
+    gdk_info:gdk
+    gdk_pixbuf_2_info:gdk_pixbuf_2
+    gdk_pixbuf_xlib_2_info:gdk_pixbuf_xlib_2
+    gdk_x11_2_info:gdk_x11_2
+    gtkp_2_info:gtkp_2
+    gtkp_x11_2_info:gtkp_x11_2
+    lapack64__opt_info:ALL             # usage recommended (general ILP64 LAPACK, 64_ symbol suffix)
+    lapack_atlas_3_10_info:atlas
+    lapack_atlas_3_10_threads_info:atlas
+    lapack_atlas_info:atlas
+    lapack_atlas_threads_info:atlas
+    lapack_ilp64_opt_info:ALL          # usage recommended (general ILP64 LAPACK)
+    lapack_ilp64_plain_opt_info:ALL    # usage recommended (general ILP64 LAPACK, no symbol suffix)
+    lapack_info:lapack
+    lapack_mkl_info:mkl
+    lapack_opt_info:ALL                # usage recommended
+    lapack_src_info:lapack_src
+    mkl_info:mkl
+    numarray_info:numarray
+    numerix_info:numerix
+    numpy_info:numpy
+    openblas64__info:openblas64_
+    openblas64__lapack_info:openblas64_
+    openblas_clapack_info:openblas
+    openblas_ilp64_info:openblas_ilp64
+    openblas_ilp64_lapack_info:openblas_ilp64
+    openblas_info:openblas
+    openblas_lapack_info:openblas
+    sfftw_info:fftw
+    sfftw_threads_info:fftw
+    system_info:ALL
+    umfpack_info:umfpack
+    wx_info:wx
+    x11_info:x11
+    xft_info:xft
+
+Note that blas_opt_info and lapack_opt_info honor the NPY_BLAS_ORDER
+and NPY_LAPACK_ORDER environment variables to determine the order in which
+specific BLAS and LAPACK libraries are searched for.
+
+This search (or autodetection) can be bypassed by defining the environment
+variables NPY_BLAS_LIBS and NPY_LAPACK_LIBS, which should then contain the
+exact linker flags to use (language will be set to F77). Building against
+Netlib BLAS/LAPACK or stub files, in order to be able to switch BLAS and LAPACK
+implementations at runtime. If using this to build NumPy itself, it is
+recommended to also define NPY_CBLAS_LIBS (assuming your BLAS library has a
+CBLAS interface) to enable CBLAS usage for matrix multiplication (unoptimized
+otherwise).
 
 Example:
 ----------
 [DEFAULT]
+# default section
 library_dirs = /usr/lib:/usr/local/lib:/opt/lib
 include_dirs = /usr/include:/usr/local/include:/opt/include
 src_dirs = /usr/local/src:/opt/src
@@ -83,20 +138,20 @@
 search_static_first = 0
 
 [fftw]
-fftw_libs = rfftw, fftw
-fftw_opt_libs = rfftw_threaded, fftw_threaded
-# if the above aren't found, look for {s,d}fftw_libs and {s,d}fftw_opt_libs
+libraries = rfftw, fftw
 
 [atlas]
 library_dirs = /usr/lib/3dnow:/usr/lib/3dnow/atlas
 # for overriding the names of the atlas libraries
-atlas_libs = lapack, f77blas, cblas, atlas
+libraries = lapack, f77blas, cblas, atlas
 
 [x11]
 library_dirs = /usr/X11R6/lib
 include_dirs = /usr/X11R6/include
 ----------
 
+Note that the ``libraries`` key is the default setting for libraries.
+
 Authors:
   Pearu Peterson <pearu@cens.ioc.ee>, February 2002
   David M. Cooke <cookedm@physics.mcmaster.ca>, April 2002
@@ -104,52 +159,221 @@
 Copyright 2002 Pearu Peterson all rights reserved,
 Pearu Peterson <pearu@cens.ioc.ee>
 Permission to use, modify, and distribute this software is given under the
-terms of the SciPy (BSD style) license.  See LICENSE.txt that came with
+terms of the NumPy (BSD style) license.  See LICENSE.txt that came with
 this distribution for specifics.
 
 NO WARRANTY IS EXPRESSED OR IMPLIED.  USE AT YOUR OWN RISK.
+
 """
-
-import sys,os,re,copy
+import sys
+import os
+import re
+import copy
 import warnings
+import subprocess
+import textwrap
+
+from glob import glob
+from functools import reduce
+from configparser import NoOptionError
+from configparser import RawConfigParser as ConfigParser
+# It seems that some people are importing ConfigParser from here so is
+# good to keep its class name. Use of RawConfigParser is needed in
+# order to be able to load path names with percent in them, like
+# `feature%2Fcool` which is common on git flow branch names.
+
 from distutils.errors import DistutilsError
-from glob import glob
-import ConfigParser
-from exec_command import find_executable, exec_command, get_pythonexe
-from numpy.distutils.misc_util import is_sequence, is_string
-import distutils.sysconfig
-
-from distutils.sysconfig import get_config_vars
+from distutils.dist import Distribution
+import sysconfig
+from numpy.distutils import log
+from distutils.util import get_platform
+
+from numpy.distutils.exec_command import (
+    find_executable, filepath_from_subprocess_output,
+    )
+from numpy.distutils.misc_util import (is_sequence, is_string,
+                                       get_shared_lib_extension)
+from numpy.distutils.command.config import config as cmd_config
+from numpy.distutils import customized_ccompiler as _customized_ccompiler
+from numpy.distutils import _shell_utils
+import distutils.ccompiler
+import tempfile
+import shutil
+
+__all__ = ['system_info']
+
+# Determine number of bits
+import platform
+_bits = {'32bit': 32, '64bit': 64}
+platform_bits = _bits[platform.architecture()[0]]
+
+
+global_compiler = None
+
+def customized_ccompiler():
+    global global_compiler
+    if not global_compiler:
+        global_compiler = _customized_ccompiler()
+    return global_compiler
+
+
+def _c_string_literal(s):
+    """
+    Convert a python string into a literal suitable for inclusion into C code
+    """
+    # only these three characters are forbidden in C strings
+    s = s.replace('\\', r'\\')
+    s = s.replace('"',  r'\"')
+    s = s.replace('\n', r'\n')
+    return '"{}"'.format(s)
+
+
+def libpaths(paths, bits):
+    """Return a list of library paths valid on 32 or 64 bit systems.
+
+    Inputs:
+      paths : sequence
+        A sequence of strings (typically paths)
+      bits : int
+        An integer, the only valid values are 32 or 64.  A ValueError exception
+      is raised otherwise.
+
+    Examples:
+
+    Consider a list of directories
+    >>> paths = ['/usr/X11R6/lib','/usr/X11/lib','/usr/lib']
+
+    For a 32-bit platform, this is already valid:
+    >>> np.distutils.system_info.libpaths(paths,32)
+    ['/usr/X11R6/lib', '/usr/X11/lib', '/usr/lib']
+
+    On 64 bits, we prepend the '64' postfix
+    >>> np.distutils.system_info.libpaths(paths,64)
+    ['/usr/X11R6/lib64', '/usr/X11R6/lib', '/usr/X11/lib64', '/usr/X11/lib',
+    '/usr/lib64', '/usr/lib']
+    """
+    if bits not in (32, 64):
+        raise ValueError("Invalid bit size in libpaths: 32 or 64 only")
+
+    # Handle 32bit case
+    if bits == 32:
+        return paths
+
+    # Handle 64bit case
+    out = []
+    for p in paths:
+        out.extend([p + '64', p])
+
+    return out
+
 
 if sys.platform == 'win32':
     default_lib_dirs = ['C:\\',
-                        os.path.join(distutils.sysconfig.EXEC_PREFIX,
+                        os.path.join(sysconfig.get_config_var('exec_prefix'),
                                      'libs')]
+    default_runtime_dirs = []
     default_include_dirs = []
     default_src_dirs = ['.']
     default_x11_lib_dirs = []
     default_x11_include_dirs = []
+    _include_dirs = [
+        'include',
+        'include/suitesparse',
+    ]
+    _lib_dirs = [
+        'lib',
+    ]
+
+    _include_dirs = [d.replace('/', os.sep) for d in _include_dirs]
+    _lib_dirs = [d.replace('/', os.sep) for d in _lib_dirs]
+    def add_system_root(library_root):
+        """Add a package manager root to the include directories"""
+        global default_lib_dirs
+        global default_include_dirs
+
+        library_root = os.path.normpath(library_root)
+
+        default_lib_dirs.extend(
+            os.path.join(library_root, d) for d in _lib_dirs)
+        default_include_dirs.extend(
+            os.path.join(library_root, d) for d in _include_dirs)
+
+    # VCpkg is the de-facto package manager on windows for C/C++
+    # libraries. If it is on the PATH, then we append its paths here.
+    vcpkg = shutil.which('vcpkg')
+    if vcpkg:
+        vcpkg_dir = os.path.dirname(vcpkg)
+        if platform.architecture()[0] == '32bit':
+            specifier = 'x86'
+        else:
+            specifier = 'x64'
+
+        vcpkg_installed = os.path.join(vcpkg_dir, 'installed')
+        for vcpkg_root in [
+            os.path.join(vcpkg_installed, specifier + '-windows'),
+            os.path.join(vcpkg_installed, specifier + '-windows-static'),
+        ]:
+            add_system_root(vcpkg_root)
+
+    # Conda is another popular package manager that provides libraries
+    conda = shutil.which('conda')
+    if conda:
+        conda_dir = os.path.dirname(conda)
+        add_system_root(os.path.join(conda_dir, '..', 'Library'))
+        add_system_root(os.path.join(conda_dir, 'Library'))
+
 else:
-    default_lib_dirs = ['/usr/local/lib', '/opt/lib', '/usr/lib',
-                        '/opt/local/lib', '/sw/lib']
+    default_lib_dirs = libpaths(['/usr/local/lib', '/opt/lib', '/usr/lib',
+                                 '/opt/local/lib', '/sw/lib'], platform_bits)
+    default_runtime_dirs = []
     default_include_dirs = ['/usr/local/include',
-                            '/opt/include', '/usr/include',
-                            '/opt/local/include', '/sw/include']
-    default_src_dirs = ['.','/usr/local/src', '/opt/src','/sw/src']
-    default_x11_lib_dirs = ['/usr/X11R6/lib','/usr/X11/lib','/usr/lib']
-    default_x11_include_dirs = ['/usr/X11R6/include','/usr/X11/include',
-                                '/usr/include']
+                            '/opt/include',
+                            # path of umfpack under macports
+                            '/opt/local/include/ufsparse',
+                            '/opt/local/include', '/sw/include',
+                            '/usr/include/suitesparse']
+    default_src_dirs = ['.', '/usr/local/src', '/opt/src', '/sw/src']
+
+    default_x11_lib_dirs = libpaths(['/usr/X11R6/lib', '/usr/X11/lib',
+                                     '/usr/lib'], platform_bits)
+    default_x11_include_dirs = ['/usr/X11R6/include', '/usr/X11/include']
+
+    if os.path.exists('/usr/lib/X11'):
+        globbed_x11_dir = glob('/usr/lib/*/libX11.so')
+        if globbed_x11_dir:
+            x11_so_dir = os.path.split(globbed_x11_dir[0])[0]
+            default_x11_lib_dirs.extend([x11_so_dir, '/usr/lib/X11'])
+            default_x11_include_dirs.extend(['/usr/lib/X11/include',
+                                             '/usr/include/X11'])
+
+    with open(os.devnull, 'w') as tmp:
+        try:
+            p = subprocess.Popen(["gcc", "-print-multiarch"], stdout=subprocess.PIPE,
+                         stderr=tmp)
+        except (OSError, DistutilsError):
+            # OSError if gcc is not installed, or SandboxViolation (DistutilsError
+            # subclass) if an old setuptools bug is triggered (see gh-3160).
+            pass
+        else:
+            triplet = str(p.communicate()[0].decode().strip())
+            if p.returncode == 0:
+                # gcc supports the "-print-multiarch" option
+                default_x11_lib_dirs += [os.path.join("/usr/lib/", triplet)]
+                default_lib_dirs += [os.path.join("/usr/lib/", triplet)]
+
 
 if os.path.join(sys.prefix, 'lib') not in default_lib_dirs:
-    default_lib_dirs.insert(0,os.path.join(sys.prefix, 'lib'))
+    default_lib_dirs.insert(0, os.path.join(sys.prefix, 'lib'))
     default_include_dirs.append(os.path.join(sys.prefix, 'include'))
     default_src_dirs.append(os.path.join(sys.prefix, 'src'))
 
-default_lib_dirs = filter(os.path.isdir, default_lib_dirs)
-default_include_dirs = filter(os.path.isdir, default_include_dirs)
-default_src_dirs = filter(os.path.isdir, default_src_dirs)
-
-so_ext = get_config_vars('SO')[0] or ''
+default_lib_dirs = [_m for _m in default_lib_dirs if os.path.isdir(_m)]
+default_runtime_dirs = [_m for _m in default_runtime_dirs if os.path.isdir(_m)]
+default_include_dirs = [_m for _m in default_include_dirs if os.path.isdir(_m)]
+default_src_dirs = [_m for _m in default_src_dirs if os.path.isdir(_m)]
+
+so_ext = get_shared_lib_extension()
+
 
 def get_standard_file(fname):
     """Returns a list of files named 'fname' from
@@ -163,16 +387,15 @@
         f = __file__
     except NameError:
         f = sys.argv[0]
-    else:
-        sysfile = os.path.join(os.path.split(os.path.abspath(f))[0],
-                               fname)
-        if os.path.isfile(sysfile):
-            filenames.append(sysfile)
+    sysfile = os.path.join(os.path.split(os.path.abspath(f))[0],
+                           fname)
+    if os.path.isfile(sysfile):
+        filenames.append(sysfile)
 
     # Home directory
     # And look for the user config file
     try:
-        f = os.environ['HOME']
+        f = os.path.expanduser('~')
     except KeyError:
         pass
     else:
@@ -186,77 +409,206 @@
 
     return filenames
 
-def get_info(name,notfound_action=0):
+
+def _parse_env_order(base_order, env):
+    """ Parse an environment variable `env` by splitting with "," and only returning elements from `base_order`
+
+    This method will sequence the environment variable and check for their
+    individual elements in `base_order`.
+
+    The items in the environment variable may be negated via '^item' or '!itema,itemb'.
+    It must start with ^/! to negate all options.
+
+    Raises
+    ------
+    ValueError: for mixed negated and non-negated orders or multiple negated orders
+
+    Parameters
+    ----------
+    base_order : list of str
+       the base list of orders
+    env : str
+       the environment variable to be parsed, if none is found, `base_order` is returned
+
+    Returns
+    -------
+    allow_order : list of str
+        allowed orders in lower-case
+    unknown_order : list of str
+        for values not overlapping with `base_order`
+    """
+    order_str = os.environ.get(env, None)
+
+    # ensure all base-orders are lower-case (for easier comparison)
+    base_order = [order.lower() for order in base_order]
+    if order_str is None:
+        return base_order, []
+
+    neg = order_str.startswith('^') or order_str.startswith('!')
+    # Check format
+    order_str_l = list(order_str)
+    sum_neg = order_str_l.count('^') + order_str_l.count('!')
+    if neg:
+        if sum_neg > 1:
+            raise ValueError(f"Environment variable '{env}' may only contain a single (prefixed) negation: {order_str}")
+        # remove prefix
+        order_str = order_str[1:]
+    elif sum_neg > 0:
+        raise ValueError(f"Environment variable '{env}' may not mix negated an non-negated items: {order_str}")
+
+    # Split and lower case
+    orders = order_str.lower().split(',')
+
+    # to inform callee about non-overlapping elements
+    unknown_order = []
+
+    # if negated, we have to remove from the order
+    if neg:
+        allow_order = base_order.copy()
+
+        for order in orders:
+            if not order:
+                continue
+
+            if order not in base_order:
+                unknown_order.append(order)
+                continue
+
+            if order in allow_order:
+                allow_order.remove(order)
+
+    else:
+        allow_order = []
+
+        for order in orders:
+            if not order:
+                continue
+
+            if order not in base_order:
+                unknown_order.append(order)
+                continue
+
+            if order not in allow_order:
+                allow_order.append(order)
+
+    return allow_order, unknown_order
+
+
+def get_info(name, notfound_action=0):
     """
     notfound_action:
       0 - do nothing
       1 - display warning message
       2 - raise error
     """
-    cl = {'atlas':atlas_info,  # use lapack_opt or blas_opt instead
-          'atlas_threads':atlas_threads_info,                # ditto
-          'atlas_blas':atlas_blas_info,
-          'atlas_blas_threads':atlas_blas_threads_info,
-          'lapack_atlas':lapack_atlas_info,  # use lapack_opt instead
-          'lapack_atlas_threads':lapack_atlas_threads_info,  # ditto
-          'mkl':mkl_info,
-          'lapack_mkl':lapack_mkl_info,      # use lapack_opt instead
-          'blas_mkl':blas_mkl_info,          # use blas_opt instead
-          'x11':x11_info,
-          'fft_opt':fft_opt_info,
-          'fftw':fftw_info,
-          'fftw2':fftw2_info,
-          'fftw3':fftw3_info,
-          'dfftw':dfftw_info,
-          'sfftw':sfftw_info,
-          'fftw_threads':fftw_threads_info,
-          'dfftw_threads':dfftw_threads_info,
-          'sfftw_threads':sfftw_threads_info,
-          'djbfft':djbfft_info,
-          'blas':blas_info,                  # use blas_opt instead
-          'lapack':lapack_info,              # use lapack_opt instead
-          'lapack_src':lapack_src_info,
-          'blas_src':blas_src_info,
-          'numpy':numpy_info,
-          'f2py':f2py_info,
-          'Numeric':Numeric_info,
-          'numeric':Numeric_info,
-          'numarray':numarray_info,
-          'numerix':numerix_info,
-          'lapack_opt':lapack_opt_info,
-          'blas_opt':blas_opt_info,
-          'boost_python':boost_python_info,
-          'agg2':agg2_info,
-          'wx':wx_info,
-          'gdk_pixbuf_xlib_2':gdk_pixbuf_xlib_2_info,
-          'gdk-pixbuf-xlib-2.0':gdk_pixbuf_xlib_2_info,
-          'gdk_pixbuf_2':gdk_pixbuf_2_info,
-          'gdk-pixbuf-2.0':gdk_pixbuf_2_info,
-          'gdk':gdk_info,
-          'gdk_2':gdk_2_info,
-          'gdk-2.0':gdk_2_info,
-          'gdk_x11_2':gdk_x11_2_info,
-          'gdk-x11-2.0':gdk_x11_2_info,
-          'gtkp_x11_2':gtkp_x11_2_info,
-          'gtk+-x11-2.0':gtkp_x11_2_info,
-          'gtkp_2':gtkp_2_info,
-          'gtk+-2.0':gtkp_2_info,
-          'xft':xft_info,
-          'freetype2':freetype2_info,
-          'umfpack':umfpack_info,
-          'amd':amd_info,
-          }.get(name.lower(),system_info)
+    cl = {'armpl': armpl_info,
+          'blas_armpl': blas_armpl_info,
+          'lapack_armpl': lapack_armpl_info,
+          'fftw3_armpl': fftw3_armpl_info,
+          'atlas': atlas_info,  # use lapack_opt or blas_opt instead
+          'atlas_threads': atlas_threads_info,                # ditto
+          'atlas_blas': atlas_blas_info,
+          'atlas_blas_threads': atlas_blas_threads_info,
+          'lapack_atlas': lapack_atlas_info,  # use lapack_opt instead
+          'lapack_atlas_threads': lapack_atlas_threads_info,  # ditto
+          'atlas_3_10': atlas_3_10_info,  # use lapack_opt or blas_opt instead
+          'atlas_3_10_threads': atlas_3_10_threads_info,                # ditto
+          'atlas_3_10_blas': atlas_3_10_blas_info,
+          'atlas_3_10_blas_threads': atlas_3_10_blas_threads_info,
+          'lapack_atlas_3_10': lapack_atlas_3_10_info,  # use lapack_opt instead
+          'lapack_atlas_3_10_threads': lapack_atlas_3_10_threads_info,  # ditto
+          'flame': flame_info,          # use lapack_opt instead
+          'mkl': mkl_info,
+          # openblas which may or may not have embedded lapack
+          'openblas': openblas_info,          # use blas_opt instead
+          # openblas with embedded lapack
+          'openblas_lapack': openblas_lapack_info, # use blas_opt instead
+          'openblas_clapack': openblas_clapack_info, # use blas_opt instead
+          'blis': blis_info,                  # use blas_opt instead
+          'lapack_mkl': lapack_mkl_info,      # use lapack_opt instead
+          'blas_mkl': blas_mkl_info,          # use blas_opt instead
+          'accelerate': accelerate_info,      # use blas_opt instead
+          'openblas64_': openblas64__info,
+          'openblas64__lapack': openblas64__lapack_info,
+          'openblas_ilp64': openblas_ilp64_info,
+          'openblas_ilp64_lapack': openblas_ilp64_lapack_info,
+          'x11': x11_info,
+          'fft_opt': fft_opt_info,
+          'fftw': fftw_info,
+          'fftw2': fftw2_info,
+          'fftw3': fftw3_info,
+          'dfftw': dfftw_info,
+          'sfftw': sfftw_info,
+          'fftw_threads': fftw_threads_info,
+          'dfftw_threads': dfftw_threads_info,
+          'sfftw_threads': sfftw_threads_info,
+          'djbfft': djbfft_info,
+          'blas': blas_info,                  # use blas_opt instead
+          'lapack': lapack_info,              # use lapack_opt instead
+          'lapack_src': lapack_src_info,
+          'blas_src': blas_src_info,
+          'numpy': numpy_info,
+          'f2py': f2py_info,
+          'Numeric': Numeric_info,
+          'numeric': Numeric_info,
+          'numarray': numarray_info,
+          'numerix': numerix_info,
+          'lapack_opt': lapack_opt_info,
+          'lapack_ilp64_opt': lapack_ilp64_opt_info,
+          'lapack_ilp64_plain_opt': lapack_ilp64_plain_opt_info,
+          'lapack64__opt': lapack64__opt_info,
+          'blas_opt': blas_opt_info,
+          'blas_ilp64_opt': blas_ilp64_opt_info,
+          'blas_ilp64_plain_opt': blas_ilp64_plain_opt_info,
+          'blas64__opt': blas64__opt_info,
+          'boost_python': boost_python_info,
+          'agg2': agg2_info,
+          'wx': wx_info,
+          'gdk_pixbuf_xlib_2': gdk_pixbuf_xlib_2_info,
+          'gdk-pixbuf-xlib-2.0': gdk_pixbuf_xlib_2_info,
+          'gdk_pixbuf_2': gdk_pixbuf_2_info,
+          'gdk-pixbuf-2.0': gdk_pixbuf_2_info,
+          'gdk': gdk_info,
+          'gdk_2': gdk_2_info,
+          'gdk-2.0': gdk_2_info,
+          'gdk_x11_2': gdk_x11_2_info,
+          'gdk-x11-2.0': gdk_x11_2_info,
+          'gtkp_x11_2': gtkp_x11_2_info,
+          'gtk+-x11-2.0': gtkp_x11_2_info,
+          'gtkp_2': gtkp_2_info,
+          'gtk+-2.0': gtkp_2_info,
+          'xft': xft_info,
+          'freetype2': freetype2_info,
+          'umfpack': umfpack_info,
+          'amd': amd_info,
+          }.get(name.lower(), system_info)
     return cl().get_info(notfound_action)
+
 
 class NotFoundError(DistutilsError):
     """Some third-party program or library is not found."""
 
+
+class AliasedOptionError(DistutilsError):
+    """
+    Aliases entries in config files should not be existing.
+    In section '{section}' we found multiple appearances of options {options}."""
+
+
 class AtlasNotFoundError(NotFoundError):
     """
-    Atlas (http://math-atlas.sourceforge.net/) libraries not found.
+    Atlas (http://github.com/math-atlas/math-atlas) libraries not found.
     Directories to search for the libraries can be specified in the
     numpy/distutils/site.cfg file (section [atlas]) or by setting
     the ATLAS environment variable."""
+
+
+class FlameNotFoundError(NotFoundError):
+    """
+    FLAME (http://www.cs.utexas.edu/~flame/web/) libraries not found.
+    Directories to search for the libraries can be specified in the
+    numpy/distutils/site.cfg file (section [flame])."""
+
 
 class LapackNotFoundError(NotFoundError):
     """
@@ -265,6 +617,7 @@
     numpy/distutils/site.cfg file (section [lapack]) or by setting
     the LAPACK environment variable."""
 
+
 class LapackSrcNotFoundError(LapackNotFoundError):
     """
     Lapack (http://www.netlib.org/lapack/) sources not found.
@@ -272,6 +625,21 @@
     numpy/distutils/site.cfg file (section [lapack_src]) or by setting
     the LAPACK_SRC environment variable."""
 
+
+class LapackILP64NotFoundError(NotFoundError):
+    """
+    64-bit Lapack libraries not found.
+    Known libraries in numpy/distutils/site.cfg file are:
+    openblas64_, openblas_ilp64
+    """
+
+class BlasOptNotFoundError(NotFoundError):
+    """
+    Optimized (vendor) Blas libraries are not found.
+    Falls back to netlib Blas library which has worse performance.
+    A better performance should be easily gained by switching
+    Blas library."""
+
 class BlasNotFoundError(NotFoundError):
     """
     Blas (http://www.netlib.org/blas/) libraries not found.
@@ -279,6 +647,13 @@
     numpy/distutils/site.cfg file (section [blas]) or by setting
     the BLAS environment variable."""
 
+class BlasILP64NotFoundError(NotFoundError):
+    """
+    64-bit Blas libraries not found.
+    Known libraries in numpy/distutils/site.cfg file are:
+    openblas64_, openblas_ilp64
+    """
+
 class BlasSrcNotFoundError(BlasNotFoundError):
     """
     Blas (http://www.netlib.org/blas/) sources not found.
@@ -286,6 +661,7 @@
     numpy/distutils/site.cfg file (section [blas_src]) or by setting
     the BLAS_SRC environment variable."""
 
+
 class FFTWNotFoundError(NotFoundError):
     """
     FFTW (http://www.fftw.org/) libraries not found.
@@ -293,125 +669,203 @@
     numpy/distutils/site.cfg file (section [fftw]) or by setting
     the FFTW environment variable."""
 
+
 class DJBFFTNotFoundError(NotFoundError):
     """
-    DJBFFT (http://cr.yp.to/djbfft.html) libraries not found.
+    DJBFFT (https://cr.yp.to/djbfft.html) libraries not found.
     Directories to search for the libraries can be specified in the
     numpy/distutils/site.cfg file (section [djbfft]) or by setting
     the DJBFFT environment variable."""
 
+
 class NumericNotFoundError(NotFoundError):
     """
-    Numeric (http://www.numpy.org/) module not found.
+    Numeric (https://www.numpy.org/) module not found.
     Get it from above location, install it, and retry setup.py."""
+
 
 class X11NotFoundError(NotFoundError):
     """X11 libraries not found."""
 
+
 class UmfpackNotFoundError(NotFoundError):
     """
-    UMFPACK sparse solver (http://www.cise.ufl.edu/research/sparse/umfpack/)
+    UMFPACK sparse solver (https://www.cise.ufl.edu/research/sparse/umfpack/)
     not found. Directories to search for the libraries can be specified in the
     numpy/distutils/site.cfg file (section [umfpack]) or by setting
     the UMFPACK environment variable."""
 
+
 class system_info:
 
     """ get_info() is the only public method. Don't use others.
     """
-    section = 'DEFAULT'
     dir_env_var = None
-    search_static_first = 0 # XXX: disabled by default, may disappear in
-                            # future unless it is proved to be useful.
-    verbosity = 1
+    # XXX: search_static_first is disabled by default, may disappear in
+    # future unless it is proved to be useful.
+    search_static_first = 0
+    # The base-class section name is a random word "ALL" and is not really
+    # intended for general use. It cannot be None nor can it be DEFAULT as
+    # these break the ConfigParser. See gh-15338
+    section = 'ALL'
     saved_results = {}
 
     notfounderror = NotFoundError
 
-    def __init__ (self,
+    def __init__(self,
                   default_lib_dirs=default_lib_dirs,
                   default_include_dirs=default_include_dirs,
-                  verbosity = 1,
                   ):
         self.__class__.info = {}
         self.local_prefixes = []
-        defaults = {}
-        defaults['libraries'] = ''
-        defaults['library_dirs'] = os.pathsep.join(default_lib_dirs)
-        defaults['include_dirs'] = os.pathsep.join(default_include_dirs)
-        defaults['src_dirs'] = os.pathsep.join(default_src_dirs)
-        defaults['search_static_first'] = str(self.search_static_first)
-        self.cp = ConfigParser.ConfigParser(defaults)
-        self.files = get_standard_file('site.cfg')
+        defaults = {'library_dirs': os.pathsep.join(default_lib_dirs),
+                    'include_dirs': os.pathsep.join(default_include_dirs),
+                    'runtime_library_dirs': os.pathsep.join(default_runtime_dirs),
+                    'rpath': '',
+                    'src_dirs': os.pathsep.join(default_src_dirs),
+                    'search_static_first': str(self.search_static_first),
+                    'extra_compile_args': '', 'extra_link_args': ''}
+        self.cp = ConfigParser(defaults)
+        self.files = []
+        self.files.extend(get_standard_file('.numpy-site.cfg'))
+        self.files.extend(get_standard_file('site.cfg'))
         self.parse_config_files()
-        self.search_static_first = self.cp.getboolean(self.section,
-                                                      'search_static_first')
+
+        if self.section is not None:
+            self.search_static_first = self.cp.getboolean(
+                self.section, 'search_static_first')
         assert isinstance(self.search_static_first, int)
 
     def parse_config_files(self):
         self.cp.read(self.files)
         if not self.cp.has_section(self.section):
-            self.cp.add_section(self.section)
+            if self.section is not None:
+                self.cp.add_section(self.section)
 
     def calc_libraries_info(self):
         libs = self.get_libraries()
         dirs = self.get_lib_dirs()
+        # The extensions use runtime_library_dirs
+        r_dirs = self.get_runtime_lib_dirs()
+        # Intrinsic distutils use rpath, we simply append both entries
+        # as though they were one entry
+        r_dirs.extend(self.get_runtime_lib_dirs(key='rpath'))
         info = {}
         for lib in libs:
-            i = None
-            for d in dirs:
-                i = self.check_libs(d,[lib])
+            i = self.check_libs(dirs, [lib])
+            if i is not None:
+                dict_append(info, **i)
+            else:
+                log.info('Library %s was not found. Ignoring' % (lib))
+
+            if r_dirs:
+                i = self.check_libs(r_dirs, [lib])
                 if i is not None:
-                    break
-            if i is not None:
-                dict_append(info,**i)
-            else:
-                print 'Library %s was not found. Ignoring' % (lib)
+                    # Swap library keywords found to runtime_library_dirs
+                    # the libraries are insisting on the user having defined
+                    # them using the library_dirs, and not necessarily by
+                    # runtime_library_dirs
+                    del i['libraries']
+                    i['runtime_library_dirs'] = i.pop('library_dirs')
+                    dict_append(info, **i)
+                else:
+                    log.info('Runtime library %s was not found. Ignoring' % (lib))
+
         return info
 
-    def set_info(self,**info):
+    def set_info(self, **info):
         if info:
             lib_info = self.calc_libraries_info()
-            dict_append(info,**lib_info)
+            dict_append(info, **lib_info)
+            # Update extra information
+            extra_info = self.calc_extra_info()
+            dict_append(info, **extra_info)
         self.saved_results[self.__class__.__name__] = info
 
+    def get_option_single(self, *options):
+        """ Ensure that only one of `options` are found in the section
+
+        Parameters
+        ----------
+        *options : list of str
+           a list of options to be found in the section (``self.section``)
+
+        Returns
+        -------
+        str :
+            the option that is uniquely found in the section
+
+        Raises
+        ------
+        AliasedOptionError :
+            in case more than one of the options are found
+        """
+        found = [self.cp.has_option(self.section, opt) for opt in options]
+        if sum(found) == 1:
+            return options[found.index(True)]
+        elif sum(found) == 0:
+            # nothing is found anyways
+            return options[0]
+
+        # Else we have more than 1 key found
+        if AliasedOptionError.__doc__ is None:
+            raise AliasedOptionError()
+        raise AliasedOptionError(AliasedOptionError.__doc__.format(
+            section=self.section, options='[{}]'.format(', '.join(options))))
+
+
     def has_info(self):
-        return self.saved_results.has_key(self.__class__.__name__)
-
-    def get_info(self,notfound_action=0):
-        """ Return a dictonary with items that are compatible
+        return self.__class__.__name__ in self.saved_results
+
+    def calc_extra_info(self):
+        """ Updates the information in the current information with
+        respect to these flags:
+          extra_compile_args
+          extra_link_args
+        """
+        info = {}
+        for key in ['extra_compile_args', 'extra_link_args']:
+            # Get values
+            opt = self.cp.get(self.section, key)
+            opt = _shell_utils.NativeParser.split(opt)
+            if opt:
+                tmp = {key: opt}
+                dict_append(info, **tmp)
+        return info
+
+    def get_info(self, notfound_action=0):
+        """ Return a dictionary with items that are compatible
             with numpy.distutils.setup keyword arguments.
         """
         flag = 0
         if not self.has_info():
             flag = 1
-            if self.verbosity>0:
-                print self.__class__.__name__ + ':'
+            log.info(self.__class__.__name__ + ':')
             if hasattr(self, 'calc_info'):
                 self.calc_info()
             if notfound_action:
                 if not self.has_info():
-                    if notfound_action==1:
-                        warnings.warn(self.notfounderror.__doc__)
-                    elif notfound_action==2:
-                        raise self.notfounderror,self.notfounderror.__doc__
+                    if notfound_action == 1:
+                        warnings.warn(self.notfounderror.__doc__, stacklevel=2)
+                    elif notfound_action == 2:
+                        raise self.notfounderror(self.notfounderror.__doc__)
                     else:
                         raise ValueError(repr(notfound_action))
 
-            if self.verbosity>0:
-                if not self.has_info():
-                    print '  NOT AVAILABLE'
-                    self.set_info()
-                else:
-                    print '  FOUND:'
+            if not self.has_info():
+                log.info('  NOT AVAILABLE')
+                self.set_info()
+            else:
+                log.info('  FOUND:')
 
         res = self.saved_results.get(self.__class__.__name__)
-        if self.verbosity>0 and flag:
-            for k,v in res.items():
+        if log.get_threshold() <= log.INFO and flag:
+            for k, v in res.items():
                 v = str(v)
-                if k=='sources' and len(v)>200: v = v[:60]+' ...\n... '+v[-60:]
-                print '    %s = %s'%(k,v)
-            print
+                if k in ['sources', 'libraries'] and len(v) > 270:
+                    v = v[:120] + '...\n...\n...' + v[-120:]
+                log.info('    %s = %s', k, v)
+            log.info('')
 
         return copy.deepcopy(res)
 
@@ -422,27 +876,28 @@
             if is_sequence(env_var):
                 e0 = env_var[-1]
                 for e in env_var:
-                    if os.environ.has_key(e):
+                    if e in os.environ:
                         e0 = e
                         break
-                if not env_var[0]==e0:
-                    print 'Setting %s=%s' % (env_var[0],e0)
+                if not env_var[0] == e0:
+                    log.info('Setting %s=%s' % (env_var[0], e0))
                 env_var = e0
-        if env_var and os.environ.has_key(env_var):
+        if env_var and env_var in os.environ:
             d = os.environ[env_var]
-            if d=='None':
-                print 'Disabled',self.__class__.__name__,'(%s is None)' \
-                      % (self.dir_env_var)
+            if d == 'None':
+                log.info('Disabled %s: %s',
+                         self.__class__.__name__, '(%s is None)'
+                         % (env_var,))
                 return []
             if os.path.isfile(d):
                 dirs = [os.path.dirname(d)] + dirs
-                l = getattr(self,'_lib_names',[])
-                if len(l)==1:
+                l = getattr(self, '_lib_names', [])
+                if len(l) == 1:
                     b = os.path.basename(d)
                     b = os.path.splitext(b)[0]
-                    if b[:3]=='lib':
-                        print 'Replacing _lib_names[0]==%r with %r' \
-                              % (self._lib_names[0], b[3:])
+                    if b[:3] == 'lib':
+                        log.info('Replacing _lib_names[0]==%r with %r' \
+                              % (self._lib_names[0], b[3:]))
                         self._lib_names[0] = b[3:]
             else:
                 ds = d.split(os.pathsep)
@@ -450,21 +905,33 @@
                 for d in ds:
                     if os.path.isdir(d):
                         ds2.append(d)
-                        for dd in ['include','lib']:
-                            d1 = os.path.join(d,dd)
+                        for dd in ['include', 'lib']:
+                            d1 = os.path.join(d, dd)
                             if os.path.isdir(d1):
                                 ds2.append(d1)
                 dirs = ds2 + dirs
-        default_dirs = self.cp.get('DEFAULT', key).split(os.pathsep)
+        default_dirs = self.cp.get(self.section, key).split(os.pathsep)
         dirs.extend(default_dirs)
         ret = []
-        [ret.append(d) for d in dirs if os.path.isdir(d) and d not in ret]
-        if self.verbosity>1:
-            print '(',key,'=',':'.join(ret),')'
+        for d in dirs:
+            if len(d) > 0 and not os.path.isdir(d):
+                warnings.warn('Specified path %s is invalid.' % d, stacklevel=2)
+                continue
+
+            if d not in ret:
+                ret.append(d)
+
+        log.debug('( %s = %s )', key, ':'.join(ret))
         return ret
 
     def get_lib_dirs(self, key='library_dirs'):
         return self.get_paths(self.section, key)
+
+    def get_runtime_lib_dirs(self, key='runtime_library_dirs'):
+        path = self.get_paths(self.section, key)
+        if path == ['']:
+            path = []
+        return path
 
     def get_include_dirs(self, key='include_dirs'):
         return self.get_paths(self.section, key)
@@ -475,7 +942,7 @@
     def get_libs(self, key, default):
         try:
             libs = self.cp.get(self.section, key)
-        except ConfigParser.NoOptionError:
+        except NoOptionError:
             if not default:
                 return []
             if is_string(default):
@@ -484,67 +951,125 @@
         return [b for b in [a.strip() for a in libs.split(',')] if b]
 
     def get_libraries(self, key='libraries'):
-        return self.get_libs(key,'')
-
-    def check_libs(self,lib_dir,libs,opt_libs =[]):
-        """ If static or shared libraries are available then return
-            their info dictionary. """
+        if hasattr(self, '_lib_names'):
+            return self.get_libs(key, default=self._lib_names)
+        else:
+            return self.get_libs(key, '')
+
+    def library_extensions(self):
+        c = customized_ccompiler()
+        static_exts = []
+        if c.compiler_type != 'msvc':
+            # MSVC doesn't understand binutils
+            static_exts.append('.a')
+        if sys.platform == 'win32':
+            static_exts.append('.lib')  # .lib is used by MSVC and others
         if self.search_static_first:
-            exts = ['.a',so_ext]
+            exts = static_exts + [so_ext]
         else:
-            exts = [so_ext,'.a']
-        if sys.platform=='cygwin':
+            exts = [so_ext] + static_exts
+        if sys.platform == 'cygwin':
             exts.append('.dll.a')
+        if sys.platform == 'darwin':
+            exts.append('.dylib')
+        return exts
+
+    def check_libs(self, lib_dirs, libs, opt_libs=[]):
+        """If static or shared libraries are available then return
+        their info dictionary.
+
+        Checks for all libraries as shared libraries first, then
+        static (or vice versa if self.search_static_first is True).
+        """
+        exts = self.library_extensions()
+        info = None
         for ext in exts:
-            info = self._check_libs(lib_dir,libs,opt_libs,[ext])
-            if info is not None: return info
-        return
-
-    def check_libs2(self,lib_dir,libs,opt_libs =[]):
-        """ If static or shared libraries are available then return
-            their info dictionary. """
-        if self.search_static_first:
-            exts = ['.a',so_ext]
+            info = self._check_libs(lib_dirs, libs, opt_libs, [ext])
+            if info is not None:
+                break
+        if not info:
+            log.info('  libraries %s not found in %s', ','.join(libs),
+                     lib_dirs)
+        return info
+
+    def check_libs2(self, lib_dirs, libs, opt_libs=[]):
+        """If static or shared libraries are available then return
+        their info dictionary.
+
+        Checks each library for shared or static.
+        """
+        exts = self.library_extensions()
+        info = self._check_libs(lib_dirs, libs, opt_libs, exts)
+        if not info:
+            log.info('  libraries %s not found in %s', ','.join(libs),
+                     lib_dirs)
+
+        return info
+
+    def _find_lib(self, lib_dir, lib, exts):
+        assert is_string(lib_dir)
+        # under windows first try without 'lib' prefix
+        if sys.platform == 'win32':
+            lib_prefixes = ['', 'lib']
         else:
-            exts = [so_ext,'.a']
-        if sys.platform=='cygwin':
-            exts.append('.dll.a')
-        info = self._check_libs(lib_dir,libs,opt_libs,exts)
-        if info is not None: return info
-        return
-
-    def _lib_list(self, lib_dir, libs, exts):
-        assert is_string(lib_dir)
-        liblist = []
-        for l in libs:
-            for ext in exts:
-                p = self.combine_paths(lib_dir, 'lib'+l+ext)
+            lib_prefixes = ['lib']
+        # for each library name, see if we can find a file for it.
+        for ext in exts:
+            for prefix in lib_prefixes:
+                p = self.combine_paths(lib_dir, prefix + lib + ext)
                 if p:
-                    assert len(p)==1
-                    liblist.append(p[0])
                     break
-        return liblist
-
-    def _extract_lib_names(self,libs):
-        return [os.path.splitext(os.path.basename(p))[0][3:] \
-                for p in libs]
-
-    def _check_libs(self,lib_dir,libs, opt_libs, exts):
-        found_libs = self._lib_list(lib_dir, libs, exts)
-        if len(found_libs) == len(libs):
-            found_libs = self._extract_lib_names(found_libs)
-            info = {'libraries' : found_libs, 'library_dirs' : [lib_dir]}
-            opt_found_libs = self._lib_list(lib_dir, opt_libs, exts)
-            if len(opt_found_libs) == len(opt_libs):
-                opt_found_libs = self._extract_lib_names(opt_found_libs)
-                info['libraries'].extend(opt_found_libs)
+            if p:
+                assert len(p) == 1
+                # ??? splitext on p[0] would do this for cygwin
+                # doesn't seem correct
+                if ext == '.dll.a':
+                    lib += '.dll'
+                if ext == '.lib':
+                    lib = prefix + lib
+                return lib
+
+        return False
+
+    def _find_libs(self, lib_dirs, libs, exts):
+        # make sure we preserve the order of libs, as it can be important
+        found_dirs, found_libs = [], []
+        for lib in libs:
+            for lib_dir in lib_dirs:
+                found_lib = self._find_lib(lib_dir, lib, exts)
+                if found_lib:
+                    found_libs.append(found_lib)
+                    if lib_dir not in found_dirs:
+                        found_dirs.append(lib_dir)
+                    break
+        return found_dirs, found_libs
+
+    def _check_libs(self, lib_dirs, libs, opt_libs, exts):
+        """Find mandatory and optional libs in expected paths.
+
+        Missing optional libraries are silently forgotten.
+        """
+        if not is_sequence(lib_dirs):
+            lib_dirs = [lib_dirs]
+        # First, try to find the mandatory libraries
+        found_dirs, found_libs = self._find_libs(lib_dirs, libs, exts)
+        if len(found_libs) > 0 and len(found_libs) == len(libs):
+            # Now, check for optional libraries
+            opt_found_dirs, opt_found_libs = self._find_libs(lib_dirs, opt_libs, exts)
+            found_libs.extend(opt_found_libs)
+            for lib_dir in opt_found_dirs:
+                if lib_dir not in found_dirs:
+                    found_dirs.append(lib_dir)
+            info = {'libraries': found_libs, 'library_dirs': found_dirs}
             return info
         else:
-            print '  looking libraries %s in %s but found %s' % \
-                  (','.join(libs), lib_dir, ','.join(found_libs) or None)
-
-    def combine_paths(self,*args):
-        return combine_paths(*args,**{'verbosity':self.verbosity})
+            return None
+
+    def combine_paths(self, *args):
+        """Return a list of existing paths composed by all combinations
+        of items from the arguments.
+        """
+        return combine_paths(*args)
 
 
 class fft_opt_info(system_info):
@@ -554,9 +1079,9 @@
         fftw_info = get_info('fftw3') or get_info('fftw2') or get_info('dfftw')
         djbfft_info = get_info('djbfft')
         if fftw_info:
-            dict_append(info,**fftw_info)
+            dict_append(info, **fftw_info)
             if djbfft_info:
-                dict_append(info,**djbfft_info)
+                dict_append(info, **djbfft_info)
             self.set_info(**info)
             return
 
@@ -566,117 +1091,126 @@
     section = 'fftw'
     dir_env_var = 'FFTW'
     notfounderror = FFTWNotFoundError
-    ver_info  = [ { 'name':'fftw3',
+    ver_info = [{'name':'fftw3',
                     'libs':['fftw3'],
                     'includes':['fftw3.h'],
-                    'macros':[('SCIPY_FFTW3_H',None)]},
-                  { 'name':'fftw2',
+                    'macros':[('SCIPY_FFTW3_H', None)]},
+                  {'name':'fftw2',
                     'libs':['rfftw', 'fftw'],
-                    'includes':['fftw.h','rfftw.h'],
-                    'macros':[('SCIPY_FFTW_H',None)]}]
-
-    def __init__(self):
-        system_info.__init__(self)
-
-    def calc_ver_info(self,ver_param):
+                    'includes':['fftw.h', 'rfftw.h'],
+                    'macros':[('SCIPY_FFTW_H', None)]}]
+
+    def calc_ver_info(self, ver_param):
         """Returns True on successful version detection, else False"""
         lib_dirs = self.get_lib_dirs()
         incl_dirs = self.get_include_dirs()
-        incl_dir = None
-        libs = self.get_libs(self.section+'_libs', ver_param['libs'])
-        info = None
-        for d in lib_dirs:
-            r = self.check_libs(d,libs)
-            if r is not None:
-                info = r
-                break
+
+        opt = self.get_option_single(self.section + '_libs', 'libraries')
+        libs = self.get_libs(opt, ver_param['libs'])
+        info = self.check_libs(lib_dirs, libs)
         if info is not None:
             flag = 0
             for d in incl_dirs:
-                if len(self.combine_paths(d,ver_param['includes']))==len(ver_param['includes']):
-                    dict_append(info,include_dirs=[d])
+                if len(self.combine_paths(d, ver_param['includes'])) \
+                   == len(ver_param['includes']):
+                    dict_append(info, include_dirs=[d])
                     flag = 1
-                    incl_dirs = [d]
-                    incl_dir = d
                     break
             if flag:
-                dict_append(info,define_macros=ver_param['macros'])
+                dict_append(info, define_macros=ver_param['macros'])
             else:
                 info = None
         if info is not None:
             self.set_info(**info)
             return True
         else:
-            if self.verbosity>0:
-                print '  %s not found' % (ver_param['name'])
+            log.info('  %s not found' % (ver_param['name']))
             return False
 
     def calc_info(self):
         for i in self.ver_info:
             if self.calc_ver_info(i):
                 break
+
 
 class fftw2_info(fftw_info):
     #variables to override
     section = 'fftw'
     dir_env_var = 'FFTW'
     notfounderror = FFTWNotFoundError
-    ver_info  = [ { 'name':'fftw2',
+    ver_info = [{'name':'fftw2',
                     'libs':['rfftw', 'fftw'],
-                    'includes':['fftw.h','rfftw.h'],
-                    'macros':[('SCIPY_FFTW_H',None)]}
+                    'includes':['fftw.h', 'rfftw.h'],
+                    'macros':[('SCIPY_FFTW_H', None)]}
                   ]
+
 
 class fftw3_info(fftw_info):
     #variables to override
     section = 'fftw3'
     dir_env_var = 'FFTW3'
     notfounderror = FFTWNotFoundError
-    ver_info  = [ { 'name':'fftw3',
+    ver_info = [{'name':'fftw3',
                     'libs':['fftw3'],
                     'includes':['fftw3.h'],
-                    'macros':[('SCIPY_FFTW3_H',None)]},
+                    'macros':[('SCIPY_FFTW3_H', None)]},
                   ]
+
+    
+class fftw3_armpl_info(fftw_info):
+    section = 'fftw3'
+    dir_env_var = 'ARMPL_DIR'
+    notfounderror = FFTWNotFoundError
+    ver_info = [{'name': 'fftw3',
+                    'libs': ['armpl_lp64_mp'],
+                    'includes': ['fftw3.h'],
+                    'macros': [('SCIPY_FFTW3_H', None)]}]
+
 
 class dfftw_info(fftw_info):
     section = 'fftw'
     dir_env_var = 'FFTW'
-    ver_info  = [ { 'name':'dfftw',
-                    'libs':['drfftw','dfftw'],
-                    'includes':['dfftw.h','drfftw.h'],
-                    'macros':[('SCIPY_DFFTW_H',None)]} ]
+    ver_info = [{'name':'dfftw',
+                    'libs':['drfftw', 'dfftw'],
+                    'includes':['dfftw.h', 'drfftw.h'],
+                    'macros':[('SCIPY_DFFTW_H', None)]}]
+
 
 class sfftw_info(fftw_info):
     section = 'fftw'
     dir_env_var = 'FFTW'
-    ver_info  = [ { 'name':'sfftw',
-                    'libs':['srfftw','sfftw'],
-                    'includes':['sfftw.h','srfftw.h'],
-                    'macros':[('SCIPY_SFFTW_H',None)]} ]
+    ver_info = [{'name':'sfftw',
+                    'libs':['srfftw', 'sfftw'],
+                    'includes':['sfftw.h', 'srfftw.h'],
+                    'macros':[('SCIPY_SFFTW_H', None)]}]
+
 
 class fftw_threads_info(fftw_info):
     section = 'fftw'
     dir_env_var = 'FFTW'
-    ver_info  = [ { 'name':'fftw threads',
-                    'libs':['rfftw_threads','fftw_threads'],
-                    'includes':['fftw_threads.h','rfftw_threads.h'],
-                    'macros':[('SCIPY_FFTW_THREADS_H',None)]} ]
+    ver_info = [{'name':'fftw threads',
+                    'libs':['rfftw_threads', 'fftw_threads'],
+                    'includes':['fftw_threads.h', 'rfftw_threads.h'],
+                    'macros':[('SCIPY_FFTW_THREADS_H', None)]}]
+
 
 class dfftw_threads_info(fftw_info):
     section = 'fftw'
     dir_env_var = 'FFTW'
-    ver_info  = [ { 'name':'dfftw threads',
-                    'libs':['drfftw_threads','dfftw_threads'],
-                    'includes':['dfftw_threads.h','drfftw_threads.h'],
-                    'macros':[('SCIPY_DFFTW_THREADS_H',None)]} ]
+    ver_info = [{'name':'dfftw threads',
+                    'libs':['drfftw_threads', 'dfftw_threads'],
+                    'includes':['dfftw_threads.h', 'drfftw_threads.h'],
+                    'macros':[('SCIPY_DFFTW_THREADS_H', None)]}]
+
 
 class sfftw_threads_info(fftw_info):
     section = 'fftw'
     dir_env_var = 'FFTW'
-    ver_info  = [ { 'name':'sfftw threads',
-                    'libs':['srfftw_threads','sfftw_threads'],
-                    'includes':['sfftw_threads.h','srfftw_threads.h'],
-                    'macros':[('SCIPY_SFFTW_THREADS_H',None)]} ]
+    ver_info = [{'name':'sfftw threads',
+                    'libs':['srfftw_threads', 'sfftw_threads'],
+                    'includes':['sfftw_threads.h', 'srfftw_threads.h'],
+                    'macros':[('SCIPY_SFFTW_THREADS_H', None)]}]
+
 
 class djbfft_info(system_info):
     section = 'djbfft'
@@ -687,60 +1221,64 @@
         pre_dirs = system_info.get_paths(self, section, key)
         dirs = []
         for d in pre_dirs:
-            dirs.extend(self.combine_paths(d,['djbfft'])+[d])
-        return [ d for d in dirs if os.path.isdir(d) ]
+            dirs.extend(self.combine_paths(d, ['djbfft']) + [d])
+        return [d for d in dirs if os.path.isdir(d)]
 
     def calc_info(self):
         lib_dirs = self.get_lib_dirs()
         incl_dirs = self.get_include_dirs()
         info = None
         for d in lib_dirs:
-            p = self.combine_paths (d,['djbfft.a'])
+            p = self.combine_paths(d, ['djbfft.a'])
             if p:
-                info = {'extra_objects':p}
+                info = {'extra_objects': p}
                 break
-            p = self.combine_paths (d,['libdjbfft.a','libdjbfft'+so_ext])
+            p = self.combine_paths(d, ['libdjbfft.a', 'libdjbfft' + so_ext])
             if p:
-                info = {'libraries':['djbfft'],'library_dirs':[d]}
+                info = {'libraries': ['djbfft'], 'library_dirs': [d]}
                 break
         if info is None:
             return
         for d in incl_dirs:
-            if len(self.combine_paths(d,['fftc8.h','fftfreq.h']))==2:
-                dict_append(info,include_dirs=[d],
-                            define_macros=[('SCIPY_DJBFFT_H',None)])
+            if len(self.combine_paths(d, ['fftc8.h', 'fftfreq.h'])) == 2:
+                dict_append(info, include_dirs=[d],
+                            define_macros=[('SCIPY_DJBFFT_H', None)])
                 self.set_info(**info)
                 return
         return
 
+
 class mkl_info(system_info):
     section = 'mkl'
-    dir_env_var = 'MKL'
-    _lib_mkl = ['mkl','vml','guide']
+    dir_env_var = 'MKLROOT'
+    _lib_mkl = ['mkl_rt']
 
     def get_mkl_rootdir(self):
-        mklroot = os.environ.get('MKLROOT',None)
+        mklroot = os.environ.get('MKLROOT', None)
         if mklroot is not None:
             return mklroot
-        paths = os.environ.get('LD_LIBRARY_PATH','').split(os.pathsep)
+        paths = os.environ.get('LD_LIBRARY_PATH', '').split(os.pathsep)
         ld_so_conf = '/etc/ld.so.conf'
         if os.path.isfile(ld_so_conf):
-            for d in open(ld_so_conf,'r').readlines():
-                d = d.strip()
-                if d: paths.append(d)
+            with open(ld_so_conf, 'r') as f:
+                for d in f:
+                    d = d.strip()
+                    if d:
+                        paths.append(d)
         intel_mkl_dirs = []
         for path in paths:
             path_atoms = path.split(os.sep)
             for m in path_atoms:
                 if m.startswith('mkl'):
-                    d = os.sep.join(path_atoms[:path_atoms.index(m)+2])
+                    d = os.sep.join(path_atoms[:path_atoms.index(m) + 2])
                     intel_mkl_dirs.append(d)
                     break
         for d in paths:
-            dirs = glob(os.path.join(d,'mkl','*')) + glob(os.path.join(d,'mkl*'))
-            for d in dirs:
-                if os.path.isdir(os.path.join(d,'lib')):
-                    return d
+            dirs = glob(os.path.join(d, 'mkl', '*'))
+            dirs += glob(os.path.join(d, 'mkl*'))
+            for sub_dir in dirs:
+                if os.path.isdir(os.path.join(sub_dir, 'lib')):
+                    return sub_dir
         return None
 
     def __init__(self):
@@ -748,58 +1286,75 @@
         if mklroot is None:
             system_info.__init__(self)
         else:
-            from cpuinfo import cpu
-            l = 'mkl' # use shared library
+            from .cpuinfo import cpu
             if cpu.is_Itanium():
                 plt = '64'
-                #l = 'mkl_ipf'
-            elif cpu.is_Xeon():
-                plt = 'em64t'
-                #l = 'mkl_em64t'
+            elif cpu.is_Intel() and cpu.is_64bit():
+                plt = 'intel64'
             else:
                 plt = '32'
-                #l = 'mkl_ia32'
-            if l not in self._lib_mkl:
-                self._lib_mkl.insert(0,l)
-            system_info.__init__(self,
-                                 default_lib_dirs=[os.path.join(mklroot,'lib',plt)],
-                                 default_include_dirs=[os.path.join(mklroot,'include')])
+            system_info.__init__(
+                self,
+                default_lib_dirs=[os.path.join(mklroot, 'lib', plt)],
+                default_include_dirs=[os.path.join(mklroot, 'include')])
 
     def calc_info(self):
         lib_dirs = self.get_lib_dirs()
         incl_dirs = self.get_include_dirs()
-        mkl_libs = self.get_libs('mkl_libs',self._lib_mkl)
-        mkl = None
-        for d in lib_dirs:
-            mkl = self.check_libs2(d,mkl_libs)
-            if mkl is not None:
-                break
-        if mkl is None:
+        opt = self.get_option_single('mkl_libs', 'libraries')
+        mkl_libs = self.get_libs(opt, self._lib_mkl)
+        info = self.check_libs2(lib_dirs, mkl_libs)
+        if info is None:
             return
-        info = {}
-        dict_append(info,**mkl)
-        dict_append(info,libraries = ['pthread'], include_dirs = incl_dirs)
+        dict_append(info,
+                    define_macros=[('SCIPY_MKL_H', None),
+                                   ('HAVE_CBLAS', None)],
+                    include_dirs=incl_dirs)
+        if sys.platform == 'win32':
+            pass  # win32 has no pthread library
+        else:
+            dict_append(info, libraries=['pthread'])
         self.set_info(**info)
 
+
 class lapack_mkl_info(mkl_info):
-
-    def calc_info(self):
-        mkl = get_info('mkl')
-        if not mkl:
-            return
-        lapack_libs = self.get_libs('lapack_libs',['mkl_lapack32','mkl_lapack64'])
-        info = {'libraries': lapack_libs}
-        dict_append(info,**mkl)
-        self.set_info(**info)
+    pass
+
 
 class blas_mkl_info(mkl_info):
     pass
+
+
+class armpl_info(system_info):
+    section = 'armpl'
+    dir_env_var = 'ARMPL_DIR'
+    _lib_armpl = ['armpl_lp64_mp']
+
+    def calc_info(self):
+        lib_dirs = self.get_lib_dirs()
+        incl_dirs = self.get_include_dirs()
+        armpl_libs = self.get_libs('armpl_libs', self._lib_armpl)
+        info = self.check_libs2(lib_dirs, armpl_libs)
+        if info is None:
+            return
+        dict_append(info,
+                    define_macros=[('SCIPY_MKL_H', None),
+                                   ('HAVE_CBLAS', None)],
+                    include_dirs=incl_dirs)
+        self.set_info(**info)
+
+class lapack_armpl_info(armpl_info):
+    pass
+
+class blas_armpl_info(armpl_info):
+    pass
+
 
 class atlas_info(system_info):
     section = 'atlas'
     dir_env_var = 'ATLAS'
-    _lib_names = ['f77blas','cblas']
-    if sys.platform[:7]=='freebsd':
+    _lib_names = ['f77blas', 'cblas']
+    if sys.platform[:7] == 'freebsd':
         _lib_atlas = ['atlas_r']
         _lib_lapack = ['alapack_r']
     else:
@@ -812,62 +1367,58 @@
         pre_dirs = system_info.get_paths(self, section, key)
         dirs = []
         for d in pre_dirs:
-            dirs.extend(self.combine_paths(d,['atlas*','ATLAS*',
-                                         'sse','3dnow','sse2'])+[d])
-        return [ d for d in dirs if os.path.isdir(d) ]
+            dirs.extend(self.combine_paths(d, ['atlas*', 'ATLAS*',
+                                         'sse', '3dnow', 'sse2']) + [d])
+        return [d for d in dirs if os.path.isdir(d)]
 
     def calc_info(self):
         lib_dirs = self.get_lib_dirs()
         info = {}
-        atlas_libs = self.get_libs('atlas_libs',
-                                   self._lib_names + self._lib_atlas)
-        lapack_libs = self.get_libs('lapack_libs',self._lib_lapack)
+        opt = self.get_option_single('atlas_libs', 'libraries')
+        atlas_libs = self.get_libs(opt, self._lib_names + self._lib_atlas)
+        lapack_libs = self.get_libs('lapack_libs', self._lib_lapack)
         atlas = None
         lapack = None
         atlas_1 = None
         for d in lib_dirs:
-            atlas = self.check_libs(d,atlas_libs,[])
-            lapack_atlas = self.check_libs(d,['lapack_atlas'],[])
+            atlas = self.check_libs2(d, atlas_libs, [])
             if atlas is not None:
-                lib_dirs2 = self.combine_paths(d,['atlas*','ATLAS*'])+[d]
-                for d2 in lib_dirs2:
-                    lapack = self.check_libs(d2,lapack_libs,[])
-                    if lapack is not None:
-                        break
-                else:
-                    lapack = None
+                lib_dirs2 = [d] + self.combine_paths(d, ['atlas*', 'ATLAS*'])
+                lapack = self.check_libs2(lib_dirs2, lapack_libs, [])
                 if lapack is not None:
                     break
             if atlas:
                 atlas_1 = atlas
-        print self.__class__
+        log.info(self.__class__)
         if atlas is None:
             atlas = atlas_1
         if atlas is None:
             return
         include_dirs = self.get_include_dirs()
-        h = (self.combine_paths(lib_dirs+include_dirs,'cblas.h') or [None])[0]
+        h = (self.combine_paths(lib_dirs + include_dirs, 'cblas.h') or [None])
+        h = h[0]
         if h:
             h = os.path.dirname(h)
-            dict_append(info,include_dirs=[h])
+            dict_append(info, include_dirs=[h])
         info['language'] = 'c'
         if lapack is not None:
-            dict_append(info,**lapack)
-            dict_append(info,**atlas)
+            dict_append(info, **lapack)
+            dict_append(info, **atlas)
         elif 'lapack_atlas' in atlas['libraries']:
-            dict_append(info,**atlas)
-            dict_append(info,define_macros=[('ATLAS_WITH_LAPACK_ATLAS',None)])
+            dict_append(info, **atlas)
+            dict_append(info,
+                        define_macros=[('ATLAS_WITH_LAPACK_ATLAS', None)])
             self.set_info(**info)
             return
         else:
-            dict_append(info,**atlas)
-            dict_append(info,define_macros=[('ATLAS_WITHOUT_LAPACK',None)])
-            message = """
-*********************************************************************
-    Could not find lapack library within the ATLAS installation.
-*********************************************************************
-"""
-            warnings.warn(message)
+            dict_append(info, **atlas)
+            dict_append(info, define_macros=[('ATLAS_WITHOUT_LAPACK', None)])
+            message = textwrap.dedent("""
+                *********************************************************************
+                    Could not find lapack library within the ATLAS installation.
+                *********************************************************************
+                """)
+            warnings.warn(message, stacklevel=2)
             self.set_info(**info)
             return
 
@@ -875,70 +1426,140 @@
         lapack_dir = lapack['library_dirs'][0]
         lapack_name = lapack['libraries'][0]
         lapack_lib = None
-        for e in ['.a',so_ext]:
-            fn = os.path.join(lapack_dir,'lib'+lapack_name+e)
-            if os.path.exists(fn):
-                lapack_lib = fn
+        lib_prefixes = ['lib']
+        if sys.platform == 'win32':
+            lib_prefixes.append('')
+        for e in self.library_extensions():
+            for prefix in lib_prefixes:
+                fn = os.path.join(lapack_dir, prefix + lapack_name + e)
+                if os.path.exists(fn):
+                    lapack_lib = fn
+                    break
+            if lapack_lib:
                 break
         if lapack_lib is not None:
             sz = os.stat(lapack_lib)[6]
-            if sz <= 4000*1024:
-                message = """
-*********************************************************************
-    Lapack library (from ATLAS) is probably incomplete:
-      size of %s is %sk (expected >4000k)
-
-    Follow the instructions in the KNOWN PROBLEMS section of the file
-    numpy/INSTALL.txt.
-*********************************************************************
-""" % (lapack_lib,sz/1024)
-                warnings.warn(message)
+            if sz <= 4000 * 1024:
+                message = textwrap.dedent("""
+                    *********************************************************************
+                        Lapack library (from ATLAS) is probably incomplete:
+                          size of %s is %sk (expected >4000k)
+
+                        Follow the instructions in the KNOWN PROBLEMS section of the file
+                        numpy/INSTALL.txt.
+                    *********************************************************************
+                    """) % (lapack_lib, sz / 1024)
+                warnings.warn(message, stacklevel=2)
             else:
                 info['language'] = 'f77'
 
+        atlas_version, atlas_extra_info = get_atlas_version(**atlas)
+        dict_append(info, **atlas_extra_info)
+
         self.set_info(**info)
 
+
 class atlas_blas_info(atlas_info):
-    _lib_names = ['f77blas','cblas']
+    _lib_names = ['f77blas', 'cblas']
 
     def calc_info(self):
         lib_dirs = self.get_lib_dirs()
         info = {}
-        atlas_libs = self.get_libs('atlas_libs',
-                                   self._lib_names + self._lib_atlas)
-        atlas = None
-        for d in lib_dirs:
-            atlas = self.check_libs(d,atlas_libs,[])
-            if atlas is not None:
-                break
+        opt = self.get_option_single('atlas_libs', 'libraries')
+        atlas_libs = self.get_libs(opt, self._lib_names + self._lib_atlas)
+        atlas = self.check_libs2(lib_dirs, atlas_libs, [])
         if atlas is None:
             return
         include_dirs = self.get_include_dirs()
-        h = (self.combine_paths(lib_dirs+include_dirs,'cblas.h') or [None])[0]
+        h = (self.combine_paths(lib_dirs + include_dirs, 'cblas.h') or [None])
+        h = h[0]
         if h:
             h = os.path.dirname(h)
-            dict_append(info,include_dirs=[h])
+            dict_append(info, include_dirs=[h])
         info['language'] = 'c'
-
-        dict_append(info,**atlas)
+        info['define_macros'] = [('HAVE_CBLAS', None)]
+
+        atlas_version, atlas_extra_info = get_atlas_version(**atlas)
+        dict_append(atlas, **atlas_extra_info)
+
+        dict_append(info, **atlas)
 
         self.set_info(**info)
         return
 
 
 class atlas_threads_info(atlas_info):
-    dir_env_var = ['PTATLAS','ATLAS']
-    _lib_names = ['ptf77blas','ptcblas']
+    dir_env_var = ['PTATLAS', 'ATLAS']
+    _lib_names = ['ptf77blas', 'ptcblas']
+
 
 class atlas_blas_threads_info(atlas_blas_info):
-    dir_env_var = ['PTATLAS','ATLAS']
-    _lib_names = ['ptf77blas','ptcblas']
+    dir_env_var = ['PTATLAS', 'ATLAS']
+    _lib_names = ['ptf77blas', 'ptcblas']
+
 
 class lapack_atlas_info(atlas_info):
     _lib_names = ['lapack_atlas'] + atlas_info._lib_names
 
+
 class lapack_atlas_threads_info(atlas_threads_info):
     _lib_names = ['lapack_atlas'] + atlas_threads_info._lib_names
+
+
+class atlas_3_10_info(atlas_info):
+    _lib_names = ['satlas']
+    _lib_atlas = _lib_names
+    _lib_lapack = _lib_names
+
+
+class atlas_3_10_blas_info(atlas_3_10_info):
+    _lib_names = ['satlas']
+
+    def calc_info(self):
+        lib_dirs = self.get_lib_dirs()
+        info = {}
+        opt = self.get_option_single('atlas_lib', 'libraries')
+        atlas_libs = self.get_libs(opt, self._lib_names)
+        atlas = self.check_libs2(lib_dirs, atlas_libs, [])
+        if atlas is None:
+            return
+        include_dirs = self.get_include_dirs()
+        h = (self.combine_paths(lib_dirs + include_dirs, 'cblas.h') or [None])
+        h = h[0]
+        if h:
+            h = os.path.dirname(h)
+            dict_append(info, include_dirs=[h])
+        info['language'] = 'c'
+        info['define_macros'] = [('HAVE_CBLAS', None)]
+
+        atlas_version, atlas_extra_info = get_atlas_version(**atlas)
+        dict_append(atlas, **atlas_extra_info)
+
+        dict_append(info, **atlas)
+
+        self.set_info(**info)
+        return
+
+
+class atlas_3_10_threads_info(atlas_3_10_info):
+    dir_env_var = ['PTATLAS', 'ATLAS']
+    _lib_names = ['tatlas']
+    _lib_atlas = _lib_names
+    _lib_lapack = _lib_names
+
+
+class atlas_3_10_blas_threads_info(atlas_3_10_blas_info):
+    dir_env_var = ['PTATLAS', 'ATLAS']
+    _lib_names = ['tatlas']
+
+
+class lapack_atlas_3_10_info(atlas_3_10_info):
+    pass
+
+
+class lapack_atlas_3_10_threads_info(atlas_3_10_threads_info):
+    pass
+
 
 class lapack_info(system_info):
     section = 'lapack'
@@ -949,18 +1570,19 @@
     def calc_info(self):
         lib_dirs = self.get_lib_dirs()
 
-        lapack_libs = self.get_libs('lapack_libs', self._lib_names)
-        for d in lib_dirs:
-            lapack = self.check_libs(d,lapack_libs,[])
-            if lapack is not None:
-                info = lapack
-                break
-        else:
+        opt = self.get_option_single('lapack_libs', 'libraries')
+        lapack_libs = self.get_libs(opt, self._lib_names)
+        info = self.check_libs(lib_dirs, lapack_libs, [])
+        if info is None:
             return
         info['language'] = 'f77'
         self.set_info(**info)
 
+
 class lapack_src_info(system_info):
+    # LAPACK_SRC is deprecated, please do not use this!
+    # Build or install a BLAS library via your package manager or from
+    # source separately.
     section = 'lapack_src'
     dir_env_var = 'LAPACK_SRC'
     notfounderror = LapackSrcNotFoundError
@@ -969,23 +1591,26 @@
         pre_dirs = system_info.get_paths(self, section, key)
         dirs = []
         for d in pre_dirs:
-            dirs.extend([d] + self.combine_paths(d,['LAPACK*/SRC','SRC']))
-        return [ d for d in dirs if os.path.isdir(d) ]
+            dirs.extend([d] + self.combine_paths(d, ['LAPACK*/SRC', 'SRC']))
+        return [d for d in dirs if os.path.isdir(d)]
 
     def calc_info(self):
         src_dirs = self.get_src_dirs()
         src_dir = ''
         for d in src_dirs:
-            if os.path.isfile(os.path.join(d,'dgesv.f')):
+            if os.path.isfile(os.path.join(d, 'dgesv.f')):
                 src_dir = d
                 break
         if not src_dir:
             #XXX: Get sources from netlib. May be ask first.
             return
-        # The following is extracted from LAPACK-3.0/SRC/Makefile
-        allaux='''
+        # The following is extracted from LAPACK-3.0/SRC/Makefile.
+        # Added missing names from lapack-lite-3.1.1/SRC/Makefile
+        # while keeping removed names for Lapack-3.0 compatibility.
+        allaux = '''
         ilaenv ieeeck lsame lsamen xerbla
-        ''' # *.f
+        iparmq
+        '''  # *.f
         laux = '''
         bdsdc bdsqr disna labad lacpy ladiv lae2 laebz laed0 laed1
         laed2 laed3 laed4 laed5 laed6 laed7 laed8 laed9 laeda laev2
@@ -994,7 +1619,10 @@
         lasd5 lasd6 lasd7 lasd8 lasd9 lasda lasdq lasdt laset lasq1
         lasq2 lasq3 lasq4 lasq5 lasq6 lasr lasrt lassq lasv2 pttrf
         stebz stedc steqr sterf
-        ''' # [s|d]*.f
+
+        larra larrc larrd larr larrk larrj larrr laneg laisnan isnan
+        lazq3 lazq4
+        '''  # [s|d]*.f
         lasrc = '''
         gbbrd gbcon gbequ gbrfs gbsv gbsvx gbtf2 gbtrf gbtrs gebak
         gebal gebd2 gebrd gecon geequ gees geesx geev geevx gegs gegv
@@ -1017,7 +1645,9 @@
         tgexc tgsen tgsja tgsna tgsy2 tgsyl tpcon tprfs tptri tptrs
         trcon trevc trexc trrfs trsen trsna trsyl trti2 trtri trtrs
         tzrqf tzrzf
-        ''' # [s|c|d|z]*.f
+
+        lacn2 lahr2 stemr laqr0 laqr1 laqr2 laqr3 laqr4 laqr5
+        '''  # [s|c|d|z]*.f
         sd_lasrc = '''
         laexc lag2 lagv2 laln2 lanv2 laqtr lasy2 opgtr opmtr org2l
         org2r orgbr orghr orgl2 orglq orgql orgqr orgr2 orgrq orgtr
@@ -1026,7 +1656,7 @@
         sbtrd spev spevd spevx spgst spgv spgvd spgvx sptrd stev stevd
         stevr stevx syev syevd syevr syevx sygs2 sygst sygv sygvd
         sygvx sytd2 sytrd
-        ''' # [s|d]*.f
+        '''  # [s|d]*.f
         cz_lasrc = '''
         bdsqr hbev hbevd hbevx hbgst hbgv hbgvd hbgvx hbtrd hecon heev
         heevd heevr heevx hegs2 hegst hegv hegvd hegvx herfs hesv
@@ -1038,264 +1668,451 @@
         spr stedc steqr symv syr ung2l ung2r ungbr unghr ungl2 unglq
         ungql ungqr ungr2 ungrq ungtr unm2l unm2r unmbr unmhr unml2
         unmlq unmql unmqr unmr2 unmr3 unmrq unmrz unmtr upgtr upmtr
-        ''' # [c|z]*.f
+        '''  # [c|z]*.f
         #######
         sclaux = laux + ' econd '                  # s*.f
         dzlaux = laux + ' secnd '                  # d*.f
         slasrc = lasrc + sd_lasrc                  # s*.f
         dlasrc = lasrc + sd_lasrc                  # d*.f
-        clasrc = lasrc + cz_lasrc + ' srot srscl ' # c*.f
-        zlasrc = lasrc + cz_lasrc + ' drot drscl ' # z*.f
+        clasrc = lasrc + cz_lasrc + ' srot srscl '  # c*.f
+        zlasrc = lasrc + cz_lasrc + ' drot drscl '  # z*.f
         oclasrc = ' icmax1 scsum1 '                # *.f
         ozlasrc = ' izmax1 dzsum1 '                # *.f
-        sources = ['s%s.f'%f for f in (sclaux+slasrc).split()] \
-                  + ['d%s.f'%f for f in (dzlaux+dlasrc).split()] \
-                  + ['c%s.f'%f for f in (clasrc).split()] \
-                  + ['z%s.f'%f for f in (zlasrc).split()] \
-                  + ['%s.f'%f for f in (allaux+oclasrc+ozlasrc).split()]
-        sources = [os.path.join(src_dir,f) for f in sources]
-        #XXX: should we check here actual existence of source files?
-        info = {'sources':sources,'language':'f77'}
+        sources = ['s%s.f' % f for f in (sclaux + slasrc).split()] \
+                  + ['d%s.f' % f for f in (dzlaux + dlasrc).split()] \
+                  + ['c%s.f' % f for f in (clasrc).split()] \
+                  + ['z%s.f' % f for f in (zlasrc).split()] \
+                  + ['%s.f' % f for f in (allaux + oclasrc + ozlasrc).split()]
+        sources = [os.path.join(src_dir, f) for f in sources]
+        # Lapack 3.1:
+        src_dir2 = os.path.join(src_dir, '..', 'INSTALL')
+        sources += [os.path.join(src_dir2, p + 'lamch.f') for p in 'sdcz']
+        # Lapack 3.2.1:
+        sources += [os.path.join(src_dir, p + 'larfp.f') for p in 'sdcz']
+        sources += [os.path.join(src_dir, 'ila' + p + 'lr.f') for p in 'sdcz']
+        sources += [os.path.join(src_dir, 'ila' + p + 'lc.f') for p in 'sdcz']
+        # Should we check here actual existence of source files?
+        # Yes, the file listing is different between 3.0 and 3.1
+        # versions.
+        sources = [f for f in sources if os.path.isfile(f)]
+        info = {'sources': sources, 'language': 'f77'}
         self.set_info(**info)
 
 atlas_version_c_text = r'''
-/* This file is generated from numpy_distutils/system_info.py */
-#ifdef __CPLUSPLUS__
-extern "C" {
-#endif
-#include "Python.h"
-static PyMethodDef module_methods[] = { {NULL,NULL} };
-PyMODINIT_FUNC initatlas_version(void) {
-  void ATL_buildinfo(void);
+/* This file is generated from numpy/distutils/system_info.py */
+void ATL_buildinfo(void);
+int main(void) {
   ATL_buildinfo();
-  Py_InitModule("atlas_version", module_methods);
+  return 0;
 }
-#ifdef __CPLUSCPLUS__
-}
-#endif
 '''
 
-def _get_build_temp():
-    from distutils.util import get_platform
-    plat_specifier = ".%s-%s" % (get_platform(), sys.version[0:3])
-    return os.path.join('build','temp'+plat_specifier)
+_cached_atlas_version = {}
+
 
 def get_atlas_version(**config):
-    os.environ['NO_SCIPY_IMPORT']='get_atlas_version'
-    from core import Extension, setup
-    from misc_util import get_cmd
-    import log
-    magic = hex(hash(repr(config)))
-    def atlas_version_c(extension, build_dir,magic=magic):
-        source = os.path.join(build_dir,'atlas_version_%s.c' % (magic))
-        if os.path.isfile(source):
-            from distutils.dep_util import newer
-            if newer(source,__file__):
-                return source
-        f = open(source,'w')
-        f.write(atlas_version_c_text)
-        f.close()
-        return source
-    ext = Extension('atlas_version',
-                    sources=[atlas_version_c],
-                    **config)
-    build_dir = _get_build_temp()
-    extra_args = ['--build-lib',build_dir]
-    for a in sys.argv:
-        if re.match('[-][-]compiler[=]',a):
-            extra_args.append(a)
-    import distutils.core
-    old_dist = distutils.core._setup_distribution
-    distutils.core._setup_distribution = None
-    return_flag = True
+    libraries = config.get('libraries', [])
+    library_dirs = config.get('library_dirs', [])
+    key = (tuple(libraries), tuple(library_dirs))
+    if key in _cached_atlas_version:
+        return _cached_atlas_version[key]
+    c = cmd_config(Distribution())
+    atlas_version = None
+    info = {}
     try:
-        dist = setup(ext_modules=[ext],
-                     script_name = 'get_atlas_version',
-                     script_args = ['build_src','build_ext']+extra_args)
-        return_flag = False
-    except Exception,msg:
-        print "##### msg: %s" % msg
-        if not msg:
-            msg = "Unknown Exception"
-        log.warn(msg)
-    distutils.core._setup_distribution = old_dist
-
-    if return_flag:
-        return
-
-    from distutils.sysconfig import get_config_var
-    so_ext = get_config_var('SO')
-    target = os.path.join(build_dir,'atlas_version'+so_ext)
-    cmd = [get_pythonexe(),'-c',
-           '"import imp,os;os.environ[\\"NO_SCIPY_IMPORT\\"]='\
-           '\\"system_info.get_atlas_version:load atlas_version\\";'\
-           'imp.load_dynamic(\\"atlas_version\\",\\"%s\\")"'\
-           % (os.path.basename(target))]
-    s,o = exec_command(cmd,execute_in=os.path.dirname(target),use_tee=0)
-    atlas_version = None
+        s, o = c.get_output(atlas_version_c_text,
+                            libraries=libraries, library_dirs=library_dirs,
+                           )
+        if s and re.search(r'undefined reference to `_gfortran', o, re.M):
+            s, o = c.get_output(atlas_version_c_text,
+                                libraries=libraries + ['gfortran'],
+                                library_dirs=library_dirs,
+                               )
+            if not s:
+                warnings.warn(textwrap.dedent("""
+                    *****************************************************
+                    Linkage with ATLAS requires gfortran. Use
+
+                      python setup.py config_fc --fcompiler=gnu95 ...
+
+                    when building extension libraries that use ATLAS.
+                    Make sure that -lgfortran is used for C++ extensions.
+                    *****************************************************
+                    """), stacklevel=2)
+                dict_append(info, language='f90',
+                            define_macros=[('ATLAS_REQUIRES_GFORTRAN', None)])
+    except Exception:  # failed to get version from file -- maybe on Windows
+        # look at directory name
+        for o in library_dirs:
+            m = re.search(r'ATLAS_(?P<version>\d+[.]\d+[.]\d+)_', o)
+            if m:
+                atlas_version = m.group('version')
+            if atlas_version is not None:
+                break
+
+        # final choice --- look at ATLAS_VERSION environment
+        #   variable
+        if atlas_version is None:
+            atlas_version = os.environ.get('ATLAS_VERSION', None)
+        if atlas_version:
+            dict_append(info, define_macros=[(
+                'ATLAS_INFO', _c_string_literal(atlas_version))
+            ])
+        else:
+            dict_append(info, define_macros=[('NO_ATLAS_INFO', -1)])
+        return atlas_version or '?.?.?', info
+
     if not s:
-        m = re.search(r'ATLAS version (?P<version>\d+[.]\d+[.]\d+)',o)
+        m = re.search(r'ATLAS version (?P<version>\d+[.]\d+[.]\d+)', o)
         if m:
             atlas_version = m.group('version')
     if atlas_version is None:
-        if re.search(r'undefined symbol: ATL_buildinfo',o,re.M):
+        if re.search(r'undefined symbol: ATL_buildinfo', o, re.M):
             atlas_version = '3.2.1_pre3.3.6'
         else:
-            print 'Command:',' '.join(cmd)
-            print 'Status:',s
-            print 'Output:',o
-    return atlas_version
+            log.info('Status: %d', s)
+            log.info('Output: %s', o)
+
+    elif atlas_version == '3.2.1_pre3.3.6':
+        dict_append(info, define_macros=[('NO_ATLAS_INFO', -2)])
+    else:
+        dict_append(info, define_macros=[(
+            'ATLAS_INFO', _c_string_literal(atlas_version))
+        ])
+    result = _cached_atlas_version[key] = atlas_version, info
+    return result
 
 
 class lapack_opt_info(system_info):
+    notfounderror = LapackNotFoundError
+
+    # List of all known LAPACK libraries, in the default order
+    lapack_order = ['armpl', 'mkl', 'openblas', 'flame',
+                    'accelerate', 'atlas', 'lapack']
+    order_env_var_name = 'NPY_LAPACK_ORDER'
+    
+    def _calc_info_armpl(self):
+        info = get_info('lapack_armpl')
+        if info:
+            self.set_info(**info)
+            return True
+        return False
+
+    def _calc_info_mkl(self):
+        info = get_info('lapack_mkl')
+        if info:
+            self.set_info(**info)
+            return True
+        return False
+
+    def _calc_info_openblas(self):
+        info = get_info('openblas_lapack')
+        if info:
+            self.set_info(**info)
+            return True
+        info = get_info('openblas_clapack')
+        if info:
+            self.set_info(**info)
+            return True
+        return False
+
+    def _calc_info_flame(self):
+        info = get_info('flame')
+        if info:
+            self.set_info(**info)
+            return True
+        return False
+
+    def _calc_info_atlas(self):
+        info = get_info('atlas_3_10_threads')
+        if not info:
+            info = get_info('atlas_3_10')
+        if not info:
+            info = get_info('atlas_threads')
+        if not info:
+            info = get_info('atlas')
+        if info:
+            # Figure out if ATLAS has lapack...
+            # If not we need the lapack library, but not BLAS!
+            l = info.get('define_macros', [])
+            if ('ATLAS_WITH_LAPACK_ATLAS', None) in l \
+               or ('ATLAS_WITHOUT_LAPACK', None) in l:
+                # Get LAPACK (with possible warnings)
+                # If not found we don't accept anything
+                # since we can't use ATLAS with LAPACK!
+                lapack_info = self._get_info_lapack()
+                if not lapack_info:
+                    return False
+                dict_append(info, **lapack_info)
+            self.set_info(**info)
+            return True
+        return False
+
+    def _calc_info_accelerate(self):
+        info = get_info('accelerate')
+        if info:
+            self.set_info(**info)
+            return True
+        return False
+
+    def _get_info_blas(self):
+        # Default to get the optimized BLAS implementation
+        info = get_info('blas_opt')
+        if not info:
+            warnings.warn(BlasNotFoundError.__doc__ or '', stacklevel=3)
+            info_src = get_info('blas_src')
+            if not info_src:
+                warnings.warn(BlasSrcNotFoundError.__doc__ or '', stacklevel=3)
+                return {}
+            dict_append(info, libraries=[('fblas_src', info_src)])
+        return info
+
+    def _get_info_lapack(self):
+        info = get_info('lapack')
+        if not info:
+            warnings.warn(LapackNotFoundError.__doc__ or '', stacklevel=3)
+            info_src = get_info('lapack_src')
+            if not info_src:
+                warnings.warn(LapackSrcNotFoundError.__doc__ or '', stacklevel=3)
+                return {}
+            dict_append(info, libraries=[('flapack_src', info_src)])
+        return info
+
+    def _calc_info_lapack(self):
+        info = self._get_info_lapack()
+        if info:
+            info_blas = self._get_info_blas()
+            dict_append(info, **info_blas)
+            dict_append(info, define_macros=[('NO_ATLAS_INFO', 1)])
+            self.set_info(**info)
+            return True
+        return False
+
+    def _calc_info_from_envvar(self):
+        info = {}
+        info['language'] = 'f77'
+        info['libraries'] = []
+        info['include_dirs'] = []
+        info['define_macros'] = []
+        info['extra_link_args'] = os.environ['NPY_LAPACK_LIBS'].split()
+        self.set_info(**info)
+        return True
+
+    def _calc_info(self, name):
+        return getattr(self, '_calc_info_{}'.format(name))()
 
     def calc_info(self):
-
-        if sys.platform=='darwin' and not os.environ.get('ATLAS',None):
-            args = []
-            link_args = []
-            if os.path.exists('/System/Library/Frameworks/Accelerate.framework/'):
-                args.extend(['-faltivec'])
-                link_args.extend(['-Wl,-framework','-Wl,Accelerate'])
-            elif os.path.exists('/System/Library/Frameworks/vecLib.framework/'):
-                args.extend(['-faltivec'])
-                link_args.extend(['-Wl,-framework','-Wl,vecLib'])
-            if args:
-                self.set_info(extra_compile_args=args,
-                              extra_link_args=link_args,
-                              define_macros=[('NO_ATLAS_INFO',3)])
+        lapack_order, unknown_order = _parse_env_order(self.lapack_order, self.order_env_var_name)
+        if len(unknown_order) > 0:
+            raise ValueError("lapack_opt_info user defined "
+                             "LAPACK order has unacceptable "
+                             "values: {}".format(unknown_order))
+
+        if 'NPY_LAPACK_LIBS' in os.environ:
+            # Bypass autodetection, set language to F77 and use env var linker
+            # flags directly
+            self._calc_info_from_envvar()
+            return
+
+        for lapack in lapack_order:
+            if self._calc_info(lapack):
                 return
 
-        lapack_mkl_info = get_info('lapack_mkl')
-        if lapack_mkl_info:
-            self.set_info(**lapack_mkl_info)
+        if 'lapack' not in lapack_order:
+            # Since the user may request *not* to use any library, we still need
+            # to raise warnings to signal missing packages!
+            warnings.warn(LapackNotFoundError.__doc__ or '', stacklevel=2)
+            warnings.warn(LapackSrcNotFoundError.__doc__ or '', stacklevel=2)
+
+
+class _ilp64_opt_info_mixin:
+    symbol_suffix = None
+    symbol_prefix = None
+
+    def _check_info(self, info):
+        macros = dict(info.get('define_macros', []))
+        prefix = macros.get('BLAS_SYMBOL_PREFIX', '')
+        suffix = macros.get('BLAS_SYMBOL_SUFFIX', '')
+
+        if self.symbol_prefix not in (None, prefix):
+            return False
+
+        if self.symbol_suffix not in (None, suffix):
+            return False
+
+        return bool(info)
+
+
+class lapack_ilp64_opt_info(lapack_opt_info, _ilp64_opt_info_mixin):
+    notfounderror = LapackILP64NotFoundError
+    lapack_order = ['openblas64_', 'openblas_ilp64']
+    order_env_var_name = 'NPY_LAPACK_ILP64_ORDER'
+
+    def _calc_info(self, name):
+        info = get_info(name + '_lapack')
+        if self._check_info(info):
+            self.set_info(**info)
+            return True
+        return False
+
+
+class lapack_ilp64_plain_opt_info(lapack_ilp64_opt_info):
+    # Same as lapack_ilp64_opt_info, but fix symbol names
+    symbol_prefix = ''
+    symbol_suffix = ''
+
+
+class lapack64__opt_info(lapack_ilp64_opt_info):
+    symbol_prefix = ''
+    symbol_suffix = '64_'
+
+
+class blas_opt_info(system_info):
+    notfounderror = BlasNotFoundError
+    # List of all known BLAS libraries, in the default order
+
+    blas_order = ['armpl', 'mkl', 'blis', 'openblas',
+                  'accelerate', 'atlas', 'blas']
+    order_env_var_name = 'NPY_BLAS_ORDER'
+    
+    def _calc_info_armpl(self):
+        info = get_info('blas_armpl')
+        if info:
+            self.set_info(**info)
+            return True
+        return False
+
+    def _calc_info_mkl(self):
+        info = get_info('blas_mkl')
+        if info:
+            self.set_info(**info)
+            return True
+        return False
+
+    def _calc_info_blis(self):
+        info = get_info('blis')
+        if info:
+            self.set_info(**info)
+            return True
+        return False
+
+    def _calc_info_openblas(self):
+        info = get_info('openblas')
+        if info:
+            self.set_info(**info)
+            return True
+        return False
+
+    def _calc_info_atlas(self):
+        info = get_info('atlas_3_10_blas_threads')
+        if not info:
+            info = get_info('atlas_3_10_blas')
+        if not info:
+            info = get_info('atlas_blas_threads')
+        if not info:
+            info = get_info('atlas_blas')
+        if info:
+            self.set_info(**info)
+            return True
+        return False
+
+    def _calc_info_accelerate(self):
+        info = get_info('accelerate')
+        if info:
+            self.set_info(**info)
+            return True
+        return False
+
+    def _calc_info_blas(self):
+        # Warn about a non-optimized BLAS library
+        warnings.warn(BlasOptNotFoundError.__doc__ or '', stacklevel=3)
+        info = {}
+        dict_append(info, define_macros=[('NO_ATLAS_INFO', 1)])
+
+        blas = get_info('blas')
+        if blas:
+            dict_append(info, **blas)
+        else:
+            # Not even BLAS was found!
+            warnings.warn(BlasNotFoundError.__doc__ or '', stacklevel=3)
+
+            blas_src = get_info('blas_src')
+            if not blas_src:
+                warnings.warn(BlasSrcNotFoundError.__doc__ or '', stacklevel=3)
+                return False
+            dict_append(info, libraries=[('fblas_src', blas_src)])
+
+        self.set_info(**info)
+        return True
+
+    def _calc_info_from_envvar(self):
+        info = {}
+        info['language'] = 'f77'
+        info['libraries'] = []
+        info['include_dirs'] = []
+        info['define_macros'] = []
+        info['extra_link_args'] = os.environ['NPY_BLAS_LIBS'].split()
+        if 'NPY_CBLAS_LIBS' in os.environ:
+            info['define_macros'].append(('HAVE_CBLAS', None))
+            info['extra_link_args'].extend(
+                                        os.environ['NPY_CBLAS_LIBS'].split())
+        self.set_info(**info)
+        return True
+
+    def _calc_info(self, name):
+        return getattr(self, '_calc_info_{}'.format(name))()
+
+    def calc_info(self):
+        blas_order, unknown_order = _parse_env_order(self.blas_order, self.order_env_var_name)
+        if len(unknown_order) > 0:
+            raise ValueError("blas_opt_info user defined BLAS order has unacceptable values: {}".format(unknown_order))
+
+        if 'NPY_BLAS_LIBS' in os.environ:
+            # Bypass autodetection, set language to F77 and use env var linker
+            # flags directly
+            self._calc_info_from_envvar()
             return
 
-        atlas_info = get_info('atlas_threads')
-        if not atlas_info:
-            atlas_info = get_info('atlas')
-        #atlas_info = {} ## uncomment for testing
-        atlas_version = None
-        need_lapack = 0
-        need_blas = 0
-        info = {}
-        if atlas_info:
-            version_info = atlas_info.copy()
-            atlas_version = get_atlas_version(**version_info)
-            if not atlas_info.has_key('define_macros'):
-                atlas_info['define_macros'] = []
-            if atlas_version is None:
-                atlas_info['define_macros'].append(('NO_ATLAS_INFO',2))
-            else:
-                atlas_info['define_macros'].append(('ATLAS_INFO',
-                                                    '"\\"%s\\""' % atlas_version))
-            if atlas_version=='3.2.1_pre3.3.6':
-                atlas_info['define_macros'].append(('NO_ATLAS_INFO',4))
-            l = atlas_info.get('define_macros',[])
-            if ('ATLAS_WITH_LAPACK_ATLAS',None) in l \
-                   or ('ATLAS_WITHOUT_LAPACK',None) in l:
-                need_lapack = 1
-            info = atlas_info
-        else:
-            warnings.warn(AtlasNotFoundError.__doc__)
-            need_blas = 1
-            need_lapack = 1
-            dict_append(info,define_macros=[('NO_ATLAS_INFO',1)])
-
-        if need_lapack:
-            lapack_info = get_info('lapack')
-            #lapack_info = {} ## uncomment for testing
-            if lapack_info:
-                dict_append(info,**lapack_info)
-            else:
-                warnings.warn(LapackNotFoundError.__doc__)
-                lapack_src_info = get_info('lapack_src')
-                if not lapack_src_info:
-                    warnings.warn(LapackSrcNotFoundError.__doc__)
-                    return
-                dict_append(info,libraries=[('flapack_src',lapack_src_info)])
-
-        if need_blas:
-            blas_info = get_info('blas')
-            #blas_info = {} ## uncomment for testing
-            if blas_info:
-                dict_append(info,**blas_info)
-            else:
-                warnings.warn(BlasNotFoundError.__doc__)
-                blas_src_info = get_info('blas_src')
-                if not blas_src_info:
-                    warnings.warn(BlasSrcNotFoundError.__doc__)
-                    return
-                dict_append(info,libraries=[('fblas_src',blas_src_info)])
-
-        self.set_info(**info)
-        return
-
-
-class blas_opt_info(system_info):
-
-    def calc_info(self):
-
-        if sys.platform=='darwin' and not os.environ.get('ATLAS',None):
-            args = []
-            link_args = []
-            if os.path.exists('/System/Library/Frameworks/Accelerate.framework/'):
-                args.extend(['-faltivec',
-                    '-I/System/Library/Frameworks/vecLib.framework/Headers',
-                    ])
-                link_args.extend(['-Wl,-framework','-Wl,Accelerate'])
-            elif os.path.exists('/System/Library/Frameworks/vecLib.framework/'):
-                args.extend(['-faltivec',
-                    '-I/System/Library/Frameworks/vecLib.framework/Headers',
-                    ])
-                link_args.extend(['-Wl,-framework','-Wl,vecLib'])
-            if args:
-                self.set_info(extra_compile_args=args,
-                              extra_link_args=link_args,
-                              define_macros=[('NO_ATLAS_INFO',3)])
+        for blas in blas_order:
+            if self._calc_info(blas):
                 return
 
-        blas_mkl_info = get_info('blas_mkl')
-        if blas_mkl_info:
-            self.set_info(**blas_mkl_info)
-            return
-
-        atlas_info = get_info('atlas_blas_threads')
-        if not atlas_info:
-            atlas_info = get_info('atlas_blas')
-        atlas_version = None
-        need_blas = 0
-        info = {}
-        if atlas_info:
-            version_info = atlas_info.copy()
-            atlas_version = get_atlas_version(**version_info)
-            if not atlas_info.has_key('define_macros'):
-                atlas_info['define_macros'] = []
-            if atlas_version is None:
-                atlas_info['define_macros'].append(('NO_ATLAS_INFO',2))
-            else:
-                atlas_info['define_macros'].append(('ATLAS_INFO',
-                                                    '"\\"%s\\""' % atlas_version))
-            info = atlas_info
-        else:
-            warnings.warn(AtlasNotFoundError.__doc__)
-            need_blas = 1
-            dict_append(info,define_macros=[('NO_ATLAS_INFO',1)])
-
-        if need_blas:
-            blas_info = get_info('blas')
-            if blas_info:
-                dict_append(info,**blas_info)
-            else:
-                warnings.warn(BlasNotFoundError.__doc__)
-                blas_src_info = get_info('blas_src')
-                if not blas_src_info:
-                    warnings.warn(BlasSrcNotFoundError.__doc__)
-                    return
-                dict_append(info,libraries=[('fblas_src',blas_src_info)])
-
-        self.set_info(**info)
-        return
+        if 'blas' not in blas_order:
+            # Since the user may request *not* to use any library, we still need
+            # to raise warnings to signal missing packages!
+            warnings.warn(BlasNotFoundError.__doc__ or '', stacklevel=2)
+            warnings.warn(BlasSrcNotFoundError.__doc__ or '', stacklevel=2)
+
+
+class blas_ilp64_opt_info(blas_opt_info, _ilp64_opt_info_mixin):
+    notfounderror = BlasILP64NotFoundError
+    blas_order = ['openblas64_', 'openblas_ilp64']
+    order_env_var_name = 'NPY_BLAS_ILP64_ORDER'
+
+    def _calc_info(self, name):
+        info = get_info(name)
+        if self._check_info(info):
+            self.set_info(**info)
+            return True
+        return False
+
+
+class blas_ilp64_plain_opt_info(blas_ilp64_opt_info):
+    symbol_prefix = ''
+    symbol_suffix = ''
+
+
+class blas64__opt_info(blas_ilp64_opt_info):
+    symbol_prefix = ''
+    symbol_suffix = '64_'
+
+
+class cblas_info(system_info):
+    section = 'cblas'
+    dir_env_var = 'CBLAS'
+    # No default as it's used only in blas_info
+    _lib_names = []
+    notfounderror = BlasNotFoundError
 
 
 class blas_info(system_info):
@@ -1306,20 +2123,442 @@
 
     def calc_info(self):
         lib_dirs = self.get_lib_dirs()
-
-        blas_libs = self.get_libs('blas_libs', self._lib_names)
-        for d in lib_dirs:
-            blas = self.check_libs(d,blas_libs,[])
-            if blas is not None:
-                info = blas
-                break
+        opt = self.get_option_single('blas_libs', 'libraries')
+        blas_libs = self.get_libs(opt, self._lib_names)
+        info = self.check_libs(lib_dirs, blas_libs, [])
+        if info is None:
+            return
         else:
+            info['include_dirs'] = self.get_include_dirs()
+        if platform.system() == 'Windows':
+            # The check for windows is needed because get_cblas_libs uses the
+            # same compiler that was used to compile Python and msvc is
+            # often not installed when mingw is being used. This rough
+            # treatment is not desirable, but windows is tricky.
+            info['language'] = 'f77'  # XXX: is it generally true?
+            # If cblas is given as an option, use those
+            cblas_info_obj = cblas_info()
+            cblas_opt = cblas_info_obj.get_option_single('cblas_libs', 'libraries')
+            cblas_libs = cblas_info_obj.get_libs(cblas_opt, None)
+            if cblas_libs:
+                info['libraries'] = cblas_libs + blas_libs
+                info['define_macros'] = [('HAVE_CBLAS', None)]
+        else:
+            lib = self.get_cblas_libs(info)
+            if lib is not None:
+                info['language'] = 'c'
+                info['libraries'] = lib
+                info['define_macros'] = [('HAVE_CBLAS', None)]
+        self.set_info(**info)
+
+    def get_cblas_libs(self, info):
+        """ Check whether we can link with CBLAS interface
+
+        This method will search through several combinations of libraries
+        to check whether CBLAS is present:
+
+        1. Libraries in ``info['libraries']``, as is
+        2. As 1. but also explicitly adding ``'cblas'`` as a library
+        3. As 1. but also explicitly adding ``'blas'`` as a library
+        4. Check only library ``'cblas'``
+        5. Check only library ``'blas'``
+
+        Parameters
+        ----------
+        info : dict
+           system information dictionary for compilation and linking
+
+        Returns
+        -------
+        libraries : list of str or None
+            a list of libraries that enables the use of CBLAS interface.
+            Returns None if not found or a compilation error occurs.
+
+            Since 1.17 returns a list.
+        """
+        # primitive cblas check by looking for the header and trying to link
+        # cblas or blas
+        c = customized_ccompiler()
+        tmpdir = tempfile.mkdtemp()
+        s = textwrap.dedent("""\
+            #include <cblas.h>
+            int main(int argc, const char *argv[])
+            {
+                double a[4] = {1,2,3,4};
+                double b[4] = {5,6,7,8};
+                return cblas_ddot(4, a, 1, b, 1) > 10;
+            }""")
+        src = os.path.join(tmpdir, 'source.c')
+        try:
+            with open(src, 'wt') as f:
+                f.write(s)
+
+            try:
+                # check we can compile (find headers)
+                obj = c.compile([src], output_dir=tmpdir,
+                                include_dirs=self.get_include_dirs())
+            except (distutils.ccompiler.CompileError, distutils.ccompiler.LinkError):
+                return None
+
+            # check we can link (find library)
+            # some systems have separate cblas and blas libs.
+            for libs in [info['libraries'], ['cblas'] + info['libraries'],
+                         ['blas'] + info['libraries'], ['cblas'], ['blas']]:
+                try:
+                    c.link_executable(obj, os.path.join(tmpdir, "a.out"),
+                                      libraries=libs,
+                                      library_dirs=info['library_dirs'],
+                                      extra_postargs=info.get('extra_link_args', []))
+                    return libs
+                except distutils.ccompiler.LinkError:
+                    pass
+        finally:
+            shutil.rmtree(tmpdir)
+        return None
+
+
+class openblas_info(blas_info):
+    section = 'openblas'
+    dir_env_var = 'OPENBLAS'
+    _lib_names = ['openblas']
+    _require_symbols = []
+    notfounderror = BlasNotFoundError
+
+    @property
+    def symbol_prefix(self):
+        try:
+            return self.cp.get(self.section, 'symbol_prefix')
+        except NoOptionError:
+            return ''
+
+    @property
+    def symbol_suffix(self):
+        try:
+            return self.cp.get(self.section, 'symbol_suffix')
+        except NoOptionError:
+            return ''
+
+    def _calc_info(self):
+        c = customized_ccompiler()
+
+        lib_dirs = self.get_lib_dirs()
+
+        # Prefer to use libraries over openblas_libs
+        opt = self.get_option_single('openblas_libs', 'libraries')
+        openblas_libs = self.get_libs(opt, self._lib_names)
+
+        info = self.check_libs(lib_dirs, openblas_libs, [])
+
+        if c.compiler_type == "msvc" and info is None:
+            from numpy.distutils.fcompiler import new_fcompiler
+            f = new_fcompiler(c_compiler=c)
+            if f and f.compiler_type == 'gnu95':
+                # Try gfortran-compatible library files
+                info = self.check_msvc_gfortran_libs(lib_dirs, openblas_libs)
+                # Skip lapack check, we'd need build_ext to do it
+                skip_symbol_check = True
+        elif info:
+            skip_symbol_check = False
+            info['language'] = 'c'
+
+        if info is None:
+            return None
+
+        # Add extra info for OpenBLAS
+        extra_info = self.calc_extra_info()
+        dict_append(info, **extra_info)
+
+        if not (skip_symbol_check or self.check_symbols(info)):
+            return None
+
+        info['define_macros'] = [('HAVE_CBLAS', None)]
+        if self.symbol_prefix:
+            info['define_macros'] += [('BLAS_SYMBOL_PREFIX', self.symbol_prefix)]
+        if self.symbol_suffix:
+            info['define_macros'] += [('BLAS_SYMBOL_SUFFIX', self.symbol_suffix)]
+
+        return info
+
+    def calc_info(self):
+        info = self._calc_info()
+        if info is not None:
+            self.set_info(**info)
+
+    def check_msvc_gfortran_libs(self, library_dirs, libraries):
+        # First, find the full path to each library directory
+        library_paths = []
+        for library in libraries:
+            for library_dir in library_dirs:
+                # MinGW static ext will be .a
+                fullpath = os.path.join(library_dir, library + '.a')
+                if os.path.isfile(fullpath):
+                    library_paths.append(fullpath)
+                    break
+            else:
+                return None
+
+        # Generate numpy.distutils virtual static library file
+        basename = self.__class__.__name__
+        tmpdir = os.path.join(os.getcwd(), 'build', basename)
+        if not os.path.isdir(tmpdir):
+            os.makedirs(tmpdir)
+
+        info = {'library_dirs': [tmpdir],
+                'libraries': [basename],
+                'language': 'f77'}
+
+        fake_lib_file = os.path.join(tmpdir, basename + '.fobjects')
+        fake_clib_file = os.path.join(tmpdir, basename + '.cobjects')
+        with open(fake_lib_file, 'w') as f:
+            f.write("\n".join(library_paths))
+        with open(fake_clib_file, 'w') as f:
+            pass
+
+        return info
+
+    def check_symbols(self, info):
+        res = False
+        c = customized_ccompiler()
+
+        tmpdir = tempfile.mkdtemp()
+
+        prototypes = "\n".join("void %s%s%s();" % (self.symbol_prefix,
+                                                   symbol_name,
+                                                   self.symbol_suffix)
+                               for symbol_name in self._require_symbols)
+        calls = "\n".join("%s%s%s();" % (self.symbol_prefix,
+                                         symbol_name,
+                                         self.symbol_suffix)
+                          for symbol_name in self._require_symbols)
+        s = textwrap.dedent("""\
+            %(prototypes)s
+            int main(int argc, const char *argv[])
+            {
+                %(calls)s
+                return 0;
+            }""") % dict(prototypes=prototypes, calls=calls)
+        src = os.path.join(tmpdir, 'source.c')
+        out = os.path.join(tmpdir, 'a.out')
+        # Add the additional "extra" arguments
+        try:
+            extra_args = info['extra_link_args']
+        except Exception:
+            extra_args = []
+        try:
+            with open(src, 'wt') as f:
+                f.write(s)
+            obj = c.compile([src], output_dir=tmpdir)
+            try:
+                c.link_executable(obj, out, libraries=info['libraries'],
+                                  library_dirs=info['library_dirs'],
+                                  extra_postargs=extra_args)
+                res = True
+            except distutils.ccompiler.LinkError:
+                res = False
+        finally:
+            shutil.rmtree(tmpdir)
+        return res
+
+class openblas_lapack_info(openblas_info):
+    section = 'openblas'
+    dir_env_var = 'OPENBLAS'
+    _lib_names = ['openblas']
+    _require_symbols = ['zungqr_']
+    notfounderror = BlasNotFoundError
+
+class openblas_clapack_info(openblas_lapack_info):
+    _lib_names = ['openblas', 'lapack']
+
+class openblas_ilp64_info(openblas_info):
+    section = 'openblas_ilp64'
+    dir_env_var = 'OPENBLAS_ILP64'
+    _lib_names = ['openblas64']
+    _require_symbols = ['dgemm_', 'cblas_dgemm']
+    notfounderror = BlasILP64NotFoundError
+
+    def _calc_info(self):
+        info = super()._calc_info()
+        if info is not None:
+            info['define_macros'] += [('HAVE_BLAS_ILP64', None)]
+        return info
+
+class openblas_ilp64_lapack_info(openblas_ilp64_info):
+    _require_symbols = ['dgemm_', 'cblas_dgemm', 'zungqr_', 'LAPACKE_zungqr']
+
+    def _calc_info(self):
+        info = super()._calc_info()
+        if info:
+            info['define_macros'] += [('HAVE_LAPACKE', None)]
+        return info
+
+class openblas64__info(openblas_ilp64_info):
+    # ILP64 Openblas, with default symbol suffix
+    section = 'openblas64_'
+    dir_env_var = 'OPENBLAS64_'
+    _lib_names = ['openblas64_']
+    symbol_suffix = '64_'
+    symbol_prefix = ''
+
+class openblas64__lapack_info(openblas_ilp64_lapack_info, openblas64__info):
+    pass
+
+class blis_info(blas_info):
+    section = 'blis'
+    dir_env_var = 'BLIS'
+    _lib_names = ['blis']
+    notfounderror = BlasNotFoundError
+
+    def calc_info(self):
+        lib_dirs = self.get_lib_dirs()
+        opt = self.get_option_single('blis_libs', 'libraries')
+        blis_libs = self.get_libs(opt, self._lib_names)
+        info = self.check_libs2(lib_dirs, blis_libs, [])
+        if info is None:
             return
-        info['language'] = 'f77'  # XXX: is it generally true?
+
+        # Add include dirs
+        incl_dirs = self.get_include_dirs()
+        dict_append(info,
+                    language='c',
+                    define_macros=[('HAVE_CBLAS', None)],
+                    include_dirs=incl_dirs)
         self.set_info(**info)
 
 
+class flame_info(system_info):
+    """ Usage of libflame for LAPACK operations
+
+    This requires libflame to be compiled with lapack wrappers:
+
+    ./configure --enable-lapack2flame ...
+
+    Be aware that libflame 5.1.0 has some missing names in the shared library, so
+    if you have problems, try the static flame library.
+    """
+    section = 'flame'
+    _lib_names = ['flame']
+    notfounderror = FlameNotFoundError
+
+    def check_embedded_lapack(self, info):
+        """ libflame does not necessarily have a wrapper for fortran LAPACK, we need to check """
+        c = customized_ccompiler()
+
+        tmpdir = tempfile.mkdtemp()
+        s = textwrap.dedent("""\
+            void zungqr_();
+            int main(int argc, const char *argv[])
+            {
+                zungqr_();
+                return 0;
+            }""")
+        src = os.path.join(tmpdir, 'source.c')
+        out = os.path.join(tmpdir, 'a.out')
+        # Add the additional "extra" arguments
+        extra_args = info.get('extra_link_args', [])
+        try:
+            with open(src, 'wt') as f:
+                f.write(s)
+            obj = c.compile([src], output_dir=tmpdir)
+            try:
+                c.link_executable(obj, out, libraries=info['libraries'],
+                                  library_dirs=info['library_dirs'],
+                                  extra_postargs=extra_args)
+                return True
+            except distutils.ccompiler.LinkError:
+                return False
+        finally:
+            shutil.rmtree(tmpdir)
+
+    def calc_info(self):
+        lib_dirs = self.get_lib_dirs()
+        flame_libs = self.get_libs('libraries', self._lib_names)
+
+        info = self.check_libs2(lib_dirs, flame_libs, [])
+        if info is None:
+            return
+
+        # Add the extra flag args to info
+        extra_info = self.calc_extra_info()
+        dict_append(info, **extra_info)
+
+        if self.check_embedded_lapack(info):
+            # check if the user has supplied all information required
+            self.set_info(**info)
+        else:
+            # Try and get the BLAS lib to see if we can get it to work
+            blas_info = get_info('blas_opt')
+            if not blas_info:
+                # since we already failed once, this ain't going to work either
+                return
+
+            # Now we need to merge the two dictionaries
+            for key in blas_info:
+                if isinstance(blas_info[key], list):
+                    info[key] = info.get(key, []) + blas_info[key]
+                elif isinstance(blas_info[key], tuple):
+                    info[key] = info.get(key, ()) + blas_info[key]
+                else:
+                    info[key] = info.get(key, '') + blas_info[key]
+
+            # Now check again
+            if self.check_embedded_lapack(info):
+                self.set_info(**info)
+
+
+class accelerate_info(system_info):
+    section = 'accelerate'
+    _lib_names = ['accelerate', 'veclib']
+    notfounderror = BlasNotFoundError
+
+    def calc_info(self):
+        # Make possible to enable/disable from config file/env var
+        libraries = os.environ.get('ACCELERATE')
+        if libraries:
+            libraries = [libraries]
+        else:
+            libraries = self.get_libs('libraries', self._lib_names)
+        libraries = [lib.strip().lower() for lib in libraries]
+
+        if (sys.platform == 'darwin' and
+                not os.getenv('_PYTHON_HOST_PLATFORM', None)):
+            # Use the system BLAS from Accelerate or vecLib under OSX
+            args = []
+            link_args = []
+            if get_platform()[-4:] == 'i386' or 'intel' in get_platform() or \
+               'x86_64' in get_platform() or \
+               'i386' in platform.platform():
+                intel = 1
+            else:
+                intel = 0
+            if (os.path.exists('/System/Library/Frameworks'
+                              '/Accelerate.framework/') and
+                    'accelerate' in libraries):
+                if intel:
+                    args.extend(['-msse3'])
+                args.extend([
+                    '-I/System/Library/Frameworks/vecLib.framework/Headers'])
+                link_args.extend(['-Wl,-framework', '-Wl,Accelerate'])
+            elif (os.path.exists('/System/Library/Frameworks'
+                                 '/vecLib.framework/') and
+                      'veclib' in libraries):
+                if intel:
+                    args.extend(['-msse3'])
+                args.extend([
+                    '-I/System/Library/Frameworks/vecLib.framework/Headers'])
+                link_args.extend(['-Wl,-framework', '-Wl,vecLib'])
+
+            if args:
+                self.set_info(extra_compile_args=args,
+                              extra_link_args=link_args,
+                              define_macros=[('NO_ATLAS_INFO', 3),
+                                             ('HAVE_CBLAS', None)])
+
+        return
+
 class blas_src_info(system_info):
+    # BLAS_SRC is deprecated, please do not use this!
+    # Build or install a BLAS library via your package manager or from
+    # source separately.
     section = 'blas_src'
     dir_env_var = 'BLAS_SRC'
     notfounderror = BlasSrcNotFoundError
@@ -1328,14 +2567,14 @@
         pre_dirs = system_info.get_paths(self, section, key)
         dirs = []
         for d in pre_dirs:
-            dirs.extend([d] + self.combine_paths(d,['blas']))
-        return [ d for d in dirs if os.path.isdir(d) ]
+            dirs.extend([d] + self.combine_paths(d, ['blas']))
+        return [d for d in dirs if os.path.isdir(d)]
 
     def calc_info(self):
         src_dirs = self.get_src_dirs()
         src_dir = ''
         for d in src_dirs:
-            if os.path.isfile(os.path.join(d,'daxpy.f')):
+            if os.path.isfile(os.path.join(d, 'daxpy.f')):
                 src_dir = d
                 break
         if not src_dir:
@@ -1347,6 +2586,7 @@
         srotmg zdrot cdotu daxpy drotm idamax scopy sscal zdscal crotg
         dcabs1 drotmg isamax sdot sswap zrotg cscal dcopy dscal izamax
         snrm2 zaxpy zscal csrot ddot dswap sasum srot zcopy zswap
+        scabs1
         '''
         blas2 = '''
         cgbmv chpmv ctrsv dsymv dtrsv sspr2 strmv zhemv ztpmv cgemv
@@ -1362,15 +2602,18 @@
         dgemm dtrmm ssymm strsm zher2k zsyrk cher2k csyrk dsymm dtrsm
         ssyr2k zherk ztrmm cherk ctrmm dsyr2k ssyrk zgemm zsymm ztrsm
         '''
-        sources = [os.path.join(src_dir,f+'.f') \
-                   for f in (blas1+blas2+blas3).split()]
+        sources = [os.path.join(src_dir, f + '.f') \
+                   for f in (blas1 + blas2 + blas3).split()]
         #XXX: should we check here actual existence of source files?
-        info = {'sources':sources,'language':'f77'}
+        sources = [f for f in sources if os.path.isfile(f)]
+        info = {'sources': sources, 'language': 'f77'}
         self.set_info(**info)
+
 
 class x11_info(system_info):
     section = 'x11'
     notfounderror = X11NotFoundError
+    _lib_names = ['X11']
 
     def __init__(self):
         system_info.__init__(self,
@@ -1382,12 +2625,10 @@
             return
         lib_dirs = self.get_lib_dirs()
         include_dirs = self.get_include_dirs()
-        x11_libs = self.get_libs('x11_libs', ['X11'])
-        for lib_dir in lib_dirs:
-            info = self.check_libs(lib_dir, x11_libs, [])
-            if info is not None:
-                break
-        else:
+        opt = self.get_option_single('x11_libs', 'libraries')
+        x11_libs = self.get_libs(opt, self._lib_names)
+        info = self.check_libs(lib_dirs, x11_libs, [])
+        if info is None:
             return
         inc_dir = None
         for d in include_dirs:
@@ -1398,26 +2639,37 @@
             dict_append(info, include_dirs=[inc_dir])
         self.set_info(**info)
 
+
 class _numpy_info(system_info):
     section = 'Numeric'
     modulename = 'Numeric'
     notfounderror = NumericNotFoundError
 
     def __init__(self):
-        from distutils.sysconfig import get_python_inc
         include_dirs = []
         try:
             module = __import__(self.modulename)
             prefix = []
             for name in module.__file__.split(os.sep):
-                if name=='lib':
+                if name == 'lib':
                     break
                 prefix.append(name)
-            include_dirs.append(get_python_inc(prefix=os.sep.join(prefix)))
+
+            # Ask numpy for its own include path before attempting
+            # anything else
+            try:
+                include_dirs.append(getattr(module, 'get_include')())
+            except AttributeError:
+                pass
+
+            include_dirs.append(sysconfig.get_path('include'))
         except ImportError:
             pass
-        py_incl_dir = get_python_inc()
+        py_incl_dir = sysconfig.get_path('include')
         include_dirs.append(py_incl_dir)
+        py_pincl_dir = sysconfig.get_path('platinclude')
+        if py_pincl_dir not in include_dirs:
+            include_dirs.append(py_pincl_dir)
         for d in default_include_dirs:
             d = os.path.join(d, os.path.basename(py_incl_dir))
             if d not in include_dirs:
@@ -1433,22 +2685,15 @@
             return
         info = {}
         macros = []
-        for v in ['__version__','version']:
-            vrs = getattr(module,v,None)
+        for v in ['__version__', 'version']:
+            vrs = getattr(module, v, None)
             if vrs is None:
                 continue
-            macros = [(self.modulename.upper()+'_VERSION',
-                      '"\\"%s\\""' % (vrs)),
-                      (self.modulename.upper(),None)]
+            macros = [(self.modulename.upper() + '_VERSION',
+                      _c_string_literal(vrs)),
+                      (self.modulename.upper(), None)]
             break
-##         try:
-##             macros.append(
-##                 (self.modulename.upper()+'_VERSION_HEX',
-##                  hex(vstr2hex(module.__version__))),
-##                 )
-##         except Exception,msg:
-##             print msg
-        dict_append(info, define_macros = macros)
+        dict_append(info, define_macros=macros)
         include_dirs = self.get_include_dirs()
         inc_dir = None
         for d in include_dirs:
@@ -1463,22 +2708,26 @@
             self.set_info(**info)
         return
 
+
 class numarray_info(_numpy_info):
     section = 'numarray'
     modulename = 'numarray'
 
+
 class Numeric_info(_numpy_info):
     section = 'Numeric'
     modulename = 'Numeric'
 
+
 class numpy_info(_numpy_info):
     section = 'numpy'
     modulename = 'numpy'
 
+
 class numerix_info(system_info):
     section = 'numerix'
+
     def calc_info(self):
-        import sys, os
         which = None, None
         if os.getenv("NUMERIX"):
             which = os.getenv("NUMERIX"), "environment var"
@@ -1486,26 +2735,31 @@
         if which[0] is None:
             which = "numpy", "defaulted"
             try:
-                import numpy
+                import numpy  # noqa: F401
                 which = "numpy", "defaulted"
-            except ImportError,msg1:
+            except ImportError as e:
+                msg1 = str(e)
                 try:
-                    import Numeric
+                    import Numeric  # noqa: F401
                     which = "numeric", "defaulted"
-                except ImportError,msg2:
+                except ImportError as e:
+                    msg2 = str(e)
                     try:
-                        import numarray
+                        import numarray  # noqa: F401
                         which = "numarray", "defaulted"
-                    except ImportError,msg3:
-                        print msg1
-                        print msg2
-                        print msg3
+                    except ImportError as e:
+                        msg3 = str(e)
+                        log.info(msg1)
+                        log.info(msg2)
+                        log.info(msg3)
         which = which[0].strip().lower(), which[1]
         if which[0] not in ["numeric", "numarray", "numpy"]:
             raise ValueError("numerix selector must be either 'Numeric' "
                              "or 'numarray' or 'numpy' but the value obtained"
                              " from the %s was '%s'." % (which[1], which[0]))
+        os.environ['NUMERIX'] = which[0]
         self.set_info(**get_info(which[0]))
+
 
 class f2py_info(system_info):
     def calc_info(self):
@@ -1513,10 +2767,11 @@
             import numpy.f2py as f2py
         except ImportError:
             return
-        f2py_dir = os.path.join(os.path.dirname(f2py.__file__),'src')
-        self.set_info(sources = [os.path.join(f2py_dir,'fortranobject.c')],
-                      include_dirs = [f2py_dir])
+        f2py_dir = os.path.join(os.path.dirname(f2py.__file__), 'src')
+        self.set_info(sources=[os.path.join(f2py_dir, 'fortranobject.c')],
+                      include_dirs=[f2py_dir])
         return
+
 
 class boost_python_info(system_info):
     section = 'boost_python'
@@ -1526,31 +2781,37 @@
         pre_dirs = system_info.get_paths(self, section, key)
         dirs = []
         for d in pre_dirs:
-            dirs.extend([d] + self.combine_paths(d,['boost*']))
-        return [ d for d in dirs if os.path.isdir(d) ]
+            dirs.extend([d] + self.combine_paths(d, ['boost*']))
+        return [d for d in dirs if os.path.isdir(d)]
 
     def calc_info(self):
-        from distutils.sysconfig import get_python_inc
         src_dirs = self.get_src_dirs()
         src_dir = ''
         for d in src_dirs:
-            if os.path.isfile(os.path.join(d,'libs','python','src','module.cpp')):
+            if os.path.isfile(os.path.join(d, 'libs', 'python', 'src',
+                                           'module.cpp')):
                 src_dir = d
                 break
         if not src_dir:
             return
-        py_incl_dir = get_python_inc()
-        srcs_dir = os.path.join(src_dir,'libs','python','src')
-        bpl_srcs = glob(os.path.join(srcs_dir,'*.cpp'))
-        bpl_srcs += glob(os.path.join(srcs_dir,'*','*.cpp'))
-        info = {'libraries':[('boost_python_src',{'include_dirs':[src_dir,py_incl_dir],
-                                                  'sources':bpl_srcs})],
-                'include_dirs':[src_dir],
+        py_incl_dirs = [sysconfig.get_path('include')]
+        py_pincl_dir = sysconfig.get_path('platinclude')
+        if py_pincl_dir not in py_incl_dirs:
+            py_incl_dirs.append(py_pincl_dir)
+        srcs_dir = os.path.join(src_dir, 'libs', 'python', 'src')
+        bpl_srcs = glob(os.path.join(srcs_dir, '*.cpp'))
+        bpl_srcs += glob(os.path.join(srcs_dir, '*', '*.cpp'))
+        info = {'libraries': [('boost_python_src',
+                               {'include_dirs': [src_dir] + py_incl_dirs,
+                                'sources':bpl_srcs}
+                              )],
+                'include_dirs': [src_dir],
                 }
         if info:
             self.set_info(**info)
         return
 
+
 class agg2_info(system_info):
     section = 'agg2'
     dir_env_var = 'AGG2'
@@ -1559,32 +2820,39 @@
         pre_dirs = system_info.get_paths(self, section, key)
         dirs = []
         for d in pre_dirs:
-            dirs.extend([d] + self.combine_paths(d,['agg2*']))
-        return [ d for d in dirs if os.path.isdir(d) ]
+            dirs.extend([d] + self.combine_paths(d, ['agg2*']))
+        return [d for d in dirs if os.path.isdir(d)]
 
     def calc_info(self):
         src_dirs = self.get_src_dirs()
         src_dir = ''
         for d in src_dirs:
-            if os.path.isfile(os.path.join(d,'src','agg_affine_matrix.cpp')):
+            if os.path.isfile(os.path.join(d, 'src', 'agg_affine_matrix.cpp')):
                 src_dir = d
                 break
         if not src_dir:
             return
-        if sys.platform=='win32':
-            agg2_srcs = glob(os.path.join(src_dir,'src','platform','win32','agg_win32_bmp.cpp'))
+        if sys.platform == 'win32':
+            agg2_srcs = glob(os.path.join(src_dir, 'src', 'platform',
+                                          'win32', 'agg_win32_bmp.cpp'))
         else:
-            agg2_srcs = glob(os.path.join(src_dir,'src','*.cpp'))
-            agg2_srcs += [os.path.join(src_dir,'src','platform','X11','agg_platform_support.cpp')]
-
-        info = {'libraries':[('agg2_src',{'sources':agg2_srcs,
-                                          'include_dirs':[os.path.join(src_dir,'include')],
-                                          })],
-                'include_dirs':[os.path.join(src_dir,'include')],
+            agg2_srcs = glob(os.path.join(src_dir, 'src', '*.cpp'))
+            agg2_srcs += [os.path.join(src_dir, 'src', 'platform',
+                                       'X11',
+                                       'agg_platform_support.cpp')]
+
+        info = {'libraries':
+                [('agg2_src',
+                  {'sources': agg2_srcs,
+                   'include_dirs': [os.path.join(src_dir, 'include')],
+                  }
+                 )],
+                'include_dirs': [os.path.join(src_dir, 'include')],
                 }
         if info:
             self.set_info(**info)
         return
+
 
 class _pkg_config_info(system_info):
     section = None
@@ -1597,19 +2865,25 @@
     cflags_flag = '--cflags'
 
     def get_config_exe(self):
-        if os.environ.has_key(self.config_env_var):
+        if self.config_env_var in os.environ:
             return os.environ[self.config_env_var]
         return self.default_config_exe
+
     def get_config_output(self, config_exe, option):
-        s,o = exec_command(config_exe+' '+self.append_config_exe+' '+option,use_tee=0)
-        if not s:
+        cmd = config_exe + ' ' + self.append_config_exe + ' ' + option
+        try:
+            o = subprocess.check_output(cmd)
+        except (OSError, subprocess.CalledProcessError):
+            pass
+        else:
+            o = filepath_from_subprocess_output(o)
             return o
 
     def calc_info(self):
         config_exe = find_executable(self.get_config_exe())
-        if not os.path.isfile(config_exe):
-            print 'File not found: %s. Cannot determine %s info.' \
-                  % (config_exe, self.section)
+        if not config_exe:
+            log.warn('File not found: %s. Cannot determine %s info.' \
+                  % (config_exe, self.section))
             return
         info = {}
         macros = []
@@ -1618,47 +2892,56 @@
         include_dirs = []
         extra_link_args = []
         extra_compile_args = []
-        version = self.get_config_output(config_exe,self.version_flag)
+        version = self.get_config_output(config_exe, self.version_flag)
         if version:
             macros.append((self.__class__.__name__.split('.')[-1].upper(),
-                           '"\\"%s\\""' % (version)))
+                           _c_string_literal(version)))
             if self.version_macro_name:
-                macros.append((self.version_macro_name+'_%s' % (version.replace('.','_')),None))
+                macros.append((self.version_macro_name + '_%s'
+                               % (version.replace('.', '_')), None))
         if self.release_macro_name:
-            release = self.get_config_output(config_exe,'--release')
+            release = self.get_config_output(config_exe, '--release')
             if release:
-                macros.append((self.release_macro_name+'_%s' % (release.replace('.','_')),None))
-        opts = self.get_config_output(config_exe,'--libs')
+                macros.append((self.release_macro_name + '_%s'
+                               % (release.replace('.', '_')), None))
+        opts = self.get_config_output(config_exe, '--libs')
         if opts:
             for opt in opts.split():
-                if opt[:2]=='-l':
+                if opt[:2] == '-l':
                     libraries.append(opt[2:])
-                elif opt[:2]=='-L':
+                elif opt[:2] == '-L':
                     library_dirs.append(opt[2:])
                 else:
                     extra_link_args.append(opt)
-        opts = self.get_config_output(config_exe,self.cflags_flag)
+        opts = self.get_config_output(config_exe, self.cflags_flag)
         if opts:
             for opt in opts.split():
-                if opt[:2]=='-I':
+                if opt[:2] == '-I':
                     include_dirs.append(opt[2:])
-                elif opt[:2]=='-D':
+                elif opt[:2] == '-D':
                     if '=' in opt:
-                        n,v = opt[2:].split('=')
-                        macros.append((n,v))
+                        n, v = opt[2:].split('=')
+                        macros.append((n, v))
                     else:
-                        macros.append((opt[2:],None))
+                        macros.append((opt[2:], None))
                 else:
                     extra_compile_args.append(opt)
-        if macros: dict_append(info, define_macros = macros)
-        if libraries: dict_append(info, libraries = libraries)
-        if library_dirs: dict_append(info, library_dirs = library_dirs)
-        if include_dirs: dict_append(info, include_dirs = include_dirs)
-        if extra_link_args: dict_append(info, extra_link_args = extra_link_args)
-        if extra_compile_args: dict_append(info, extra_compile_args = extra_compile_args)
+        if macros:
+            dict_append(info, define_macros=macros)
+        if libraries:
+            dict_append(info, libraries=libraries)
+        if library_dirs:
+            dict_append(info, library_dirs=library_dirs)
+        if include_dirs:
+            dict_append(info, include_dirs=include_dirs)
+        if extra_link_args:
+            dict_append(info, extra_link_args=extra_link_args)
+        if extra_compile_args:
+            dict_append(info, extra_compile_args=extra_compile_args)
         if info:
             self.set_info(**info)
         return
+
 
 class wx_info(_pkg_config_info):
     section = 'wx'
@@ -1670,31 +2953,37 @@
     version_flag = '--version'
     cflags_flag = '--cxxflags'
 
+
 class gdk_pixbuf_xlib_2_info(_pkg_config_info):
     section = 'gdk_pixbuf_xlib_2'
     append_config_exe = 'gdk-pixbuf-xlib-2.0'
     version_macro_name = 'GDK_PIXBUF_XLIB_VERSION'
 
+
 class gdk_pixbuf_2_info(_pkg_config_info):
     section = 'gdk_pixbuf_2'
     append_config_exe = 'gdk-pixbuf-2.0'
     version_macro_name = 'GDK_PIXBUF_VERSION'
 
+
 class gdk_x11_2_info(_pkg_config_info):
     section = 'gdk_x11_2'
     append_config_exe = 'gdk-x11-2.0'
     version_macro_name = 'GDK_X11_VERSION'
 
+
 class gdk_2_info(_pkg_config_info):
     section = 'gdk_2'
     append_config_exe = 'gdk-2.0'
     version_macro_name = 'GDK_VERSION'
 
+
 class gdk_info(_pkg_config_info):
     section = 'gdk'
     append_config_exe = 'gdk'
     version_macro_name = 'GDK_VERSION'
 
+
 class gtkp_x11_2_info(_pkg_config_info):
     section = 'gtkp_x11_2'
     append_config_exe = 'gtk+-x11-2.0'
@@ -1706,16 +2995,19 @@
     append_config_exe = 'gtk+-2.0'
     version_macro_name = 'GTK_VERSION'
 
+
 class xft_info(_pkg_config_info):
     section = 'xft'
     append_config_exe = 'xft'
     version_macro_name = 'XFT_VERSION'
 
+
 class freetype2_info(_pkg_config_info):
     section = 'freetype2'
     append_config_exe = 'freetype2'
     version_macro_name = 'FREETYPE2_VERSION'
 
+
 class amd_info(system_info):
     section = 'amd'
     dir_env_var = 'AMD'
@@ -1724,30 +3016,28 @@
     def calc_info(self):
         lib_dirs = self.get_lib_dirs()
 
-        amd_libs = self.get_libs('amd_libs', self._lib_names)
-        for d in lib_dirs:
-            amd = self.check_libs(d,amd_libs,[])
-            if amd is not None:
-                info = amd
-                break
-        else:
+        opt = self.get_option_single('amd_libs', 'libraries')
+        amd_libs = self.get_libs(opt, self._lib_names)
+        info = self.check_libs(lib_dirs, amd_libs, [])
+        if info is None:
             return
 
         include_dirs = self.get_include_dirs()
 
         inc_dir = None
         for d in include_dirs:
-            p = self.combine_paths(d,'amd.h')
+            p = self.combine_paths(d, 'amd.h')
             if p:
                 inc_dir = os.path.dirname(p[0])
                 break
         if inc_dir is not None:
             dict_append(info, include_dirs=[inc_dir],
-                        define_macros=[('SCIPY_AMD_H',None)],
-                        swig_opts = ['-I' + inc_dir])
+                        define_macros=[('SCIPY_AMD_H', None)],
+                        swig_opts=['-I' + inc_dir])
 
         self.set_info(**info)
         return
+
 
 class umfpack_info(system_info):
     section = 'umfpack'
@@ -1758,110 +3048,125 @@
     def calc_info(self):
         lib_dirs = self.get_lib_dirs()
 
-        umfpack_libs = self.get_libs('umfpack_libs', self._lib_names)
-        for d in lib_dirs:
-            umf = self.check_libs(d,umfpack_libs,[])
-            if umf is not None:
-                info = umf
-                break
-        else:
+        opt = self.get_option_single('umfpack_libs', 'libraries')
+        umfpack_libs = self.get_libs(opt, self._lib_names)
+        info = self.check_libs(lib_dirs, umfpack_libs, [])
+        if info is None:
             return
 
         include_dirs = self.get_include_dirs()
 
         inc_dir = None
         for d in include_dirs:
-            p = self.combine_paths(d,['','umfpack'],'umfpack.h')
+            p = self.combine_paths(d, ['', 'umfpack'], 'umfpack.h')
             if p:
                 inc_dir = os.path.dirname(p[0])
                 break
         if inc_dir is not None:
             dict_append(info, include_dirs=[inc_dir],
-                        define_macros=[('SCIPY_UMFPACK_H',None)],
-                        swig_opts = ['-I' + inc_dir])
-
-        amd = get_info('amd')
+                        define_macros=[('SCIPY_UMFPACK_H', None)],
+                        swig_opts=['-I' + inc_dir])
+
         dict_append(info, **get_info('amd'))
 
         self.set_info(**info)
         return
 
-## def vstr2hex(version):
-##     bits = []
-##     n = [24,16,8,4,0]
-##     r = 0
-##     for s in version.split('.'):
-##         r |= int(s) << n[0]
-##         del n[0]
-##     return r
-
-#--------------------------------------------------------------------
-
-def combine_paths(*args,**kws):
+
+def combine_paths(*args, **kws):
     """ Return a list of existing paths composed by all combinations of
         items from arguments.
     """
     r = []
     for a in args:
-        if not a: continue
+        if not a:
+            continue
         if is_string(a):
             a = [a]
         r.append(a)
     args = r
-    if not args: return []
-    if len(args)==1:
-        result = reduce(lambda a,b:a+b,map(glob,args[0]),[])
-    elif len (args)==2:
+    if not args:
+        return []
+    if len(args) == 1:
+        result = reduce(lambda a, b: a + b, map(glob, args[0]), [])
+    elif len(args) == 2:
         result = []
         for a0 in args[0]:
             for a1 in args[1]:
-                result.extend(glob(os.path.join(a0,a1)))
+                result.extend(glob(os.path.join(a0, a1)))
     else:
-        result = combine_paths(*(combine_paths(args[0],args[1])+args[2:]))
-    verbosity = kws.get('verbosity',1)
-    if verbosity>1 and result:
-        print '(','paths:',','.join(result),')'
+        result = combine_paths(*(combine_paths(args[0], args[1]) + args[2:]))
+    log.debug('(paths: %s)', ','.join(result))
     return result
 
-language_map = {'c':0,'c++':1,'f77':2,'f90':3}
-inv_language_map = {0:'c',1:'c++',2:'f77',3:'f90'}
-def dict_append(d,**kws):
+language_map = {'c': 0, 'c++': 1, 'f77': 2, 'f90': 3}
+inv_language_map = {0: 'c', 1: 'c++', 2: 'f77', 3: 'f90'}
+
+
+def dict_append(d, **kws):
     languages = []
-    for k,v in kws.items():
-        if k=='language':
+    for k, v in kws.items():
+        if k == 'language':
             languages.append(v)
             continue
-        if d.has_key(k):
-            if k in ['library_dirs','include_dirs','define_macros']:
+        if k in d:
+            if k in ['library_dirs', 'include_dirs',
+                     'extra_compile_args', 'extra_link_args',
+                     'runtime_library_dirs', 'define_macros']:
                 [d[k].append(vv) for vv in v if vv not in d[k]]
             else:
                 d[k].extend(v)
         else:
             d[k] = v
     if languages:
-        l = inv_language_map[max([language_map.get(l,0) for l in languages])]
+        l = inv_language_map[max([language_map.get(l, 0) for l in languages])]
         d['language'] = l
     return
 
-def show_all():
-    import system_info
-    import pprint
-    match_info = re.compile(r'.*?_info').match
+
+def parseCmdLine(argv=(None,)):
+    import optparse
+    parser = optparse.OptionParser("usage: %prog [-v] [info objs]")
+    parser.add_option('-v', '--verbose', action='store_true', dest='verbose',
+                      default=False,
+                      help='be verbose and print more messages')
+
+    opts, args = parser.parse_args(args=argv[1:])
+    return opts, args
+
+
+def show_all(argv=None):
+    import inspect
+    if argv is None:
+        argv = sys.argv
+    opts, args = parseCmdLine(argv)
+    if opts.verbose:
+        log.set_threshold(log.DEBUG)
+    else:
+        log.set_threshold(log.INFO)
     show_only = []
-    for n in sys.argv[1:]:
+    for n in args:
         if n[-5:] != '_info':
             n = n + '_info'
         show_only.append(n)
     show_all = not show_only
-    for n in filter(match_info,dir(system_info)):
-        if n in ['system_info','get_info']: continue
+    _gdict_ = globals().copy()
+    for name, c in _gdict_.items():
+        if not inspect.isclass(c):
+            continue
+        if not issubclass(c, system_info) or c is system_info:
+            continue
         if not show_all:
-            if n not in show_only: continue
-            del show_only[show_only.index(n)]
-        c = getattr(system_info,n)()
-        c.verbosity = 2
-        r = c.get_info()
+            if name not in show_only:
+                continue
+            del show_only[show_only.index(name)]
+        conf = c()
+        conf.verbosity = 2
+        # we don't need the result, but we want
+        # the side effect of printing diagnostics
+        conf.get_info()
     if show_only:
-        print 'Info classes not defined:',','.join(show_only)
+        log.info('Info classes not defined: %s', ','.join(show_only))
+
 if __name__ == "__main__":
     show_all()
('numpy/distutils', '__init__.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,19 +1,64 @@
+"""
+An enhanced distutils, providing support for Fortran compilers, for BLAS,
+LAPACK and other common libraries for numerical computing, and more.
 
-from __version__ import version as __version__
+Public submodules are::
+
+    misc_util
+    system_info
+    cpu_info
+    log
+    exec_command
+
+For details, please see the *Packaging* and *NumPy Distutils User Guide*
+sections of the NumPy Reference Guide.
+
+For configuring the preference for and location of libraries like BLAS and
+LAPACK, and for setting include paths and similar build options, please see
+``site.cfg.example`` in the root of the NumPy repository or sdist.
+
+"""
+
+import warnings
+
 # Must import local ccompiler ASAP in order to get
 # customized CCompiler.spawn effective.
-import ccompiler
-import unixccompiler
+from . import ccompiler
+from . import unixccompiler
 
-from info import __doc__
+from .npy_pkg_config import *
 
+warnings.warn("\n\n"
+    "  `numpy.distutils` is deprecated since NumPy 1.23.0, as a result\n"
+    "  of the deprecation of `distutils` itself. It will be removed for\n"
+    "  Python >= 3.12. For older Python versions it will remain present.\n"
+    "  It is recommended to use `setuptools < 60.0` for those Python versions.\n"
+    "  For more details, see:\n"
+    "    https://numpy.org/devdocs/reference/distutils_status_migration.html \n\n",
+    DeprecationWarning, stacklevel=2
+)
+del warnings
+
+# If numpy is installed, add distutils.test()
 try:
-    import __config__
-    _INSTALLED = True
+    from . import __config__
+    # Normally numpy is installed if the above import works, but an interrupted
+    # in-place build could also have left a __config__.py.  In that case the
+    # next import may still fail, so keep it inside the try block.
+    from numpy._pytesttester import PytestTester
+    test = PytestTester(__name__)
+    del PytestTester
 except ImportError:
-    _INSTALLED = False
+    pass
 
-if _INSTALLED:
-    def test(level=1, verbosity=1):
-        from numpy.testing import NumpyTest
-        return NumpyTest().test(level, verbosity)
+
+def customized_fcompiler(plat=None, compiler=None):
+    from numpy.distutils.fcompiler import new_fcompiler
+    c = new_fcompiler(plat=plat, compiler=compiler)
+    c.customize()
+    return c
+
+def customized_ccompiler(plat=None, compiler=None, verbose=1):
+    c = ccompiler.new_compiler(plat=plat, compiler=compiler, verbose=verbose)
+    c.customize('')
+    return c
('numpy/distutils', 'core.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,36 +1,36 @@
-
 import sys
-from distutils.core import *
-try:
+from distutils.core import Distribution
+
+if 'setuptools' in sys.modules:
+    have_setuptools = True
     from setuptools import setup as old_setup
-    # very old setuptools don't have this
-    from setuptools.command import bdist_egg
     # easy_install imports math, it may be picked up from cwd
-    from setuptools.command import develop, easy_install
-    have_setuptools = 1
-except ImportError:
+    from setuptools.command import easy_install
+    try:
+        # very old versions of setuptools don't have this
+        from setuptools.command import bdist_egg
+    except ImportError:
+        have_setuptools = False
+else:
     from distutils.core import setup as old_setup
-    have_setuptools = 0
-
-from numpy.distutils.extension import Extension
-from numpy.distutils.command import config
-from numpy.distutils.command import build
-from numpy.distutils.command import build_py
-from numpy.distutils.command import config_compiler
-from numpy.distutils.command import build_ext
-from numpy.distutils.command import build_clib
-from numpy.distutils.command import build_src
-from numpy.distutils.command import build_scripts
-from numpy.distutils.command import sdist
-from numpy.distutils.command import install_data
-from numpy.distutils.command import install_headers
-from numpy.distutils.command import install
-from numpy.distutils.command import bdist_rpm
-from numpy.distutils.misc_util import get_data_files, is_sequence, is_string
+    have_setuptools = False
+
+import warnings
+import distutils.core
+import distutils.dist
+
+from numpy.distutils.extension import Extension  # noqa: F401
+from numpy.distutils.numpy_distribution import NumpyDistribution
+from numpy.distutils.command import config, config_compiler, \
+     build, build_py, build_ext, build_clib, build_src, build_scripts, \
+     sdist, install_data, install_headers, install, bdist_rpm, \
+     install_clib
+from numpy.distutils.misc_util import is_sequence, is_string
 
 numpy_cmdclass = {'build':            build.build,
                   'build_src':        build_src.build_src,
                   'build_scripts':    build_scripts.build_scripts,
+                  'config_cc':        config_compiler.config_cc,
                   'config_fc':        config_compiler.config_fc,
                   'config':           config.config,
                   'build_ext':        build_ext.build_ext,
@@ -39,44 +39,44 @@
                   'sdist':            sdist.sdist,
                   'install_data':     install_data.install_data,
                   'install_headers':  install_headers.install_headers,
+                  'install_clib':     install_clib.install_clib,
                   'install':          install.install,
                   'bdist_rpm':        bdist_rpm.bdist_rpm,
                   }
 if have_setuptools:
-    from numpy.distutils.command import egg_info
+    # Use our own versions of develop and egg_info to ensure that build_src is
+    # handled appropriately.
+    from numpy.distutils.command import develop, egg_info
     numpy_cmdclass['bdist_egg'] = bdist_egg.bdist_egg
     numpy_cmdclass['develop'] = develop.develop
     numpy_cmdclass['easy_install'] = easy_install.easy_install
     numpy_cmdclass['egg_info'] = egg_info.egg_info
 
 def _dict_append(d, **kws):
-    for k,v in kws.items():
-        if not d.has_key(k):
+    for k, v in kws.items():
+        if k not in d:
             d[k] = v
             continue
         dv = d[k]
         if isinstance(dv, tuple):
-            dv += tuple(v)
-            continue
-        if isinstance(dv, list):
-            dv += list(v)
-            continue
-        if isinstance(dv, dict):
+            d[k] = dv + tuple(v)
+        elif isinstance(dv, list):
+            d[k] = dv + list(v)
+        elif isinstance(dv, dict):
             _dict_append(dv, **v)
-            continue
-        if isinstance(dv, str):
-            assert isinstance(v,str),`type(v)`
-            d[k] = v
-            continue
-        raise TypeError,`type(dv)`
-    return
-
-def _command_line_ok(_cache=[]):
+        elif is_string(dv):
+            d[k] = dv + v
+        else:
+            raise TypeError(repr(type(dv)))
+
+def _command_line_ok(_cache=None):
     """ Return True if command line does not contain any
     help or display requests.
     """
     if _cache:
         return _cache[0]
+    elif _cache is None:
+        _cache = []
     ok = True
     display_opts = ['--'+n for n in Distribution.display_option_names]
     for o in Distribution.display_options:
@@ -89,66 +89,62 @@
     _cache.append(ok)
     return ok
 
-def _exit_interactive_session(_cache=[]):
-    if _cache:
-        return # been here
-    _cache.append(1)
-    print '-'*72
-    raw_input('Press ENTER to close the interactive session..')
-    print '='*72
+def get_distribution(always=False):
+    dist = distutils.core._setup_distribution
+    # XXX Hack to get numpy installable with easy_install.
+    # The problem is easy_install runs it's own setup(), which
+    # sets up distutils.core._setup_distribution. However,
+    # when our setup() runs, that gets overwritten and lost.
+    # We can't use isinstance, as the DistributionWithoutHelpCommands
+    # class is local to a function in setuptools.command.easy_install
+    if dist is not None and \
+            'DistributionWithoutHelpCommands' in repr(dist):
+        dist = None
+    if always and dist is None:
+        dist = NumpyDistribution()
+    return dist
 
 def setup(**attr):
 
-    if len(sys.argv)<=1:
-        from interactive import interactive_sys_argv
-        import atexit
-        atexit.register(_exit_interactive_session)
-        sys.argv[:] = interactive_sys_argv(sys.argv)
-        if len(sys.argv)>1:
-            return setup(**attr)
-
     cmdclass = numpy_cmdclass.copy()
 
     new_attr = attr.copy()
-    if new_attr.has_key('cmdclass'):
+    if 'cmdclass' in new_attr:
         cmdclass.update(new_attr['cmdclass'])
     new_attr['cmdclass'] = cmdclass
 
-    if new_attr.has_key('configuration'):
+    if 'configuration' in new_attr:
         # To avoid calling configuration if there are any errors
         # or help request in command in the line.
         configuration = new_attr.pop('configuration')
 
-        import distutils.core
         old_dist = distutils.core._setup_distribution
         old_stop = distutils.core._setup_stop_after
         distutils.core._setup_distribution = None
         distutils.core._setup_stop_after = "commandline"
         try:
             dist = setup(**new_attr)
+        finally:
             distutils.core._setup_distribution = old_dist
             distutils.core._setup_stop_after = old_stop
-        except Exception,msg:
-            distutils.core._setup_distribution = old_dist
-            distutils.core._setup_stop_after = old_stop
-            raise msg
         if dist.help or not _command_line_ok():
             # probably displayed help, skip running any commands
             return dist
 
         # create setup dictionary and append to new_attr
         config = configuration()
-        if hasattr(config,'todict'): config = config.todict()
+        if hasattr(config, 'todict'):
+            config = config.todict()
         _dict_append(new_attr, **config)
 
     # Move extension source libraries to libraries
     libraries = []
-    for ext in new_attr.get('ext_modules',[]):
+    for ext in new_attr.get('ext_modules', []):
         new_libraries = []
         for item in ext.libraries:
             if is_sequence(item):
                 lib_name, build_info = item
-                _check_append_ext_library(libraries, item)
+                _check_append_ext_library(libraries, lib_name, build_info)
                 new_libraries.append(lib_name)
             elif is_string(item):
                 new_libraries.append(item)
@@ -157,20 +153,22 @@
                                 "library %r" % (item,))
         ext.libraries = new_libraries
     if libraries:
-        if not new_attr.has_key('libraries'):
+        if 'libraries' not in new_attr:
             new_attr['libraries'] = []
         for item in libraries:
             _check_append_library(new_attr['libraries'], item)
 
     # sources in ext_modules or libraries may contain header files
-    if (new_attr.has_key('ext_modules') or new_attr.has_key('libraries')) \
-       and not new_attr.has_key('headers'):
+    if ('ext_modules' in new_attr or 'libraries' in new_attr) \
+       and 'headers' not in new_attr:
         new_attr['headers'] = []
 
+    # Use our custom NumpyDistribution class instead of distutils' one
+    new_attr['distclass'] = NumpyDistribution
+
     return old_setup(**new_attr)
 
 def _check_append_library(libraries, item):
-    import warnings
     for libitem in libraries:
         if is_sequence(libitem):
             if is_sequence(item):
@@ -178,38 +176,40 @@
                     if item[1] is libitem[1]:
                         return
                     warnings.warn("[0] libraries list contains %r with"
-                                  " different build_info" % (item[0],))
+                                  " different build_info" % (item[0],),
+                                  stacklevel=2)
                     break
             else:
                 if item==libitem[0]:
                     warnings.warn("[1] libraries list contains %r with"
-                                  " no build_info" % (item[0],))
+                                  " no build_info" % (item[0],),
+                                  stacklevel=2)
                     break
         else:
             if is_sequence(item):
                 if item[0]==libitem:
                     warnings.warn("[2] libraries list contains %r with"
-                                  " no build_info" % (item[0],))
+                                  " no build_info" % (item[0],),
+                                  stacklevel=2)
                     break
             else:
                 if item==libitem:
                     return
     libraries.append(item)
-    return
-
-def _check_append_ext_library(libraries, (lib_name,build_info)):
-    import warnings
+
+def _check_append_ext_library(libraries, lib_name, build_info):
     for item in libraries:
         if is_sequence(item):
             if item[0]==lib_name:
                 if item[1] is build_info:
                     return
                 warnings.warn("[3] libraries list contains %r with"
-                              " different build_info" % (lib_name,))
+                              " different build_info" % (lib_name,),
+                              stacklevel=2)
                 break
         elif item==lib_name:
             warnings.warn("[4] libraries list contains %r with"
-                          " no build_info" % (lib_name,))
+                          " no build_info" % (lib_name,),
+                          stacklevel=2)
             break
-    libraries.append((lib_name,build_info))
-    return
+    libraries.append((lib_name, build_info))
('numpy/distutils', 'exec_command.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,4 +1,3 @@
-#!/usr/bin/env python
 """
 exec_command
 
@@ -9,9 +8,9 @@
 takes keyword arguments for (re-)defining environment variables.
 
 Provides functions:
+
   exec_command  --- execute command in a specified directory and
                     in the modified environment.
-  splitcmdline  --- inverse of ' '.join(argv)
   find_executable --- locate a command using info from environment
                     variable PATH. Equivalent to posix `which`
                     command.
@@ -21,193 +20,195 @@
 
 Requires: Python 2.x
 
-Succesfully tested on:
-  os.name | sys.platform | comments
-  --------+--------------+----------
-  posix   | linux2       | Debian (sid) Linux, Python 2.1.3+, 2.2.3+, 2.3.3
-                           PyCrust 0.9.3, Idle 1.0.2
-  posix   | linux2       | Red Hat 9 Linux, Python 2.1.3, 2.2.2, 2.3.2
-  posix   | sunos5       | SunOS 5.9, Python 2.2, 2.3.2
-  posix   | darwin       | Darwin 7.2.0, Python 2.3
-  nt      | win32        | Windows Me
-                           Python 2.3(EE), Idle 1.0, PyCrust 0.7.2
-                           Python 2.1.1 Idle 0.8
-  nt      | win32        | Windows 98, Python 2.1.1. Idle 0.8
-  nt      | win32        | Cygwin 98-4.10, Python 2.1.1(MSC) - echo tests
-                           fail i.e. redefining environment variables may
-                           not work. FIXED: don't use cygwin echo!
-                           Comment: also `cmd /c echo` will not work
-                           but redefining environment variables do work.
-  posix   | cygwin       | Cygwin 98-4.10, Python 2.3.3(cygming special)
-  nt      | win32        | Windows XP, Python 2.3.3
+Successfully tested on:
+
+========  ============  =================================================
+os.name   sys.platform  comments
+========  ============  =================================================
+posix     linux2        Debian (sid) Linux, Python 2.1.3+, 2.2.3+, 2.3.3
+                        PyCrust 0.9.3, Idle 1.0.2
+posix     linux2        Red Hat 9 Linux, Python 2.1.3, 2.2.2, 2.3.2
+posix     sunos5        SunOS 5.9, Python 2.2, 2.3.2
+posix     darwin        Darwin 7.2.0, Python 2.3
+nt        win32         Windows Me
+                        Python 2.3(EE), Idle 1.0, PyCrust 0.7.2
+                        Python 2.1.1 Idle 0.8
+nt        win32         Windows 98, Python 2.1.1. Idle 0.8
+nt        win32         Cygwin 98-4.10, Python 2.1.1(MSC) - echo tests
+                        fail i.e. redefining environment variables may
+                        not work. FIXED: don't use cygwin echo!
+                        Comment: also `cmd /c echo` will not work
+                        but redefining environment variables do work.
+posix     cygwin        Cygwin 98-4.10, Python 2.3.3(cygming special)
+nt        win32         Windows XP, Python 2.3.3
+========  ============  =================================================
 
 Known bugs:
-- Tests, that send messages to stderr, fail when executed from MSYS prompt
+
+* Tests, that send messages to stderr, fail when executed from MSYS prompt
   because the messages are lost at some point.
+
 """
-
-__all__ = ['exec_command','find_executable']
+__all__ = ['exec_command', 'find_executable']
 
 import os
-import re
 import sys
-import tempfile
-
-from numpy.distutils.misc_util import is_sequence
-
-############################################################
-
-from log import _global_log as log
-
-############################################################
+import subprocess
+import locale
+import warnings
+
+from numpy.distutils.misc_util import is_sequence, make_temp_file
+from numpy.distutils import log
+
+def filepath_from_subprocess_output(output):
+    """
+    Convert `bytes` in the encoding used by a subprocess into a filesystem-appropriate `str`.
+
+    Inherited from `exec_command`, and possibly incorrect.
+    """
+    mylocale = locale.getpreferredencoding(False)
+    if mylocale is None:
+        mylocale = 'ascii'
+    output = output.decode(mylocale, errors='replace')
+    output = output.replace('\r\n', '\n')
+    # Another historical oddity
+    if output[-1:] == '\n':
+        output = output[:-1]
+    return output
+
+
+def forward_bytes_to_stdout(val):
+    """
+    Forward bytes from a subprocess call to the console, without attempting to
+    decode them.
+
+    The assumption is that the subprocess call already returned bytes in
+    a suitable encoding.
+    """
+    if hasattr(sys.stdout, 'buffer'):
+        # use the underlying binary output if there is one
+        sys.stdout.buffer.write(val)
+    elif hasattr(sys.stdout, 'encoding'):
+        # round-trip the encoding if necessary
+        sys.stdout.write(val.decode(sys.stdout.encoding))
+    else:
+        # make a best-guess at the encoding
+        sys.stdout.write(val.decode('utf8', errors='replace'))
+
+
+def temp_file_name():
+    # 2019-01-30, 1.17
+    warnings.warn('temp_file_name is deprecated since NumPy v1.17, use '
+                  'tempfile.mkstemp instead', DeprecationWarning, stacklevel=1)
+    fo, name = make_temp_file()
+    fo.close()
+    return name
 
 def get_pythonexe():
     pythonexe = sys.executable
-    if os.name in ['nt','dos']:
-        fdir,fn = os.path.split(pythonexe)
-        fn = fn.upper().replace('PYTHONW','PYTHON')
-        pythonexe = os.path.join(fdir,fn)
+    if os.name in ['nt', 'dos']:
+        fdir, fn = os.path.split(pythonexe)
+        fn = fn.upper().replace('PYTHONW', 'PYTHON')
+        pythonexe = os.path.join(fdir, fn)
         assert os.path.isfile(pythonexe), '%r is not a file' % (pythonexe,)
     return pythonexe
 
-############################################################
-
-def splitcmdline(line):
-    """ Inverse of ' '.join(sys.argv).
-    """
-    log.debug('splitcmdline(%r)' % (line))
-    lst = []
-    flag = 0
-    s,pc,cc = '','',''
-    for nc in line+' ':
-        if flag==0:
-            flag = (pc != '\\' and \
-                     ((cc=='"' and 1) or (cc=="'" and 2) or \
-                       (cc==' ' and pc!=' ' and -2))) or flag
-        elif flag==1:
-            flag = (cc=='"' and pc!='\\' and nc==' ' and -1) or flag
-        elif flag==2:
-            flag = (cc=="'" and pc!='\\' and nc==' ' and -1) or flag
-        if flag!=-2:
-            s += cc
-        if flag<0:
-            flag = 0
-            s = s.strip()
-            if s:
-                lst.append(s)
-                s = ''
-        pc,cc = cc,nc
-    else:
-        s = s.strip()
-        if s:
-            lst.append(s)
-    log.debug('splitcmdline -> %r' % (lst))
-    return lst
-
-def test_splitcmdline():
-    l = splitcmdline('a   b  cc')
-    assert l==['a','b','cc'], repr(l)
-    l = splitcmdline('a')
-    assert l==['a'], repr(l)
-    l = splitcmdline('a "  b  cc"')
-    assert l==['a','"  b  cc"'], repr(l)
-    l = splitcmdline('"a bcc"  -h')
-    assert l==['"a bcc"','-h'], repr(l)
-    l = splitcmdline(r'"\"a \" bcc" -h')
-    assert l==[r'"\"a \" bcc"','-h'], repr(l)
-    l = splitcmdline(" 'a bcc'  -h")
-    assert l==["'a bcc'",'-h'], repr(l)
-    l = splitcmdline(r"'\'a \' bcc' -h")
-    assert l==[r"'\'a \' bcc'",'-h'], repr(l)
-
-############################################################
-
-def find_executable(exe, path=None):
-    """ Return full path of a executable.
-    """
+def find_executable(exe, path=None, _cache={}):
+    """Return full path of a executable or None.
+
+    Symbolic links are not followed.
+    """
+    key = exe, path
+    try:
+        return _cache[key]
+    except KeyError:
+        pass
     log.debug('find_executable(%r)' % exe)
     orig_exe = exe
+
     if path is None:
-        path = os.environ.get('PATH',os.defpath)
-    if os.name=='posix' and sys.version[:3]>'2.1':
+        path = os.environ.get('PATH', os.defpath)
+    if os.name=='posix':
         realpath = os.path.realpath
     else:
         realpath = lambda a:a
-    if exe[0]=='"':
+
+    if exe.startswith('"'):
         exe = exe[1:-1]
-    suffices = ['']
-    if os.name in ['nt','dos','os2']:
-        fn,ext = os.path.splitext(exe)
-        extra_suffices = ['.exe','.com','.bat']
-        if ext.lower() not in extra_suffices:
-            suffices = extra_suffices
+
+    suffixes = ['']
+    if os.name in ['nt', 'dos', 'os2']:
+        fn, ext = os.path.splitext(exe)
+        extra_suffixes = ['.exe', '.com', '.bat']
+        if ext.lower() not in extra_suffixes:
+            suffixes = extra_suffixes
+
     if os.path.isabs(exe):
         paths = ['']
     else:
-        paths = map(os.path.abspath, path.split(os.pathsep))
-        if 0 and os.name == 'nt':
-            new_paths = []
-            cygwin_paths = []
-            for path in paths:
-                d,p = os.path.splitdrive(path)
-                if p.lower().find('cygwin') >= 0:
-                    cygwin_paths.append(path)
-                else:
-                    new_paths.append(path)
-            paths = new_paths + cygwin_paths
+        paths = [ os.path.abspath(p) for p in path.split(os.pathsep) ]
+
     for path in paths:
-        fn = os.path.join(path,exe)
-        for s in suffices:
+        fn = os.path.join(path, exe)
+        for s in suffixes:
             f_ext = fn+s
             if not os.path.islink(f_ext):
-                # see comment below.
                 f_ext = realpath(f_ext)
-            if os.path.isfile(f_ext) and os.access(f_ext,os.X_OK):
-                log.debug('Found executable %s' % f_ext)
+            if os.path.isfile(f_ext) and os.access(f_ext, os.X_OK):
+                log.info('Found executable %s' % f_ext)
+                _cache[key] = f_ext
                 return f_ext
-    if os.path.islink(exe):
-        # Don't follow symbolic links. E.g. when using colorgcc then
-        # gcc -> /usr/bin/colorgcc
-        # g77 -> /usr/bin/colorgcc
-        pass
-    else:
-        exe = realpath(exe)
-    if not os.path.isfile(exe) or os.access(exe,os.X_OK):
-        log.warn('Could not locate executable %s' % orig_exe)
-        return orig_exe
-    return exe
+
+    log.warn('Could not locate executable %s' % orig_exe)
+    return None
 
 ############################################################
 
 def _preserve_environment( names ):
     log.debug('_preserve_environment(%r)' % (names))
-    env = {}
-    for name in names:
-        env[name] = os.environ.get(name)
+    env = {name: os.environ.get(name) for name in names}
     return env
 
 def _update_environment( **env ):
     log.debug('_update_environment(...)')
-    for name,value in env.items():
+    for name, value in env.items():
         os.environ[name] = value or ''
 
-def exec_command( command,
-                  execute_in='', use_shell=None, use_tee = None,
-                  _with_python = 1,
-                  **env ):
-    """ Return (status,output) of executed command.
-
-    command is a concatenated string of executable and arguments.
-    The output contains both stdout and stderr messages.
-    The following special keyword arguments can be used:
-      use_shell - execute `sh -c command`
-      use_tee   - pipe the output of command through tee
-      execute_in - before command `cd execute_in` and after `cd -`.
-
+def exec_command(command, execute_in='', use_shell=None, use_tee=None,
+                 _with_python = 1, **env ):
+    """
+    Return (status,output) of executed command.
+
+    .. deprecated:: 1.17
+        Use subprocess.Popen instead
+
+    Parameters
+    ----------
+    command : str
+        A concatenated string of executable and arguments.
+    execute_in : str
+        Before running command ``cd execute_in`` and after ``cd -``.
+    use_shell : {bool, None}, optional
+        If True, execute ``sh -c command``. Default None (True)
+    use_tee : {bool, None}, optional
+        If True use tee. Default None (True)
+
+
+    Returns
+    -------
+    res : str
+        Both stdout and stderr messages.
+
+    Notes
+    -----
     On NT, DOS systems the returned status is correct for external commands.
     Wild cards will not work for non-posix systems or when use_shell=0.
-    """
-    log.debug('exec_command(%r,%s)' % (command,\
+
+    """
+    # 2019-01-30, 1.17
+    warnings.warn('exec_command is deprecated since NumPy v1.17, use '
+                  'subprocess.Popen instead', DeprecationWarning, stacklevel=1)
+    log.debug('exec_command(%r,%s)' % (command,
          ','.join(['%s=%r'%kv for kv in env.items()])))
 
     if use_tee is None:
@@ -232,31 +233,14 @@
     else:
         log.debug('Retaining cwd: %s' % oldcwd)
 
-    oldenv = _preserve_environment( env.keys() )
+    oldenv = _preserve_environment( list(env.keys()) )
     _update_environment( **env )
 
     try:
-        # _exec_command is robust but slow, it relies on
-        # usable sys.std*.fileno() descriptors. If they
-        # are bad (like in win32 Idle, PyCrust environments)
-        # then _exec_command_python (even slower)
-        # will be used as a last resort.
-        #
-        # _exec_command_posix uses os.system and is faster
-        # but not on all platforms os.system will return
-        # a correct status.
-        if _with_python and (0 or sys.__stdout__.fileno()==-1):
-            st = _exec_command_python(command,
-                                      exec_command_dir = exec_dir,
-                                      **env)
-        elif os.name=='posix':
-            st = _exec_command_posix(command,
-                                     use_shell=use_shell,
-                                     use_tee=use_tee,
-                                     **env)
-        else:
-            st = _exec_command(command, use_shell=use_shell,
-                               use_tee=use_tee,**env)
+        st = _exec_command(command,
+                           use_shell=use_shell,
+                           use_tee=use_tee,
+                           **env)
     finally:
         if oldcwd!=execute_in:
             os.chdir(oldcwd)
@@ -265,384 +249,68 @@
 
     return st
 
-def _exec_command_posix( command,
-                         use_shell = None,
-                         use_tee = None,
-                         **env ):
-    log.debug('_exec_command_posix(...)')
-
-    if is_sequence(command):
-        command_str = ' '.join(list(command))
-    else:
-        command_str = command
-
-    tmpfile = tempfile.mktemp()
-    stsfile = None
-    if use_tee:
-        stsfile = tempfile.mktemp()
-        filter = ''
-        if use_tee == 2:
-            filter = r'| tr -cd "\n" | tr "\n" "."; echo'
-        command_posix = '( %s ; echo $? > %s ) 2>&1 | tee %s %s'\
-                      % (command_str,stsfile,tmpfile,filter)
-    else:
-        stsfile = tempfile.mktemp()
-        command_posix = '( %s ; echo $? > %s ) > %s 2>&1'\
-                        % (command_str,stsfile,tmpfile)
-        #command_posix = '( %s ) > %s 2>&1' % (command_str,tmpfile)
-
-    log.debug('Running os.system(%r)' % (command_posix))
-    status = os.system(command_posix)
-
-    if use_tee:
-        if status:
-            # if command_tee fails then fall back to robust exec_command
-            log.warn('_exec_command_posix failed (status=%s)' % status)
-            return _exec_command(command, use_shell=use_shell, **env)
-
-    if stsfile is not None:
-        f = open(stsfile,'r')
-        status_text = f.read()
-        status = int(status_text)
-        f.close()
-        os.remove(stsfile)
-
-    f = open(tmpfile,'r')
-    text = f.read()
-    f.close()
-    os.remove(tmpfile)
-
-    if text[-1:]=='\n':
-        text = text[:-1]
-
-    return status, text
-
-
-def _exec_command_python(command,
-                         exec_command_dir='', **env):
-    log.debug('_exec_command_python(...)')
-
-    python_exe = get_pythonexe()
-    cmdfile = tempfile.mktemp()
-    stsfile = tempfile.mktemp()
-    outfile = tempfile.mktemp()
-
-    f = open(cmdfile,'w')
-    f.write('import os\n')
-    f.write('import sys\n')
-    f.write('sys.path.insert(0,%r)\n' % (exec_command_dir))
-    f.write('from exec_command import exec_command\n')
-    f.write('del sys.path[0]\n')
-    f.write('cmd = %r\n' % command)
-    f.write('os.environ = %r\n' % (os.environ))
-    f.write('s,o = exec_command(cmd, _with_python=0, **%r)\n' % (env))
-    f.write('f=open(%r,"w")\nf.write(str(s))\nf.close()\n' % (stsfile))
-    f.write('f=open(%r,"w")\nf.write(o)\nf.close()\n' % (outfile))
-    f.close()
-
-    cmd = '%s %s' % (python_exe, cmdfile)
-    status = os.system(cmd)
-    if status:
-        raise RuntimeError("%r failed" % (cmd,))
-    os.remove(cmdfile)
-
-    f = open(stsfile,'r')
-    status = int(f.read())
-    f.close()
-    os.remove(stsfile)
-
-    f = open(outfile,'r')
-    text = f.read()
-    f.close()
-    os.remove(outfile)
-
-    return status, text
-
-def quote_arg(arg):
-    if arg[0]!='"' and ' ' in arg:
-        return '"%s"' % arg
-    return arg
-
-def _exec_command( command, use_shell=None, use_tee = None, **env ):
-    log.debug('_exec_command(...)')
-
+
+def _exec_command(command, use_shell=None, use_tee = None, **env):
+    """
+    Internal workhorse for exec_command().
+    """
     if use_shell is None:
         use_shell = os.name=='posix'
     if use_tee is None:
         use_tee = os.name=='posix'
 
-    using_command = 0
-    if use_shell:
-        # We use shell (unless use_shell==0) so that wildcards can be
-        # used.
-        sh = os.environ.get('SHELL','/bin/sh')
+    if os.name == 'posix' and use_shell:
+        # On POSIX, subprocess always uses /bin/sh, override
+        sh = os.environ.get('SHELL', '/bin/sh')
         if is_sequence(command):
-            argv = [sh,'-c',' '.join(list(command))]
+            command = [sh, '-c', ' '.join(command)]
         else:
-            argv = [sh,'-c',command]
-    else:
-        # On NT, DOS we avoid using command.com as it's exit status is
-        # not related to the exit status of a command.
-        if is_sequence(command):
-            argv = command[:]
-        else:
-            argv = splitcmdline(command)
-
-    if hasattr(os,'spawnvpe'):
-        spawn_command = os.spawnvpe
-    else:
-        spawn_command = os.spawnve
-        argv[0] = find_executable(argv[0])
-        if not os.path.isfile(argv[0]):
-            log.warn('Executable %s does not exist' % (argv[0]))
-            if os.name in ['nt','dos']:
-                # argv[0] might be internal command
-                argv = [os.environ['COMSPEC'],'/C'] + argv
-                using_command = 1
-
-    # sys.__std*__ is used instead of sys.std* because environments
-    # like IDLE, PyCrust, etc overwrite sys.std* commands.
-    so_fileno = sys.__stdout__.fileno()
-    se_fileno = sys.__stderr__.fileno()
-    so_flush = sys.__stdout__.flush
-    se_flush = sys.__stderr__.flush
-    so_dup = os.dup(so_fileno)
-    se_dup = os.dup(se_fileno)
-
-    outfile = tempfile.mktemp()
-    fout = open(outfile,'w')
-    if using_command:
-        errfile = tempfile.mktemp()
-        ferr = open(errfile,'w')
-
-    log.debug('Running %s(%s,%r,%r,os.environ)' \
-              % (spawn_command.__name__,os.P_WAIT,argv[0],argv))
-
-    argv0 = argv[0]
-    if not using_command:
-        argv[0] = quote_arg(argv0)
-
-    so_flush()
-    se_flush()
-    os.dup2(fout.fileno(),so_fileno)
-    if using_command:
-        #XXX: disabled for now as it does not work from cmd under win32.
-        #     Tests fail on msys
-        os.dup2(ferr.fileno(),se_fileno)
-    else:
-        os.dup2(fout.fileno(),se_fileno)
+            command = [sh, '-c', command]
+        use_shell = False
+
+    elif os.name == 'nt' and is_sequence(command):
+        # On Windows, join the string for CreateProcess() ourselves as
+        # subprocess does it a bit differently
+        command = ' '.join(_quote_arg(arg) for arg in command)
+
+    # Inherit environment by default
+    env = env or None
     try:
-        status = spawn_command(os.P_WAIT,argv0,argv,os.environ)
-    except OSError,errmess:
-        status = 999
-        sys.stderr.write('%s: %s'%(errmess,argv[0]))
-
-    so_flush()
-    se_flush()
-    os.dup2(so_dup,so_fileno)
-    os.dup2(se_dup,se_fileno)
-
-    fout.close()
-    fout = open(outfile,'r')
-    text = fout.read()
-    fout.close()
-    os.remove(outfile)
-
-    if using_command:
-        ferr.close()
-        ferr = open(errfile,'r')
-        errmess = ferr.read()
-        ferr.close()
-        os.remove(errfile)
-        if errmess and not status:
-            # Not sure how to handle the case where errmess
-            # contains only warning messages and that should
-            # not be treated as errors.
-            #status = 998
-            if text:
-                text = text + '\n'
-            #text = '%sCOMMAND %r FAILED: %s' %(text,command,errmess)
-            text = text + errmess
-            print errmess
-    if text[-1:]=='\n':
+        # universal_newlines is set to False so that communicate()
+        # will return bytes. We need to decode the output ourselves
+        # so that Python will not raise a UnicodeDecodeError when
+        # it encounters an invalid character; rather, we simply replace it
+        proc = subprocess.Popen(command, shell=use_shell, env=env,
+                                stdout=subprocess.PIPE,
+                                stderr=subprocess.STDOUT,
+                                universal_newlines=False)
+    except OSError:
+        # Return 127, as os.spawn*() and /bin/sh do
+        return 127, ''
+
+    text, err = proc.communicate()
+    mylocale = locale.getpreferredencoding(False)
+    if mylocale is None:
+        mylocale = 'ascii'
+    text = text.decode(mylocale, errors='replace')
+    text = text.replace('\r\n', '\n')
+    # Another historical oddity
+    if text[-1:] == '\n':
         text = text[:-1]
-    if status is None:
-        status = 0
-
-    if use_tee:
-        print text
-
-    return status, text
-
-
-def test_nt(**kws):
-    pythonexe = get_pythonexe()
-    echo = find_executable('echo')
-    using_cygwin_echo = echo != 'echo'
-    if using_cygwin_echo:
-        log.warn('Using cygwin echo in win32 environment is not supported')
-
-        s,o=exec_command(pythonexe\
-                         +' -c "import os;print os.environ.get(\'AAA\',\'\')"')
-        assert s==0 and o=='',(s,o)
-
-        s,o=exec_command(pythonexe\
-                         +' -c "import os;print os.environ.get(\'AAA\')"',
-                         AAA='Tere')
-        assert s==0 and o=='Tere',(s,o)
-
-        os.environ['BBB'] = 'Hi'
-        s,o=exec_command(pythonexe\
-                         +' -c "import os;print os.environ.get(\'BBB\',\'\')"')
-        assert s==0 and o=='Hi',(s,o)
-
-        s,o=exec_command(pythonexe\
-                         +' -c "import os;print os.environ.get(\'BBB\',\'\')"',
-                         BBB='Hey')
-        assert s==0 and o=='Hey',(s,o)
-
-        s,o=exec_command(pythonexe\
-                         +' -c "import os;print os.environ.get(\'BBB\',\'\')"')
-        assert s==0 and o=='Hi',(s,o)
-    elif 0:
-        s,o=exec_command('echo Hello')
-        assert s==0 and o=='Hello',(s,o)
-
-        s,o=exec_command('echo a%AAA%')
-        assert s==0 and o=='a',(s,o)
-
-        s,o=exec_command('echo a%AAA%',AAA='Tere')
-        assert s==0 and o=='aTere',(s,o)
-
-        os.environ['BBB'] = 'Hi'
-        s,o=exec_command('echo a%BBB%')
-        assert s==0 and o=='aHi',(s,o)
-
-        s,o=exec_command('echo a%BBB%',BBB='Hey')
-        assert s==0 and o=='aHey', (s,o)
-        s,o=exec_command('echo a%BBB%')
-        assert s==0 and o=='aHi',(s,o)
-
-        s,o=exec_command('this_is_not_a_command')
-        assert s and o!='',(s,o)
-
-        s,o=exec_command('type not_existing_file')
-        assert s and o!='',(s,o)
-
-    s,o=exec_command('echo path=%path%')
-    assert s==0 and o!='',(s,o)
-
-    s,o=exec_command('%s -c "import sys;sys.stderr.write(sys.platform)"' \
-                     % pythonexe)
-    assert s==0 and o=='win32',(s,o)
-
-    s,o=exec_command('%s -c "raise \'Ignore me.\'"' % pythonexe)
-    assert s==1 and o,(s,o)
-
-    s,o=exec_command('%s -c "import sys;sys.stderr.write(\'0\');sys.stderr.write(\'1\');sys.stderr.write(\'2\')"'\
-                     % pythonexe)
-    assert s==0 and o=='012',(s,o)
-
-    s,o=exec_command('%s -c "import sys;sys.exit(15)"' % pythonexe)
-    assert s==15 and o=='',(s,o)
-
-    s,o=exec_command('%s -c "print \'Heipa\'"' % pythonexe)
-    assert s==0 and o=='Heipa',(s,o)
-
-    print 'ok'
-
-def test_posix(**kws):
-    s,o=exec_command("echo Hello",**kws)
-    assert s==0 and o=='Hello',(s,o)
-
-    s,o=exec_command('echo $AAA',**kws)
-    assert s==0 and o=='',(s,o)
-
-    s,o=exec_command('echo "$AAA"',AAA='Tere',**kws)
-    assert s==0 and o=='Tere',(s,o)
-
-
-    s,o=exec_command('echo "$AAA"',**kws)
-    assert s==0 and o=='',(s,o)
-
-    os.environ['BBB'] = 'Hi'
-    s,o=exec_command('echo "$BBB"',**kws)
-    assert s==0 and o=='Hi',(s,o)
-
-    s,o=exec_command('echo "$BBB"',BBB='Hey',**kws)
-    assert s==0 and o=='Hey',(s,o)
-
-    s,o=exec_command('echo "$BBB"',**kws)
-    assert s==0 and o=='Hi',(s,o)
-
-
-    s,o=exec_command('this_is_not_a_command',**kws)
-    assert s!=0 and o!='',(s,o)
-
-    s,o=exec_command('echo path=$PATH',**kws)
-    assert s==0 and o!='',(s,o)
-
-    s,o=exec_command('python -c "import sys,os;sys.stderr.write(os.name)"',**kws)
-    assert s==0 and o=='posix',(s,o)
-
-    s,o=exec_command('python -c "raise \'Ignore me.\'"',**kws)
-    assert s==1 and o,(s,o)
-
-    s,o=exec_command('python -c "import sys;sys.stderr.write(\'0\');sys.stderr.write(\'1\');sys.stderr.write(\'2\')"',**kws)
-    assert s==0 and o=='012',(s,o)
-
-    s,o=exec_command('python -c "import sys;sys.exit(15)"',**kws)
-    assert s==15 and o=='',(s,o)
-
-    s,o=exec_command('python -c "print \'Heipa\'"',**kws)
-    assert s==0 and o=='Heipa',(s,o)
-
-    print 'ok'
-
-def test_execute_in(**kws):
-    pythonexe = get_pythonexe()
-    tmpfile = tempfile.mktemp()
-    fn = os.path.basename(tmpfile)
-    tmpdir = os.path.dirname(tmpfile)
-    f = open(tmpfile,'w')
-    f.write('Hello')
-    f.close()
-
-    s,o = exec_command('%s -c "print \'Ignore the following IOError:\','\
-                       'open(%r,\'r\')"' % (pythonexe,fn),**kws)
-    assert s and o!='',(s,o)
-    s,o = exec_command('%s -c "print open(%r,\'r\').read()"' % (pythonexe,fn),
-                       execute_in = tmpdir,**kws)
-    assert s==0 and o=='Hello',(s,o)
-    os.remove(tmpfile)
-    print 'ok'
-
-def test_svn(**kws):
-    s,o = exec_command(['svn','status'],**kws)
-    assert s,(s,o)
-    print 'svn ok'
-
-def test_cl(**kws):
-    if os.name=='nt':
-        s,o = exec_command(['cl','/V'],**kws)
-        assert s,(s,o)
-        print 'cl ok'
-
-if os.name=='posix':
-    test = test_posix
-elif os.name in ['nt','dos']:
-    test = test_nt
-else:
-    raise NotImplementedError,'exec_command tests for '+os.name
+
+    if use_tee and text:
+        print(text)
+    return proc.returncode, text
+
+
+def _quote_arg(arg):
+    """
+    Quote the argument for safe use in a shell command line.
+    """
+    # If there is a quote in the string, assume relevants parts of the
+    # string are already quoted (e.g. '-I"C:\\Program Files\\..."')
+    if '"' not in arg and ' ' in arg:
+        return '"%s"' % arg
+    return arg
 
 ############################################################
-
-if __name__ == "__main__":
-
-    test_splitcmdline()
-    test(use_tee=0)
-    test(use_tee=1)
-    test_execute_in(use_tee=0)
-    test_execute_in(use_tee=1)
-    test_svn(use_tee=1)
-    test_cl(use_tee=1)
('numpy/distutils', 'from_template.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,4 +1,4 @@
-#!/usr/bin/python
+#!/usr/bin/env python3
 """
 
 process_file(filename)
@@ -11,7 +11,7 @@
   All function and subroutine blocks in a source file with names that
   contain '<..>' will be replicated according to the rules in '<..>'.
 
-  The number of comma-separeted words in '<..>' will determine the number of
+  The number of comma-separated words in '<..>' will determine the number of
   replicates.
 
   '<..>' may have two different forms, named and short. For example,
@@ -45,22 +45,15 @@
   <ctypereal=float,double,\\0,\\1>
 
 """
-
-__all__ = ['process_str','process_file']
-
-import string,os,sys
-if sys.version[:3]>='2.3':
-    import re
-else:
-    import pre as re
-    False = 0
-    True = 1
-if sys.version[:5]=='2.2.1':
-    import re
-
-routine_start_re = re.compile(r'(\n|\A)((     (\$|\*))|)\s*(subroutine|function)\b',re.I)
-routine_end_re = re.compile(r'\n\s*end\s*(subroutine|function)\b.*(\n|\Z)',re.I)
-function_start_re = re.compile(r'\n     (\$|\*)\s*function\b',re.I)
+__all__ = ['process_str', 'process_file']
+
+import os
+import sys
+import re
+
+routine_start_re = re.compile(r'(\n|\A)((     (\$|\*))|)\s*(subroutine|function)\b', re.I)
+routine_end_re = re.compile(r'\n\s*end\s*(subroutine|function)\b.*(\n|\Z)', re.I)
+function_start_re = re.compile(r'\n     (\$|\*)\s*function\b', re.I)
 
 def parse_structure(astr):
     """ Return a list of tuples for each function or subroutine each
@@ -70,23 +63,23 @@
 
     spanlist = []
     ind = 0
-    while 1:
-        m = routine_start_re.search(astr,ind)
+    while True:
+        m = routine_start_re.search(astr, ind)
         if m is None:
             break
         start = m.start()
-        if function_start_re.match(astr,start,m.end()):
-            while 1:
-                i = astr.rfind('\n',ind,start)
+        if function_start_re.match(astr, start, m.end()):
+            while True:
+                i = astr.rfind('\n', ind, start)
                 if i==-1:
                     break
                 start = i
                 if astr[i:i+7]!='\n     $':
                     break
         start += 1
-        m = routine_end_re.search(astr,m.end())
+        m = routine_end_re.search(astr, m.end())
         ind = end = m and m.end()-1 or len(astr)
-        spanlist.append((start,end))
+        spanlist.append((start, end))
     return spanlist
 
 template_re = re.compile(r"<\s*(\w[\w\d]*)\s*>")
@@ -98,10 +91,15 @@
     names = {}
     for rep in reps:
         name = rep[0].strip() or unique_key(names)
-        repl = rep[1].replace('\,','@comma@')
+        repl = rep[1].replace(r'\,', '@comma@')
         thelist = conv(repl)
         names[name] = thelist
     return names
+
+def find_and_remove_repl_patterns(astr):
+    names = find_repl_patterns(astr)
+    astr = re.subn(named_re, '', astr)[0]
+    return astr, names
 
 item_re = re.compile(r"\A\\(?P<index>\d+)\Z")
 def conv(astr):
@@ -116,7 +114,7 @@
 
 def unique_key(adict):
     """ Obtain a unique key given a dictionary."""
-    allkeys = adict.keys()
+    allkeys = list(adict.keys())
     done = False
     n = 1
     while not done:
@@ -129,14 +127,14 @@
 
 
 template_name_re = re.compile(r'\A\s*(\w[\w\d]*)\s*\Z')
-def expand_sub(substr,names):
-    substr = substr.replace('\>','@rightarrow@')
-    substr = substr.replace('\<','@leftarrow@')
+def expand_sub(substr, names):
+    substr = substr.replace(r'\>', '@rightarrow@')
+    substr = substr.replace(r'\<', '@leftarrow@')
     lnames = find_repl_patterns(substr)
-    substr = named_re.sub(r"<\1>",substr)  # get rid of definition templates
+    substr = named_re.sub(r"<\1>", substr)  # get rid of definition templates
 
     def listrepl(mobj):
-        thelist = conv(mobj.group(1).replace('\,','@comma@'))
+        thelist = conv(mobj.group(1).replace(r'\,', '@comma@'))
         if template_name_re.match(thelist):
             return "<%s>" % (thelist)
         name = None
@@ -155,13 +153,13 @@
     base_rule = None
     rules = {}
     for r in template_re.findall(substr):
-        if not rules.has_key(r):
-            thelist = lnames.get(r,names.get(r,None))
+        if r not in rules:
+            thelist = lnames.get(r, names.get(r, None))
             if thelist is None:
-                raise ValueError,'No replicates found for <%s>' % (r)
-            if not names.has_key(r) and not thelist.startswith('_'):
+                raise ValueError('No replicates found for <%s>' % (r))
+            if r not in names and not thelist.startswith('_'):
                 names[r] = thelist
-            rule = [i.replace('@comma@',',') for i in thelist.split(',')]
+            rule = [i.replace('@comma@', ',') for i in thelist.split(',')]
             num = len(rule)
 
             if numsubs is None:
@@ -171,28 +169,27 @@
             elif num == numsubs:
                 rules[r] = rule
             else:
-                print "Mismatch in number of replacements (base <%s=%s>)"\
-                      " for <%s=%s>. Ignoring." % (base_rule,
-                                                  ','.join(rules[base_rule]),
-                                                  r,thelist)
+                print("Mismatch in number of replacements (base <%s=%s>)"
+                      " for <%s=%s>. Ignoring." %
+                      (base_rule, ','.join(rules[base_rule]), r, thelist))
     if not rules:
         return substr
 
     def namerepl(mobj):
         name = mobj.group(1)
-        return rules.get(name,(k+1)*[name])[k]
+        return rules.get(name, (k+1)*[name])[k]
 
     newstr = ''
     for k in range(numsubs):
         newstr += template_re.sub(namerepl, substr) + '\n\n'
 
-    newstr = newstr.replace('@rightarrow@','>')
-    newstr = newstr.replace('@leftarrow@','<')
+    newstr = newstr.replace('@rightarrow@', '>')
+    newstr = newstr.replace('@leftarrow@', '<')
     return newstr
 
 def process_str(allstr):
     newstr = allstr
-    writestr = '' #_head # using _head will break free-format files
+    writestr = ''
 
     struct = parse_structure(newstr)
 
@@ -200,34 +197,33 @@
     names = {}
     names.update(_special_names)
     for sub in struct:
-        writestr += newstr[oldend:sub[0]]
-        names.update(find_repl_patterns(newstr[oldend:sub[0]]))
-        writestr += expand_sub(newstr[sub[0]:sub[1]],names)
+        cleanedstr, defs = find_and_remove_repl_patterns(newstr[oldend:sub[0]])
+        writestr += cleanedstr
+        names.update(defs)
+        writestr += expand_sub(newstr[sub[0]:sub[1]], names)
         oldend =  sub[1]
     writestr += newstr[oldend:]
 
     return writestr
 
-include_src_re = re.compile(r"(\n|\A)\s*include\s*['\"](?P<name>[\w\d./\\]+[.]src)['\"]",re.I)
+include_src_re = re.compile(r"(\n|\A)\s*include\s*['\"](?P<name>[\w\d./\\]+\.src)['\"]", re.I)
 
 def resolve_includes(source):
     d = os.path.dirname(source)
-    fid = open(source)
-    lines = []
-    for line in fid.readlines():
-        m = include_src_re.match(line)
-        if m:
-            fn = m.group('name')
-            if not os.path.isabs(fn):
-                fn = os.path.join(d,fn)
-            if os.path.isfile(fn):
-                print 'Including file',fn
-                lines.extend(resolve_includes(fn))
+    with open(source) as fid:
+        lines = []
+        for line in fid:
+            m = include_src_re.match(line)
+            if m:
+                fn = m.group('name')
+                if not os.path.isabs(fn):
+                    fn = os.path.join(d, fn)
+                if os.path.isfile(fn):
+                    lines.extend(resolve_includes(fn))
+                else:
+                    lines.append(line)
             else:
                 lines.append(line)
-        else:
-            lines.append(line)
-    fid.close()
     return lines
 
 def process_file(source):
@@ -244,19 +240,22 @@
 <ctypereal=float,double,\\0,\\1>
 ''')
 
-if __name__ == "__main__":
-
+def main():
     try:
         file = sys.argv[1]
     except IndexError:
         fid = sys.stdin
         outfile = sys.stdout
     else:
-        fid = open(file,'r')
+        fid = open(file, 'r')
         (base, ext) = os.path.splitext(file)
         newname = base
-        outfile = open(newname,'w')
+        outfile = open(newname, 'w')
 
     allstr = fid.read()
     writestr = process_str(allstr)
     outfile.write(writestr)
+
+
+if __name__ == "__main__":
+    main()
('numpy/distutils', 'mingw32ccompiler.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -7,14 +7,16 @@
     # 3. Force windows to use g77
 
 """
-
 import os
+import platform
 import sys
-import log
+import subprocess
+import re
+import textwrap
 
 # Overwrite certain distutils.ccompiler functions:
-import numpy.distutils.ccompiler
-
+import numpy.distutils.ccompiler  # noqa: F401
+from numpy.distutils import log
 # NT stuff
 # 1. Make sure libpython<version>.a exists for gcc.  If not, build it.
 # 2. Force windows to use gcc (we're struggling with MSVC and g77 support)
@@ -22,11 +24,23 @@
 # 3. Force windows to use g77
 
 import distutils.cygwinccompiler
-from distutils.version import StrictVersion
-from numpy.distutils.ccompiler import gen_preprocess_options, gen_lib_options
-from distutils.errors import DistutilsExecError, CompileError, UnknownFileError
-
 from distutils.unixccompiler import UnixCCompiler
+from distutils.msvccompiler import get_build_version as get_build_msvc_version
+from distutils.errors import UnknownFileError
+from numpy.distutils.misc_util import (msvc_runtime_library,
+                                       msvc_runtime_version,
+                                       msvc_runtime_major,
+                                       get_build_architecture)
+
+def get_msvcr_replacement():
+    """Replacement for outdated version of get_msvcr from cygwinccompiler"""
+    msvcr = msvc_runtime_library()
+    return [] if msvcr is None else [msvcr]
+
+
+# Useful to generate table of symbols from a dll
+_START = re.compile(r'\[Ordinal/Name Pointer\] Table')
+_TABLE = re.compile(r'^\s+\[([\s*[0-9]*)\] ([a-zA-Z0-9_]*)')
 
 # the same as cygwin plus some additional parameters
 class Mingw32CCompiler(distutils.cygwinccompiler.CygwinCCompiler):
@@ -41,70 +55,52 @@
                   dry_run=0,
                   force=0):
 
-        distutils.cygwinccompiler.CygwinCCompiler.__init__ (self,
-                                                       verbose,dry_run, force)
-
-        # we need to support 3.2 which doesn't match the standard
-        # get_versions methods regex
-        if self.gcc_version is None:
-            import re
-            out = os.popen('gcc -dumpversion','r')
-            out_string = out.read()
-            out.close()
-            result = re.search('(\d+\.\d+)',out_string)
-            if result:
-                self.gcc_version = StrictVersion(result.group(1))
-
-        # A real mingw32 doesn't need to specify a different entry point,
-        # but cygwin 2.91.57 in no-cygwin-mode needs it.
-        if self.gcc_version <= "2.91.57":
-            entry_point = '--entry _DllMain@12'
-        else:
-            entry_point = ''
-
-        if self.linker_dll == 'dllwrap':
-            # Commented out '--driver-name g++' part that fixes weird
-            #   g++.exe: g++: No such file or directory
-            # error (mingw 1.0 in Enthon24 tree, gcc-3.4.5).
-            # If the --driver-name part is required for some environment
-            # then make the inclusion of this part specific to that environment.
-            self.linker = 'dllwrap' #  --driver-name g++'
-        elif self.linker_dll == 'gcc':
-            self.linker = 'g++'
+        distutils.cygwinccompiler.CygwinCCompiler.__init__ (self, verbose,
+                                                            dry_run, force)
 
         # **changes: eric jones 4/11/01
         # 1. Check for import library on Windows.  Build if it doesn't exist.
 
         build_import_library()
 
-        # **changes: eric jones 4/11/01
-        # 2. increased optimization and turned off all warnings
-        # 3. also added --driver-name g++
-        #self.set_executables(compiler='gcc -mno-cygwin -O2 -w',
-        #                     compiler_so='gcc -mno-cygwin -mdll -O2 -w',
-        #                     linker_exe='gcc -mno-cygwin',
-        #                     linker_so='%s --driver-name g++ -mno-cygwin -mdll -static %s'
-        #                                % (self.linker, entry_point))
-        if self.gcc_version <= "3.0.0":
-            self.set_executables(compiler='gcc -mno-cygwin -O2 -w',
-                                 compiler_so='gcc -mno-cygwin -mdll -O2 -w -Wstrict-prototypes',
-                                 linker_exe='g++ -mno-cygwin',
-                                 linker_so='%s -mno-cygwin -mdll -static %s'
-                                 % (self.linker, entry_point))
+        # Check for custom msvc runtime library on Windows. Build if it doesn't exist.
+        msvcr_success = build_msvcr_library()
+        msvcr_dbg_success = build_msvcr_library(debug=True)
+        if msvcr_success or msvcr_dbg_success:
+            # add preprocessor statement for using customized msvcr lib
+            self.define_macro('NPY_MINGW_USE_CUSTOM_MSVCR')
+
+        # Define the MSVC version as hint for MinGW
+        msvcr_version = msvc_runtime_version()
+        if msvcr_version:
+            self.define_macro('__MSVCRT_VERSION__', '0x%04i' % msvcr_version)
+
+        # MS_WIN64 should be defined when building for amd64 on windows,
+        # but python headers define it only for MS compilers, which has all
+        # kind of bad consequences, like using Py_ModuleInit4 instead of
+        # Py_ModuleInit4_64, etc... So we add it here
+        if get_build_architecture() == 'AMD64':
+            self.set_executables(
+                compiler='gcc -g -DDEBUG -DMS_WIN64 -O0 -Wall',
+                compiler_so='gcc -g -DDEBUG -DMS_WIN64 -O0 -Wall '
+                            '-Wstrict-prototypes',
+                linker_exe='gcc -g',
+                linker_so='gcc -g -shared')
         else:
-            self.set_executables(compiler='gcc -mno-cygwin -O2 -Wall',
-                                 compiler_so='gcc -O2 -Wall -Wstrict-prototypes',
-                                 linker_exe='g++ ',
-                                 linker_so='g++ -shared')
+            self.set_executables(
+                compiler='gcc -O2 -Wall',
+                compiler_so='gcc -O2 -Wall -Wstrict-prototypes',
+                linker_exe='g++ ',
+                linker_so='g++ -shared')
         # added for python2.3 support
         # we can't pass it through set_executables because pre 2.2 would fail
         self.compiler_cxx = ['g++']
 
-        # Maybe we should also append -mthreads, but then the finished
-        # dlls need another dll (mingwm10.dll see Mingw32 docs)
-        # (-mthreads: Support thread-safe exception handling on `Mingw32')
-
-        # no additional libraries needed -- maybe need msvcr71
+        # Maybe we should also append -mthreads, but then the finished dlls
+        # need another dll (mingwm10.dll see Mingw32 docs) (-mthreads: Support
+        # thread-safe exception handling on `Mingw32')
+
+        # no additional libraries needed
         #self.dll_libraries=[]
         return
 
@@ -124,11 +120,13 @@
              extra_postargs=None,
              build_temp=None,
              target_lang=None):
-        if sys.version[:3] > '2.3':
-            if libraries:
-                libraries.append('msvcr71')
-            else:
-                libraries = ['msvcr71']
+        # Include the appropriate MSVC runtime library if Python was built
+        # with MSVC >= 7.0 (MinGW standard is msvcrt)
+        runtime_library = msvc_runtime_library()
+        if runtime_library:
+            if not libraries:
+                libraries = []
+            libraries.append(runtime_library)
         args = (self,
                 target_desc,
                 objects,
@@ -143,11 +141,8 @@
                 extra_postargs,
                 build_temp,
                 target_lang)
-        if self.gcc_version < "3.0.0":
-            func = distutils.cygwinccompiler.CygwinCCompiler.link
-        else:
-            func = UnixCCompiler.link
-        func(*args[:func.im_func.func_code.co_argcount])
+        func = UnixCCompiler.link
+        func(*args[:func.__code__.co_argcount])
         return
 
     def object_filenames (self,
@@ -163,14 +158,14 @@
             # added these lines to strip off windows drive letters
             # without it, .o files are placed next to .c files
             # instead of the build directory
-            drv,base = os.path.splitdrive(base)
+            drv, base = os.path.splitdrive(base)
             if drv:
                 base = base[1:]
 
-            if ext not in (self.src_extensions + ['.rc','.res']):
-                raise UnknownFileError, \
+            if ext not in (self.src_extensions + ['.rc', '.res']):
+                raise UnknownFileError(
                       "unknown file type '%s' (from '%s')" % \
-                      (ext, src_name)
+                      (ext, src_name))
             if strip_dir:
                 base = os.path.basename (base)
             if ext == '.res' or ext == '.rc':
@@ -185,40 +180,416 @@
     # object_filenames ()
 
 
+def find_python_dll():
+    # We can't do much here:
+    # - find it in the virtualenv (sys.prefix)
+    # - find it in python main dir (sys.base_prefix, if in a virtualenv)
+    # - sys.real_prefix is main dir for virtualenvs in Python 2.7
+    # - in system32,
+    # - ortherwise (Sxs), I don't know how to get it.
+    stems = [sys.prefix]
+    if hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix:
+        stems.append(sys.base_prefix)
+    elif hasattr(sys, 'real_prefix') and sys.real_prefix != sys.prefix:
+        stems.append(sys.real_prefix)
+
+    sub_dirs = ['', 'lib', 'bin']
+    # generate possible combinations of directory trees and sub-directories
+    lib_dirs = []
+    for stem in stems:
+        for folder in sub_dirs:
+            lib_dirs.append(os.path.join(stem, folder))
+
+    # add system directory as well
+    if 'SYSTEMROOT' in os.environ:
+        lib_dirs.append(os.path.join(os.environ['SYSTEMROOT'], 'System32'))
+
+    # search in the file system for possible candidates
+    major_version, minor_version = tuple(sys.version_info[:2])
+    implementation = platform.python_implementation()
+    if implementation == 'CPython':
+        dllname = f'python{major_version}{minor_version}.dll'
+    elif implementation == 'PyPy':
+        dllname = f'libpypy{major_version}-c.dll'
+    else:
+        dllname = f'Unknown platform {implementation}' 
+    print("Looking for %s" % dllname)
+    for folder in lib_dirs:
+        dll = os.path.join(folder, dllname)
+        if os.path.exists(dll):
+            return dll
+ 
+    raise ValueError("%s not found in %s" % (dllname, lib_dirs))
+
+def dump_table(dll):
+    st = subprocess.check_output(["objdump.exe", "-p", dll])
+    return st.split(b'\n')
+
+def generate_def(dll, dfile):
+    """Given a dll file location,  get all its exported symbols and dump them
+    into the given def file.
+
+    The .def file will be overwritten"""
+    dump = dump_table(dll)
+    for i in range(len(dump)):
+        if _START.match(dump[i].decode()):
+            break
+    else:
+        raise ValueError("Symbol table not found")
+
+    syms = []
+    for j in range(i+1, len(dump)):
+        m = _TABLE.match(dump[j].decode())
+        if m:
+            syms.append((int(m.group(1).strip()), m.group(2)))
+        else:
+            break
+
+    if len(syms) == 0:
+        log.warn('No symbols found in %s' % dll)
+
+    with open(dfile, 'w') as d:
+        d.write('LIBRARY        %s\n' % os.path.basename(dll))
+        d.write(';CODE          PRELOAD MOVEABLE DISCARDABLE\n')
+        d.write(';DATA          PRELOAD SINGLE\n')
+        d.write('\nEXPORTS\n')
+        for s in syms:
+            #d.write('@%d    %s\n' % (s[0], s[1]))
+            d.write('%s\n' % s[1])
+
+def find_dll(dll_name):
+
+    arch = {'AMD64' : 'amd64',
+            'Intel' : 'x86'}[get_build_architecture()]
+
+    def _find_dll_in_winsxs(dll_name):
+        # Walk through the WinSxS directory to find the dll.
+        winsxs_path = os.path.join(os.environ.get('WINDIR', r'C:\WINDOWS'),
+                                   'winsxs')
+        if not os.path.exists(winsxs_path):
+            return None
+        for root, dirs, files in os.walk(winsxs_path):
+            if dll_name in files and arch in root:
+                return os.path.join(root, dll_name)
+        return None
+
+    def _find_dll_in_path(dll_name):
+        # First, look in the Python directory, then scan PATH for
+        # the given dll name.
+        for path in [sys.prefix] + os.environ['PATH'].split(';'):
+            filepath = os.path.join(path, dll_name)
+            if os.path.exists(filepath):
+                return os.path.abspath(filepath)
+
+    return _find_dll_in_winsxs(dll_name) or _find_dll_in_path(dll_name)
+
+def build_msvcr_library(debug=False):
+    if os.name != 'nt':
+        return False
+
+    # If the version number is None, then we couldn't find the MSVC runtime at
+    # all, because we are running on a Python distribution which is customed
+    # compiled; trust that the compiler is the same as the one available to us
+    # now, and that it is capable of linking with the correct runtime without
+    # any extra options.
+    msvcr_ver = msvc_runtime_major()
+    if msvcr_ver is None:
+        log.debug('Skip building import library: '
+                  'Runtime is not compiled with MSVC')
+        return False
+
+    # Skip using a custom library for versions < MSVC 8.0
+    if msvcr_ver < 80:
+        log.debug('Skip building msvcr library:'
+                  ' custom functionality not present')
+        return False
+
+    msvcr_name = msvc_runtime_library()
+    if debug:
+        msvcr_name += 'd'
+
+    # Skip if custom library already exists
+    out_name = "lib%s.a" % msvcr_name
+    out_file = os.path.join(sys.prefix, 'libs', out_name)
+    if os.path.isfile(out_file):
+        log.debug('Skip building msvcr library: "%s" exists' %
+                  (out_file,))
+        return True
+
+    # Find the msvcr dll
+    msvcr_dll_name = msvcr_name + '.dll'
+    dll_file = find_dll(msvcr_dll_name)
+    if not dll_file:
+        log.warn('Cannot build msvcr library: "%s" not found' %
+                 msvcr_dll_name)
+        return False
+
+    def_name = "lib%s.def" % msvcr_name
+    def_file = os.path.join(sys.prefix, 'libs', def_name)
+
+    log.info('Building msvcr library: "%s" (from %s)' \
+             % (out_file, dll_file))
+
+    # Generate a symbol definition file from the msvcr dll
+    generate_def(dll_file, def_file)
+
+    # Create a custom mingw library for the given symbol definitions
+    cmd = ['dlltool', '-d', def_file, '-l', out_file]
+    retcode = subprocess.call(cmd)
+
+    # Clean up symbol definitions
+    os.remove(def_file)
+
+    return (not retcode)
+
 def build_import_library():
+    if os.name != 'nt':
+        return
+
+    arch = get_build_architecture()
+    if arch == 'AMD64':
+        return _build_import_library_amd64()
+    elif arch == 'Intel':
+        return _build_import_library_x86()
+    else:
+        raise ValueError("Unhandled arch %s" % arch)
+
+def _check_for_import_lib():
+    """Check if an import library for the Python runtime already exists."""
+    major_version, minor_version = tuple(sys.version_info[:2])
+
+    # patterns for the file name of the library itself
+    patterns = ['libpython%d%d.a',
+                'libpython%d%d.dll.a',
+                'libpython%d.%d.dll.a']
+
+    # directory trees that may contain the library
+    stems = [sys.prefix]
+    if hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix:
+        stems.append(sys.base_prefix)
+    elif hasattr(sys, 'real_prefix') and sys.real_prefix != sys.prefix:
+        stems.append(sys.real_prefix)
+
+    # possible subdirectories within those trees where it is placed
+    sub_dirs = ['libs', 'lib']
+
+    # generate a list of candidate locations
+    candidates = []
+    for pat in patterns:
+        filename = pat % (major_version, minor_version)
+        for stem_dir in stems:
+            for folder in sub_dirs:
+                candidates.append(os.path.join(stem_dir, folder, filename))
+
+    # test the filesystem to see if we can find any of these
+    for fullname in candidates:
+        if os.path.isfile(fullname):
+            # already exists, in location given
+            return (True, fullname)
+
+    # needs to be built, preferred location given first
+    return (False, candidates[0])
+
+def _build_import_library_amd64():
+    out_exists, out_file = _check_for_import_lib()
+    if out_exists:
+        log.debug('Skip building import library: "%s" exists', out_file)
+        return
+
+    # get the runtime dll for which we are building import library
+    dll_file = find_python_dll()
+    log.info('Building import library (arch=AMD64): "%s" (from %s)' %
+             (out_file, dll_file))
+
+    # generate symbol list from this library
+    def_name = "python%d%d.def" % tuple(sys.version_info[:2])
+    def_file = os.path.join(sys.prefix, 'libs', def_name)
+    generate_def(dll_file, def_file)
+
+    # generate import library from this symbol list
+    cmd = ['dlltool', '-d', def_file, '-l', out_file]
+    subprocess.check_call(cmd)
+
+def _build_import_library_x86():
     """ Build the import libraries for Mingw32-gcc on Windows
     """
-    if os.name != 'nt':
+    out_exists, out_file = _check_for_import_lib()
+    if out_exists:
+        log.debug('Skip building import library: "%s" exists', out_file)
         return
+
     lib_name = "python%d%d.lib" % tuple(sys.version_info[:2])
-    lib_file = os.path.join(sys.prefix,'libs',lib_name)
-    out_name = "libpython%d%d.a" % tuple(sys.version_info[:2])
-    out_file = os.path.join(sys.prefix,'libs',out_name)
+    lib_file = os.path.join(sys.prefix, 'libs', lib_name)
     if not os.path.isfile(lib_file):
-        log.warn('Cannot build import library: "%s" not found' % (lib_file))
-        return
-    if os.path.isfile(out_file):
-        log.debug('Skip building import library: "%s" exists' % (out_file))
-        return
-    log.info('Building import library: "%s"' % (out_file))
+        # didn't find library file in virtualenv, try base distribution, too,
+        # and use that instead if found there. for Python 2.7 venvs, the base
+        # directory is in attribute real_prefix instead of base_prefix.
+        if hasattr(sys, 'base_prefix'):
+            base_lib = os.path.join(sys.base_prefix, 'libs', lib_name)
+        elif hasattr(sys, 'real_prefix'):
+            base_lib = os.path.join(sys.real_prefix, 'libs', lib_name)
+        else:
+            base_lib = ''  # os.path.isfile('') == False
+
+        if os.path.isfile(base_lib):
+            lib_file = base_lib
+        else:
+            log.warn('Cannot build import library: "%s" not found', lib_file)
+            return
+    log.info('Building import library (ARCH=x86): "%s"', out_file)
 
     from numpy.distutils import lib2def
 
     def_name = "python%d%d.def" % tuple(sys.version_info[:2])
-    def_file = os.path.join(sys.prefix,'libs',def_name)
-    nm_cmd = '%s %s' % (lib2def.DEFAULT_NM, lib_file)
-    nm_output = lib2def.getnm(nm_cmd)
+    def_file = os.path.join(sys.prefix, 'libs', def_name)
+    nm_output = lib2def.getnm(
+            lib2def.DEFAULT_NM + [lib_file], shell=False)
     dlist, flist = lib2def.parse_nm(nm_output)
-    lib2def.output_def(dlist, flist, lib2def.DEF_HEADER, open(def_file, 'w'))
-
-    dll_name = "python%d%d.dll" % tuple(sys.version_info[:2])
-    args = (dll_name,def_file,out_file)
-    cmd = 'dlltool --dllname %s --def %s --output-lib %s' % args
-    status = os.system(cmd)
-    # for now, fail silently
+    with open(def_file, 'w') as fid:
+        lib2def.output_def(dlist, flist, lib2def.DEF_HEADER, fid)
+
+    dll_name = find_python_dll ()
+
+    cmd = ["dlltool",
+           "--dllname", dll_name,
+           "--def", def_file,
+           "--output-lib", out_file]
+    status = subprocess.check_output(cmd)
     if status:
         log.warn('Failed to build import library for gcc. Linking will fail.')
-    #if not success:
-    #    msg = "Couldn't find import library, and failed to build it."
-    #    raise DistutilsPlatformError, msg
     return
+
+#=====================================
+# Dealing with Visual Studio MANIFESTS
+#=====================================
+
+# Functions to deal with visual studio manifests. Manifest are a mechanism to
+# enforce strong DLL versioning on windows, and has nothing to do with
+# distutils MANIFEST. manifests are XML files with version info, and used by
+# the OS loader; they are necessary when linking against a DLL not in the
+# system path; in particular, official python 2.6 binary is built against the
+# MS runtime 9 (the one from VS 2008), which is not available on most windows
+# systems; python 2.6 installer does install it in the Win SxS (Side by side)
+# directory, but this requires the manifest for this to work. This is a big
+# mess, thanks MS for a wonderful system.
+
+# XXX: ideally, we should use exactly the same version as used by python. I
+# submitted a patch to get this version, but it was only included for python
+# 2.6.1 and above. So for versions below, we use a "best guess".
+_MSVCRVER_TO_FULLVER = {}
+if sys.platform == 'win32':
+    try:
+        import msvcrt
+        # I took one version in my SxS directory: no idea if it is the good
+        # one, and we can't retrieve it from python
+        _MSVCRVER_TO_FULLVER['80'] = "8.0.50727.42"
+        _MSVCRVER_TO_FULLVER['90'] = "9.0.21022.8"
+        # Value from msvcrt.CRT_ASSEMBLY_VERSION under Python 3.3.0
+        # on Windows XP:
+        _MSVCRVER_TO_FULLVER['100'] = "10.0.30319.460"
+        crt_ver = getattr(msvcrt, 'CRT_ASSEMBLY_VERSION', None)
+        if crt_ver is not None:  # Available at least back to Python 3.3
+            maj, min = re.match(r'(\d+)\.(\d)', crt_ver).groups()
+            _MSVCRVER_TO_FULLVER[maj + min] = crt_ver
+            del maj, min
+        del crt_ver
+    except ImportError:
+        # If we are here, means python was not built with MSVC. Not sure what
+        # to do in that case: manifest building will fail, but it should not be
+        # used in that case anyway
+        log.warn('Cannot import msvcrt: using manifest will not be possible')
+
+def msvc_manifest_xml(maj, min):
+    """Given a major and minor version of the MSVCR, returns the
+    corresponding XML file."""
+    try:
+        fullver = _MSVCRVER_TO_FULLVER[str(maj * 10 + min)]
+    except KeyError:
+        raise ValueError("Version %d,%d of MSVCRT not supported yet" %
+                         (maj, min)) from None
+    # Don't be fooled, it looks like an XML, but it is not. In particular, it
+    # should not have any space before starting, and its size should be
+    # divisible by 4, most likely for alignment constraints when the xml is
+    # embedded in the binary...
+    # This template was copied directly from the python 2.6 binary (using
+    # strings.exe from mingw on python.exe).
+    template = textwrap.dedent("""\
+        <assembly xmlns="urn:schemas-microsoft-com:asm.v1" manifestVersion="1.0">
+          <trustInfo xmlns="urn:schemas-microsoft-com:asm.v3">
+            <security>
+              <requestedPrivileges>
+                <requestedExecutionLevel level="asInvoker" uiAccess="false"></requestedExecutionLevel>
+              </requestedPrivileges>
+            </security>
+          </trustInfo>
+          <dependency>
+            <dependentAssembly>
+              <assemblyIdentity type="win32" name="Microsoft.VC%(maj)d%(min)d.CRT" version="%(fullver)s" processorArchitecture="*" publicKeyToken="1fc8b3b9a1e18e3b"></assemblyIdentity>
+            </dependentAssembly>
+          </dependency>
+        </assembly>""")
+
+    return template % {'fullver': fullver, 'maj': maj, 'min': min}
+
+def manifest_rc(name, type='dll'):
+    """Return the rc file used to generate the res file which will be embedded
+    as manifest for given manifest file name, of given type ('dll' or
+    'exe').
+
+    Parameters
+    ----------
+    name : str
+            name of the manifest file to embed
+    type : str {'dll', 'exe'}
+            type of the binary which will embed the manifest
+
+    """
+    if type == 'dll':
+        rctype = 2
+    elif type == 'exe':
+        rctype = 1
+    else:
+        raise ValueError("Type %s not supported" % type)
+
+    return """\
+#include "winuser.h"
+%d RT_MANIFEST %s""" % (rctype, name)
+
+def check_embedded_msvcr_match_linked(msver):
+    """msver is the ms runtime version used for the MANIFEST."""
+    # check msvcr major version are the same for linking and
+    # embedding
+    maj = msvc_runtime_major()
+    if maj:
+        if not maj == int(msver):
+            raise ValueError(
+                  "Discrepancy between linked msvcr " \
+                  "(%d) and the one about to be embedded " \
+                  "(%d)" % (int(msver), maj))
+
+def configtest_name(config):
+    base = os.path.basename(config._gen_temp_sourcefile("yo", [], "c"))
+    return os.path.splitext(base)[0]
+
+def manifest_name(config):
+    # Get configest name (including suffix)
+    root = configtest_name(config)
+    exext = config.compiler.exe_extension
+    return root + exext + ".manifest"
+
+def rc_name(config):
+    # Get configtest name (including suffix)
+    root = configtest_name(config)
+    return root + ".rc"
+
+def generate_manifest(config):
+    msver = get_build_msvc_version()
+    if msver is not None:
+        if msver >= 8:
+            check_embedded_msvcr_match_linked(msver)
+            ma_str, mi_str = str(msver).split('.')
+            # Write the manifest file
+            manxml = msvc_manifest_xml(int(ma_str), int(mi_str))
+            with open(manifest_name(config), "w") as man:
+                config.temp_files.append(manifest_name(config))
+                man.write(manxml)
('numpy/distutils', 'setup.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,15 +1,17 @@
-#!/usr/bin/env python
-
+#!/usr/bin/env python3
 def configuration(parent_package='',top_path=None):
     from numpy.distutils.misc_util import Configuration
-    config = Configuration('distutils',parent_package,top_path)
+    config = Configuration('distutils', parent_package, top_path)
     config.add_subpackage('command')
     config.add_subpackage('fcompiler')
-    config.add_data_dir('tests')
+    config.add_subpackage('tests')
     config.add_data_files('site.cfg')
+    config.add_data_files('mingw/gfortran_vs2003_hack.c')
+    config.add_data_dir('checks')
+    config.add_data_files('*.pyi')
     config.make_config_py()
     return config
 
 if __name__ == '__main__':
     from numpy.distutils.core      import setup
-    setup(**configuration(top_path='').todict())
+    setup(configuration=configuration)
('numpy/distutils', 'extension.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -4,50 +4,79 @@
 modules in setup scripts.
 
 Overridden to support f2py.
+
 """
-
-__revision__ = "$Id: extension.py,v 1.1 2005/04/09 19:29:34 pearu Exp $"
-
+import re
 from distutils.extension import Extension as old_Extension
 
-import re
-cxx_ext_re = re.compile(r'.*[.](cpp|cxx|cc)\Z',re.I).match
-fortran_pyf_ext_re = re.compile(r'.*[.](f90|f95|f77|for|ftn|f|pyf)\Z',re.I).match
+
+cxx_ext_re = re.compile(r'.*\.(cpp|cxx|cc)\Z', re.I).match
+fortran_pyf_ext_re = re.compile(r'.*\.(f90|f95|f77|for|ftn|f|pyf)\Z', re.I).match
+
 
 class Extension(old_Extension):
-    def __init__ (self, name, sources,
-                  include_dirs=None,
-                  define_macros=None,
-                  undef_macros=None,
-                  library_dirs=None,
-                  libraries=None,
-                  runtime_library_dirs=None,
-                  extra_objects=None,
-                  extra_compile_args=None,
-                  extra_link_args=None,
-                  export_symbols=None,
-                  swig_opts=None,
-                  depends=None,
-                  language=None,
-                  f2py_options=None,
-                  module_dirs=None,
-                 ):
-        old_Extension.__init__(self,name, [],
-                               include_dirs,
-                               define_macros,
-                               undef_macros,
-                               library_dirs,
-                               libraries,
-                               runtime_library_dirs,
-                               extra_objects,
-                               extra_compile_args,
-                               extra_link_args,
-                               export_symbols)
+    """
+    Parameters
+    ----------
+    name : str
+        Extension name.
+    sources : list of str
+        List of source file locations relative to the top directory of
+        the package.
+    extra_compile_args : list of str
+        Extra command line arguments to pass to the compiler.
+    extra_f77_compile_args : list of str
+        Extra command line arguments to pass to the fortran77 compiler.
+    extra_f90_compile_args : list of str
+        Extra command line arguments to pass to the fortran90 compiler.
+    """
+    def __init__(
+            self, name, sources,
+            include_dirs=None,
+            define_macros=None,
+            undef_macros=None,
+            library_dirs=None,
+            libraries=None,
+            runtime_library_dirs=None,
+            extra_objects=None,
+            extra_compile_args=None,
+            extra_link_args=None,
+            export_symbols=None,
+            swig_opts=None,
+            depends=None,
+            language=None,
+            f2py_options=None,
+            module_dirs=None,
+            extra_c_compile_args=None,
+            extra_cxx_compile_args=None,
+            extra_f77_compile_args=None,
+            extra_f90_compile_args=None,):
+
+        old_Extension.__init__(
+                self, name, [],
+                include_dirs=include_dirs,
+                define_macros=define_macros,
+                undef_macros=undef_macros,
+                library_dirs=library_dirs,
+                libraries=libraries,
+                runtime_library_dirs=runtime_library_dirs,
+                extra_objects=extra_objects,
+                extra_compile_args=extra_compile_args,
+                extra_link_args=extra_link_args,
+                export_symbols=export_symbols)
+
         # Avoid assert statements checking that sources contains strings:
         self.sources = sources
 
         # Python 2.4 distutils new features
         self.swig_opts = swig_opts or []
+        # swig_opts is assumed to be a list. Here we handle the case where it
+        # is specified as a string instead.
+        if isinstance(self.swig_opts, str):
+            import warnings
+            msg = "swig_opts is specified as a string instead of a list"
+            warnings.warn(msg, SyntaxWarning, stacklevel=2)
+            self.swig_opts = self.swig_opts.split()
 
         # Python 2.3 distutils new features
         self.depends = depends or []
@@ -56,6 +85,10 @@
         # numpy_distutils features
         self.f2py_options = f2py_options or []
         self.module_dirs = module_dirs or []
+        self.extra_c_compile_args = extra_c_compile_args or []
+        self.extra_cxx_compile_args = extra_cxx_compile_args or []
+        self.extra_f77_compile_args = extra_f77_compile_args or []
+        self.extra_f90_compile_args = extra_f90_compile_args or []
 
         return
 
('numpy/distutils', 'intelccompiler.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,30 +1,111 @@
+import platform
 
-import os
 from distutils.unixccompiler import UnixCCompiler
 from numpy.distutils.exec_command import find_executable
+from numpy.distutils.ccompiler import simple_version_match
+if platform.system() == 'Windows':
+    from numpy.distutils.msvc9compiler import MSVCCompiler
+
 
 class IntelCCompiler(UnixCCompiler):
-
-    """ A modified Intel compiler compatible with an gcc built Python.
-    """
-
+    """A modified Intel compiler compatible with a GCC-built Python."""
     compiler_type = 'intel'
     cc_exe = 'icc'
+    cc_args = 'fPIC'
 
-    def __init__ (self, verbose=0, dry_run=0, force=0):
-        UnixCCompiler.__init__ (self, verbose,dry_run, force)
+    def __init__(self, verbose=0, dry_run=0, force=0):
+        UnixCCompiler.__init__(self, verbose, dry_run, force)
+
+        v = self.get_version()
+        mpopt = 'openmp' if v and v < '15' else 'qopenmp'
+        self.cc_exe = ('icc -fPIC -fp-model strict -O3 '
+                       '-fomit-frame-pointer -{}').format(mpopt)
         compiler = self.cc_exe
+
+        if platform.system() == 'Darwin':
+            shared_flag = '-Wl,-undefined,dynamic_lookup'
+        else:
+            shared_flag = '-shared'
         self.set_executables(compiler=compiler,
                              compiler_so=compiler,
                              compiler_cxx=compiler,
-                             linker_exe=compiler,
-                             linker_so=compiler + ' -shared')
+                             archiver='xiar' + ' cru',
+                             linker_exe=compiler + ' -shared-intel',
+                             linker_so=compiler + ' ' + shared_flag +
+                             ' -shared-intel')
+
 
 class IntelItaniumCCompiler(IntelCCompiler):
     compiler_type = 'intele'
 
     # On Itanium, the Intel Compiler used to be called ecc, let's search for
     # it (now it's also icc, so ecc is last in the search).
-    for cc_exe in map(find_executable,['icc','ecc']):
-        if os.path.isfile(cc_exe):
+    for cc_exe in map(find_executable, ['icc', 'ecc']):
+        if cc_exe:
             break
+
+
+class IntelEM64TCCompiler(UnixCCompiler):
+    """
+    A modified Intel x86_64 compiler compatible with a 64bit GCC-built Python.
+    """
+    compiler_type = 'intelem'
+    cc_exe = 'icc -m64'
+    cc_args = '-fPIC'
+
+    def __init__(self, verbose=0, dry_run=0, force=0):
+        UnixCCompiler.__init__(self, verbose, dry_run, force)
+
+        v = self.get_version()
+        mpopt = 'openmp' if v and v < '15' else 'qopenmp'
+        self.cc_exe = ('icc -std=c99 -m64 -fPIC -fp-model strict -O3 '
+                       '-fomit-frame-pointer -{}').format(mpopt)
+        compiler = self.cc_exe
+
+        if platform.system() == 'Darwin':
+            shared_flag = '-Wl,-undefined,dynamic_lookup'
+        else:
+            shared_flag = '-shared'
+        self.set_executables(compiler=compiler,
+                             compiler_so=compiler,
+                             compiler_cxx=compiler,
+                             archiver='xiar' + ' cru',
+                             linker_exe=compiler + ' -shared-intel',
+                             linker_so=compiler + ' ' + shared_flag +
+                             ' -shared-intel')
+
+
+if platform.system() == 'Windows':
+    class IntelCCompilerW(MSVCCompiler):
+        """
+        A modified Intel compiler compatible with an MSVC-built Python.
+        """
+        compiler_type = 'intelw'
+        compiler_cxx = 'icl'
+
+        def __init__(self, verbose=0, dry_run=0, force=0):
+            MSVCCompiler.__init__(self, verbose, dry_run, force)
+            version_match = simple_version_match(start=r'Intel\(R\).*?32,')
+            self.__version = version_match
+
+        def initialize(self, plat_name=None):
+            MSVCCompiler.initialize(self, plat_name)
+            self.cc = self.find_exe('icl.exe')
+            self.lib = self.find_exe('xilib')
+            self.linker = self.find_exe('xilink')
+            self.compile_options = ['/nologo', '/O3', '/MD', '/W3',
+                                    '/Qstd=c99']
+            self.compile_options_debug = ['/nologo', '/Od', '/MDd', '/W3',
+                                          '/Qstd=c99', '/Z7', '/D_DEBUG']
+
+    class IntelEM64TCCompilerW(IntelCCompilerW):
+        """
+        A modified Intel x86_64 compiler compatible with
+        a 64bit MSVC-built Python.
+        """
+        compiler_type = 'intelemw'
+
+        def __init__(self, verbose=0, dry_run=0, force=0):
+            MSVCCompiler.__init__(self, verbose, dry_run, force)
+            version_match = simple_version_match(start=r'Intel\(R\).*?64,')
+            self.__version = version_match
('numpy/distutils/fcompiler', 'gnu.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,74 +1,137 @@
-
 import re
 import os
 import sys
 import warnings
-
-from numpy.distutils.cpuinfo import cpu
-from numpy.distutils.ccompiler import simple_version_match
+import platform
+import tempfile
+import hashlib
+import base64
+import subprocess
+from subprocess import Popen, PIPE, STDOUT
+from numpy.distutils.exec_command import filepath_from_subprocess_output
 from numpy.distutils.fcompiler import FCompiler
-from numpy.distutils.exec_command import exec_command, find_executable
-from numpy.distutils.misc_util import mingw32
+from distutils.version import LooseVersion
+
+compilers = ['GnuFCompiler', 'Gnu95FCompiler']
+
+TARGET_R = re.compile(r"Target: ([a-zA-Z0-9_\-]*)")
+
+# XXX: handle cross compilation
+
+
+def is_win64():
+    return sys.platform == "win32" and platform.architecture()[0] == "64bit"
+
 
 class GnuFCompiler(FCompiler):
-
     compiler_type = 'gnu'
-    version_match = simple_version_match(start=r'GNU Fortran (?!95)')
-
-    # 'g77 --version' results
-    # SunOS: GNU Fortran (GCC 3.2) 3.2 20020814 (release)
-    # Debian: GNU Fortran (GCC) 3.3.3 20040110 (prerelease) (Debian)
-    #         GNU Fortran (GCC) 3.3.3 (Debian 20040401)
-    #         GNU Fortran 0.5.25 20010319 (prerelease)
-    # Redhat: GNU Fortran (GCC 3.2.2 20030222 (Red Hat Linux 3.2.2-5)) 3.2.2 20030222 (Red Hat Linux 3.2.2-5)
-
-    for fc_exe in map(find_executable,['g77','f77']):
-        if os.path.isfile(fc_exe):
-            break
+    compiler_aliases = ('g77', )
+    description = 'GNU Fortran 77 compiler'
+
+    def gnu_version_match(self, version_string):
+        """Handle the different versions of GNU fortran compilers"""
+        # Strip warning(s) that may be emitted by gfortran
+        while version_string.startswith('gfortran: warning'):
+            version_string =\
+                version_string[version_string.find('\n') + 1:].strip()
+
+        # Gfortran versions from after 2010 will output a simple string
+        # (usually "x.y", "x.y.z" or "x.y.z-q") for ``-dumpversion``; older
+        # gfortrans may still return long version strings (``-dumpversion`` was
+        # an alias for ``--version``)
+        if len(version_string) <= 20:
+            # Try to find a valid version string
+            m = re.search(r'([0-9.]+)', version_string)
+            if m:
+                # g77 provides a longer version string that starts with GNU
+                # Fortran
+                if version_string.startswith('GNU Fortran'):
+                    return ('g77', m.group(1))
+
+                # gfortran only outputs a version string such as #.#.#, so check
+                # if the match is at the start of the string
+                elif m.start() == 0:
+                    return ('gfortran', m.group(1))
+        else:
+            # Output probably from --version, try harder:
+            m = re.search(r'GNU Fortran\s+95.*?([0-9-.]+)', version_string)
+            if m:
+                return ('gfortran', m.group(1))
+            m = re.search(
+                r'GNU Fortran.*?\-?([0-9-.]+\.[0-9-.]+)', version_string)
+            if m:
+                v = m.group(1)
+                if v.startswith('0') or v.startswith('2') or v.startswith('3'):
+                    # the '0' is for early g77's
+                    return ('g77', v)
+                else:
+                    # at some point in the 4.x series, the ' 95' was dropped
+                    # from the version string
+                    return ('gfortran', v)
+
+        # If still nothing, raise an error to make the problem easy to find.
+        err = 'A valid Fortran version was not found in this string:\n'
+        raise ValueError(err + version_string)
+
+    def version_match(self, version_string):
+        v = self.gnu_version_match(version_string)
+        if not v or v[0] != 'g77':
+            return None
+        return v[1]
+
+    possible_executables = ['g77', 'f77']
     executables = {
-        'version_cmd'  : [fc_exe,"--version"],
-        'compiler_f77' : [fc_exe,"-Wall","-fno-second-underscore"],
-        'compiler_f90' : None,
+        'version_cmd'  : [None, "-dumpversion"],
+        'compiler_f77' : [None, "-g", "-Wall", "-fno-second-underscore"],
+        'compiler_f90' : None,  # Use --fcompiler=gnu95 for f90 codes
         'compiler_fix' : None,
-        'linker_so'    : [fc_exe,"-Wall"],
+        'linker_so'    : [None, "-g", "-Wall"],
         'archiver'     : ["ar", "-cr"],
         'ranlib'       : ["ranlib"],
-        'linker_exe'   : [fc_exe,"-Wall"]
-        }
+        'linker_exe'   : [None, "-g", "-Wall"]
+    }
     module_dir_switch = None
     module_include_switch = None
 
-    # Cygwin: f771: warning: -fPIC ignored for target (all code is position independent)
-    if os.name != 'nt' and sys.platform!='cygwin':
+    # Cygwin: f771: warning: -fPIC ignored for target (all code is
+    # position independent)
+    if os.name != 'nt' and sys.platform != 'cygwin':
         pic_flags = ['-fPIC']
 
+    # use -mno-cygwin for g77 when Python is not Cygwin-Python
+    if sys.platform == 'win32':
+        for key in ['version_cmd', 'compiler_f77', 'linker_so', 'linker_exe']:
+            executables[key].append('-mno-cygwin')
+
     g2c = 'g2c'
-
-    #def get_linker_so(self):
-    #    # win32 linking should be handled by standard linker
-    #    # Darwin g77 cannot be used as a linker.
-    #    #if re.match(r'(darwin)', sys.platform):
-    #    #    return
-    #    return FCompiler.get_linker_so(self)
+    suggested_f90_compiler = 'gnu95'
 
     def get_flags_linker_so(self):
-        opt = []
-        if sys.platform=='darwin':
+        opt = self.linker_so[1:]
+        if sys.platform == 'darwin':
             target = os.environ.get('MACOSX_DEPLOYMENT_TARGET', None)
-            if target is None:
-                target = '10.3'
-            major, minor = target.split('.')
-            if int(minor) < 3:
-                minor = '3'
-                warnings.warn('Environment variable '
-                    'MACOSX_DEPLOYMENT_TARGET reset to 10.3')
-            os.environ['MACOSX_DEPLOYMENT_TARGET'] = '%s.%s' % (major,
-                minor)
-
+            # If MACOSX_DEPLOYMENT_TARGET is set, we simply trust the value
+            # and leave it alone.  But, distutils will complain if the
+            # environment's value is different from the one in the Python
+            # Makefile used to build Python.  We let distutils handle this
+            # error checking.
+            if not target:
+                # If MACOSX_DEPLOYMENT_TARGET is not set in the environment,
+                # we try to get it first from sysconfig and then
+                # fall back to setting it to 10.9 This is a reasonable default
+                # even when using the official Python dist and those derived
+                # from it.
+                import sysconfig
+                target = sysconfig.get_config_var('MACOSX_DEPLOYMENT_TARGET')
+                if not target:
+                    target = '10.9'
+                    s = f'Env. variable MACOSX_DEPLOYMENT_TARGET set to {target}'
+                    warnings.warn(s, stacklevel=2)
+                os.environ['MACOSX_DEPLOYMENT_TARGET'] = str(target)
             opt.extend(['-undefined', 'dynamic_lookup', '-bundle'])
         else:
             opt.append("-shared")
-        if sys.platform[:5]=='sunos':
+        if sys.platform.startswith('sunos'):
             # SunOS often has dynamically loaded symbols defined in the
             # static library libg2c.a  The linker doesn't like this.  To
             # ignore the problem, use the -mimpure-text flag.  It isn't
@@ -79,18 +142,57 @@
         return opt
 
     def get_libgcc_dir(self):
-        status, output = exec_command('%s -print-libgcc-file-name' \
-                                      % (self.compiler_f77[0]),use_tee=0)
-        if not status:
+        try:
+            output = subprocess.check_output(self.compiler_f77 +
+                                            ['-print-libgcc-file-name'])
+        except (OSError, subprocess.CalledProcessError):
+            pass
+        else:
+            output = filepath_from_subprocess_output(output)
             return os.path.dirname(output)
-        return
+        return None
+
+    def get_libgfortran_dir(self):
+        if sys.platform[:5] == 'linux':
+            libgfortran_name = 'libgfortran.so'
+        elif sys.platform == 'darwin':
+            libgfortran_name = 'libgfortran.dylib'
+        else:
+            libgfortran_name = None
+
+        libgfortran_dir = None
+        if libgfortran_name:
+            find_lib_arg = ['-print-file-name={0}'.format(libgfortran_name)]
+            try:
+                output = subprocess.check_output(
+                                       self.compiler_f77 + find_lib_arg)
+            except (OSError, subprocess.CalledProcessError):
+                pass
+            else:
+                output = filepath_from_subprocess_output(output)
+                libgfortran_dir = os.path.dirname(output)
+        return libgfortran_dir
 
     def get_library_dirs(self):
         opt = []
         if sys.platform[:5] != 'linux':
             d = self.get_libgcc_dir()
             if d:
+                # if windows and not cygwin, libg2c lies in a different folder
+                if sys.platform == 'win32' and not d.startswith('/usr/lib'):
+                    d = os.path.normpath(d)
+                    path = os.path.join(d, "lib%s.a" % self.g2c)
+                    if not os.path.exists(path):
+                        root = os.path.join(d, *((os.pardir, ) * 4))
+                        d2 = os.path.abspath(os.path.join(root, 'lib'))
+                        path = os.path.join(d2, "lib%s.a" % self.g2c)
+                        if os.path.exists(path):
+                            opt.append(d2)
                 opt.append(d)
+        # For Macports / Linux, libgfortran and libgcc are not co-located
+        lib_gfortran_dir = self.get_libgfortran_dir()
+        if lib_gfortran_dir:
+            opt.append(lib_gfortran_dir)
         return opt
 
     def get_libraries(self):
@@ -99,18 +201,17 @@
         if d is not None:
             g2c = self.g2c + '-pic'
             f = self.static_lib_format % (g2c, self.static_lib_extension)
-            if not os.path.isfile(os.path.join(d,f)):
+            if not os.path.isfile(os.path.join(d, f)):
                 g2c = self.g2c
         else:
             g2c = self.g2c
 
-        if sys.platform=='win32':
-            # To avoid undefined reference __EH_FRAME_BEGIN__ linker error,
-            # don't use -lgcc option for mingw32 g77 linker.
-            if not mingw32():
-                opt.append('gcc')
         if g2c is not None:
             opt.append(g2c)
+        c_compiler = self.c_compiler
+        if sys.platform == 'win32' and c_compiler and \
+                c_compiler.compiler_type == 'msvc':
+            opt.append('gcc')
         if sys.platform == 'darwin':
             opt.append('cc_dynamic')
         return opt
@@ -119,7 +220,8 @@
         return ['-g']
 
     def get_flags_opt(self):
-        if self.get_version()<='3.3.3':
+        v = self.get_version()
+        if v and v <= '3.3.3':
             # With this compiler version building Fortran BLAS/LAPACK
             # with -O3 caused failures in lib.lapack heevr,syevr tests.
             opt = ['-O2']
@@ -128,136 +230,320 @@
         opt.append('-funroll-loops')
         return opt
 
+    def _c_arch_flags(self):
+        """ Return detected arch flags from CFLAGS """
+        import sysconfig
+        try:
+            cflags = sysconfig.get_config_vars()['CFLAGS']
+        except KeyError:
+            return []
+        arch_re = re.compile(r"-arch\s+(\w+)")
+        arch_flags = []
+        for arch in arch_re.findall(cflags):
+            arch_flags += ['-arch', arch]
+        return arch_flags
+
     def get_flags_arch(self):
-        opt = []
-        if sys.platform=='darwin':
-            if os.name != 'posix':
-                # this should presumably correspond to Apple
-                if cpu.is_ppc():
-                    opt.append('-arch ppc')
-                elif cpu.is_i386():
-                    opt.append('-arch i386')
-            for a in '601 602 603 603e 604 604e 620 630 740 7400 7450 750'\
-                    '403 505 801 821 823 860'.split():
-                if getattr(cpu,'is_ppc%s'%a)():
-                    opt.append('-mcpu='+a)
-                    opt.append('-mtune='+a)
-                    break
-            return opt
-
-        # default march options in case we find nothing better
-        if cpu.is_i686():
-            march_opt = '-march=i686'
-        elif cpu.is_i586():
-            march_opt = '-march=i586'
-        elif cpu.is_i486():
-            march_opt = '-march=i486'
-        elif cpu.is_i386():
-            march_opt = '-march=i386'
-        else:
-            march_opt = ''
-
-        gnu_ver =  self.get_version()
-
-        if gnu_ver >= '0.5.26': # gcc 3.0
-            if cpu.is_AthlonK6():
-                march_opt = '-march=k6'
-            elif cpu.is_AthlonK7():
-                march_opt = '-march=athlon'
-
-        if gnu_ver >= '3.1.1':
-            if cpu.is_AthlonK6_2():
-                march_opt = '-march=k6-2'
-            elif cpu.is_AthlonK6_3():
-                march_opt = '-march=k6-3'
-            elif cpu.is_AthlonMP():
-                march_opt = '-march=athlon-mp'
-                # there's also: athlon-tbird, athlon-4, athlon-xp
-            elif cpu.is_Nocona():
-                march_opt = '-march=nocona'
-            elif cpu.is_Prescott():
-                march_opt = '-march=prescott'
-            elif cpu.is_PentiumIV():
-                march_opt = '-march=pentium4'
-            elif cpu.is_PentiumIII():
-                march_opt = '-march=pentium3'
-            elif cpu.is_PentiumM():
-                march_opt = '-march=pentium3'
-            elif cpu.is_PentiumII():
-                march_opt = '-march=pentium2'
-
-        if gnu_ver >= '3.4':
-            if cpu.is_Opteron():
-                march_opt = '-march=opteron'
-            elif cpu.is_Athlon64():
-                march_opt = '-march=athlon64'
-
-        if gnu_ver >= '3.4.4':
-            if cpu.is_PentiumM():
-                march_opt = '-march=pentium-m'
-
-        # Note: gcc 3.2 on win32 has breakage with -march specified
-        if '3.1.1' <= gnu_ver <= '3.4' and sys.platform=='win32':
-            march_opt = ''
-
-        if march_opt:
-            opt.append(march_opt)
-
-        # other CPU flags
-        if gnu_ver >= '3.1.1':
-            if cpu.has_mmx(): opt.append('-mmmx')
-            if cpu.has_3dnow(): opt.append('-m3dnow')
-
-        if gnu_ver > '3.2.2':
-            if cpu.has_sse2(): opt.append('-msse2')
-            if cpu.has_sse(): opt.append('-msse')
-        if gnu_ver >= '3.4':
-            if cpu.has_sse3(): opt.append('-msse3')
-        if cpu.is_Intel():
-            opt.append('-fomit-frame-pointer')
-            if cpu.is_32bit():
-                opt.append('-malign-double')
-        return opt
+        return []
+
+    def runtime_library_dir_option(self, dir):
+        if sys.platform == 'win32' or sys.platform == 'cygwin':
+            # Linux/Solaris/Unix support RPATH, Windows does not
+            raise NotImplementedError
+
+        # TODO: could use -Xlinker here, if it's supported
+        assert "," not in dir
+
+        if sys.platform == 'darwin':
+            return f'-Wl,-rpath,{dir}'
+        elif sys.platform[:3] == 'aix':
+            # AIX RPATH is called LIBPATH
+            return f'-Wl,-blibpath:{dir}'
+        else:
+            return f'-Wl,-rpath={dir}'
+
 
 class Gnu95FCompiler(GnuFCompiler):
-
     compiler_type = 'gnu95'
-    version_match = simple_version_match(start='GNU Fortran 95')
-
-    # 'gfortran --version' results:
-    # Debian: GNU Fortran 95 (GCC 4.0.3 20051023 (prerelease) (Debian 4.0.2-3))
-    # OS X: GNU Fortran 95 (GCC) 4.1.0
-    #       GNU Fortran 95 (GCC) 4.2.0 20060218 (experimental)
-
-    for fc_exe in map(find_executable,['gfortran','f95']):
-        if os.path.isfile(fc_exe):
-            break
+    compiler_aliases = ('gfortran', )
+    description = 'GNU Fortran 95 compiler'
+
+    def version_match(self, version_string):
+        v = self.gnu_version_match(version_string)
+        if not v or v[0] != 'gfortran':
+            return None
+        v = v[1]
+        if LooseVersion(v) >= "4":
+            # gcc-4 series releases do not support -mno-cygwin option
+            pass
+        else:
+            # use -mno-cygwin flag for gfortran when Python is not
+            # Cygwin-Python
+            if sys.platform == 'win32':
+                for key in [
+                        'version_cmd', 'compiler_f77', 'compiler_f90',
+                        'compiler_fix', 'linker_so', 'linker_exe'
+                ]:
+                    self.executables[key].append('-mno-cygwin')
+        return v
+
+    possible_executables = ['gfortran', 'f95']
     executables = {
-        'version_cmd'  : [fc_exe,"--version"],
-        'compiler_f77' : [fc_exe,"-Wall","-ffixed-form","-fno-second-underscore"],
-        'compiler_f90' : [fc_exe,"-Wall","-fno-second-underscore"],
-        'compiler_fix' : [fc_exe,"-Wall","-ffixed-form","-fno-second-underscore"],
-        'linker_so'    : [fc_exe,"-Wall"],
+        'version_cmd'  : ["<F90>", "-dumpversion"],
+        'compiler_f77' : [None, "-Wall", "-g", "-ffixed-form",
+                          "-fno-second-underscore"],
+        'compiler_f90' : [None, "-Wall", "-g",
+                          "-fno-second-underscore"],
+        'compiler_fix' : [None, "-Wall",  "-g","-ffixed-form",
+                          "-fno-second-underscore"],
+        'linker_so'    : ["<F90>", "-Wall", "-g"],
         'archiver'     : ["ar", "-cr"],
         'ranlib'       : ["ranlib"],
-        'linker_exe'   : [fc_exe,"-Wall"]
-        }
+        'linker_exe'   : [None, "-Wall"]
+    }
+
     module_dir_switch = '-J'
     module_include_switch = '-I'
 
+    if sys.platform[:3] == 'aix':
+        executables['linker_so'].append('-lpthread')
+        if platform.architecture()[0][:2] == '64':
+            for key in ['compiler_f77', 'compiler_f90','compiler_fix','linker_so', 'linker_exe']:
+                executables[key].append('-maix64')
+
     g2c = 'gfortran'
+
+    def _universal_flags(self, cmd):
+        """Return a list of -arch flags for every supported architecture."""
+        if not sys.platform == 'darwin':
+            return []
+        arch_flags = []
+        # get arches the C compiler gets.
+        c_archs = self._c_arch_flags()
+        if "i386" in c_archs:
+            c_archs[c_archs.index("i386")] = "i686"
+        # check the arches the Fortran compiler supports, and compare with
+        # arch flags from C compiler
+        for arch in ["ppc", "i686", "x86_64", "ppc64", "s390x"]:
+            if _can_target(cmd, arch) and arch in c_archs:
+                arch_flags.extend(["-arch", arch])
+        return arch_flags
+
+    def get_flags(self):
+        flags = GnuFCompiler.get_flags(self)
+        arch_flags = self._universal_flags(self.compiler_f90)
+        if arch_flags:
+            flags[:0] = arch_flags
+        return flags
+
+    def get_flags_linker_so(self):
+        flags = GnuFCompiler.get_flags_linker_so(self)
+        arch_flags = self._universal_flags(self.linker_so)
+        if arch_flags:
+            flags[:0] = arch_flags
+        return flags
+
+    def get_library_dirs(self):
+        opt = GnuFCompiler.get_library_dirs(self)
+        if sys.platform == 'win32':
+            c_compiler = self.c_compiler
+            if c_compiler and c_compiler.compiler_type == "msvc":
+                target = self.get_target()
+                if target:
+                    d = os.path.normpath(self.get_libgcc_dir())
+                    root = os.path.join(d, *((os.pardir, ) * 4))
+                    path = os.path.join(root, "lib")
+                    mingwdir = os.path.normpath(path)
+                    if os.path.exists(os.path.join(mingwdir, "libmingwex.a")):
+                        opt.append(mingwdir)
+        # For Macports / Linux, libgfortran and libgcc are not co-located
+        lib_gfortran_dir = self.get_libgfortran_dir()
+        if lib_gfortran_dir:
+            opt.append(lib_gfortran_dir)
+        return opt
 
     def get_libraries(self):
         opt = GnuFCompiler.get_libraries(self)
         if sys.platform == 'darwin':
-            opt.remove('cc_dynamic')        
+            opt.remove('cc_dynamic')
+        if sys.platform == 'win32':
+            c_compiler = self.c_compiler
+            if c_compiler and c_compiler.compiler_type == "msvc":
+                if "gcc" in opt:
+                    i = opt.index("gcc")
+                    opt.insert(i + 1, "mingwex")
+                    opt.insert(i + 1, "mingw32")
+            c_compiler = self.c_compiler
+            if c_compiler and c_compiler.compiler_type == "msvc":
+                return []
+            else:
+                pass
         return opt
+
+    def get_target(self):
+        try:
+            output = subprocess.check_output(self.compiler_f77 + ['-v'])
+        except (OSError, subprocess.CalledProcessError):
+            pass
+        else:
+            output = filepath_from_subprocess_output(output)
+            m = TARGET_R.search(output)
+            if m:
+                return m.group(1)
+        return ""
+
+    def _hash_files(self, filenames):
+        h = hashlib.sha1()
+        for fn in filenames:
+            with open(fn, 'rb') as f:
+                while True:
+                    block = f.read(131072)
+                    if not block:
+                        break
+                    h.update(block)
+        text = base64.b32encode(h.digest())
+        text = text.decode('ascii')
+        return text.rstrip('=')
+
+    def _link_wrapper_lib(self, objects, output_dir, extra_dll_dir,
+                          chained_dlls, is_archive):
+        """Create a wrapper shared library for the given objects
+
+        Return an MSVC-compatible lib
+        """
+
+        c_compiler = self.c_compiler
+        if c_compiler.compiler_type != "msvc":
+            raise ValueError("This method only supports MSVC")
+
+        object_hash = self._hash_files(list(objects) + list(chained_dlls))
+
+        if is_win64():
+            tag = 'win_amd64'
+        else:
+            tag = 'win32'
+
+        basename = 'lib' + os.path.splitext(
+            os.path.basename(objects[0]))[0][:8]
+        root_name = basename + '.' + object_hash + '.gfortran-' + tag
+        dll_name = root_name + '.dll'
+        def_name = root_name + '.def'
+        lib_name = root_name + '.lib'
+        dll_path = os.path.join(extra_dll_dir, dll_name)
+        def_path = os.path.join(output_dir, def_name)
+        lib_path = os.path.join(output_dir, lib_name)
+
+        if os.path.isfile(lib_path):
+            # Nothing to do
+            return lib_path, dll_path
+
+        if is_archive:
+            objects = (["-Wl,--whole-archive"] + list(objects) +
+                       ["-Wl,--no-whole-archive"])
+        self.link_shared_object(
+            objects,
+            dll_name,
+            output_dir=extra_dll_dir,
+            extra_postargs=list(chained_dlls) + [
+                '-Wl,--allow-multiple-definition',
+                '-Wl,--output-def,' + def_path,
+                '-Wl,--export-all-symbols',
+                '-Wl,--enable-auto-import',
+                '-static',
+                '-mlong-double-64',
+            ])
+
+        # No PowerPC!
+        if is_win64():
+            specifier = '/MACHINE:X64'
+        else:
+            specifier = '/MACHINE:X86'
+
+        # MSVC specific code
+        lib_args = ['/def:' + def_path, '/OUT:' + lib_path, specifier]
+        if not c_compiler.initialized:
+            c_compiler.initialize()
+        c_compiler.spawn([c_compiler.lib] + lib_args)
+
+        return lib_path, dll_path
+
+    def can_ccompiler_link(self, compiler):
+        # MSVC cannot link objects compiled by GNU fortran
+        return compiler.compiler_type not in ("msvc", )
+
+    def wrap_unlinkable_objects(self, objects, output_dir, extra_dll_dir):
+        """
+        Convert a set of object files that are not compatible with the default
+        linker, to a file that is compatible.
+        """
+        if self.c_compiler.compiler_type == "msvc":
+            # Compile a DLL and return the lib for the DLL as
+            # the object. Also keep track of previous DLLs that
+            # we have compiled so that we can link against them.
+
+            # If there are .a archives, assume they are self-contained
+            # static libraries, and build separate DLLs for each
+            archives = []
+            plain_objects = []
+            for obj in objects:
+                if obj.lower().endswith('.a'):
+                    archives.append(obj)
+                else:
+                    plain_objects.append(obj)
+
+            chained_libs = []
+            chained_dlls = []
+            for archive in archives[::-1]:
+                lib, dll = self._link_wrapper_lib(
+                    [archive],
+                    output_dir,
+                    extra_dll_dir,
+                    chained_dlls=chained_dlls,
+                    is_archive=True)
+                chained_libs.insert(0, lib)
+                chained_dlls.insert(0, dll)
+
+            if not plain_objects:
+                return chained_libs
+
+            lib, dll = self._link_wrapper_lib(
+                plain_objects,
+                output_dir,
+                extra_dll_dir,
+                chained_dlls=chained_dlls,
+                is_archive=False)
+            return [lib] + chained_libs
+        else:
+            raise ValueError("Unsupported C compiler")
+
+
+def _can_target(cmd, arch):
+    """Return true if the architecture supports the -arch flag"""
+    newcmd = cmd[:]
+    fid, filename = tempfile.mkstemp(suffix=".f")
+    os.close(fid)
+    try:
+        d = os.path.dirname(filename)
+        output = os.path.splitext(filename)[0] + ".o"
+        try:
+            newcmd.extend(["-arch", arch, "-c", filename])
+            p = Popen(newcmd, stderr=STDOUT, stdout=PIPE, cwd=d)
+            p.communicate()
+            return p.returncode == 0
+        finally:
+            if os.path.exists(output):
+                os.remove(output)
+    finally:
+        os.remove(filename)
+
 
 if __name__ == '__main__':
     from distutils import log
+    from numpy.distutils import customized_fcompiler
     log.set_verbosity(2)
-    from numpy.distutils.fcompiler import new_fcompiler
-    #compiler = new_fcompiler(compiler='gnu')
-    compiler = GnuFCompiler()
-    compiler.customize()
-    print compiler.get_version()
+
+    print(customized_fcompiler('gnu').get_version())
+    try:
+        print(customized_fcompiler('g95').get_version())
+    except Exception as e:
+        print(e)
('numpy/distutils/fcompiler', 'compaq.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,15 +1,22 @@
 
 #http://www.compaq.com/fortran/docs/
-
 import os
 import sys
 
-from numpy.distutils.cpuinfo import cpu
 from numpy.distutils.fcompiler import FCompiler
+from distutils.errors import DistutilsPlatformError
+
+compilers = ['CompaqFCompiler']
+if os.name != 'posix' or sys.platform[:6] == 'cygwin' :
+    # Otherwise we'd get a false positive on posix systems with
+    # case-insensitive filesystems (like darwin), because we'll pick
+    # up /bin/df
+    compilers.append('CompaqVisualFCompiler')
 
 class CompaqFCompiler(FCompiler):
 
     compiler_type = 'compaq'
+    description = 'Compaq Fortran Compiler'
     version_pattern = r'Compaq Fortran (?P<version>[^\s]*).*'
 
     if sys.platform[:5]=='linux':
@@ -18,11 +25,11 @@
         fc_exe = 'f90'
 
     executables = {
-        'version_cmd'  : [fc_exe, "-version"],
-        'compiler_f77' : [fc_exe, "-f77rtl","-fixed"],
+        'version_cmd'  : ['<F90>', "-version"],
+        'compiler_f77' : [fc_exe, "-f77rtl", "-fixed"],
         'compiler_fix' : [fc_exe, "-fixed"],
         'compiler_f90' : [fc_exe],
-        'linker_so'    : [fc_exe],
+        'linker_so'    : ['<F90>'],
         'archiver'     : ["ar", "-cr"],
         'ranlib'       : ["ranlib"]
         }
@@ -31,24 +38,25 @@
     module_include_switch = '-I'
 
     def get_flags(self):
-        return ['-assume no2underscore','-nomixed_str_len_arg']
+        return ['-assume no2underscore', '-nomixed_str_len_arg']
     def get_flags_debug(self):
-        return ['-g','-check bounds']
+        return ['-g', '-check bounds']
     def get_flags_opt(self):
-        return ['-O4','-align dcommons','-assume bigarrays',
-                '-assume nozsize','-math_library fast']
+        return ['-O4', '-align dcommons', '-assume bigarrays',
+                '-assume nozsize', '-math_library fast']
     def get_flags_arch(self):
         return ['-arch host', '-tune host']
     def get_flags_linker_so(self):
         if sys.platform[:5]=='linux':
             return ['-shared']
-        return ['-shared','-Wl,-expect_unresolved,*']
+        return ['-shared', '-Wl,-expect_unresolved,*']
 
 class CompaqVisualFCompiler(FCompiler):
 
     compiler_type = 'compaqv'
-    version_pattern = r'(DIGITAL|Compaq) Visual Fortran Optimizing Compiler'\
-                      ' Version (?P<version>[^\s]*).*'
+    description = 'DIGITAL or Compaq Visual Fortran Compiler'
+    version_pattern = (r'(DIGITAL|Compaq) Visual Fortran Optimizing Compiler'
+                       r' Version (?P<version>[^\s]*).*')
 
     compile_switch = '/compile_only'
     object_switch = '/object:'
@@ -61,25 +69,45 @@
 
     ar_exe = 'lib.exe'
     fc_exe = 'DF'
+
     if sys.platform=='win32':
-        from distutils.msvccompiler import MSVCCompiler
-        ar_exe = MSVCCompiler().lib
+        from numpy.distutils.msvccompiler import MSVCCompiler
+
+        try:
+            m = MSVCCompiler()
+            m.initialize()
+            ar_exe = m.lib
+        except DistutilsPlatformError:
+            pass
+        except AttributeError as e:
+            if '_MSVCCompiler__root' in str(e):
+                print('Ignoring "%s" (I think it is msvccompiler.py bug)' % (e))
+            else:
+                raise
+        except OSError as e:
+            if not "vcvarsall.bat" in str(e):
+                print("Unexpected OSError in", __file__)
+                raise
+        except ValueError as e:
+            if not "'path'" in str(e):
+                print("Unexpected ValueError in", __file__)
+                raise
 
     executables = {
-        'version_cmd'  : ['DF', "/what"],
-        'compiler_f77' : ['DF', "/f77rtl","/fixed"],
-        'compiler_fix' : ['DF', "/fixed"],
-        'compiler_f90' : ['DF'],
-        'linker_so'    : ['DF'],
+        'version_cmd'  : ['<F90>', "/what"],
+        'compiler_f77' : [fc_exe, "/f77rtl", "/fixed"],
+        'compiler_fix' : [fc_exe, "/fixed"],
+        'compiler_f90' : [fc_exe],
+        'linker_so'    : ['<F90>'],
         'archiver'     : [ar_exe, "/OUT:"],
         'ranlib'       : None
         }
 
     def get_flags(self):
-        return ['/nologo','/MD','/WX','/iface=(cref,nomixed_str_len_arg)',
-                '/names:lowercase','/assume:underscore']
+        return ['/nologo', '/MD', '/WX', '/iface=(cref,nomixed_str_len_arg)',
+                '/names:lowercase', '/assume:underscore']
     def get_flags_opt(self):
-        return ['/Ox','/fast','/optimize:5','/unroll:0','/math_library:fast']
+        return ['/Ox', '/fast', '/optimize:5', '/unroll:0', '/math_library:fast']
     def get_flags_arch(self):
         return ['/threads']
     def get_flags_debug(self):
@@ -88,7 +116,5 @@
 if __name__ == '__main__':
     from distutils import log
     log.set_verbosity(2)
-    from numpy.distutils.fcompiler import new_fcompiler
-    compiler = new_fcompiler(compiler='compaq')
-    compiler.customize()
-    print compiler.get_version()
+    from numpy.distutils import customized_fcompiler
+    print(customized_fcompiler(compiler='compaq').get_version())
('numpy/distutils/fcompiler', 'intel.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,200 +1,211 @@
 # http://developer.intel.com/software/products/compilers/flin/
-
-import os
 import sys
 
-from numpy.distutils.cpuinfo import cpu
+from numpy.distutils.ccompiler import simple_version_match
 from numpy.distutils.fcompiler import FCompiler, dummy_fortran_file
-from numpy.distutils.exec_command import find_executable
-
-class IntelFCompiler(FCompiler):
+
+compilers = ['IntelFCompiler', 'IntelVisualFCompiler',
+             'IntelItaniumFCompiler', 'IntelItaniumVisualFCompiler',
+             'IntelEM64VisualFCompiler', 'IntelEM64TFCompiler']
+
+
+def intel_version_match(type):
+    # Match against the important stuff in the version string
+    return simple_version_match(start=r'Intel.*?Fortran.*?(?:%s).*?Version' % (type,))
+
+
+class BaseIntelFCompiler(FCompiler):
+    def update_executables(self):
+        f = dummy_fortran_file()
+        self.executables['version_cmd'] = ['<F77>', '-FI', '-V', '-c',
+                                           f + '.f', '-o', f + '.o']
+
+    def runtime_library_dir_option(self, dir):
+        # TODO: could use -Xlinker here, if it's supported
+        assert "," not in dir
+
+        return '-Wl,-rpath=%s' % dir
+
+
+class IntelFCompiler(BaseIntelFCompiler):
 
     compiler_type = 'intel'
-    version_pattern = r'Intel\(R\) Fortran Compiler for 32-bit '\
-                      'applications, Version (?P<version>[^\s*]*)'
-
-    for fc_exe in map(find_executable,['ifort','ifc']):
-        if os.path.isfile(fc_exe):
-            break
-
-    executables = {
-        'version_cmd'  : [fc_exe, "-FI -V -c %(fname)s.f -o %(fname)s.o" \
-                          % {'fname':dummy_fortran_file()}],
-        'compiler_f77' : [fc_exe,"-72","-w90","-w95"],
-        'compiler_fix' : [fc_exe,"-FI"],
-        'compiler_f90' : [fc_exe],
-        'linker_so'    : [fc_exe,"-shared"],
+    compiler_aliases = ('ifort',)
+    description = 'Intel Fortran Compiler for 32-bit apps'
+    version_match = intel_version_match('32-bit|IA-32')
+
+    possible_executables = ['ifort', 'ifc']
+
+    executables = {
+        'version_cmd'  : None,          # set by update_executables
+        'compiler_f77' : [None, "-72", "-w90", "-w95"],
+        'compiler_f90' : [None],
+        'compiler_fix' : [None, "-FI"],
+        'linker_so'    : ["<F90>", "-shared"],
         'archiver'     : ["ar", "-cr"],
         'ranlib'       : ["ranlib"]
         }
 
-    pic_flags = ['-KPIC']
-    module_dir_switch = '-module ' # Don't remove ending space!
+    pic_flags = ['-fPIC']
+    module_dir_switch = '-module '  # Don't remove ending space!
     module_include_switch = '-I'
 
+    def get_flags_free(self):
+        return ['-FR']
+
     def get_flags(self):
-        opt = self.pic_flags + ["-cm"]
-        return opt
-
-    def get_flags_free(self):
-        return ["-FR"]
-
-    def get_flags_opt(self):
-        return ['-O3','-unroll']
+        return ['-fPIC']
+
+    def get_flags_opt(self):  # Scipy test failures with -O2
+        v = self.get_version()
+        mpopt = 'openmp' if v and v < '15' else 'qopenmp'
+        return ['-fp-model', 'strict', '-O1',
+                '-assume', 'minus0', '-{}'.format(mpopt)]
 
     def get_flags_arch(self):
-        opt = []
-        if cpu.has_fdiv_bug():
-            opt.append('-fdiv_check')
-        if cpu.has_f00f_bug():
-            opt.append('-0f_check')
-        if cpu.is_PentiumPro() or cpu.is_PentiumII() or cpu.is_PentiumIII():
-            opt.extend(['-tpp6'])
-        elif cpu.is_PentiumM():
-            opt.extend(['-tpp7','-xB'])
-        elif cpu.is_Pentium():
-            opt.append('-tpp5')
-        elif cpu.is_PentiumIV() or cpu.is_Xeon():
-            opt.extend(['-tpp7','-xW'])
-        if cpu.has_mmx() and not cpu.is_Xeon():
-            opt.append('-xM')
-        if cpu.has_sse2():
-            opt.append('-arch SSE2')
-        elif cpu.has_sse():
-            opt.append('-arch SSE')
-        return opt
+        return []
 
     def get_flags_linker_so(self):
         opt = FCompiler.get_flags_linker_so(self)
         v = self.get_version()
         if v and v >= '8.0':
             opt.append('-nofor_main')
-        opt.extend(self.get_flags_arch())
+        if sys.platform == 'darwin':
+            # Here, it's -dynamiclib
+            try:
+                idx = opt.index('-shared')
+                opt.remove('-shared')
+            except ValueError:
+                idx = 0
+            opt[idx:idx] = ['-dynamiclib', '-Wl,-undefined,dynamic_lookup']
         return opt
+
 
 class IntelItaniumFCompiler(IntelFCompiler):
     compiler_type = 'intele'
-    version_pattern = r'Intel\(R\) Fortran 90 Compiler Itanium\(TM\) Compiler'\
-                      ' for the Itanium\(TM\)-based applications,'\
-                      ' Version (?P<version>[^\s*]*)'
-
-    for fc_exe in map(find_executable,['ifort','efort','efc']):
-        if os.path.isfile(fc_exe):
-            break
-
-    executables = {
-        'version_cmd'  : [fc_exe, "-FI -V -c %(fname)s.f -o %(fname)s.o" \
-                          % {'fname':dummy_fortran_file()}],
-        'compiler_f77' : [fc_exe,"-FI","-w90","-w95"],
-        'compiler_fix' : [fc_exe,"-FI"],
-        'compiler_f90' : [fc_exe],
-        'linker_so'    : [fc_exe,"-shared"],
+    compiler_aliases = ()
+    description = 'Intel Fortran Compiler for Itanium apps'
+
+    version_match = intel_version_match('Itanium|IA-64')
+
+    possible_executables = ['ifort', 'efort', 'efc']
+
+    executables = {
+        'version_cmd'  : None,
+        'compiler_f77' : [None, "-FI", "-w90", "-w95"],
+        'compiler_fix' : [None, "-FI"],
+        'compiler_f90' : [None],
+        'linker_so'    : ['<F90>', "-shared"],
         'archiver'     : ["ar", "-cr"],
         'ranlib'       : ["ranlib"]
         }
 
+
 class IntelEM64TFCompiler(IntelFCompiler):
     compiler_type = 'intelem'
-
-    version_pattern = r'Intel\(R\) Fortran Compiler for Intel\(R\) EM64T-based '\
-                      'applications, Version (?P<version>[^\s*]*)'
-
-    for fc_exe in map(find_executable,['ifort','efort','efc']):
-        if os.path.isfile(fc_exe):
-            break
-
-    executables = {
-        'version_cmd'  : [fc_exe, "-FI -V -c %(fname)s.f -o %(fname)s.o" \
-                          % {'fname':dummy_fortran_file()}],
-        'compiler_f77' : [fc_exe,"-FI","-w90","-w95"],
-        'compiler_fix' : [fc_exe,"-FI"],
-        'compiler_f90' : [fc_exe],
-        'linker_so'    : [fc_exe,"-shared"],
+    compiler_aliases = ()
+    description = 'Intel Fortran Compiler for 64-bit apps'
+
+    version_match = intel_version_match('EM64T-based|Intel\\(R\\) 64|64|IA-64|64-bit')
+
+    possible_executables = ['ifort', 'efort', 'efc']
+
+    executables = {
+        'version_cmd'  : None,
+        'compiler_f77' : [None, "-FI"],
+        'compiler_fix' : [None, "-FI"],
+        'compiler_f90' : [None],
+        'linker_so'    : ['<F90>', "-shared"],
         'archiver'     : ["ar", "-cr"],
         'ranlib'       : ["ranlib"]
         }
 
-class IntelVisualFCompiler(FCompiler):
-
+# Is there no difference in the version string between the above compilers
+# and the Visual compilers?
+
+
+class IntelVisualFCompiler(BaseIntelFCompiler):
     compiler_type = 'intelv'
-    version_pattern = r'Intel\(R\) Fortran Compiler for 32-bit applications, '\
-                      'Version (?P<version>[^\s*]*)'
+    description = 'Intel Visual Fortran Compiler for 32-bit apps'
+    version_match = intel_version_match('32-bit|IA-32')
+
+    def update_executables(self):
+        f = dummy_fortran_file()
+        self.executables['version_cmd'] = ['<F77>', '/FI', '/c',
+                                           f + '.f', '/o', f + '.o']
 
     ar_exe = 'lib.exe'
-    fc_exe = 'ifl'
-    if sys.platform=='win32':
-        from distutils.msvccompiler import MSVCCompiler
-        ar_exe = MSVCCompiler().lib
-
-    executables = {
-        'version_cmd'  : [fc_exe, "-FI -V -c %(fname)s.f -o %(fname)s.o" \
-                          % {'fname':dummy_fortran_file()}],
-        'compiler_f77' : [fc_exe,"-FI","-w90","-w95"],
-        'compiler_fix' : [fc_exe,"-FI","-4L72","-w"],
-        'compiler_f90' : [fc_exe],
-        'linker_so'    : [fc_exe,"-shared"],
+    possible_executables = ['ifort', 'ifl']
+
+    executables = {
+        'version_cmd'  : None,
+        'compiler_f77' : [None],
+        'compiler_fix' : [None],
+        'compiler_f90' : [None],
+        'linker_so'    : [None],
         'archiver'     : [ar_exe, "/verbose", "/OUT:"],
         'ranlib'       : None
         }
 
     compile_switch = '/c '
-    object_switch = '/Fo'     #No space after /Fo!
-    library_switch = '/OUT:'  #No space after /OUT:!
-    module_dir_switch = '/module:' #No space after /module:
+    object_switch = '/Fo'     # No space after /Fo!
+    library_switch = '/OUT:'  # No space after /OUT:!
+    module_dir_switch = '/module:'  # No space after /module:
     module_include_switch = '/I'
 
     def get_flags(self):
-        opt = ['/nologo','/MD','/nbs','/Qlowercase','/us']
+        opt = ['/nologo', '/MD', '/nbs', '/names:lowercase', 
+               '/assume:underscore', '/fpp']
         return opt
 
     def get_flags_free(self):
-        return ["-FR"]
+        return []
 
     def get_flags_debug(self):
-        return ['/4Yb','/d2']
+        return ['/4Yb', '/d2']
 
     def get_flags_opt(self):
-        return ['/O3','/Qip','/Qipo','/Qipo_obj']
+        return ['/O1', '/assume:minus0']  # Scipy test failures with /O2
 
     def get_flags_arch(self):
-        opt = []
-        if cpu.is_PentiumPro() or cpu.is_PentiumII():
-            opt.extend(['/G6','/Qaxi'])
-        elif cpu.is_PentiumIII():
-            opt.extend(['/G6','/QaxK'])
-        elif cpu.is_Pentium():
-            opt.append('/G5')
-        elif cpu.is_PentiumIV():
-            opt.extend(['/G7','/QaxW'])
-        if cpu.has_mmx():
-            opt.append('/QaxM')
-        return opt
+        return ["/arch:IA32", "/QaxSSE3"]
+
+    def runtime_library_dir_option(self, dir):
+        raise NotImplementedError
+
 
 class IntelItaniumVisualFCompiler(IntelVisualFCompiler):
-
     compiler_type = 'intelev'
-    version_pattern = r'Intel\(R\) Fortran 90 Compiler Itanium\(TM\) Compiler'\
-                      ' for the Itanium\(TM\)-based applications,'\
-                      ' Version (?P<version>[^\s*]*)'
-
-    fc_exe = 'efl' # XXX this is a wild guess
+    description = 'Intel Visual Fortran Compiler for Itanium apps'
+
+    version_match = intel_version_match('Itanium')
+
+    possible_executables = ['efl']  # XXX this is a wild guess
     ar_exe = IntelVisualFCompiler.ar_exe
 
     executables = {
-        'version_cmd'  : [fc_exe, "-FI -V -c %(fname)s.f -o %(fname)s.o" \
-                          % {'fname':dummy_fortran_file()}],
-        'compiler_f77' : [fc_exe,"-FI","-w90","-w95"],
-        'compiler_fix' : [fc_exe,"-FI","-4L72","-w"],
-        'compiler_f90' : [fc_exe],
-        'linker_so'    : [fc_exe,"-shared"],
+        'version_cmd'  : None,
+        'compiler_f77' : [None, "-FI", "-w90", "-w95"],
+        'compiler_fix' : [None, "-FI", "-4L72", "-w"],
+        'compiler_f90' : [None],
+        'linker_so'    : ['<F90>', "-shared"],
         'archiver'     : [ar_exe, "/verbose", "/OUT:"],
         'ranlib'       : None
         }
+
+
+class IntelEM64VisualFCompiler(IntelVisualFCompiler):
+    compiler_type = 'intelvem'
+    description = 'Intel Visual Fortran Compiler for 64-bit apps'
+
+    version_match = simple_version_match(start=r'Intel\(R\).*?64,')
+
+    def get_flags_arch(self):
+        return []
+
 
 if __name__ == '__main__':
     from distutils import log
     log.set_verbosity(2)
-    from numpy.distutils.fcompiler import new_fcompiler
-    compiler = new_fcompiler(compiler='intel')
-    compiler.customize()
-    print compiler.get_version()
+    from numpy.distutils import customized_fcompiler
+    print(customized_fcompiler(compiler='intel').get_version())
('numpy/distutils/fcompiler', 'none.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,24 +1,28 @@
+from numpy.distutils.fcompiler import FCompiler
+from numpy.distutils import customized_fcompiler
 
-from numpy.distutils.fcompiler import FCompiler
+compilers = ['NoneFCompiler']
 
 class NoneFCompiler(FCompiler):
 
     compiler_type = 'none'
+    description = 'Fake Fortran compiler'
 
-    executables = {'compiler_f77':['/path/to/nowhere/none'],
-                   'compiler_f90':['/path/to/nowhere/none'],
-                   'compiler_fix':['/path/to/nowhere/none'],
-                   'linker_so':['/path/to/nowhere/none'],
-                   'archiver':['/path/to/nowhere/none'],
-                   'ranlib':['/path/to/nowhere/none'],
-                   'version_cmd':['/path/to/nowhere/none'],
+    executables = {'compiler_f77': None,
+                   'compiler_f90': None,
+                   'compiler_fix': None,
+                   'linker_so': None,
+                   'linker_exe': None,
+                   'archiver': None,
+                   'ranlib': None,
+                   'version_cmd': None,
                    }
+
+    def find_executables(self):
+        pass
 
 
 if __name__ == '__main__':
     from distutils import log
     log.set_verbosity(2)
-    from numpy.distutils.fcompiler import new_fcompiler
-    compiler = NoneFCompiler()
-    compiler.customize()
-    print compiler.get_version()
+    print(customized_fcompiler(compiler='none').get_version())
('numpy/distutils/fcompiler', 'nag.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,39 +1,87 @@
-import os
 import sys
-
-from numpy.distutils.cpuinfo import cpu
+import re
 from numpy.distutils.fcompiler import FCompiler
 
-class NAGFCompiler(FCompiler):
+compilers = ['NAGFCompiler', 'NAGFORCompiler']
+
+class BaseNAGFCompiler(FCompiler):
+    version_pattern = r'NAG.* Release (?P<version>[^(\s]*)'
+
+    def version_match(self, version_string):
+        m = re.search(self.version_pattern, version_string)
+        if m:
+            return m.group('version')
+        else:
+            return None
+
+    def get_flags_linker_so(self):
+        return ["-Wl,-shared"]
+    def get_flags_opt(self):
+        return ['-O4']
+    def get_flags_arch(self):
+        return []
+
+class NAGFCompiler(BaseNAGFCompiler):
 
     compiler_type = 'nag'
-    version_pattern =  r'NAGWare Fortran 95 compiler Release (?P<version>[^\s]*)'
+    description = 'NAGWare Fortran 95 Compiler'
 
     executables = {
-        'version_cmd'  : ["f95", "-V"],
+        'version_cmd'  : ["<F90>", "-V"],
         'compiler_f77' : ["f95", "-fixed"],
         'compiler_fix' : ["f95", "-fixed"],
         'compiler_f90' : ["f95"],
-        'linker_so'    : ["f95"],
+        'linker_so'    : ["<F90>"],
         'archiver'     : ["ar", "-cr"],
         'ranlib'       : ["ranlib"]
         }
 
     def get_flags_linker_so(self):
-        if sys.platform=='darwin':
-            return ['-unsharedf95','-Wl,-bundle,-flat_namespace,-undefined,suppress']
-        return ["-Wl,-shared"]
-    def get_flags_opt(self):
-        return ['-O4']
+        if sys.platform == 'darwin':
+            return ['-unsharedf95', '-Wl,-bundle,-flat_namespace,-undefined,suppress']
+        return BaseNAGFCompiler.get_flags_linker_so(self)
     def get_flags_arch(self):
-        return ['-target=native']
+        version = self.get_version()
+        if version and version < '5.1':
+            return ['-target=native']
+        else:
+            return BaseNAGFCompiler.get_flags_arch(self)
     def get_flags_debug(self):
-        return ['-g','-gline','-g90','-nan','-C']
+        return ['-g', '-gline', '-g90', '-nan', '-C']
+
+class NAGFORCompiler(BaseNAGFCompiler):
+
+    compiler_type = 'nagfor'
+    description = 'NAG Fortran Compiler'
+
+    executables = {
+        'version_cmd'  : ["nagfor", "-V"],
+        'compiler_f77' : ["nagfor", "-fixed"],
+        'compiler_fix' : ["nagfor", "-fixed"],
+        'compiler_f90' : ["nagfor"],
+        'linker_so'    : ["nagfor"],
+        'archiver'     : ["ar", "-cr"],
+        'ranlib'       : ["ranlib"]
+        }
+
+    def get_flags_linker_so(self):
+        if sys.platform == 'darwin':
+            return ['-unsharedrts',
+                    '-Wl,-bundle,-flat_namespace,-undefined,suppress']
+        return BaseNAGFCompiler.get_flags_linker_so(self)
+    def get_flags_debug(self):
+        version = self.get_version()
+        if version and version > '6.1':
+            return ['-g', '-u', '-nan', '-C=all', '-thread_safe',
+                    '-kind=unique', '-Warn=allocation', '-Warn=subnormal']
+        else:
+            return ['-g', '-nan', '-C=all', '-u', '-thread_safe']
+
 
 if __name__ == '__main__':
     from distutils import log
     log.set_verbosity(2)
-    from numpy.distutils.fcompiler import new_fcompiler
-    compiler = new_fcompiler(compiler='nag')
-    compiler.customize()
-    print compiler.get_version()
+    from numpy.distutils import customized_fcompiler
+    compiler = customized_fcompiler(compiler='nagfor')
+    print(compiler.get_version())
+    print(compiler.get_flags_debug())
('numpy/distutils/fcompiler', 'pg.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,42 +1,128 @@
-
 # http://www.pgroup.com
-
-import os
 import sys
 
-from numpy.distutils.cpuinfo import cpu
 from numpy.distutils.fcompiler import FCompiler
+from sys import platform
+from os.path import join, dirname, normpath
+
+compilers = ['PGroupFCompiler', 'PGroupFlangCompiler']
+
 
 class PGroupFCompiler(FCompiler):
 
     compiler_type = 'pg'
-    version_pattern =  r'\s*pg(f77|f90|hpf) (?P<version>[\d.-]+).*'
+    description = 'Portland Group Fortran Compiler'
+    version_pattern = r'\s*pg(f77|f90|hpf|fortran) (?P<version>[\d.-]+).*'
 
-    executables = {
-        'version_cmd'  : ["pgf77", "-V 2>/dev/null"],
-        'compiler_f77' : ["pgf77"],
-        'compiler_fix' : ["pgf90", "-Mfixed"],
-        'compiler_f90' : ["pgf90"],
-        'linker_so'    : ["pgf90","-shared","-fpic"],
-        'archiver'     : ["ar", "-cr"],
-        'ranlib'       : ["ranlib"]
+    if platform == 'darwin':
+        executables = {
+            'version_cmd': ["<F77>", "-V"],
+            'compiler_f77': ["pgfortran", "-dynamiclib"],
+            'compiler_fix': ["pgfortran", "-Mfixed", "-dynamiclib"],
+            'compiler_f90': ["pgfortran", "-dynamiclib"],
+            'linker_so': ["libtool"],
+            'archiver': ["ar", "-cr"],
+            'ranlib': ["ranlib"]
         }
-    pic_flags = ['-fpic']
+        pic_flags = ['']
+    else:
+        executables = {
+            'version_cmd': ["<F77>", "-V"],
+            'compiler_f77': ["pgfortran"],
+            'compiler_fix': ["pgfortran", "-Mfixed"],
+            'compiler_f90': ["pgfortran"],
+            'linker_so': ["<F90>"],
+            'archiver': ["ar", "-cr"],
+            'ranlib': ["ranlib"]
+        }
+        pic_flags = ['-fpic']
+
     module_dir_switch = '-module '
     module_include_switch = '-I'
 
     def get_flags(self):
-        opt = ['-Minform=inform','-Mnosecond_underscore']
+        opt = ['-Minform=inform', '-Mnosecond_underscore']
         return self.pic_flags + opt
+
     def get_flags_opt(self):
         return ['-fast']
+
     def get_flags_debug(self):
         return ['-g']
+
+    if platform == 'darwin':
+        def get_flags_linker_so(self):
+            return ["-dynamic", '-undefined', 'dynamic_lookup']
+
+    else:
+        def get_flags_linker_so(self):
+            return ["-shared", '-fpic']
+
+    def runtime_library_dir_option(self, dir):
+        return '-R%s' % dir
+
+
+import functools
+
+class PGroupFlangCompiler(FCompiler):
+    compiler_type = 'flang'
+    description = 'Portland Group Fortran LLVM Compiler'
+    version_pattern = r'\s*(flang|clang) version (?P<version>[\d.-]+).*'
+
+    ar_exe = 'lib.exe'
+    possible_executables = ['flang']
+
+    executables = {
+        'version_cmd': ["<F77>", "--version"],
+        'compiler_f77': ["flang"],
+        'compiler_fix': ["flang"],
+        'compiler_f90': ["flang"],
+        'linker_so': [None],
+        'archiver': [ar_exe, "/verbose", "/OUT:"],
+        'ranlib': None
+    }
+
+    library_switch = '/OUT:'  # No space after /OUT:!
+    module_dir_switch = '-module '  # Don't remove ending space!
+
+    def get_libraries(self):
+        opt = FCompiler.get_libraries(self)
+        opt.extend(['flang', 'flangrti', 'ompstub'])
+        return opt
+
+    @functools.lru_cache(maxsize=128)
+    def get_library_dirs(self):
+        """List of compiler library directories."""
+        opt = FCompiler.get_library_dirs(self)
+        flang_dir = dirname(self.executables['compiler_f77'][0])
+        opt.append(normpath(join(flang_dir, '..', 'lib')))
+
+        return opt
+
+    def get_flags(self):
+        return []
+
+    def get_flags_free(self):
+        return []
+
+    def get_flags_debug(self):
+        return ['-g']
+
+    def get_flags_opt(self):
+        return ['-O3']
+
+    def get_flags_arch(self):
+        return []
+
+    def runtime_library_dir_option(self, dir):
+        raise NotImplementedError
+
 
 if __name__ == '__main__':
     from distutils import log
     log.set_verbosity(2)
-    from numpy.distutils.fcompiler import new_fcompiler
-    compiler = new_fcompiler(compiler='pg')
-    compiler.customize()
-    print compiler.get_version()
+    from numpy.distutils import customized_fcompiler
+    if 'flang' in sys.argv:
+        print(customized_fcompiler(compiler='flang').get_version())
+    else:
+        print(customized_fcompiler(compiler='pg').get_version())
('numpy/distutils/fcompiler', 'ibm.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,17 +1,23 @@
 import os
 import re
 import sys
+import subprocess
 
 from numpy.distutils.fcompiler import FCompiler
+from numpy.distutils.exec_command import find_executable
+from numpy.distutils.misc_util import make_temp_file
 from distutils import log
 
-class IbmFCompiler(FCompiler):
+compilers = ['IBMFCompiler']
 
+class IBMFCompiler(FCompiler):
     compiler_type = 'ibm'
-    version_pattern =  r'xlf\(1\)\s*IBM XL Fortran (Advanced Edition |)Version (?P<version>[^\s*]*)'
+    description = 'IBM XL Fortran Compiler'
+    version_pattern =  r'(xlf\(1\)\s*|)IBM XL Fortran ((Advanced Edition |)Version |Enterprise Edition V|for AIX, V)(?P<version>[^\s*]*)'
+    #IBM XL Fortran Enterprise Edition V10.1 for AIX \nVersion: 10.01.0000.0004
 
     executables = {
-        'version_cmd'  : ["xlf"],
+        'version_cmd'  : ["<F77>", "-qversion"],
         'compiler_f77' : ["xlf"],
         'compiler_fix' : ["xlf90", "-qfixed"],
         'compiler_f90' : ["xlf90"],
@@ -22,15 +28,29 @@
 
     def get_version(self,*args,**kwds):
         version = FCompiler.get_version(self,*args,**kwds)
+
+        if version is None and sys.platform.startswith('aix'):
+            # use lslpp to find out xlf version
+            lslpp = find_executable('lslpp')
+            xlf = find_executable('xlf')
+            if os.path.exists(xlf) and os.path.exists(lslpp):
+                try:
+                    o = subprocess.check_output([lslpp, '-Lc', 'xlfcmp'])
+                except (OSError, subprocess.CalledProcessError):
+                    pass
+                else:
+                    m = re.search(r'xlfcmp:(?P<version>\d+([.]\d+)+)', o)
+                    if m: version = m.group('version')
+
         xlf_dir = '/etc/opt/ibmcmp/xlf'
         if version is None and os.path.isdir(xlf_dir):
+            # linux:
             # If the output of xlf does not contain version info
             # (that's the case with xlf 8.1, for instance) then
             # let's try another method:
-            l = os.listdir(xlf_dir)
-            l.sort()
+            l = sorted(os.listdir(xlf_dir))
             l.reverse()
-            l = [d for d in l if os.path.isfile(os.path.join(xlf_dir,d,'xlf.cfg'))]
+            l = [d for d in l if os.path.isfile(os.path.join(xlf_dir, d, 'xlf.cfg'))]
             if l:
                 from distutils.version import LooseVersion
                 self.version = version = LooseVersion(l[0])
@@ -48,34 +68,30 @@
             opt.append('-Wl,-bundle,-flat_namespace,-undefined,suppress')
         else:
             opt.append('-bshared')
-        version = self.get_version(ok_status=[0,40])
+        version = self.get_version(ok_status=[0, 40])
         if version is not None:
-            import tempfile
-            xlf_cfg = '/etc/opt/ibmcmp/xlf/%s/xlf.cfg' % version
-            new_cfg = tempfile.mktemp()+'_xlf.cfg'
+            if sys.platform.startswith('aix'):
+                xlf_cfg = '/etc/xlf.cfg'
+            else:
+                xlf_cfg = '/etc/opt/ibmcmp/xlf/%s/xlf.cfg' % version
+            fo, new_cfg = make_temp_file(suffix='_xlf.cfg')
             log.info('Creating '+new_cfg)
-            fi = open(xlf_cfg,'r')
-            fo = open(new_cfg,'w')
-            crt1_match = re.compile(r'\s*crt\s*[=]\s*(?P<path>.*)/crt1.o').match
-            for line in fi.readlines():
-                m = crt1_match(line)
-                if m:
-                    fo.write('crt = %s/bundle1.o\n' % (m.group('path')))
-                else:
-                    fo.write(line)
-            fi.close()
+            with open(xlf_cfg, 'r') as fi:
+                crt1_match = re.compile(r'\s*crt\s*=\s*(?P<path>.*)/crt1.o').match
+                for line in fi:
+                    m = crt1_match(line)
+                    if m:
+                        fo.write('crt = %s/bundle1.o\n' % (m.group('path')))
+                    else:
+                        fo.write(line)
             fo.close()
             opt.append('-F'+new_cfg)
         return opt
 
     def get_flags_opt(self):
-        return ['-O5']
+        return ['-O3']
 
 if __name__ == '__main__':
-    from distutils import log
+    from numpy.distutils import customized_fcompiler
     log.set_verbosity(2)
-    from numpy.distutils.fcompiler import new_fcompiler
-    #compiler = new_fcompiler(compiler='ibm')
-    compiler = IbmFCompiler()
-    compiler.customize()
-    print compiler.get_version()
+    print(customized_fcompiler(compiler='ibm').get_version())
('numpy/distutils/fcompiler', 'sun.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,24 +1,23 @@
-import os
-import sys
-
-from numpy.distutils.cpuinfo import cpu
 from numpy.distutils.ccompiler import simple_version_match
 from numpy.distutils.fcompiler import FCompiler
+
+compilers = ['SunFCompiler']
 
 class SunFCompiler(FCompiler):
 
     compiler_type = 'sun'
+    description = 'Sun or Forte Fortran 95 Compiler'
     # ex:
     # f90: Sun WorkShop 6 update 2 Fortran 95 6.2 Patch 111690-10 2003/08/28
     version_match = simple_version_match(
                       start=r'f9[05]: (Sun|Forte|WorkShop).*Fortran 95')
 
     executables = {
-        'version_cmd'  : ["f90", "-V"],
+        'version_cmd'  : ["<F90>", "-V"],
         'compiler_f77' : ["f90"],
         'compiler_fix' : ["f90", "-fixed"],
         'compiler_f90' : ["f90"],
-        'linker_so'    : ["f90","-Bdynamic","-G"],
+        'linker_so'    : ["<F90>", "-Bdynamic", "-G"],
         'archiver'     : ["ar", "-cr"],
         'ranlib'       : ["ranlib"]
         }
@@ -34,18 +33,19 @@
             ret.append("-fixed")
         return ret
     def get_opt(self):
-        return ['-fast','-dalign']
+        return ['-fast', '-dalign']
     def get_arch(self):
         return ['-xtarget=generic']
     def get_libraries(self):
         opt = []
-        opt.extend(['fsu','sunmath','mvec','f77compat'])
+        opt.extend(['fsu', 'sunmath', 'mvec'])
         return opt
+
+    def runtime_library_dir_option(self, dir):
+        return '-R%s' % dir
 
 if __name__ == '__main__':
     from distutils import log
     log.set_verbosity(2)
-    from numpy.distutils.fcompiler import new_fcompiler
-    compiler = new_fcompiler(compiler='sun')
-    compiler.customize()
-    print compiler.get_version()
+    from numpy.distutils import customized_fcompiler
+    print(customized_fcompiler(compiler='sun').get_version())
('numpy/distutils/fcompiler', 'lahey.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,20 +1,21 @@
 import os
-import sys
 
-from numpy.distutils.cpuinfo import cpu
 from numpy.distutils.fcompiler import FCompiler
+
+compilers = ['LaheyFCompiler']
 
 class LaheyFCompiler(FCompiler):
 
     compiler_type = 'lahey'
+    description = 'Lahey/Fujitsu Fortran 95 Compiler'
     version_pattern =  r'Lahey/Fujitsu Fortran 95 Compiler Release (?P<version>[^\s*]*)'
 
     executables = {
-        'version_cmd'  : ["lf95", "--version"],
+        'version_cmd'  : ["<F90>", "--version"],
         'compiler_f77' : ["lf95", "--fix"],
         'compiler_fix' : ["lf95", "--fix"],
         'compiler_f90' : ["lf95"],
-        'linker_so'    : ["lf95","-shared"],
+        'linker_so'    : ["lf95", "-shared"],
         'archiver'     : ["ar", "-cr"],
         'ranlib'       : ["ranlib"]
         }
@@ -25,12 +26,12 @@
     def get_flags_opt(self):
         return ['-O']
     def get_flags_debug(self):
-        return ['-g','--chk','--chkglobal']
+        return ['-g', '--chk', '--chkglobal']
     def get_library_dirs(self):
         opt = []
         d = os.environ.get('LAHEY')
         if d:
-            opt.append(os.path.join(d,'lib'))
+            opt.append(os.path.join(d, 'lib'))
         return opt
     def get_libraries(self):
         opt = []
@@ -40,7 +41,5 @@
 if __name__ == '__main__':
     from distutils import log
     log.set_verbosity(2)
-    from numpy.distutils.fcompiler import new_fcompiler
-    compiler = new_fcompiler(compiler='lahey')
-    compiler.customize()
-    print compiler.get_version()
+    from numpy.distutils import customized_fcompiler
+    print(customized_fcompiler(compiler='lahey').get_version())
('numpy/distutils/fcompiler', '__init__.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -2,44 +2,75 @@
 
 Contains FCompiler, an abstract base class that defines the interface
 for the numpy.distutils Fortran compiler abstraction model.
+
+Terminology:
+
+To be consistent, where the term 'executable' is used, it means the single
+file, like 'gcc', that is executed, and should be a string. In contrast,
+'command' means the entire command line, like ['gcc', '-c', 'file.c'], and
+should be a list.
+
+But note that FCompiler.executables is actually a dictionary of commands.
+
 """
-
-__all__ = ['FCompiler','new_fcompiler','show_fcompilers',
+__all__ = ['FCompiler', 'new_fcompiler', 'show_fcompilers',
            'dummy_fortran_file']
 
 import os
 import sys
 import re
-from types import StringType,NoneType
-from distutils.sysconfig import get_config_var
+
+from distutils.sysconfig import get_python_lib
 from distutils.fancy_getopt import FancyGetopt
-from distutils.errors import DistutilsModuleError,DistutilsArgError,\
-     DistutilsExecError,CompileError,LinkError,DistutilsPlatformError
-from distutils.util import split_quoted
+from distutils.errors import DistutilsModuleError, \
+     DistutilsExecError, CompileError, LinkError, DistutilsPlatformError
+from distutils.util import split_quoted, strtobool
 
 from numpy.distutils.ccompiler import CCompiler, gen_lib_options
 from numpy.distutils import log
-from numpy.distutils.command.config_compiler import config_fc
-from numpy.distutils.misc_util import is_string, is_sequence
-from distutils.spawn import _nt_quote_args
+from numpy.distutils.misc_util import is_string, all_strings, is_sequence, \
+    make_temp_file, get_shared_lib_extension
+from numpy.distutils.exec_command import find_executable
+from numpy.distutils import _shell_utils
+
+from .environment import EnvironmentConfig
+
+__metaclass__ = type
+
+class CompilerNotFound(Exception):
+    pass
+
+def flaglist(s):
+    if is_string(s):
+        return split_quoted(s)
+    else:
+        return s
+
+def str2bool(s):
+    if is_string(s):
+        return strtobool(s)
+    return bool(s)
+
+def is_sequence_of_strings(seq):
+    return is_sequence(seq) and all_strings(seq)
 
 class FCompiler(CCompiler):
-    """ Abstract base class to define the interface that must be implemented
+    """Abstract base class to define the interface that must be implemented
     by real Fortran compiler classes.
 
     Methods that subclasses may redefine:
 
-        get_version_cmd(), get_linker_so(), get_version()
+        update_executables(), find_executables(), get_version()
         get_flags(), get_flags_opt(), get_flags_arch(), get_flags_debug()
         get_flags_f77(), get_flags_opt_f77(), get_flags_arch_f77(),
         get_flags_debug_f77(), get_flags_f90(), get_flags_opt_f90(),
         get_flags_arch_f90(), get_flags_debug_f90(),
-        get_flags_fix(), get_flags_linker_so(), get_flags_version()
+        get_flags_fix(), get_flags_linker_so()
 
     DON'T call these methods (except get_version) after
     constructing a compiler instance or inside any other method.
-    All methods, except get_version_cmd() and get_flags_version(), may
-    call get_version() method.
+    All methods, except update_executables() and find_executables(),
+    may call the get_version() method.
 
     After constructing a compiler instance, always call customize(dist=None)
     method that finalizes compiler construction and makes the following
@@ -54,30 +85,91 @@
       library_dirs
     """
 
-
-    language_map = {'.f':'f77',
-                    '.for':'f77',
-                    '.F':'f77',    # XXX: needs preprocessor
-                    '.ftn':'f77',
-                    '.f77':'f77',
-                    '.f90':'f90',
-                    '.F90':'f90',  # XXX: needs preprocessor
-                    '.f95':'f90',
+    # These are the environment variables and distutils keys used.
+    # Each configuration description is
+    # (<hook name>, <environment variable>, <key in distutils.cfg>, <convert>, <append>)
+    # The hook names are handled by the self._environment_hook method.
+    #  - names starting with 'self.' call methods in this class
+    #  - names starting with 'exe.' return the key in the executables dict
+    #  - names like 'flags.YYY' return self.get_flag_YYY()
+    # convert is either None or a function to convert a string to the
+    # appropriate type used.
+
+    distutils_vars = EnvironmentConfig(
+        distutils_section='config_fc',
+        noopt = (None, None, 'noopt', str2bool, False),
+        noarch = (None, None, 'noarch', str2bool, False),
+        debug = (None, None, 'debug', str2bool, False),
+        verbose = (None, None, 'verbose', str2bool, False),
+    )
+
+    command_vars = EnvironmentConfig(
+        distutils_section='config_fc',
+        compiler_f77 = ('exe.compiler_f77', 'F77', 'f77exec', None, False),
+        compiler_f90 = ('exe.compiler_f90', 'F90', 'f90exec', None, False),
+        compiler_fix = ('exe.compiler_fix', 'F90', 'f90exec', None, False),
+        version_cmd = ('exe.version_cmd', None, None, None, False),
+        linker_so = ('exe.linker_so', 'LDSHARED', 'ldshared', None, False),
+        linker_exe = ('exe.linker_exe', 'LD', 'ld', None, False),
+        archiver = (None, 'AR', 'ar', None, False),
+        ranlib = (None, 'RANLIB', 'ranlib', None, False),
+    )
+
+    flag_vars = EnvironmentConfig(
+        distutils_section='config_fc',
+        f77 = ('flags.f77', 'F77FLAGS', 'f77flags', flaglist, True),
+        f90 = ('flags.f90', 'F90FLAGS', 'f90flags', flaglist, True),
+        free = ('flags.free', 'FREEFLAGS', 'freeflags', flaglist, True),
+        fix = ('flags.fix', None, None, flaglist, False),
+        opt = ('flags.opt', 'FOPT', 'opt', flaglist, True),
+        opt_f77 = ('flags.opt_f77', None, None, flaglist, False),
+        opt_f90 = ('flags.opt_f90', None, None, flaglist, False),
+        arch = ('flags.arch', 'FARCH', 'arch', flaglist, False),
+        arch_f77 = ('flags.arch_f77', None, None, flaglist, False),
+        arch_f90 = ('flags.arch_f90', None, None, flaglist, False),
+        debug = ('flags.debug', 'FDEBUG', 'fdebug', flaglist, True),
+        debug_f77 = ('flags.debug_f77', None, None, flaglist, False),
+        debug_f90 = ('flags.debug_f90', None, None, flaglist, False),
+        flags = ('self.get_flags', 'FFLAGS', 'fflags', flaglist, True),
+        linker_so = ('flags.linker_so', 'LDFLAGS', 'ldflags', flaglist, True),
+        linker_exe = ('flags.linker_exe', 'LDFLAGS', 'ldflags', flaglist, True),
+        ar = ('flags.ar', 'ARFLAGS', 'arflags', flaglist, True),
+    )
+
+    language_map = {'.f': 'f77',
+                    '.for': 'f77',
+                    '.F': 'f77',    # XXX: needs preprocessor
+                    '.ftn': 'f77',
+                    '.f77': 'f77',
+                    '.f90': 'f90',
+                    '.F90': 'f90',  # XXX: needs preprocessor
+                    '.f95': 'f90',
                     }
-    language_order = ['f90','f77']
-
+    language_order = ['f90', 'f77']
+
+
+    # These will be set by the subclass
+
+    compiler_type = None
+    compiler_aliases = ()
     version_pattern = None
 
+    possible_executables = []
     executables = {
-        'version_cmd'  : ["f77","-v"],
-        'compiler_f77' : ["f77"],
-        'compiler_f90' : ["f90"],
-        'compiler_fix' : ["f90","-fixed"],
-        'linker_so'    : ["f90","-shared"],
-        'linker_exe'   : ["f90"],
-        'archiver'     : ["ar","-cr"],
-        'ranlib'       : None,
+        'version_cmd': ["f77", "-v"],
+        'compiler_f77': ["f77"],
+        'compiler_f90': ["f90"],
+        'compiler_fix': ["f90", "-fixed"],
+        'linker_so': ["f90", "-shared"],
+        'linker_exe': ["f90"],
+        'archiver': ["ar", "-cr"],
+        'ranlib': None,
         }
+
+    # If compiler does not support compiling Fortran 90 then it can
+    # suggest using another compiler. For example, gnu would suggest
+    # gnu95 compiler type when there are F90 sources.
+    suggested_f90_compiler = None
 
     compile_switch = "-c"
     object_switch = "-o "   # Ending space matters! It will be stripped
@@ -96,122 +188,224 @@
 
     pic_flags = []           # Flags to create position-independent code
 
-    src_extensions = ['.for','.ftn','.f77','.f','.f90','.f95','.F','.F90']
+    src_extensions = ['.for', '.ftn', '.f77', '.f', '.f90', '.f95', '.F', '.F90', '.FOR']
     obj_extension = ".o"
-    shared_lib_extension = get_config_var('SO')  # or .dll
+
+    shared_lib_extension = get_shared_lib_extension()
     static_lib_extension = ".a"  # or .lib
     static_lib_format = "lib%s%s" # or %s%s
     shared_lib_format = "%s%s"
     exe_extension = ""
 
+    _exe_cache = {}
+
+    _executable_keys = ['version_cmd', 'compiler_f77', 'compiler_f90',
+                        'compiler_fix', 'linker_so', 'linker_exe', 'archiver',
+                        'ranlib']
+
+    # This will be set by new_fcompiler when called in
+    # command/{build_ext.py, build_clib.py, config.py} files.
+    c_compiler = None
+
+    # extra_{f77,f90}_compile_args are set by build_ext.build_extension method
+    extra_f77_compile_args = []
+    extra_f90_compile_args = []
+
+    def __init__(self, *args, **kw):
+        CCompiler.__init__(self, *args, **kw)
+        self.distutils_vars = self.distutils_vars.clone(self._environment_hook)
+        self.command_vars = self.command_vars.clone(self._environment_hook)
+        self.flag_vars = self.flag_vars.clone(self._environment_hook)
+        self.executables = self.executables.copy()
+        for e in self._executable_keys:
+            if e not in self.executables:
+                self.executables[e] = None
+
+        # Some methods depend on .customize() being called first, so
+        # this keeps track of whether that's happened yet.
+        self._is_customised = False
+
+    def __copy__(self):
+        obj = self.__new__(self.__class__)
+        obj.__dict__.update(self.__dict__)
+        obj.distutils_vars = obj.distutils_vars.clone(obj._environment_hook)
+        obj.command_vars = obj.command_vars.clone(obj._environment_hook)
+        obj.flag_vars = obj.flag_vars.clone(obj._environment_hook)
+        obj.executables = obj.executables.copy()
+        return obj
+
+    def copy(self):
+        return self.__copy__()
+
+    # Use properties for the attributes used by CCompiler. Setting them
+    # as attributes from the self.executables dictionary is error-prone,
+    # so we get them from there each time.
+    def _command_property(key):
+        def fget(self):
+            assert self._is_customised
+            return self.executables[key]
+        return property(fget=fget)
+    version_cmd = _command_property('version_cmd')
+    compiler_f77 = _command_property('compiler_f77')
+    compiler_f90 = _command_property('compiler_f90')
+    compiler_fix = _command_property('compiler_fix')
+    linker_so = _command_property('linker_so')
+    linker_exe = _command_property('linker_exe')
+    archiver = _command_property('archiver')
+    ranlib = _command_property('ranlib')
+
+    # Make our terminology consistent.
+    def set_executable(self, key, value):
+        self.set_command(key, value)
+
+    def set_commands(self, **kw):
+        for k, v in kw.items():
+            self.set_command(k, v)
+
+    def set_command(self, key, value):
+        if not key in self._executable_keys:
+            raise ValueError(
+                "unknown executable '%s' for class %s" %
+                (key, self.__class__.__name__))
+        if is_string(value):
+            value = split_quoted(value)
+        assert value is None or is_sequence_of_strings(value[1:]), (key, value)
+        self.executables[key] = value
+
     ######################################################################
     ## Methods that subclasses may redefine. But don't call these methods!
     ## They are private to FCompiler class and may return unexpected
     ## results if used elsewhere. So, you have been warned..
 
-    def get_version_cmd(self):
-        """ Compiler command to print out version information. """
-        f77 = self.executables['compiler_f77']
-        if f77 is not None:
-            f77 = f77[0]
-        cmd = self.executables['version_cmd']
-        if cmd is not None:
-            cmd = cmd[0]
-            if cmd==f77:
-                cmd = self.compiler_f77[0]
+    def find_executables(self):
+        """Go through the self.executables dictionary, and attempt to
+        find and assign appropriate executables.
+
+        Executable names are looked for in the environment (environment
+        variables, the distutils.cfg, and command line), the 0th-element of
+        the command list, and the self.possible_executables list.
+
+        Also, if the 0th element is "<F77>" or "<F90>", the Fortran 77
+        or the Fortran 90 compiler executable is used, unless overridden
+        by an environment setting.
+
+        Subclasses should call this if overridden.
+        """
+        assert self._is_customised
+        exe_cache = self._exe_cache
+        def cached_find_executable(exe):
+            if exe in exe_cache:
+                return exe_cache[exe]
+            fc_exe = find_executable(exe)
+            exe_cache[exe] = exe_cache[fc_exe] = fc_exe
+            return fc_exe
+        def verify_command_form(name, value):
+            if value is not None and not is_sequence_of_strings(value):
+                raise ValueError(
+                    "%s value %r is invalid in class %s" %
+                    (name, value, self.__class__.__name__))
+        def set_exe(exe_key, f77=None, f90=None):
+            cmd = self.executables.get(exe_key, None)
+            if not cmd:
+                return None
+            # Note that we get cmd[0] here if the environment doesn't
+            # have anything set
+            exe_from_environ = getattr(self.command_vars, exe_key)
+            if not exe_from_environ:
+                possibles = [f90, f77] + self.possible_executables
             else:
-                f90 = self.executables['compiler_f90']
-                if f90 is not None:
-                    f90 = f90[0]
-                if cmd==f90:
-                    cmd = self.compiler_f90[0]
-        return cmd
-
-    def get_linker_so(self):
-        """ Linker command to build shared libraries. """
-        f77 = self.executables['compiler_f77']
-        if f77 is not None:
-            f77 = f77[0]
-        ln = self.executables['linker_so']
-        if ln is not None:
-            ln = ln[0]
-            if ln==f77:
-                ln = self.compiler_f77[0]
+                possibles = [exe_from_environ] + self.possible_executables
+
+            seen = set()
+            unique_possibles = []
+            for e in possibles:
+                if e == '<F77>':
+                    e = f77
+                elif e == '<F90>':
+                    e = f90
+                if not e or e in seen:
+                    continue
+                seen.add(e)
+                unique_possibles.append(e)
+
+            for exe in unique_possibles:
+                fc_exe = cached_find_executable(exe)
+                if fc_exe:
+                    cmd[0] = fc_exe
+                    return fc_exe
+            self.set_command(exe_key, None)
+            return None
+
+        ctype = self.compiler_type
+        f90 = set_exe('compiler_f90')
+        if not f90:
+            f77 = set_exe('compiler_f77')
+            if f77:
+                log.warn('%s: no Fortran 90 compiler found' % ctype)
             else:
-                f90 = self.executables['compiler_f90']
-                if f90 is not None:
-                    f90 = f90[0]
-                if ln==f90:
-                    ln = self.compiler_f90[0]
-        return ln
-
-    def get_linker_exe(self):
-        """ Linker command to build shared libraries. """
-        f77 = self.executables['compiler_f77']
-        if f77 is not None:
-            f77 = f77[0]
-        ln = self.executables.get('linker_exe')
-        if ln is not None:
-            ln = ln[0]
-            if ln==f77:
-                ln = self.compiler_f77[0]
-            else:
-                f90 = self.executables['compiler_f90']
-                if f90 is not None:
-                    f90 = f90[0]
-                if ln==f90:
-                    ln = self.compiler_f90[0]
-        return ln
+                raise CompilerNotFound('%s: f90 nor f77' % ctype)
+        else:
+            f77 = set_exe('compiler_f77', f90=f90)
+            if not f77:
+                log.warn('%s: no Fortran 77 compiler found' % ctype)
+            set_exe('compiler_fix', f90=f90)
+
+        set_exe('linker_so', f77=f77, f90=f90)
+        set_exe('linker_exe', f77=f77, f90=f90)
+        set_exe('version_cmd', f77=f77, f90=f90)
+        set_exe('archiver')
+        set_exe('ranlib')
+
+    def update_executables(self):
+        """Called at the beginning of customisation. Subclasses should
+        override this if they need to set up the executables dictionary.
+
+        Note that self.find_executables() is run afterwards, so the
+        self.executables dictionary values can contain <F77> or <F90> as
+        the command, which will be replaced by the found F77 or F90
+        compiler.
+        """
+        pass
 
     def get_flags(self):
-        """ List of flags common to all compiler types. """
+        """List of flags common to all compiler types."""
         return [] + self.pic_flags
-    def get_flags_version(self):
-        """ List of compiler flags to print out version information. """
-        if self.executables['version_cmd']:
-            return self.executables['version_cmd'][1:]
-        return []
+
+    def _get_command_flags(self, key):
+        cmd = self.executables.get(key, None)
+        if cmd is None:
+            return []
+        return cmd[1:]
+
     def get_flags_f77(self):
-        """ List of Fortran 77 specific flags. """
-        if self.executables['compiler_f77']:
-            return self.executables['compiler_f77'][1:]
-        return []
+        """List of Fortran 77 specific flags."""
+        return self._get_command_flags('compiler_f77')
     def get_flags_f90(self):
-        """ List of Fortran 90 specific flags. """
-        if self.executables['compiler_f90']:
-            return self.executables['compiler_f90'][1:]
-        return []
+        """List of Fortran 90 specific flags."""
+        return self._get_command_flags('compiler_f90')
     def get_flags_free(self):
-        """ List of Fortran 90 free format specific flags. """
+        """List of Fortran 90 free format specific flags."""
         return []
     def get_flags_fix(self):
-        """ List of Fortran 90 fixed format specific flags. """
-        if self.executables['compiler_fix']:
-            return self.executables['compiler_fix'][1:]
-        return []
+        """List of Fortran 90 fixed format specific flags."""
+        return self._get_command_flags('compiler_fix')
     def get_flags_linker_so(self):
-        """ List of linker flags to build a shared library. """
-        if self.executables['linker_so']:
-            return self.executables['linker_so'][1:]
-        return []
+        """List of linker flags to build a shared library."""
+        return self._get_command_flags('linker_so')
     def get_flags_linker_exe(self):
-        """ List of linker flags to build an executable. """
-        if self.executables['linker_exe']:
-            return self.executables['linker_exe'][1:]
-        return []
+        """List of linker flags to build an executable."""
+        return self._get_command_flags('linker_exe')
     def get_flags_ar(self):
-        """ List of archiver flags. """
-        if self.executables['archiver']:
-            return self.executables['archiver'][1:]
-        return []
+        """List of archiver flags. """
+        return self._get_command_flags('archiver')
     def get_flags_opt(self):
-        """ List of architecture independent compiler flags. """
+        """List of architecture independent compiler flags."""
         return []
     def get_flags_arch(self):
-        """ List of architecture dependent compiler flags. """
+        """List of architecture dependent compiler flags."""
         return []
     def get_flags_debug(self):
-        """ List of compiler flags to compile with debugging information. """
+        """List of compiler flags to compile with debugging information."""
         return []
 
     get_flags_opt_f77 = get_flags_opt_f90 = get_flags_opt
@@ -219,153 +413,144 @@
     get_flags_debug_f77 = get_flags_debug_f90 = get_flags_debug
 
     def get_libraries(self):
-        """ List of compiler libraries. """
+        """List of compiler libraries."""
         return self.libraries[:]
     def get_library_dirs(self):
-        """ List of compiler library directories. """
+        """List of compiler library directories."""
         return self.library_dirs[:]
 
+    def get_version(self, force=False, ok_status=[0]):
+        assert self._is_customised
+        version = CCompiler.get_version(self, force=force, ok_status=ok_status)
+        if version is None:
+            raise CompilerNotFound()
+        return version
+
+
     ############################################################
 
     ## Public methods:
 
-    def customize(self, dist=None):
-        """ Customize Fortran compiler.
+    def customize(self, dist = None):
+        """Customize Fortran compiler.
 
         This method gets Fortran compiler specific information from
         (i) class definition, (ii) environment, (iii) distutils config
-        files, and (iv) command line.
+        files, and (iv) command line (later overrides earlier).
 
         This method should be always called after constructing a
         compiler instance. But not in __init__ because Distribution
         instance is needed for (iii) and (iv).
         """
         log.info('customize %s' % (self.__class__.__name__))
-        from distutils.dist import Distribution
-        if dist is None:
-            # These hooks are for testing only!
-            dist = Distribution()
-            dist.script_name = os.path.basename(sys.argv[0])
-            dist.script_args = ['config_fc'] + sys.argv[1:]
-            dist.cmdclass['config_fc'] = config_fc
-            dist.parse_config_files()
-            dist.parse_command_line()
-        if isinstance(dist,Distribution):
-            conf = dist.get_option_dict('config_fc')
-        else:
-            assert isinstance(dist,dict)
-            conf = dist
-        noopt = conf.get('noopt',[None,0])[1]
-        if 0: # change to `if 1:` when making release.
-            # Don't use architecture dependent compiler flags:
-            noarch = 1
-        else:
-            noarch = conf.get('noarch',[None,noopt])[1]
-        debug = conf.get('debug',[None,0])[1]
-
-
-        f77 = self.__get_cmd('compiler_f77','F77',(conf,'f77exec'))
-        f90 = self.__get_cmd('compiler_f90','F90',(conf,'f90exec'))
-        # Temporarily setting f77,f90 compilers so that
-        # version_cmd can use their executables.
+
+        self._is_customised = True
+
+        self.distutils_vars.use_distribution(dist)
+        self.command_vars.use_distribution(dist)
+        self.flag_vars.use_distribution(dist)
+
+        self.update_executables()
+
+        # find_executables takes care of setting the compiler commands,
+        # version_cmd, linker_so, linker_exe, ar, and ranlib
+        self.find_executables()
+
+        noopt = self.distutils_vars.get('noopt', False)
+        noarch = self.distutils_vars.get('noarch', noopt)
+        debug = self.distutils_vars.get('debug', False)
+
+        f77 = self.command_vars.compiler_f77
+        f90 = self.command_vars.compiler_f90
+
+        f77flags = []
+        f90flags = []
+        freeflags = []
+        fixflags = []
+
         if f77:
-            self.set_executables(compiler_f77=[f77])
+            f77 = _shell_utils.NativeParser.split(f77)
+            f77flags = self.flag_vars.f77
         if f90:
-            self.set_executables(compiler_f90=[f90])
-
-        # Must set version_cmd before others as self.get_flags*
-        # methods may call self.get_version.
-        vers_cmd = self.__get_cmd(self.get_version_cmd)
-        if vers_cmd:
-            vflags = self.__get_flags(self.get_flags_version)
-            self.set_executables(version_cmd=[vers_cmd]+vflags)
+            f90 = _shell_utils.NativeParser.split(f90)
+            f90flags = self.flag_vars.f90
+            freeflags = self.flag_vars.free
+        # XXX Assuming that free format is default for f90 compiler.
+        fix = self.command_vars.compiler_fix
+        # NOTE: this and similar examples are probably just
+        # excluding --coverage flag when F90 = gfortran --coverage
+        # instead of putting that flag somewhere more appropriate
+        # this and similar examples where a Fortran compiler
+        # environment variable has been customized by CI or a user
+        # should perhaps eventually be more thoroughly tested and more
+        # robustly handled
+        if fix:
+            fix = _shell_utils.NativeParser.split(fix)
+            fixflags = self.flag_vars.fix + f90flags
+
+        oflags, aflags, dflags = [], [], []
+        # examine get_flags_<tag>_<compiler> for extra flags
+        # only add them if the method is different from get_flags_<tag>
+        def get_flags(tag, flags):
+            # note that self.flag_vars.<tag> calls self.get_flags_<tag>()
+            flags.extend(getattr(self.flag_vars, tag))
+            this_get = getattr(self, 'get_flags_' + tag)
+            for name, c, flagvar in [('f77', f77, f77flags),
+                                     ('f90', f90, f90flags),
+                                     ('f90', fix, fixflags)]:
+                t = '%s_%s' % (tag, name)
+                if c and this_get is not getattr(self, 'get_flags_' + t):
+                    flagvar.extend(getattr(self.flag_vars, t))
+        if not noopt:
+            get_flags('opt', oflags)
+            if not noarch:
+                get_flags('arch', aflags)
+        if debug:
+            get_flags('debug', dflags)
+
+        fflags = self.flag_vars.flags + dflags + oflags + aflags
 
         if f77:
-            f77flags = self.__get_flags(self.get_flags_f77,'F77FLAGS',
-                                   (conf,'f77flags'))
+            self.set_commands(compiler_f77=f77+f77flags+fflags)
         if f90:
-            f90flags = self.__get_flags(self.get_flags_f90,'F90FLAGS',
-                                       (conf,'f90flags'))
-            freeflags = self.__get_flags(self.get_flags_free,'FREEFLAGS',
-                                         (conf,'freeflags'))
-        # XXX Assuming that free format is default for f90 compiler.
-        fix = self.__get_cmd('compiler_fix','F90',(conf,'f90exec'))
+            self.set_commands(compiler_f90=f90+freeflags+f90flags+fflags)
         if fix:
-            fixflags = self.__get_flags(self.get_flags_fix) + f90flags
-
-        oflags,aflags,dflags = [],[],[]
-        if not noopt:
-            oflags = self.__get_flags(self.get_flags_opt,'FOPT',(conf,'opt'))
-            if f77 and self.get_flags_opt is not self.get_flags_opt_f77:
-                f77flags += self.__get_flags(self.get_flags_opt_f77)
-            if f90 and self.get_flags_opt is not self.get_flags_opt_f90:
-                f90flags += self.__get_flags(self.get_flags_opt_f90)
-            if fix and self.get_flags_opt is not self.get_flags_opt_f90:
-                fixflags += self.__get_flags(self.get_flags_opt_f90)
-            if not noarch:
-                aflags = self.__get_flags(self.get_flags_arch,'FARCH',
-                                          (conf,'arch'))
-                if f77 and self.get_flags_arch is not self.get_flags_arch_f77:
-                    f77flags += self.__get_flags(self.get_flags_arch_f77)
-                if f90 and self.get_flags_arch is not self.get_flags_arch_f90:
-                    f90flags += self.__get_flags(self.get_flags_arch_f90)
-                if fix and self.get_flags_arch is not self.get_flags_arch_f90:
-                    fixflags += self.__get_flags(self.get_flags_arch_f90)
-        if debug:
-            dflags = self.__get_flags(self.get_flags_debug,'FDEBUG')
-            if f77  and self.get_flags_debug is not self.get_flags_debug_f77:
-                f77flags += self.__get_flags(self.get_flags_debug_f77)
-            if f90  and self.get_flags_debug is not self.get_flags_debug_f90:
-                f90flags += self.__get_flags(self.get_flags_debug_f90)
-            if fix and self.get_flags_debug is not self.get_flags_debug_f90:
-                fixflags += self.__get_flags(self.get_flags_debug_f90)
-
-        fflags = self.__get_flags(self.get_flags,'FFLAGS') \
-                 + dflags + oflags + aflags
-
-        if f77:
-            self.set_executables(compiler_f77=[f77]+f77flags+fflags)
-        if f90:
-            self.set_executables(compiler_f90=[f90]+freeflags+f90flags+fflags)
-        if fix:
-            self.set_executables(compiler_fix=[fix]+fixflags+fflags)
+            self.set_commands(compiler_fix=fix+fixflags+fflags)
+
+
         #XXX: Do we need LDSHARED->SOSHARED, LDFLAGS->SOFLAGS
-        linker_so = self.__get_cmd(self.get_linker_so,'LDSHARED')
+        linker_so = self.linker_so
         if linker_so:
-            linker_so_flags = self.__get_flags(self.get_flags_linker_so,'LDFLAGS')
-            self.set_executables(linker_so=[linker_so]+linker_so_flags)
-
-        linker_exe = self.__get_cmd(self.get_linker_exe,'LD')
+            linker_so_flags = self.flag_vars.linker_so
+            if sys.platform.startswith('aix'):
+                python_lib = get_python_lib(standard_lib=1)
+                ld_so_aix = os.path.join(python_lib, 'config', 'ld_so_aix')
+                python_exp = os.path.join(python_lib, 'config', 'python.exp')
+                linker_so = [ld_so_aix] + linker_so + ['-bI:'+python_exp]
+            self.set_commands(linker_so=linker_so+linker_so_flags)
+
+        linker_exe = self.linker_exe
         if linker_exe:
-            linker_exe_flags = self.__get_flags(self.get_flags_linker_exe,'LDFLAGS')
-            self.set_executables(linker_exe=[linker_exe]+linker_exe_flags)
-        ar = self.__get_cmd('archiver','AR')
+            linker_exe_flags = self.flag_vars.linker_exe
+            self.set_commands(linker_exe=linker_exe+linker_exe_flags)
+
+        ar = self.command_vars.archiver
         if ar:
-            arflags = self.__get_flags(self.get_flags_ar,'ARFLAGS')
-            self.set_executables(archiver=[ar]+arflags)
-
-        ranlib = self.__get_cmd('ranlib','RANLIB')
-        if ranlib:
-            self.set_executables(ranlib=[ranlib])
+            arflags = self.flag_vars.ar
+            self.set_commands(archiver=[ar]+arflags)
 
         self.set_library_dirs(self.get_library_dirs())
         self.set_libraries(self.get_libraries())
 
-
-        verbose = conf.get('verbose',[None,0])[1]
-        if verbose:
-            self.dump_properties()
-        return
-
     def dump_properties(self):
-        """ Print out the attributes of a compiler instance. """
+        """Print out the attributes of a compiler instance."""
         props = []
-        for key in self.executables.keys() + \
-                ['version','libraries','library_dirs',
-                 'object_switch','compile_switch']:
-            if hasattr(self,key):
-                v = getattr(self,key)
-                props.append((key, None, '= '+`v`))
+        for key in list(self.executables.keys()) + \
+                ['version', 'libraries', 'library_dirs',
+                 'object_switch', 'compile_switch']:
+            if hasattr(self, key):
+                v = getattr(self, key)
+                props.append((key, None, '= '+repr(v)))
         props.sort()
 
         pretty_printer = FancyGetopt(props)
@@ -373,65 +558,76 @@
                                               % (self.__class__.__name__)):
             if l[:4]=='  --':
                 l = '  ' + l[4:]
-            print l
-        return
+            print(l)
 
     ###################
 
     def _compile(self, obj, src, ext, cc_args, extra_postargs, pp_opts):
         """Compile 'src' to product 'obj'."""
+        src_flags = {}
         if is_f_file(src) and not has_f90_header(src):
             flavor = ':f77'
             compiler = self.compiler_f77
+            src_flags = get_f77flags(src)
+            extra_compile_args = self.extra_f77_compile_args or []
         elif is_free_format(src):
             flavor = ':f90'
             compiler = self.compiler_f90
             if compiler is None:
-                raise DistutilsExecError, 'f90 not supported by '\
-                      +self.__class__.__name__
+                raise DistutilsExecError('f90 not supported by %s needed for %s'\
+                      % (self.__class__.__name__, src))
+            extra_compile_args = self.extra_f90_compile_args or []
         else:
             flavor = ':fix'
             compiler = self.compiler_fix
             if compiler is None:
-                raise DistutilsExecError, 'f90 (fixed) not supported by '\
-                      +self.__class__.__name__
+                raise DistutilsExecError('f90 (fixed) not supported by %s needed for %s'\
+                      % (self.__class__.__name__, src))
+            extra_compile_args = self.extra_f90_compile_args or []
         if self.object_switch[-1]==' ':
-            o_args = [self.object_switch.strip(),obj]
+            o_args = [self.object_switch.strip(), obj]
         else:
             o_args = [self.object_switch.strip()+obj]
 
         assert self.compile_switch.strip()
         s_args = [self.compile_switch, src]
 
-        if os.name == 'nt':
-            compiler = _nt_quote_args(compiler)
-        command = compiler + cc_args + s_args + o_args + extra_postargs
+        if extra_compile_args:
+            log.info('extra %s options: %r' \
+                     % (flavor[1:], ' '.join(extra_compile_args)))
+
+        extra_flags = src_flags.get(self.compiler_type, [])
+        if extra_flags:
+            log.info('using compile options from source: %r' \
+                     % ' '.join(extra_flags))
+
+        command = compiler + cc_args + extra_flags + s_args + o_args \
+                  + extra_postargs + extra_compile_args
 
         display = '%s: %s' % (os.path.basename(compiler[0]) + flavor,
                               src)
         try:
-            self.spawn(command,display=display)
-        except DistutilsExecError, msg:
-            raise CompileError, msg
-
-        return
+            self.spawn(command, display=display)
+        except DistutilsExecError as e:
+            msg = str(e)
+            raise CompileError(msg) from None
 
     def module_options(self, module_dirs, module_build_dir):
         options = []
         if self.module_dir_switch is not None:
             if self.module_dir_switch[-1]==' ':
-                options.extend([self.module_dir_switch.strip(),module_build_dir])
+                options.extend([self.module_dir_switch.strip(), module_build_dir])
             else:
                 options.append(self.module_dir_switch.strip()+module_build_dir)
         else:
-            print 'XXX: module_build_dir=%r option ignored' % (module_build_dir)
-            print 'XXX: Fix module_dir_switch for ',self.__class__.__name__
+            print('XXX: module_build_dir=%r option ignored' % (module_build_dir))
+            print('XXX: Fix module_dir_switch for ', self.__class__.__name__)
         if self.module_include_switch is not None:
             for d in [module_build_dir]+module_dirs:
                 options.append('%s%s' % (self.module_include_switch, d))
         else:
-            print 'XXX: module_dirs=%r option ignored' % (module_dirs)
-            print 'XXX: Fix module_include_switch for ',self.__class__.__name__
+            print('XXX: module_dirs=%r option ignored' % (module_dirs))
+            print('XXX: Fix module_include_switch for ', self.__class__.__name__)
         return options
 
     def library_option(self, lib):
@@ -453,11 +649,11 @@
         if is_string(output_dir):
             output_filename = os.path.join(output_dir, output_filename)
         elif output_dir is not None:
-            raise TypeError, "'output_dir' must be a string or None"
+            raise TypeError("'output_dir' must be a string or None")
 
         if self._need_link(objects, output_filename):
             if self.library_switch[-1]==' ':
-                o_args = [self.library_switch.strip(),output_filename]
+                o_args = [self.library_switch.strip(), output_filename]
             else:
                 o_args = [self.library_switch.strip()+output_filename]
 
@@ -477,286 +673,352 @@
                 linker = self.linker_exe[:]
             else:
                 linker = self.linker_so[:]
-            if os.name == 'nt':
-                linker = _nt_quote_args(linker)
             command = linker + ld_args
             try:
                 self.spawn(command)
-            except DistutilsExecError, msg:
-                raise LinkError, msg
+            except DistutilsExecError as e:
+                msg = str(e)
+                raise LinkError(msg) from None
         else:
             log.debug("skipping %s (up-to-date)", output_filename)
+
+    def _environment_hook(self, name, hook_name):
+        if hook_name is None:
+            return None
+        if is_string(hook_name):
+            if hook_name.startswith('self.'):
+                hook_name = hook_name[5:]
+                hook = getattr(self, hook_name)
+                return hook()
+            elif hook_name.startswith('exe.'):
+                hook_name = hook_name[4:]
+                var = self.executables[hook_name]
+                if var:
+                    return var[0]
+                else:
+                    return None
+            elif hook_name.startswith('flags.'):
+                hook_name = hook_name[6:]
+                hook = getattr(self, 'get_flags_' + hook_name)
+                return hook()
+        else:
+            return hook_name()
+
+    def can_ccompiler_link(self, ccompiler):
+        """
+        Check if the given C compiler can link objects produced by
+        this compiler.
+        """
+        return True
+
+    def wrap_unlinkable_objects(self, objects, output_dir, extra_dll_dir):
+        """
+        Convert a set of object files that are not compatible with the default
+        linker, to a file that is compatible.
+
+        Parameters
+        ----------
+        objects : list
+            List of object files to include.
+        output_dir : str
+            Output directory to place generated object files.
+        extra_dll_dir : str
+            Output directory to place extra DLL files that need to be
+            included on Windows.
+
+        Returns
+        -------
+        converted_objects : list of str
+             List of converted object files.
+             Note that the number of output files is not necessarily
+             the same as inputs.
+
+        """
+        raise NotImplementedError()
+
+    ## class FCompiler
+
+_default_compilers = (
+    # sys.platform mappings
+    ('win32', ('gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95',
+               'intelvem', 'intelem', 'flang')),
+    ('cygwin.*', ('gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95')),
+    ('linux.*', ('arm', 'gnu95', 'intel', 'lahey', 'pg', 'nv', 'absoft', 'nag',
+                 'vast', 'compaq', 'intele', 'intelem', 'gnu', 'g95', 
+                 'pathf95', 'nagfor', 'fujitsu')),
+    ('darwin.*', ('gnu95', 'nag', 'nagfor', 'absoft', 'ibm', 'intel', 'gnu',
+                 'g95', 'pg')),
+    ('sunos.*', ('sun', 'gnu', 'gnu95', 'g95')),
+    ('irix.*', ('mips', 'gnu', 'gnu95',)),
+    ('aix.*', ('ibm', 'gnu', 'gnu95',)),
+    # os.name mappings
+    ('posix', ('gnu', 'gnu95',)),
+    ('nt', ('gnu', 'gnu95',)),
+    ('mac', ('gnu95', 'gnu', 'pg')),
+    )
+
+fcompiler_class = None
+fcompiler_aliases = None
+
+def load_all_fcompiler_classes():
+    """Cache all the FCompiler classes found in modules in the
+    numpy.distutils.fcompiler package.
+    """
+    from glob import glob
+    global fcompiler_class, fcompiler_aliases
+    if fcompiler_class is not None:
         return
-
-
-    ## Private methods:
-
-    def __get_cmd(self, command, envvar=None, confvar=None):
-        if command is None:
-            var = None
-        elif is_string(command):
-            var = self.executables[command]
-            if var is not None:
-                var = var[0]
-        else:
-            var = command()
-        if envvar is not None:
-            var = os.environ.get(envvar, var)
-        if confvar is not None:
-            var = confvar[0].get(confvar[1], [None,var])[1]
-        return var
-
-    def __get_flags(self, command, envvar=None, confvar=None):
-        if command is None:
-            var = []
-        elif is_string(command):
-            var = self.executables[command][1:]
-        else:
-            var = command()
-        if envvar is not None:
-            var = os.environ.get(envvar, var)
-        if confvar is not None:
-            var = confvar[0].get(confvar[1], [None,var])[1]
-        if is_string(var):
-            var = split_quoted(var)
-        return var
-
-    ## class FCompiler
-
-fcompiler_class = {'gnu':('gnu','GnuFCompiler',
-                          "GNU Fortran Compiler"),
-                   'gnu95':('gnu','Gnu95FCompiler',
-                            "GNU 95 Fortran Compiler"),
-                   'g95':('g95','G95FCompiler',
-                          "GNU Fortran 95 Compiler"),
-                   'pg':('pg','PGroupFCompiler',
-                         "Portland Group Fortran Compiler"),
-                   'absoft':('absoft','AbsoftFCompiler',
-                             "Absoft Corp Fortran Compiler"),
-                   'mips':('mips','MipsFCompiler',
-                           "MIPSpro Fortran Compiler"),
-                   'sun':('sun','SunFCompiler',
-                          "Sun|Forte Fortran 95 Compiler"),
-                   'intel':('intel','IntelFCompiler',
-                            "Intel Fortran Compiler for 32-bit apps"),
-                   'intelv':('intel','IntelVisualFCompiler',
-                             "Intel Visual Fortran Compiler for 32-bit apps"),
-                   'intele':('intel','IntelItaniumFCompiler',
-                             "Intel Fortran Compiler for Itanium apps"),
-                   'intelev':('intel','IntelItaniumVisualFCompiler',
-                              "Intel Visual Fortran Compiler for Itanium apps"),
-                   'intelem':('intel','IntelEM64TFCompiler',
-                             "Intel Fortran Compiler for EM64T-based apps"),
-                   'nag':('nag','NAGFCompiler',
-                          "NAGWare Fortran 95 Compiler"),
-                   'compaq':('compaq','CompaqFCompiler',
-                             "Compaq Fortran Compiler"),
-                   'compaqv':('compaq','CompaqVisualFCompiler',
-                             "DIGITAL|Compaq Visual Fortran Compiler"),
-                   'vast':('vast','VastFCompiler',
-                           "Pacific-Sierra Research Fortran 90 Compiler"),
-                   'hpux':('hpux','HPUXFCompiler',
-                           "HP Fortran 90 Compiler"),
-                   'lahey':('lahey','LaheyFCompiler',
-                            "Lahey/Fujitsu Fortran 95 Compiler"),
-                   'ibm':('ibm','IbmFCompiler',
-                          "IBM XL Fortran Compiler"),
-                   'f':('f','FFCompiler',
-                        "Fortran Company/NAG F Compiler"),
-                   'none':('none','NoneFCompiler',"Fake Fortran compiler")
-                   }
-
-_default_compilers = (
-    # Platform mappings
-    ('win32',('gnu','intelv','absoft','compaqv','intelev','gnu95','g95')),
-    ('cygwin.*',('gnu','intelv','absoft','compaqv','intelev','gnu95','g95')),
-    ('linux.*',('gnu','intel','lahey','pg','absoft','nag','vast','compaq',
-                'intele','intelem','gnu95','g95')),
-    ('darwin.*',('nag','absoft','ibm','gnu','gnu95','g95')),
-    ('sunos.*',('sun','gnu','gnu95','g95')),
-    ('irix.*',('mips','gnu','gnu95',)),
-    ('aix.*',('ibm','gnu','gnu95',)),
-    # OS mappings
-    ('posix',('gnu','gnu95',)),
-    ('nt',('gnu','gnu95',)),
-    ('mac',('gnu','gnu95',)),
-    )
-
-def _find_existing_fcompiler(compilers, osname=None, platform=None):
-    for compiler in compilers:
+    pys = os.path.join(os.path.dirname(__file__), '*.py')
+    fcompiler_class = {}
+    fcompiler_aliases = {}
+    for fname in glob(pys):
+        module_name, ext = os.path.splitext(os.path.basename(fname))
+        module_name = 'numpy.distutils.fcompiler.' + module_name
+        __import__ (module_name)
+        module = sys.modules[module_name]
+        if hasattr(module, 'compilers'):
+            for cname in module.compilers:
+                klass = getattr(module, cname)
+                desc = (klass.compiler_type, klass, klass.description)
+                fcompiler_class[klass.compiler_type] = desc
+                for alias in klass.compiler_aliases:
+                    if alias in fcompiler_aliases:
+                        raise ValueError("alias %r defined for both %s and %s"
+                                         % (alias, klass.__name__,
+                                            fcompiler_aliases[alias][1].__name__))
+                    fcompiler_aliases[alias] = desc
+
+def _find_existing_fcompiler(compiler_types,
+                             osname=None, platform=None,
+                             requiref90=False,
+                             c_compiler=None):
+    from numpy.distutils.core import get_distribution
+    dist = get_distribution(always=True)
+    for compiler_type in compiler_types:
         v = None
         try:
-            c = new_fcompiler(plat=platform, compiler=compiler)
-            c.customize()
+            c = new_fcompiler(plat=platform, compiler=compiler_type,
+                              c_compiler=c_compiler)
+            c.customize(dist)
             v = c.get_version()
+            if requiref90 and c.compiler_f90 is None:
+                v = None
+                new_compiler = c.suggested_f90_compiler
+                if new_compiler:
+                    log.warn('Trying %r compiler as suggested by %r '
+                             'compiler for f90 support.' % (compiler_type,
+                                                            new_compiler))
+                    c = new_fcompiler(plat=platform, compiler=new_compiler,
+                                      c_compiler=c_compiler)
+                    c.customize(dist)
+                    v = c.get_version()
+                    if v is not None:
+                        compiler_type = new_compiler
+            if requiref90 and c.compiler_f90 is None:
+                raise ValueError('%s does not support compiling f90 codes, '
+                                 'skipping.' % (c.__class__.__name__))
         except DistutilsModuleError:
-            pass
-        except Exception, msg:
-            log.warn(msg)
+            log.debug("_find_existing_fcompiler: compiler_type='%s' raised DistutilsModuleError", compiler_type)
+        except CompilerNotFound:
+            log.debug("_find_existing_fcompiler: compiler_type='%s' not found", compiler_type)
         if v is not None:
-            return compiler
-    return
-
-def get_default_fcompiler(osname=None, platform=None):
-    """ Determine the default Fortran compiler to use for the given platform. """
+            return compiler_type
+    return None
+
+def available_fcompilers_for_platform(osname=None, platform=None):
     if osname is None:
         osname = os.name
     if platform is None:
         platform = sys.platform
-    matching_compilers = []
-    for pattern, compiler in _default_compilers:
-        if re.match(pattern, platform) is not None or \
-               re.match(pattern, osname) is not None:
-            if is_sequence(compiler):
-                matching_compilers.extend(list(compiler))
-            else:
-                matching_compilers.append(compiler)
-    if not matching_compilers:
-        matching_compilers.append('gnu')
-    compiler =  _find_existing_fcompiler(matching_compilers,
-                                         osname=osname,
-                                         platform=platform)
-    if compiler is not None:
-        return compiler
-    return matching_compilers[0]
+    matching_compiler_types = []
+    for pattern, compiler_type in _default_compilers:
+        if re.match(pattern, platform) or re.match(pattern, osname):
+            for ct in compiler_type:
+                if ct not in matching_compiler_types:
+                    matching_compiler_types.append(ct)
+    if not matching_compiler_types:
+        matching_compiler_types.append('gnu')
+    return matching_compiler_types
+
+def get_default_fcompiler(osname=None, platform=None, requiref90=False,
+                          c_compiler=None):
+    """Determine the default Fortran compiler to use for the given
+    platform."""
+    matching_compiler_types = available_fcompilers_for_platform(osname,
+                                                                platform)
+    log.info("get_default_fcompiler: matching types: '%s'",
+             matching_compiler_types)
+    compiler_type =  _find_existing_fcompiler(matching_compiler_types,
+                                              osname=osname,
+                                              platform=platform,
+                                              requiref90=requiref90,
+                                              c_compiler=c_compiler)
+    return compiler_type
+
+# Flag to avoid rechecking for Fortran compiler every time
+failed_fcompilers = set()
 
 def new_fcompiler(plat=None,
                   compiler=None,
                   verbose=0,
                   dry_run=0,
-                  force=0):
-    """ Generate an instance of some FCompiler subclass for the supplied
+                  force=0,
+                  requiref90=False,
+                  c_compiler = None):
+    """Generate an instance of some FCompiler subclass for the supplied
     platform/compiler combination.
     """
+    global failed_fcompilers
+    fcompiler_key = (plat, compiler)
+    if fcompiler_key in failed_fcompilers:
+        return None
+
+    load_all_fcompiler_classes()
     if plat is None:
         plat = os.name
-    try:
-        if compiler is None:
-            compiler = get_default_fcompiler(plat)
-        (module_name, class_name, long_description) = fcompiler_class[compiler]
-    except KeyError:
+    if compiler is None:
+        compiler = get_default_fcompiler(plat, requiref90=requiref90,
+                                         c_compiler=c_compiler)
+    if compiler in fcompiler_class:
+        module_name, klass, long_description = fcompiler_class[compiler]
+    elif compiler in fcompiler_aliases:
+        module_name, klass, long_description = fcompiler_aliases[compiler]
+    else:
         msg = "don't know how to compile Fortran code on platform '%s'" % plat
         if compiler is not None:
             msg = msg + " with '%s' compiler." % compiler
             msg = msg + " Supported compilers are: %s)" \
                   % (','.join(fcompiler_class.keys()))
-        raise DistutilsPlatformError, msg
-
-    try:
-        module_name = 'numpy.distutils.fcompiler.'+module_name
-        __import__ (module_name)
-        module = sys.modules[module_name]
-        klass = vars(module)[class_name]
-    except ImportError:
-        raise DistutilsModuleError, \
-              "can't compile Fortran code: unable to load module '%s'" % \
-              module_name
-    except KeyError:
-        raise DistutilsModuleError, \
-              ("can't compile Fortran code: unable to find class '%s' " +
-               "in module '%s'") % (class_name, module_name)
-    compiler = klass(None, dry_run, force)
-    log.debug('new_fcompiler returns %s' % (klass))
+        log.warn(msg)
+        failed_fcompilers.add(fcompiler_key)
+        return None
+
+    compiler = klass(verbose=verbose, dry_run=dry_run, force=force)
+    compiler.c_compiler = c_compiler
     return compiler
 
-def show_fcompilers(dist = None):
-    """ Print list of available compilers (used by the "--help-fcompiler"
+def show_fcompilers(dist=None):
+    """Print list of available compilers (used by the "--help-fcompiler"
     option to "config_fc").
     """
     if dist is None:
         from distutils.dist import Distribution
+        from numpy.distutils.command.config_compiler import config_fc
         dist = Distribution()
         dist.script_name = os.path.basename(sys.argv[0])
         dist.script_args = ['config_fc'] + sys.argv[1:]
+        try:
+            dist.script_args.remove('--help-fcompiler')
+        except ValueError:
+            pass
         dist.cmdclass['config_fc'] = config_fc
         dist.parse_config_files()
         dist.parse_command_line()
-
     compilers = []
     compilers_na = []
     compilers_ni = []
-    for compiler in fcompiler_class.keys():
-        v = 'N/A'
+    if not fcompiler_class:
+        load_all_fcompiler_classes()
+    platform_compilers = available_fcompilers_for_platform()
+    for compiler in platform_compilers:
+        v = None
+        log.set_verbosity(-2)
         try:
-            c = new_fcompiler(compiler=compiler)
+            c = new_fcompiler(compiler=compiler, verbose=dist.verbose)
             c.customize(dist)
             v = c.get_version()
-        except DistutilsModuleError:
-            pass
-        except Exception, msg:
-            log.warn(msg)
+        except (DistutilsModuleError, CompilerNotFound) as e:
+            log.debug("show_fcompilers: %s not found" % (compiler,))
+            log.debug(repr(e))
+
         if v is None:
             compilers_na.append(("fcompiler="+compiler, None,
                               fcompiler_class[compiler][2]))
-        elif v=='N/A':
-            compilers_ni.append(("fcompiler="+compiler, None,
-                                 fcompiler_class[compiler][2]))
         else:
+            c.dump_properties()
             compilers.append(("fcompiler="+compiler, None,
                               fcompiler_class[compiler][2] + ' (%s)' % v))
 
+    compilers_ni = list(set(fcompiler_class.keys()) - set(platform_compilers))
+    compilers_ni = [("fcompiler="+fc, None, fcompiler_class[fc][2])
+                    for fc in compilers_ni]
+
     compilers.sort()
     compilers_na.sort()
+    compilers_ni.sort()
     pretty_printer = FancyGetopt(compilers)
-    pretty_printer.print_help("List of available Fortran compilers:")
+    pretty_printer.print_help("Fortran compilers found:")
     pretty_printer = FancyGetopt(compilers_na)
-    pretty_printer.print_help("List of unavailable Fortran compilers:")
+    pretty_printer.print_help("Compilers available for this "
+                              "platform, but not found:")
     if compilers_ni:
         pretty_printer = FancyGetopt(compilers_ni)
-        pretty_printer.print_help("List of unimplemented Fortran compilers:")
-    print "For compiler details, run 'config_fc --verbose' setup command."
+        pretty_printer.print_help("Compilers not available on this platform:")
+    print("For compiler details, run 'config_fc --verbose' setup command.")
+
 
 def dummy_fortran_file():
-    import atexit
-    import tempfile
-    dummy_name = tempfile.mktemp()+'__dummy'
-    dummy = open(dummy_name+'.f','w')
-    dummy.write("      subroutine dummy()\n      end\n")
-    dummy.close()
-    def rm_file(name=dummy_name,log_threshold=log._global_log.threshold):
-        save_th = log._global_log.threshold
-        log.set_threshold(log_threshold)
-        try: os.remove(name+'.f'); log.debug('removed '+name+'.f')
-        except OSError: pass
-        try: os.remove(name+'.o'); log.debug('removed '+name+'.o')
-        except OSError: pass
-        log.set_threshold(save_th)
-    atexit.register(rm_file)
-    return dummy_name
-
-is_f_file = re.compile(r'.*[.](for|ftn|f77|f)\Z',re.I).match
-_has_f_header = re.compile(r'-[*]-\s*fortran\s*-[*]-',re.I).search
-_has_f90_header = re.compile(r'-[*]-\s*f90\s*-[*]-',re.I).search
-_has_fix_header = re.compile(r'-[*]-\s*fix\s*-[*]-',re.I).search
-_free_f90_start = re.compile(r'[^c*]\s*[^\s\d\t]',re.I).match
+    fo, name = make_temp_file(suffix='.f')
+    fo.write("      subroutine dummy()\n      end\n")
+    fo.close()
+    return name[:-2]
+
+
+is_f_file = re.compile(r'.*\.(for|ftn|f77|f)\Z', re.I).match
+_has_f_header = re.compile(r'-\*-\s*fortran\s*-\*-', re.I).search
+_has_f90_header = re.compile(r'-\*-\s*f90\s*-\*-', re.I).search
+_has_fix_header = re.compile(r'-\*-\s*fix\s*-\*-', re.I).search
+_free_f90_start = re.compile(r'[^c*!]\s*[^\s\d\t]', re.I).match
+
 def is_free_format(file):
     """Check if file is in free format Fortran."""
     # f90 allows both fixed and free format, assuming fixed unless
     # signs of free format are detected.
     result = 0
-    f = open(file,'r')
-    line = f.readline()
-    n = 15 # the number of non-comment lines to scan for hints
-    if _has_f_header(line):
-        n = 0
-    elif _has_f90_header(line):
-        n = 0
-        result = 1
-    while n>0 and line:
-        if line[0]!='!':
-            n -= 1
-            if (line[0]!='\t' and _free_f90_start(line[:5])) or line[-2:-1]=='&':
-                result = 1
-                break
+    with open(file, encoding='latin1') as f:
         line = f.readline()
-    f.close()
+        n = 10000 # the number of non-comment lines to scan for hints
+        if _has_f_header(line) or _has_fix_header(line):
+            n = 0
+        elif _has_f90_header(line):
+            n = 0
+            result = 1
+        while n>0 and line:
+            line = line.rstrip()
+            if line and line[0]!='!':
+                n -= 1
+                if (line[0]!='\t' and _free_f90_start(line[:5])) or line[-1:]=='&':
+                    result = 1
+                    break
+            line = f.readline()
     return result
 
 def has_f90_header(src):
-    f = open(src,'r')
-    line = f.readline()
-    f.close()
+    with open(src, encoding='latin1') as f:
+        line = f.readline()
     return _has_f90_header(line) or _has_fix_header(line)
+
+_f77flags_re = re.compile(r'(c|)f77flags\s*\(\s*(?P<fcname>\w+)\s*\)\s*=\s*(?P<fflags>.*)', re.I)
+def get_f77flags(src):
+    """
+    Search the first 20 lines of fortran 77 code for line pattern
+      `CF77FLAGS(<fcompiler type>)=<f77 flags>`
+    Return a dictionary {<fcompiler type>:<f77 flags>}.
+    """
+    flags = {}
+    with open(src, encoding='latin1') as f:
+        i = 0
+        for line in f:
+            i += 1
+            if i>20: break
+            m = _f77flags_re.match(line)
+            if not m: continue
+            fcname = m.group('fcname').strip()
+            fflags = m.group('fflags').strip()
+            flags[fcname] = split_quoted(fflags)
+    return flags
+
+# TODO: implement get_f90flags and use it in _compile similarly to get_f77flags
 
 if __name__ == '__main__':
     show_fcompilers()
('numpy/distutils/fcompiler', 'g95.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,22 +1,26 @@
 # http://g95.sourceforge.net/
-
-import os
-import sys
-
-from numpy.distutils.cpuinfo import cpu
 from numpy.distutils.fcompiler import FCompiler
 
+compilers = ['G95FCompiler']
+
 class G95FCompiler(FCompiler):
+    compiler_type = 'g95'
+    description = 'G95 Fortran Compiler'
 
-    compiler_type = 'g95'
-    version_pattern = r'G95.*\(experimental\) \(g95!\) (?P<version>.*)\).*'
+#    version_pattern = r'G95 \((GCC (?P<gccversion>[\d.]+)|.*?) \(g95!\) (?P<version>.*)\).*'
+    # $ g95 --version
+    # G95 (GCC 4.0.3 (g95!) May 22 2006)
+
+    version_pattern = r'G95 \((GCC (?P<gccversion>[\d.]+)|.*?) \(g95 (?P<version>.*)!\) (?P<date>.*)\).*'
+    # $ g95 --version
+    # G95 (GCC 4.0.3 (g95 0.90!) Aug 22 2006)
 
     executables = {
-        'version_cmd'  : ["g95", "--version"],
+        'version_cmd'  : ["<F90>", "--version"],
         'compiler_f77' : ["g95", "-ffixed-form"],
         'compiler_fix' : ["g95", "-ffixed-form"],
         'compiler_f90' : ["g95"],
-        'linker_so'    : ["g95","-shared"],
+        'linker_so'    : ["<F90>", "-shared"],
         'archiver'     : ["ar", "-cr"],
         'ranlib'       : ["ranlib"]
         }
@@ -33,9 +37,6 @@
 
 if __name__ == '__main__':
     from distutils import log
+    from numpy.distutils import customized_fcompiler
     log.set_verbosity(2)
-    from numpy.distutils.fcompiler import new_fcompiler
-    #compiler = new_fcompiler(compiler='g95')
-    compiler = G95FCompiler()
-    compiler.customize()
-    print compiler.get_version()
+    print(customized_fcompiler('g95').get_version())
('numpy/distutils/fcompiler', 'mips.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,20 +1,20 @@
-import os
-import sys
-
 from numpy.distutils.cpuinfo import cpu
 from numpy.distutils.fcompiler import FCompiler
 
-class MipsFCompiler(FCompiler):
+compilers = ['MIPSFCompiler']
+
+class MIPSFCompiler(FCompiler):
 
     compiler_type = 'mips'
+    description = 'MIPSpro Fortran Compiler'
     version_pattern =  r'MIPSpro Compilers: Version (?P<version>[^\s*,]*)'
 
     executables = {
-        'version_cmd'  : ["f90", "-version"],
+        'version_cmd'  : ["<F90>", "-version"],
         'compiler_f77' : ["f77", "-f77"],
         'compiler_fix' : ["f90", "-fixedform"],
         'compiler_f90' : ["f90"],
-        'linker_so'    : ["f90","-shared"],
+        'linker_so'    : ["f90", "-shared"],
         'archiver'     : ["ar", "-cr"],
         'ranlib'       : None
         }
@@ -29,7 +29,7 @@
     def get_flags_arch(self):
         opt = []
         for a in '19 20 21 22_4k 22_5k 24 25 26 27 28 30 32_5k 32_10k'.split():
-            if getattr(cpu,'is_IP%s'%a)():
+            if getattr(cpu, 'is_IP%s'%a)():
                 opt.append('-TARG:platform=IP%s' % a)
                 break
         return opt
@@ -50,7 +50,5 @@
         return r
 
 if __name__ == '__main__':
-    from numpy.distutils.fcompiler import new_fcompiler
-    compiler = new_fcompiler(compiler='mips')
-    compiler.customize()
-    print compiler.get_version()
+    from numpy.distutils import customized_fcompiler
+    print(customized_fcompiler(compiler='mips').get_version())
('numpy/distutils/fcompiler', 'hpux.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,12 +1,11 @@
-import os
-import sys
+from numpy.distutils.fcompiler import FCompiler
 
-from numpy.distutils.cpuinfo import cpu
-from numpy.distutils.fcompiler import FCompiler
+compilers = ['HPUXFCompiler']
 
 class HPUXFCompiler(FCompiler):
 
     compiler_type = 'hpux'
+    description = 'HP Fortran 90 Compiler'
     version_pattern =  r'HP F90 (?P<version>[^\s*,]*)'
 
     executables = {
@@ -14,28 +13,29 @@
         'compiler_f77' : ["f90"],
         'compiler_fix' : ["f90"],
         'compiler_f90' : ["f90"],
-        'linker_so'    : None,
+        'linker_so'    : ["ld", "-b"],
         'archiver'     : ["ar", "-cr"],
         'ranlib'       : ["ranlib"]
         }
     module_dir_switch = None #XXX: fix me
     module_include_switch = None #XXX: fix me
-    pic_flags = ['+pic=long']
+    pic_flags = ['+Z']
     def get_flags(self):
-        return self.pic_flags + ['+ppu']
+        return self.pic_flags + ['+ppu', '+DD64']
     def get_flags_opt(self):
         return ['-O3']
     def get_libraries(self):
         return ['m']
-    def get_version(self, force=0, ok_status=[256,0]):
+    def get_library_dirs(self):
+        opt = ['/usr/lib/hpux64']
+        return opt
+    def get_version(self, force=0, ok_status=[256, 0, 1]):
         # XXX status==256 may indicate 'unrecognized option' or
         #     'no input file'. So, version_cmd needs more work.
-        return FCompiler.get_version(self,force,ok_status)
+        return FCompiler.get_version(self, force, ok_status)
 
 if __name__ == '__main__':
     from distutils import log
     log.set_verbosity(10)
-    from numpy.distutils.fcompiler import new_fcompiler
-    compiler = new_fcompiler(compiler='hpux')
-    compiler.customize()
-    print compiler.get_version()
+    from numpy.distutils import customized_fcompiler
+    print(customized_fcompiler(compiler='hpux').get_version())
('numpy/distutils/fcompiler', 'absoft.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -5,17 +5,18 @@
 # Notes:
 # - when using -g77 then use -DUNDERSCORE_G77 to compile f2py
 #   generated extension modules (works for f2py v2.45.241_1936 and up)
-
 import os
-import sys
 
 from numpy.distutils.cpuinfo import cpu
 from numpy.distutils.fcompiler import FCompiler, dummy_fortran_file
 from numpy.distutils.misc_util import cyg2win32
 
+compilers = ['AbsoftFCompiler']
+
 class AbsoftFCompiler(FCompiler):
 
     compiler_type = 'absoft'
+    description = 'Absoft Corp Fortran Compiler'
     #version_pattern = r'FORTRAN 77 Compiler (?P<version>[^\s*,]*).*?Absoft Corp'
     version_pattern = r'(f90:.*?(Absoft Pro FORTRAN Version|FORTRAN 77 Compiler|Absoft Fortran Compiler Version|Copyright Absoft Corporation.*?Version))'+\
                        r' (?P<version>[^\s*,]*)(.*?Absoft Corp|)'
@@ -28,12 +29,11 @@
     # Note that fink installs g77 as f77, so need to use f90 for detection.
 
     executables = {
-        'version_cmd'  : ["f90", "-V -c %(fname)s.f -o %(fname)s.o" \
-                          % {'fname':cyg2win32(dummy_fortran_file())}],
+        'version_cmd'  : None,          # set by update_executables
         'compiler_f77' : ["f77"],
         'compiler_fix' : ["f90"],
         'compiler_f90' : ["f90"],
-        'linker_so'    : ["f90"],
+        'linker_so'    : ["<F90>"],
         'archiver'     : ["ar", "-cr"],
         'ranlib'       : ["ranlib"]
         }
@@ -43,6 +43,11 @@
 
     module_dir_switch = None
     module_include_switch = '-p'
+
+    def update_executables(self):
+        f = cyg2win32(dummy_fortran_file())
+        self.executables['version_cmd'] = ['<F90>', '-V', '-c',
+                                           f+'.f', '-o', f+'.o']
 
     def get_flags_linker_so(self):
         if os.name=='nt':
@@ -54,12 +59,12 @@
         elif self.get_version() >= '9.0':
             opt = ['-shared']
         else:
-            opt = ["-K","shared"]
+            opt = ["-K", "shared"]
         return opt
 
     def library_dir_option(self, dir):
         if os.name=='nt':
-            return ['-link','/PATH:"%s"' % (dir)]
+            return ['-link', '/PATH:%s' % (dir)]
         return "-L" + dir
 
     def library_option(self, lib):
@@ -85,12 +90,14 @@
 
     def get_libraries(self):
         opt = FCompiler.get_libraries(self)
-        if self.get_version() >= '10.0':
+        if self.get_version() >= '11.0':
+            opt.extend(['af90math', 'afio', 'af77math', 'amisc'])
+        elif self.get_version() >= '10.0':
             opt.extend(['af90math', 'afio', 'af77math', 'U77'])
         elif self.get_version() >= '8.0':
-            opt.extend(['f90math','fio','f77math','U77'])
+            opt.extend(['f90math', 'fio', 'f77math', 'U77'])
         else:
-            opt.extend(['fio','f90math','fmath','U77'])
+            opt.extend(['fio', 'f90math', 'fmath', 'U77'])
         if os.name =='nt':
             opt.append('COMDLG32')
         return opt
@@ -106,11 +113,11 @@
 
     def get_flags_f77(self):
         opt = FCompiler.get_flags_f77(self)
-        opt.extend(['-N22','-N90','-N110'])
+        opt.extend(['-N22', '-N90', '-N110'])
         v = self.get_version()
         if os.name == 'nt':
             if v and v>='8.0':
-                opt.extend(['-f','-N15'])
+                opt.extend(['-f', '-N15'])
         else:
             opt.append('-f')
             if v:
@@ -124,8 +131,8 @@
 
     def get_flags_f90(self):
         opt = FCompiler.get_flags_f90(self)
-        opt.extend(["-YCFRL=1","-YCOM_NAMES=LCS","-YCOM_PFX","-YEXT_PFX",
-                    "-YCOM_SFX=_","-YEXT_SFX=_","-YEXT_NAMES=LCS"])
+        opt.extend(["-YCFRL=1", "-YCOM_NAMES=LCS", "-YCOM_PFX", "-YEXT_PFX",
+                    "-YCOM_SFX=_", "-YEXT_SFX=_", "-YEXT_NAMES=LCS"])
         if self.get_version():
             if self.get_version()>'4.6':
                 opt.extend(["-YDEALLOC=ALL"])
@@ -133,9 +140,9 @@
 
     def get_flags_fix(self):
         opt = FCompiler.get_flags_fix(self)
-        opt.extend(["-YCFRL=1","-YCOM_NAMES=LCS","-YCOM_PFX","-YEXT_PFX",
-                    "-YCOM_SFX=_","-YEXT_SFX=_","-YEXT_NAMES=LCS"])
-        opt.extend(["-f","fixed"])
+        opt.extend(["-YCFRL=1", "-YCOM_NAMES=LCS", "-YCOM_PFX", "-YEXT_PFX",
+                    "-YCOM_SFX=_", "-YEXT_SFX=_", "-YEXT_NAMES=LCS"])
+        opt.extend(["-f", "fixed"])
         return opt
 
     def get_flags_opt(self):
@@ -145,7 +152,5 @@
 if __name__ == '__main__':
     from distutils import log
     log.set_verbosity(2)
-    from numpy.distutils.fcompiler import new_fcompiler
-    compiler = new_fcompiler(compiler='absoft')
-    compiler.customize()
-    print compiler.get_version()
+    from numpy.distutils import customized_fcompiler
+    print(customized_fcompiler(compiler='absoft').get_version())
('numpy/distutils/fcompiler', 'vast.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,14 +1,15 @@
 import os
-import sys
 
-from numpy.distutils.cpuinfo import cpu
 from numpy.distutils.fcompiler.gnu import GnuFCompiler
 
+compilers = ['VastFCompiler']
+
 class VastFCompiler(GnuFCompiler):
-
     compiler_type = 'vast'
-    version_pattern = r'\s*Pacific-Sierra Research vf90 '\
-                      '(Personal|Professional)\s+(?P<version>[^\s]*)'
+    compiler_aliases = ()
+    description = 'Pacific-Sierra Research Fortran 90 Compiler'
+    version_pattern = (r'\s*Pacific-Sierra Research vf90 '
+                       r'(Personal|Professional)\s+(?P<version>[^\s]*)')
 
     # VAST f90 does not support -o with -c. So, object files are created
     # to the current directory and then moved to build directory
@@ -19,23 +20,26 @@
         'compiler_f77' : ["g77"],
         'compiler_fix' : ["f90", "-Wv,-ya"],
         'compiler_f90' : ["f90"],
-        'linker_so'    : ["f90"],
+        'linker_so'    : ["<F90>"],
         'archiver'     : ["ar", "-cr"],
         'ranlib'       : ["ranlib"]
         }
     module_dir_switch = None  #XXX Fix me
     module_include_switch = None #XXX Fix me
 
+    def find_executables(self):
+        pass
+
     def get_version_cmd(self):
         f90 = self.compiler_f90[0]
-        d,b = os.path.split(f90)
-        vf90 = os.path.join(d,'v'+b)
+        d, b = os.path.split(f90)
+        vf90 = os.path.join(d, 'v'+b)
         return vf90
 
     def get_flags_arch(self):
         vast_version = self.get_version()
         gnu = GnuFCompiler()
-        gnu.customize()
+        gnu.customize(None)
         self.version = gnu.get_version()
         opt = GnuFCompiler.get_flags_arch(self)
         self.version = vast_version
@@ -44,7 +48,5 @@
 if __name__ == '__main__':
     from distutils import log
     log.set_verbosity(2)
-    from numpy.distutils.fcompiler import new_fcompiler
-    compiler = new_fcompiler(compiler='vast')
-    compiler.customize()
-    print compiler.get_version()
+    from numpy.distutils import customized_fcompiler
+    print(customized_fcompiler(compiler='vast').get_version())
('numpy/distutils/command', 'build.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -2,17 +2,61 @@
 import sys
 from distutils.command.build import build as old_build
 from distutils.util import get_platform
+from numpy.distutils.command.config_compiler import show_fortran_compilers
 
 class build(old_build):
 
-    sub_commands = [('config_fc',     lambda *args: 1),
+    sub_commands = [('config_cc',     lambda *args: True),
+                    ('config_fc',     lambda *args: True),
                     ('build_src',     old_build.has_ext_modules),
                     ] + old_build.sub_commands
+
+    user_options = old_build.user_options + [
+        ('fcompiler=', None,
+         "specify the Fortran compiler type"),
+        ('warn-error', None,
+         "turn all warnings into errors (-Werror)"),
+        ('cpu-baseline=', None,
+         "specify a list of enabled baseline CPU optimizations"),
+        ('cpu-dispatch=', None,
+         "specify a list of dispatched CPU optimizations"),
+        ('disable-optimization', None,
+         "disable CPU optimized code(dispatch,simd,fast...)"),
+        ('simd-test=', None,
+         "specify a list of CPU optimizations to be tested against NumPy SIMD interface"),
+        ]
+
+    help_options = old_build.help_options + [
+        ('help-fcompiler', None, "list available Fortran compilers",
+         show_fortran_compilers),
+        ]
+
+    def initialize_options(self):
+        old_build.initialize_options(self)
+        self.fcompiler = None
+        self.warn_error = False
+        self.cpu_baseline = "min"
+        self.cpu_dispatch = "max -xop -fma4" # drop AMD legacy features by default
+        self.disable_optimization = False
+        """
+        the '_simd' module is a very large. Adding more dispatched features
+        will increase binary size and compile time. By default we minimize
+        the targeted features to those most commonly used by the NumPy SIMD interface(NPYV),
+        NOTE: any specified features will be ignored if they're:
+            - part of the baseline(--cpu-baseline)
+            - not part of dispatch-able features(--cpu-dispatch)
+            - not supported by compiler or platform
+        """
+        self.simd_test = "BASELINE SSE2 SSE42 XOP FMA4 (FMA3 AVX2) AVX512F " \
+                         "AVX512_SKX VSX VSX2 VSX3 VSX4 NEON ASIMD VX VXE VXE2"
 
     def finalize_options(self):
         build_scripts = self.build_scripts
         old_build.finalize_options(self)
-        plat_specifier = ".%s-%s" % (get_platform(), sys.version[0:3])
+        plat_specifier = ".{}-{}.{}".format(get_platform(), *sys.version_info[:2])
         if build_scripts is None:
             self.build_scripts = os.path.join(self.build_base,
                                               'scripts' + plat_specifier)
+
+    def run(self):
+        old_build.run(self)
('numpy/distutils/command', 'config_compiler.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,20 +1,19 @@
+from distutils.core import Command
+from numpy.distutils import log
 
-import sys
-from distutils.core import Command
-
-#XXX: Implement confic_cc for enhancing C/C++ compiler options.
 #XXX: Linker flags
 
-def show_fortran_compilers(_cache=[]):
-    # Using cache to prevent infinite recursion
-    if _cache: return
+def show_fortran_compilers(_cache=None):
+    # Using cache to prevent infinite recursion.
+    if _cache:
+        return
+    elif _cache is None:
+        _cache = []
     _cache.append(1)
-    
     from numpy.distutils.fcompiler import show_fcompilers
     import distutils.core
     dist = distutils.core._setup_distribution
     show_fcompilers(dist)
-    return
 
 class config_fc(Command):
     """ Distutils command to hold user specified options
@@ -23,25 +22,27 @@
     config_fc command is used by the FCompiler.customize() method.
     """
 
+    description = "specify Fortran 77/Fortran 90 compiler information"
+
     user_options = [
-        ('fcompiler=',None,"specify Fortran compiler type"),
+        ('fcompiler=', None, "specify Fortran compiler type"),
         ('f77exec=', None, "specify F77 compiler command"),
         ('f90exec=', None, "specify F90 compiler command"),
-        ('f77flags=',None,"specify F77 compiler flags"),
-        ('f90flags=',None,"specify F90 compiler flags"),
-        ('opt=',None,"specify optimization flags"),
-        ('arch=',None,"specify architecture specific optimization flags"),
-        ('debug','g',"compile with debugging information"),
-        ('noopt',None,"compile without optimization"),
-        ('noarch',None,"compile without arch-dependent optimization"),
+        ('f77flags=', None, "specify F77 compiler flags"),
+        ('f90flags=', None, "specify F90 compiler flags"),
+        ('opt=', None, "specify optimization flags"),
+        ('arch=', None, "specify architecture specific optimization flags"),
+        ('debug', 'g', "compile with debugging information"),
+        ('noopt', None, "compile without optimization"),
+        ('noarch', None, "compile without arch-dependent optimization"),
         ]
 
     help_options = [
-        ('help-fcompiler',None, "list available Fortran compilers",
+        ('help-fcompiler', None, "list available Fortran compilers",
          show_fortran_compilers),
         ]
 
-    boolean_options = ['debug','noopt','noarch']
+    boolean_options = ['debug', 'noopt', 'noarch']
 
     def initialize_options(self):
         self.fcompiler = None
@@ -54,10 +55,70 @@
         self.debug = None
         self.noopt = None
         self.noarch = None
+
+    def finalize_options(self):
+        log.info('unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options')
+        build_clib = self.get_finalized_command('build_clib')
+        build_ext = self.get_finalized_command('build_ext')
+        config = self.get_finalized_command('config')
+        build = self.get_finalized_command('build')
+        cmd_list = [self, config, build_clib, build_ext, build]
+        for a in ['fcompiler']:
+            l = []
+            for c in cmd_list:
+                v = getattr(c, a)
+                if v is not None:
+                    if not isinstance(v, str): v = v.compiler_type
+                    if v not in l: l.append(v)
+            if not l: v1 = None
+            else: v1 = l[0]
+            if len(l)>1:
+                log.warn('  commands have different --%s options: %s'\
+                         ', using first in list as default' % (a, l))
+            if v1:
+                for c in cmd_list:
+                    if getattr(c, a) is None: setattr(c, a, v1)
+
+    def run(self):
+        # Do nothing.
         return
 
+class config_cc(Command):
+    """ Distutils command to hold user specified options
+    to C/C++ compilers.
+    """
+
+    description = "specify C/C++ compiler information"
+
+    user_options = [
+        ('compiler=', None, "specify C/C++ compiler type"),
+        ]
+
+    def initialize_options(self):
+        self.compiler = None
+
     def finalize_options(self):
-        # Do nothing.
+        log.info('unifing config_cc, config, build_clib, build_ext, build commands --compiler options')
+        build_clib = self.get_finalized_command('build_clib')
+        build_ext = self.get_finalized_command('build_ext')
+        config = self.get_finalized_command('config')
+        build = self.get_finalized_command('build')
+        cmd_list = [self, config, build_clib, build_ext, build]
+        for a in ['compiler']:
+            l = []
+            for c in cmd_list:
+                v = getattr(c, a)
+                if v is not None:
+                    if not isinstance(v, str): v = v.compiler_type
+                    if v not in l: l.append(v)
+            if not l: v1 = None
+            else: v1 = l[0]
+            if len(l)>1:
+                log.warn('  commands have different --%s options: %s'\
+                         ', using first in list as default' % (a, l))
+            if v1:
+                for c in cmd_list:
+                    if getattr(c, a) is None: setattr(c, a, v1)
         return
 
     def run(self):
('numpy/distutils/command', 'build_ext.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,19 +1,25 @@
 """ Modified version of build_ext that handles fortran source files.
+
 """
-
 import os
-import string
-import sys
+import subprocess
 from glob import glob
 
 from distutils.dep_util import newer_group
 from distutils.command.build_ext import build_ext as old_build_ext
+from distutils.errors import DistutilsFileError, DistutilsSetupError,\
+    DistutilsError
+from distutils.file_util import copy_file
 
 from numpy.distutils import log
-from numpy.distutils.misc_util import filter_sources, has_f_sources, \
-     has_cxx_sources, get_ext_source_files, all_strings, \
-     get_numpy_include_dirs, is_sequence
-from distutils.errors import DistutilsFileError, DistutilsSetupError
+from numpy.distutils.exec_command import filepath_from_subprocess_output
+from numpy.distutils.system_info import combine_paths
+from numpy.distutils.misc_util import (
+    filter_sources, get_ext_source_files, get_numpy_include_dirs,
+    has_cxx_sources, has_f_sources, is_sequence
+)
+from numpy.distutils.command.config_compiler import show_fortran_compilers
+from numpy.distutils.ccompiler_opt import new_ccompiler_opt, CCompilerOpt
 
 class build_ext (old_build_ext):
 
@@ -22,33 +28,96 @@
     user_options = old_build_ext.user_options + [
         ('fcompiler=', None,
          "specify the Fortran compiler type"),
-        ]
+        ('parallel=', 'j',
+         "number of parallel jobs"),
+        ('warn-error', None,
+         "turn all warnings into errors (-Werror)"),
+        ('cpu-baseline=', None,
+         "specify a list of enabled baseline CPU optimizations"),
+        ('cpu-dispatch=', None,
+         "specify a list of dispatched CPU optimizations"),
+        ('disable-optimization', None,
+         "disable CPU optimized code(dispatch,simd,fast...)"),
+        ('simd-test=', None,
+         "specify a list of CPU optimizations to be tested against NumPy SIMD interface"),
+    ]
+
+    help_options = old_build_ext.help_options + [
+        ('help-fcompiler', None, "list available Fortran compilers",
+         show_fortran_compilers),
+    ]
+
+    boolean_options = old_build_ext.boolean_options + ['warn-error', 'disable-optimization']
 
     def initialize_options(self):
         old_build_ext.initialize_options(self)
         self.fcompiler = None
-        return
+        self.parallel = None
+        self.warn_error = None
+        self.cpu_baseline = None
+        self.cpu_dispatch = None
+        self.disable_optimization = None
+        self.simd_test = None
 
     def finalize_options(self):
-        incl_dirs = self.include_dirs
+        if self.parallel:
+            try:
+                self.parallel = int(self.parallel)
+            except ValueError as e:
+                raise ValueError("--parallel/-j argument must be an integer") from e
+
+        # Ensure that self.include_dirs and self.distribution.include_dirs
+        # refer to the same list object. finalize_options will modify
+        # self.include_dirs, but self.distribution.include_dirs is used
+        # during the actual build.
+        # self.include_dirs is None unless paths are specified with
+        # --include-dirs.
+        # The include paths will be passed to the compiler in the order:
+        # numpy paths, --include-dirs paths, Python include path.
+        if isinstance(self.include_dirs, str):
+            self.include_dirs = self.include_dirs.split(os.pathsep)
+        incl_dirs = self.include_dirs or []
+        if self.distribution.include_dirs is None:
+            self.distribution.include_dirs = []
+        self.include_dirs = self.distribution.include_dirs
+        self.include_dirs.extend(incl_dirs)
+
         old_build_ext.finalize_options(self)
-        if incl_dirs is not None:
-            self.include_dirs.extend(self.distribution.include_dirs or [])
-        self.set_undefined_options('config_fc',
-                                   ('fcompiler', 'fcompiler'))
-        return
+        self.set_undefined_options('build',
+                                        ('parallel', 'parallel'),
+                                        ('warn_error', 'warn_error'),
+                                        ('cpu_baseline', 'cpu_baseline'),
+                                        ('cpu_dispatch', 'cpu_dispatch'),
+                                        ('disable_optimization', 'disable_optimization'),
+                                        ('simd_test', 'simd_test')
+                                  )
+        CCompilerOpt.conf_target_groups["simd_test"] = self.simd_test
 
     def run(self):
         if not self.extensions:
             return
 
         # Make sure that extension sources are complete.
-        for ext in self.extensions:
-            if not all_strings(ext.sources):
-                self.run_command('build_src')
+        self.run_command('build_src')
 
         if self.distribution.has_c_libraries():
-            build_clib = self.get_finalized_command('build_clib')
+            if self.inplace:
+                if self.distribution.have_run.get('build_clib'):
+                    log.warn('build_clib already run, it is too late to '
+                             'ensure in-place build of build_clib')
+                    build_clib = self.distribution.get_command_obj(
+                        'build_clib')
+                else:
+                    build_clib = self.distribution.get_command_obj(
+                        'build_clib')
+                    build_clib.inplace = 1
+                    build_clib.ensure_finalized()
+                    build_clib.run()
+                    self.distribution.have_run['build_clib'] = 1
+
+            else:
+                self.run_command('build_clib')
+                build_clib = self.get_finalized_command('build_clib')
             self.library_dirs.append(build_clib.build_clib)
         else:
             build_clib = None
@@ -58,68 +127,242 @@
         # bogus linking commands. Extensions must
         # explicitly specify the C libraries that they use.
 
-        # Determine if Fortran compiler is needed.
-        if build_clib and build_clib.fcompiler is not None:
-            need_f_compiler = 1
-        else:
-            need_f_compiler = 0
-            for ext in self.extensions:
-                if has_f_sources(ext.sources):
-                    need_f_compiler = 1
-                    break
-                if getattr(ext,'language','c') in ['f77','f90']:
-                    need_f_compiler = 1
-                    break
-
-        # Determine if C++ compiler is needed.
-        need_cxx_compiler = 0
-        for ext in self.extensions:
-            if has_cxx_sources(ext.sources):
-                need_cxx_compiler = 1
-                break
-            if getattr(ext,'language','c')=='c++':
-                need_cxx_compiler = 1
-                break
-
         from distutils.ccompiler import new_compiler
-        self.compiler = new_compiler(compiler=self.compiler,
+        from numpy.distutils.fcompiler import new_fcompiler
+
+        compiler_type = self.compiler
+        # Initialize C compiler:
+        self.compiler = new_compiler(compiler=compiler_type,
                                      verbose=self.verbose,
                                      dry_run=self.dry_run,
                                      force=self.force)
-        self.compiler.customize(self.distribution,need_cxx=need_cxx_compiler)
+        self.compiler.customize(self.distribution)
         self.compiler.customize_cmd(self)
+
+        if self.warn_error:
+            self.compiler.compiler.append('-Werror')
+            self.compiler.compiler_so.append('-Werror')
+
         self.compiler.show_customization()
 
-        # Initialize Fortran/C++ compilers if needed.
-        if need_f_compiler:
-            from numpy.distutils.fcompiler import new_fcompiler
-            self.fcompiler = new_fcompiler(compiler=self.fcompiler,
-                                           verbose=self.verbose,
-                                           dry_run=self.dry_run,
-                                           force=self.force)
-            if self.fcompiler.get_version():
-                self.fcompiler.customize(self.distribution)
-                self.fcompiler.customize_cmd(self)
-                self.fcompiler.show_customization()
+        if not self.disable_optimization:
+            dispatch_hpath = os.path.join("numpy", "distutils", "include", "npy_cpu_dispatch_config.h")
+            dispatch_hpath = os.path.join(self.get_finalized_command("build_src").build_src, dispatch_hpath)
+            opt_cache_path = os.path.abspath(
+                os.path.join(self.build_temp, 'ccompiler_opt_cache_ext.py')
+            )
+            if hasattr(self, "compiler_opt"):
+                # By default `CCompilerOpt` update the cache at the exit of
+                # the process, which may lead to duplicate building
+                # (see build_extension()/force_rebuild) if run() called
+                # multiple times within the same os process/thread without
+                # giving the chance the previous instances of `CCompilerOpt`
+                # to update the cache.
+                self.compiler_opt.cache_flush()
+
+            self.compiler_opt = new_ccompiler_opt(
+                compiler=self.compiler, dispatch_hpath=dispatch_hpath,
+                cpu_baseline=self.cpu_baseline, cpu_dispatch=self.cpu_dispatch,
+                cache_path=opt_cache_path
+            )
+            def report(copt):
+                log.info("\n########### EXT COMPILER OPTIMIZATION ###########")
+                log.info(copt.report(full=True))
+
+            import atexit
+            atexit.register(report, self.compiler_opt)
+
+        # Setup directory for storing generated extra DLL files on Windows
+        self.extra_dll_dir = os.path.join(self.build_temp, '.libs')
+        if not os.path.isdir(self.extra_dll_dir):
+            os.makedirs(self.extra_dll_dir)
+
+        # Create mapping of libraries built by build_clib:
+        clibs = {}
+        if build_clib is not None:
+            for libname, build_info in build_clib.libraries or []:
+                if libname in clibs and clibs[libname] != build_info:
+                    log.warn('library %r defined more than once,'
+                             ' overwriting build_info\n%s... \nwith\n%s...'
+                             % (libname, repr(clibs[libname])[:300], repr(build_info)[:300]))
+                clibs[libname] = build_info
+        # .. and distribution libraries:
+        for libname, build_info in self.distribution.libraries or []:
+            if libname in clibs:
+                # build_clib libraries have a precedence before distribution ones
+                continue
+            clibs[libname] = build_info
+
+        # Determine if C++/Fortran 77/Fortran 90 compilers are needed.
+        # Update extension libraries, library_dirs, and macros.
+        all_languages = set()
+        for ext in self.extensions:
+            ext_languages = set()
+            c_libs = []
+            c_lib_dirs = []
+            macros = []
+            for libname in ext.libraries:
+                if libname in clibs:
+                    binfo = clibs[libname]
+                    c_libs += binfo.get('libraries', [])
+                    c_lib_dirs += binfo.get('library_dirs', [])
+                    for m in binfo.get('macros', []):
+                        if m not in macros:
+                            macros.append(m)
+
+                for l in clibs.get(libname, {}).get('source_languages', []):
+                    ext_languages.add(l)
+            if c_libs:
+                new_c_libs = ext.libraries + c_libs
+                log.info('updating extension %r libraries from %r to %r'
+                         % (ext.name, ext.libraries, new_c_libs))
+                ext.libraries = new_c_libs
+                ext.library_dirs = ext.library_dirs + c_lib_dirs
+            if macros:
+                log.info('extending extension %r defined_macros with %r'
+                         % (ext.name, macros))
+                ext.define_macros = ext.define_macros + macros
+
+            # determine extension languages
+            if has_f_sources(ext.sources):
+                ext_languages.add('f77')
+            if has_cxx_sources(ext.sources):
+                ext_languages.add('c++')
+            l = ext.language or self.compiler.detect_language(ext.sources)
+            if l:
+                ext_languages.add(l)
+
+            # reset language attribute for choosing proper linker
+            #
+            # When we build extensions with multiple languages, we have to
+            # choose a linker. The rules here are:
+            #   1. if there is Fortran code, always prefer the Fortran linker,
+            #   2. otherwise prefer C++ over C,
+            #   3. Users can force a particular linker by using
+            #          `language='c'`  # or 'c++', 'f90', 'f77'
+            #      in their config.add_extension() calls.
+            if 'c++' in ext_languages:
+                ext_language = 'c++'
             else:
-                self.warn('fcompiler=%s is not available.' % (self.fcompiler.compiler_type))
-                self.fcompiler = None
+                ext_language = 'c'  # default
+
+            has_fortran = False
+            if 'f90' in ext_languages:
+                ext_language = 'f90'
+                has_fortran = True
+            elif 'f77' in ext_languages:
+                ext_language = 'f77'
+                has_fortran = True
+
+            if not ext.language or has_fortran:
+                if l and l != ext_language and ext.language:
+                    log.warn('resetting extension %r language from %r to %r.' %
+                             (ext.name, l, ext_language))
+
+                ext.language = ext_language
+
+            # global language
+            all_languages.update(ext_languages)
+
+        need_f90_compiler = 'f90' in all_languages
+        need_f77_compiler = 'f77' in all_languages
+        need_cxx_compiler = 'c++' in all_languages
+
+        # Initialize C++ compiler:
+        if need_cxx_compiler:
+            self._cxx_compiler = new_compiler(compiler=compiler_type,
+                                              verbose=self.verbose,
+                                              dry_run=self.dry_run,
+                                              force=self.force)
+            compiler = self._cxx_compiler
+            compiler.customize(self.distribution, need_cxx=need_cxx_compiler)
+            compiler.customize_cmd(self)
+            compiler.show_customization()
+            self._cxx_compiler = compiler.cxx_compiler()
+        else:
+            self._cxx_compiler = None
+
+        # Initialize Fortran 77 compiler:
+        if need_f77_compiler:
+            ctype = self.fcompiler
+            self._f77_compiler = new_fcompiler(compiler=self.fcompiler,
+                                               verbose=self.verbose,
+                                               dry_run=self.dry_run,
+                                               force=self.force,
+                                               requiref90=False,
+                                               c_compiler=self.compiler)
+            fcompiler = self._f77_compiler
+            if fcompiler:
+                ctype = fcompiler.compiler_type
+                fcompiler.customize(self.distribution)
+            if fcompiler and fcompiler.get_version():
+                fcompiler.customize_cmd(self)
+                fcompiler.show_customization()
+            else:
+                self.warn('f77_compiler=%s is not available.' %
+                          (ctype))
+                self._f77_compiler = None
+        else:
+            self._f77_compiler = None
+
+        # Initialize Fortran 90 compiler:
+        if need_f90_compiler:
+            ctype = self.fcompiler
+            self._f90_compiler = new_fcompiler(compiler=self.fcompiler,
+                                               verbose=self.verbose,
+                                               dry_run=self.dry_run,
+                                               force=self.force,
+                                               requiref90=True,
+                                               c_compiler=self.compiler)
+            fcompiler = self._f90_compiler
+            if fcompiler:
+                ctype = fcompiler.compiler_type
+                fcompiler.customize(self.distribution)
+            if fcompiler and fcompiler.get_version():
+                fcompiler.customize_cmd(self)
+                fcompiler.show_customization()
+            else:
+                self.warn('f90_compiler=%s is not available.' %
+                          (ctype))
+                self._f90_compiler = None
+        else:
+            self._f90_compiler = None
 
         # Build extensions
         self.build_extensions()
-        return
-
-    def swig_sources(self, sources):
-        # Do nothing. Swig sources have beed handled in build_src command.
+
+        # Copy over any extra DLL files
+        # FIXME: In the case where there are more than two packages,
+        # we blindly assume that both packages need all of the libraries,
+        # resulting in a larger wheel than is required. This should be fixed,
+        # but it's so rare that I won't bother to handle it.
+        pkg_roots = {
+            self.get_ext_fullname(ext.name).split('.')[0]
+            for ext in self.extensions
+        }
+        for pkg_root in pkg_roots:
+            shared_lib_dir = os.path.join(pkg_root, '.libs')
+            if not self.inplace:
+                shared_lib_dir = os.path.join(self.build_lib, shared_lib_dir)
+            for fn in os.listdir(self.extra_dll_dir):
+                if not os.path.isdir(shared_lib_dir):
+                    os.makedirs(shared_lib_dir)
+                if not fn.lower().endswith('.dll'):
+                    continue
+                runtime_lib = os.path.join(self.extra_dll_dir, fn)
+                copy_file(runtime_lib, shared_lib_dir)
+
+    def swig_sources(self, sources, extensions=None):
+        # Do nothing. Swig sources have been handled in build_src command.
         return sources
 
     def build_extension(self, ext):
         sources = ext.sources
         if sources is None or not is_sequence(sources):
-            raise DistutilsSetupError, \
-                  ("in 'ext_modules' option (extension '%s'), " +
-                   "'sources' must be present and must be " +
-                   "a list of source filenames") % ext.name
+            raise DistutilsSetupError(
+                ("in 'ext_modules' option (extension '%s'), " +
+                 "'sources' must be present and must be " +
+                 "a list of source filenames") % ext.name)
         sources = list(sources)
 
         if not sources:
@@ -127,10 +370,9 @@
 
         fullname = self.get_ext_fullname(ext.name)
         if self.inplace:
-            modpath = string.split(fullname, '.')
-            package = string.join(modpath[0:-1], '.')
+            modpath = fullname.split('.')
+            package = '.'.join(modpath[0:-1])
             base = modpath[-1]
-
             build_py = self.get_finalized_command('build_py')
             package_dir = build_py.get_package_dir(package)
             ext_filename = os.path.join(package_dir,
@@ -140,29 +382,28 @@
                                         self.get_ext_filename(fullname))
         depends = sources + ext.depends
 
-        if not (self.force or newer_group(depends, ext_filename, 'newer')):
+        force_rebuild = self.force
+        if not self.disable_optimization and not self.compiler_opt.is_cached():
+            log.debug("Detected changes on compiler optimizations")
+            force_rebuild = True
+        if not (force_rebuild or newer_group(depends, ext_filename, 'newer')):
             log.debug("skipping '%s' extension (up-to-date)", ext.name)
             return
         else:
             log.info("building '%s' extension", ext.name)
 
         extra_args = ext.extra_compile_args or []
+        extra_cflags = getattr(ext, 'extra_c_compile_args', None) or []
+        extra_cxxflags = getattr(ext, 'extra_cxx_compile_args', None) or []
+
         macros = ext.define_macros[:]
         for undef in ext.undef_macros:
             macros.append((undef,))
 
-        clib_libraries = []
-        clib_library_dirs = []
-        if self.distribution.libraries:
-            for libname,build_info in self.distribution.libraries:
-                if libname in ext.libraries:
-                    macros.extend(build_info.get('macros',[]))
-                    clib_libraries.extend(build_info.get('libraries',[]))
-                    clib_library_dirs.extend(build_info.get('library_dirs',[]))
-
         c_sources, cxx_sources, f_sources, fmodule_sources = \
-                   filter_sources(ext.sources)
-        if self.compiler.compiler_type=='msvc':
+            filter_sources(ext.sources)
+
+        if self.compiler.compiler_type == 'msvc':
             if cxx_sources:
                 # Needed to compile kiva.agg._agg extension.
                 extra_args.append('/Zm1000')
@@ -171,173 +412,315 @@
             c_sources += cxx_sources
             cxx_sources = []
 
-
-        kws = {'depends':ext.depends}
+        # Set Fortran/C++ compilers for compilation and linking.
+        if ext.language == 'f90':
+            fcompiler = self._f90_compiler
+        elif ext.language == 'f77':
+            fcompiler = self._f77_compiler
+        else:  # in case ext.language is c++, for instance
+            fcompiler = self._f90_compiler or self._f77_compiler
+        if fcompiler is not None:
+            fcompiler.extra_f77_compile_args = (ext.extra_f77_compile_args or []) if hasattr(
+                ext, 'extra_f77_compile_args') else []
+            fcompiler.extra_f90_compile_args = (ext.extra_f90_compile_args or []) if hasattr(
+                ext, 'extra_f90_compile_args') else []
+        cxx_compiler = self._cxx_compiler
+
+        # check for the availability of required compilers
+        if cxx_sources and cxx_compiler is None:
+            raise DistutilsError("extension %r has C++ sources"
+                                 "but no C++ compiler found" % (ext.name))
+        if (f_sources or fmodule_sources) and fcompiler is None:
+            raise DistutilsError("extension %r has Fortran sources "
+                                 "but no Fortran compiler found" % (ext.name))
+        if ext.language in ['f77', 'f90'] and fcompiler is None:
+            self.warn("extension %r has Fortran libraries "
+                      "but no Fortran linker found, using default linker" % (ext.name))
+        if ext.language == 'c++' and cxx_compiler is None:
+            self.warn("extension %r has C++ libraries "
+                      "but no C++ linker found, using default linker" % (ext.name))
+
+        kws = {'depends': ext.depends}
         output_dir = self.build_temp
 
         include_dirs = ext.include_dirs + get_numpy_include_dirs()
 
+        # filtering C dispatch-table sources when optimization is not disabled,
+        # otherwise treated as normal sources.
+        copt_c_sources = []
+        copt_cxx_sources = []
+        copt_baseline_flags = []
+        copt_macros = []
+        if not self.disable_optimization:
+            bsrc_dir = self.get_finalized_command("build_src").build_src
+            dispatch_hpath = os.path.join("numpy", "distutils", "include")
+            dispatch_hpath = os.path.join(bsrc_dir, dispatch_hpath)
+            include_dirs.append(dispatch_hpath)
+
+            copt_build_src = None if self.inplace else bsrc_dir
+            for _srcs, _dst, _ext in (
+                ((c_sources,), copt_c_sources, ('.dispatch.c',)),
+                ((c_sources, cxx_sources), copt_cxx_sources,
+                    ('.dispatch.cpp', '.dispatch.cxx'))
+            ):
+                for _src in _srcs:
+                    _dst += [
+                        _src.pop(_src.index(s))
+                        for s in _src[:] if s.endswith(_ext)
+                    ]
+            copt_baseline_flags = self.compiler_opt.cpu_baseline_flags()
+        else:
+            copt_macros.append(("NPY_DISABLE_OPTIMIZATION", 1))
+
         c_objects = []
+        if copt_cxx_sources:
+            log.info("compiling C++ dispatch-able sources")
+            c_objects += self.compiler_opt.try_dispatch(
+                copt_cxx_sources,
+                output_dir=output_dir,
+                src_dir=copt_build_src,
+                macros=macros + copt_macros,
+                include_dirs=include_dirs,
+                debug=self.debug,
+                extra_postargs=extra_args + extra_cxxflags,
+                ccompiler=cxx_compiler,
+                **kws
+            )
+        if copt_c_sources:
+            log.info("compiling C dispatch-able sources")
+            c_objects += self.compiler_opt.try_dispatch(
+                copt_c_sources,
+                output_dir=output_dir,
+                src_dir=copt_build_src,
+                macros=macros + copt_macros,
+                include_dirs=include_dirs,
+                debug=self.debug,
+                extra_postargs=extra_args + extra_cflags,
+                **kws)
         if c_sources:
             log.info("compiling C sources")
-            c_objects = self.compiler.compile(c_sources,
-                                              output_dir=output_dir,
-                                              macros=macros,
-                                              include_dirs=include_dirs,
-                                              debug=self.debug,
-                                              extra_postargs=extra_args,
-                                              **kws)
+            c_objects += self.compiler.compile(
+                c_sources,
+                output_dir=output_dir,
+                macros=macros + copt_macros,
+                include_dirs=include_dirs,
+                debug=self.debug,
+                extra_postargs=(extra_args + copt_baseline_flags +
+                                extra_cflags),
+                **kws)
         if cxx_sources:
             log.info("compiling C++ sources")
-
-            old_compiler = self.compiler.compiler_so[0]
-            self.compiler.compiler_so[0] = self.compiler.compiler_cxx[0]
-
-            c_objects += self.compiler.compile(cxx_sources,
-                                              output_dir=output_dir,
-                                              macros=macros,
-                                              include_dirs=include_dirs,
-                                              debug=self.debug,
-                                              extra_postargs=extra_args,
-                                              **kws)
-            self.compiler.compiler_so[0] = old_compiler
-
-        check_for_f90_modules = not not fmodule_sources
-
-        if f_sources or fmodule_sources:
-            extra_postargs = []
+            c_objects += cxx_compiler.compile(
+                cxx_sources,
+                output_dir=output_dir,
+                macros=macros + copt_macros,
+                include_dirs=include_dirs,
+                debug=self.debug,
+                extra_postargs=(extra_args + copt_baseline_flags +
+                                extra_cxxflags),
+                **kws)
+
+        extra_postargs = []
+        f_objects = []
+        if fmodule_sources:
+            log.info("compiling Fortran 90 module sources")
             module_dirs = ext.module_dirs[:]
-
-            #if self.fcompiler.compiler_type=='ibm':
-            macros = []
-
-            if check_for_f90_modules:
-                module_build_dir = os.path.join(\
-                    self.build_temp,os.path.dirname(\
+            module_build_dir = os.path.join(
+                self.build_temp, os.path.dirname(
                     self.get_ext_filename(fullname)))
 
-                self.mkpath(module_build_dir)
-                if self.fcompiler.module_dir_switch is None:
-                    existing_modules = glob('*.mod')
-                extra_postargs += self.fcompiler.module_options(\
-                    module_dirs,module_build_dir)
-
-            f_objects = []
-            if fmodule_sources:
-                log.info("compiling Fortran 90 module sources")
-                f_objects = self.fcompiler.compile(fmodule_sources,
-                                                   output_dir=self.build_temp,
-                                                   macros=macros,
-                                                   include_dirs=include_dirs,
-                                                   debug=self.debug,
-                                                   extra_postargs=extra_postargs,
-                                                   depends=ext.depends)
-
-            if check_for_f90_modules \
-                   and self.fcompiler.module_dir_switch is None:
+            self.mkpath(module_build_dir)
+            if fcompiler.module_dir_switch is None:
+                existing_modules = glob('*.mod')
+            extra_postargs += fcompiler.module_options(
+                module_dirs, module_build_dir)
+            f_objects += fcompiler.compile(fmodule_sources,
+                                           output_dir=self.build_temp,
+                                           macros=macros,
+                                           include_dirs=include_dirs,
+                                           debug=self.debug,
+                                           extra_postargs=extra_postargs,
+                                           depends=ext.depends)
+
+            if fcompiler.module_dir_switch is None:
                 for f in glob('*.mod'):
                     if f in existing_modules:
                         continue
+                    t = os.path.join(module_build_dir, f)
+                    if os.path.abspath(f) == os.path.abspath(t):
+                        continue
+                    if os.path.isfile(t):
+                        os.remove(t)
                     try:
                         self.move_file(f, module_build_dir)
-                    except DistutilsFileError:  # already exists in destination
-                        os.remove(f)
-
-            if f_sources:
-                log.info("compiling Fortran sources")
-                f_objects += self.fcompiler.compile(f_sources,
-                                                    output_dir=self.build_temp,
-                                                    macros=macros,
-                                                    include_dirs=include_dirs,
-                                                    debug=self.debug,
-                                                    extra_postargs=extra_postargs,
-                                                    depends=ext.depends)
+                    except DistutilsFileError:
+                        log.warn('failed to move %r to %r' %
+                                 (f, module_build_dir))
+        if f_sources:
+            log.info("compiling Fortran sources")
+            f_objects += fcompiler.compile(f_sources,
+                                           output_dir=self.build_temp,
+                                           macros=macros,
+                                           include_dirs=include_dirs,
+                                           debug=self.debug,
+                                           extra_postargs=extra_postargs,
+                                           depends=ext.depends)
+
+        if f_objects and not fcompiler.can_ccompiler_link(self.compiler):
+            unlinkable_fobjects = f_objects
+            objects = c_objects
         else:
-            f_objects = []
-
-        objects = c_objects + f_objects
+            unlinkable_fobjects = []
+            objects = c_objects + f_objects
 
         if ext.extra_objects:
             objects.extend(ext.extra_objects)
         extra_args = ext.extra_link_args or []
-
-        try:
-            old_linker_so_0 = self.compiler.linker_so[0]
-        except:
-            pass
-
-        use_fortran_linker = getattr(ext,'language','c') in ['f77','f90'] \
-                             and self.fcompiler is not None
-        c_libraries = []
-        c_library_dirs = []
-        if use_fortran_linker or f_sources:
-            use_fortran_linker = 1
-        elif self.distribution.has_c_libraries():
-            build_clib = self.get_finalized_command('build_clib')
-            f_libs = []
-            for (lib_name, build_info) in build_clib.libraries:
-                if has_f_sources(build_info.get('sources',[])):
-                    f_libs.append(lib_name)
-                if lib_name in ext.libraries:
-                    # XXX: how to determine if c_libraries contain
-                    # fortran compiled sources?
-                    c_libraries.extend(build_info.get('libraries',[]))
-                    c_library_dirs.extend(build_info.get('library_dirs',[]))
-            for l in ext.libraries:
-                if l in f_libs:
-                    use_fortran_linker = 1
+        libraries = self.get_libraries(ext)[:]
+        library_dirs = ext.library_dirs[:]
+
+        linker = self.compiler.link_shared_object
+        # Always use system linker when using MSVC compiler.
+        if self.compiler.compiler_type in ('msvc', 'intelw', 'intelemw'):
+            # expand libraries with fcompiler libraries as we are
+            # not using fcompiler linker
+            self._libs_with_msvc_and_fortran(
+                fcompiler, libraries, library_dirs)
+
+        elif ext.language in ['f77', 'f90'] and fcompiler is not None:
+            linker = fcompiler.link_shared_object
+        if ext.language == 'c++' and cxx_compiler is not None:
+            linker = cxx_compiler.link_shared_object
+
+        if fcompiler is not None:
+            objects, libraries = self._process_unlinkable_fobjects(
+                    objects, libraries,
+                    fcompiler, library_dirs,
+                    unlinkable_fobjects)
+
+        linker(objects, ext_filename,
+               libraries=libraries,
+               library_dirs=library_dirs,
+               runtime_library_dirs=ext.runtime_library_dirs,
+               extra_postargs=extra_args,
+               export_symbols=self.get_export_symbols(ext),
+               debug=self.debug,
+               build_temp=self.build_temp,
+               target_lang=ext.language)
+
+    def _add_dummy_mingwex_sym(self, c_sources):
+        build_src = self.get_finalized_command("build_src").build_src
+        build_clib = self.get_finalized_command("build_clib").build_clib
+        objects = self.compiler.compile([os.path.join(build_src,
+                                                      "gfortran_vs2003_hack.c")],
+                                        output_dir=self.build_temp)
+        self.compiler.create_static_lib(
+            objects, "_gfortran_workaround", output_dir=build_clib, debug=self.debug)
+
+    def _process_unlinkable_fobjects(self, objects, libraries,
+                                     fcompiler, library_dirs,
+                                     unlinkable_fobjects):
+        libraries = list(libraries)
+        objects = list(objects)
+        unlinkable_fobjects = list(unlinkable_fobjects)
+
+        # Expand possible fake static libraries to objects;
+        # make sure to iterate over a copy of the list as
+        # "fake" libraries will be removed as they are
+        # encountered
+        for lib in libraries[:]:
+            for libdir in library_dirs:
+                fake_lib = os.path.join(libdir, lib + '.fobjects')
+                if os.path.isfile(fake_lib):
+                    # Replace fake static library
+                    libraries.remove(lib)
+                    with open(fake_lib, 'r') as f:
+                        unlinkable_fobjects.extend(f.read().splitlines())
+
+                    # Expand C objects
+                    c_lib = os.path.join(libdir, lib + '.cobjects')
+                    with open(c_lib, 'r') as f:
+                        objects.extend(f.read().splitlines())
+
+        # Wrap unlinkable objects to a linkable one
+        if unlinkable_fobjects:
+            fobjects = [os.path.abspath(obj) for obj in unlinkable_fobjects]
+            wrapped = fcompiler.wrap_unlinkable_objects(
+                    fobjects, output_dir=self.build_temp,
+                    extra_dll_dir=self.extra_dll_dir)
+            objects.extend(wrapped)
+
+        return objects, libraries
+
+    def _libs_with_msvc_and_fortran(self, fcompiler, c_libraries,
+                                    c_library_dirs):
+        if fcompiler is None:
+            return
+
+        for libname in c_libraries:
+            if libname.startswith('msvc'):
+                continue
+            fileexists = False
+            for libdir in c_library_dirs or []:
+                libfile = os.path.join(libdir, '%s.lib' % (libname))
+                if os.path.isfile(libfile):
+                    fileexists = True
                     break
+            if fileexists:
+                continue
+            # make g77-compiled static libs available to MSVC
+            fileexists = False
+            for libdir in c_library_dirs:
+                libfile = os.path.join(libdir, 'lib%s.a' % (libname))
+                if os.path.isfile(libfile):
+                    # copy libname.a file to name.lib so that MSVC linker
+                    # can find it
+                    libfile2 = os.path.join(self.build_temp, libname + '.lib')
+                    copy_file(libfile, libfile2)
+                    if self.build_temp not in c_library_dirs:
+                        c_library_dirs.append(self.build_temp)
+                    fileexists = True
+                    break
+            if fileexists:
+                continue
+            log.warn('could not find library %r in directories %s'
+                     % (libname, c_library_dirs))
 
         # Always use system linker when using MSVC compiler.
-        if self.compiler.compiler_type=='msvc' and use_fortran_linker:
-            c_libraries.extend(self.fcompiler.libraries)
-            c_library_dirs.extend(self.fcompiler.library_dirs)
-            use_fortran_linker = 0
-
-        if use_fortran_linker:
-            if cxx_sources:
-                # XXX: Which linker should be used, Fortran or C++?
-                log.warn('mixing Fortran and C++ is untested')
-            link = self.fcompiler.link_shared_object
-            language = ext.language or self.fcompiler.detect_language(f_sources)
-        else:
-            link = self.compiler.link_shared_object
-            if sys.version[:3]>='2.3':
-                language = ext.language or self.compiler.detect_language(sources)
-            else:
-                language = ext.language
-            if cxx_sources:
-                self.compiler.linker_so[0] = self.compiler.compiler_cxx[0]
-
-        if sys.version[:3]>='2.3':
-            kws = {'target_lang':language}
-        else:
-            kws = {}
-
-        link(objects, ext_filename,
-             libraries=self.get_libraries(ext) + c_libraries + clib_libraries,
-             library_dirs=ext.library_dirs + c_library_dirs + clib_library_dirs,
-             runtime_library_dirs=ext.runtime_library_dirs,
-             extra_postargs=extra_args,
-             export_symbols=self.get_export_symbols(ext),
-             debug=self.debug,
-             build_temp=self.build_temp,**kws)
-
-        try:
-            self.compiler.linker_so[0] = old_linker_so_0
-        except:
-            pass
-
-        return
-
-    def get_source_files (self):
+        f_lib_dirs = []
+        for dir in fcompiler.library_dirs:
+            # correct path when compiling in Cygwin but with normal Win
+            # Python
+            if dir.startswith('/usr/lib'):
+                try:
+                    dir = subprocess.check_output(['cygpath', '-w', dir])
+                except (OSError, subprocess.CalledProcessError):
+                    pass
+                else:
+                    dir = filepath_from_subprocess_output(dir)
+            f_lib_dirs.append(dir)
+        c_library_dirs.extend(f_lib_dirs)
+
+        # make g77-compiled static libs available to MSVC
+        for lib in fcompiler.libraries:
+            if not lib.startswith('msvc'):
+                c_libraries.append(lib)
+                p = combine_paths(f_lib_dirs, 'lib' + lib + '.a')
+                if p:
+                    dst_name = os.path.join(self.build_temp, lib + '.lib')
+                    if not os.path.isfile(dst_name):
+                        copy_file(p[0], dst_name)
+                    if self.build_temp not in c_library_dirs:
+                        c_library_dirs.append(self.build_temp)
+
+    def get_source_files(self):
         self.check_extensions_list(self.extensions)
         filenames = []
         for ext in self.extensions:
             filenames.extend(get_ext_source_files(ext))
         return filenames
 
-    def get_outputs (self):
+    def get_outputs(self):
         self.check_extensions_list(self.extensions)
 
         outputs = []
('numpy/distutils/command', 'config.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -2,62 +2,516 @@
 # try_compile call. try_run works but is untested for most of Fortran
 # compilers (they must define linker_exe first).
 # Pearu Peterson
+import os
+import signal
+import subprocess
+import sys
+import textwrap
+import warnings
 
 from distutils.command.config import config as old_config
 from distutils.command.config import LANG_EXT
+from distutils import log
+from distutils.file_util import copy_file
+from distutils.ccompiler import CompileError, LinkError
+import distutils
+from numpy.distutils.exec_command import filepath_from_subprocess_output
+from numpy.distutils.mingw32ccompiler import generate_manifest
+from numpy.distutils.command.autodist import (check_gcc_function_attribute,
+                                              check_gcc_function_attribute_with_intrinsics,
+                                              check_gcc_variable_attribute,
+                                              check_gcc_version_at_least,
+                                              check_inline,
+                                              check_restrict,
+                                              check_compiler_gcc)
+
 LANG_EXT['f77'] = '.f'
 LANG_EXT['f90'] = '.f90'
 
 class config(old_config):
     old_config.user_options += [
-        ('fcompiler=', None,
-         "specify the Fortran compiler type"),
+        ('fcompiler=', None, "specify the Fortran compiler type"),
         ]
 
     def initialize_options(self):
         self.fcompiler = None
         old_config.initialize_options(self)
-        return
-
-    def finalize_options(self):
-        old_config.finalize_options(self)
-        f = self.distribution.get_command_obj('config_fc')
-        self.set_undefined_options('config_fc',
-                                   ('fcompiler', 'fcompiler'))
-        return
 
     def _check_compiler (self):
         old_config._check_compiler(self)
         from numpy.distutils.fcompiler import FCompiler, new_fcompiler
+
+        if sys.platform == 'win32' and (self.compiler.compiler_type in
+                                        ('msvc', 'intelw', 'intelemw')):
+            # XXX: hack to circumvent a python 2.6 bug with msvc9compiler:
+            # initialize call query_vcvarsall, which throws an IOError, and
+            # causes an error along the way without much information. We try to
+            # catch it here, hoping it is early enough, and print an helpful
+            # message instead of Error: None.
+            if not self.compiler.initialized:
+                try:
+                    self.compiler.initialize()
+                except IOError as e:
+                    msg = textwrap.dedent("""\
+                        Could not initialize compiler instance: do you have Visual Studio
+                        installed?  If you are trying to build with MinGW, please use "python setup.py
+                        build -c mingw32" instead.  If you have Visual Studio installed, check it is
+                        correctly installed, and the right version (VS 2008 for python 2.6, 2.7 and 3.2,
+                        VS 2010 for >= 3.3).
+
+                        Original exception was: %s, and the Compiler class was %s
+                        ============================================================================""") \
+                        % (e, self.compiler.__class__.__name__)
+                    print(textwrap.dedent("""\
+                        ============================================================================"""))
+                    raise distutils.errors.DistutilsPlatformError(msg) from e
+
+            # After MSVC is initialized, add an explicit /MANIFEST to linker
+            # flags.  See issues gh-4245 and gh-4101 for details.  Also
+            # relevant are issues 4431 and 16296 on the Python bug tracker.
+            from distutils import msvc9compiler
+            if msvc9compiler.get_build_version() >= 10:
+                for ldflags in [self.compiler.ldflags_shared,
+                                self.compiler.ldflags_shared_debug]:
+                    if '/MANIFEST' not in ldflags:
+                        ldflags.append('/MANIFEST')
+
         if not isinstance(self.fcompiler, FCompiler):
             self.fcompiler = new_fcompiler(compiler=self.fcompiler,
-                                           dry_run=self.dry_run, force=1)
-            self.fcompiler.customize(self.distribution)
-            self.fcompiler.customize_cmd(self)
-            self.fcompiler.show_customization()
-        return
-
-    def _wrap_method(self,mth,lang,args):
+                                           dry_run=self.dry_run, force=1,
+                                           c_compiler=self.compiler)
+            if self.fcompiler is not None:
+                self.fcompiler.customize(self.distribution)
+                if self.fcompiler.get_version():
+                    self.fcompiler.customize_cmd(self)
+                    self.fcompiler.show_customization()
+
+    def _wrap_method(self, mth, lang, args):
         from distutils.ccompiler import CompileError
         from distutils.errors import DistutilsExecError
         save_compiler = self.compiler
-        if lang in ['f77','f90']:
+        if lang in ['f77', 'f90']:
             self.compiler = self.fcompiler
+        if self.compiler is None:
+            raise CompileError('%s compiler is not set' % (lang,))
         try:
             ret = mth(*((self,)+args))
-        except (DistutilsExecError,CompileError),msg:
+        except (DistutilsExecError, CompileError) as e:
             self.compiler = save_compiler
-            raise CompileError
+            raise CompileError from e
         self.compiler = save_compiler
         return ret
 
     def _compile (self, body, headers, include_dirs, lang):
-        return self._wrap_method(old_config._compile,lang,
-                                 (body, headers, include_dirs, lang))
+        src, obj = self._wrap_method(old_config._compile, lang,
+                                     (body, headers, include_dirs, lang))
+        # _compile in unixcompiler.py sometimes creates .d dependency files.
+        # Clean them up.
+        self.temp_files.append(obj + '.d')
+        return src, obj
 
     def _link (self, body,
                headers, include_dirs,
                libraries, library_dirs, lang):
-        return self._wrap_method(old_config._link,lang,
+        if self.compiler.compiler_type=='msvc':
+            libraries = (libraries or [])[:]
+            library_dirs = (library_dirs or [])[:]
+            if lang in ['f77', 'f90']:
+                lang = 'c' # always use system linker when using MSVC compiler
+                if self.fcompiler:
+                    for d in self.fcompiler.library_dirs or []:
+                        # correct path when compiling in Cygwin but with
+                        # normal Win Python
+                        if d.startswith('/usr/lib'):
+                            try:
+                                d = subprocess.check_output(['cygpath',
+                                                             '-w', d])
+                            except (OSError, subprocess.CalledProcessError):
+                                pass
+                            else:
+                                d = filepath_from_subprocess_output(d)
+                        library_dirs.append(d)
+                    for libname in self.fcompiler.libraries or []:
+                        if libname not in libraries:
+                            libraries.append(libname)
+            for libname in libraries:
+                if libname.startswith('msvc'): continue
+                fileexists = False
+                for libdir in library_dirs or []:
+                    libfile = os.path.join(libdir, '%s.lib' % (libname))
+                    if os.path.isfile(libfile):
+                        fileexists = True
+                        break
+                if fileexists: continue
+                # make g77-compiled static libs available to MSVC
+                fileexists = False
+                for libdir in library_dirs:
+                    libfile = os.path.join(libdir, 'lib%s.a' % (libname))
+                    if os.path.isfile(libfile):
+                        # copy libname.a file to name.lib so that MSVC linker
+                        # can find it
+                        libfile2 = os.path.join(libdir, '%s.lib' % (libname))
+                        copy_file(libfile, libfile2)
+                        self.temp_files.append(libfile2)
+                        fileexists = True
+                        break
+                if fileexists: continue
+                log.warn('could not find library %r in directories %s' \
+                         % (libname, library_dirs))
+        elif self.compiler.compiler_type == 'mingw32':
+            generate_manifest(self)
+        return self._wrap_method(old_config._link, lang,
                                  (body, headers, include_dirs,
                                   libraries, library_dirs, lang))
+
+    def check_header(self, header, include_dirs=None, library_dirs=None, lang='c'):
+        self._check_compiler()
+        return self.try_compile(
+                "/* we need a dummy line to make distutils happy */",
+                [header], include_dirs)
+
+    def check_decl(self, symbol,
+                   headers=None, include_dirs=None):
+        self._check_compiler()
+        body = textwrap.dedent("""
+            int main(void)
+            {
+            #ifndef %s
+                (void) %s;
+            #endif
+                ;
+                return 0;
+            }""") % (symbol, symbol)
+
+        return self.try_compile(body, headers, include_dirs)
+
+    def check_macro_true(self, symbol,
+                         headers=None, include_dirs=None):
+        self._check_compiler()
+        body = textwrap.dedent("""
+            int main(void)
+            {
+            #if %s
+            #else
+            #error false or undefined macro
+            #endif
+                ;
+                return 0;
+            }""") % (symbol,)
+
+        return self.try_compile(body, headers, include_dirs)
+
+    def check_type(self, type_name, headers=None, include_dirs=None,
+            library_dirs=None):
+        """Check type availability. Return True if the type can be compiled,
+        False otherwise"""
+        self._check_compiler()
+
+        # First check the type can be compiled
+        body = textwrap.dedent(r"""
+            int main(void) {
+              if ((%(name)s *) 0)
+                return 0;
+              if (sizeof (%(name)s))
+                return 0;
+            }
+            """) % {'name': type_name}
+
+        st = False
+        try:
+            try:
+                self._compile(body % {'type': type_name},
+                        headers, include_dirs, 'c')
+                st = True
+            except distutils.errors.CompileError:
+                st = False
+        finally:
+            self._clean()
+
+        return st
+
+    def check_type_size(self, type_name, headers=None, include_dirs=None, library_dirs=None, expected=None):
+        """Check size of a given type."""
+        self._check_compiler()
+
+        # First check the type can be compiled
+        body = textwrap.dedent(r"""
+            typedef %(type)s npy_check_sizeof_type;
+            int main (void)
+            {
+                static int test_array [1 - 2 * !(((long) (sizeof (npy_check_sizeof_type))) >= 0)];
+                test_array [0] = 0
+
+                ;
+                return 0;
+            }
+            """)
+        self._compile(body % {'type': type_name},
+                headers, include_dirs, 'c')
+        self._clean()
+
+        if expected:
+            body = textwrap.dedent(r"""
+                typedef %(type)s npy_check_sizeof_type;
+                int main (void)
+                {
+                    static int test_array [1 - 2 * !(((long) (sizeof (npy_check_sizeof_type))) == %(size)s)];
+                    test_array [0] = 0
+
+                    ;
+                    return 0;
+                }
+                """)
+            for size in expected:
+                try:
+                    self._compile(body % {'type': type_name, 'size': size},
+                            headers, include_dirs, 'c')
+                    self._clean()
+                    return size
+                except CompileError:
+                    pass
+
+        # this fails to *compile* if size > sizeof(type)
+        body = textwrap.dedent(r"""
+            typedef %(type)s npy_check_sizeof_type;
+            int main (void)
+            {
+                static int test_array [1 - 2 * !(((long) (sizeof (npy_check_sizeof_type))) <= %(size)s)];
+                test_array [0] = 0
+
+                ;
+                return 0;
+            }
+            """)
+
+        # The principle is simple: we first find low and high bounds of size
+        # for the type, where low/high are looked up on a log scale. Then, we
+        # do a binary search to find the exact size between low and high
+        low = 0
+        mid = 0
+        while True:
+            try:
+                self._compile(body % {'type': type_name, 'size': mid},
+                        headers, include_dirs, 'c')
+                self._clean()
+                break
+            except CompileError:
+                #log.info("failure to test for bound %d" % mid)
+                low = mid + 1
+                mid = 2 * mid + 1
+
+        high = mid
+        # Binary search:
+        while low != high:
+            mid = (high - low) // 2 + low
+            try:
+                self._compile(body % {'type': type_name, 'size': mid},
+                        headers, include_dirs, 'c')
+                self._clean()
+                high = mid
+            except CompileError:
+                low = mid + 1
+        return low
+
+    def check_func(self, func,
+                   headers=None, include_dirs=None,
+                   libraries=None, library_dirs=None,
+                   decl=False, call=False, call_args=None):
+        # clean up distutils's config a bit: add void to main(), and
+        # return a value.
+        self._check_compiler()
+        body = []
+        if decl:
+            if type(decl) == str:
+                body.append(decl)
+            else:
+                body.append("int %s (void);" % func)
+        # Handle MSVC intrinsics: force MS compiler to make a function call.
+        # Useful to test for some functions when built with optimization on, to
+        # avoid build error because the intrinsic and our 'fake' test
+        # declaration do not match.
+        body.append("#ifdef _MSC_VER")
+        body.append("#pragma function(%s)" % func)
+        body.append("#endif")
+        body.append("int main (void) {")
+        if call:
+            if call_args is None:
+                call_args = ''
+            body.append("  %s(%s);" % (func, call_args))
+        else:
+            body.append("  %s;" % func)
+        body.append("  return 0;")
+        body.append("}")
+        body = '\n'.join(body) + "\n"
+
+        return self.try_link(body, headers, include_dirs,
+                             libraries, library_dirs)
+
+    def check_funcs_once(self, funcs,
+                   headers=None, include_dirs=None,
+                   libraries=None, library_dirs=None,
+                   decl=False, call=False, call_args=None):
+        """Check a list of functions at once.
+
+        This is useful to speed up things, since all the functions in the funcs
+        list will be put in one compilation unit.
+
+        Arguments
+        ---------
+        funcs : seq
+            list of functions to test
+        include_dirs : seq
+            list of header paths
+        libraries : seq
+            list of libraries to link the code snippet to
+        library_dirs : seq
+            list of library paths
+        decl : dict
+            for every (key, value), the declaration in the value will be
+            used for function in key. If a function is not in the
+            dictionary, no declaration will be used.
+        call : dict
+            for every item (f, value), if the value is True, a call will be
+            done to the function f.
+        """
+        self._check_compiler()
+        body = []
+        if decl:
+            for f, v in decl.items():
+                if v:
+                    body.append("int %s (void);" % f)
+
+        # Handle MS intrinsics. See check_func for more info.
+        body.append("#ifdef _MSC_VER")
+        for func in funcs:
+            body.append("#pragma function(%s)" % func)
+        body.append("#endif")
+
+        body.append("int main (void) {")
+        if call:
+            for f in funcs:
+                if f in call and call[f]:
+                    if not (call_args and f in call_args and call_args[f]):
+                        args = ''
+                    else:
+                        args = call_args[f]
+                    body.append("  %s(%s);" % (f, args))
+                else:
+                    body.append("  %s;" % f)
+        else:
+            for f in funcs:
+                body.append("  %s;" % f)
+        body.append("  return 0;")
+        body.append("}")
+        body = '\n'.join(body) + "\n"
+
+        return self.try_link(body, headers, include_dirs,
+                             libraries, library_dirs)
+
+    def check_inline(self):
+        """Return the inline keyword recognized by the compiler, empty string
+        otherwise."""
+        return check_inline(self)
+
+    def check_restrict(self):
+        """Return the restrict keyword recognized by the compiler, empty string
+        otherwise."""
+        return check_restrict(self)
+
+    def check_compiler_gcc(self):
+        """Return True if the C compiler is gcc"""
+        return check_compiler_gcc(self)
+
+    def check_gcc_function_attribute(self, attribute, name):
+        return check_gcc_function_attribute(self, attribute, name)
+
+    def check_gcc_function_attribute_with_intrinsics(self, attribute, name,
+                                                     code, include):
+        return check_gcc_function_attribute_with_intrinsics(self, attribute,
+                                                            name, code, include)
+
+    def check_gcc_variable_attribute(self, attribute):
+        return check_gcc_variable_attribute(self, attribute)
+
+    def check_gcc_version_at_least(self, major, minor=0, patchlevel=0):
+        """Return True if the GCC version is greater than or equal to the
+        specified version."""
+        return check_gcc_version_at_least(self, major, minor, patchlevel)
+
+    def get_output(self, body, headers=None, include_dirs=None,
+                   libraries=None, library_dirs=None,
+                   lang="c", use_tee=None):
+        """Try to compile, link to an executable, and run a program
+        built from 'body' and 'headers'. Returns the exit status code
+        of the program and its output.
+        """
+        # 2008-11-16, RemoveMe
+        warnings.warn("\n+++++++++++++++++++++++++++++++++++++++++++++++++\n"
+                      "Usage of get_output is deprecated: please do not \n"
+                      "use it anymore, and avoid configuration checks \n"
+                      "involving running executable on the target machine.\n"
+                      "+++++++++++++++++++++++++++++++++++++++++++++++++\n",
+                      DeprecationWarning, stacklevel=2)
+        self._check_compiler()
+        exitcode, output = 255, ''
+        try:
+            grabber = GrabStdout()
+            try:
+                src, obj, exe = self._link(body, headers, include_dirs,
+                                           libraries, library_dirs, lang)
+                grabber.restore()
+            except Exception:
+                output = grabber.data
+                grabber.restore()
+                raise
+            exe = os.path.join('.', exe)
+            try:
+                # specify cwd arg for consistency with
+                # historic usage pattern of exec_command()
+                # also, note that exe appears to be a string,
+                # which exec_command() handled, but we now
+                # use a list for check_output() -- this assumes
+                # that exe is always a single command
+                output = subprocess.check_output([exe], cwd='.')
+            except subprocess.CalledProcessError as exc:
+                exitstatus = exc.returncode
+                output = ''
+            except OSError:
+                # preserve the EnvironmentError exit status
+                # used historically in exec_command()
+                exitstatus = 127
+                output = ''
+            else:
+                output = filepath_from_subprocess_output(output)
+            if hasattr(os, 'WEXITSTATUS'):
+                exitcode = os.WEXITSTATUS(exitstatus)
+                if os.WIFSIGNALED(exitstatus):
+                    sig = os.WTERMSIG(exitstatus)
+                    log.error('subprocess exited with signal %d' % (sig,))
+                    if sig == signal.SIGINT:
+                        # control-C
+                        raise KeyboardInterrupt
+            else:
+                exitcode = exitstatus
+            log.info("success!")
+        except (CompileError, LinkError):
+            log.info("failure.")
+        self._clean()
+        return exitcode, output
+
+class GrabStdout:
+
+    def __init__(self):
+        self.sys_stdout = sys.stdout
+        self.data = ''
+        sys.stdout = self
+
+    def write (self, data):
+        self.sys_stdout.write(data)
+        self.data += data
+
+    def flush (self):
+        self.sys_stdout.flush()
+
+    def restore(self):
+        sys.stdout = self.sys_stdout
('numpy/distutils/command', 'install_headers.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -10,7 +10,7 @@
 
         prefix = os.path.dirname(self.install_dir)
         for header in headers:
-            if isinstance(header,tuple):
+            if isinstance(header, tuple):
                 # Kind of a hack, but I don't know where else to change this...
                 if header[0] == 'numpy.core':
                     header = ('numpy', header[1])
('numpy/distutils/command', 'build_py.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,24 +1,30 @@
-
 from distutils.command.build_py import build_py as old_build_py
 from numpy.distutils.misc_util import is_string
 
 class build_py(old_build_py):
+
+    def run(self):
+        build_src = self.get_finalized_command('build_src')
+        if build_src.py_modules_dict and self.packages is None:
+            self.packages = list(build_src.py_modules_dict.keys ())
+        old_build_py.run(self)
 
     def find_package_modules(self, package, package_dir):
         modules = old_build_py.find_package_modules(self, package, package_dir)
 
         # Find build_src generated *.py files.
         build_src = self.get_finalized_command('build_src')
-        modules += build_src.py_modules_dict.get(package,[])
+        modules += build_src.py_modules_dict.get(package, [])
 
         return modules
 
     def find_modules(self):
         old_py_modules = self.py_modules[:]
-        new_py_modules = filter(is_string, self.py_modules)
+        new_py_modules = [_m for _m in self.py_modules if is_string(_m)]
         self.py_modules[:] = new_py_modules
         modules = old_build_py.find_modules(self)
         self.py_modules[:] = old_py_modules
+
         return modules
 
     # XXX: Fix find_source_files for item in py_modules such that item is 3-tuple
('numpy/distutils/command', 'build_src.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,35 +1,62 @@
-""" Build swig, f2py, weave, sources.
+""" Build swig and f2py sources.
 """
-
 import os
 import re
 import sys
+import shlex
+import copy
 
 from distutils.command import build_ext
 from distutils.dep_util import newer_group, newer
 from distutils.util import get_platform
-
+from distutils.errors import DistutilsError, DistutilsSetupError
+
+
+# this import can't be done here, as it uses numpy stuff only available
+# after it's installed
+#import numpy.f2py
 from numpy.distutils import log
-from numpy.distutils.misc_util import fortran_ext_match, \
-     appendpath, is_string, is_sequence
+from numpy.distutils.misc_util import (
+    fortran_ext_match, appendpath, is_string, is_sequence, get_cmd
+    )
 from numpy.distutils.from_template import process_file as process_f_file
 from numpy.distutils.conv_template import process_file as process_c_file
 
+def subst_vars(target, source, d):
+    """Substitute any occurrence of @foo@ by d['foo'] from source file into
+    target."""
+    var = re.compile('@([a-zA-Z_]+)@')
+    with open(source, 'r') as fs:
+        with open(target, 'w') as ft:
+            for l in fs:
+                m = var.search(l)
+                if m:
+                    ft.write(l.replace('@%s@' % m.group(1), d[m.group(1)]))
+                else:
+                    ft.write(l)
+
 class build_src(build_ext.build_ext):
 
     description = "build sources from SWIG, F2PY files or a function"
 
     user_options = [
         ('build-src=', 'd', "directory to \"build\" sources to"),
-        ('f2pyflags=', None, "additonal flags to f2py"),
-        ('swigflags=', None, "additional flags to swig"),
+        ('f2py-opts=', None, "list of f2py command line options"),
+        ('swig=', None, "path to the SWIG executable"),
+        ('swig-opts=', None, "list of SWIG command line options"),
+        ('swig-cpp', None, "make SWIG create C++ files (default is autodetected from sources)"),
+        ('f2pyflags=', None, "additional flags to f2py (use --f2py-opts= instead)"), # obsolete
+        ('swigflags=', None, "additional flags to swig (use --swig-opts= instead)"), # obsolete
         ('force', 'f', "forcibly build everything (ignore file timestamps)"),
         ('inplace', 'i',
          "ignore build-lib and put compiled extensions into the source " +
          "directory alongside your pure Python modules"),
+        ('verbose-cfg', None,
+         "change logging level from WARN to INFO which will show all " +
+         "compiler output")
         ]
 
-    boolean_options = ['force','inplace']
+    boolean_options = ['force', 'inplace', 'verbose-cfg']
 
     help_options = []
 
@@ -44,9 +71,13 @@
         self.force = None
         self.inplace = None
         self.package_dir = None
-        self.f2pyflags = None
-        self.swigflags = None
-        return
+        self.f2pyflags = None # obsolete
+        self.f2py_opts = None
+        self.swigflags = None # obsolete
+        self.swig_opts = None
+        self.swig_cpp = None
+        self.swig = None
+        self.verbose_cfg = None
 
     def finalize_options(self):
         self.set_undefined_options('build',
@@ -61,38 +92,62 @@
         self.data_files = self.distribution.data_files or []
 
         if self.build_src is None:
-            plat_specifier = ".%s-%s" % (get_platform(), sys.version[0:3])
+            plat_specifier = ".{}-{}.{}".format(get_platform(), *sys.version_info[:2])
             self.build_src = os.path.join(self.build_base, 'src'+plat_specifier)
-        if self.inplace is None:
-            build_ext = self.get_finalized_command('build_ext')
-            self.inplace = build_ext.inplace
 
         # py_modules_dict is used in build_py.find_package_modules
         self.py_modules_dict = {}
 
-        if self.f2pyflags is None:
-            self.f2pyflags = []
-        else:
-            self.f2pyflags = self.f2pyflags.split() # XXX spaces??
-
-        if self.swigflags is None:
-            self.swigflags = []
-        else:
-            self.swigflags = self.swigflags.split() # XXX spaces??
-        return
+        if self.f2pyflags:
+            if self.f2py_opts:
+                log.warn('ignoring --f2pyflags as --f2py-opts already used')
+            else:
+                self.f2py_opts = self.f2pyflags
+            self.f2pyflags = None
+        if self.f2py_opts is None:
+            self.f2py_opts = []
+        else:
+            self.f2py_opts = shlex.split(self.f2py_opts)
+
+        if self.swigflags:
+            if self.swig_opts:
+                log.warn('ignoring --swigflags as --swig-opts already used')
+            else:
+                self.swig_opts = self.swigflags
+            self.swigflags = None
+
+        if self.swig_opts is None:
+            self.swig_opts = []
+        else:
+            self.swig_opts = shlex.split(self.swig_opts)
+
+        # use options from build_ext command
+        build_ext = self.get_finalized_command('build_ext')
+        if self.inplace is None:
+            self.inplace = build_ext.inplace
+        if self.swig_cpp is None:
+            self.swig_cpp = build_ext.swig_cpp
+        for c in ['swig', 'swig_opt']:
+            o = '--'+c.replace('_', '-')
+            v = getattr(build_ext, c, None)
+            if v:
+                if getattr(self, c):
+                    log.warn('both build_src and build_ext define %s option' % (o))
+                else:
+                    log.info('using "%s=%s" option from build_ext command' % (o, v))
+                    setattr(self, c, v)
 
     def run(self):
+        log.info("build_src")
         if not (self.extensions or self.libraries):
             return
         self.build_sources()
 
-        return
-
     def build_sources(self):
 
         if self.inplace:
-            self.get_package_dir = self.get_finalized_command('build_py')\
-                                   .get_package_dir
+            self.get_package_dir = \
+                     self.get_finalized_command('build_py').get_package_dir
 
         self.build_py_modules_sources()
 
@@ -106,8 +161,7 @@
                 self.build_extension_sources(ext)
 
         self.build_data_files_sources()
-
-        return
+        self.build_npy_pkg_config()
 
     def build_data_files_sources(self):
         if not self.data_files:
@@ -116,34 +170,86 @@
         from numpy.distutils.misc_util import get_data_files
         new_data_files = []
         for data in self.data_files:
-            if isinstance(data,str):
+            if isinstance(data, str):
                 new_data_files.append(data)
-            elif isinstance(data,tuple):
-                d,files = data
+            elif isinstance(data, tuple):
+                d, files = data
                 if self.inplace:
                     build_dir = self.get_package_dir('.'.join(d.split(os.sep)))
                 else:
-                    build_dir = os.path.join(self.build_src,d)
-                funcs = filter(callable,files)
-                files = filter(lambda f:not callable(f), files)
+                    build_dir = os.path.join(self.build_src, d)
+                funcs = [f for f in files if hasattr(f, '__call__')]
+                files = [f for f in files if not hasattr(f, '__call__')]
                 for f in funcs:
-                    if f.func_code.co_argcount==1:
+                    if f.__code__.co_argcount==1:
                         s = f(build_dir)
                     else:
                         s = f()
                     if s is not None:
-                        if isinstance(s,list):
+                        if isinstance(s, list):
                             files.extend(s)
-                        elif isinstance(s,str):
+                        elif isinstance(s, str):
                             files.append(s)
                         else:
                             raise TypeError(repr(s))
-                filenames = get_data_files((d,files))
+                filenames = get_data_files((d, files))
                 new_data_files.append((d, filenames))
             else:
-                raise
+                raise TypeError(repr(data))
         self.data_files[:] = new_data_files
-        return
+
+
+    def _build_npy_pkg_config(self, info, gd):
+        template, install_dir, subst_dict = info
+        template_dir = os.path.dirname(template)
+        for k, v in gd.items():
+            subst_dict[k] = v
+
+        if self.inplace == 1:
+            generated_dir = os.path.join(template_dir, install_dir)
+        else:
+            generated_dir = os.path.join(self.build_src, template_dir,
+                    install_dir)
+        generated = os.path.basename(os.path.splitext(template)[0])
+        generated_path = os.path.join(generated_dir, generated)
+        if not os.path.exists(generated_dir):
+            os.makedirs(generated_dir)
+
+        subst_vars(generated_path, template, subst_dict)
+
+        # Where to install relatively to install prefix
+        full_install_dir = os.path.join(template_dir, install_dir)
+        return full_install_dir, generated_path
+
+    def build_npy_pkg_config(self):
+        log.info('build_src: building npy-pkg config files')
+
+        # XXX: another ugly workaround to circumvent distutils brain damage. We
+        # need the install prefix here, but finalizing the options of the
+        # install command when only building sources cause error. Instead, we
+        # copy the install command instance, and finalize the copy so that it
+        # does not disrupt how distutils want to do things when with the
+        # original install command instance.
+        install_cmd = copy.copy(get_cmd('install'))
+        if not install_cmd.finalized == 1:
+            install_cmd.finalize_options()
+        build_npkg = False
+        if self.inplace == 1:
+            top_prefix = '.'
+            build_npkg = True
+        elif hasattr(install_cmd, 'install_libbase'):
+            top_prefix = install_cmd.install_libbase
+            build_npkg = True
+
+        if build_npkg:
+            for pkg, infos in self.distribution.installed_pkg_config.items():
+                pkg_path = self.distribution.package_dir[pkg]
+                prefix = os.path.join(os.path.abspath(top_prefix), pkg_path)
+                d = {'prefix': prefix}
+                for info in infos:
+                    install_dir, generated = self._build_npy_pkg_config(info, d)
+                    self.distribution.data_files.append((install_dir,
+                        [generated]))
 
     def build_py_modules_sources(self):
         if not self.py_modules:
@@ -158,22 +264,21 @@
                 else:
                     build_dir = os.path.join(self.build_src,
                                              os.path.join(*package.split('.')))
-                if callable(source):
+                if hasattr(source, '__call__'):
                     target = os.path.join(build_dir, module_base + '.py')
                     source = source(target)
                 if source is None:
                     continue
                 modules = [(package, module_base, source)]
-                if not self.py_modules_dict.has_key(package):
+                if package not in self.py_modules_dict:
                     self.py_modules_dict[package] = []
                 self.py_modules_dict[package] += modules
             else:
                 new_py_modules.append(source)
         self.py_modules[:] = new_py_modules
-        return
 
     def build_library_sources(self, lib_name, build_info):
-        sources = list(build_info.get('sources',[]))
+        sources = list(build_info.get('sources', []))
 
         if not sources:
             return
@@ -187,7 +292,8 @@
         sources, h_files = self.filter_h_files(sources)
 
         if h_files:
-            print self.package,'- nothing done with h_files=',h_files
+            log.info('%s - nothing done with h_files = %s',
+                     self.package, h_files)
 
         #for f in h_files:
         #    self.distribution.headers.append((lib_name,f))
@@ -210,18 +316,14 @@
             self.ext_target_dir = self.get_package_dir(package)
 
         sources = self.generate_sources(sources, ext)
-
         sources = self.template_sources(sources, ext)
-
         sources = self.swig_sources(sources, ext)
-
         sources = self.f2py_sources(sources, ext)
-
         sources = self.pyrex_sources(sources, ext)
 
         sources, py_files = self.filter_py_files(sources)
 
-        if not self.py_modules_dict.has_key(package):
+        if package not in self.py_modules_dict:
             self.py_modules_dict[package] = []
         modules = []
         for f in py_files:
@@ -232,13 +334,12 @@
         sources, h_files = self.filter_h_files(sources)
 
         if h_files:
-            print package,'- nothing done with h_files=',h_files
+            log.info('%s - nothing done with h_files = %s',
+                     package, h_files)
         #for f in h_files:
         #    self.distribution.headers.append((package,f))
 
         ext.sources = sources
-
-        return
 
     def generate_sources(self, sources, extension):
         new_sources = []
@@ -255,7 +356,7 @@
         else:
             if is_sequence(extension):
                 name = extension[0]
-            #    if not extension[1].has_key('include_dirs'):
+            #    if 'include_dirs' not in extension[1]:
             #        extension[1]['include_dirs'] = []
             #    incl_dirs = extension[1]['include_dirs']
             else:
@@ -263,9 +364,16 @@
             #    incl_dirs = extension.include_dirs
             #if self.build_src not in incl_dirs:
             #    incl_dirs.append(self.build_src)
-            build_dir = os.path.join(*([self.build_src]\
+            build_dir = os.path.join(*([self.build_src]
                                        +name.split('.')[:-1]))
         self.mkpath(build_dir)
+
+        if self.verbose_cfg:
+            new_level = log.INFO
+        else:
+            new_level = log.WARN
+        old_level = log.set_threshold(new_level)
+
         for func in func_sources:
             source = func(extension, build_dir)
             if not source:
@@ -276,14 +384,14 @@
             else:
                 log.info("  adding '%s' to sources." % (source,))
                 new_sources.append(source)
-
+        log.set_threshold(old_level)
         return new_sources
 
     def filter_py_files(self, sources):
-        return self.filter_files(sources,['.py'])
+        return self.filter_files(sources, ['.py'])
 
     def filter_h_files(self, sources):
-        return self.filter_files(sources,['.h','.hpp','.inc'])
+        return self.filter_files(sources, ['.h', '.hpp', '.inc'])
 
     def filter_files(self, sources, exts = []):
         new_sources = []
@@ -312,7 +420,7 @@
                 else:
                     target_dir = appendpath(self.build_src, os.path.dirname(base))
                 self.mkpath(target_dir)
-                target_file = os.path.join(target_dir,os.path.basename(base))
+                target_file = os.path.join(target_dir, os.path.basename(base))
                 if (self.force or newer_group([source] + depends, target_file)):
                     if _f_pyf_ext_match(base):
                         log.info("from_template:> %s" % (target_file))
@@ -320,9 +428,8 @@
                     else:
                         log.info("conv_template:> %s" % (target_file))
                         outstr = process_c_file(source)
-                    fid = open(target_file,'w')
-                    fid.write(outstr)
-                    fid.close()
+                    with open(target_file, 'w') as fid:
+                        fid.write(outstr)
                 if _header_ext_match(target_file):
                     d = os.path.dirname(target_file)
                     if d not in include_dirs:
@@ -334,43 +441,27 @@
         return new_sources
 
     def pyrex_sources(self, sources, extension):
-        have_pyrex = False
-        try:
-            import Pyrex
-            have_pyrex = True
-        except ImportError:
-            pass
+        """Pyrex not supported; this remains for Cython support (see below)"""
         new_sources = []
         ext_name = extension.name.split('.')[-1]
         for source in sources:
             (base, ext) = os.path.splitext(source)
             if ext == '.pyx':
-                if self.inplace or not have_pyrex:
-                    target_dir = os.path.dirname(base)
-                else:
-                    target_dir = appendpath(self.build_src, os.path.dirname(base))
-                target_file = os.path.join(target_dir, ext_name + '.c')
-                depends = [source] + extension.depends
-                if (self.force or newer_group(depends, target_file, 'newer')):
-                    if have_pyrex:
-                        log.info("pyrexc:> %s" % (target_file))
-                        self.mkpath(target_dir)
-                        from Pyrex.Compiler import Main
-                        options = Main.CompilationOptions(
-                            defaults=Main.default_options,
-                            output_file=target_file)
-                        pyrex_result = Main.compile(source, options=options)
-                        if pyrex_result.num_errors != 0:
-                            raise RuntimeError("%d errors in Pyrex compile" %
-                                               pyrex_result.num_errors)
-                    else:
-                        log.warn("Pyrex needed to compile %s but not available."\
-                                 " Using old target %s"\
-                                 % (source, target_file))
+                target_file = self.generate_a_pyrex_source(base, ext_name,
+                                                           source,
+                                                           extension)
                 new_sources.append(target_file)
             else:
                 new_sources.append(source)
         return new_sources
+
+    def generate_a_pyrex_source(self, base, ext_name, source, extension):
+        """Pyrex is not supported, but some projects monkeypatch this method.
+
+        That allows compiling Cython code, see gh-6955.
+        This method will remain here for compatibility reasons.
+        """
+        return []
 
     def f2py_sources(self, sources, extension):
         new_sources = []
@@ -391,27 +482,27 @@
                 if os.path.isfile(source):
                     name = get_f2py_modulename(source)
                     if name != ext_name:
-                        raise ValueError('mismatch of extension names: %s '
-                                         'provides %r but expected %r' % (
-                                          source, name, ext_name))
-                    target_file = os.path.join(target_dir,name+'module.c')
+                        raise DistutilsSetupError('mismatch of extension names: %s '
+                                                  'provides %r but expected %r' % (
+                            source, name, ext_name))
+                    target_file = os.path.join(target_dir, name+'module.c')
                 else:
                     log.debug('  source %s does not exist: skipping f2py\'ing.' \
                               % (source))
                     name = ext_name
                     skip_f2py = 1
-                    target_file = os.path.join(target_dir,name+'module.c')
+                    target_file = os.path.join(target_dir, name+'module.c')
                     if not os.path.isfile(target_file):
-                        log.debug('  target %s does not exist:\n   '\
-                                  'Assuming %smodule.c was generated with '\
-                                  '"build_src --inplace" command.' \
-                                  % (target_file, name))
+                        log.warn('  target %s does not exist:\n   '\
+                                 'Assuming %smodule.c was generated with '\
+                                 '"build_src --inplace" command.' \
+                                 % (target_file, name))
                         target_dir = os.path.dirname(base)
-                        target_file = os.path.join(target_dir,name+'module.c')
+                        target_file = os.path.join(target_dir, name+'module.c')
                         if not os.path.isfile(target_file):
-                            raise ValueError("%r missing" % (target_file,))
-                        log.debug('   Yes! Using %s as up-to-date target.' \
-                                  % (target_file))
+                            raise DistutilsSetupError("%r missing" % (target_file,))
+                        log.info('   Yes! Using %r as up-to-date target.' \
+                                 % (target_file))
                 target_dirs.append(target_dir)
                 f2py_sources.append(source)
                 f2py_targets[source] = target_file
@@ -424,31 +515,33 @@
         if not (f2py_sources or f_sources):
             return new_sources
 
-        map(self.mkpath, target_dirs)
-
-        f2py_options = extension.f2py_options + self.f2pyflags
+        for d in target_dirs:
+            self.mkpath(d)
+
+        f2py_options = extension.f2py_options + self.f2py_opts
 
         if self.distribution.libraries:
-            for name,build_info in self.distribution.libraries:
+            for name, build_info in self.distribution.libraries:
                 if name in extension.libraries:
-                    f2py_options.extend(build_info.get('f2py_options',[]))
+                    f2py_options.extend(build_info.get('f2py_options', []))
 
         log.info("f2py options: %s" % (f2py_options))
 
         if f2py_sources:
             if len(f2py_sources) != 1:
-                raise ValueError(
+                raise DistutilsSetupError(
                     'only one .pyf file is allowed per extension module but got'\
                     ' more: %r' % (f2py_sources,))
             source = f2py_sources[0]
             target_file = f2py_targets[source]
             target_dir = os.path.dirname(target_file) or '.'
             depends = [source] + extension.depends
-            if (self.force or newer_group(depends, target_file,'newer')) \
+            if (self.force or newer_group(depends, target_file, 'newer')) \
                    and not skip_f2py:
                 log.info("f2py: %s" % (source))
-                import numpy.f2py as f2py2e
-                f2py2e.run_main(f2py_options + ['--build-dir',target_dir,source])
+                import numpy.f2py
+                numpy.f2py.run_main(f2py_options
+                                    + ['--build-dir', target_dir, source])
             else:
                 log.debug("  skipping '%s' f2py interface (up-to-date)" % (source))
         else:
@@ -456,52 +549,52 @@
             if is_sequence(extension):
                 name = extension[0]
             else: name = extension.name
-            target_dir = os.path.join(*([self.build_src]\
+            target_dir = os.path.join(*([self.build_src]
                                         +name.split('.')[:-1]))
-            target_file = os.path.join(target_dir,ext_name + 'module.c')
+            target_file = os.path.join(target_dir, ext_name + 'module.c')
             new_sources.append(target_file)
             depends = f_sources + extension.depends
             if (self.force or newer_group(depends, target_file, 'newer')) \
                    and not skip_f2py:
-                import numpy.f2py as f2py2e
                 log.info("f2py:> %s" % (target_file))
                 self.mkpath(target_dir)
-                f2py2e.run_main(f2py_options + ['--lower',
-                                                '--build-dir',target_dir]+\
-                                ['-m',ext_name]+f_sources)
+                import numpy.f2py
+                numpy.f2py.run_main(f2py_options + ['--lower',
+                                                '--build-dir', target_dir]+\
+                                ['-m', ext_name]+f_sources)
             else:
                 log.debug("  skipping f2py fortran files for '%s' (up-to-date)"\
                           % (target_file))
 
         if not os.path.isfile(target_file):
-            raise ValueError("%r missing" % (target_file,))
-
-        target_c = os.path.join(self.build_src,'fortranobject.c')
-        target_h = os.path.join(self.build_src,'fortranobject.h')
+            raise DistutilsError("f2py target file %r not generated" % (target_file,))
+
+        build_dir = os.path.join(self.build_src, target_dir)
+        target_c = os.path.join(build_dir, 'fortranobject.c')
+        target_h = os.path.join(build_dir, 'fortranobject.h')
         log.info("  adding '%s' to sources." % (target_c))
         new_sources.append(target_c)
-        if self.build_src not in extension.include_dirs:
-            log.info("  adding '%s' to include_dirs." \
-                     % (self.build_src))
-            extension.include_dirs.append(self.build_src)
+        if build_dir not in extension.include_dirs:
+            log.info("  adding '%s' to include_dirs." % (build_dir))
+            extension.include_dirs.append(build_dir)
 
         if not skip_f2py:
-            import numpy.f2py as f2py2e
-            d = os.path.dirname(f2py2e.__file__)
-            source_c = os.path.join(d,'src','fortranobject.c')
-            source_h = os.path.join(d,'src','fortranobject.h')
-            if newer(source_c,target_c) or newer(source_h,target_h):
+            import numpy.f2py
+            d = os.path.dirname(numpy.f2py.__file__)
+            source_c = os.path.join(d, 'src', 'fortranobject.c')
+            source_h = os.path.join(d, 'src', 'fortranobject.h')
+            if newer(source_c, target_c) or newer(source_h, target_h):
                 self.mkpath(os.path.dirname(target_c))
-                self.copy_file(source_c,target_c)
-                self.copy_file(source_h,target_h)
+                self.copy_file(source_c, target_c)
+                self.copy_file(source_h, target_h)
         else:
             if not os.path.isfile(target_c):
-                raise ValueError("%r missing" % (target_c,))
+                raise DistutilsSetupError("f2py target_c file %r not found" % (target_c,))
             if not os.path.isfile(target_h):
-                raise ValueError("%r missing" % (target_h,))
-
-        for name_ext in ['-f2pywrappers.f','-f2pywrappers2.f90']:
-            filename = os.path.join(target_dir,ext_name + name_ext)
+                raise DistutilsSetupError("f2py target_h file %r not found" % (target_h,))
+
+        for name_ext in ['-f2pywrappers.f', '-f2pywrappers2.f90']:
+            filename = os.path.join(target_dir, ext_name + name_ext)
             if os.path.isfile(filename):
                 log.info("  adding '%s' to sources." % (filename))
                 f_sources.append(filename)
@@ -518,14 +611,24 @@
         target_dirs = []
         py_files = []     # swig generated .py files
         target_ext = '.c'
-        typ = None
-        is_cpp = 0
+        if '-c++' in extension.swig_opts:
+            typ = 'c++'
+            is_cpp = True
+            extension.swig_opts.remove('-c++')
+        elif self.swig_cpp:
+            typ = 'c++'
+            is_cpp = True
+        else:
+            typ = None
+            is_cpp = False
         skip_swig = 0
         ext_name = extension.name.split('.')[-1]
 
         for source in sources:
             (base, ext) = os.path.splitext(source)
             if ext == '.i': # SWIG interface file
+                # the code below assumes that the sources list
+                # contains not more than one .i SWIG interface file
                 if self.inplace:
                     target_dir = os.path.dirname(base)
                     py_target_dir = self.ext_target_dir
@@ -535,35 +638,46 @@
                 if os.path.isfile(source):
                     name = get_swig_modulename(source)
                     if name != ext_name[1:]:
-                        raise ValueError(
+                        raise DistutilsSetupError(
                             'mismatch of extension names: %s provides %r'
                             ' but expected %r' % (source, name, ext_name[1:]))
                     if typ is None:
                         typ = get_swig_target(source)
                         is_cpp = typ=='c++'
-                        if is_cpp:
-                            target_ext = '.cpp'
                     else:
-                        assert typ == get_swig_target(source), repr(typ)
-                    target_file = os.path.join(target_dir,'%s_wrap%s' \
+                        typ2 = get_swig_target(source)
+                        if typ2 is None:
+                            log.warn('source %r does not define swig target, assuming %s swig target' \
+                                     % (source, typ))
+                        elif typ!=typ2:
+                            log.warn('expected %r but source %r defines %r swig target' \
+                                     % (typ, source, typ2))
+                            if typ2=='c++':
+                                log.warn('resetting swig target to c++ (some targets may have .c extension)')
+                                is_cpp = True
+                            else:
+                                log.warn('assuming that %r has c++ swig target' % (source))
+                    if is_cpp:
+                        target_ext = '.cpp'
+                    target_file = os.path.join(target_dir, '%s_wrap%s' \
                                                % (name, target_ext))
                 else:
-                    log.debug('  source %s does not exist: skipping swig\'ing.' \
+                    log.warn('  source %s does not exist: skipping swig\'ing.' \
                              % (source))
                     name = ext_name[1:]
                     skip_swig = 1
                     target_file = _find_swig_target(target_dir, name)
                     if not os.path.isfile(target_file):
-                        log.debug('  target %s does not exist:\n   '\
-                                  'Assuming %s_wrap.{c,cpp} was generated with '\
-                                  '"build_src --inplace" command.' \
+                        log.warn('  target %s does not exist:\n   '\
+                                 'Assuming %s_wrap.{c,cpp} was generated with '\
+                                 '"build_src --inplace" command.' \
                                  % (target_file, name))
                         target_dir = os.path.dirname(base)
                         target_file = _find_swig_target(target_dir, name)
                         if not os.path.isfile(target_file):
-                            raise ValueError("%r missing" % (target_file,))
-                        log.debug('   Yes! Using %s as up-to-date target.' \
-                                  % (target_file))
+                            raise DistutilsSetupError("%r missing" % (target_file,))
+                        log.warn('   Yes! Using %r as up-to-date target.' \
+                                 % (target_file))
                 target_dirs.append(target_dir)
                 new_sources.append(target_file)
                 py_files.append(os.path.join(py_target_dir, name+'.py'))
@@ -578,9 +692,11 @@
         if skip_swig:
             return new_sources + py_files
 
-        map(self.mkpath, target_dirs)
-        swig = self.find_swig()
-        swig_cmd = [swig, "-python"]
+        for d in target_dirs:
+            self.mkpath(d)
+
+        swig = self.swig or self.find_swig()
+        swig_cmd = [swig, "-python"] + extension.swig_opts
         if is_cpp:
             swig_cmd.append('-c++')
         for d in extension.include_dirs:
@@ -591,7 +707,7 @@
             if self.force or newer_group(depends, target, 'newer'):
                 log.info("%s: %s" % (os.path.basename(swig) \
                                      + (is_cpp and '++' or ''), source))
-                self.spawn(swig_cmd + self.swigflags \
+                self.spawn(swig_cmd + self.swig_opts \
                            + ["-o", target, '-outdir', py_target_dir, source])
             else:
                 log.debug("  skipping '%s' swig interface (up-to-date)" \
@@ -599,41 +715,38 @@
 
         return new_sources + py_files
 
-_f_pyf_ext_match = re.compile(r'.*[.](f90|f95|f77|for|ftn|f|pyf)\Z',re.I).match
-_header_ext_match = re.compile(r'.*[.](inc|h|hpp)\Z',re.I).match
+_f_pyf_ext_match = re.compile(r'.*\.(f90|f95|f77|for|ftn|f|pyf)\Z', re.I).match
+_header_ext_match = re.compile(r'.*\.(inc|h|hpp)\Z', re.I).match
 
 #### SWIG related auxiliary functions ####
 _swig_module_name_match = re.compile(r'\s*%module\s*(.*\(\s*package\s*=\s*"(?P<package>[\w_]+)".*\)|)\s*(?P<name>[\w_]+)',
                                      re.I).match
-_has_c_header = re.compile(r'-[*]-\s*c\s*-[*]-',re.I).search
-_has_cpp_header = re.compile(r'-[*]-\s*c[+][+]\s*-[*]-',re.I).search
+_has_c_header = re.compile(r'-\*-\s*c\s*-\*-', re.I).search
+_has_cpp_header = re.compile(r'-\*-\s*c\+\+\s*-\*-', re.I).search
 
 def get_swig_target(source):
-    f = open(source,'r')
-    result = 'c'
-    line = f.readline()
-    if _has_cpp_header(line):
-        result = 'c++'
-    if _has_c_header(line):
-        result = 'c'
-    f.close()
+    with open(source, 'r') as f:
+        result = None
+        line = f.readline()
+        if _has_cpp_header(line):
+            result = 'c++'
+        if _has_c_header(line):
+            result = 'c'
     return result
 
 def get_swig_modulename(source):
-    f = open(source,'r')
-    f_readlines = getattr(f,'xreadlines',f.readlines)
-    name = None
-    for line in f_readlines():
-        m = _swig_module_name_match(line)
-        if m:
-            name = m.group('name')
-            break
-    f.close()
+    with open(source, 'r') as f:
+        name = None
+        for line in f:
+            m = _swig_module_name_match(line)
+            if m:
+                name = m.group('name')
+                break
     return name
 
-def _find_swig_target(target_dir,name):
-    for ext in ['.cpp','.c']:
-        target = os.path.join(target_dir,'%s_wrap%s' % (name, ext))
+def _find_swig_target(target_dir, name):
+    for ext in ['.cpp', '.c']:
+        target = os.path.join(target_dir, '%s_wrap%s' % (name, ext))
         if os.path.isfile(target):
             break
     return target
@@ -641,22 +754,20 @@
 #### F2PY related auxiliary functions ####
 
 _f2py_module_name_match = re.compile(r'\s*python\s*module\s*(?P<name>[\w_]+)',
-                                re.I).match
-_f2py_user_module_name_match = re.compile(r'\s*python\s*module\s*(?P<name>[\w_]*?'\
-                                     '__user__[\w_]*)',re.I).match
+                                     re.I).match
+_f2py_user_module_name_match = re.compile(r'\s*python\s*module\s*(?P<name>[\w_]*?'
+                                          r'__user__[\w_]*)', re.I).match
 
 def get_f2py_modulename(source):
     name = None
-    f = open(source)
-    f_readlines = getattr(f,'xreadlines',f.readlines)
-    for line in f_readlines():
-        m = _f2py_module_name_match(line)
-        if m:
-            if _f2py_user_module_name_match(line): # skip *__user__* names
-                continue
-            name = m.group('name')
-            break
-    f.close()
+    with open(source) as f:
+        for line in f:
+            m = _f2py_module_name_match(line)
+            if m:
+                if _f2py_user_module_name_match(line): # skip *__user__* names
+                    continue
+                name = m.group('name')
+                break
     return name
 
 ##########################################
('numpy/distutils/command', '__init__.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,31 +1,41 @@
 """distutils.command
 
 Package containing implementation of all the standard Distutils
-commands."""
+commands.
+
+"""
+def test_na_writable_attributes_deletion():
+    a = np.NA(2)
+    attr =  ['payload', 'dtype']
+    for s in attr:
+        assert_raises(AttributeError, delattr, a, s)
+
 
 __revision__ = "$Id: __init__.py,v 1.3 2005/05/16 11:08:49 pearu Exp $"
 
-distutils_all = [  'build_py',
+distutils_all = [  #'build_py',
                    'clean',
-                   'install_lib',
+                   'install_clib',
                    'install_scripts',
                    'bdist',
                    'bdist_dumb',
                    'bdist_wininst',
                 ]
 
-__import__('distutils.command',globals(),locals(),distutils_all)
+__import__('distutils.command', globals(), locals(), distutils_all)
 
 __all__ = ['build',
            'config_compiler',
            'config',
            'build_src',
+           'build_py',
            'build_ext',
            'build_clib',
            'build_scripts',
            'install',
            'install_data',
            'install_headers',
+           'install_lib',
            'bdist_rpm',
            'sdist',
           ] + distutils_all
('numpy/distutils/command', 'sdist.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,4 +1,9 @@
-from distutils.command.sdist import sdist as old_sdist
+import sys
+if 'setuptools' in sys.modules:
+    from setuptools.command.sdist import sdist as old_sdist
+else:
+    from distutils.command.sdist import sdist as old_sdist
+
 from numpy.distutils.misc_util import get_data_files
 
 class sdist(old_sdist):
@@ -15,7 +20,7 @@
         if dist.has_headers():
             headers = []
             for h in dist.headers:
-                if isinstance(h,str): headers.append(h)
+                if isinstance(h, str): headers.append(h)
                 else: headers.append(h[1])
             self.filelist.extend(headers)
 
('numpy/distutils/command', 'build_scripts.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,6 +1,6 @@
 """ Modified version of build_scripts that handles building scripts from functions.
+
 """
-
 from distutils.command.build_scripts import build_scripts as old_build_scripts
 from numpy.distutils import log
 from numpy.distutils.misc_util import is_string
@@ -37,6 +37,10 @@
             return
 
         self.scripts = self.generate_scripts(self.scripts)
+        # Now make sure that the distribution object has this list of scripts.
+        # setuptools' develop command requires that this be a list of filenames,
+        # not functions.
+        self.distribution.scripts = self.scripts
 
         return old_build_scripts.run(self)
 
('numpy/distutils/command', 'bdist_rpm.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,16 +1,22 @@
 import os
 import sys
-from distutils.command.bdist_rpm import bdist_rpm as old_bdist_rpm
+if 'setuptools' in sys.modules:
+    from setuptools.command.bdist_rpm import bdist_rpm as old_bdist_rpm
+else:
+    from distutils.command.bdist_rpm import bdist_rpm as old_bdist_rpm
 
 class bdist_rpm(old_bdist_rpm):
 
     def _make_spec_file(self):
         spec_file = old_bdist_rpm._make_spec_file(self)
+
+        # Replace hardcoded setup.py script name
+        # with the real setup script name.
         setup_py = os.path.basename(sys.argv[0])
         if setup_py == 'setup.py':
             return spec_file
         new_spec_file = []
         for line in spec_file:
-            line = line.replace('setup.py',setup_py)
+            line = line.replace('setup.py', setup_py)
             new_spec_file.append(line)
         return new_spec_file
('numpy/distutils/command', 'build_clib.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,13 +1,27 @@
 """ Modified version of build_clib that handles fortran source files.
 """
-
+import os
+from glob import glob
+import shutil
 from distutils.command.build_clib import build_clib as old_build_clib
-from distutils.errors import DistutilsSetupError
+from distutils.errors import DistutilsSetupError, DistutilsError, \
+    DistutilsFileError
 
 from numpy.distutils import log
 from distutils.dep_util import newer_group
-from numpy.distutils.misc_util import filter_sources, has_f_sources,\
-     has_cxx_sources, all_strings, get_lib_source_files, is_sequence
+from numpy.distutils.misc_util import (
+    filter_sources, get_lib_source_files, get_numpy_include_dirs,
+    has_cxx_sources, has_f_sources, is_sequence
+)
+from numpy.distutils.ccompiler_opt import new_ccompiler_opt
+
+# Fix Python distutils bug sf #1718574:
+_l = old_build_clib.user_options
+for _i in range(len(_l)):
+    if _l[_i][0] in ['build-clib', 'build-temp']:
+        _l[_i] = (_l[_i][0] + '=',) + _l[_i][1:]
+#
+
 
 class build_clib(old_build_clib):
 
@@ -16,28 +30,57 @@
     user_options = old_build_clib.user_options + [
         ('fcompiler=', None,
          "specify the Fortran compiler type"),
-        ]
+        ('inplace', 'i', 'Build in-place'),
+        ('parallel=', 'j',
+         "number of parallel jobs"),
+        ('warn-error', None,
+         "turn all warnings into errors (-Werror)"),
+        ('cpu-baseline=', None,
+         "specify a list of enabled baseline CPU optimizations"),
+        ('cpu-dispatch=', None,
+         "specify a list of dispatched CPU optimizations"),
+        ('disable-optimization', None,
+         "disable CPU optimized code(dispatch,simd,fast...)"),
+    ]
+
+    boolean_options = old_build_clib.boolean_options + \
+    ['inplace', 'warn-error', 'disable-optimization']
 
     def initialize_options(self):
         old_build_clib.initialize_options(self)
         self.fcompiler = None
-        return
+        self.inplace = 0
+        self.parallel = None
+        self.warn_error = None
+        self.cpu_baseline = None
+        self.cpu_dispatch = None
+        self.disable_optimization = None
+
 
     def finalize_options(self):
+        if self.parallel:
+            try:
+                self.parallel = int(self.parallel)
+            except ValueError as e:
+                raise ValueError("--parallel/-j argument must be an integer") from e
         old_build_clib.finalize_options(self)
-        self.set_undefined_options('build_ext',
-                                   ('fcompiler', 'fcompiler'))
-        return
+        self.set_undefined_options('build',
+                                        ('parallel', 'parallel'),
+                                        ('warn_error', 'warn_error'),
+                                        ('cpu_baseline', 'cpu_baseline'),
+                                        ('cpu_dispatch', 'cpu_dispatch'),
+                                        ('disable_optimization', 'disable_optimization')
+                                  )
 
     def have_f_sources(self):
         for (lib_name, build_info) in self.libraries:
-            if has_f_sources(build_info.get('sources',[])):
+            if has_f_sources(build_info.get('sources', [])):
                 return True
         return False
 
     def have_cxx_sources(self):
         for (lib_name, build_info) in self.libraries:
-            if has_cxx_sources(build_info.get('sources',[])):
+            if has_cxx_sources(build_info.get('sources', [])):
                 return True
         return False
 
@@ -46,9 +89,15 @@
             return
 
         # Make sure that library sources are complete.
+        languages = []
+
+        # Make sure that extension sources are complete.
+        self.run_command('build_src')
+
         for (lib_name, build_info) in self.libraries:
-            if not all_strings(build_info.get('sources',[])):
-                self.run_command('build_src')
+            l = build_info.get('language', None)
+            if l and l not in languages:
+                languages.append(l)
 
         from distutils.ccompiler import new_compiler
         self.compiler = new_compiler(compiler=self.compiler,
@@ -57,6 +106,10 @@
         self.compiler.customize(self.distribution,
                                 need_cxx=self.have_cxx_sources())
 
+        if self.warn_error:
+            self.compiler.compiler.append('-Werror')
+            self.compiler.compiler_so.append('-Werror')
+
         libraries = self.libraries
         self.libraries = None
         self.compiler.customize_cmd(self)
@@ -64,23 +117,62 @@
 
         self.compiler.show_customization()
 
+        if not self.disable_optimization:
+            dispatch_hpath = os.path.join("numpy", "distutils", "include", "npy_cpu_dispatch_config.h")
+            dispatch_hpath = os.path.join(self.get_finalized_command("build_src").build_src, dispatch_hpath)
+            opt_cache_path = os.path.abspath(
+                os.path.join(self.build_temp, 'ccompiler_opt_cache_clib.py')
+            )
+            if hasattr(self, "compiler_opt"):
+                # By default `CCompilerOpt` update the cache at the exit of
+                # the process, which may lead to duplicate building
+                # (see build_extension()/force_rebuild) if run() called
+                # multiple times within the same os process/thread without
+                # giving the chance the previous instances of `CCompilerOpt`
+                # to update the cache.
+                self.compiler_opt.cache_flush()
+
+            self.compiler_opt = new_ccompiler_opt(
+                compiler=self.compiler, dispatch_hpath=dispatch_hpath,
+                cpu_baseline=self.cpu_baseline, cpu_dispatch=self.cpu_dispatch,
+                cache_path=opt_cache_path
+            )
+            def report(copt):
+                log.info("\n########### CLIB COMPILER OPTIMIZATION ###########")
+                log.info(copt.report(full=True))
+
+            import atexit
+            atexit.register(report, self.compiler_opt)
+
         if self.have_f_sources():
             from numpy.distutils.fcompiler import new_fcompiler
-            self.fcompiler = new_fcompiler(compiler=self.fcompiler,
-                                           verbose=self.verbose,
-                                           dry_run=self.dry_run,
-                                           force=self.force)
-            self.fcompiler.customize(self.distribution)
-
-            libraries = self.libraries
-            self.libraries = None
-            self.fcompiler.customize_cmd(self)
-            self.libraries = libraries
-
-            self.fcompiler.show_customization()
+            self._f_compiler = new_fcompiler(compiler=self.fcompiler,
+                                             verbose=self.verbose,
+                                             dry_run=self.dry_run,
+                                             force=self.force,
+                                             requiref90='f90' in languages,
+                                             c_compiler=self.compiler)
+            if self._f_compiler is not None:
+                self._f_compiler.customize(self.distribution)
+
+                libraries = self.libraries
+                self.libraries = None
+                self._f_compiler.customize_cmd(self)
+                self.libraries = libraries
+
+                self._f_compiler.show_customization()
+        else:
+            self._f_compiler = None
 
         self.build_libraries(self.libraries)
-        return
+
+        if self.inplace:
+            for l in self.distribution.installed_libraries:
+                libname = self.compiler.library_filename(l.name)
+                source = os.path.join(self.build_clib, libname)
+                target = os.path.join(l.target_dir, libname)
+                self.mkpath(l.target_dir)
+                shutil.copy(source, target)
 
     def get_source_files(self):
         self.check_library_list(self.libraries)
@@ -90,107 +182,287 @@
         return filenames
 
     def build_libraries(self, libraries):
-
-
         for (lib_name, build_info) in libraries:
-            # default compilers
-            compiler = self.compiler
-            fcompiler = self.fcompiler
-
-            sources = build_info.get('sources')
-            if sources is None or not is_sequence(sources):
-                raise DistutilsSetupError, \
-                      ("in 'libraries' option (library '%s'), " +
-                       "'sources' must be present and must be " +
-                       "a list of source filenames") % lib_name
-            sources = list(sources)
-
-
-            
-            lib_file = compiler.library_filename(lib_name,
-                                                 output_dir=self.build_clib)
-
-            depends = sources + build_info.get('depends',[])
-            if not (self.force or newer_group(depends, lib_file, 'newer')):
-                log.debug("skipping '%s' library (up-to-date)", lib_name)
-                continue
+            self.build_a_library(build_info, lib_name, libraries)
+
+    def assemble_flags(self, in_flags):
+        """ Assemble flags from flag list
+
+        Parameters
+        ----------
+        in_flags : None or sequence
+            None corresponds to empty list.  Sequence elements can be strings
+            or callables that return lists of strings. Callable takes `self` as
+            single parameter.
+
+        Returns
+        -------
+        out_flags : list
+        """
+        if in_flags is None:
+            return []
+        out_flags = []
+        for in_flag in in_flags:
+            if callable(in_flag):
+                out_flags += in_flag(self)
             else:
-                log.info("building '%s' library", lib_name)
-
-
-            config_fc = build_info.get('config_fc',{})
-            if fcompiler is not None and config_fc:
-                log.info('using setup script specified config_fc '\
-                         'for fortran compiler: %s' \
-                         % (config_fc))
-                from numpy.distutils.fcompiler import new_fcompiler
-                fcompiler = new_fcompiler(compiler=self.fcompiler.compiler_type,
-                                          verbose=self.verbose,
-                                          dry_run=self.dry_run,
-                                          force=self.force)
-                fcompiler.customize(config_fc)
-
-            macros = build_info.get('macros')
-            include_dirs = build_info.get('include_dirs')
-            extra_postargs = build_info.get('extra_compiler_args') or []
-
-            c_sources, cxx_sources, f_sources, fmodule_sources \
-                       = filter_sources(sources)
-
-            if self.compiler.compiler_type=='msvc':
-                # this hack works around the msvc compiler attributes
-                # problem, msvc uses its own convention :(
-                c_sources += cxx_sources
-                cxx_sources = []
+                out_flags.append(in_flag)
+        return out_flags
+
+    def build_a_library(self, build_info, lib_name, libraries):
+        # default compilers
+        compiler = self.compiler
+        fcompiler = self._f_compiler
+
+        sources = build_info.get('sources')
+        if sources is None or not is_sequence(sources):
+            raise DistutilsSetupError(("in 'libraries' option (library '%s'), " +
+                                       "'sources' must be present and must be " +
+                                       "a list of source filenames") % lib_name)
+        sources = list(sources)
+
+        c_sources, cxx_sources, f_sources, fmodule_sources \
+            = filter_sources(sources)
+        requiref90 = not not fmodule_sources or \
+            build_info.get('language', 'c') == 'f90'
+
+        # save source type information so that build_ext can use it.
+        source_languages = []
+        if c_sources:
+            source_languages.append('c')
+        if cxx_sources:
+            source_languages.append('c++')
+        if requiref90:
+            source_languages.append('f90')
+        elif f_sources:
+            source_languages.append('f77')
+        build_info['source_languages'] = source_languages
+
+        lib_file = compiler.library_filename(lib_name,
+                                             output_dir=self.build_clib)
+        depends = sources + build_info.get('depends', [])
+
+        force_rebuild = self.force
+        if not self.disable_optimization and not self.compiler_opt.is_cached():
+            log.debug("Detected changes on compiler optimizations")
+            force_rebuild = True
+        if not (force_rebuild or newer_group(depends, lib_file, 'newer')):
+            log.debug("skipping '%s' library (up-to-date)", lib_name)
+            return
+        else:
+            log.info("building '%s' library", lib_name)
+
+        config_fc = build_info.get('config_fc', {})
+        if fcompiler is not None and config_fc:
+            log.info('using additional config_fc from setup script '
+                     'for fortran compiler: %s'
+                     % (config_fc,))
+            from numpy.distutils.fcompiler import new_fcompiler
+            fcompiler = new_fcompiler(compiler=fcompiler.compiler_type,
+                                      verbose=self.verbose,
+                                      dry_run=self.dry_run,
+                                      force=self.force,
+                                      requiref90=requiref90,
+                                      c_compiler=self.compiler)
+            if fcompiler is not None:
+                dist = self.distribution
+                base_config_fc = dist.get_option_dict('config_fc').copy()
+                base_config_fc.update(config_fc)
+                fcompiler.customize(base_config_fc)
+
+        # check availability of Fortran compilers
+        if (f_sources or fmodule_sources) and fcompiler is None:
+            raise DistutilsError("library %s has Fortran sources"
+                                 " but no Fortran compiler found" % (lib_name))
+
+        if fcompiler is not None:
+            fcompiler.extra_f77_compile_args = build_info.get(
+                'extra_f77_compile_args') or []
+            fcompiler.extra_f90_compile_args = build_info.get(
+                'extra_f90_compile_args') or []
+
+        macros = build_info.get('macros')
+        if macros is None:
+            macros = []
+        include_dirs = build_info.get('include_dirs')
+        if include_dirs is None:
+            include_dirs = []
+        # Flags can be strings, or callables that return a list of strings.
+        extra_postargs = self.assemble_flags(
+            build_info.get('extra_compiler_args'))
+        extra_cflags = self.assemble_flags(
+            build_info.get('extra_cflags'))
+        extra_cxxflags = self.assemble_flags(
+            build_info.get('extra_cxxflags'))
+
+        include_dirs.extend(get_numpy_include_dirs())
+        # where compiled F90 module files are:
+        module_dirs = build_info.get('module_dirs') or []
+        module_build_dir = os.path.dirname(lib_file)
+        if requiref90:
+            self.mkpath(module_build_dir)
+
+        if compiler.compiler_type == 'msvc':
+            # this hack works around the msvc compiler attributes
+            # problem, msvc uses its own convention :(
+            c_sources += cxx_sources
+            cxx_sources = []
+
+        # filtering C dispatch-table sources when optimization is not disabled,
+        # otherwise treated as normal sources.
+        copt_c_sources = []
+        copt_cxx_sources = []
+        copt_baseline_flags = []
+        copt_macros = []
+        if not self.disable_optimization:
+            bsrc_dir = self.get_finalized_command("build_src").build_src
+            dispatch_hpath = os.path.join("numpy", "distutils", "include")
+            dispatch_hpath = os.path.join(bsrc_dir, dispatch_hpath)
+            include_dirs.append(dispatch_hpath)
+
+            copt_build_src = None if self.inplace else bsrc_dir
+            for _srcs, _dst, _ext in (
+                ((c_sources,), copt_c_sources, ('.dispatch.c',)),
+                ((c_sources, cxx_sources), copt_cxx_sources,
+                    ('.dispatch.cpp', '.dispatch.cxx'))
+            ):
+                for _src in _srcs:
+                    _dst += [
+                        _src.pop(_src.index(s))
+                        for s in _src[:] if s.endswith(_ext)
+                    ]
+            copt_baseline_flags = self.compiler_opt.cpu_baseline_flags()
+        else:
+            copt_macros.append(("NPY_DISABLE_OPTIMIZATION", 1))
+
+        objects = []
+        if copt_cxx_sources:
+            log.info("compiling C++ dispatch-able sources")
+            objects += self.compiler_opt.try_dispatch(
+                copt_c_sources,
+                output_dir=self.build_temp,
+                src_dir=copt_build_src,
+                macros=macros + copt_macros,
+                include_dirs=include_dirs,
+                debug=self.debug,
+                extra_postargs=extra_postargs + extra_cxxflags,
+                ccompiler=cxx_compiler
+            )
+
+        if copt_c_sources:
+            log.info("compiling C dispatch-able sources")
+            objects += self.compiler_opt.try_dispatch(
+                copt_c_sources,
+                output_dir=self.build_temp,
+                src_dir=copt_build_src,
+                macros=macros + copt_macros,
+                include_dirs=include_dirs,
+                debug=self.debug,
+                extra_postargs=extra_postargs + extra_cflags)
+
+        if c_sources:
+            log.info("compiling C sources")
+            objects += compiler.compile(
+                c_sources,
+                output_dir=self.build_temp,
+                macros=macros + copt_macros,
+                include_dirs=include_dirs,
+                debug=self.debug,
+                extra_postargs=(extra_postargs +
+                                copt_baseline_flags +
+                                extra_cflags))
+
+        if cxx_sources:
+            log.info("compiling C++ sources")
+            cxx_compiler = compiler.cxx_compiler()
+            cxx_objects = cxx_compiler.compile(
+                cxx_sources,
+                output_dir=self.build_temp,
+                macros=macros + copt_macros,
+                include_dirs=include_dirs,
+                debug=self.debug,
+                extra_postargs=(extra_postargs +
+                                copt_baseline_flags +
+                                extra_cxxflags))
+            objects.extend(cxx_objects)
+
+        if f_sources or fmodule_sources:
+            extra_postargs = []
+            f_objects = []
+
+            if requiref90:
+                if fcompiler.module_dir_switch is None:
+                    existing_modules = glob('*.mod')
+                extra_postargs += fcompiler.module_options(
+                    module_dirs, module_build_dir)
 
             if fmodule_sources:
-                print 'XXX: Fortran 90 module support not implemented or tested'
-                f_sources.extend(fmodule_sources)
-
-            objects = []
-            if c_sources:
-                log.info("compiling C sources")
-                objects = compiler.compile(c_sources,
-                                           output_dir=self.build_temp,
-                                           macros=macros,
-                                           include_dirs=include_dirs,
-                                           debug=self.debug,
-                                           extra_postargs=extra_postargs)
-
-            if cxx_sources:
-                log.info("compiling C++ sources")
-                old_compiler = self.compiler.compiler_so[0]
-                self.compiler.compiler_so[0] = self.compiler.compiler_cxx[0]
-
-                cxx_objects = compiler.compile(cxx_sources,
+                log.info("compiling Fortran 90 module sources")
+                f_objects += fcompiler.compile(fmodule_sources,
                                                output_dir=self.build_temp,
                                                macros=macros,
                                                include_dirs=include_dirs,
                                                debug=self.debug,
                                                extra_postargs=extra_postargs)
-                objects.extend(cxx_objects)
-
-                self.compiler.compiler_so[0] = old_compiler
+
+            if requiref90 and self._f_compiler.module_dir_switch is None:
+                # move new compiled F90 module files to module_build_dir
+                for f in glob('*.mod'):
+                    if f in existing_modules:
+                        continue
+                    t = os.path.join(module_build_dir, f)
+                    if os.path.abspath(f) == os.path.abspath(t):
+                        continue
+                    if os.path.isfile(t):
+                        os.remove(t)
+                    try:
+                        self.move_file(f, module_build_dir)
+                    except DistutilsFileError:
+                        log.warn('failed to move %r to %r'
+                                 % (f, module_build_dir))
 
             if f_sources:
                 log.info("compiling Fortran sources")
-                f_objects = fcompiler.compile(f_sources,
-                                              output_dir=self.build_temp,
-                                              macros=macros,
-                                              include_dirs=include_dirs,
-                                              debug=self.debug,
-                                              extra_postargs=[])
-                objects.extend(f_objects)
-
-            self.compiler.create_static_lib(objects, lib_name,
-                                            output_dir=self.build_clib,
-                                            debug=self.debug)
-
-            clib_libraries = build_info.get('libraries',[])
-            for lname,binfo in libraries:
-                if lname in clib_libraries:
-                    clib_libraries.extend(binfo[1].get('libraries',[]))
-            if clib_libraries:
-                build_info['libraries'] = clib_libraries
-
-        return
+                f_objects += fcompiler.compile(f_sources,
+                                               output_dir=self.build_temp,
+                                               macros=macros,
+                                               include_dirs=include_dirs,
+                                               debug=self.debug,
+                                               extra_postargs=extra_postargs)
+        else:
+            f_objects = []
+
+        if f_objects and not fcompiler.can_ccompiler_link(compiler):
+            # Default linker cannot link Fortran object files, and results
+            # need to be wrapped later. Instead of creating a real static
+            # library, just keep track of the object files.
+            listfn = os.path.join(self.build_clib,
+                                  lib_name + '.fobjects')
+            with open(listfn, 'w') as f:
+                f.write("\n".join(os.path.abspath(obj) for obj in f_objects))
+
+            listfn = os.path.join(self.build_clib,
+                                  lib_name + '.cobjects')
+            with open(listfn, 'w') as f:
+                f.write("\n".join(os.path.abspath(obj) for obj in objects))
+
+            # create empty "library" file for dependency tracking
+            lib_fname = os.path.join(self.build_clib,
+                                     lib_name + compiler.static_lib_extension)
+            with open(lib_fname, 'wb') as f:
+                pass
+        else:
+            # assume that default linker is suitable for
+            # linking Fortran object files
+            objects.extend(f_objects)
+            compiler.create_static_lib(objects, lib_name,
+                                       output_dir=self.build_clib,
+                                       debug=self.debug)
+
+        # fix library dependencies
+        clib_libraries = build_info.get('libraries', [])
+        for lname, binfo in libraries:
+            if lname in clib_libraries:
+                clib_libraries.extend(binfo.get('libraries', []))
+        if clib_libraries:
+            build_info['libraries'] = clib_libraries
('numpy/distutils/command', 'egg_info.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,6 +1,25 @@
+import sys
+
 from setuptools.command.egg_info import egg_info as _egg_info
 
 class egg_info(_egg_info):
     def run(self):
+        if 'sdist' in sys.argv:
+            import warnings
+            import textwrap
+            msg = textwrap.dedent("""
+                `build_src` is being run, this may lead to missing
+                files in your sdist!  You want to use distutils.sdist
+                instead of the setuptools version:
+
+                    from distutils.command.sdist import sdist
+                    cmdclass={'sdist': sdist}"
+
+                See numpy's setup.py or gh-7131 for details.""")
+            warnings.warn(msg, UserWarning, stacklevel=2)
+
+        # We need to ensure that build_src has been executed in order to give
+        # setuptools' egg_info command real filenames instead of functions which
+        # generate files.
         self.run_command("build_src")
         _egg_info.run(self)
('numpy/distutils/command', 'install.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,28 +1,76 @@
-from distutils.command.install import install as old_install
+import sys
+if 'setuptools' in sys.modules:
+    import setuptools.command.install as old_install_mod
+    have_setuptools = True
+else:
+    import distutils.command.install as old_install_mod
+    have_setuptools = False
 from distutils.file_util import write_file
 
+old_install = old_install_mod.install
+
 class install(old_install):
+
+    # Always run install_clib - the command is cheap, so no need to bypass it;
+    # but it's not run by setuptools -- so it's run again in install_data
+    sub_commands = old_install.sub_commands + [
+        ('install_clib', lambda x: True)
+    ]
 
     def finalize_options (self):
         old_install.finalize_options(self)
         self.install_lib = self.install_libbase
 
+    def setuptools_run(self):
+        """ The setuptools version of the .run() method.
+
+        We must pull in the entire code so we can override the level used in the
+        _getframe() call since we wrap this call by one more level.
+        """
+        from distutils.command.install import install as distutils_install
+
+        # Explicit request for old-style install?  Just do it
+        if self.old_and_unmanageable or self.single_version_externally_managed:
+            return distutils_install.run(self)
+
+        # Attempt to detect whether we were called from setup() or by another
+        # command.  If we were called by setup(), our caller will be the
+        # 'run_command' method in 'distutils.dist', and *its* caller will be
+        # the 'run_commands' method.  If we were called any other way, our
+        # immediate caller *might* be 'run_command', but it won't have been
+        # called by 'run_commands'.  This is slightly kludgy, but seems to
+        # work.
+        #
+        caller = sys._getframe(3)
+        caller_module = caller.f_globals.get('__name__', '')
+        caller_name = caller.f_code.co_name
+
+        if caller_module != 'distutils.dist' or caller_name!='run_commands':
+            # We weren't called from the command line or setup(), so we
+            # should run in backward-compatibility mode to support bdist_*
+            # commands.
+            distutils_install.run(self)
+        else:
+            self.do_egg_install()
+
     def run(self):
-        r = old_install.run(self)
+        if not have_setuptools:
+            r = old_install.run(self)
+        else:
+            r = self.setuptools_run()
         if self.record:
             # bdist_rpm fails when INSTALLED_FILES contains
             # paths with spaces. Such paths must be enclosed
             # with double-quotes.
-            f = open(self.record,'r')
-            lines = []
-            need_rewrite = False
-            for l in f.readlines():
-                l = l.rstrip()
-                if ' ' in l:
-                    need_rewrite = True
-                    l = '"%s"' % (l)
-                lines.append(l)
-            f.close()
+            with open(self.record, 'r') as f:
+                lines = []
+                need_rewrite = False
+                for l in f:
+                    l = l.rstrip()
+                    if ' ' in l:
+                        need_rewrite = True
+                        l = '"%s"' % (l)
+                    lines.append(l)
             if need_rewrite:
                 self.execute(write_file,
                              (self.record, lines),
('numpy/distutils/command', 'install_data.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,3 +1,6 @@
+import sys
+have_setuptools = ('setuptools' in sys.modules)
+
 from distutils.command.install_data import install_data as old_install_data
 
 #data installer with improved intelligence over distutils
@@ -5,6 +8,14 @@
 #of willy-nilly
 class install_data (old_install_data):
 
+    def run(self):
+        old_install_data.run(self)
+
+        if have_setuptools:
+            # Run install_clib again, since setuptools does not run sub-commands
+            # of install automatically
+            self.run_command('install_clib')
+
     def finalize_options (self):
         self.set_undefined_options('install',
                                    ('install_lib', 'install_dir'),
('numpy/core', '_internal.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,19 +1,54 @@
-
-#A place for code to be called from C-code
-#  that implements more complicated stuff.
-
+"""
+A place for internal code
+
+Some things are more easily handled Python.
+
+"""
+import ast
 import re
-from multiarray import dtype, ndarray
-
-# make sure the tuple entries are PyArray_Descr
-#          or convert them
-#
-# make sure offsets are all interpretable
-#          as positive integers and
-#          convert them to positive integers if so
-#
-#
-# return totalsize from last offset and size
+import sys
+import platform
+import warnings
+
+from .multiarray import dtype, array, ndarray, promote_types
+try:
+    import ctypes
+except ImportError:
+    ctypes = None
+
+IS_PYPY = platform.python_implementation() == 'PyPy'
+
+if sys.byteorder == 'little':
+    _nbo = '<'
+else:
+    _nbo = '>'
+
+def _makenames_list(adict, align):
+    allfields = []
+
+    for fname, obj in adict.items():
+        n = len(obj)
+        if not isinstance(obj, tuple) or n not in (2, 3):
+            raise ValueError("entry not a 2- or 3- tuple")
+        if n > 2 and obj[2] == fname:
+            continue
+        num = int(obj[1])
+        if num < 0:
+            raise ValueError("invalid offset.")
+        format = dtype(obj[0], align=align)
+        if n > 2:
+            title = obj[2]
+        else:
+            title = None
+        allfields.append((fname, format, num, title))
+    # sort by offsets
+    allfields.sort(key=lambda x: x[2])
+    names = [x[0] for x in allfields]
+    formats = [x[1] for x in allfields]
+    offsets = [x[2] for x in allfields]
+    titles = [x[3] for x in allfields]
+
+    return names, formats, offsets, titles
 
 # Called in PyArray_DescrConverter function when
 #  a dictionary without "names" and "formats"
@@ -24,32 +59,7 @@
     except KeyError:
         names = None
     if names is None:
-        allfields = []
-        fnames = adict.keys()
-        for fname in fnames:
-            obj = adict[fname]
-            n = len(obj)
-            if not isinstance(obj, tuple) or n not in [2,3]:
-                raise ValueError, "entry not a 2- or 3- tuple"
-            if (n > 2) and (obj[2] == fname):
-                continue
-            num = int(obj[1])
-            if (num < 0):
-                raise ValueError, "invalid offset."
-            format = dtype(obj[0])
-            if (format.itemsize == 0):
-                raise ValueError, "all itemsizes must be fixed."
-            if (n > 2):
-                title = obj[2]
-            else:
-                title = None
-            allfields.append((fname, format, num, title))
-        # sort by offsets
-        allfields.sort(lambda x,y: cmp(x[2],y[2]))
-        names = [x[0] for x in allfields]
-        formats = [x[1] for x in allfields]
-        offsets = [x[2] for x in allfields]
-        titles = [x[3] for x in allfields]
+        names, formats, offsets, titles = _makenames_list(adict, align)
     else:
         formats = []
         offsets = []
@@ -58,15 +68,15 @@
             res = adict[name]
             formats.append(res[0])
             offsets.append(res[1])
-            if (len(res) > 2):
+            if len(res) > 2:
                 titles.append(res[2])
             else:
                 titles.append(None)
 
-    return dtype({"names" : names,
-                  "formats" : formats,
-                  "offsets" : offsets,
-                  "titles" : titles}, align)
+    return dtype({"names": names,
+                  "formats": formats,
+                  "offsets": offsets,
+                  "titles": titles}, align)
 
 
 # construct an array_protocol descriptor list
@@ -78,16 +88,34 @@
 def _array_descr(descriptor):
     fields = descriptor.fields
     if fields is None:
-        return descriptor.str
-
-    ordered_fields = [fields[x] + (x,) for x in fields[-1]]
+        subdtype = descriptor.subdtype
+        if subdtype is None:
+            if descriptor.metadata is None:
+                return descriptor.str
+            else:
+                new = descriptor.metadata.copy()
+                if new:
+                    return (descriptor.str, new)
+                else:
+                    return descriptor.str
+        else:
+            return (_array_descr(subdtype[0]), subdtype[1])
+
+    names = descriptor.names
+    ordered_fields = [fields[x] + (x,) for x in names]
     result = []
     offset = 0
     for field in ordered_fields:
         if field[1] > offset:
-            result.append(('','|V%d' % (field[1]-offset)))
+            num = field[1] - offset
+            result.append(('', f'|V{num}'))
+            offset += num
+        elif field[1] < offset:
+            raise ValueError(
+                "dtype.descr is not defined for types with overlapping or "
+                "out-of-order fields")
         if len(field) > 3:
-            name = (field[2],field[3])
+            name = (field[2], field[3])
         else:
             name = field[2]
         if field[0].subdtype:
@@ -98,70 +126,808 @@
         offset += field[0].itemsize
         result.append(tup)
 
+    if descriptor.itemsize > offset:
+        num = descriptor.itemsize - offset
+        result.append(('', f'|V{num}'))
+
     return result
 
+# Build a new array from the information in a pickle.
+# Note that the name numpy.core._internal._reconstruct is embedded in
+# pickles of ndarrays made with NumPy before release 1.0
+# so don't remove the name here, or you'll
+# break backward compatibility.
 def _reconstruct(subtype, shape, dtype):
     return ndarray.__new__(subtype, shape, dtype)
 
 
-# format_re and _split were taken from numarray by J. Todd Miller
-
-def _split(input):
-    """Split the input formats string into field formats without splitting
-       the tuple used to specify multi-dimensional arrays."""
-
-    newlist = []
-    hold = ''
-
-    for element in input.split(','):
-        if hold != '':
-            item = hold + ',' + element
-        else:
-            item = element
-        left = item.count('(')
-        right = item.count(')')
-
-        # if the parenthesis is not balanced, hold the string
-        if left > right :
-            hold = item
-
-        # when balanced, append to the output list and reset the hold
-        elif left == right:
-            newlist.append(item.strip())
-            hold = ''
-
-        # too many close parenthesis is unacceptable
-        else:
-            raise SyntaxError, item
-
-    # if there is string left over in hold
-    if hold != '':
-        raise SyntaxError, hold
-
-    return newlist
-
-format_re = re.compile(r'(?P<repeat> *[(]?[ ,0-9]*[)]? *)(?P<dtype>[><|A-Za-z0-9.]*)')
+# format_re was originally from numarray by J. Todd Miller
+
+format_re = re.compile(r'(?P<order1>[<>|=]?)'
+                       r'(?P<repeats> *[(]?[ ,0-9]*[)]? *)'
+                       r'(?P<order2>[<>|=]?)'
+                       r'(?P<dtype>[A-Za-z0-9.?]*(?:\[[a-zA-Z0-9,.]+\])?)')
+sep_re = re.compile(r'\s*,\s*')
+space_re = re.compile(r'\s+$')
 
 # astr is a string (perhaps comma separated)
 
+_convorder = {'=': _nbo}
+
 def _commastring(astr):
-    res = _split(astr)
-    if (len(res)) < 1:
-        raise ValueError, "unrecognized formant"
+    startindex = 0
     result = []
-    for k,item in enumerate(res):
-        # convert item
+    while startindex < len(astr):
+        mo = format_re.match(astr, pos=startindex)
         try:
-            (repeats, dtype) = format_re.match(item).groups()
+            (order1, repeats, order2, dtype) = mo.groups()
         except (TypeError, AttributeError):
-            raise ValueError('format %s is not recognized' % item)
-
+            raise ValueError(
+                f'format number {len(result)+1} of "{astr}" is not recognized'
+                ) from None
+        startindex = mo.end()
+        # Separator or ending padding
+        if startindex < len(astr):
+            if space_re.match(astr, pos=startindex):
+                startindex = len(astr)
+            else:
+                mo = sep_re.match(astr, pos=startindex)
+                if not mo:
+                    raise ValueError(
+                        'format number %d of "%s" is not recognized' %
+                        (len(result)+1, astr))
+                startindex = mo.end()
+
+        if order2 == '':
+            order = order1
+        elif order1 == '':
+            order = order2
+        else:
+            order1 = _convorder.get(order1, order1)
+            order2 = _convorder.get(order2, order2)
+            if (order1 != order2):
+                raise ValueError(
+                    'inconsistent byte-order specification %s and %s' %
+                    (order1, order2))
+            order = order1
+
+        if order in ('|', '=', _nbo):
+            order = ''
+        dtype = order + dtype
         if (repeats == ''):
             newitem = dtype
         else:
-            newitem = (dtype, eval(repeats))
+            newitem = (dtype, ast.literal_eval(repeats))
         result.append(newitem)
 
     return result
 
-
+class dummy_ctype:
+    def __init__(self, cls):
+        self._cls = cls
+    def __mul__(self, other):
+        return self
+    def __call__(self, *other):
+        return self._cls(other)
+    def __eq__(self, other):
+        return self._cls == other._cls
+    def __ne__(self, other):
+        return self._cls != other._cls
+
+def _getintp_ctype():
+    val = _getintp_ctype.cache
+    if val is not None:
+        return val
+    if ctypes is None:
+        import numpy as np
+        val = dummy_ctype(np.intp)
+    else:
+        char = dtype('p').char
+        if char == 'i':
+            val = ctypes.c_int
+        elif char == 'l':
+            val = ctypes.c_long
+        elif char == 'q':
+            val = ctypes.c_longlong
+        else:
+            val = ctypes.c_long
+    _getintp_ctype.cache = val
+    return val
+_getintp_ctype.cache = None
+
+# Used for .ctypes attribute of ndarray
+
+class _missing_ctypes:
+    def cast(self, num, obj):
+        return num.value
+
+    class c_void_p:
+        def __init__(self, ptr):
+            self.value = ptr
+
+
+class _ctypes:
+    def __init__(self, array, ptr=None):
+        self._arr = array
+
+        if ctypes:
+            self._ctypes = ctypes
+            self._data = self._ctypes.c_void_p(ptr)
+        else:
+            # fake a pointer-like object that holds onto the reference
+            self._ctypes = _missing_ctypes()
+            self._data = self._ctypes.c_void_p(ptr)
+            self._data._objects = array
+
+        if self._arr.ndim == 0:
+            self._zerod = True
+        else:
+            self._zerod = False
+
+    def data_as(self, obj):
+        """
+        Return the data pointer cast to a particular c-types object.
+        For example, calling ``self._as_parameter_`` is equivalent to
+        ``self.data_as(ctypes.c_void_p)``. Perhaps you want to use the data as a
+        pointer to a ctypes array of floating-point data:
+        ``self.data_as(ctypes.POINTER(ctypes.c_double))``.
+
+        The returned pointer will keep a reference to the array.
+        """
+        # _ctypes.cast function causes a circular reference of self._data in
+        # self._data._objects. Attributes of self._data cannot be released
+        # until gc.collect is called. Make a copy of the pointer first then let
+        # it hold the array reference. This is a workaround to circumvent the
+        # CPython bug https://bugs.python.org/issue12836
+        ptr = self._ctypes.cast(self._data, obj)
+        ptr._arr = self._arr
+        return ptr
+
+    def shape_as(self, obj):
+        """
+        Return the shape tuple as an array of some other c-types
+        type. For example: ``self.shape_as(ctypes.c_short)``.
+        """
+        if self._zerod:
+            return None
+        return (obj*self._arr.ndim)(*self._arr.shape)
+
+    def strides_as(self, obj):
+        """
+        Return the strides tuple as an array of some other
+        c-types type. For example: ``self.strides_as(ctypes.c_longlong)``.
+        """
+        if self._zerod:
+            return None
+        return (obj*self._arr.ndim)(*self._arr.strides)
+
+    @property
+    def data(self):
+        """
+        A pointer to the memory area of the array as a Python integer.
+        This memory area may contain data that is not aligned, or not in correct
+        byte-order. The memory area may not even be writeable. The array
+        flags and data-type of this array should be respected when passing this
+        attribute to arbitrary C-code to avoid trouble that can include Python
+        crashing. User Beware! The value of this attribute is exactly the same
+        as ``self._array_interface_['data'][0]``.
+
+        Note that unlike ``data_as``, a reference will not be kept to the array:
+        code like ``ctypes.c_void_p((a + b).ctypes.data)`` will result in a
+        pointer to a deallocated array, and should be spelt
+        ``(a + b).ctypes.data_as(ctypes.c_void_p)``
+        """
+        return self._data.value
+
+    @property
+    def shape(self):
+        """
+        (c_intp*self.ndim): A ctypes array of length self.ndim where
+        the basetype is the C-integer corresponding to ``dtype('p')`` on this
+        platform (see `~numpy.ctypeslib.c_intp`). This base-type could be
+        `ctypes.c_int`, `ctypes.c_long`, or `ctypes.c_longlong` depending on
+        the platform. The ctypes array contains the shape of
+        the underlying array.
+        """
+        return self.shape_as(_getintp_ctype())
+
+    @property
+    def strides(self):
+        """
+        (c_intp*self.ndim): A ctypes array of length self.ndim where
+        the basetype is the same as for the shape attribute. This ctypes array
+        contains the strides information from the underlying array. This strides
+        information is important for showing how many bytes must be jumped to
+        get to the next element in the array.
+        """
+        return self.strides_as(_getintp_ctype())
+
+    @property
+    def _as_parameter_(self):
+        """
+        Overrides the ctypes semi-magic method
+
+        Enables `c_func(some_array.ctypes)`
+        """
+        return self.data_as(ctypes.c_void_p)
+
+    # Numpy 1.21.0, 2021-05-18
+
+    def get_data(self):
+        """Deprecated getter for the `_ctypes.data` property.
+
+        .. deprecated:: 1.21
+        """
+        warnings.warn('"get_data" is deprecated. Use "data" instead',
+                      DeprecationWarning, stacklevel=2)
+        return self.data
+
+    def get_shape(self):
+        """Deprecated getter for the `_ctypes.shape` property.
+
+        .. deprecated:: 1.21
+        """
+        warnings.warn('"get_shape" is deprecated. Use "shape" instead',
+                      DeprecationWarning, stacklevel=2)
+        return self.shape
+
+    def get_strides(self):
+        """Deprecated getter for the `_ctypes.strides` property.
+
+        .. deprecated:: 1.21
+        """
+        warnings.warn('"get_strides" is deprecated. Use "strides" instead',
+                      DeprecationWarning, stacklevel=2)
+        return self.strides
+
+    def get_as_parameter(self):
+        """Deprecated getter for the `_ctypes._as_parameter_` property.
+
+        .. deprecated:: 1.21
+        """
+        warnings.warn(
+            '"get_as_parameter" is deprecated. Use "_as_parameter_" instead',
+            DeprecationWarning, stacklevel=2,
+        )
+        return self._as_parameter_
+
+
+def _newnames(datatype, order):
+    """
+    Given a datatype and an order object, return a new names tuple, with the
+    order indicated
+    """
+    oldnames = datatype.names
+    nameslist = list(oldnames)
+    if isinstance(order, str):
+        order = [order]
+    seen = set()
+    if isinstance(order, (list, tuple)):
+        for name in order:
+            try:
+                nameslist.remove(name)
+            except ValueError:
+                if name in seen:
+                    raise ValueError(f"duplicate field name: {name}") from None
+                else:
+                    raise ValueError(f"unknown field name: {name}") from None
+            seen.add(name)
+        return tuple(list(order) + nameslist)
+    raise ValueError(f"unsupported order value: {order}")
+
+def _copy_fields(ary):
+    """Return copy of structured array with padding between fields removed.
+
+    Parameters
+    ----------
+    ary : ndarray
+       Structured array from which to remove padding bytes
+
+    Returns
+    -------
+    ary_copy : ndarray
+       Copy of ary with padding bytes removed
+    """
+    dt = ary.dtype
+    copy_dtype = {'names': dt.names,
+                  'formats': [dt.fields[name][0] for name in dt.names]}
+    return array(ary, dtype=copy_dtype, copy=True)
+
+def _promote_fields(dt1, dt2):
+    """ Perform type promotion for two structured dtypes.
+
+    Parameters
+    ----------
+    dt1 : structured dtype
+        First dtype.
+    dt2 : structured dtype
+        Second dtype.
+
+    Returns
+    -------
+    out : dtype
+        The promoted dtype
+
+    Notes
+    -----
+    If one of the inputs is aligned, the result will be.  The titles of
+    both descriptors must match (point to the same field).
+    """
+    # Both must be structured and have the same names in the same order
+    if (dt1.names is None or dt2.names is None) or dt1.names != dt2.names:
+        raise TypeError("invalid type promotion")
+
+    # if both are identical, we can (maybe!) just return the same dtype.
+    identical = dt1 is dt2
+    new_fields = []
+    for name in dt1.names:
+        field1 = dt1.fields[name]
+        field2 = dt2.fields[name]
+        new_descr = promote_types(field1[0], field2[0])
+        identical = identical and new_descr is field1[0]
+
+        # Check that the titles match (if given):
+        if field1[2:] != field2[2:]:
+            raise TypeError("invalid type promotion")
+        if len(field1) == 2:
+            new_fields.append((name, new_descr))
+        else:
+            new_fields.append(((field1[2], name), new_descr))
+
+    res = dtype(new_fields, align=dt1.isalignedstruct or dt2.isalignedstruct)
+
+    # Might as well preserve identity (and metadata) if the dtype is identical
+    # and the itemsize, offsets are also unmodified.  This could probably be
+    # sped up, but also probably just be removed entirely.
+    if identical and res.itemsize == dt1.itemsize:
+        for name in dt1.names:
+            if dt1.fields[name][1] != res.fields[name][1]:
+                return res  # the dtype changed.
+        return dt1
+
+    return res
+
+
+def _getfield_is_safe(oldtype, newtype, offset):
+    """ Checks safety of getfield for object arrays.
+
+    As in _view_is_safe, we need to check that memory containing objects is not
+    reinterpreted as a non-object datatype and vice versa.
+
+    Parameters
+    ----------
+    oldtype : data-type
+        Data type of the original ndarray.
+    newtype : data-type
+        Data type of the field being accessed by ndarray.getfield
+    offset : int
+        Offset of the field being accessed by ndarray.getfield
+
+    Raises
+    ------
+    TypeError
+        If the field access is invalid
+
+    """
+    if newtype.hasobject or oldtype.hasobject:
+        if offset == 0 and newtype == oldtype:
+            return
+        if oldtype.names is not None:
+            for name in oldtype.names:
+                if (oldtype.fields[name][1] == offset and
+                        oldtype.fields[name][0] == newtype):
+                    return
+        raise TypeError("Cannot get/set field of an object array")
+    return
+
+def _view_is_safe(oldtype, newtype):
+    """ Checks safety of a view involving object arrays, for example when
+    doing::
+
+        np.zeros(10, dtype=oldtype).view(newtype)
+
+    Parameters
+    ----------
+    oldtype : data-type
+        Data type of original ndarray
+    newtype : data-type
+        Data type of the view
+
+    Raises
+    ------
+    TypeError
+        If the new type is incompatible with the old type.
+
+    """
+
+    # if the types are equivalent, there is no problem.
+    # for example: dtype((np.record, 'i4,i4')) == dtype((np.void, 'i4,i4'))
+    if oldtype == newtype:
+        return
+
+    if newtype.hasobject or oldtype.hasobject:
+        raise TypeError("Cannot change data-type for object array.")
+    return
+
+# Given a string containing a PEP 3118 format specifier,
+# construct a NumPy dtype
+
+_pep3118_native_map = {
+    '?': '?',
+    'c': 'S1',
+    'b': 'b',
+    'B': 'B',
+    'h': 'h',
+    'H': 'H',
+    'i': 'i',
+    'I': 'I',
+    'l': 'l',
+    'L': 'L',
+    'q': 'q',
+    'Q': 'Q',
+    'e': 'e',
+    'f': 'f',
+    'd': 'd',
+    'g': 'g',
+    'Zf': 'F',
+    'Zd': 'D',
+    'Zg': 'G',
+    's': 'S',
+    'w': 'U',
+    'O': 'O',
+    'x': 'V',  # padding
+}
+_pep3118_native_typechars = ''.join(_pep3118_native_map.keys())
+
+_pep3118_standard_map = {
+    '?': '?',
+    'c': 'S1',
+    'b': 'b',
+    'B': 'B',
+    'h': 'i2',
+    'H': 'u2',
+    'i': 'i4',
+    'I': 'u4',
+    'l': 'i4',
+    'L': 'u4',
+    'q': 'i8',
+    'Q': 'u8',
+    'e': 'f2',
+    'f': 'f',
+    'd': 'd',
+    'Zf': 'F',
+    'Zd': 'D',
+    's': 'S',
+    'w': 'U',
+    'O': 'O',
+    'x': 'V',  # padding
+}
+_pep3118_standard_typechars = ''.join(_pep3118_standard_map.keys())
+
+_pep3118_unsupported_map = {
+    'u': 'UCS-2 strings',
+    '&': 'pointers',
+    't': 'bitfields',
+    'X': 'function pointers',
+}
+
+class _Stream:
+    def __init__(self, s):
+        self.s = s
+        self.byteorder = '@'
+
+    def advance(self, n):
+        res = self.s[:n]
+        self.s = self.s[n:]
+        return res
+
+    def consume(self, c):
+        if self.s[:len(c)] == c:
+            self.advance(len(c))
+            return True
+        return False
+
+    def consume_until(self, c):
+        if callable(c):
+            i = 0
+            while i < len(self.s) and not c(self.s[i]):
+                i = i + 1
+            return self.advance(i)
+        else:
+            i = self.s.index(c)
+            res = self.advance(i)
+            self.advance(len(c))
+            return res
+
+    @property
+    def next(self):
+        return self.s[0]
+
+    def __bool__(self):
+        return bool(self.s)
+
+
+def _dtype_from_pep3118(spec):
+    stream = _Stream(spec)
+    dtype, align = __dtype_from_pep3118(stream, is_subdtype=False)
+    return dtype
+
+def __dtype_from_pep3118(stream, is_subdtype):
+    field_spec = dict(
+        names=[],
+        formats=[],
+        offsets=[],
+        itemsize=0
+    )
+    offset = 0
+    common_alignment = 1
+    is_padding = False
+
+    # Parse spec
+    while stream:
+        value = None
+
+        # End of structure, bail out to upper level
+        if stream.consume('}'):
+            break
+
+        # Sub-arrays (1)
+        shape = None
+        if stream.consume('('):
+            shape = stream.consume_until(')')
+            shape = tuple(map(int, shape.split(',')))
+
+        # Byte order
+        if stream.next in ('@', '=', '<', '>', '^', '!'):
+            byteorder = stream.advance(1)
+            if byteorder == '!':
+                byteorder = '>'
+            stream.byteorder = byteorder
+
+        # Byte order characters also control native vs. standard type sizes
+        if stream.byteorder in ('@', '^'):
+            type_map = _pep3118_native_map
+            type_map_chars = _pep3118_native_typechars
+        else:
+            type_map = _pep3118_standard_map
+            type_map_chars = _pep3118_standard_typechars
+
+        # Item sizes
+        itemsize_str = stream.consume_until(lambda c: not c.isdigit())
+        if itemsize_str:
+            itemsize = int(itemsize_str)
+        else:
+            itemsize = 1
+
+        # Data types
+        is_padding = False
+
+        if stream.consume('T{'):
+            value, align = __dtype_from_pep3118(
+                stream, is_subdtype=True)
+        elif stream.next in type_map_chars:
+            if stream.next == 'Z':
+                typechar = stream.advance(2)
+            else:
+                typechar = stream.advance(1)
+
+            is_padding = (typechar == 'x')
+            dtypechar = type_map[typechar]
+            if dtypechar in 'USV':
+                dtypechar += '%d' % itemsize
+                itemsize = 1
+            numpy_byteorder = {'@': '=', '^': '='}.get(
+                stream.byteorder, stream.byteorder)
+            value = dtype(numpy_byteorder + dtypechar)
+            align = value.alignment
+        elif stream.next in _pep3118_unsupported_map:
+            desc = _pep3118_unsupported_map[stream.next]
+            raise NotImplementedError(
+                "Unrepresentable PEP 3118 data type {!r} ({})"
+                .format(stream.next, desc))
+        else:
+            raise ValueError("Unknown PEP 3118 data type specifier %r" % stream.s)
+
+        #
+        # Native alignment may require padding
+        #
+        # Here we assume that the presence of a '@' character implicitly implies
+        # that the start of the array is *already* aligned.
+        #
+        extra_offset = 0
+        if stream.byteorder == '@':
+            start_padding = (-offset) % align
+            intra_padding = (-value.itemsize) % align
+
+            offset += start_padding
+
+            if intra_padding != 0:
+                if itemsize > 1 or (shape is not None and _prod(shape) > 1):
+                    # Inject internal padding to the end of the sub-item
+                    value = _add_trailing_padding(value, intra_padding)
+                else:
+                    # We can postpone the injection of internal padding,
+                    # as the item appears at most once
+                    extra_offset += intra_padding
+
+            # Update common alignment
+            common_alignment = _lcm(align, common_alignment)
+
+        # Convert itemsize to sub-array
+        if itemsize != 1:
+            value = dtype((value, (itemsize,)))
+
+        # Sub-arrays (2)
+        if shape is not None:
+            value = dtype((value, shape))
+
+        # Field name
+        if stream.consume(':'):
+            name = stream.consume_until(':')
+        else:
+            name = None
+
+        if not (is_padding and name is None):
+            if name is not None and name in field_spec['names']:
+                raise RuntimeError(f"Duplicate field name '{name}' in PEP3118 format")
+            field_spec['names'].append(name)
+            field_spec['formats'].append(value)
+            field_spec['offsets'].append(offset)
+
+        offset += value.itemsize
+        offset += extra_offset
+
+        field_spec['itemsize'] = offset
+
+    # extra final padding for aligned types
+    if stream.byteorder == '@':
+        field_spec['itemsize'] += (-offset) % common_alignment
+
+    # Check if this was a simple 1-item type, and unwrap it
+    if (field_spec['names'] == [None]
+            and field_spec['offsets'][0] == 0
+            and field_spec['itemsize'] == field_spec['formats'][0].itemsize
+            and not is_subdtype):
+        ret = field_spec['formats'][0]
+    else:
+        _fix_names(field_spec)
+        ret = dtype(field_spec)
+
+    # Finished
+    return ret, common_alignment
+
+def _fix_names(field_spec):
+    """ Replace names which are None with the next unused f%d name """
+    names = field_spec['names']
+    for i, name in enumerate(names):
+        if name is not None:
+            continue
+
+        j = 0
+        while True:
+            name = f'f{j}'
+            if name not in names:
+                break
+            j = j + 1
+        names[i] = name
+
+def _add_trailing_padding(value, padding):
+    """Inject the specified number of padding bytes at the end of a dtype"""
+    if value.fields is None:
+        field_spec = dict(
+            names=['f0'],
+            formats=[value],
+            offsets=[0],
+            itemsize=value.itemsize
+        )
+    else:
+        fields = value.fields
+        names = value.names
+        field_spec = dict(
+            names=names,
+            formats=[fields[name][0] for name in names],
+            offsets=[fields[name][1] for name in names],
+            itemsize=value.itemsize
+        )
+
+    field_spec['itemsize'] += padding
+    return dtype(field_spec)
+
+def _prod(a):
+    p = 1
+    for x in a:
+        p *= x
+    return p
+
+def _gcd(a, b):
+    """Calculate the greatest common divisor of a and b"""
+    while b:
+        a, b = b, a % b
+    return a
+
+def _lcm(a, b):
+    return a // _gcd(a, b) * b
+
+def array_ufunc_errmsg_formatter(dummy, ufunc, method, *inputs, **kwargs):
+    """ Format the error message for when __array_ufunc__ gives up. """
+    args_string = ', '.join(['{!r}'.format(arg) for arg in inputs] +
+                            ['{}={!r}'.format(k, v)
+                             for k, v in kwargs.items()])
+    args = inputs + kwargs.get('out', ())
+    types_string = ', '.join(repr(type(arg).__name__) for arg in args)
+    return ('operand type(s) all returned NotImplemented from '
+            '__array_ufunc__({!r}, {!r}, {}): {}'
+            .format(ufunc, method, args_string, types_string))
+
+
+def array_function_errmsg_formatter(public_api, types):
+    """ Format the error message for when __array_ufunc__ gives up. """
+    func_name = '{}.{}'.format(public_api.__module__, public_api.__name__)
+    return ("no implementation found for '{}' on types that implement "
+            '__array_function__: {}'.format(func_name, list(types)))
+
+
+def _ufunc_doc_signature_formatter(ufunc):
+    """
+    Builds a signature string which resembles PEP 457
+
+    This is used to construct the first line of the docstring
+    """
+
+    # input arguments are simple
+    if ufunc.nin == 1:
+        in_args = 'x'
+    else:
+        in_args = ', '.join(f'x{i+1}' for i in range(ufunc.nin))
+
+    # output arguments are both keyword or positional
+    if ufunc.nout == 0:
+        out_args = ', /, out=()'
+    elif ufunc.nout == 1:
+        out_args = ', /, out=None'
+    else:
+        out_args = '[, {positional}], / [, out={default}]'.format(
+            positional=', '.join(
+                'out{}'.format(i+1) for i in range(ufunc.nout)),
+            default=repr((None,)*ufunc.nout)
+        )
+
+    # keyword only args depend on whether this is a gufunc
+    kwargs = (
+        ", casting='same_kind'"
+        ", order='K'"
+        ", dtype=None"
+        ", subok=True"
+    )
+
+    # NOTE: gufuncs may or may not support the `axis` parameter
+    if ufunc.signature is None:
+        kwargs = f", where=True{kwargs}[, signature, extobj]"
+    else:
+        kwargs += "[, signature, extobj, axes, axis]"
+
+    # join all the parts together
+    return '{name}({in_args}{out_args}, *{kwargs})'.format(
+        name=ufunc.__name__,
+        in_args=in_args,
+        out_args=out_args,
+        kwargs=kwargs
+    )
+
+
+def npy_ctypes_check(cls):
+    # determine if a class comes from ctypes, in order to work around
+    # a bug in the buffer protocol for those objects, bpo-10746
+    try:
+        # ctypes class are new-style, so have an __mro__. This probably fails
+        # for ctypes classes with multiple inheritance.
+        if IS_PYPY:
+            # (..., _ctypes.basics._CData, Bufferable, object)
+            ctype_base = cls.__mro__[-3]
+        else:
+            # # (..., _ctypes._CData, object)
+            ctype_base = cls.__mro__[-2]
+        # right now, they're part of the _ctypes module
+        return '_ctypes' in ctype_base.__module__
+    except Exception:
+        return False
('numpy/core', 'records.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,10 +1,56 @@
-__all__ = ['record', 'recarray', 'format_parser']
-
-import numeric as sb
-from defchararray import chararray
-import numerictypes as nt
-import types
-import stat, os
+"""
+Record Arrays
+=============
+Record arrays expose the fields of structured arrays as properties.
+
+Most commonly, ndarrays contain elements of a single type, e.g. floats,
+integers, bools etc.  However, it is possible for elements to be combinations
+of these using structured types, such as::
+
+  >>> a = np.array([(1, 2.0), (1, 2.0)], dtype=[('x', np.int64), ('y', np.float64)])
+  >>> a
+  array([(1, 2.), (1, 2.)], dtype=[('x', '<i8'), ('y', '<f8')])
+
+Here, each element consists of two fields: x (and int), and y (a float).
+This is known as a structured array.  The different fields are analogous
+to columns in a spread-sheet.  The different fields can be accessed as
+one would a dictionary::
+
+  >>> a['x']
+  array([1, 1])
+
+  >>> a['y']
+  array([2., 2.])
+
+Record arrays allow us to access fields as properties::
+
+  >>> ar = np.rec.array(a)
+
+  >>> ar.x
+  array([1, 1])
+
+  >>> ar.y
+  array([2., 2.])
+
+"""
+import warnings
+from collections import Counter
+from contextlib import nullcontext
+
+from . import numeric as sb
+from . import numerictypes as nt
+from numpy.compat import os_fspath
+from numpy.core.overrides import set_module
+from .arrayprint import _get_legacy_print_mode
+
+# All of the functions allow formats to be a dtype
+__all__ = [
+    'record', 'recarray', 'format_parser',
+    'fromarrays', 'fromrecords', 'fromstring', 'fromfile', 'array',
+]
+
+
+ndarray = sb.ndarray
 
 _byteorderconv = {'b':'>',
                   'l':'<',
@@ -22,38 +68,104 @@
                   'i':'|'}
 
 # formats regular expression
-# allows multidimension spec with a tuple syntax in front
+# allows multidimensional spec with a tuple syntax in front
 # of the letter code '(2,3)f4' and ' (  2 ,  3  )  f4  '
 # are equally allowed
 
-numfmt = nt.typeDict
-_typestr = nt._typestr
+numfmt = nt.sctypeDict
+
 
 def find_duplicate(list):
     """Find duplication in a list, return a list of duplicated elements"""
-    dup = []
-    for i in range(len(list)):
-        if (list[i] in list[i+1:]):
-            if (list[i] not in dup):
-                dup.append(list[i])
-    return dup
-
-
+    return [
+        item
+        for item, counts in Counter(list).items()
+        if counts > 1
+    ]
+
+
+@set_module('numpy')
 class format_parser:
-    def __init__(self, formats, names, titles, aligned=False):
+    """
+    Class to convert formats, names, titles description to a dtype.
+
+    After constructing the format_parser object, the dtype attribute is
+    the converted data-type:
+    ``dtype = format_parser(formats, names, titles).dtype``
+
+    Attributes
+    ----------
+    dtype : dtype
+        The converted data-type.
+
+    Parameters
+    ----------
+    formats : str or list of str
+        The format description, either specified as a string with
+        comma-separated format descriptions in the form ``'f8, i4, a5'``, or
+        a list of format description strings  in the form
+        ``['f8', 'i4', 'a5']``.
+    names : str or list/tuple of str
+        The field names, either specified as a comma-separated string in the
+        form ``'col1, col2, col3'``, or as a list or tuple of strings in the
+        form ``['col1', 'col2', 'col3']``.
+        An empty list can be used, in that case default field names
+        ('f0', 'f1', ...) are used.
+    titles : sequence
+        Sequence of title strings. An empty list can be used to leave titles
+        out.
+    aligned : bool, optional
+        If True, align the fields by padding as the C-compiler would.
+        Default is False.
+    byteorder : str, optional
+        If specified, all the fields will be changed to the
+        provided byte-order.  Otherwise, the default byte-order is
+        used. For all available string specifiers, see `dtype.newbyteorder`.
+
+    See Also
+    --------
+    dtype, typename, sctype2char
+
+    Examples
+    --------
+    >>> np.format_parser(['<f8', '<i4', '<a5'], ['col1', 'col2', 'col3'],
+    ...                  ['T1', 'T2', 'T3']).dtype
+    dtype([(('T1', 'col1'), '<f8'), (('T2', 'col2'), '<i4'), (('T3', 'col3'), 'S5')])
+
+    `names` and/or `titles` can be empty lists. If `titles` is an empty list,
+    titles will simply not appear. If `names` is empty, default field names
+    will be used.
+
+    >>> np.format_parser(['f8', 'i4', 'a5'], ['col1', 'col2', 'col3'],
+    ...                  []).dtype
+    dtype([('col1', '<f8'), ('col2', '<i4'), ('col3', '<S5')])
+    >>> np.format_parser(['<f8', '<i4', '<a5'], [], []).dtype
+    dtype([('f0', '<f8'), ('f1', '<i4'), ('f2', 'S5')])
+
+    """
+
+    def __init__(self, formats, names, titles, aligned=False, byteorder=None):
         self._parseFormats(formats, aligned)
         self._setfieldnames(names, titles)
-        self._createdescr()
-
-    def _parseFormats(self, formats, aligned=0):
+        self._createdtype(byteorder)
+
+    def _parseFormats(self, formats, aligned=False):
         """ Parse the field formats """
 
-        dtype = sb.dtype(formats, aligned)
+        if formats is None:
+            raise ValueError("Need formats argument")
+        if isinstance(formats, list):
+            dtype = sb.dtype(
+                [('f{}'.format(i), format_) for i, format_ in enumerate(formats)],
+                aligned,
+            )
+        else:
+            dtype = sb.dtype(formats, aligned)
         fields = dtype.fields
         if fields is None:
-            dtype = sb.dtype([formats], aligned)
+            dtype = sb.dtype([('f1', dtype)], aligned)
             fields = dtype.fields
-        keys = fields[-1]
+        keys = dtype.names
         self._f_formats = [fields[key][0] for key in keys]
         self._offsets = [fields[key][1] for key in keys]
         self._nfields = len(keys)
@@ -62,80 +174,128 @@
         """convert input field names into a list and assign to the _names
         attribute """
 
-        if (names):
-            if (type(names) in [types.ListType, types.TupleType]):
+        if names:
+            if type(names) in [list, tuple]:
                 pass
-            elif (type(names) == types.StringType):
+            elif isinstance(names, str):
                 names = names.split(',')
             else:
-                raise NameError, "illegal input names %s" % `names`
+                raise NameError("illegal input names %s" % repr(names))
 
             self._names = [n.strip() for n in names[:self._nfields]]
         else:
             self._names = []
 
-        # if the names are not specified, they will be assigned as "f1, f2,..."
-        # if not enough names are specified, they will be assigned as "f[n+1],
-        # f[n+2],..." etc. where n is the number of specified names..."
-        self._names += ['f%d' % i for i in range(len(self._names)+1,
-                                                 self._nfields+1)]
+        # if the names are not specified, they will be assigned as
+        #  "f0, f1, f2,..."
+        # if not enough names are specified, they will be assigned as "f[n],
+        # f[n+1],..." etc. where n is the number of specified names..."
+        self._names += ['f%d' % i for i in range(len(self._names),
+                                                 self._nfields)]
         # check for redundant names
         _dup = find_duplicate(self._names)
         if _dup:
-            raise ValueError, "Duplicate field names: %s" % _dup
-
-        if (titles):
+            raise ValueError("Duplicate field names: %s" % _dup)
+
+        if titles:
             self._titles = [n.strip() for n in titles[:self._nfields]]
         else:
             self._titles = []
             titles = []
 
-        if (self._nfields > len(titles)):
-            self._titles += [None]*(self._nfields-len(titles))
-
-    def _createdescr(self):
-        self._descr = sb.dtype({'names':self._names,
-                                'formats':self._f_formats,
-                                'offsets':self._offsets,
-                                'titles':self._titles})
+        if self._nfields > len(titles):
+            self._titles += [None] * (self._nfields - len(titles))
+
+    def _createdtype(self, byteorder):
+        dtype = sb.dtype({
+            'names': self._names,
+            'formats': self._f_formats,
+            'offsets': self._offsets,
+            'titles': self._titles,
+        })
+        if byteorder is not None:
+            byteorder = _byteorderconv[byteorder[0]]
+            dtype = dtype.newbyteorder(byteorder)
+
+        self.dtype = dtype
+
 
 class record(nt.void):
+    """A data-type scalar that allows field access as attribute lookup.
+    """
+
+    # manually set name and module so that this class's type shows up
+    # as numpy.record when printed
+    __name__ = 'record'
+    __module__ = 'numpy'
+
     def __repr__(self):
-        return self.__str__()
+        if _get_legacy_print_mode() <= 113:
+            return self.__str__()
+        return super().__repr__()
 
     def __str__(self):
-        return str(self.item())
+        if _get_legacy_print_mode() <= 113:
+            return str(self.item())
+        return super().__str__()
 
     def __getattribute__(self, attr):
-        if attr in ['setfield', 'getfield', 'dtype']:
+        if attr in ('setfield', 'getfield', 'dtype'):
             return nt.void.__getattribute__(self, attr)
         try:
             return nt.void.__getattribute__(self, attr)
         except AttributeError:
             pass
         fielddict = nt.void.__getattribute__(self, 'dtype').fields
-        res = fielddict.get(attr,None)
+        res = fielddict.get(attr, None)
         if res:
-            return self.getfield(*res[:2])
-        else:
-            raise AttributeError, "'record' object has no "\
-                  "attribute '%s'" % attr
-        
+            obj = self.getfield(*res[:2])
+            # if it has fields return a record,
+            # otherwise return the object
+            try:
+                dt = obj.dtype
+            except AttributeError:
+                #happens if field is Object type
+                return obj
+            if dt.names is not None:
+                return obj.view((self.__class__, obj.dtype))
+            return obj
+        else:
+            raise AttributeError("'record' object has no "
+                    "attribute '%s'" % attr)
 
     def __setattr__(self, attr, val):
-        if attr in ['setfield', 'getfield', 'dtype']:
-            raise AttributeError, "Cannot set '%s' attribute" % attr;
-        try:
-            return nt.void.__setattr__(self,attr,val)
-        except AttributeError:
-            pass
-        fielddict = nt.void.__getattribute__(self,'dtype').fields
-        res = fielddict.get(attr,None)
+        if attr in ('setfield', 'getfield', 'dtype'):
+            raise AttributeError("Cannot set '%s' attribute" % attr)
+        fielddict = nt.void.__getattribute__(self, 'dtype').fields
+        res = fielddict.get(attr, None)
         if res:
-            return self.setfield(val,*res[:2])
-        else:
-            raise AttributeError, "'record' object has no "\
-                  "attribute '%s'" % attr
+            return self.setfield(val, *res[:2])
+        else:
+            if getattr(self, attr, None):
+                return nt.void.__setattr__(self, attr, val)
+            else:
+                raise AttributeError("'record' object has no "
+                        "attribute '%s'" % attr)
+
+    def __getitem__(self, indx):
+        obj = nt.void.__getitem__(self, indx)
+
+        # copy behavior of record.__getattribute__,
+        if isinstance(obj, nt.void) and obj.dtype.names is not None:
+            return obj.view((self.__class__, obj.dtype))
+        else:
+            # return a single element
+            return obj
+
+    def pprint(self):
+        """Pretty-print all fields."""
+        # pretty-print all fields
+        names = self.dtype.names
+        maxlen = max(len(name) for name in names)
+        fmt = '%% %ds: %%s' % maxlen
+        rows = [fmt % (name, getattr(self, name)) for name in names]
+        return "\n".join(rows)
 
 # The recarray is almost identical to a standard array (which supports
 #   named fields already)  The biggest difference is that it can use
@@ -145,313 +305,795 @@
 # If byteorder is given it forces a particular byteorder on all
 #  the fields (and any subfields)
 
-class recarray(sb.ndarray):
-    def __new__(subtype, shape, formats, names=None, titles=None,
-                buf=None, offset=0, strides=None, byteorder=None,
-                aligned=0):
-
-        if isinstance(formats, sb.dtype):
-            descr = formats
-        else:
-            parsed = format_parser(formats, names, titles, aligned)
-            descr = parsed._descr
-
-        if (byteorder is not None):
-            byteorder = _byteorderconv[byteorder[0]]
-            descr = descr.newbyteorder(byteorder)
+class recarray(ndarray):
+    """Construct an ndarray that allows field access using attributes.
+
+    Arrays may have a data-types containing fields, analogous
+    to columns in a spread sheet.  An example is ``[(x, int), (y, float)]``,
+    where each entry in the array is a pair of ``(int, float)``.  Normally,
+    these attributes are accessed using dictionary lookups such as ``arr['x']``
+    and ``arr['y']``.  Record arrays allow the fields to be accessed as members
+    of the array, using ``arr.x`` and ``arr.y``.
+
+    Parameters
+    ----------
+    shape : tuple
+        Shape of output array.
+    dtype : data-type, optional
+        The desired data-type.  By default, the data-type is determined
+        from `formats`, `names`, `titles`, `aligned` and `byteorder`.
+    formats : list of data-types, optional
+        A list containing the data-types for the different columns, e.g.
+        ``['i4', 'f8', 'i4']``.  `formats` does *not* support the new
+        convention of using types directly, i.e. ``(int, float, int)``.
+        Note that `formats` must be a list, not a tuple.
+        Given that `formats` is somewhat limited, we recommend specifying
+        `dtype` instead.
+    names : tuple of str, optional
+        The name of each column, e.g. ``('x', 'y', 'z')``.
+    buf : buffer, optional
+        By default, a new array is created of the given shape and data-type.
+        If `buf` is specified and is an object exposing the buffer interface,
+        the array will use the memory from the existing buffer.  In this case,
+        the `offset` and `strides` keywords are available.
+
+    Other Parameters
+    ----------------
+    titles : tuple of str, optional
+        Aliases for column names.  For example, if `names` were
+        ``('x', 'y', 'z')`` and `titles` is
+        ``('x_coordinate', 'y_coordinate', 'z_coordinate')``, then
+        ``arr['x']`` is equivalent to both ``arr.x`` and ``arr.x_coordinate``.
+    byteorder : {'<', '>', '='}, optional
+        Byte-order for all fields.
+    aligned : bool, optional
+        Align the fields in memory as the C-compiler would.
+    strides : tuple of ints, optional
+        Buffer (`buf`) is interpreted according to these strides (strides
+        define how many bytes each array element, row, column, etc.
+        occupy in memory).
+    offset : int, optional
+        Start reading buffer (`buf`) from this offset onwards.
+    order : {'C', 'F'}, optional
+        Row-major (C-style) or column-major (Fortran-style) order.
+
+    Returns
+    -------
+    rec : recarray
+        Empty array of the given shape and type.
+
+    See Also
+    --------
+    core.records.fromrecords : Construct a record array from data.
+    record : fundamental data-type for `recarray`.
+    format_parser : determine a data-type from formats, names, titles.
+
+    Notes
+    -----
+    This constructor can be compared to ``empty``: it creates a new record
+    array but does not fill it with data.  To create a record array from data,
+    use one of the following methods:
+
+    1. Create a standard ndarray and convert it to a record array,
+       using ``arr.view(np.recarray)``
+    2. Use the `buf` keyword.
+    3. Use `np.rec.fromrecords`.
+
+    Examples
+    --------
+    Create an array with two fields, ``x`` and ``y``:
+
+    >>> x = np.array([(1.0, 2), (3.0, 4)], dtype=[('x', '<f8'), ('y', '<i8')])
+    >>> x
+    array([(1., 2), (3., 4)], dtype=[('x', '<f8'), ('y', '<i8')])
+
+    >>> x['x']
+    array([1., 3.])
+
+    View the array as a record array:
+
+    >>> x = x.view(np.recarray)
+
+    >>> x.x
+    array([1., 3.])
+
+    >>> x.y
+    array([2, 4])
+
+    Create a new, empty record array:
+
+    >>> np.recarray((2,),
+    ... dtype=[('x', int), ('y', float), ('z', int)]) #doctest: +SKIP
+    rec.array([(-1073741821, 1.2249118382103472e-301, 24547520),
+           (3471280, 1.2134086255804012e-316, 0)],
+          dtype=[('x', '<i4'), ('y', '<f8'), ('z', '<i4')])
+
+    """
+
+    # manually set name and module so that this class's type shows
+    # up as "numpy.recarray" when printed
+    __name__ = 'recarray'
+    __module__ = 'numpy'
+
+    def __new__(subtype, shape, dtype=None, buf=None, offset=0, strides=None,
+                formats=None, names=None, titles=None,
+                byteorder=None, aligned=False, order='C'):
+
+        if dtype is not None:
+            descr = sb.dtype(dtype)
+        else:
+            descr = format_parser(formats, names, titles, aligned, byteorder).dtype
 
         if buf is None:
-            self = sb.ndarray.__new__(subtype, shape, (record, descr))
-        else:
-            self = sb.ndarray.__new__(subtype, shape, (record, descr),
+            self = ndarray.__new__(subtype, shape, (record, descr), order=order)
+        else:
+            self = ndarray.__new__(subtype, shape, (record, descr),
                                       buffer=buf, offset=offset,
-                                      strides=strides)
+                                      strides=strides, order=order)
         return self
 
+    def __array_finalize__(self, obj):
+        if self.dtype.type is not record and self.dtype.names is not None:
+            # if self.dtype is not np.record, invoke __setattr__ which will
+            # convert it to a record if it is a void dtype.
+            self.dtype = self.dtype
+
     def __getattribute__(self, attr):
+        # See if ndarray has this attr, and return it if so. (note that this
+        # means a field with the same name as an ndarray attr cannot be
+        # accessed by attribute).
         try:
-            return object.__getattribute__(self,attr)
-        except AttributeError: # attr must be a fieldname
+            return object.__getattribute__(self, attr)
+        except AttributeError:  # attr must be a fieldname
             pass
-        fielddict = sb.ndarray.__getattribute__(self,'dtype').fields
+
+        # look for a field with this name
+        fielddict = ndarray.__getattribute__(self, 'dtype').fields
         try:
             res = fielddict[attr][:2]
-        except KeyError:
-            raise AttributeError, "record array has no attribute %s" % attr
+        except (TypeError, KeyError) as e:
+            raise AttributeError("recarray has no attribute %s" % attr) from e
         obj = self.getfield(*res)
-        # if it has fields return a recarray, otherwise return
-        # normal array
-        if obj.dtype.fields:
+
+        # At this point obj will always be a recarray, since (see
+        # PyArray_GetField) the type of obj is inherited. Next, if obj.dtype is
+        # non-structured, convert it to an ndarray. Then if obj is structured
+        # with void type convert it to the same dtype.type (eg to preserve
+        # numpy.record type if present), since nested structured fields do not
+        # inherit type. Don't do this for non-void structures though.
+        if obj.dtype.names is not None:
+            if issubclass(obj.dtype.type, nt.void):
+                return obj.view(dtype=(self.dtype.type, obj.dtype))
             return obj
-        if obj.dtype.char in 'SU':
-            return obj.view(chararray)
-        return obj.view(sb.ndarray)
-
+        else:
+            return obj.view(ndarray)
+
+    # Save the dictionary.
+    # If the attr is a field name and not in the saved dictionary
+    # Undo any "setting" of the attribute and do a setfield
+    # Thus, you can't create attributes on-the-fly that are field names.
     def __setattr__(self, attr, val):
+
+        # Automatically convert (void) structured types to records
+        # (but not non-void structures, subarrays, or non-structured voids)
+        if attr == 'dtype' and issubclass(val.type, nt.void) and val.names is not None:
+            val = sb.dtype((record, val))
+
+        newattr = attr not in self.__dict__
         try:
-            return object.__setattr__(self, attr, val)
-        except AttributeError: # Must be a fieldname
-            pass
-        fielddict = sb.ndarray.__getattribute__(self,'dtype').fields
+            ret = object.__setattr__(self, attr, val)
+        except Exception:
+            fielddict = ndarray.__getattribute__(self, 'dtype').fields or {}
+            if attr not in fielddict:
+                raise
+        else:
+            fielddict = ndarray.__getattribute__(self, 'dtype').fields or {}
+            if attr not in fielddict:
+                return ret
+            if newattr:
+                # We just added this one or this setattr worked on an
+                # internal attribute.
+                try:
+                    object.__delattr__(self, attr)
+                except Exception:
+                    return ret
         try:
             res = fielddict[attr][:2]
-        except KeyError:
-            raise AttributeError, "record array has no attribute %s" % attr 
-        return self.setfield(val,*res)
-
-    def field(self,attr, val=None):
-        fielddict = sb.ndarray.__getattribute__(self,'dtype').fields
-
-        if isinstance(attr,int):
-            attr=fielddict[-1][attr]
+        except (TypeError, KeyError) as e:
+            raise AttributeError(
+                "record array has no attribute %s" % attr
+            ) from e
+        return self.setfield(val, *res)
+
+    def __getitem__(self, indx):
+        obj = super().__getitem__(indx)
+
+        # copy behavior of getattr, except that here
+        # we might also be returning a single element
+        if isinstance(obj, ndarray):
+            if obj.dtype.names is not None:
+                obj = obj.view(type(self))
+                if issubclass(obj.dtype.type, nt.void):
+                    return obj.view(dtype=(self.dtype.type, obj.dtype))
+                return obj
+            else:
+                return obj.view(type=ndarray)
+        else:
+            # return a single element
+            return obj
+
+    def __repr__(self):
+
+        repr_dtype = self.dtype
+        if self.dtype.type is record or not issubclass(self.dtype.type, nt.void):
+            # If this is a full record array (has numpy.record dtype),
+            # or if it has a scalar (non-void) dtype with no records,
+            # represent it using the rec.array function. Since rec.array
+            # converts dtype to a numpy.record for us, convert back
+            # to non-record before printing
+            if repr_dtype.type is record:
+                repr_dtype = sb.dtype((nt.void, repr_dtype))
+            prefix = "rec.array("
+            fmt = 'rec.array(%s,%sdtype=%s)'
+        else:
+            # otherwise represent it using np.array plus a view
+            # This should only happen if the user is playing
+            # strange games with dtypes.
+            prefix = "array("
+            fmt = 'array(%s,%sdtype=%s).view(numpy.recarray)'
+
+        # get data/shape string. logic taken from numeric.array_repr
+        if self.size > 0 or self.shape == (0,):
+            lst = sb.array2string(
+                self, separator=', ', prefix=prefix, suffix=',')
+        else:
+            # show zero-length shape unless it is (0,)
+            lst = "[], shape=%s" % (repr(self.shape),)
+
+        lf = '\n'+' '*len(prefix)
+        if _get_legacy_print_mode() <= 113:
+            lf = ' ' + lf  # trailing space
+        return fmt % (lst, lf, repr_dtype)
+
+    def field(self, attr, val=None):
+        if isinstance(attr, int):
+            names = ndarray.__getattribute__(self, 'dtype').names
+            attr = names[attr]
+
+        fielddict = ndarray.__getattribute__(self, 'dtype').fields
 
         res = fielddict[attr][:2]
 
         if val is None:
             obj = self.getfield(*res)
-            if obj.dtype.fields:
+            if obj.dtype.names is not None:
                 return obj
-            if obj.dtype.char in 'SU':
-                return obj.view(chararray)
-            return obj.view(sb.ndarray)
+            return obj.view(ndarray)
         else:
             return self.setfield(val, *res)
 
-    def view(self, obj):
-        try:
-            if issubclass(obj, sb.ndarray):
-                return sb.ndarray.view(self, obj)
-        except TypeError:
-            pass
-        dtype = sb.dtype(obj)
-        if dtype.fields is None:
-            return self.__array__().view(dtype)
-        return sb.ndarray.view(self, obj)
-            
-
-def fromarrays(arrayList, formats=None, names=None, titles=None, shape=None,
-               aligned=0):
-    """ create a record array from a (flat) list of arrays
-
-    >>> x1=array([1,2,3,4])
-    >>> x2=array(['a','dd','xyz','12'])
-    >>> x3=array([1.1,2,3,4])
-    >>> r=fromarrays([x1,x2,x3],names='a,b,c')
-    >>> print r[1]
-    (2, 'dd', 2.0)
+
+def _deprecate_shape_0_as_None(shape):
+    if shape == 0:
+        warnings.warn(
+            "Passing `shape=0` to have the shape be inferred is deprecated, "
+            "and in future will be equivalent to `shape=(0,)`. To infer "
+            "the shape and suppress this warning, pass `shape=None` instead.",
+            FutureWarning, stacklevel=3)
+        return None
+    else:
+        return shape
+
+
+@set_module("numpy.rec")
+def fromarrays(arrayList, dtype=None, shape=None, formats=None,
+               names=None, titles=None, aligned=False, byteorder=None):
+    """Create a record array from a (flat) list of arrays
+
+    Parameters
+    ----------
+    arrayList : list or tuple
+        List of array-like objects (such as lists, tuples,
+        and ndarrays).
+    dtype : data-type, optional
+        valid dtype for all arrays
+    shape : int or tuple of ints, optional
+        Shape of the resulting array. If not provided, inferred from
+        ``arrayList[0]``.
+    formats, names, titles, aligned, byteorder :
+        If `dtype` is ``None``, these arguments are passed to
+        `numpy.format_parser` to construct a dtype. See that function for
+        detailed documentation.
+
+    Returns
+    -------
+    np.recarray
+        Record array consisting of given arrayList columns.
+
+    Examples
+    --------
+    >>> x1=np.array([1,2,3,4])
+    >>> x2=np.array(['a','dd','xyz','12'])
+    >>> x3=np.array([1.1,2,3,4])
+    >>> r = np.core.records.fromarrays([x1,x2,x3],names='a,b,c')
+    >>> print(r[1])
+    (2, 'dd', 2.0) # may vary
     >>> x1[1]=34
     >>> r.a
     array([1, 2, 3, 4])
+
+    >>> x1 = np.array([1, 2, 3, 4])
+    >>> x2 = np.array(['a', 'dd', 'xyz', '12'])
+    >>> x3 = np.array([1.1, 2, 3,4])
+    >>> r = np.core.records.fromarrays(
+    ...     [x1, x2, x3],
+    ...     dtype=np.dtype([('a', np.int32), ('b', 'S3'), ('c', np.float32)]))
+    >>> r
+    rec.array([(1, b'a', 1.1), (2, b'dd', 2. ), (3, b'xyz', 3. ),
+               (4, b'12', 4. )],
+              dtype=[('a', '<i4'), ('b', 'S3'), ('c', '<f4')])
     """
 
-    if shape is None or shape == 0:
+    arrayList = [sb.asarray(x) for x in arrayList]
+
+    # NumPy 1.19.0, 2020-01-01
+    shape = _deprecate_shape_0_as_None(shape)
+
+    if shape is None:
         shape = arrayList[0].shape
-
-    if isinstance(shape, int):
+    elif isinstance(shape, int):
         shape = (shape,)
 
-    if formats is None:
+    if formats is None and dtype is None:
         # go through each object in the list to see if it is an ndarray
         # and determine the formats.
-        formats = ''
-        for obj in arrayList:
-            if not isinstance(obj, sb.ndarray):
-                raise ValueError, "item in the array list must be an ndarray."
-            formats += _typestr[obj.dtype.type]
-            if issubclass(obj.dtype.type, nt.flexible):
-                formats += `obj.itemsize`
-            formats += ','
-        formats=formats[:-1]
-
-    for obj in arrayList:
-        if obj.shape != shape:
-            raise ValueError, "array has different shape"
-
-    parsed = format_parser(formats, names, titles, aligned)
-    _names = parsed._names
-    _array = recarray(shape, parsed._descr)
+        formats = [obj.dtype for obj in arrayList]
+
+    if dtype is not None:
+        descr = sb.dtype(dtype)
+    else:
+        descr = format_parser(formats, names, titles, aligned, byteorder).dtype
+    _names = descr.names
+
+    # Determine shape from data-type.
+    if len(descr) != len(arrayList):
+        raise ValueError("mismatch between the number of fields "
+                "and the number of arrays")
+
+    d0 = descr[0].shape
+    nn = len(d0)
+    if nn > 0:
+        shape = shape[:-nn]
+
+    _array = recarray(shape, descr)
 
     # populate the record array (makes a copy)
-    for i in range(len(arrayList)):
-        _array[_names[i]] = arrayList[i]
+    for k, obj in enumerate(arrayList):
+        nn = descr[k].ndim
+        testshape = obj.shape[:obj.ndim - nn]
+        name = _names[k]
+        if testshape != shape:
+            raise ValueError(f'array-shape mismatch in array {k} ("{name}")')
+
+        _array[name] = obj
 
     return _array
 
-# shape must be 1-d if you use list of lists...
-def fromrecords(recList, formats=None, names=None, titles=None, shape=None,
-                aligned=0):
-    """ create a recarray from a list of records in text form
-
-        The data in the same field can be heterogeneous, they will be promoted
-        to the highest data type.  This method is intended for creating
-        smaller record arrays.  If used to create large array without formats
-        defined
-
-        r=fromrecords([(2,3.,'abc')]*100000)
-
-        it can be slow.
-
-        If formats is None, then this will auto-detect formats. Use list of
-        tuples rather than list of lists for faster processing.
-
-    >>> r=fromrecords([(456,'dbe',1.2),(2,'de',1.3)],names='col1,col2,col3')
-    >>> print r[0]
+
+@set_module("numpy.rec")
+def fromrecords(recList, dtype=None, shape=None, formats=None, names=None,
+                titles=None, aligned=False, byteorder=None):
+    """Create a recarray from a list of records in text form.
+
+    Parameters
+    ----------
+    recList : sequence
+        data in the same field may be heterogeneous - they will be promoted
+        to the highest data type.
+    dtype : data-type, optional
+        valid dtype for all arrays
+    shape : int or tuple of ints, optional
+        shape of each array.
+    formats, names, titles, aligned, byteorder :
+        If `dtype` is ``None``, these arguments are passed to
+        `numpy.format_parser` to construct a dtype. See that function for
+        detailed documentation.
+
+        If both `formats` and `dtype` are None, then this will auto-detect
+        formats. Use list of tuples rather than list of lists for faster
+        processing.
+
+    Returns
+    -------
+    np.recarray
+        record array consisting of given recList rows.
+
+    Examples
+    --------
+    >>> r=np.core.records.fromrecords([(456,'dbe',1.2),(2,'de',1.3)],
+    ... names='col1,col2,col3')
+    >>> print(r[0])
     (456, 'dbe', 1.2)
     >>> r.col1
     array([456,   2])
     >>> r.col2
-    chararray(['dbe', 'de'])
-    >>> import cPickle
-    >>> print cPickle.loads(cPickle.dumps(r))
-    recarray[
-    (456, 'dbe', 1.2),
-    (2, 'de', 1.3)
-    ]
+    array(['dbe', 'de'], dtype='<U3')
+    >>> import pickle
+    >>> pickle.loads(pickle.dumps(r))
+    rec.array([(456, 'dbe', 1.2), (  2, 'de', 1.3)],
+              dtype=[('col1', '<i8'), ('col2', '<U3'), ('col3', '<f8')])
     """
 
-    nfields = len(recList[0])
-    if formats is None:  # slower
-        obj = sb.array(recList,dtype=object)
-        arrlist = [sb.array(obj[...,i].tolist()) for i in xrange(nfields)]
+    if formats is None and dtype is None:  # slower
+        obj = sb.array(recList, dtype=object)
+        arrlist = [sb.array(obj[..., i].tolist()) for i in range(obj.shape[-1])]
         return fromarrays(arrlist, formats=formats, shape=shape, names=names,
-                          titles=titles, aligned=aligned)
-
-    parsed = format_parser(formats, names, titles, aligned)
+                          titles=titles, aligned=aligned, byteorder=byteorder)
+
+    if dtype is not None:
+        descr = sb.dtype((record, dtype))
+    else:
+        descr = format_parser(formats, names, titles, aligned, byteorder).dtype
+
     try:
-        retval = sb.array(recList, dtype = parsed._descr)
-    except TypeError:  # list of lists instead of list of tuples
-        if (shape is None or shape == 0):
+        retval = sb.array(recList, dtype=descr)
+    except (TypeError, ValueError):
+        # NumPy 1.19.0, 2020-01-01
+        shape = _deprecate_shape_0_as_None(shape)
+        if shape is None:
             shape = len(recList)
-        if isinstance(shape, (int, long)):
+        if isinstance(shape, int):
             shape = (shape,)
         if len(shape) > 1:
-            raise ValueError, "Can only deal with 1-d array."
-        _array = recarray(shape, parsed._descr)
-        for k in xrange(_array.size):
+            raise ValueError("Can only deal with 1-d array.")
+        _array = recarray(shape, descr)
+        for k in range(_array.size):
             _array[k] = tuple(recList[k])
+        # list of lists instead of list of tuples ?
+        # 2018-02-07, 1.14.1
+        warnings.warn(
+            "fromrecords expected a list of tuples, may have received a list "
+            "of lists instead. In the future that will raise an error",
+            FutureWarning, stacklevel=2)
         return _array
     else:
         if shape is not None and retval.shape != shape:
             retval.shape = shape
 
     res = retval.view(recarray)
-    res.dtype = sb.dtype((record, res.dtype))
+
     return res
 
 
-def fromstring(datastring, formats, shape=None, names=None, titles=None,
-               byteorder=None, aligned=0, offset=0):
-    """ create a (read-only) record array from binary data contained in
-    a string"""
-
-    parsed = format_parser(formats, names, titles, aligned)
-    itemsize = parsed._descr.itemsize
-    if (shape is None or shape == 0 or shape == -1):
-        shape = (len(datastring)-offset) / itemsize
-
-    _array = recarray(shape, parsed._descr, names=names,
-                      titles=titles, buf=datastring, offset=offset,
-                      byteorder=byteorder)
+@set_module("numpy.rec")
+def fromstring(datastring, dtype=None, shape=None, offset=0, formats=None,
+               names=None, titles=None, aligned=False, byteorder=None):
+    r"""Create a record array from binary data
+
+    Note that despite the name of this function it does not accept `str`
+    instances.
+
+    Parameters
+    ----------
+    datastring : bytes-like
+        Buffer of binary data
+    dtype : data-type, optional
+        Valid dtype for all arrays
+    shape : int or tuple of ints, optional
+        Shape of each array.
+    offset : int, optional
+        Position in the buffer to start reading from.
+    formats, names, titles, aligned, byteorder :
+        If `dtype` is ``None``, these arguments are passed to
+        `numpy.format_parser` to construct a dtype. See that function for
+        detailed documentation.
+
+
+    Returns
+    -------
+    np.recarray
+        Record array view into the data in datastring. This will be readonly
+        if `datastring` is readonly.
+
+    See Also
+    --------
+    numpy.frombuffer
+
+    Examples
+    --------
+    >>> a = b'\x01\x02\x03abc'
+    >>> np.core.records.fromstring(a, dtype='u1,u1,u1,S3')
+    rec.array([(1, 2, 3, b'abc')],
+            dtype=[('f0', 'u1'), ('f1', 'u1'), ('f2', 'u1'), ('f3', 'S3')])
+
+    >>> grades_dtype = [('Name', (np.str_, 10)), ('Marks', np.float64),
+    ...                 ('GradeLevel', np.int32)]
+    >>> grades_array = np.array([('Sam', 33.3, 3), ('Mike', 44.4, 5),
+    ...                         ('Aadi', 66.6, 6)], dtype=grades_dtype)
+    >>> np.core.records.fromstring(grades_array.tobytes(), dtype=grades_dtype)
+    rec.array([('Sam', 33.3, 3), ('Mike', 44.4, 5), ('Aadi', 66.6, 6)],
+            dtype=[('Name', '<U10'), ('Marks', '<f8'), ('GradeLevel', '<i4')])
+
+    >>> s = '\x01\x02\x03abc'
+    >>> np.core.records.fromstring(s, dtype='u1,u1,u1,S3')
+    Traceback (most recent call last)
+       ...
+    TypeError: a bytes-like object is required, not 'str'
+    """
+
+    if dtype is None and formats is None:
+        raise TypeError("fromstring() needs a 'dtype' or 'formats' argument")
+
+    if dtype is not None:
+        descr = sb.dtype(dtype)
+    else:
+        descr = format_parser(formats, names, titles, aligned, byteorder).dtype
+
+    itemsize = descr.itemsize
+
+    # NumPy 1.19.0, 2020-01-01
+    shape = _deprecate_shape_0_as_None(shape)
+
+    if shape in (None, -1):
+        shape = (len(datastring) - offset) // itemsize
+
+    _array = recarray(shape, descr, buf=datastring, offset=offset)
     return _array
 
-def fromfile(fd, formats, shape=None, names=None, titles=None,
-             byteorder=None, aligned=0, offset=0):
+def get_remaining_size(fd):
+    pos = fd.tell()
+    try:
+        fd.seek(0, 2)
+        return fd.tell() - pos
+    finally:
+        fd.seek(pos, 0)
+
+
+@set_module("numpy.rec")
+def fromfile(fd, dtype=None, shape=None, offset=0, formats=None,
+             names=None, titles=None, aligned=False, byteorder=None):
     """Create an array from binary file data
 
-    If file is a string then that file is opened, else it is assumed
-    to be a file object.
-
-    >>> import testdata, sys
-    >>> fd=open(testdata.filename)
-    >>> fd.seek(2880*2)
-    >>> r=fromfile(fd, formats='f8,i4,a5', shape=3, byteorder='big')
-    >>> print r[0]
-    (5.1000000000000005, 61, 'abcde')
-    >>> r._shape
-    (3,)
+    Parameters
+    ----------
+    fd : str or file type
+        If file is a string or a path-like object then that file is opened,
+        else it is assumed to be a file object. The file object must
+        support random access (i.e. it must have tell and seek methods).
+    dtype : data-type, optional
+        valid dtype for all arrays
+    shape : int or tuple of ints, optional
+        shape of each array.
+    offset : int, optional
+        Position in the file to start reading from.
+    formats, names, titles, aligned, byteorder :
+        If `dtype` is ``None``, these arguments are passed to
+        `numpy.format_parser` to construct a dtype. See that function for
+        detailed documentation
+
+    Returns
+    -------
+    np.recarray
+        record array consisting of data enclosed in file.
+
+    Examples
+    --------
+    >>> from tempfile import TemporaryFile
+    >>> a = np.empty(10,dtype='f8,i4,a5')
+    >>> a[5] = (0.5,10,'abcde')
+    >>>
+    >>> fd=TemporaryFile()
+    >>> a = a.newbyteorder('<')
+    >>> a.tofile(fd)
+    >>>
+    >>> _ = fd.seek(0)
+    >>> r=np.core.records.fromfile(fd, formats='f8,i4,a5', shape=10,
+    ... byteorder='<')
+    >>> print(r[5])
+    (0.5, 10, 'abcde')
+    >>> r.shape
+    (10,)
     """
 
-    if (shape is None or shape == 0):
+    if dtype is None and formats is None:
+        raise TypeError("fromfile() needs a 'dtype' or 'formats' argument")
+
+    # NumPy 1.19.0, 2020-01-01
+    shape = _deprecate_shape_0_as_None(shape)
+
+    if shape is None:
         shape = (-1,)
-    elif isinstance(shape, (int, long)):
+    elif isinstance(shape, int):
         shape = (shape,)
 
-    name = 0
-    if isinstance(fd, str):
-        name = 1
-        fd = open(fd, 'rb')
-    if (offset > 0):
-        fd.seek(offset, 1)
-    try:
-        size = os.fstat(fd.fileno())[stat.ST_SIZE] - fd.tell()
-    except:
-        size = os.path.getsize(fd.name) - fd.tell()
-
-    parsed = format_parser(formats, names, titles, aligned)
-    itemsize = parsed._descr.itemsize
-
-    shapeprod = sb.array(shape).prod()
-    shapesize = shapeprod*itemsize
-    if shapesize < 0:
-        shape = list(shape)
-        shape[ shape.index(-1) ] = size / -shapesize
-        shape = tuple(shape)
-        shapeprod = sb.array(shape).prod()
-
-    nbytes = shapeprod*itemsize
-
-    if nbytes > size:
-        raise ValueError(
-                "Not enough bytes left in file for specified shape and type")
-
-    # create the array
-    _array = recarray(shape, parsed._descr, byteorder=byteorder)
-    nbytesread = fd.readinto(_array.data)
-    if nbytesread != nbytes:
-        raise IOError("Didn't read as many bytes as expected")
-    if name:
-        fd.close()
+    if hasattr(fd, 'readinto'):
+        # GH issue 2504. fd supports io.RawIOBase or io.BufferedIOBase interface.
+        # Example of fd: gzip, BytesIO, BufferedReader
+        # file already opened
+        ctx = nullcontext(fd)
+    else:
+        # open file
+        ctx = open(os_fspath(fd), 'rb')
+
+    with ctx as fd:
+        if offset > 0:
+            fd.seek(offset, 1)
+        size = get_remaining_size(fd)
+
+        if dtype is not None:
+            descr = sb.dtype(dtype)
+        else:
+            descr = format_parser(formats, names, titles, aligned, byteorder).dtype
+
+        itemsize = descr.itemsize
+
+        shapeprod = sb.array(shape).prod(dtype=nt.intp)
+        shapesize = shapeprod * itemsize
+        if shapesize < 0:
+            shape = list(shape)
+            shape[shape.index(-1)] = size // -shapesize
+            shape = tuple(shape)
+            shapeprod = sb.array(shape).prod(dtype=nt.intp)
+
+        nbytes = shapeprod * itemsize
+
+        if nbytes > size:
+            raise ValueError(
+                    "Not enough bytes left in file for specified shape and type")
+
+        # create the array
+        _array = recarray(shape, descr)
+        nbytesread = fd.readinto(_array.data)
+        if nbytesread != nbytes:
+            raise OSError("Didn't read as many bytes as expected")
 
     return _array
 
 
-def array(obj, formats=None, names=None, titles=None, shape=None,
-          byteorder=None, aligned=0, offset=0, strides=None):
-
-    if isinstance(obj, (type(None), str, file)) and (formats is None):
-        raise ValueError("Must define formats if object is "\
+@set_module("numpy.rec")
+def array(obj, dtype=None, shape=None, offset=0, strides=None, formats=None,
+          names=None, titles=None, aligned=False, byteorder=None, copy=True):
+    """
+    Construct a record array from a wide-variety of objects.
+
+    A general-purpose record array constructor that dispatches to the
+    appropriate `recarray` creation function based on the inputs (see Notes).
+
+    Parameters
+    ----------
+    obj : any
+        Input object. See Notes for details on how various input types are
+        treated.
+    dtype : data-type, optional
+        Valid dtype for array.
+    shape : int or tuple of ints, optional
+        Shape of each array.
+    offset : int, optional
+        Position in the file or buffer to start reading from.
+    strides : tuple of ints, optional
+        Buffer (`buf`) is interpreted according to these strides (strides
+        define how many bytes each array element, row, column, etc.
+        occupy in memory).
+    formats, names, titles, aligned, byteorder :
+        If `dtype` is ``None``, these arguments are passed to
+        `numpy.format_parser` to construct a dtype. See that function for
+        detailed documentation.
+    copy : bool, optional
+        Whether to copy the input object (True), or to use a reference instead.
+        This option only applies when the input is an ndarray or recarray.
+        Defaults to True.
+
+    Returns
+    -------
+    np.recarray
+        Record array created from the specified object.
+
+    Notes
+    -----
+    If `obj` is ``None``, then call the `~numpy.recarray` constructor. If
+    `obj` is a string, then call the `fromstring` constructor. If `obj` is a
+    list or a tuple, then if the first object is an `~numpy.ndarray`, call
+    `fromarrays`, otherwise call `fromrecords`. If `obj` is a
+    `~numpy.recarray`, then make a copy of the data in the recarray
+    (if ``copy=True``) and use the new formats, names, and titles. If `obj`
+    is a file, then call `fromfile`. Finally, if obj is an `ndarray`, then
+    return ``obj.view(recarray)``, making a copy of the data if ``copy=True``.
+
+    Examples
+    --------
+    >>> a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
+    array([[1, 2, 3],
+           [4, 5, 6],
+           [7, 8, 9]])
+
+    >>> np.core.records.array(a)
+    rec.array([[1, 2, 3],
+               [4, 5, 6],
+               [7, 8, 9]],
+        dtype=int32)
+
+    >>> b = [(1, 1), (2, 4), (3, 9)]
+    >>> c = np.core.records.array(b, formats = ['i2', 'f2'], names = ('x', 'y'))
+    >>> c
+    rec.array([(1, 1.0), (2, 4.0), (3, 9.0)],
+              dtype=[('x', '<i2'), ('y', '<f2')])
+
+    >>> c.x
+    rec.array([1, 2, 3], dtype=int16)
+
+    >>> c.y
+    rec.array([ 1.0,  4.0,  9.0], dtype=float16)
+
+    >>> r = np.rec.array(['abc','def'], names=['col1','col2'])
+    >>> print(r.col1)
+    abc
+
+    >>> r.col1
+    array('abc', dtype='<U3')
+
+    >>> r.col2
+    array('def', dtype='<U3')
+    """
+
+    if ((isinstance(obj, (type(None), str)) or hasattr(obj, 'readinto')) and
+           formats is None and dtype is None):
+        raise ValueError("Must define formats (or dtype) if object is "
                          "None, string, or an open file")
 
-    elif obj is None:
+    kwds = {}
+    if dtype is not None:
+        dtype = sb.dtype(dtype)
+    elif formats is not None:
+        dtype = format_parser(formats, names, titles,
+                              aligned, byteorder).dtype
+    else:
+        kwds = {'formats': formats,
+                'names': names,
+                'titles': titles,
+                'aligned': aligned,
+                'byteorder': byteorder
+                }
+
+    if obj is None:
         if shape is None:
             raise ValueError("Must define a shape if obj is None")
-        return recarray(shape, formats, names=names, titles=titles,
-                        buf=obj, offset=offset, strides=strides,
-                        byteorder=byteorder, aligned=aligned)
-    elif isinstance(obj, str):
-        return fromstring(obj, formats, names=names, titles=titles,
-                          shape=shape, byteorder=byteorder, aligned=aligned,
-                          offset=offset)
+        return recarray(shape, dtype, buf=obj, offset=offset, strides=strides)
+
+    elif isinstance(obj, bytes):
+        return fromstring(obj, dtype, shape=shape, offset=offset, **kwds)
+
     elif isinstance(obj, (list, tuple)):
-        if isinstance(obj[0], sb.ndarray):
-            return fromarrays(obj, formats=formats, names=names, titles=titles,
-                              shape=shape, aligned=aligned)
-        else:
-            return fromrecords(obj, formats=formats, names=names, titles=titles,
-                               shape=shape, aligned=aligned)
+        if isinstance(obj[0], (tuple, list)):
+            return fromrecords(obj, dtype=dtype, shape=shape, **kwds)
+        else:
+            return fromarrays(obj, dtype=dtype, shape=shape, **kwds)
+
     elif isinstance(obj, recarray):
-        new = obj.copy()
-        parsed = format_parser(formats, names, titles, aligned)
-        new.dtype = parsed._descr
+        if dtype is not None and (obj.dtype != dtype):
+            new = obj.view(dtype)
+        else:
+            new = obj
+        if copy:
+            new = new.copy()
         return new
-    elif isinstance(obj, file):
-        return fromfile(obj, formats=formats, names=names, titles=titles,
-                        shape=shape, byteorder=byteorder, aligned=aligned,
-                        offset=offset)
-    elif isinstance(obj, sb.ndarray):
-        res = obj.view(recarray)
-        if issubclass(res.dtype.type, nt.void):
-            res.dtype = sb.dtype((record, res.dtype))
-        return res
+
+    elif hasattr(obj, 'readinto'):
+        return fromfile(obj, dtype=dtype, shape=shape, offset=offset)
+
+    elif isinstance(obj, ndarray):
+        if dtype is not None and (obj.dtype != dtype):
+            new = obj.view(dtype)
+        else:
+            new = obj
+        if copy:
+            new = new.copy()
+        return new.view(recarray)
+
     else:
-        raise ValueError("Unknown input type")
+        interface = getattr(obj, "__array_interface__", None)
+        if interface is None or not isinstance(interface, dict):
+            raise ValueError("Unknown input type")
+        obj = sb.array(obj)
+        if dtype is not None and (obj.dtype != dtype):
+            obj = obj.view(dtype)
+        return obj.view(recarray)
('numpy/core', '__init__.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,32 +1,176 @@
+"""
+Contains the core of NumPy: ndarray, ufuncs, dtypes, etc.
 
-from info import __doc__
+Please note that this module is private.  All functions and objects
+are available in the main ``numpy`` namespace - use that instead.
+
+"""
+
 from numpy.version import version as __version__
 
-import multiarray
-import umath
-import numerictypes as nt
-multiarray.set_typeDict(nt.typeDict)
-import _sort
-from numeric import *
-from oldnumeric import *
-from defmatrix import *
-import ma
-import defchararray as char
-import records as rec
-from records import *
-from memmap import *
-from defchararray import *
-import scalarmath
-del scalarmath
+import os
+import warnings
+
+# disables OpenBLAS affinity setting of the main thread that limits
+# python threads or processes to one core
+env_added = []
+for envkey in ['OPENBLAS_MAIN_FREE', 'GOTOBLAS_MAIN_FREE']:
+    if envkey not in os.environ:
+        os.environ[envkey] = '1'
+        env_added.append(envkey)
+
+try:
+    from . import multiarray
+except ImportError as exc:
+    import sys
+    msg = """
+
+IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!
+
+Importing the numpy C-extensions failed. This error can happen for
+many reasons, often due to issues with your setup or how NumPy was
+installed.
+
+We have compiled some common reasons and troubleshooting tips at:
+
+    https://numpy.org/devdocs/user/troubleshooting-importerror.html
+
+Please note and check the following:
+
+  * The Python version is: Python%d.%d from "%s"
+  * The NumPy version is: "%s"
+
+and make sure that they are the versions you expect.
+Please carefully study the documentation linked above for further help.
+
+Original error was: %s
+""" % (sys.version_info[0], sys.version_info[1], sys.executable,
+        __version__, exc)
+    raise ImportError(msg)
+finally:
+    for envkey in env_added:
+        del os.environ[envkey]
+del envkey
+del env_added
+del os
+
+from . import umath
+
+# Check that multiarray,umath are pure python modules wrapping
+# _multiarray_umath and not either of the old c-extension modules
+if not (hasattr(multiarray, '_multiarray_umath') and
+        hasattr(umath, '_multiarray_umath')):
+    import sys
+    path = sys.modules['numpy'].__path__
+    msg = ("Something is wrong with the numpy installation. "
+        "While importing we detected an older version of "
+        "numpy in {}. One method of fixing this is to repeatedly uninstall "
+        "numpy until none is found, then reinstall this version.")
+    raise ImportError(msg.format(path))
+
+from . import numerictypes as nt
+multiarray.set_typeDict(nt.sctypeDict)
+from . import numeric
+from .numeric import *
+from . import fromnumeric
+from .fromnumeric import *
+from . import defchararray as char
+from . import records as rec
+from .records import record, recarray, format_parser
+from .memmap import *
+from .defchararray import chararray
+from . import function_base
+from .function_base import *
+from . import _machar
+from ._machar import *
+from . import getlimits
+from .getlimits import *
+from . import shape_base
+from .shape_base import *
+from . import einsumfunc
+from .einsumfunc import *
 del nt
 
-__all__ = ['char','rec','memmap','ma']
+from .fromnumeric import amax as max, amin as min, round_ as round
+from .numeric import absolute as abs
+
+# do this after everything else, to minimize the chance of this misleadingly
+# appearing in an import-time traceback
+from . import _add_newdocs
+from . import _add_newdocs_scalars
+# add these for module-freeze analysis (like PyInstaller)
+from . import _dtype_ctypes
+from . import _internal
+from . import _dtype
+from . import _methods
+
+__all__ = ['char', 'rec', 'memmap']
 __all__ += numeric.__all__
-__all__ += oldnumeric.__all__
-__all__ += defmatrix.__all__
-__all__ += rec.__all__
-__all__ += char.__all__
+__all__ += ['record', 'recarray', 'format_parser']
+__all__ += ['chararray']
+__all__ += function_base.__all__
+__all__ += getlimits.__all__
+__all__ += shape_base.__all__
+__all__ += einsumfunc.__all__
 
-def test(level=1, verbosity=1):
-    from numpy.testing import NumpyTest
-    return NumpyTest().test(level, verbosity)
+# We used to use `np.core._ufunc_reconstruct` to unpickle. This is unnecessary,
+# but old pickles saved before 1.20 will be using it, and there is no reason
+# to break loading them.
+def _ufunc_reconstruct(module, name):
+    # The `fromlist` kwarg is required to ensure that `mod` points to the
+    # inner-most module rather than the parent package when module name is
+    # nested. This makes it possible to pickle non-toplevel ufuncs such as
+    # scipy.special.expit for instance.
+    mod = __import__(module, fromlist=[name])
+    return getattr(mod, name)
+
+
+def _ufunc_reduce(func):
+    # Report the `__name__`. pickle will try to find the module. Note that
+    # pickle supports for this `__name__` to be a `__qualname__`. It may
+    # make sense to add a `__qualname__` to ufuncs, to allow this more
+    # explicitly (Numba has ufuncs as attributes).
+    # See also: https://github.com/dask/distributed/issues/3450
+    return func.__name__
+
+
+def _DType_reconstruct(scalar_type):
+    # This is a work-around to pickle type(np.dtype(np.float64)), etc.
+    # and it should eventually be replaced with a better solution, e.g. when
+    # DTypes become HeapTypes.
+    return type(dtype(scalar_type))
+
+
+def _DType_reduce(DType):
+    # To pickle a DType without having to add top-level names, pickle the
+    # scalar type for now (and assume that reconstruction will be possible).
+    if DType is dtype:
+        return "dtype"  # must pickle `np.dtype` as a singleton.
+    scalar_type = DType.type  # pickle the scalar type for reconstruction
+    return _DType_reconstruct, (scalar_type,)
+
+
+def __getattr__(name):
+    # Deprecated 2021-10-20, NumPy 1.22
+    if name == "machar":
+        warnings.warn(
+            "The `np.core.machar` module is deprecated (NumPy 1.22)",
+            DeprecationWarning, stacklevel=2,
+        )
+        return _machar
+    raise AttributeError(f"Module {__name__!r} has no attribute {name!r}")
+
+
+import copyreg
+
+copyreg.pickle(ufunc, _ufunc_reduce)
+copyreg.pickle(type(dtype), _DType_reduce, _DType_reconstruct)
+
+# Unclutter namespace (must keep _*_reconstruct for unpickling)
+del copyreg
+del _ufunc_reduce
+del _DType_reduce
+
+from numpy._pytesttester import PytestTester
+test = PytestTester(__name__)
+del PytestTester
('numpy/core', 'memmap.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,12 +1,15 @@
+from contextlib import nullcontext
+
+import numpy as np
+from .numeric import uint8, ndarray, dtype
+from numpy.compat import os_fspath, is_pathlib_path
+from numpy.core.overrides import set_module
+
 __all__ = ['memmap']
-
-import mmap
-from numeric import uint8, ndarray, dtype
-from numerictypes import nbytes
 
 dtypedescr = dtype
 valid_filemodes = ["r", "c", "r+", "w+"]
-writeable_filemodes = ["r+","w+"]
+writeable_filemodes = ["r+", "w+"]
 
 mode_equivalents = {
     "readonly":"r",
@@ -15,77 +18,320 @@
     "write":"w+"
     }
 
+
+@set_module('numpy')
 class memmap(ndarray):
+    """Create a memory-map to an array stored in a *binary* file on disk.
+
+    Memory-mapped files are used for accessing small segments of large files
+    on disk, without reading the entire file into memory.  NumPy's
+    memmap's are array-like objects.  This differs from Python's ``mmap``
+    module, which uses file-like objects.
+
+    This subclass of ndarray has some unpleasant interactions with
+    some operations, because it doesn't quite fit properly as a subclass.
+    An alternative to using this subclass is to create the ``mmap``
+    object yourself, then create an ndarray with ndarray.__new__ directly,
+    passing the object created in its 'buffer=' parameter.
+
+    This class may at some point be turned into a factory function
+    which returns a view into an mmap buffer.
+
+    Flush the memmap instance to write the changes to the file. Currently there
+    is no API to close the underlying ``mmap``. It is tricky to ensure the
+    resource is actually closed, since it may be shared between different
+    memmap instances.
+
+
+    Parameters
+    ----------
+    filename : str, file-like object, or pathlib.Path instance
+        The file name or file object to be used as the array data buffer.
+    dtype : data-type, optional
+        The data-type used to interpret the file contents.
+        Default is `uint8`.
+    mode : {'r+', 'r', 'w+', 'c'}, optional
+        The file is opened in this mode:
+
+        +------+-------------------------------------------------------------+
+        | 'r'  | Open existing file for reading only.                        |
+        +------+-------------------------------------------------------------+
+        | 'r+' | Open existing file for reading and writing.                 |
+        +------+-------------------------------------------------------------+
+        | 'w+' | Create or overwrite existing file for reading and writing.  |
+        +------+-------------------------------------------------------------+
+        | 'c'  | Copy-on-write: assignments affect data in memory, but       |
+        |      | changes are not saved to disk.  The file on disk is         |
+        |      | read-only.                                                  |
+        +------+-------------------------------------------------------------+
+
+        Default is 'r+'.
+    offset : int, optional
+        In the file, array data starts at this offset. Since `offset` is
+        measured in bytes, it should normally be a multiple of the byte-size
+        of `dtype`. When ``mode != 'r'``, even positive offsets beyond end of
+        file are valid; The file will be extended to accommodate the
+        additional data. By default, ``memmap`` will start at the beginning of
+        the file, even if ``filename`` is a file pointer ``fp`` and
+        ``fp.tell() != 0``.
+    shape : tuple, optional
+        The desired shape of the array. If ``mode == 'r'`` and the number
+        of remaining bytes after `offset` is not a multiple of the byte-size
+        of `dtype`, you must specify `shape`. By default, the returned array
+        will be 1-D with the number of elements determined by file size
+        and data-type.
+    order : {'C', 'F'}, optional
+        Specify the order of the ndarray memory layout:
+        :term:`row-major`, C-style or :term:`column-major`,
+        Fortran-style.  This only has an effect if the shape is
+        greater than 1-D.  The default order is 'C'.
+
+    Attributes
+    ----------
+    filename : str or pathlib.Path instance
+        Path to the mapped file.
+    offset : int
+        Offset position in the file.
+    mode : str
+        File mode.
+
+    Methods
+    -------
+    flush
+        Flush any changes in memory to file on disk.
+        When you delete a memmap object, flush is called first to write
+        changes to disk.
+
+
+    See also
+    --------
+    lib.format.open_memmap : Create or load a memory-mapped ``.npy`` file.
+
+    Notes
+    -----
+    The memmap object can be used anywhere an ndarray is accepted.
+    Given a memmap ``fp``, ``isinstance(fp, numpy.ndarray)`` returns
+    ``True``.
+
+    Memory-mapped files cannot be larger than 2GB on 32-bit systems.
+
+    When a memmap causes a file to be created or extended beyond its
+    current size in the filesystem, the contents of the new part are
+    unspecified. On systems with POSIX filesystem semantics, the extended
+    part will be filled with zero bytes.
+
+    Examples
+    --------
+    >>> data = np.arange(12, dtype='float32')
+    >>> data.resize((3,4))
+
+    This example uses a temporary file so that doctest doesn't write
+    files to your directory. You would use a 'normal' filename.
+
+    >>> from tempfile import mkdtemp
+    >>> import os.path as path
+    >>> filename = path.join(mkdtemp(), 'newfile.dat')
+
+    Create a memmap with dtype and shape that matches our data:
+
+    >>> fp = np.memmap(filename, dtype='float32', mode='w+', shape=(3,4))
+    >>> fp
+    memmap([[0., 0., 0., 0.],
+            [0., 0., 0., 0.],
+            [0., 0., 0., 0.]], dtype=float32)
+
+    Write data to memmap array:
+
+    >>> fp[:] = data[:]
+    >>> fp
+    memmap([[  0.,   1.,   2.,   3.],
+            [  4.,   5.,   6.,   7.],
+            [  8.,   9.,  10.,  11.]], dtype=float32)
+
+    >>> fp.filename == path.abspath(filename)
+    True
+
+    Flushes memory changes to disk in order to read them back
+
+    >>> fp.flush()
+
+    Load the memmap and verify data was stored:
+
+    >>> newfp = np.memmap(filename, dtype='float32', mode='r', shape=(3,4))
+    >>> newfp
+    memmap([[  0.,   1.,   2.,   3.],
+            [  4.,   5.,   6.,   7.],
+            [  8.,   9.,  10.,  11.]], dtype=float32)
+
+    Read-only memmap:
+
+    >>> fpr = np.memmap(filename, dtype='float32', mode='r', shape=(3,4))
+    >>> fpr.flags.writeable
+    False
+
+    Copy-on-write memmap:
+
+    >>> fpc = np.memmap(filename, dtype='float32', mode='c', shape=(3,4))
+    >>> fpc.flags.writeable
+    True
+
+    It's possible to assign to copy-on-write array, but values are only
+    written into the memory copy of the array, and not written to disk:
+
+    >>> fpc
+    memmap([[  0.,   1.,   2.,   3.],
+            [  4.,   5.,   6.,   7.],
+            [  8.,   9.,  10.,  11.]], dtype=float32)
+    >>> fpc[0,:] = 0
+    >>> fpc
+    memmap([[  0.,   0.,   0.,   0.],
+            [  4.,   5.,   6.,   7.],
+            [  8.,   9.,  10.,  11.]], dtype=float32)
+
+    File on disk is unchanged:
+
+    >>> fpr
+    memmap([[  0.,   1.,   2.,   3.],
+            [  4.,   5.,   6.,   7.],
+            [  8.,   9.,  10.,  11.]], dtype=float32)
+
+    Offset into a memmap:
+
+    >>> fpo = np.memmap(filename, dtype='float32', mode='r', offset=16)
+    >>> fpo
+    memmap([  4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.], dtype=float32)
+
+    """
+
     __array_priority__ = -100.0
-    def __new__(subtype, name, dtype=uint8, mode='r+', offset=0,
-                shape=None, order=0):
+
+    def __new__(subtype, filename, dtype=uint8, mode='r+', offset=0,
+                shape=None, order='C'):
+        # Import here to minimize 'import numpy' overhead
+        import mmap
+        import os.path
         try:
             mode = mode_equivalents[mode]
-        except KeyError:
+        except KeyError as e:
             if mode not in valid_filemodes:
-                raise ValueError("mode must be one of %s" % \
-                                 (valid_filemodes + mode_equivalents.keys()))
-
-        fid = file(name, (mode == 'c' and 'r' or mode)+'b')
-
-        if (mode == 'w+') and shape is None:
-            raise ValueError, "shape must be given"
-
-        fid.seek(0,2)
-        flen = fid.tell()
-        descr = dtypedescr(dtype)
-        _dbytes = descr.itemsize
-
-        if shape is None:
-            bytes = flen-offset
-            if (bytes % _dbytes):
-                fid.close()
-                raise ValueError, "Size of available data is not a "\
-                      "multiple of data-type size."
-            size = bytes // _dbytes
-            shape = (size,)
+                raise ValueError(
+                    "mode must be one of {!r} (got {!r})"
+                    .format(valid_filemodes + list(mode_equivalents.keys()), mode)
+                ) from None
+
+        if mode == 'w+' and shape is None:
+            raise ValueError("shape must be given")
+
+        if hasattr(filename, 'read'):
+            f_ctx = nullcontext(filename)
         else:
-            if not isinstance(shape, tuple):
-                shape = (shape,)
-            size = 1
-            for k in shape:
-                size *= k
-
-        bytes = offset + size*_dbytes
-
-        if mode == 'w+' or (mode == 'r+' and flen < bytes):
-            fid.seek(bytes-1,0)
-            fid.write(chr(0))
-            fid.flush()
-
-        if mode == 'c':
-            acc = mmap.ACCESS_COPY
-        elif mode == 'r':
-            acc = mmap.ACCESS_READ
+            f_ctx = open(os_fspath(filename), ('r' if mode == 'c' else mode)+'b')
+
+        with f_ctx as fid:
+            fid.seek(0, 2)
+            flen = fid.tell()
+            descr = dtypedescr(dtype)
+            _dbytes = descr.itemsize
+
+            if shape is None:
+                bytes = flen - offset
+                if bytes % _dbytes:
+                    raise ValueError("Size of available data is not a "
+                            "multiple of the data-type size.")
+                size = bytes // _dbytes
+                shape = (size,)
+            else:
+                if not isinstance(shape, tuple):
+                    shape = (shape,)
+                size = np.intp(1)  # avoid default choice of np.int_, which might overflow
+                for k in shape:
+                    size *= k
+
+            bytes = int(offset + size*_dbytes)
+
+            if mode in ('w+', 'r+') and flen < bytes:
+                fid.seek(bytes - 1, 0)
+                fid.write(b'\0')
+                fid.flush()
+
+            if mode == 'c':
+                acc = mmap.ACCESS_COPY
+            elif mode == 'r':
+                acc = mmap.ACCESS_READ
+            else:
+                acc = mmap.ACCESS_WRITE
+
+            start = offset - offset % mmap.ALLOCATIONGRANULARITY
+            bytes -= start
+            array_offset = offset - start
+            mm = mmap.mmap(fid.fileno(), bytes, access=acc, offset=start)
+
+            self = ndarray.__new__(subtype, shape, dtype=descr, buffer=mm,
+                                   offset=array_offset, order=order)
+            self._mmap = mm
+            self.offset = offset
+            self.mode = mode
+
+            if is_pathlib_path(filename):
+                # special case - if we were constructed with a pathlib.path,
+                # then filename is a path object, not a string
+                self.filename = filename.resolve()
+            elif hasattr(fid, "name") and isinstance(fid.name, str):
+                # py3 returns int for TemporaryFile().name
+                self.filename = os.path.abspath(fid.name)
+            # same as memmap copies (e.g. memmap + 1)
+            else:
+                self.filename = None
+
+        return self
+
+    def __array_finalize__(self, obj):
+        if hasattr(obj, '_mmap') and np.may_share_memory(self, obj):
+            self._mmap = obj._mmap
+            self.filename = obj.filename
+            self.offset = obj.offset
+            self.mode = obj.mode
         else:
-            acc = mmap.ACCESS_WRITE
-
-        mm = mmap.mmap(fid.fileno(), bytes, access=acc)
-
-        self = ndarray.__new__(subtype, shape, dtype=descr, buffer=mm,
-                               offset=offset, order=order)
-        self._mmap = mm
-        self._offset = offset
-        self._mode = mode
-        self._size = size
-        self._name = name
-        fid.close()
-        return self
-
-    def __array_finalize__(self, obj):
-        if obj is not None and not isinstance(obj, memmap):
-            raise ValueError, "Cannot create a memmap array that way"
-        self._mmap = None
-
-    def sync(self):
-        self._mmap.flush()
-
-    def __del__(self):
-        if self._mmap is not None:
-            self._mmap.flush()
-            del self._mmap
+            self._mmap = None
+            self.filename = None
+            self.offset = None
+            self.mode = None
+
+    def flush(self):
+        """
+        Write any changes in the array to the file on disk.
+
+        For further information, see `memmap`.
+
+        Parameters
+        ----------
+        None
+
+        See Also
+        --------
+        memmap
+
+        """
+        if self.base is not None and hasattr(self.base, 'flush'):
+            self.base.flush()
+
+    def __array_wrap__(self, arr, context=None):
+        arr = super().__array_wrap__(arr, context)
+
+        # Return a memmap if a memmap was given as the output of the
+        # ufunc. Leave the arr class unchanged if self is not a memmap
+        # to keep original memmap subclasses behavior
+        if self is arr or type(self) is not memmap:
+            return arr
+        # Return scalar instead of 0d memmap, e.g. for np.sum with
+        # axis=None
+        if arr.shape == ():
+            return arr[()]
+        # Return ndarray otherwise
+        return arr.view(np.ndarray)
+
+    def __getitem__(self, index):
+        res = super().__getitem__(index)
+        if type(res) is memmap and res._mmap is None:
+            return res.view(type=ndarray)
+        return res
('numpy/core', 'defchararray.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,27 +1,1967 @@
-from numerictypes import string, unicode_, integer, object_
-from numeric import ndarray, broadcast, empty
-from numeric import array as narray
-import sys
-
-__all__ = ['chararray']
+"""
+This module contains a set of functions for vectorized string
+operations and methods.
+
+.. note::
+   The `chararray` class exists for backwards compatibility with
+   Numarray, it is not recommended for new development. Starting from numpy
+   1.4, if one needs arrays of strings, it is recommended to use arrays of
+   `dtype` `object_`, `string_` or `unicode_`, and use the free functions
+   in the `numpy.char` module for fast vectorized string operations.
+
+Some methods will only be available if the corresponding string method is
+available in your version of Python.
+
+The preferred alias for `defchararray` is `numpy.char`.
+
+"""
+import functools
+from .numerictypes import (
+    string_, unicode_, integer, int_, object_, bool_, character)
+from .numeric import ndarray, compare_chararrays
+from .numeric import array as narray
+from numpy.core.multiarray import _vec_string
+from numpy.core.overrides import set_module
+from numpy.core import overrides
+from numpy.compat import asbytes
+import numpy
+
+__all__ = [
+    'equal', 'not_equal', 'greater_equal', 'less_equal',
+    'greater', 'less', 'str_len', 'add', 'multiply', 'mod', 'capitalize',
+    'center', 'count', 'decode', 'encode', 'endswith', 'expandtabs',
+    'find', 'index', 'isalnum', 'isalpha', 'isdigit', 'islower', 'isspace',
+    'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'partition',
+    'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit',
+    'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase',
+    'title', 'translate', 'upper', 'zfill', 'isnumeric', 'isdecimal',
+    'array', 'asarray'
+    ]
+
 
 _globalvar = 0
-_unicode = unicode
-
-# special sub-class for character arrays (string and unicode_)
-# This adds equality testing and methods of str and unicode types
-#  which operate on an element-by-element basis
-
-
+
+array_function_dispatch = functools.partial(
+    overrides.array_function_dispatch, module='numpy.char')
+
+
+def _use_unicode(*args):
+    """
+    Helper function for determining the output type of some string
+    operations.
+
+    For an operation on two ndarrays, if at least one is unicode, the
+    result should be unicode.
+    """
+    for x in args:
+        if (isinstance(x, str) or
+                issubclass(numpy.asarray(x).dtype.type, unicode_)):
+            return unicode_
+    return string_
+
+def _to_string_or_unicode_array(result):
+    """
+    Helper function to cast a result back into a string or unicode array
+    if an object array must be used as an intermediary.
+    """
+    return numpy.asarray(result.tolist())
+
+def _clean_args(*args):
+    """
+    Helper function for delegating arguments to Python string
+    functions.
+
+    Many of the Python string operations that have optional arguments
+    do not use 'None' to indicate a default value.  In these cases,
+    we need to remove all None arguments, and those following them.
+    """
+    newargs = []
+    for chk in args:
+        if chk is None:
+            break
+        newargs.append(chk)
+    return newargs
+
+def _get_num_chars(a):
+    """
+    Helper function that returns the number of characters per field in
+    a string or unicode array.  This is to abstract out the fact that
+    for a unicode array this is itemsize / 4.
+    """
+    if issubclass(a.dtype.type, unicode_):
+        return a.itemsize // 4
+    return a.itemsize
+
+
+def _binary_op_dispatcher(x1, x2):
+    return (x1, x2)
+
+
+@array_function_dispatch(_binary_op_dispatcher)
+def equal(x1, x2):
+    """
+    Return (x1 == x2) element-wise.
+
+    Unlike `numpy.equal`, this comparison is performed by first
+    stripping whitespace characters from the end of the string.  This
+    behavior is provided for backward-compatibility with numarray.
+
+    Parameters
+    ----------
+    x1, x2 : array_like of str or unicode
+        Input arrays of the same shape.
+
+    Returns
+    -------
+    out : ndarray
+        Output array of bools.
+
+    See Also
+    --------
+    not_equal, greater_equal, less_equal, greater, less
+    """
+    return compare_chararrays(x1, x2, '==', True)
+
+
+@array_function_dispatch(_binary_op_dispatcher)
+def not_equal(x1, x2):
+    """
+    Return (x1 != x2) element-wise.
+
+    Unlike `numpy.not_equal`, this comparison is performed by first
+    stripping whitespace characters from the end of the string.  This
+    behavior is provided for backward-compatibility with numarray.
+
+    Parameters
+    ----------
+    x1, x2 : array_like of str or unicode
+        Input arrays of the same shape.
+
+    Returns
+    -------
+    out : ndarray
+        Output array of bools.
+
+    See Also
+    --------
+    equal, greater_equal, less_equal, greater, less
+    """
+    return compare_chararrays(x1, x2, '!=', True)
+
+
+@array_function_dispatch(_binary_op_dispatcher)
+def greater_equal(x1, x2):
+    """
+    Return (x1 >= x2) element-wise.
+
+    Unlike `numpy.greater_equal`, this comparison is performed by
+    first stripping whitespace characters from the end of the string.
+    This behavior is provided for backward-compatibility with
+    numarray.
+
+    Parameters
+    ----------
+    x1, x2 : array_like of str or unicode
+        Input arrays of the same shape.
+
+    Returns
+    -------
+    out : ndarray
+        Output array of bools.
+
+    See Also
+    --------
+    equal, not_equal, less_equal, greater, less
+    """
+    return compare_chararrays(x1, x2, '>=', True)
+
+
+@array_function_dispatch(_binary_op_dispatcher)
+def less_equal(x1, x2):
+    """
+    Return (x1 <= x2) element-wise.
+
+    Unlike `numpy.less_equal`, this comparison is performed by first
+    stripping whitespace characters from the end of the string.  This
+    behavior is provided for backward-compatibility with numarray.
+
+    Parameters
+    ----------
+    x1, x2 : array_like of str or unicode
+        Input arrays of the same shape.
+
+    Returns
+    -------
+    out : ndarray
+        Output array of bools.
+
+    See Also
+    --------
+    equal, not_equal, greater_equal, greater, less
+    """
+    return compare_chararrays(x1, x2, '<=', True)
+
+
+@array_function_dispatch(_binary_op_dispatcher)
+def greater(x1, x2):
+    """
+    Return (x1 > x2) element-wise.
+
+    Unlike `numpy.greater`, this comparison is performed by first
+    stripping whitespace characters from the end of the string.  This
+    behavior is provided for backward-compatibility with numarray.
+
+    Parameters
+    ----------
+    x1, x2 : array_like of str or unicode
+        Input arrays of the same shape.
+
+    Returns
+    -------
+    out : ndarray
+        Output array of bools.
+
+    See Also
+    --------
+    equal, not_equal, greater_equal, less_equal, less
+    """
+    return compare_chararrays(x1, x2, '>', True)
+
+
+@array_function_dispatch(_binary_op_dispatcher)
+def less(x1, x2):
+    """
+    Return (x1 < x2) element-wise.
+
+    Unlike `numpy.greater`, this comparison is performed by first
+    stripping whitespace characters from the end of the string.  This
+    behavior is provided for backward-compatibility with numarray.
+
+    Parameters
+    ----------
+    x1, x2 : array_like of str or unicode
+        Input arrays of the same shape.
+
+    Returns
+    -------
+    out : ndarray
+        Output array of bools.
+
+    See Also
+    --------
+    equal, not_equal, greater_equal, less_equal, greater
+    """
+    return compare_chararrays(x1, x2, '<', True)
+
+
+def _unary_op_dispatcher(a):
+    return (a,)
+
+
+@array_function_dispatch(_unary_op_dispatcher)
+def str_len(a):
+    """
+    Return len(a) element-wise.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    Returns
+    -------
+    out : ndarray
+        Output array of integers
+
+    See Also
+    --------
+    builtins.len
+    """
+    # Note: __len__, etc. currently return ints, which are not C-integers.
+    # Generally intp would be expected for lengths, although int is sufficient
+    # due to the dtype itemsize limitation.
+    return _vec_string(a, int_, '__len__')
+
+
+@array_function_dispatch(_binary_op_dispatcher)
+def add(x1, x2):
+    """
+    Return element-wise string concatenation for two arrays of str or unicode.
+
+    Arrays `x1` and `x2` must have the same shape.
+
+    Parameters
+    ----------
+    x1 : array_like of str or unicode
+        Input array.
+    x2 : array_like of str or unicode
+        Input array.
+
+    Returns
+    -------
+    add : ndarray
+        Output array of `string_` or `unicode_`, depending on input types
+        of the same shape as `x1` and `x2`.
+
+    """
+    arr1 = numpy.asarray(x1)
+    arr2 = numpy.asarray(x2)
+    out_size = _get_num_chars(arr1) + _get_num_chars(arr2)
+    dtype = _use_unicode(arr1, arr2)
+    return _vec_string(arr1, (dtype, out_size), '__add__', (arr2,))
+
+
+def _multiply_dispatcher(a, i):
+    return (a,)
+
+
+@array_function_dispatch(_multiply_dispatcher)
+def multiply(a, i):
+    """
+    Return (a * i), that is string multiple concatenation,
+    element-wise.
+
+    Values in `i` of less than 0 are treated as 0 (which yields an
+    empty string).
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    i : array_like of ints
+
+    Returns
+    -------
+    out : ndarray
+        Output array of str or unicode, depending on input types
+
+    """
+    a_arr = numpy.asarray(a)
+    i_arr = numpy.asarray(i)
+    if not issubclass(i_arr.dtype.type, integer):
+        raise ValueError("Can only multiply by integers")
+    out_size = _get_num_chars(a_arr) * max(int(i_arr.max()), 0)
+    return _vec_string(
+        a_arr, (a_arr.dtype.type, out_size), '__mul__', (i_arr,))
+
+
+def _mod_dispatcher(a, values):
+    return (a, values)
+
+
+@array_function_dispatch(_mod_dispatcher)
+def mod(a, values):
+    """
+    Return (a % i), that is pre-Python 2.6 string formatting
+    (interpolation), element-wise for a pair of array_likes of str
+    or unicode.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    values : array_like of values
+       These values will be element-wise interpolated into the string.
+
+    Returns
+    -------
+    out : ndarray
+        Output array of str or unicode, depending on input types
+
+    See Also
+    --------
+    str.__mod__
+
+    """
+    return _to_string_or_unicode_array(
+        _vec_string(a, object_, '__mod__', (values,)))
+
+
+@array_function_dispatch(_unary_op_dispatcher)
+def capitalize(a):
+    """
+    Return a copy of `a` with only the first character of each element
+    capitalized.
+
+    Calls `str.capitalize` element-wise.
+
+    For 8-bit strings, this method is locale-dependent.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+        Input array of strings to capitalize.
+
+    Returns
+    -------
+    out : ndarray
+        Output array of str or unicode, depending on input
+        types
+
+    See Also
+    --------
+    str.capitalize
+
+    Examples
+    --------
+    >>> c = np.array(['a1b2','1b2a','b2a1','2a1b'],'S4'); c
+    array(['a1b2', '1b2a', 'b2a1', '2a1b'],
+        dtype='|S4')
+    >>> np.char.capitalize(c)
+    array(['A1b2', '1b2a', 'B2a1', '2a1b'],
+        dtype='|S4')
+
+    """
+    a_arr = numpy.asarray(a)
+    return _vec_string(a_arr, a_arr.dtype, 'capitalize')
+
+
+def _center_dispatcher(a, width, fillchar=None):
+    return (a,)
+
+
+@array_function_dispatch(_center_dispatcher)
+def center(a, width, fillchar=' '):
+    """
+    Return a copy of `a` with its elements centered in a string of
+    length `width`.
+
+    Calls `str.center` element-wise.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    width : int
+        The length of the resulting strings
+    fillchar : str or unicode, optional
+        The padding character to use (default is space).
+
+    Returns
+    -------
+    out : ndarray
+        Output array of str or unicode, depending on input
+        types
+
+    See Also
+    --------
+    str.center
+
+    """
+    a_arr = numpy.asarray(a)
+    width_arr = numpy.asarray(width)
+    size = int(numpy.max(width_arr.flat))
+    if numpy.issubdtype(a_arr.dtype, numpy.string_):
+        fillchar = asbytes(fillchar)
+    return _vec_string(
+        a_arr, (a_arr.dtype.type, size), 'center', (width_arr, fillchar))
+
+
+def _count_dispatcher(a, sub, start=None, end=None):
+    return (a,)
+
+
+@array_function_dispatch(_count_dispatcher)
+def count(a, sub, start=0, end=None):
+    """
+    Returns an array with the number of non-overlapping occurrences of
+    substring `sub` in the range [`start`, `end`].
+
+    Calls `str.count` element-wise.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    sub : str or unicode
+       The substring to search for.
+
+    start, end : int, optional
+       Optional arguments `start` and `end` are interpreted as slice
+       notation to specify the range in which to count.
+
+    Returns
+    -------
+    out : ndarray
+        Output array of ints.
+
+    See Also
+    --------
+    str.count
+
+    Examples
+    --------
+    >>> c = np.array(['aAaAaA', '  aA  ', 'abBABba'])
+    >>> c
+    array(['aAaAaA', '  aA  ', 'abBABba'], dtype='<U7')
+    >>> np.char.count(c, 'A')
+    array([3, 1, 1])
+    >>> np.char.count(c, 'aA')
+    array([3, 1, 0])
+    >>> np.char.count(c, 'A', start=1, end=4)
+    array([2, 1, 1])
+    >>> np.char.count(c, 'A', start=1, end=3)
+    array([1, 0, 0])
+
+    """
+    return _vec_string(a, int_, 'count', [sub, start] + _clean_args(end))
+
+
+def _code_dispatcher(a, encoding=None, errors=None):
+    return (a,)
+
+
+@array_function_dispatch(_code_dispatcher)
+def decode(a, encoding=None, errors=None):
+    """
+    Calls `str.decode` element-wise.
+
+    The set of available codecs comes from the Python standard library,
+    and may be extended at runtime.  For more information, see the
+    :mod:`codecs` module.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    encoding : str, optional
+       The name of an encoding
+
+    errors : str, optional
+       Specifies how to handle encoding errors
+
+    Returns
+    -------
+    out : ndarray
+
+    See Also
+    --------
+    str.decode
+
+    Notes
+    -----
+    The type of the result will depend on the encoding specified.
+
+    Examples
+    --------
+    >>> c = np.array(['aAaAaA', '  aA  ', 'abBABba'])
+    >>> c
+    array(['aAaAaA', '  aA  ', 'abBABba'], dtype='<U7')
+    >>> np.char.encode(c, encoding='cp037')
+    array(['\\x81\\xc1\\x81\\xc1\\x81\\xc1', '@@\\x81\\xc1@@',
+        '\\x81\\x82\\xc2\\xc1\\xc2\\x82\\x81'],
+        dtype='|S7')
+
+    """
+    return _to_string_or_unicode_array(
+        _vec_string(a, object_, 'decode', _clean_args(encoding, errors)))
+
+
+@array_function_dispatch(_code_dispatcher)
+def encode(a, encoding=None, errors=None):
+    """
+    Calls `str.encode` element-wise.
+
+    The set of available codecs comes from the Python standard library,
+    and may be extended at runtime. For more information, see the codecs
+    module.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    encoding : str, optional
+       The name of an encoding
+
+    errors : str, optional
+       Specifies how to handle encoding errors
+
+    Returns
+    -------
+    out : ndarray
+
+    See Also
+    --------
+    str.encode
+
+    Notes
+    -----
+    The type of the result will depend on the encoding specified.
+
+    """
+    return _to_string_or_unicode_array(
+        _vec_string(a, object_, 'encode', _clean_args(encoding, errors)))
+
+
+def _endswith_dispatcher(a, suffix, start=None, end=None):
+    return (a,)
+
+
+@array_function_dispatch(_endswith_dispatcher)
+def endswith(a, suffix, start=0, end=None):
+    """
+    Returns a boolean array which is `True` where the string element
+    in `a` ends with `suffix`, otherwise `False`.
+
+    Calls `str.endswith` element-wise.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    suffix : str
+
+    start, end : int, optional
+        With optional `start`, test beginning at that position. With
+        optional `end`, stop comparing at that position.
+
+    Returns
+    -------
+    out : ndarray
+        Outputs an array of bools.
+
+    See Also
+    --------
+    str.endswith
+
+    Examples
+    --------
+    >>> s = np.array(['foo', 'bar'])
+    >>> s[0] = 'foo'
+    >>> s[1] = 'bar'
+    >>> s
+    array(['foo', 'bar'], dtype='<U3')
+    >>> np.char.endswith(s, 'ar')
+    array([False,  True])
+    >>> np.char.endswith(s, 'a', start=1, end=2)
+    array([False,  True])
+
+    """
+    return _vec_string(
+        a, bool_, 'endswith', [suffix, start] + _clean_args(end))
+
+
+def _expandtabs_dispatcher(a, tabsize=None):
+    return (a,)
+
+
+@array_function_dispatch(_expandtabs_dispatcher)
+def expandtabs(a, tabsize=8):
+    """
+    Return a copy of each string element where all tab characters are
+    replaced by one or more spaces.
+
+    Calls `str.expandtabs` element-wise.
+
+    Return a copy of each string element where all tab characters are
+    replaced by one or more spaces, depending on the current column
+    and the given `tabsize`. The column number is reset to zero after
+    each newline occurring in the string. This doesn't understand other
+    non-printing characters or escape sequences.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+        Input array
+    tabsize : int, optional
+        Replace tabs with `tabsize` number of spaces.  If not given defaults
+        to 8 spaces.
+
+    Returns
+    -------
+    out : ndarray
+        Output array of str or unicode, depending on input type
+
+    See Also
+    --------
+    str.expandtabs
+
+    """
+    return _to_string_or_unicode_array(
+        _vec_string(a, object_, 'expandtabs', (tabsize,)))
+
+
+@array_function_dispatch(_count_dispatcher)
+def find(a, sub, start=0, end=None):
+    """
+    For each element, return the lowest index in the string where
+    substring `sub` is found.
+
+    Calls `str.find` element-wise.
+
+    For each element, return the lowest index in the string where
+    substring `sub` is found, such that `sub` is contained in the
+    range [`start`, `end`].
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    sub : str or unicode
+
+    start, end : int, optional
+        Optional arguments `start` and `end` are interpreted as in
+        slice notation.
+
+    Returns
+    -------
+    out : ndarray or int
+        Output array of ints.  Returns -1 if `sub` is not found.
+
+    See Also
+    --------
+    str.find
+
+    """
+    return _vec_string(
+        a, int_, 'find', [sub, start] + _clean_args(end))
+
+
+@array_function_dispatch(_count_dispatcher)
+def index(a, sub, start=0, end=None):
+    """
+    Like `find`, but raises `ValueError` when the substring is not found.
+
+    Calls `str.index` element-wise.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    sub : str or unicode
+
+    start, end : int, optional
+
+    Returns
+    -------
+    out : ndarray
+        Output array of ints.  Returns -1 if `sub` is not found.
+
+    See Also
+    --------
+    find, str.find
+
+    """
+    return _vec_string(
+        a, int_, 'index', [sub, start] + _clean_args(end))
+
+
+@array_function_dispatch(_unary_op_dispatcher)
+def isalnum(a):
+    """
+    Returns true for each element if all characters in the string are
+    alphanumeric and there is at least one character, false otherwise.
+
+    Calls `str.isalnum` element-wise.
+
+    For 8-bit strings, this method is locale-dependent.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    Returns
+    -------
+    out : ndarray
+        Output array of str or unicode, depending on input type
+
+    See Also
+    --------
+    str.isalnum
+    """
+    return _vec_string(a, bool_, 'isalnum')
+
+
+@array_function_dispatch(_unary_op_dispatcher)
+def isalpha(a):
+    """
+    Returns true for each element if all characters in the string are
+    alphabetic and there is at least one character, false otherwise.
+
+    Calls `str.isalpha` element-wise.
+
+    For 8-bit strings, this method is locale-dependent.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    Returns
+    -------
+    out : ndarray
+        Output array of bools
+
+    See Also
+    --------
+    str.isalpha
+    """
+    return _vec_string(a, bool_, 'isalpha')
+
+
+@array_function_dispatch(_unary_op_dispatcher)
+def isdigit(a):
+    """
+    Returns true for each element if all characters in the string are
+    digits and there is at least one character, false otherwise.
+
+    Calls `str.isdigit` element-wise.
+
+    For 8-bit strings, this method is locale-dependent.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    Returns
+    -------
+    out : ndarray
+        Output array of bools
+
+    See Also
+    --------
+    str.isdigit
+    """
+    return _vec_string(a, bool_, 'isdigit')
+
+
+@array_function_dispatch(_unary_op_dispatcher)
+def islower(a):
+    """
+    Returns true for each element if all cased characters in the
+    string are lowercase and there is at least one cased character,
+    false otherwise.
+
+    Calls `str.islower` element-wise.
+
+    For 8-bit strings, this method is locale-dependent.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    Returns
+    -------
+    out : ndarray
+        Output array of bools
+
+    See Also
+    --------
+    str.islower
+    """
+    return _vec_string(a, bool_, 'islower')
+
+
+@array_function_dispatch(_unary_op_dispatcher)
+def isspace(a):
+    """
+    Returns true for each element if there are only whitespace
+    characters in the string and there is at least one character,
+    false otherwise.
+
+    Calls `str.isspace` element-wise.
+
+    For 8-bit strings, this method is locale-dependent.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    Returns
+    -------
+    out : ndarray
+        Output array of bools
+
+    See Also
+    --------
+    str.isspace
+    """
+    return _vec_string(a, bool_, 'isspace')
+
+
+@array_function_dispatch(_unary_op_dispatcher)
+def istitle(a):
+    """
+    Returns true for each element if the element is a titlecased
+    string and there is at least one character, false otherwise.
+
+    Call `str.istitle` element-wise.
+
+    For 8-bit strings, this method is locale-dependent.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    Returns
+    -------
+    out : ndarray
+        Output array of bools
+
+    See Also
+    --------
+    str.istitle
+    """
+    return _vec_string(a, bool_, 'istitle')
+
+
+@array_function_dispatch(_unary_op_dispatcher)
+def isupper(a):
+    """
+    Returns true for each element if all cased characters in the
+    string are uppercase and there is at least one character, false
+    otherwise.
+
+    Call `str.isupper` element-wise.
+
+    For 8-bit strings, this method is locale-dependent.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    Returns
+    -------
+    out : ndarray
+        Output array of bools
+
+    See Also
+    --------
+    str.isupper
+    """
+    return _vec_string(a, bool_, 'isupper')
+
+
+def _join_dispatcher(sep, seq):
+    return (sep, seq)
+
+
+@array_function_dispatch(_join_dispatcher)
+def join(sep, seq):
+    """
+    Return a string which is the concatenation of the strings in the
+    sequence `seq`.
+
+    Calls `str.join` element-wise.
+
+    Parameters
+    ----------
+    sep : array_like of str or unicode
+    seq : array_like of str or unicode
+
+    Returns
+    -------
+    out : ndarray
+        Output array of str or unicode, depending on input types
+
+    See Also
+    --------
+    str.join
+    """
+    return _to_string_or_unicode_array(
+        _vec_string(sep, object_, 'join', (seq,)))
+
+
+
+def _just_dispatcher(a, width, fillchar=None):
+    return (a,)
+
+
+@array_function_dispatch(_just_dispatcher)
+def ljust(a, width, fillchar=' '):
+    """
+    Return an array with the elements of `a` left-justified in a
+    string of length `width`.
+
+    Calls `str.ljust` element-wise.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    width : int
+        The length of the resulting strings
+    fillchar : str or unicode, optional
+        The character to use for padding
+
+    Returns
+    -------
+    out : ndarray
+        Output array of str or unicode, depending on input type
+
+    See Also
+    --------
+    str.ljust
+
+    """
+    a_arr = numpy.asarray(a)
+    width_arr = numpy.asarray(width)
+    size = int(numpy.max(width_arr.flat))
+    if numpy.issubdtype(a_arr.dtype, numpy.string_):
+        fillchar = asbytes(fillchar)
+    return _vec_string(
+        a_arr, (a_arr.dtype.type, size), 'ljust', (width_arr, fillchar))
+
+
+@array_function_dispatch(_unary_op_dispatcher)
+def lower(a):
+    """
+    Return an array with the elements converted to lowercase.
+
+    Call `str.lower` element-wise.
+
+    For 8-bit strings, this method is locale-dependent.
+
+    Parameters
+    ----------
+    a : array_like, {str, unicode}
+        Input array.
+
+    Returns
+    -------
+    out : ndarray, {str, unicode}
+        Output array of str or unicode, depending on input type
+
+    See Also
+    --------
+    str.lower
+
+    Examples
+    --------
+    >>> c = np.array(['A1B C', '1BCA', 'BCA1']); c
+    array(['A1B C', '1BCA', 'BCA1'], dtype='<U5')
+    >>> np.char.lower(c)
+    array(['a1b c', '1bca', 'bca1'], dtype='<U5')
+
+    """
+    a_arr = numpy.asarray(a)
+    return _vec_string(a_arr, a_arr.dtype, 'lower')
+
+
+def _strip_dispatcher(a, chars=None):
+    return (a,)
+
+
+@array_function_dispatch(_strip_dispatcher)
+def lstrip(a, chars=None):
+    """
+    For each element in `a`, return a copy with the leading characters
+    removed.
+
+    Calls `str.lstrip` element-wise.
+
+    Parameters
+    ----------
+    a : array-like, {str, unicode}
+        Input array.
+
+    chars : {str, unicode}, optional
+        The `chars` argument is a string specifying the set of
+        characters to be removed. If omitted or None, the `chars`
+        argument defaults to removing whitespace. The `chars` argument
+        is not a prefix; rather, all combinations of its values are
+        stripped.
+
+    Returns
+    -------
+    out : ndarray, {str, unicode}
+        Output array of str or unicode, depending on input type
+
+    See Also
+    --------
+    str.lstrip
+
+    Examples
+    --------
+    >>> c = np.array(['aAaAaA', '  aA  ', 'abBABba'])
+    >>> c
+    array(['aAaAaA', '  aA  ', 'abBABba'], dtype='<U7')
+
+    The 'a' variable is unstripped from c[1] because whitespace leading.
+
+    >>> np.char.lstrip(c, 'a')
+    array(['AaAaA', '  aA  ', 'bBABba'], dtype='<U7')
+
+
+    >>> np.char.lstrip(c, 'A') # leaves c unchanged
+    array(['aAaAaA', '  aA  ', 'abBABba'], dtype='<U7')
+    >>> (np.char.lstrip(c, ' ') == np.char.lstrip(c, '')).all()
+    ... # XXX: is this a regression? This used to return True
+    ... # np.char.lstrip(c,'') does not modify c at all.
+    False
+    >>> (np.char.lstrip(c, ' ') == np.char.lstrip(c, None)).all()
+    True
+
+    """
+    a_arr = numpy.asarray(a)
+    return _vec_string(a_arr, a_arr.dtype, 'lstrip', (chars,))
+
+
+def _partition_dispatcher(a, sep):
+    return (a,)
+
+
+@array_function_dispatch(_partition_dispatcher)
+def partition(a, sep):
+    """
+    Partition each element in `a` around `sep`.
+
+    Calls `str.partition` element-wise.
+
+    For each element in `a`, split the element as the first
+    occurrence of `sep`, and return 3 strings containing the part
+    before the separator, the separator itself, and the part after
+    the separator. If the separator is not found, return 3 strings
+    containing the string itself, followed by two empty strings.
+
+    Parameters
+    ----------
+    a : array_like, {str, unicode}
+        Input array
+    sep : {str, unicode}
+        Separator to split each string element in `a`.
+
+    Returns
+    -------
+    out : ndarray, {str, unicode}
+        Output array of str or unicode, depending on input type.
+        The output array will have an extra dimension with 3
+        elements per input element.
+
+    See Also
+    --------
+    str.partition
+
+    """
+    return _to_string_or_unicode_array(
+        _vec_string(a, object_, 'partition', (sep,)))
+
+
+def _replace_dispatcher(a, old, new, count=None):
+    return (a,)
+
+
+@array_function_dispatch(_replace_dispatcher)
+def replace(a, old, new, count=None):
+    """
+    For each element in `a`, return a copy of the string with all
+    occurrences of substring `old` replaced by `new`.
+
+    Calls `str.replace` element-wise.
+
+    Parameters
+    ----------
+    a : array-like of str or unicode
+
+    old, new : str or unicode
+
+    count : int, optional
+        If the optional argument `count` is given, only the first
+        `count` occurrences are replaced.
+
+    Returns
+    -------
+    out : ndarray
+        Output array of str or unicode, depending on input type
+
+    See Also
+    --------
+    str.replace
+
+    """
+    return _to_string_or_unicode_array(
+        _vec_string(
+            a, object_, 'replace', [old, new] + _clean_args(count)))
+
+
+@array_function_dispatch(_count_dispatcher)
+def rfind(a, sub, start=0, end=None):
+    """
+    For each element in `a`, return the highest index in the string
+    where substring `sub` is found, such that `sub` is contained
+    within [`start`, `end`].
+
+    Calls `str.rfind` element-wise.
+
+    Parameters
+    ----------
+    a : array-like of str or unicode
+
+    sub : str or unicode
+
+    start, end : int, optional
+        Optional arguments `start` and `end` are interpreted as in
+        slice notation.
+
+    Returns
+    -------
+    out : ndarray
+       Output array of ints.  Return -1 on failure.
+
+    See Also
+    --------
+    str.rfind
+
+    """
+    return _vec_string(
+        a, int_, 'rfind', [sub, start] + _clean_args(end))
+
+
+@array_function_dispatch(_count_dispatcher)
+def rindex(a, sub, start=0, end=None):
+    """
+    Like `rfind`, but raises `ValueError` when the substring `sub` is
+    not found.
+
+    Calls `str.rindex` element-wise.
+
+    Parameters
+    ----------
+    a : array-like of str or unicode
+
+    sub : str or unicode
+
+    start, end : int, optional
+
+    Returns
+    -------
+    out : ndarray
+       Output array of ints.
+
+    See Also
+    --------
+    rfind, str.rindex
+
+    """
+    return _vec_string(
+        a, int_, 'rindex', [sub, start] + _clean_args(end))
+
+
+@array_function_dispatch(_just_dispatcher)
+def rjust(a, width, fillchar=' '):
+    """
+    Return an array with the elements of `a` right-justified in a
+    string of length `width`.
+
+    Calls `str.rjust` element-wise.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    width : int
+        The length of the resulting strings
+    fillchar : str or unicode, optional
+        The character to use for padding
+
+    Returns
+    -------
+    out : ndarray
+        Output array of str or unicode, depending on input type
+
+    See Also
+    --------
+    str.rjust
+
+    """
+    a_arr = numpy.asarray(a)
+    width_arr = numpy.asarray(width)
+    size = int(numpy.max(width_arr.flat))
+    if numpy.issubdtype(a_arr.dtype, numpy.string_):
+        fillchar = asbytes(fillchar)
+    return _vec_string(
+        a_arr, (a_arr.dtype.type, size), 'rjust', (width_arr, fillchar))
+
+
+@array_function_dispatch(_partition_dispatcher)
+def rpartition(a, sep):
+    """
+    Partition (split) each element around the right-most separator.
+
+    Calls `str.rpartition` element-wise.
+
+    For each element in `a`, split the element as the last
+    occurrence of `sep`, and return 3 strings containing the part
+    before the separator, the separator itself, and the part after
+    the separator. If the separator is not found, return 3 strings
+    containing the string itself, followed by two empty strings.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+        Input array
+    sep : str or unicode
+        Right-most separator to split each element in array.
+
+    Returns
+    -------
+    out : ndarray
+        Output array of string or unicode, depending on input
+        type.  The output array will have an extra dimension with
+        3 elements per input element.
+
+    See Also
+    --------
+    str.rpartition
+
+    """
+    return _to_string_or_unicode_array(
+        _vec_string(a, object_, 'rpartition', (sep,)))
+
+
+def _split_dispatcher(a, sep=None, maxsplit=None):
+    return (a,)
+
+
+@array_function_dispatch(_split_dispatcher)
+def rsplit(a, sep=None, maxsplit=None):
+    """
+    For each element in `a`, return a list of the words in the
+    string, using `sep` as the delimiter string.
+
+    Calls `str.rsplit` element-wise.
+
+    Except for splitting from the right, `rsplit`
+    behaves like `split`.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    sep : str or unicode, optional
+        If `sep` is not specified or None, any whitespace string
+        is a separator.
+    maxsplit : int, optional
+        If `maxsplit` is given, at most `maxsplit` splits are done,
+        the rightmost ones.
+
+    Returns
+    -------
+    out : ndarray
+       Array of list objects
+
+    See Also
+    --------
+    str.rsplit, split
+
+    """
+    # This will return an array of lists of different sizes, so we
+    # leave it as an object array
+    return _vec_string(
+        a, object_, 'rsplit', [sep] + _clean_args(maxsplit))
+
+
+def _strip_dispatcher(a, chars=None):
+    return (a,)
+
+
+@array_function_dispatch(_strip_dispatcher)
+def rstrip(a, chars=None):
+    """
+    For each element in `a`, return a copy with the trailing
+    characters removed.
+
+    Calls `str.rstrip` element-wise.
+
+    Parameters
+    ----------
+    a : array-like of str or unicode
+
+    chars : str or unicode, optional
+       The `chars` argument is a string specifying the set of
+       characters to be removed. If omitted or None, the `chars`
+       argument defaults to removing whitespace. The `chars` argument
+       is not a suffix; rather, all combinations of its values are
+       stripped.
+
+    Returns
+    -------
+    out : ndarray
+        Output array of str or unicode, depending on input type
+
+    See Also
+    --------
+    str.rstrip
+
+    Examples
+    --------
+    >>> c = np.array(['aAaAaA', 'abBABba'], dtype='S7'); c
+    array(['aAaAaA', 'abBABba'],
+        dtype='|S7')
+    >>> np.char.rstrip(c, b'a')
+    array(['aAaAaA', 'abBABb'],
+        dtype='|S7')
+    >>> np.char.rstrip(c, b'A')
+    array(['aAaAa', 'abBABba'],
+        dtype='|S7')
+
+    """
+    a_arr = numpy.asarray(a)
+    return _vec_string(a_arr, a_arr.dtype, 'rstrip', (chars,))
+
+
+@array_function_dispatch(_split_dispatcher)
+def split(a, sep=None, maxsplit=None):
+    """
+    For each element in `a`, return a list of the words in the
+    string, using `sep` as the delimiter string.
+
+    Calls `str.split` element-wise.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    sep : str or unicode, optional
+       If `sep` is not specified or None, any whitespace string is a
+       separator.
+
+    maxsplit : int, optional
+        If `maxsplit` is given, at most `maxsplit` splits are done.
+
+    Returns
+    -------
+    out : ndarray
+        Array of list objects
+
+    See Also
+    --------
+    str.split, rsplit
+
+    """
+    # This will return an array of lists of different sizes, so we
+    # leave it as an object array
+    return _vec_string(
+        a, object_, 'split', [sep] + _clean_args(maxsplit))
+
+
+def _splitlines_dispatcher(a, keepends=None):
+    return (a,)
+
+
+@array_function_dispatch(_splitlines_dispatcher)
+def splitlines(a, keepends=None):
+    """
+    For each element in `a`, return a list of the lines in the
+    element, breaking at line boundaries.
+
+    Calls `str.splitlines` element-wise.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    keepends : bool, optional
+        Line breaks are not included in the resulting list unless
+        keepends is given and true.
+
+    Returns
+    -------
+    out : ndarray
+        Array of list objects
+
+    See Also
+    --------
+    str.splitlines
+
+    """
+    return _vec_string(
+        a, object_, 'splitlines', _clean_args(keepends))
+
+
+def _startswith_dispatcher(a, prefix, start=None, end=None):
+    return (a,)
+
+
+@array_function_dispatch(_startswith_dispatcher)
+def startswith(a, prefix, start=0, end=None):
+    """
+    Returns a boolean array which is `True` where the string element
+    in `a` starts with `prefix`, otherwise `False`.
+
+    Calls `str.startswith` element-wise.
+
+    Parameters
+    ----------
+    a : array_like of str or unicode
+
+    prefix : str
+
+    start, end : int, optional
+        With optional `start`, test beginning at that position. With
+        optional `end`, stop comparing at that position.
+
+    Returns
+    -------
+    out : ndarray
+        Array of booleans
+
+    See Also
+    --------
+    str.startswith
+
+    """
+    return _vec_string(
+        a, bool_, 'startswith', [prefix, start] + _clean_args(end))
+
+
+@array_function_dispatch(_strip_dispatcher)
+def strip(a, chars=None):
+    """
+    For each element in `a`, return a copy with the leading and
+    trailing characters removed.
+
+    Calls `str.strip` element-wise.
+
+    Parameters
+    ----------
+    a : array-like of str or unicode
+
+    chars : str or unicode, optional
+       The `chars` argument is a string specifying the set of
+       characters to be removed. If omitted or None, the `chars`
+       argument defaults to removing whitespace. The `chars` argument
+       is not a prefix or suffix; rather, all combinations of its
+       values are stripped.
+
+    Returns
+    -------
+    out : ndarray
+        Output array of str or unicode, depending on input type
+
+    See Also
+    --------
+    str.strip
+
+    Examples
+    --------
+    >>> c = np.array(['aAaAaA', '  aA  ', 'abBABba'])
+    >>> c
+    array(['aAaAaA', '  aA  ', 'abBABba'], dtype='<U7')
+    >>> np.char.strip(c)
+    array(['aAaAaA', 'aA', 'abBABba'], dtype='<U7')
+    >>> np.char.strip(c, 'a') # 'a' unstripped from c[1] because whitespace leads
+    array(['AaAaA', '  aA  ', 'bBABb'], dtype='<U7')
+    >>> np.char.strip(c, 'A') # 'A' unstripped from c[1] because (unprinted) ws trails
+    array(['aAaAa', '  aA  ', 'abBABba'], dtype='<U7')
+
+    """
+    a_arr = numpy.asarray(a)
+    return _vec_string(a_arr, a_arr.dtype, 'strip', _clean_args(chars))
+
+
+@array_function_dispatch(_unary_op_dispatcher)
+def swapcase(a):
+    """
+    Return element-wise a copy of the string with
+    uppercase characters converted to lowercase and vice versa.
+
+    Calls `str.swapcase` element-wise.
+
+    For 8-bit strings, this method is locale-dependent.
+
+    Parameters
+    ----------
+    a : array_like, {str, unicode}
+        Input array.
+
+    Returns
+    -------
+    out : ndarray, {str, unicode}
+        Output array of str or unicode, depending on input type
+
+    See Also
+    --------
+    str.swapcase
+
+    Examples
+    --------
+    >>> c=np.array(['a1B c','1b Ca','b Ca1','cA1b'],'S5'); c
+    array(['a1B c', '1b Ca', 'b Ca1', 'cA1b'],
+        dtype='|S5')
+    >>> np.char.swapcase(c)
+    array(['A1b C', '1B cA', 'B cA1', 'Ca1B'],
+        dtype='|S5')
+
+    """
+    a_arr = numpy.asarray(a)
+    return _vec_string(a_arr, a_arr.dtype, 'swapcase')
+
+
+@array_function_dispatch(_unary_op_dispatcher)
+def title(a):
+    """
+    Return element-wise title cased version of string or unicode.
+
+    Title case words start with uppercase characters, all remaining cased
+    characters are lowercase.
+
+    Calls `str.title` element-wise.
+
+    For 8-bit strings, this method is locale-dependent.
+
+    Parameters
+    ----------
+    a : array_like, {str, unicode}
+        Input array.
+
+    Returns
+    -------
+    out : ndarray
+        Output array of str or unicode, depending on input type
+
+    See Also
+    --------
+    str.title
+
+    Examples
+    --------
+    >>> c=np.array(['a1b c','1b ca','b ca1','ca1b'],'S5'); c
+    array(['a1b c', '1b ca', 'b ca1', 'ca1b'],
+        dtype='|S5')
+    >>> np.char.title(c)
+    array(['A1B C', '1B Ca', 'B Ca1', 'Ca1B'],
+        dtype='|S5')
+
+    """
+    a_arr = numpy.asarray(a)
+    return _vec_string(a_arr, a_arr.dtype, 'title')
+
+
+def _translate_dispatcher(a, table, deletechars=None):
+    return (a,)
+
+
+@array_function_dispatch(_translate_dispatcher)
+def translate(a, table, deletechars=None):
+    """
+    For each element in `a`, return a copy of the string where all
+    characters occurring in the optional argument `deletechars` are
+    removed, and the remaining characters have been mapped through the
+    given translation table.
+
+    Calls `str.translate` element-wise.
+
+    Parameters
+    ----------
+    a : array-like of str or unicode
+
+    table : str of length 256
+
+    deletechars : str
+
+    Returns
+    -------
+    out : ndarray
+        Output array of str or unicode, depending on input type
+
+    See Also
+    --------
+    str.translate
+
+    """
+    a_arr = numpy.asarray(a)
+    if issubclass(a_arr.dtype.type, unicode_):
+        return _vec_string(
+            a_arr, a_arr.dtype, 'translate', (table,))
+    else:
+        return _vec_string(
+            a_arr, a_arr.dtype, 'translate', [table] + _clean_args(deletechars))
+
+
+@array_function_dispatch(_unary_op_dispatcher)
+def upper(a):
+    """
+    Return an array with the elements converted to uppercase.
+
+    Calls `str.upper` element-wise.
+
+    For 8-bit strings, this method is locale-dependent.
+
+    Parameters
+    ----------
+    a : array_like, {str, unicode}
+        Input array.
+
+    Returns
+    -------
+    out : ndarray, {str, unicode}
+        Output array of str or unicode, depending on input type
+
+    See Also
+    --------
+    str.upper
+
+    Examples
+    --------
+    >>> c = np.array(['a1b c', '1bca', 'bca1']); c
+    array(['a1b c', '1bca', 'bca1'], dtype='<U5')
+    >>> np.char.upper(c)
+    array(['A1B C', '1BCA', 'BCA1'], dtype='<U5')
+
+    """
+    a_arr = numpy.asarray(a)
+    return _vec_string(a_arr, a_arr.dtype, 'upper')
+
+
+def _zfill_dispatcher(a, width):
+    return (a,)
+
+
+@array_function_dispatch(_zfill_dispatcher)
+def zfill(a, width):
+    """
+    Return the numeric string left-filled with zeros
+
+    Calls `str.zfill` element-wise.
+
+    Parameters
+    ----------
+    a : array_like, {str, unicode}
+        Input array.
+    width : int
+        Width of string to left-fill elements in `a`.
+
+    Returns
+    -------
+    out : ndarray, {str, unicode}
+        Output array of str or unicode, depending on input type
+
+    See Also
+    --------
+    str.zfill
+
+    """
+    a_arr = numpy.asarray(a)
+    width_arr = numpy.asarray(width)
+    size = int(numpy.max(width_arr.flat))
+    return _vec_string(
+        a_arr, (a_arr.dtype.type, size), 'zfill', (width_arr,))
+
+
+@array_function_dispatch(_unary_op_dispatcher)
+def isnumeric(a):
+    """
+    For each element, return True if there are only numeric
+    characters in the element.
+
+    Calls `unicode.isnumeric` element-wise.
+
+    Numeric characters include digit characters, and all characters
+    that have the Unicode numeric value property, e.g. ``U+2155,
+    VULGAR FRACTION ONE FIFTH``.
+
+    Parameters
+    ----------
+    a : array_like, unicode
+        Input array.
+
+    Returns
+    -------
+    out : ndarray, bool
+        Array of booleans of same shape as `a`.
+
+    See Also
+    --------
+    unicode.isnumeric
+
+    """
+    if _use_unicode(a) != unicode_:
+        raise TypeError("isnumeric is only available for Unicode strings and arrays")
+    return _vec_string(a, bool_, 'isnumeric')
+
+
+@array_function_dispatch(_unary_op_dispatcher)
+def isdecimal(a):
+    """
+    For each element, return True if there are only decimal
+    characters in the element.
+
+    Calls `unicode.isdecimal` element-wise.
+
+    Decimal characters include digit characters, and all characters
+    that can be used to form decimal-radix numbers,
+    e.g. ``U+0660, ARABIC-INDIC DIGIT ZERO``.
+
+    Parameters
+    ----------
+    a : array_like, unicode
+        Input array.
+
+    Returns
+    -------
+    out : ndarray, bool
+        Array of booleans identical in shape to `a`.
+
+    See Also
+    --------
+    unicode.isdecimal
+
+    """
+    if _use_unicode(a) != unicode_:
+        raise TypeError("isnumeric is only available for Unicode strings and arrays")
+    return _vec_string(a, bool_, 'isdecimal')
+
+
+@set_module('numpy')
 class chararray(ndarray):
+    """
+    chararray(shape, itemsize=1, unicode=False, buffer=None, offset=0,
+              strides=None, order=None)
+
+    Provides a convenient view on arrays of string and unicode values.
+
+    .. note::
+       The `chararray` class exists for backwards compatibility with
+       Numarray, it is not recommended for new development. Starting from numpy
+       1.4, if one needs arrays of strings, it is recommended to use arrays of
+       `dtype` `object_`, `string_` or `unicode_`, and use the free functions
+       in the `numpy.char` module for fast vectorized string operations.
+
+    Versus a regular NumPy array of type `str` or `unicode`, this
+    class adds the following functionality:
+
+      1) values automatically have whitespace removed from the end
+         when indexed
+
+      2) comparison operators automatically remove whitespace from the
+         end when comparing values
+
+      3) vectorized string operations are provided as methods
+         (e.g. `.endswith`) and infix operators (e.g. ``"+", "*", "%"``)
+
+    chararrays should be created using `numpy.char.array` or
+    `numpy.char.asarray`, rather than this constructor directly.
+
+    This constructor creates the array, using `buffer` (with `offset`
+    and `strides`) if it is not ``None``. If `buffer` is ``None``, then
+    constructs a new array with `strides` in "C order", unless both
+    ``len(shape) >= 2`` and ``order='F'``, in which case `strides`
+    is in "Fortran order".
+
+    Methods
+    -------
+    astype
+    argsort
+    copy
+    count
+    decode
+    dump
+    dumps
+    encode
+    endswith
+    expandtabs
+    fill
+    find
+    flatten
+    getfield
+    index
+    isalnum
+    isalpha
+    isdecimal
+    isdigit
+    islower
+    isnumeric
+    isspace
+    istitle
+    isupper
+    item
+    join
+    ljust
+    lower
+    lstrip
+    nonzero
+    put
+    ravel
+    repeat
+    replace
+    reshape
+    resize
+    rfind
+    rindex
+    rjust
+    rsplit
+    rstrip
+    searchsorted
+    setfield
+    setflags
+    sort
+    split
+    splitlines
+    squeeze
+    startswith
+    strip
+    swapaxes
+    swapcase
+    take
+    title
+    tofile
+    tolist
+    tostring
+    translate
+    transpose
+    upper
+    view
+    zfill
+
+    Parameters
+    ----------
+    shape : tuple
+        Shape of the array.
+    itemsize : int, optional
+        Length of each array element, in number of characters. Default is 1.
+    unicode : bool, optional
+        Are the array elements of type unicode (True) or string (False).
+        Default is False.
+    buffer : object exposing the buffer interface or str, optional
+        Memory address of the start of the array data.  Default is None,
+        in which case a new array is created.
+    offset : int, optional
+        Fixed stride displacement from the beginning of an axis?
+        Default is 0. Needs to be >=0.
+    strides : array_like of ints, optional
+        Strides for the array (see `ndarray.strides` for full description).
+        Default is None.
+    order : {'C', 'F'}, optional
+        The order in which the array data is stored in memory: 'C' ->
+        "row major" order (the default), 'F' -> "column major"
+        (Fortran) order.
+
+    Examples
+    --------
+    >>> charar = np.chararray((3, 3))
+    >>> charar[:] = 'a'
+    >>> charar
+    chararray([[b'a', b'a', b'a'],
+               [b'a', b'a', b'a'],
+               [b'a', b'a', b'a']], dtype='|S1')
+
+    >>> charar = np.chararray(charar.shape, itemsize=5)
+    >>> charar[:] = 'abc'
+    >>> charar
+    chararray([[b'abc', b'abc', b'abc'],
+               [b'abc', b'abc', b'abc'],
+               [b'abc', b'abc', b'abc']], dtype='|S5')
+
+    """
     def __new__(subtype, shape, itemsize=1, unicode=False, buffer=None,
-                offset=0, strides=None, order=None):
+                offset=0, strides=None, order='C'):
         global _globalvar
 
         if unicode:
             dtype = unicode_
         else:
-            dtype = string
+            dtype = string_
+
+        # force itemsize to be a Python int, since using NumPy integer
+        # types results in itemsize.itemsize being used as the size of
+        # strings in the new array.
+        itemsize = int(itemsize)
+
+        if isinstance(buffer, str):
+            # unicode objects do not have the buffer interface
+            filler = buffer
+            buffer = None
+        else:
+            filler = None
 
         _globalvar = 1
         if buffer is None:
@@ -32,307 +1972,825 @@
                                    buffer=buffer,
                                    offset=offset, strides=strides,
                                    order=order)
+        if filler is not None:
+            self[...] = filler
         _globalvar = 0
         return self
 
     def __array_finalize__(self, obj):
-        # The b is a special case because it is used for reconstructing. 
-        if not _globalvar and self.dtype.char not in 'SUb':
-            raise ValueError, "Can only create a chararray from string data."
-
-
-##    def _richcmpfunc(self, other, op):
-##        b = broadcast(self, other)
-##        result = empty(b.shape, dtype=bool)
-##        res = result.flat
-##        for k, val in enumerate(b):
-##            r1 = val[0].rstrip('\x00')
-##            r2 = val[1]
-##            res[k] = eval("r1 %s r2" % op, {'r1':r1,'r2':r2})
-##        return result
-
-    # these have been moved to C
-##    def __eq__(self, other):
-##        return self._richcmpfunc(other, '==')
-
-##    def __ne__(self, other):
-##        return self._richcmpfunc(other, '!=')
-
-##    def __ge__(self, other):
-##        return self._richcmpfunc(other, '>=')
-
-##    def __le__(self, other):
-##        return self._richcmpfunc(other, '<=')
-
-##    def __gt__(self, other):
-##        return self._richcmpfunc(other, '>')
-
-##    def __lt__(self, other):
-##        return self._richcmpfunc(other, '<')
+        # The b is a special case because it is used for reconstructing.
+        if not _globalvar and self.dtype.char not in 'SUbc':
+            raise ValueError("Can only create a chararray from string data.")
+
+    def __getitem__(self, obj):
+        val = ndarray.__getitem__(self, obj)
+
+        if isinstance(val, character):
+            temp = val.rstrip()
+            if len(temp) == 0:
+                val = ''
+            else:
+                val = temp
+
+        return val
+
+    # IMPLEMENTATION NOTE: Most of the methods of this class are
+    # direct delegations to the free functions in this module.
+    # However, those that return an array of strings should instead
+    # return a chararray, so some extra wrapping is required.
+
+    def __eq__(self, other):
+        """
+        Return (self == other) element-wise.
+
+        See Also
+        --------
+        equal
+        """
+        return equal(self, other)
+
+    def __ne__(self, other):
+        """
+        Return (self != other) element-wise.
+
+        See Also
+        --------
+        not_equal
+        """
+        return not_equal(self, other)
+
+    def __ge__(self, other):
+        """
+        Return (self >= other) element-wise.
+
+        See Also
+        --------
+        greater_equal
+        """
+        return greater_equal(self, other)
+
+    def __le__(self, other):
+        """
+        Return (self <= other) element-wise.
+
+        See Also
+        --------
+        less_equal
+        """
+        return less_equal(self, other)
+
+    def __gt__(self, other):
+        """
+        Return (self > other) element-wise.
+
+        See Also
+        --------
+        greater
+        """
+        return greater(self, other)
+
+    def __lt__(self, other):
+        """
+        Return (self < other) element-wise.
+
+        See Also
+        --------
+        less
+        """
+        return less(self, other)
 
     def __add__(self, other):
-        b = broadcast(self, other)
-        arr = b.iters[1].base
-        outitem = self.itemsize + arr.itemsize
-        result = chararray(b.shape, outitem, self.dtype is unicode_)
-        res = result.flat
-        for k, val in enumerate(b):
-            res[k] = (val[0] + val[1])
-        return result
+        """
+        Return (self + other), that is string concatenation,
+        element-wise for a pair of array_likes of str or unicode.
+
+        See Also
+        --------
+        add
+        """
+        return asarray(add(self, other))
 
     def __radd__(self, other):
-        b = broadcast(other, self)
-        outitem = b.iters[0].base.itemsize + \
-                  b.iters[1].base.itemsize
-        result = chararray(b.shape, outitem, self.dtype is unicode_)
-        res = result.flat
-        for k, val in enumerate(b):
-            res[k] = (val[0] + val[1])
-        return result
-
-    def __mul__(self, other):
-        b = broadcast(self, other)
-        arr = b.iters[1].base
-        if not issubclass(arr.dtype.type, integer):
-            raise ValueError, "Can only multiply by integers"
-        outitem = b.iters[0].base.itemsize * arr.max()
-        result = chararray(b.shape, outitem, self.dtype is unicode_)
-        res = result.flat
-        for k, val in enumerate(b):
-            res[k] = val[0]*val[1]
-        return result
-
-    def __rmul__(self, other):
-        b = broadcast(self, other)
-        arr = b.iters[1].base
-        if not issubclass(arr.dtype.type, integer):
-            raise ValueError, "Can only multiply by integers"
-        outitem = b.iters[0].base.itemsize * arr.max()
-        result = chararray(b.shape, outitem, self.dtype is unicode_)
-        res = result.flat
-        for k, val in enumerate(b):
-            res[k] = val[0]*val[1]
-        return result
-
-    def __mod__(self, other):
-        b = broadcast(self, other)
-        res = [None]*b.size
-        maxsize = -1
-        for k,val in enumerate(b):
-            newval = val[0] % val[1]
-            maxsize = max(len(newval), maxsize)
-            res[k] = newval
-        newarr = chararray(b.shape, maxsize, self.dtype is unicode_)
-        newarr[:] = res
-        return newarr
+        """
+        Return (other + self), that is string concatenation,
+        element-wise for a pair of array_likes of `string_` or `unicode_`.
+
+        See Also
+        --------
+        add
+        """
+        return asarray(add(numpy.asarray(other), self))
+
+    def __mul__(self, i):
+        """
+        Return (self * i), that is string multiple concatenation,
+        element-wise.
+
+        See Also
+        --------
+        multiply
+        """
+        return asarray(multiply(self, i))
+
+    def __rmul__(self, i):
+        """
+        Return (self * i), that is string multiple concatenation,
+        element-wise.
+
+        See Also
+        --------
+        multiply
+        """
+        return asarray(multiply(self, i))
+
+    def __mod__(self, i):
+        """
+        Return (self % i), that is pre-Python 2.6 string formatting
+        (interpolation), element-wise for a pair of array_likes of `string_`
+        or `unicode_`.
+
+        See Also
+        --------
+        mod
+        """
+        return asarray(mod(self, i))
 
     def __rmod__(self, other):
         return NotImplemented
 
-    def _generalmethod(self, name, myiter):
-        res = [None]*myiter.size
-        maxsize = -1
-        for k, val in enumerate(myiter):
-            newval = []
-            for chk in val[1:]:
-                if chk.dtype is object_ and chk.item() is None:
-                    break
-                newval.append(chk)
-            newitem = getattr(val[0],name)(*newval)
-            maxsize = max(len(newitem), maxsize)
-            res[k] = newitem
-        newarr = chararray(myiter.shape, maxsize, self.dtype is unicode_)
-        print res, maxsize
-        newarr[:] = res
-        return newarr
-
-    def _typedmethod(self, name, myiter, dtype):
-        result = empty(myiter.shape, dtype=dtype)
-        res = result.flat
-        for k, val in enumerate(myiter):
-            newval = []
-            for chk in val[1:]:
-                if chk.dtype is object_ and chk.item() is None:
-                    break
-                newval.append(chk)
-            this_str = val[0].rstrip('\x00')
-            newitem = getattr(this_str,name)(*newval)
-            res[k] = newitem
-        return result
-
-    def _samemethod(self, name):
-        result = self.copy()
-        res = result.flat
-        for k, val in enumerate(self.flat):
-            res[k] = getattr(val, name)()
-        return result
+    def argsort(self, axis=-1, kind=None, order=None):
+        """
+        Return the indices that sort the array lexicographically.
+
+        For full documentation see `numpy.argsort`, for which this method is
+        in fact merely a "thin wrapper."
+
+        Examples
+        --------
+        >>> c = np.array(['a1b c', '1b ca', 'b ca1', 'Ca1b'], 'S5')
+        >>> c = c.view(np.chararray); c
+        chararray(['a1b c', '1b ca', 'b ca1', 'Ca1b'],
+              dtype='|S5')
+        >>> c[c.argsort()]
+        chararray(['1b ca', 'Ca1b', 'a1b c', 'b ca1'],
+              dtype='|S5')
+
+        """
+        return self.__array__().argsort(axis, kind, order)
+    argsort.__doc__ = ndarray.argsort.__doc__
 
     def capitalize(self):
-        return self._samemethod('capitalize')
-
-    if sys.version[:3] >= '2.4':
-        def center(self, width, fillchar=' '):
-            return self._generalmethod('center',
-                                       broadcast(self, width, fillchar))
-        def ljust(self, width, fillchar=' '):
-            return self._generalmethod('ljust',
-                                       broadcast(self, width, fillchar))
-        def rjust(self, width, fillchar=' '):
-            return self._generalmethod('rjust',
-                                       broadcast(self, width, fillchar))
-        def rsplit(self, sep=None, maxsplit=None):
-            return self._typedmethod('rsplit', broadcast(self, sep, maxsplit),
-                                     object)
-    else:
-        def ljust(self, width):
-            return self._generalmethod('ljust', broadcast(self, width))
-        def rjust(self, width):
-            return self._generalmethod('rjust', broadcast(self, width))
-        def center(self, width):
-            return self._generalmethod('center', broadcast(self, width))
-
-    def count(self, sub, start=None, end=None):
-        return self._typedmethod('count', broadcast(self, sub, start, end), int)
-
-    def decode(self,encoding=None,errors=None):
-        return self._generalmethod('decode', broadcast(self, encoding, errors))
-
-    def encode(self,encoding=None,errors=None):
-        return self._generalmethod('encode', broadcast(self, encoding, errors))
-
-    def endswith(self, suffix, start=None, end=None):
-        return self._typedmethod('endswith', broadcast(self, suffix, start, end), bool)
-
-    def expandtabs(self, tabsize=None):
-        return self._generalmethod('endswith', broadcast(self, tabsize))
-
-    def find(self, sub, start=None, end=None):
-        return self._typedmethod('find', broadcast(self, sub, start, end), int)
-
-    def index(self, sub, start=None, end=None):
-        return self._typedmethod('index', broadcast(self, sub, start, end), int)
-
-    def _ismethod(self, name):
-        result = empty(self.shape, dtype=bool)
-        res = result.flat
-        for k, val in enumerate(self.flat):
-            item = val.rstrip('\x00')
-            res[k] = getattr(item, name)()
-        return result
+        """
+        Return a copy of `self` with only the first character of each element
+        capitalized.
+
+        See Also
+        --------
+        char.capitalize
+
+        """
+        return asarray(capitalize(self))
+
+    def center(self, width, fillchar=' '):
+        """
+        Return a copy of `self` with its elements centered in a
+        string of length `width`.
+
+        See Also
+        --------
+        center
+        """
+        return asarray(center(self, width, fillchar))
+
+    def count(self, sub, start=0, end=None):
+        """
+        Returns an array with the number of non-overlapping occurrences of
+        substring `sub` in the range [`start`, `end`].
+
+        See Also
+        --------
+        char.count
+
+        """
+        return count(self, sub, start, end)
+
+    def decode(self, encoding=None, errors=None):
+        """
+        Calls `str.decode` element-wise.
+
+        See Also
+        --------
+        char.decode
+
+        """
+        return decode(self, encoding, errors)
+
+    def encode(self, encoding=None, errors=None):
+        """
+        Calls `str.encode` element-wise.
+
+        See Also
+        --------
+        char.encode
+
+        """
+        return encode(self, encoding, errors)
+
+    def endswith(self, suffix, start=0, end=None):
+        """
+        Returns a boolean array which is `True` where the string element
+        in `self` ends with `suffix`, otherwise `False`.
+
+        See Also
+        --------
+        char.endswith
+
+        """
+        return endswith(self, suffix, start, end)
+
+    def expandtabs(self, tabsize=8):
+        """
+        Return a copy of each string element where all tab characters are
+        replaced by one or more spaces.
+
+        See Also
+        --------
+        char.expandtabs
+
+        """
+        return asarray(expandtabs(self, tabsize))
+
+    def find(self, sub, start=0, end=None):
+        """
+        For each element, return the lowest index in the string where
+        substring `sub` is found.
+
+        See Also
+        --------
+        char.find
+
+        """
+        return find(self, sub, start, end)
+
+    def index(self, sub, start=0, end=None):
+        """
+        Like `find`, but raises `ValueError` when the substring is not found.
+
+        See Also
+        --------
+        char.index
+
+        """
+        return index(self, sub, start, end)
 
     def isalnum(self):
-        return self._ismethod('isalnum')
+        """
+        Returns true for each element if all characters in the string
+        are alphanumeric and there is at least one character, false
+        otherwise.
+
+        See Also
+        --------
+        char.isalnum
+
+        """
+        return isalnum(self)
 
     def isalpha(self):
-        return self._ismethod('isalpha')
+        """
+        Returns true for each element if all characters in the string
+        are alphabetic and there is at least one character, false
+        otherwise.
+
+        See Also
+        --------
+        char.isalpha
+
+        """
+        return isalpha(self)
 
     def isdigit(self):
-        return self._ismethod('isdigit')
+        """
+        Returns true for each element if all characters in the string are
+        digits and there is at least one character, false otherwise.
+
+        See Also
+        --------
+        char.isdigit
+
+        """
+        return isdigit(self)
 
     def islower(self):
-        return self._ismethod('islower')
+        """
+        Returns true for each element if all cased characters in the
+        string are lowercase and there is at least one cased character,
+        false otherwise.
+
+        See Also
+        --------
+        char.islower
+
+        """
+        return islower(self)
 
     def isspace(self):
-        return self._ismethod('isspace')
+        """
+        Returns true for each element if there are only whitespace
+        characters in the string and there is at least one character,
+        false otherwise.
+
+        See Also
+        --------
+        char.isspace
+
+        """
+        return isspace(self)
 
     def istitle(self):
-        return self._ismethod('istitle')
+        """
+        Returns true for each element if the element is a titlecased
+        string and there is at least one character, false otherwise.
+
+        See Also
+        --------
+        char.istitle
+
+        """
+        return istitle(self)
 
     def isupper(self):
-        return self._ismethod('isupper')
+        """
+        Returns true for each element if all cased characters in the
+        string are uppercase and there is at least one character, false
+        otherwise.
+
+        See Also
+        --------
+        char.isupper
+
+        """
+        return isupper(self)
 
     def join(self, seq):
-        return self._generalmethod('join', broadcast(self, seq))
+        """
+        Return a string which is the concatenation of the strings in the
+        sequence `seq`.
+
+        See Also
+        --------
+        char.join
+
+        """
+        return join(self, seq)
+
+    def ljust(self, width, fillchar=' '):
+        """
+        Return an array with the elements of `self` left-justified in a
+        string of length `width`.
+
+        See Also
+        --------
+        char.ljust
+
+        """
+        return asarray(ljust(self, width, fillchar))
 
     def lower(self):
-        return self._samemethod('lower')
-
-    def lstrip(self, chars):
-        return self._generalmethod('lstrip', broadcast(self, chars))
+        """
+        Return an array with the elements of `self` converted to
+        lowercase.
+
+        See Also
+        --------
+        char.lower
+
+        """
+        return asarray(lower(self))
+
+    def lstrip(self, chars=None):
+        """
+        For each element in `self`, return a copy with the leading characters
+        removed.
+
+        See Also
+        --------
+        char.lstrip
+
+        """
+        return asarray(lstrip(self, chars))
+
+    def partition(self, sep):
+        """
+        Partition each element in `self` around `sep`.
+
+        See Also
+        --------
+        partition
+        """
+        return asarray(partition(self, sep))
 
     def replace(self, old, new, count=None):
-        return self._generalmethod('replace', broadcast(self, old, new, count))
-
-    def rfind(self, sub, start=None, end=None):
-        return self._typedmethod('rfind', broadcast(self, sub, start, end), int)
-
-    def rindex(self, sub, start=None, end=None):
-        return self._typedmethod('rindex', broadcast(self, sub, start, end), int)
+        """
+        For each element in `self`, return a copy of the string with all
+        occurrences of substring `old` replaced by `new`.
+
+        See Also
+        --------
+        char.replace
+
+        """
+        return asarray(replace(self, old, new, count))
+
+    def rfind(self, sub, start=0, end=None):
+        """
+        For each element in `self`, return the highest index in the string
+        where substring `sub` is found, such that `sub` is contained
+        within [`start`, `end`].
+
+        See Also
+        --------
+        char.rfind
+
+        """
+        return rfind(self, sub, start, end)
+
+    def rindex(self, sub, start=0, end=None):
+        """
+        Like `rfind`, but raises `ValueError` when the substring `sub` is
+        not found.
+
+        See Also
+        --------
+        char.rindex
+
+        """
+        return rindex(self, sub, start, end)
+
+    def rjust(self, width, fillchar=' '):
+        """
+        Return an array with the elements of `self`
+        right-justified in a string of length `width`.
+
+        See Also
+        --------
+        char.rjust
+
+        """
+        return asarray(rjust(self, width, fillchar))
+
+    def rpartition(self, sep):
+        """
+        Partition each element in `self` around `sep`.
+
+        See Also
+        --------
+        rpartition
+        """
+        return asarray(rpartition(self, sep))
+
+    def rsplit(self, sep=None, maxsplit=None):
+        """
+        For each element in `self`, return a list of the words in
+        the string, using `sep` as the delimiter string.
+
+        See Also
+        --------
+        char.rsplit
+
+        """
+        return rsplit(self, sep, maxsplit)
 
     def rstrip(self, chars=None):
-        return self._generalmethod('rstrip', broadcast(self, chars))
+        """
+        For each element in `self`, return a copy with the trailing
+        characters removed.
+
+        See Also
+        --------
+        char.rstrip
+
+        """
+        return asarray(rstrip(self, chars))
 
     def split(self, sep=None, maxsplit=None):
-        return self._typedmethod('split', broadcast(self, sep, maxsplit), object)
+        """
+        For each element in `self`, return a list of the words in the
+        string, using `sep` as the delimiter string.
+
+        See Also
+        --------
+        char.split
+
+        """
+        return split(self, sep, maxsplit)
 
     def splitlines(self, keepends=None):
-        return self._typedmethod('splitlines', broadcast(self, keepends), object)
-
-    def startswith(self, prefix, start=None, end=None):
-        return self._typedmethod('startswith', broadcast(self, prefix, start, end), bool)
+        """
+        For each element in `self`, return a list of the lines in the
+        element, breaking at line boundaries.
+
+        See Also
+        --------
+        char.splitlines
+
+        """
+        return splitlines(self, keepends)
+
+    def startswith(self, prefix, start=0, end=None):
+        """
+        Returns a boolean array which is `True` where the string element
+        in `self` starts with `prefix`, otherwise `False`.
+
+        See Also
+        --------
+        char.startswith
+
+        """
+        return startswith(self, prefix, start, end)
 
     def strip(self, chars=None):
-        return self._generalmethod('strip', broadcast(self, chars))
+        """
+        For each element in `self`, return a copy with the leading and
+        trailing characters removed.
+
+        See Also
+        --------
+        char.strip
+
+        """
+        return asarray(strip(self, chars))
 
     def swapcase(self):
-        return self._samemethod('swapcase')
+        """
+        For each element in `self`, return a copy of the string with
+        uppercase characters converted to lowercase and vice versa.
+
+        See Also
+        --------
+        char.swapcase
+
+        """
+        return asarray(swapcase(self))
 
     def title(self):
-        return self._samemethod('title')
+        """
+        For each element in `self`, return a titlecased version of the
+        string: words start with uppercase characters, all remaining cased
+        characters are lowercase.
+
+        See Also
+        --------
+        char.title
+
+        """
+        return asarray(title(self))
 
     def translate(self, table, deletechars=None):
-        if self.dtype is unicode_:
-            return self._generalmethod('translate', broadcast(self, table))
-        else:
-            return self._generalmethod('translate', broadcast(self, table, deletechars))
+        """
+        For each element in `self`, return a copy of the string where
+        all characters occurring in the optional argument
+        `deletechars` are removed, and the remaining characters have
+        been mapped through the given translation table.
+
+        See Also
+        --------
+        char.translate
+
+        """
+        return asarray(translate(self, table, deletechars))
 
     def upper(self):
-        return self._samemethod('upper')
+        """
+        Return an array with the elements of `self` converted to
+        uppercase.
+
+        See Also
+        --------
+        char.upper
+
+        """
+        return asarray(upper(self))
 
     def zfill(self, width):
-        return self._generalmethod('zfill', broadcast(self, width))
-
-
-def array(obj, itemsize=None, copy=True, unicode=False, order=None):
-
-    if isinstance(obj, chararray):
+        """
+        Return the numeric string left-filled with zeros in a string of
+        length `width`.
+
+        See Also
+        --------
+        char.zfill
+
+        """
+        return asarray(zfill(self, width))
+
+    def isnumeric(self):
+        """
+        For each element in `self`, return True if there are only
+        numeric characters in the element.
+
+        See Also
+        --------
+        char.isnumeric
+
+        """
+        return isnumeric(self)
+
+    def isdecimal(self):
+        """
+        For each element in `self`, return True if there are only
+        decimal characters in the element.
+
+        See Also
+        --------
+        char.isdecimal
+
+        """
+        return isdecimal(self)
+
+
+@set_module("numpy.char")
+def array(obj, itemsize=None, copy=True, unicode=None, order=None):
+    """
+    Create a `chararray`.
+
+    .. note::
+       This class is provided for numarray backward-compatibility.
+       New code (not concerned with numarray compatibility) should use
+       arrays of type `string_` or `unicode_` and use the free functions
+       in :mod:`numpy.char <numpy.core.defchararray>` for fast
+       vectorized string operations instead.
+
+    Versus a regular NumPy array of type `str` or `unicode`, this
+    class adds the following functionality:
+
+      1) values automatically have whitespace removed from the end
+         when indexed
+
+      2) comparison operators automatically remove whitespace from the
+         end when comparing values
+
+      3) vectorized string operations are provided as methods
+         (e.g. `str.endswith`) and infix operators (e.g. ``+, *, %``)
+
+    Parameters
+    ----------
+    obj : array of str or unicode-like
+
+    itemsize : int, optional
+        `itemsize` is the number of characters per scalar in the
+        resulting array.  If `itemsize` is None, and `obj` is an
+        object array or a Python list, the `itemsize` will be
+        automatically determined.  If `itemsize` is provided and `obj`
+        is of type str or unicode, then the `obj` string will be
+        chunked into `itemsize` pieces.
+
+    copy : bool, optional
+        If true (default), then the object is copied.  Otherwise, a copy
+        will only be made if __array__ returns a copy, if obj is a
+        nested sequence, or if a copy is needed to satisfy any of the other
+        requirements (`itemsize`, unicode, `order`, etc.).
+
+    unicode : bool, optional
+        When true, the resulting `chararray` can contain Unicode
+        characters, when false only 8-bit characters.  If unicode is
+        None and `obj` is one of the following:
+
+          - a `chararray`,
+          - an ndarray of type `str` or `unicode`
+          - a Python str or unicode object,
+
+        then the unicode setting of the output array will be
+        automatically determined.
+
+    order : {'C', 'F', 'A'}, optional
+        Specify the order of the array.  If order is 'C' (default), then the
+        array will be in C-contiguous order (last-index varies the
+        fastest).  If order is 'F', then the returned array
+        will be in Fortran-contiguous order (first-index varies the
+        fastest).  If order is 'A', then the returned array may
+        be in any order (either C-, Fortran-contiguous, or even
+        discontiguous).
+    """
+    if isinstance(obj, (bytes, str)):
+        if unicode is None:
+            if isinstance(obj, str):
+                unicode = True
+            else:
+                unicode = False
+
+        if itemsize is None:
+            itemsize = len(obj)
+        shape = len(obj) // itemsize
+
+        return chararray(shape, itemsize=itemsize, unicode=unicode,
+                         buffer=obj, order=order)
+
+    if isinstance(obj, (list, tuple)):
+        obj = numpy.asarray(obj)
+
+    if isinstance(obj, ndarray) and issubclass(obj.dtype.type, character):
+        # If we just have a vanilla chararray, create a chararray
+        # view around it.
+        if not isinstance(obj, chararray):
+            obj = obj.view(chararray)
+
         if itemsize is None:
             itemsize = obj.itemsize
-        if copy or (itemsize != obj.itemsize) \
-           or (not unicode and obj.dtype == unicode_) \
-           or (unicode and obj.dtype == string):
-            return obj.astype("%s%d" % (obj.dtype.char, itemsize))
+            # itemsize is in 8-bit chars, so for Unicode, we need
+            # to divide by the size of a single Unicode character,
+            # which for NumPy is always 4
+            if issubclass(obj.dtype.type, unicode_):
+                itemsize //= 4
+
+        if unicode is None:
+            if issubclass(obj.dtype.type, unicode_):
+                unicode = True
+            else:
+                unicode = False
+
+        if unicode:
+            dtype = unicode_
         else:
-            return obj
-
-    if isinstance(obj, ndarray) and (obj.dtype in [unicode_, string]):
-        new = obj.view(chararray)
-        if unicode and obj.dtype == string:
-            return new.astype((unicode_, obj.itemsize))
-        elif obj.dtype == unicode_:
-            return new.astype((string, obj.itemsize))
-
-        if copy: return new.copy()
-        else: return new
-
-    if unicode: dtype = "U"
-    else: dtype = "S"
-
-    if itemsize is not None:
-        dtype += str(itemsize)
-
-    if isinstance(obj, (str, _unicode)):
+            dtype = string_
+
+        if order is not None:
+            obj = numpy.asarray(obj, order=order)
+        if (copy or
+                (itemsize != obj.itemsize) or
+                (not unicode and isinstance(obj, unicode_)) or
+                (unicode and isinstance(obj, string_))):
+            obj = obj.astype((dtype, int(itemsize)))
+        return obj
+
+    if isinstance(obj, ndarray) and issubclass(obj.dtype.type, object):
         if itemsize is None:
-            itemsize = len(obj)
-        shape = len(obj) / itemsize
-        return chararray(shape, itemsize=itemsize, unicode=unicode,
-                         buffer=obj)
-
-    # default
-    val = narray(obj, dtype=dtype, order=order, subok=1)
-
+            # Since no itemsize was specified, convert the input array to
+            # a list so the ndarray constructor will automatically
+            # determine the itemsize for us.
+            obj = obj.tolist()
+            # Fall through to the default case
+
+    if unicode:
+        dtype = unicode_
+    else:
+        dtype = string_
+
+    if itemsize is None:
+        val = narray(obj, dtype=dtype, order=order, subok=True)
+    else:
+        val = narray(obj, dtype=(dtype, itemsize), order=order, subok=True)
     return val.view(chararray)
 
-def asarray(obj, itemsize=None, unicode=False, order=None):
+
+@set_module("numpy.char")
+def asarray(obj, itemsize=None, unicode=None, order=None):
+    """
+    Convert the input to a `chararray`, copying the data only if
+    necessary.
+
+    Versus a regular NumPy array of type `str` or `unicode`, this
+    class adds the following functionality:
+
+      1) values automatically have whitespace removed from the end
+         when indexed
+
+      2) comparison operators automatically remove whitespace from the
+         end when comparing values
+
+      3) vectorized string operations are provided as methods
+         (e.g. `str.endswith`) and infix operators (e.g. ``+``, ``*``,``%``)
+
+    Parameters
+    ----------
+    obj : array of str or unicode-like
+
+    itemsize : int, optional
+        `itemsize` is the number of characters per scalar in the
+        resulting array.  If `itemsize` is None, and `obj` is an
+        object array or a Python list, the `itemsize` will be
+        automatically determined.  If `itemsize` is provided and `obj`
+        is of type str or unicode, then the `obj` string will be
+        chunked into `itemsize` pieces.
+
+    unicode : bool, optional
+        When true, the resulting `chararray` can contain Unicode
+        characters, when false only 8-bit characters.  If unicode is
+        None and `obj` is one of the following:
+
+          - a `chararray`,
+          - an ndarray of type `str` or 'unicode`
+          - a Python str or unicode object,
+
+        then the unicode setting of the output array will be
+        automatically determined.
+
+    order : {'C', 'F'}, optional
+        Specify the order of the array.  If order is 'C' (default), then the
+        array will be in C-contiguous order (last-index varies the
+        fastest).  If order is 'F', then the returned array
+        will be in Fortran-contiguous order (first-index varies the
+        fastest).
+    """
     return array(obj, itemsize, copy=False,
                  unicode=unicode, order=order)
('numpy/core', 'setup.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,306 +1,1208 @@
-
-import imp
 import os
 import sys
+import pickle
+import copy
+import warnings
+import platform
+import textwrap
+import glob
 from os.path import join
-from glob import glob
-from distutils.dep_util import newer,newer_group
+
+from numpy.distutils import log
+from distutils.dep_util import newer
+from sysconfig import get_config_var
+from numpy.compat import npy_load_module
+from setup_common import *  # noqa: F403
+
+# Set to True to enable relaxed strides checking. This (mostly) means
+# that `strides[dim]` is ignored if `shape[dim] == 1` when setting flags.
+NPY_RELAXED_STRIDES_CHECKING = (os.environ.get('NPY_RELAXED_STRIDES_CHECKING', "1") != "0")
+if not NPY_RELAXED_STRIDES_CHECKING:
+    raise SystemError(
+        "Support for NPY_RELAXED_STRIDES_CHECKING=0 has been remove as of "
+        "NumPy 1.23.  This error will eventually be removed entirely.")
+
+# Put NPY_RELAXED_STRIDES_DEBUG=1 in the environment if you want numpy to use a
+# bogus value for affected strides in order to help smoke out bad stride usage
+# when relaxed stride checking is enabled.
+NPY_RELAXED_STRIDES_DEBUG = (os.environ.get('NPY_RELAXED_STRIDES_DEBUG', "0") != "0")
+NPY_RELAXED_STRIDES_DEBUG = NPY_RELAXED_STRIDES_DEBUG and NPY_RELAXED_STRIDES_CHECKING
+
+# Set NPY_DISABLE_SVML=1 in the environment to disable the vendored SVML
+# library. This option only has significance on a Linux x86_64 host and is most
+# useful to avoid improperly requiring SVML when cross compiling.
+NPY_DISABLE_SVML = (os.environ.get('NPY_DISABLE_SVML', "0") == "1")
+
+# XXX: ugly, we use a class to avoid calling twice some expensive functions in
+# config.h/numpyconfig.h. I don't see a better way because distutils force
+# config.h generation inside an Extension class, and as such sharing
+# configuration information between extensions is not easy.
+# Using a pickled-based memoize does not work because config_cmd is an instance
+# method, which cPickle does not like.
+#
+# Use pickle in all cases, as cPickle is gone in python3 and the difference
+# in time is only in build. -- Charles Harris, 2013-03-30
+
+class CallOnceOnly:
+    def __init__(self):
+        self._check_types = None
+        self._check_ieee_macros = None
+        self._check_complex = None
+
+    def check_types(self, *a, **kw):
+        if self._check_types is None:
+            out = check_types(*a, **kw)
+            self._check_types = pickle.dumps(out)
+        else:
+            out = copy.deepcopy(pickle.loads(self._check_types))
+        return out
+
+    def check_ieee_macros(self, *a, **kw):
+        if self._check_ieee_macros is None:
+            out = check_ieee_macros(*a, **kw)
+            self._check_ieee_macros = pickle.dumps(out)
+        else:
+            out = copy.deepcopy(pickle.loads(self._check_ieee_macros))
+        return out
+
+    def check_complex(self, *a, **kw):
+        if self._check_complex is None:
+            out = check_complex(*a, **kw)
+            self._check_complex = pickle.dumps(out)
+        else:
+            out = copy.deepcopy(pickle.loads(self._check_complex))
+        return out
+
+def can_link_svml():
+    """SVML library is supported only on x86_64 architecture and currently
+    only on linux
+    """
+    if NPY_DISABLE_SVML:
+        return False
+    machine = platform.machine()
+    system = platform.system()
+    return "x86_64" in machine and system == "Linux"
+
+def check_svml_submodule(svmlpath):
+    if not os.path.exists(svmlpath + "/README.md"):
+        raise RuntimeError("Missing `SVML` submodule! Run `git submodule "
+                           "update --init` to fix this.")
+    return True
+
+def pythonlib_dir():
+    """return path where libpython* is."""
+    if sys.platform == 'win32':
+        return os.path.join(sys.prefix, "libs")
+    else:
+        return get_config_var('LIBDIR')
+
+def is_npy_no_signal():
+    """Return True if the NPY_NO_SIGNAL symbol must be defined in configuration
+    header."""
+    return sys.platform == 'win32'
+
+def is_npy_no_smp():
+    """Return True if the NPY_NO_SMP symbol must be defined in public
+    header (when SMP support cannot be reliably enabled)."""
+    # Perhaps a fancier check is in order here.
+    #  so that threads are only enabled if there
+    #  are actually multiple CPUS? -- but
+    #  threaded code can be nice even on a single
+    #  CPU so that long-calculating code doesn't
+    #  block.
+    return 'NPY_NOSMP' in os.environ
+
+def win32_checks(deflist):
+    from numpy.distutils.misc_util import get_build_architecture
+    a = get_build_architecture()
+
+    # Distutils hack on AMD64 on windows
+    print('BUILD_ARCHITECTURE: %r, os.name=%r, sys.platform=%r' %
+          (a, os.name, sys.platform))
+    if a == 'AMD64':
+        deflist.append('DISTUTILS_USE_SDK')
+
+    # On win32, force long double format string to be 'g', not
+    # 'Lg', since the MS runtime does not support long double whose
+    # size is > sizeof(double)
+    if a == "Intel" or a == "AMD64":
+        deflist.append('FORCE_NO_LONG_DOUBLE_FORMATTING')
+
+def check_math_capabilities(config, ext, moredefs, mathlibs):
+    def check_func(
+        func_name,
+        decl=False,
+        headers=["feature_detection_math.h"],
+    ):
+        return config.check_func(
+            func_name,
+            libraries=mathlibs,
+            decl=decl,
+            call=True,
+            call_args=FUNC_CALL_ARGS[func_name],
+            headers=headers,
+        )
+
+    def check_funcs_once(funcs_name, headers=["feature_detection_math.h"]):
+        call = dict([(f, True) for f in funcs_name])
+        call_args = dict([(f, FUNC_CALL_ARGS[f]) for f in funcs_name])
+        st = config.check_funcs_once(
+            funcs_name,
+            libraries=mathlibs,
+            decl=False,
+            call=call,
+            call_args=call_args,
+            headers=headers,
+        )
+        if st:
+            moredefs.extend([(fname2def(f), 1) for f in funcs_name])
+        return st
+
+    def check_funcs(funcs_name, headers=["feature_detection_math.h"]):
+        # Use check_funcs_once first, and if it does not work, test func per
+        # func. Return success only if all the functions are available
+        if not check_funcs_once(funcs_name, headers=headers):
+            # Global check failed, check func per func
+            for f in funcs_name:
+                if check_func(f, headers=headers):
+                    moredefs.append((fname2def(f), 1))
+            return 0
+        else:
+            return 1
+
+    #use_msvc = config.check_decl("_MSC_VER")
+    if not check_funcs_once(MANDATORY_FUNCS):
+        raise SystemError("One of the required function to build numpy is not"
+                " available (the list is %s)." % str(MANDATORY_FUNCS))
+
+    # Standard functions which may not be available and for which we have a
+    # replacement implementation. Note that some of these are C99 functions.
+
+    # XXX: hack to circumvent cpp pollution from python: python put its
+    # config.h in the public namespace, so we have a clash for the common
+    # functions we test. We remove every function tested by python's
+    # autoconf, hoping their own test are correct
+    for f in OPTIONAL_STDFUNCS_MAYBE:
+        if config.check_decl(fname2def(f),
+                    headers=["Python.h", "math.h"]):
+            if f in OPTIONAL_STDFUNCS:
+                OPTIONAL_STDFUNCS.remove(f)
+            else:
+                OPTIONAL_FILE_FUNCS.remove(f)
+
+
+    check_funcs(OPTIONAL_STDFUNCS)
+    check_funcs(OPTIONAL_FILE_FUNCS, headers=["feature_detection_stdio.h"])
+    check_funcs(OPTIONAL_MISC_FUNCS, headers=["feature_detection_misc.h"])
+    
+
+
+    for h in OPTIONAL_HEADERS:
+        if config.check_func("", decl=False, call=False, headers=[h]):
+            h = h.replace(".", "_").replace(os.path.sep, "_")
+            moredefs.append((fname2def(h), 1))
+
+    # Try with both "locale.h" and "xlocale.h"
+    locale_headers = [
+        "stdlib.h",
+        "xlocale.h",
+        "feature_detection_locale.h",
+    ]
+    if not check_funcs(OPTIONAL_LOCALE_FUNCS, headers=locale_headers):
+        # It didn't work with xlocale.h, maybe it will work with locale.h?
+        locale_headers[1] = "locale.h"
+        check_funcs(OPTIONAL_LOCALE_FUNCS, headers=locale_headers)
+
+    for tup in OPTIONAL_INTRINSICS:
+        headers = None
+        if len(tup) == 2:
+            f, args, m = tup[0], tup[1], fname2def(tup[0])
+        elif len(tup) == 3:
+            f, args, headers, m = tup[0], tup[1], [tup[2]], fname2def(tup[0])
+        else:
+            f, args, headers, m = tup[0], tup[1], [tup[2]], fname2def(tup[3])
+        if config.check_func(f, decl=False, call=True, call_args=args,
+                             headers=headers):
+            moredefs.append((m, 1))
+
+    for dec, fn in OPTIONAL_FUNCTION_ATTRIBUTES:
+        if config.check_gcc_function_attribute(dec, fn):
+            moredefs.append((fname2def(fn), 1))
+            if fn == 'attribute_target_avx512f':
+                # GH-14787: Work around GCC<8.4 bug when compiling with AVX512
+                # support on Windows-based platforms
+                if (sys.platform in ('win32', 'cygwin') and
+                        config.check_compiler_gcc() and
+                        not config.check_gcc_version_at_least(8, 4)):
+                    ext.extra_compile_args.extend(
+                            ['-ffixed-xmm%s' % n for n in range(16, 32)])
+
+    for dec, fn, code, header in OPTIONAL_FUNCTION_ATTRIBUTES_WITH_INTRINSICS:
+        if config.check_gcc_function_attribute_with_intrinsics(dec, fn, code,
+                                                               header):
+            moredefs.append((fname2def(fn), 1))
+
+    for fn in OPTIONAL_VARIABLE_ATTRIBUTES:
+        if config.check_gcc_variable_attribute(fn):
+            m = fn.replace("(", "_").replace(")", "_")
+            moredefs.append((fname2def(m), 1))
+
+    # C99 functions: float and long double versions
+    check_funcs(C99_FUNCS_SINGLE)
+    check_funcs(C99_FUNCS_EXTENDED)
+
+def check_complex(config, mathlibs):
+    priv = []
+    pub = []
+
+    try:
+        if os.uname()[0] == "Interix":
+            warnings.warn("Disabling broken complex support. See #1365", stacklevel=2)
+            return priv, pub
+    except Exception:
+        # os.uname not available on all platforms. blanket except ugly but safe
+        pass
+
+    # Check for complex support
+    st = config.check_header('complex.h')
+    if st:
+        priv.append(('HAVE_COMPLEX_H', 1))
+        pub.append(('NPY_USE_C99_COMPLEX', 1))
+
+        for t in C99_COMPLEX_TYPES:
+            st = config.check_type(t, headers=["complex.h"])
+            if st:
+                pub.append(('NPY_HAVE_%s' % type2def(t), 1))
+
+        def check_prec(prec):
+            flist = [f + prec for f in C99_COMPLEX_FUNCS]
+            decl = dict([(f, True) for f in flist])
+            if not config.check_funcs_once(flist, call=decl, decl=decl,
+                                           libraries=mathlibs):
+                for f in flist:
+                    if config.check_func(f, call=True, decl=True,
+                                         libraries=mathlibs):
+                        priv.append((fname2def(f), 1))
+            else:
+                priv.extend([(fname2def(f), 1) for f in flist])
+
+        check_prec('')
+        check_prec('f')
+        check_prec('l')
+
+    return priv, pub
+
+def check_ieee_macros(config):
+    priv = []
+    pub = []
+
+    macros = []
+
+    def _add_decl(f):
+        priv.append(fname2def("decl_%s" % f))
+        pub.append('NPY_%s' % fname2def("decl_%s" % f))
+
+    # XXX: hack to circumvent cpp pollution from python: python put its
+    # config.h in the public namespace, so we have a clash for the common
+    # functions we test. We remove every function tested by python's
+    # autoconf, hoping their own test are correct
+    _macros = ["isnan", "isinf", "signbit", "isfinite"]
+    for f in _macros:
+        py_symbol = fname2def("decl_%s" % f)
+        already_declared = config.check_decl(py_symbol,
+                headers=["Python.h", "math.h"])
+        if already_declared:
+            if config.check_macro_true(py_symbol,
+                    headers=["Python.h", "math.h"]):
+                pub.append('NPY_%s' % fname2def("decl_%s" % f))
+        else:
+            macros.append(f)
+    # Normally, isnan and isinf are macro (C99), but some platforms only have
+    # func, or both func and macro version. Check for macro only, and define
+    # replacement ones if not found.
+    # Note: including Python.h is necessary because it modifies some math.h
+    # definitions
+    for f in macros:
+        st = config.check_decl(f, headers=["Python.h", "math.h"])
+        if st:
+            _add_decl(f)
+
+    return priv, pub
+
+def check_types(config_cmd, ext, build_dir):
+    private_defines = []
+    public_defines = []
+
+    # Expected size (in number of bytes) for each type. This is an
+    # optimization: those are only hints, and an exhaustive search for the size
+    # is done if the hints are wrong.
+    expected = {'short': [2], 'int': [4], 'long': [8, 4],
+                'float': [4], 'double': [8], 'long double': [16, 12, 8],
+                'Py_intptr_t': [8, 4], 'PY_LONG_LONG': [8], 'long long': [8],
+                'off_t': [8, 4]}
+
+    # Check we have the python header (-dev* packages on Linux)
+    result = config_cmd.check_header('Python.h')
+    if not result:
+        python = 'python'
+        if '__pypy__' in sys.builtin_module_names:
+            python = 'pypy'
+        raise SystemError(
+                "Cannot compile 'Python.h'. Perhaps you need to "
+                "install {0}-dev|{0}-devel.".format(python))
+    res = config_cmd.check_header("endian.h")
+    if res:
+        private_defines.append(('HAVE_ENDIAN_H', 1))
+        public_defines.append(('NPY_HAVE_ENDIAN_H', 1))
+    res = config_cmd.check_header("sys/endian.h")
+    if res:
+        private_defines.append(('HAVE_SYS_ENDIAN_H', 1))
+        public_defines.append(('NPY_HAVE_SYS_ENDIAN_H', 1))
+
+    # Check basic types sizes
+    for type in ('short', 'int', 'long'):
+        res = config_cmd.check_decl("SIZEOF_%s" % sym2def(type), headers=["Python.h"])
+        if res:
+            public_defines.append(('NPY_SIZEOF_%s' % sym2def(type), "SIZEOF_%s" % sym2def(type)))
+        else:
+            res = config_cmd.check_type_size(type, expected=expected[type])
+            if res >= 0:
+                public_defines.append(('NPY_SIZEOF_%s' % sym2def(type), '%d' % res))
+            else:
+                raise SystemError("Checking sizeof (%s) failed !" % type)
+
+    for type in ('float', 'double', 'long double'):
+        already_declared = config_cmd.check_decl("SIZEOF_%s" % sym2def(type),
+                                                 headers=["Python.h"])
+        res = config_cmd.check_type_size(type, expected=expected[type])
+        if res >= 0:
+            public_defines.append(('NPY_SIZEOF_%s' % sym2def(type), '%d' % res))
+            if not already_declared and not type == 'long double':
+                private_defines.append(('SIZEOF_%s' % sym2def(type), '%d' % res))
+        else:
+            raise SystemError("Checking sizeof (%s) failed !" % type)
+
+        # Compute size of corresponding complex type: used to check that our
+        # definition is binary compatible with C99 complex type (check done at
+        # build time in npy_common.h)
+        complex_def = "struct {%s __x; %s __y;}" % (type, type)
+        res = config_cmd.check_type_size(complex_def,
+                                         expected=[2 * x for x in expected[type]])
+        if res >= 0:
+            public_defines.append(('NPY_SIZEOF_COMPLEX_%s' % sym2def(type), '%d' % res))
+        else:
+            raise SystemError("Checking sizeof (%s) failed !" % complex_def)
+
+    for type in ('Py_intptr_t', 'off_t'):
+        res = config_cmd.check_type_size(type, headers=["Python.h"],
+                library_dirs=[pythonlib_dir()],
+                expected=expected[type])
+
+        if res >= 0:
+            private_defines.append(('SIZEOF_%s' % sym2def(type), '%d' % res))
+            public_defines.append(('NPY_SIZEOF_%s' % sym2def(type), '%d' % res))
+        else:
+            raise SystemError("Checking sizeof (%s) failed !" % type)
+
+    # We check declaration AND type because that's how distutils does it.
+    if config_cmd.check_decl('PY_LONG_LONG', headers=['Python.h']):
+        res = config_cmd.check_type_size('PY_LONG_LONG',  headers=['Python.h'],
+                library_dirs=[pythonlib_dir()],
+                expected=expected['PY_LONG_LONG'])
+        if res >= 0:
+            private_defines.append(('SIZEOF_%s' % sym2def('PY_LONG_LONG'), '%d' % res))
+            public_defines.append(('NPY_SIZEOF_%s' % sym2def('PY_LONG_LONG'), '%d' % res))
+        else:
+            raise SystemError("Checking sizeof (%s) failed !" % 'PY_LONG_LONG')
+
+        res = config_cmd.check_type_size('long long',
+                expected=expected['long long'])
+        if res >= 0:
+            #private_defines.append(('SIZEOF_%s' % sym2def('long long'), '%d' % res))
+            public_defines.append(('NPY_SIZEOF_%s' % sym2def('long long'), '%d' % res))
+        else:
+            raise SystemError("Checking sizeof (%s) failed !" % 'long long')
+
+    if not config_cmd.check_decl('CHAR_BIT', headers=['Python.h']):
+        raise RuntimeError(
+            "Config wo CHAR_BIT is not supported"
+            ", please contact the maintainers")
+
+    return private_defines, public_defines
+
+def check_mathlib(config_cmd):
+    # Testing the C math library
+    mathlibs = []
+    mathlibs_choices = [[], ["m"], ["cpml"]]
+    mathlib = os.environ.get("MATHLIB")
+    if mathlib:
+        mathlibs_choices.insert(0, mathlib.split(","))
+    for libs in mathlibs_choices:
+        if config_cmd.check_func(
+            "log",
+            libraries=libs,
+            call_args="0",
+            decl="double log(double);",
+            call=True
+        ):
+            mathlibs = libs
+            break
+    else:
+        raise RuntimeError(
+            "math library missing; rerun setup.py after setting the "
+            "MATHLIB env variable"
+        )
+    return mathlibs
+
+
+def visibility_define(config):
+    """Return the define value to use for NPY_VISIBILITY_HIDDEN (may be empty
+    string)."""
+    hide = '__attribute__((visibility("hidden")))'
+    if config.check_gcc_function_attribute(hide, 'hideme'):
+        return hide
+    else:
+        return ''
 
 def configuration(parent_package='',top_path=None):
-    from numpy.distutils.misc_util import Configuration,dot_join
-    from numpy.distutils.system_info import get_info, default_lib_dirs
-
-    config = Configuration('core',parent_package,top_path)
+    from numpy.distutils.misc_util import (Configuration, dot_join,
+                                           exec_mod_from_location)
+    from numpy.distutils.system_info import (get_info, blas_opt_info,
+                                             lapack_opt_info)
+    from numpy.distutils.ccompiler_opt import NPY_CXX_FLAGS
+    from numpy.version import release as is_released
+
+    config = Configuration('core', parent_package, top_path)
     local_dir = config.local_path
-    codegen_dir = join(local_dir,'code_generators')
-
-    generate_umath_py = join(codegen_dir,'generate_umath.py')
-    n = dot_join(config.name,'generate_umath')
-    generate_umath = imp.load_module('_'.join(n.split('.')),
-                                     open(generate_umath_py,'U'),generate_umath_py,
-                                     ('.py','U',1))
-
-    header_dir = 'include/numpy' # this is relative to config.path_in_package
+    codegen_dir = join(local_dir, 'code_generators')
+
+    if is_released:
+        warnings.simplefilter('error', MismatchCAPIWarning)
+
+    # Check whether we have a mismatch between the set C API VERSION and the
+    # actual C API VERSION
+    check_api_version(C_API_VERSION, codegen_dir)
+
+    generate_umath_py = join(codegen_dir, 'generate_umath.py')
+    n = dot_join(config.name, 'generate_umath')
+    generate_umath = exec_mod_from_location('_'.join(n.split('.')),
+                                            generate_umath_py)
+
+    header_dir = 'include/numpy'  # this is relative to config.path_in_package
+
+    cocache = CallOnceOnly()
 
     def generate_config_h(ext, build_dir):
-        target = join(build_dir,'config.h')
-        if newer(__file__,target):
+        target = join(build_dir, header_dir, 'config.h')
+        d = os.path.dirname(target)
+        if not os.path.exists(d):
+            os.makedirs(d)
+
+        if newer(__file__, target):
             config_cmd = config.get_config_cmd()
-            print 'Generating',target
-            #
-            tc = generate_testcode(target)
-            from distutils import sysconfig
-            python_include = sysconfig.get_python_inc()
-            result = config_cmd.try_run(tc,include_dirs=[python_include],
-                                        library_dirs = default_lib_dirs)
-            if not result:
-                raise "ERROR: Failed to test configuration"
-            moredefs = []
-
-            #
-            mathlibs = []
-            tc = testcode_mathlib()
-            mathlibs_choices = [[],['m'],['cpml']]
-            mathlib = os.environ.get('MATHLIB')
-            if mathlib:
-                mathlibs_choices.insert(0,mathlib.split(','))
-            for libs in mathlibs_choices:
-                if config_cmd.try_run(tc,libraries=libs):
-                    mathlibs = libs
-                    break
+            log.info('Generating %s', target)
+
+            # Check sizeof
+            moredefs, ignored = cocache.check_types(config_cmd, ext, build_dir)
+
+            # Check math library and C99 math funcs availability
+            mathlibs = check_mathlib(config_cmd)
+            moredefs.append(('MATHLIB', ','.join(mathlibs)))
+
+            check_math_capabilities(config_cmd, ext, moredefs, mathlibs)
+            moredefs.extend(cocache.check_ieee_macros(config_cmd)[0])
+            moredefs.extend(cocache.check_complex(config_cmd, mathlibs)[0])
+
+            # Signal check
+            if is_npy_no_signal():
+                moredefs.append('__NPY_PRIVATE_NO_SIGNAL')
+
+            # Windows checks
+            if sys.platform == 'win32' or os.name == 'nt':
+                win32_checks(moredefs)
+
+            # C99 restrict keyword
+            moredefs.append(('NPY_RESTRICT', config_cmd.check_restrict()))
+
+            # Inline check
+            inline = config_cmd.check_inline()
+
+            if can_link_svml():
+                moredefs.append(('NPY_CAN_LINK_SVML', 1))
+
+            # Use bogus stride debug aid to flush out bugs where users use
+            # strides of dimensions with length 1 to index a full contiguous
+            # array.
+            if NPY_RELAXED_STRIDES_DEBUG:
+                moredefs.append(('NPY_RELAXED_STRIDES_DEBUG', 1))
             else:
-                raise "math library missing; rerun setup.py after setting the MATHLIB env variable"
-            ext.libraries.extend(mathlibs)
-            moredefs.append(('MATHLIB',','.join(mathlibs)))
-
-            libs = mathlibs
-            kws_args = {'libraries':libs,'decl':0,'headers':['math.h']}
-            if config_cmd.check_func('expl', **kws_args):
-                moredefs.append('HAVE_LONGDOUBLE_FUNCS')
-            if config_cmd.check_func('expf', **kws_args):
-                moredefs.append('HAVE_FLOAT_FUNCS')
-            if config_cmd.check_func('log1p', **kws_args):
-                moredefs.append('HAVE_LOG1P')
-            if config_cmd.check_func('expm1', **kws_args):
-                moredefs.append('HAVE_EXPM1')
-            if config_cmd.check_func('asinh', **kws_args):
-                moredefs.append('HAVE_INVERSE_HYPERBOLIC')
-            if config_cmd.check_func('atanhf', **kws_args):
-                moredefs.append('HAVE_INVERSE_HYPERBOLIC_FLOAT')
-            if config_cmd.check_func('atanhl', **kws_args):
-                moredefs.append('HAVE_INVERSE_HYPERBOLIC_LONGDOUBLE')
-            if config_cmd.check_func('isnan', **kws_args):
-                moredefs.append('HAVE_ISNAN')
-            if config_cmd.check_func('isinf', **kws_args):
-                moredefs.append('HAVE_ISINF')
-            if config_cmd.check_func('rint', **kws_args):
-                moredefs.append('HAVE_RINT')
-
-            if sys.version[:3] < '2.4':
-                kws_args['headers'].append('stdlib.h')
-                if config_cmd.check_func('strtod', **kws_args):
-                    moredefs.append(('PyOS_ascii_strtod', 'strtod'))
-
-            if moredefs:
-                target_f = open(target,'a')
+                moredefs.append(('NPY_RELAXED_STRIDES_DEBUG', 0))
+
+            # Get long double representation
+            rep = check_long_double_representation(config_cmd)
+            moredefs.append(('HAVE_LDOUBLE_%s' % rep, 1))
+
+            if check_for_right_shift_internal_compiler_error(config_cmd):
+                moredefs.append('NPY_DO_NOT_OPTIMIZE_LONG_right_shift')
+                moredefs.append('NPY_DO_NOT_OPTIMIZE_ULONG_right_shift')
+                moredefs.append('NPY_DO_NOT_OPTIMIZE_LONGLONG_right_shift')
+                moredefs.append('NPY_DO_NOT_OPTIMIZE_ULONGLONG_right_shift')
+
+            # Generate the config.h file from moredefs
+            with open(target, 'w') as target_f:
                 for d in moredefs:
-                    if isinstance(d,str):
+                    if isinstance(d, str):
                         target_f.write('#define %s\n' % (d))
                     else:
-                        target_f.write('#define %s %s\n' % (d[0],d[1]))
-                target_f.close()
+                        target_f.write('#define %s %s\n' % (d[0], d[1]))
+
+                # define inline to our keyword, or nothing
+                target_f.write('#ifndef __cplusplus\n')
+                if inline == 'inline':
+                    target_f.write('/* #undef inline */\n')
+                else:
+                    target_f.write('#define inline %s\n' % inline)
+                target_f.write('#endif\n')
+
+                # add the guard to make sure config.h is never included directly,
+                # but always through npy_config.h
+                target_f.write(textwrap.dedent("""
+                    #ifndef NUMPY_CORE_SRC_COMMON_NPY_CONFIG_H_
+                    #error config.h should never be included directly, include npy_config.h instead
+                    #endif
+                    """))
+
+            log.info('File: %s' % target)
+            with open(target) as target_f:
+                log.info(target_f.read())
+            log.info('EOF')
         else:
             mathlibs = []
-            target_f = open(target)
-            for line in target_f.readlines():
-                s = '#define MATHLIB'
-                if line.startswith(s):
-                    value = line[len(s):].strip()
-                    if value:
-                        mathlibs.extend(value.split(','))
-            target_f.close()
-
-        ext.libraries.extend(mathlibs)
+            with open(target) as target_f:
+                for line in target_f:
+                    s = '#define MATHLIB'
+                    if line.startswith(s):
+                        value = line[len(s):].strip()
+                        if value:
+                            mathlibs.extend(value.split(','))
+
+        # Ugly: this can be called within a library and not an extension,
+        # in which case there is no libraries attributes (and none is
+        # needed).
+        if hasattr(ext, 'libraries'):
+            ext.libraries.extend(mathlibs)
 
         incl_dir = os.path.dirname(target)
         if incl_dir not in config.numpy_include_dirs:
             config.numpy_include_dirs.append(incl_dir)
 
-        config.add_data_files((header_dir,target))
         return target
 
-    def generate_api_func(header_file, module_name):
-        def generate_api(ext,build_dir):
-            target = join(build_dir, header_file)
+    def generate_numpyconfig_h(ext, build_dir):
+        """Depends on config.h: generate_config_h has to be called before !"""
+        # put common include directory in build_dir on search path
+        # allows using code generation in headers
+        config.add_include_dirs(join(build_dir, "src", "common"))
+        config.add_include_dirs(join(build_dir, "src", "npymath"))
+
+        target = join(build_dir, header_dir, '_numpyconfig.h')
+        d = os.path.dirname(target)
+        if not os.path.exists(d):
+            os.makedirs(d)
+        if newer(__file__, target):
+            config_cmd = config.get_config_cmd()
+            log.info('Generating %s', target)
+
+            # Check sizeof
+            ignored, moredefs = cocache.check_types(config_cmd, ext, build_dir)
+
+            if is_npy_no_signal():
+                moredefs.append(('NPY_NO_SIGNAL', 1))
+
+            if is_npy_no_smp():
+                moredefs.append(('NPY_NO_SMP', 1))
+            else:
+                moredefs.append(('NPY_NO_SMP', 0))
+
+            mathlibs = check_mathlib(config_cmd)
+            moredefs.extend(cocache.check_ieee_macros(config_cmd)[1])
+            moredefs.extend(cocache.check_complex(config_cmd, mathlibs)[1])
+
+            if NPY_RELAXED_STRIDES_DEBUG:
+                moredefs.append(('NPY_RELAXED_STRIDES_DEBUG', 1))
+
+            # Check whether we can use inttypes (C99) formats
+            if config_cmd.check_decl('PRIdPTR', headers=['inttypes.h']):
+                moredefs.append(('NPY_USE_C99_FORMATS', 1))
+
+            # visibility check
+            hidden_visibility = visibility_define(config_cmd)
+            moredefs.append(('NPY_VISIBILITY_HIDDEN', hidden_visibility))
+
+            # Add the C API/ABI versions
+            moredefs.append(('NPY_ABI_VERSION', '0x%.8X' % C_ABI_VERSION))
+            moredefs.append(('NPY_API_VERSION', '0x%.8X' % C_API_VERSION))
+
+            # Add moredefs to header
+            with open(target, 'w') as target_f:
+                for d in moredefs:
+                    if isinstance(d, str):
+                        target_f.write('#define %s\n' % (d))
+                    else:
+                        target_f.write('#define %s %s\n' % (d[0], d[1]))
+
+                # Define __STDC_FORMAT_MACROS
+                target_f.write(textwrap.dedent("""
+                    #ifndef __STDC_FORMAT_MACROS
+                    #define __STDC_FORMAT_MACROS 1
+                    #endif
+                    """))
+
+            # Dump the numpyconfig.h header to stdout
+            log.info('File: %s' % target)
+            with open(target) as target_f:
+                log.info(target_f.read())
+            log.info('EOF')
+        config.add_data_files((header_dir, target))
+        return target
+
+    def generate_api_func(module_name):
+        def generate_api(ext, build_dir):
             script = join(codegen_dir, module_name + '.py')
-            if newer(script, target):
-                sys.path.insert(0, codegen_dir)
-                try:
-                    m = __import__(module_name)
-                    print 'executing',script
-                    m.generate_api(build_dir)
-                finally:
-                    del sys.path[0]
-            config.add_data_files((header_dir,target))
-            return target
+            sys.path.insert(0, codegen_dir)
+            try:
+                m = __import__(module_name)
+                log.info('executing %s', script)
+                h_file, c_file, doc_file = m.generate_api(os.path.join(build_dir, header_dir))
+            finally:
+                del sys.path[0]
+            config.add_data_files((header_dir, h_file),
+                                  (header_dir, doc_file))
+            return (h_file,)
         return generate_api
 
-    generate_array_api = generate_api_func('__multiarray_api.h',
-                                           'generate_array_api')
-    generate_ufunc_api = generate_api_func('__ufunc_api.h',
-                                           'generate_ufunc_api')
-
-    def generate_umath_c(ext,build_dir):
-        target = join(build_dir,'__umath_generated.c')
+    generate_numpy_api = generate_api_func('generate_numpy_api')
+    generate_ufunc_api = generate_api_func('generate_ufunc_api')
+
+    config.add_include_dirs(join(local_dir, "src", "common"))
+    config.add_include_dirs(join(local_dir, "src"))
+    config.add_include_dirs(join(local_dir))
+
+    config.add_data_dir('include/numpy')
+    config.add_include_dirs(join('src', 'npymath'))
+    config.add_include_dirs(join('src', 'multiarray'))
+    config.add_include_dirs(join('src', 'umath'))
+    config.add_include_dirs(join('src', 'npysort'))
+    config.add_include_dirs(join('src', '_simd'))
+
+    config.add_define_macros([("NPY_INTERNAL_BUILD", "1")]) # this macro indicates that Numpy build is in process
+    config.add_define_macros([("HAVE_NPY_CONFIG_H", "1")])
+    if sys.platform[:3] == "aix":
+        config.add_define_macros([("_LARGE_FILES", None)])
+    else:
+        config.add_define_macros([("_FILE_OFFSET_BITS", "64")])
+        config.add_define_macros([('_LARGEFILE_SOURCE', '1')])
+        config.add_define_macros([('_LARGEFILE64_SOURCE', '1')])
+
+    config.numpy_include_dirs.extend(config.paths('include'))
+
+    deps = [join('src', 'npymath', '_signbit.c'),
+            join('include', 'numpy', '*object.h'),
+            join(codegen_dir, 'genapi.py'),
+            ]
+
+    #######################################################################
+    #                          npymath library                            #
+    #######################################################################
+
+    subst_dict = dict([("sep", os.path.sep), ("pkgname", "numpy.core")])
+
+    def get_mathlib_info(*args):
+        # Another ugly hack: the mathlib info is known once build_src is run,
+        # but we cannot use add_installed_pkg_config here either, so we only
+        # update the substitution dictionary during npymath build
+        config_cmd = config.get_config_cmd()
+        # Check that the toolchain works, to fail early if it doesn't
+        # (avoid late errors with MATHLIB which are confusing if the
+        # compiler does not work).
+        for lang, test_code, note in (
+            ('c', 'int main(void) { return 0;}', ''),
+            ('c++', (
+                    'int main(void)'
+                    '{ auto x = 0.0; return static_cast<int>(x); }'
+                ), (
+                    'note: A compiler with support for C++11 language '
+                    'features is required.'
+                )
+             ),
+        ):
+            is_cpp = lang == 'c++'
+            if is_cpp:
+                # this a workaround to get rid of invalid c++ flags
+                # without doing big changes to config.
+                # c tested first, compiler should be here
+                bk_c = config_cmd.compiler
+                config_cmd.compiler = bk_c.cxx_compiler()
+
+                # Check that Linux compiler actually support the default flags
+                if hasattr(config_cmd.compiler, 'compiler'):
+                    config_cmd.compiler.compiler.extend(NPY_CXX_FLAGS)
+                    config_cmd.compiler.compiler_so.extend(NPY_CXX_FLAGS)
+
+            st = config_cmd.try_link(test_code, lang=lang)
+            if not st:
+                # rerun the failing command in verbose mode
+                config_cmd.compiler.verbose = True
+                config_cmd.try_link(test_code, lang=lang)
+                raise RuntimeError(
+                    f"Broken toolchain: cannot link a simple {lang.upper()} "
+                    f"program. {note}"
+                )
+            if is_cpp:
+                config_cmd.compiler = bk_c
+        mlibs = check_mathlib(config_cmd)
+
+        posix_mlib = ' '.join(['-l%s' % l for l in mlibs])
+        msvc_mlib = ' '.join(['%s.lib' % l for l in mlibs])
+        subst_dict["posix_mathlib"] = posix_mlib
+        subst_dict["msvc_mathlib"] = msvc_mlib
+
+    npymath_sources = [join('src', 'npymath', 'npy_math_internal.h.src'),
+                       join('src', 'npymath', 'npy_math.c'),
+                       # join('src', 'npymath', 'ieee754.cpp'),
+                       join('src', 'npymath', 'ieee754.c.src'),
+                       join('src', 'npymath', 'npy_math_complex.c.src'),
+                       join('src', 'npymath', 'halffloat.c')
+                       ]
+
+    def opts_if_msvc(build_cmd):
+        """ Add flags if we are using MSVC compiler
+
+        We can't see `build_cmd` in our scope, because we have not initialized
+        the distutils build command, so use this deferred calculation to run
+        when we are building the library.
+        """
+        if build_cmd.compiler.compiler_type != 'msvc':
+            return []
+        # Explicitly disable whole-program optimization.
+        flags = ['/GL-']
+        # Disable voltbl section for vc142 to allow link using mingw-w64; see:
+        # https://github.com/matthew-brett/dll_investigation/issues/1#issuecomment-1100468171
+        if build_cmd.compiler_opt.cc_test_flags(['-d2VolatileMetadata-']):
+            flags.append('-d2VolatileMetadata-')
+        return flags
+
+    config.add_installed_library('npymath',
+            sources=npymath_sources + [get_mathlib_info],
+            install_dir='lib',
+            build_info={
+                'include_dirs' : [],  # empty list required for creating npy_math_internal.h
+                'extra_compiler_args': [opts_if_msvc],
+            })
+    config.add_npy_pkg_config("npymath.ini.in", "lib/npy-pkg-config",
+            subst_dict)
+    config.add_npy_pkg_config("mlib.ini.in", "lib/npy-pkg-config",
+            subst_dict)
+
+    #######################################################################
+    #                     multiarray_tests module                         #
+    #######################################################################
+
+    config.add_extension('_multiarray_tests',
+                    sources=[join('src', 'multiarray', '_multiarray_tests.c.src'),
+                             join('src', 'common', 'mem_overlap.c'),
+                             join('src', 'common', 'npy_argparse.c'),
+                             join('src', 'common', 'npy_hashtable.c')],
+                    depends=[join('src', 'common', 'mem_overlap.h'),
+                             join('src', 'common', 'npy_argparse.h'),
+                             join('src', 'common', 'npy_hashtable.h'),
+                             join('src', 'common', 'npy_extint128.h')],
+                    libraries=['npymath'])
+
+    #######################################################################
+    #             _multiarray_umath module - common part                  #
+    #######################################################################
+
+    common_deps = [
+            join('src', 'common', 'dlpack', 'dlpack.h'),
+            join('src', 'common', 'array_assign.h'),
+            join('src', 'common', 'binop_override.h'),
+            join('src', 'common', 'cblasfuncs.h'),
+            join('src', 'common', 'lowlevel_strided_loops.h'),
+            join('src', 'common', 'mem_overlap.h'),
+            join('src', 'common', 'npy_argparse.h'),
+            join('src', 'common', 'npy_cblas.h'),
+            join('src', 'common', 'npy_config.h'),
+            join('src', 'common', 'npy_ctypes.h'),
+            join('src', 'common', 'npy_dlpack.h'),
+            join('src', 'common', 'npy_extint128.h'),
+            join('src', 'common', 'npy_import.h'),
+            join('src', 'common', 'npy_hashtable.h'),
+            join('src', 'common', 'npy_longdouble.h'),
+            join('src', 'common', 'npy_svml.h'),
+            join('src', 'common', 'templ_common.h.src'),
+            join('src', 'common', 'ucsnarrow.h'),
+            join('src', 'common', 'ufunc_override.h'),
+            join('src', 'common', 'umathmodule.h'),
+            join('src', 'common', 'numpyos.h'),
+            join('src', 'common', 'npy_cpu_dispatch.h'),
+            join('src', 'common', 'simd', 'simd.h'),
+            ]
+
+    common_src = [
+            join('src', 'common', 'array_assign.c'),
+            join('src', 'common', 'mem_overlap.c'),
+            join('src', 'common', 'npy_argparse.c'),
+            join('src', 'common', 'npy_hashtable.c'),
+            join('src', 'common', 'npy_longdouble.c'),
+            join('src', 'common', 'templ_common.h.src'),
+            join('src', 'common', 'ucsnarrow.c'),
+            join('src', 'common', 'ufunc_override.c'),
+            join('src', 'common', 'numpyos.c'),
+            join('src', 'common', 'npy_cpu_features.c'),
+            ]
+
+    if os.environ.get('NPY_USE_BLAS_ILP64', "0") != "0":
+        blas_info = get_info('blas_ilp64_opt', 2)
+    else:
+        blas_info = get_info('blas_opt', 0)
+
+    have_blas = blas_info and ('HAVE_CBLAS', None) in blas_info.get('define_macros', [])
+
+    if have_blas:
+        extra_info = blas_info
+        # These files are also in MANIFEST.in so that they are always in
+        # the source distribution independently of HAVE_CBLAS.
+        common_src.extend([join('src', 'common', 'cblasfuncs.c'),
+                           join('src', 'common', 'python_xerbla.c'),
+                          ])
+    else:
+        extra_info = {}
+
+    #######################################################################
+    #             _multiarray_umath module - multiarray part              #
+    #######################################################################
+
+    multiarray_deps = [
+            join('src', 'multiarray', 'abstractdtypes.h'),
+            join('src', 'multiarray', 'arrayobject.h'),
+            join('src', 'multiarray', 'arraytypes.h.src'),
+            join('src', 'multiarray', 'arrayfunction_override.h'),
+            join('src', 'multiarray', 'array_coercion.h'),
+            join('src', 'multiarray', 'array_method.h'),
+            join('src', 'multiarray', 'npy_buffer.h'),
+            join('src', 'multiarray', 'calculation.h'),
+            join('src', 'multiarray', 'common.h'),
+            join('src', 'multiarray', 'common_dtype.h'),
+            join('src', 'multiarray', 'convert_datatype.h'),
+            join('src', 'multiarray', 'convert.h'),
+            join('src', 'multiarray', 'conversion_utils.h'),
+            join('src', 'multiarray', 'ctors.h'),
+            join('src', 'multiarray', 'descriptor.h'),
+            join('src', 'multiarray', 'dtypemeta.h'),
+            join('src', 'multiarray', 'dtype_transfer.h'),
+            join('src', 'multiarray', 'dragon4.h'),
+            join('src', 'multiarray', 'einsum_debug.h'),
+            join('src', 'multiarray', 'einsum_sumprod.h'),
+            join('src', 'multiarray', 'experimental_public_dtype_api.h'),
+            join('src', 'multiarray', 'getset.h'),
+            join('src', 'multiarray', 'hashdescr.h'),
+            join('src', 'multiarray', 'iterators.h'),
+            join('src', 'multiarray', 'legacy_dtype_implementation.h'),
+            join('src', 'multiarray', 'mapping.h'),
+            join('src', 'multiarray', 'methods.h'),
+            join('src', 'multiarray', 'multiarraymodule.h'),
+            join('src', 'multiarray', 'nditer_impl.h'),
+            join('src', 'multiarray', 'number.h'),
+            join('src', 'multiarray', 'refcount.h'),
+            join('src', 'multiarray', 'scalartypes.h'),
+            join('src', 'multiarray', 'sequence.h'),
+            join('src', 'multiarray', 'shape.h'),
+            join('src', 'multiarray', 'strfuncs.h'),
+            join('src', 'multiarray', 'typeinfo.h'),
+            join('src', 'multiarray', 'usertypes.h'),
+            join('src', 'multiarray', 'vdot.h'),
+            join('src', 'multiarray', 'textreading', 'readtext.h'),
+            join('include', 'numpy', 'arrayobject.h'),
+            join('include', 'numpy', '_neighborhood_iterator_imp.h'),
+            join('include', 'numpy', 'npy_endian.h'),
+            join('include', 'numpy', 'arrayscalars.h'),
+            join('include', 'numpy', 'noprefix.h'),
+            join('include', 'numpy', 'npy_interrupt.h'),
+            join('include', 'numpy', 'npy_3kcompat.h'),
+            join('include', 'numpy', 'npy_math.h'),
+            join('include', 'numpy', 'halffloat.h'),
+            join('include', 'numpy', 'npy_common.h'),
+            join('include', 'numpy', 'npy_os.h'),
+            join('include', 'numpy', 'utils.h'),
+            join('include', 'numpy', 'ndarrayobject.h'),
+            join('include', 'numpy', 'npy_cpu.h'),
+            join('include', 'numpy', 'numpyconfig.h'),
+            join('include', 'numpy', 'ndarraytypes.h'),
+            join('include', 'numpy', 'npy_1_7_deprecated_api.h'),
+            # add library sources as distuils does not consider libraries
+            # dependencies
+            ] + npymath_sources
+
+    multiarray_src = [
+            join('src', 'multiarray', 'abstractdtypes.c'),
+            join('src', 'multiarray', 'alloc.c'),
+            join('src', 'multiarray', 'arrayobject.c'),
+            join('src', 'multiarray', 'arraytypes.h.src'),
+            join('src', 'multiarray', 'arraytypes.c.src'),
+            join('src', 'multiarray', 'argfunc.dispatch.c.src'),
+            join('src', 'multiarray', 'array_coercion.c'),
+            join('src', 'multiarray', 'array_method.c'),
+            join('src', 'multiarray', 'array_assign_scalar.c'),
+            join('src', 'multiarray', 'array_assign_array.c'),
+            join('src', 'multiarray', 'arrayfunction_override.c'),
+            join('src', 'multiarray', 'buffer.c'),
+            join('src', 'multiarray', 'calculation.c'),
+            join('src', 'multiarray', 'compiled_base.c'),
+            join('src', 'multiarray', 'common.c'),
+            join('src', 'multiarray', 'common_dtype.c'),
+            join('src', 'multiarray', 'convert.c'),
+            join('src', 'multiarray', 'convert_datatype.c'),
+            join('src', 'multiarray', 'conversion_utils.c'),
+            join('src', 'multiarray', 'ctors.c'),
+            join('src', 'multiarray', 'datetime.c'),
+            join('src', 'multiarray', 'datetime_strings.c'),
+            join('src', 'multiarray', 'datetime_busday.c'),
+            join('src', 'multiarray', 'datetime_busdaycal.c'),
+            join('src', 'multiarray', 'descriptor.c'),
+            join('src', 'multiarray', 'dlpack.c'),
+            join('src', 'multiarray', 'dtypemeta.c'),
+            join('src', 'multiarray', 'dragon4.c'),
+            join('src', 'multiarray', 'dtype_transfer.c'),
+            join('src', 'multiarray', 'einsum.c.src'),
+            join('src', 'multiarray', 'einsum_sumprod.c.src'),
+            join('src', 'multiarray', 'experimental_public_dtype_api.c'),
+            join('src', 'multiarray', 'flagsobject.c'),
+            join('src', 'multiarray', 'getset.c'),
+            join('src', 'multiarray', 'hashdescr.c'),
+            join('src', 'multiarray', 'item_selection.c'),
+            join('src', 'multiarray', 'iterators.c'),
+            join('src', 'multiarray', 'legacy_dtype_implementation.c'),
+            join('src', 'multiarray', 'lowlevel_strided_loops.c.src'),
+            join('src', 'multiarray', 'mapping.c'),
+            join('src', 'multiarray', 'methods.c'),
+            join('src', 'multiarray', 'multiarraymodule.c'),
+            join('src', 'multiarray', 'nditer_templ.c.src'),
+            join('src', 'multiarray', 'nditer_api.c'),
+            join('src', 'multiarray', 'nditer_constr.c'),
+            join('src', 'multiarray', 'nditer_pywrap.c'),
+            join('src', 'multiarray', 'number.c'),
+            join('src', 'multiarray', 'refcount.c'),
+            join('src', 'multiarray', 'sequence.c'),
+            join('src', 'multiarray', 'shape.c'),
+            join('src', 'multiarray', 'scalarapi.c'),
+            join('src', 'multiarray', 'scalartypes.c.src'),
+            join('src', 'multiarray', 'strfuncs.c'),
+            join('src', 'multiarray', 'temp_elide.c'),
+            join('src', 'multiarray', 'typeinfo.c'),
+            join('src', 'multiarray', 'usertypes.c'),
+            join('src', 'multiarray', 'vdot.c'),
+            join('src', 'common', 'npy_sort.h.src'),
+            join('src', 'npysort', 'x86-qsort.dispatch.cpp'),
+            join('src', 'npysort', 'quicksort.cpp'),
+            join('src', 'npysort', 'mergesort.cpp'),
+            join('src', 'npysort', 'timsort.cpp'),
+            join('src', 'npysort', 'heapsort.cpp'),
+            join('src', 'npysort', 'radixsort.cpp'),
+            join('src', 'common', 'npy_partition.h'),
+            join('src', 'npysort', 'selection.cpp'),
+            join('src', 'common', 'npy_binsearch.h'),
+            join('src', 'npysort', 'binsearch.cpp'),
+            join('src', 'multiarray', 'textreading', 'conversions.c'),
+            join('src', 'multiarray', 'textreading', 'field_types.c'),
+            join('src', 'multiarray', 'textreading', 'growth.c'),
+            join('src', 'multiarray', 'textreading', 'readtext.c'),
+            join('src', 'multiarray', 'textreading', 'rows.c'),
+            join('src', 'multiarray', 'textreading', 'stream_pyobject.c'),
+            join('src', 'multiarray', 'textreading', 'str_to_int.c'),
+            join('src', 'multiarray', 'textreading', 'tokenize.cpp'),
+            ]
+
+    #######################################################################
+    #             _multiarray_umath module - umath part                   #
+    #######################################################################
+
+    def generate_umath_c(ext, build_dir):
+        target = join(build_dir, header_dir, '__umath_generated.c')
+        dir = os.path.dirname(target)
+        if not os.path.exists(dir):
+            os.makedirs(dir)
         script = generate_umath_py
-        if newer(script,target):
-            f = open(target,'w')
-            f.write(generate_umath.make_code(generate_umath.defdict,
-                                             generate_umath.__file__))
-            f.close()
+        if newer(script, target):
+            with open(target, 'w') as f:
+                f.write(generate_umath.make_code(generate_umath.defdict,
+                                                 generate_umath.__file__))
         return []
 
-    config.add_data_files('include/numpy/*.h')
-    config.add_include_dirs('src')
-
-    config.numpy_include_dirs.extend(config.paths('include'))
-
-    deps = [join('src','arrayobject.c'),
-            join('src','arraymethods.c'),
-            join('src','scalartypes.inc.src'),
-            join('src','arraytypes.inc.src'),
-            join('src','_signbit.c'),
-            join('src','_isnan.c'),
-            join('src','ucsnarrow.c'),
-            join('include','numpy','*object.h'),
-            'include/numpy/fenv/fenv.c',
-            'include/numpy/fenv/fenv.h',
-            join(codegen_dir,'genapi.py'),
-            join(codegen_dir,'*.txt')
+    def generate_umath_doc_header(ext, build_dir):
+        from numpy.distutils.misc_util import exec_mod_from_location
+
+        target = join(build_dir, header_dir, '_umath_doc_generated.h')
+        dir = os.path.dirname(target)
+        if not os.path.exists(dir):
+            os.makedirs(dir)
+
+        generate_umath_doc_py = join(codegen_dir, 'generate_umath_doc.py')
+        if newer(generate_umath_doc_py, target):
+            n = dot_join(config.name, 'generate_umath_doc')
+            generate_umath_doc = exec_mod_from_location(
+                '_'.join(n.split('.')), generate_umath_doc_py)
+            generate_umath_doc.write_code(target)
+
+    umath_src = [
+            join('src', 'umath', 'umathmodule.c'),
+            join('src', 'umath', 'reduction.c'),
+            join('src', 'umath', 'funcs.inc.src'),
+            join('src', 'umath', 'simd.inc.src'),
+            join('src', 'umath', 'loops.h.src'),
+            join('src', 'umath', 'loops_utils.h.src'),
+            join('src', 'umath', 'loops.c.src'),
+            join('src', 'umath', 'loops_unary_fp.dispatch.c.src'),
+            join('src', 'umath', 'loops_arithm_fp.dispatch.c.src'),
+            join('src', 'umath', 'loops_arithmetic.dispatch.c.src'),
+            join('src', 'umath', 'loops_minmax.dispatch.c.src'),
+            join('src', 'umath', 'loops_trigonometric.dispatch.c.src'),
+            join('src', 'umath', 'loops_umath_fp.dispatch.c.src'),
+            join('src', 'umath', 'loops_exponent_log.dispatch.c.src'),
+            join('src', 'umath', 'loops_hyperbolic.dispatch.c.src'),
+            join('src', 'umath', 'loops_modulo.dispatch.c.src'),
+            join('src', 'umath', 'matmul.h.src'),
+            join('src', 'umath', 'matmul.c.src'),
+            join('src', 'umath', 'clip.h'),
+            join('src', 'umath', 'clip.cpp'),
+            join('src', 'umath', 'dispatching.c'),
+            join('src', 'umath', 'legacy_array_method.c'),
+            join('src', 'umath', 'wrapping_array_method.c'),
+            join('src', 'umath', 'ufunc_object.c'),
+            join('src', 'umath', 'extobj.c'),
+            join('src', 'umath', 'scalarmath.c.src'),
+            join('src', 'umath', 'ufunc_type_resolution.c'),
+            join('src', 'umath', 'override.c'),
+            # For testing. Eventually, should use public API and be separate:
+            join('src', 'umath', '_scaled_float_dtype.c'),
             ]
 
-    # Don't install fenv unless we need them.
-    if sys.platform == 'cygwin':
-        config.add_data_dir('include/numpy/fenv')
-
-    config.add_extension('multiarray',
-                         sources = [join('src','multiarraymodule.c'),
-                                    generate_config_h,
-                                    generate_array_api,
-                                    join('src','scalartypes.inc.src'),
-                                    join('src','arraytypes.inc.src'),
-                                    join(codegen_dir,'generate_array_api.py'),
-                                    join('*.py')
-                                    ],
-                         depends = deps,
-                         )
-
-    config.add_extension('umath',
-                         sources = [generate_config_h,
-                                    join('src','umathmodule.c.src'),
-                                    generate_umath_c,
-                                    generate_ufunc_api,
-                                    join('src','scalartypes.inc.src'),
-                                    join('src','arraytypes.inc.src'),
-                                    ],
-                         depends = [join('src','ufuncobject.c'),
-                                    generate_umath_py,
-                                    join(codegen_dir,'generate_ufunc_api.py'),
-                                    ]+deps,
-                         )
-
-    config.add_extension('_sort',
-                         sources=[join('src','_sortmodule.c.src'),
-                                  generate_config_h,
-                                  generate_array_api,
-                                  ],
-                         )
-
-    config.add_extension('scalarmath',
-                         sources=[join('src','scalarmathmodule.c.src'),
-                                  generate_config_h,
-                                  generate_array_api,
-                                  generate_ufunc_api],
-                         )
-
-    # Configure blasdot
-    blas_info = get_info('blas_opt',0)
-    #blas_info = {}
-    def get_dotblas_sources(ext, build_dir):
-        if blas_info:
-            return ext.depends[:1]
-        return None # no extension module will be built
-
-    config.add_extension('_dotblas',
-                         sources = [get_dotblas_sources],
-                         depends=[join('blasdot','_dotblas.c'),
-                                  join('blasdot','cblas.h'),
-                                  ],
-                         include_dirs = ['blasdot'],
-                         extra_info = blas_info
-                         )
-
-
-    config.add_data_dir('tests')
+    umath_deps = [
+            generate_umath_py,
+            join('include', 'numpy', 'npy_math.h'),
+            join('include', 'numpy', 'halffloat.h'),
+            join('src', 'multiarray', 'common.h'),
+            join('src', 'multiarray', 'number.h'),
+            join('src', 'common', 'templ_common.h.src'),
+            join('src', 'umath', 'simd.inc.src'),
+            join('src', 'umath', 'override.h'),
+            join(codegen_dir, 'generate_ufunc_api.py'),
+            join(codegen_dir, 'ufunc_docstrings.py'),
+            ]
+
+    svml_path = join('numpy', 'core', 'src', 'umath', 'svml')
+    svml_objs = []
+    # we have converted the following into universal intrinsics
+    # so we can bring the benefits of performance for all platforms
+    # not just for avx512 on linux without performance/accuracy regression,
+    # actually the other way around, better performance and
+    # after all maintainable code.
+    svml_filter = (
+        'svml_z0_tanh_d_la.s', 'svml_z0_tanh_s_la.s'
+    )
+    if can_link_svml() and check_svml_submodule(svml_path):
+        svml_objs = glob.glob(svml_path + '/**/*.s', recursive=True)
+        svml_objs = [o for o in svml_objs if not o.endswith(svml_filter)]
+
+        # The ordering of names returned by glob is undefined, so we sort
+        # to make builds reproducible.
+        svml_objs.sort()
+
+    config.add_extension('_multiarray_umath',
+                         # Forcing C language even though we have C++ sources.
+                         # It forces the C linker and don't link C++ runtime.
+                         language = 'c',
+                         sources=multiarray_src + umath_src +
+                                 common_src +
+                                 [generate_config_h,
+                                  generate_numpyconfig_h,
+                                  generate_numpy_api,
+                                  join(codegen_dir, 'generate_numpy_api.py'),
+                                  join('*.py'),
+                                  generate_umath_c,
+                                  generate_umath_doc_header,
+                                  generate_ufunc_api,
+                                 ],
+                         depends=deps + multiarray_deps + umath_deps +
+                                common_deps,
+                         libraries=['npymath'],
+                         extra_objects=svml_objs,
+                         extra_info=extra_info,
+                         extra_cxx_compile_args=NPY_CXX_FLAGS)
+
+    #######################################################################
+    #                        umath_tests module                           #
+    #######################################################################
+
+    config.add_extension('_umath_tests', sources=[
+        join('src', 'umath', '_umath_tests.c.src'),
+        join('src', 'umath', '_umath_tests.dispatch.c'),
+        join('src', 'common', 'npy_cpu_features.c'),
+    ])
+
+    #######################################################################
+    #                   custom rational dtype module                      #
+    #######################################################################
+
+    config.add_extension('_rational_tests',
+                    sources=[join('src', 'umath', '_rational_tests.c')])
+
+    #######################################################################
+    #                        struct_ufunc_test module                     #
+    #######################################################################
+
+    config.add_extension('_struct_ufunc_tests',
+                    sources=[join('src', 'umath', '_struct_ufunc_tests.c')])
+
+
+    #######################################################################
+    #                        operand_flag_tests module                    #
+    #######################################################################
+
+    config.add_extension('_operand_flag_tests',
+                    sources=[join('src', 'umath', '_operand_flag_tests.c')])
+
+    #######################################################################
+    #                        SIMD module                                  #
+    #######################################################################
+
+    config.add_extension('_simd', sources=[
+        join('src', 'common', 'npy_cpu_features.c'),
+        join('src', '_simd', '_simd.c'),
+        join('src', '_simd', '_simd_inc.h.src'),
+        join('src', '_simd', '_simd_data.inc.src'),
+        join('src', '_simd', '_simd.dispatch.c.src'),
+    ], depends=[
+        join('src', 'common', 'npy_cpu_dispatch.h'),
+        join('src', 'common', 'simd', 'simd.h'),
+        join('src', '_simd', '_simd.h'),
+        join('src', '_simd', '_simd_inc.h.src'),
+        join('src', '_simd', '_simd_data.inc.src'),
+        join('src', '_simd', '_simd_arg.inc'),
+        join('src', '_simd', '_simd_convert.inc'),
+        join('src', '_simd', '_simd_easyintrin.inc'),
+        join('src', '_simd', '_simd_vector.inc'),
+    ])
+
+    config.add_subpackage('tests')
+    config.add_data_dir('tests/data')
+    config.add_data_dir('tests/examples')
+    config.add_data_files('*.pyi')
+
     config.make_svn_version_py()
 
     return config
 
-def testcode_mathlib():
-    return """\
-/* check whether libm is broken */
-#include <math.h>
-int main(int argc, char *argv[])
-{
-  return exp(-720.) > 1.0;  /* typically an IEEE denormal */
-}
-"""
-
-import sys
-def generate_testcode(target):
-    if sys.platform == 'win32':
-        target = target.replace('\\','\\\\')
-    testcode = [r'''
-#include <Python.h>
-#include <limits.h>
-#include <stdio.h>
-
-int main(int argc, char **argv)
-{
-
-        FILE *fp;
-
-        fp = fopen("'''+target+'''","w");
-        ''']
-
-    c_size_test = r'''
-#ifndef %(sz)s
-          fprintf(fp,"#define %(sz)s %%d\n", sizeof(%(type)s));
-#else
-          fprintf(fp,"/* #define %(sz)s %%d */\n", %(sz)s);
-#endif
-'''
-    for sz, t in [('SIZEOF_SHORT', 'short'),
-                  ('SIZEOF_INT', 'int'),
-                  ('SIZEOF_LONG', 'long'),
-                  ('SIZEOF_FLOAT', 'float'),
-                  ('SIZEOF_DOUBLE', 'double'),
-                  ('SIZEOF_LONG_DOUBLE', 'long double'),
-                  ('SIZEOF_PY_INTPTR_T', 'Py_intptr_t'),
-                  ]:
-        testcode.append(c_size_test % {'sz' : sz, 'type' : t})
-
-    testcode.append('#ifdef PY_LONG_LONG')
-    testcode.append(c_size_test % {'sz' : 'SIZEOF_LONG_LONG',
-                                   'type' : 'PY_LONG_LONG'})
-    testcode.append(c_size_test % {'sz' : 'SIZEOF_PY_LONG_LONG',
-                                   'type' : 'PY_LONG_LONG'})
-
-
-    testcode.append(r'''
-#else
-        fprintf(fp, "/* PY_LONG_LONG not defined */\n");
-#endif
-#ifndef CHAR_BIT
-          {
-             unsigned char var = 2;
-             int i=0;
-             while (var >= 2) {
-                     var = var << 1;
-                     i++;
-             }
-             fprintf(fp,"#define CHAR_BIT %d\n", i+1);
-          }
-#else
-          fprintf(fp, "/* #define CHAR_BIT %d */\n", CHAR_BIT);
-#endif
-          fclose(fp);
-          return 0;
-}
-''')
-    testcode = '\n'.join(testcode)
-    return testcode
-
-if __name__=='__main__':
+if __name__ == '__main__':
     from numpy.distutils.core import setup
-    setup(**configuration(top_path='').todict())
+    setup(configuration=configuration)
('numpy/core', 'numeric.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,496 +1,2554 @@
-__all__ = ['newaxis', 'ndarray', 'flatiter', 'ufunc',
-           'arange', 'array', 'zeros', 'empty', 'broadcast', 'dtype',
-           'fromstring', 'fromfile', 'frombuffer','newbuffer',
-           'getbuffer', 'where', 'argwhere',
-           'concatenate', 'fastCopyAndTranspose', 'lexsort',
-           'register_dtype', 'set_numeric_ops', 'can_cast',
-           'asarray', 'asanyarray', 'ascontiguousarray', 'asfortranarray',
-           'isfortran', 'empty_like', 'zeros_like',
-           'correlate', 'convolve', 'inner', 'dot', 'outer', 'vdot',
-           'alterdot', 'restoredot', 'cross',
-           'array2string', 'get_printoptions', 'set_printoptions',
-           'array_repr', 'array_str', 'set_string_function',
-           'little_endian',
-           'indices', 'fromfunction',
-           'load', 'loads', 'isscalar', 'binary_repr', 'base_repr',
-           'ones', 'identity', 'allclose',
-           'seterr', 'geterr', 'setbufsize', 'getbufsize',
-           'seterrcall', 'geterrcall',
-           'Inf', 'inf', 'infty', 'Infinity',
-           'nan', 'NaN', 'False_', 'True_']
-
+import functools
+import itertools
+import operator
 import sys
-import multiarray
-import umath
-from umath import *
-import numerictypes
-from numerictypes import *
-
-# from Fernando Perez's IPython
-def zeros_like(a):
-    """Return an array of zeros of the shape and typecode of a.
-
-    If you don't explicitly need the array to be zeroed, you should instead
-    use empty_like(), which is faster as it only allocates memory."""
-    try:
-        return zeros(a.shape, a.dtype, a.flags.fnc)
-    except AttributeError:
-        try:
-            wrap = a.__array_wrap__
-        except AttributeError:
-            wrap = None
-        a = asarray(a)
-        res = zeros(a.shape, a.dtype)
-        if wrap:
-            res = wrap(res)
-        return res
-
-def empty_like(a):
-    """Return an empty (uninitialized) array of the shape and typecode of a.
-
-    Note that this does NOT initialize the returned array.  If you require
-    your array to be initialized, you should use zeros_like().
-
-    """
-    try:
-        return empty(a.shape, a.dtype, a.flags.fnc)
-    except AttributeError:
-        try:
-            wrap = a.__array_wrap__
-        except AttributeError:
-            wrap = None
-        a = asarray(a)
-        res = empty(a.shape, a.dtype)
-        if wrap:
-            res = wrap(res)
-        return res
-
-# end Fernando's utilities
-
-def extend_all(module):
-    adict = {}
-    for a in __all__:
-        adict[a] = 1
-    try:
-        mall = getattr(module, '__all__')
-    except AttributeError:
-        mall = [k for k in module.__dict__.keys() if not k.startswith('_')]
-    for a in mall:
-        if a not in adict:
-            __all__.append(a)
-
-extend_all(umath)
-extend_all(numerictypes)
-
+import warnings
+import numbers
+
+import numpy as np
+from . import multiarray
+from .multiarray import (
+    _fastCopyAndTranspose as fastCopyAndTranspose, ALLOW_THREADS,
+    BUFSIZE, CLIP, MAXDIMS, MAY_SHARE_BOUNDS, MAY_SHARE_EXACT, RAISE,
+    WRAP, arange, array, asarray, asanyarray, ascontiguousarray,
+    asfortranarray, broadcast, can_cast, compare_chararrays,
+    concatenate, copyto, dot, dtype, empty,
+    empty_like, flatiter, frombuffer, from_dlpack, fromfile, fromiter,
+    fromstring, inner, lexsort, matmul, may_share_memory,
+    min_scalar_type, ndarray, nditer, nested_iters, promote_types,
+    putmask, result_type, set_numeric_ops, shares_memory, vdot, where,
+    zeros, normalize_axis_index)
+
+from . import overrides
+from . import umath
+from . import shape_base
+from .overrides import set_array_function_like_doc, set_module
+from .umath import (multiply, invert, sin, PINF, NAN)
+from . import numerictypes
+from .numerictypes import longlong, intc, int_, float_, complex_, bool_
+from ._exceptions import TooHardError, AxisError
+from ._ufunc_config import errstate
+
+bitwise_not = invert
+ufunc = type(sin)
 newaxis = None
 
-ndarray = multiarray.ndarray
-flatiter = multiarray.flatiter
-broadcast = multiarray.broadcast
-dtype = multiarray.dtype
-ufunc = type(sin)
-
-arange = multiarray.arange
-array = multiarray.array
-zeros = multiarray.zeros
-empty = multiarray.empty
-fromstring = multiarray.fromstring
-fromfile = multiarray.fromfile
-frombuffer = multiarray.frombuffer
-newbuffer = multiarray.newbuffer
-getbuffer = multiarray.getbuffer
-where = multiarray.where
-concatenate = multiarray.concatenate
-fastCopyAndTranspose = multiarray._fastCopyAndTranspose
-register_dtype = multiarray.register_dtype
-set_numeric_ops = multiarray.set_numeric_ops
-can_cast = multiarray.can_cast
-lexsort = multiarray.lexsort
-
-
-def asarray(a, dtype=None, order=None):
-    """returns a as an array.  Unlike array(),
-    no copy is performed if a is already an array.  Subclasses are converted
-    to base class ndarray.
-    """
-    return array(a, dtype, copy=False, order=order)
-
-def asanyarray(a, dtype=None, order=None):
-    """will pass subclasses through...
-    """
-    return array(a, dtype, copy=False, order=order, subok=1)
-
-def ascontiguousarray(a, dtype=None):
-    return array(a, dtype, copy=False, order='C', ndmin=1)
-
-def asfortranarray(a, dtype=None):
-    return array(a, dtype, copy=False, order='F', ndmin=1)
-
+array_function_dispatch = functools.partial(
+    overrides.array_function_dispatch, module='numpy')
+
+
+__all__ = [
+    'newaxis', 'ndarray', 'flatiter', 'nditer', 'nested_iters', 'ufunc',
+    'arange', 'array', 'asarray', 'asanyarray', 'ascontiguousarray',
+    'asfortranarray', 'zeros', 'count_nonzero', 'empty', 'broadcast', 'dtype',
+    'fromstring', 'fromfile', 'frombuffer', 'from_dlpack', 'where',
+    'argwhere', 'copyto', 'concatenate', 'fastCopyAndTranspose', 'lexsort',
+    'set_numeric_ops', 'can_cast', 'promote_types', 'min_scalar_type',
+    'result_type', 'isfortran', 'empty_like', 'zeros_like', 'ones_like',
+    'correlate', 'convolve', 'inner', 'dot', 'outer', 'vdot', 'roll',
+    'rollaxis', 'moveaxis', 'cross', 'tensordot', 'little_endian',
+    'fromiter', 'array_equal', 'array_equiv', 'indices', 'fromfunction',
+    'isclose', 'isscalar', 'binary_repr', 'base_repr', 'ones',
+    'identity', 'allclose', 'compare_chararrays', 'putmask',
+    'flatnonzero', 'Inf', 'inf', 'infty', 'Infinity', 'nan', 'NaN',
+    'False_', 'True_', 'bitwise_not', 'CLIP', 'RAISE', 'WRAP', 'MAXDIMS',
+    'BUFSIZE', 'ALLOW_THREADS', 'ComplexWarning', 'full', 'full_like',
+    'matmul', 'shares_memory', 'may_share_memory', 'MAY_SHARE_BOUNDS',
+    'MAY_SHARE_EXACT', 'TooHardError', 'AxisError']
+
+
+@set_module('numpy')
+class ComplexWarning(RuntimeWarning):
+    """
+    The warning raised when casting a complex dtype to a real dtype.
+
+    As implemented, casting a complex number to a real discards its imaginary
+    part, but this behavior may not be what the user actually wants.
+
+    """
+    pass
+
+
+def _zeros_like_dispatcher(a, dtype=None, order=None, subok=None, shape=None):
+    return (a,)
+
+
+@array_function_dispatch(_zeros_like_dispatcher)
+def zeros_like(a, dtype=None, order='K', subok=True, shape=None):
+    """
+    Return an array of zeros with the same shape and type as a given array.
+
+    Parameters
+    ----------
+    a : array_like
+        The shape and data-type of `a` define these same attributes of
+        the returned array.
+    dtype : data-type, optional
+        Overrides the data type of the result.
+
+        .. versionadded:: 1.6.0
+    order : {'C', 'F', 'A', or 'K'}, optional
+        Overrides the memory layout of the result. 'C' means C-order,
+        'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,
+        'C' otherwise. 'K' means match the layout of `a` as closely
+        as possible.
+
+        .. versionadded:: 1.6.0
+    subok : bool, optional.
+        If True, then the newly created array will use the sub-class
+        type of `a`, otherwise it will be a base-class array. Defaults
+        to True.
+    shape : int or sequence of ints, optional.
+        Overrides the shape of the result. If order='K' and the number of
+        dimensions is unchanged, will try to keep order, otherwise,
+        order='C' is implied.
+
+        .. versionadded:: 1.17.0
+
+    Returns
+    -------
+    out : ndarray
+        Array of zeros with the same shape and type as `a`.
+
+    See Also
+    --------
+    empty_like : Return an empty array with shape and type of input.
+    ones_like : Return an array of ones with shape and type of input.
+    full_like : Return a new array with shape of input filled with value.
+    zeros : Return a new array setting values to zero.
+
+    Examples
+    --------
+    >>> x = np.arange(6)
+    >>> x = x.reshape((2, 3))
+    >>> x
+    array([[0, 1, 2],
+           [3, 4, 5]])
+    >>> np.zeros_like(x)
+    array([[0, 0, 0],
+           [0, 0, 0]])
+
+    >>> y = np.arange(3, dtype=float)
+    >>> y
+    array([0., 1., 2.])
+    >>> np.zeros_like(y)
+    array([0.,  0.,  0.])
+
+    """
+    res = empty_like(a, dtype=dtype, order=order, subok=subok, shape=shape)
+    # needed instead of a 0 to get same result as zeros for string dtypes
+    z = zeros(1, dtype=res.dtype)
+    multiarray.copyto(res, z, casting='unsafe')
+    return res
+
+
+def _ones_dispatcher(shape, dtype=None, order=None, *, like=None):
+    return(like,)
+
+
+@set_array_function_like_doc
+@set_module('numpy')
+def ones(shape, dtype=None, order='C', *, like=None):
+    """
+    Return a new array of given shape and type, filled with ones.
+
+    Parameters
+    ----------
+    shape : int or sequence of ints
+        Shape of the new array, e.g., ``(2, 3)`` or ``2``.
+    dtype : data-type, optional
+        The desired data-type for the array, e.g., `numpy.int8`.  Default is
+        `numpy.float64`.
+    order : {'C', 'F'}, optional, default: C
+        Whether to store multi-dimensional data in row-major
+        (C-style) or column-major (Fortran-style) order in
+        memory.
+    ${ARRAY_FUNCTION_LIKE}
+
+        .. versionadded:: 1.20.0
+
+    Returns
+    -------
+    out : ndarray
+        Array of ones with the given shape, dtype, and order.
+
+    See Also
+    --------
+    ones_like : Return an array of ones with shape and type of input.
+    empty : Return a new uninitialized array.
+    zeros : Return a new array setting values to zero.
+    full : Return a new array of given shape filled with value.
+
+
+    Examples
+    --------
+    >>> np.ones(5)
+    array([1., 1., 1., 1., 1.])
+
+    >>> np.ones((5,), dtype=int)
+    array([1, 1, 1, 1, 1])
+
+    >>> np.ones((2, 1))
+    array([[1.],
+           [1.]])
+
+    >>> s = (2,2)
+    >>> np.ones(s)
+    array([[1.,  1.],
+           [1.,  1.]])
+
+    """
+    if like is not None:
+        return _ones_with_like(shape, dtype=dtype, order=order, like=like)
+
+    a = empty(shape, dtype, order)
+    multiarray.copyto(a, 1, casting='unsafe')
+    return a
+
+
+_ones_with_like = array_function_dispatch(
+    _ones_dispatcher
+)(ones)
+
+
+def _ones_like_dispatcher(a, dtype=None, order=None, subok=None, shape=None):
+    return (a,)
+
+
+@array_function_dispatch(_ones_like_dispatcher)
+def ones_like(a, dtype=None, order='K', subok=True, shape=None):
+    """
+    Return an array of ones with the same shape and type as a given array.
+
+    Parameters
+    ----------
+    a : array_like
+        The shape and data-type of `a` define these same attributes of
+        the returned array.
+    dtype : data-type, optional
+        Overrides the data type of the result.
+
+        .. versionadded:: 1.6.0
+    order : {'C', 'F', 'A', or 'K'}, optional
+        Overrides the memory layout of the result. 'C' means C-order,
+        'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,
+        'C' otherwise. 'K' means match the layout of `a` as closely
+        as possible.
+
+        .. versionadded:: 1.6.0
+    subok : bool, optional.
+        If True, then the newly created array will use the sub-class
+        type of `a`, otherwise it will be a base-class array. Defaults
+        to True.
+    shape : int or sequence of ints, optional.
+        Overrides the shape of the result. If order='K' and the number of
+        dimensions is unchanged, will try to keep order, otherwise,
+        order='C' is implied.
+
+        .. versionadded:: 1.17.0
+
+    Returns
+    -------
+    out : ndarray
+        Array of ones with the same shape and type as `a`.
+
+    See Also
+    --------
+    empty_like : Return an empty array with shape and type of input.
+    zeros_like : Return an array of zeros with shape and type of input.
+    full_like : Return a new array with shape of input filled with value.
+    ones : Return a new array setting values to one.
+
+    Examples
+    --------
+    >>> x = np.arange(6)
+    >>> x = x.reshape((2, 3))
+    >>> x
+    array([[0, 1, 2],
+           [3, 4, 5]])
+    >>> np.ones_like(x)
+    array([[1, 1, 1],
+           [1, 1, 1]])
+
+    >>> y = np.arange(3, dtype=float)
+    >>> y
+    array([0., 1., 2.])
+    >>> np.ones_like(y)
+    array([1.,  1.,  1.])
+
+    """
+    res = empty_like(a, dtype=dtype, order=order, subok=subok, shape=shape)
+    multiarray.copyto(res, 1, casting='unsafe')
+    return res
+
+
+def _full_dispatcher(shape, fill_value, dtype=None, order=None, *, like=None):
+    return(like,)
+
+
+@set_array_function_like_doc
+@set_module('numpy')
+def full(shape, fill_value, dtype=None, order='C', *, like=None):
+    """
+    Return a new array of given shape and type, filled with `fill_value`.
+
+    Parameters
+    ----------
+    shape : int or sequence of ints
+        Shape of the new array, e.g., ``(2, 3)`` or ``2``.
+    fill_value : scalar or array_like
+        Fill value.
+    dtype : data-type, optional
+        The desired data-type for the array  The default, None, means
+         ``np.array(fill_value).dtype``.
+    order : {'C', 'F'}, optional
+        Whether to store multidimensional data in C- or Fortran-contiguous
+        (row- or column-wise) order in memory.
+    ${ARRAY_FUNCTION_LIKE}
+
+        .. versionadded:: 1.20.0
+
+    Returns
+    -------
+    out : ndarray
+        Array of `fill_value` with the given shape, dtype, and order.
+
+    See Also
+    --------
+    full_like : Return a new array with shape of input filled with value.
+    empty : Return a new uninitialized array.
+    ones : Return a new array setting values to one.
+    zeros : Return a new array setting values to zero.
+
+    Examples
+    --------
+    >>> np.full((2, 2), np.inf)
+    array([[inf, inf],
+           [inf, inf]])
+    >>> np.full((2, 2), 10)
+    array([[10, 10],
+           [10, 10]])
+
+    >>> np.full((2, 2), [1, 2])
+    array([[1, 2],
+           [1, 2]])
+
+    """
+    if like is not None:
+        return _full_with_like(shape, fill_value, dtype=dtype, order=order, like=like)
+
+    if dtype is None:
+        fill_value = asarray(fill_value)
+        dtype = fill_value.dtype
+    a = empty(shape, dtype, order)
+    multiarray.copyto(a, fill_value, casting='unsafe')
+    return a
+
+
+_full_with_like = array_function_dispatch(
+    _full_dispatcher
+)(full)
+
+
+def _full_like_dispatcher(a, fill_value, dtype=None, order=None, subok=None, shape=None):
+    return (a,)
+
+
+@array_function_dispatch(_full_like_dispatcher)
+def full_like(a, fill_value, dtype=None, order='K', subok=True, shape=None):
+    """
+    Return a full array with the same shape and type as a given array.
+
+    Parameters
+    ----------
+    a : array_like
+        The shape and data-type of `a` define these same attributes of
+        the returned array.
+    fill_value : array_like
+        Fill value.
+    dtype : data-type, optional
+        Overrides the data type of the result.
+    order : {'C', 'F', 'A', or 'K'}, optional
+        Overrides the memory layout of the result. 'C' means C-order,
+        'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,
+        'C' otherwise. 'K' means match the layout of `a` as closely
+        as possible.
+    subok : bool, optional.
+        If True, then the newly created array will use the sub-class
+        type of `a`, otherwise it will be a base-class array. Defaults
+        to True.
+    shape : int or sequence of ints, optional.
+        Overrides the shape of the result. If order='K' and the number of
+        dimensions is unchanged, will try to keep order, otherwise,
+        order='C' is implied.
+
+        .. versionadded:: 1.17.0
+
+    Returns
+    -------
+    out : ndarray
+        Array of `fill_value` with the same shape and type as `a`.
+
+    See Also
+    --------
+    empty_like : Return an empty array with shape and type of input.
+    ones_like : Return an array of ones with shape and type of input.
+    zeros_like : Return an array of zeros with shape and type of input.
+    full : Return a new array of given shape filled with value.
+
+    Examples
+    --------
+    >>> x = np.arange(6, dtype=int)
+    >>> np.full_like(x, 1)
+    array([1, 1, 1, 1, 1, 1])
+    >>> np.full_like(x, 0.1)
+    array([0, 0, 0, 0, 0, 0])
+    >>> np.full_like(x, 0.1, dtype=np.double)
+    array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1])
+    >>> np.full_like(x, np.nan, dtype=np.double)
+    array([nan, nan, nan, nan, nan, nan])
+
+    >>> y = np.arange(6, dtype=np.double)
+    >>> np.full_like(y, 0.1)
+    array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1])
+
+    >>> y = np.zeros([2, 2, 3], dtype=int)
+    >>> np.full_like(y, [0, 0, 255])
+    array([[[  0,   0, 255],
+            [  0,   0, 255]],
+           [[  0,   0, 255],
+            [  0,   0, 255]]])
+    """
+    res = empty_like(a, dtype=dtype, order=order, subok=subok, shape=shape)
+    multiarray.copyto(res, fill_value, casting='unsafe')
+    return res
+
+
+def _count_nonzero_dispatcher(a, axis=None, *, keepdims=None):
+    return (a,)
+
+
+@array_function_dispatch(_count_nonzero_dispatcher)
+def count_nonzero(a, axis=None, *, keepdims=False):
+    """
+    Counts the number of non-zero values in the array ``a``.
+
+    The word "non-zero" is in reference to the Python 2.x
+    built-in method ``__nonzero__()`` (renamed ``__bool__()``
+    in Python 3.x) of Python objects that tests an object's
+    "truthfulness". For example, any number is considered
+    truthful if it is nonzero, whereas any string is considered
+    truthful if it is not the empty string. Thus, this function
+    (recursively) counts how many elements in ``a`` (and in
+    sub-arrays thereof) have their ``__nonzero__()`` or ``__bool__()``
+    method evaluated to ``True``.
+
+    Parameters
+    ----------
+    a : array_like
+        The array for which to count non-zeros.
+    axis : int or tuple, optional
+        Axis or tuple of axes along which to count non-zeros.
+        Default is None, meaning that non-zeros will be counted
+        along a flattened version of ``a``.
+
+        .. versionadded:: 1.12.0
+
+    keepdims : bool, optional
+        If this is set to True, the axes that are counted are left
+        in the result as dimensions with size one. With this option,
+        the result will broadcast correctly against the input array.
+
+        .. versionadded:: 1.19.0
+
+    Returns
+    -------
+    count : int or array of int
+        Number of non-zero values in the array along a given axis.
+        Otherwise, the total number of non-zero values in the array
+        is returned.
+
+    See Also
+    --------
+    nonzero : Return the coordinates of all the non-zero values.
+
+    Examples
+    --------
+    >>> np.count_nonzero(np.eye(4))
+    4
+    >>> a = np.array([[0, 1, 7, 0],
+    ...               [3, 0, 2, 19]])
+    >>> np.count_nonzero(a)
+    5
+    >>> np.count_nonzero(a, axis=0)
+    array([1, 1, 2, 1])
+    >>> np.count_nonzero(a, axis=1)
+    array([2, 3])
+    >>> np.count_nonzero(a, axis=1, keepdims=True)
+    array([[2],
+           [3]])
+    """
+    if axis is None and not keepdims:
+        return multiarray.count_nonzero(a)
+
+    a = asanyarray(a)
+
+    # TODO: this works around .astype(bool) not working properly (gh-9847)
+    if np.issubdtype(a.dtype, np.character):
+        a_bool = a != a.dtype.type()
+    else:
+        a_bool = a.astype(np.bool_, copy=False)
+
+    return a_bool.sum(axis=axis, dtype=np.intp, keepdims=keepdims)
+
+
+@set_module('numpy')
 def isfortran(a):
+    """
+    Check if the array is Fortran contiguous but *not* C contiguous.
+
+    This function is obsolete and, because of changes due to relaxed stride
+    checking, its return value for the same array may differ for versions
+    of NumPy >= 1.10.0 and previous versions. If you only want to check if an
+    array is Fortran contiguous use ``a.flags.f_contiguous`` instead.
+
+    Parameters
+    ----------
+    a : ndarray
+        Input array.
+
+    Returns
+    -------
+    isfortran : bool
+        Returns True if the array is Fortran contiguous but *not* C contiguous.
+
+
+    Examples
+    --------
+
+    np.array allows to specify whether the array is written in C-contiguous
+    order (last index varies the fastest), or FORTRAN-contiguous order in
+    memory (first index varies the fastest).
+
+    >>> a = np.array([[1, 2, 3], [4, 5, 6]], order='C')
+    >>> a
+    array([[1, 2, 3],
+           [4, 5, 6]])
+    >>> np.isfortran(a)
+    False
+
+    >>> b = np.array([[1, 2, 3], [4, 5, 6]], order='F')
+    >>> b
+    array([[1, 2, 3],
+           [4, 5, 6]])
+    >>> np.isfortran(b)
+    True
+
+
+    The transpose of a C-ordered array is a FORTRAN-ordered array.
+
+    >>> a = np.array([[1, 2, 3], [4, 5, 6]], order='C')
+    >>> a
+    array([[1, 2, 3],
+           [4, 5, 6]])
+    >>> np.isfortran(a)
+    False
+    >>> b = a.T
+    >>> b
+    array([[1, 4],
+           [2, 5],
+           [3, 6]])
+    >>> np.isfortran(b)
+    True
+
+    C-ordered arrays evaluate as False even if they are also FORTRAN-ordered.
+
+    >>> np.isfortran(np.array([1, 2], order='F'))
+    False
+
+    """
     return a.flags.fnc
 
+
+def _argwhere_dispatcher(a):
+    return (a,)
+
+
+@array_function_dispatch(_argwhere_dispatcher)
 def argwhere(a):
-    """Return a 2-d array of shape N x a.ndim where each row
-    is a sequence of indices into a.  This sequence must be
-    converted to a tuple in order to be used to index into a.
-    """
-    if a.ndim == 0:
-        return array([],dtype=intp)
-    else:
-        b = a.nonzero()
-        retarr = empty((b[0].shape[0],a.ndim),dtype=intp)
-        for k in xrange(a.ndim):
-            retarr[:,k] = b[k]
-    return retarr
-
+    """
+    Find the indices of array elements that are non-zero, grouped by element.
+
+    Parameters
+    ----------
+    a : array_like
+        Input data.
+
+    Returns
+    -------
+    index_array : (N, a.ndim) ndarray
+        Indices of elements that are non-zero. Indices are grouped by element.
+        This array will have shape ``(N, a.ndim)`` where ``N`` is the number of
+        non-zero items.
+
+    See Also
+    --------
+    where, nonzero
+
+    Notes
+    -----
+    ``np.argwhere(a)`` is almost the same as ``np.transpose(np.nonzero(a))``,
+    but produces a result of the correct shape for a 0D array.
+
+    The output of ``argwhere`` is not suitable for indexing arrays.
+    For this purpose use ``nonzero(a)`` instead.
+
+    Examples
+    --------
+    >>> x = np.arange(6).reshape(2,3)
+    >>> x
+    array([[0, 1, 2],
+           [3, 4, 5]])
+    >>> np.argwhere(x>1)
+    array([[0, 2],
+           [1, 0],
+           [1, 1],
+           [1, 2]])
+
+    """
+    # nonzero does not behave well on 0d, so promote to 1d
+    if np.ndim(a) == 0:
+        a = shape_base.atleast_1d(a)
+        # then remove the added dimension
+        return argwhere(a)[:,:0]
+    return transpose(nonzero(a))
+
+
+def _flatnonzero_dispatcher(a):
+    return (a,)
+
+
+@array_function_dispatch(_flatnonzero_dispatcher)
+def flatnonzero(a):
+    """
+    Return indices that are non-zero in the flattened version of a.
+
+    This is equivalent to ``np.nonzero(np.ravel(a))[0]``.
+
+    Parameters
+    ----------
+    a : array_like
+        Input data.
+
+    Returns
+    -------
+    res : ndarray
+        Output array, containing the indices of the elements of ``a.ravel()``
+        that are non-zero.
+
+    See Also
+    --------
+    nonzero : Return the indices of the non-zero elements of the input array.
+    ravel : Return a 1-D array containing the elements of the input array.
+
+    Examples
+    --------
+    >>> x = np.arange(-2, 3)
+    >>> x
+    array([-2, -1,  0,  1,  2])
+    >>> np.flatnonzero(x)
+    array([0, 1, 3, 4])
+
+    Use the indices of the non-zero elements as an index array to extract
+    these elements:
+
+    >>> x.ravel()[np.flatnonzero(x)]
+    array([-2, -1,  1,  2])
+
+    """
+    return np.nonzero(np.ravel(a))[0]
+
+
+def _correlate_dispatcher(a, v, mode=None):
+    return (a, v)
+
+
+@array_function_dispatch(_correlate_dispatcher)
+def correlate(a, v, mode='valid'):
+    r"""
+    Cross-correlation of two 1-dimensional sequences.
+
+    This function computes the correlation as generally defined in signal
+    processing texts:
+
+    .. math:: c_k = \sum_n a_{n+k} \cdot \overline{v_n}
+
+    with a and v sequences being zero-padded where necessary and
+    :math:`\overline x` denoting complex conjugation.
+
+    Parameters
+    ----------
+    a, v : array_like
+        Input sequences.
+    mode : {'valid', 'same', 'full'}, optional
+        Refer to the `convolve` docstring.  Note that the default
+        is 'valid', unlike `convolve`, which uses 'full'.
+    old_behavior : bool
+        `old_behavior` was removed in NumPy 1.10. If you need the old
+        behavior, use `multiarray.correlate`.
+
+    Returns
+    -------
+    out : ndarray
+        Discrete cross-correlation of `a` and `v`.
+
+    See Also
+    --------
+    convolve : Discrete, linear convolution of two one-dimensional sequences.
+    multiarray.correlate : Old, no conjugate, version of correlate.
+    scipy.signal.correlate : uses FFT which has superior performance on large arrays. 
+
+    Notes
+    -----
+    The definition of correlation above is not unique and sometimes correlation
+    may be defined differently. Another common definition is:
+
+    .. math:: c'_k = \sum_n a_{n} \cdot \overline{v_{n+k}}
+
+    which is related to :math:`c_k` by :math:`c'_k = c_{-k}`.
+
+    `numpy.correlate` may perform slowly in large arrays (i.e. n = 1e5) because it does
+    not use the FFT to compute the convolution; in that case, `scipy.signal.correlate` might
+    be preferable.
     
-_mode_from_name_dict = {'v': 0,
-                        's' : 1,
-                        'f' : 2}
-
-def _mode_from_name(mode):
-    if isinstance(mode, type("")):
-        return _mode_from_name_dict[mode.lower()[0]]
-    return mode
-
-def correlate(a,v,mode='valid'):
-    mode = _mode_from_name(mode)
-    return multiarray.correlate(a,v,mode)
-
-
-def convolve(a,v,mode='full'):
-    """Returns the discrete, linear convolution of 1-D
-    sequences a and v; mode can be 0 (valid), 1 (same), or 2 (full)
-    to specify size of the resulting sequence.
-    """
+
+    Examples
+    --------
+    >>> np.correlate([1, 2, 3], [0, 1, 0.5])
+    array([3.5])
+    >>> np.correlate([1, 2, 3], [0, 1, 0.5], "same")
+    array([2. ,  3.5,  3. ])
+    >>> np.correlate([1, 2, 3], [0, 1, 0.5], "full")
+    array([0.5,  2. ,  3.5,  3. ,  0. ])
+
+    Using complex sequences:
+
+    >>> np.correlate([1+1j, 2, 3-1j], [0, 1, 0.5j], 'full')
+    array([ 0.5-0.5j,  1.0+0.j ,  1.5-1.5j,  3.0-1.j ,  0.0+0.j ])
+
+    Note that you get the time reversed, complex conjugated result
+    (:math:`\overline{c_{-k}}`) when the two input sequences a and v change 
+    places:
+
+    >>> np.correlate([0, 1, 0.5j], [1+1j, 2, 3-1j], 'full')
+    array([ 0.0+0.j ,  3.0+1.j ,  1.5+1.5j,  1.0+0.j ,  0.5+0.5j])
+
+    """
+    return multiarray.correlate2(a, v, mode)
+
+
+def _convolve_dispatcher(a, v, mode=None):
+    return (a, v)
+
+
+@array_function_dispatch(_convolve_dispatcher)
+def convolve(a, v, mode='full'):
+    """
+    Returns the discrete, linear convolution of two one-dimensional sequences.
+
+    The convolution operator is often seen in signal processing, where it
+    models the effect of a linear time-invariant system on a signal [1]_.  In
+    probability theory, the sum of two independent random variables is
+    distributed according to the convolution of their individual
+    distributions.
+
+    If `v` is longer than `a`, the arrays are swapped before computation.
+
+    Parameters
+    ----------
+    a : (N,) array_like
+        First one-dimensional input array.
+    v : (M,) array_like
+        Second one-dimensional input array.
+    mode : {'full', 'valid', 'same'}, optional
+        'full':
+          By default, mode is 'full'.  This returns the convolution
+          at each point of overlap, with an output shape of (N+M-1,). At
+          the end-points of the convolution, the signals do not overlap
+          completely, and boundary effects may be seen.
+
+        'same':
+          Mode 'same' returns output of length ``max(M, N)``.  Boundary
+          effects are still visible.
+
+        'valid':
+          Mode 'valid' returns output of length
+          ``max(M, N) - min(M, N) + 1``.  The convolution product is only given
+          for points where the signals overlap completely.  Values outside
+          the signal boundary have no effect.
+
+    Returns
+    -------
+    out : ndarray
+        Discrete, linear convolution of `a` and `v`.
+
+    See Also
+    --------
+    scipy.signal.fftconvolve : Convolve two arrays using the Fast Fourier
+                               Transform.
+    scipy.linalg.toeplitz : Used to construct the convolution operator.
+    polymul : Polynomial multiplication. Same output as convolve, but also
+              accepts poly1d objects as input.
+
+    Notes
+    -----
+    The discrete convolution operation is defined as
+
+    .. math:: (a * v)_n = \\sum_{m = -\\infty}^{\\infty} a_m v_{n - m}
+
+    It can be shown that a convolution :math:`x(t) * y(t)` in time/space
+    is equivalent to the multiplication :math:`X(f) Y(f)` in the Fourier
+    domain, after appropriate padding (padding is necessary to prevent
+    circular convolution).  Since multiplication is more efficient (faster)
+    than convolution, the function `scipy.signal.fftconvolve` exploits the
+    FFT to calculate the convolution of large data-sets.
+
+    References
+    ----------
+    .. [1] Wikipedia, "Convolution",
+        https://en.wikipedia.org/wiki/Convolution
+
+    Examples
+    --------
+    Note how the convolution operator flips the second array
+    before "sliding" the two across one another:
+
+    >>> np.convolve([1, 2, 3], [0, 1, 0.5])
+    array([0. , 1. , 2.5, 4. , 1.5])
+
+    Only return the middle values of the convolution.
+    Contains boundary effects, where zeros are taken
+    into account:
+
+    >>> np.convolve([1,2,3],[0,1,0.5], 'same')
+    array([1. ,  2.5,  4. ])
+
+    The two arrays are of the same length, so there
+    is only one position where they completely overlap:
+
+    >>> np.convolve([1,2,3],[0,1,0.5], 'valid')
+    array([2.5])
+
+    """
+    a, v = array(a, copy=False, ndmin=1), array(v, copy=False, ndmin=1)
     if (len(v) > len(a)):
         a, v = v, a
-    mode = _mode_from_name(mode)
-    return multiarray.correlate(a,asarray(v)[::-1],mode)
-
-
-inner = multiarray.inner
-dot = multiarray.dot
-
-def outer(a,b):
-    """outer(a,b) returns the outer product of two vectors.
-    result(i,j) = a(i)*b(j) when a and b are vectors
-    Will accept any arguments that can be made into vectors.
+    if len(a) == 0:
+        raise ValueError('a cannot be empty')
+    if len(v) == 0:
+        raise ValueError('v cannot be empty')
+    return multiarray.correlate(a, v[::-1], mode)
+
+
+def _outer_dispatcher(a, b, out=None):
+    return (a, b, out)
+
+
+@array_function_dispatch(_outer_dispatcher)
+def outer(a, b, out=None):
+    """
+    Compute the outer product of two vectors.
+
+    Given two vectors, ``a = [a0, a1, ..., aM]`` and
+    ``b = [b0, b1, ..., bN]``,
+    the outer product [1]_ is::
+
+      [[a0*b0  a0*b1 ... a0*bN ]
+       [a1*b0    .
+       [ ...          .
+       [aM*b0            aM*bN ]]
+
+    Parameters
+    ----------
+    a : (M,) array_like
+        First input vector.  Input is flattened if
+        not already 1-dimensional.
+    b : (N,) array_like
+        Second input vector.  Input is flattened if
+        not already 1-dimensional.
+    out : (M, N) ndarray, optional
+        A location where the result is stored
+
+        .. versionadded:: 1.9.0
+
+    Returns
+    -------
+    out : (M, N) ndarray
+        ``out[i, j] = a[i] * b[j]``
+
+    See also
+    --------
+    inner
+    einsum : ``einsum('i,j->ij', a.ravel(), b.ravel())`` is the equivalent.
+    ufunc.outer : A generalization to dimensions other than 1D and other
+                  operations. ``np.multiply.outer(a.ravel(), b.ravel())``
+                  is the equivalent.
+    tensordot : ``np.tensordot(a.ravel(), b.ravel(), axes=((), ()))``
+                is the equivalent.
+
+    References
+    ----------
+    .. [1] : G. H. Golub and C. F. Van Loan, *Matrix Computations*, 3rd
+             ed., Baltimore, MD, Johns Hopkins University Press, 1996,
+             pg. 8.
+
+    Examples
+    --------
+    Make a (*very* coarse) grid for computing a Mandelbrot set:
+
+    >>> rl = np.outer(np.ones((5,)), np.linspace(-2, 2, 5))
+    >>> rl
+    array([[-2., -1.,  0.,  1.,  2.],
+           [-2., -1.,  0.,  1.,  2.],
+           [-2., -1.,  0.,  1.,  2.],
+           [-2., -1.,  0.,  1.,  2.],
+           [-2., -1.,  0.,  1.,  2.]])
+    >>> im = np.outer(1j*np.linspace(2, -2, 5), np.ones((5,)))
+    >>> im
+    array([[0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j],
+           [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],
+           [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],
+           [0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j],
+           [0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j]])
+    >>> grid = rl + im
+    >>> grid
+    array([[-2.+2.j, -1.+2.j,  0.+2.j,  1.+2.j,  2.+2.j],
+           [-2.+1.j, -1.+1.j,  0.+1.j,  1.+1.j,  2.+1.j],
+           [-2.+0.j, -1.+0.j,  0.+0.j,  1.+0.j,  2.+0.j],
+           [-2.-1.j, -1.-1.j,  0.-1.j,  1.-1.j,  2.-1.j],
+           [-2.-2.j, -1.-2.j,  0.-2.j,  1.-2.j,  2.-2.j]])
+
+    An example using a "vector" of letters:
+
+    >>> x = np.array(['a', 'b', 'c'], dtype=object)
+    >>> np.outer(x, [1, 2, 3])
+    array([['a', 'aa', 'aaa'],
+           ['b', 'bb', 'bbb'],
+           ['c', 'cc', 'ccc']], dtype=object)
+
     """
     a = asarray(a)
     b = asarray(b)
-    return a.ravel()[:,newaxis]*b.ravel()[newaxis,:]
-
-def vdot(a, b):
-    """Returns the dot product of 2 vectors (or anything that can be made into
-    a vector). NB: this is not the same as `dot`, as it takes the conjugate
-    of its first argument if complex and always returns a scalar."""
-    return dot(asarray(a).ravel().conj(), asarray(b).ravel())
-
-# try to import blas optimized dot if available
-try:
-    # importing this changes the dot function for basic 4 types
-    # to blas-optimized versions.
-    from _dotblas import dot, vdot, inner, alterdot, restoredot
-except ImportError:
-    def alterdot():
-        pass
-    def restoredot():
-        pass
-
-
-def _move_axis_to_0(a, axis):
-    if axis == 0:
-        return a
+    return multiply(a.ravel()[:, newaxis], b.ravel()[newaxis, :], out)
+
+
+def _tensordot_dispatcher(a, b, axes=None):
+    return (a, b)
+
+
+@array_function_dispatch(_tensordot_dispatcher)
+def tensordot(a, b, axes=2):
+    """
+    Compute tensor dot product along specified axes.
+
+    Given two tensors, `a` and `b`, and an array_like object containing
+    two array_like objects, ``(a_axes, b_axes)``, sum the products of
+    `a`'s and `b`'s elements (components) over the axes specified by
+    ``a_axes`` and ``b_axes``. The third argument can be a single non-negative
+    integer_like scalar, ``N``; if it is such, then the last ``N`` dimensions
+    of `a` and the first ``N`` dimensions of `b` are summed over.
+
+    Parameters
+    ----------
+    a, b : array_like
+        Tensors to "dot".
+
+    axes : int or (2,) array_like
+        * integer_like
+          If an int N, sum over the last N axes of `a` and the first N axes
+          of `b` in order. The sizes of the corresponding axes must match.
+        * (2,) array_like
+          Or, a list of axes to be summed over, first sequence applying to `a`,
+          second to `b`. Both elements array_like must be of the same length.
+
+    Returns
+    -------
+    output : ndarray
+        The tensor dot product of the input.
+
+    See Also
+    --------
+    dot, einsum
+
+    Notes
+    -----
+    Three common use cases are:
+        * ``axes = 0`` : tensor product :math:`a\\otimes b`
+        * ``axes = 1`` : tensor dot product :math:`a\\cdot b`
+        * ``axes = 2`` : (default) tensor double contraction :math:`a:b`
+
+    When `axes` is integer_like, the sequence for evaluation will be: first
+    the -Nth axis in `a` and 0th axis in `b`, and the -1th axis in `a` and
+    Nth axis in `b` last.
+
+    When there is more than one axis to sum over - and they are not the last
+    (first) axes of `a` (`b`) - the argument `axes` should consist of
+    two sequences of the same length, with the first axis to sum over given
+    first in both sequences, the second axis second, and so forth.
+
+    The shape of the result consists of the non-contracted axes of the
+    first tensor, followed by the non-contracted axes of the second.
+
+    Examples
+    --------
+    A "traditional" example:
+
+    >>> a = np.arange(60.).reshape(3,4,5)
+    >>> b = np.arange(24.).reshape(4,3,2)
+    >>> c = np.tensordot(a,b, axes=([1,0],[0,1]))
+    >>> c.shape
+    (5, 2)
+    >>> c
+    array([[4400., 4730.],
+           [4532., 4874.],
+           [4664., 5018.],
+           [4796., 5162.],
+           [4928., 5306.]])
+    >>> # A slower but equivalent way of computing the same...
+    >>> d = np.zeros((5,2))
+    >>> for i in range(5):
+    ...   for j in range(2):
+    ...     for k in range(3):
+    ...       for n in range(4):
+    ...         d[i,j] += a[k,n,i] * b[n,k,j]
+    >>> c == d
+    array([[ True,  True],
+           [ True,  True],
+           [ True,  True],
+           [ True,  True],
+           [ True,  True]])
+
+    An extended example taking advantage of the overloading of + and \\*:
+
+    >>> a = np.array(range(1, 9))
+    >>> a.shape = (2, 2, 2)
+    >>> A = np.array(('a', 'b', 'c', 'd'), dtype=object)
+    >>> A.shape = (2, 2)
+    >>> a; A
+    array([[[1, 2],
+            [3, 4]],
+           [[5, 6],
+            [7, 8]]])
+    array([['a', 'b'],
+           ['c', 'd']], dtype=object)
+
+    >>> np.tensordot(a, A) # third argument default is 2 for double-contraction
+    array(['abbcccdddd', 'aaaaabbbbbbcccccccdddddddd'], dtype=object)
+
+    >>> np.tensordot(a, A, 1)
+    array([[['acc', 'bdd'],
+            ['aaacccc', 'bbbdddd']],
+           [['aaaaacccccc', 'bbbbbdddddd'],
+            ['aaaaaaacccccccc', 'bbbbbbbdddddddd']]], dtype=object)
+
+    >>> np.tensordot(a, A, 0) # tensor product (result too long to incl.)
+    array([[[[['a', 'b'],
+              ['c', 'd']],
+              ...
+
+    >>> np.tensordot(a, A, (0, 1))
+    array([[['abbbbb', 'cddddd'],
+            ['aabbbbbb', 'ccdddddd']],
+           [['aaabbbbbbb', 'cccddddddd'],
+            ['aaaabbbbbbbb', 'ccccdddddddd']]], dtype=object)
+
+    >>> np.tensordot(a, A, (2, 1))
+    array([[['abb', 'cdd'],
+            ['aaabbbb', 'cccdddd']],
+           [['aaaaabbbbbb', 'cccccdddddd'],
+            ['aaaaaaabbbbbbbb', 'cccccccdddddddd']]], dtype=object)
+
+    >>> np.tensordot(a, A, ((0, 1), (0, 1)))
+    array(['abbbcccccddddddd', 'aabbbbccccccdddddddd'], dtype=object)
+
+    >>> np.tensordot(a, A, ((2, 1), (1, 0)))
+    array(['acccbbdddd', 'aaaaacccccccbbbbbbdddddddd'], dtype=object)
+
+    """
+    try:
+        iter(axes)
+    except Exception:
+        axes_a = list(range(-axes, 0))
+        axes_b = list(range(0, axes))
+    else:
+        axes_a, axes_b = axes
+    try:
+        na = len(axes_a)
+        axes_a = list(axes_a)
+    except TypeError:
+        axes_a = [axes_a]
+        na = 1
+    try:
+        nb = len(axes_b)
+        axes_b = list(axes_b)
+    except TypeError:
+        axes_b = [axes_b]
+        nb = 1
+
+    a, b = asarray(a), asarray(b)
+    as_ = a.shape
+    nda = a.ndim
+    bs = b.shape
+    ndb = b.ndim
+    equal = True
+    if na != nb:
+        equal = False
+    else:
+        for k in range(na):
+            if as_[axes_a[k]] != bs[axes_b[k]]:
+                equal = False
+                break
+            if axes_a[k] < 0:
+                axes_a[k] += nda
+            if axes_b[k] < 0:
+                axes_b[k] += ndb
+    if not equal:
+        raise ValueError("shape-mismatch for sum")
+
+    # Move the axes to sum over to the end of "a"
+    # and to the front of "b"
+    notin = [k for k in range(nda) if k not in axes_a]
+    newaxes_a = notin + axes_a
+    N2 = 1
+    for axis in axes_a:
+        N2 *= as_[axis]
+    newshape_a = (int(multiply.reduce([as_[ax] for ax in notin])), N2)
+    olda = [as_[axis] for axis in notin]
+
+    notin = [k for k in range(ndb) if k not in axes_b]
+    newaxes_b = axes_b + notin
+    N2 = 1
+    for axis in axes_b:
+        N2 *= bs[axis]
+    newshape_b = (N2, int(multiply.reduce([bs[ax] for ax in notin])))
+    oldb = [bs[axis] for axis in notin]
+
+    at = a.transpose(newaxes_a).reshape(newshape_a)
+    bt = b.transpose(newaxes_b).reshape(newshape_b)
+    res = dot(at, bt)
+    return res.reshape(olda + oldb)
+
+
+def _roll_dispatcher(a, shift, axis=None):
+    return (a,)
+
+
+@array_function_dispatch(_roll_dispatcher)
+def roll(a, shift, axis=None):
+    """
+    Roll array elements along a given axis.
+
+    Elements that roll beyond the last position are re-introduced at
+    the first.
+
+    Parameters
+    ----------
+    a : array_like
+        Input array.
+    shift : int or tuple of ints
+        The number of places by which elements are shifted.  If a tuple,
+        then `axis` must be a tuple of the same size, and each of the
+        given axes is shifted by the corresponding number.  If an int
+        while `axis` is a tuple of ints, then the same value is used for
+        all given axes.
+    axis : int or tuple of ints, optional
+        Axis or axes along which elements are shifted.  By default, the
+        array is flattened before shifting, after which the original
+        shape is restored.
+
+    Returns
+    -------
+    res : ndarray
+        Output array, with the same shape as `a`.
+
+    See Also
+    --------
+    rollaxis : Roll the specified axis backwards, until it lies in a
+               given position.
+
+    Notes
+    -----
+    .. versionadded:: 1.12.0
+
+    Supports rolling over multiple dimensions simultaneously.
+
+    Examples
+    --------
+    >>> x = np.arange(10)
+    >>> np.roll(x, 2)
+    array([8, 9, 0, 1, 2, 3, 4, 5, 6, 7])
+    >>> np.roll(x, -2)
+    array([2, 3, 4, 5, 6, 7, 8, 9, 0, 1])
+
+    >>> x2 = np.reshape(x, (2, 5))
+    >>> x2
+    array([[0, 1, 2, 3, 4],
+           [5, 6, 7, 8, 9]])
+    >>> np.roll(x2, 1)
+    array([[9, 0, 1, 2, 3],
+           [4, 5, 6, 7, 8]])
+    >>> np.roll(x2, -1)
+    array([[1, 2, 3, 4, 5],
+           [6, 7, 8, 9, 0]])
+    >>> np.roll(x2, 1, axis=0)
+    array([[5, 6, 7, 8, 9],
+           [0, 1, 2, 3, 4]])
+    >>> np.roll(x2, -1, axis=0)
+    array([[5, 6, 7, 8, 9],
+           [0, 1, 2, 3, 4]])
+    >>> np.roll(x2, 1, axis=1)
+    array([[4, 0, 1, 2, 3],
+           [9, 5, 6, 7, 8]])
+    >>> np.roll(x2, -1, axis=1)
+    array([[1, 2, 3, 4, 0],
+           [6, 7, 8, 9, 5]])
+    >>> np.roll(x2, (1, 1), axis=(1, 0))
+    array([[9, 5, 6, 7, 8],
+           [4, 0, 1, 2, 3]])
+    >>> np.roll(x2, (2, 1), axis=(1, 0))
+    array([[8, 9, 5, 6, 7],
+           [3, 4, 0, 1, 2]])
+
+    """
+    a = asanyarray(a)
+    if axis is None:
+        return roll(a.ravel(), shift, 0).reshape(a.shape)
+
+    else:
+        axis = normalize_axis_tuple(axis, a.ndim, allow_duplicate=True)
+        broadcasted = broadcast(shift, axis)
+        if broadcasted.ndim > 1:
+            raise ValueError(
+                "'shift' and 'axis' should be scalars or 1D sequences")
+        shifts = {ax: 0 for ax in range(a.ndim)}
+        for sh, ax in broadcasted:
+            shifts[ax] += sh
+
+        rolls = [((slice(None), slice(None)),)] * a.ndim
+        for ax, offset in shifts.items():
+            offset %= a.shape[ax] or 1  # If `a` is empty, nothing matters.
+            if offset:
+                # (original, result), (original, result)
+                rolls[ax] = ((slice(None, -offset), slice(offset, None)),
+                             (slice(-offset, None), slice(None, offset)))
+
+        result = empty_like(a)
+        for indices in itertools.product(*rolls):
+            arr_index, res_index = zip(*indices)
+            result[res_index] = a[arr_index]
+
+        return result
+
+
+def _rollaxis_dispatcher(a, axis, start=None):
+    return (a,)
+
+
+@array_function_dispatch(_rollaxis_dispatcher)
+def rollaxis(a, axis, start=0):
+    """
+    Roll the specified axis backwards, until it lies in a given position.
+
+    This function continues to be supported for backward compatibility, but you
+    should prefer `moveaxis`. The `moveaxis` function was added in NumPy
+    1.11.
+
+    Parameters
+    ----------
+    a : ndarray
+        Input array.
+    axis : int
+        The axis to be rolled. The positions of the other axes do not
+        change relative to one another.
+    start : int, optional
+        When ``start <= axis``, the axis is rolled back until it lies in
+        this position. When ``start > axis``, the axis is rolled until it
+        lies before this position. The default, 0, results in a "complete"
+        roll. The following table describes how negative values of ``start``
+        are interpreted:
+
+        .. table::
+           :align: left
+
+           +-------------------+----------------------+
+           |     ``start``     | Normalized ``start`` |
+           +===================+======================+
+           | ``-(arr.ndim+1)`` | raise ``AxisError``  |
+           +-------------------+----------------------+
+           | ``-arr.ndim``     | 0                    |
+           +-------------------+----------------------+
+           | |vdots|           | |vdots|              |
+           +-------------------+----------------------+
+           | ``-1``            | ``arr.ndim-1``       |
+           +-------------------+----------------------+
+           | ``0``             | ``0``                |
+           +-------------------+----------------------+
+           | |vdots|           | |vdots|              |
+           +-------------------+----------------------+
+           | ``arr.ndim``      | ``arr.ndim``         |
+           +-------------------+----------------------+
+           | ``arr.ndim + 1``  | raise ``AxisError``  |
+           +-------------------+----------------------+
+           
+        .. |vdots|   unicode:: U+22EE .. Vertical Ellipsis
+
+    Returns
+    -------
+    res : ndarray
+        For NumPy >= 1.10.0 a view of `a` is always returned. For earlier
+        NumPy versions a view of `a` is returned only if the order of the
+        axes is changed, otherwise the input array is returned.
+
+    See Also
+    --------
+    moveaxis : Move array axes to new positions.
+    roll : Roll the elements of an array by a number of positions along a
+        given axis.
+
+    Examples
+    --------
+    >>> a = np.ones((3,4,5,6))
+    >>> np.rollaxis(a, 3, 1).shape
+    (3, 6, 4, 5)
+    >>> np.rollaxis(a, 2).shape
+    (5, 3, 4, 6)
+    >>> np.rollaxis(a, 1, 4).shape
+    (3, 5, 6, 4)
+
+    """
     n = a.ndim
-    if axis < 0:
-        axis += n
-    axes = range(1, axis+1) + [0,] + range(axis+1, n)
+    axis = normalize_axis_index(axis, n)
+    if start < 0:
+        start += n
+    msg = "'%s' arg requires %d <= %s < %d, but %d was passed in"
+    if not (0 <= start < n + 1):
+        raise AxisError(msg % ('start', -n, 'start', n + 1, start))
+    if axis < start:
+        # it's been removed
+        start -= 1
+    if axis == start:
+        return a[...]
+    axes = list(range(0, n))
+    axes.remove(axis)
+    axes.insert(start, axis)
     return a.transpose(axes)
 
+
+def normalize_axis_tuple(axis, ndim, argname=None, allow_duplicate=False):
+    """
+    Normalizes an axis argument into a tuple of non-negative integer axes.
+
+    This handles shorthands such as ``1`` and converts them to ``(1,)``,
+    as well as performing the handling of negative indices covered by
+    `normalize_axis_index`.
+
+    By default, this forbids axes from being specified multiple times.
+
+    Used internally by multi-axis-checking logic.
+
+    .. versionadded:: 1.13.0
+
+    Parameters
+    ----------
+    axis : int, iterable of int
+        The un-normalized index or indices of the axis.
+    ndim : int
+        The number of dimensions of the array that `axis` should be normalized
+        against.
+    argname : str, optional
+        A prefix to put before the error message, typically the name of the
+        argument.
+    allow_duplicate : bool, optional
+        If False, the default, disallow an axis from being specified twice.
+
+    Returns
+    -------
+    normalized_axes : tuple of int
+        The normalized axis index, such that `0 <= normalized_axis < ndim`
+
+    Raises
+    ------
+    AxisError
+        If any axis provided is out of range
+    ValueError
+        If an axis is repeated
+
+    See also
+    --------
+    normalize_axis_index : normalizing a single scalar axis
+    """
+    # Optimization to speed-up the most common cases.
+    if type(axis) not in (tuple, list):
+        try:
+            axis = [operator.index(axis)]
+        except TypeError:
+            pass
+    # Going via an iterator directly is slower than via list comprehension.
+    axis = tuple([normalize_axis_index(ax, ndim, argname) for ax in axis])
+    if not allow_duplicate and len(set(axis)) != len(axis):
+        if argname:
+            raise ValueError('repeated axis in `{}` argument'.format(argname))
+        else:
+            raise ValueError('repeated axis')
+    return axis
+
+
+def _moveaxis_dispatcher(a, source, destination):
+    return (a,)
+
+
+@array_function_dispatch(_moveaxis_dispatcher)
+def moveaxis(a, source, destination):
+    """
+    Move axes of an array to new positions.
+
+    Other axes remain in their original order.
+
+    .. versionadded:: 1.11.0
+
+    Parameters
+    ----------
+    a : np.ndarray
+        The array whose axes should be reordered.
+    source : int or sequence of int
+        Original positions of the axes to move. These must be unique.
+    destination : int or sequence of int
+        Destination positions for each of the original axes. These must also be
+        unique.
+
+    Returns
+    -------
+    result : np.ndarray
+        Array with moved axes. This array is a view of the input array.
+
+    See Also
+    --------
+    transpose : Permute the dimensions of an array.
+    swapaxes : Interchange two axes of an array.
+
+    Examples
+    --------
+    >>> x = np.zeros((3, 4, 5))
+    >>> np.moveaxis(x, 0, -1).shape
+    (4, 5, 3)
+    >>> np.moveaxis(x, -1, 0).shape
+    (5, 3, 4)
+
+    These all achieve the same result:
+
+    >>> np.transpose(x).shape
+    (5, 4, 3)
+    >>> np.swapaxes(x, 0, -1).shape
+    (5, 4, 3)
+    >>> np.moveaxis(x, [0, 1], [-1, -2]).shape
+    (5, 4, 3)
+    >>> np.moveaxis(x, [0, 1, 2], [-1, -2, -3]).shape
+    (5, 4, 3)
+
+    """
+    try:
+        # allow duck-array types if they define transpose
+        transpose = a.transpose
+    except AttributeError:
+        a = asarray(a)
+        transpose = a.transpose
+
+    source = normalize_axis_tuple(source, a.ndim, 'source')
+    destination = normalize_axis_tuple(destination, a.ndim, 'destination')
+    if len(source) != len(destination):
+        raise ValueError('`source` and `destination` arguments must have '
+                         'the same number of elements')
+
+    order = [n for n in range(a.ndim) if n not in source]
+
+    for dest, src in sorted(zip(destination, source)):
+        order.insert(dest, src)
+
+    result = transpose(order)
+    return result
+
+
+def _cross_dispatcher(a, b, axisa=None, axisb=None, axisc=None, axis=None):
+    return (a, b)
+
+
+@array_function_dispatch(_cross_dispatcher)
 def cross(a, b, axisa=-1, axisb=-1, axisc=-1, axis=None):
-    """Return the cross product of two (arrays of) vectors.
-
-    The cross product is performed over the last axis of a and b by default,
-    and can handle axes with dimensions 2 and 3. For a dimension of 2,
-    the z-component of the equivalent three-dimensional cross product is
-    returned.
+    """
+    Return the cross product of two (arrays of) vectors.
+
+    The cross product of `a` and `b` in :math:`R^3` is a vector perpendicular
+    to both `a` and `b`.  If `a` and `b` are arrays of vectors, the vectors
+    are defined by the last axis of `a` and `b` by default, and these axes
+    can have dimensions 2 or 3.  Where the dimension of either `a` or `b` is
+    2, the third component of the input vector is assumed to be zero and the
+    cross product calculated accordingly.  In cases where both input vectors
+    have dimension 2, the z-component of the cross product is returned.
+
+    Parameters
+    ----------
+    a : array_like
+        Components of the first vector(s).
+    b : array_like
+        Components of the second vector(s).
+    axisa : int, optional
+        Axis of `a` that defines the vector(s).  By default, the last axis.
+    axisb : int, optional
+        Axis of `b` that defines the vector(s).  By default, the last axis.
+    axisc : int, optional
+        Axis of `c` containing the cross product vector(s).  Ignored if
+        both input vectors have dimension 2, as the return is scalar.
+        By default, the last axis.
+    axis : int, optional
+        If defined, the axis of `a`, `b` and `c` that defines the vector(s)
+        and cross product(s).  Overrides `axisa`, `axisb` and `axisc`.
+
+    Returns
+    -------
+    c : ndarray
+        Vector cross product(s).
+
+    Raises
+    ------
+    ValueError
+        When the dimension of the vector(s) in `a` and/or `b` does not
+        equal 2 or 3.
+
+    See Also
+    --------
+    inner : Inner product
+    outer : Outer product.
+    ix_ : Construct index arrays.
+
+    Notes
+    -----
+    .. versionadded:: 1.9.0
+
+    Supports full broadcasting of the inputs.
+
+    Examples
+    --------
+    Vector cross-product.
+
+    >>> x = [1, 2, 3]
+    >>> y = [4, 5, 6]
+    >>> np.cross(x, y)
+    array([-3,  6, -3])
+
+    One vector with dimension 2.
+
+    >>> x = [1, 2]
+    >>> y = [4, 5, 6]
+    >>> np.cross(x, y)
+    array([12, -6, -3])
+
+    Equivalently:
+
+    >>> x = [1, 2, 0]
+    >>> y = [4, 5, 6]
+    >>> np.cross(x, y)
+    array([12, -6, -3])
+
+    Both vectors with dimension 2.
+
+    >>> x = [1,2]
+    >>> y = [4,5]
+    >>> np.cross(x, y)
+    array(-3)
+
+    Multiple vector cross-products. Note that the direction of the cross
+    product vector is defined by the *right-hand rule*.
+
+    >>> x = np.array([[1,2,3], [4,5,6]])
+    >>> y = np.array([[4,5,6], [1,2,3]])
+    >>> np.cross(x, y)
+    array([[-3,  6, -3],
+           [ 3, -6,  3]])
+
+    The orientation of `c` can be changed using the `axisc` keyword.
+
+    >>> np.cross(x, y, axisc=0)
+    array([[-3,  3],
+           [ 6, -6],
+           [-3,  3]])
+
+    Change the vector definition of `x` and `y` using `axisa` and `axisb`.
+
+    >>> x = np.array([[1,2,3], [4,5,6], [7, 8, 9]])
+    >>> y = np.array([[7, 8, 9], [4,5,6], [1,2,3]])
+    >>> np.cross(x, y)
+    array([[ -6,  12,  -6],
+           [  0,   0,   0],
+           [  6, -12,   6]])
+    >>> np.cross(x, y, axisa=0, axisb=0)
+    array([[-24,  48, -24],
+           [-30,  60, -30],
+           [-36,  72, -36]])
+
     """
     if axis is not None:
-        axisa,axisb,axisc=(axis,)*3
-    a = _move_axis_to_0(asarray(a), axisa)
-    b = _move_axis_to_0(asarray(b), axisb)
-    msg = "incompatible dimensions for cross product\n"\
-          "(dimension must be 2 or 3)"
-    if (a.shape[0] not in [2,3]) or (b.shape[0] not in [2,3]):
+        axisa, axisb, axisc = (axis,) * 3
+    a = asarray(a)
+    b = asarray(b)
+    # Check axisa and axisb are within bounds
+    axisa = normalize_axis_index(axisa, a.ndim, msg_prefix='axisa')
+    axisb = normalize_axis_index(axisb, b.ndim, msg_prefix='axisb')
+
+    # Move working axis to the end of the shape
+    a = moveaxis(a, axisa, -1)
+    b = moveaxis(b, axisb, -1)
+    msg = ("incompatible dimensions for cross product\n"
+           "(dimension must be 2 or 3)")
+    if a.shape[-1] not in (2, 3) or b.shape[-1] not in (2, 3):
         raise ValueError(msg)
-    if a.shape[0] == 2:
-        if (b.shape[0] == 2):
-            cp = a[0]*b[1] - a[1]*b[0]
-            if cp.ndim == 0:
-                return cp
-            else:
-                return cp.swapaxes(0,axisc)
+
+    # Create the output array
+    shape = broadcast(a[..., 0], b[..., 0]).shape
+    if a.shape[-1] == 3 or b.shape[-1] == 3:
+        shape += (3,)
+        # Check axisc is within bounds
+        axisc = normalize_axis_index(axisc, len(shape), msg_prefix='axisc')
+    dtype = promote_types(a.dtype, b.dtype)
+    cp = empty(shape, dtype)
+
+    # create local aliases for readability
+    a0 = a[..., 0]
+    a1 = a[..., 1]
+    if a.shape[-1] == 3:
+        a2 = a[..., 2]
+    b0 = b[..., 0]
+    b1 = b[..., 1]
+    if b.shape[-1] == 3:
+        b2 = b[..., 2]
+    if cp.ndim != 0 and cp.shape[-1] == 3:
+        cp0 = cp[..., 0]
+        cp1 = cp[..., 1]
+        cp2 = cp[..., 2]
+
+    if a.shape[-1] == 2:
+        if b.shape[-1] == 2:
+            # a0 * b1 - a1 * b0
+            multiply(a0, b1, out=cp)
+            cp -= a1 * b0
+            return cp
         else:
-            x = a[1]*b[2]
-            y = -a[0]*b[2]
-            z = a[0]*b[1] - a[1]*b[0]
-    elif a.shape[0] == 3:
-        if (b.shape[0] == 3):
-            x = a[1]*b[2] - a[2]*b[1]
-            y = a[2]*b[0] - a[0]*b[2]
-            z = a[0]*b[1] - a[1]*b[0]
+            assert b.shape[-1] == 3
+            # cp0 = a1 * b2 - 0  (a2 = 0)
+            # cp1 = 0 - a0 * b2  (a2 = 0)
+            # cp2 = a0 * b1 - a1 * b0
+            multiply(a1, b2, out=cp0)
+            multiply(a0, b2, out=cp1)
+            negative(cp1, out=cp1)
+            multiply(a0, b1, out=cp2)
+            cp2 -= a1 * b0
+    else:
+        assert a.shape[-1] == 3
+        if b.shape[-1] == 3:
+            # cp0 = a1 * b2 - a2 * b1
+            # cp1 = a2 * b0 - a0 * b2
+            # cp2 = a0 * b1 - a1 * b0
+            multiply(a1, b2, out=cp0)
+            tmp = array(a2 * b1)
+            cp0 -= tmp
+            multiply(a2, b0, out=cp1)
+            multiply(a0, b2, out=tmp)
+            cp1 -= tmp
+            multiply(a0, b1, out=cp2)
+            multiply(a1, b0, out=tmp)
+            cp2 -= tmp
         else:
-            x = -a[2]*b[1]
-            y = a[2]*b[0]
-            z = a[0]*b[1] - a[1]*b[0]
-    cp = array([x,y,z])
-    if cp.ndim == 1:
-        return cp
+            assert b.shape[-1] == 2
+            # cp0 = 0 - a2 * b1  (b2 = 0)
+            # cp1 = a2 * b0 - 0  (b2 = 0)
+            # cp2 = a0 * b1 - a1 * b0
+            multiply(a2, b1, out=cp0)
+            negative(cp0, out=cp0)
+            multiply(a2, b0, out=cp1)
+            multiply(a0, b1, out=cp2)
+            cp2 -= a1 * b0
+
+    return moveaxis(cp, -1, axisc)
+
+
+little_endian = (sys.byteorder == 'little')
+
+
+@set_module('numpy')
+def indices(dimensions, dtype=int, sparse=False):
+    """
+    Return an array representing the indices of a grid.
+
+    Compute an array where the subarrays contain index values 0, 1, ...
+    varying only along the corresponding axis.
+
+    Parameters
+    ----------
+    dimensions : sequence of ints
+        The shape of the grid.
+    dtype : dtype, optional
+        Data type of the result.
+    sparse : boolean, optional
+        Return a sparse representation of the grid instead of a dense
+        representation. Default is False.
+
+        .. versionadded:: 1.17
+
+    Returns
+    -------
+    grid : one ndarray or tuple of ndarrays
+        If sparse is False:
+            Returns one array of grid indices,
+            ``grid.shape = (len(dimensions),) + tuple(dimensions)``.
+        If sparse is True:
+            Returns a tuple of arrays, with
+            ``grid[i].shape = (1, ..., 1, dimensions[i], 1, ..., 1)`` with
+            dimensions[i] in the ith place
+
+    See Also
+    --------
+    mgrid, ogrid, meshgrid
+
+    Notes
+    -----
+    The output shape in the dense case is obtained by prepending the number
+    of dimensions in front of the tuple of dimensions, i.e. if `dimensions`
+    is a tuple ``(r0, ..., rN-1)`` of length ``N``, the output shape is
+    ``(N, r0, ..., rN-1)``.
+
+    The subarrays ``grid[k]`` contains the N-D array of indices along the
+    ``k-th`` axis. Explicitly::
+
+        grid[k, i0, i1, ..., iN-1] = ik
+
+    Examples
+    --------
+    >>> grid = np.indices((2, 3))
+    >>> grid.shape
+    (2, 2, 3)
+    >>> grid[0]        # row indices
+    array([[0, 0, 0],
+           [1, 1, 1]])
+    >>> grid[1]        # column indices
+    array([[0, 1, 2],
+           [0, 1, 2]])
+
+    The indices can be used as an index into an array.
+
+    >>> x = np.arange(20).reshape(5, 4)
+    >>> row, col = np.indices((2, 3))
+    >>> x[row, col]
+    array([[0, 1, 2],
+           [4, 5, 6]])
+
+    Note that it would be more straightforward in the above example to
+    extract the required elements directly with ``x[:2, :3]``.
+
+    If sparse is set to true, the grid will be returned in a sparse
+    representation.
+
+    >>> i, j = np.indices((2, 3), sparse=True)
+    >>> i.shape
+    (2, 1)
+    >>> j.shape
+    (1, 3)
+    >>> i        # row indices
+    array([[0],
+           [1]])
+    >>> j        # column indices
+    array([[0, 1, 2]])
+
+    """
+    dimensions = tuple(dimensions)
+    N = len(dimensions)
+    shape = (1,)*N
+    if sparse:
+        res = tuple()
     else:
-        return cp.swapaxes(0,axisc)
-
-
-#Use numarray's printing function
-from arrayprint import array2string, get_printoptions, set_printoptions
-
-_typelessdata = [int_, float_, complex_]
-if issubclass(intc, int):
-    _typelessdata.append(intc)
-
-if issubclass(longlong, int):
-    _typelessdata.append(longlong)
-
-def array_repr(arr, max_line_width=None, precision=None, suppress_small=None):
-    if arr.size > 0 or arr.shape==(0,):
-        lst = array2string(arr, max_line_width, precision, suppress_small,
-                           ', ', "array(")
-    else: # show zero-length shape unless it is (0,)
-        lst = "[], shape=%s" % (repr(arr.shape),)
-    typeless = arr.dtype.type in _typelessdata
-
-    if arr.__class__ is not ndarray:
-        cName= arr.__class__.__name__
+        res = empty((N,)+dimensions, dtype=dtype)
+    for i, dim in enumerate(dimensions):
+        idx = arange(dim, dtype=dtype).reshape(
+            shape[:i] + (dim,) + shape[i+1:]
+        )
+        if sparse:
+            res = res + (idx,)
+        else:
+            res[i] = idx
+    return res
+
+
+def _fromfunction_dispatcher(function, shape, *, dtype=None, like=None, **kwargs):
+    return (like,)
+
+
+@set_array_function_like_doc
+@set_module('numpy')
+def fromfunction(function, shape, *, dtype=float, like=None, **kwargs):
+    """
+    Construct an array by executing a function over each coordinate.
+
+    The resulting array therefore has a value ``fn(x, y, z)`` at
+    coordinate ``(x, y, z)``.
+
+    Parameters
+    ----------
+    function : callable
+        The function is called with N parameters, where N is the rank of
+        `shape`.  Each parameter represents the coordinates of the array
+        varying along a specific axis.  For example, if `shape`
+        were ``(2, 2)``, then the parameters would be
+        ``array([[0, 0], [1, 1]])`` and ``array([[0, 1], [0, 1]])``
+    shape : (N,) tuple of ints
+        Shape of the output array, which also determines the shape of
+        the coordinate arrays passed to `function`.
+    dtype : data-type, optional
+        Data-type of the coordinate arrays passed to `function`.
+        By default, `dtype` is float.
+    ${ARRAY_FUNCTION_LIKE}
+
+        .. versionadded:: 1.20.0
+
+    Returns
+    -------
+    fromfunction : any
+        The result of the call to `function` is passed back directly.
+        Therefore the shape of `fromfunction` is completely determined by
+        `function`.  If `function` returns a scalar value, the shape of
+        `fromfunction` would not match the `shape` parameter.
+
+    See Also
+    --------
+    indices, meshgrid
+
+    Notes
+    -----
+    Keywords other than `dtype` are passed to `function`.
+
+    Examples
+    --------
+    >>> np.fromfunction(lambda i, j: i, (2, 2), dtype=float)
+    array([[0., 0.],
+           [1., 1.]])
+           
+    >>> np.fromfunction(lambda i, j: j, (2, 2), dtype=float)    
+    array([[0., 1.],
+           [0., 1.]])
+           
+    >>> np.fromfunction(lambda i, j: i == j, (3, 3), dtype=int)
+    array([[ True, False, False],
+           [False,  True, False],
+           [False, False,  True]])
+
+    >>> np.fromfunction(lambda i, j: i + j, (3, 3), dtype=int)
+    array([[0, 1, 2],
+           [1, 2, 3],
+           [2, 3, 4]])
+
+    """
+    if like is not None:
+        return _fromfunction_with_like(function, shape, dtype=dtype, like=like, **kwargs)
+
+    args = indices(shape, dtype=dtype)
+    return function(*args, **kwargs)
+
+
+_fromfunction_with_like = array_function_dispatch(
+    _fromfunction_dispatcher
+)(fromfunction)
+
+
+def _frombuffer(buf, dtype, shape, order):
+    return frombuffer(buf, dtype=dtype).reshape(shape, order=order)
+
+
+@set_module('numpy')
+def isscalar(element):
+    """
+    Returns True if the type of `element` is a scalar type.
+
+    Parameters
+    ----------
+    element : any
+        Input argument, can be of any type and shape.
+
+    Returns
+    -------
+    val : bool
+        True if `element` is a scalar type, False if it is not.
+
+    See Also
+    --------
+    ndim : Get the number of dimensions of an array
+
+    Notes
+    -----
+    If you need a stricter way to identify a *numerical* scalar, use
+    ``isinstance(x, numbers.Number)``, as that returns ``False`` for most
+    non-numerical elements such as strings.
+
+    In most cases ``np.ndim(x) == 0`` should be used instead of this function,
+    as that will also return true for 0d arrays. This is how numpy overloads
+    functions in the style of the ``dx`` arguments to `gradient` and the ``bins``
+    argument to `histogram`. Some key differences:
+
+    +--------------------------------------+---------------+-------------------+
+    | x                                    |``isscalar(x)``|``np.ndim(x) == 0``|
+    +======================================+===============+===================+
+    | PEP 3141 numeric objects (including  | ``True``      | ``True``          |
+    | builtins)                            |               |                   |
+    +--------------------------------------+---------------+-------------------+
+    | builtin string and buffer objects    | ``True``      | ``True``          |
+    +--------------------------------------+---------------+-------------------+
+    | other builtin objects, like          | ``False``     | ``True``          |
+    | `pathlib.Path`, `Exception`,         |               |                   |
+    | the result of `re.compile`           |               |                   |
+    +--------------------------------------+---------------+-------------------+
+    | third-party objects like             | ``False``     | ``True``          |
+    | `matplotlib.figure.Figure`           |               |                   |
+    +--------------------------------------+---------------+-------------------+
+    | zero-dimensional numpy arrays        | ``False``     | ``True``          |
+    +--------------------------------------+---------------+-------------------+
+    | other numpy arrays                   | ``False``     | ``False``         |
+    +--------------------------------------+---------------+-------------------+
+    | `list`, `tuple`, and other sequence  | ``False``     | ``False``         |
+    | objects                              |               |                   |
+    +--------------------------------------+---------------+-------------------+
+
+    Examples
+    --------
+    >>> np.isscalar(3.1)
+    True
+    >>> np.isscalar(np.array(3.1))
+    False
+    >>> np.isscalar([3.1])
+    False
+    >>> np.isscalar(False)
+    True
+    >>> np.isscalar('numpy')
+    True
+
+    NumPy supports PEP 3141 numbers:
+
+    >>> from fractions import Fraction
+    >>> np.isscalar(Fraction(5, 17))
+    True
+    >>> from numbers import Number
+    >>> np.isscalar(Number())
+    True
+
+    """
+    return (isinstance(element, generic)
+            or type(element) in ScalarType
+            or isinstance(element, numbers.Number))
+
+
+@set_module('numpy')
+def binary_repr(num, width=None):
+    """
+    Return the binary representation of the input number as a string.
+
+    For negative numbers, if width is not given, a minus sign is added to the
+    front. If width is given, the two's complement of the number is
+    returned, with respect to that width.
+
+    In a two's-complement system negative numbers are represented by the two's
+    complement of the absolute value. This is the most common method of
+    representing signed integers on computers [1]_. A N-bit two's-complement
+    system can represent every integer in the range
+    :math:`-2^{N-1}` to :math:`+2^{N-1}-1`.
+
+    Parameters
+    ----------
+    num : int
+        Only an integer decimal number can be used.
+    width : int, optional
+        The length of the returned string if `num` is positive, or the length
+        of the two's complement if `num` is negative, provided that `width` is
+        at least a sufficient number of bits for `num` to be represented in the
+        designated form.
+
+        If the `width` value is insufficient, it will be ignored, and `num` will
+        be returned in binary (`num` > 0) or two's complement (`num` < 0) form
+        with its width equal to the minimum number of bits needed to represent
+        the number in the designated form. This behavior is deprecated and will
+        later raise an error.
+
+        .. deprecated:: 1.12.0
+
+    Returns
+    -------
+    bin : str
+        Binary representation of `num` or two's complement of `num`.
+
+    See Also
+    --------
+    base_repr: Return a string representation of a number in the given base
+               system.
+    bin: Python's built-in binary representation generator of an integer.
+
+    Notes
+    -----
+    `binary_repr` is equivalent to using `base_repr` with base 2, but about 25x
+    faster.
+
+    References
+    ----------
+    .. [1] Wikipedia, "Two's complement",
+        https://en.wikipedia.org/wiki/Two's_complement
+
+    Examples
+    --------
+    >>> np.binary_repr(3)
+    '11'
+    >>> np.binary_repr(-3)
+    '-11'
+    >>> np.binary_repr(3, width=4)
+    '0011'
+
+    The two's complement is returned when the input number is negative and
+    width is specified:
+
+    >>> np.binary_repr(-3, width=3)
+    '101'
+    >>> np.binary_repr(-3, width=5)
+    '11101'
+
+    """
+    def warn_if_insufficient(width, binwidth):
+        if width is not None and width < binwidth:
+            warnings.warn(
+                "Insufficient bit width provided. This behavior "
+                "will raise an error in the future.", DeprecationWarning,
+                stacklevel=3)
+
+    # Ensure that num is a Python integer to avoid overflow or unwanted
+    # casts to floating point.
+    num = operator.index(num)
+
+    if num == 0:
+        return '0' * (width or 1)
+
+    elif num > 0:
+        binary = bin(num)[2:]
+        binwidth = len(binary)
+        outwidth = (binwidth if width is None
+                    else max(binwidth, width))
+        warn_if_insufficient(width, binwidth)
+        return binary.zfill(outwidth)
+
     else:
-        cName = "array"
-    if typeless and arr.size:
-        return cName + "(%s)" % lst
-    else:
-        typename=arr.dtype.type.__name__[:-6]
-        lf = ''
-        if issubclass(arr.dtype.type, flexible):
-            typename = str(arr.dtype)
-            lf = '\n'+' '*len("array(")
-        return cName + "(%s, %sdtype=%s)" % (lst, lf, typename)
-
-def array_str(a, max_line_width=None, precision=None, suppress_small=None):
-    return array2string(a, max_line_width, precision, suppress_small, ' ', "", str)
-
-set_string_function = multiarray.set_string_function
-set_string_function(array_str, 0)
-set_string_function(array_repr, 1)
-
-
-little_endian = (sys.byteorder == 'little')
-
-def indices(dimensions, dtype=int_):
-    """indices(dimensions,dtype=int_) returns an array representing a grid
-    of indices with row-only, and column-only variation.
-    """
-    tmp = ones(dimensions, dtype)
-    lst = []
-    for i in range(len(dimensions)):
-        lst.append( add.accumulate(tmp, i, )-1 )
-    return array(lst)
-
-def fromfunction(function, dimensions, **kwargs):
-    """fromfunction(function, dimensions) returns an array constructed by
-    calling function on a tuple of number grids.  The function should
-    accept as many arguments as there are dimensions which is a list of
-    numbers indicating the length of the desired output for each axis.
-
-    The function can also accept keyword arguments which will be
-    passed in as well.
-    """
-    args = indices(dimensions)
-    return function(*args,**kwargs)
-
-def isscalar(num):
-    if isinstance(num, generic):
-        return True
-    else:
-        return type(num) in ScalarType
-
-_lkup = {'0':'000',
-         '1':'001',
-         '2':'010',
-         '3':'011',
-         '4':'100',
-         '5':'101',
-         '6':'110',
-         '7':'111',
-         'L':''}
-
-def binary_repr(num):
-    """Return the binary representation of the input number as a string.
-
-    This is equivalent to using base_repr with base 2, but about 25x
-    faster.
-    """
-    ostr = oct(num)
-    bin = ''
-    for ch in ostr[1:]:
-        bin += _lkup[ch]
-    ind = 0
-    while bin[ind] == '0':
-        ind += 1
-    return bin[ind:]
-
-def base_repr (number, base=2, padding=0):
-    """Return the representation of a number in any given base.
-    """
-    chars = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'
-
-    import math
-    lnb = math.log(base)
-    res = padding*chars[0]
-    if number == 0:
-        return res + chars[0]
-    exponent = int (math.log (number)/lnb)
-    while(exponent >= 0):
-        term = long(base)**exponent
-        lead_digit = int(number / term)
-        res += chars[lead_digit]
-        number -= term*lead_digit
-        exponent -= 1
-    return res
-
-from cPickle import load, loads
-_cload = load
-_file = file
-
-def load(file):
-    if isinstance(file, type("")):
-        file = _file(file,"rb")
-    return _cload(file)
+        if width is None:
+            return '-' + bin(-num)[2:]
+
+        else:
+            poswidth = len(bin(-num)[2:])
+
+            # See gh-8679: remove extra digit
+            # for numbers at boundaries.
+            if 2**(poswidth - 1) == -num:
+                poswidth -= 1
+
+            twocomp = 2**(poswidth + 1) + num
+            binary = bin(twocomp)[2:]
+            binwidth = len(binary)
+
+            outwidth = max(binwidth, width)
+            warn_if_insufficient(width, binwidth)
+            return '1' * (outwidth - binwidth) + binary
+
+
+@set_module('numpy')
+def base_repr(number, base=2, padding=0):
+    """
+    Return a string representation of a number in the given base system.
+
+    Parameters
+    ----------
+    number : int
+        The value to convert. Positive and negative values are handled.
+    base : int, optional
+        Convert `number` to the `base` number system. The valid range is 2-36,
+        the default value is 2.
+    padding : int, optional
+        Number of zeros padded on the left. Default is 0 (no padding).
+
+    Returns
+    -------
+    out : str
+        String representation of `number` in `base` system.
+
+    See Also
+    --------
+    binary_repr : Faster version of `base_repr` for base 2.
+
+    Examples
+    --------
+    >>> np.base_repr(5)
+    '101'
+    >>> np.base_repr(6, 5)
+    '11'
+    >>> np.base_repr(7, base=5, padding=3)
+    '00012'
+
+    >>> np.base_repr(10, base=16)
+    'A'
+    >>> np.base_repr(32, base=16)
+    '20'
+
+    """
+    digits = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'
+    if base > len(digits):
+        raise ValueError("Bases greater than 36 not handled in base_repr.")
+    elif base < 2:
+        raise ValueError("Bases less than 2 not handled in base_repr.")
+
+    num = abs(number)
+    res = []
+    while num:
+        res.append(digits[num % base])
+        num //= base
+    if padding:
+        res.append('0' * padding)
+    if number < 0:
+        res.append('-')
+    return ''.join(reversed(res or '0'))
+
 
 # These are all essentially abbreviations
 # These might wind up in a special abbreviations module
 
-def ones(shape, dtype=int_, order='C'):
-    """ones(shape, dtype=int_) returns an array of the given
-    dimensions which is initialized to all ones.
-    """
-    a = empty(shape, dtype, order)
-    a.fill(1)
-    # Above is faster now after addition of fast loops.
-    #a = zeros(shape, dtype, order)
-    #a+=1
-    return a
-
-def identity(n,dtype=int_):
-    """identity(n) returns the identity 2-d array of shape n x n.
-    """
-    a = array([1]+n*[0],dtype=dtype)
-    b = empty((n,n),dtype=dtype)
-    b.flat = a
-    return b
-
-def allclose (a, b, rtol=1.e-5, atol=1.e-8):
-    """ allclose(a,b,rtol=1.e-5,atol=1.e-8)
-        Returns true if all components of a and b are equal
-        subject to given tolerances.
-        The relative error rtol must be positive and << 1.0
-        The absolute error atol comes into play for those elements
-        of y that are very small or zero; it says how small x must be also.
-    """
-    x = array(a, copy=False)
-    y = array(b, copy=False)
-    d = less(absolute(x-y), atol + rtol * absolute(y))
-    return d.ravel().all()
-
-
-
-_errdict = {"ignore":ERR_IGNORE,
-            "warn":ERR_WARN,
-            "raise":ERR_RAISE,
-            "call":ERR_CALL}
-
-_errdict_rev = {}
-for key in _errdict.keys():
-    _errdict_rev[_errdict[key]] = key
-del key
-
-def seterr(divide=None, over=None, under=None, invalid=None):
-
-    pyvals = umath.geterrobj()
-    old = geterr()
-
-    if divide is None: divide = old['divide']
-    if over is None: over = old['over']
-    if under is None: under = old['under']
-    if invalid is None: invalid = old['invalid']
-
-    maskvalue = ((_errdict[divide] << SHIFT_DIVIDEBYZERO) +
-                 (_errdict[over] << SHIFT_OVERFLOW ) +
-                 (_errdict[under] << SHIFT_UNDERFLOW) +
-                 (_errdict[invalid] << SHIFT_INVALID))
-
-    pyvals[1] = maskvalue
-    umath.seterrobj(pyvals)
-    return old
-
-def geterr():
-    maskvalue = umath.geterrobj()[1]
-    mask = 3
-    res = {}
-    val = (maskvalue >> SHIFT_DIVIDEBYZERO) & mask
-    res['divide'] = _errdict_rev[val]
-    val = (maskvalue >> SHIFT_OVERFLOW) & mask
-    res['over'] = _errdict_rev[val]
-    val = (maskvalue >> SHIFT_UNDERFLOW) & mask
-    res['under'] = _errdict_rev[val]
-    val = (maskvalue >> SHIFT_INVALID) & mask
-    res['invalid'] = _errdict_rev[val]
-    return res
-
-def setbufsize(size):
-    if size > 10e6:
-        raise ValueError, "Very big buffers.. %s" % size
-
-    pyvals = umath.geterrobj()
-    old = getbufsize()
-    pyvals[0] = size
-    umath.seterrobj(pyvals)
-    return old
-
-def getbufsize():
-    return umath.geterrobj()[0]
-
-def seterrcall(func):
-    if not callable(func):
-        raise ValueError, "Only callable can be used as callback"
-    pyvals = umath.geterrobj()
-    old = geterrcall()
-    pyvals[2] = func
-    umath.seterrobj(pyvals)
-    return old
-
-def geterrcall():
-    return umath.geterrobj()[2]
-
-def _setdef():
-    defval = [UFUNC_BUFSIZE_DEFAULT, ERR_DEFAULT, None]
-    umath.seterrobj(defval)
-
-# set the default values
-_setdef()
+
+def _maketup(descr, val):
+    dt = dtype(descr)
+    # Place val in all scalar tuples:
+    fields = dt.fields
+    if fields is None:
+        return val
+    else:
+        res = [_maketup(fields[name][0], val) for name in dt.names]
+        return tuple(res)
+
+
+def _identity_dispatcher(n, dtype=None, *, like=None):
+    return (like,)
+
+
+@set_array_function_like_doc
+@set_module('numpy')
+def identity(n, dtype=None, *, like=None):
+    """
+    Return the identity array.
+
+    The identity array is a square array with ones on
+    the main diagonal.
+
+    Parameters
+    ----------
+    n : int
+        Number of rows (and columns) in `n` x `n` output.
+    dtype : data-type, optional
+        Data-type of the output.  Defaults to ``float``.
+    ${ARRAY_FUNCTION_LIKE}
+
+        .. versionadded:: 1.20.0
+
+    Returns
+    -------
+    out : ndarray
+        `n` x `n` array with its main diagonal set to one,
+        and all other elements 0.
+
+    Examples
+    --------
+    >>> np.identity(3)
+    array([[1.,  0.,  0.],
+           [0.,  1.,  0.],
+           [0.,  0.,  1.]])
+
+    """
+    if like is not None:
+        return _identity_with_like(n, dtype=dtype, like=like)
+
+    from numpy import eye
+    return eye(n, dtype=dtype, like=like)
+
+
+_identity_with_like = array_function_dispatch(
+    _identity_dispatcher
+)(identity)
+
+
+def _allclose_dispatcher(a, b, rtol=None, atol=None, equal_nan=None):
+    return (a, b)
+
+
+@array_function_dispatch(_allclose_dispatcher)
+def allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
+    """
+    Returns True if two arrays are element-wise equal within a tolerance.
+
+    The tolerance values are positive, typically very small numbers.  The
+    relative difference (`rtol` * abs(`b`)) and the absolute difference
+    `atol` are added together to compare against the absolute difference
+    between `a` and `b`.
+
+    NaNs are treated as equal if they are in the same place and if
+    ``equal_nan=True``.  Infs are treated as equal if they are in the same
+    place and of the same sign in both arrays.
+
+    Parameters
+    ----------
+    a, b : array_like
+        Input arrays to compare.
+    rtol : float
+        The relative tolerance parameter (see Notes).
+    atol : float
+        The absolute tolerance parameter (see Notes).
+    equal_nan : bool
+        Whether to compare NaN's as equal.  If True, NaN's in `a` will be
+        considered equal to NaN's in `b` in the output array.
+
+        .. versionadded:: 1.10.0
+
+    Returns
+    -------
+    allclose : bool
+        Returns True if the two arrays are equal within the given
+        tolerance; False otherwise.
+
+    See Also
+    --------
+    isclose, all, any, equal
+
+    Notes
+    -----
+    If the following equation is element-wise True, then allclose returns
+    True.
+
+     absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))
+
+    The above equation is not symmetric in `a` and `b`, so that
+    ``allclose(a, b)`` might be different from ``allclose(b, a)`` in
+    some rare cases.
+
+    The comparison of `a` and `b` uses standard broadcasting, which
+    means that `a` and `b` need not have the same shape in order for
+    ``allclose(a, b)`` to evaluate to True.  The same is true for
+    `equal` but not `array_equal`.
+
+    `allclose` is not defined for non-numeric data types.
+    `bool` is considered a numeric data-type for this purpose.
+
+    Examples
+    --------
+    >>> np.allclose([1e10,1e-7], [1.00001e10,1e-8])
+    False
+    >>> np.allclose([1e10,1e-8], [1.00001e10,1e-9])
+    True
+    >>> np.allclose([1e10,1e-8], [1.0001e10,1e-9])
+    False
+    >>> np.allclose([1.0, np.nan], [1.0, np.nan])
+    False
+    >>> np.allclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)
+    True
+
+    """
+    res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))
+    return bool(res)
+
+
+def _isclose_dispatcher(a, b, rtol=None, atol=None, equal_nan=None):
+    return (a, b)
+
+
+@array_function_dispatch(_isclose_dispatcher)
+def isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
+    """
+    Returns a boolean array where two arrays are element-wise equal within a
+    tolerance.
+
+    The tolerance values are positive, typically very small numbers.  The
+    relative difference (`rtol` * abs(`b`)) and the absolute difference
+    `atol` are added together to compare against the absolute difference
+    between `a` and `b`.
+
+    .. warning:: The default `atol` is not appropriate for comparing numbers
+                 that are much smaller than one (see Notes).
+
+    Parameters
+    ----------
+    a, b : array_like
+        Input arrays to compare.
+    rtol : float
+        The relative tolerance parameter (see Notes).
+    atol : float
+        The absolute tolerance parameter (see Notes).
+    equal_nan : bool
+        Whether to compare NaN's as equal.  If True, NaN's in `a` will be
+        considered equal to NaN's in `b` in the output array.
+
+    Returns
+    -------
+    y : array_like
+        Returns a boolean array of where `a` and `b` are equal within the
+        given tolerance. If both `a` and `b` are scalars, returns a single
+        boolean value.
+
+    See Also
+    --------
+    allclose
+    math.isclose
+
+    Notes
+    -----
+    .. versionadded:: 1.7.0
+
+    For finite values, isclose uses the following equation to test whether
+    two floating point values are equivalent.
+
+     absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))
+
+    Unlike the built-in `math.isclose`, the above equation is not symmetric
+    in `a` and `b` -- it assumes `b` is the reference value -- so that
+    `isclose(a, b)` might be different from `isclose(b, a)`. Furthermore,
+    the default value of atol is not zero, and is used to determine what
+    small values should be considered close to zero. The default value is
+    appropriate for expected values of order unity: if the expected values
+    are significantly smaller than one, it can result in false positives.
+    `atol` should be carefully selected for the use case at hand. A zero value
+    for `atol` will result in `False` if either `a` or `b` is zero.
+
+    `isclose` is not defined for non-numeric data types.
+    `bool` is considered a numeric data-type for this purpose.
+
+    Examples
+    --------
+    >>> np.isclose([1e10,1e-7], [1.00001e10,1e-8])
+    array([ True, False])
+    >>> np.isclose([1e10,1e-8], [1.00001e10,1e-9])
+    array([ True, True])
+    >>> np.isclose([1e10,1e-8], [1.0001e10,1e-9])
+    array([False,  True])
+    >>> np.isclose([1.0, np.nan], [1.0, np.nan])
+    array([ True, False])
+    >>> np.isclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)
+    array([ True, True])
+    >>> np.isclose([1e-8, 1e-7], [0.0, 0.0])
+    array([ True, False])
+    >>> np.isclose([1e-100, 1e-7], [0.0, 0.0], atol=0.0)
+    array([False, False])
+    >>> np.isclose([1e-10, 1e-10], [1e-20, 0.0])
+    array([ True,  True])
+    >>> np.isclose([1e-10, 1e-10], [1e-20, 0.999999e-10], atol=0.0)
+    array([False,  True])
+    """
+    def within_tol(x, y, atol, rtol):
+        with errstate(invalid='ignore'):
+            return less_equal(abs(x-y), atol + rtol * abs(y))
+
+    x = asanyarray(a)
+    y = asanyarray(b)
+
+    # Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
+    # This will cause casting of x later. Also, make sure to allow subclasses
+    # (e.g., for numpy.ma).
+    # NOTE: We explicitly allow timedelta, which used to work. This could
+    #       possibly be deprecated. See also gh-18286.
+    #       timedelta works if `atol` is an integer or also a timedelta.
+    #       Although, the default tolerances are unlikely to be useful
+    if y.dtype.kind != "m":
+        dt = multiarray.result_type(y, 1.)
+        y = asanyarray(y, dtype=dt)
+
+    xfin = isfinite(x)
+    yfin = isfinite(y)
+    if all(xfin) and all(yfin):
+        return within_tol(x, y, atol, rtol)
+    else:
+        finite = xfin & yfin
+        cond = zeros_like(finite, subok=True)
+        # Because we're using boolean indexing, x & y must be the same shape.
+        # Ideally, we'd just do x, y = broadcast_arrays(x, y). It's in
+        # lib.stride_tricks, though, so we can't import it here.
+        x = x * ones_like(cond)
+        y = y * ones_like(cond)
+        # Avoid subtraction with infinite/nan values...
+        cond[finite] = within_tol(x[finite], y[finite], atol, rtol)
+        # Check for equality of infinite values...
+        cond[~finite] = (x[~finite] == y[~finite])
+        if equal_nan:
+            # Make NaN == NaN
+            both_nan = isnan(x) & isnan(y)
+
+            # Needed to treat masked arrays correctly. = True would not work.
+            cond[both_nan] = both_nan[both_nan]
+
+        return cond[()]  # Flatten 0d arrays to scalars
+
+
+def _array_equal_dispatcher(a1, a2, equal_nan=None):
+    return (a1, a2)
+
+
+@array_function_dispatch(_array_equal_dispatcher)
+def array_equal(a1, a2, equal_nan=False):
+    """
+    True if two arrays have the same shape and elements, False otherwise.
+
+    Parameters
+    ----------
+    a1, a2 : array_like
+        Input arrays.
+    equal_nan : bool
+        Whether to compare NaN's as equal. If the dtype of a1 and a2 is
+        complex, values will be considered equal if either the real or the
+        imaginary component of a given value is ``nan``.
+
+        .. versionadded:: 1.19.0
+
+    Returns
+    -------
+    b : bool
+        Returns True if the arrays are equal.
+
+    See Also
+    --------
+    allclose: Returns True if two arrays are element-wise equal within a
+              tolerance.
+    array_equiv: Returns True if input arrays are shape consistent and all
+                 elements equal.
+
+    Examples
+    --------
+    >>> np.array_equal([1, 2], [1, 2])
+    True
+    >>> np.array_equal(np.array([1, 2]), np.array([1, 2]))
+    True
+    >>> np.array_equal([1, 2], [1, 2, 3])
+    False
+    >>> np.array_equal([1, 2], [1, 4])
+    False
+    >>> a = np.array([1, np.nan])
+    >>> np.array_equal(a, a)
+    False
+    >>> np.array_equal(a, a, equal_nan=True)
+    True
+
+    When ``equal_nan`` is True, complex values with nan components are
+    considered equal if either the real *or* the imaginary components are nan.
+
+    >>> a = np.array([1 + 1j])
+    >>> b = a.copy()
+    >>> a.real = np.nan
+    >>> b.imag = np.nan
+    >>> np.array_equal(a, b, equal_nan=True)
+    True
+    """
+    try:
+        a1, a2 = asarray(a1), asarray(a2)
+    except Exception:
+        return False
+    if a1.shape != a2.shape:
+        return False
+    if not equal_nan:
+        return bool(asarray(a1 == a2).all())
+    # Handling NaN values if equal_nan is True
+    a1nan, a2nan = isnan(a1), isnan(a2)
+    # NaN's occur at different locations
+    if not (a1nan == a2nan).all():
+        return False
+    # Shapes of a1, a2 and masks are guaranteed to be consistent by this point
+    return bool(asarray(a1[~a1nan] == a2[~a1nan]).all())
+
+
+def _array_equiv_dispatcher(a1, a2):
+    return (a1, a2)
+
+
+@array_function_dispatch(_array_equiv_dispatcher)
+def array_equiv(a1, a2):
+    """
+    Returns True if input arrays are shape consistent and all elements equal.
+
+    Shape consistent means they are either the same shape, or one input array
+    can be broadcasted to create the same shape as the other one.
+
+    Parameters
+    ----------
+    a1, a2 : array_like
+        Input arrays.
+
+    Returns
+    -------
+    out : bool
+        True if equivalent, False otherwise.
+
+    Examples
+    --------
+    >>> np.array_equiv([1, 2], [1, 2])
+    True
+    >>> np.array_equiv([1, 2], [1, 3])
+    False
+
+    Showing the shape equivalence:
+
+    >>> np.array_equiv([1, 2], [[1, 2], [1, 2]])
+    True
+    >>> np.array_equiv([1, 2], [[1, 2, 1, 2], [1, 2, 1, 2]])
+    False
+
+    >>> np.array_equiv([1, 2], [[1, 2], [1, 3]])
+    False
+
+    """
+    try:
+        a1, a2 = asarray(a1), asarray(a2)
+    except Exception:
+        return False
+    try:
+        multiarray.broadcast(a1, a2)
+    except Exception:
+        return False
+
+    return bool(asarray(a1 == a2).all())
+
 
 Inf = inf = infty = Infinity = PINF
 nan = NaN = NAN
 False_ = bool_(False)
 True_ = bool_(True)
 
-import oldnumeric
-from oldnumeric import *
-extend_all(oldnumeric)
+
+def extend_all(module):
+    existing = set(__all__)
+    mall = getattr(module, '__all__')
+    for a in mall:
+        if a not in existing:
+            __all__.append(a)
+
+
+from .umath import *
+from .numerictypes import *
+from . import fromnumeric
+from .fromnumeric import *
+from . import arrayprint
+from .arrayprint import *
+from . import _asarray
+from ._asarray import *
+from . import _ufunc_config
+from ._ufunc_config import *
+extend_all(fromnumeric)
+extend_all(umath)
+extend_all(numerictypes)
+extend_all(arrayprint)
+extend_all(_asarray)
+extend_all(_ufunc_config)
('numpy/core', 'numerictypes.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,10 +1,11 @@
-"""numerictypes: Define the numeric type objects
-
-This module is designed so 'from numerictypes import *' is safe.
+"""
+numerictypes: Define the numeric type objects
+
+This module is designed so "from numerictypes import \\*" is safe.
 Exported symbols include:
 
   Dictionary with all registered number types (including aliases):
-    typeDict
+    sctypeDict
 
   Type objects (not all will be available, depends on platform):
       see variable sctypes for which ones you have
@@ -15,6 +16,7 @@
     uint8 uint16 uint32 uint64 uint128
     float16 float32 float64 float96 float128 float256
     complex32 complex64 complex128 complex192 complex256 complex512
+    datetime64 timedelta64
 
     c-based names
 
@@ -35,272 +37,89 @@
     float_, complex_,
     longfloat, clongfloat,
 
-    As part of the type-hierarchy:    xx -- is bit-width
-
-     generic
-       bool_
-       number
-         integer
-           signedinteger   (intxx)
-             byte
-             short
-             intc
-             intp           int0
-             int_
-             longlong
-           unsignedinteger  (uintxx)
-             ubyte
-             ushort
-             uintc
-             uintp          uint0
-             uint_
-             ulonglong
-         floating           (floatxx)
-             single
-             float_  (double)
-             longfloat
-         complexfloating    (complexxx)
-             csingle
-             complex_ (cfloat, cdouble)
-             clongfloat
-
-       flexible
-         character
-           str_     (string)
-           unicode_
-         void
-
-       object_ (not used much)
-
-$Id: numerictypes.py,v 1.17 2005/09/09 22:20:06 teoliphant Exp $
+   As part of the type-hierarchy:    xx -- is bit-width
+
+   generic
+     +-> bool_                                  (kind=b)
+     +-> number
+     |   +-> integer
+     |   |   +-> signedinteger     (intxx)      (kind=i)
+     |   |   |     byte
+     |   |   |     short
+     |   |   |     intc
+     |   |   |     intp            int0
+     |   |   |     int_
+     |   |   |     longlong
+     |   |   \\-> unsignedinteger  (uintxx)     (kind=u)
+     |   |         ubyte
+     |   |         ushort
+     |   |         uintc
+     |   |         uintp           uint0
+     |   |         uint_
+     |   |         ulonglong
+     |   +-> inexact
+     |       +-> floating          (floatxx)    (kind=f)
+     |       |     half
+     |       |     single
+     |       |     float_          (double)
+     |       |     longfloat
+     |       \\-> complexfloating  (complexxx)  (kind=c)
+     |             csingle         (singlecomplex)
+     |             complex_        (cfloat, cdouble)
+     |             clongfloat      (longcomplex)
+     +-> flexible
+     |   +-> character
+     |   |     str_     (string_, bytes_)       (kind=S)    [Python 2]
+     |   |     unicode_                         (kind=U)    [Python 2]
+     |   |
+     |   |     bytes_   (string_)               (kind=S)    [Python 3]
+     |   |     str_     (unicode_)              (kind=U)    [Python 3]
+     |   |
+     |   \\-> void                              (kind=V)
+     \\-> object_ (not used much)               (kind=O)
+
 """
+import numbers
+
+from numpy.core.multiarray import (
+        ndarray, array, dtype, datetime_data, datetime_as_string,
+        busday_offset, busday_count, is_busday, busdaycalendar
+        )
+from numpy.core.overrides import set_module
 
 # we add more at the bottom
-__all__ = ['typeDict', 'typeNA', 'sctypes', 'ScalarType', 'obj2sctype', 'cast', 'nbytes',
-           'sctype2char', 'maximum_sctype', 'issctype']
-
-from multiarray import typeinfo, ndarray, array, empty
-import types as _types
+__all__ = ['sctypeDict', 'sctypes',
+           'ScalarType', 'obj2sctype', 'cast', 'nbytes', 'sctype2char',
+           'maximum_sctype', 'issctype', 'typecodes', 'find_common_type',
+           'issubdtype', 'datetime_data', 'datetime_as_string',
+           'busday_offset', 'busday_count', 'is_busday', 'busdaycalendar',
+           ]
+
+# we don't need all these imports, but we need to keep them for compatibility
+# for users using np.core.numerictypes.UPPER_TABLE
+from ._string_helpers import (
+    english_lower, english_upper, english_capitalize, LOWER_TABLE, UPPER_TABLE
+)
+
+from ._type_aliases import (
+    sctypeDict,
+    allTypes,
+    bitname,
+    sctypes,
+    _concrete_types,
+    _concrete_typeinfo,
+    _bits_of,
+)
+from ._dtype import _kind_name
 
 # we don't export these for import *, but we do want them accessible
 # as numerictypes.bool, etc.
-from __builtin__ import bool, int, long, float, complex, object, unicode, str
-
-typeDict = {}      # Contains all leaf-node numeric types with aliases
-typeNA = {}        # Contails all leaf-node types -> numarray type equivalences
-allTypes = {}      # Collect the types we will add to the module here
-
-def _evalname(name):
-    k = 0
-    for ch in name:
-        if ch in '0123456789':
-            break
-        k += 1
-    try:
-        bits = int(name[k:])
-    except ValueError:
-        bits = 0
-    base = name[:k]
-    return base, bits
-
-def bitname(obj):
-    """Return a bit-width name for a given type object"""
-    name = obj.__name__[:-6]
-    base = ''
-    char = ''
-    try:
-        info = typeinfo[name.upper()]
-        assert(info[-1] == obj)  # sanity check
-        bits = info[2]
-
-    except KeyError:     # bit-width name
-        base, bits = _evalname(name)
-        char = base[0]
-
-    if name == 'bool':
-        char = 'b'
-        base = 'bool'
-    elif name=='string':
-        char = 'S'
-        base = 'string'
-    elif name=='unicode':
-        char = 'U'
-        base = 'unicode'
-    elif name=='void':
-        char = 'V'
-        base = 'void'
-    elif name=='object':
-        char = 'O'
-        base = 'object'
-        bits = 0
-
-    bytes = bits / 8
-
-    if char != '' and bytes != 0:
-        char = "%s%d" % (char, bytes)
-
-    return base, bits, char
-
-
-def _add_types():
-    for a in typeinfo.keys():
-        name = a.lower()
-        if isinstance(typeinfo[a], tuple):
-            typeobj = typeinfo[a][-1]
-
-            # define C-name and insert typenum and typechar references also
-            allTypes[name] = typeobj
-            typeDict[name] = typeobj
-            typeDict[typeinfo[a][0]] = typeobj
-            typeDict[typeinfo[a][1]] = typeobj
-
-        else:  # generic class
-            allTypes[name] = typeinfo[a]
-_add_types()
-
-def _add_aliases():
-    for a in typeinfo.keys():
-        name = a.lower()
-        if not isinstance(typeinfo[a], tuple):
-            continue
-        typeobj = typeinfo[a][-1]
-        # insert bit-width version for this class (if relevant)
-        base, bit, char = bitname(typeobj)
-        if base[-3:] == 'int': continue
-        if base != '':
-            myname = "%s%d" % (base, bit)
-            if (name != 'longdouble' and name != 'clongdouble') or \
-                   myname not in allTypes.keys():
-                allTypes[myname] = typeobj
-                typeDict[myname] = typeobj
-                if base == 'complex':
-                    na_name = '%s%d' % (base.capitalize(), bit/2)
-                elif base == 'bool':
-                    na_name = base.capitalize()
-                    typeDict[na_name] = typeobj
-                else:
-                    na_name = "%s%d" % (base.capitalize(), bit)
-                    typeDict[na_name] = typeobj
-                typeNA[na_name] = typeobj
-                typeNA[typeobj] = na_name
-                typeNA[typeinfo[a][0]] = na_name
-        if char != '':
-            typeDict[char] = typeobj
-            typeNA[char] = na_name
-_add_aliases()
-
-# Integers handled so that
-# The int32, int64 types should agree exactly with
-#  PyArray_INT32, PyArray_INT64 in C
-# We need to enforce the same checking as is done
-#  in arrayobject.h where the order of getting a
-#  bit-width match is:
-#       long, longlong, int, short, char
-#   for int8, int16, int32, int64, int128
-
-def _add_integer_aliases():
-    _ctypes = ['LONG', 'LONGLONG', 'INT', 'SHORT', 'BYTE']
-    for ctype in _ctypes:
-        val = typeinfo[ctype]
-        bits = val[2]
-        intname = 'int%d' % bits
-        UIntname = 'UInt%d' % bits
-        Intname = 'Int%d' % bits
-        uval = typeinfo['U'+ctype]
-        typeobj = val[-1]
-        utypeobj = uval[-1]        
-        if intname not in allTypes.keys():
-            uintname = 'uint%d' % bits
-            allTypes[intname] = typeobj
-            allTypes[uintname] = utypeobj
-            typeDict[intname] = typeobj
-            typeDict[uintname] = utypeobj
-            typeDict[Intname] = typeobj
-            typeDict[UIntname] = utypeobj
-            typeNA[Intname] = typeobj
-            typeNA[UIntname] = utypeobj
-        typeNA[typeobj] = Intname
-        typeNA[utypeobj] = UIntname
-        typeNA[val[0]] = Intname
-        typeNA[uval[0]] = UIntname
-_add_integer_aliases()
-
-# We use these later
-void = allTypes['void']
+from builtins import bool, int, float, complex, object, str, bytes
+from numpy.compat import long, unicode
+
+
+# We use this later
 generic = allTypes['generic']
-
-#
-# Rework the Python names (so that float and complex and int are consistent
-#                            with Python usage)
-#
-def _set_up_aliases():
-    type_pairs = [('complex_', 'cdouble'),
-                  ('int0', 'intp'),
-                  ('uint0', 'uintp'),
-                  ('single', 'float'),
-                  ('csingle', 'cfloat'),
-                  ('float_', 'double'),
-                  ('intc', 'int'),
-                  ('uintc', 'uint'),
-                  ('int_', 'long'),
-                  ('uint', 'ulong'),
-                  ('cfloat', 'cdouble'),
-                  ('longfloat', 'longdouble'),
-                  ('clongfloat', 'clongdouble'),
-                  ('bool_', 'bool'),
-                  ('unicode_', 'unicode'),
-                  ('str_', 'string'),
-                  ('object_', 'object')]
-    for alias, t in type_pairs:
-        allTypes[alias] = allTypes[t]
-        typeDict[alias] = typeDict[t]
-    # Remove aliases overriding python types
-    for t in ['ulong', 'object', 'unicode', 'int', 'long', 'float',
-              'complex', 'bool']:
-        try:
-            del allTypes[t]
-            del typeDict[t]
-        except KeyError:
-            pass
-_set_up_aliases()
-
-# Now, construct dictionary to lookup character codes from types
-_sctype2char_dict = {}
-def _construct_char_code_lookup():
-    for name in typeinfo.keys():
-        tup = typeinfo[name]
-        if isinstance(tup, tuple):
-            if tup[0] not in ['p','P']:
-                _sctype2char_dict[tup[-1]] = tup[0]
-_construct_char_code_lookup()
-
-
-sctypes = {'int': [],
-           'uint':[],
-           'float':[],
-           'complex':[],
-           'others':[bool,object,str,unicode,void]}
-
-def _add_array_type(typename, bits):
-    try:
-        t = allTypes['%s%d' % (typename, bits)]
-    except KeyError:
-        pass
-    else:
-        sctypes[typename].append(t)
-
-def _set_array_types():
-    ibytes = [1, 2, 4, 8, 16, 32, 64]
-    fbytes = [2, 4, 8, 10, 12, 16, 32, 64]
-    for bytes in ibytes:
-        bits = 8*bytes
-        _add_array_type('int', bits)
-        _add_array_type('uint', bits)
-    for bytes in fbytes:
-        bits = 8*bytes
-        _add_array_type('float', bits)
-        _add_array_type('complex', bits)
-_set_array_types()
 
 genericTypeRank = ['bool', 'int8', 'uint8', 'int16', 'uint16',
                    'int32', 'uint32', 'int64', 'uint64', 'int128',
@@ -310,58 +129,307 @@
                    'complex32', 'complex64', 'complex128', 'complex160',
                    'complex192', 'complex256', 'complex512', 'object']
 
+@set_module('numpy')
 def maximum_sctype(t):
-    """returns the sctype of highest precision of the same general kind as 't'"""
+    """
+    Return the scalar type of highest precision of the same kind as the input.
+
+    Parameters
+    ----------
+    t : dtype or dtype specifier
+        The input data type. This can be a `dtype` object or an object that
+        is convertible to a `dtype`.
+
+    Returns
+    -------
+    out : dtype
+        The highest precision data type of the same kind (`dtype.kind`) as `t`.
+
+    See Also
+    --------
+    obj2sctype, mintypecode, sctype2char
+    dtype
+
+    Examples
+    --------
+    >>> np.maximum_sctype(int)
+    <class 'numpy.int64'>
+    >>> np.maximum_sctype(np.uint8)
+    <class 'numpy.uint64'>
+    >>> np.maximum_sctype(complex)
+    <class 'numpy.complex256'> # may vary
+
+    >>> np.maximum_sctype(str)
+    <class 'numpy.str_'>
+
+    >>> np.maximum_sctype('i2')
+    <class 'numpy.int64'>
+    >>> np.maximum_sctype('f4')
+    <class 'numpy.float128'> # may vary
+
+    """
     g = obj2sctype(t)
     if g is None:
         return t
     t = g
-    name = t.__name__[:-6]
-    base, bits = _evalname(name)
-    if bits == 0:
+    base = _kind_name(dtype(t))
+    if base in sctypes:
+        return sctypes[base][-1]
+    else:
         return t
-    else:
-        return sctypes[base][-1]
-
-_python_types = {int : 'int_',
-                 float: 'float_',
-                 complex: 'complex_',
-                 bool: 'bool_',
-                 str: 'string',
-                 unicode: 'unicode_',
-                 _types.BufferType: 'void',
-                }
-def _python_type(t):
-    """returns the type corresponding to a certain Python type"""
-    if not isinstance(t, _types.TypeType):
-        t = type(t)
-    return allTypes[_python_types.get(t, 'object_')]
-
+
+
+@set_module('numpy')
 def issctype(rep):
-    """Determines whether the given object represents
-    a numeric array type."""
+    """
+    Determines whether the given object represents a scalar data-type.
+
+    Parameters
+    ----------
+    rep : any
+        If `rep` is an instance of a scalar dtype, True is returned. If not,
+        False is returned.
+
+    Returns
+    -------
+    out : bool
+        Boolean result of check whether `rep` is a scalar dtype.
+
+    See Also
+    --------
+    issubsctype, issubdtype, obj2sctype, sctype2char
+
+    Examples
+    --------
+    >>> np.issctype(np.int32)
+    True
+    >>> np.issctype(list)
+    False
+    >>> np.issctype(1.1)
+    False
+
+    Strings are also a scalar type:
+
+    >>> np.issctype(np.dtype('str'))
+    True
+
+    """
+    if not isinstance(rep, (type, dtype)):
+        return False
     try:
-        char = sctype2char(rep)
-        return True
-    except (KeyError, ValueError, TypeError):
+        res = obj2sctype(rep)
+        if res and res != object_:
+            return True
         return False
-
+    except Exception:
+        return False
+
+
+@set_module('numpy')
 def obj2sctype(rep, default=None):
-    try:
-        if issubclass(rep, generic):
-            return rep
-    except TypeError:
-        pass
-    if isinstance(rep, type):
-        return _python_type(rep)
+    """
+    Return the scalar dtype or NumPy equivalent of Python type of an object.
+
+    Parameters
+    ----------
+    rep : any
+        The object of which the type is returned.
+    default : any, optional
+        If given, this is returned for objects whose types can not be
+        determined. If not given, None is returned for those objects.
+
+    Returns
+    -------
+    dtype : dtype or Python type
+        The data type of `rep`.
+
+    See Also
+    --------
+    sctype2char, issctype, issubsctype, issubdtype, maximum_sctype
+
+    Examples
+    --------
+    >>> np.obj2sctype(np.int32)
+    <class 'numpy.int32'>
+    >>> np.obj2sctype(np.array([1., 2.]))
+    <class 'numpy.float64'>
+    >>> np.obj2sctype(np.array([1.j]))
+    <class 'numpy.complex128'>
+
+    >>> np.obj2sctype(dict)
+    <class 'numpy.object_'>
+    >>> np.obj2sctype('string')
+
+    >>> np.obj2sctype(1, default=list)
+    <class 'list'>
+
+    """
+    # prevent abstract classes being upcast
+    if isinstance(rep, type) and issubclass(rep, generic):
+        return rep
+    # extract dtype from arrays
     if isinstance(rep, ndarray):
         return rep.dtype.type
-    res = typeDict.get(rep, default)
-    return res
-
-
-# This dictionary allows look up based on any alias for an array type
+    # fall back on dtype to convert
+    try:
+        res = dtype(rep)
+    except Exception:
+        return default
+    else:
+        return res.type
+
+
+@set_module('numpy')
+def issubclass_(arg1, arg2):
+    """
+    Determine if a class is a subclass of a second class.
+
+    `issubclass_` is equivalent to the Python built-in ``issubclass``,
+    except that it returns False instead of raising a TypeError if one
+    of the arguments is not a class.
+
+    Parameters
+    ----------
+    arg1 : class
+        Input class. True is returned if `arg1` is a subclass of `arg2`.
+    arg2 : class or tuple of classes.
+        Input class. If a tuple of classes, True is returned if `arg1` is a
+        subclass of any of the tuple elements.
+
+    Returns
+    -------
+    out : bool
+        Whether `arg1` is a subclass of `arg2` or not.
+
+    See Also
+    --------
+    issubsctype, issubdtype, issctype
+
+    Examples
+    --------
+    >>> np.issubclass_(np.int32, int)
+    False
+    >>> np.issubclass_(np.int32, float)
+    False
+    >>> np.issubclass_(np.float64, float)
+    True
+
+    """
+    try:
+        return issubclass(arg1, arg2)
+    except TypeError:
+        return False
+
+
+@set_module('numpy')
+def issubsctype(arg1, arg2):
+    """
+    Determine if the first argument is a subclass of the second argument.
+
+    Parameters
+    ----------
+    arg1, arg2 : dtype or dtype specifier
+        Data-types.
+
+    Returns
+    -------
+    out : bool
+        The result.
+
+    See Also
+    --------
+    issctype, issubdtype, obj2sctype
+
+    Examples
+    --------
+    >>> np.issubsctype('S8', str)
+    False
+    >>> np.issubsctype(np.array([1]), int)
+    True
+    >>> np.issubsctype(np.array([1]), float)
+    False
+
+    """
+    return issubclass(obj2sctype(arg1), obj2sctype(arg2))
+
+
+@set_module('numpy')
+def issubdtype(arg1, arg2):
+    r"""
+    Returns True if first argument is a typecode lower/equal in type hierarchy.
+
+    This is like the builtin :func:`issubclass`, but for `dtype`\ s.
+
+    Parameters
+    ----------
+    arg1, arg2 : dtype_like
+        `dtype` or object coercible to one
+
+    Returns
+    -------
+    out : bool
+
+    See Also
+    --------
+    :ref:`arrays.scalars` : Overview of the numpy type hierarchy.
+    issubsctype, issubclass_
+
+    Examples
+    --------
+    `issubdtype` can be used to check the type of arrays:
+
+    >>> ints = np.array([1, 2, 3], dtype=np.int32)
+    >>> np.issubdtype(ints.dtype, np.integer)
+    True
+    >>> np.issubdtype(ints.dtype, np.floating)
+    False
+
+    >>> floats = np.array([1, 2, 3], dtype=np.float32)
+    >>> np.issubdtype(floats.dtype, np.integer)
+    False
+    >>> np.issubdtype(floats.dtype, np.floating)
+    True
+
+    Similar types of different sizes are not subdtypes of each other:
+
+    >>> np.issubdtype(np.float64, np.float32)
+    False
+    >>> np.issubdtype(np.float32, np.float64)
+    False
+
+    but both are subtypes of `floating`:
+
+    >>> np.issubdtype(np.float64, np.floating)
+    True
+    >>> np.issubdtype(np.float32, np.floating)
+    True
+
+    For convenience, dtype-like objects are allowed too:
+
+    >>> np.issubdtype('S1', np.string_)
+    True
+    >>> np.issubdtype('i4', np.signedinteger)
+    True
+
+    """
+    if not issubclass_(arg1, generic):
+        arg1 = dtype(arg1).type
+    if not issubclass_(arg2, generic):
+        arg2 = dtype(arg2).type
+
+    return issubclass(arg1, arg2)
+
+
+# This dictionary allows look up based on any alias for an array data-type
 class _typedict(dict):
+    """
+    Base object for a dictionary for look-up with any alias for an array dtype.
+
+    Instances of `_typedict` can not be used as dictionaries directly,
+    first they have to be populated.
+
+    """
+
     def __getitem__(self, obj):
         return dict.__getitem__(self, obj2sctype(obj))
 
@@ -370,50 +438,88 @@
 _maxvals = _typedict()
 _minvals = _typedict()
 def _construct_lookups():
-    for name, val in typeinfo.iteritems():
-        if not isinstance(val, tuple):
-            continue
-        obj = val[-1]
-        nbytes[obj] = val[2] / 8
-        _alignment[obj] = val[3]
-        if (len(val) > 5):
-            _maxvals[obj] = val[4]
-            _minvals[obj] = val[5]
+    for name, info in _concrete_typeinfo.items():
+        obj = info.type
+        nbytes[obj] = info.bits // 8
+        _alignment[obj] = info.alignment
+        if len(info) > 5:
+            _maxvals[obj] = info.max
+            _minvals[obj] = info.min
         else:
             _maxvals[obj] = None
             _minvals[obj] = None
 
 _construct_lookups()
 
+
+@set_module('numpy')
 def sctype2char(sctype):
+    """
+    Return the string representation of a scalar dtype.
+
+    Parameters
+    ----------
+    sctype : scalar dtype or object
+        If a scalar dtype, the corresponding string character is
+        returned. If an object, `sctype2char` tries to infer its scalar type
+        and then return the corresponding string character.
+
+    Returns
+    -------
+    typechar : str
+        The string character corresponding to the scalar type.
+
+    Raises
+    ------
+    ValueError
+        If `sctype` is an object for which the type can not be inferred.
+
+    See Also
+    --------
+    obj2sctype, issctype, issubsctype, mintypecode
+
+    Examples
+    --------
+    >>> for sctype in [np.int32, np.double, np.complex_, np.string_, np.ndarray]:
+    ...     print(np.sctype2char(sctype))
+    l # may vary
+    d
+    D
+    S
+    O
+
+    >>> x = np.array([1., 2-1.j])
+    >>> np.sctype2char(x)
+    'D'
+    >>> np.sctype2char(list)
+    'O'
+
+    """
     sctype = obj2sctype(sctype)
     if sctype is None:
-        raise ValueError, "unrecognized type"
-    return _sctype2char_dict[sctype]
+        raise ValueError("unrecognized type")
+    if sctype not in _concrete_types:
+        # for compatibility
+        raise KeyError(sctype)
+    return dtype(sctype).char
 
 # Create dictionary of casting functions that wrap sequences
 # indexed by type or type character
-
-
 cast = _typedict()
-ScalarType = [_types.IntType, _types.FloatType,
-              _types.ComplexType, _types.LongType, _types.BooleanType,
-              _types.StringType, _types.UnicodeType, _types.BufferType]
-ScalarType.extend(_sctype2char_dict.keys())
+for key in _concrete_types:
+    cast[key] = lambda x, k=key: array(x, copy=False).astype(k)
+
+
+def _scalar_type_key(typ):
+    """A ``key`` function for `sorted`."""
+    dt = dtype(typ)
+    return (dt.kind.lower(), dt.itemsize)
+
+
+ScalarType = [int, float, complex, bool, bytes, str, memoryview]
+ScalarType += sorted(_concrete_types, key=_scalar_type_key)
 ScalarType = tuple(ScalarType)
-for key in _sctype2char_dict.keys():
-    cast[key] = lambda x, k=key : array(x, copy=False).astype(k)
-
-
-_unicodesize = array('u','U1').itemsize
-
-# Create the typestring lookup dictionary
-_typestr = _typedict()
-for key in _sctype2char_dict.keys():
-    if issubclass(key, allTypes['flexible']):
-        _typestr[key] = _sctype2char_dict[key]
-    else:
-        _typestr[key] = empty((1,),key).dtype.str[1:]
+
 
 # Now add the types we've determined to this module
 for key in allTypes:
@@ -421,3 +527,144 @@
     __all__.append(key)
 
 del key
+
+typecodes = {'Character':'c',
+             'Integer':'bhilqp',
+             'UnsignedInteger':'BHILQP',
+             'Float':'efdg',
+             'Complex':'FDG',
+             'AllInteger':'bBhHiIlLqQpP',
+             'AllFloat':'efdgFDG',
+             'Datetime': 'Mm',
+             'All':'?bhilqpBHILQPefdgFDGSUVOMm'}
+
+# backwards compatibility --- deprecated name
+# Formal deprecation: Numpy 1.20.0, 2020-10-19 (see numpy/__init__.py)
+typeDict = sctypeDict
+
+# b -> boolean
+# u -> unsigned integer
+# i -> signed integer
+# f -> floating point
+# c -> complex
+# M -> datetime
+# m -> timedelta
+# S -> string
+# U -> Unicode string
+# V -> record
+# O -> Python object
+_kind_list = ['b', 'u', 'i', 'f', 'c', 'S', 'U', 'V', 'O', 'M', 'm']
+
+__test_types = '?'+typecodes['AllInteger'][:-2]+typecodes['AllFloat']+'O'
+__len_test_types = len(__test_types)
+
+# Keep incrementing until a common type both can be coerced to
+#  is found.  Otherwise, return None
+def _find_common_coerce(a, b):
+    if a > b:
+        return a
+    try:
+        thisind = __test_types.index(a.char)
+    except ValueError:
+        return None
+    return _can_coerce_all([a, b], start=thisind)
+
+# Find a data-type that all data-types in a list can be coerced to
+def _can_coerce_all(dtypelist, start=0):
+    N = len(dtypelist)
+    if N == 0:
+        return None
+    if N == 1:
+        return dtypelist[0]
+    thisind = start
+    while thisind < __len_test_types:
+        newdtype = dtype(__test_types[thisind])
+        numcoerce = len([x for x in dtypelist if newdtype >= x])
+        if numcoerce == N:
+            return newdtype
+        thisind += 1
+    return None
+
+def _register_types():
+    numbers.Integral.register(integer)
+    numbers.Complex.register(inexact)
+    numbers.Real.register(floating)
+    numbers.Number.register(number)
+
+_register_types()
+
+
+@set_module('numpy')
+def find_common_type(array_types, scalar_types):
+    """
+    Determine common type following standard coercion rules.
+
+    Parameters
+    ----------
+    array_types : sequence
+        A list of dtypes or dtype convertible objects representing arrays.
+    scalar_types : sequence
+        A list of dtypes or dtype convertible objects representing scalars.
+
+    Returns
+    -------
+    datatype : dtype
+        The common data type, which is the maximum of `array_types` ignoring
+        `scalar_types`, unless the maximum of `scalar_types` is of a
+        different kind (`dtype.kind`). If the kind is not understood, then
+        None is returned.
+
+    See Also
+    --------
+    dtype, common_type, can_cast, mintypecode
+
+    Examples
+    --------
+    >>> np.find_common_type([], [np.int64, np.float32, complex])
+    dtype('complex128')
+    >>> np.find_common_type([np.int64, np.float32], [])
+    dtype('float64')
+
+    The standard casting rules ensure that a scalar cannot up-cast an
+    array unless the scalar is of a fundamentally different kind of data
+    (i.e. under a different hierarchy in the data type hierarchy) then
+    the array:
+
+    >>> np.find_common_type([np.float32], [np.int64, np.float64])
+    dtype('float32')
+
+    Complex is of a different type, so it up-casts the float in the
+    `array_types` argument:
+
+    >>> np.find_common_type([np.float32], [complex])
+    dtype('complex128')
+
+    Type specifier strings are convertible to dtypes and can therefore
+    be used instead of dtypes:
+
+    >>> np.find_common_type(['f4', 'f4', 'i4'], ['c8'])
+    dtype('complex128')
+
+    """
+    array_types = [dtype(x) for x in array_types]
+    scalar_types = [dtype(x) for x in scalar_types]
+
+    maxa = _can_coerce_all(array_types)
+    maxsc = _can_coerce_all(scalar_types)
+
+    if maxa is None:
+        return maxsc
+
+    if maxsc is None:
+        return maxa
+
+    try:
+        index_a = _kind_list.index(maxa.kind)
+        index_sc = _kind_list.index(maxsc.kind)
+    except ValueError:
+        return None
+
+    if index_sc > index_a:
+        return _find_common_coerce(maxsc, maxa)
+    else:
+        return maxa
('numpy/core', 'arrayprint.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,9 +1,12 @@
 """Array printing function
 
 $Id: arrayprint.py,v 1.9 2005/09/13 13:58:44 teoliphant Exp $
+
 """
-__all__ = ["set_summary", "summary_off", "set_precision", "set_line_width",
-           "array2string"]
+__all__ = ["array2string", "array_str", "array_repr", "set_string_function",
+           "set_printoptions", "get_printoptions", "printoptions",
+           "format_float_positional", "format_float_scientific"]
+__docformat__ = 'restructuredtext'
 
 #
 # Written by Konrad Hinsen <hinsenk@ere.umontreal.ca>
@@ -12,208 +15,770 @@
 # and by Perry Greenfield 2000-4-1 for numarray
 # and by Travis Oliphant  2005-8-22 for numpy
 
+
+# Note: Both scalartypes.c.src and arrayprint.py implement strs for numpy
+# scalars but for different purposes. scalartypes.c.src has str/reprs for when
+# the scalar is printed on its own, while arrayprint.py has strs for when
+# scalars are printed inside an ndarray. Only the latter strs are currently
+# user-customizable.
+
+import functools
+import numbers
 import sys
-import numeric      as _gen
-import numerictypes as _nt
-import umath        as _uf
-from multiarray import format_longfloat
-from oldnumeric import ravel
-_nc = _gen
-
-# The following functions are emergency substitutes for numeric functions
-# which sometimes get broken during development.
-
-def product(x, y): return x*y
-
-def _maximum_reduce(arr):
-    maximum = arr[0]
-    for i in xrange(1, arr.nelements()):
-        if arr[i] > maximum: maximum = arr[i]
-    return maximum
-
-def _minimum_reduce(arr):
-    minimum = arr[0]
-    for i in xrange(1, arr.nelements()):
-        if arr[i] < minimum: minimum = arr[i]
-    return minimum
-
-def _numeric_compress(arr):
-    nonzero = 0
-    for i in xrange(arr.nelements()):
-        if arr[i] != 0: nonzero += 1
-    retarr = _nc.zeros((nonzero,))
-    nonzero = 0
-    for i in xrange(arr.nelements()):
-        if arr[i] != 0:
-            retarr[nonzero] = abs(arr[i])
-            nonzero += 1
-    return retarr
-
-_failsafe = 0
-if _failsafe:
-    max_reduce = _maximum_reduce
-    min_reduce = _minimum_reduce
-else:
-    max_reduce = _uf.maximum.reduce
-    min_reduce = _uf.minimum.reduce
-
-_summaryEdgeItems = 3     # repr N leading and trailing items of each dimension
-_summaryThreshhold = 1000 # total items > triggers array summarization
-
-_float_output_precision = 8
-_float_output_suppress_small = False
-_line_width = 75
-
-
+try:
+    from _thread import get_ident
+except ImportError:
+    from _dummy_thread import get_ident
+
+import numpy as np
+from . import numerictypes as _nt
+from .umath import absolute, isinf, isfinite, isnat
+from . import multiarray
+from .multiarray import (array, dragon4_positional, dragon4_scientific,
+                         datetime_as_string, datetime_data, ndarray,
+                         set_legacy_print_mode)
+from .fromnumeric import any
+from .numeric import concatenate, asarray, errstate
+from .numerictypes import (longlong, intc, int_, float_, complex_, bool_,
+                           flexible)
+from .overrides import array_function_dispatch, set_module
+import operator
+import warnings
+import contextlib
+
+_format_options = {
+    'edgeitems': 3,  # repr N leading and trailing items of each dimension
+    'threshold': 1000,  # total items > triggers array summarization
+    'floatmode': 'maxprec',
+    'precision': 8,  # precision of floating point representations
+    'suppress': False,  # suppress printing small floating values in exp format
+    'linewidth': 75,
+    'nanstr': 'nan',
+    'infstr': 'inf',
+    'sign': '-',
+    'formatter': None,
+    # Internally stored as an int to simplify comparisons; converted from/to
+    # str/False on the way in/out.
+    'legacy': sys.maxsize}
+
+def _make_options_dict(precision=None, threshold=None, edgeitems=None,
+                       linewidth=None, suppress=None, nanstr=None, infstr=None,
+                       sign=None, formatter=None, floatmode=None, legacy=None):
+    """
+    Make a dictionary out of the non-None arguments, plus conversion of
+    *legacy* and sanity checks.
+    """
+
+    options = {k: v for k, v in locals().items() if v is not None}
+
+    if suppress is not None:
+        options['suppress'] = bool(suppress)
+
+    modes = ['fixed', 'unique', 'maxprec', 'maxprec_equal']
+    if floatmode not in modes + [None]:
+        raise ValueError("floatmode option must be one of " +
+                         ", ".join('"{}"'.format(m) for m in modes))
+
+    if sign not in [None, '-', '+', ' ']:
+        raise ValueError("sign option must be one of ' ', '+', or '-'")
+
+    if legacy == False:
+        options['legacy'] = sys.maxsize
+    elif legacy == '1.13':
+        options['legacy'] = 113
+    elif legacy == '1.21':
+        options['legacy'] = 121
+    elif legacy is None:
+        pass  # OK, do nothing.
+    else:
+        warnings.warn(
+            "legacy printing option can currently only be '1.13', '1.21', or "
+            "`False`", stacklevel=3)
+
+    if threshold is not None:
+        # forbid the bad threshold arg suggested by stack overflow, gh-12351
+        if not isinstance(threshold, numbers.Number):
+            raise TypeError("threshold must be numeric")
+        if np.isnan(threshold):
+            raise ValueError("threshold must be non-NAN, try "
+                             "sys.maxsize for untruncated representation")
+
+    if precision is not None:
+        # forbid the bad precision arg as suggested by issue #18254
+        try:
+            options['precision'] = operator.index(precision)
+        except TypeError as e:
+            raise TypeError('precision must be an integer') from e
+
+    return options
+
+
+@set_module('numpy')
 def set_printoptions(precision=None, threshold=None, edgeitems=None,
-                     linewidth=None, suppress=None):
-    """Set options associated with printing.
-
-    precision  the default number of digits of precision for floating
-                   point output
-                   (default 8)
-    threshold  total number of array elements which trigger summarization
-                   rather than full repr.
-                   (default 1000)
-    edgeitems  number of array items in summary at beginning and end of
-                   each dimension.
-                   (default 3)
-    linewidth  the number of characters per line for the purpose of inserting
-                   line breaks.
-                   (default 75)
-    suppress    Boolean value indicating whether or not suppress printing
-                   of small floating point values using scientific notation
-                   (default False)
-    """
-
-    global _summaryThreshhold, _summaryEdgeItems, _float_output_precision, \
-           _line_width, _float_output_suppress_small
-    if (linewidth is not None):
-        _line_width = linewidth
-    if (threshold is not None):
-        _summaryThreshhold = threshold
-    if (edgeitems is not None):
-        _summaryEdgeItems = edgeitems
-    if (precision is not None):
-        _float_output_precision = precision
-    if (suppress is not None):
-        _float_output_suppress_small = not not suppress
-    return
-
+                     linewidth=None, suppress=None, nanstr=None, infstr=None,
+                     formatter=None, sign=None, floatmode=None, *, legacy=None):
+    """
+    Set printing options.
+
+    These options determine the way floating point numbers, arrays and
+    other NumPy objects are displayed.
+
+    Parameters
+    ----------
+    precision : int or None, optional
+        Number of digits of precision for floating point output (default 8).
+        May be None if `floatmode` is not `fixed`, to print as many digits as
+        necessary to uniquely specify the value.
+    threshold : int, optional
+        Total number of array elements which trigger summarization
+        rather than full repr (default 1000).
+        To always use the full repr without summarization, pass `sys.maxsize`.
+    edgeitems : int, optional
+        Number of array items in summary at beginning and end of
+        each dimension (default 3).
+    linewidth : int, optional
+        The number of characters per line for the purpose of inserting
+        line breaks (default 75).
+    suppress : bool, optional
+        If True, always print floating point numbers using fixed point
+        notation, in which case numbers equal to zero in the current precision
+        will print as zero.  If False, then scientific notation is used when
+        absolute value of the smallest number is < 1e-4 or the ratio of the
+        maximum absolute value to the minimum is > 1e3. The default is False.
+    nanstr : str, optional
+        String representation of floating point not-a-number (default nan).
+    infstr : str, optional
+        String representation of floating point infinity (default inf).
+    sign : string, either '-', '+', or ' ', optional
+        Controls printing of the sign of floating-point types. If '+', always
+        print the sign of positive values. If ' ', always prints a space
+        (whitespace character) in the sign position of positive values.  If
+        '-', omit the sign character of positive values. (default '-')
+    formatter : dict of callables, optional
+        If not None, the keys should indicate the type(s) that the respective
+        formatting function applies to.  Callables should return a string.
+        Types that are not specified (by their corresponding keys) are handled
+        by the default formatters.  Individual types for which a formatter
+        can be set are:
+
+        - 'bool'
+        - 'int'
+        - 'timedelta' : a `numpy.timedelta64`
+        - 'datetime' : a `numpy.datetime64`
+        - 'float'
+        - 'longfloat' : 128-bit floats
+        - 'complexfloat'
+        - 'longcomplexfloat' : composed of two 128-bit floats
+        - 'numpystr' : types `numpy.string_` and `numpy.unicode_`
+        - 'object' : `np.object_` arrays
+
+        Other keys that can be used to set a group of types at once are:
+
+        - 'all' : sets all types
+        - 'int_kind' : sets 'int'
+        - 'float_kind' : sets 'float' and 'longfloat'
+        - 'complex_kind' : sets 'complexfloat' and 'longcomplexfloat'
+        - 'str_kind' : sets 'numpystr'
+    floatmode : str, optional
+        Controls the interpretation of the `precision` option for
+        floating-point types. Can take the following values
+        (default maxprec_equal):
+
+        * 'fixed': Always print exactly `precision` fractional digits,
+                even if this would print more or fewer digits than
+                necessary to specify the value uniquely.
+        * 'unique': Print the minimum number of fractional digits necessary
+                to represent each value uniquely. Different elements may
+                have a different number of digits. The value of the
+                `precision` option is ignored.
+        * 'maxprec': Print at most `precision` fractional digits, but if
+                an element can be uniquely represented with fewer digits
+                only print it with that many.
+        * 'maxprec_equal': Print at most `precision` fractional digits,
+                but if every element in the array can be uniquely
+                represented with an equal number of fewer digits, use that
+                many digits for all elements.
+    legacy : string or `False`, optional
+        If set to the string `'1.13'` enables 1.13 legacy printing mode. This
+        approximates numpy 1.13 print output by including a space in the sign
+        position of floats and different behavior for 0d arrays. This also
+        enables 1.21 legacy printing mode (described below).
+
+        If set to the string `'1.21'` enables 1.21 legacy printing mode. This
+        approximates numpy 1.21 print output of complex structured dtypes
+        by not inserting spaces after commas that separate fields and after
+        colons.
+
+        If set to `False`, disables legacy mode.
+
+        Unrecognized strings will be ignored with a warning for forward
+        compatibility.
+
+        .. versionadded:: 1.14.0
+        .. versionchanged:: 1.22.0
+
+    See Also
+    --------
+    get_printoptions, printoptions, set_string_function, array2string
+
+    Notes
+    -----
+    `formatter` is always reset with a call to `set_printoptions`.
+
+    Use `printoptions` as a context manager to set the values temporarily.
+
+    Examples
+    --------
+    Floating point precision can be set:
+
+    >>> np.set_printoptions(precision=4)
+    >>> np.array([1.123456789])
+    [1.1235]
+
+    Long arrays can be summarised:
+
+    >>> np.set_printoptions(threshold=5)
+    >>> np.arange(10)
+    array([0, 1, 2, ..., 7, 8, 9])
+
+    Small results can be suppressed:
+
+    >>> eps = np.finfo(float).eps
+    >>> x = np.arange(4.)
+    >>> x**2 - (x + eps)**2
+    array([-4.9304e-32, -4.4409e-16,  0.0000e+00,  0.0000e+00])
+    >>> np.set_printoptions(suppress=True)
+    >>> x**2 - (x + eps)**2
+    array([-0., -0.,  0.,  0.])
+
+    A custom formatter can be used to display array elements as desired:
+
+    >>> np.set_printoptions(formatter={'all':lambda x: 'int: '+str(-x)})
+    >>> x = np.arange(3)
+    >>> x
+    array([int: 0, int: -1, int: -2])
+    >>> np.set_printoptions()  # formatter gets reset
+    >>> x
+    array([0, 1, 2])
+
+    To put back the default options, you can use:
+
+    >>> np.set_printoptions(edgeitems=3, infstr='inf',
+    ... linewidth=75, nanstr='nan', precision=8,
+    ... suppress=False, threshold=1000, formatter=None)
+
+    Also to temporarily override options, use `printoptions` as a context manager:
+
+    >>> with np.printoptions(precision=2, suppress=True, threshold=5):
+    ...     np.linspace(0, 10, 10)
+    array([ 0.  ,  1.11,  2.22, ...,  7.78,  8.89, 10.  ])
+
+    """
+    opt = _make_options_dict(precision, threshold, edgeitems, linewidth,
+                             suppress, nanstr, infstr, sign, formatter,
+                             floatmode, legacy)
+    # formatter is always reset
+    opt['formatter'] = formatter
+    _format_options.update(opt)
+
+    # set the C variable for legacy mode
+    if _format_options['legacy'] == 113:
+        set_legacy_print_mode(113)
+        # reset the sign option in legacy mode to avoid confusion
+        _format_options['sign'] = '-'
+    elif _format_options['legacy'] == 121:
+        set_legacy_print_mode(121)
+    elif _format_options['legacy'] == sys.maxsize:
+        set_legacy_print_mode(0)
+
+
+@set_module('numpy')
 def get_printoptions():
-    return _float_output_precision, _summaryThreshhold, _summaryEdgeItems, \
-           _line_width, _float_output_suppress_small
-
-
-def _leading_trailing(a):
-    if a.ndim == 1:
-        if len(a) > 2*_summaryEdgeItems:
-            b = _gen.concatenate((a[:_summaryEdgeItems],
-                                     a[-_summaryEdgeItems:]))
+    """
+    Return the current print options.
+
+    Returns
+    -------
+    print_opts : dict
+        Dictionary of current print options with keys
+
+          - precision : int
+          - threshold : int
+          - edgeitems : int
+          - linewidth : int
+          - suppress : bool
+          - nanstr : str
+          - infstr : str
+          - formatter : dict of callables
+          - sign : str
+
+        For a full description of these options, see `set_printoptions`.
+
+    See Also
+    --------
+    set_printoptions, printoptions, set_string_function
+
+    """
+    opts = _format_options.copy()
+    opts['legacy'] = {
+        113: '1.13', 121: '1.21', sys.maxsize: False,
+    }[opts['legacy']]
+    return opts
+
+
+def _get_legacy_print_mode():
+    """Return the legacy print mode as an int."""
+    return _format_options['legacy']
+
+
+@set_module('numpy')
+@contextlib.contextmanager
+def printoptions(*args, **kwargs):
+    """Context manager for setting print options.
+
+    Set print options for the scope of the `with` block, and restore the old
+    options at the end. See `set_printoptions` for the full description of
+    available options.
+
+    Examples
+    --------
+
+    >>> from numpy.testing import assert_equal
+    >>> with np.printoptions(precision=2):
+    ...     np.array([2.0]) / 3
+    array([0.67])
+
+    The `as`-clause of the `with`-statement gives the current print options:
+
+    >>> with np.printoptions(precision=2) as opts:
+    ...      assert_equal(opts, np.get_printoptions())
+
+    See Also
+    --------
+    set_printoptions, get_printoptions
+
+    """
+    opts = np.get_printoptions()
+    try:
+        np.set_printoptions(*args, **kwargs)
+        yield np.get_printoptions()
+    finally:
+        np.set_printoptions(**opts)
+
+
+def _leading_trailing(a, edgeitems, index=()):
+    """
+    Keep only the N-D corners (leading and trailing edges) of an array.
+
+    Should be passed a base-class ndarray, since it makes no guarantees about
+    preserving subclasses.
+    """
+    axis = len(index)
+    if axis == a.ndim:
+        return a[index]
+
+    if a.shape[axis] > 2*edgeitems:
+        return concatenate((
+            _leading_trailing(a, edgeitems, index + np.index_exp[ :edgeitems]),
+            _leading_trailing(a, edgeitems, index + np.index_exp[-edgeitems:])
+        ), axis=axis)
+    else:
+        return _leading_trailing(a, edgeitems, index + np.index_exp[:])
+
+
+def _object_format(o):
+    """ Object arrays containing lists should be printed unambiguously """
+    if type(o) is list:
+        fmt = 'list({!r})'
+    else:
+        fmt = '{!r}'
+    return fmt.format(o)
+
+def repr_format(x):
+    return repr(x)
+
+def str_format(x):
+    return str(x)
+
+def _get_formatdict(data, *, precision, floatmode, suppress, sign, legacy,
+                    formatter, **kwargs):
+    # note: extra arguments in kwargs are ignored
+
+    # wrapped in lambdas to avoid taking a code path with the wrong type of data
+    formatdict = {
+        'bool': lambda: BoolFormat(data),
+        'int': lambda: IntegerFormat(data),
+        'float': lambda: FloatingFormat(
+            data, precision, floatmode, suppress, sign, legacy=legacy),
+        'longfloat': lambda: FloatingFormat(
+            data, precision, floatmode, suppress, sign, legacy=legacy),
+        'complexfloat': lambda: ComplexFloatingFormat(
+            data, precision, floatmode, suppress, sign, legacy=legacy),
+        'longcomplexfloat': lambda: ComplexFloatingFormat(
+            data, precision, floatmode, suppress, sign, legacy=legacy),
+        'datetime': lambda: DatetimeFormat(data, legacy=legacy),
+        'timedelta': lambda: TimedeltaFormat(data),
+        'object': lambda: _object_format,
+        'void': lambda: str_format,
+        'numpystr': lambda: repr_format}
+
+    # we need to wrap values in `formatter` in a lambda, so that the interface
+    # is the same as the above values.
+    def indirect(x):
+        return lambda: x
+
+    if formatter is not None:
+        fkeys = [k for k in formatter.keys() if formatter[k] is not None]
+        if 'all' in fkeys:
+            for key in formatdict.keys():
+                formatdict[key] = indirect(formatter['all'])
+        if 'int_kind' in fkeys:
+            for key in ['int']:
+                formatdict[key] = indirect(formatter['int_kind'])
+        if 'float_kind' in fkeys:
+            for key in ['float', 'longfloat']:
+                formatdict[key] = indirect(formatter['float_kind'])
+        if 'complex_kind' in fkeys:
+            for key in ['complexfloat', 'longcomplexfloat']:
+                formatdict[key] = indirect(formatter['complex_kind'])
+        if 'str_kind' in fkeys:
+            formatdict['numpystr'] = indirect(formatter['str_kind'])
+        for key in formatdict.keys():
+            if key in fkeys:
+                formatdict[key] = indirect(formatter[key])
+
+    return formatdict
+
+def _get_format_function(data, **options):
+    """
+    find the right formatting function for the dtype_
+    """
+    dtype_ = data.dtype
+    dtypeobj = dtype_.type
+    formatdict = _get_formatdict(data, **options)
+    if dtypeobj is None:
+        return formatdict["numpystr"]()
+    elif issubclass(dtypeobj, _nt.bool_):
+        return formatdict['bool']()
+    elif issubclass(dtypeobj, _nt.integer):
+        if issubclass(dtypeobj, _nt.timedelta64):
+            return formatdict['timedelta']()
         else:
-            b = a
+            return formatdict['int']()
+    elif issubclass(dtypeobj, _nt.floating):
+        if issubclass(dtypeobj, _nt.longfloat):
+            return formatdict['longfloat']()
+        else:
+            return formatdict['float']()
+    elif issubclass(dtypeobj, _nt.complexfloating):
+        if issubclass(dtypeobj, _nt.clongfloat):
+            return formatdict['longcomplexfloat']()
+        else:
+            return formatdict['complexfloat']()
+    elif issubclass(dtypeobj, (_nt.unicode_, _nt.string_)):
+        return formatdict['numpystr']()
+    elif issubclass(dtypeobj, _nt.datetime64):
+        return formatdict['datetime']()
+    elif issubclass(dtypeobj, _nt.object_):
+        return formatdict['object']()
+    elif issubclass(dtypeobj, _nt.void):
+        if dtype_.names is not None:
+            return StructuredVoidFormat.from_data(data, **options)
+        else:
+            return formatdict['void']()
     else:
-        if len(a) > 2*_summaryEdgeItems:
-            l = [_leading_trailing(a[i]) for i in range(
-                min(len(a), _summaryEdgeItems))]
-            l.extend([_leading_trailing(a[-i]) for i in range(
-                min(len(a), _summaryEdgeItems),0,-1)])
-        else:
-            l = [_leading_trailing(a[i]) for i in range(0, len(a))]
-        b = _gen.concatenate(tuple(l))
-    return b
-
-def _array2string(a, max_line_width, precision, suppress_small, separator=' ',
-                  prefix=""):
-
-    if max_line_width is None:
-        max_line_width = _line_width
-
-    if precision is None:
-        precision = _float_output_precision
-
-    if suppress_small is None:
-        suppress_small = _float_output_suppress_small
-
-    if a.size > _summaryThreshhold:
-        summary_insert = "..., "
-        data = _leading_trailing(a)
+        return formatdict['numpystr']()
+
+
+def _recursive_guard(fillvalue='...'):
+    """
+    Like the python 3.2 reprlib.recursive_repr, but forwards *args and **kwargs
+
+    Decorates a function such that if it calls itself with the same first
+    argument, it returns `fillvalue` instead of recursing.
+
+    Largely copied from reprlib.recursive_repr
+    """
+
+    def decorating_function(f):
+        repr_running = set()
+
+        @functools.wraps(f)
+        def wrapper(self, *args, **kwargs):
+            key = id(self), get_ident()
+            if key in repr_running:
+                return fillvalue
+            repr_running.add(key)
+            try:
+                return f(self, *args, **kwargs)
+            finally:
+                repr_running.discard(key)
+
+        return wrapper
+
+    return decorating_function
+
+
+# gracefully handle recursive calls, when object arrays contain themselves
+@_recursive_guard()
+def _array2string(a, options, separator=' ', prefix=""):
+    # The formatter __init__s in _get_format_function cannot deal with
+    # subclasses yet, and we also need to avoid recursion issues in
+    # _formatArray with subclasses which return 0d arrays in place of scalars
+    data = asarray(a)
+    if a.shape == ():
+        a = data
+
+    if a.size > options['threshold']:
+        summary_insert = "..."
+        data = _leading_trailing(data, options['edgeitems'])
     else:
         summary_insert = ""
-        data = ravel(a)
-
-    try:
-        format_function = a._format
-    except AttributeError:
-        dtype = a.dtype.type
-        if issubclass(dtype, _nt.bool):
-            format = "%s"
-            format_function = lambda x: format % x
-        if issubclass(dtype, _nt.integer):
-            max_str_len = max(len(str(max_reduce(data))),
-                              len(str(min_reduce(data))))
-            format = '%' + str(max_str_len) + 'd'
-            format_function = lambda x: _formatInteger(x, format)
-        elif issubclass(dtype, _nt.floating):
-            if issubclass(dtype, _nt.longfloat):
-                format_function = _longfloatFormatter(precision)
-            else:
-                format = _floatFormat(data, precision, suppress_small)
-                format_function = lambda x: _formatFloat(x, format)
-        elif issubclass(dtype, _nt.complexfloating):
-            if issubclass(dtype, _nt.clongfloat):
-                format_function = _clongfloatFormatter(precision)
-            else:
-                real_format = _floatFormat(
-                    data.real, precision, suppress_small, sign=0)
-                imag_format = _floatFormat(
-                    data.imag, precision, suppress_small, sign=1)
-                format_function = lambda x: \
-                              _formatComplex(x, real_format, imag_format)
-        elif issubclass(dtype, _nt.unicode_):
-            format = "%s"
-            format_function = lambda x: repr(x)
-        else:
-            format = '%s'
-            format_function = lambda x: format % str(x)
-
-    next_line_prefix = " " # skip over "["
-    next_line_prefix += " "*len(prefix)                  # skip over array(
-
-
-    lst = _formatArray(a, format_function, len(a.shape), max_line_width,
-                       next_line_prefix, separator,
-                       _summaryEdgeItems, summary_insert)[:-1]
-
+
+    # find the right formatting function for the array
+    format_function = _get_format_function(data, **options)
+
+    # skip over "["
+    next_line_prefix = " "
+    # skip over array(
+    next_line_prefix += " "*len(prefix)
+
+    lst = _formatArray(a, format_function, options['linewidth'],
+                       next_line_prefix, separator, options['edgeitems'],
+                       summary_insert, options['legacy'])
     return lst
 
-def array2string(a, max_line_width = None, precision = None,
-                 suppress_small = None, separator=' ', prefix="",
-                 style=repr):
-
-    if a.shape == ():
-        x = a.item()
-        try:
-            lst = a._format(x)
-        except AttributeError:
-            lst = style(x)
-    elif reduce(product, a.shape) == 0:
-        # treat as a null array if any of shape elements == 0
-        lst = "[]"
-    else:
-        lst = _array2string(a, max_line_width, precision, suppress_small,
-                            separator, prefix)
-    return lst
-
-def _extendLine(s, line, word, max_line_len, next_line_prefix):
-    if len(line.rstrip()) + len(word.rstrip()) >= max_line_len:
+
+def _array2string_dispatcher(
+        a, max_line_width=None, precision=None,
+        suppress_small=None, separator=None, prefix=None,
+        style=None, formatter=None, threshold=None,
+        edgeitems=None, sign=None, floatmode=None, suffix=None,
+        *, legacy=None):
+    return (a,)
+
+
+@array_function_dispatch(_array2string_dispatcher, module='numpy')
+def array2string(a, max_line_width=None, precision=None,
+                 suppress_small=None, separator=' ', prefix="",
+                 style=np._NoValue, formatter=None, threshold=None,
+                 edgeitems=None, sign=None, floatmode=None, suffix="",
+                 *, legacy=None):
+    """
+    Return a string representation of an array.
+
+    Parameters
+    ----------
+    a : ndarray
+        Input array.
+    max_line_width : int, optional
+        Inserts newlines if text is longer than `max_line_width`.
+        Defaults to ``numpy.get_printoptions()['linewidth']``.
+    precision : int or None, optional
+        Floating point precision.
+        Defaults to ``numpy.get_printoptions()['precision']``.
+    suppress_small : bool, optional
+        Represent numbers "very close" to zero as zero; default is False.
+        Very close is defined by precision: if the precision is 8, e.g.,
+        numbers smaller (in absolute value) than 5e-9 are represented as
+        zero.
+        Defaults to ``numpy.get_printoptions()['suppress']``.
+    separator : str, optional
+        Inserted between elements.
+    prefix : str, optional
+    suffix : str, optional
+        The length of the prefix and suffix strings are used to respectively
+        align and wrap the output. An array is typically printed as::
+
+          prefix + array2string(a) + suffix
+
+        The output is left-padded by the length of the prefix string, and
+        wrapping is forced at the column ``max_line_width - len(suffix)``.
+        It should be noted that the content of prefix and suffix strings are
+        not included in the output.
+    style : _NoValue, optional
+        Has no effect, do not use.
+
+        .. deprecated:: 1.14.0
+    formatter : dict of callables, optional
+        If not None, the keys should indicate the type(s) that the respective
+        formatting function applies to.  Callables should return a string.
+        Types that are not specified (by their corresponding keys) are handled
+        by the default formatters.  Individual types for which a formatter
+        can be set are:
+
+        - 'bool'
+        - 'int'
+        - 'timedelta' : a `numpy.timedelta64`
+        - 'datetime' : a `numpy.datetime64`
+        - 'float'
+        - 'longfloat' : 128-bit floats
+        - 'complexfloat'
+        - 'longcomplexfloat' : composed of two 128-bit floats
+        - 'void' : type `numpy.void`
+        - 'numpystr' : types `numpy.string_` and `numpy.unicode_`
+
+        Other keys that can be used to set a group of types at once are:
+
+        - 'all' : sets all types
+        - 'int_kind' : sets 'int'
+        - 'float_kind' : sets 'float' and 'longfloat'
+        - 'complex_kind' : sets 'complexfloat' and 'longcomplexfloat'
+        - 'str_kind' : sets 'numpystr'
+    threshold : int, optional
+        Total number of array elements which trigger summarization
+        rather than full repr.
+        Defaults to ``numpy.get_printoptions()['threshold']``.
+    edgeitems : int, optional
+        Number of array items in summary at beginning and end of
+        each dimension.
+        Defaults to ``numpy.get_printoptions()['edgeitems']``.
+    sign : string, either '-', '+', or ' ', optional
+        Controls printing of the sign of floating-point types. If '+', always
+        print the sign of positive values. If ' ', always prints a space
+        (whitespace character) in the sign position of positive values.  If
+        '-', omit the sign character of positive values.
+        Defaults to ``numpy.get_printoptions()['sign']``.
+    floatmode : str, optional
+        Controls the interpretation of the `precision` option for
+        floating-point types.
+        Defaults to ``numpy.get_printoptions()['floatmode']``.
+        Can take the following values:
+
+        - 'fixed': Always print exactly `precision` fractional digits,
+          even if this would print more or fewer digits than
+          necessary to specify the value uniquely.
+        - 'unique': Print the minimum number of fractional digits necessary
+          to represent each value uniquely. Different elements may
+          have a different number of digits.  The value of the
+          `precision` option is ignored.
+        - 'maxprec': Print at most `precision` fractional digits, but if
+          an element can be uniquely represented with fewer digits
+          only print it with that many.
+        - 'maxprec_equal': Print at most `precision` fractional digits,
+          but if every element in the array can be uniquely
+          represented with an equal number of fewer digits, use that
+          many digits for all elements.
+    legacy : string or `False`, optional
+        If set to the string `'1.13'` enables 1.13 legacy printing mode. This
+        approximates numpy 1.13 print output by including a space in the sign
+        position of floats and different behavior for 0d arrays. If set to
+        `False`, disables legacy mode. Unrecognized strings will be ignored
+        with a warning for forward compatibility.
+
+        .. versionadded:: 1.14.0
+
+    Returns
+    -------
+    array_str : str
+        String representation of the array.
+
+    Raises
+    ------
+    TypeError
+        if a callable in `formatter` does not return a string.
+
+    See Also
+    --------
+    array_str, array_repr, set_printoptions, get_printoptions
+
+    Notes
+    -----
+    If a formatter is specified for a certain type, the `precision` keyword is
+    ignored for that type.
+
+    This is a very flexible function; `array_repr` and `array_str` are using
+    `array2string` internally so keywords with the same name should work
+    identically in all three functions.
+
+    Examples
+    --------
+    >>> x = np.array([1e-16,1,2,3])
+    >>> np.array2string(x, precision=2, separator=',',
+    ...                       suppress_small=True)
+    '[0.,1.,2.,3.]'
+
+    >>> x  = np.arange(3.)
+    >>> np.array2string(x, formatter={'float_kind':lambda x: "%.2f" % x})
+    '[0.00 1.00 2.00]'
+
+    >>> x  = np.arange(3)
+    >>> np.array2string(x, formatter={'int':lambda x: hex(x)})
+    '[0x0 0x1 0x2]'
+
+    """
+
+    overrides = _make_options_dict(precision, threshold, edgeitems,
+                                   max_line_width, suppress_small, None, None,
+                                   sign, formatter, floatmode, legacy)
+    options = _format_options.copy()
+    options.update(overrides)
+
+    if options['legacy'] <= 113:
+        if style is np._NoValue:
+            style = repr
+
+        if a.shape == () and a.dtype.names is None:
+            return style(a.item())
+    elif style is not np._NoValue:
+        # Deprecation 11-9-2017  v1.14
+        warnings.warn("'style' argument is deprecated and no longer functional"
+                      " except in 1.13 'legacy' mode",
+                      DeprecationWarning, stacklevel=3)
+
+    if options['legacy'] > 113:
+        options['linewidth'] -= len(suffix)
+
+    # treat as a null array if any of shape elements == 0
+    if a.size == 0:
+        return "[]"
+
+    return _array2string(a, options, separator, prefix)
+
+
+def _extendLine(s, line, word, line_width, next_line_prefix, legacy):
+    needs_wrap = len(line) + len(word) > line_width
+    if legacy > 113:
+        # don't wrap lines if it won't help
+        if len(line) <= len(next_line_prefix):
+            needs_wrap = False
+
+    if needs_wrap:
         s += line.rstrip() + "\n"
         line = next_line_prefix
     line += word
     return s, line
 
-def _formatArray(a, format_function, rank, max_line_len,
-                 next_line_prefix, separator, edge_items, summary_insert):
+
+def _extendLine_pretty(s, line, word, line_width, next_line_prefix, legacy):
+    """
+    Extends line with nicely formatted (possibly multi-line) string ``word``.
+    """
+    words = word.splitlines()
+    if len(words) == 1 or legacy <= 113:
+        return _extendLine(s, line, word, line_width, next_line_prefix, legacy)
+
+    max_word_length = max(len(word) for word in words)
+    if (len(line) + max_word_length > line_width and
+            len(line) > len(next_line_prefix)):
+        s += line.rstrip() + '\n'
+        line = next_line_prefix + words[0]
+        indent = next_line_prefix
+    else:
+        indent = len(line)*' '
+        line += words[0]
+
+    for word in words[1::]:
+        s += line.rstrip() + '\n'
+        line = indent + word
+
+    suffix_length = max_word_length - len(words[-1])
+    line += suffix_length*' '
+
+    return s, line
+
+def _formatArray(a, format_function, line_width, next_line_prefix,
+                 separator, edge_items, summary_insert, legacy):
     """formatArray is designed for two modes of operation:
 
     1. Full output
@@ -221,163 +786,920 @@
     2. Summarized output
 
     """
-    if rank == 0:
+    def recurser(index, hanging_indent, curr_width):
+        """
+        By using this local function, we don't need to recurse with all the
+        arguments. Since this function is not created recursively, the cost is
+        not significant
+        """
+        axis = len(index)
+        axes_left = a.ndim - axis
+
+        if axes_left == 0:
+            return format_function(a[index])
+
+        # when recursing, add a space to align with the [ added, and reduce the
+        # length of the line by 1
+        next_hanging_indent = hanging_indent + ' '
+        if legacy <= 113:
+            next_width = curr_width
+        else:
+            next_width = curr_width - len(']')
+
+        a_len = a.shape[axis]
+        show_summary = summary_insert and 2*edge_items < a_len
+        if show_summary:
+            leading_items = edge_items
+            trailing_items = edge_items
+        else:
+            leading_items = 0
+            trailing_items = a_len
+
+        # stringify the array with the hanging indent on the first line too
+        s = ''
+
+        # last axis (rows) - wrap elements if they would not fit on one line
+        if axes_left == 1:
+            # the length up until the beginning of the separator / bracket
+            if legacy <= 113:
+                elem_width = curr_width - len(separator.rstrip())
+            else:
+                elem_width = curr_width - max(len(separator.rstrip()), len(']'))
+
+            line = hanging_indent
+            for i in range(leading_items):
+                word = recurser(index + (i,), next_hanging_indent, next_width)
+                s, line = _extendLine_pretty(
+                    s, line, word, elem_width, hanging_indent, legacy)
+                line += separator
+
+            if show_summary:
+                s, line = _extendLine(
+                    s, line, summary_insert, elem_width, hanging_indent, legacy)
+                if legacy <= 113:
+                    line += ", "
+                else:
+                    line += separator
+
+            for i in range(trailing_items, 1, -1):
+                word = recurser(index + (-i,), next_hanging_indent, next_width)
+                s, line = _extendLine_pretty(
+                    s, line, word, elem_width, hanging_indent, legacy)
+                line += separator
+
+            if legacy <= 113:
+                # width of the separator is not considered on 1.13
+                elem_width = curr_width
+            word = recurser(index + (-1,), next_hanging_indent, next_width)
+            s, line = _extendLine_pretty(
+                s, line, word, elem_width, hanging_indent, legacy)
+
+            s += line
+
+        # other axes - insert newlines between rows
+        else:
+            s = ''
+            line_sep = separator.rstrip() + '\n'*(axes_left - 1)
+
+            for i in range(leading_items):
+                nested = recurser(index + (i,), next_hanging_indent, next_width)
+                s += hanging_indent + nested + line_sep
+
+            if show_summary:
+                if legacy <= 113:
+                    # trailing space, fixed nbr of newlines, and fixed separator
+                    s += hanging_indent + summary_insert + ", \n"
+                else:
+                    s += hanging_indent + summary_insert + line_sep
+
+            for i in range(trailing_items, 1, -1):
+                nested = recurser(index + (-i,), next_hanging_indent,
+                                  next_width)
+                s += hanging_indent + nested + line_sep
+
+            nested = recurser(index + (-1,), next_hanging_indent, next_width)
+            s += hanging_indent + nested
+
+        # remove the hanging indent, and wrap in []
+        s = '[' + s[len(hanging_indent):] + ']'
+        return s
+
+    try:
+        # invoke the recursive part with an initial index and prefix
+        return recurser(index=(),
+                        hanging_indent=next_line_prefix,
+                        curr_width=line_width)
+    finally:
+        # recursive closures have a cyclic reference to themselves, which
+        # requires gc to collect (gh-10620). To avoid this problem, for
+        # performance and PyPy friendliness, we break the cycle:
+        recurser = None
+
+def _none_or_positive_arg(x, name):
+    if x is None:
+        return -1
+    if x < 0:
+        raise ValueError("{} must be >= 0".format(name))
+    return x
+
+class FloatingFormat:
+    """ Formatter for subtypes of np.floating """
+    def __init__(self, data, precision, floatmode, suppress_small, sign=False,
+                 *, legacy=None):
+        # for backcompatibility, accept bools
+        if isinstance(sign, bool):
+            sign = '+' if sign else '-'
+
+        self._legacy = legacy
+        if self._legacy <= 113:
+            # when not 0d, legacy does not support '-'
+            if data.shape != () and sign == '-':
+                sign = ' '
+
+        self.floatmode = floatmode
+        if floatmode == 'unique':
+            self.precision = None
+        else:
+            self.precision = precision
+
+        self.precision = _none_or_positive_arg(self.precision, 'precision')
+
+        self.suppress_small = suppress_small
+        self.sign = sign
+        self.exp_format = False
+        self.large_exponent = False
+
+        self.fillFormat(data)
+
+    def fillFormat(self, data):
+        # only the finite values are used to compute the number of digits
+        finite_vals = data[isfinite(data)]
+
+        # choose exponential mode based on the non-zero finite values:
+        abs_non_zero = absolute(finite_vals[finite_vals != 0])
+        if len(abs_non_zero) != 0:
+            max_val = np.max(abs_non_zero)
+            min_val = np.min(abs_non_zero)
+            with errstate(over='ignore'):  # division can overflow
+                if max_val >= 1.e8 or (not self.suppress_small and
+                        (min_val < 0.0001 or max_val/min_val > 1000.)):
+                    self.exp_format = True
+
+        # do a first pass of printing all the numbers, to determine sizes
+        if len(finite_vals) == 0:
+            self.pad_left = 0
+            self.pad_right = 0
+            self.trim = '.'
+            self.exp_size = -1
+            self.unique = True
+            self.min_digits = None
+        elif self.exp_format:
+            trim, unique = '.', True
+            if self.floatmode == 'fixed' or self._legacy <= 113:
+                trim, unique = 'k', False
+            strs = (dragon4_scientific(x, precision=self.precision,
+                               unique=unique, trim=trim, sign=self.sign == '+')
+                    for x in finite_vals)
+            frac_strs, _, exp_strs = zip(*(s.partition('e') for s in strs))
+            int_part, frac_part = zip(*(s.split('.') for s in frac_strs))
+            self.exp_size = max(len(s) for s in exp_strs) - 1
+
+            self.trim = 'k'
+            self.precision = max(len(s) for s in frac_part)
+            self.min_digits = self.precision
+            self.unique = unique
+
+            # for back-compat with np 1.13, use 2 spaces & sign and full prec
+            if self._legacy <= 113:
+                self.pad_left = 3
+            else:
+                # this should be only 1 or 2. Can be calculated from sign.
+                self.pad_left = max(len(s) for s in int_part)
+            # pad_right is only needed for nan length calculation
+            self.pad_right = self.exp_size + 2 + self.precision
+        else:
+            trim, unique = '.', True
+            if self.floatmode == 'fixed':
+                trim, unique = 'k', False
+            strs = (dragon4_positional(x, precision=self.precision,
+                                       fractional=True,
+                                       unique=unique, trim=trim,
+                                       sign=self.sign == '+')
+                    for x in finite_vals)
+            int_part, frac_part = zip(*(s.split('.') for s in strs))
+            if self._legacy <= 113:
+                self.pad_left = 1 + max(len(s.lstrip('-+')) for s in int_part)
+            else:
+                self.pad_left = max(len(s) for s in int_part)
+            self.pad_right = max(len(s) for s in frac_part)
+            self.exp_size = -1
+            self.unique = unique
+
+            if self.floatmode in ['fixed', 'maxprec_equal']:
+                self.precision = self.min_digits = self.pad_right
+                self.trim = 'k'
+            else:
+                self.trim = '.'
+                self.min_digits = 0
+
+        if self._legacy > 113:
+            # account for sign = ' ' by adding one to pad_left
+            if self.sign == ' ' and not any(np.signbit(finite_vals)):
+                self.pad_left += 1
+
+        # if there are non-finite values, may need to increase pad_left
+        if data.size != finite_vals.size:
+            neginf = self.sign != '-' or any(data[isinf(data)] < 0)
+            nanlen = len(_format_options['nanstr'])
+            inflen = len(_format_options['infstr']) + neginf
+            offset = self.pad_right + 1  # +1 for decimal pt
+            self.pad_left = max(self.pad_left, nanlen - offset, inflen - offset)
+
+    def __call__(self, x):
+        if not np.isfinite(x):
+            with errstate(invalid='ignore'):
+                if np.isnan(x):
+                    sign = '+' if self.sign == '+' else ''
+                    ret = sign + _format_options['nanstr']
+                else:  # isinf
+                    sign = '-' if x < 0 else '+' if self.sign == '+' else ''
+                    ret = sign + _format_options['infstr']
+                return ' '*(self.pad_left + self.pad_right + 1 - len(ret)) + ret
+
+        if self.exp_format:
+            return dragon4_scientific(x,
+                                      precision=self.precision,
+                                      min_digits=self.min_digits,
+                                      unique=self.unique,
+                                      trim=self.trim,
+                                      sign=self.sign == '+',
+                                      pad_left=self.pad_left,
+                                      exp_digits=self.exp_size)
+        else:
+            return dragon4_positional(x,
+                                      precision=self.precision,
+                                      min_digits=self.min_digits,
+                                      unique=self.unique,
+                                      fractional=True,
+                                      trim=self.trim,
+                                      sign=self.sign == '+',
+                                      pad_left=self.pad_left,
+                                      pad_right=self.pad_right)
+
+
+@set_module('numpy')
+def format_float_scientific(x, precision=None, unique=True, trim='k',
+                            sign=False, pad_left=None, exp_digits=None,
+                            min_digits=None):
+    """
+    Format a floating-point scalar as a decimal string in scientific notation.
+
+    Provides control over rounding, trimming and padding. Uses and assumes
+    IEEE unbiased rounding. Uses the "Dragon4" algorithm.
+
+    Parameters
+    ----------
+    x : python float or numpy floating scalar
+        Value to format.
+    precision : non-negative integer or None, optional
+        Maximum number of digits to print. May be None if `unique` is
+        `True`, but must be an integer if unique is `False`.
+    unique : boolean, optional
+        If `True`, use a digit-generation strategy which gives the shortest
+        representation which uniquely identifies the floating-point number from
+        other values of the same type, by judicious rounding. If `precision`
+        is given fewer digits than necessary can be printed. If `min_digits`
+        is given more can be printed, in which cases the last digit is rounded
+        with unbiased rounding.
+        If `False`, digits are generated as if printing an infinite-precision
+        value and stopping after `precision` digits, rounding the remaining
+        value with unbiased rounding
+    trim : one of 'k', '.', '0', '-', optional
+        Controls post-processing trimming of trailing digits, as follows:
+
+        * 'k' : keep trailing zeros, keep decimal point (no trimming)
+        * '.' : trim all trailing zeros, leave decimal point
+        * '0' : trim all but the zero before the decimal point. Insert the
+          zero if it is missing.
+        * '-' : trim trailing zeros and any trailing decimal point
+    sign : boolean, optional
+        Whether to show the sign for positive values.
+    pad_left : non-negative integer, optional
+        Pad the left side of the string with whitespace until at least that
+        many characters are to the left of the decimal point.
+    exp_digits : non-negative integer, optional
+        Pad the exponent with zeros until it contains at least this many digits.
+        If omitted, the exponent will be at least 2 digits.
+    min_digits : non-negative integer or None, optional
+        Minimum number of digits to print. This only has an effect for
+        `unique=True`. In that case more digits than necessary to uniquely
+        identify the value may be printed and rounded unbiased.
+
+        -- versionadded:: 1.21.0
+        
+    Returns
+    -------
+    rep : string
+        The string representation of the floating point value
+
+    See Also
+    --------
+    format_float_positional
+
+    Examples
+    --------
+    >>> np.format_float_scientific(np.float32(np.pi))
+    '3.1415927e+00'
+    >>> s = np.float32(1.23e24)
+    >>> np.format_float_scientific(s, unique=False, precision=15)
+    '1.230000071797338e+24'
+    >>> np.format_float_scientific(s, exp_digits=4)
+    '1.23e+0024'
+    """
+    precision = _none_or_positive_arg(precision, 'precision')
+    pad_left = _none_or_positive_arg(pad_left, 'pad_left')
+    exp_digits = _none_or_positive_arg(exp_digits, 'exp_digits')
+    min_digits = _none_or_positive_arg(min_digits, 'min_digits')
+    if min_digits > 0 and precision > 0 and min_digits > precision:
+        raise ValueError("min_digits must be less than or equal to precision")
+    return dragon4_scientific(x, precision=precision, unique=unique,
+                              trim=trim, sign=sign, pad_left=pad_left,
+                              exp_digits=exp_digits, min_digits=min_digits)
+
+
+@set_module('numpy')
+def format_float_positional(x, precision=None, unique=True,
+                            fractional=True, trim='k', sign=False,
+                            pad_left=None, pad_right=None, min_digits=None):
+    """
+    Format a floating-point scalar as a decimal string in positional notation.
+
+    Provides control over rounding, trimming and padding. Uses and assumes
+    IEEE unbiased rounding. Uses the "Dragon4" algorithm.
+
+    Parameters
+    ----------
+    x : python float or numpy floating scalar
+        Value to format.
+    precision : non-negative integer or None, optional
+        Maximum number of digits to print. May be None if `unique` is
+        `True`, but must be an integer if unique is `False`.
+    unique : boolean, optional
+        If `True`, use a digit-generation strategy which gives the shortest
+        representation which uniquely identifies the floating-point number from
+        other values of the same type, by judicious rounding. If `precision`
+        is given fewer digits than necessary can be printed, or if `min_digits`
+        is given more can be printed, in which cases the last digit is rounded
+        with unbiased rounding.
+        If `False`, digits are generated as if printing an infinite-precision
+        value and stopping after `precision` digits, rounding the remaining
+        value with unbiased rounding
+    fractional : boolean, optional
+        If `True`, the cutoffs of `precision` and `min_digits` refer to the
+        total number of digits after the decimal point, including leading
+        zeros.
+        If `False`, `precision` and `min_digits` refer to the total number of
+        significant digits, before or after the decimal point, ignoring leading
+        zeros.
+    trim : one of 'k', '.', '0', '-', optional
+        Controls post-processing trimming of trailing digits, as follows:
+
+        * 'k' : keep trailing zeros, keep decimal point (no trimming)
+        * '.' : trim all trailing zeros, leave decimal point
+        * '0' : trim all but the zero before the decimal point. Insert the
+          zero if it is missing.
+        * '-' : trim trailing zeros and any trailing decimal point
+    sign : boolean, optional
+        Whether to show the sign for positive values.
+    pad_left : non-negative integer, optional
+        Pad the left side of the string with whitespace until at least that
+        many characters are to the left of the decimal point.
+    pad_right : non-negative integer, optional
+        Pad the right side of the string with whitespace until at least that
+        many characters are to the right of the decimal point.
+    min_digits : non-negative integer or None, optional
+        Minimum number of digits to print. Only has an effect if `unique=True`
+        in which case additional digits past those necessary to uniquely
+        identify the value may be printed, rounding the last additional digit.
+        
+        -- versionadded:: 1.21.0
+
+    Returns
+    -------
+    rep : string
+        The string representation of the floating point value
+
+    See Also
+    --------
+    format_float_scientific
+
+    Examples
+    --------
+    >>> np.format_float_positional(np.float32(np.pi))
+    '3.1415927'
+    >>> np.format_float_positional(np.float16(np.pi))
+    '3.14'
+    >>> np.format_float_positional(np.float16(0.3))
+    '0.3'
+    >>> np.format_float_positional(np.float16(0.3), unique=False, precision=10)
+    '0.3000488281'
+    """
+    precision = _none_or_positive_arg(precision, 'precision')
+    pad_left = _none_or_positive_arg(pad_left, 'pad_left')
+    pad_right = _none_or_positive_arg(pad_right, 'pad_right')
+    min_digits = _none_or_positive_arg(min_digits, 'min_digits')
+    if not fractional and precision == 0:
+        raise ValueError("precision must be greater than 0 if "
+                         "fractional=False")
+    if min_digits > 0 and precision > 0 and min_digits > precision:
+        raise ValueError("min_digits must be less than or equal to precision")
+    return dragon4_positional(x, precision=precision, unique=unique,
+                              fractional=fractional, trim=trim,
+                              sign=sign, pad_left=pad_left,
+                              pad_right=pad_right, min_digits=min_digits)
+
+
+class IntegerFormat:
+    def __init__(self, data):
+        if data.size > 0:
+            max_str_len = max(len(str(np.max(data))),
+                              len(str(np.min(data))))
+        else:
+            max_str_len = 0
+        self.format = '%{}d'.format(max_str_len)
+
+    def __call__(self, x):
+        return self.format % x
+
+
+class BoolFormat:
+    def __init__(self, data, **kwargs):
+        # add an extra space so " True" and "False" have the same length and
+        # array elements align nicely when printed, except in 0d arrays
+        self.truestr = ' True' if data.shape != () else 'True'
+
+    def __call__(self, x):
+        return self.truestr if x else "False"
+
+
+class ComplexFloatingFormat:
+    """ Formatter for subtypes of np.complexfloating """
+    def __init__(self, x, precision, floatmode, suppress_small,
+                 sign=False, *, legacy=None):
+        # for backcompatibility, accept bools
+        if isinstance(sign, bool):
+            sign = '+' if sign else '-'
+
+        floatmode_real = floatmode_imag = floatmode
+        if legacy <= 113:
+            floatmode_real = 'maxprec_equal'
+            floatmode_imag = 'maxprec'
+
+        self.real_format = FloatingFormat(
+            x.real, precision, floatmode_real, suppress_small,
+            sign=sign, legacy=legacy
+        )
+        self.imag_format = FloatingFormat(
+            x.imag, precision, floatmode_imag, suppress_small,
+            sign='+', legacy=legacy
+        )
+
+    def __call__(self, x):
+        r = self.real_format(x.real)
+        i = self.imag_format(x.imag)
+
+        # add the 'j' before the terminal whitespace in i
+        sp = len(i.rstrip())
+        i = i[:sp] + 'j' + i[sp:]
+
+        return r + i
+
+
+class _TimelikeFormat:
+    def __init__(self, data):
+        non_nat = data[~isnat(data)]
+        if len(non_nat) > 0:
+            # Max str length of non-NaT elements
+            max_str_len = max(len(self._format_non_nat(np.max(non_nat))),
+                              len(self._format_non_nat(np.min(non_nat))))
+        else:
+            max_str_len = 0
+        if len(non_nat) < data.size:
+            # data contains a NaT
+            max_str_len = max(max_str_len, 5)
+        self._format = '%{}s'.format(max_str_len)
+        self._nat = "'NaT'".rjust(max_str_len)
+
+    def _format_non_nat(self, x):
+        # override in subclass
+        raise NotImplementedError
+
+    def __call__(self, x):
+        if isnat(x):
+            return self._nat
+        else:
+            return self._format % self._format_non_nat(x)
+
+
+class DatetimeFormat(_TimelikeFormat):
+    def __init__(self, x, unit=None, timezone=None, casting='same_kind',
+                 legacy=False):
+        # Get the unit from the dtype
+        if unit is None:
+            if x.dtype.kind == 'M':
+                unit = datetime_data(x.dtype)[0]
+            else:
+                unit = 's'
+
+        if timezone is None:
+            timezone = 'naive'
+        self.timezone = timezone
+        self.unit = unit
+        self.casting = casting
+        self.legacy = legacy
+
+        # must be called after the above are configured
+        super().__init__(x)
+
+    def __call__(self, x):
+        if self.legacy <= 113:
+            return self._format_non_nat(x)
+        return super().__call__(x)
+
+    def _format_non_nat(self, x):
+        return "'%s'" % datetime_as_string(x,
+                                    unit=self.unit,
+                                    timezone=self.timezone,
+                                    casting=self.casting)
+
+
+class TimedeltaFormat(_TimelikeFormat):
+    def _format_non_nat(self, x):
+        return str(x.astype('i8'))
+
+
+class SubArrayFormat:
+    def __init__(self, format_function):
+        self.format_function = format_function
+
+    def __call__(self, arr):
+        if arr.ndim <= 1:
+            return "[" + ", ".join(self.format_function(a) for a in arr) + "]"
+        return "[" + ", ".join(self.__call__(a) for a in arr) + "]"
+
+
+class StructuredVoidFormat:
+    """
+    Formatter for structured np.void objects.
+
+    This does not work on structured alias types like np.dtype(('i4', 'i2,i2')),
+    as alias scalars lose their field information, and the implementation
+    relies upon np.void.__getitem__.
+    """
+    def __init__(self, format_functions):
+        self.format_functions = format_functions
+
+    @classmethod
+    def from_data(cls, data, **options):
+        """
+        This is a second way to initialize StructuredVoidFormat, using the raw data
+        as input. Added to avoid changing the signature of __init__.
+        """
+        format_functions = []
+        for field_name in data.dtype.names:
+            format_function = _get_format_function(data[field_name], **options)
+            if data.dtype[field_name].shape != ():
+                format_function = SubArrayFormat(format_function)
+            format_functions.append(format_function)
+        return cls(format_functions)
+
+    def __call__(self, x):
+        str_fields = [
+            format_function(field)
+            for field, format_function in zip(x, self.format_functions)
+        ]
+        if len(str_fields) == 1:
+            return "({},)".format(str_fields[0])
+        else:
+            return "({})".format(", ".join(str_fields))
+
+
+def _void_scalar_repr(x):
+    """
+    Implements the repr for structured-void scalars. It is called from the
+    scalartypes.c.src code, and is placed here because it uses the elementwise
+    formatters defined above.
+    """
+    return StructuredVoidFormat.from_data(array(x), **_format_options)(x)
+
+
+_typelessdata = [int_, float_, complex_, bool_]
+if issubclass(intc, int):
+    _typelessdata.append(intc)
+if issubclass(longlong, int):
+    _typelessdata.append(longlong)
+
+
+def dtype_is_implied(dtype):
+    """
+    Determine if the given dtype is implied by the representation of its values.
+
+    Parameters
+    ----------
+    dtype : dtype
+        Data type
+
+    Returns
+    -------
+    implied : bool
+        True if the dtype is implied by the representation of its values.
+
+    Examples
+    --------
+    >>> np.core.arrayprint.dtype_is_implied(int)
+    True
+    >>> np.array([1, 2, 3], int)
+    array([1, 2, 3])
+    >>> np.core.arrayprint.dtype_is_implied(np.int8)
+    False
+    >>> np.array([1, 2, 3], np.int8)
+    array([1, 2, 3], dtype=int8)
+    """
+    dtype = np.dtype(dtype)
+    if _format_options['legacy'] <= 113 and dtype.type == bool_:
+        return False
+
+    # not just void types can be structured, and names are not part of the repr
+    if dtype.names is not None:
+        return False
+
+    return dtype.type in _typelessdata
+
+
+def dtype_short_repr(dtype):
+    """
+    Convert a dtype to a short form which evaluates to the same dtype.
+
+    The intent is roughly that the following holds
+
+    >>> from numpy import *
+    >>> dt = np.int64([1, 2]).dtype
+    >>> assert eval(dtype_short_repr(dt)) == dt
+    """
+    if type(dtype).__repr__ != np.dtype.__repr__:
+        # TODO: Custom repr for user DTypes, logic should likely move.
+        return repr(dtype)
+    if dtype.names is not None:
+        # structured dtypes give a list or tuple repr
+        return str(dtype)
+    elif issubclass(dtype.type, flexible):
+        # handle these separately so they don't give garbage like str256
+        return "'%s'" % str(dtype)
+
+    typename = dtype.name
+    # quote typenames which can't be represented as python variable names
+    if typename and not (typename[0].isalpha() and typename.isalnum()):
+        typename = repr(typename)
+
+    return typename
+
+
+def _array_repr_implementation(
+        arr, max_line_width=None, precision=None, suppress_small=None,
+        array2string=array2string):
+    """Internal version of array_repr() that allows overriding array2string."""
+    if max_line_width is None:
+        max_line_width = _format_options['linewidth']
+
+    if type(arr) is not ndarray:
+        class_name = type(arr).__name__
+    else:
+        class_name = "array"
+
+    skipdtype = dtype_is_implied(arr.dtype) and arr.size > 0
+
+    prefix = class_name + "("
+    suffix = ")" if skipdtype else ","
+
+    if (_format_options['legacy'] <= 113 and
+            arr.shape == () and not arr.dtype.names):
+        lst = repr(arr.item())
+    elif arr.size > 0 or arr.shape == (0,):
+        lst = array2string(arr, max_line_width, precision, suppress_small,
+                           ', ', prefix, suffix=suffix)
+    else:  # show zero-length shape unless it is (0,)
+        lst = "[], shape=%s" % (repr(arr.shape),)
+
+    arr_str = prefix + lst + suffix
+
+    if skipdtype:
+        return arr_str
+
+    dtype_str = "dtype={})".format(dtype_short_repr(arr.dtype))
+
+    # compute whether we should put dtype on a new line: Do so if adding the
+    # dtype would extend the last line past max_line_width.
+    # Note: This line gives the correct result even when rfind returns -1.
+    last_line_len = len(arr_str) - (arr_str.rfind('\n') + 1)
+    spacer = " "
+    if _format_options['legacy'] <= 113:
+        if issubclass(arr.dtype.type, flexible):
+            spacer = '\n' + ' '*len(class_name + "(")
+    elif last_line_len + len(dtype_str) + 1 > max_line_width:
+        spacer = '\n' + ' '*len(class_name + "(")
+
+    return arr_str + spacer + dtype_str
+
+
+def _array_repr_dispatcher(
+        arr, max_line_width=None, precision=None, suppress_small=None):
+    return (arr,)
+
+
+@array_function_dispatch(_array_repr_dispatcher, module='numpy')
+def array_repr(arr, max_line_width=None, precision=None, suppress_small=None):
+    """
+    Return the string representation of an array.
+
+    Parameters
+    ----------
+    arr : ndarray
+        Input array.
+    max_line_width : int, optional
+        Inserts newlines if text is longer than `max_line_width`.
+        Defaults to ``numpy.get_printoptions()['linewidth']``.
+    precision : int, optional
+        Floating point precision.
+        Defaults to ``numpy.get_printoptions()['precision']``.
+    suppress_small : bool, optional
+        Represent numbers "very close" to zero as zero; default is False.
+        Very close is defined by precision: if the precision is 8, e.g.,
+        numbers smaller (in absolute value) than 5e-9 are represented as
+        zero.
+        Defaults to ``numpy.get_printoptions()['suppress']``.
+
+    Returns
+    -------
+    string : str
+      The string representation of an array.
+
+    See Also
+    --------
+    array_str, array2string, set_printoptions
+
+    Examples
+    --------
+    >>> np.array_repr(np.array([1,2]))
+    'array([1, 2])'
+    >>> np.array_repr(np.ma.array([0.]))
+    'MaskedArray([0.])'
+    >>> np.array_repr(np.array([], np.int32))
+    'array([], dtype=int32)'
+
+    >>> x = np.array([1e-6, 4e-7, 2, 3])
+    >>> np.array_repr(x, precision=6, suppress_small=True)
+    'array([0.000001,  0.      ,  2.      ,  3.      ])'
+
+    """
+    return _array_repr_implementation(
+        arr, max_line_width, precision, suppress_small)
+
+
+@_recursive_guard()
+def _guarded_repr_or_str(v):
+    if isinstance(v, bytes):
+        return repr(v)
+    return str(v)
+
+
+def _array_str_implementation(
+        a, max_line_width=None, precision=None, suppress_small=None,
+        array2string=array2string):
+    """Internal version of array_str() that allows overriding array2string."""
+    if (_format_options['legacy'] <= 113 and
+            a.shape == () and not a.dtype.names):
         return str(a.item())
 
-    if summary_insert and 2*edge_items < len(a):
-        leading_items, trailing_items, summary_insert1 = \
-                       edge_items, edge_items, summary_insert
+    # the str of 0d arrays is a special case: It should appear like a scalar,
+    # so floats are not truncated by `precision`, and strings are not wrapped
+    # in quotes. So we return the str of the scalar value.
+    if a.shape == ():
+        # obtain a scalar and call str on it, avoiding problems for subclasses
+        # for which indexing with () returns a 0d instead of a scalar by using
+        # ndarray's getindex. Also guard against recursive 0d object arrays.
+        return _guarded_repr_or_str(np.ndarray.__getitem__(a, ()))
+
+    return array2string(a, max_line_width, precision, suppress_small, ' ', "")
+
+
+def _array_str_dispatcher(
+        a, max_line_width=None, precision=None, suppress_small=None):
+    return (a,)
+
+
+@array_function_dispatch(_array_str_dispatcher, module='numpy')
+def array_str(a, max_line_width=None, precision=None, suppress_small=None):
+    """
+    Return a string representation of the data in an array.
+
+    The data in the array is returned as a single string.  This function is
+    similar to `array_repr`, the difference being that `array_repr` also
+    returns information on the kind of array and its data type.
+
+    Parameters
+    ----------
+    a : ndarray
+        Input array.
+    max_line_width : int, optional
+        Inserts newlines if text is longer than `max_line_width`.
+        Defaults to ``numpy.get_printoptions()['linewidth']``.
+    precision : int, optional
+        Floating point precision.
+        Defaults to ``numpy.get_printoptions()['precision']``.
+    suppress_small : bool, optional
+        Represent numbers "very close" to zero as zero; default is False.
+        Very close is defined by precision: if the precision is 8, e.g.,
+        numbers smaller (in absolute value) than 5e-9 are represented as
+        zero.
+        Defaults to ``numpy.get_printoptions()['suppress']``.
+
+    See Also
+    --------
+    array2string, array_repr, set_printoptions
+
+    Examples
+    --------
+    >>> np.array_str(np.arange(3))
+    '[0 1 2]'
+
+    """
+    return _array_str_implementation(
+        a, max_line_width, precision, suppress_small)
+
+
+# needed if __array_function__ is disabled
+_array2string_impl = getattr(array2string, '__wrapped__', array2string)
+_default_array_str = functools.partial(_array_str_implementation,
+                                       array2string=_array2string_impl)
+_default_array_repr = functools.partial(_array_repr_implementation,
+                                        array2string=_array2string_impl)
+
+
+def set_string_function(f, repr=True):
+    """
+    Set a Python function to be used when pretty printing arrays.
+
+    Parameters
+    ----------
+    f : function or None
+        Function to be used to pretty print arrays. The function should expect
+        a single array argument and return a string of the representation of
+        the array. If None, the function is reset to the default NumPy function
+        to print arrays.
+    repr : bool, optional
+        If True (default), the function for pretty printing (``__repr__``)
+        is set, if False the function that returns the default string
+        representation (``__str__``) is set.
+
+    See Also
+    --------
+    set_printoptions, get_printoptions
+
+    Examples
+    --------
+    >>> def pprint(arr):
+    ...     return 'HA! - What are you going to do now?'
+    ...
+    >>> np.set_string_function(pprint)
+    >>> a = np.arange(10)
+    >>> a
+    HA! - What are you going to do now?
+    >>> _ = a
+    >>> # [0 1 2 3 4 5 6 7 8 9]
+
+    We can reset the function to the default:
+
+    >>> np.set_string_function(None)
+    >>> a
+    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
+
+    `repr` affects either pretty printing or normal string representation.
+    Note that ``__repr__`` is still affected by setting ``__str__``
+    because the width of each array element in the returned string becomes
+    equal to the length of the result of ``__str__()``.
+
+    >>> x = np.arange(4)
+    >>> np.set_string_function(lambda x:'random', repr=False)
+    >>> x.__str__()
+    'random'
+    >>> x.__repr__()
+    'array([0, 1, 2, 3])'
+
+    """
+    if f is None:
+        if repr:
+            return multiarray.set_string_function(_default_array_repr, 1)
+        else:
+            return multiarray.set_string_function(_default_array_str, 0)
     else:
-        leading_items, trailing_items, summary_insert1 = 0, len(a), ""
-
-    if rank == 1:
-
-        s = ""
-        line = next_line_prefix
-        for i in xrange(leading_items):
-            word = format_function(a[i]) + separator
-            s, line = _extendLine(s, line, word, max_line_len, next_line_prefix)
-
-        if summary_insert1:
-            s, line = _extendLine(s, line, summary_insert1, max_line_len, next_line_prefix)
-
-        for i in xrange(trailing_items, 1, -1):
-            word = format_function(a[-i]) + separator
-            s, line = _extendLine(s, line, word, max_line_len, next_line_prefix)
-
-        word = format_function(a[-1])
-        s, line = _extendLine(s, line, word, max_line_len, next_line_prefix)
-        s += line + "]\n"
-        s = '[' + s[len(next_line_prefix):]
-    else:
-        s = '['
-        sep = separator.rstrip()
-        for i in xrange(leading_items):
-            if i > 0:
-                s += next_line_prefix
-            s += _formatArray(a[i], format_function, rank-1, max_line_len,
-                              " " + next_line_prefix, separator, edge_items,
-                              summary_insert)
-            s = s.rstrip() + sep.rstrip() + '\n'*max(rank-1,1)
-
-        if summary_insert1:
-            s += next_line_prefix + summary_insert1 + "\n"
-
-        for i in xrange(trailing_items, 1, -1):
-            if leading_items or i != trailing_items:
-                s += next_line_prefix
-            s += _formatArray(a[-i], format_function, rank-1, max_line_len,
-                              " " + next_line_prefix, separator, edge_items,
-                              summary_insert)
-            s = s.rstrip() + sep.rstrip() + '\n'*max(rank-1,1)
-        if leading_items or trailing_items > 1:
-            s += next_line_prefix
-        s += _formatArray(a[-1], format_function, rank-1, max_line_len,
-                          " " + next_line_prefix, separator, edge_items,
-                          summary_insert).rstrip()+']\n'
-    return s
-
-def _floatFormat(data, precision, suppress_small, sign = 0):
-    exp_format = 0
-    non_zero = _uf.absolute(data.compress(_uf.not_equal(data, 0)))
-    ##non_zero = _numeric_compress(data) ##
-    if len(non_zero) == 0:
-        max_val = 0.
-        min_val = 0.
-    else:
-        max_val = max_reduce(non_zero)
-        min_val = min_reduce(non_zero)
-        if max_val >= 1.e8:
-            exp_format = 1
-        if not suppress_small and (min_val < 0.0001
-                                   or max_val/min_val > 1000.):
-            exp_format = 1
-    if exp_format:
-        large_exponent = 0 < min_val < 1e-99 or max_val >= 1e100
-        max_str_len = 8 + precision + large_exponent
-        if sign: format = '%+'
-        else: format = '%'
-        format = format + str(max_str_len) + '.' + str(precision) + 'e'
-        if large_exponent: format = format + '3'
-    else:
-        format = '%.' + str(precision) + 'f'
-        precision = min(precision, max([_digits(x, precision, format)
-                                        for x in data]))
-        max_str_len = len(str(int(max_val))) + precision + 2
-        if sign: format = '%#+'
-        else: format = '%#'
-        format = format + str(max_str_len) + '.' + str(precision) + 'f'
-    return format
-
-def _digits(x, precision, format):
-    s = format % x
-    zeros = len(s)
-    while s[zeros-1] == '0': zeros = zeros-1
-    return precision-len(s)+zeros
-
-
-_MAXINT = sys.maxint
-_MININT = -sys.maxint-1
-def _formatInteger(x, format):
-    if (x < _MAXINT) and (x > _MININT):
-        return format % x
-    else:
-        return "%s" % x
-
-def _longfloatFormatter(precision):
-    def formatter(x):
-        return format_longfloat(x, precision)
-    return formatter
-
-def _formatFloat(x, format, strip_zeros = 1):
-    if format[-1] == '3':
-        # 3-digit exponent
-        format = format[:-1]
-        s = format % x
-        third = s[-3]
-        if third == '+' or third == '-':
-            s = s[1:-2] + '0' + s[-2:]
-    elif format[-1] == 'e':
-        # 2-digit exponent
-        s = format % x
-        if s[-3] == '0':
-            s = ' ' + s[:-3] + s[-2:]
-    elif format[-1] == 'f':
-        s = format % x
-        if strip_zeros:
-            zeros = len(s)
-            while s[zeros-1] == '0': zeros = zeros-1
-            s = s[:zeros] + (len(s)-zeros)*' '
-    else:
-        s = format % x
-    return s
-
-def _clongfloatFormatter(precision):
-    def formatter(x):
-        r = format_longfloat(x.real, precision)
-        i = format_longfloat(x.imag, precision)
-        if x.imag < 0:
-            i = '-' + i
-        else:
-            i = '+' + i
-        return r + i + 'j'
-    return formatter
-
-def _formatComplex(x, real_format, imag_format):
-    r = _formatFloat(x.real, real_format)
-    i = _formatFloat(x.imag, imag_format, 0)
-    if imag_format[-1] == 'f':
-        zeros = len(i)
-        while zeros > 2 and i[zeros-1] == '0': zeros = zeros-1
-        i = i[:zeros] + 'j' + (len(i)-zeros)*' '
-    else:
-        i = i + 'j'
-    return r + i
-
-def _formatGeneral(x):
-    return str(x) + ' '
-
-if __name__ == '__main__':
-    a = _nc.arange(10)
-    print array2string(a)
-    print array2string(_nc.array([[],[]]))
+        return multiarray.set_string_function(f, repr)
('numpy/core/code_generators', 'generate_ufunc_api.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,10 +1,14 @@
 import os
 import genapi
+
+import numpy_api
+
+from genapi import TypeApi, FunctionApi
 
 h_template = r"""
 #ifdef _UMATHMODULE
 
-static PyTypeObject PyUFunc_Type;
+extern NPY_NO_EXPORT PyTypeObject PyUFunc_Type;
 
 %s
 
@@ -24,29 +28,81 @@
 #endif
 #endif
 
-#define PyUFunc_Type (*(PyTypeObject *)PyUFunc_API[0])
-
 %s
 
-static int
-import_umath(void)
+static NPY_INLINE int
+_import_umath(void)
 {
-  PyObject *numpy = PyImport_ImportModule("numpy.core.umath");
+  PyObject *numpy = PyImport_ImportModule("numpy.core._multiarray_umath");
   PyObject *c_api = NULL;
 
-  if (numpy == NULL) return -1;
+  if (numpy == NULL) {
+      PyErr_SetString(PyExc_ImportError,
+                      "numpy.core._multiarray_umath failed to import");
+      return -1;
+  }
   c_api = PyObject_GetAttrString(numpy, "_UFUNC_API");
-  if (c_api == NULL) {Py_DECREF(numpy); return -1;}
-  if (PyCObject_Check(c_api)) {
-      PyUFunc_API = (void **)PyCObject_AsVoidPtr(c_api);
+  Py_DECREF(numpy);
+  if (c_api == NULL) {
+      PyErr_SetString(PyExc_AttributeError, "_UFUNC_API not found");
+      return -1;
   }
+
+  if (!PyCapsule_CheckExact(c_api)) {
+      PyErr_SetString(PyExc_RuntimeError, "_UFUNC_API is not PyCapsule object");
+      Py_DECREF(c_api);
+      return -1;
+  }
+  PyUFunc_API = (void **)PyCapsule_GetPointer(c_api, NULL);
   Py_DECREF(c_api);
-  Py_DECREF(numpy);
-  if (PyUFunc_API == NULL) return -1;
+  if (PyUFunc_API == NULL) {
+      PyErr_SetString(PyExc_RuntimeError, "_UFUNC_API is NULL pointer");
+      return -1;
+  }
   return 0;
 }
 
-#define import_ufunc import_umath
+#define import_umath() \
+    do {\
+        UFUNC_NOFPE\
+        if (_import_umath() < 0) {\
+            PyErr_Print();\
+            PyErr_SetString(PyExc_ImportError,\
+                    "numpy.core.umath failed to import");\
+            return NULL;\
+        }\
+    } while(0)
+
+#define import_umath1(ret) \
+    do {\
+        UFUNC_NOFPE\
+        if (_import_umath() < 0) {\
+            PyErr_Print();\
+            PyErr_SetString(PyExc_ImportError,\
+                    "numpy.core.umath failed to import");\
+            return ret;\
+        }\
+    } while(0)
+
+#define import_umath2(ret, msg) \
+    do {\
+        UFUNC_NOFPE\
+        if (_import_umath() < 0) {\
+            PyErr_Print();\
+            PyErr_SetString(PyExc_ImportError, msg);\
+            return ret;\
+        }\
+    } while(0)
+
+#define import_ufunc() \
+    do {\
+        UFUNC_NOFPE\
+        if (_import_umath() < 0) {\
+            PyErr_Print();\
+            PyErr_SetString(PyExc_ImportError,\
+                    "numpy.core.umath failed to import");\
+        }\
+    } while(0)
 
 #endif
 """
@@ -57,37 +113,81 @@
 */
 
 void *PyUFunc_API[] = {
-        (void *) &PyUFunc_Type,
 %s
 };
 """
 
-def generate_api(output_dir):
-    ufunc_api_list = genapi.get_api_functions('UFUNC_API',
-                                              'ufunc_api_order.txt')
+def generate_api(output_dir, force=False):
+    basename = 'ufunc_api'
 
-    # API fixes for __arrayobject_api.h
+    h_file = os.path.join(output_dir, '__%s.h' % basename)
+    c_file = os.path.join(output_dir, '__%s.c' % basename)
+    d_file = os.path.join(output_dir, '%s.txt' % basename)
+    targets = (h_file, c_file, d_file)
 
-    fixed = 1
-    nummulti = len(ufunc_api_list)
-    numtotal = fixed + nummulti
+    sources = ['ufunc_api_order.txt']
 
+    if (not force and not genapi.should_rebuild(targets, sources + [__file__])):
+        return targets
+    else:
+        do_generate_api(targets, sources)
+
+    return targets
+
+def do_generate_api(targets, sources):
+    header_file = targets[0]
+    c_file = targets[1]
+    doc_file = targets[2]
+
+    ufunc_api_index = genapi.merge_api_dicts((
+            numpy_api.ufunc_funcs_api,
+            numpy_api.ufunc_types_api))
+    genapi.check_api_dict(ufunc_api_index)
+
+    ufunc_api_list = genapi.get_api_functions('UFUNC_API', numpy_api.ufunc_funcs_api)
+
+    # Create dict name -> *Api instance
+    ufunc_api_dict = {}
+    api_name = 'PyUFunc_API'
+    for f in ufunc_api_list:
+        name = f.name
+        index = ufunc_api_index[name][0]
+        annotations = ufunc_api_index[name][1:]
+        ufunc_api_dict[name] = FunctionApi(f.name, index, annotations,
+                                           f.return_type, f.args, api_name)
+
+    for name, val in numpy_api.ufunc_types_api.items():
+        index = val[0]
+        ufunc_api_dict[name] = TypeApi(name, index, 'PyTypeObject', api_name)
+
+    # set up object API
     module_list = []
     extension_list = []
     init_list = []
 
-    #setup object API
-    genapi.add_api_list(fixed, 'PyUFunc_API', ufunc_api_list,
-                        module_list, extension_list, init_list)
+    for name, index in genapi.order_dict(ufunc_api_index):
+        api_item = ufunc_api_dict[name]
+        extension_list.append(api_item.define_from_array_api_string())
+        init_list.append(api_item.array_api_define())
+        module_list.append(api_item.internal_define())
 
     # Write to header
-    fid = open(os.path.join(output_dir, '__ufunc_api.h'),'w')
     s = h_template % ('\n'.join(module_list), '\n'.join(extension_list))
-    fid.write(s)
-    fid.close()
+    genapi.write_file(header_file, s)
 
     # Write to c-code
-    fid = open(os.path.join(output_dir, '__ufunc_api.c'),'w')
-    s = c_template % '\n'.join(init_list)
-    fid.write(s)
-    fid.close()
+    s = c_template % ',\n'.join(init_list)
+    genapi.write_file(c_file, s)
+
+    # Write to documentation
+    s = '''
+=================
+NumPy Ufunc C-API
+=================
+'''
+    for func in ufunc_api_list:
+        s += func.to_ReST()
+        s += '\n\n'
+    genapi.write_file(doc_file, s)
+
+    return targets
('numpy/core/code_generators', 'genapi.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,48 +1,135 @@
-import sys, os, re
-import md5
-
-API_FILES = ['arraymethods.c',
-             'arrayobject.c',
-             'arraytypes.inc.src',
-             'multiarraymodule.c',
-             'scalartypes.inc.src',
-             'ufuncobject.c',
+"""
+Get API information encoded in C files.
+
+See ``find_function`` for how functions should be formatted, and
+``read_order`` for how the order of the functions should be
+specified.
+
+"""
+from numpy.distutils.conv_template import process_file as process_c_file
+
+import hashlib
+import io
+import os
+import re
+import sys
+import textwrap
+
+from os.path import join
+
+__docformat__ = 'restructuredtext'
+
+# The files under src/ that are scanned for API functions
+API_FILES = [join('multiarray', 'alloc.c'),
+             join('multiarray', 'abstractdtypes.c'),
+             join('multiarray', 'arrayfunction_override.c'),
+             join('multiarray', 'array_assign_array.c'),
+             join('multiarray', 'array_assign_scalar.c'),
+             join('multiarray', 'array_coercion.c'),
+             join('multiarray', 'array_method.c'),
+             join('multiarray', 'arrayobject.c'),
+             join('multiarray', 'arraytypes.c.src'),
+             join('multiarray', 'buffer.c'),
+             join('multiarray', 'calculation.c'),
+             join('multiarray', 'common_dtype.c'),
+             join('multiarray', 'conversion_utils.c'),
+             join('multiarray', 'convert.c'),
+             join('multiarray', 'convert_datatype.c'),
+             join('multiarray', 'ctors.c'),
+             join('multiarray', 'datetime.c'),
+             join('multiarray', 'datetime_busday.c'),
+             join('multiarray', 'datetime_busdaycal.c'),
+             join('multiarray', 'datetime_strings.c'),
+             join('multiarray', 'descriptor.c'),
+             join('multiarray', 'dlpack.c'),
+             join('multiarray', 'dtypemeta.c'),
+             join('multiarray', 'einsum.c.src'),
+             join('multiarray', 'flagsobject.c'),
+             join('multiarray', 'getset.c'),
+             join('multiarray', 'item_selection.c'),
+             join('multiarray', 'iterators.c'),
+             join('multiarray', 'mapping.c'),
+             join('multiarray', 'methods.c'),
+             join('multiarray', 'multiarraymodule.c'),
+             join('multiarray', 'nditer_api.c'),
+             join('multiarray', 'nditer_constr.c'),
+             join('multiarray', 'nditer_pywrap.c'),
+             join('multiarray', 'nditer_templ.c.src'),
+             join('multiarray', 'number.c'),
+             join('multiarray', 'refcount.c'),
+             join('multiarray', 'scalartypes.c.src'),
+             join('multiarray', 'scalarapi.c'),
+             join('multiarray', 'sequence.c'),
+             join('multiarray', 'shape.c'),
+             join('multiarray', 'strfuncs.c'),
+             join('multiarray', 'usertypes.c'),
+             join('umath', 'loops.c.src'),
+             join('umath', 'ufunc_object.c'),
+             join('umath', 'ufunc_type_resolution.c'),
+             join('umath', 'reduction.c'),
             ]
 THIS_DIR = os.path.dirname(__file__)
 API_FILES = [os.path.join(THIS_DIR, '..', 'src', a) for a in API_FILES]
 
+def file_in_this_dir(filename):
+    return os.path.join(THIS_DIR, filename)
+
 def remove_whitespace(s):
     return ''.join(s.split())
 
-class Function(object):
+def _repl(str):
+    return str.replace('Bool', 'npy_bool')
+
+
+class StealRef:
+    def __init__(self, arg):
+        self.arg = arg # counting from 1
+
+    def __str__(self):
+        try:
+            return ' '.join('NPY_STEALS_REF_TO_ARG(%d)' % x for x in self.arg)
+        except TypeError:
+            return 'NPY_STEALS_REF_TO_ARG(%d)' % self.arg
+
+
+class Function:
     def __init__(self, name, return_type, args, doc=''):
         self.name = name
-        self.return_type = return_type
+        self.return_type = _repl(return_type)
         self.args = args
         self.doc = doc
 
-    def _format_arg(self, (typename, name)):
+    def _format_arg(self, typename, name):
         if typename.endswith('*'):
             return typename + name
         else:
             return typename + ' ' + name
 
-    def argtypes_string(self):
-        if not self.args:
-            return 'void'
-        argstr = ', '.join([a[0] for a in self.args])
-        return argstr
-
     def __str__(self):
-        argstr = ', '.join([self._format_arg(a) for a in self.args])
+        argstr = ', '.join([self._format_arg(*a) for a in self.args])
         if self.doc:
             doccomment = '/* %s */\n' % self.doc
         else:
             doccomment = ''
         return '%s%s %s(%s)' % (doccomment, self.return_type, self.name, argstr)
 
+    def to_ReST(self):
+        lines = ['::', '', '  ' + self.return_type]
+        argstr = ',\000'.join([self._format_arg(*a) for a in self.args])
+        name = '  %s' % (self.name,)
+        s = textwrap.wrap('(%s)' % (argstr,), width=72,
+                          initial_indent=name,
+                          subsequent_indent=' ' * (len(name)+1),
+                          break_long_words=False)
+        for l in s:
+            lines.append(l.replace('\000', ' ').rstrip())
+        lines.append('')
+        if self.doc:
+            lines.append(textwrap.dedent(self.doc))
+        return '\n'.join(lines)
+
     def api_hash(self):
-        m = md5.new()
+        m = hashlib.md5()
         m.update(remove_whitespace(self.return_type))
         m.update('\000')
         m.update(self.name)
@@ -74,14 +161,12 @@
 
 def split_arguments(argstr):
     arguments = []
-    bracket_counts = {'(': 0, '[': 0}
     current_argument = []
-    state = 0
     i = 0
     def finish_arg():
         if current_argument:
             argstr = ''.join(current_argument).strip()
-            m = re.match(r'(.*(\s+|[*]))(\w+)$', argstr)
+            m = re.match(r'(.*(\s+|\*))(\w+)$', argstr)
             if m:
                 typename = m.group(1).strip()
                 name = m.group(3)
@@ -106,13 +191,37 @@
 
 
 def find_functions(filename, tag='API'):
-    fo = open(filename, 'r')
+    """
+    Scan the file, looking for tagged functions.
+
+    Assuming ``tag=='API'``, a tagged function looks like::
+
+        /*API*/
+        static returntype*
+        function_name(argtype1 arg1, argtype2 arg2)
+        {
+        }
+
+    where the return type must be on a separate line, the function
+    name must start the line, and the opening ``{`` must start the line.
+
+    An optional documentation comment in ReST format may follow the tag,
+    as in::
+
+        /*API
+          This function does foo...
+         */
+    """
+    if filename.endswith(('.c.src', '.h.src')):
+        fo = io.StringIO(process_c_file(filename))
+    else:
+        fo = open(filename, 'r')
     functions = []
     return_type = None
     function_name = None
     function_args = []
     doclist = []
-    SCANNING, STATE_DOC, STATE_RETTYPE, STATE_NAME, STATE_ARGS = range(5)
+    SCANNING, STATE_DOC, STATE_RETTYPE, STATE_NAME, STATE_ARGS = list(range(5))
     state = SCANNING
     tagcomment = '/*' + tag
     for lineno, line in enumerate(fo):
@@ -130,95 +239,284 @@
                 else:
                     line = line.lstrip(' *')
                     doclist.append(line)
-            elif state == STATE_RETTYPE: #first line of declaration with return type
-                m = re.match(r'static\s+(.*)$', line)
+            elif state == STATE_RETTYPE:
+                # first line of declaration with return type
+                m = re.match(r'NPY_NO_EXPORT\s+(.*)$', line)
                 if m:
                     line = m.group(1)
                 return_type = line
                 state = STATE_NAME
-            elif state == STATE_NAME: # second line, with function name
+            elif state == STATE_NAME:
+                # second line, with function name
                 m = re.match(r'(\w+)\s*\(', line)
                 if m:
                     function_name = m.group(1)
                 else:
-                    raise ParseError(filename, lineno+1, 'could not find function name')
+                    raise ParseError(filename, lineno+1,
+                                     'could not find function name')
                 function_args.append(line[m.end():])
                 state = STATE_ARGS
             elif state == STATE_ARGS:
-                if line.startswith('{'): # finished
-                    fargs_str = ' '.join(function_args).rstrip(' )')
+                if line.startswith('{'):
+                    # finished
+                    # remove any white space and the closing bracket:
+                    fargs_str = ' '.join(function_args).rstrip()[:-1].rstrip()
                     fargs = split_arguments(fargs_str)
                     f = Function(function_name, return_type, fargs,
-                                 ' '.join(doclist))
+                                 '\n'.join(doclist))
                     functions.append(f)
                     return_type = None
                     function_name = None
                     function_args = []
                     doclist = []
-                    state = 0
+                    state = SCANNING
                 else:
                     function_args.append(line)
-        except:
-            print filename, lineno+1
+        except ParseError:
             raise
+        except Exception as e:
+            msg = "see chained exception for details"
+            raise ParseError(filename, lineno + 1, msg) from e
     fo.close()
     return functions
 
-def read_order(order_file):
-    fo = open(order_file, 'r')
-    order = {}
-    i = 0
-    for line in fo:
-        line = line.strip()
-        if not line.startswith('#'):
-            order[line] = i
-            i += 1
-    fo.close()
-    return order
-
-def get_api_functions(tagname, order_file):
-    if not os.path.exists(order_file):
-        order_file = os.path.join(THIS_DIR, order_file)
-    order = read_order(order_file)
+def should_rebuild(targets, source_files):
+    from distutils.dep_util import newer_group
+    for t in targets:
+        if not os.path.exists(t):
+            return True
+    sources = API_FILES + list(source_files) + [__file__]
+    if newer_group(sources, targets[0], missing='newer'):
+        return True
+    return False
+
+def write_file(filename, data):
+    """
+    Write data to filename
+    Only write changed data to avoid updating timestamps unnecessarily
+    """
+    if os.path.exists(filename):
+        with open(filename) as f:
+            if data == f.read():
+                return
+
+    with open(filename, 'w') as fid:
+        fid.write(data)
+
+
+# Those *Api classes instances know how to output strings for the generated code
+class TypeApi:
+    def __init__(self, name, index, ptr_cast, api_name, internal_type=None):
+        self.index = index
+        self.name = name
+        self.ptr_cast = ptr_cast
+        self.api_name = api_name
+        # The type used internally, if None, same as exported (ptr_cast)
+        self.internal_type = internal_type
+
+    def define_from_array_api_string(self):
+        return "#define %s (*(%s *)%s[%d])" % (self.name,
+                                               self.ptr_cast,
+                                               self.api_name,
+                                               self.index)
+
+    def array_api_define(self):
+        return "        (void *) &%s" % self.name
+
+    def internal_define(self):
+        if self.internal_type is None:
+            return f"extern NPY_NO_EXPORT {self.ptr_cast} {self.name};\n"
+
+        # If we are here, we need to define a larger struct internally, which
+        # the type can be cast safely. But we want to normally use the original
+        # type, so name mangle:
+        mangled_name = f"{self.name}Full"
+        astr = (
+            # Create the mangled name:
+            f"extern NPY_NO_EXPORT {self.internal_type} {mangled_name};\n"
+            # And define the name as: (*(type *)(&mangled_name))
+            f"#define {self.name} (*({self.ptr_cast} *)(&{mangled_name}))\n"
+        )
+        return astr
+
+class GlobalVarApi:
+    def __init__(self, name, index, type, api_name):
+        self.name = name
+        self.index = index
+        self.type = type
+        self.api_name = api_name
+
+    def define_from_array_api_string(self):
+        return "#define %s (*(%s *)%s[%d])" % (self.name,
+                                                        self.type,
+                                                        self.api_name,
+                                                        self.index)
+
+    def array_api_define(self):
+        return "        (%s *) &%s" % (self.type, self.name)
+
+    def internal_define(self):
+        astr = """\
+extern NPY_NO_EXPORT %(type)s %(name)s;
+""" % {'type': self.type, 'name': self.name}
+        return astr
+
+# Dummy to be able to consistently use *Api instances for all items in the
+# array api
+class BoolValuesApi:
+    def __init__(self, name, index, api_name):
+        self.name = name
+        self.index = index
+        self.type = 'PyBoolScalarObject'
+        self.api_name = api_name
+
+    def define_from_array_api_string(self):
+        return "#define %s ((%s *)%s[%d])" % (self.name,
+                                              self.type,
+                                              self.api_name,
+                                              self.index)
+
+    def array_api_define(self):
+        return "        (void *) &%s" % self.name
+
+    def internal_define(self):
+        astr = """\
+extern NPY_NO_EXPORT PyBoolScalarObject _PyArrayScalar_BoolValues[2];
+"""
+        return astr
+
+class FunctionApi:
+    def __init__(self, name, index, annotations, return_type, args, api_name):
+        self.name = name
+        self.index = index
+        self.annotations = annotations
+        self.return_type = return_type
+        self.args = args
+        self.api_name = api_name
+
+    def _argtypes_string(self):
+        if not self.args:
+            return 'void'
+        argstr = ', '.join([_repl(a[0]) for a in self.args])
+        return argstr
+
+    def define_from_array_api_string(self):
+        define = """\
+#define %s \\\n        (*(%s (*)(%s)) \\
+         %s[%d])""" % (self.name,
+                                self.return_type,
+                                self._argtypes_string(),
+                                self.api_name,
+                                self.index)
+        return define
+
+    def array_api_define(self):
+        return "        (void *) %s" % self.name
+
+    def internal_define(self):
+        annstr = [str(a) for a in self.annotations]
+        annstr = ' '.join(annstr)
+        astr = """\
+NPY_NO_EXPORT %s %s %s \\\n       (%s);""" % (annstr, self.return_type,
+                                              self.name,
+                                              self._argtypes_string())
+        return astr
+
+def order_dict(d):
+    """Order dict by its values."""
+    o = list(d.items())
+    def _key(x):
+        return x[1] + (x[0],)
+    return sorted(o, key=_key)
+
+def merge_api_dicts(dicts):
+    ret = {}
+    for d in dicts:
+        for k, v in d.items():
+            ret[k] = v
+
+    return ret
+
+def check_api_dict(d):
+    """Check that an api dict is valid (does not use the same index twice)."""
+    # remove the extra value fields that aren't the index
+    index_d = {k: v[0] for k, v in d.items()}
+
+    # We have if a same index is used twice: we 'revert' the dict so that index
+    # become keys. If the length is different, it means one index has been used
+    # at least twice
+    revert_dict = {v: k for k, v in index_d.items()}
+    if not len(revert_dict) == len(index_d):
+        # We compute a dict index -> list of associated items
+        doubled = {}
+        for name, index in index_d.items():
+            try:
+                doubled[index].append(name)
+            except KeyError:
+                doubled[index] = [name]
+        fmt = "Same index has been used twice in api definition: {}"
+        val = ''.join(
+            '\n\tindex {} -> {}'.format(index, names)
+            for index, names in doubled.items() if len(names) != 1
+        )
+        raise ValueError(fmt.format(val))
+
+    # No 'hole' in the indexes may be allowed, and it must starts at 0
+    indexes = set(index_d.values())
+    expected = set(range(len(indexes)))
+    if indexes != expected:
+        diff = expected.symmetric_difference(indexes)
+        msg = "There are some holes in the API indexing: " \
+              "(symmetric diff is %s)" % diff
+        raise ValueError(msg)
+
+def get_api_functions(tagname, api_dict):
+    """Parse source files to get functions tagged by the given tag."""
     functions = []
     for f in API_FILES:
         functions.extend(find_functions(f, tagname))
-    dfunctions = []
-    for func in functions:
-        o = order[func.name]
-        dfunctions.append( (o, func) )
+    dfunctions = [(api_dict[func.name][0], func) for func in functions]
     dfunctions.sort()
     return [a[1] for a in dfunctions]
 
-def add_api_list(offset, APIname, api_list,
-                 module_list, extension_list, init_list):
-    """Add the API function declerations to the appropiate lists for use in
-    the headers.
-    """
-    for k, func in enumerate(api_list):
-        num = offset + k
-        astr = "static %s %s \\\n       (%s);" % \
-               (func.return_type, func.name, func.argtypes_string())
-        module_list.append(astr)
-        astr = "#define %s \\\n        (*(%s (*)(%s)) \\\n"\
-               "         %s[%d])" % (func.name,func.return_type,
-                                     func.argtypes_string(), APIname, num)
-        extension_list.append(astr)
-        astr = "        (void *) %s," % func.name
-        init_list.append(astr)
-
+def fullapi_hash(api_dicts):
+    """Given a list of api dicts defining the numpy C API, compute a checksum
+    of the list of items in the API (as a string)."""
+    a = []
+    for d in api_dicts:
+        for name, data in order_dict(d):
+            a.extend(name)
+            a.extend(','.join(map(str, data)))
+
+    return hashlib.md5(''.join(a).encode('ascii')).hexdigest()
+
+# To parse strings like 'hex = checksum' where hex is e.g. 0x1234567F and
+# checksum a 128 bits md5 checksum (hex format as well)
+VERRE = re.compile(r'(^0x[\da-f]{8})\s*=\s*([\da-f]{32})')
+
+def get_versions_hash():
+    d = []
+
+    file = os.path.join(os.path.dirname(__file__), 'cversions.txt')
+    with open(file, 'r') as fid:
+        for line in fid:
+            m = VERRE.match(line)
+            if m:
+                d.append((int(m.group(1), 16), m.group(2)))
+
+    return dict(d)
 
 def main():
     tagname = sys.argv[1]
     order_file = sys.argv[2]
     functions = get_api_functions(tagname, order_file)
-    m = md5.new(tagname)
+    m = hashlib.md5(tagname)
     for func in functions:
-        print func
+        print(func)
         ah = func.api_hash()
         m.update(ah)
-        print hex(int(ah,16))
-    print hex(int(m.hexdigest()[:8],16))
+        print(hex(int(ah, 16)))
+    print(hex(int(m.hexdigest()[:8], 16)))
 
 if __name__ == '__main__':
     main()
('numpy/core/code_generators', 'generate_umath.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,20 +1,84 @@
-import string
+import os
 import re
-
-Zero = "PyUFunc_Zero"
-One = "PyUFunc_One"
-None_ = "PyUFunc_None"
-
-class TypeDescription(object):
-    def __init__(self, type, f=None, in_=None, out=None):
+import struct
+import sys
+import textwrap
+
+Zero = "PyLong_FromLong(0)"
+One = "PyLong_FromLong(1)"
+True_ = "(Py_INCREF(Py_True), Py_True)"
+False_ = "(Py_INCREF(Py_False), Py_False)"
+None_ = object()
+AllOnes = "PyLong_FromLong(-1)"
+MinusInfinity = 'PyFloat_FromDouble(-NPY_INFINITY)'
+ReorderableNone = "(Py_INCREF(Py_None), Py_None)"
+
+class docstrings:
+    @staticmethod
+    def get(place):
+        """
+        Returns the C #definition name of docstring according
+        to ufunc place. C #definitions are generated by generate_umath_doc.py
+        in a separate C header.
+        """
+        return 'DOC_' + place.upper().replace('.', '_')
+
+# Sentinel value to specify using the full type description in the
+# function name
+class FullTypeDescr:
+    pass
+
+class FuncNameSuffix:
+    """Stores the suffix to append when generating functions names.
+    """
+    def __init__(self, suffix):
+        self.suffix = suffix
+
+class TypeDescription:
+    """Type signature for a ufunc.
+
+    Attributes
+    ----------
+    type : str
+        Character representing the nominal type.
+    func_data : str or None or FullTypeDescr or FuncNameSuffix, optional
+        The string representing the expression to insert into the data
+        array, if any.
+    in_ : str or None, optional
+        The typecode(s) of the inputs.
+    out : str or None, optional
+        The typecode(s) of the outputs.
+    astype : dict or None, optional
+        If astype['x'] is 'y', uses PyUFunc_x_x_As_y_y/PyUFunc_xx_x_As_yy_y
+        instead of PyUFunc_x_x/PyUFunc_xx_x.
+    cfunc_alias : str or none, optional
+        Appended to inner loop C function name, e.g., FLOAT_{cfunc_alias}. See make_arrays.
+        NOTE: it doesn't support 'astype'
+    simd : list
+        Available SIMD ufunc loops, dispatched at runtime in specified order
+        Currently only supported for simples types (see make_arrays)
+    dispatch : str or None, optional
+        Dispatch-able source name without its extension '.dispatch.c' that
+        contains the definition of ufunc, dispatched at runtime depending on the
+        specified targets of the dispatch-able source.
+        NOTE: it doesn't support 'astype'
+    """
+    def __init__(self, type, f=None, in_=None, out=None, astype=None, cfunc_alias=None,
+                 simd=None, dispatch=None):
         self.type = type
         self.func_data = f
+        if astype is None:
+            astype = {}
+        self.astype_dict = astype
         if in_ is not None:
-            in_ = in_.replace('.', type)
+            in_ = in_.replace('P', type)
         self.in_ = in_
         if out is not None:
-            out = out.replace('.', type)
+            out = out.replace('P', type)
         self.out = out
+        self.cfunc_alias = cfunc_alias
+        self.simd = simd
+        self.dispatch = dispatch
 
     def finish_signature(self, nin, nout):
         if self.in_ is None:
@@ -23,22 +87,30 @@
         if self.out is None:
             self.out = self.type * nout
         assert len(self.out) == nout
-
-_fdata_map = dict(f='%sf', d='%s', g='%sl',
-                  F='nc_%sf', D='nc_%s', G='nc_%sl')
+        self.astype = self.astype_dict.get(self.type, None)
+
+_fdata_map = dict(
+    e='npy_%sf',
+    f='npy_%sf',
+    d='npy_%s',
+    g='npy_%sl',
+    F='nc_%sf',
+    D='nc_%s',
+    G='nc_%sl'
+)
+
 def build_func_data(types, f):
-    func_data = []
-    for t in types:
-        d = _fdata_map.get(t, '%s') % (f,)
-        func_data.append(d)
+    func_data = [_fdata_map.get(t, '%s') % (f,) for t in types]
     return func_data
 
-def TD(types, f=None, in_=None, out=None):
+def TD(types, f=None, astype=None, in_=None, out=None, cfunc_alias=None,
+       simd=None, dispatch=None):
     if f is not None:
         if isinstance(f, str):
             func_data = build_func_data(types, f)
+        elif len(f) != len(types):
+            raise ValueError("Number of types and f do not match")
         else:
-            assert len(f) == len(types)
             func_data = f
     else:
         func_data = (None,) * len(types)
@@ -46,31 +118,96 @@
         in_ = (in_,) * len(types)
     elif in_ is None:
         in_ = (None,) * len(types)
+    elif len(in_) != len(types):
+        raise ValueError("Number of types and inputs do not match")
     if isinstance(out, str):
         out = (out,) * len(types)
     elif out is None:
         out = (None,) * len(types)
+    elif len(out) != len(types):
+        raise ValueError("Number of types and outputs do not match")
     tds = []
     for t, fd, i, o in zip(types, func_data, in_, out):
-        tds.append(TypeDescription(t, f=fd, in_=i, out=o))
+        # [(simd-name, list of types)]
+        if simd is not None:
+            simdt = [k for k, v in simd if t in v]
+        else:
+            simdt = []
+
+        # [(dispatch file name without extension '.dispatch.c*', list of types)]
+        if dispatch:
+            dispt = ([k for k, v in dispatch if t in v]+[None])[0]
+        else:
+            dispt = None
+        tds.append(TypeDescription(
+            t, f=fd, in_=i, out=o, astype=astype, cfunc_alias=cfunc_alias,
+            simd=simdt, dispatch=dispt
+        ))
     return tds
 
-class Ufunc(object):
-    def __init__(self, nin, nout, identity, docstring,
-                 *type_descriptions):
+class Ufunc:
+    """Description of a ufunc.
+
+    Attributes
+    ----------
+    nin : number of input arguments
+    nout : number of output arguments
+    identity : identity element for a two-argument function
+    docstring : docstring for the ufunc
+    type_descriptions : list of TypeDescription objects
+    """
+    def __init__(self, nin, nout, identity, docstring, typereso,
+                 *type_descriptions, signature=None):
         self.nin = nin
         self.nout = nout
         if identity is None:
             identity = None_
         self.identity = identity
         self.docstring = docstring
+        self.typereso = typereso
         self.type_descriptions = []
+        self.signature = signature
         for td in type_descriptions:
             self.type_descriptions.extend(td)
         for td in self.type_descriptions:
             td.finish_signature(self.nin, self.nout)
 
-#each entry in defdict is
+# String-handling utilities to avoid locale-dependence.
+
+import string
+UPPER_TABLE = bytes.maketrans(bytes(string.ascii_lowercase, "ascii"),
+                              bytes(string.ascii_uppercase, "ascii"))
+
+def english_upper(s):
+    """ Apply English case rules to convert ASCII strings to all upper case.
+
+    This is an internal utility function to replace calls to str.upper() such
+    that we can avoid changing behavior with changing locales. In particular,
+    Turkish has distinct dotted and dotless variants of the Latin letter "I" in
+    both lowercase and uppercase. Thus, "i".upper() != "I" in a "tr" locale.
+
+    Parameters
+    ----------
+    s : str
+
+    Returns
+    -------
+    uppered : str
+
+    Examples
+    --------
+    >>> from numpy.lib.utils import english_upper
+    >>> s = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_'
+    >>> english_upper(s)
+    'ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_'
+    >>> english_upper('')
+    ''
+    """
+    uppered = s.translate(UPPER_TABLE)
+    return uppered
+
+
+#each entry in defdict is a Ufunc object.
 
 #name: [string of chars for which it is defined,
 #       string of characters using func interface,
@@ -81,466 +218,844 @@
 #       output specification (optional)
 #       ]
 
-all = '?bBhHiIlLqQfdgFDGO'
+chartoname = {
+    '?': 'bool',
+    'b': 'byte',
+    'B': 'ubyte',
+    'h': 'short',
+    'H': 'ushort',
+    'i': 'int',
+    'I': 'uint',
+    'l': 'long',
+    'L': 'ulong',
+    'q': 'longlong',
+    'Q': 'ulonglong',
+    'e': 'half',
+    'f': 'float',
+    'd': 'double',
+    'g': 'longdouble',
+    'F': 'cfloat',
+    'D': 'cdouble',
+    'G': 'clongdouble',
+    'M': 'datetime',
+    'm': 'timedelta',
+    'O': 'OBJECT',
+    # '.' is like 'O', but calls a method of the object instead
+    # of a function
+    'P': 'OBJECT',
+}
+
+noobj = '?bBhHiIlLqQefdgFDGmM'
+all = '?bBhHiIlLqQefdgFDGOmM'
+
 O = 'O'
-M = 'M'
+P = 'P'
 ints = 'bBhHiIlLqQ'
+sints = 'bhilq'
+uints = 'BHILQ'
+times = 'Mm'
+timedeltaonly = 'm'
 intsO = ints + O
 bints = '?' + ints
 bintsO = bints + O
-flts = 'fdg'
+flts = 'efdg'
 fltsO = flts + O
-fltsM = flts + M
+fltsP = flts + P
 cmplx = 'FDG'
+cmplxvec = 'FD'
 cmplxO = cmplx + O
-cmplxM = cmplx + M
+cmplxP = cmplx + P
 inexact = flts + cmplx
+inexactvec = 'fd'
 noint = inexact+O
-nointM = inexact+M
-allM = bints+flts+cmplxM
-nobool = all[1:]
-nobool_or_obj = all[1:-1]
+nointP = inexact+P
+allP = bints+times+flts+cmplxP
+nobool_or_obj = noobj[1:]
+nobool_or_datetime = noobj[1:-1] + O # includes m - timedelta64
 intflt = ints+flts
-nocmplx = bints+flts
+intfltcmplx = ints+flts+cmplx
+nocmplx = bints+times+flts
 nocmplxO = nocmplx+O
-nocmplxM = nocmplx+M
-noobj = all[:-1]
-
+nocmplxP = nocmplx+P
+notimes_or_obj = bints + inexact
+nodatetime_or_obj = bints + inexact
+
+# Find which code corresponds to int64.
+int64 = ''
+uint64 = ''
+for code in 'bhilq':
+    if struct.calcsize(code) == 8:
+        int64 = code
+        uint64 = english_upper(code)
+        break
+
+# This dictionary describes all the ufunc implementations, generating
+# all the function names and their corresponding ufunc signatures.  TD is
+# an object which expands a list of character codes into an array of
+# TypeDescriptions.
 defdict = {
-'add' :
+'add':
     Ufunc(2, 1, Zero,
-          'adds the arguments elementwise.',
+          docstrings.get('numpy.core.umath.add'),
+          'PyUFunc_AdditionTypeResolver',
+          TD(notimes_or_obj, simd=[('avx2', ints)], dispatch=[('loops_arithm_fp', 'fdFD')]),
+          [TypeDescription('M', FullTypeDescr, 'Mm', 'M'),
+           TypeDescription('m', FullTypeDescr, 'mm', 'm'),
+           TypeDescription('M', FullTypeDescr, 'mM', 'M'),
+          ],
+          TD(O, f='PyNumber_Add'),
+          ),
+'subtract':
+    Ufunc(2, 1, None, # Zero is only a unit to the right, not the left
+          docstrings.get('numpy.core.umath.subtract'),
+          'PyUFunc_SubtractionTypeResolver',
+          TD(ints + inexact, simd=[('avx2', ints)], dispatch=[('loops_arithm_fp', 'fdFD')]),
+          [TypeDescription('M', FullTypeDescr, 'Mm', 'M'),
+           TypeDescription('m', FullTypeDescr, 'mm', 'm'),
+           TypeDescription('M', FullTypeDescr, 'MM', 'm'),
+          ],
+          TD(O, f='PyNumber_Subtract'),
+          ),
+'multiply':
+    Ufunc(2, 1, One,
+          docstrings.get('numpy.core.umath.multiply'),
+          'PyUFunc_MultiplicationTypeResolver',
+          TD(notimes_or_obj, simd=[('avx2', ints)], dispatch=[('loops_arithm_fp', 'fdFD')]),
+          [TypeDescription('m', FullTypeDescr, 'mq', 'm'),
+           TypeDescription('m', FullTypeDescr, 'qm', 'm'),
+           TypeDescription('m', FullTypeDescr, 'md', 'm'),
+           TypeDescription('m', FullTypeDescr, 'dm', 'm'),
+          ],
+          TD(O, f='PyNumber_Multiply'),
+          ),
+#'true_divide' : aliased to divide in umathmodule.c:initumath
+'floor_divide':
+    Ufunc(2, 1, None, # One is only a unit to the right, not the left
+          docstrings.get('numpy.core.umath.floor_divide'),
+          'PyUFunc_DivisionTypeResolver',
+          TD(ints, cfunc_alias='divide',
+              dispatch=[('loops_arithmetic', 'bBhHiIlLqQ')]),
+          TD(flts),
+          [TypeDescription('m', FullTypeDescr, 'mq', 'm'),
+           TypeDescription('m', FullTypeDescr, 'md', 'm'),
+           TypeDescription('m', FullTypeDescr, 'mm', 'q'),
+          ],
+          TD(O, f='PyNumber_FloorDivide'),
+          ),
+'divide':
+    Ufunc(2, 1, None, # One is only a unit to the right, not the left
+          docstrings.get('numpy.core.umath.divide'),
+          'PyUFunc_TrueDivisionTypeResolver',
+          TD(flts+cmplx, cfunc_alias='divide', dispatch=[('loops_arithm_fp', 'fd')]),
+          [TypeDescription('m', FullTypeDescr, 'mq', 'm', cfunc_alias='divide'),
+           TypeDescription('m', FullTypeDescr, 'md', 'm', cfunc_alias='divide'),
+           TypeDescription('m', FullTypeDescr, 'mm', 'd', cfunc_alias='divide'),
+          ],
+          TD(O, f='PyNumber_TrueDivide'),
+          ),
+'conjugate':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.conjugate'),
+          None,
+          TD(ints+flts+cmplx, simd=[('avx2', ints), ('avx512f', cmplxvec)]),
+          TD(P, f='conjugate'),
+          ),
+'fmod':
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.fmod'),
+          None,
+          TD(ints, dispatch=[('loops_modulo', ints)]),
+          TD(flts, f='fmod', astype={'e': 'f'}),
+          TD(P, f='fmod'),
+          ),
+'square':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.square'),
+          None,
+          TD(ints+inexact, simd=[('avx2', ints), ('avx512f', 'FD')], dispatch=[('loops_unary_fp', 'fd')]),
+          TD(O, f='Py_square'),
+          ),
+'reciprocal':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.reciprocal'),
+          None,
+          TD(ints+inexact, simd=[('avx2', ints)], dispatch=[('loops_unary_fp', 'fd')]),
+          TD(O, f='Py_reciprocal'),
+          ),
+# This is no longer used as numpy.ones_like, however it is
+# still used by some internal calls.
+'_ones_like':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath._ones_like'),
+          'PyUFunc_OnesLikeTypeResolver',
           TD(noobj),
-          TD(O, f='PyNumber_Add'),
-          ),
-'subtract' :
-    Ufunc(2, 1, Zero,
-          'subtracts the arguments elementwise.',
-          TD(noobj),
-          TD(O, f='PyNumber_Subtract'),
-          ),
-'multiply' :
-    Ufunc(2, 1, One,
-          'multiplies the arguments elementwise.',
-          TD(nocmplx),
-          TD(cmplx, f='prod'),
-          TD(O, f='PyNumber_Multiply'),
-          ),
-'divide' :
-    Ufunc(2, 1, One,
-          'divides the arguments elementwise.',
-          TD(intflt),
-          TD(cmplx, f='quot'),
-          TD(O, f='PyNumber_Divide'),
-          ),
-'floor_divide' :
-    Ufunc(2, 1, One,
-          'floor divides the arguments elementwise.',
-          TD(intflt),
-          TD(cmplx, f='floor_quot'),
-          TD(O, f='PyNumber_FloorDivide'),
-          ),
-'true_divide' :
-    Ufunc(2, 1, One,
-          'true divides the arguments elementwise.',
-          TD('bBhH', out='f'),
-          TD('iIlLqQ', out='d'),
-          TD(flts),
-          TD(cmplx, f='quot'),
-          TD(O, f='PyNumber_TrueDivide'),
-          ),
-'conjugate' :
-    Ufunc(1, 1, None,
-          'takes the conjugate of x elementwise.',
-          TD(nobool_or_obj),
-          TD(M, f='conjugate'),
-          ),
-'fmod' :
-    Ufunc(2, 1, Zero,
-          'computes (C-like) x1 % x2 elementwise.',
+          TD(O, f='Py_get_one'),
+          ),
+'power':
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.power'),
+          None,
           TD(ints),
-          TD(flts, f='fmod'),
-          TD(M, f='fmod'),
-          ),
-'square' :
-    Ufunc(1, 1, None,
-          'compute x**2.',
-          TD(nobool_or_obj),
-          TD(O, f='Py_square'),
-          ),
-'reciprocal' :
-    Ufunc(1, 1, None,
-          'compute 1/x',
-          TD(nobool_or_obj),
-          TD(O, f='Py_reciprocal'),
-          ),
-'ones_like' :
-    Ufunc(1, 1, None,
-          'return 1',
-          TD(nobool_or_obj),
-          TD(O, f='Py_get_one'),
-          ),
-'power' :
-    Ufunc(2, 1, One,
-          'computes x1**x2 elementwise.',
-          TD(ints),
-          TD(inexact, f='pow'),
-          TD(O, f='PyNumber_Power'),
-          ),
-'absolute' :
-    Ufunc(1, 1, None,
-          'takes |x| elementwise.',
-          TD(nocmplx),
+          TD(inexact, f='pow', astype={'e': 'f'}),
+          TD(O, f='npy_ObjectPower'),
+          ),
+'float_power':
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.float_power'),
+          None,
+          TD('dgDG', f='pow'),
+          ),
+'absolute':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.absolute'),
+          'PyUFunc_AbsoluteTypeResolver',
+          TD(bints+flts+timedeltaonly, dispatch=[('loops_unary_fp', 'fd')]),
+          TD(cmplx, simd=[('avx512f', cmplxvec)], out=('f', 'd', 'g')),
+          TD(O, f='PyNumber_Absolute'),
+          ),
+'_arg':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath._arg'),
+          None,
           TD(cmplx, out=('f', 'd', 'g')),
-          TD(O, f='PyNumber_Absolute'),
-          ),
-'negative' :
-    Ufunc(1, 1, None,
-          'determines -x elementwise',
-          TD(nocmplx),
+          ),
+'negative':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.negative'),
+          'PyUFunc_NegativeTypeResolver',
+          TD(ints+flts+timedeltaonly, simd=[('avx2', ints)]),
           TD(cmplx, f='neg'),
           TD(O, f='PyNumber_Negative'),
           ),
-'sign' :
-    Ufunc(1, 1, None,
-          'returns -1 if x < 0 and 0 if x==0 and 1 if x > 0',
-          TD(nobool),
-          ),
-'greater' :
-    Ufunc(2, 1, None,
-          'returns elementwise x1 > x2 in a bool array.',
-          TD(all, out='?'),
-          ),
-'greater_equal' :
-    Ufunc(2, 1, None,
-          'returns elementwise x1 >= x2 in a bool array.',
-          TD(all, out='?'),
-          ),
-'less' :
-    Ufunc(2, 1, None,
-          'returns elementwise x1 < x2 in a bool array.',
-          TD(all, out='?'),
-          ),
-'less_equal' :
-    Ufunc(2, 1, None,
-          'returns elementwise x1 <= x2 in a bool array',
-          TD(all, out='?'),
-          ),
-'equal' :
-    Ufunc(2, 1, None,
-          'returns elementwise x1 == x2 in a bool array',
-          TD(all, out='?'),
-          ),
-'not_equal' :
-    Ufunc(2, 1, None,
-          'returns elementwise x1 |= x2',
-          TD(all, out='?'),
-          ),
-'logical_and' :
-    Ufunc(2, 1, One,
-          'returns x1 and x2 elementwise.',
-          TD(noobj, out='?'),
-          TD(M, f='logical_and', out='?'),
-          ),
-'logical_not' :
-    Ufunc(1, 1, None,
-          'returns not x elementwise.',
-          TD(noobj, out='?'),
-          TD(M, f='logical_not', out='?'),
-          ),
-'logical_or' :
+'positive':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.positive'),
+          'PyUFunc_SimpleUniformOperationTypeResolver',
+          TD(ints+flts+timedeltaonly),
+          TD(cmplx, f='pos'),
+          TD(O, f='PyNumber_Positive'),
+          ),
+'sign':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.sign'),
+          'PyUFunc_SimpleUniformOperationTypeResolver',
+          TD(nobool_or_datetime),
+          ),
+'greater':
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.greater'),
+          'PyUFunc_SimpleBinaryComparisonTypeResolver',
+          TD(all, out='?', simd=[('avx2', ints)]),
+          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],
+          TD('O', out='?'),
+          ),
+'greater_equal':
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.greater_equal'),
+          'PyUFunc_SimpleBinaryComparisonTypeResolver',
+          TD(all, out='?', simd=[('avx2', ints)]),
+          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],
+          TD('O', out='?'),
+          ),
+'less':
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.less'),
+          'PyUFunc_SimpleBinaryComparisonTypeResolver',
+          TD(all, out='?', simd=[('avx2', ints)]),
+          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],
+          TD('O', out='?'),
+          ),
+'less_equal':
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.less_equal'),
+          'PyUFunc_SimpleBinaryComparisonTypeResolver',
+          TD(all, out='?', simd=[('avx2', ints)]),
+          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],
+          TD('O', out='?'),
+          ),
+'equal':
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.equal'),
+          'PyUFunc_SimpleBinaryComparisonTypeResolver',
+          TD(all, out='?', simd=[('avx2', ints)]),
+          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],
+          TD('O', out='?'),
+          ),
+'not_equal':
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.not_equal'),
+          'PyUFunc_SimpleBinaryComparisonTypeResolver',
+          TD(all, out='?', simd=[('avx2', ints)]),
+          [TypeDescription('O', FullTypeDescr, 'OO', 'O')],
+          TD('O', out='?'),
+          ),
+'logical_and':
+    Ufunc(2, 1, True_,
+          docstrings.get('numpy.core.umath.logical_and'),
+          'PyUFunc_SimpleBinaryComparisonTypeResolver',
+          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)]),
+          TD(O, f='npy_ObjectLogicalAnd'),
+          ),
+'logical_not':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.logical_not'),
+          None,
+          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)]),
+          TD(O, f='npy_ObjectLogicalNot'),
+          ),
+'logical_or':
+    Ufunc(2, 1, False_,
+          docstrings.get('numpy.core.umath.logical_or'),
+          'PyUFunc_SimpleBinaryComparisonTypeResolver',
+          TD(nodatetime_or_obj, out='?', simd=[('avx2', ints)]),
+          TD(O, f='npy_ObjectLogicalOr'),
+          ),
+'logical_xor':
+    Ufunc(2, 1, False_,
+          docstrings.get('numpy.core.umath.logical_xor'),
+          'PyUFunc_SimpleBinaryComparisonTypeResolver',
+          TD(nodatetime_or_obj, out='?'),
+          # TODO: using obj.logical_xor() seems pretty much useless:
+          TD(P, f='logical_xor'),
+          ),
+'maximum':
+    Ufunc(2, 1, ReorderableNone,
+          docstrings.get('numpy.core.umath.maximum'),
+          'PyUFunc_SimpleUniformOperationTypeResolver',
+          TD(noobj, dispatch=[('loops_minmax', ints+'fdg')]),
+          TD(O, f='npy_ObjectMax')
+          ),
+'minimum':
+    Ufunc(2, 1, ReorderableNone,
+          docstrings.get('numpy.core.umath.minimum'),
+          'PyUFunc_SimpleUniformOperationTypeResolver',
+          TD(noobj, dispatch=[('loops_minmax', ints+'fdg')]),
+          TD(O, f='npy_ObjectMin')
+          ),
+'clip':
+    Ufunc(3, 1, ReorderableNone,
+          docstrings.get('numpy.core.umath.clip'),
+          'PyUFunc_SimpleUniformOperationTypeResolver',
+          TD(noobj),
+          [TypeDescription('O', 'npy_ObjectClip', 'OOO', 'O')]
+          ),
+'fmax':
+    Ufunc(2, 1, ReorderableNone,
+          docstrings.get('numpy.core.umath.fmax'),
+          'PyUFunc_SimpleUniformOperationTypeResolver',
+          TD('fdg', dispatch=[('loops_minmax', 'fdg')]),
+          TD(noobj),
+          TD(O, f='npy_ObjectMax')
+          ),
+'fmin':
+    Ufunc(2, 1, ReorderableNone,
+          docstrings.get('numpy.core.umath.fmin'),
+          'PyUFunc_SimpleUniformOperationTypeResolver',
+          TD('fdg', dispatch=[('loops_minmax', 'fdg')]),
+          TD(noobj),
+          TD(O, f='npy_ObjectMin')
+          ),
+'logaddexp':
+    Ufunc(2, 1, MinusInfinity,
+          docstrings.get('numpy.core.umath.logaddexp'),
+          None,
+          TD(flts, f="logaddexp", astype={'e': 'f'})
+          ),
+'logaddexp2':
+    Ufunc(2, 1, MinusInfinity,
+          docstrings.get('numpy.core.umath.logaddexp2'),
+          None,
+          TD(flts, f="logaddexp2", astype={'e': 'f'})
+          ),
+'bitwise_and':
+    Ufunc(2, 1, AllOnes,
+          docstrings.get('numpy.core.umath.bitwise_and'),
+          None,
+          TD(bints, simd=[('avx2', ints)]),
+          TD(O, f='PyNumber_And'),
+          ),
+'bitwise_or':
     Ufunc(2, 1, Zero,
-          'returns x1 or x2 elementwise.',
-          TD(noobj, out='?'),
-          TD(M, f='logical_or', out='?'),
-          ),
-'logical_xor' :
-    Ufunc(2, 1, None,
-          'returns x1 xor x2 elementwise.',
-          TD(noobj, out='?'),
-          TD(M, f='logical_xor', out='?'),
-          ),
-'maximum' :
-    Ufunc(2, 1, None,
-          'returns maximum (if x1 > x2: x1;  else: x2) elementwise.',
-          TD(noobj),
-          ),
-'minimum' :
-    Ufunc(2, 1, None,
-          'returns minimum (if x1 < x2: x1;  else: x2) elementwise',
-          TD(noobj),
-          ),
-'bitwise_and' :
-    Ufunc(2, 1, One,
-          'computes x1 & x2 elementwise.',
-          TD(bints),
-          TD(O, f='PyNumber_And'),
-          ),
-'bitwise_or' :
+          docstrings.get('numpy.core.umath.bitwise_or'),
+          None,
+          TD(bints, simd=[('avx2', ints)]),
+          TD(O, f='PyNumber_Or'),
+          ),
+'bitwise_xor':
     Ufunc(2, 1, Zero,
-          'computes x1 | x2 elementwise.',
-          TD(bints),
-          TD(O, f='PyNumber_Or'),
-          ),
-'bitwise_xor' :
-    Ufunc(2, 1, None,
-          'computes x1 ^ x2 elementwise.',
-          TD(bints),
+          docstrings.get('numpy.core.umath.bitwise_xor'),
+          None,
+          TD(bints, simd=[('avx2', ints)]),
           TD(O, f='PyNumber_Xor'),
           ),
-'invert' :
-    Ufunc(1, 1, None,
-          'computes ~x (bit inversion) elementwise.',
-          TD(bints),
+'invert':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.invert'),
+          None,
+          TD(bints, simd=[('avx2', ints)]),
           TD(O, f='PyNumber_Invert'),
           ),
-'left_shift' :
-    Ufunc(2, 1, None,
-          'computes x1 << x2 (x1 shifted to left by x2 bits) elementwise.',
+'left_shift':
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.left_shift'),
+          None,
+          TD(ints, simd=[('avx2', ints)]),
+          TD(O, f='PyNumber_Lshift'),
+          ),
+'right_shift':
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.right_shift'),
+          None,
+          TD(ints, simd=[('avx2', ints)]),
+          TD(O, f='PyNumber_Rshift'),
+          ),
+'heaviside':
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.heaviside'),
+          None,
+          TD(flts, f='heaviside', astype={'e': 'f'}),
+          ),
+'degrees':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.degrees'),
+          None,
+          TD(fltsP, f='degrees', astype={'e': 'f'}),
+          ),
+'rad2deg':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.rad2deg'),
+          None,
+          TD(fltsP, f='rad2deg', astype={'e': 'f'}),
+          ),
+'radians':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.radians'),
+          None,
+          TD(fltsP, f='radians', astype={'e': 'f'}),
+          ),
+'deg2rad':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.deg2rad'),
+          None,
+          TD(fltsP, f='deg2rad', astype={'e': 'f'}),
+          ),
+'arccos':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.arccos'),
+          None,
+          TD('e', f='acos', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),
+          TD(inexact, f='acos', astype={'e': 'f'}),
+          TD(P, f='arccos'),
+          ),
+'arccosh':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.arccosh'),
+          None,
+          TD('e', f='acosh', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),
+          TD(inexact, f='acosh', astype={'e': 'f'}),
+          TD(P, f='arccosh'),
+          ),
+'arcsin':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.arcsin'),
+          None,
+          TD('e', f='asin', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),
+          TD(inexact, f='asin', astype={'e': 'f'}),
+          TD(P, f='arcsin'),
+          ),
+'arcsinh':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.arcsinh'),
+          None,
+          TD('e', f='asinh', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),
+          TD(inexact, f='asinh', astype={'e': 'f'}),
+          TD(P, f='arcsinh'),
+          ),
+'arctan':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.arctan'),
+          None,
+          TD('e', f='atan', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),
+          TD(inexact, f='atan', astype={'e': 'f'}),
+          TD(P, f='arctan'),
+          ),
+'arctanh':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.arctanh'),
+          None,
+          TD('e', f='atanh', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),
+          TD(inexact, f='atanh', astype={'e': 'f'}),
+          TD(P, f='arctanh'),
+          ),
+'cos':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.cos'),
+          None,
+          TD('e', f='cos', astype={'e': 'f'}),
+          TD('f', dispatch=[('loops_trigonometric', 'f')]),
+          TD('d', dispatch=[('loops_umath_fp', 'd')]),
+          TD('fdg' + cmplx, f='cos'),
+          TD(P, f='cos'),
+          ),
+'sin':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.sin'),
+          None,
+          TD('e', f='sin', astype={'e': 'f'}),
+          TD('f', dispatch=[('loops_trigonometric', 'f')]),
+          TD('d', dispatch=[('loops_umath_fp', 'd')]),
+          TD('fdg' + cmplx, f='sin'),
+          TD(P, f='sin'),
+          ),
+'tan':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.tan'),
+          None,
+          TD('e', f='tan', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),
+          TD(inexact, f='tan', astype={'e': 'f'}),
+          TD(P, f='tan'),
+          ),
+'cosh':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.cosh'),
+          None,
+          TD('e', f='cosh', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),
+          TD(inexact, f='cosh', astype={'e': 'f'}),
+          TD(P, f='cosh'),
+          ),
+'sinh':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.sinh'),
+          None,
+          TD('e', f='sinh', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),
+          TD(inexact, f='sinh', astype={'e': 'f'}),
+          TD(P, f='sinh'),
+          ),
+'tanh':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.tanh'),
+          None,
+          TD('e', f='tanh', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_hyperbolic', 'fd')]),
+          TD(inexact, f='tanh', astype={'e': 'f'}),
+          TD(P, f='tanh'),
+          ),
+'exp':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.exp'),
+          None,
+          TD('e', f='exp', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_exponent_log', 'fd')]),
+          TD('fdg' + cmplx, f='exp'),
+          TD(P, f='exp'),
+          ),
+'exp2':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.exp2'),
+          None,
+          TD('e', f='exp2', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),
+          TD(inexact, f='exp2', astype={'e': 'f'}),
+          TD(P, f='exp2'),
+          ),
+'expm1':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.expm1'),
+          None,
+          TD('e', f='expm1', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),
+          TD(inexact, f='expm1', astype={'e': 'f'}),
+          TD(P, f='expm1'),
+          ),
+'log':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.log'),
+          None,
+          TD('e', f='log', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_exponent_log', 'fd')]),
+          TD('fdg' + cmplx, f='log'),
+          TD(P, f='log'),
+          ),
+'log2':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.log2'),
+          None,
+          TD('e', f='log2', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),
+          TD(inexact, f='log2', astype={'e': 'f'}),
+          TD(P, f='log2'),
+          ),
+'log10':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.log10'),
+          None,
+          TD('e', f='log10', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),
+          TD(inexact, f='log10', astype={'e': 'f'}),
+          TD(P, f='log10'),
+          ),
+'log1p':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.log1p'),
+          None,
+          TD('e', f='log1p', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),
+          TD(inexact, f='log1p', astype={'e': 'f'}),
+          TD(P, f='log1p'),
+          ),
+'sqrt':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.sqrt'),
+          None,
+          TD('e', f='sqrt', astype={'e': 'f'}),
+          TD(inexactvec, dispatch=[('loops_unary_fp', 'fd')]),
+          TD('fdg' + cmplx, f='sqrt'),
+          TD(P, f='sqrt'),
+          ),
+'cbrt':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.cbrt'),
+          None,
+          TD('e', f='cbrt', astype={'e': 'f'}),
+          TD('fd', dispatch=[('loops_umath_fp', 'fd')]),
+          TD(flts, f='cbrt', astype={'e': 'f'}),
+          TD(P, f='cbrt'),
+          ),
+'ceil':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.ceil'),
+          None,
+          TD('e', f='ceil', astype={'e': 'f'}),
+          TD(inexactvec, dispatch=[('loops_unary_fp', 'fd')]),
+          TD('fdg', f='ceil'),
+          TD(O, f='npy_ObjectCeil'),
+          ),
+'trunc':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.trunc'),
+          None,
+          TD('e', f='trunc', astype={'e': 'f'}),
+          TD(inexactvec, dispatch=[('loops_unary_fp', 'fd')]),
+          TD('fdg', f='trunc'),
+          TD(O, f='npy_ObjectTrunc'),
+          ),
+'fabs':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.fabs'),
+          None,
+          TD(flts, f='fabs', astype={'e': 'f'}),
+          TD(P, f='fabs'),
+       ),
+'floor':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.floor'),
+          None,
+          TD('e', f='floor', astype={'e': 'f'}),
+          TD(inexactvec, dispatch=[('loops_unary_fp', 'fd')]),
+          TD('fdg', f='floor'),
+          TD(O, f='npy_ObjectFloor'),
+          ),
+'rint':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.rint'),
+          None,
+          TD('e', f='rint', astype={'e': 'f'}),
+          TD(inexactvec, dispatch=[('loops_unary_fp', 'fd')]),
+          TD('fdg' + cmplx, f='rint'),
+          TD(P, f='rint'),
+          ),
+'arctan2':
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.arctan2'),
+          None,
+          TD(flts, f='atan2', astype={'e': 'f'}),
+          TD(P, f='arctan2'),
+          ),
+'remainder':
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.remainder'),
+          'PyUFunc_RemainderTypeResolver',
+          TD(ints, dispatch=[('loops_modulo', ints)]),
+          TD(flts),
+          [TypeDescription('m', FullTypeDescr, 'mm', 'm')],
+          TD(O, f='PyNumber_Remainder'),
+          ),
+'divmod':
+    Ufunc(2, 2, None,
+          docstrings.get('numpy.core.umath.divmod'),
+          'PyUFunc_DivmodTypeResolver',
+          TD(ints, dispatch=[('loops_modulo', ints)]),
+          TD(flts),
+          [TypeDescription('m', FullTypeDescr, 'mm', 'qm')],
+          # TD(O, f='PyNumber_Divmod'),  # gh-9730
+          ),
+'hypot':
+    Ufunc(2, 1, Zero,
+          docstrings.get('numpy.core.umath.hypot'),
+          None,
+          TD(flts, f='hypot', astype={'e': 'f'}),
+          TD(P, f='hypot'),
+          ),
+'isnan':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.isnan'),
+          'PyUFunc_IsFiniteTypeResolver',
+          TD(noobj, simd=[('avx512_skx', 'fd')], out='?'),
+          ),
+'isnat':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.isnat'),
+          'PyUFunc_IsNaTTypeResolver',
+          TD(times, out='?'),
+          ),
+'isinf':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.isinf'),
+          'PyUFunc_IsFiniteTypeResolver',
+          TD(noobj, simd=[('avx512_skx', 'fd')], out='?'),
+          ),
+'isfinite':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.isfinite'),
+          'PyUFunc_IsFiniteTypeResolver',
+          TD(noobj, simd=[('avx512_skx', 'fd')], out='?'),
+          ),
+'signbit':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.signbit'),
+          None,
+          TD(flts, simd=[('avx512_skx', 'fd')], out='?'),
+          ),
+'copysign':
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.copysign'),
+          None,
+          TD(flts),
+          ),
+'nextafter':
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.nextafter'),
+          None,
+          TD(flts),
+          ),
+'spacing':
+    Ufunc(1, 1, None,
+          docstrings.get('numpy.core.umath.spacing'),
+          None,
+          TD(flts),
+          ),
+'modf':
+    Ufunc(1, 2, None,
+          docstrings.get('numpy.core.umath.modf'),
+          None,
+          TD(flts),
+          ),
+'ldexp' :
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.ldexp'),
+          None,
+          [TypeDescription('e', None, 'ei', 'e'),
+          TypeDescription('f', None, 'fi', 'f', dispatch='loops_exponent_log'),
+          TypeDescription('e', FuncNameSuffix('long'), 'el', 'e'),
+          TypeDescription('f', FuncNameSuffix('long'), 'fl', 'f'),
+          TypeDescription('d', None, 'di', 'd', dispatch='loops_exponent_log'),
+          TypeDescription('d', FuncNameSuffix('long'), 'dl', 'd'),
+          TypeDescription('g', None, 'gi', 'g'),
+          TypeDescription('g', FuncNameSuffix('long'), 'gl', 'g'),
+          ],
+          ),
+'frexp' :
+    Ufunc(1, 2, None,
+          docstrings.get('numpy.core.umath.frexp'),
+          None,
+          [TypeDescription('e', None, 'e', 'ei'),
+          TypeDescription('f', None, 'f', 'fi', dispatch='loops_exponent_log'),
+          TypeDescription('d', None, 'd', 'di', dispatch='loops_exponent_log'),
+          TypeDescription('g', None, 'g', 'gi'),
+          ],
+          ),
+'gcd' :
+    Ufunc(2, 1, Zero,
+          docstrings.get('numpy.core.umath.gcd'),
+          "PyUFunc_SimpleUniformOperationTypeResolver",
           TD(ints),
-          TD(O, f='PyNumber_Lshift'),
-          ),
-'right_shift' :
-    Ufunc(2, 1, None,
-          'computes x1 >> x2 (x1 shifted to right by x2 bits) elementwise.',
+          TD('O', f='npy_ObjectGCD'),
+          ),
+'lcm' :
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.lcm'),
+          "PyUFunc_SimpleUniformOperationTypeResolver",
           TD(ints),
-          TD(O, f='PyNumber_Rshift'),
-          ),
-'arccos' :
-    Ufunc(1, 1, None,
-          'inverse cosine elementwise.',
-          TD(inexact, f='acos'),
-          TD(M, f='arccos'),
-          ),
-'arccosh' :
-    Ufunc(1, 1, None,
-          'inverse hyperbolic cosine elementwise.',
-          TD(inexact, f='acosh'),
-          TD(M, f='arccosh'),
-          ),
-'arcsin' :
-    Ufunc(1, 1, None,
-          'inverse sine elementwise.',
-          TD(inexact, f='asin'),
-          TD(M, f='arcsin'),
-          ),
-'arcsinh' :
-    Ufunc(1, 1, None,
-          'inverse hyperbolic sine elementwise.',
-          TD(inexact, f='asinh'),
-          TD(M, f='arcsinh'),
-          ),
-'arctan' :
-    Ufunc(1, 1, None,
-          'inverse tangent elementwise.',
-          TD(inexact, f='atan'),
-          TD(M, f='arctan'),
-          ),
-'arctanh' :
-    Ufunc(1, 1, None,
-          'inverse hyperbolic tangent elementwise.',
-          TD(inexact, f='atanh'),
-          TD(M, f='arctanh'),
-          ),
-'cos' :
-    Ufunc(1, 1, None,
-          'cosine elementwise.',
-          TD(inexact, f='cos'),
-          TD(M, f='cos'),
-          ),
-'sin' :
-    Ufunc(1, 1, None,
-          'sine elementwise.',
-          TD(inexact, f='sin'),
-          TD(M, f='sin'),
-          ),
-'tan' :
-    Ufunc(1, 1, None,
-          'tangent elementwise.',
-          TD(inexact, f='tan'),
-          TD(M, f='tan'),
-          ),
-'cosh' :
-    Ufunc(1, 1, None,
-          'hyperbolic cosine elementwise.',
-          TD(inexact, f='cosh'),
-          TD(M, f='cosh'),
-          ),
-'sinh' :
-    Ufunc(1, 1, None,
-          'hyperbolic sine elementwise.',
-          TD(inexact, f='sinh'),
-          TD(M, f='sinh'),
-          ),
-'tanh' :
-    Ufunc(1, 1, None,
-          'hyperbolic tangent elementwise.',
-          TD(inexact, f='tanh'),
-          TD(M, f='tanh'),
-          ),
-'exp' :
-    Ufunc(1, 1, None,
-          'e**x elementwise.',
-          TD(inexact, f='exp'),
-          TD(M, f='exp'),
-          ),
-'expm1' :
-    Ufunc(1, 1, None,
-          'e**x-1 elementwise.',
-          TD(inexact, f='expm1'),
-          TD(M, f='expm1'),
-          ),
-'log' :
-    Ufunc(1, 1, None,
-          'logarithm base e elementwise.',
-          TD(inexact, f='log'),
-          TD(M, f='log'),
-          ),
-'log10' :
-    Ufunc(1, 1, None,
-          'logarithm base 10 elementwise.',
-          TD(inexact, f='log10'),
-          TD(M, f='log10'),
-          ),
-'log1p' :
-    Ufunc(1, 1, None,
-          'log(1+x) to base e elementwise.',
-          TD(inexact, f='log1p'),
-          TD(M, f='log1p'),
-          ),
-'sqrt' :
-    Ufunc(1, 1, None,
-          'square-root elementwise. For real x, the domain is restricted to x>=0.',
-          TD(inexact, f='sqrt'),
-          TD(M, f='sqrt'),
-          ),
-'ceil' :
-    Ufunc(1, 1, None,
-          'elementwise smallest integer >= x.',
-          TD(flts, f='ceil'),
-          TD(M, f='ceil'),
-          ),
-'fabs' :
-    Ufunc(1, 1, None,
-          'absolute values.',
-          TD(flts, f='fabs'),
-          TD(M, f='fabs'),
-       ),
-'floor' :
-    Ufunc(1, 1, None,
-          'elementwise largest integer <= x',
-          TD(flts, f='floor'),
-          TD(M, f='floor'),
-          ),
-'rint' :
-    Ufunc(1, 1, None,
-          'round x elementwise to the nearest integer, round halfway cases away from zero',
-          TD(flts, f='rint'),
-          TD(M, f='rint'),
-          ),
-'arctan2' :
-    Ufunc(2, 1, None,
-          'a safe and correct arctan(x1/x2)',
-          TD(flts, f='atan2'),
-          TD(M, f='arctan2'),
-          ),
-'remainder' :
-    Ufunc(2, 1, None,
-          'computes x1-n*x2 where n is floor(x1 / x2)',
-          TD(intflt),
-          TD(O, f='PyNumber_Remainder'),
-          ),
-'hypot' :
-    Ufunc(2, 1, None,
-          'sqrt(x1**2 + x2**2) elementwise',
-          TD(flts, f='hypot'),
-          TD(M, f='hypot'),
-          ),
-'isnan' :
-    Ufunc(1, 1, None,
-          'returns True where x is Not-A-Number',
-          TD(inexact, out='?'),
-          ),
-'isinf' :
-    Ufunc(1, 1, None,
-          'returns True where x is +inf or -inf',
-          TD(inexact, out='?'),
-          ),
-'isfinite' :
-    Ufunc(1, 1, None,
-          'returns True where x is finite',
-          TD(inexact, out='?'),
-          ),
-'signbit' :
-    Ufunc(1, 1, None,
-          'returns True where signbit of x is set (x<0).',
-          TD(flts, out='?'),
-          ),
-'modf' :
-    Ufunc(1, 2, None,
-          'breaks x into fractional (y1) and integral (y2) parts.\\n\\n    Each output has the same sign as the input.',
-          TD(flts),
+          TD('O', f='npy_ObjectLCM'),
+          ),
+'matmul' :
+    Ufunc(2, 1, None,
+          docstrings.get('numpy.core.umath.matmul'),
+          "PyUFunc_SimpleUniformOperationTypeResolver",
+          TD(notimes_or_obj),
+          TD(O),
+          signature='(n?,k),(k,m?)->(n?,m?)',
           ),
 }
 
-def indent(st,spaces):
-    indention = ' '*spaces
-    indented = indention + string.replace(st,'\n','\n'+indention)
+def indent(st, spaces):
+    indentation = ' '*spaces
+    indented = indentation + st.replace('\n', '\n'+indentation)
     # trim off any trailing spaces
-    indented = re.sub(r' +$',r'',indented)
+    indented = re.sub(r' +$', r'', indented)
     return indented
 
-chartoname = {'?': 'bool',
-              'b': 'byte',
-              'B': 'ubyte',
-              'h': 'short',
-              'H': 'ushort',
-              'i': 'int',
-              'I': 'uint',
-              'l': 'long',
-              'L': 'ulong',
-              'q': 'longlong',
-              'Q': 'ulonglong',
-              'f': 'float',
-              'd': 'double',
-              'g': 'longdouble',
-              'F': 'cfloat',
-              'D': 'cdouble',
-              'G': 'clongdouble',
-              'O': 'OBJECT',
-              'M': 'OBJECT',
-              }
-
-chartotype1 = {'f': 'f_f',
-               'd': 'd_d',
-               'g': 'g_g',
-               'F': 'F_F',
-               'D': 'D_D',
-               'G': 'G_G',
-               'O': 'O_O',
-               'M': 'O_O_method'}
-
-chartotype2 = {'f': 'ff_f',
-               'd': 'dd_d',
-               'g': 'gg_g',
-               'F': 'FF_F',
-               'D': 'DD_D',
-               'G': 'GG_G',
-               'O': 'OO_O',
-               'M': 'OO_O_method'}
+# maps [nin, nout][type] to a suffix
+arity_lookup = {
+    (1, 1): {
+        'e': 'e_e',
+        'f': 'f_f',
+        'd': 'd_d',
+        'g': 'g_g',
+        'F': 'F_F',
+        'D': 'D_D',
+        'G': 'G_G',
+        'O': 'O_O',
+        'P': 'O_O_method',
+    },
+    (2, 1): {
+        'e': 'ee_e',
+        'f': 'ff_f',
+        'd': 'dd_d',
+        'g': 'gg_g',
+        'F': 'FF_F',
+        'D': 'DD_D',
+        'G': 'GG_G',
+        'O': 'OO_O',
+        'P': 'OO_O_method',
+    },
+    (3, 1): {
+        'O': 'OOO_O',
+    }
+}
+
 #for each name
 # 1) create functions, data, and signature
 # 2) fill in functions and data in InitOperators
 # 3) add function.
 
 def make_arrays(funcdict):
-    # functions array contains an entry for every type implemented
-    #   NULL should be placed where PyUfunc_ style function will be filled in later
-    #
+    # functions array contains an entry for every type implemented NULL
+    # should be placed where PyUfunc_ style function will be filled in
+    # later
     code1list = []
     code2list = []
-    names = funcdict.keys()
-    names.sort()
+    dispdict  = {}
+    names = sorted(funcdict.keys())
     for name in names:
         uf = funcdict[name]
         funclist = []
@@ -549,98 +1064,184 @@
         k = 0
         sub = 0
 
-        if uf.nin > 1:
-            assert uf.nin == 2
-            thedict = chartotype2  # two inputs and one output
-        else:
-            thedict = chartotype1  # one input and one output
-
         for t in uf.type_descriptions:
-            if t.func_data is not None:
-                funclist.append('NULL')
-                astr = '%s_functions[%d] = PyUFunc_%s;' % \
-                       (name, k, thedict[t.type])
+            cfunc_alias = t.cfunc_alias if t.cfunc_alias else name
+            cfunc_fname = None
+            if t.func_data is FullTypeDescr:
+                tname = english_upper(chartoname[t.type])
+                datalist.append('(void *)NULL')
+                cfunc_fname = f"{tname}_{t.in_}_{t.out}_{cfunc_alias}"
+            elif isinstance(t.func_data, FuncNameSuffix):
+                datalist.append('(void *)NULL')
+                tname = english_upper(chartoname[t.type])
+                cfunc_fname = f"{tname}_{cfunc_alias}_{t.func_data.suffix}"
+            elif t.func_data is None:
+                datalist.append('(void *)NULL')
+                tname = english_upper(chartoname[t.type])
+                cfunc_fname = f"{tname}_{cfunc_alias}"
+                if t.simd is not None:
+                    for vt in t.simd:
+                        code2list.append(textwrap.dedent("""\
+                        #ifdef HAVE_ATTRIBUTE_TARGET_{ISA}
+                        if (NPY_CPU_HAVE({ISA})) {{
+                            {fname}_functions[{idx}] = {cname}_{isa};
+                        }}
+                        #endif
+                        """).format(
+                            ISA=vt.upper(), isa=vt,
+                            fname=name, cname=cfunc_fname, idx=k
+                        ))
+            else:
+                try:
+                    thedict = arity_lookup[uf.nin, uf.nout]
+                except KeyError as e:
+                    raise ValueError(
+                        f"Could not handle {name}[{t.type}] "
+                        f"with nin={uf.nin}, nout={uf.nout}"
+                    ) from None
+
+                astype = ''
+                if not t.astype is None:
+                    astype = '_As_%s' % thedict[t.astype]
+                astr = ('%s_functions[%d] = PyUFunc_%s%s;' %
+                           (name, k, thedict[t.type], astype))
                 code2list.append(astr)
                 if t.type == 'O':
-                    astr = '%s_data[%d] = (void *) %s;' % \
-                           (name, k, t.func_data)
+                    astr = ('%s_data[%d] = (void *) %s;' %
+                               (name, k, t.func_data))
                     code2list.append(astr)
                     datalist.append('(void *)NULL')
-                elif t.type == 'M':
+                elif t.type == 'P':
                     datalist.append('(void *)"%s"' % t.func_data)
                 else:
-                    datalist.append('(void *)%s' % t.func_data)
+                    astr = ('%s_data[%d] = (void *) %s;' %
+                               (name, k, t.func_data))
+                    code2list.append(astr)
+                    datalist.append('(void *)NULL')
+                    #datalist.append('(void *)%s' % t.func_data)
                 sub += 1
+
+            if cfunc_fname:
+                funclist.append(cfunc_fname)
+                if t.dispatch:
+                    dispdict.setdefault(t.dispatch, []).append((name, k, cfunc_fname))
             else:
-                datalist.append('(void *)NULL');
-                tname = chartoname[t.type].upper()
-                funclist.append('%s_%s' % (tname, name))
+                funclist.append('NULL')
 
             for x in t.in_ + t.out:
-                siglist.append('PyArray_%s' % (chartoname[x].upper(),))
+                siglist.append('NPY_%s' % (english_upper(chartoname[x]),))
 
             k += 1
 
         funcnames = ', '.join(funclist)
         signames = ', '.join(siglist)
         datanames = ', '.join(datalist)
-        code1list.append("static PyUFuncGenericFunction %s_functions[] = { %s };" \
+        code1list.append("static PyUFuncGenericFunction %s_functions[] = {%s};"
                          % (name, funcnames))
-        code1list.append("static void * %s_data[] = { %s };" \
+        code1list.append("static void * %s_data[] = {%s};"
                          % (name, datanames))
-        code1list.append("static char %s_signatures[] = { %s };" \
+        code1list.append("static char %s_signatures[] = {%s};"
                          % (name, signames))
-    return "\n".join(code1list),"\n".join(code2list)
+
+    for dname, funcs in dispdict.items():
+        code2list.append(textwrap.dedent(f"""
+            #ifndef NPY_DISABLE_OPTIMIZATION
+            #include "{dname}.dispatch.h"
+            #endif
+        """))
+        for (ufunc_name, func_idx, cfunc_name) in funcs:
+            code2list.append(textwrap.dedent(f"""\
+                NPY_CPU_DISPATCH_CALL_XB({ufunc_name}_functions[{func_idx}] = {cfunc_name});
+            """))
+    return "\n".join(code1list), "\n".join(code2list)
 
 def make_ufuncs(funcdict):
     code3list = []
-    names = funcdict.keys()
-    names.sort()
+    names = sorted(funcdict.keys())
     for name in names:
         uf = funcdict[name]
         mlist = []
-        mlist.append(\
-r"""f = PyUFunc_FromFuncAndData(%s_functions, %s_data, %s_signatures, %d,
-                                %d, %d, %s, "%s",
-                                "%s", 0);""" % (name, name, name,
-                                                len(uf.type_descriptions),
-                                                uf.nin, uf.nout,
-                                                uf.identity,
-                                                name, uf.docstring))
+        if uf.signature is None:
+            sig = "NULL"
+        else:
+            sig = '"{}"'.format(uf.signature)
+        fmt = textwrap.dedent("""\
+            identity = {identity_expr};
+            if ({has_identity} && identity == NULL) {{
+                return -1;
+            }}
+            f = PyUFunc_FromFuncAndDataAndSignatureAndIdentity(
+                {name}_functions, {name}_data, {name}_signatures, {nloops},
+                {nin}, {nout}, {identity}, "{name}",
+                {doc}, 0, {sig}, identity
+            );
+            if ({has_identity}) {{
+                Py_DECREF(identity);
+            }}
+            if (f == NULL) {{
+                return -1;
+            }}
+        """)
+        args = dict(
+            name=name, nloops=len(uf.type_descriptions),
+            nin=uf.nin, nout=uf.nout,
+            has_identity='0' if uf.identity is None_ else '1',
+            identity='PyUFunc_IdentityValue',
+            identity_expr=uf.identity,
+            doc=uf.docstring,
+            sig=sig,
+        )
+
+        # Only PyUFunc_None means don't reorder - we pass this using the old
+        # argument
+        if uf.identity is None_:
+            args['identity'] = 'PyUFunc_None'
+            args['identity_expr'] = 'NULL'
+
+        mlist.append(fmt.format(**args))
+        if uf.typereso is not None:
+            mlist.append(
+                r"((PyUFuncObject *)f)->type_resolver = &%s;" % uf.typereso)
         mlist.append(r"""PyDict_SetItemString(dictionary, "%s", f);""" % name)
         mlist.append(r"""Py_DECREF(f);""")
         code3list.append('\n'.join(mlist))
     return '\n'.join(code3list)
 
 
-def make_code(funcdict,filename):
+def make_code(funcdict, filename):
     code1, code2 = make_arrays(funcdict)
     code3 = make_ufuncs(funcdict)
-    code2 = indent(code2,4)
-    code3 = indent(code3,4)
-    code = r"""
-
-/** Warning this file is autogenerated!!!
-
-    Please make changes to the code generator program (%s)
-**/
-
-%s
-
-static void
-InitOperators(PyObject *dictionary) {
-    PyObject *f;
-
-%s
-%s
-}
-""" % (filename, code1, code2, code3)
-    return code;
+    code2 = indent(code2, 4)
+    code3 = indent(code3, 4)
+    code = textwrap.dedent(r"""
+
+    /** Warning this file is autogenerated!!!
+
+        Please make changes to the code generator program (%s)
+    **/
+    #include "ufunc_object.h"
+    #include "ufunc_type_resolution.h"
+    #include "loops.h"
+    #include "matmul.h"
+    #include "clip.h"
+    #include "_umath_doc_generated.h"
+    %s
+
+    static int
+    InitOperators(PyObject *dictionary) {
+        PyObject *f, *identity;
+
+    %s
+    %s
+
+        return 0;
+    }
+    """) % (filename, code1, code2, code3)
+    return code
 
 
 if __name__ == "__main__":
     filename = __file__
-    fid = open('__umath_generated.c','w')
     code = make_code(defdict, filename)
-    fid.write(code)
-    fid.close()
+    with open('__umath_generated.c', 'w') as fid:
+        fid.write(code)
('numpy/linalg', '__init__.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,8 +1,80 @@
+"""
+``numpy.linalg``
+================
+
+The NumPy linear algebra functions rely on BLAS and LAPACK to provide efficient
+low level implementations of standard linear algebra algorithms. Those
+libraries may be provided by NumPy itself using C versions of a subset of their
+reference implementations but, when possible, highly optimized libraries that
+take advantage of specialized processor functionality are preferred. Examples
+of such libraries are OpenBLAS, MKL (TM), and ATLAS. Because those libraries
+are multithreaded and processor dependent, environmental variables and external
+packages such as threadpoolctl may be needed to control the number of threads
+or specify the processor architecture.
+
+- OpenBLAS: https://www.openblas.net/
+- threadpoolctl: https://github.com/joblib/threadpoolctl
+
+Please note that the most-used linear algebra functions in NumPy are present in
+the main ``numpy`` namespace rather than in ``numpy.linalg``.  There are:
+``dot``, ``vdot``, ``inner``, ``outer``, ``matmul``, ``tensordot``, ``einsum``,
+``einsum_path`` and ``kron``.
+
+Functions present in numpy.linalg are listed below.
+
+
+Matrix and vector products
+--------------------------
+
+   multi_dot
+   matrix_power
+
+Decompositions
+--------------
+
+   cholesky
+   qr
+   svd
+
+Matrix eigenvalues
+------------------
+
+   eig
+   eigh
+   eigvals
+   eigvalsh
+
+Norms and other numbers
+-----------------------
+
+   norm
+   cond
+   det
+   matrix_rank
+   slogdet
+
+Solving equations and inverting matrices
+----------------------------------------
+
+   solve
+   tensorsolve
+   lstsq
+   inv
+   pinv
+   tensorinv
+
+Exceptions
+----------
+
+   LinAlgError
+
+"""
 # To get sub-modules
-from info import __doc__
+from . import linalg
+from .linalg import *
 
-from linalg import *
+__all__ = linalg.__all__.copy()
 
-def test(level=1, verbosity=1):
-    from numpy.testing import NumpyTest
-    return NumpyTest().test(level, verbosity)
+from numpy._pytesttester import PytestTester
+test = PytestTester(__name__)
+del PytestTester
('numpy/linalg', 'setup.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,31 +1,84 @@
+import os
+import sys
 
-from os.path import join
+def configuration(parent_package='', top_path=None):
+    from numpy.distutils.misc_util import Configuration
+    from numpy.distutils.ccompiler_opt import NPY_CXX_FLAGS
+    from numpy.distutils.system_info import get_info, system_info
+    config = Configuration('linalg', parent_package, top_path)
 
-def configuration(parent_package='',top_path=None):
-    from numpy.distutils.misc_util import Configuration
-    from numpy.distutils.system_info import get_info
-    config = Configuration('linalg',parent_package,top_path)
+    config.add_subpackage('tests')
 
     # Configure lapack_lite
-    lapack_info = get_info('lapack_opt',0)
+
+    src_dir = 'lapack_lite'
+    lapack_lite_src = [
+        os.path.join(src_dir, 'python_xerbla.c'),
+        os.path.join(src_dir, 'f2c_z_lapack.c'),
+        os.path.join(src_dir, 'f2c_c_lapack.c'),
+        os.path.join(src_dir, 'f2c_d_lapack.c'),
+        os.path.join(src_dir, 'f2c_s_lapack.c'),
+        os.path.join(src_dir, 'f2c_lapack.c'),
+        os.path.join(src_dir, 'f2c_blas.c'),
+        os.path.join(src_dir, 'f2c_config.c'),
+        os.path.join(src_dir, 'f2c.c'),
+    ]
+    all_sources = config.paths(lapack_lite_src)
+
+    if os.environ.get('NPY_USE_BLAS_ILP64', "0") != "0":
+        lapack_info = get_info('lapack_ilp64_opt', 2)
+    else:
+        lapack_info = get_info('lapack_opt', 0)  # and {}
+
+    use_lapack_lite = not lapack_info
+
+    if use_lapack_lite:
+        # This makes numpy.distutils write the fact that lapack_lite
+        # is being used to numpy.__config__
+        class numpy_linalg_lapack_lite(system_info):
+            def calc_info(self):
+                info = {'language': 'c'}
+                if sys.maxsize > 2**32:
+                    # Build lapack-lite in 64-bit integer mode.
+                    # The suffix is arbitrary (lapack_lite symbols follow it),
+                    # but use the "64_" convention here.
+                    info['define_macros'] = [
+                        ('HAVE_BLAS_ILP64', None),
+                        ('BLAS_SYMBOL_SUFFIX', '64_')
+                    ]
+                self.set_info(**info)
+
+        lapack_info = numpy_linalg_lapack_lite().get_info(2)
+
     def get_lapack_lite_sources(ext, build_dir):
-        if not lapack_info:
-            print "### Warning:  Using unoptimized lapack ###"
-            return ext.depends[:-1]
+        if use_lapack_lite:
+            print("### Warning:  Using unoptimized lapack ###")
+            return all_sources
         else:
-            return ext.depends[:1]
+            if sys.platform == 'win32':
+                print("### Warning:  python_xerbla.c is disabled ###")
+                return []
+            return [all_sources[0]]
 
-    config.add_extension('lapack_lite',
-                         sources = [get_lapack_lite_sources],
-                         depends=  ['lapack_litemodule.c',
-                                   'zlapack_lite.c', 'dlapack_lite.c',
-                                   'blas_lite.c', 'dlamch.c',
-                                   'f2c_lite.c','f2c.h'],
-                         extra_info = lapack_info
-                         )
+    config.add_extension(
+        'lapack_lite',
+        sources=['lapack_litemodule.c', get_lapack_lite_sources],
+        depends=['lapack_lite/f2c.h'],
+        extra_info=lapack_info,
+    )
 
+    # umath_linalg module
+    config.add_extension(
+        '_umath_linalg',
+        sources=['umath_linalg.cpp', get_lapack_lite_sources],
+        depends=['lapack_lite/f2c.h'],
+        extra_info=lapack_info,
+        extra_cxx_compile_args=NPY_CXX_FLAGS,
+        libraries=['npymath'],
+    )
+    config.add_data_files('*.pyi')
     return config
 
 if __name__ == '__main__':
     from numpy.distutils.core import setup
-    setup(**configuration(top_path='').todict())
+    setup(configuration=configuration)
('numpy/linalg', 'linalg.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,619 +1,2810 @@
-
 """Lite version of scipy.linalg.
+
+Notes
+-----
+This module is a lite version of the linalg.py module in SciPy which
+contains high-level Python interface to the LAPACK library.  The lite
+version only accesses the following LAPACK functions: dgesv, zgesv,
+dgeev, zgeev, dgesdd, zgesdd, dgelsd, zgelsd, dsyevd, zheevd, dgetrf,
+zgetrf, dpotrf, zpotrf, dgeqrf, zgeqrf, zungqr, dorgqr.
 """
-# This module is a lite version of the linalg.py module in SciPy which contains
-# high-level Python interface to the LAPACK library.  The lite version
-# only accesses the following LAPACK functions: dgesv, zgesv, dgeev,
-# zgeev, dgesdd, zgesdd, dgelsd, zgelsd, dsyevd, zheevd, dgetrf, dpotrf.
-
-__all__ = ['solve',
-           'inv', 'cholesky',
-           'eigvals',
-           'eigvalsh', 'pinv',
-           'det', 'svd',
-           'eig', 'eigh','lstsq', 'norm',
-           'LinAlgError'
-           ]
-
-from numpy.core import *
-from numpy.lib import *
-import lapack_lite
-
-# Error object
+
+__all__ = ['matrix_power', 'solve', 'tensorsolve', 'tensorinv', 'inv',
+           'cholesky', 'eigvals', 'eigvalsh', 'pinv', 'slogdet', 'det',
+           'svd', 'eig', 'eigh', 'lstsq', 'norm', 'qr', 'cond', 'matrix_rank',
+           'LinAlgError', 'multi_dot']
+
+import functools
+import operator
+import warnings
+
+from numpy.core import (
+    array, asarray, zeros, empty, empty_like, intc, single, double,
+    csingle, cdouble, inexact, complexfloating, newaxis, all, Inf, dot,
+    add, multiply, sqrt, fastCopyAndTranspose, sum, isfinite,
+    finfo, errstate, geterrobj, moveaxis, amin, amax, product, abs,
+    atleast_2d, intp, asanyarray, object_, matmul,
+    swapaxes, divide, count_nonzero, isnan, sign, argsort, sort,
+    reciprocal
+)
+from numpy.core.multiarray import normalize_axis_index
+from numpy.core.overrides import set_module
+from numpy.core import overrides
+from numpy.lib.twodim_base import triu, eye
+from numpy.linalg import _umath_linalg
+
+
+array_function_dispatch = functools.partial(
+    overrides.array_function_dispatch, module='numpy.linalg')
+
+
+fortran_int = intc
+
+
+@set_module('numpy.linalg')
 class LinAlgError(Exception):
-    pass
-
-# Helper routines
-_array_kind = {'i':0, 'l': 0, 'q': 0, 'f': 0, 'd': 0, 'F': 1, 'D': 1}
-_array_precision = {'i': 1, 'l': 1, 'q': 1, 'f': 0, 'd': 1, 'F': 0, 'D': 1}
-_array_type = [['f', 'd'], ['F', 'D']]
+    """
+    Generic Python-exception-derived object raised by linalg functions.
+
+    General purpose exception class, derived from Python's exception.Exception
+    class, programmatically raised in linalg functions when a Linear
+    Algebra-related condition would prevent further correct execution of the
+    function.
+
+    Parameters
+    ----------
+    None
+
+    Examples
+    --------
+    >>> from numpy import linalg as LA
+    >>> LA.inv(np.zeros((2,2)))
+    Traceback (most recent call last):
+      File "<stdin>", line 1, in <module>
+      File "...linalg.py", line 350,
+        in inv return wrap(solve(a, identity(a.shape[0], dtype=a.dtype)))
+      File "...linalg.py", line 249,
+        in solve
+        raise LinAlgError('Singular matrix')
+    numpy.linalg.LinAlgError: Singular matrix
+
+    """
+
+
+def _determine_error_states():
+    errobj = geterrobj()
+    bufsize = errobj[0]
+
+    with errstate(invalid='call', over='ignore',
+                  divide='ignore', under='ignore'):
+        invalid_call_errmask = geterrobj()[1]
+
+    return [bufsize, invalid_call_errmask, None]
+
+# Dealing with errors in _umath_linalg
+_linalg_error_extobj = _determine_error_states()
+del _determine_error_states
+
+def _raise_linalgerror_singular(err, flag):
+    raise LinAlgError("Singular matrix")
+
+def _raise_linalgerror_nonposdef(err, flag):
+    raise LinAlgError("Matrix is not positive definite")
+
+def _raise_linalgerror_eigenvalues_nonconvergence(err, flag):
+    raise LinAlgError("Eigenvalues did not converge")
+
+def _raise_linalgerror_svd_nonconvergence(err, flag):
+    raise LinAlgError("SVD did not converge")
+
+def _raise_linalgerror_lstsq(err, flag):
+    raise LinAlgError("SVD did not converge in Linear Least Squares")
+
+def _raise_linalgerror_qr(err, flag):
+    raise LinAlgError("Incorrect argument found while performing "
+                      "QR factorization")
+
+def get_linalg_error_extobj(callback):
+    extobj = list(_linalg_error_extobj)  # make a copy
+    extobj[2] = callback
+    return extobj
 
 def _makearray(a):
     new = asarray(a)
-    wrap = getattr(a, "__array_wrap__", new.__array_wrap__)
+    wrap = getattr(a, "__array_prepare__", new.__array_wrap__)
     return new, wrap
 
+def isComplexType(t):
+    return issubclass(t, complexfloating)
+
+_real_types_map = {single : single,
+                   double : double,
+                   csingle : single,
+                   cdouble : double}
+
+_complex_types_map = {single : csingle,
+                      double : cdouble,
+                      csingle : csingle,
+                      cdouble : cdouble}
+
+def _realType(t, default=double):
+    return _real_types_map.get(t, default)
+
+def _complexType(t, default=cdouble):
+    return _complex_types_map.get(t, default)
+
 def _commonType(*arrays):
-    kind = 0
-#    precision = 0
-#   force higher precision in lite version
-    precision = 1
+    # in lite version, use higher precision (always double or cdouble)
+    result_type = single
+    is_complex = False
     for a in arrays:
-        t = a.dtype.char
-        kind = max(kind, _array_kind[t])
-        precision = max(precision, _array_precision[t])
-    return _array_type[kind][precision]
-
-def _castCopyAndTranspose(type, *arrays):
-    if len(arrays) == 1:
-        return transpose(arrays[0]).astype(type)
+        if issubclass(a.dtype.type, inexact):
+            if isComplexType(a.dtype.type):
+                is_complex = True
+            rt = _realType(a.dtype.type, default=None)
+            if rt is None:
+                # unsupported inexact scalar
+                raise TypeError("array type %s is unsupported in linalg" %
+                        (a.dtype.name,))
+        else:
+            rt = double
+        if rt is double:
+            result_type = double
+    if is_complex:
+        t = cdouble
+        result_type = _complex_types_map[result_type]
     else:
-        return [transpose(a).astype(type) for a in arrays]
-
-# _fastCopyAndTranpose is an optimized version of _castCopyAndTranspose.
-# It assumes the input is 2D (as all the calls in here are).
+        t = double
+    return t, result_type
+
+
+# _fastCopyAndTranpose assumes the input is 2D (as all the calls in here are).
 
 _fastCT = fastCopyAndTranspose
+
+def _to_native_byte_order(*arrays):
+    ret = []
+    for arr in arrays:
+        if arr.dtype.byteorder not in ('=', '|'):
+            ret.append(asarray(arr, dtype=arr.dtype.newbyteorder('=')))
+        else:
+            ret.append(arr)
+    if len(ret) == 1:
+        return ret[0]
+    else:
+        return ret
 
 def _fastCopyAndTranspose(type, *arrays):
     cast_arrays = ()
     for a in arrays:
-        if a.dtype.char == type:
-            cast_arrays = cast_arrays + (_fastCT(a),)
-        else:
-            cast_arrays = cast_arrays + (_fastCT(a.astype(type)),)
+        if a.dtype.type is not type:
+            a = a.astype(type)
+        cast_arrays = cast_arrays + (_fastCT(a),)
     if len(cast_arrays) == 1:
         return cast_arrays[0]
     else:
         return cast_arrays
 
-def _assertRank2(*arrays):
+def _assert_2d(*arrays):
     for a in arrays:
-        if len(a.shape) != 2:
-            raise LinAlgError, 'Array must be two-dimensional'
-
-def _assertSquareness(*arrays):
+        if a.ndim != 2:
+            raise LinAlgError('%d-dimensional array given. Array must be '
+                    'two-dimensional' % a.ndim)
+
+def _assert_stacked_2d(*arrays):
     for a in arrays:
-        if max(a.shape) != min(a.shape):
-            raise LinAlgError, 'Array must be square'
+        if a.ndim < 2:
+            raise LinAlgError('%d-dimensional array given. Array must be '
+                    'at least two-dimensional' % a.ndim)
+
+def _assert_stacked_square(*arrays):
+    for a in arrays:
+        m, n = a.shape[-2:]
+        if m != n:
+            raise LinAlgError('Last 2 dimensions of the array must be square')
+
+def _assert_finite(*arrays):
+    for a in arrays:
+        if not isfinite(a).all():
+            raise LinAlgError("Array must not contain infs or NaNs")
+
+def _is_empty_2d(arr):
+    # check size first for efficiency
+    return arr.size == 0 and product(arr.shape[-2:]) == 0
+
+
+def transpose(a):
+    """
+    Transpose each matrix in a stack of matrices.
+
+    Unlike np.transpose, this only swaps the last two axes, rather than all of
+    them
+
+    Parameters
+    ----------
+    a : (...,M,N) array_like
+
+    Returns
+    -------
+    aT : (...,N,M) ndarray
+    """
+    return swapaxes(a, -1, -2)
 
 # Linear equations
 
+def _tensorsolve_dispatcher(a, b, axes=None):
+    return (a, b)
+
+
+@array_function_dispatch(_tensorsolve_dispatcher)
+def tensorsolve(a, b, axes=None):
+    """
+    Solve the tensor equation ``a x = b`` for x.
+
+    It is assumed that all indices of `x` are summed over in the product,
+    together with the rightmost indices of `a`, as is done in, for example,
+    ``tensordot(a, x, axes=b.ndim)``.
+
+    Parameters
+    ----------
+    a : array_like
+        Coefficient tensor, of shape ``b.shape + Q``. `Q`, a tuple, equals
+        the shape of that sub-tensor of `a` consisting of the appropriate
+        number of its rightmost indices, and must be such that
+        ``prod(Q) == prod(b.shape)`` (in which sense `a` is said to be
+        'square').
+    b : array_like
+        Right-hand tensor, which can be of any shape.
+    axes : tuple of ints, optional
+        Axes in `a` to reorder to the right, before inversion.
+        If None (default), no reordering is done.
+
+    Returns
+    -------
+    x : ndarray, shape Q
+
+    Raises
+    ------
+    LinAlgError
+        If `a` is singular or not 'square' (in the above sense).
+
+    See Also
+    --------
+    numpy.tensordot, tensorinv, numpy.einsum
+
+    Examples
+    --------
+    >>> a = np.eye(2*3*4)
+    >>> a.shape = (2*3, 4, 2, 3, 4)
+    >>> b = np.random.randn(2*3, 4)
+    >>> x = np.linalg.tensorsolve(a, b)
+    >>> x.shape
+    (2, 3, 4)
+    >>> np.allclose(np.tensordot(a, x, axes=3), b)
+    True
+
+    """
+    a, wrap = _makearray(a)
+    b = asarray(b)
+    an = a.ndim
+
+    if axes is not None:
+        allaxes = list(range(0, an))
+        for k in axes:
+            allaxes.remove(k)
+            allaxes.insert(an, k)
+        a = a.transpose(allaxes)
+
+    oldshape = a.shape[-(an-b.ndim):]
+    prod = 1
+    for k in oldshape:
+        prod *= k
+
+    if a.size != prod ** 2:
+        raise LinAlgError(
+            "Input arrays must satisfy the requirement \
+            prod(a.shape[b.ndim:]) == prod(a.shape[:b.ndim])"
+        )
+
+    a = a.reshape(prod, prod)
+    b = b.ravel()
+    res = wrap(solve(a, b))
+    res.shape = oldshape
+    return res
+
+
+def _solve_dispatcher(a, b):
+    return (a, b)
+
+
+@array_function_dispatch(_solve_dispatcher)
 def solve(a, b):
-    one_eq = len(b.shape) == 1
-    if one_eq:
-        b = b[:, newaxis]
-    _assertRank2(a, b)
-    _assertSquareness(a)
-    n_eq = a.shape[0]
-    n_rhs = b.shape[1]
-    if n_eq != b.shape[0]:
-        raise LinAlgError, 'Incompatible dimensions'
-    t =_commonType(a, b)
-#    lapack_routine = _findLapackRoutine('gesv', t)
-    if _array_kind[t] == 1: # Complex routines take different arguments
-        lapack_routine = lapack_lite.zgesv
+    """
+    Solve a linear matrix equation, or system of linear scalar equations.
+
+    Computes the "exact" solution, `x`, of the well-determined, i.e., full
+    rank, linear matrix equation `ax = b`.
+
+    Parameters
+    ----------
+    a : (..., M, M) array_like
+        Coefficient matrix.
+    b : {(..., M,), (..., M, K)}, array_like
+        Ordinate or "dependent variable" values.
+
+    Returns
+    -------
+    x : {(..., M,), (..., M, K)} ndarray
+        Solution to the system a x = b.  Returned shape is identical to `b`.
+
+    Raises
+    ------
+    LinAlgError
+        If `a` is singular or not square.
+
+    See Also
+    --------
+    scipy.linalg.solve : Similar function in SciPy.
+
+    Notes
+    -----
+
+    .. versionadded:: 1.8.0
+
+    Broadcasting rules apply, see the `numpy.linalg` documentation for
+    details.
+
+    The solutions are computed using LAPACK routine ``_gesv``.
+
+    `a` must be square and of full-rank, i.e., all rows (or, equivalently,
+    columns) must be linearly independent; if either is not true, use
+    `lstsq` for the least-squares best "solution" of the
+    system/equation.
+
+    References
+    ----------
+    .. [1] G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando,
+           FL, Academic Press, Inc., 1980, pg. 22.
+
+    Examples
+    --------
+    Solve the system of equations ``x0 + 2 * x1 = 1`` and ``3 * x0 + 5 * x1 = 2``:
+
+    >>> a = np.array([[1, 2], [3, 5]])
+    >>> b = np.array([1, 2])
+    >>> x = np.linalg.solve(a, b)
+    >>> x
+    array([-1.,  1.])
+
+    Check that the solution is correct:
+
+    >>> np.allclose(np.dot(a, x), b)
+    True
+
+    """
+    a, _ = _makearray(a)
+    _assert_stacked_2d(a)
+    _assert_stacked_square(a)
+    b, wrap = _makearray(b)
+    t, result_t = _commonType(a, b)
+
+    # We use the b = (..., M,) logic, only if the number of extra dimensions
+    # match exactly
+    if b.ndim == a.ndim - 1:
+        gufunc = _umath_linalg.solve1
     else:
-        lapack_routine = lapack_lite.dgesv
-    a, b = _fastCopyAndTranspose(t, a, b)
-    pivots = zeros(n_eq, 'i')
-    results = lapack_routine(n_eq, n_rhs, a, n_eq, pivots, b, n_eq, 0)
-    if results['info'] > 0:
-        raise LinAlgError, 'Singular matrix'
-    if one_eq:
-        return ravel(b) # I see no need to copy here
+        gufunc = _umath_linalg.solve
+
+    signature = 'DD->D' if isComplexType(t) else 'dd->d'
+    extobj = get_linalg_error_extobj(_raise_linalgerror_singular)
+    r = gufunc(a, b, signature=signature, extobj=extobj)
+
+    return wrap(r.astype(result_t, copy=False))
+
+
+def _tensorinv_dispatcher(a, ind=None):
+    return (a,)
+
+
+@array_function_dispatch(_tensorinv_dispatcher)
+def tensorinv(a, ind=2):
+    """
+    Compute the 'inverse' of an N-dimensional array.
+
+    The result is an inverse for `a` relative to the tensordot operation
+    ``tensordot(a, b, ind)``, i. e., up to floating-point accuracy,
+    ``tensordot(tensorinv(a), a, ind)`` is the "identity" tensor for the
+    tensordot operation.
+
+    Parameters
+    ----------
+    a : array_like
+        Tensor to 'invert'. Its shape must be 'square', i. e.,
+        ``prod(a.shape[:ind]) == prod(a.shape[ind:])``.
+    ind : int, optional
+        Number of first indices that are involved in the inverse sum.
+        Must be a positive integer, default is 2.
+
+    Returns
+    -------
+    b : ndarray
+        `a`'s tensordot inverse, shape ``a.shape[ind:] + a.shape[:ind]``.
+
+    Raises
+    ------
+    LinAlgError
+        If `a` is singular or not 'square' (in the above sense).
+
+    See Also
+    --------
+    numpy.tensordot, tensorsolve
+
+    Examples
+    --------
+    >>> a = np.eye(4*6)
+    >>> a.shape = (4, 6, 8, 3)
+    >>> ainv = np.linalg.tensorinv(a, ind=2)
+    >>> ainv.shape
+    (8, 3, 4, 6)
+    >>> b = np.random.randn(4, 6)
+    >>> np.allclose(np.tensordot(ainv, b), np.linalg.tensorsolve(a, b))
+    True
+
+    >>> a = np.eye(4*6)
+    >>> a.shape = (24, 8, 3)
+    >>> ainv = np.linalg.tensorinv(a, ind=1)
+    >>> ainv.shape
+    (8, 3, 24)
+    >>> b = np.random.randn(24)
+    >>> np.allclose(np.tensordot(ainv, b, 1), np.linalg.tensorsolve(a, b))
+    True
+
+    """
+    a = asarray(a)
+    oldshape = a.shape
+    prod = 1
+    if ind > 0:
+        invshape = oldshape[ind:] + oldshape[:ind]
+        for k in oldshape[ind:]:
+            prod *= k
     else:
-        return transpose(b) # no need to copy
+        raise ValueError("Invalid ind argument.")
+    a = a.reshape(prod, -1)
+    ia = inv(a)
+    return ia.reshape(*invshape)
 
 
 # Matrix inversion
 
+def _unary_dispatcher(a):
+    return (a,)
+
+
+@array_function_dispatch(_unary_dispatcher)
 def inv(a):
+    """
+    Compute the (multiplicative) inverse of a matrix.
+
+    Given a square matrix `a`, return the matrix `ainv` satisfying
+    ``dot(a, ainv) = dot(ainv, a) = eye(a.shape[0])``.
+
+    Parameters
+    ----------
+    a : (..., M, M) array_like
+        Matrix to be inverted.
+
+    Returns
+    -------
+    ainv : (..., M, M) ndarray or matrix
+        (Multiplicative) inverse of the matrix `a`.
+
+    Raises
+    ------
+    LinAlgError
+        If `a` is not square or inversion fails.
+
+    See Also
+    --------
+    scipy.linalg.inv : Similar function in SciPy.
+
+    Notes
+    -----
+
+    .. versionadded:: 1.8.0
+
+    Broadcasting rules apply, see the `numpy.linalg` documentation for
+    details.
+
+    Examples
+    --------
+    >>> from numpy.linalg import inv
+    >>> a = np.array([[1., 2.], [3., 4.]])
+    >>> ainv = inv(a)
+    >>> np.allclose(np.dot(a, ainv), np.eye(2))
+    True
+    >>> np.allclose(np.dot(ainv, a), np.eye(2))
+    True
+
+    If a is a matrix object, then the return value is a matrix as well:
+
+    >>> ainv = inv(np.matrix(a))
+    >>> ainv
+    matrix([[-2. ,  1. ],
+            [ 1.5, -0.5]])
+
+    Inverses of several matrices can be computed at once:
+
+    >>> a = np.array([[[1., 2.], [3., 4.]], [[1, 3], [3, 5]]])
+    >>> inv(a)
+    array([[[-2.  ,  1.  ],
+            [ 1.5 , -0.5 ]],
+           [[-1.25,  0.75],
+            [ 0.75, -0.25]]])
+
+    """
     a, wrap = _makearray(a)
-    return wrap(solve(a, identity(a.shape[0])))
+    _assert_stacked_2d(a)
+    _assert_stacked_square(a)
+    t, result_t = _commonType(a)
+
+    signature = 'D->D' if isComplexType(t) else 'd->d'
+    extobj = get_linalg_error_extobj(_raise_linalgerror_singular)
+    ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj)
+    return wrap(ainv.astype(result_t, copy=False))
+
+
+def _matrix_power_dispatcher(a, n):
+    return (a,)
+
+
+@array_function_dispatch(_matrix_power_dispatcher)
+def matrix_power(a, n):
+    """
+    Raise a square matrix to the (integer) power `n`.
+
+    For positive integers `n`, the power is computed by repeated matrix
+    squarings and matrix multiplications. If ``n == 0``, the identity matrix
+    of the same shape as M is returned. If ``n < 0``, the inverse
+    is computed and then raised to the ``abs(n)``.
+
+    .. note:: Stacks of object matrices are not currently supported.
+
+    Parameters
+    ----------
+    a : (..., M, M) array_like
+        Matrix to be "powered".
+    n : int
+        The exponent can be any integer or long integer, positive,
+        negative, or zero.
+
+    Returns
+    -------
+    a**n : (..., M, M) ndarray or matrix object
+        The return value is the same shape and type as `M`;
+        if the exponent is positive or zero then the type of the
+        elements is the same as those of `M`. If the exponent is
+        negative the elements are floating-point.
+
+    Raises
+    ------
+    LinAlgError
+        For matrices that are not square or that (for negative powers) cannot
+        be inverted numerically.
+
+    Examples
+    --------
+    >>> from numpy.linalg import matrix_power
+    >>> i = np.array([[0, 1], [-1, 0]]) # matrix equiv. of the imaginary unit
+    >>> matrix_power(i, 3) # should = -i
+    array([[ 0, -1],
+           [ 1,  0]])
+    >>> matrix_power(i, 0)
+    array([[1, 0],
+           [0, 1]])
+    >>> matrix_power(i, -3) # should = 1/(-i) = i, but w/ f.p. elements
+    array([[ 0.,  1.],
+           [-1.,  0.]])
+
+    Somewhat more sophisticated example
+
+    >>> q = np.zeros((4, 4))
+    >>> q[0:2, 0:2] = -i
+    >>> q[2:4, 2:4] = i
+    >>> q # one of the three quaternion units not equal to 1
+    array([[ 0., -1.,  0.,  0.],
+           [ 1.,  0.,  0.,  0.],
+           [ 0.,  0.,  0.,  1.],
+           [ 0.,  0., -1.,  0.]])
+    >>> matrix_power(q, 2) # = -np.eye(4)
+    array([[-1.,  0.,  0.,  0.],
+           [ 0., -1.,  0.,  0.],
+           [ 0.,  0., -1.,  0.],
+           [ 0.,  0.,  0., -1.]])
+
+    """
+    a = asanyarray(a)
+    _assert_stacked_2d(a)
+    _assert_stacked_square(a)
+
+    try:
+        n = operator.index(n)
+    except TypeError as e:
+        raise TypeError("exponent must be an integer") from e
+
+    # Fall back on dot for object arrays. Object arrays are not supported by
+    # the current implementation of matmul using einsum
+    if a.dtype != object:
+        fmatmul = matmul
+    elif a.ndim == 2:
+        fmatmul = dot
+    else:
+        raise NotImplementedError(
+            "matrix_power not supported for stacks of object arrays")
+
+    if n == 0:
+        a = empty_like(a)
+        a[...] = eye(a.shape[-2], dtype=a.dtype)
+        return a
+
+    elif n < 0:
+        a = inv(a)
+        n = abs(n)
+
+    # short-cuts.
+    if n == 1:
+        return a
+
+    elif n == 2:
+        return fmatmul(a, a)
+
+    elif n == 3:
+        return fmatmul(fmatmul(a, a), a)
+
+    # Use binary decomposition to reduce the number of matrix multiplications.
+    # Here, we iterate over the bits of n, from LSB to MSB, raise `a` to
+    # increasing powers of 2, and multiply into the result as needed.
+    z = result = None
+    while n > 0:
+        z = a if z is None else fmatmul(z, z)
+        n, bit = divmod(n, 2)
+        if bit:
+            result = z if result is None else fmatmul(result, z)
+
+    return result
+
 
 # Cholesky decomposition
 
+
+@array_function_dispatch(_unary_dispatcher)
 def cholesky(a):
-    _assertRank2(a)
-    _assertSquareness(a)
-    t =_commonType(a)
-    a = _castCopyAndTranspose(t, a)
-    m = a.shape[0]
-    n = a.shape[1]
-    if _array_kind[t] == 1:
-        lapack_routine = lapack_lite.zpotrf
+    """
+    Cholesky decomposition.
+
+    Return the Cholesky decomposition, `L * L.H`, of the square matrix `a`,
+    where `L` is lower-triangular and .H is the conjugate transpose operator
+    (which is the ordinary transpose if `a` is real-valued).  `a` must be
+    Hermitian (symmetric if real-valued) and positive-definite. No
+    checking is performed to verify whether `a` is Hermitian or not.
+    In addition, only the lower-triangular and diagonal elements of `a`
+    are used. Only `L` is actually returned.
+
+    Parameters
+    ----------
+    a : (..., M, M) array_like
+        Hermitian (symmetric if all elements are real), positive-definite
+        input matrix.
+
+    Returns
+    -------
+    L : (..., M, M) array_like
+        Lower-triangular Cholesky factor of `a`.  Returns a matrix object if
+        `a` is a matrix object.
+
+    Raises
+    ------
+    LinAlgError
+       If the decomposition fails, for example, if `a` is not
+       positive-definite.
+
+    See Also
+    --------
+    scipy.linalg.cholesky : Similar function in SciPy.
+    scipy.linalg.cholesky_banded : Cholesky decompose a banded Hermitian
+                                   positive-definite matrix.
+    scipy.linalg.cho_factor : Cholesky decomposition of a matrix, to use in
+                              `scipy.linalg.cho_solve`.
+
+    Notes
+    -----
+
+    .. versionadded:: 1.8.0
+
+    Broadcasting rules apply, see the `numpy.linalg` documentation for
+    details.
+
+    The Cholesky decomposition is often used as a fast way of solving
+
+    .. math:: A \\mathbf{x} = \\mathbf{b}
+
+    (when `A` is both Hermitian/symmetric and positive-definite).
+
+    First, we solve for :math:`\\mathbf{y}` in
+
+    .. math:: L \\mathbf{y} = \\mathbf{b},
+
+    and then for :math:`\\mathbf{x}` in
+
+    .. math:: L.H \\mathbf{x} = \\mathbf{y}.
+
+    Examples
+    --------
+    >>> A = np.array([[1,-2j],[2j,5]])
+    >>> A
+    array([[ 1.+0.j, -0.-2.j],
+           [ 0.+2.j,  5.+0.j]])
+    >>> L = np.linalg.cholesky(A)
+    >>> L
+    array([[1.+0.j, 0.+0.j],
+           [0.+2.j, 1.+0.j]])
+    >>> np.dot(L, L.T.conj()) # verify that L * L.H = A
+    array([[1.+0.j, 0.-2.j],
+           [0.+2.j, 5.+0.j]])
+    >>> A = [[1,-2j],[2j,5]] # what happens if A is only array_like?
+    >>> np.linalg.cholesky(A) # an ndarray object is returned
+    array([[1.+0.j, 0.+0.j],
+           [0.+2.j, 1.+0.j]])
+    >>> # But a matrix object is returned if A is a matrix object
+    >>> np.linalg.cholesky(np.matrix(A))
+    matrix([[ 1.+0.j,  0.+0.j],
+            [ 0.+2.j,  1.+0.j]])
+
+    """
+    extobj = get_linalg_error_extobj(_raise_linalgerror_nonposdef)
+    gufunc = _umath_linalg.cholesky_lo
+    a, wrap = _makearray(a)
+    _assert_stacked_2d(a)
+    _assert_stacked_square(a)
+    t, result_t = _commonType(a)
+    signature = 'D->D' if isComplexType(t) else 'd->d'
+    r = gufunc(a, signature=signature, extobj=extobj)
+    return wrap(r.astype(result_t, copy=False))
+
+
+# QR decomposition
+
+def _qr_dispatcher(a, mode=None):
+    return (a,)
+
+
+@array_function_dispatch(_qr_dispatcher)
+def qr(a, mode='reduced'):
+    """
+    Compute the qr factorization of a matrix.
+
+    Factor the matrix `a` as *qr*, where `q` is orthonormal and `r` is
+    upper-triangular.
+
+    Parameters
+    ----------
+    a : array_like, shape (..., M, N)
+        An array-like object with the dimensionality of at least 2.
+    mode : {'reduced', 'complete', 'r', 'raw'}, optional
+        If K = min(M, N), then
+
+        * 'reduced'  : returns q, r with dimensions
+                       (..., M, K), (..., K, N) (default)
+        * 'complete' : returns q, r with dimensions (..., M, M), (..., M, N)
+        * 'r'        : returns r only with dimensions (..., K, N)
+        * 'raw'      : returns h, tau with dimensions (..., N, M), (..., K,)
+
+        The options 'reduced', 'complete, and 'raw' are new in numpy 1.8,
+        see the notes for more information. The default is 'reduced', and to
+        maintain backward compatibility with earlier versions of numpy both
+        it and the old default 'full' can be omitted. Note that array h
+        returned in 'raw' mode is transposed for calling Fortran. The
+        'economic' mode is deprecated.  The modes 'full' and 'economic' may
+        be passed using only the first letter for backwards compatibility,
+        but all others must be spelled out. See the Notes for more
+        explanation.
+
+
+    Returns
+    -------
+    q : ndarray of float or complex, optional
+        A matrix with orthonormal columns. When mode = 'complete' the
+        result is an orthogonal/unitary matrix depending on whether or not
+        a is real/complex. The determinant may be either +/- 1 in that
+        case. In case the number of dimensions in the input array is
+        greater than 2 then a stack of the matrices with above properties
+        is returned.
+    r : ndarray of float or complex, optional
+        The upper-triangular matrix or a stack of upper-triangular
+        matrices if the number of dimensions in the input array is greater
+        than 2.
+    (h, tau) : ndarrays of np.double or np.cdouble, optional
+        The array h contains the Householder reflectors that generate q
+        along with r. The tau array contains scaling factors for the
+        reflectors. In the deprecated  'economic' mode only h is returned.
+
+    Raises
+    ------
+    LinAlgError
+        If factoring fails.
+
+    See Also
+    --------
+    scipy.linalg.qr : Similar function in SciPy.
+    scipy.linalg.rq : Compute RQ decomposition of a matrix.
+
+    Notes
+    -----
+    This is an interface to the LAPACK routines ``dgeqrf``, ``zgeqrf``,
+    ``dorgqr``, and ``zungqr``.
+
+    For more information on the qr factorization, see for example:
+    https://en.wikipedia.org/wiki/QR_factorization
+
+    Subclasses of `ndarray` are preserved except for the 'raw' mode. So if
+    `a` is of type `matrix`, all the return values will be matrices too.
+
+    New 'reduced', 'complete', and 'raw' options for mode were added in
+    NumPy 1.8.0 and the old option 'full' was made an alias of 'reduced'.  In
+    addition the options 'full' and 'economic' were deprecated.  Because
+    'full' was the previous default and 'reduced' is the new default,
+    backward compatibility can be maintained by letting `mode` default.
+    The 'raw' option was added so that LAPACK routines that can multiply
+    arrays by q using the Householder reflectors can be used. Note that in
+    this case the returned arrays are of type np.double or np.cdouble and
+    the h array is transposed to be FORTRAN compatible.  No routines using
+    the 'raw' return are currently exposed by numpy, but some are available
+    in lapack_lite and just await the necessary work.
+
+    Examples
+    --------
+    >>> a = np.random.randn(9, 6)
+    >>> q, r = np.linalg.qr(a)
+    >>> np.allclose(a, np.dot(q, r))  # a does equal qr
+    True
+    >>> r2 = np.linalg.qr(a, mode='r')
+    >>> np.allclose(r, r2)  # mode='r' returns the same r as mode='full'
+    True
+    >>> a = np.random.normal(size=(3, 2, 2)) # Stack of 2 x 2 matrices as input
+    >>> q, r = np.linalg.qr(a)
+    >>> q.shape
+    (3, 2, 2)
+    >>> r.shape
+    (3, 2, 2)
+    >>> np.allclose(a, np.matmul(q, r))
+    True
+
+    Example illustrating a common use of `qr`: solving of least squares
+    problems
+
+    What are the least-squares-best `m` and `y0` in ``y = y0 + mx`` for
+    the following data: {(0,1), (1,0), (1,2), (2,1)}. (Graph the points
+    and you'll see that it should be y0 = 0, m = 1.)  The answer is provided
+    by solving the over-determined matrix equation ``Ax = b``, where::
+
+      A = array([[0, 1], [1, 1], [1, 1], [2, 1]])
+      x = array([[y0], [m]])
+      b = array([[1], [0], [2], [1]])
+
+    If A = qr such that q is orthonormal (which is always possible via
+    Gram-Schmidt), then ``x = inv(r) * (q.T) * b``.  (In numpy practice,
+    however, we simply use `lstsq`.)
+
+    >>> A = np.array([[0, 1], [1, 1], [1, 1], [2, 1]])
+    >>> A
+    array([[0, 1],
+           [1, 1],
+           [1, 1],
+           [2, 1]])
+    >>> b = np.array([1, 2, 2, 3])
+    >>> q, r = np.linalg.qr(A)
+    >>> p = np.dot(q.T, b)
+    >>> np.dot(np.linalg.inv(r), p)
+    array([  1.,   1.])
+
+    """
+    if mode not in ('reduced', 'complete', 'r', 'raw'):
+        if mode in ('f', 'full'):
+            # 2013-04-01, 1.8
+            msg = "".join((
+                    "The 'full' option is deprecated in favor of 'reduced'.\n",
+                    "For backward compatibility let mode default."))
+            warnings.warn(msg, DeprecationWarning, stacklevel=3)
+            mode = 'reduced'
+        elif mode in ('e', 'economic'):
+            # 2013-04-01, 1.8
+            msg = "The 'economic' option is deprecated."
+            warnings.warn(msg, DeprecationWarning, stacklevel=3)
+            mode = 'economic'
+        else:
+            raise ValueError(f"Unrecognized mode '{mode}'")
+
+    a, wrap = _makearray(a)
+    _assert_stacked_2d(a)
+    m, n = a.shape[-2:]
+    t, result_t = _commonType(a)
+    a = a.astype(t, copy=True)
+    a = _to_native_byte_order(a)
+    mn = min(m, n)
+
+    if m <= n:
+        gufunc = _umath_linalg.qr_r_raw_m
     else:
-        lapack_routine = lapack_lite.dpotrf
-    results = lapack_routine('L', n, a, m, 0)
-    if results['info'] > 0:
-        raise LinAlgError, 'Matrix is not positive definite - Cholesky decomposition cannot be computed'
-    return transpose(triu(a,k=0)).copy()
-
+        gufunc = _umath_linalg.qr_r_raw_n
+
+    signature = 'D->D' if isComplexType(t) else 'd->d'
+    extobj = get_linalg_error_extobj(_raise_linalgerror_qr)
+    tau = gufunc(a, signature=signature, extobj=extobj)
+
+    # handle modes that don't return q
+    if mode == 'r':
+        r = triu(a[..., :mn, :])
+        r = r.astype(result_t, copy=False)
+        return wrap(r)
+
+    if mode == 'raw':
+        q = transpose(a)
+        q = q.astype(result_t, copy=False)
+        tau = tau.astype(result_t, copy=False)
+        return wrap(q), tau
+
+    if mode == 'economic':
+        a = a.astype(result_t, copy=False)
+        return wrap(a)
+
+    # mc is the number of columns in the resulting q
+    # matrix. If the mode is complete then it is 
+    # same as number of rows, and if the mode is reduced,
+    # then it is the minimum of number of rows and columns.
+    if mode == 'complete' and m > n:
+        mc = m
+        gufunc = _umath_linalg.qr_complete
+    else:
+        mc = mn
+        gufunc = _umath_linalg.qr_reduced
+
+    signature = 'DD->D' if isComplexType(t) else 'dd->d'
+    extobj = get_linalg_error_extobj(_raise_linalgerror_qr)
+    q = gufunc(a, tau, signature=signature, extobj=extobj)
+    r = triu(a[..., :mc, :])
+
+    q = q.astype(result_t, copy=False)
+    r = r.astype(result_t, copy=False)
+
+    return wrap(q), wrap(r)
 
 # Eigenvalues
+
+
+@array_function_dispatch(_unary_dispatcher)
 def eigvals(a):
-    _assertRank2(a)
-    _assertSquareness(a)
-    t =_commonType(a)
-    real_t = _array_type[0][_array_precision[t]]
-    a = _fastCopyAndTranspose(t, a)
-    n = a.shape[0]
-    dummy = zeros((1,), t)
-    if _array_kind[t] == 1: # Complex routines take different arguments
-        lapack_routine = lapack_lite.zgeev
-        w = zeros((n,), t)
-        rwork = zeros((n,),real_t)
-        lwork = 1
-        work = zeros((lwork,), t)
-        results = lapack_routine('N', 'N', n, a, n, w,
-                                 dummy, 1, dummy, 1, work, -1, rwork, 0)
-        lwork = int(abs(work[0]))
-        work = zeros((lwork,), t)
-        results = lapack_routine('N', 'N', n, a, n, w,
-                                 dummy, 1, dummy, 1, work, lwork, rwork, 0)
+    """
+    Compute the eigenvalues of a general matrix.
+
+    Main difference between `eigvals` and `eig`: the eigenvectors aren't
+    returned.
+
+    Parameters
+    ----------
+    a : (..., M, M) array_like
+        A complex- or real-valued matrix whose eigenvalues will be computed.
+
+    Returns
+    -------
+    w : (..., M,) ndarray
+        The eigenvalues, each repeated according to its multiplicity.
+        They are not necessarily ordered, nor are they necessarily
+        real for real matrices.
+
+    Raises
+    ------
+    LinAlgError
+        If the eigenvalue computation does not converge.
+
+    See Also
+    --------
+    eig : eigenvalues and right eigenvectors of general arrays
+    eigvalsh : eigenvalues of real symmetric or complex Hermitian
+               (conjugate symmetric) arrays.
+    eigh : eigenvalues and eigenvectors of real symmetric or complex
+           Hermitian (conjugate symmetric) arrays.
+    scipy.linalg.eigvals : Similar function in SciPy.
+
+    Notes
+    -----
+
+    .. versionadded:: 1.8.0
+
+    Broadcasting rules apply, see the `numpy.linalg` documentation for
+    details.
+
+    This is implemented using the ``_geev`` LAPACK routines which compute
+    the eigenvalues and eigenvectors of general square arrays.
+
+    Examples
+    --------
+    Illustration, using the fact that the eigenvalues of a diagonal matrix
+    are its diagonal elements, that multiplying a matrix on the left
+    by an orthogonal matrix, `Q`, and on the right by `Q.T` (the transpose
+    of `Q`), preserves the eigenvalues of the "middle" matrix.  In other words,
+    if `Q` is orthogonal, then ``Q * A * Q.T`` has the same eigenvalues as
+    ``A``:
+
+    >>> from numpy import linalg as LA
+    >>> x = np.random.random()
+    >>> Q = np.array([[np.cos(x), -np.sin(x)], [np.sin(x), np.cos(x)]])
+    >>> LA.norm(Q[0, :]), LA.norm(Q[1, :]), np.dot(Q[0, :],Q[1, :])
+    (1.0, 1.0, 0.0)
+
+    Now multiply a diagonal matrix by ``Q`` on one side and by ``Q.T`` on the other:
+
+    >>> D = np.diag((-1,1))
+    >>> LA.eigvals(D)
+    array([-1.,  1.])
+    >>> A = np.dot(Q, D)
+    >>> A = np.dot(A, Q.T)
+    >>> LA.eigvals(A)
+    array([ 1., -1.]) # random
+
+    """
+    a, wrap = _makearray(a)
+    _assert_stacked_2d(a)
+    _assert_stacked_square(a)
+    _assert_finite(a)
+    t, result_t = _commonType(a)
+
+    extobj = get_linalg_error_extobj(
+        _raise_linalgerror_eigenvalues_nonconvergence)
+    signature = 'D->D' if isComplexType(t) else 'd->D'
+    w = _umath_linalg.eigvals(a, signature=signature, extobj=extobj)
+
+    if not isComplexType(t):
+        if all(w.imag == 0):
+            w = w.real
+            result_t = _realType(result_t)
+        else:
+            result_t = _complexType(result_t)
+
+    return w.astype(result_t, copy=False)
+
+
+def _eigvalsh_dispatcher(a, UPLO=None):
+    return (a,)
+
+
+@array_function_dispatch(_eigvalsh_dispatcher)
+def eigvalsh(a, UPLO='L'):
+    """
+    Compute the eigenvalues of a complex Hermitian or real symmetric matrix.
+
+    Main difference from eigh: the eigenvectors are not computed.
+
+    Parameters
+    ----------
+    a : (..., M, M) array_like
+        A complex- or real-valued matrix whose eigenvalues are to be
+        computed.
+    UPLO : {'L', 'U'}, optional
+        Specifies whether the calculation is done with the lower triangular
+        part of `a` ('L', default) or the upper triangular part ('U').
+        Irrespective of this value only the real parts of the diagonal will
+        be considered in the computation to preserve the notion of a Hermitian
+        matrix. It therefore follows that the imaginary part of the diagonal
+        will always be treated as zero.
+
+    Returns
+    -------
+    w : (..., M,) ndarray
+        The eigenvalues in ascending order, each repeated according to
+        its multiplicity.
+
+    Raises
+    ------
+    LinAlgError
+        If the eigenvalue computation does not converge.
+
+    See Also
+    --------
+    eigh : eigenvalues and eigenvectors of real symmetric or complex Hermitian
+           (conjugate symmetric) arrays.
+    eigvals : eigenvalues of general real or complex arrays.
+    eig : eigenvalues and right eigenvectors of general real or complex
+          arrays.
+    scipy.linalg.eigvalsh : Similar function in SciPy.
+
+    Notes
+    -----
+
+    .. versionadded:: 1.8.0
+
+    Broadcasting rules apply, see the `numpy.linalg` documentation for
+    details.
+
+    The eigenvalues are computed using LAPACK routines ``_syevd``, ``_heevd``.
+
+    Examples
+    --------
+    >>> from numpy import linalg as LA
+    >>> a = np.array([[1, -2j], [2j, 5]])
+    >>> LA.eigvalsh(a)
+    array([ 0.17157288,  5.82842712]) # may vary
+
+    >>> # demonstrate the treatment of the imaginary part of the diagonal
+    >>> a = np.array([[5+2j, 9-2j], [0+2j, 2-1j]])
+    >>> a
+    array([[5.+2.j, 9.-2.j],
+           [0.+2.j, 2.-1.j]])
+    >>> # with UPLO='L' this is numerically equivalent to using LA.eigvals()
+    >>> # with:
+    >>> b = np.array([[5.+0.j, 0.-2.j], [0.+2.j, 2.-0.j]])
+    >>> b
+    array([[5.+0.j, 0.-2.j],
+           [0.+2.j, 2.+0.j]])
+    >>> wa = LA.eigvalsh(a)
+    >>> wb = LA.eigvals(b)
+    >>> wa; wb
+    array([1., 6.])
+    array([6.+0.j, 1.+0.j])
+
+    """
+    UPLO = UPLO.upper()
+    if UPLO not in ('L', 'U'):
+        raise ValueError("UPLO argument must be 'L' or 'U'")
+
+    extobj = get_linalg_error_extobj(
+        _raise_linalgerror_eigenvalues_nonconvergence)
+    if UPLO == 'L':
+        gufunc = _umath_linalg.eigvalsh_lo
     else:
-        lapack_routine = lapack_lite.dgeev
-        wr = zeros((n,), t)
-        wi = zeros((n,), t)
-        lwork = 1
-        work = zeros((lwork,), t)
-        results = lapack_routine('N', 'N', n, a, n, wr, wi,
-                                 dummy, 1, dummy, 1, work, -1, 0)
-        lwork = int(work[0])
-        work = zeros((lwork,), t)
-        results = lapack_routine('N', 'N', n, a, n, wr, wi,
-                                 dummy, 1, dummy, 1, work, lwork, 0)
-        if logical_and.reduce(equal(wi, 0.)):
-            w = wr
+        gufunc = _umath_linalg.eigvalsh_up
+
+    a, wrap = _makearray(a)
+    _assert_stacked_2d(a)
+    _assert_stacked_square(a)
+    t, result_t = _commonType(a)
+    signature = 'D->d' if isComplexType(t) else 'd->d'
+    w = gufunc(a, signature=signature, extobj=extobj)
+    return w.astype(_realType(result_t), copy=False)
+
+def _convertarray(a):
+    t, result_t = _commonType(a)
+    a = _fastCT(a.astype(t))
+    return a, t, result_t
+
+
+# Eigenvectors
+
+
+@array_function_dispatch(_unary_dispatcher)
+def eig(a):
+    """
+    Compute the eigenvalues and right eigenvectors of a square array.
+
+    Parameters
+    ----------
+    a : (..., M, M) array
+        Matrices for which the eigenvalues and right eigenvectors will
+        be computed
+
+    Returns
+    -------
+    w : (..., M) array
+        The eigenvalues, each repeated according to its multiplicity.
+        The eigenvalues are not necessarily ordered. The resulting
+        array will be of complex type, unless the imaginary part is
+        zero in which case it will be cast to a real type. When `a`
+        is real the resulting eigenvalues will be real (0 imaginary
+        part) or occur in conjugate pairs
+
+    v : (..., M, M) array
+        The normalized (unit "length") eigenvectors, such that the
+        column ``v[:,i]`` is the eigenvector corresponding to the
+        eigenvalue ``w[i]``.
+
+    Raises
+    ------
+    LinAlgError
+        If the eigenvalue computation does not converge.
+
+    See Also
+    --------
+    eigvals : eigenvalues of a non-symmetric array.
+    eigh : eigenvalues and eigenvectors of a real symmetric or complex
+           Hermitian (conjugate symmetric) array.
+    eigvalsh : eigenvalues of a real symmetric or complex Hermitian
+               (conjugate symmetric) array.
+    scipy.linalg.eig : Similar function in SciPy that also solves the
+                       generalized eigenvalue problem.
+    scipy.linalg.schur : Best choice for unitary and other non-Hermitian
+                         normal matrices.
+
+    Notes
+    -----
+
+    .. versionadded:: 1.8.0
+
+    Broadcasting rules apply, see the `numpy.linalg` documentation for
+    details.
+
+    This is implemented using the ``_geev`` LAPACK routines which compute
+    the eigenvalues and eigenvectors of general square arrays.
+
+    The number `w` is an eigenvalue of `a` if there exists a vector
+    `v` such that ``a @ v = w * v``. Thus, the arrays `a`, `w`, and
+    `v` satisfy the equations ``a @ v[:,i] = w[i] * v[:,i]``
+    for :math:`i \\in \\{0,...,M-1\\}`.
+
+    The array `v` of eigenvectors may not be of maximum rank, that is, some
+    of the columns may be linearly dependent, although round-off error may
+    obscure that fact. If the eigenvalues are all different, then theoretically
+    the eigenvectors are linearly independent and `a` can be diagonalized by
+    a similarity transformation using `v`, i.e, ``inv(v) @ a @ v`` is diagonal.
+
+    For non-Hermitian normal matrices the SciPy function `scipy.linalg.schur`
+    is preferred because the matrix `v` is guaranteed to be unitary, which is
+    not the case when using `eig`. The Schur factorization produces an
+    upper triangular matrix rather than a diagonal matrix, but for normal
+    matrices only the diagonal of the upper triangular matrix is needed, the
+    rest is roundoff error.
+
+    Finally, it is emphasized that `v` consists of the *right* (as in
+    right-hand side) eigenvectors of `a`.  A vector `y` satisfying
+    ``y.T @ a = z * y.T`` for some number `z` is called a *left*
+    eigenvector of `a`, and, in general, the left and right eigenvectors
+    of a matrix are not necessarily the (perhaps conjugate) transposes
+    of each other.
+
+    References
+    ----------
+    G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando, FL,
+    Academic Press, Inc., 1980, Various pp.
+
+    Examples
+    --------
+    >>> from numpy import linalg as LA
+
+    (Almost) trivial example with real e-values and e-vectors.
+
+    >>> w, v = LA.eig(np.diag((1, 2, 3)))
+    >>> w; v
+    array([1., 2., 3.])
+    array([[1., 0., 0.],
+           [0., 1., 0.],
+           [0., 0., 1.]])
+
+    Real matrix possessing complex e-values and e-vectors; note that the
+    e-values are complex conjugates of each other.
+
+    >>> w, v = LA.eig(np.array([[1, -1], [1, 1]]))
+    >>> w; v
+    array([1.+1.j, 1.-1.j])
+    array([[0.70710678+0.j        , 0.70710678-0.j        ],
+           [0.        -0.70710678j, 0.        +0.70710678j]])
+
+    Complex-valued matrix with real e-values (but complex-valued e-vectors);
+    note that ``a.conj().T == a``, i.e., `a` is Hermitian.
+
+    >>> a = np.array([[1, 1j], [-1j, 1]])
+    >>> w, v = LA.eig(a)
+    >>> w; v
+    array([2.+0.j, 0.+0.j])
+    array([[ 0.        +0.70710678j,  0.70710678+0.j        ], # may vary
+           [ 0.70710678+0.j        , -0.        +0.70710678j]])
+
+    Be careful about round-off error!
+
+    >>> a = np.array([[1 + 1e-9, 0], [0, 1 - 1e-9]])
+    >>> # Theor. e-values are 1 +/- 1e-9
+    >>> w, v = LA.eig(a)
+    >>> w; v
+    array([1., 1.])
+    array([[1., 0.],
+           [0., 1.]])
+
+    """
+    a, wrap = _makearray(a)
+    _assert_stacked_2d(a)
+    _assert_stacked_square(a)
+    _assert_finite(a)
+    t, result_t = _commonType(a)
+
+    extobj = get_linalg_error_extobj(
+        _raise_linalgerror_eigenvalues_nonconvergence)
+    signature = 'D->DD' if isComplexType(t) else 'd->DD'
+    w, vt = _umath_linalg.eig(a, signature=signature, extobj=extobj)
+
+    if not isComplexType(t) and all(w.imag == 0.0):
+        w = w.real
+        vt = vt.real
+        result_t = _realType(result_t)
+    else:
+        result_t = _complexType(result_t)
+
+    vt = vt.astype(result_t, copy=False)
+    return w.astype(result_t, copy=False), wrap(vt)
+
+
+@array_function_dispatch(_eigvalsh_dispatcher)
+def eigh(a, UPLO='L'):
+    """
+    Return the eigenvalues and eigenvectors of a complex Hermitian
+    (conjugate symmetric) or a real symmetric matrix.
+
+    Returns two objects, a 1-D array containing the eigenvalues of `a`, and
+    a 2-D square array or matrix (depending on the input type) of the
+    corresponding eigenvectors (in columns).
+
+    Parameters
+    ----------
+    a : (..., M, M) array
+        Hermitian or real symmetric matrices whose eigenvalues and
+        eigenvectors are to be computed.
+    UPLO : {'L', 'U'}, optional
+        Specifies whether the calculation is done with the lower triangular
+        part of `a` ('L', default) or the upper triangular part ('U').
+        Irrespective of this value only the real parts of the diagonal will
+        be considered in the computation to preserve the notion of a Hermitian
+        matrix. It therefore follows that the imaginary part of the diagonal
+        will always be treated as zero.
+
+    Returns
+    -------
+    w : (..., M) ndarray
+        The eigenvalues in ascending order, each repeated according to
+        its multiplicity.
+    v : {(..., M, M) ndarray, (..., M, M) matrix}
+        The column ``v[:, i]`` is the normalized eigenvector corresponding
+        to the eigenvalue ``w[i]``.  Will return a matrix object if `a` is
+        a matrix object.
+
+    Raises
+    ------
+    LinAlgError
+        If the eigenvalue computation does not converge.
+
+    See Also
+    --------
+    eigvalsh : eigenvalues of real symmetric or complex Hermitian
+               (conjugate symmetric) arrays.
+    eig : eigenvalues and right eigenvectors for non-symmetric arrays.
+    eigvals : eigenvalues of non-symmetric arrays.
+    scipy.linalg.eigh : Similar function in SciPy (but also solves the
+                        generalized eigenvalue problem).
+
+    Notes
+    -----
+
+    .. versionadded:: 1.8.0
+
+    Broadcasting rules apply, see the `numpy.linalg` documentation for
+    details.
+
+    The eigenvalues/eigenvectors are computed using LAPACK routines ``_syevd``,
+    ``_heevd``.
+
+    The eigenvalues of real symmetric or complex Hermitian matrices are
+    always real. [1]_ The array `v` of (column) eigenvectors is unitary
+    and `a`, `w`, and `v` satisfy the equations
+    ``dot(a, v[:, i]) = w[i] * v[:, i]``.
+
+    References
+    ----------
+    .. [1] G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando,
+           FL, Academic Press, Inc., 1980, pg. 222.
+
+    Examples
+    --------
+    >>> from numpy import linalg as LA
+    >>> a = np.array([[1, -2j], [2j, 5]])
+    >>> a
+    array([[ 1.+0.j, -0.-2.j],
+           [ 0.+2.j,  5.+0.j]])
+    >>> w, v = LA.eigh(a)
+    >>> w; v
+    array([0.17157288, 5.82842712])
+    array([[-0.92387953+0.j        , -0.38268343+0.j        ], # may vary
+           [ 0.        +0.38268343j,  0.        -0.92387953j]])
+
+    >>> np.dot(a, v[:, 0]) - w[0] * v[:, 0] # verify 1st e-val/vec pair
+    array([5.55111512e-17+0.0000000e+00j, 0.00000000e+00+1.2490009e-16j])
+    >>> np.dot(a, v[:, 1]) - w[1] * v[:, 1] # verify 2nd e-val/vec pair
+    array([0.+0.j, 0.+0.j])
+
+    >>> A = np.matrix(a) # what happens if input is a matrix object
+    >>> A
+    matrix([[ 1.+0.j, -0.-2.j],
+            [ 0.+2.j,  5.+0.j]])
+    >>> w, v = LA.eigh(A)
+    >>> w; v
+    array([0.17157288, 5.82842712])
+    matrix([[-0.92387953+0.j        , -0.38268343+0.j        ], # may vary
+            [ 0.        +0.38268343j,  0.        -0.92387953j]])
+
+    >>> # demonstrate the treatment of the imaginary part of the diagonal
+    >>> a = np.array([[5+2j, 9-2j], [0+2j, 2-1j]])
+    >>> a
+    array([[5.+2.j, 9.-2.j],
+           [0.+2.j, 2.-1.j]])
+    >>> # with UPLO='L' this is numerically equivalent to using LA.eig() with:
+    >>> b = np.array([[5.+0.j, 0.-2.j], [0.+2.j, 2.-0.j]])
+    >>> b
+    array([[5.+0.j, 0.-2.j],
+           [0.+2.j, 2.+0.j]])
+    >>> wa, va = LA.eigh(a)
+    >>> wb, vb = LA.eig(b)
+    >>> wa; wb
+    array([1., 6.])
+    array([6.+0.j, 1.+0.j])
+    >>> va; vb
+    array([[-0.4472136 +0.j        , -0.89442719+0.j        ], # may vary
+           [ 0.        +0.89442719j,  0.        -0.4472136j ]])
+    array([[ 0.89442719+0.j       , -0.        +0.4472136j],
+           [-0.        +0.4472136j,  0.89442719+0.j       ]])
+    """
+    UPLO = UPLO.upper()
+    if UPLO not in ('L', 'U'):
+        raise ValueError("UPLO argument must be 'L' or 'U'")
+
+    a, wrap = _makearray(a)
+    _assert_stacked_2d(a)
+    _assert_stacked_square(a)
+    t, result_t = _commonType(a)
+
+    extobj = get_linalg_error_extobj(
+        _raise_linalgerror_eigenvalues_nonconvergence)
+    if UPLO == 'L':
+        gufunc = _umath_linalg.eigh_lo
+    else:
+        gufunc = _umath_linalg.eigh_up
+
+    signature = 'D->dD' if isComplexType(t) else 'd->dd'
+    w, vt = gufunc(a, signature=signature, extobj=extobj)
+    w = w.astype(_realType(result_t), copy=False)
+    vt = vt.astype(result_t, copy=False)
+    return w, wrap(vt)
+
+
+# Singular value decomposition
+
+def _svd_dispatcher(a, full_matrices=None, compute_uv=None, hermitian=None):
+    return (a,)
+
+
+@array_function_dispatch(_svd_dispatcher)
+def svd(a, full_matrices=True, compute_uv=True, hermitian=False):
+    """
+    Singular Value Decomposition.
+
+    When `a` is a 2D array, and ``full_matrices=False``, then it is
+    factorized as ``u @ np.diag(s) @ vh = (u * s) @ vh``, where
+    `u` and the Hermitian transpose of `vh` are 2D arrays with
+    orthonormal columns and `s` is a 1D array of `a`'s singular
+    values. When `a` is higher-dimensional, SVD is applied in
+    stacked mode as explained below.
+
+    Parameters
+    ----------
+    a : (..., M, N) array_like
+        A real or complex array with ``a.ndim >= 2``.
+    full_matrices : bool, optional
+        If True (default), `u` and `vh` have the shapes ``(..., M, M)`` and
+        ``(..., N, N)``, respectively.  Otherwise, the shapes are
+        ``(..., M, K)`` and ``(..., K, N)``, respectively, where
+        ``K = min(M, N)``.
+    compute_uv : bool, optional
+        Whether or not to compute `u` and `vh` in addition to `s`.  True
+        by default.
+    hermitian : bool, optional
+        If True, `a` is assumed to be Hermitian (symmetric if real-valued),
+        enabling a more efficient method for finding singular values.
+        Defaults to False.
+
+        .. versionadded:: 1.17.0
+
+    Returns
+    -------
+    u : { (..., M, M), (..., M, K) } array
+        Unitary array(s). The first ``a.ndim - 2`` dimensions have the same
+        size as those of the input `a`. The size of the last two dimensions
+        depends on the value of `full_matrices`. Only returned when
+        `compute_uv` is True.
+    s : (..., K) array
+        Vector(s) with the singular values, within each vector sorted in
+        descending order. The first ``a.ndim - 2`` dimensions have the same
+        size as those of the input `a`.
+    vh : { (..., N, N), (..., K, N) } array
+        Unitary array(s). The first ``a.ndim - 2`` dimensions have the same
+        size as those of the input `a`. The size of the last two dimensions
+        depends on the value of `full_matrices`. Only returned when
+        `compute_uv` is True.
+
+    Raises
+    ------
+    LinAlgError
+        If SVD computation does not converge.
+
+    See Also
+    --------
+    scipy.linalg.svd : Similar function in SciPy.
+    scipy.linalg.svdvals : Compute singular values of a matrix.
+
+    Notes
+    -----
+
+    .. versionchanged:: 1.8.0
+       Broadcasting rules apply, see the `numpy.linalg` documentation for
+       details.
+
+    The decomposition is performed using LAPACK routine ``_gesdd``.
+
+    SVD is usually described for the factorization of a 2D matrix :math:`A`.
+    The higher-dimensional case will be discussed below. In the 2D case, SVD is
+    written as :math:`A = U S V^H`, where :math:`A = a`, :math:`U= u`,
+    :math:`S= \\mathtt{np.diag}(s)` and :math:`V^H = vh`. The 1D array `s`
+    contains the singular values of `a` and `u` and `vh` are unitary. The rows
+    of `vh` are the eigenvectors of :math:`A^H A` and the columns of `u` are
+    the eigenvectors of :math:`A A^H`. In both cases the corresponding
+    (possibly non-zero) eigenvalues are given by ``s**2``.
+
+    If `a` has more than two dimensions, then broadcasting rules apply, as
+    explained in :ref:`routines.linalg-broadcasting`. This means that SVD is
+    working in "stacked" mode: it iterates over all indices of the first
+    ``a.ndim - 2`` dimensions and for each combination SVD is applied to the
+    last two indices. The matrix `a` can be reconstructed from the
+    decomposition with either ``(u * s[..., None, :]) @ vh`` or
+    ``u @ (s[..., None] * vh)``. (The ``@`` operator can be replaced by the
+    function ``np.matmul`` for python versions below 3.5.)
+
+    If `a` is a ``matrix`` object (as opposed to an ``ndarray``), then so are
+    all the return values.
+
+    Examples
+    --------
+    >>> a = np.random.randn(9, 6) + 1j*np.random.randn(9, 6)
+    >>> b = np.random.randn(2, 7, 8, 3) + 1j*np.random.randn(2, 7, 8, 3)
+
+    Reconstruction based on full SVD, 2D case:
+
+    >>> u, s, vh = np.linalg.svd(a, full_matrices=True)
+    >>> u.shape, s.shape, vh.shape
+    ((9, 9), (6,), (6, 6))
+    >>> np.allclose(a, np.dot(u[:, :6] * s, vh))
+    True
+    >>> smat = np.zeros((9, 6), dtype=complex)
+    >>> smat[:6, :6] = np.diag(s)
+    >>> np.allclose(a, np.dot(u, np.dot(smat, vh)))
+    True
+
+    Reconstruction based on reduced SVD, 2D case:
+
+    >>> u, s, vh = np.linalg.svd(a, full_matrices=False)
+    >>> u.shape, s.shape, vh.shape
+    ((9, 6), (6,), (6, 6))
+    >>> np.allclose(a, np.dot(u * s, vh))
+    True
+    >>> smat = np.diag(s)
+    >>> np.allclose(a, np.dot(u, np.dot(smat, vh)))
+    True
+
+    Reconstruction based on full SVD, 4D case:
+
+    >>> u, s, vh = np.linalg.svd(b, full_matrices=True)
+    >>> u.shape, s.shape, vh.shape
+    ((2, 7, 8, 8), (2, 7, 3), (2, 7, 3, 3))
+    >>> np.allclose(b, np.matmul(u[..., :3] * s[..., None, :], vh))
+    True
+    >>> np.allclose(b, np.matmul(u[..., :3], s[..., None] * vh))
+    True
+
+    Reconstruction based on reduced SVD, 4D case:
+
+    >>> u, s, vh = np.linalg.svd(b, full_matrices=False)
+    >>> u.shape, s.shape, vh.shape
+    ((2, 7, 8, 3), (2, 7, 3), (2, 7, 3, 3))
+    >>> np.allclose(b, np.matmul(u * s[..., None, :], vh))
+    True
+    >>> np.allclose(b, np.matmul(u, s[..., None] * vh))
+    True
+
+    """
+    import numpy as _nx
+    a, wrap = _makearray(a)
+
+    if hermitian:
+        # note: lapack svd returns eigenvalues with s ** 2 sorted descending,
+        # but eig returns s sorted ascending, so we re-order the eigenvalues
+        # and related arrays to have the correct order
+        if compute_uv:
+            s, u = eigh(a)
+            sgn = sign(s)
+            s = abs(s)
+            sidx = argsort(s)[..., ::-1]
+            sgn = _nx.take_along_axis(sgn, sidx, axis=-1)
+            s = _nx.take_along_axis(s, sidx, axis=-1)
+            u = _nx.take_along_axis(u, sidx[..., None, :], axis=-1)
+            # singular values are unsigned, move the sign into v
+            vt = transpose(u * sgn[..., None, :]).conjugate()
+            return wrap(u), s, wrap(vt)
         else:
-            w = wr+1j*wi
-    if results['info'] > 0:
-        raise LinAlgError, 'Eigenvalues did not converge'
-    return w
-
-
-def eigvalsh(a, UPLO='L'):
-    _assertRank2(a)
-    _assertSquareness(a)
-    t =_commonType(a)
-    real_t = _array_type[0][_array_precision[t]]
-    a = _castCopyAndTranspose(t, a)
-    n = a.shape[0]
-    liwork = 5*n+3
-    iwork = zeros((liwork,),'i')
-    if _array_kind[t] == 1: # Complex routines take different arguments
-        lapack_routine = lapack_lite.zheevd
-        w = zeros((n,), real_t)
-        lwork = 1
-        work = zeros((lwork,), t)
-        lrwork = 1
-        rwork = zeros((lrwork,),real_t)
-        results = lapack_routine('N', UPLO, n, a, n,w, work, -1, rwork, -1, iwork, liwork,  0)
-        lwork = int(abs(work[0]))
-        work = zeros((lwork,), t)
-        lrwork = int(rwork[0])
-        rwork = zeros((lrwork,),real_t)
-        results = lapack_routine('N', UPLO, n, a, n,w, work, lwork, rwork, lrwork, iwork, liwork,  0)
-    else:
-        lapack_routine = lapack_lite.dsyevd
-        w = zeros((n,), t)
-        lwork = 1
-        work = zeros((lwork,), t)
-        results = lapack_routine('N', UPLO, n, a, n,w, work, -1, iwork, liwork, 0)
-        lwork = int(work[0])
-        work = zeros((lwork,), t)
-        results = lapack_routine('N', UPLO, n, a, n,w, work, lwork, iwork, liwork, 0)
-    if results['info'] > 0:
-        raise LinAlgError, 'Eigenvalues did not converge'
-    return w
-
-def _convertarray(a):
-    if issubclass(a.dtype.type, complexfloating):
-        if a.dtype.char == 'D':
-            a = _fastCT(a)
-        else:
-            a = _fastCT(a.astype('D'))
-    else:
-        if a.dtype.char == 'd':
-            a = _fastCT(a)
-        else:
-            a = _fastCT(a.astype('d'))
-    return a, a.dtype.char
-
-# Eigenvectors
-
-def eig(a):
-    """eig(a) returns u,v  where u is the eigenvalues and
-v is a matrix of eigenvectors with vector v[:,i] corresponds to
-eigenvalue u[i].  Satisfies the equation dot(a, v[:,i]) = u[i]*v[:,i]
-"""
-    a, wrap = _makearray(a)
-    _assertRank2(a)
-    _assertSquareness(a)
-    a,t = _convertarray(a) # convert to float_ or complex_ type
-    real_t = 'd'
-    n = a.shape[0]
-    dummy = zeros((1,), t)
-    if t == 'D': # Complex routines take different arguments
-        lapack_routine = lapack_lite.zgeev
-        w = zeros((n,), t)
-        v = zeros((n,n), t)
-        lwork = 1
-        work = zeros((lwork,),t)
-        rwork = zeros((2*n,),real_t)
-        results = lapack_routine('N', 'V', n, a, n, w,
-                                  dummy, 1, v, n, work, -1, rwork, 0)
-        lwork = int(abs(work[0]))
-        work = zeros((lwork,),t)
-        results = lapack_routine('N', 'V', n, a, n, w,
-                                  dummy, 1, v, n, work, lwork, rwork, 0)
-    else:
-        lapack_routine = lapack_lite.dgeev
-        wr = zeros((n,), t)
-        wi = zeros((n,), t)
-        vr = zeros((n,n), t)
-        lwork = 1
-        work = zeros((lwork,),t)
-        results = lapack_routine('N', 'V', n, a, n, wr, wi,
-                                  dummy, 1, vr, n, work, -1, 0)
-        lwork = int(work[0])
-        work = zeros((lwork,),t)
-        results = lapack_routine('N', 'V', n, a, n, wr, wi,
-                                  dummy, 1, vr, n, work, lwork, 0)
-        if logical_and.reduce(equal(wi, 0.)):
-            w = wr
-            v = vr
-        else:
-            w = wr+1j*wi
-            v = array(vr,Complex)
-            ind = nonzero(
-                          equal(
-                              equal(wi,0.0) # true for real e-vals
-                                       ,0)          # true for complex e-vals
-                                 )                  # indices of complex e-vals
-            for i in range(len(ind)/2):
-                v[ind[2*i]] = vr[ind[2*i]] + 1j*vr[ind[2*i+1]]
-                v[ind[2*i+1]] = vr[ind[2*i]] - 1j*vr[ind[2*i+1]]
-    if results['info'] > 0:
-        raise LinAlgError, 'Eigenvalues did not converge'
-    return w,wrap(v.transpose())
-
-
-def eigh(a, UPLO='L'):
-    a, wrap = _makearray(a)
-    _assertRank2(a)
-    _assertSquareness(a)
-    t =_commonType(a)
-    real_t = _array_type[0][_array_precision[t]]
-    a = _castCopyAndTranspose(t, a)
-    n = a.shape[0]
-    liwork = 5*n+3
-    iwork = zeros((liwork,),'i')
-    if _array_kind[t] == 1: # Complex routines take different arguments
-        lapack_routine = lapack_lite.zheevd
-        w = zeros((n,), real_t)
-        lwork = 1
-        work = zeros((lwork,), t)
-        lrwork = 1
-        rwork = zeros((lrwork,),real_t)
-        results = lapack_routine('V', UPLO, n, a, n,w, work, -1, rwork, -1, iwork, liwork,  0)
-        lwork = int(abs(work[0]))
-        work = zeros((lwork,), t)
-        lrwork = int(rwork[0])
-        rwork = zeros((lrwork,),real_t)
-        results = lapack_routine('V', UPLO, n, a, n,w, work, lwork, rwork, lrwork, iwork, liwork,  0)
-    else:
-        lapack_routine = lapack_lite.dsyevd
-        w = zeros((n,), t)
-        lwork = 1
-        work = zeros((lwork,),t)
-        results = lapack_routine('V', UPLO, n, a, n,w, work, -1, iwork, liwork, 0)
-        lwork = int(work[0])
-        work = zeros((lwork,),t)
-        results = lapack_routine('V', UPLO, n, a, n,w, work, lwork, iwork, liwork, 0)
-    if results['info'] > 0:
-        raise LinAlgError, 'Eigenvalues did not converge'
-    return w,wrap(a.transpose())
-
-
-# Singular value decomposition
-
-def svd(a, full_matrices=1, compute_uv=1):
-    a, wrap = _makearray(a)
-    _assertRank2(a)
-    m, n = a.shape
-    t =_commonType(a)
-    real_t = _array_type[0][_array_precision[t]]
-    a = _fastCopyAndTranspose(t, a)
-    s = zeros((min(n,m),), real_t)
+            s = eigvalsh(a)
+            s = s[..., ::-1]
+            s = abs(s)
+            return sort(s)[..., ::-1]
+
+    _assert_stacked_2d(a)
+    t, result_t = _commonType(a)
+
+    extobj = get_linalg_error_extobj(_raise_linalgerror_svd_nonconvergence)
+
+    m, n = a.shape[-2:]
     if compute_uv:
         if full_matrices:
-            nu = m
-            nvt = n
-            option = 'A'
+            if m < n:
+                gufunc = _umath_linalg.svd_m_f
+            else:
+                gufunc = _umath_linalg.svd_n_f
         else:
-            nu = min(n,m)
-            nvt = min(n,m)
-            option = 'S'
-        u = zeros((nu, m), t)
-        vt = zeros((n, nvt), t)
+            if m < n:
+                gufunc = _umath_linalg.svd_m_s
+            else:
+                gufunc = _umath_linalg.svd_n_s
+
+        signature = 'D->DdD' if isComplexType(t) else 'd->ddd'
+        u, s, vh = gufunc(a, signature=signature, extobj=extobj)
+        u = u.astype(result_t, copy=False)
+        s = s.astype(_realType(result_t), copy=False)
+        vh = vh.astype(result_t, copy=False)
+        return wrap(u), s, wrap(vh)
     else:
-        option = 'N'
-        nu = 1
-        nvt = 1
-        u = empty((1,1),t) 
-        vt = empty((1,1),t) 
-
-    iwork = zeros((8*min(m,n),), 'i')
-    if _array_kind[t] == 1: # Complex routines take different arguments
-        lapack_routine = lapack_lite.zgesdd
-        rwork = zeros((5*min(m,n)*min(m,n) + 5*min(m,n),), real_t)
-        lwork = 1
-        work = zeros((lwork,), t)
-        results = lapack_routine(option, m, n, a, m, s, u, m, vt, nvt,
-                                 work, -1, rwork, iwork, 0)
-        lwork = int(abs(work[0]))
-        work = zeros((lwork,), t)
-        results = lapack_routine(option, m, n, a, m, s, u, m, vt, nvt,
-                                 work, lwork, rwork, iwork, 0)
+        if m < n:
+            gufunc = _umath_linalg.svd_m
+        else:
+            gufunc = _umath_linalg.svd_n
+
+        signature = 'D->d' if isComplexType(t) else 'd->d'
+        s = gufunc(a, signature=signature, extobj=extobj)
+        s = s.astype(_realType(result_t), copy=False)
+        return s
+
+
+def _cond_dispatcher(x, p=None):
+    return (x,)
+
+
+@array_function_dispatch(_cond_dispatcher)
+def cond(x, p=None):
+    """
+    Compute the condition number of a matrix.
+
+    This function is capable of returning the condition number using
+    one of seven different norms, depending on the value of `p` (see
+    Parameters below).
+
+    Parameters
+    ----------
+    x : (..., M, N) array_like
+        The matrix whose condition number is sought.
+    p : {None, 1, -1, 2, -2, inf, -inf, 'fro'}, optional
+        Order of the norm used in the condition number computation:
+
+        =====  ============================
+        p      norm for matrices
+        =====  ============================
+        None   2-norm, computed directly using the ``SVD``
+        'fro'  Frobenius norm
+        inf    max(sum(abs(x), axis=1))
+        -inf   min(sum(abs(x), axis=1))
+        1      max(sum(abs(x), axis=0))
+        -1     min(sum(abs(x), axis=0))
+        2      2-norm (largest sing. value)
+        -2     smallest singular value
+        =====  ============================
+
+        inf means the `numpy.inf` object, and the Frobenius norm is
+        the root-of-sum-of-squares norm.
+
+    Returns
+    -------
+    c : {float, inf}
+        The condition number of the matrix. May be infinite.
+
+    See Also
+    --------
+    numpy.linalg.norm
+
+    Notes
+    -----
+    The condition number of `x` is defined as the norm of `x` times the
+    norm of the inverse of `x` [1]_; the norm can be the usual L2-norm
+    (root-of-sum-of-squares) or one of a number of other matrix norms.
+
+    References
+    ----------
+    .. [1] G. Strang, *Linear Algebra and Its Applications*, Orlando, FL,
+           Academic Press, Inc., 1980, pg. 285.
+
+    Examples
+    --------
+    >>> from numpy import linalg as LA
+    >>> a = np.array([[1, 0, -1], [0, 1, 0], [1, 0, 1]])
+    >>> a
+    array([[ 1,  0, -1],
+           [ 0,  1,  0],
+           [ 1,  0,  1]])
+    >>> LA.cond(a)
+    1.4142135623730951
+    >>> LA.cond(a, 'fro')
+    3.1622776601683795
+    >>> LA.cond(a, np.inf)
+    2.0
+    >>> LA.cond(a, -np.inf)
+    1.0
+    >>> LA.cond(a, 1)
+    2.0
+    >>> LA.cond(a, -1)
+    1.0
+    >>> LA.cond(a, 2)
+    1.4142135623730951
+    >>> LA.cond(a, -2)
+    0.70710678118654746 # may vary
+    >>> min(LA.svd(a, compute_uv=False))*min(LA.svd(LA.inv(a), compute_uv=False))
+    0.70710678118654746 # may vary
+
+    """
+    x = asarray(x)  # in case we have a matrix
+    if _is_empty_2d(x):
+        raise LinAlgError("cond is not defined on empty arrays")
+    if p is None or p == 2 or p == -2:
+        s = svd(x, compute_uv=False)
+        with errstate(all='ignore'):
+            if p == -2:
+                r = s[..., -1] / s[..., 0]
+            else:
+                r = s[..., 0] / s[..., -1]
     else:
-        lapack_routine = lapack_lite.dgesdd
-        lwork = 1
-        work = zeros((lwork,), t)
-        results = lapack_routine(option, m, n, a, m, s, u, m, vt, nvt,
-                                 work, -1, iwork, 0)
-        lwork = int(work[0])
-        if option == 'N' and lwork==1:
-            # there seems to be a bug in dgesdd of lapack
-            #   (NNemec, 060310)
-            # returning the wrong lwork size for option == 'N'
-            results = lapack_routine('A', m, n, a, m, s, u, m, vt, n,
-                                     work, -1, iwork, 0)
-            lwork = int(work[0])
-            assert lwork > 1
-
-        work = zeros((lwork,), t)
-        results = lapack_routine(option, m, n, a, m, s, u, m, vt, nvt,
-                                 work, lwork, iwork, 0)
-    if results['info'] > 0:
-        raise LinAlgError, 'SVD did not converge'
-    if compute_uv:
-        return wrap(transpose(u)), s, \
-               wrap(transpose(vt)) # why copy here?
+        # Call inv(x) ignoring errors. The result array will
+        # contain nans in the entries where inversion failed.
+        _assert_stacked_2d(x)
+        _assert_stacked_square(x)
+        t, result_t = _commonType(x)
+        signature = 'D->D' if isComplexType(t) else 'd->d'
+        with errstate(all='ignore'):
+            invx = _umath_linalg.inv(x, signature=signature)
+            r = norm(x, p, axis=(-2, -1)) * norm(invx, p, axis=(-2, -1))
+        r = r.astype(result_t, copy=False)
+
+    # Convert nans to infs unless the original array had nan entries
+    r = asarray(r)
+    nan_mask = isnan(r)
+    if nan_mask.any():
+        nan_mask &= ~isnan(x).any(axis=(-2, -1))
+        if r.ndim > 0:
+            r[nan_mask] = Inf
+        elif nan_mask:
+            r[()] = Inf
+
+    # Convention is to return scalars instead of 0d arrays
+    if r.ndim == 0:
+        r = r[()]
+
+    return r
+
+
+def _matrix_rank_dispatcher(A, tol=None, hermitian=None):
+    return (A,)
+
+
+@array_function_dispatch(_matrix_rank_dispatcher)
+def matrix_rank(A, tol=None, hermitian=False):
+    """
+    Return matrix rank of array using SVD method
+
+    Rank of the array is the number of singular values of the array that are
+    greater than `tol`.
+
+    .. versionchanged:: 1.14
+       Can now operate on stacks of matrices
+
+    Parameters
+    ----------
+    A : {(M,), (..., M, N)} array_like
+        Input vector or stack of matrices.
+    tol : (...) array_like, float, optional
+        Threshold below which SVD values are considered zero. If `tol` is
+        None, and ``S`` is an array with singular values for `M`, and
+        ``eps`` is the epsilon value for datatype of ``S``, then `tol` is
+        set to ``S.max() * max(M, N) * eps``.
+
+        .. versionchanged:: 1.14
+           Broadcasted against the stack of matrices
+    hermitian : bool, optional
+        If True, `A` is assumed to be Hermitian (symmetric if real-valued),
+        enabling a more efficient method for finding singular values.
+        Defaults to False.
+
+        .. versionadded:: 1.14
+
+    Returns
+    -------
+    rank : (...) array_like
+        Rank of A.
+
+    Notes
+    -----
+    The default threshold to detect rank deficiency is a test on the magnitude
+    of the singular values of `A`.  By default, we identify singular values less
+    than ``S.max() * max(M, N) * eps`` as indicating rank deficiency (with
+    the symbols defined above). This is the algorithm MATLAB uses [1].  It also
+    appears in *Numerical recipes* in the discussion of SVD solutions for linear
+    least squares [2].
+
+    This default threshold is designed to detect rank deficiency accounting for
+    the numerical errors of the SVD computation.  Imagine that there is a column
+    in `A` that is an exact (in floating point) linear combination of other
+    columns in `A`. Computing the SVD on `A` will not produce a singular value
+    exactly equal to 0 in general: any difference of the smallest SVD value from
+    0 will be caused by numerical imprecision in the calculation of the SVD.
+    Our threshold for small SVD values takes this numerical imprecision into
+    account, and the default threshold will detect such numerical rank
+    deficiency.  The threshold may declare a matrix `A` rank deficient even if
+    the linear combination of some columns of `A` is not exactly equal to
+    another column of `A` but only numerically very close to another column of
+    `A`.
+
+    We chose our default threshold because it is in wide use.  Other thresholds
+    are possible.  For example, elsewhere in the 2007 edition of *Numerical
+    recipes* there is an alternative threshold of ``S.max() *
+    np.finfo(A.dtype).eps / 2. * np.sqrt(m + n + 1.)``. The authors describe
+    this threshold as being based on "expected roundoff error" (p 71).
+
+    The thresholds above deal with floating point roundoff error in the
+    calculation of the SVD.  However, you may have more information about the
+    sources of error in `A` that would make you consider other tolerance values
+    to detect *effective* rank deficiency.  The most useful measure of the
+    tolerance depends on the operations you intend to use on your matrix.  For
+    example, if your data come from uncertain measurements with uncertainties
+    greater than floating point epsilon, choosing a tolerance near that
+    uncertainty may be preferable.  The tolerance may be absolute if the
+    uncertainties are absolute rather than relative.
+
+    References
+    ----------
+    .. [1] MATLAB reference documentation, "Rank"
+           https://www.mathworks.com/help/techdoc/ref/rank.html
+    .. [2] W. H. Press, S. A. Teukolsky, W. T. Vetterling and B. P. Flannery,
+           "Numerical Recipes (3rd edition)", Cambridge University Press, 2007,
+           page 795.
+
+    Examples
+    --------
+    >>> from numpy.linalg import matrix_rank
+    >>> matrix_rank(np.eye(4)) # Full rank matrix
+    4
+    >>> I=np.eye(4); I[-1,-1] = 0. # rank deficient matrix
+    >>> matrix_rank(I)
+    3
+    >>> matrix_rank(np.ones((4,))) # 1 dimension - rank 1 unless all 0
+    1
+    >>> matrix_rank(np.zeros((4,)))
+    0
+    """
+    A = asarray(A)
+    if A.ndim < 2:
+        return int(not all(A==0))
+    S = svd(A, compute_uv=False, hermitian=hermitian)
+    if tol is None:
+        tol = S.max(axis=-1, keepdims=True) * max(A.shape[-2:]) * finfo(S.dtype).eps
     else:
-        return s
+        tol = asarray(tol)[..., newaxis]
+    return count_nonzero(S > tol, axis=-1)
+
 
 # Generalized inverse
 
-def pinv(a, rcond = 1.e-10):
+def _pinv_dispatcher(a, rcond=None, hermitian=None):
+    return (a,)
+
+
+@array_function_dispatch(_pinv_dispatcher)
+def pinv(a, rcond=1e-15, hermitian=False):
+    """
+    Compute the (Moore-Penrose) pseudo-inverse of a matrix.
+
+    Calculate the generalized inverse of a matrix using its
+    singular-value decomposition (SVD) and including all
+    *large* singular values.
+
+    .. versionchanged:: 1.14
+       Can now operate on stacks of matrices
+
+    Parameters
+    ----------
+    a : (..., M, N) array_like
+        Matrix or stack of matrices to be pseudo-inverted.
+    rcond : (...) array_like of float
+        Cutoff for small singular values.
+        Singular values less than or equal to
+        ``rcond * largest_singular_value`` are set to zero.
+        Broadcasts against the stack of matrices.
+    hermitian : bool, optional
+        If True, `a` is assumed to be Hermitian (symmetric if real-valued),
+        enabling a more efficient method for finding singular values.
+        Defaults to False.
+
+        .. versionadded:: 1.17.0
+
+    Returns
+    -------
+    B : (..., N, M) ndarray
+        The pseudo-inverse of `a`. If `a` is a `matrix` instance, then so
+        is `B`.
+
+    Raises
+    ------
+    LinAlgError
+        If the SVD computation does not converge.
+
+    See Also
+    --------
+    scipy.linalg.pinv : Similar function in SciPy.
+    scipy.linalg.pinvh : Compute the (Moore-Penrose) pseudo-inverse of a
+                         Hermitian matrix.
+
+    Notes
+    -----
+    The pseudo-inverse of a matrix A, denoted :math:`A^+`, is
+    defined as: "the matrix that 'solves' [the least-squares problem]
+    :math:`Ax = b`," i.e., if :math:`\\bar{x}` is said solution, then
+    :math:`A^+` is that matrix such that :math:`\\bar{x} = A^+b`.
+
+    It can be shown that if :math:`Q_1 \\Sigma Q_2^T = A` is the singular
+    value decomposition of A, then
+    :math:`A^+ = Q_2 \\Sigma^+ Q_1^T`, where :math:`Q_{1,2}` are
+    orthogonal matrices, :math:`\\Sigma` is a diagonal matrix consisting
+    of A's so-called singular values, (followed, typically, by
+    zeros), and then :math:`\\Sigma^+` is simply the diagonal matrix
+    consisting of the reciprocals of A's singular values
+    (again, followed by zeros). [1]_
+
+    References
+    ----------
+    .. [1] G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando,
+           FL, Academic Press, Inc., 1980, pp. 139-142.
+
+    Examples
+    --------
+    The following example checks that ``a * a+ * a == a`` and
+    ``a+ * a * a+ == a+``:
+
+    >>> a = np.random.randn(9, 6)
+    >>> B = np.linalg.pinv(a)
+    >>> np.allclose(a, np.dot(a, np.dot(B, a)))
+    True
+    >>> np.allclose(B, np.dot(B, np.dot(a, B)))
+    True
+
+    """
     a, wrap = _makearray(a)
-    if a.dtype.char in typecodes['Complex']:
-        a = conjugate(a)
-    u, s, vt = svd(a, 0)
-    m = u.shape[0]
-    n = vt.shape[1]
-    cutoff = rcond*maximum.reduce(s)
-    for i in range(min(n,m)):
-        if s[i] > cutoff:
-            s[i] = 1./s[i]
+    rcond = asarray(rcond)
+    if _is_empty_2d(a):
+        m, n = a.shape[-2:]
+        res = empty(a.shape[:-2] + (n, m), dtype=a.dtype)
+        return wrap(res)
+    a = a.conjugate()
+    u, s, vt = svd(a, full_matrices=False, hermitian=hermitian)
+
+    # discard small singular values
+    cutoff = rcond[..., newaxis] * amax(s, axis=-1, keepdims=True)
+    large = s > cutoff
+    s = divide(1, s, where=large, out=s)
+    s[~large] = 0
+
+    res = matmul(transpose(vt), multiply(s[..., newaxis], transpose(u)))
+    return wrap(res)
+
+
+# Determinant
+
+
+@array_function_dispatch(_unary_dispatcher)
+def slogdet(a):
+    """
+    Compute the sign and (natural) logarithm of the determinant of an array.
+
+    If an array has a very small or very large determinant, then a call to
+    `det` may overflow or underflow. This routine is more robust against such
+    issues, because it computes the logarithm of the determinant rather than
+    the determinant itself.
+
+    Parameters
+    ----------
+    a : (..., M, M) array_like
+        Input array, has to be a square 2-D array.
+
+    Returns
+    -------
+    sign : (...) array_like
+        A number representing the sign of the determinant. For a real matrix,
+        this is 1, 0, or -1. For a complex matrix, this is a complex number
+        with absolute value 1 (i.e., it is on the unit circle), or else 0.
+    logdet : (...) array_like
+        The natural log of the absolute value of the determinant.
+
+    If the determinant is zero, then `sign` will be 0 and `logdet` will be
+    -Inf. In all cases, the determinant is equal to ``sign * np.exp(logdet)``.
+
+    See Also
+    --------
+    det
+
+    Notes
+    -----
+
+    .. versionadded:: 1.8.0
+
+    Broadcasting rules apply, see the `numpy.linalg` documentation for
+    details.
+
+    .. versionadded:: 1.6.0
+
+    The determinant is computed via LU factorization using the LAPACK
+    routine ``z/dgetrf``.
+
+
+    Examples
+    --------
+    The determinant of a 2-D array ``[[a, b], [c, d]]`` is ``ad - bc``:
+
+    >>> a = np.array([[1, 2], [3, 4]])
+    >>> (sign, logdet) = np.linalg.slogdet(a)
+    >>> (sign, logdet)
+    (-1, 0.69314718055994529) # may vary
+    >>> sign * np.exp(logdet)
+    -2.0
+
+    Computing log-determinants for a stack of matrices:
+
+    >>> a = np.array([ [[1, 2], [3, 4]], [[1, 2], [2, 1]], [[1, 3], [3, 1]] ])
+    >>> a.shape
+    (3, 2, 2)
+    >>> sign, logdet = np.linalg.slogdet(a)
+    >>> (sign, logdet)
+    (array([-1., -1., -1.]), array([ 0.69314718,  1.09861229,  2.07944154]))
+    >>> sign * np.exp(logdet)
+    array([-2., -3., -8.])
+
+    This routine succeeds where ordinary `det` does not:
+
+    >>> np.linalg.det(np.eye(500) * 0.1)
+    0.0
+    >>> np.linalg.slogdet(np.eye(500) * 0.1)
+    (1, -1151.2925464970228)
+
+    """
+    a = asarray(a)
+    _assert_stacked_2d(a)
+    _assert_stacked_square(a)
+    t, result_t = _commonType(a)
+    real_t = _realType(result_t)
+    signature = 'D->Dd' if isComplexType(t) else 'd->dd'
+    sign, logdet = _umath_linalg.slogdet(a, signature=signature)
+    sign = sign.astype(result_t, copy=False)
+    logdet = logdet.astype(real_t, copy=False)
+    return sign, logdet
+
+
+@array_function_dispatch(_unary_dispatcher)
+def det(a):
+    """
+    Compute the determinant of an array.
+
+    Parameters
+    ----------
+    a : (..., M, M) array_like
+        Input array to compute determinants for.
+
+    Returns
+    -------
+    det : (...) array_like
+        Determinant of `a`.
+
+    See Also
+    --------
+    slogdet : Another way to represent the determinant, more suitable
+      for large matrices where underflow/overflow may occur.
+    scipy.linalg.det : Similar function in SciPy.
+
+    Notes
+    -----
+
+    .. versionadded:: 1.8.0
+
+    Broadcasting rules apply, see the `numpy.linalg` documentation for
+    details.
+
+    The determinant is computed via LU factorization using the LAPACK
+    routine ``z/dgetrf``.
+
+    Examples
+    --------
+    The determinant of a 2-D array [[a, b], [c, d]] is ad - bc:
+
+    >>> a = np.array([[1, 2], [3, 4]])
+    >>> np.linalg.det(a)
+    -2.0 # may vary
+
+    Computing determinants for a stack of matrices:
+
+    >>> a = np.array([ [[1, 2], [3, 4]], [[1, 2], [2, 1]], [[1, 3], [3, 1]] ])
+    >>> a.shape
+    (3, 2, 2)
+    >>> np.linalg.det(a)
+    array([-2., -3., -8.])
+
+    """
+    a = asarray(a)
+    _assert_stacked_2d(a)
+    _assert_stacked_square(a)
+    t, result_t = _commonType(a)
+    signature = 'D->D' if isComplexType(t) else 'd->d'
+    r = _umath_linalg.det(a, signature=signature)
+    r = r.astype(result_t, copy=False)
+    return r
+
+
+# Linear Least Squares
+
+def _lstsq_dispatcher(a, b, rcond=None):
+    return (a, b)
+
+
+@array_function_dispatch(_lstsq_dispatcher)
+def lstsq(a, b, rcond="warn"):
+    r"""
+    Return the least-squares solution to a linear matrix equation.
+
+    Computes the vector `x` that approximately solves the equation
+    ``a @ x = b``. The equation may be under-, well-, or over-determined
+    (i.e., the number of linearly independent rows of `a` can be less than,
+    equal to, or greater than its number of linearly independent columns).
+    If `a` is square and of full rank, then `x` (but for round-off error)
+    is the "exact" solution of the equation. Else, `x` minimizes the
+    Euclidean 2-norm :math:`||b - ax||`. If there are multiple minimizing
+    solutions, the one with the smallest 2-norm :math:`||x||` is returned.
+
+    Parameters
+    ----------
+    a : (M, N) array_like
+        "Coefficient" matrix.
+    b : {(M,), (M, K)} array_like
+        Ordinate or "dependent variable" values. If `b` is two-dimensional,
+        the least-squares solution is calculated for each of the `K` columns
+        of `b`.
+    rcond : float, optional
+        Cut-off ratio for small singular values of `a`.
+        For the purposes of rank determination, singular values are treated
+        as zero if they are smaller than `rcond` times the largest singular
+        value of `a`.
+
+        .. versionchanged:: 1.14.0
+           If not set, a FutureWarning is given. The previous default
+           of ``-1`` will use the machine precision as `rcond` parameter,
+           the new default will use the machine precision times `max(M, N)`.
+           To silence the warning and use the new default, use ``rcond=None``,
+           to keep using the old behavior, use ``rcond=-1``.
+
+    Returns
+    -------
+    x : {(N,), (N, K)} ndarray
+        Least-squares solution. If `b` is two-dimensional,
+        the solutions are in the `K` columns of `x`.
+    residuals : {(1,), (K,), (0,)} ndarray
+        Sums of squared residuals: Squared Euclidean 2-norm for each column in
+        ``b - a @ x``.
+        If the rank of `a` is < N or M <= N, this is an empty array.
+        If `b` is 1-dimensional, this is a (1,) shape array.
+        Otherwise the shape is (K,).
+    rank : int
+        Rank of matrix `a`.
+    s : (min(M, N),) ndarray
+        Singular values of `a`.
+
+    Raises
+    ------
+    LinAlgError
+        If computation does not converge.
+
+    See Also
+    --------
+    scipy.linalg.lstsq : Similar function in SciPy.
+
+    Notes
+    -----
+    If `b` is a matrix, then all array results are returned as matrices.
+
+    Examples
+    --------
+    Fit a line, ``y = mx + c``, through some noisy data-points:
+
+    >>> x = np.array([0, 1, 2, 3])
+    >>> y = np.array([-1, 0.2, 0.9, 2.1])
+
+    By examining the coefficients, we see that the line should have a
+    gradient of roughly 1 and cut the y-axis at, more or less, -1.
+
+    We can rewrite the line equation as ``y = Ap``, where ``A = [[x 1]]``
+    and ``p = [[m], [c]]``.  Now use `lstsq` to solve for `p`:
+
+    >>> A = np.vstack([x, np.ones(len(x))]).T
+    >>> A
+    array([[ 0.,  1.],
+           [ 1.,  1.],
+           [ 2.,  1.],
+           [ 3.,  1.]])
+
+    >>> m, c = np.linalg.lstsq(A, y, rcond=None)[0]
+    >>> m, c
+    (1.0 -0.95) # may vary
+
+    Plot the data along with the fitted line:
+
+    >>> import matplotlib.pyplot as plt
+    >>> _ = plt.plot(x, y, 'o', label='Original data', markersize=10)
+    >>> _ = plt.plot(x, m*x + c, 'r', label='Fitted line')
+    >>> _ = plt.legend()
+    >>> plt.show()
+
+    """
+    a, _ = _makearray(a)
+    b, wrap = _makearray(b)
+    is_1d = b.ndim == 1
+    if is_1d:
+        b = b[:, newaxis]
+    _assert_2d(a, b)
+    m, n = a.shape[-2:]
+    m2, n_rhs = b.shape[-2:]
+    if m != m2:
+        raise LinAlgError('Incompatible dimensions')
+
+    t, result_t = _commonType(a, b)
+    result_real_t = _realType(result_t)
+
+    # Determine default rcond value
+    if rcond == "warn":
+        # 2017-08-19, 1.14.0
+        warnings.warn("`rcond` parameter will change to the default of "
+                      "machine precision times ``max(M, N)`` where M and N "
+                      "are the input matrix dimensions.\n"
+                      "To use the future default and silence this warning "
+                      "we advise to pass `rcond=None`, to keep using the old, "
+                      "explicitly pass `rcond=-1`.",
+                      FutureWarning, stacklevel=3)
+        rcond = -1
+    if rcond is None:
+        rcond = finfo(t).eps * max(n, m)
+
+    if m <= n:
+        gufunc = _umath_linalg.lstsq_m
+    else:
+        gufunc = _umath_linalg.lstsq_n
+
+    signature = 'DDd->Ddid' if isComplexType(t) else 'ddd->ddid'
+    extobj = get_linalg_error_extobj(_raise_linalgerror_lstsq)
+    if n_rhs == 0:
+        # lapack can't handle n_rhs = 0 - so allocate the array one larger in that axis
+        b = zeros(b.shape[:-2] + (m, n_rhs + 1), dtype=b.dtype)
+    x, resids, rank, s = gufunc(a, b, rcond, signature=signature, extobj=extobj)
+    if m == 0:
+        x[...] = 0
+    if n_rhs == 0:
+        # remove the item we added
+        x = x[..., :n_rhs]
+        resids = resids[..., :n_rhs]
+
+    # remove the axis we added
+    if is_1d:
+        x = x.squeeze(axis=-1)
+        # we probably should squeeze resids too, but we can't
+        # without breaking compatibility.
+
+    # as documented
+    if rank != n or m <= n:
+        resids = array([], result_real_t)
+
+    # coerce output arrays
+    s = s.astype(result_real_t, copy=False)
+    resids = resids.astype(result_real_t, copy=False)
+    x = x.astype(result_t, copy=True)  # Copying lets the memory in r_parts be freed
+    return wrap(x), wrap(resids), rank, s
+
+
+def _multi_svd_norm(x, row_axis, col_axis, op):
+    """Compute a function of the singular values of the 2-D matrices in `x`.
+
+    This is a private utility function used by `numpy.linalg.norm()`.
+
+    Parameters
+    ----------
+    x : ndarray
+    row_axis, col_axis : int
+        The axes of `x` that hold the 2-D matrices.
+    op : callable
+        This should be either numpy.amin or `numpy.amax` or `numpy.sum`.
+
+    Returns
+    -------
+    result : float or ndarray
+        If `x` is 2-D, the return values is a float.
+        Otherwise, it is an array with ``x.ndim - 2`` dimensions.
+        The return values are either the minimum or maximum or sum of the
+        singular values of the matrices, depending on whether `op`
+        is `numpy.amin` or `numpy.amax` or `numpy.sum`.
+
+    """
+    y = moveaxis(x, (row_axis, col_axis), (-2, -1))
+    result = op(svd(y, compute_uv=False), axis=-1)
+    return result
+
+
+def _norm_dispatcher(x, ord=None, axis=None, keepdims=None):
+    return (x,)
+
+
+@array_function_dispatch(_norm_dispatcher)
+def norm(x, ord=None, axis=None, keepdims=False):
+    """
+    Matrix or vector norm.
+
+    This function is able to return one of eight different matrix norms,
+    or one of an infinite number of vector norms (described below), depending
+    on the value of the ``ord`` parameter.
+
+    Parameters
+    ----------
+    x : array_like
+        Input array.  If `axis` is None, `x` must be 1-D or 2-D, unless `ord`
+        is None. If both `axis` and `ord` are None, the 2-norm of
+        ``x.ravel`` will be returned.
+    ord : {non-zero int, inf, -inf, 'fro', 'nuc'}, optional
+        Order of the norm (see table under ``Notes``). inf means numpy's
+        `inf` object. The default is None.
+    axis : {None, int, 2-tuple of ints}, optional.
+        If `axis` is an integer, it specifies the axis of `x` along which to
+        compute the vector norms.  If `axis` is a 2-tuple, it specifies the
+        axes that hold 2-D matrices, and the matrix norms of these matrices
+        are computed.  If `axis` is None then either a vector norm (when `x`
+        is 1-D) or a matrix norm (when `x` is 2-D) is returned. The default
+        is None.
+
+        .. versionadded:: 1.8.0
+
+    keepdims : bool, optional
+        If this is set to True, the axes which are normed over are left in the
+        result as dimensions with size one.  With this option the result will
+        broadcast correctly against the original `x`.
+
+        .. versionadded:: 1.10.0
+
+    Returns
+    -------
+    n : float or ndarray
+        Norm of the matrix or vector(s).
+
+    See Also
+    --------
+    scipy.linalg.norm : Similar function in SciPy.
+
+    Notes
+    -----
+    For values of ``ord < 1``, the result is, strictly speaking, not a
+    mathematical 'norm', but it may still be useful for various numerical
+    purposes.
+
+    The following norms can be calculated:
+
+    =====  ============================  ==========================
+    ord    norm for matrices             norm for vectors
+    =====  ============================  ==========================
+    None   Frobenius norm                2-norm
+    'fro'  Frobenius norm                --
+    'nuc'  nuclear norm                  --
+    inf    max(sum(abs(x), axis=1))      max(abs(x))
+    -inf   min(sum(abs(x), axis=1))      min(abs(x))
+    0      --                            sum(x != 0)
+    1      max(sum(abs(x), axis=0))      as below
+    -1     min(sum(abs(x), axis=0))      as below
+    2      2-norm (largest sing. value)  as below
+    -2     smallest singular value       as below
+    other  --                            sum(abs(x)**ord)**(1./ord)
+    =====  ============================  ==========================
+
+    The Frobenius norm is given by [1]_:
+
+        :math:`||A||_F = [\\sum_{i,j} abs(a_{i,j})^2]^{1/2}`
+
+    The nuclear norm is the sum of the singular values.
+
+    Both the Frobenius and nuclear norm orders are only defined for
+    matrices and raise a ValueError when ``x.ndim != 2``.
+
+    References
+    ----------
+    .. [1] G. H. Golub and C. F. Van Loan, *Matrix Computations*,
+           Baltimore, MD, Johns Hopkins University Press, 1985, pg. 15
+
+    Examples
+    --------
+    >>> from numpy import linalg as LA
+    >>> a = np.arange(9) - 4
+    >>> a
+    array([-4, -3, -2, ...,  2,  3,  4])
+    >>> b = a.reshape((3, 3))
+    >>> b
+    array([[-4, -3, -2],
+           [-1,  0,  1],
+           [ 2,  3,  4]])
+
+    >>> LA.norm(a)
+    7.745966692414834
+    >>> LA.norm(b)
+    7.745966692414834
+    >>> LA.norm(b, 'fro')
+    7.745966692414834
+    >>> LA.norm(a, np.inf)
+    4.0
+    >>> LA.norm(b, np.inf)
+    9.0
+    >>> LA.norm(a, -np.inf)
+    0.0
+    >>> LA.norm(b, -np.inf)
+    2.0
+
+    >>> LA.norm(a, 1)
+    20.0
+    >>> LA.norm(b, 1)
+    7.0
+    >>> LA.norm(a, -1)
+    -4.6566128774142013e-010
+    >>> LA.norm(b, -1)
+    6.0
+    >>> LA.norm(a, 2)
+    7.745966692414834
+    >>> LA.norm(b, 2)
+    7.3484692283495345
+
+    >>> LA.norm(a, -2)
+    0.0
+    >>> LA.norm(b, -2)
+    1.8570331885190563e-016 # may vary
+    >>> LA.norm(a, 3)
+    5.8480354764257312 # may vary
+    >>> LA.norm(a, -3)
+    0.0
+
+    Using the `axis` argument to compute vector norms:
+
+    >>> c = np.array([[ 1, 2, 3],
+    ...               [-1, 1, 4]])
+    >>> LA.norm(c, axis=0)
+    array([ 1.41421356,  2.23606798,  5.        ])
+    >>> LA.norm(c, axis=1)
+    array([ 3.74165739,  4.24264069])
+    >>> LA.norm(c, ord=1, axis=1)
+    array([ 6.,  6.])
+
+    Using the `axis` argument to compute matrix norms:
+
+    >>> m = np.arange(8).reshape(2,2,2)
+    >>> LA.norm(m, axis=(1,2))
+    array([  3.74165739,  11.22497216])
+    >>> LA.norm(m[0, :, :]), LA.norm(m[1, :, :])
+    (3.7416573867739413, 11.224972160321824)
+
+    """
+    x = asarray(x)
+
+    if not issubclass(x.dtype.type, (inexact, object_)):
+        x = x.astype(float)
+
+    # Immediately handle some default, simple, fast, and common cases.
+    if axis is None:
+        ndim = x.ndim
+        if ((ord is None) or
+            (ord in ('f', 'fro') and ndim == 2) or
+            (ord == 2 and ndim == 1)):
+
+            x = x.ravel(order='K')
+            if isComplexType(x.dtype.type):
+                x_real = x.real
+                x_imag = x.imag
+                sqnorm = x_real.dot(x_real) + x_imag.dot(x_imag)
+            else:
+                sqnorm = x.dot(x)
+            ret = sqrt(sqnorm)
+            if keepdims:
+                ret = ret.reshape(ndim*[1])
+            return ret
+
+    # Normalize the `axis` argument to a tuple.
+    nd = x.ndim
+    if axis is None:
+        axis = tuple(range(nd))
+    elif not isinstance(axis, tuple):
+        try:
+            axis = int(axis)
+        except Exception as e:
+            raise TypeError("'axis' must be None, an integer or a tuple of integers") from e
+        axis = (axis,)
+
+    if len(axis) == 1:
+        if ord == Inf:
+            return abs(x).max(axis=axis, keepdims=keepdims)
+        elif ord == -Inf:
+            return abs(x).min(axis=axis, keepdims=keepdims)
+        elif ord == 0:
+            # Zero norm
+            return (x != 0).astype(x.real.dtype).sum(axis=axis, keepdims=keepdims)
+        elif ord == 1:
+            # special case for speedup
+            return add.reduce(abs(x), axis=axis, keepdims=keepdims)
+        elif ord is None or ord == 2:
+            # special case for speedup
+            s = (x.conj() * x).real
+            return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))
+        # None of the str-type keywords for ord ('fro', 'nuc')
+        # are valid for vectors
+        elif isinstance(ord, str):
+            raise ValueError(f"Invalid norm order '{ord}' for vectors")
         else:
-            s[i] = 0.;
-    return wrap(dot(transpose(vt),
-                       multiply(s[:, newaxis],transpose(u))))
-
-# Determinant
-
-def det(a):
-    a = asarray(a)
-    _assertRank2(a)
-    _assertSquareness(a)
-    t =_commonType(a)
-    a = _fastCopyAndTranspose(t, a)
-    n = a.shape[0]
-    if _array_kind[t] == 1:
-        lapack_routine = lapack_lite.zgetrf
+            absx = abs(x)
+            absx **= ord
+            ret = add.reduce(absx, axis=axis, keepdims=keepdims)
+            ret **= reciprocal(ord, dtype=ret.dtype)
+            return ret
+    elif len(axis) == 2:
+        row_axis, col_axis = axis
+        row_axis = normalize_axis_index(row_axis, nd)
+        col_axis = normalize_axis_index(col_axis, nd)
+        if row_axis == col_axis:
+            raise ValueError('Duplicate axes given.')
+        if ord == 2:
+            ret =  _multi_svd_norm(x, row_axis, col_axis, amax)
+        elif ord == -2:
+            ret = _multi_svd_norm(x, row_axis, col_axis, amin)
+        elif ord == 1:
+            if col_axis > row_axis:
+                col_axis -= 1
+            ret = add.reduce(abs(x), axis=row_axis).max(axis=col_axis)
+        elif ord == Inf:
+            if row_axis > col_axis:
+                row_axis -= 1
+            ret = add.reduce(abs(x), axis=col_axis).max(axis=row_axis)
+        elif ord == -1:
+            if col_axis > row_axis:
+                col_axis -= 1
+            ret = add.reduce(abs(x), axis=row_axis).min(axis=col_axis)
+        elif ord == -Inf:
+            if row_axis > col_axis:
+                row_axis -= 1
+            ret = add.reduce(abs(x), axis=col_axis).min(axis=row_axis)
+        elif ord in [None, 'fro', 'f']:
+            ret = sqrt(add.reduce((x.conj() * x).real, axis=axis))
+        elif ord == 'nuc':
+            ret = _multi_svd_norm(x, row_axis, col_axis, sum)
+        else:
+            raise ValueError("Invalid norm order for matrices.")
+        if keepdims:
+            ret_shape = list(x.shape)
+            ret_shape[axis[0]] = 1
+            ret_shape[axis[1]] = 1
+            ret = ret.reshape(ret_shape)
+        return ret
     else:
-        lapack_routine = lapack_lite.dgetrf
-    pivots = zeros((n,), 'i')
-    results = lapack_routine(n, n, a, n, pivots, 0)
-    sign = add.reduce(not_equal(pivots, arrayrange(1, n+1))) % 2
-    return (1.-2.*sign)*multiply.reduce(diagonal(a),axis=-1)
-
-# Linear Least Squares
-
-def lstsq(a, b, rcond=1.e-10):
-    """returns x,resids,rank,s
-where x minimizes 2-norm(|b - Ax|)
-      resids is the sum square residuals
-      rank is the rank of A
-      s is the rank of the singular values of A in descending order
-
-If b is a matrix then x is also a matrix with corresponding columns.
-If the rank of A is less than the number of columns of A or greater than
-the number of rows, then residuals will be returned as an empty array
-otherwise resids = sum((b-dot(A,x)**2).
-Singular values less than s[0]*rcond are treated as zero.
-"""
-    import math
-    a = asarray(a)
-    b, wrap = _makearray(b)
-    one_eq = len(b.shape) == 1
-    if one_eq:
-        b = b[:, newaxis]
-    _assertRank2(a, b)
-    m  = a.shape[0]
-    n  = a.shape[1]
-    n_rhs = b.shape[1]
-    ldb = max(n,m)
-    if m != b.shape[0]:
-        raise LinAlgError, 'Incompatible dimensions'
-    t =_commonType(a, b)
-    real_t = _array_type[0][_array_precision[t]]
-    bstar = zeros((ldb,n_rhs),t)
-    bstar[:b.shape[0],:n_rhs] = b.copy()
-    a,bstar = _castCopyAndTranspose(t, a, bstar)
-    s = zeros((min(m,n),),real_t)
-    nlvl = max( 0, int( math.log( float(min( m,n ))/2. ) ) + 1 )
-    iwork = zeros((3*min(m,n)*nlvl+11*min(m,n),), 'i')
-    if _array_kind[t] == 1: # Complex routines take different arguments
-        lapack_routine = lapack_lite.zgelsd
-        lwork = 1
-        rwork = zeros((lwork,), real_t)
-        work = zeros((lwork,),t)
-        results = lapack_routine( m, n, n_rhs, a, m, bstar,ldb , s, rcond,
-                        0,work,-1,rwork,iwork,0 )
-        lwork = int(abs(work[0]))
-        rwork = zeros((lwork,),real_t)
-        a_real = zeros((m,n),real_t)
-        bstar_real = zeros((ldb,n_rhs,),real_t)
-        results = lapack_lite.dgelsd( m, n, n_rhs, a_real, m, bstar_real,ldb , s, rcond,
-                        0,rwork,-1,iwork,0 )
-        lrwork = int(rwork[0])
-        work = zeros((lwork,), t)
-        rwork = zeros((lrwork,), real_t)
-        results = lapack_routine( m, n, n_rhs, a, m, bstar,ldb , s, rcond,
-                        0,work,lwork,rwork,iwork,0 )
+        raise ValueError("Improper number of dimensions to norm.")
+
+
+# multi_dot
+
+def _multidot_dispatcher(arrays, *, out=None):
+    yield from arrays
+    yield out
+
+
+@array_function_dispatch(_multidot_dispatcher)
+def multi_dot(arrays, *, out=None):
+    """
+    Compute the dot product of two or more arrays in a single function call,
+    while automatically selecting the fastest evaluation order.
+
+    `multi_dot` chains `numpy.dot` and uses optimal parenthesization
+    of the matrices [1]_ [2]_. Depending on the shapes of the matrices,
+    this can speed up the multiplication a lot.
+
+    If the first argument is 1-D it is treated as a row vector.
+    If the last argument is 1-D it is treated as a column vector.
+    The other arguments must be 2-D.
+
+    Think of `multi_dot` as::
+
+        def multi_dot(arrays): return functools.reduce(np.dot, arrays)
+
+
+    Parameters
+    ----------
+    arrays : sequence of array_like
+        If the first argument is 1-D it is treated as row vector.
+        If the last argument is 1-D it is treated as column vector.
+        The other arguments must be 2-D.
+    out : ndarray, optional
+        Output argument. This must have the exact kind that would be returned
+        if it was not used. In particular, it must have the right type, must be
+        C-contiguous, and its dtype must be the dtype that would be returned
+        for `dot(a, b)`. This is a performance feature. Therefore, if these
+        conditions are not met, an exception is raised, instead of attempting
+        to be flexible.
+
+        .. versionadded:: 1.19.0
+
+    Returns
+    -------
+    output : ndarray
+        Returns the dot product of the supplied arrays.
+
+    See Also
+    --------
+    numpy.dot : dot multiplication with two arguments.
+
+    References
+    ----------
+
+    .. [1] Cormen, "Introduction to Algorithms", Chapter 15.2, p. 370-378
+    .. [2] https://en.wikipedia.org/wiki/Matrix_chain_multiplication
+
+    Examples
+    --------
+    `multi_dot` allows you to write::
+
+    >>> from numpy.linalg import multi_dot
+    >>> # Prepare some data
+    >>> A = np.random.random((10000, 100))
+    >>> B = np.random.random((100, 1000))
+    >>> C = np.random.random((1000, 5))
+    >>> D = np.random.random((5, 333))
+    >>> # the actual dot multiplication
+    >>> _ = multi_dot([A, B, C, D])
+
+    instead of::
+
+    >>> _ = np.dot(np.dot(np.dot(A, B), C), D)
+    >>> # or
+    >>> _ = A.dot(B).dot(C).dot(D)
+
+    Notes
+    -----
+    The cost for a matrix multiplication can be calculated with the
+    following function::
+
+        def cost(A, B):
+            return A.shape[0] * A.shape[1] * B.shape[1]
+
+    Assume we have three matrices
+    :math:`A_{10x100}, B_{100x5}, C_{5x50}`.
+
+    The costs for the two different parenthesizations are as follows::
+
+        cost((AB)C) = 10*100*5 + 10*5*50   = 5000 + 2500   = 7500
+        cost(A(BC)) = 10*100*50 + 100*5*50 = 50000 + 25000 = 75000
+
+    """
+    n = len(arrays)
+    # optimization only makes sense for len(arrays) > 2
+    if n < 2:
+        raise ValueError("Expecting at least two arrays.")
+    elif n == 2:
+        return dot(arrays[0], arrays[1], out=out)
+
+    arrays = [asanyarray(a) for a in arrays]
+
+    # save original ndim to reshape the result array into the proper form later
+    ndim_first, ndim_last = arrays[0].ndim, arrays[-1].ndim
+    # Explicitly convert vectors to 2D arrays to keep the logic of the internal
+    # _multi_dot_* functions as simple as possible.
+    if arrays[0].ndim == 1:
+        arrays[0] = atleast_2d(arrays[0])
+    if arrays[-1].ndim == 1:
+        arrays[-1] = atleast_2d(arrays[-1]).T
+    _assert_2d(*arrays)
+
+    # _multi_dot_three is much faster than _multi_dot_matrix_chain_order
+    if n == 3:
+        result = _multi_dot_three(arrays[0], arrays[1], arrays[2], out=out)
     else:
-        lapack_routine = lapack_lite.dgelsd
-        lwork = 1
-        work = zeros((lwork,), t)
-        results = lapack_routine( m, n, n_rhs, a, m, bstar,ldb , s, rcond,
-                        0,work,-1,iwork,0 )
-        lwork = int(work[0])
-        work = zeros((lwork,), t)
-        results = lapack_routine( m, n, n_rhs, a, m, bstar,ldb , s, rcond,
-                        0,work,lwork,iwork,0 )
-    if results['info'] > 0:
-        raise LinAlgError, 'SVD did not converge in Linear Least Squares'
-    resids = array([],t)
-    if one_eq:
-        x = ravel(bstar)[:n].copy()
-        if (results['rank']==n) and (m>n):
-            resids = array([sum((ravel(bstar)[n:])**2)])
+        order = _multi_dot_matrix_chain_order(arrays)
+        result = _multi_dot(arrays, order, 0, n - 1, out=out)
+
+    # return proper shape
+    if ndim_first == 1 and ndim_last == 1:
+        return result[0, 0]  # scalar
+    elif ndim_first == 1 or ndim_last == 1:
+        return result.ravel()  # 1-D
     else:
-        x = transpose(bstar)[:n,:].copy()
-        if (results['rank']==n) and (m>n):
-            resids = sum((transpose(bstar)[n:,:])**2).copy()
-    return wrap(x),resids,results['rank'],s[:min(n,m)].copy()
-
-def norm(x, ord=None):
-    """ norm(x, ord=None) -> n
-
-    Matrix or vector norm.
-
-    Inputs:
-
-      x -- a rank-1 (vector) or rank-2 (matrix) array
-      ord -- the order of the norm.
-
-     Comments:
-       For arrays of any rank, if ord is None:
-         calculate the square norm (Euclidean norm for vectors, Frobenius norm for matrices)
-
-       For vectors ord can be any real number including Inf or -Inf.
-         ord = Inf, computes the maximum of the magnitudes
-         ord = -Inf, computes minimum of the magnitudes
-         ord is finite, computes sum(abs(x)**ord)**(1.0/ord)
-
-       For matrices ord can only be one of the following values:
-         ord = 2 computes the largest singular value
-         ord = -2 computes the smallest singular value
-         ord = 1 computes the largest column sum of absolute values
-         ord = -1 computes the smallest column sum of absolute values
-         ord = Inf computes the largest row sum of absolute values
-         ord = -Inf computes the smallest row sum of absolute values
-         ord = 'fro' computes the frobenius norm sqrt(sum(diag(X.H * X)))
-
-       For values ord < 0, the result is, strictly speaking, not a
-       mathematical 'norm', but it may still be useful for numerical purposes.
-    """
-    x = asarray(x)
-    nd = len(x.shape)    
-    if ord is None: # check the default case first and handle it immediately
-        return sqrt(add.reduce((x.conj() * x).ravel().real))
-
-    if nd == 1:
-        if ord == Inf:
-            return abs(x).max()
-        elif ord == -Inf:
-            return abs(x).min()
-        elif ord == 1:
-            return abs(x).sum() # special case for speedup
-        elif ord == 2:
-            return sqrt(((x.conj()*x).real).sum()) # special case for speedup
-        else:
-            return ((abs(x)**ord).sum())**(1.0/ord)
-    elif nd == 2:
-        if ord == 2:
-            return svd(x,compute_uv=0).max()
-        elif ord == -2:
-            return svd(x,compute_uv=0).min()
-        elif ord == 1:
-            return abs(x).sum(axis=0).max()
-        elif ord == Inf:
-            return abs(x).sum(axis=1).max()
-        elif ord == -1:
-            return abs(x).sum(axis=0).min()
-        elif ord == -Inf:
-            return abs(x).sum(axis=1).min()
-        elif ord in ['fro','f']:
-            return sqrt(add.reduce((x.conj() * x).real.ravel()))
-        else:
-            raise ValueError, "Invalid norm order for matrices."
+        return result
+
+
+def _multi_dot_three(A, B, C, out=None):
+    """
+    Find the best order for three arrays and do the multiplication.
+
+    For three arguments `_multi_dot_three` is approximately 15 times faster
+    than `_multi_dot_matrix_chain_order`
+
+    """
+    a0, a1b0 = A.shape
+    b1c0, c1 = C.shape
+    # cost1 = cost((AB)C) = a0*a1b0*b1c0 + a0*b1c0*c1
+    cost1 = a0 * b1c0 * (a1b0 + c1)
+    # cost2 = cost(A(BC)) = a1b0*b1c0*c1 + a0*a1b0*c1
+    cost2 = a1b0 * c1 * (a0 + b1c0)
+
+    if cost1 < cost2:
+        return dot(dot(A, B), C, out=out)
     else:
-        raise ValueError, "Improper number of dimensions to norm."
-
-if __name__ == '__main__':
-    def test(a, b):
-
-        print "All numbers printed should be (almost) zero:"
-
-        x = solve(a, b)
-        check = b - matrixmultiply(a, x)
-        print check
-
-
-        a_inv = inv(a)
-        check = matrixmultiply(a, a_inv)-identity(a.shape[0])
-        print check
-
-
-        ev = eigvals(a)
-
-        evalues, evectors = eig(a)
-        check = ev-evalues
-        print check
-
-        evectors = transpose(evectors)
-        check = matrixmultiply(a, evectors)-evectors*evalues
-        print check
-
-
-        u, s, vt = svd(a,0)
-        check = a - matrixmultiply(u*s, vt)
-        print check
-
-
-        a_ginv = pinv(a)
-        check = matrixmultiply(a, a_ginv)-identity(a.shape[0])
-        print check
-
-
-        det = det(a)
-        check = det-multiply.reduce(evalues)
-        print check
-
-        x, residuals, rank, sv = lstsq(a, b)
-        check = b - matrixmultiply(a, x)
-        print check
-        print rank-a.shape[0]
-        print sv-s
-
-    a = array([[1.,2.], [3.,4.]])
-    b = array([2., 1.])
-    test(a, b)
-
-    a = a+0j
-    b = b+0j
-    test(a, b)
+        return dot(A, dot(B, C), out=out)
+
+
+def _multi_dot_matrix_chain_order(arrays, return_costs=False):
+    """
+    Return a np.array that encodes the optimal order of mutiplications.
+
+    The optimal order array is then used by `_multi_dot()` to do the
+    multiplication.
+
+    Also return the cost matrix if `return_costs` is `True`
+
+    The implementation CLOSELY follows Cormen, "Introduction to Algorithms",
+    Chapter 15.2, p. 370-378.  Note that Cormen uses 1-based indices.
+
+        cost[i, j] = min([
+            cost[prefix] + cost[suffix] + cost_mult(prefix, suffix)
+            for k in range(i, j)])
+
+    """
+    n = len(arrays)
+    # p stores the dimensions of the matrices
+    # Example for p: A_{10x100}, B_{100x5}, C_{5x50} --> p = [10, 100, 5, 50]
+    p = [a.shape[0] for a in arrays] + [arrays[-1].shape[1]]
+    # m is a matrix of costs of the subproblems
+    # m[i,j]: min number of scalar multiplications needed to compute A_{i..j}
+    m = zeros((n, n), dtype=double)
+    # s is the actual ordering
+    # s[i, j] is the value of k at which we split the product A_i..A_j
+    s = empty((n, n), dtype=intp)
+
+    for l in range(1, n):
+        for i in range(n - l):
+            j = i + l
+            m[i, j] = Inf
+            for k in range(i, j):
+                q = m[i, k] + m[k+1, j] + p[i]*p[k+1]*p[j+1]
+                if q < m[i, j]:
+                    m[i, j] = q
+                    s[i, j] = k  # Note that Cormen uses 1-based index
+
+    return (s, m) if return_costs else s
+
+
+def _multi_dot(arrays, order, i, j, out=None):
+    """Actually do the multiplication with the given order."""
+    if i == j:
+        # the initial call with non-None out should never get here
+        assert out is None
+
+        return arrays[i]
+    else:
+        return dot(_multi_dot(arrays, order, i, order[i, j]),
+                   _multi_dot(arrays, order, order[i, j] + 1, j),
+                   out=out)
('numpy/f2py', 'cfuncs.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,4 +1,4 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 """
 
 C declarations, CPP macros, and C functions for f2py2e.
@@ -12,97 +12,97 @@
 NO WARRANTY IS EXPRESSED OR IMPLIED.  USE AT YOUR OWN RISK.
 $Date: 2005/05/06 11:42:34 $
 Pearu Peterson
-"""
-
-__version__ = "$Revision: 1.75 $"[10:-1]
-
-import __version__
+
+"""
+import sys
+import copy
+
+from . import __version__
+
 f2py_version = __version__.version
-
-import types,sys,copy,os
-errmess=sys.stderr.write
+errmess = sys.stderr.write
 
 ##################### Definitions ##################
 
-outneeds={'includes0':[],'includes':[],'typedefs':[],'typedefs_generated':[],
-          'userincludes':[],
-          'cppmacros':[],'cfuncs':[],'callbacks':[],'f90modhooks':[],
-          'commonhooks':[]}
-needs={}
-includes0={'includes0':'/*need_includes0*/'}
-includes={'includes':'/*need_includes*/'}
-userincludes={'userincludes':'/*need_userincludes*/'}
-typedefs={'typedefs':'/*need_typedefs*/'}
-typedefs_generated={'typedefs_generated':'/*need_typedefs_generated*/'}
-cppmacros={'cppmacros':'/*need_cppmacros*/'}
-cfuncs={'cfuncs':'/*need_cfuncs*/'}
-callbacks={'callbacks':'/*need_callbacks*/'}
-f90modhooks={'f90modhooks':'/*need_f90modhooks*/',
-             'initf90modhooksstatic':'/*initf90modhooksstatic*/',
-             'initf90modhooksdynamic':'/*initf90modhooksdynamic*/',
-             }
-commonhooks={'commonhooks':'/*need_commonhooks*/',
-             'initcommonhooks':'/*need_initcommonhooks*/',
-             }
+outneeds = {'includes0': [], 'includes': [], 'typedefs': [], 'typedefs_generated': [],
+            'userincludes': [],
+            'cppmacros': [], 'cfuncs': [], 'callbacks': [], 'f90modhooks': [],
+            'commonhooks': []}
+needs = {}
+includes0 = {'includes0': '/*need_includes0*/'}
+includes = {'includes': '/*need_includes*/'}
+userincludes = {'userincludes': '/*need_userincludes*/'}
+typedefs = {'typedefs': '/*need_typedefs*/'}
+typedefs_generated = {'typedefs_generated': '/*need_typedefs_generated*/'}
+cppmacros = {'cppmacros': '/*need_cppmacros*/'}
+cfuncs = {'cfuncs': '/*need_cfuncs*/'}
+callbacks = {'callbacks': '/*need_callbacks*/'}
+f90modhooks = {'f90modhooks': '/*need_f90modhooks*/',
+               'initf90modhooksstatic': '/*initf90modhooksstatic*/',
+               'initf90modhooksdynamic': '/*initf90modhooksdynamic*/',
+               }
+commonhooks = {'commonhooks': '/*need_commonhooks*/',
+               'initcommonhooks': '/*need_initcommonhooks*/',
+               }
 
 ############ Includes ###################
 
-includes0['math.h']='#include <math.h>'
-includes0['string.h']='#include <string.h>'
-includes0['setjmp.h']='#include <setjmp.h>'
-
-includes['Python.h']='#include "Python.h"'
-needs['arrayobject.h']=['Python.h']
-includes['arrayobject.h']='''#define PY_ARRAY_UNIQUE_SYMBOL PyArray_API
+includes0['math.h'] = '#include <math.h>'
+includes0['string.h'] = '#include <string.h>'
+includes0['setjmp.h'] = '#include <setjmp.h>'
+
+includes['arrayobject.h'] = '''#define PY_ARRAY_UNIQUE_SYMBOL PyArray_API
 #include "arrayobject.h"'''
 
-includes['arrayobject.h']='#include "fortranobject.h"'
+includes['arrayobject.h'] = '#include "fortranobject.h"'
+includes['stdarg.h'] = '#include <stdarg.h>'
 
 ############# Type definitions ###############
 
-typedefs['unsigned_char']='typedef unsigned char unsigned_char;'
-typedefs['unsigned_short']='typedef unsigned short unsigned_short;'
-typedefs['unsigned_long']='typedef unsigned long unsigned_long;'
-typedefs['signed_char']='typedef signed char signed_char;'
-typedefs['long_long']="""\
-#ifdef _WIN32
+typedefs['unsigned_char'] = 'typedef unsigned char unsigned_char;'
+typedefs['unsigned_short'] = 'typedef unsigned short unsigned_short;'
+typedefs['unsigned_long'] = 'typedef unsigned long unsigned_long;'
+typedefs['signed_char'] = 'typedef signed char signed_char;'
+typedefs['long_long'] = """\
+#if defined(NPY_OS_WIN32)
 typedef __int64 long_long;
 #else
 typedef long long long_long;
 typedef unsigned long long unsigned_long_long;
 #endif
 """
-typedefs['insinged_long_long']="""\
-#ifdef _WIN32
+typedefs['unsigned_long_long'] = """\
+#if defined(NPY_OS_WIN32)
 typedef __uint64 long_long;
 #else
 typedef unsigned long long unsigned_long_long;
 #endif
 """
-typedefs['long_double']="""\
+typedefs['long_double'] = """\
 #ifndef _LONG_DOUBLE
 typedef long double long_double;
 #endif
 """
-typedefs['complex_long_double']='typedef struct {long double r,i;} complex_long_double;'
-typedefs['complex_float']='typedef struct {float r,i;} complex_float;'
-typedefs['complex_double']='typedef struct {double r,i;} complex_double;'
-typedefs['string']="""typedef char * string;"""
+typedefs[
+    'complex_long_double'] = 'typedef struct {long double r,i;} complex_long_double;'
+typedefs['complex_float'] = 'typedef struct {float r,i;} complex_float;'
+typedefs['complex_double'] = 'typedef struct {double r,i;} complex_double;'
+typedefs['string'] = """typedef char * string;"""
 
 
 ############### CPP macros ####################
-cppmacros['CFUNCSMESS']="""\
+cppmacros['CFUNCSMESS'] = """\
 #ifdef DEBUGCFUNCS
 #define CFUNCSMESS(mess) fprintf(stderr,\"debug-capi:\"mess);
 #define CFUNCSMESSPY(mess,obj) CFUNCSMESS(mess) \\
-\tPyObject_Print((PyObject *)obj,stderr,Py_PRINT_RAW);\\
-\tfprintf(stderr,\"\\n\");
+    PyObject_Print((PyObject *)obj,stderr,Py_PRINT_RAW);\\
+    fprintf(stderr,\"\\n\");
 #else
 #define CFUNCSMESS(mess)
 #define CFUNCSMESSPY(mess,obj)
 #endif
 """
-cppmacros['F_FUNC']="""\
+cppmacros['F_FUNC'] = """\
 #if defined(PREPEND_FORTRAN)
 #if defined(NO_APPEND_FORTRAN)
 #if defined(UPPERCASE_FORTRAN)
@@ -138,7 +138,7 @@
 #define F_FUNC_US(f,F) F_FUNC(f,F)
 #endif
 """
-cppmacros['F_WRAPPEDFUNC']="""\
+cppmacros['F_WRAPPEDFUNC'] = """\
 #if defined(PREPEND_FORTRAN)
 #if defined(NO_APPEND_FORTRAN)
 #if defined(UPPERCASE_FORTRAN)
@@ -174,7 +174,7 @@
 #define F_WRAPPEDFUNC_US(f,F) F_WRAPPEDFUNC(f,F)
 #endif
 """
-cppmacros['F_MODFUNC']="""\
+cppmacros['F_MODFUNC'] = """\
 #if defined(F90MOD2CCONV1) /*E.g. Compaq Fortran */
 #if defined(NO_APPEND_FORTRAN)
 #define F_MODFUNCNAME(m,f) $ ## m ## $ ## f
@@ -208,26 +208,33 @@
 
 #define F_MODFUNC(m,f) (*(f2pymodstruct##m##.##f))
 """
-cppmacros['SWAPUNSAFE']="""\
+cppmacros['SWAPUNSAFE'] = """\
 #define SWAP(a,b) (size_t)(a) = ((size_t)(a) ^ (size_t)(b));\\
  (size_t)(b) = ((size_t)(a) ^ (size_t)(b));\\
  (size_t)(a) = ((size_t)(a) ^ (size_t)(b))
 """
-cppmacros['SWAP']="""\
+cppmacros['SWAP'] = """\
 #define SWAP(a,b,t) {\\
-\tt *c;\\
-\tc = a;\\
-\ta = b;\\
-\tb = c;}
-"""
-#cppmacros['ISCONTIGUOUS']='#define ISCONTIGUOUS(m) ((m)->flags & CONTIGUOUS)'
-cppmacros['PRINTPYOBJERR']="""\
+    t *c;\\
+    c = a;\\
+    a = b;\\
+    b = c;}
+"""
+# cppmacros['ISCONTIGUOUS']='#define ISCONTIGUOUS(m) (PyArray_FLAGS(m) &
+# NPY_ARRAY_C_CONTIGUOUS)'
+cppmacros['PRINTPYOBJERR'] = """\
 #define PRINTPYOBJERR(obj)\\
-\tfprintf(stderr,\"#modulename#.error is related to \");\\
-\tPyObject_Print((PyObject *)obj,stderr,Py_PRINT_RAW);\\
-\tfprintf(stderr,\"\\n\");
-"""
-cppmacros['MINMAX']="""\
+    fprintf(stderr,\"#modulename#.error is related to \");\\
+    PyObject_Print((PyObject *)obj,stderr,Py_PRINT_RAW);\\
+    fprintf(stderr,\"\\n\");
+"""
+cppmacros['MINMAX'] = """\
+#ifndef max
+#define max(a,b) ((a > b) ? (a) : (b))
+#endif
+#ifndef min
+#define min(a,b) ((a < b) ? (a) : (b))
+#endif
 #ifndef MAX
 #define MAX(a,b) ((a > b) ? (a) : (b))
 #endif
@@ -235,26 +242,57 @@
 #define MIN(a,b) ((a < b) ? (a) : (b))
 #endif
 """
-cppmacros['len..']="""\
+needs['len..'] = ['f2py_size']
+cppmacros['len..'] = """\
 #define rank(var) var ## _Rank
 #define shape(var,dim) var ## _Dims[dim]
-#define old_rank(var) (((PyArrayObject *)(capi_ ## var ## _tmp))->nd)
-#define old_shape(var,dim) (((PyArrayObject *)(capi_ ## var ## _tmp))->dimensions[dim])
+#define old_rank(var) (PyArray_NDIM((PyArrayObject *)(capi_ ## var ## _tmp)))
+#define old_shape(var,dim) PyArray_DIM(((PyArrayObject *)(capi_ ## var ## _tmp)),dim)
 #define fshape(var,dim) shape(var,rank(var)-dim-1)
 #define len(var) shape(var,0)
 #define flen(var) fshape(var,0)
-#define size(var) PyArray_SIZE((PyArrayObject *)(capi_ ## var ## _tmp))
+#define old_size(var) PyArray_SIZE((PyArrayObject *)(capi_ ## var ## _tmp))
 /* #define index(i) capi_i ## i */
 #define slen(var) capi_ ## var ## _len
-"""
-
-cppmacros['pyobj_from_char1']='#define pyobj_from_char1(v) (PyInt_FromLong(v))'
-cppmacros['pyobj_from_short1']='#define pyobj_from_short1(v) (PyInt_FromLong(v))'
-needs['pyobj_from_int1']=['signed_char']
-cppmacros['pyobj_from_int1']='#define pyobj_from_int1(v) (PyInt_FromLong(v))'
-cppmacros['pyobj_from_long1']='#define pyobj_from_long1(v) (PyLong_FromLong(v))'
-needs['pyobj_from_long_long1']=['long_long']
-cppmacros['pyobj_from_long_long1']="""\
+#define size(var, ...) f2py_size((PyArrayObject *)(capi_ ## var ## _tmp), ## __VA_ARGS__, -1)
+"""
+needs['f2py_size'] = ['stdarg.h']
+cfuncs['f2py_size'] = """\
+static int f2py_size(PyArrayObject* var, ...)
+{
+  npy_int sz = 0;
+  npy_int dim;
+  npy_int rank;
+  va_list argp;
+  va_start(argp, var);
+  dim = va_arg(argp, npy_int);
+  if (dim==-1)
+    {
+      sz = PyArray_SIZE(var);
+    }
+  else
+    {
+      rank = PyArray_NDIM(var);
+      if (dim>=1 && dim<=rank)
+        sz = PyArray_DIM(var, dim-1);
+      else
+        fprintf(stderr, \"f2py_size: 2nd argument value=%d fails to satisfy 1<=value<=%d. Result will be 0.\\n\", dim, rank);
+    }
+  va_end(argp);
+  return sz;
+}
+"""
+
+cppmacros[
+    'pyobj_from_char1'] = '#define pyobj_from_char1(v) (PyLong_FromLong(v))'
+cppmacros[
+    'pyobj_from_short1'] = '#define pyobj_from_short1(v) (PyLong_FromLong(v))'
+needs['pyobj_from_int1'] = ['signed_char']
+cppmacros['pyobj_from_int1'] = '#define pyobj_from_int1(v) (PyLong_FromLong(v))'
+cppmacros[
+    'pyobj_from_long1'] = '#define pyobj_from_long1(v) (PyLong_FromLong(v))'
+needs['pyobj_from_long_long1'] = ['long_long']
+cppmacros['pyobj_from_long_long1'] = """\
 #ifdef HAVE_LONG_LONG
 #define pyobj_from_long_long1(v) (PyLong_FromLongLong(v))
 #else
@@ -262,249 +300,317 @@
 #define pyobj_from_long_long1(v) (PyLong_FromLong(v))
 #endif
 """
-needs['pyobj_from_long_double1']=['long_double']
-cppmacros['pyobj_from_long_double1']='#define pyobj_from_long_double1(v) (PyFloat_FromDouble(v))'
-cppmacros['pyobj_from_double1']='#define pyobj_from_double1(v) (PyFloat_FromDouble(v))'
-cppmacros['pyobj_from_float1']='#define pyobj_from_float1(v) (PyFloat_FromDouble(v))'
-needs['pyobj_from_complex_long_double1']=['complex_long_double']
-cppmacros['pyobj_from_complex_long_double1']='#define pyobj_from_complex_long_double1(v) (PyComplex_FromDoubles(v.r,v.i))'
-needs['pyobj_from_complex_double1']=['complex_double']
-cppmacros['pyobj_from_complex_double1']='#define pyobj_from_complex_double1(v) (PyComplex_FromDoubles(v.r,v.i))'
-needs['pyobj_from_complex_float1']=['complex_float']
-cppmacros['pyobj_from_complex_float1']='#define pyobj_from_complex_float1(v) (PyComplex_FromDoubles(v.r,v.i))'
-needs['pyobj_from_string1']=['string']
-cppmacros['pyobj_from_string1']='#define pyobj_from_string1(v) (PyString_FromString((char *)v))'
-needs['TRYPYARRAYTEMPLATE']=['PRINTPYOBJERR']
-cppmacros['TRYPYARRAYTEMPLATE']="""\
+needs['pyobj_from_long_double1'] = ['long_double']
+cppmacros[
+    'pyobj_from_long_double1'] = '#define pyobj_from_long_double1(v) (PyFloat_FromDouble(v))'
+cppmacros[
+    'pyobj_from_double1'] = '#define pyobj_from_double1(v) (PyFloat_FromDouble(v))'
+cppmacros[
+    'pyobj_from_float1'] = '#define pyobj_from_float1(v) (PyFloat_FromDouble(v))'
+needs['pyobj_from_complex_long_double1'] = ['complex_long_double']
+cppmacros[
+    'pyobj_from_complex_long_double1'] = '#define pyobj_from_complex_long_double1(v) (PyComplex_FromDoubles(v.r,v.i))'
+needs['pyobj_from_complex_double1'] = ['complex_double']
+cppmacros[
+    'pyobj_from_complex_double1'] = '#define pyobj_from_complex_double1(v) (PyComplex_FromDoubles(v.r,v.i))'
+needs['pyobj_from_complex_float1'] = ['complex_float']
+cppmacros[
+    'pyobj_from_complex_float1'] = '#define pyobj_from_complex_float1(v) (PyComplex_FromDoubles(v.r,v.i))'
+needs['pyobj_from_string1'] = ['string']
+cppmacros[
+    'pyobj_from_string1'] = '#define pyobj_from_string1(v) (PyUnicode_FromString((char *)v))'
+needs['pyobj_from_string1size'] = ['string']
+cppmacros[
+    'pyobj_from_string1size'] = '#define pyobj_from_string1size(v,len) (PyUnicode_FromStringAndSize((char *)v, len))'
+needs['TRYPYARRAYTEMPLATE'] = ['PRINTPYOBJERR']
+cppmacros['TRYPYARRAYTEMPLATE'] = """\
 /* New SciPy */
-#define TRYPYARRAYTEMPLATECHAR case PyArray_STRING: *(char *)(arr->data)=*v; break;
-#define TRYPYARRAYTEMPLATELONG case PyArray_LONG: *(long *)(arr->data)=*v; break;
-#define TRYPYARRAYTEMPLATEOBJECT case PyArray_OBJECT: (arr->descr->f->setitem)(pyobj_from_ ## ctype ## 1(*v),arr->data); break;
+#define TRYPYARRAYTEMPLATECHAR case NPY_STRING: *(char *)(PyArray_DATA(arr))=*v; break;
+#define TRYPYARRAYTEMPLATELONG case NPY_LONG: *(long *)(PyArray_DATA(arr))=*v; break;
+#define TRYPYARRAYTEMPLATEOBJECT case NPY_OBJECT: PyArray_SETITEM(arr,PyArray_DATA(arr),pyobj_from_ ## ctype ## 1(*v)); break;
 
 #define TRYPYARRAYTEMPLATE(ctype,typecode) \\
         PyArrayObject *arr = NULL;\\
         if (!obj) return -2;\\
         if (!PyArray_Check(obj)) return -1;\\
         if (!(arr=(PyArrayObject *)obj)) {fprintf(stderr,\"TRYPYARRAYTEMPLATE:\");PRINTPYOBJERR(obj);return 0;}\\
-        if (arr->descr->type==typecode)  {*(ctype *)(arr->data)=*v; return 1;}\\
-        switch (arr->descr->type_num) {\\
-                case PyArray_DOUBLE: *(double *)(arr->data)=*v; break;\\
-                case PyArray_INT: *(int *)(arr->data)=*v; break;\\
-                case PyArray_LONG: *(long *)(arr->data)=*v; break;\\
-                case PyArray_FLOAT: *(float *)(arr->data)=*v; break;\\
-                case PyArray_CDOUBLE: *(double *)(arr->data)=*v; break;\\
-                case PyArray_CFLOAT: *(float *)(arr->data)=*v; break;\\
-                case PyArray_BOOL: *(Bool *)(arr->data)=(*v!=0); break;\\
-                case PyArray_UBYTE: *(unsigned char *)(arr->data)=*v; break;\\
-                case PyArray_BYTE: *(signed char *)(arr->data)=*v; break;\\
-                case PyArray_SHORT: *(short *)(arr->data)=*v; break;\\
-                case PyArray_USHORT: *(ushort *)(arr->data)=*v; break;\\
-                case PyArray_UINT: *(uint *)(arr->data)=*v; break;\\
-                case PyArray_ULONG: *(ulong *)(arr->data)=*v; break;\\
-                case PyArray_LONGLONG: *(longlong *)(arr->data)=*v; break;\\
-                case PyArray_ULONGLONG: *(ulonglong *)(arr->data)=*v; break;\\
-                case PyArray_LONGDOUBLE: *(longdouble *)(arr->data)=*v; break;\\
-                case PyArray_CLONGDOUBLE: *(longdouble *)(arr->data)=*v; break;\\
-                case PyArray_OBJECT: (arr->descr->f->setitem)(pyobj_from_ ## ctype ## 1(*v),arr->data, arr); break;\\
+        if (PyArray_DESCR(arr)->type==typecode)  {*(ctype *)(PyArray_DATA(arr))=*v; return 1;}\\
+        switch (PyArray_TYPE(arr)) {\\
+                case NPY_DOUBLE: *(npy_double *)(PyArray_DATA(arr))=*v; break;\\
+                case NPY_INT: *(npy_int *)(PyArray_DATA(arr))=*v; break;\\
+                case NPY_LONG: *(npy_long *)(PyArray_DATA(arr))=*v; break;\\
+                case NPY_FLOAT: *(npy_float *)(PyArray_DATA(arr))=*v; break;\\
+                case NPY_CDOUBLE: *(npy_double *)(PyArray_DATA(arr))=*v; break;\\
+                case NPY_CFLOAT: *(npy_float *)(PyArray_DATA(arr))=*v; break;\\
+                case NPY_BOOL: *(npy_bool *)(PyArray_DATA(arr))=(*v!=0); break;\\
+                case NPY_UBYTE: *(npy_ubyte *)(PyArray_DATA(arr))=*v; break;\\
+                case NPY_BYTE: *(npy_byte *)(PyArray_DATA(arr))=*v; break;\\
+                case NPY_SHORT: *(npy_short *)(PyArray_DATA(arr))=*v; break;\\
+                case NPY_USHORT: *(npy_ushort *)(PyArray_DATA(arr))=*v; break;\\
+                case NPY_UINT: *(npy_uint *)(PyArray_DATA(arr))=*v; break;\\
+                case NPY_ULONG: *(npy_ulong *)(PyArray_DATA(arr))=*v; break;\\
+                case NPY_LONGLONG: *(npy_longlong *)(PyArray_DATA(arr))=*v; break;\\
+                case NPY_ULONGLONG: *(npy_ulonglong *)(PyArray_DATA(arr))=*v; break;\\
+                case NPY_LONGDOUBLE: *(npy_longdouble *)(PyArray_DATA(arr))=*v; break;\\
+                case NPY_CLONGDOUBLE: *(npy_longdouble *)(PyArray_DATA(arr))=*v; break;\\
+                case NPY_OBJECT: PyArray_SETITEM(arr, PyArray_DATA(arr), pyobj_from_ ## ctype ## 1(*v)); break;\\
         default: return -2;\\
         };\\
         return 1
 """
 
-needs['TRYCOMPLEXPYARRAYTEMPLATE']=['PRINTPYOBJERR']
-cppmacros['TRYCOMPLEXPYARRAYTEMPLATE']="""\
-#define TRYCOMPLEXPYARRAYTEMPLATEOBJECT case PyArray_OBJECT: (arr->descr->f->setitem)(pyobj_from_complex_ ## ctype ## 1((*v)),arr->data, arr); break;
+needs['TRYCOMPLEXPYARRAYTEMPLATE'] = ['PRINTPYOBJERR']
+cppmacros['TRYCOMPLEXPYARRAYTEMPLATE'] = """\
+#define TRYCOMPLEXPYARRAYTEMPLATEOBJECT case NPY_OBJECT: PyArray_SETITEM(arr, PyArray_DATA(arr), pyobj_from_complex_ ## ctype ## 1((*v))); break;
 #define TRYCOMPLEXPYARRAYTEMPLATE(ctype,typecode)\\
         PyArrayObject *arr = NULL;\\
         if (!obj) return -2;\\
         if (!PyArray_Check(obj)) return -1;\\
         if (!(arr=(PyArrayObject *)obj)) {fprintf(stderr,\"TRYCOMPLEXPYARRAYTEMPLATE:\");PRINTPYOBJERR(obj);return 0;}\\
-        if (arr->descr->type==typecode) {\\
-            *(ctype *)(arr->data)=(*v).r;\\
-            *(ctype *)(arr->data+sizeof(ctype))=(*v).i;\\
+        if (PyArray_DESCR(arr)->type==typecode) {\\
+            *(ctype *)(PyArray_DATA(arr))=(*v).r;\\
+            *(ctype *)(PyArray_DATA(arr)+sizeof(ctype))=(*v).i;\\
             return 1;\\
         }\\
-        switch (arr->descr->type_num) {\\
-                case PyArray_CDOUBLE: *(double *)(arr->data)=(*v).r;*(double *)(arr->data+sizeof(double))=(*v).i;break;\\
-                case PyArray_CFLOAT: *(float *)(arr->data)=(*v).r;*(float *)(arr->data+sizeof(float))=(*v).i;break;\\
-                case PyArray_DOUBLE: *(double *)(arr->data)=(*v).r; break;\\
-                case PyArray_LONG: *(long *)(arr->data)=(*v).r; break;\\
-                case PyArray_FLOAT: *(float *)(arr->data)=(*v).r; break;\\
-                case PyArray_INT: *(int *)(arr->data)=(*v).r; break;\\
-                case PyArray_SHORT: *(short *)(arr->data)=(*v).r; break;\\
-                case PyArray_UBYTE: *(unsigned char *)(arr->data)=(*v).r; break;\\
-                case PyArray_BYTE: *(signed char *)(arr->data)=(*v).r; break;\\
-                case PyArray_BOOL: *(Bool *)(arr->data)=((*v).r!=0 && (*v).i!=0)); break;\\
-                case PyArray_UBYTE: *(unsigned char *)(arr->data)=(*v).r; break;\\
-                case PyArray_BYTE: *(signed char *)(arr->data)=(*v).r; break;\\
-                case PyArray_SHORT: *(short *)(arr->data)=(*v).r; break;\\
-                case PyArray_USHORT: *(ushort *)(arr->data)=(*v).r; break;\\
-                case PyArray_UINT: *(uint *)(arr->data)=(*v).r; break;\\
-                case PyArray_ULONG: *(ulong *)(arr->data)=(*v).r; break;\\
-                case PyArray_LONGLONG: *(longlong *)(arr->data)=(*v).r; break;\\
-                case PyArray_ULONGLONG: *(ulonglong *)(arr->data)=(*v).r; break;\\
-                case PyArray_LONGDOUBLE: *(longdouble *)(arr->data)=(*v).r; break;\\
-                case PyArray_CLONGDOUBLE: *(longdouble *)(arr->data)=(*v).r;*(longdouble *)(arr->data+sizeof(longdouble))=(*v).i;break;\\
-                case PyArray_OBJECT: (arr->descr->f->setitem)(pyobj_from_complex_ ## ctype ## 1((*v)),arr->data, arr); break;\\
+        switch (PyArray_TYPE(arr)) {\\
+                case NPY_CDOUBLE: *(npy_double *)(PyArray_DATA(arr))=(*v).r;\\
+                                  *(npy_double *)(PyArray_DATA(arr)+sizeof(npy_double))=(*v).i;\\
+                                  break;\\
+                case NPY_CFLOAT: *(npy_float *)(PyArray_DATA(arr))=(*v).r;\\
+                                 *(npy_float *)(PyArray_DATA(arr)+sizeof(npy_float))=(*v).i;\\
+                                 break;\\
+                case NPY_DOUBLE: *(npy_double *)(PyArray_DATA(arr))=(*v).r; break;\\
+                case NPY_LONG: *(npy_long *)(PyArray_DATA(arr))=(*v).r; break;\\
+                case NPY_FLOAT: *(npy_float *)(PyArray_DATA(arr))=(*v).r; break;\\
+                case NPY_INT: *(npy_int *)(PyArray_DATA(arr))=(*v).r; break;\\
+                case NPY_SHORT: *(npy_short *)(PyArray_DATA(arr))=(*v).r; break;\\
+                case NPY_UBYTE: *(npy_ubyte *)(PyArray_DATA(arr))=(*v).r; break;\\
+                case NPY_BYTE: *(npy_byte *)(PyArray_DATA(arr))=(*v).r; break;\\
+                case NPY_BOOL: *(npy_bool *)(PyArray_DATA(arr))=((*v).r!=0 && (*v).i!=0); break;\\
+                case NPY_USHORT: *(npy_ushort *)(PyArray_DATA(arr))=(*v).r; break;\\
+                case NPY_UINT: *(npy_uint *)(PyArray_DATA(arr))=(*v).r; break;\\
+                case NPY_ULONG: *(npy_ulong *)(PyArray_DATA(arr))=(*v).r; break;\\
+                case NPY_LONGLONG: *(npy_longlong *)(PyArray_DATA(arr))=(*v).r; break;\\
+                case NPY_ULONGLONG: *(npy_ulonglong *)(PyArray_DATA(arr))=(*v).r; break;\\
+                case NPY_LONGDOUBLE: *(npy_longdouble *)(PyArray_DATA(arr))=(*v).r; break;\\
+                case NPY_CLONGDOUBLE: *(npy_longdouble *)(PyArray_DATA(arr))=(*v).r;\\
+                                      *(npy_longdouble *)(PyArray_DATA(arr)+sizeof(npy_longdouble))=(*v).i;\\
+                                      break;\\
+                case NPY_OBJECT: PyArray_SETITEM(arr, PyArray_DATA(arr), pyobj_from_complex_ ## ctype ## 1((*v))); break;\\
                 default: return -2;\\
         };\\
         return -1;
 """
-## cppmacros['NUMFROMARROBJ']="""\
-## #define NUMFROMARROBJ(typenum,ctype) \\
-## \tif (PyArray_Check(obj)) arr = (PyArrayObject *)obj;\\
-## \telse arr = (PyArrayObject *)PyArray_ContiguousFromObject(obj,typenum,0,0);\\
-## \tif (arr) {\\
-## \t\tif (arr->descr->type_num==PyArray_OBJECT) {\\
-## \t\t\tif (!ctype ## _from_pyobj(v,(arr->descr->getitem)(arr->data),\"\"))\\
-## \t\t\tgoto capi_fail;\\
-## \t\t} else {\\
-## \t\t\t(arr->descr->cast[typenum])(arr->data,1,(char*)v,1,1);\\
-## \t\t}\\
-## \t\tif ((PyObject *)arr != obj) { Py_DECREF(arr); }\\
-## \t\treturn 1;\\
-## \t}
-## """
-## #XXX: Note that CNUMFROMARROBJ is identical with NUMFROMARROBJ
-## cppmacros['CNUMFROMARROBJ']="""\
-## #define CNUMFROMARROBJ(typenum,ctype) \\
-## \tif (PyArray_Check(obj)) arr = (PyArrayObject *)obj;\\
-## \telse arr = (PyArrayObject *)PyArray_ContiguousFromObject(obj,typenum,0,0);\\
-## \tif (arr) {\\
-## \t\tif (arr->descr->type_num==PyArray_OBJECT) {\\
-## \t\t\tif (!ctype ## _from_pyobj(v,(arr->descr->getitem)(arr->data),\"\"))\\
-## \t\t\tgoto capi_fail;\\
-## \t\t} else {\\
-## \t\t\t(arr->descr->cast[typenum])((void *)(arr->data),1,(void *)(v),1,1);\\
-## \t\t}\\
-## \t\tif ((PyObject *)arr != obj) { Py_DECREF(arr); }\\
-## \t\treturn 1;\\
-## \t}
-## """
-
-
-needs['GETSTRFROMPYTUPLE']=['STRINGCOPYN','PRINTPYOBJERR']
-cppmacros['GETSTRFROMPYTUPLE']="""\
+# cppmacros['NUMFROMARROBJ']="""\
+# define NUMFROMARROBJ(typenum,ctype) \\
+#     if (PyArray_Check(obj)) arr = (PyArrayObject *)obj;\\
+#     else arr = (PyArrayObject *)PyArray_ContiguousFromObject(obj,typenum,0,0);\\
+#     if (arr) {\\
+#         if (PyArray_TYPE(arr)==NPY_OBJECT) {\\
+#             if (!ctype ## _from_pyobj(v,(PyArray_DESCR(arr)->getitem)(PyArray_DATA(arr)),\"\"))\\
+#             goto capi_fail;\\
+#         } else {\\
+#             (PyArray_DESCR(arr)->cast[typenum])(PyArray_DATA(arr),1,(char*)v,1,1);\\
+#         }\\
+#         if ((PyObject *)arr != obj) { Py_DECREF(arr); }\\
+#         return 1;\\
+#     }
+# """
+# XXX: Note that CNUMFROMARROBJ is identical with NUMFROMARROBJ
+# cppmacros['CNUMFROMARROBJ']="""\
+# define CNUMFROMARROBJ(typenum,ctype) \\
+#     if (PyArray_Check(obj)) arr = (PyArrayObject *)obj;\\
+#     else arr = (PyArrayObject *)PyArray_ContiguousFromObject(obj,typenum,0,0);\\
+#     if (arr) {\\
+#         if (PyArray_TYPE(arr)==NPY_OBJECT) {\\
+#             if (!ctype ## _from_pyobj(v,(PyArray_DESCR(arr)->getitem)(PyArray_DATA(arr)),\"\"))\\
+#             goto capi_fail;\\
+#         } else {\\
+#             (PyArray_DESCR(arr)->cast[typenum])((void *)(PyArray_DATA(arr)),1,(void *)(v),1,1);\\
+#         }\\
+#         if ((PyObject *)arr != obj) { Py_DECREF(arr); }\\
+#         return 1;\\
+#     }
+# """
+
+
+needs['GETSTRFROMPYTUPLE'] = ['STRINGCOPYN', 'PRINTPYOBJERR']
+cppmacros['GETSTRFROMPYTUPLE'] = """\
 #define GETSTRFROMPYTUPLE(tuple,index,str,len) {\\
-\t\tPyObject *rv_cb_str = PyTuple_GetItem((tuple),(index));\\
-\t\tif (rv_cb_str == NULL)\\
-\t\t\tgoto capi_fail;\\
-\t\tif (PyString_Check(rv_cb_str)) {\\
-\t\t\tstr[len-1]='\\0';\\
-\t\t\tSTRINGCOPYN((str),PyString_AS_STRING((PyStringObject*)rv_cb_str),(len));\\
-\t\t} else {\\
-\t\t\tPRINTPYOBJERR(rv_cb_str);\\
-\t\t\tPyErr_SetString(#modulename#_error,\"string object expected\");\\
-\t\t\tgoto capi_fail;\\
-\t\t}\\
-\t}
-"""
-cppmacros['GETSCALARFROMPYTUPLE']="""\
+        PyObject *rv_cb_str = PyTuple_GetItem((tuple),(index));\\
+        if (rv_cb_str == NULL)\\
+            goto capi_fail;\\
+        if (PyBytes_Check(rv_cb_str)) {\\
+            str[len-1]='\\0';\\
+            STRINGCOPYN((str),PyBytes_AS_STRING((PyBytesObject*)rv_cb_str),(len));\\
+        } else {\\
+            PRINTPYOBJERR(rv_cb_str);\\
+            PyErr_SetString(#modulename#_error,\"string object expected\");\\
+            goto capi_fail;\\
+        }\\
+    }
+"""
+cppmacros['GETSCALARFROMPYTUPLE'] = """\
 #define GETSCALARFROMPYTUPLE(tuple,index,var,ctype,mess) {\\
-\t\tif ((capi_tmp = PyTuple_GetItem((tuple),(index)))==NULL) goto capi_fail;\\
-\t\tif (!(ctype ## _from_pyobj((var),capi_tmp,mess)))\\
-\t\t\tgoto capi_fail;\\
-\t}
-"""
-
-needs['MEMCOPY']=['string.h']
-cppmacros['MEMCOPY']="""\
+        if ((capi_tmp = PyTuple_GetItem((tuple),(index)))==NULL) goto capi_fail;\\
+        if (!(ctype ## _from_pyobj((var),capi_tmp,mess)))\\
+            goto capi_fail;\\
+    }
+"""
+
+cppmacros['FAILNULL'] = """\\
+#define FAILNULL(p) do {                                            \\
+    if ((p) == NULL) {                                              \\
+        PyErr_SetString(PyExc_MemoryError, "NULL pointer found");   \\
+        goto capi_fail;                                             \\
+    }                                                               \\
+} while (0)
+"""
+needs['MEMCOPY'] = ['string.h', 'FAILNULL']
+cppmacros['MEMCOPY'] = """\
 #define MEMCOPY(to,from,n)\\
-\tif ((memcpy(to,from,n)) == NULL) {\\
-\t\tPyErr_SetString(PyExc_MemoryError, \"memcpy failed\");\\
-\t\tgoto capi_fail;\\
-\t}
-"""
-cppmacros['STRINGMALLOC']="""\
+    do { FAILNULL(to); FAILNULL(from); (void)memcpy(to,from,n); } while (0)
+"""
+cppmacros['STRINGMALLOC'] = """\
 #define STRINGMALLOC(str,len)\\
-\tif ((str = (string)malloc(sizeof(char)*(len+1))) == NULL) {\\
-\t\tPyErr_SetString(PyExc_MemoryError, \"out of memory\");\\
-\t\tgoto capi_fail;\\
-\t} else {\\
-\t\t(str)[len] = '\\0';\\
-\t}
-"""
-cppmacros['STRINGFREE']="""\
-#define STRINGFREE(str)\\
-\tif (!(str == NULL)) free(str);
-"""
-needs['STRINGCOPYN']=['string.h']
-cppmacros['STRINGCOPYN']="""\
-#define STRINGCOPYN(to,from,n)\\
-\tif ((strncpy(to,from,sizeof(char)*(n))) == NULL) {\\
-\t\tPyErr_SetString(PyExc_MemoryError, \"strncpy failed\");\\
-\t\tgoto capi_fail;\\
-\t} else if (strlen(to)<(n)) {\\
-\t\tmemset((to)+strlen(to), ' ', (n)-strlen(to));\\
-\t} /* Padding with spaces instead of nulls. */
-"""
-needs['STRINGCOPY']=['string.h']
-cppmacros['STRINGCOPY']="""\
+    if ((str = (string)malloc(len+1)) == NULL) {\\
+        PyErr_SetString(PyExc_MemoryError, \"out of memory\");\\
+        goto capi_fail;\\
+    } else {\\
+        (str)[len] = '\\0';\\
+    }
+"""
+cppmacros['STRINGFREE'] = """\
+#define STRINGFREE(str) do {if (!(str == NULL)) free(str);} while (0)
+"""
+needs['STRINGPADN'] = ['string.h']
+cppmacros['STRINGPADN'] = """\
+/*
+STRINGPADN replaces null values with padding values from the right.
+
+`to` must have size of at least N bytes.
+
+If the `to[N-1]` has null value, then replace it and all the
+preceding, nulls with the given padding.
+
+STRINGPADN(to, N, PADDING, NULLVALUE) is an inverse operation.
+*/
+#define STRINGPADN(to, N, NULLVALUE, PADDING)                   \\
+    do {                                                        \\
+        int _m = (N);                                           \\
+        char *_to = (to);                                       \\
+        for (_m -= 1; _m >= 0 && _to[_m] == NULLVALUE; _m--) {  \\
+             _to[_m] = PADDING;                                 \\
+        }                                                       \\
+    } while (0)
+"""
+needs['STRINGCOPYN'] = ['string.h', 'FAILNULL']
+cppmacros['STRINGCOPYN'] = """\
+/*
+STRINGCOPYN copies N bytes.
+
+`to` and `from` buffers must have sizes of at least N bytes.
+*/
+#define STRINGCOPYN(to,from,N)                                  \\
+    do {                                                        \\
+        int _m = (N);                                           \\
+        char *_to = (to);                                       \\
+        char *_from = (from);                                   \\
+        FAILNULL(_to); FAILNULL(_from);                         \\
+        (void)strncpy(_to, _from, _m);             \\
+    } while (0)
+"""
+needs['STRINGCOPY'] = ['string.h', 'FAILNULL']
+cppmacros['STRINGCOPY'] = """\
 #define STRINGCOPY(to,from)\\
-\tif ((strcpy(to,from)) == NULL) {\\
-\t\tPyErr_SetString(PyExc_MemoryError, \"strcpy failed\");\\
-\t\tgoto capi_fail;\\
-\t}
-"""
-cppmacros['CHECKGENERIC']="""\
+    do { FAILNULL(to); FAILNULL(from); (void)strcpy(to,from); } while (0)
+"""
+cppmacros['CHECKGENERIC'] = """\
 #define CHECKGENERIC(check,tcheck,name) \\
-\tif (!(check)) {\\
-\t\tPyErr_SetString(#modulename#_error,\"(\"tcheck\") failed for \"name);\\
-\t\t/*goto capi_fail;*/\\
-\t} else """
-cppmacros['CHECKARRAY']="""\
+    if (!(check)) {\\
+        PyErr_SetString(#modulename#_error,\"(\"tcheck\") failed for \"name);\\
+        /*goto capi_fail;*/\\
+    } else """
+cppmacros['CHECKARRAY'] = """\
 #define CHECKARRAY(check,tcheck,name) \\
-\tif (!(check)) {\\
-\t\tPyErr_SetString(#modulename#_error,\"(\"tcheck\") failed for \"name);\\
-\t\t/*goto capi_fail;*/\\
-\t} else """
-cppmacros['CHECKSTRING']="""\
+    if (!(check)) {\\
+        PyErr_SetString(#modulename#_error,\"(\"tcheck\") failed for \"name);\\
+        /*goto capi_fail;*/\\
+    } else """
+cppmacros['CHECKSTRING'] = """\
 #define CHECKSTRING(check,tcheck,name,show,var)\\
-\tif (!(check)) {\\
-\t\tPyErr_SetString(#modulename#_error,\"(\"tcheck\") failed for \"name);\\
-\t\tfprintf(stderr,show\"\\n\",slen(var),var);\\
-\t\t/*goto capi_fail;*/\\
-\t} else """
-cppmacros['CHECKSCALAR']="""\
+    if (!(check)) {\\
+        char errstring[256];\\
+        sprintf(errstring, \"%s: \"show, \"(\"tcheck\") failed for \"name, slen(var), var);\\
+        PyErr_SetString(#modulename#_error, errstring);\\
+        /*goto capi_fail;*/\\
+    } else """
+cppmacros['CHECKSCALAR'] = """\
 #define CHECKSCALAR(check,tcheck,name,show,var)\\
-\tif (!(check)) {\\
-\t\tPyErr_SetString(#modulename#_error,\"(\"tcheck\") failed for \"name);\\
-\t\tfprintf(stderr,show\"\\n\",var);\\
-\t\t/*goto capi_fail;*/\\
-\t} else """
-## cppmacros['CHECKDIMS']="""\
-## #define CHECKDIMS(dims,rank) \\
-## \tfor (int i=0;i<(rank);i++)\\
-## \t\tif (dims[i]<0) {\\
-## \t\t\tfprintf(stderr,\"Unspecified array argument requires a complete dimension specification.\\n\");\\
-## \t\t\tgoto capi_fail;\\
-## \t\t}
-## """
-cppmacros['ARRSIZE']='#define ARRSIZE(dims,rank) (_PyArray_multiply_list(dims,rank))'
-cppmacros['OLDPYNUM']="""\
+    if (!(check)) {\\
+        char errstring[256];\\
+        sprintf(errstring, \"%s: \"show, \"(\"tcheck\") failed for \"name, var);\\
+        PyErr_SetString(#modulename#_error,errstring);\\
+        /*goto capi_fail;*/\\
+    } else """
+# cppmacros['CHECKDIMS']="""\
+# define CHECKDIMS(dims,rank) \\
+#     for (int i=0;i<(rank);i++)\\
+#         if (dims[i]<0) {\\
+#             fprintf(stderr,\"Unspecified array argument requires a complete dimension specification.\\n\");\\
+#             goto capi_fail;\\
+#         }
+# """
+cppmacros[
+    'ARRSIZE'] = '#define ARRSIZE(dims,rank) (_PyArray_multiply_list(dims,rank))'
+cppmacros['OLDPYNUM'] = """\
 #ifdef OLDPYNUM
-#error You need to intall Numeric Python version 13 or higher. Get it from http:/sourceforge.net/project/?group_id=1369
+#error You need to install NumPy version 0.13 or higher. See https://scipy.org/install.html
+#endif
+"""
+cppmacros["F2PY_THREAD_LOCAL_DECL"] = """\
+#ifndef F2PY_THREAD_LOCAL_DECL
+#if defined(_MSC_VER)
+#define F2PY_THREAD_LOCAL_DECL __declspec(thread)
+#elif defined(NPY_OS_MINGW)
+#define F2PY_THREAD_LOCAL_DECL __thread
+#elif defined(__STDC_VERSION__) \\
+      && (__STDC_VERSION__ >= 201112L) \\
+      && !defined(__STDC_NO_THREADS__) \\
+      && (!defined(__GLIBC__) || __GLIBC__ > 2 || (__GLIBC__ == 2 && __GLIBC_MINOR__ > 12)) \\
+      && !defined(NPY_OS_OPENBSD)
+/* __STDC_NO_THREADS__ was first defined in a maintenance release of glibc 2.12,
+   see https://lists.gnu.org/archive/html/commit-hurd/2012-07/msg00180.html,
+   so `!defined(__STDC_NO_THREADS__)` may give false positive for the existence
+   of `threads.h` when using an older release of glibc 2.12
+   See gh-19437 for details on OpenBSD */
+#include <threads.h>
+#define F2PY_THREAD_LOCAL_DECL thread_local
+#elif defined(__GNUC__) \\
+      && (__GNUC__ > 4 || (__GNUC__ == 4 && (__GNUC_MINOR__ >= 4)))
+#define F2PY_THREAD_LOCAL_DECL __thread
+#endif
 #endif
 """
 ################# C functions ###############
 
-cfuncs['calcarrindex']="""\
+cfuncs['calcarrindex'] = """\
 static int calcarrindex(int *i,PyArrayObject *arr) {
-\tint k,ii = i[0];
-\tfor (k=1; k < arr->nd; k++)
-\t\tii += (ii*(arr->dimensions[k] - 1)+i[k]); /* assuming contiguous arr */
-\treturn ii;
+    int k,ii = i[0];
+    for (k=1; k < PyArray_NDIM(arr); k++)
+        ii += (ii*(PyArray_DIM(arr,k) - 1)+i[k]); /* assuming contiguous arr */
+    return ii;
 }"""
-cfuncs['calcarrindextr']="""\
+cfuncs['calcarrindextr'] = """\
 static int calcarrindextr(int *i,PyArrayObject *arr) {
-\tint k,ii = i[arr->nd-1];
-\tfor (k=1; k < arr->nd; k++)
-\t\tii += (ii*(arr->dimensions[arr->nd-k-1] - 1)+i[arr->nd-k-1]); /* assuming contiguous arr */
-\treturn ii;
+    int k,ii = i[PyArray_NDIM(arr)-1];
+    for (k=1; k < PyArray_NDIM(arr); k++)
+        ii += (ii*(PyArray_DIM(arr,PyArray_NDIM(arr)-k-1) - 1)+i[PyArray_NDIM(arr)-k-1]); /* assuming contiguous arr */
+    return ii;
 }"""
-cfuncs['forcomb']="""\
-static struct { int nd;intp *d;int *i,*i_tr,tr; } forcombcache;
-static int initforcomb(intp *dims,int nd,int tr) {
+cfuncs['forcomb'] = """\
+static struct { int nd;npy_intp *d;int *i,*i_tr,tr; } forcombcache;
+static int initforcomb(npy_intp *dims,int nd,int tr) {
   int k;
   if (dims==NULL) return 0;
   if (nd<0) return 0;
@@ -542,593 +648,820 @@
   if (forcombcache.tr) return i_tr;
   return i;
 }"""
-needs['try_pyarr_from_string']=['STRINGCOPYN','PRINTPYOBJERR','string']
-cfuncs['try_pyarr_from_string']="""\
-static int try_pyarr_from_string(PyObject *obj,const string str) {
-\tPyArrayObject *arr = NULL;
-\tif (PyArray_Check(obj) && (!((arr = (PyArrayObject *)obj) == NULL)))
-\t\t{ STRINGCOPYN(arr->data,str,PyArray_NBYTES(arr)); }
-\treturn 1;
+needs['try_pyarr_from_string'] = ['STRINGCOPYN', 'PRINTPYOBJERR', 'string']
+cfuncs['try_pyarr_from_string'] = """\
+/*
+  try_pyarr_from_string copies str[:len(obj)] to the data of an `ndarray`.
+
+  If obj is an `ndarray`, it is assumed to be contiguous.
+
+  If the specified len==-1, str must be null-terminated.
+*/
+static int try_pyarr_from_string(PyObject *obj,
+                                 const string str, const int len) {
+#ifdef DEBUGCFUNCS
+fprintf(stderr, "try_pyarr_from_string(str='%s', len=%d, obj=%p)\\n",
+        (char*)str,len, obj);
+#endif
+    if (PyArray_Check(obj)) {
+        PyArrayObject *arr = (PyArrayObject *)obj;
+        assert(ISCONTIGUOUS(arr));
+        string buf = PyArray_DATA(arr);
+        npy_intp n = len;
+        if (n == -1) {
+            /* Assuming null-terminated str. */
+            n = strlen(str);
+        }
+        if (n > PyArray_NBYTES(arr)) {
+            n = PyArray_NBYTES(arr);
+        }
+        STRINGCOPYN(buf, str, n);
+        return 1;
+    }
 capi_fail:
-\tPRINTPYOBJERR(obj);
-\tPyErr_SetString(#modulename#_error,\"try_pyarr_from_string failed\");
-\treturn 0;
-}
-"""
-needs['string_from_pyobj']=['string','STRINGMALLOC','STRINGCOPYN']
-cfuncs['string_from_pyobj']="""\
-static int string_from_pyobj(string *str,int *len,const string inistr,PyObject *obj,const char *errmess) {
-\tPyArrayObject *arr = NULL;
-\tPyObject *tmp = NULL;
+    PRINTPYOBJERR(obj);
+    PyErr_SetString(#modulename#_error, \"try_pyarr_from_string failed\");
+    return 0;
+}
+"""
+needs['string_from_pyobj'] = ['string', 'STRINGMALLOC', 'STRINGCOPYN']
+cfuncs['string_from_pyobj'] = """\
+/*
+  Create a new string buffer `str` of at most length `len` from a
+  Python string-like object `obj`.
+
+  The string buffer has given size (len) or the size of inistr when len==-1.
+
+  The string buffer is padded with blanks: in Fortran, trailing blanks
+  are insignificant contrary to C nulls.
+ */
+static int
+string_from_pyobj(string *str, int *len, const string inistr, PyObject *obj,
+                  const char *errmess)
+{
+    PyObject *tmp = NULL;
+    string buf = NULL;
+    npy_intp n = -1;
 #ifdef DEBUGCFUNCS
-fprintf(stderr,\"string_from_pyobj(str='%s',len=%d,inistr='%s',obj=%p)\\n\",(char*)str,*len,(char *)inistr,obj);
-#endif
-\tif (obj == Py_None) {
-\t\tif (*len == -1)
-\t\t\t*len = strlen(inistr); /* Will this cause problems? */
-\t\tSTRINGMALLOC(*str,*len);
-\t\tSTRINGCOPYN(*str,inistr,*len);
-\t\treturn 1;
-\t}
-\tif (PyArray_Check(obj)) {
-\t\tif ((arr = (PyArrayObject *)obj) == NULL)
-\t\t\tgoto capi_fail;
-\t\tif (!ISCONTIGUOUS(arr)) {
-\t\t\tPyErr_SetString(PyExc_ValueError,\"array object is non-contiguous.\");
-\t\t\tgoto capi_fail;
-\t\t}
-\t\tif (*len == -1)
-\t\t\t*len = (arr->descr->elsize)*PyArray_SIZE(arr);
-\t\tSTRINGMALLOC(*str,*len);
-\t\tSTRINGCOPYN(*str,arr->data,*len);
-\t\treturn 1;
-\t}
-\tif (PyString_Check(obj)) {
-\t\ttmp = obj;
-\t\tPy_INCREF(tmp);
-\t}
-\telse
-\t\ttmp = PyObject_Str(obj);
-\tif (tmp == NULL) goto capi_fail;
-\tif (*len == -1)
-\t\t*len = PyString_GET_SIZE(tmp);
-\tSTRINGMALLOC(*str,*len);
-\tSTRINGCOPYN(*str,PyString_AS_STRING(tmp),*len);
-\tPy_DECREF(tmp);
-\treturn 1;
+fprintf(stderr,\"string_from_pyobj(str='%s',len=%d,inistr='%s',obj=%p)\\n\",
+               (char*)str, *len, (char *)inistr, obj);
+#endif
+    if (obj == Py_None) {
+        n = strlen(inistr);
+        buf = inistr;
+    }
+    else if (PyArray_Check(obj)) {
+        PyArrayObject *arr = (PyArrayObject *)obj;
+        if (!ISCONTIGUOUS(arr)) {
+            PyErr_SetString(PyExc_ValueError,
+                            \"array object is non-contiguous.\");
+            goto capi_fail;
+        }
+        n = PyArray_NBYTES(arr);
+        buf = PyArray_DATA(arr);
+        n = strnlen(buf, n);
+    }
+    else {
+        if (PyBytes_Check(obj)) {
+            tmp = obj;
+            Py_INCREF(tmp);
+        }
+        else if (PyUnicode_Check(obj)) {
+            tmp = PyUnicode_AsASCIIString(obj);
+        }
+        else {
+            PyObject *tmp2;
+            tmp2 = PyObject_Str(obj);
+            if (tmp2) {
+                tmp = PyUnicode_AsASCIIString(tmp2);
+                Py_DECREF(tmp2);
+            }
+            else {
+                tmp = NULL;
+            }
+        }
+        if (tmp == NULL) goto capi_fail;
+        n = PyBytes_GET_SIZE(tmp);
+        buf = PyBytes_AS_STRING(tmp);
+    }
+    if (*len == -1) {
+        /* TODO: change the type of `len` so that we can remove this */
+        if (n > NPY_MAX_INT) {
+            PyErr_SetString(PyExc_OverflowError,
+                            "object too large for a 32-bit int");
+            goto capi_fail;
+        }
+        *len = n;
+    }
+    else if (*len < n) {
+        /* discard the last (len-n) bytes of input buf */
+        n = *len;
+    }
+    if (n < 0 || *len < 0 || buf == NULL) {
+        goto capi_fail;
+    }
+    STRINGMALLOC(*str, *len);  // *str is allocated with size (*len + 1)
+    if (n < *len) {
+        /*
+          Pad fixed-width string with nulls. The caller will replace
+          nulls with blanks when the corresponding argument is not
+          intent(c).
+        */
+        memset(*str + n, '\\0', *len - n);
+    }
+    STRINGCOPYN(*str, buf, n);
+    Py_XDECREF(tmp);
+    return 1;
 capi_fail:
-\tPy_XDECREF(tmp);
-\t{
-\t\tPyObject* err = PyErr_Occurred();
-\t\tif (err==NULL) err = #modulename#_error;
-\t\tPyErr_SetString(err,errmess);
-\t}
-\treturn 0;
-}
-"""
-needs['char_from_pyobj']=['int_from_pyobj']
-cfuncs['char_from_pyobj']="""\
-static int char_from_pyobj(char* v,PyObject *obj,const char *errmess) {
-\tint i=0;
-\tif (int_from_pyobj(&i,obj,errmess)) {
-\t\t*v = (char)i;
-\t\treturn 1;
-\t}
-\treturn 0;
-}
-"""
-needs['signed_char_from_pyobj']=['int_from_pyobj','signed_char']
-cfuncs['signed_char_from_pyobj']="""\
-static int signed_char_from_pyobj(signed_char* v,PyObject *obj,const char *errmess) {
-\tint i=0;
-\tif (int_from_pyobj(&i,obj,errmess)) {
-\t\t*v = (signed_char)i;
-\t\treturn 1;
-\t}
-\treturn 0;
-}
-"""
-needs['short_from_pyobj']=['int_from_pyobj']
-cfuncs['short_from_pyobj']="""\
-static int short_from_pyobj(short* v,PyObject *obj,const char *errmess) {
-\tint i=0;
-\tif (int_from_pyobj(&i,obj,errmess)) {
-\t\t*v = (short)i;
-\t\treturn 1;
-\t}
-\treturn 0;
-}
-"""
-cfuncs['int_from_pyobj']="""\
-static int int_from_pyobj(int* v,PyObject *obj,const char *errmess) {
-\tPyObject* tmp = NULL;
-\tif (PyInt_Check(obj)) {
-\t\t*v = (int)PyInt_AS_LONG(obj);
-\t\treturn 1;
-\t}
-\ttmp = PyNumber_Int(obj);
-\tif (tmp) {
-\t\t*v = PyInt_AS_LONG(tmp);
-\t\tPy_DECREF(tmp);
-\t\treturn 1;
-\t}
-\tif (PyComplex_Check(obj))
-\t\ttmp = PyObject_GetAttrString(obj,\"real\");
-\telse if (PyString_Check(obj))
-\t\t/*pass*/;
-\telse if (PySequence_Check(obj))
-\t\ttmp = PySequence_GetItem(obj,0);
-\tif (tmp) {
-\t\tPyErr_Clear();
-\t\tif (int_from_pyobj(v,tmp,errmess)) {Py_DECREF(tmp); return 1;}
-\t\tPy_DECREF(tmp);
-\t}
-\t{
-\t\tPyObject* err = PyErr_Occurred();
-\t\tif (err==NULL) err = #modulename#_error;
-\t\tPyErr_SetString(err,errmess);
-\t}
-\treturn 0;
-}
-"""
-cfuncs['long_from_pyobj']="""\
-static int long_from_pyobj(long* v,PyObject *obj,const char *errmess) {
-\tPyObject* tmp = NULL;
-\tif (PyInt_Check(obj)) {
-\t\t*v = PyInt_AS_LONG(obj);
-\t\treturn 1;
-\t}
-\ttmp = PyNumber_Int(obj);
-\tif (tmp) {
-\t\t*v = PyInt_AS_LONG(tmp);
-\t\tPy_DECREF(tmp);
-\t\treturn 1;
-\t}
-\tif (PyComplex_Check(obj))
-\t\ttmp = PyObject_GetAttrString(obj,\"real\");
-\telse if (PyString_Check(obj))
-\t\t/*pass*/;
-\telse if (PySequence_Check(obj))
-\t\ttmp = PySequence_GetItem(obj,0);
-\tif (tmp) {
-\t\tPyErr_Clear();
-\t\tif (long_from_pyobj(v,tmp,errmess)) {Py_DECREF(tmp); return 1;}
-\t\tPy_DECREF(tmp);
-\t}
-\t{
-\t\tPyObject* err = PyErr_Occurred();
-\t\tif (err==NULL) err = #modulename#_error;
-\t\tPyErr_SetString(err,errmess);
-\t}
-\treturn 0;
-}
-"""
-needs['long_long_from_pyobj']=['long_long']
-cfuncs['long_long_from_pyobj']="""\
-static int long_long_from_pyobj(long_long* v,PyObject *obj,const char *errmess) {
-\tPyObject* tmp = NULL;
-\tif (PyLong_Check(obj)) {
-\t\t*v = PyLong_AsLongLong(obj);
-\t\treturn (!PyErr_Occurred());
-\t}
-\tif (PyInt_Check(obj)) {
-\t\t*v = (long_long)PyInt_AS_LONG(obj);
-\t\treturn 1;
-\t}
-\ttmp = PyNumber_Long(obj);
-\tif (tmp) {
-\t\t*v = PyLong_AsLongLong(tmp);
-\t\tPy_DECREF(tmp);
-\t\treturn (!PyErr_Occurred());
-\t}
-\tif (PyComplex_Check(obj))
-\t\ttmp = PyObject_GetAttrString(obj,\"real\");
-\telse if (PyString_Check(obj))
-\t\t/*pass*/;
-\telse if (PySequence_Check(obj))
-\t\ttmp = PySequence_GetItem(obj,0);
-\tif (tmp) {
-\t\tPyErr_Clear();
-\t\tif (long_long_from_pyobj(v,tmp,errmess)) {Py_DECREF(tmp); return 1;}
-\t\tPy_DECREF(tmp);
-\t}
-\t{
-\t\tPyObject* err = PyErr_Occurred();
-\t\tif (err==NULL) err = #modulename#_error;
-\t\tPyErr_SetString(err,errmess);
-\t}
-\treturn 0;
-}
-"""
-needs['long_double_from_pyobj']=['double_from_pyobj','long_double']
-cfuncs['long_double_from_pyobj']="""\
-static int long_double_from_pyobj(long_double* v,PyObject *obj,const char *errmess) {
-\tdouble d=0;
-\tif (PyArray_CheckScalar(obj)){
-\t\tif PyArray_IsScalar(obj, LongDouble) {
-\t\t\tPyArray_ScalarAsCtype(obj, v);
-\t\t\treturn 1;
-\t\t}
-\t\telse if (PyArray_Check(obj) && PyArray_TYPE(obj)==PyArray_LONGDOUBLE) {
-\t\t\t(*v) = *((longdouble *)PyArray_DATA(obj))
-\t\t\treturn 1;
-\t\t}
-\t}
-\tif (double_from_pyobj(&d,obj,errmess)) {
-\t\t*v = (long_double)d;
-\t\treturn 1;
-\t}
-\treturn 0;
-}
-"""
-cfuncs['double_from_pyobj']="""\
-static int double_from_pyobj(double* v,PyObject *obj,const char *errmess) {
-\tPyObject* tmp = NULL;
-\tif (PyFloat_Check(obj)) {
-#ifdef __sgi
-\t\t*v = PyFloat_AsDouble(obj);
-#else
-\t\t*v = PyFloat_AS_DOUBLE(obj);
-#endif
-\t\treturn 1;
-\t}
-\ttmp = PyNumber_Float(obj);
-\tif (tmp) {
-#ifdef __sgi
-\t\t*v = PyFloat_AsDouble(tmp);
-#else
-\t\t*v = PyFloat_AS_DOUBLE(tmp);
-#endif
-\t\tPy_DECREF(tmp);
-\t\treturn 1;
-\t}
-\tif (PyComplex_Check(obj))
-\t\ttmp = PyObject_GetAttrString(obj,\"real\");
-\telse if (PyString_Check(obj))
-\t\t/*pass*/;
-\telse if (PySequence_Check(obj))
-\t\ttmp = PySequence_GetItem(obj,0);
-\tif (tmp) {
-\t\tPyErr_Clear();
-\t\tif (double_from_pyobj(v,tmp,errmess)) {Py_DECREF(tmp); return 1;}
-\t\tPy_DECREF(tmp);
-\t}
-\t{
-\t\tPyObject* err = PyErr_Occurred();
-\t\tif (err==NULL) err = #modulename#_error;
-\t\tPyErr_SetString(err,errmess);
-\t}
-\treturn 0;
-}
-"""
-needs['float_from_pyobj']=['double_from_pyobj']
-cfuncs['float_from_pyobj']="""\
-static int float_from_pyobj(float* v,PyObject *obj,const char *errmess) {
-\tdouble d=0.0;
-\tif (double_from_pyobj(&d,obj,errmess)) {
-\t\t*v = (float)d;
-\t\treturn 1;
-\t}
-\treturn 0;
-}
-"""
-needs['complex_long_double_from_pyobj']=['complex_long_double','long_double',
-                                         'complex_double_from_pyobj']
-cfuncs['complex_long_double_from_pyobj']="""\
-static int complex_long_double_from_pyobj(complex_long_double* v,PyObject *obj,const char *errmess) {
-\tcomplex_double cd={0.0,0.0};
-\tif (PyArray_CheckScalar(obj)){
-\t\tif PyArray_IsScalar(obj, CLongDouble) {
-\t\t\tPyArray_ScalarAsCtype(obj, v);
-\t\t\treturn 1;
-\t\t}
-\t\telse if (PyArray_Check(obj) && PyArray_TYPE(obj)==PyArray_CLONGDOUBLE) {
-\t\t\t(*v).r = ((clongdouble *)PyArray_DATA(obj))->real;
-\t\t\t(*v).i = ((clongdouble *)PyArray_DATA(obj))->imag;
-\t\t\treturn 1;
-\t\t}
-\t}
-\tif (complex_double_from_pyobj(&cd,obj,errmess)) {
-\t\t(*v).r = (long_double)cd.r;
-\t\t(*v).i = (long_double)cd.i;
-\t\treturn 1;
-\t}
-\treturn 0;
-}
-"""
-needs['complex_double_from_pyobj']=['complex_double']
-cfuncs['complex_double_from_pyobj']="""\
-static int complex_double_from_pyobj(complex_double* v,PyObject *obj,const char *errmess) {
-\tPy_complex c;
-\tif (PyComplex_Check(obj)) {
-\t\tc=PyComplex_AsCComplex(obj);
-\t\t(*v).r=c.real, (*v).i=c.imag;
-\t\treturn 1;
-\t}
-\tif (PyArray_IsScalar(obj, ComplexFloating)) {
-\t\tif (PyArray_IsScalar(obj, CFloat)) {
-\t\t\tcfloat new;
-\t\t\tPyArray_ScalarAsCtype(obj, &new);
-\t\t\t(*v).r = (double)new.real;
-\t\t\t(*v).i = (double)new.imag;
-\t\t}
-\t\telse if (PyArray_IsScalar(obj, CLongDouble)) {
-\t\t\tclongdouble new;
-\t\t\tPyArray_ScalarAsCtype(obj, &new);
-\t\t\t(*v).r = (double)new.real;
-\t\t\t(*v).i = (double)new.imag;
-\t\t}
-\t\telse { /* if (PyArray_IsScalar(obj, CDouble)) */
-\t\t\tPyArray_ScalarAsCtype(obj, v);
-\t\t}
-\t\treturn 1;
-\t}
-\tif (PyArray_CheckScalar(obj)) { /* 0-dim array or still array scalar */
-\t\tPyObject *arr;
-\t\tif (PyArray_Check(obj)) {
-\t\t\tarr = PyArray_Cast((PyArrayObject *)obj, PyArray_CDOUBLE);
-\t\t}
-\t\telse {
-\t\t\tarr = PyArray_FromScalar(obj, PyArray_DescrFromType(PyArray_CDOUBLE));
-\t\t}
-\t\tif (arr==NULL) return 0;
-\t\t(*v).r = ((cdouble *)PyArray_DATA(arr))->real;
-\t\t(*v).i = ((cdouble *)PyArray_DATA(arr))->imag;
-\t\treturn 1;
-\t}
-\t/* Python does not provide PyNumber_Complex function :-( */
-\t(*v).i=0.0;
-\tif (PyFloat_Check(obj)) {
-#ifdef __sgi
-\t\t(*v).r = PyFloat_AsDouble(obj);
-#else
-\t\t(*v).r = PyFloat_AS_DOUBLE(obj);
-#endif
-\t\treturn 1;
-\t}
-\tif (PyInt_Check(obj)) {
-\t\t(*v).r = (double)PyInt_AS_LONG(obj);
-\t\treturn 1;
-\t}
-\tif (PyLong_Check(obj)) {
-\t\t(*v).r = PyLong_AsDouble(obj);
-\t\treturn (!PyErr_Occurred());
-\t}
-\tif (PySequence_Check(obj) && (!PyString_Check(obj))) {
-\t\tPyObject *tmp = PySequence_GetItem(obj,0);
-\t\tif (tmp) {
-\t\t\tif (complex_double_from_pyobj(v,tmp,errmess)) {
-\t\t\t\tPy_DECREF(tmp);
-\t\t\t\treturn 1;
-\t\t\t}
-\t\t\tPy_DECREF(tmp);
-\t\t}
-\t}
-\t{
-\t\tPyObject* err = PyErr_Occurred();
-\t\tif (err==NULL)
-\t\t\terr = PyExc_TypeError;
-\t\tPyErr_SetString(err,errmess);
-\t}
-\treturn 0;
-}
-"""
-needs['complex_float_from_pyobj']=['complex_float','complex_double_from_pyobj']
-cfuncs['complex_float_from_pyobj']="""\
-static int complex_float_from_pyobj(complex_float* v,PyObject *obj,const char *errmess) {
-\tcomplex_double cd={0.0,0.0};
-\tif (complex_double_from_pyobj(&cd,obj,errmess)) {
-\t\t(*v).r = (float)cd.r;
-\t\t(*v).i = (float)cd.i;
-\t\treturn 1;
-\t}
-\treturn 0;
-}
-"""
-needs['try_pyarr_from_char']=['pyobj_from_char1','TRYPYARRAYTEMPLATE']
-cfuncs['try_pyarr_from_char']='static int try_pyarr_from_char(PyObject* obj,char* v) {\n\tTRYPYARRAYTEMPLATE(char,\'c\');\n}\n'
-needs['try_pyarr_from_signed_char']=['TRYPYARRAYTEMPLATE','unsigned_char']
-cfuncs['try_pyarr_from_unsigned_char']='static int try_pyarr_from_unsigned_char(PyObject* obj,unsigned_char* v) {\n\tTRYPYARRAYTEMPLATE(unsigned_char,\'b\');\n}\n'
-needs['try_pyarr_from_signed_char']=['TRYPYARRAYTEMPLATE','signed_char']
-cfuncs['try_pyarr_from_signed_char']='static int try_pyarr_from_signed_char(PyObject* obj,signed_char* v) {\n\tTRYPYARRAYTEMPLATE(signed_char,\'1\');\n}\n'
-needs['try_pyarr_from_short']=['pyobj_from_short1','TRYPYARRAYTEMPLATE']
-cfuncs['try_pyarr_from_short']='static int try_pyarr_from_short(PyObject* obj,short* v) {\n\tTRYPYARRAYTEMPLATE(short,\'s\');\n}\n'
-needs['try_pyarr_from_int']=['pyobj_from_int1','TRYPYARRAYTEMPLATE']
-cfuncs['try_pyarr_from_int']='static int try_pyarr_from_int(PyObject* obj,int* v) {\n\tTRYPYARRAYTEMPLATE(int,\'i\');\n}\n'
-needs['try_pyarr_from_long']=['pyobj_from_long1','TRYPYARRAYTEMPLATE']
-cfuncs['try_pyarr_from_long']='static int try_pyarr_from_long(PyObject* obj,long* v) {\n\tTRYPYARRAYTEMPLATE(long,\'l\');\n}\n'
-needs['try_pyarr_from_long_long']=['pyobj_from_long_long1','TRYPYARRAYTEMPLATE','long_long']
-cfuncs['try_pyarr_from_long_long']='static int try_pyarr_from_long_long(PyObject* obj,long_long* v) {\n\tTRYPYARRAYTEMPLATE(long_long,\'L\');\n}\n'
-needs['try_pyarr_from_float']=['pyobj_from_float1','TRYPYARRAYTEMPLATE']
-cfuncs['try_pyarr_from_float']='static int try_pyarr_from_float(PyObject* obj,float* v) {\n\tTRYPYARRAYTEMPLATE(float,\'f\');\n}\n'
-needs['try_pyarr_from_double']=['pyobj_from_double1','TRYPYARRAYTEMPLATE']
-cfuncs['try_pyarr_from_double']='static int try_pyarr_from_double(PyObject* obj,double* v) {\n\tTRYPYARRAYTEMPLATE(double,\'d\');\n}\n'
-needs['try_pyarr_from_complex_float']=['pyobj_from_complex_float1','TRYCOMPLEXPYARRAYTEMPLATE','complex_float']
-cfuncs['try_pyarr_from_complex_float']='static int try_pyarr_from_complex_float(PyObject* obj,complex_float* v) {\n\tTRYCOMPLEXPYARRAYTEMPLATE(float,\'F\');\n}\n'
-needs['try_pyarr_from_complex_double']=['pyobj_from_complex_double1','TRYCOMPLEXPYARRAYTEMPLATE','complex_double']
-cfuncs['try_pyarr_from_complex_double']='static int try_pyarr_from_complex_double(PyObject* obj,complex_double* v) {\n\tTRYCOMPLEXPYARRAYTEMPLATE(double,\'D\');\n}\n'
-
-needs['create_cb_arglist']=['CFUNCSMESS','PRINTPYOBJERR','MINMAX']
-cfuncs['create_cb_arglist']="""\
-static int create_cb_arglist(PyObject* fun,PyTupleObject* xa,const int maxnofargs,const int nofoptargs,int *nofargs,PyTupleObject **args,const char *errmess) {
-\tPyObject *tmp = NULL;
-\tPyObject *tmp_fun = NULL;
-\tint tot,opt,ext,siz,i,di=0;
-\tCFUNCSMESS(\"create_cb_arglist\\n\");
-\ttot=opt=ext=siz=0;
-\t/* Get the total number of arguments */
-\tif (PyFunction_Check(fun))
-\t\ttmp_fun = fun;
-\telse {
-\t\tdi = 1;
-\t\tif (PyObject_HasAttrString(fun,\"im_func\")) {
-\t\t\ttmp_fun = PyObject_GetAttrString(fun,\"im_func\");
-\t\t}
-\t\telse if (PyObject_HasAttrString(fun,\"__call__\")) {
-\t\t\ttmp = PyObject_GetAttrString(fun,\"__call__\");
-\t\t\tif (PyObject_HasAttrString(tmp,\"im_func\"))
-\t\t\t\ttmp_fun = PyObject_GetAttrString(tmp,\"im_func\");
-\t\t\telse {
-\t\t\t\ttmp_fun = fun; /* built-in function */
-\t\t\t\ttot = maxnofargs;
-\t\t\t\tif (xa != NULL)
-\t\t\t\t\ttot += PyTuple_Size((PyObject *)xa);
-\t\t\t}
-\t\t\tPy_XDECREF(tmp);
-\t\t}
-\t\telse if (PyFortran_Check(fun) || PyFortran_Check1(fun)) {
-\t\t\ttot = maxnofargs;
-\t\t\tif (xa != NULL)
-\t\t\t\ttot += PyTuple_Size((PyObject *)xa);
-\t\t\ttmp_fun = fun;
-\t\t}
-\t\telse if (PyCObject_Check(fun)) {
-\t\t\ttot = maxnofargs;
-\t\t\tif (xa != NULL)
-\t\t\t\text = PyTuple_Size((PyObject *)xa);
-\t\t\tif(ext>0) {
-\t\t\t\tfprintf(stderr,\"extra arguments tuple cannot be used with CObject call-back\\n\");
-\t\t\t\tgoto capi_fail;
-\t\t\t}
-\t\t\ttmp_fun = fun;
-\t\t}
-\t}
-if (tmp_fun==NULL) {
-fprintf(stderr,\"Call-back argument must be function|instance|instance.__call__|f2py-function but got %s.\\n\",(fun==NULL?\"NULL\":fun->ob_type->tp_name));
-goto capi_fail;
-}
-\tif (PyObject_HasAttrString(tmp_fun,\"func_code\")) {
-\t\tif (PyObject_HasAttrString(tmp = PyObject_GetAttrString(tmp_fun,\"func_code\"),\"co_argcount\"))
-\t\t\ttot = PyInt_AsLong(PyObject_GetAttrString(tmp,\"co_argcount\")) - di;
-\t\tPy_XDECREF(tmp);
-\t}
-\t/* Get the number of optional arguments */
-\tif (PyObject_HasAttrString(tmp_fun,\"func_defaults\"))
-\t\tif (PyTuple_Check(tmp = PyObject_GetAttrString(tmp_fun,\"func_defaults\")))
-\t\t\topt = PyTuple_Size(tmp);
-\t\tPy_XDECREF(tmp);
-\t/* Get the number of extra arguments */
-\tif (xa != NULL)
-\t\text = PyTuple_Size((PyObject *)xa);
-\t/* Calculate the size of call-backs argument list */
-\tsiz = MIN(maxnofargs+ext,tot);
-\t*nofargs = MAX(0,siz-ext);
+    Py_XDECREF(tmp);
+    {
+        PyObject* err = PyErr_Occurred();
+        if (err == NULL) {
+            err = #modulename#_error;
+        }
+        PyErr_SetString(err, errmess);
+    }
+    return 0;
+}
+"""
+
+needs['char_from_pyobj'] = ['int_from_pyobj']
+cfuncs['char_from_pyobj'] = """\
+static int
+char_from_pyobj(char* v, PyObject *obj, const char *errmess) {
+    int i = 0;
+    if (int_from_pyobj(&i, obj, errmess)) {
+        *v = (char)i;
+        return 1;
+    }
+    return 0;
+}
+"""
+
+
+needs['signed_char_from_pyobj'] = ['int_from_pyobj', 'signed_char']
+cfuncs['signed_char_from_pyobj'] = """\
+static int
+signed_char_from_pyobj(signed_char* v, PyObject *obj, const char *errmess) {
+    int i = 0;
+    if (int_from_pyobj(&i, obj, errmess)) {
+        *v = (signed_char)i;
+        return 1;
+    }
+    return 0;
+}
+"""
+
+
+needs['short_from_pyobj'] = ['int_from_pyobj']
+cfuncs['short_from_pyobj'] = """\
+static int
+short_from_pyobj(short* v, PyObject *obj, const char *errmess) {
+    int i = 0;
+    if (int_from_pyobj(&i, obj, errmess)) {
+        *v = (short)i;
+        return 1;
+    }
+    return 0;
+}
+"""
+
+
+cfuncs['int_from_pyobj'] = """\
+static int
+int_from_pyobj(int* v, PyObject *obj, const char *errmess)
+{
+    PyObject* tmp = NULL;
+
+    if (PyLong_Check(obj)) {
+        *v = Npy__PyLong_AsInt(obj);
+        return !(*v == -1 && PyErr_Occurred());
+    }
+
+    tmp = PyNumber_Long(obj);
+    if (tmp) {
+        *v = Npy__PyLong_AsInt(tmp);
+        Py_DECREF(tmp);
+        return !(*v == -1 && PyErr_Occurred());
+    }
+
+    if (PyComplex_Check(obj)) {
+        PyErr_Clear();
+        tmp = PyObject_GetAttrString(obj,\"real\");
+    }
+    else if (PyBytes_Check(obj) || PyUnicode_Check(obj)) {
+        /*pass*/;
+    }
+    else if (PySequence_Check(obj)) {
+        PyErr_Clear();
+        tmp = PySequence_GetItem(obj, 0);
+    }
+
+    if (tmp) {
+        if (int_from_pyobj(v, tmp, errmess)) {
+            Py_DECREF(tmp);
+            return 1;
+        }
+        Py_DECREF(tmp);
+    }
+
+    {
+        PyObject* err = PyErr_Occurred();
+        if (err == NULL) {
+            err = #modulename#_error;
+        }
+        PyErr_SetString(err, errmess);
+    }
+    return 0;
+}
+"""
+
+
+cfuncs['long_from_pyobj'] = """\
+static int
+long_from_pyobj(long* v, PyObject *obj, const char *errmess) {
+    PyObject* tmp = NULL;
+
+    if (PyLong_Check(obj)) {
+        *v = PyLong_AsLong(obj);
+        return !(*v == -1 && PyErr_Occurred());
+    }
+
+    tmp = PyNumber_Long(obj);
+    if (tmp) {
+        *v = PyLong_AsLong(tmp);
+        Py_DECREF(tmp);
+        return !(*v == -1 && PyErr_Occurred());
+    }
+
+    if (PyComplex_Check(obj)) {
+        PyErr_Clear();
+        tmp = PyObject_GetAttrString(obj,\"real\");
+    }
+    else if (PyBytes_Check(obj) || PyUnicode_Check(obj)) {
+        /*pass*/;
+    }
+    else if (PySequence_Check(obj)) {
+        PyErr_Clear();
+        tmp = PySequence_GetItem(obj, 0);
+    }
+
+    if (tmp) {
+        if (long_from_pyobj(v, tmp, errmess)) {
+            Py_DECREF(tmp);
+            return 1;
+        }
+        Py_DECREF(tmp);
+    }
+    {
+        PyObject* err = PyErr_Occurred();
+        if (err == NULL) {
+            err = #modulename#_error;
+        }
+        PyErr_SetString(err, errmess);
+    }
+    return 0;
+}
+"""
+
+
+needs['long_long_from_pyobj'] = ['long_long']
+cfuncs['long_long_from_pyobj'] = """\
+static int
+long_long_from_pyobj(long_long* v, PyObject *obj, const char *errmess)
+{
+    PyObject* tmp = NULL;
+
+    if (PyLong_Check(obj)) {
+        *v = PyLong_AsLongLong(obj);
+        return !(*v == -1 && PyErr_Occurred());
+    }
+
+    tmp = PyNumber_Long(obj);
+    if (tmp) {
+        *v = PyLong_AsLongLong(tmp);
+        Py_DECREF(tmp);
+        return !(*v == -1 && PyErr_Occurred());
+    }
+
+    if (PyComplex_Check(obj)) {
+        PyErr_Clear();
+        tmp = PyObject_GetAttrString(obj,\"real\");
+    }
+    else if (PyBytes_Check(obj) || PyUnicode_Check(obj)) {
+        /*pass*/;
+    }
+    else if (PySequence_Check(obj)) {
+        PyErr_Clear();
+        tmp = PySequence_GetItem(obj, 0);
+    }
+
+    if (tmp) {
+        if (long_long_from_pyobj(v, tmp, errmess)) {
+            Py_DECREF(tmp);
+            return 1;
+        }
+        Py_DECREF(tmp);
+    }
+    {
+        PyObject* err = PyErr_Occurred();
+        if (err == NULL) {
+            err = #modulename#_error;
+        }
+        PyErr_SetString(err,errmess);
+    }
+    return 0;
+}
+"""
+
+
+needs['long_double_from_pyobj'] = ['double_from_pyobj', 'long_double']
+cfuncs['long_double_from_pyobj'] = """\
+static int
+long_double_from_pyobj(long_double* v, PyObject *obj, const char *errmess)
+{
+    double d=0;
+    if (PyArray_CheckScalar(obj)){
+        if PyArray_IsScalar(obj, LongDouble) {
+            PyArray_ScalarAsCtype(obj, v);
+            return 1;
+        }
+        else if (PyArray_Check(obj) && PyArray_TYPE(obj) == NPY_LONGDOUBLE) {
+            (*v) = *((npy_longdouble *)PyArray_DATA(obj));
+            return 1;
+        }
+    }
+    if (double_from_pyobj(&d, obj, errmess)) {
+        *v = (long_double)d;
+        return 1;
+    }
+    return 0;
+}
+"""
+
+
+cfuncs['double_from_pyobj'] = """\
+static int
+double_from_pyobj(double* v, PyObject *obj, const char *errmess)
+{
+    PyObject* tmp = NULL;
+    if (PyFloat_Check(obj)) {
+        *v = PyFloat_AsDouble(obj);
+        return !(*v == -1.0 && PyErr_Occurred());
+    }
+
+    tmp = PyNumber_Float(obj);
+    if (tmp) {
+        *v = PyFloat_AsDouble(tmp);
+        Py_DECREF(tmp);
+        return !(*v == -1.0 && PyErr_Occurred());
+    }
+
+    if (PyComplex_Check(obj)) {
+        PyErr_Clear();
+        tmp = PyObject_GetAttrString(obj,\"real\");
+    }
+    else if (PyBytes_Check(obj) || PyUnicode_Check(obj)) {
+        /*pass*/;
+    }
+    else if (PySequence_Check(obj)) {
+        PyErr_Clear();
+        tmp = PySequence_GetItem(obj, 0);
+    }
+
+    if (tmp) {
+        if (double_from_pyobj(v,tmp,errmess)) {Py_DECREF(tmp); return 1;}
+        Py_DECREF(tmp);
+    }
+    {
+        PyObject* err = PyErr_Occurred();
+        if (err==NULL) err = #modulename#_error;
+        PyErr_SetString(err,errmess);
+    }
+    return 0;
+}
+"""
+
+
+needs['float_from_pyobj'] = ['double_from_pyobj']
+cfuncs['float_from_pyobj'] = """\
+static int
+float_from_pyobj(float* v, PyObject *obj, const char *errmess)
+{
+    double d=0.0;
+    if (double_from_pyobj(&d,obj,errmess)) {
+        *v = (float)d;
+        return 1;
+    }
+    return 0;
+}
+"""
+
+
+needs['complex_long_double_from_pyobj'] = ['complex_long_double', 'long_double',
+                                           'complex_double_from_pyobj']
+cfuncs['complex_long_double_from_pyobj'] = """\
+static int
+complex_long_double_from_pyobj(complex_long_double* v, PyObject *obj, const char *errmess)
+{
+    complex_double cd = {0.0,0.0};
+    if (PyArray_CheckScalar(obj)){
+        if PyArray_IsScalar(obj, CLongDouble) {
+            PyArray_ScalarAsCtype(obj, v);
+            return 1;
+        }
+        else if (PyArray_Check(obj) && PyArray_TYPE(obj)==NPY_CLONGDOUBLE) {
+            (*v).r = ((npy_clongdouble *)PyArray_DATA(obj))->real;
+            (*v).i = ((npy_clongdouble *)PyArray_DATA(obj))->imag;
+            return 1;
+        }
+    }
+    if (complex_double_from_pyobj(&cd,obj,errmess)) {
+        (*v).r = (long_double)cd.r;
+        (*v).i = (long_double)cd.i;
+        return 1;
+    }
+    return 0;
+}
+"""
+
+
+needs['complex_double_from_pyobj'] = ['complex_double']
+cfuncs['complex_double_from_pyobj'] = """\
+static int
+complex_double_from_pyobj(complex_double* v, PyObject *obj, const char *errmess) {
+    Py_complex c;
+    if (PyComplex_Check(obj)) {
+        c = PyComplex_AsCComplex(obj);
+        (*v).r = c.real;
+        (*v).i = c.imag;
+        return 1;
+    }
+    if (PyArray_IsScalar(obj, ComplexFloating)) {
+        if (PyArray_IsScalar(obj, CFloat)) {
+            npy_cfloat new;
+            PyArray_ScalarAsCtype(obj, &new);
+            (*v).r = (double)new.real;
+            (*v).i = (double)new.imag;
+        }
+        else if (PyArray_IsScalar(obj, CLongDouble)) {
+            npy_clongdouble new;
+            PyArray_ScalarAsCtype(obj, &new);
+            (*v).r = (double)new.real;
+            (*v).i = (double)new.imag;
+        }
+        else { /* if (PyArray_IsScalar(obj, CDouble)) */
+            PyArray_ScalarAsCtype(obj, v);
+        }
+        return 1;
+    }
+    if (PyArray_CheckScalar(obj)) { /* 0-dim array or still array scalar */
+        PyObject *arr;
+        if (PyArray_Check(obj)) {
+            arr = PyArray_Cast((PyArrayObject *)obj, NPY_CDOUBLE);
+        }
+        else {
+            arr = PyArray_FromScalar(obj, PyArray_DescrFromType(NPY_CDOUBLE));
+        }
+        if (arr == NULL) {
+            return 0;
+        }
+        (*v).r = ((npy_cdouble *)PyArray_DATA(arr))->real;
+        (*v).i = ((npy_cdouble *)PyArray_DATA(arr))->imag;
+        Py_DECREF(arr);
+        return 1;
+    }
+    /* Python does not provide PyNumber_Complex function :-( */
+    (*v).i = 0.0;
+    if (PyFloat_Check(obj)) {
+        (*v).r = PyFloat_AsDouble(obj);
+        return !((*v).r == -1.0 && PyErr_Occurred());
+    }
+    if (PyLong_Check(obj)) {
+        (*v).r = PyLong_AsDouble(obj);
+        return !((*v).r == -1.0 && PyErr_Occurred());
+    }
+    if (PySequence_Check(obj) && !(PyBytes_Check(obj) || PyUnicode_Check(obj))) {
+        PyObject *tmp = PySequence_GetItem(obj,0);
+        if (tmp) {
+            if (complex_double_from_pyobj(v,tmp,errmess)) {
+                Py_DECREF(tmp);
+                return 1;
+            }
+            Py_DECREF(tmp);
+        }
+    }
+    {
+        PyObject* err = PyErr_Occurred();
+        if (err==NULL)
+            err = PyExc_TypeError;
+        PyErr_SetString(err,errmess);
+    }
+    return 0;
+}
+"""
+
+
+needs['complex_float_from_pyobj'] = [
+    'complex_float', 'complex_double_from_pyobj']
+cfuncs['complex_float_from_pyobj'] = """\
+static int
+complex_float_from_pyobj(complex_float* v,PyObject *obj,const char *errmess)
+{
+    complex_double cd={0.0,0.0};
+    if (complex_double_from_pyobj(&cd,obj,errmess)) {
+        (*v).r = (float)cd.r;
+        (*v).i = (float)cd.i;
+        return 1;
+    }
+    return 0;
+}
+"""
+
+
+needs['try_pyarr_from_char'] = ['pyobj_from_char1', 'TRYPYARRAYTEMPLATE']
+cfuncs[
+    'try_pyarr_from_char'] = 'static int try_pyarr_from_char(PyObject* obj,char* v) {\n    TRYPYARRAYTEMPLATE(char,\'c\');\n}\n'
+needs['try_pyarr_from_signed_char'] = ['TRYPYARRAYTEMPLATE', 'unsigned_char']
+cfuncs[
+    'try_pyarr_from_unsigned_char'] = 'static int try_pyarr_from_unsigned_char(PyObject* obj,unsigned_char* v) {\n    TRYPYARRAYTEMPLATE(unsigned_char,\'b\');\n}\n'
+needs['try_pyarr_from_signed_char'] = ['TRYPYARRAYTEMPLATE', 'signed_char']
+cfuncs[
+    'try_pyarr_from_signed_char'] = 'static int try_pyarr_from_signed_char(PyObject* obj,signed_char* v) {\n    TRYPYARRAYTEMPLATE(signed_char,\'1\');\n}\n'
+needs['try_pyarr_from_short'] = ['pyobj_from_short1', 'TRYPYARRAYTEMPLATE']
+cfuncs[
+    'try_pyarr_from_short'] = 'static int try_pyarr_from_short(PyObject* obj,short* v) {\n    TRYPYARRAYTEMPLATE(short,\'s\');\n}\n'
+needs['try_pyarr_from_int'] = ['pyobj_from_int1', 'TRYPYARRAYTEMPLATE']
+cfuncs[
+    'try_pyarr_from_int'] = 'static int try_pyarr_from_int(PyObject* obj,int* v) {\n    TRYPYARRAYTEMPLATE(int,\'i\');\n}\n'
+needs['try_pyarr_from_long'] = ['pyobj_from_long1', 'TRYPYARRAYTEMPLATE']
+cfuncs[
+    'try_pyarr_from_long'] = 'static int try_pyarr_from_long(PyObject* obj,long* v) {\n    TRYPYARRAYTEMPLATE(long,\'l\');\n}\n'
+needs['try_pyarr_from_long_long'] = [
+    'pyobj_from_long_long1', 'TRYPYARRAYTEMPLATE', 'long_long']
+cfuncs[
+    'try_pyarr_from_long_long'] = 'static int try_pyarr_from_long_long(PyObject* obj,long_long* v) {\n    TRYPYARRAYTEMPLATE(long_long,\'L\');\n}\n'
+needs['try_pyarr_from_float'] = ['pyobj_from_float1', 'TRYPYARRAYTEMPLATE']
+cfuncs[
+    'try_pyarr_from_float'] = 'static int try_pyarr_from_float(PyObject* obj,float* v) {\n    TRYPYARRAYTEMPLATE(float,\'f\');\n}\n'
+needs['try_pyarr_from_double'] = ['pyobj_from_double1', 'TRYPYARRAYTEMPLATE']
+cfuncs[
+    'try_pyarr_from_double'] = 'static int try_pyarr_from_double(PyObject* obj,double* v) {\n    TRYPYARRAYTEMPLATE(double,\'d\');\n}\n'
+needs['try_pyarr_from_complex_float'] = [
+    'pyobj_from_complex_float1', 'TRYCOMPLEXPYARRAYTEMPLATE', 'complex_float']
+cfuncs[
+    'try_pyarr_from_complex_float'] = 'static int try_pyarr_from_complex_float(PyObject* obj,complex_float* v) {\n    TRYCOMPLEXPYARRAYTEMPLATE(float,\'F\');\n}\n'
+needs['try_pyarr_from_complex_double'] = [
+    'pyobj_from_complex_double1', 'TRYCOMPLEXPYARRAYTEMPLATE', 'complex_double']
+cfuncs[
+    'try_pyarr_from_complex_double'] = 'static int try_pyarr_from_complex_double(PyObject* obj,complex_double* v) {\n    TRYCOMPLEXPYARRAYTEMPLATE(double,\'D\');\n}\n'
+
+
+needs['create_cb_arglist'] = ['CFUNCSMESS', 'PRINTPYOBJERR', 'MINMAX']
+# create the list of arguments to be used when calling back to python
+cfuncs['create_cb_arglist'] = """\
+static int
+create_cb_arglist(PyObject* fun, PyTupleObject* xa , const int maxnofargs,
+                  const int nofoptargs, int *nofargs, PyTupleObject **args,
+                  const char *errmess)
+{
+    PyObject *tmp = NULL;
+    PyObject *tmp_fun = NULL;
+    Py_ssize_t tot, opt, ext, siz, i, di = 0;
+    CFUNCSMESS(\"create_cb_arglist\\n\");
+    tot=opt=ext=siz=0;
+    /* Get the total number of arguments */
+    if (PyFunction_Check(fun)) {
+        tmp_fun = fun;
+        Py_INCREF(tmp_fun);
+    }
+    else {
+        di = 1;
+        if (PyObject_HasAttrString(fun,\"im_func\")) {
+            tmp_fun = PyObject_GetAttrString(fun,\"im_func\");
+        }
+        else if (PyObject_HasAttrString(fun,\"__call__\")) {
+            tmp = PyObject_GetAttrString(fun,\"__call__\");
+            if (PyObject_HasAttrString(tmp,\"im_func\"))
+                tmp_fun = PyObject_GetAttrString(tmp,\"im_func\");
+            else {
+                tmp_fun = fun; /* built-in function */
+                Py_INCREF(tmp_fun);
+                tot = maxnofargs;
+                if (PyCFunction_Check(fun)) {
+                    /* In case the function has a co_argcount (like on PyPy) */
+                    di = 0;
+                }
+                if (xa != NULL)
+                    tot += PyTuple_Size((PyObject *)xa);
+            }
+            Py_XDECREF(tmp);
+        }
+        else if (PyFortran_Check(fun) || PyFortran_Check1(fun)) {
+            tot = maxnofargs;
+            if (xa != NULL)
+                tot += PyTuple_Size((PyObject *)xa);
+            tmp_fun = fun;
+            Py_INCREF(tmp_fun);
+        }
+        else if (F2PyCapsule_Check(fun)) {
+            tot = maxnofargs;
+            if (xa != NULL)
+                ext = PyTuple_Size((PyObject *)xa);
+            if(ext>0) {
+                fprintf(stderr,\"extra arguments tuple cannot be used with CObject call-back\\n\");
+                goto capi_fail;
+            }
+            tmp_fun = fun;
+            Py_INCREF(tmp_fun);
+        }
+    }
+
+    if (tmp_fun == NULL) {
+        fprintf(stderr,
+                \"Call-back argument must be function|instance|instance.__call__|f2py-function \"
+                \"but got %s.\\n\",
+                ((fun == NULL) ? \"NULL\" : Py_TYPE(fun)->tp_name));
+        goto capi_fail;
+    }
+
+    if (PyObject_HasAttrString(tmp_fun,\"__code__\")) {
+        if (PyObject_HasAttrString(tmp = PyObject_GetAttrString(tmp_fun,\"__code__\"),\"co_argcount\")) {
+            PyObject *tmp_argcount = PyObject_GetAttrString(tmp,\"co_argcount\");
+            Py_DECREF(tmp);
+            if (tmp_argcount == NULL) {
+                goto capi_fail;
+            }
+            tot = PyLong_AsSsize_t(tmp_argcount) - di;
+            Py_DECREF(tmp_argcount);
+        }
+    }
+    /* Get the number of optional arguments */
+    if (PyObject_HasAttrString(tmp_fun,\"__defaults__\")) {
+        if (PyTuple_Check(tmp = PyObject_GetAttrString(tmp_fun,\"__defaults__\")))
+            opt = PyTuple_Size(tmp);
+        Py_XDECREF(tmp);
+    }
+    /* Get the number of extra arguments */
+    if (xa != NULL)
+        ext = PyTuple_Size((PyObject *)xa);
+    /* Calculate the size of call-backs argument list */
+    siz = MIN(maxnofargs+ext,tot);
+    *nofargs = MAX(0,siz-ext);
+
 #ifdef DEBUGCFUNCS
-\tfprintf(stderr,\"debug-capi:create_cb_arglist:maxnofargs(-nofoptargs),tot,opt,ext,siz,nofargs=%d(-%d),%d,%d,%d,%d,%d\\n\",maxnofargs,nofoptargs,tot,opt,ext,siz,*nofargs);
-#endif
-\tif (siz<tot-opt) {
-\t\tfprintf(stderr,\"create_cb_arglist: Failed to build argument list (siz) with enough arguments (tot-opt) required by user-supplied function (siz,tot,opt=%d,%d,%d).\\n\",siz,tot,opt);
-\t\tgoto capi_fail;
-\t}
-\t/* Initialize argument list */
-\t*args = (PyTupleObject *)PyTuple_New(siz);
-\tfor (i=0;i<*nofargs;i++) {
-\t\tPy_INCREF(Py_None);
-\t\tPyTuple_SET_ITEM((PyObject *)(*args),i,Py_None);
-\t}
-\tif (xa != NULL)
-\t\tfor (i=(*nofargs);i<siz;i++) {
-\t\t\ttmp = PyTuple_GetItem((PyObject *)xa,i-(*nofargs));
-\t\t\tPy_INCREF(tmp);
-\t\t\tPyTuple_SET_ITEM(*args,i,tmp);
-\t\t}
-\tCFUNCSMESS(\"create_cb_arglist-end\\n\");
-\treturn 1;
+    fprintf(stderr,
+            \"debug-capi:create_cb_arglist:maxnofargs(-nofoptargs),\"
+            \"tot,opt,ext,siz,nofargs = %d(-%d), %zd, %zd, %zd, %zd, %d\\n\",
+            maxnofargs, nofoptargs, tot, opt, ext, siz, *nofargs);
+#endif
+
+    if (siz < tot-opt) {
+        fprintf(stderr,
+                \"create_cb_arglist: Failed to build argument list \"
+                \"(siz) with enough arguments (tot-opt) required by \"
+                \"user-supplied function (siz,tot,opt=%zd, %zd, %zd).\\n\",
+                siz, tot, opt);
+        goto capi_fail;
+    }
+
+    /* Initialize argument list */
+    *args = (PyTupleObject *)PyTuple_New(siz);
+    for (i=0;i<*nofargs;i++) {
+        Py_INCREF(Py_None);
+        PyTuple_SET_ITEM((PyObject *)(*args),i,Py_None);
+    }
+    if (xa != NULL)
+        for (i=(*nofargs);i<siz;i++) {
+            tmp = PyTuple_GetItem((PyObject *)xa,i-(*nofargs));
+            Py_INCREF(tmp);
+            PyTuple_SET_ITEM(*args,i,tmp);
+        }
+    CFUNCSMESS(\"create_cb_arglist-end\\n\");
+    Py_DECREF(tmp_fun);
+    return 1;
+
 capi_fail:
-\tif ((PyErr_Occurred())==NULL)
-\t\tPyErr_SetString(#modulename#_error,errmess);
-\treturn 0;
-}
-"""
+    if (PyErr_Occurred() == NULL)
+        PyErr_SetString(#modulename#_error, errmess);
+    Py_XDECREF(tmp_fun);
+    return 0;
+}
+"""
+
 
 def buildcfuncs():
-    from capi_maps import c2capi_map
+    from .capi_maps import c2capi_map
     for k in c2capi_map.keys():
-        m='pyarr_from_p_%s1'%k
-        cppmacros[m]='#define %s(v) (PyArray_SimpleNewFromData(0,NULL,%s,(char *)v))'%(m,c2capi_map[k])
-    k='string'
-    m='pyarr_from_p_%s1'%k
-    cppmacros[m]='#define %s(v,dims) (PyArray_SimpleNewFromData(1,dims,PyArray_CHAR,(char *)v))'%(m)
+        m = 'pyarr_from_p_%s1' % k
+        cppmacros[
+            m] = '#define %s(v) (PyArray_SimpleNewFromData(0,NULL,%s,(char *)v))' % (m, c2capi_map[k])
+    k = 'string'
+    m = 'pyarr_from_p_%s1' % k
+    # NPY_CHAR compatibility, NPY_STRING with itemsize 1
+    cppmacros[
+        m] = '#define %s(v,dims) (PyArray_New(&PyArray_Type, 1, dims, NPY_STRING, NULL, v, 1, NPY_ARRAY_CARRAY, NULL))' % (m)
 
 
 ############ Auxiliary functions for sorting needs ###################
 
-def append_needs(need,flag=1):
-    global outneeds,needs
-    if type(need)==types.ListType:
+def append_needs(need, flag=1):
+    # This function modifies the contents of the global `outneeds` dict.
+    if isinstance(need, list):
         for n in need:
-            append_needs(n,flag)
-    elif type(need)==types.StringType:
-        if not need: return
-        if includes0.has_key(need): n = 'includes0'
-        elif includes.has_key(need): n = 'includes'
-        elif typedefs.has_key(need): n = 'typedefs'
-        elif typedefs_generated.has_key(need): n = 'typedefs_generated'
-        elif cppmacros.has_key(need): n = 'cppmacros'
-        elif cfuncs.has_key(need): n = 'cfuncs'
-        elif callbacks.has_key(need): n = 'callbacks'
-        elif f90modhooks.has_key(need): n = 'f90modhooks'
-        elif commonhooks.has_key(need): n = 'commonhooks'
+            append_needs(n, flag)
+    elif isinstance(need, str):
+        if not need:
+            return
+        if need in includes0:
+            n = 'includes0'
+        elif need in includes:
+            n = 'includes'
+        elif need in typedefs:
+            n = 'typedefs'
+        elif need in typedefs_generated:
+            n = 'typedefs_generated'
+        elif need in cppmacros:
+            n = 'cppmacros'
+        elif need in cfuncs:
+            n = 'cfuncs'
+        elif need in callbacks:
+            n = 'callbacks'
+        elif need in f90modhooks:
+            n = 'f90modhooks'
+        elif need in commonhooks:
+            n = 'commonhooks'
         else:
-            errmess('append_needs: unknown need %s\n'%(`need`))
+            errmess('append_needs: unknown need %s\n' % (repr(need)))
             return
-        if need in outneeds[n]: return
+        if need in outneeds[n]:
+            return
         if flag:
-            tmp={}
-            if needs.has_key(need):
+            tmp = {}
+            if need in needs:
                 for nn in needs[need]:
-                    t=append_needs(nn,0)
-                    if type(t)==types.DictType:
+                    t = append_needs(nn, 0)
+                    if isinstance(t, dict):
                         for nnn in t.keys():
-                            if tmp.has_key(nnn): tmp[nnn]=tmp[nnn]+t[nnn]
-                            else: tmp[nnn]=t[nnn]
+                            if nnn in tmp:
+                                tmp[nnn] = tmp[nnn] + t[nnn]
+                            else:
+                                tmp[nnn] = t[nnn]
             for nn in tmp.keys():
                 for nnn in tmp[nn]:
                     if nnn not in outneeds[nn]:
-                        outneeds[nn]=[nnn]+outneeds[nn]
+                        outneeds[nn] = [nnn] + outneeds[nn]
             outneeds[n].append(need)
         else:
-            tmp={}
-            if needs.has_key(need):
+            tmp = {}
+            if need in needs:
                 for nn in needs[need]:
-                    t=append_needs(nn,flag)
-                    if type(t)==types.DictType:
+                    t = append_needs(nn, flag)
+                    if isinstance(t, dict):
                         for nnn in t.keys():
-                            if tmp.has_key(nnn): tmp[nnn]=t[nnn]+tmp[nnn]
-                            else: tmp[nnn]=t[nnn]
-            if not tmp.has_key(n): tmp[n]=[]
+                            if nnn in tmp:
+                                tmp[nnn] = t[nnn] + tmp[nnn]
+                            else:
+                                tmp[nnn] = t[nnn]
+            if n not in tmp:
+                tmp[n] = []
             tmp[n].append(need)
             return tmp
     else:
-        errmess('append_needs: expected list or string but got :%s\n'%(`need`))
+        errmess('append_needs: expected list or string but got :%s\n' %
+                (repr(need)))
+
 
 def get_needs():
-    global outneeds,needs
-    res={}
+    # This function modifies the contents of the global `outneeds` dict.
+    res = {}
     for n in outneeds.keys():
-        out=[]
-        saveout=copy.copy(outneeds[n])
-        while len(outneeds[n])>0:
-            if not needs.has_key(outneeds[n][0]):
+        out = []
+        saveout = copy.copy(outneeds[n])
+        while len(outneeds[n]) > 0:
+            if outneeds[n][0] not in needs:
                 out.append(outneeds[n][0])
                 del outneeds[n][0]
             else:
-                flag=0
+                flag = 0
                 for k in outneeds[n][1:]:
                     if k in needs[outneeds[n][0]]:
-                        flag=1
+                        flag = 1
                         break
                 if flag:
-                    outneeds[n]=outneeds[n][1:]+[outneeds[n][0]]
+                    outneeds[n] = outneeds[n][1:] + [outneeds[n][0]]
                 else:
                     out.append(outneeds[n][0])
                     del outneeds[n][0]
-            if saveout and (0 not in map(lambda x,y:x==y,saveout,outneeds[n])):
-                print n,saveout
-                errmess('get_needs: no progress in sorting needs, probably circular dependence, skipping.\n')
-                out=out+saveout
+            if saveout and (0 not in map(lambda x, y: x == y, saveout, outneeds[n])) \
+                    and outneeds[n] != []:
+                print(n, saveout)
+                errmess(
+                    'get_needs: no progress in sorting needs, probably circular dependence, skipping.\n')
+                out = out + saveout
                 break
-            saveout=copy.copy(outneeds[n])
-        if out==[]: out=[n]
-        res[n]=out
+            saveout = copy.copy(outneeds[n])
+        if out == []:
+            out = [n]
+        res[n] = out
     return res
('numpy/f2py', 'common_rules.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,4 +1,4 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 """
 
 Build common block mechanism for f2py2e.
@@ -11,40 +11,31 @@
 NO WARRANTY IS EXPRESSED OR IMPLIED.  USE AT YOUR OWN RISK.
 $Date: 2005/05/06 10:57:33 $
 Pearu Peterson
+
 """
-
-__version__ = "$Revision: 1.19 $"[10:-1]
-
-import __version__
+from . import __version__
 f2py_version = __version__.version
 
-import pprint
-import sys,string,time,types,copy
-errmess=sys.stderr.write
-outmess=sys.stdout.write
-show=pprint.pprint
+from .auxfuncs import (
+    hasbody, hascommon, hasnote, isintent_hide, outmess
+)
+from . import capi_maps
+from . import func2subr
+from .crackfortran import rmbadname
 
-from auxfuncs import *
-import capi_maps
-import cfuncs
-import func2subr
-from crackfortran import rmbadname
-##############
 
-def findcommonblocks(block,top=1):
+def findcommonblocks(block, top=1):
     ret = []
     if hascommon(block):
-        for n in block['common'].keys():
-            vars={}
-            for v in block['common'][n]:
-                vars[v]=block['vars'][v]
-            ret.append((n,block['common'][n],vars))
+        for key, value in block['common'].items():
+            vars_ = {v: block['vars'][v] for v in value}
+            ret.append((key, value, vars_))
     elif hasbody(block):
         for b in block['body']:
-            ret=ret+findcommonblocks(b,0)
+            ret = ret + findcommonblocks(b, 0)
     if top:
-        tret=[]
-        names=[]
+        tret = []
+        names = []
         for t in ret:
             if t[0] not in names:
                 names.append(t[0])
@@ -52,80 +43,103 @@
         return tret
     return ret
 
+
 def buildhooks(m):
-    ret = {'commonhooks':[],'initcommonhooks':[],'docs':['"COMMON blocks:\\n"']}
+    ret = {'commonhooks': [], 'initcommonhooks': [],
+           'docs': ['"COMMON blocks:\\n"']}
     fwrap = ['']
-    def fadd(line,s=fwrap): s[0] = '%s\n      %s'%(s[0],line)
+
+    def fadd(line, s=fwrap):
+        s[0] = '%s\n      %s' % (s[0], line)
     chooks = ['']
-    def cadd(line,s=chooks): s[0] = '%s\n%s'%(s[0],line)
+
+    def cadd(line, s=chooks):
+        s[0] = '%s\n%s' % (s[0], line)
     ihooks = ['']
-    def iadd(line,s=ihooks): s[0] = '%s\n%s'%(s[0],line)
+
+    def iadd(line, s=ihooks):
+        s[0] = '%s\n%s' % (s[0], line)
     doc = ['']
-    def dadd(line,s=doc): s[0] = '%s\n%s'%(s[0],line)
-    for (name,vnames,vars) in findcommonblocks(m):
-        lower_name = string.lower(name)
-        hnames,inames = [],[]
+
+    def dadd(line, s=doc):
+        s[0] = '%s\n%s' % (s[0], line)
+    for (name, vnames, vars) in findcommonblocks(m):
+        lower_name = name.lower()
+        hnames, inames = [], []
         for n in vnames:
-            if isintent_hide(vars[n]): hnames.append(n)
-            else: inames.append(n)
+            if isintent_hide(vars[n]):
+                hnames.append(n)
+            else:
+                inames.append(n)
         if hnames:
-            outmess('\t\tConstructing COMMON block support for "%s"...\n\t\t  %s\n\t\t  Hidden: %s\n'%(name,string.join(inames,','),string.join(hnames,',')))
+            outmess('\t\tConstructing COMMON block support for "%s"...\n\t\t  %s\n\t\t  Hidden: %s\n' % (
+                name, ','.join(inames), ','.join(hnames)))
         else:
-            outmess('\t\tConstructing COMMON block support for "%s"...\n\t\t  %s\n'%(name,string.join(inames,',')))
-        fadd('subroutine f2pyinit%s(setupfunc)'%name)
+            outmess('\t\tConstructing COMMON block support for "%s"...\n\t\t  %s\n' % (
+                name, ','.join(inames)))
+        fadd('subroutine f2pyinit%s(setupfunc)' % name)
         fadd('external setupfunc')
         for n in vnames:
-            fadd(func2subr.var2fixfortran(vars,n))
-        if name=='_BLNK_':
-            fadd('common %s'%(string.join(vnames,',')))
+            fadd(func2subr.var2fixfortran(vars, n))
+        if name == '_BLNK_':
+            fadd('common %s' % (','.join(vnames)))
         else:
-            fadd('common /%s/ %s'%(name,string.join(vnames,',')))
-        fadd('call setupfunc(%s)'%(string.join(inames,',')))
+            fadd('common /%s/ %s' % (name, ','.join(vnames)))
+        fadd('call setupfunc(%s)' % (','.join(inames)))
         fadd('end\n')
-        cadd('static FortranDataDef f2py_%s_def[] = {'%(name))
-        idims=[]
+        cadd('static FortranDataDef f2py_%s_def[] = {' % (name))
+        idims = []
         for n in inames:
             ct = capi_maps.getctype(vars[n])
             at = capi_maps.c2capi_map[ct]
-            dm = capi_maps.getarrdims(n,vars[n])
-            if dm['dims']: idims.append('(%s)'%(dm['dims']))
-            else: idims.append('')
-            dms=string.strip(dm['dims'])
-            if not dms: dms='-1'
-            cadd('\t{\"%s\",%s,{{%s}},%s},'%(n,dm['rank'],dms,at))
+            dm = capi_maps.getarrdims(n, vars[n])
+            if dm['dims']:
+                idims.append('(%s)' % (dm['dims']))
+            else:
+                idims.append('')
+            dms = dm['dims'].strip()
+            if not dms:
+                dms = '-1'
+            cadd('\t{\"%s\",%s,{{%s}},%s},' % (n, dm['rank'], dms, at))
         cadd('\t{NULL}\n};')
         inames1 = rmbadname(inames)
-        inames1_tps = string.join(map(lambda s:'char *'+s,inames1),',')
-        cadd('static void f2py_setup_%s(%s) {'%(name,inames1_tps))
+        inames1_tps = ','.join(['char *' + s for s in inames1])
+        cadd('static void f2py_setup_%s(%s) {' % (name, inames1_tps))
         cadd('\tint i_f2py=0;')
         for n in inames1:
-            cadd('\tf2py_%s_def[i_f2py++].data = %s;'%(name,n))
+            cadd('\tf2py_%s_def[i_f2py++].data = %s;' % (name, n))
         cadd('}')
         if '_' in lower_name:
-            F_FUNC='F_FUNC_US'
+            F_FUNC = 'F_FUNC_US'
         else:
-            F_FUNC='F_FUNC'
-        cadd('extern void %s(f2pyinit%s,F2PYINIT%s)(void(*)(%s));'\
-             %(F_FUNC,lower_name,string.upper(name),
-               string.join(['char*']*len(inames1),',')))
-        cadd('static void f2py_init_%s(void) {'%name)
-        cadd('\t%s(f2pyinit%s,F2PYINIT%s)(f2py_setup_%s);'\
-             %(F_FUNC,lower_name,string.upper(name),name))
+            F_FUNC = 'F_FUNC'
+        cadd('extern void %s(f2pyinit%s,F2PYINIT%s)(void(*)(%s));'
+             % (F_FUNC, lower_name, name.upper(),
+                ','.join(['char*'] * len(inames1))))
+        cadd('static void f2py_init_%s(void) {' % name)
+        cadd('\t%s(f2pyinit%s,F2PYINIT%s)(f2py_setup_%s);'
+             % (F_FUNC, lower_name, name.upper(), name))
         cadd('}\n')
-        iadd('\tF2PyDict_SetItemString(d, \"%s\", PyFortranObject_New(f2py_%s_def,f2py_init_%s));'%(name,name,name))
-        tname = string.replace(name,'_','\\_')
-        dadd('\\subsection{Common block \\texttt{%s}}\n'%(tname))
+        iadd('\ttmp = PyFortranObject_New(f2py_%s_def,f2py_init_%s);' % (name, name))
+        iadd('\tF2PyDict_SetItemString(d, \"%s\", tmp);' % name)
+        iadd('\tPy_DECREF(tmp);')
+        tname = name.replace('_', '\\_')
+        dadd('\\subsection{Common block \\texttt{%s}}\n' % (tname))
         dadd('\\begin{description}')
         for n in inames:
-            dadd('\\item[]{{}\\verb@%s@{}}'%(capi_maps.getarrdocsign(n,vars[n])))
+            dadd('\\item[]{{}\\verb@%s@{}}' %
+                 (capi_maps.getarrdocsign(n, vars[n])))
             if hasnote(vars[n]):
                 note = vars[n]['note']
-                if type(note) is type([]): note=string.join(note,'\n')
-                dadd('--- %s'%(note))
+                if isinstance(note, list):
+                    note = '\n'.join(note)
+                dadd('--- %s' % (note))
         dadd('\\end{description}')
-        ret['docs'].append('"\t/%s/ %s\\n"'%(name,string.join(map(lambda v,d:v+d,inames,idims),',')))
-    ret['commonhooks']=chooks
-    ret['initcommonhooks']=ihooks
-    ret['latexdoc']=doc[0]
-    if len(ret['docs'])<=1: ret['docs']=''
-    return ret,fwrap[0]
+        ret['docs'].append(
+            '"\t/%s/ %s\\n"' % (name, ','.join(map(lambda v, d: v + d, inames, idims))))
+    ret['commonhooks'] = chooks
+    ret['initcommonhooks'] = ihooks
+    ret['latexdoc'] = doc[0]
+    if len(ret['docs']) <= 1:
+        ret['docs'] = ''
+    return ret, fwrap[0]
('numpy/f2py', 'crackfortran.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,7 +1,6 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 """
 crackfortran --- read fortran (77,90) code and extract declaration information.
-    Usage is explained in the comment block below.
 
 Copyright 1999-2004 Pearu Peterson all rights reserved,
 Pearu Peterson <pearu@ioc.ee>
@@ -11,1424 +10,1898 @@
 NO WARRANTY IS EXPRESSED OR IMPLIED.  USE AT YOUR OWN RISK.
 $Date: 2005/09/27 07:13:49 $
 Pearu Peterson
+
+
+Usage of crackfortran:
+======================
+Command line keys: -quiet,-verbose,-fix,-f77,-f90,-show,-h <pyffilename>
+                   -m <module name for f77 routines>,--ignore-contains
+Functions: crackfortran, crack2fortran
+The following Fortran statements/constructions are supported
+(or will be if needed):
+   block data,byte,call,character,common,complex,contains,data,
+   dimension,double complex,double precision,end,external,function,
+   implicit,integer,intent,interface,intrinsic,
+   logical,module,optional,parameter,private,public,
+   program,real,(sequence?),subroutine,type,use,virtual,
+   include,pythonmodule
+Note: 'virtual' is mapped to 'dimension'.
+Note: 'implicit integer (z) static (z)' is 'implicit static (z)' (this is minor bug).
+Note: code after 'contains' will be ignored until its scope ends.
+Note: 'common' statement is extended: dimensions are moved to variable definitions
+Note: f2py directive: <commentchar>f2py<line> is read as <line>
+Note: pythonmodule is introduced to represent Python module
+
+Usage:
+  `postlist=crackfortran(files)`
+  `postlist` contains declaration information read from the list of files `files`.
+  `crack2fortran(postlist)` returns a fortran code to be saved to pyf-file
+
+  `postlist` has the following structure:
+ *** it is a list of dictionaries containing `blocks':
+     B = {'block','body','vars','parent_block'[,'name','prefix','args','result',
+          'implicit','externals','interfaced','common','sortvars',
+          'commonvars','note']}
+     B['block'] = 'interface' | 'function' | 'subroutine' | 'module' |
+                  'program' | 'block data' | 'type' | 'pythonmodule' |
+                  'abstract interface'
+     B['body'] --- list containing `subblocks' with the same structure as `blocks'
+     B['parent_block'] --- dictionary of a parent block:
+                             C['body'][<index>]['parent_block'] is C
+     B['vars'] --- dictionary of variable definitions
+     B['sortvars'] --- dictionary of variable definitions sorted by dependence (independent first)
+     B['name'] --- name of the block (not if B['block']=='interface')
+     B['prefix'] --- prefix string (only if B['block']=='function')
+     B['args'] --- list of argument names if B['block']== 'function' | 'subroutine'
+     B['result'] --- name of the return value (only if B['block']=='function')
+     B['implicit'] --- dictionary {'a':<variable definition>,'b':...} | None
+     B['externals'] --- list of variables being external
+     B['interfaced'] --- list of variables being external and defined
+     B['common'] --- dictionary of common blocks (list of objects)
+     B['commonvars'] --- list of variables used in common blocks (dimensions are moved to variable definitions)
+     B['from'] --- string showing the 'parents' of the current block
+     B['use'] --- dictionary of modules used in current block:
+         {<modulename>:{['only':<0|1>],['map':{<local_name1>:<use_name1>,...}]}}
+     B['note'] --- list of LaTeX comments on the block
+     B['f2pyenhancements'] --- optional dictionary
+          {'threadsafe':'','fortranname':<name>,
+           'callstatement':<C-expr>|<multi-line block>,
+           'callprotoargument':<C-expr-list>,
+           'usercode':<multi-line block>|<list of multi-line blocks>,
+           'pymethoddef:<multi-line block>'
+           }
+     B['entry'] --- dictionary {entryname:argslist,..}
+     B['varnames'] --- list of variable names given in the order of reading the
+                       Fortran code, useful for derived types.
+     B['saved_interface'] --- a string of scanned routine signature, defines explicit interface
+ *** Variable definition is a dictionary
+     D = B['vars'][<variable name>] =
+     {'typespec'[,'attrspec','kindselector','charselector','=','typename']}
+     D['typespec'] = 'byte' | 'character' | 'complex' | 'double complex' |
+                     'double precision' | 'integer' | 'logical' | 'real' | 'type'
+     D['attrspec'] --- list of attributes (e.g. 'dimension(<arrayspec>)',
+                       'external','intent(in|out|inout|hide|c|callback|cache|aligned4|aligned8|aligned16)',
+                       'optional','required', etc)
+     K = D['kindselector'] = {['*','kind']} (only if D['typespec'] =
+                         'complex' | 'integer' | 'logical' | 'real' )
+     C = D['charselector'] = {['*','len','kind']}
+                             (only if D['typespec']=='character')
+     D['='] --- initialization expression string
+     D['typename'] --- name of the type if D['typespec']=='type'
+     D['dimension'] --- list of dimension bounds
+     D['intent'] --- list of intent specifications
+     D['depend'] --- list of variable names on which current variable depends on
+     D['check'] --- list of C-expressions; if C-expr returns zero, exception is raised
+     D['note'] --- list of LaTeX comments on the variable
+ *** Meaning of kind/char selectors (few examples):
+     D['typespec>']*K['*']
+     D['typespec'](kind=K['kind'])
+     character*C['*']
+     character(len=C['len'],kind=C['kind'])
+     (see also fortran type declaration statement formats below)
+
+Fortran 90 type declaration statement format (F77 is subset of F90)
+====================================================================
+(Main source: IBM XL Fortran 5.1 Language Reference Manual)
+type declaration = <typespec> [[<attrspec>]::] <entitydecl>
+<typespec> = byte                          |
+             character[<charselector>]     |
+             complex[<kindselector>]       |
+             double complex                |
+             double precision              |
+             integer[<kindselector>]       |
+             logical[<kindselector>]       |
+             real[<kindselector>]          |
+             type(<typename>)
+<charselector> = * <charlen>               |
+             ([len=]<len>[,[kind=]<kind>]) |
+             (kind=<kind>[,len=<len>])
+<kindselector> = * <intlen>                |
+             ([kind=]<kind>)
+<attrspec> = comma separated list of attributes.
+             Only the following attributes are used in
+             building up the interface:
+                external
+                (parameter --- affects '=' key)
+                optional
+                intent
+             Other attributes are ignored.
+<intentspec> = in | out | inout
+<arrayspec> = comma separated list of dimension bounds.
+<entitydecl> = <name> [[*<charlen>][(<arrayspec>)] | [(<arrayspec>)]*<charlen>]
+                      [/<init_expr>/ | =<init_expr>] [,<entitydecl>]
+
+In addition, the following attributes are used: check,depend,note
+
+TODO:
+    * Apply 'parameter' attribute (e.g. 'integer parameter :: i=2' 'real x(i)'
+                                   -> 'real x(2)')
+    The above may be solved by creating appropriate preprocessor program, for example.
+
 """
-__version__ = "$Revision: 1.177 $"[10:-1]
-
-import __version__
+import sys
+import string
+import fileinput
+import re
+import os
+import copy
+import platform
+
+from . import __version__
+
+# The environment provided by auxfuncs.py is needed for some calls to eval.
+# As the needed functions cannot be determined by static inspection of the
+# code, it is safest to use import * pending a major refactoring of f2py.
+from .auxfuncs import *
+from . import symbolic
+
 f2py_version = __version__.version
 
-"""
- Usage of crackfortran:
- ======================
- Command line keys: -quiet,-verbose,-fix,-f77,-f90,-show,-h <pyffilename>
-                    -m <module name for f77 routines>,--ignore-contains
- Functions: crackfortran, crack2fortran
- The following Fortran statements/constructions are supported
- (or will be if needed):
-    block data,byte,call,character,common,complex,contains,data,
-    dimension,double complex,double precision,end,external,function,
-    implicit,integer,intent,interface,intrinsic,
-    logical,module,optional,parameter,private,public,
-    program,real,(sequence?),subroutine,type,use,virtual,
-    include,pythonmodule
- Note: 'virtual' is mapped to 'dimension'.
- Note: 'implicit integer (z) static (z)' is 'implicit static (z)' (this is minor bug).
- Note: code after 'contains' will be ignored until its scope ends.
- Note: 'common' statement is extended: dimensions are moved to variable definitions
- Note: f2py directive: <commentchar>f2py<line> is read as <line>
- Note: pythonmodule is introduced to represent Python module
-
- Usage:
-   `postlist=crackfortran(files,funcs)`
-   `postlist` contains declaration information read from the list of files `files`.
-   `crack2fortran(postlist)` returns a fortran code to be saved to pyf-file
-
-   `postlist` has the following structure:
-  *** it is a list of dictionaries containing `blocks':
-      B = {'block','body','vars','parent_block'[,'name','prefix','args','result',
-           'implicit','externals','interfaced','common','sortvars',
-           'commonvars','note']}
-      B['block'] = 'interface' | 'function' | 'subroutine' | 'module' |
-                   'program' | 'block data' | 'type' | 'pythonmodule'
-      B['body'] --- list containing `subblocks' with the same structure as `blocks'
-      B['parent_block'] --- dictionary of a parent block:
-                              C['body'][<index>]['parent_block'] is C
-      B['vars'] --- dictionary of variable definitions
-      B['sortvars'] --- dictionary of variable definitions sorted by dependence (independent first)
-      B['name'] --- name of the block (not if B['block']=='interface')
-      B['prefix'] --- prefix string (only if B['block']=='function')
-      B['args'] --- list of argument names if B['block']== 'function' | 'subroutine'
-      B['result'] --- name of the return value (only if B['block']=='function')
-      B['implicit'] --- dictionary {'a':<variable definition>,'b':...} | None
-      B['externals'] --- list of variables being external
-      B['interfaced'] --- list of variables being external and defined
-      B['common'] --- dictionary of common blocks (list of objects)
-      B['commonvars'] --- list of variables used in common blocks (dimensions are moved to variable definitions)
-      B['from'] --- string showing the 'parents' of the current block
-      B['use'] --- dictionary of modules used in current block:
-          {<modulename>:{['only':<0|1>],['map':{<local_name1>:<use_name1>,...}]}}
-      B['note'] --- list of LaTeX comments on the block
-      B['f2pyenhancements'] --- optional dictionary
-           {'threadsafe':'','fortranname':<name>,
-            'callstatement':<C-expr>|<multi-line block>,
-            'callprotoargument':<C-expr-list>,
-            'usercode':<multi-line block>|<list of multi-line blocks>,
-            'pymethoddef:<multi-line block>'
-            }
-      B['entry'] --- dictionary {entryname:argslist,..}
-      B['varnames'] --- list of variable names given in the order of reading the
-                        Fortran code, useful for derived types.
-  *** Variable definition is a dictionary
-      D = B['vars'][<variable name>] =
-      {'typespec'[,'attrspec','kindselector','charselector','=','typename']}
-      D['typespec'] = 'byte' | 'character' | 'complex' | 'double complex' |
-                      'double precision' | 'integer' | 'logical' | 'real' | 'type'
-      D['attrspec'] --- list of attributes (e.g. 'dimension(<arrayspec>)',
-                        'external','intent(in|out|inout|hide|c|callback|cache)',
-                        'optional','required', etc)
-      K = D['kindselector'] = {['*','kind']} (only if D['typespec'] =
-                          'complex' | 'integer' | 'logical' | 'real' )
-      C = D['charselector'] = {['*','len','kind']}
-                              (only if D['typespec']=='character')
-      D['='] --- initialization expression string
-      D['typename'] --- name of the type if D['typespec']=='type'
-      D['dimension'] --- list of dimension bounds
-      D['intent'] --- list of intent specifications
-      D['depend'] --- list of variable names on which current variable depends on
-      D['check'] --- list of C-expressions; if C-expr returns zero, exception is raised
-      D['note'] --- list of LaTeX comments on the variable
-  *** Meaning of kind/char selectors (few examples):
-      D['typespec>']*K['*']
-      D['typespec'](kind=K['kind'])
-      character*C['*']
-      character(len=C['len'],kind=C['kind'])
-      (see also fortran type declaration statement formats below)
-
- Fortran 90 type declaration statement format (F77 is subset of F90)
-====================================================================
- (Main source: IBM XL Fortran 5.1 Language Reference Manual)
- type declaration = <typespec> [[<attrspec>]::] <entitydecl>
- <typespec> = byte                          |
-              character[<charselector>]     |
-              complex[<kindselector>]       |
-              double complex                |
-              double precision              |
-              integer[<kindselector>]       |
-              logical[<kindselector>]       |
-              real[<kindselector>]          |
-              type(<typename>)
- <charselector> = * <charlen>               |
-              ([len=]<len>[,[kind=]<kind>]) |
-              (kind=<kind>[,len=<len>])
- <kindselector> = * <intlen>                |
-              ([kind=]<kind>)
- <attrspec> = comma separated list of attributes.
-              Only the following attributes are used in
-              building up the interface:
-                 external
-                 (parameter --- affects '=' key)
-                 optional
-                 intent
-              Other attributes are ignored.
- <intentspec> = in | out | inout
- <arrayspec> = comma separated list of dimension bounds.
- <entitydecl> = <name> [[*<charlen>][(<arrayspec>)] | [(<arrayspec>)]*<charlen>]
-                       [/<init_expr>/ | =<init_expr>] [,<entitydecl>]
-
- In addition, the following attributes are used: check,depend,note
-
- TODO:
-     * Apply 'parameter' attribute (e.g. 'integer parameter :: i=2' 'real x(i)'
-                                    -> 'real x(2)')
-     The above may be solved by creating appropriate preprocessor program, for example.
-"""
-#
-import sys,string,fileinput,re,pprint,os,copy
-from auxfuncs import *
-
 # Global flags:
-strictf77=1          # Ignore `!' comments unless line[0]=='!'
-sourcecodeform='fix' # 'fix','free'
-quiet=0              # Be verbose if 0 (Obsolete: not used any more)
-verbose=1            # Be quiet if 0, extra verbose if > 1.
-tabchar=4*' '
-pyffilename=''
-f77modulename=''
-skipemptyends=0      # for old F77 programs without 'program' statement
-ignorecontains=1
-dolowercase=1
-debug=[]
-## do_analyze = 1
-
-###### global variables
-
-## use reload(crackfortran) to reset these variables
-
-groupcounter=0
-grouplist={groupcounter:[]}
-neededmodule=-1
-expectbegin=1
-skipblocksuntil=-1
-usermodules=[]
-f90modulevars={}
-gotnextfile=1
-filepositiontext=''
-currentfilename=''
-skipfunctions=[]
-skipfuncs=[]
-onlyfuncs=[]
-include_paths=[]
+strictf77 = 1          # Ignore `!' comments unless line[0]=='!'
+sourcecodeform = 'fix'  # 'fix','free'
+quiet = 0              # Be verbose if 0 (Obsolete: not used any more)
+verbose = 1            # Be quiet if 0, extra verbose if > 1.
+tabchar = 4 * ' '
+pyffilename = ''
+f77modulename = ''
+skipemptyends = 0      # for old F77 programs without 'program' statement
+ignorecontains = 1
+dolowercase = 1
+debug = []
+
+# Global variables
+beginpattern = ''
+currentfilename = ''
+expectbegin = 1
+f90modulevars = {}
+filepositiontext = ''
+gotnextfile = 1
+groupcache = None
+groupcounter = 0
+grouplist = {groupcounter: []}
+groupname = ''
+include_paths = []
+neededmodule = -1
+onlyfuncs = []
 previous_context = None
-
-###### Some helper functions
-def show(o,f=0):pprint.pprint(o)
-errmess=sys.stderr.write
-def outmess(line,flag=1):
+skipblocksuntil = -1
+skipfuncs = []
+skipfunctions = []
+usermodules = []
+
+
+def reset_global_f2py_vars():
+    global groupcounter, grouplist, neededmodule, expectbegin
+    global skipblocksuntil, usermodules, f90modulevars, gotnextfile
+    global filepositiontext, currentfilename, skipfunctions, skipfuncs
+    global onlyfuncs, include_paths, previous_context
+    global strictf77, sourcecodeform, quiet, verbose, tabchar, pyffilename
+    global f77modulename, skipemptyends, ignorecontains, dolowercase, debug
+
+    # flags
+    strictf77 = 1
+    sourcecodeform = 'fix'
+    quiet = 0
+    verbose = 1
+    tabchar = 4 * ' '
+    pyffilename = ''
+    f77modulename = ''
+    skipemptyends = 0
+    ignorecontains = 1
+    dolowercase = 1
+    debug = []
+    # variables
+    groupcounter = 0
+    grouplist = {groupcounter: []}
+    neededmodule = -1
+    expectbegin = 1
+    skipblocksuntil = -1
+    usermodules = []
+    f90modulevars = {}
+    gotnextfile = 1
+    filepositiontext = ''
+    currentfilename = ''
+    skipfunctions = []
+    skipfuncs = []
+    onlyfuncs = []
+    include_paths = []
+    previous_context = None
+
+
+def outmess(line, flag=1):
     global filepositiontext
-    if not verbose: return
+
+    if not verbose:
+        return
     if not quiet:
-        if flag:sys.stdout.write(filepositiontext)
+        if flag:
+            sys.stdout.write(filepositiontext)
         sys.stdout.write(line)
-re._MAXCACHE=50
-defaultimplicitrules={}
-for c in "abcdefghopqrstuvwxyz$_": defaultimplicitrules[c]={'typespec':'real'}
-for c in "ijklmn": defaultimplicitrules[c]={'typespec':'integer'}
-del c
-badnames={}
-invbadnames={}
-for n in ['int','double','float','char','short','long','void','case','while',
-          'return','signed','unsigned','if','for','typedef','sizeof','union',
-          'struct','static','register','new','break','do','goto','switch',
-          'continue','else','inline','extern','delete','const','auto',
-          'len','rank','shape','index','slen','size','_i',
-          'flen','fshape',
-          'string','complex_double','float_double','stdin','stderr','stdout',
-          'type','default']:
-    badnames[n]=n+'_bn'
-    invbadnames[n+'_bn']=n
+
+re._MAXCACHE = 50
+defaultimplicitrules = {}
+for c in "abcdefghopqrstuvwxyz$_":
+    defaultimplicitrules[c] = {'typespec': 'real'}
+for c in "ijklmn":
+    defaultimplicitrules[c] = {'typespec': 'integer'}
+badnames = {}
+invbadnames = {}
+for n in ['int', 'double', 'float', 'char', 'short', 'long', 'void', 'case', 'while',
+          'return', 'signed', 'unsigned', 'if', 'for', 'typedef', 'sizeof', 'union',
+          'struct', 'static', 'register', 'new', 'break', 'do', 'goto', 'switch',
+          'continue', 'else', 'inline', 'extern', 'delete', 'const', 'auto',
+          'len', 'rank', 'shape', 'index', 'slen', 'size', '_i',
+          'max', 'min',
+          'flen', 'fshape',
+          'string', 'complex_double', 'float_double', 'stdin', 'stderr', 'stdout',
+          'type', 'default']:
+    badnames[n] = n + '_bn'
+    invbadnames[n + '_bn'] = n
+
+
 def rmbadname1(name):
-    if badnames.has_key(name):
-        errmess('rmbadname1: Replacing "%s" with "%s".\n'%(name,badnames[name]))
+    if name in badnames:
+        errmess('rmbadname1: Replacing "%s" with "%s".\n' %
+                (name, badnames[name]))
         return badnames[name]
     return name
-def rmbadname(names): return map(rmbadname1,names)
+
+
+def rmbadname(names):
+    return [rmbadname1(_m) for _m in names]
+
 
 def undo_rmbadname1(name):
-    if invbadnames.has_key(name):
-        errmess('undo_rmbadname1: Replacing "%s" with "%s".\n'\
-                %(name,invbadnames[name]))
+    if name in invbadnames:
+        errmess('undo_rmbadname1: Replacing "%s" with "%s".\n'
+                % (name, invbadnames[name]))
         return invbadnames[name]
     return name
-def undo_rmbadname(names): return map(undo_rmbadname1,names)
+
+
+def undo_rmbadname(names):
+    return [undo_rmbadname1(_m) for _m in names]
+
 
 def getextension(name):
-    i=string.rfind(name,'.')
-    if i==-1: return ''
-    if '\\' in name[i:]: return ''
-    if '/' in name[i:]: return ''
-    return name[i+1:]
-
-is_f_file = re.compile(r'.*[.](for|ftn|f77|f)\Z',re.I).match
-_has_f_header = re.compile(r'-[*]-\s*fortran\s*-[*]-',re.I).search
-_has_f90_header = re.compile(r'-[*]-\s*f90\s*-[*]-',re.I).search
-_has_fix_header = re.compile(r'-[*]-\s*fix\s*-[*]-',re.I).search
-_free_f90_start = re.compile(r'[^c*]\s*[^\s\d\t]',re.I).match
+    i = name.rfind('.')
+    if i == -1:
+        return ''
+    if '\\' in name[i:]:
+        return ''
+    if '/' in name[i:]:
+        return ''
+    return name[i + 1:]
+
+is_f_file = re.compile(r'.*\.(for|ftn|f77|f)\Z', re.I).match
+_has_f_header = re.compile(r'-\*-\s*fortran\s*-\*-', re.I).search
+_has_f90_header = re.compile(r'-\*-\s*f90\s*-\*-', re.I).search
+_has_fix_header = re.compile(r'-\*-\s*fix\s*-\*-', re.I).search
+_free_f90_start = re.compile(r'[^c*]\s*[^\s\d\t]', re.I).match
+
+
 def is_free_format(file):
     """Check if file is in free format Fortran."""
     # f90 allows both fixed and free format, assuming fixed unless
     # signs of free format are detected.
     result = 0
-    f = open(file,'r')
-    line = f.readline()
-    n = 15 # the number of non-comment lines to scan for hints
-    if _has_f_header(line):
-        n = 0
-    elif _has_f90_header(line):
-        n = 0
-        result = 1
-    while n>0 and line:
-        if line[0]!='!':
-            n -= 1
-            if (line[0]!='\t' and _free_f90_start(line[:5])) or line[-2:-1]=='&':
-                result = 1
-                break
+    with open(file, 'r') as f:
         line = f.readline()
-    f.close()
+        n = 15  # the number of non-comment lines to scan for hints
+        if _has_f_header(line):
+            n = 0
+        elif _has_f90_header(line):
+            n = 0
+            result = 1
+        while n > 0 and line:
+            if line[0] != '!' and line.strip():
+                n -= 1
+                if (line[0] != '\t' and _free_f90_start(line[:5])) or line[-2:-1] == '&':
+                    result = 1
+                    break
+            line = f.readline()
     return result
 
 
-####### Read fortran (77,90) code
-def readfortrancode(ffile,dowithline=show,istop=1):
+# Read fortran (77,90) code
+def readfortrancode(ffile, dowithline=show, istop=1):
     """
     Read fortran codes from files and
      1) Get rid of comments, line continuations, and empty lines; lower cases.
      2) Call dowithline(line) on every line.
      3) Recursively call itself when statement \"include '<filename>'\" is met.
     """
-    global gotnextfile,filepositiontext,currentfilename,sourcecodeform,strictf77,\
-           beginpattern,quiet,verbose,dolowercase,include_paths
+    global gotnextfile, filepositiontext, currentfilename, sourcecodeform, strictf77
+    global beginpattern, quiet, verbose, dolowercase, include_paths
+
     if not istop:
-        saveglobals=gotnextfile,filepositiontext,currentfilename,sourcecodeform,strictf77,\
-           beginpattern,quiet,verbose,dolowercase
-    if ffile==[]: return
+        saveglobals = gotnextfile, filepositiontext, currentfilename, sourcecodeform, strictf77,\
+            beginpattern, quiet, verbose, dolowercase
+    if ffile == []:
+        return
     localdolowercase = dolowercase
-    cont=0
-    finalline=''
-    ll=''
-    commentline=re.compile(r'(?P<line>([^"]*"[^"]*"[^"!]*|[^\']*\'[^\']*\'[^\'!]*|[^!]*))!{1}(?P<rest>.*)')
-    includeline=re.compile(r'\s*include\s*(\'|")(?P<name>[^\'"]*)(\'|")',re.I)
-    cont1=re.compile(r'(?P<line>.*)&\s*\Z')
-    cont2=re.compile(r'(\s*&|)(?P<line>.*)')
+    # cont: set to True when the content of the last line read
+    # indicates statement continuation
+    cont = False
+    finalline = ''
+    ll = ''
+    includeline = re.compile(
+        r'\s*include\s*(\'|")(?P<name>[^\'"]*)(\'|")', re.I)
+    cont1 = re.compile(r'(?P<line>.*)&\s*\Z')
+    cont2 = re.compile(r'(\s*&|)(?P<line>.*)')
     mline_mark = re.compile(r".*?'''")
-    if istop: dowithline('',-1)
-    ll,l1='',''
-    spacedigits=[' ']+map(str,range(10))
-    filepositiontext=''
-    fin=fileinput.FileInput(ffile)
-    while 1:
-        l=fin.readline()
-        if not l: break
+    if istop:
+        dowithline('', -1)
+    ll, l1 = '', ''
+    spacedigits = [' '] + [str(_m) for _m in range(10)]
+    filepositiontext = ''
+    fin = fileinput.FileInput(ffile)
+    while True:
+        l = fin.readline()
+        if not l:
+            break
         if fin.isfirstline():
-            filepositiontext=''
-            currentfilename=fin.filename()
-            gotnextfile=1
-            l1=l
-            strictf77=0
-            sourcecodeform='fix'
+            filepositiontext = ''
+            currentfilename = fin.filename()
+            gotnextfile = 1
+            l1 = l
+            strictf77 = 0
+            sourcecodeform = 'fix'
             ext = os.path.splitext(currentfilename)[1]
             if is_f_file(currentfilename) and \
-                   not (_has_f90_header(l) or _has_fix_header(l)):
-                strictf77=1
+                    not (_has_f90_header(l) or _has_fix_header(l)):
+                strictf77 = 1
             elif is_free_format(currentfilename) and not _has_fix_header(l):
-                sourcecodeform='free'
-            if strictf77: beginpattern=beginpattern77
-            else: beginpattern=beginpattern90
-            outmess('\tReading file %s (format:%s%s)\n'\
-                    %(`currentfilename`,sourcecodeform,
-                      strictf77 and ',strict' or ''))
-
-        l=string.expandtabs(l).replace('\xa0',' ')
-        while not l=='':                       # Get rid of newline characters
-            if l[-1] not in "\n\r\f": break
-            l=l[:-1]
+                sourcecodeform = 'free'
+            if strictf77:
+                beginpattern = beginpattern77
+            else:
+                beginpattern = beginpattern90
+            outmess('\tReading file %s (format:%s%s)\n'
+                    % (repr(currentfilename), sourcecodeform,
+                       strictf77 and ',strict' or ''))
+
+        l = l.expandtabs().replace('\xa0', ' ')
+        # Get rid of newline characters
+        while not l == '':
+            if l[-1] not in "\n\r\f":
+                break
+            l = l[:-1]
         if not strictf77:
-            r=commentline.match(l)
-            if r:
-                l=r.group('line')+' ' # Strip comments starting with `!'
-                rl=r.group('rest')
-                if string.lower(rl[:4])=='f2py': # f2py directive
-                    l = l + 4*' '
-                    r=commentline.match(rl[4:])
-                    if r: l=l+r('line')
-                    else: l = l + rl[4:]
-        if string.strip(l)=='': # Skip empty line
-            cont=0
+            (l, rl) = split_by_unquoted(l, '!')
+            l += ' '
+            if rl[:5].lower() == '!f2py':  # f2py directive
+                l, _ = split_by_unquoted(l + 4 * ' ' + rl[5:], '!')
+        if l.strip() == '':  # Skip empty line
+            if sourcecodeform == 'free':
+                # In free form, a statement continues in the next line
+                # that is not a comment line [3.3.2.4^1], lines with
+                # blanks are comment lines [3.3.2.3^1]. Hence, the
+                # line continuation flag must retain its state.
+                pass
+            else:
+                # In fixed form, statement continuation is determined
+                # by a non-blank character at the 6-th position. Empty
+                # line indicates a start of a new statement
+                # [3.3.3.3^1]. Hence, the line continuation flag must
+                # be reset.
+                cont = False
             continue
-        if sourcecodeform=='fix':
-            if l[0] in ['*','c','!','C','#']:
-                if string.lower(l[1:5])=='f2py': # f2py directive
-                    l='     '+l[5:]
-                else: # Skip comment line
-                    cont=0
+        if sourcecodeform == 'fix':
+            if l[0] in ['*', 'c', '!', 'C', '#']:
+                if l[1:5].lower() == 'f2py':  # f2py directive
+                    l = '     ' + l[5:]
+                else:  # Skip comment line
+                    cont = False
                     continue
             elif strictf77:
-                if len(l)>72: l=l[:72]
+                if len(l) > 72:
+                    l = l[:72]
             if not (l[0] in spacedigits):
-                raise 'readfortrancode: Found non-(space,digit) char in the first column.\n\tAre you sure that this code is in fix form?\n\tline=%s'%`l`
-
-            if (not cont or strictf77) and (len(l)>5 and not l[5]==' '):
+                raise Exception('readfortrancode: Found non-(space,digit) char '
+                                'in the first column.\n\tAre you sure that '
+                                'this code is in fix form?\n\tline=%s' % repr(l))
+
+            if (not cont or strictf77) and (len(l) > 5 and not l[5] == ' '):
                 # Continuation of a previous line
-                ll=ll+l[6:]
-                finalline=''
-                origfinalline=''
+                ll = ll + l[6:]
+                finalline = ''
+                origfinalline = ''
             else:
                 if not strictf77:
                     # F90 continuation
-                    r=cont1.match(l)
-                    if r: l=r.group('line') # Continuation follows ..
+                    r = cont1.match(l)
+                    if r:
+                        l = r.group('line')  # Continuation follows ..
                     if cont:
-                        ll=ll+cont2.match(l).group('line')
-                        finalline=''
-                        origfinalline=''
+                        ll = ll + cont2.match(l).group('line')
+                        finalline = ''
+                        origfinalline = ''
                     else:
-                        l='     '+l[5:] # clean up line beginning from possible digits.
-                        if localdolowercase: finalline=string.lower(ll)
-                        else: finalline=ll
-                        origfinalline=ll
-                        ll=l
-                    cont=(r is not None)
+                        # clean up line beginning from possible digits.
+                        l = '     ' + l[5:]
+                        if localdolowercase:
+                            finalline = ll.lower()
+                        else:
+                            finalline = ll
+                        origfinalline = ll
+                        ll = l
+                    cont = (r is not None)
                 else:
-                    l='     '+l[5:] # clean up line beginning from possible digits.
-                    if localdolowercase: finalline=string.lower(ll)
-                    else: finalline=ll
-                    origfinalline =ll
-                    ll=l
-
-        elif sourcecodeform=='free':
-            if not cont and ext=='.pyf' and mline_mark.match(l):
+                    # clean up line beginning from possible digits.
+                    l = '     ' + l[5:]
+                    if localdolowercase:
+                        finalline = ll.lower()
+                    else:
+                        finalline = ll
+                    origfinalline = ll
+                    ll = l
+
+        elif sourcecodeform == 'free':
+            if not cont and ext == '.pyf' and mline_mark.match(l):
                 l = l + '\n'
-                while 1:
+                while True:
                     lc = fin.readline()
                     if not lc:
-                        errmess('Unexpected end of file when reading multiline\n')
+                        errmess(
+                            'Unexpected end of file when reading multiline\n')
                         break
                     l = l + lc
                     if mline_mark.match(lc):
                         break
                 l = l.rstrip()
-            r=cont1.match(l)
-            if r: l=r.group('line') # Continuation follows ..
+            r = cont1.match(l)
+            if r:
+                l = r.group('line')  # Continuation follows ..
             if cont:
-                ll=ll+cont2.match(l).group('line')
-                finalline=''
-                origfinalline=''
+                ll = ll + cont2.match(l).group('line')
+                finalline = ''
+                origfinalline = ''
             else:
-                if localdolowercase: finalline=string.lower(ll)
-                else: finalline=ll
-                origfinalline =ll
-                ll=l
-            cont=(r is not None)
+                if localdolowercase:
+                    finalline = ll.lower()
+                else:
+                    finalline = ll
+                origfinalline = ll
+                ll = l
+            cont = (r is not None)
         else:
-            raise ValueError,"Flag sourcecodeform must be either 'fix' or 'free': %s"%`sourcecodeform`
-        filepositiontext='Line #%d in %s:"%s"\n\t' % (fin.filelineno()-1,currentfilename,l1)
-        m=includeline.match(origfinalline)
+            raise ValueError(
+                "Flag sourcecodeform must be either 'fix' or 'free': %s" % repr(sourcecodeform))
+        filepositiontext = 'Line #%d in %s:"%s"\n\t' % (
+            fin.filelineno() - 1, currentfilename, l1)
+        m = includeline.match(origfinalline)
         if m:
-            fn=m.group('name')
+            fn = m.group('name')
             if os.path.isfile(fn):
-                readfortrancode(fn,dowithline=dowithline,istop=0)
+                readfortrancode(fn, dowithline=dowithline, istop=0)
             else:
-                include_dirs = [os.path.dirname(currentfilename)] + include_paths
+                include_dirs = [
+                    os.path.dirname(currentfilename)] + include_paths
                 foundfile = 0
                 for inc_dir in include_dirs:
-                    fn1 = os.path.join(inc_dir,fn)
+                    fn1 = os.path.join(inc_dir, fn)
                     if os.path.isfile(fn1):
                         foundfile = 1
-                        readfortrancode(fn1,dowithline=dowithline,istop=0)
+                        readfortrancode(fn1, dowithline=dowithline, istop=0)
                         break
                 if not foundfile:
-                    outmess('readfortrancode: could not find include file %s. Ignoring.\n'%(`fn`))
+                    outmess('readfortrancode: could not find include file %s in %s. Ignoring.\n' % (
+                        repr(fn), os.pathsep.join(include_dirs)))
         else:
             dowithline(finalline)
-        l1=ll
+        l1 = ll
     if localdolowercase:
-        finalline=string.lower(ll)
-    else: finalline=ll
+        finalline = ll.lower()
+    else:
+        finalline = ll
     origfinalline = ll
-    filepositiontext='Line #%d in %s:"%s"\n\t' % (fin.filelineno()-1,currentfilename,l1)
-    m=includeline.match(origfinalline)
+    filepositiontext = 'Line #%d in %s:"%s"\n\t' % (
+        fin.filelineno() - 1, currentfilename, l1)
+    m = includeline.match(origfinalline)
     if m:
-        fn=m.group('name')
-        fn1=os.path.join(os.path.dirname(currentfilename),fn)
+        fn = m.group('name')
         if os.path.isfile(fn):
-            readfortrancode(fn,dowithline=dowithline,istop=0)
-        elif os.path.isfile(fn1):
-            readfortrancode(fn1,dowithline=dowithline,istop=0)
+            readfortrancode(fn, dowithline=dowithline, istop=0)
         else:
-            outmess('readfortrancode: could not find include file %s. Ignoring.\n'%(`fn`))
+            include_dirs = [os.path.dirname(currentfilename)] + include_paths
+            foundfile = 0
+            for inc_dir in include_dirs:
+                fn1 = os.path.join(inc_dir, fn)
+                if os.path.isfile(fn1):
+                    foundfile = 1
+                    readfortrancode(fn1, dowithline=dowithline, istop=0)
+                    break
+            if not foundfile:
+                outmess('readfortrancode: could not find include file %s in %s. Ignoring.\n' % (
+                    repr(fn), os.pathsep.join(include_dirs)))
     else:
         dowithline(finalline)
-    filepositiontext=''
+    filepositiontext = ''
     fin.close()
-    if istop: dowithline('',1)
+    if istop:
+        dowithline('', 1)
     else:
-        gotnextfile,filepositiontext,currentfilename,sourcecodeform,strictf77,\
-           beginpattern,quiet,verbose,dolowercase=saveglobals
-
-########### Crack line
-beforethisafter=r'\s*(?P<before>%s(?=\s*(\b(%s)\b)))'+ \
-                          r'\s*(?P<this>(\b(%s)\b))'+ \
-                          r'\s*(?P<after>%s)\s*\Z'
+        gotnextfile, filepositiontext, currentfilename, sourcecodeform, strictf77,\
+            beginpattern, quiet, verbose, dolowercase = saveglobals
+
+# Crack line
+beforethisafter = r'\s*(?P<before>%s(?=\s*(\b(%s)\b)))' + \
+    r'\s*(?P<this>(\b(%s)\b))' + \
+    r'\s*(?P<after>%s)\s*\Z'
 ##
-fortrantypes='character|logical|integer|real|complex|double\s*(precision\s*(complex|)|complex)|type(?=\s*\([\w\s,=(*)]*\))|byte'
-typespattern=re.compile(beforethisafter%('',fortrantypes,fortrantypes,'.*'),re.I),'type'
-typespattern4implicit=re.compile(beforethisafter%('',fortrantypes+'|static|automatic|undefined',fortrantypes+'|static|automatic|undefined','.*'),re.I)
+fortrantypes = r'character|logical|integer|real|complex|double\s*(precision\s*(complex|)|complex)|type(?=\s*\([\w\s,=(*)]*\))|byte'
+typespattern = re.compile(
+    beforethisafter % ('', fortrantypes, fortrantypes, '.*'), re.I), 'type'
+typespattern4implicit = re.compile(beforethisafter % (
+    '', fortrantypes + '|static|automatic|undefined', fortrantypes + '|static|automatic|undefined', '.*'), re.I)
 #
-functionpattern=re.compile(beforethisafter%('([a-z]+[\w\s(=*+-/)]*?|)','function','function','.*'),re.I),'begin'
-subroutinepattern=re.compile(beforethisafter%('[a-z\s]*?','subroutine','subroutine','.*'),re.I),'begin'
-#modulepattern=re.compile(beforethisafter%('[a-z\s]*?','module','module','.*'),re.I),'begin'
+functionpattern = re.compile(beforethisafter % (
+    r'([a-z]+[\w\s(=*+-/)]*?|)', 'function', 'function', '.*'), re.I), 'begin'
+subroutinepattern = re.compile(beforethisafter % (
+    r'[a-z\s]*?', 'subroutine', 'subroutine', '.*'), re.I), 'begin'
+# modulepattern=re.compile(beforethisafter%('[a-z\s]*?','module','module','.*'),re.I),'begin'
 #
-groupbegins77=r'program|block\s*data'
-beginpattern77=re.compile(beforethisafter%('',groupbegins77,groupbegins77,'.*'),re.I),'begin'
-groupbegins90=groupbegins77+r'|module|python\s*module|interface|type(?!\s*\()'
-beginpattern90=re.compile(beforethisafter%('',groupbegins90,groupbegins90,'.*'),re.I),'begin'
-groupends=r'end|endprogram|endblockdata|endmodule|endpythonmodule|endinterface'
-endpattern=re.compile(beforethisafter%('',groupends,groupends,'[\w\s]*'),re.I),'end'
-#endifs='end\s*(if|do|where|select|while|forall)'
-endifs='(end\s*(if|do|where|select|while|forall))|(module\s*procedure)'
-endifpattern=re.compile(beforethisafter%('[\w]*?',endifs,endifs,'[\w\s]*'),re.I),'endif'
+groupbegins77 = r'program|block\s*data'
+beginpattern77 = re.compile(
+    beforethisafter % ('', groupbegins77, groupbegins77, '.*'), re.I), 'begin'
+groupbegins90 = groupbegins77 + \
+    r'|module(?!\s*procedure)|python\s*module|(abstract|)\s*interface|' + \
+    r'type(?!\s*\()'
+beginpattern90 = re.compile(
+    beforethisafter % ('', groupbegins90, groupbegins90, '.*'), re.I), 'begin'
+groupends = (r'end|endprogram|endblockdata|endmodule|endpythonmodule|'
+             r'endinterface|endsubroutine|endfunction')
+endpattern = re.compile(
+    beforethisafter % ('', groupends, groupends, r'.*'), re.I), 'end'
+endifs = r'end\s*(if|do|where|select|while|forall|associate|block|' + \
+         r'critical|enum|team)'
+endifpattern = re.compile(
+    beforethisafter % (r'[\w]*?', endifs, endifs, r'[\w\s]*'), re.I), 'endif'
 #
-implicitpattern=re.compile(beforethisafter%('','implicit','implicit','.*'),re.I),'implicit'
-dimensionpattern=re.compile(beforethisafter%('','dimension|virtual','dimension|virtual','.*'),re.I),'dimension'
-externalpattern=re.compile(beforethisafter%('','external','external','.*'),re.I),'external'
-optionalpattern=re.compile(beforethisafter%('','optional','optional','.*'),re.I),'optional'
-requiredpattern=re.compile(beforethisafter%('','required','required','.*'),re.I),'required'
-publicpattern=re.compile(beforethisafter%('','public','public','.*'),re.I),'public'
-privatepattern=re.compile(beforethisafter%('','private','private','.*'),re.I),'private'
-intrisicpattern=re.compile(beforethisafter%('','intrisic','intrisic','.*'),re.I),'intrisic'
-intentpattern=re.compile(beforethisafter%('','intent|depend|note|check','intent|depend|note|check','\s*\(.*?\).*'),re.I),'intent'
-parameterpattern=re.compile(beforethisafter%('','parameter','parameter','\s*\(.*'),re.I),'parameter'
-datapattern=re.compile(beforethisafter%('','data','data','.*'),re.I),'data'
-callpattern=re.compile(beforethisafter%('','call','call','.*'),re.I),'call'
-entrypattern=re.compile(beforethisafter%('','entry','entry','.*'),re.I),'entry'
-callfunpattern=re.compile(beforethisafter%('','callfun','callfun','.*'),re.I),'callfun'
-commonpattern=re.compile(beforethisafter%('','common','common','.*'),re.I),'common'
-usepattern=re.compile(beforethisafter%('','use','use','.*'),re.I),'use'
-containspattern=re.compile(beforethisafter%('','contains','contains',''),re.I),'contains'
-formatpattern=re.compile(beforethisafter%('','format','format','.*'),re.I),'format'
-## Non-fortran and f2py-specific statements
-f2pyenhancementspattern=re.compile(beforethisafter%('','threadsafe|fortranname|callstatement|callprotoargument|usercode|pymethoddef','threadsafe|fortranname|callstatement|callprotoargument|usercode|pymethoddef','.*'),re.I|re.S),'f2pyenhancements'
-multilinepattern = re.compile(r"\s*(?P<before>''')(?P<this>.*?)(?P<after>''')\s*\Z",re.S),'multiline'
+moduleprocedures = r'module\s*procedure'
+moduleprocedurepattern = re.compile(
+    beforethisafter % ('', moduleprocedures, moduleprocedures, r'.*'), re.I), \
+    'moduleprocedure'
+implicitpattern = re.compile(
+    beforethisafter % ('', 'implicit', 'implicit', '.*'), re.I), 'implicit'
+dimensionpattern = re.compile(beforethisafter % (
+    '', 'dimension|virtual', 'dimension|virtual', '.*'), re.I), 'dimension'
+externalpattern = re.compile(
+    beforethisafter % ('', 'external', 'external', '.*'), re.I), 'external'
+optionalpattern = re.compile(
+    beforethisafter % ('', 'optional', 'optional', '.*'), re.I), 'optional'
+requiredpattern = re.compile(
+    beforethisafter % ('', 'required', 'required', '.*'), re.I), 'required'
+publicpattern = re.compile(
+    beforethisafter % ('', 'public', 'public', '.*'), re.I), 'public'
+privatepattern = re.compile(
+    beforethisafter % ('', 'private', 'private', '.*'), re.I), 'private'
+intrinsicpattern = re.compile(
+    beforethisafter % ('', 'intrinsic', 'intrinsic', '.*'), re.I), 'intrinsic'
+intentpattern = re.compile(beforethisafter % (
+    '', 'intent|depend|note|check', 'intent|depend|note|check', r'\s*\(.*?\).*'), re.I), 'intent'
+parameterpattern = re.compile(
+    beforethisafter % ('', 'parameter', 'parameter', r'\s*\(.*'), re.I), 'parameter'
+datapattern = re.compile(
+    beforethisafter % ('', 'data', 'data', '.*'), re.I), 'data'
+callpattern = re.compile(
+    beforethisafter % ('', 'call', 'call', '.*'), re.I), 'call'
+entrypattern = re.compile(
+    beforethisafter % ('', 'entry', 'entry', '.*'), re.I), 'entry'
+callfunpattern = re.compile(
+    beforethisafter % ('', 'callfun', 'callfun', '.*'), re.I), 'callfun'
+commonpattern = re.compile(
+    beforethisafter % ('', 'common', 'common', '.*'), re.I), 'common'
+usepattern = re.compile(
+    beforethisafter % ('', 'use', 'use', '.*'), re.I), 'use'
+containspattern = re.compile(
+    beforethisafter % ('', 'contains', 'contains', ''), re.I), 'contains'
+formatpattern = re.compile(
+    beforethisafter % ('', 'format', 'format', '.*'), re.I), 'format'
+# Non-fortran and f2py-specific statements
+f2pyenhancementspattern = re.compile(beforethisafter % ('', 'threadsafe|fortranname|callstatement|callprotoargument|usercode|pymethoddef',
+                                                        'threadsafe|fortranname|callstatement|callprotoargument|usercode|pymethoddef', '.*'), re.I | re.S), 'f2pyenhancements'
+multilinepattern = re.compile(
+    r"\s*(?P<before>''')(?P<this>.*?)(?P<after>''')\s*\Z", re.S), 'multiline'
 ##
+
+def split_by_unquoted(line, characters):
+    """
+    Splits the line into (line[:i], line[i:]),
+    where i is the index of first occurrence of one of the characters
+    not within quotes, or len(line) if no such index exists
+    """
+    assert not (set('"\'') & set(characters)), "cannot split by unquoted quotes"
+    r = re.compile(
+        r"\A(?P<before>({single_quoted}|{double_quoted}|{not_quoted})*)"
+        r"(?P<after>{char}.*)\Z".format(
+            not_quoted="[^\"'{}]".format(re.escape(characters)),
+            char="[{}]".format(re.escape(characters)),
+            single_quoted=r"('([^'\\]|(\\.))*')",
+            double_quoted=r'("([^"\\]|(\\.))*")'))
+    m = r.match(line)
+    if m:
+        d = m.groupdict()
+        return (d["before"], d["after"])
+    return (line, "")
 
 def _simplifyargs(argsline):
     a = []
-    for n in string.split(markoutercomma(argsline),'@,@'):
+    for n in markoutercomma(argsline).split('@,@'):
         for r in '(),':
-            n = string.replace(n,r,'_')
+            n = n.replace(r, '_')
         a.append(n)
-    return string.join(a,',')
-
-crackline_re_1 = re.compile(r'\s*(?P<result>\b[a-z]+[\w]*\b)\s*[=].*',re.I)
-def crackline(line,reset=0):
+    return ','.join(a)
+
+crackline_re_1 = re.compile(r'\s*(?P<result>\b[a-z]+\w*\b)\s*=.*', re.I)
+
+
+def crackline(line, reset=0):
     """
     reset=-1  --- initialize
     reset=0   --- crack the line
-    reset=1   --- final check if mismatch of blocks occured
+    reset=1   --- final check if mismatch of blocks occurred
 
     Cracked data is saved in grouplist[0].
     """
-    global beginpattern,groupcounter,groupname,groupcache,grouplist,gotnextfile,\
-           filepositiontext,currentfilename,neededmodule,expectbegin,skipblocksuntil,\
-           skipemptyends,previous_context
-    if ';' in line and not (f2pyenhancementspattern[0].match(line) or
-                            multilinepattern[0].match(line)):
-        for l in line.split(';'):
-            assert reset==0,`reset` # XXX: non-zero reset values need testing
-            crackline(l,reset)
+    global beginpattern, groupcounter, groupname, groupcache, grouplist
+    global filepositiontext, currentfilename, neededmodule, expectbegin
+    global skipblocksuntil, skipemptyends, previous_context, gotnextfile
+
+    _, has_semicolon = split_by_unquoted(line, ";")
+    if has_semicolon and not (f2pyenhancementspattern[0].match(line) or
+                               multilinepattern[0].match(line)):
+        # XXX: non-zero reset values need testing
+        assert reset == 0, repr(reset)
+        # split line on unquoted semicolons
+        line, semicolon_line = split_by_unquoted(line, ";")
+        while semicolon_line:
+            crackline(line, reset)
+            line, semicolon_line = split_by_unquoted(semicolon_line[1:], ";")
+        crackline(line, reset)
         return
-    if reset<0:
-        groupcounter=0
-        groupname={groupcounter:''}
-        groupcache={groupcounter:{}}
-        grouplist={groupcounter:[]}
-        groupcache[groupcounter]['body']=[]
-        groupcache[groupcounter]['vars']={}
-        groupcache[groupcounter]['block']=''
-        groupcache[groupcounter]['name']=''
-        neededmodule=-1
-        skipblocksuntil=-1
+    if reset < 0:
+        groupcounter = 0
+        groupname = {groupcounter: ''}
+        groupcache = {groupcounter: {}}
+        grouplist = {groupcounter: []}
+        groupcache[groupcounter]['body'] = []
+        groupcache[groupcounter]['vars'] = {}
+        groupcache[groupcounter]['block'] = ''
+        groupcache[groupcounter]['name'] = ''
+        neededmodule = -1
+        skipblocksuntil = -1
         return
-    if reset>0:
-        fl=0
-        if f77modulename and neededmodule==groupcounter: fl=2
-        while groupcounter>fl:
-            outmess('crackline: groupcounter=%s groupname=%s\n'%(`groupcounter`,`groupname`))
-            outmess('crackline: Mismatch of blocks encountered. Trying to fix it by assuming "end" statement.\n')
-            grouplist[groupcounter-1].append(groupcache[groupcounter])
-            grouplist[groupcounter-1][-1]['body']=grouplist[groupcounter]
+    if reset > 0:
+        fl = 0
+        if f77modulename and neededmodule == groupcounter:
+            fl = 2
+        while groupcounter > fl:
+            outmess('crackline: groupcounter=%s groupname=%s\n' %
+                    (repr(groupcounter), repr(groupname)))
+            outmess(
+                'crackline: Mismatch of blocks encountered. Trying to fix it by assuming "end" statement.\n')
+            grouplist[groupcounter - 1].append(groupcache[groupcounter])
+            grouplist[groupcounter - 1][-1]['body'] = grouplist[groupcounter]
             del grouplist[groupcounter]
-            groupcounter=groupcounter-1
-        if f77modulename and neededmodule==groupcounter:
-            grouplist[groupcounter-1].append(groupcache[groupcounter])
-            grouplist[groupcounter-1][-1]['body']=grouplist[groupcounter]
+            groupcounter = groupcounter - 1
+        if f77modulename and neededmodule == groupcounter:
+            grouplist[groupcounter - 1].append(groupcache[groupcounter])
+            grouplist[groupcounter - 1][-1]['body'] = grouplist[groupcounter]
             del grouplist[groupcounter]
-            groupcounter=groupcounter-1 # end interface
-            grouplist[groupcounter-1].append(groupcache[groupcounter])
-            grouplist[groupcounter-1][-1]['body']=grouplist[groupcounter]
+            groupcounter = groupcounter - 1  # end interface
+            grouplist[groupcounter - 1].append(groupcache[groupcounter])
+            grouplist[groupcounter - 1][-1]['body'] = grouplist[groupcounter]
             del grouplist[groupcounter]
-            groupcounter=groupcounter-1 # end module
-            neededmodule=-1
+            groupcounter = groupcounter - 1  # end module
+            neededmodule = -1
         return
-    if line=='': return
-    flag=0
-    for pat in [dimensionpattern,externalpattern,intentpattern,optionalpattern,
+    if line == '':
+        return
+    flag = 0
+    for pat in [dimensionpattern, externalpattern, intentpattern, optionalpattern,
                 requiredpattern,
-                parameterpattern,datapattern,publicpattern,privatepattern,
-                intrisicpattern,
-                endifpattern,endpattern,
+                parameterpattern, datapattern, publicpattern, privatepattern,
+                intrinsicpattern,
+                endifpattern, endpattern,
                 formatpattern,
-                beginpattern,functionpattern,subroutinepattern,
-                implicitpattern,typespattern,commonpattern,
-                callpattern,usepattern,containspattern,
+                beginpattern, functionpattern, subroutinepattern,
+                implicitpattern, typespattern, commonpattern,
+                callpattern, usepattern, containspattern,
                 entrypattern,
                 f2pyenhancementspattern,
-                multilinepattern
+                multilinepattern,
+                moduleprocedurepattern
                 ]:
         m = pat[0].match(line)
         if m:
             break
-        flag=flag+1
+        flag = flag + 1
     if not m:
         re_1 = crackline_re_1
-        if 0<=skipblocksuntil<=groupcounter:return
-        if groupcache[groupcounter].has_key('externals'):
+        if 0 <= skipblocksuntil <= groupcounter:
+            return
+        if 'externals' in groupcache[groupcounter]:
             for name in groupcache[groupcounter]['externals']:
-                if invbadnames.has_key(name):
-                    name=invbadnames[name]
-                if groupcache[groupcounter].has_key('interfaced') and name in groupcache[groupcounter]['interfaced']: continue
-                m1=re.match(r'(?P<before>[^"]*)\b%s\b\s*@\(@(?P<args>[^@]*)@\)@.*\Z'%name,markouterparen(line),re.I)
+                if name in invbadnames:
+                    name = invbadnames[name]
+                if 'interfaced' in groupcache[groupcounter] and name in groupcache[groupcounter]['interfaced']:
+                    continue
+                m1 = re.match(
+                    r'(?P<before>[^"]*)\b%s\b\s*@\(@(?P<args>[^@]*)@\)@.*\Z' % name, markouterparen(line), re.I)
                 if m1:
                     m2 = re_1.match(m1.group('before'))
                     a = _simplifyargs(m1.group('args'))
                     if m2:
-                        line='callfun %s(%s) result (%s)'%(name,a,m2.group('result'))
-                    else: line='callfun %s(%s)'%(name,a)
+                        line = 'callfun %s(%s) result (%s)' % (
+                            name, a, m2.group('result'))
+                    else:
+                        line = 'callfun %s(%s)' % (name, a)
                     m = callfunpattern[0].match(line)
                     if not m:
-                        outmess('crackline: could not resolve function call for line=%s.\n'%`line`)
+                        outmess(
+                            'crackline: could not resolve function call for line=%s.\n' % repr(line))
                         return
-                    analyzeline(m,'callfun',line)
+                    analyzeline(m, 'callfun', line)
                     return
-        if verbose>1:
+        if verbose > 1 or (verbose == 1 and currentfilename.lower().endswith('.pyf')):
             previous_context = None
-            outmess('crackline:%d: No pattern for line\n'%(groupcounter))
+            outmess('crackline:%d: No pattern for line\n' % (groupcounter))
         return
-    elif pat[1]=='end':
-        if 0<=skipblocksuntil<groupcounter:
-            groupcounter=groupcounter-1
-            if skipblocksuntil<=groupcounter: return
-        if groupcounter<=0:
-            raise 'crackline: groupcounter(=%s) is nonpositive. Check the blocks.'\
-                  % (groupcounter)
+    elif pat[1] == 'end':
+        if 0 <= skipblocksuntil < groupcounter:
+            groupcounter = groupcounter - 1
+            if skipblocksuntil <= groupcounter:
+                return
+        if groupcounter <= 0:
+            raise Exception('crackline: groupcounter(=%s) is nonpositive. '
+                            'Check the blocks.'
+                            % (groupcounter))
         m1 = beginpattern[0].match((line))
-        if (m1) and (not m1.group('this')==groupname[groupcounter]):
-            raise 'crackline: End group %s does not match with previous Begin group %s\n\t%s'%(`m1.group('this')`,`groupname[groupcounter]`,filepositiontext)
-        if skipblocksuntil==groupcounter:
-            skipblocksuntil=-1
-        grouplist[groupcounter-1].append(groupcache[groupcounter])
-        grouplist[groupcounter-1][-1]['body']=grouplist[groupcounter]
+        if (m1) and (not m1.group('this') == groupname[groupcounter]):
+            raise Exception('crackline: End group %s does not match with '
+                            'previous Begin group %s\n\t%s' %
+                            (repr(m1.group('this')), repr(groupname[groupcounter]),
+                             filepositiontext)
+                            )
+        if skipblocksuntil == groupcounter:
+            skipblocksuntil = -1
+        grouplist[groupcounter - 1].append(groupcache[groupcounter])
+        grouplist[groupcounter - 1][-1]['body'] = grouplist[groupcounter]
         del grouplist[groupcounter]
-        groupcounter=groupcounter-1
+        groupcounter = groupcounter - 1
         if not skipemptyends:
-            expectbegin=1
+            expectbegin = 1
     elif pat[1] == 'begin':
-        if 0<=skipblocksuntil<=groupcounter:
-            groupcounter=groupcounter+1
+        if 0 <= skipblocksuntil <= groupcounter:
+            groupcounter = groupcounter + 1
             return
-        gotnextfile=0
-        analyzeline(m,pat[1],line)
-        expectbegin=0
-    elif pat[1]=='endif':
+        gotnextfile = 0
+        analyzeline(m, pat[1], line)
+        expectbegin = 0
+    elif pat[1] == 'endif':
         pass
-    elif pat[1]=='contains':
-        if ignorecontains: return
-        if 0<=skipblocksuntil<=groupcounter: return
-        skipblocksuntil=groupcounter
+    elif pat[1] == 'moduleprocedure':
+        analyzeline(m, pat[1], line)
+    elif pat[1] == 'contains':
+        if ignorecontains:
+            return
+        if 0 <= skipblocksuntil <= groupcounter:
+            return
+        skipblocksuntil = groupcounter
     else:
-        if 0<=skipblocksuntil<=groupcounter:return
-        analyzeline(m,pat[1],line)
+        if 0 <= skipblocksuntil <= groupcounter:
+            return
+        analyzeline(m, pat[1], line)
+
 
 def markouterparen(line):
-    l='';f=0
+    l = ''
+    f = 0
     for c in line:
-        if c=='(':
-            f=f+1
-            if f==1: l=l+'@(@'; continue
-        elif c==')':
-            f=f-1
-            if f==0: l=l+'@)@'; continue
-        l=l+c
+        if c == '(':
+            f = f + 1
+            if f == 1:
+                l = l + '@(@'
+                continue
+        elif c == ')':
+            f = f - 1
+            if f == 0:
+                l = l + '@)@'
+                continue
+        l = l + c
     return l
-def markoutercomma(line,comma=','):
-    l='';f=0
-    cc=''
-    for c in line:
-        if (not cc or cc==')') and c=='(':
-            f=f+1
-            cc = ')'
-        elif not cc and c=='\'' and (not l or l[-1]!='\\'):
-            f=f+1
-            cc = '\''
-        elif c==cc:
-            f=f-1
-            if f==0:
-                cc=''
-        elif c==comma and f==0:
-            l=l+'@'+comma+'@'
-            continue
-        l=l+c
-    assert not f,`f,line,l,cc`
+
+
+def markoutercomma(line, comma=','):
+    l = ''
+    f = 0
+    before, after = split_by_unquoted(line, comma + '()')
+    l += before
+    while after:
+        if (after[0] == comma) and (f == 0):
+            l += '@' + comma + '@'
+        else:
+            l += after[0]
+            if after[0] == '(':
+                f += 1
+            elif after[0] == ')':
+                f -= 1
+        before, after = split_by_unquoted(after[1:], comma + '()')
+        l += before
+    assert not f, repr((f, line, l))
     return l
+
 def unmarkouterparen(line):
-    r = string.replace(string.replace(line,'@(@','('),'@)@',')')
+    r = line.replace('@(@', '(').replace('@)@', ')')
     return r
-def appenddecl(decl,decl2,force=1):
-    if not decl: decl={}
-    if not decl2: return decl
-    if decl is decl2: return decl
-    for k in decl2.keys():
-        if k=='typespec':
-            if force or not decl.has_key(k): decl[k]=decl2[k]
-        elif k=='attrspec':
+
+
+def appenddecl(decl, decl2, force=1):
+    if not decl:
+        decl = {}
+    if not decl2:
+        return decl
+    if decl is decl2:
+        return decl
+    for k in list(decl2.keys()):
+        if k == 'typespec':
+            if force or k not in decl:
+                decl[k] = decl2[k]
+        elif k == 'attrspec':
             for l in decl2[k]:
-                decl=setattrspec(decl,l,force)
-        elif k=='kindselector':
-            decl=setkindselector(decl,decl2[k],force)
-        elif k=='charselector':
-            decl=setcharselector(decl,decl2[k],force)
-        elif k in ['=','typename']:
-            if force or not decl.has_key(k): decl[k]=decl2[k]
-        elif k=='note':
+                decl = setattrspec(decl, l, force)
+        elif k == 'kindselector':
+            decl = setkindselector(decl, decl2[k], force)
+        elif k == 'charselector':
+            decl = setcharselector(decl, decl2[k], force)
+        elif k in ['=', 'typename']:
+            if force or k not in decl:
+                decl[k] = decl2[k]
+        elif k == 'note':
             pass
-        elif k in ['intent','check','dimension','optional','required']:
-            errmess('appenddecl: "%s" not implemented.\n'%k)
+        elif k in ['intent', 'check', 'dimension', 'optional',
+                   'required', 'depend']:
+            errmess('appenddecl: "%s" not implemented.\n' % k)
         else:
-            raise 'appenddecl: Unknown variable definition key:', k
+            raise Exception('appenddecl: Unknown variable definition key: ' +
+                            str(k))
     return decl
 
-selectpattern=re.compile(r'\s*(?P<this>(@\(@.*?@\)@|[*][\d*]+|[*]\s*@\(@.*?@\)@|))(?P<after>.*)\Z',re.I)
-nameargspattern=re.compile(r'\s*(?P<name>\b[\w$]+\b)\s*(@\(@\s*(?P<args>[\w\s,]*)\s*@\)@|)\s*(result(\s*@\(@\s*(?P<result>\b[\w$]+\b)\s*@\)@|))*\s*\Z',re.I)
-callnameargspattern=re.compile(r'\s*(?P<name>\b[\w$]+\b)\s*@\(@\s*(?P<args>.*)\s*@\)@\s*\Z',re.I)
-real16pattern = re.compile(r'([-+]?(?:\d+(?:\.\d*)?|\d*\.\d+))[dD]((?:[-+]?\d+)?)')
-real8pattern = re.compile(r'([-+]?((?:\d+(?:\.\d*)?|\d*\.\d+))[eE]((?:[-+]?\d+)?)|(\d+\.\d*))')
-
-_intentcallbackpattern = re.compile(r'intent\s*\(.*?\bcallback\b',re.I)
+selectpattern = re.compile(
+    r'\s*(?P<this>(@\(@.*?@\)@|\*[\d*]+|\*\s*@\(@.*?@\)@|))(?P<after>.*)\Z', re.I)
+typedefpattern = re.compile(
+    r'(?:,(?P<attributes>[\w(),]+))?(::)?(?P<name>\b[a-z$_][\w$]*\b)'
+    r'(?:\((?P<params>[\w,]*)\))?\Z', re.I)
+nameargspattern = re.compile(
+    r'\s*(?P<name>\b[\w$]+\b)\s*(@\(@\s*(?P<args>[\w\s,]*)\s*@\)@|)\s*((result(\s*@\(@\s*(?P<result>\b[\w$]+\b)\s*@\)@|))|(bind\s*@\(@\s*(?P<bind>.*)\s*@\)@))*\s*\Z', re.I)
+operatorpattern = re.compile(
+    r'\s*(?P<scheme>(operator|assignment))'
+    r'@\(@\s*(?P<name>[^)]+)\s*@\)@\s*\Z', re.I)
+callnameargspattern = re.compile(
+    r'\s*(?P<name>\b[\w$]+\b)\s*@\(@\s*(?P<args>.*)\s*@\)@\s*\Z', re.I)
+real16pattern = re.compile(
+    r'([-+]?(?:\d+(?:\.\d*)?|\d*\.\d+))[dD]((?:[-+]?\d+)?)')
+real8pattern = re.compile(
+    r'([-+]?((?:\d+(?:\.\d*)?|\d*\.\d+))[eE]((?:[-+]?\d+)?)|(\d+\.\d*))')
+
+_intentcallbackpattern = re.compile(r'intent\s*\(.*?\bcallback\b', re.I)
+
+
 def _is_intent_callback(vdecl):
-    for a in vdecl.get('attrspec',[]):
+    for a in vdecl.get('attrspec', []):
         if _intentcallbackpattern.match(a):
             return 1
     return 0
 
+
+def _resolvetypedefpattern(line):
+    line = ''.join(line.split())  # removes whitespace
+    m1 = typedefpattern.match(line)
+    print(line, m1)
+    if m1:
+        attrs = m1.group('attributes')
+        attrs = [a.lower() for a in attrs.split(',')] if attrs else []
+        return m1.group('name'), attrs, m1.group('params')
+    return None, [], None
+
+
 def _resolvenameargspattern(line):
     line = markouterparen(line)
-    m1=nameargspattern.match(line)
-    if m1: return m1.group('name'),m1.group('args'),m1.group('result')
-    m1=callnameargspattern.match(line)
-    if m1: return m1.group('name'),m1.group('args'),None
-    return None,[],None
-
-def analyzeline(m,case,line):
-    global groupcounter,groupname,groupcache,grouplist,filepositiontext,\
-           currentfilename,f77modulename,neededinterface,neededmodule,expectbegin,\
-           gotnextfile,previous_context
-    block=m.group('this')
+    m1 = nameargspattern.match(line)
+    if m1:
+        return m1.group('name'), m1.group('args'), m1.group('result'), m1.group('bind')
+    m1 = operatorpattern.match(line)
+    if m1:
+        name = m1.group('scheme') + '(' + m1.group('name') + ')'
+        return name, [], None, None
+    m1 = callnameargspattern.match(line)
+    if m1:
+        return m1.group('name'), m1.group('args'), None, None
+    return None, [], None, None
+
+
+def analyzeline(m, case, line):
+    global groupcounter, groupname, groupcache, grouplist, filepositiontext
+    global currentfilename, f77modulename, neededinterface, neededmodule
+    global expectbegin, gotnextfile, previous_context
+
+    block = m.group('this')
     if case != 'multiline':
         previous_context = None
-    if expectbegin and case not in ['begin','call','callfun','type'] \
-       and not skipemptyends and groupcounter<1:
-        newname=string.split(os.path.basename(currentfilename),'.')[0]
-        outmess('analyzeline: no group yet. Creating program group with name "%s".\n'%newname)
-        gotnextfile=0
-        groupcounter=groupcounter+1
-        groupname[groupcounter]='program'
-        groupcache[groupcounter]={}
-        grouplist[groupcounter]=[]
-        groupcache[groupcounter]['body']=[]
-        groupcache[groupcounter]['vars']={}
-        groupcache[groupcounter]['block']='program'
-        groupcache[groupcounter]['name']=newname
-        groupcache[groupcounter]['from']='fromsky'
-        expectbegin=0
-    if case in ['begin','call','callfun']:
+    if expectbegin and case not in ['begin', 'call', 'callfun', 'type'] \
+       and not skipemptyends and groupcounter < 1:
+        newname = os.path.basename(currentfilename).split('.')[0]
+        outmess(
+            'analyzeline: no group yet. Creating program group with name "%s".\n' % newname)
+        gotnextfile = 0
+        groupcounter = groupcounter + 1
+        groupname[groupcounter] = 'program'
+        groupcache[groupcounter] = {}
+        grouplist[groupcounter] = []
+        groupcache[groupcounter]['body'] = []
+        groupcache[groupcounter]['vars'] = {}
+        groupcache[groupcounter]['block'] = 'program'
+        groupcache[groupcounter]['name'] = newname
+        groupcache[groupcounter]['from'] = 'fromsky'
+        expectbegin = 0
+    if case in ['begin', 'call', 'callfun']:
         # Crack line => block,name,args,result
         block = block.lower()
-        if re.match(r'block\s*data',block,re.I): block='block data'
-        if re.match(r'python\s*module',block,re.I): block='python module'
-        name,args,result = _resolvenameargspattern(m.group('after'))
+        if re.match(r'block\s*data', block, re.I):
+            block = 'block data'
+        elif re.match(r'python\s*module', block, re.I):
+            block = 'python module'
+        elif re.match(r'abstract\s*interface', block, re.I):
+            block = 'abstract interface'
+        if block == 'type':
+            name, attrs, _ = _resolvetypedefpattern(m.group('after'))
+            groupcache[groupcounter]['vars'][name] = dict(attrspec = attrs)
+            args = []
+            result = None
+        else:
+            name, args, result, _ = _resolvenameargspattern(m.group('after'))
         if name is None:
-            if block=='block data':
+            if block == 'block data':
                 name = '_BLOCK_DATA_'
             else:
                 name = ''
-            if block not in ['interface','block data']:
+            if block not in ['interface', 'block data', 'abstract interface']:
                 outmess('analyzeline: No name/args pattern found for line.\n')
 
-        previous_context = (block,name,groupcounter)
-        if args: args=rmbadname(map(string.strip,string.split(markoutercomma(args),'@,@')))
-        else: args=[]
+        previous_context = (block, name, groupcounter)
+        if args:
+            args = rmbadname([x.strip()
+                              for x in markoutercomma(args).split('@,@')])
+        else:
+            args = []
         if '' in args:
             while '' in args:
                 args.remove('')
-            outmess('analyzeline: argument list is malformed (missing argument).\n')
+            outmess(
+                'analyzeline: argument list is malformed (missing argument).\n')
 
         # end of crack line => block,name,args,result
-        needmodule=0
-        needinterface=0
-
-        if case in ['call','callfun']:
-            needinterface=1
-            if not groupcache[groupcounter].has_key('args'): return
+        needmodule = 0
+        needinterface = 0
+
+        if case in ['call', 'callfun']:
+            needinterface = 1
+            if 'args' not in groupcache[groupcounter]:
+                return
             if name not in groupcache[groupcounter]['args']:
                 return
             for it in grouplist[groupcounter]:
-                if it['name']==name: return
-            if name in groupcache[groupcounter]['interfaced']: return
-            block={'call':'subroutine','callfun':'function'}[case]
-        if f77modulename and neededmodule==-1 and groupcounter<=1:
-            neededmodule=groupcounter+2
-            needmodule=1
-            needinterface=1
+                if it['name'] == name:
+                    return
+            if name in groupcache[groupcounter]['interfaced']:
+                return
+            block = {'call': 'subroutine', 'callfun': 'function'}[case]
+        if f77modulename and neededmodule == -1 and groupcounter <= 1:
+            neededmodule = groupcounter + 2
+            needmodule = 1
+            if block not in ['interface', 'abstract interface']:
+                needinterface = 1
         # Create new block(s)
-        groupcounter=groupcounter+1
-        groupcache[groupcounter]={}
-        grouplist[groupcounter]=[]
+        groupcounter = groupcounter + 1
+        groupcache[groupcounter] = {}
+        grouplist[groupcounter] = []
         if needmodule:
-            if verbose>1:
-                outmess('analyzeline: Creating module block %s\n'%`f77modulename`,0)
-            groupname[groupcounter]='module'
-            groupcache[groupcounter]['block']='python module'
-            groupcache[groupcounter]['name']=f77modulename
-            groupcache[groupcounter]['from']=''
-            groupcache[groupcounter]['body']=[]
-            groupcache[groupcounter]['externals']=[]
-            groupcache[groupcounter]['interfaced']=[]
-            groupcache[groupcounter]['vars']={}
-            groupcounter=groupcounter+1
-            groupcache[groupcounter]={}
-            grouplist[groupcounter]=[]
+            if verbose > 1:
+                outmess('analyzeline: Creating module block %s\n' %
+                        repr(f77modulename), 0)
+            groupname[groupcounter] = 'module'
+            groupcache[groupcounter]['block'] = 'python module'
+            groupcache[groupcounter]['name'] = f77modulename
+            groupcache[groupcounter]['from'] = ''
+            groupcache[groupcounter]['body'] = []
+            groupcache[groupcounter]['externals'] = []
+            groupcache[groupcounter]['interfaced'] = []
+            groupcache[groupcounter]['vars'] = {}
+            groupcounter = groupcounter + 1
+            groupcache[groupcounter] = {}
+            grouplist[groupcounter] = []
         if needinterface:
-            if verbose>1:
-                outmess('analyzeline: Creating additional interface block.\n',0)
-            groupname[groupcounter]='interface'
-            groupcache[groupcounter]['block']='interface'
-            groupcache[groupcounter]['name']='unknown_interface'
-            groupcache[groupcounter]['from']='%s:%s'%(groupcache[groupcounter-1]['from'],groupcache[groupcounter-1]['name'])
-            groupcache[groupcounter]['body']=[]
-            groupcache[groupcounter]['externals']=[]
-            groupcache[groupcounter]['interfaced']=[]
-            groupcache[groupcounter]['vars']={}
-            groupcounter=groupcounter+1
-            groupcache[groupcounter]={}
-            grouplist[groupcounter]=[]
-        groupname[groupcounter]=block
-        groupcache[groupcounter]['block']=block
-        if not name: name='unknown_'+block
-        groupcache[groupcounter]['prefix']=m.group('before')
-        groupcache[groupcounter]['name']=rmbadname1(name)
-        groupcache[groupcounter]['result']=result
-        if groupcounter==1:
-            groupcache[groupcounter]['from']=currentfilename
+            if verbose > 1:
+                outmess('analyzeline: Creating additional interface block (groupcounter=%s).\n' % (
+                    groupcounter), 0)
+            groupname[groupcounter] = 'interface'
+            groupcache[groupcounter]['block'] = 'interface'
+            groupcache[groupcounter]['name'] = 'unknown_interface'
+            groupcache[groupcounter]['from'] = '%s:%s' % (
+                groupcache[groupcounter - 1]['from'], groupcache[groupcounter - 1]['name'])
+            groupcache[groupcounter]['body'] = []
+            groupcache[groupcounter]['externals'] = []
+            groupcache[groupcounter]['interfaced'] = []
+            groupcache[groupcounter]['vars'] = {}
+            groupcounter = groupcounter + 1
+            groupcache[groupcounter] = {}
+            grouplist[groupcounter] = []
+        groupname[groupcounter] = block
+        groupcache[groupcounter]['block'] = block
+        if not name:
+            name = 'unknown_' + block.replace(' ', '_')
+        groupcache[groupcounter]['prefix'] = m.group('before')
+        groupcache[groupcounter]['name'] = rmbadname1(name)
+        groupcache[groupcounter]['result'] = result
+        if groupcounter == 1:
+            groupcache[groupcounter]['from'] = currentfilename
         else:
-            if f77modulename and groupcounter==3:
-                groupcache[groupcounter]['from']='%s:%s'%(groupcache[groupcounter-1]['from'],currentfilename)
+            if f77modulename and groupcounter == 3:
+                groupcache[groupcounter]['from'] = '%s:%s' % (
+                    groupcache[groupcounter - 1]['from'], currentfilename)
             else:
-                groupcache[groupcounter]['from']='%s:%s'%(groupcache[groupcounter-1]['from'],groupcache[groupcounter-1]['name'])
-        for k in groupcache[groupcounter].keys():
-            if not groupcache[groupcounter][k]: del groupcache[groupcounter][k]
-        groupcache[groupcounter]['args']=args
-        groupcache[groupcounter]['body']=[]
-        groupcache[groupcounter]['externals']=[]
-        groupcache[groupcounter]['interfaced']=[]
-        groupcache[groupcounter]['vars']={}
-        groupcache[groupcounter]['entry']={}
+                groupcache[groupcounter]['from'] = '%s:%s' % (
+                    groupcache[groupcounter - 1]['from'], groupcache[groupcounter - 1]['name'])
+        for k in list(groupcache[groupcounter].keys()):
+            if not groupcache[groupcounter][k]:
+                del groupcache[groupcounter][k]
+
+        groupcache[groupcounter]['args'] = args
+        groupcache[groupcounter]['body'] = []
+        groupcache[groupcounter]['externals'] = []
+        groupcache[groupcounter]['interfaced'] = []
+        groupcache[groupcounter]['vars'] = {}
+        groupcache[groupcounter]['entry'] = {}
         # end of creation
-        if block=='type':
+        if block == 'type':
             groupcache[groupcounter]['varnames'] = []
 
-        if case in ['call','callfun']: # set parents variables
-            if name not in groupcache[groupcounter-2]['externals']:
-                groupcache[groupcounter-2]['externals'].append(name)
-            groupcache[groupcounter]['vars']=copy.deepcopy(groupcache[groupcounter-2]['vars'])
-            #try: del groupcache[groupcounter]['vars'][groupcache[groupcounter-2]['name']]
-            #except: pass
-            try: del groupcache[groupcounter]['vars'][name][groupcache[groupcounter]['vars'][name]['attrspec'].index('external')]
-            except: pass
-        if block in ['function','subroutine']: # set global attributes
-            try: groupcache[groupcounter]['vars'][name]=appenddecl(groupcache[groupcounter]['vars'][name],groupcache[groupcounter-2]['vars'][''])
-            except: pass
-            if case=='callfun': # return type
-                if result and groupcache[groupcounter]['vars'].has_key(result):
-                    if not name==result:
-                        groupcache[groupcounter]['vars'][name]=appenddecl(groupcache[groupcounter]['vars'][name],groupcache[groupcounter]['vars'][result])
-            #if groupcounter>1: # name is interfaced
-            try: groupcache[groupcounter-2]['interfaced'].append(name)
-            except: pass
-        if block=='function':
-            t=typespattern[0].match(m.group('before')+' '+name)
+        if case in ['call', 'callfun']:  # set parents variables
+            if name not in groupcache[groupcounter - 2]['externals']:
+                groupcache[groupcounter - 2]['externals'].append(name)
+            groupcache[groupcounter]['vars'] = copy.deepcopy(
+                groupcache[groupcounter - 2]['vars'])
+            try:
+                del groupcache[groupcounter]['vars'][name][
+                    groupcache[groupcounter]['vars'][name]['attrspec'].index('external')]
+            except Exception:
+                pass
+        if block in ['function', 'subroutine']:  # set global attributes
+            try:
+                groupcache[groupcounter]['vars'][name] = appenddecl(
+                    groupcache[groupcounter]['vars'][name], groupcache[groupcounter - 2]['vars'][''])
+            except Exception:
+                pass
+            if case == 'callfun':  # return type
+                if result and result in groupcache[groupcounter]['vars']:
+                    if not name == result:
+                        groupcache[groupcounter]['vars'][name] = appenddecl(
+                            groupcache[groupcounter]['vars'][name], groupcache[groupcounter]['vars'][result])
+            # if groupcounter>1: # name is interfaced
+            try:
+                groupcache[groupcounter - 2]['interfaced'].append(name)
+            except Exception:
+                pass
+        if block == 'function':
+            t = typespattern[0].match(m.group('before') + ' ' + name)
             if t:
-                typespec,selector,attr,edecl=cracktypespec0(t.group('this'),t.group('after'))
-                updatevars(typespec,selector,attr,edecl)
-        if case in ['call','callfun']:
-            grouplist[groupcounter-1].append(groupcache[groupcounter])
-            grouplist[groupcounter-1][-1]['body']=grouplist[groupcounter]
+                typespec, selector, attr, edecl = cracktypespec0(
+                    t.group('this'), t.group('after'))
+                updatevars(typespec, selector, attr, edecl)
+
+        if case in ['call', 'callfun']:
+            grouplist[groupcounter - 1].append(groupcache[groupcounter])
+            grouplist[groupcounter - 1][-1]['body'] = grouplist[groupcounter]
             del grouplist[groupcounter]
-            groupcounter=groupcounter-1 # end routine
-            grouplist[groupcounter-1].append(groupcache[groupcounter])
-            grouplist[groupcounter-1][-1]['body']=grouplist[groupcounter]
+            groupcounter = groupcounter - 1  # end routine
+            grouplist[groupcounter - 1].append(groupcache[groupcounter])
+            grouplist[groupcounter - 1][-1]['body'] = grouplist[groupcounter]
             del grouplist[groupcounter]
-            groupcounter=groupcounter-1 # end interface
-    elif case=='entry':
-        name,args,result=_resolvenameargspattern(m.group('after'))
+            groupcounter = groupcounter - 1  # end interface
+
+    elif case == 'entry':
+        name, args, result, bind = _resolvenameargspattern(m.group('after'))
         if name is not None:
             if args:
-                args=rmbadname(map(string.strip,string.split(markoutercomma(args),'@,@')))
-            else: args=[]
-            assert result is None,`result`
+                args = rmbadname([x.strip()
+                                  for x in markoutercomma(args).split('@,@')])
+            else:
+                args = []
+            assert result is None, repr(result)
             groupcache[groupcounter]['entry'][name] = args
-            previous_context = ('entry',name,groupcounter)
-    elif case=='type':
-        typespec,selector,attr,edecl=cracktypespec0(block,m.group('after'))
-        last_name = updatevars(typespec,selector,attr,edecl)
+            previous_context = ('entry', name, groupcounter)
+    elif case == 'type':
+        typespec, selector, attr, edecl = cracktypespec0(
+            block, m.group('after'))
+        last_name = updatevars(typespec, selector, attr, edecl)
         if last_name is not None:
-            previous_context = ('variable',last_name,groupcounter)
-    elif case in ['dimension','intent','optional','required','external','public','private','intrisic']:
-        edecl=groupcache[groupcounter]['vars']
-        ll=m.group('after').strip()
-        i=string.find(ll,'::')
-        if i<0 and case=='intent':
-            i=string.find(markouterparen(ll),'@)@')-2
-            ll=ll[:i+1]+'::'+ll[i+1:]
-            i=string.find(ll,'::')
-            if ll[i:]=='::' and groupcache[groupcounter].has_key('args'):
-                outmess('All arguments will have attribute %s%s\n'%(m.group('this'),ll[:i]))
-                ll = ll + string.join(groupcache[groupcounter]['args'],',')
-        if i<0:i=0;pl=''
-        else: pl=string.strip(ll[:i]);ll=ll[i+2:]
-        ch = string.split(markoutercomma(pl),'@,@')
-        if len(ch)>1:
+            previous_context = ('variable', last_name, groupcounter)
+    elif case in ['dimension', 'intent', 'optional', 'required', 'external', 'public', 'private', 'intrinsic']:
+        edecl = groupcache[groupcounter]['vars']
+        ll = m.group('after').strip()
+        i = ll.find('::')
+        if i < 0 and case == 'intent':
+            i = markouterparen(ll).find('@)@') - 2
+            ll = ll[:i + 1] + '::' + ll[i + 1:]
+            i = ll.find('::')
+            if ll[i:] == '::' and 'args' in groupcache[groupcounter]:
+                outmess('All arguments will have attribute %s%s\n' %
+                        (m.group('this'), ll[:i]))
+                ll = ll + ','.join(groupcache[groupcounter]['args'])
+        if i < 0:
+            i = 0
+            pl = ''
+        else:
+            pl = ll[:i].strip()
+            ll = ll[i + 2:]
+        ch = markoutercomma(pl).split('@,@')
+        if len(ch) > 1:
             pl = ch[0]
-            outmess('analyzeline: cannot handle multiple attributes without type specification. Ignoring %r.\n' % (','.join(ch[1:])))
+            outmess('analyzeline: cannot handle multiple attributes without type specification. Ignoring %r.\n' % (
+                ','.join(ch[1:])))
         last_name = None
-        for e in map(string.strip,string.split(markoutercomma(ll),'@,@')):
-            m1=namepattern.match(e)
+
+        for e in [x.strip() for x in markoutercomma(ll).split('@,@')]:
+            m1 = namepattern.match(e)
             if not m1:
-                if case in ['public','private']: k=''
+                if case in ['public', 'private']:
+                    k = ''
                 else:
-                    print m.groupdict()
-                    outmess('analyzeline: no name pattern found in %s statement for %s. Skipping.\n'%(case,`e`))
+                    print(m.groupdict())
+                    outmess('analyzeline: no name pattern found in %s statement for %s. Skipping.\n' % (
+                        case, repr(e)))
                     continue
             else:
-                k=rmbadname1(m1.group('name'))
-            if not edecl.has_key(k): edecl[k]={}
-            if case=='dimension': ap=case+m1.group('after')
-            if case=='intent':
-                ap=m.group('this')+pl
+                k = rmbadname1(m1.group('name'))
+            if case in ['public', 'private'] and \
+               (k == 'operator' or k == 'assignment'):
+                k += m1.group('after')
+            if k not in edecl:
+                edecl[k] = {}
+            if case == 'dimension':
+                ap = case + m1.group('after')
+            if case == 'intent':
+                ap = m.group('this') + pl
                 if _intentcallbackpattern.match(ap):
                     if k not in groupcache[groupcounter]['args']:
-                        if groupcounter>1 and \
-                               string.find(groupcache[groupcounter-2]['name'],
-                                           '__user__')==-1:
-                            outmess('analyzeline: appending intent(callback) %s'\
-                                    ' to %s arguments\n' % (k,groupcache[groupcounter]['name']))
-                            groupcache[groupcounter]['args'].append(k)
+                        if groupcounter > 1:
+                            if '__user__' not in groupcache[groupcounter - 2]['name']:
+                                outmess(
+                                    'analyzeline: missing __user__ module (could be nothing)\n')
+                            # fixes ticket 1693
+                            if k != groupcache[groupcounter]['name']:
+                                outmess('analyzeline: appending intent(callback) %s'
+                                        ' to %s arguments\n' % (k, groupcache[groupcounter]['name']))
+                                groupcache[groupcounter]['args'].append(k)
+                        else:
+                            errmess(
+                                'analyzeline: intent(callback) %s is ignored\n' % (k))
                     else:
-                        errmess('analyzeline: intent(callback) %s is already'\
-                                ' in argument list' % (k))
-            if case in ['optional','required','public','external','private','intrisic']: ap=case
-            if edecl[k].has_key('attrspec'): edecl[k]['attrspec'].append(ap)
-            else: edecl[k]['attrspec']=[ap]
-            if case=='external':
-                if groupcache[groupcounter]['block']=='program':
+                        errmess('analyzeline: intent(callback) %s is already'
+                                ' in argument list\n' % (k))
+            if case in ['optional', 'required', 'public', 'external', 'private', 'intrinsic']:
+                ap = case
+            if 'attrspec' in edecl[k]:
+                edecl[k]['attrspec'].append(ap)
+            else:
+                edecl[k]['attrspec'] = [ap]
+            if case == 'external':
+                if groupcache[groupcounter]['block'] == 'program':
                     outmess('analyzeline: ignoring program arguments\n')
                     continue
                 if k not in groupcache[groupcounter]['args']:
-                    #outmess('analyzeline: ignoring external %s (not in arguments list)\n'%(`k`))
                     continue
-                if not groupcache[groupcounter].has_key('externals'):
-                    groupcache[groupcounter]['externals']=[]
+                if 'externals' not in groupcache[groupcounter]:
+                    groupcache[groupcounter]['externals'] = []
                 groupcache[groupcounter]['externals'].append(k)
             last_name = k
-        groupcache[groupcounter]['vars']=edecl
+        groupcache[groupcounter]['vars'] = edecl
         if last_name is not None:
-            previous_context = ('variable',last_name,groupcounter)
-    elif case=='parameter':
-        edecl=groupcache[groupcounter]['vars']
-        ll=string.strip(m.group('after'))[1:-1]
+            previous_context = ('variable', last_name, groupcounter)
+    elif case == 'moduleprocedure':
+        groupcache[groupcounter]['implementedby'] = \
+            [x.strip() for x in m.group('after').split(',')]
+    elif case == 'parameter':
+        edecl = groupcache[groupcounter]['vars']
+        ll = m.group('after').strip()[1:-1]
         last_name = None
-        for e in string.split(markoutercomma(ll),'@,@'):
+        for e in markoutercomma(ll).split('@,@'):
             try:
-                k,initexpr=map(string.strip,string.split(e,'='))
-            except:
-                outmess('analyzeline: could not extract name,expr in parameter statement "%s" of "%s"\n'%(e,ll));continue
+                k, initexpr = [x.strip() for x in e.split('=')]
+            except Exception:
+                outmess(
+                    'analyzeline: could not extract name,expr in parameter statement "%s" of "%s"\n' % (e, ll))
+                continue
             params = get_parameters(edecl)
-            k=rmbadname1(k)
-            if not edecl.has_key(k): edecl[k]={}
-            if edecl[k].has_key('=') and (not edecl[k]['=']==initexpr):
-                outmess('analyzeline: Overwriting the value of parameter "%s" ("%s") with "%s".\n'%(k,edecl[k]['='],initexpr))
-            t = determineexprtype(initexpr,params)
+            k = rmbadname1(k)
+            if k not in edecl:
+                edecl[k] = {}
+            if '=' in edecl[k] and (not edecl[k]['='] == initexpr):
+                outmess('analyzeline: Overwriting the value of parameter "%s" ("%s") with "%s".\n' % (
+                    k, edecl[k]['='], initexpr))
+            t = determineexprtype(initexpr, params)
             if t:
-                if t.get('typespec')=='real':
+                if t.get('typespec') == 'real':
                     tt = list(initexpr)
                     for m in real16pattern.finditer(initexpr):
-                        tt[m.start():m.end()] = list(\
+                        tt[m.start():m.end()] = list(
                             initexpr[m.start():m.end()].lower().replace('d', 'e'))
-                    initexpr = "".join(tt)
-                elif t.get('typespec')=='complex':
-                    initexpr = initexpr[1:].lower().replace('d','e').\
-                               replace(',','+1j*(')
+                    initexpr = ''.join(tt)
+                elif t.get('typespec') == 'complex':
+                    initexpr = initexpr[1:].lower().replace('d', 'e').\
+                        replace(',', '+1j*(')
             try:
-                v = eval(initexpr,{},params)
-            except (SyntaxError,NameError),msg:
-                errmess('analyzeline: Failed to evaluate %r. Ignoring: %s\n'\
+                v = eval(initexpr, {}, params)
+            except (SyntaxError, NameError, TypeError) as msg:
+                errmess('analyzeline: Failed to evaluate %r. Ignoring: %s\n'
                         % (initexpr, msg))
                 continue
             edecl[k]['='] = repr(v)
-            if edecl[k].has_key('attrspec'):
+            if 'attrspec' in edecl[k]:
                 edecl[k]['attrspec'].append('parameter')
-            else: edecl[k]['attrspec']=['parameter']
+            else:
+                edecl[k]['attrspec'] = ['parameter']
             last_name = k
-        groupcache[groupcounter]['vars']=edecl
+        groupcache[groupcounter]['vars'] = edecl
         if last_name is not None:
-            previous_context = ('variable',last_name,groupcounter)
-    elif case=='implicit':
-        if string.lower(string.strip(m.group('after')))=='none':
-            groupcache[groupcounter]['implicit']=None
+            previous_context = ('variable', last_name, groupcounter)
+    elif case == 'implicit':
+        if m.group('after').strip().lower() == 'none':
+            groupcache[groupcounter]['implicit'] = None
         elif m.group('after'):
-            if groupcache[groupcounter].has_key('implicit'):
-                impl=groupcache[groupcounter]['implicit']
-            else: impl={}
+            if 'implicit' in groupcache[groupcounter]:
+                impl = groupcache[groupcounter]['implicit']
+            else:
+                impl = {}
             if impl is None:
-                outmess('analyzeline: Overwriting earlier "implicit none" statement.\n')
-                impl={}
-            for e in string.split(markoutercomma(m.group('after')),'@,@'):
-                decl={}
-                m1=re.match(r'\s*(?P<this>.*?)\s*(\(\s*(?P<after>[a-z-, ]+)\s*\)\s*|)\Z',e,re.I)
+                outmess(
+                    'analyzeline: Overwriting earlier "implicit none" statement.\n')
+                impl = {}
+            for e in markoutercomma(m.group('after')).split('@,@'):
+                decl = {}
+                m1 = re.match(
+                    r'\s*(?P<this>.*?)\s*(\(\s*(?P<after>[a-z-, ]+)\s*\)\s*|)\Z', e, re.I)
                 if not m1:
-                    outmess('analyzeline: could not extract info of implicit statement part "%s"\n'%(e));continue
-                m2=typespattern4implicit.match(m1.group('this'))
+                    outmess(
+                        'analyzeline: could not extract info of implicit statement part "%s"\n' % (e))
+                    continue
+                m2 = typespattern4implicit.match(m1.group('this'))
                 if not m2:
-                    outmess('analyzeline: could not extract types pattern of implicit statement part "%s"\n'%(e));continue
-                typespec,selector,attr,edecl=cracktypespec0(m2.group('this'),m2.group('after'))
-                kindselect,charselect,typename=cracktypespec(typespec,selector)
-                decl['typespec']=typespec
-                decl['kindselector']=kindselect
-                decl['charselector']=charselect
-                decl['typename']=typename
-                for k in decl.keys():
-                    if not decl[k]: del decl[k]
-                for r in string.split(markoutercomma(m1.group('after')),'@,@'):
+                    outmess(
+                        'analyzeline: could not extract types pattern of implicit statement part "%s"\n' % (e))
+                    continue
+                typespec, selector, attr, edecl = cracktypespec0(
+                    m2.group('this'), m2.group('after'))
+                kindselect, charselect, typename = cracktypespec(
+                    typespec, selector)
+                decl['typespec'] = typespec
+                decl['kindselector'] = kindselect
+                decl['charselector'] = charselect
+                decl['typename'] = typename
+                for k in list(decl.keys()):
+                    if not decl[k]:
+                        del decl[k]
+                for r in markoutercomma(m1.group('after')).split('@,@'):
                     if '-' in r:
-                        try: begc,endc=map(string.strip,string.split(r,'-'))
-                        except:
-                            outmess('analyzeline: expected "<char>-<char>" instead of "%s" in range list of implicit statement\n'%r);continue
-                    else: begc=endc=string.strip(r)
-                    if not len(begc)==len(endc)==1:
-                        outmess('analyzeline: expected "<char>-<char>" instead of "%s" in range list of implicit statement (2)\n'%r);continue
-                    for o in range(ord(begc),ord(endc)+1):
-                        impl[chr(o)]=decl
-            groupcache[groupcounter]['implicit']=impl
-    elif case=='data':
-        ll=[]
-        dl='';il='';f=0;fc=1
+                        try:
+                            begc, endc = [x.strip() for x in r.split('-')]
+                        except Exception:
+                            outmess(
+                                'analyzeline: expected "<char>-<char>" instead of "%s" in range list of implicit statement\n' % r)
+                            continue
+                    else:
+                        begc = endc = r.strip()
+                    if not len(begc) == len(endc) == 1:
+                        outmess(
+                            'analyzeline: expected "<char>-<char>" instead of "%s" in range list of implicit statement (2)\n' % r)
+                        continue
+                    for o in range(ord(begc), ord(endc) + 1):
+                        impl[chr(o)] = decl
+            groupcache[groupcounter]['implicit'] = impl
+    elif case == 'data':
+        ll = []
+        dl = ''
+        il = ''
+        f = 0
+        fc = 1
+        inp = 0
         for c in m.group('after'):
-            if c=="'": fc=not fc
-            if c=='/' and fc: f=f+1;continue
-            if f==0: dl=dl+c
-            elif f==1: il=il+c
-            elif f==2:
+            if not inp:
+                if c == "'":
+                    fc = not fc
+                if c == '/' and fc:
+                    f = f + 1
+                    continue
+            if c == '(':
+                inp = inp + 1
+            elif c == ')':
+                inp = inp - 1
+            if f == 0:
+                dl = dl + c
+            elif f == 1:
+                il = il + c
+            elif f == 2:
                 dl = dl.strip()
                 if dl.startswith(','):
                     dl = dl[1:].strip()
-                ll.append([dl,il])
-                dl=c;il='';f=0
-        if f==2:
+                ll.append([dl, il])
+                dl = c
+                il = ''
+                f = 0
+        if f == 2:
             dl = dl.strip()
             if dl.startswith(','):
                 dl = dl[1:].strip()
-            ll.append([dl,il])
-        vars={}
-        if groupcache[groupcounter].has_key('vars'):
-            vars=groupcache[groupcounter]['vars']
+            ll.append([dl, il])
+        vars = {}
+        if 'vars' in groupcache[groupcounter]:
+            vars = groupcache[groupcounter]['vars']
         last_name = None
         for l in ll:
-            l=map(string.strip,l)
-            if l[0][0]==',':l[0]=l[0][1:]
-            if l[0][0]=='(':
-                outmess('analyzeline: implied-DO list "%s" is not supported. Skipping.\n'%l[0])
+            l = [x.strip() for x in l]
+            if l[0][0] == ',':
+                l[0] = l[0][1:]
+            if l[0][0] == '(':
+                outmess(
+                    'analyzeline: implied-DO list "%s" is not supported. Skipping.\n' % l[0])
                 continue
-            #if '(' in l[0]:
-            #    #outmess('analyzeline: ignoring this data statement.\n')
-            #    continue
-            i=0;j=0;llen=len(l[1])
-            for v in rmbadname(map(string.strip,string.split(markoutercomma(l[0]),'@,@'))):
-                if v[0]=='(':
-                    outmess('analyzeline: implied-DO list "%s" is not supported. Skipping.\n'%v)
+            i = 0
+            j = 0
+            llen = len(l[1])
+            for v in rmbadname([x.strip() for x in markoutercomma(l[0]).split('@,@')]):
+                if v[0] == '(':
+                    outmess(
+                        'analyzeline: implied-DO list "%s" is not supported. Skipping.\n' % v)
                     # XXX: subsequent init expressions may get wrong values.
-                    #      Ignoring since data statements are irrelevant for wrapping.
+                    # Ignoring since data statements are irrelevant for
+                    # wrapping.
                     continue
-                fc=0
-                while (i<llen) and (fc or not l[1][i]==','):
-                    if l[1][i]=="'": fc=not fc
-                    i=i+1
-                i=i+1
-                #v,l[1][j:i-1]=name,initvalue
-                if not vars.has_key(v):
-                    vars[v]={}
-                if vars[v].has_key('=') and not vars[v]['=']==l[1][j:i-1]:
-                    outmess('analyzeline: changing init expression of "%s" ("%s") to "%s"\n'%(v,vars[v]['='],l[1][j:i-1]))
-                vars[v]['=']=l[1][j:i-1]
-                j=i
+                fc = 0
+                while (i < llen) and (fc or not l[1][i] == ','):
+                    if l[1][i] == "'":
+                        fc = not fc
+                    i = i + 1
+                i = i + 1
+                if v not in vars:
+                    vars[v] = {}
+                if '=' in vars[v] and not vars[v]['='] == l[1][j:i - 1]:
+                    outmess('analyzeline: changing init expression of "%s" ("%s") to "%s"\n' % (
+                        v, vars[v]['='], l[1][j:i - 1]))
+                vars[v]['='] = l[1][j:i - 1]
+                j = i
                 last_name = v
-        groupcache[groupcounter]['vars']=vars
+        groupcache[groupcounter]['vars'] = vars
         if last_name is not None:
-            previous_context = ('variable',last_name,groupcounter)
-    elif case=='common':
-        line=string.strip(m.group('after'))
-        if not line[0]=='/':line='//'+line
-        cl=[]
-        f=0;bn='';ol=''
+            previous_context = ('variable', last_name, groupcounter)
+    elif case == 'common':
+        line = m.group('after').strip()
+        if not line[0] == '/':
+            line = '//' + line
+        cl = []
+        f = 0
+        bn = ''
+        ol = ''
         for c in line:
-            if c=='/':f=f+1;continue
-            if f>=3:
-                bn = string.strip(bn)
-                if not bn: bn='_BLNK_'
-                cl.append([bn,ol])
-                f=f-2;bn='';ol=''
-            if f%2: bn=bn+c
-            else: ol=ol+c
-        bn = string.strip(bn)
-        if not bn: bn='_BLNK_'
-        cl.append([bn,ol])
-        commonkey={}
-        if groupcache[groupcounter].has_key('common'):
-            commonkey=groupcache[groupcounter]['common']
+            if c == '/':
+                f = f + 1
+                continue
+            if f >= 3:
+                bn = bn.strip()
+                if not bn:
+                    bn = '_BLNK_'
+                cl.append([bn, ol])
+                f = f - 2
+                bn = ''
+                ol = ''
+            if f % 2:
+                bn = bn + c
+            else:
+                ol = ol + c
+        bn = bn.strip()
+        if not bn:
+            bn = '_BLNK_'
+        cl.append([bn, ol])
+        commonkey = {}
+        if 'common' in groupcache[groupcounter]:
+            commonkey = groupcache[groupcounter]['common']
         for c in cl:
-            if commonkey.has_key(c[0]):
-                outmess('analyzeline: previously defined common block encountered. Skipping.\n')
-                continue
-            commonkey[c[0]]=[]
-            for i in map(string.strip,string.split(markoutercomma(c[1]),'@,@')):
-                if i: commonkey[c[0]].append(i)
-        groupcache[groupcounter]['common']=commonkey
-        previous_context = ('common',bn,groupcounter)
-    elif case=='use':
-        m1=re.match(r'\A\s*(?P<name>\b[\w]+\b)\s*((,(\s*\bonly\b\s*:|(?P<notonly>))\s*(?P<list>.*))|)\s*\Z',m.group('after'),re.I)
+            if c[0] not in commonkey:
+                commonkey[c[0]] = []
+            for i in [x.strip() for x in markoutercomma(c[1]).split('@,@')]:
+                if i:
+                    commonkey[c[0]].append(i)
+        groupcache[groupcounter]['common'] = commonkey
+        previous_context = ('common', bn, groupcounter)
+    elif case == 'use':
+        m1 = re.match(
+            r'\A\s*(?P<name>\b\w+\b)\s*((,(\s*\bonly\b\s*:|(?P<notonly>))\s*(?P<list>.*))|)\s*\Z', m.group('after'), re.I)
         if m1:
-            mm=m1.groupdict()
-            if not groupcache[groupcounter].has_key('use'): groupcache[groupcounter]['use']={}
-            name=m1.group('name')
-            groupcache[groupcounter]['use'][name]={}
-            isonly=0
-            if mm.has_key('list') and mm['list'] is not None:
-                if mm.has_key('notonly') and mm['notonly'] is None:isonly=1
-                groupcache[groupcounter]['use'][name]['only']=isonly
-                ll=map(string.strip,string.split(mm['list'],','))
-                rl={}
+            mm = m1.groupdict()
+            if 'use' not in groupcache[groupcounter]:
+                groupcache[groupcounter]['use'] = {}
+            name = m1.group('name')
+            groupcache[groupcounter]['use'][name] = {}
+            isonly = 0
+            if 'list' in mm and mm['list'] is not None:
+                if 'notonly' in mm and mm['notonly'] is None:
+                    isonly = 1
+                groupcache[groupcounter]['use'][name]['only'] = isonly
+                ll = [x.strip() for x in mm['list'].split(',')]
+                rl = {}
                 for l in ll:
                     if '=' in l:
-                        m2=re.match(r'\A\s*(?P<local>\b[\w]+\b)\s*=\s*>\s*(?P<use>\b[\w]+\b)\s*\Z',l,re.I)
-                        if m2: rl[string.strip(m2.group('local'))]=string.strip(m2.group('use'))
+                        m2 = re.match(
+                            r'\A\s*(?P<local>\b\w+\b)\s*=\s*>\s*(?P<use>\b\w+\b)\s*\Z', l, re.I)
+                        if m2:
+                            rl[m2.group('local').strip()] = m2.group(
+                                'use').strip()
                         else:
-                            outmess('analyzeline: Not local=>use pattern found in %s\n'%`l`)
+                            outmess(
+                                'analyzeline: Not local=>use pattern found in %s\n' % repr(l))
                     else:
-                        rl[l]=l
-                    groupcache[groupcounter]['use'][name]['map']=rl
+                        rl[l] = l
+                    groupcache[groupcounter]['use'][name]['map'] = rl
             else:
                 pass
-
         else:
-            print m.groupdict()
+            print(m.groupdict())
             outmess('analyzeline: Could not crack the use statement.\n')
     elif case in ['f2pyenhancements']:
-        if not groupcache[groupcounter].has_key ('f2pyenhancements'):
+        if 'f2pyenhancements' not in groupcache[groupcounter]:
             groupcache[groupcounter]['f2pyenhancements'] = {}
         d = groupcache[groupcounter]['f2pyenhancements']
-        if m.group('this')=='usercode' and d.has_key('usercode'):
-            if type(d['usercode']) is type(''):
+        if m.group('this') == 'usercode' and 'usercode' in d:
+            if isinstance(d['usercode'], str):
                 d['usercode'] = [d['usercode']]
             d['usercode'].append(m.group('after'))
         else:
             d[m.group('this')] = m.group('after')
-    elif case=='multiline':
+    elif case == 'multiline':
         if previous_context is None:
             if verbose:
                 outmess('analyzeline: No context for multiline block.\n')
             return
         gc = groupcounter
-        #gc = previous_context[2]
         appendmultiline(groupcache[gc],
                         previous_context[:2],
                         m.group('this'))
     else:
-        if verbose>1:
-            print m.groupdict()
+        if verbose > 1:
+            print(m.groupdict())
             outmess('analyzeline: No code implemented for line.\n')
 
-def appendmultiline(group, context_name,ml):
-    if not group.has_key('f2pymultilines'):
+
+def appendmultiline(group, context_name, ml):
+    if 'f2pymultilines' not in group:
         group['f2pymultilines'] = {}
     d = group['f2pymultilines']
-    if not d.has_key(context_name):
+    if context_name not in d:
         d[context_name] = []
     d[context_name].append(ml)
     return
 
-def cracktypespec0(typespec,ll):
-    selector=None
-    attr=None
-    if re.match(r'double\s*complex',typespec,re.I): typespec='double complex'
-    elif re.match(r'double\s*precision',typespec,re.I): typespec='double precision'
-    else: typespec=string.lower(string.strip(typespec))
-    m1=selectpattern.match(markouterparen(ll))
+
+def cracktypespec0(typespec, ll):
+    selector = None
+    attr = None
+    if re.match(r'double\s*complex', typespec, re.I):
+        typespec = 'double complex'
+    elif re.match(r'double\s*precision', typespec, re.I):
+        typespec = 'double precision'
+    else:
+        typespec = typespec.strip().lower()
+    m1 = selectpattern.match(markouterparen(ll))
     if not m1:
-        outmess('cracktypespec0: no kind/char_selector pattern found for line.\n')
+        outmess(
+            'cracktypespec0: no kind/char_selector pattern found for line.\n')
         return
-    d=m1.groupdict()
-    for k in d.keys(): d[k]=unmarkouterparen(d[k])
-    if typespec in ['complex','integer','logical','real','character','type']:
-        selector=d['this']
-        ll=d['after']
-    i=string.find(ll,'::')
-    if i>=0:
-        attr=string.strip(ll[:i])
-        ll=ll[i+2:]
-    return typespec,selector,attr,ll
+    d = m1.groupdict()
+    for k in list(d.keys()):
+        d[k] = unmarkouterparen(d[k])
+    if typespec in ['complex', 'integer', 'logical', 'real', 'character', 'type']:
+        selector = d['this']
+        ll = d['after']
+    i = ll.find('::')
+    if i >= 0:
+        attr = ll[:i].strip()
+        ll = ll[i + 2:]
+    return typespec, selector, attr, ll
 #####
-namepattern=re.compile(r'\s*(?P<name>\b[\w]+\b)\s*(?P<after>.*)\s*\Z',re.I)
-kindselector=re.compile(r'\s*(\(\s*(kind\s*=)?\s*(?P<kind>.*)\s*\)|[*]\s*(?P<kind2>.*?))\s*\Z',re.I)
-charselector=re.compile(r'\s*(\((?P<lenkind>.*)\)|[*]\s*(?P<charlen>.*))\s*\Z',re.I)
-lenkindpattern=re.compile(r'\s*(kind\s*=\s*(?P<kind>.*?)\s*(@,@\s*len\s*=\s*(?P<len>.*)|)|(len\s*=\s*|)(?P<len2>.*?)\s*(@,@\s*(kind\s*=\s*|)(?P<kind2>.*)|))\s*\Z',re.I)
-lenarraypattern=re.compile(r'\s*(@\(@\s*(?!/)\s*(?P<array>.*?)\s*@\)@\s*[*]\s*(?P<len>.*?)|([*]\s*(?P<len2>.*?)|)\s*(@\(@\s*(?!/)\s*(?P<array2>.*?)\s*@\)@|))\s*(=\s*(?P<init>.*?)|(@\(@|)/\s*(?P<init2>.*?)\s*/(@\)@|)|)\s*\Z',re.I)
+namepattern = re.compile(r'\s*(?P<name>\b\w+\b)\s*(?P<after>.*)\s*\Z', re.I)
+kindselector = re.compile(
+    r'\s*(\(\s*(kind\s*=)?\s*(?P<kind>.*)\s*\)|\*\s*(?P<kind2>.*?))\s*\Z', re.I)
+charselector = re.compile(
+    r'\s*(\((?P<lenkind>.*)\)|\*\s*(?P<charlen>.*))\s*\Z', re.I)
+lenkindpattern = re.compile(
+    r'\s*(kind\s*=\s*(?P<kind>.*?)\s*(@,@\s*len\s*=\s*(?P<len>.*)|)|(len\s*=\s*|)(?P<len2>.*?)\s*(@,@\s*(kind\s*=\s*|)(?P<kind2>.*)|))\s*\Z', re.I)
+lenarraypattern = re.compile(
+    r'\s*(@\(@\s*(?!/)\s*(?P<array>.*?)\s*@\)@\s*\*\s*(?P<len>.*?)|(\*\s*(?P<len2>.*?)|)\s*(@\(@\s*(?!/)\s*(?P<array2>.*?)\s*@\)@|))\s*(=\s*(?P<init>.*?)|(@\(@|)/\s*(?P<init2>.*?)\s*/(@\)@|)|)\s*\Z', re.I)
+
+
 def removespaces(expr):
-    expr=string.strip(expr)
-    if len(expr)<=1: return expr
-    expr2=expr[0]
-    for i in range(1,len(expr)-1):
-        if expr[i]==' ' and \
-           ((expr[i+1] in "()[]{}= ") or (expr[i-1] in "()[]{}= ")): continue
-        expr2=expr2+expr[i]
-    expr2=expr2+expr[-1]
+    expr = expr.strip()
+    if len(expr) <= 1:
+        return expr
+    expr2 = expr[0]
+    for i in range(1, len(expr) - 1):
+        if (expr[i] == ' ' and
+            ((expr[i + 1] in "()[]{}=+-/* ") or
+                (expr[i - 1] in "()[]{}=+-/* "))):
+            continue
+        expr2 = expr2 + expr[i]
+    expr2 = expr2 + expr[-1]
     return expr2
+
+
 def markinnerspaces(line):
-    l='';f=0
-    cc='\''
-    cc1='"'
-    cb=''
+    """
+    The function replace all spaces in the input variable line which are 
+    surrounded with quotation marks, with the triplet "@_@".
+
+    For instance, for the input "a 'b c'" the function returns "a 'b@_@c'"
+
+    Parameters
+    ----------
+    line : str
+
+    Returns
+    -------
+    str
+
+    """  
+    fragment = ''
+    inside = False
+    current_quote = None
+    escaped = ''
     for c in line:
-        if cb=='\\' and c in ['\\','\'','"']:
-            l=l+c;
-            cb=c
+        if escaped == '\\' and c in ['\\', '\'', '"']:
+            fragment += c
+            escaped = c
             continue
-        if f==0 and c in ['\'','"']: cc=c; cc1={'\'':'"','"':'\''}[c]
-        if c==cc:f=f+1
-        elif c==cc:f=f-1
-        elif c==' ' and f==1: l=l+'@_@'; continue
-        l=l+c;cb=c
-    return l
-def updatevars(typespec,selector,attrspec,entitydecl):
-    global groupcache,groupcounter
+        if not inside and c in ['\'', '"']:
+            current_quote = c
+        if c == current_quote:
+            inside = not inside
+        elif c == ' ' and inside:
+            fragment += '@_@'
+            continue
+        fragment += c
+        escaped = c  # reset to non-backslash
+    return fragment
+
+
+def updatevars(typespec, selector, attrspec, entitydecl):
+    global groupcache, groupcounter
+
     last_name = None
-    kindselect,charselect,typename=cracktypespec(typespec,selector)
+    kindselect, charselect, typename = cracktypespec(typespec, selector)
     if attrspec:
-        attrspec=map(string.strip,string.split(markoutercomma(attrspec),'@,@'))
+        attrspec = [x.strip() for x in markoutercomma(attrspec).split('@,@')]
         l = []
         c = re.compile(r'(?P<start>[a-zA-Z]+)')
         for a in attrspec:
+            if not a:
+                continue
             m = c.match(a)
             if m:
-                s = string.lower(m.group('start'))
+                s = m.group('start').lower()
                 a = s + a[len(s):]
             l.append(a)
         attrspec = l
-    el=map(string.strip,string.split(markoutercomma(entitydecl),'@,@'))
-    el1=[]
+    el = [x.strip() for x in markoutercomma(entitydecl).split('@,@')]
+    el1 = []
     for e in el:
-        for e1 in map(string.strip,string.split(markoutercomma(removespaces(markinnerspaces(e)),comma=' '),'@ @')):
-            if e1: el1.append(string.replace(e1,'@_@',' '))
+        for e1 in [x.strip() for x in markoutercomma(removespaces(markinnerspaces(e)), comma=' ').split('@ @')]:
+            if e1:
+                el1.append(e1.replace('@_@', ' '))
     for e in el1:
-        m=namepattern.match(e)
+        m = namepattern.match(e)
         if not m:
-            outmess('updatevars: no name pattern found for entity=%s. Skipping.\n'%(`e`))
+            outmess(
+                'updatevars: no name pattern found for entity=%s. Skipping.\n' % (repr(e)))
             continue
-        ename=rmbadname1(m.group('name'))
-        edecl={}
-        if groupcache[groupcounter]['vars'].has_key(ename):
-            edecl=groupcache[groupcounter]['vars'][ename].copy()
-            has_typespec = edecl.has_key('typespec')
-            if not has_typespec:
-                edecl['typespec']=typespec
-            elif typespec and (not typespec==edecl['typespec']):
-                outmess('updatevars: attempt to change the type of "%s" ("%s") to "%s". Ignoring.\n' % (ename,edecl['typespec'],typespec))
-            if not edecl.has_key('kindselector'):
-                edecl['kindselector']=copy.copy(kindselect)
+        ename = rmbadname1(m.group('name'))
+        edecl = {}
+        if ename in groupcache[groupcounter]['vars']:
+            edecl = groupcache[groupcounter]['vars'][ename].copy()
+            not_has_typespec = 'typespec' not in edecl
+            if not_has_typespec:
+                edecl['typespec'] = typespec
+            elif typespec and (not typespec == edecl['typespec']):
+                outmess('updatevars: attempt to change the type of "%s" ("%s") to "%s". Ignoring.\n' % (
+                    ename, edecl['typespec'], typespec))
+            if 'kindselector' not in edecl:
+                edecl['kindselector'] = copy.copy(kindselect)
             elif kindselect:
-                for k in kindselect.keys():
-                    if edecl['kindselector'].has_key(k) and (not kindselect[k]==edecl['kindselector'][k]):
-                        outmess('updatevars: attempt to change the kindselector "%s" of "%s" ("%s") to "%s". Ignoring.\n' % (k,ename,edecl['kindselector'][k],kindselect[k]))
-                    else: edecl['kindselector'][k]=copy.copy(kindselect[k])
-            if not edecl.has_key('charselector') and charselect:
-                if not has_typespec:
-                    edecl['charselector']=charselect
+                for k in list(kindselect.keys()):
+                    if k in edecl['kindselector'] and (not kindselect[k] == edecl['kindselector'][k]):
+                        outmess('updatevars: attempt to change the kindselector "%s" of "%s" ("%s") to "%s". Ignoring.\n' % (
+                            k, ename, edecl['kindselector'][k], kindselect[k]))
+                    else:
+                        edecl['kindselector'][k] = copy.copy(kindselect[k])
+            if 'charselector' not in edecl and charselect:
+                if not_has_typespec:
+                    edecl['charselector'] = charselect
                 else:
-                    errmess('updatevars:%s: attempt to change empty charselector to %r. Ignoring.\n' \
-                            %(ename,charselect))
+                    errmess('updatevars:%s: attempt to change empty charselector to %r. Ignoring.\n'
+                            % (ename, charselect))
             elif charselect:
-                for k in charselect.keys():
-                    if edecl['charselector'].has_key(k) and (not charselect[k]==edecl['charselector'][k]):
-                        outmess('updatevars: attempt to change the charselector "%s" of "%s" ("%s") to "%s". Ignoring.\n' % (k,ename,edecl['charselector'][k],charselect[k]))
-                    else: edecl['charselector'][k]=copy.copy(charselect[k])
-            if not edecl.has_key('typename'):
-                edecl['typename']=typename
-            elif typename and (not edecl['typename']==typename):
-                outmess('updatevars: attempt to change the typename of "%s" ("%s") to "%s". Ignoring.\n' % (ename,edecl['typename'],typename))
-            if not edecl.has_key('attrspec'):
-                edecl['attrspec']=copy.copy(attrspec)
+                for k in list(charselect.keys()):
+                    if k in edecl['charselector'] and (not charselect[k] == edecl['charselector'][k]):
+                        outmess('updatevars: attempt to change the charselector "%s" of "%s" ("%s") to "%s". Ignoring.\n' % (
+                            k, ename, edecl['charselector'][k], charselect[k]))
+                    else:
+                        edecl['charselector'][k] = copy.copy(charselect[k])
+            if 'typename' not in edecl:
+                edecl['typename'] = typename
+            elif typename and (not edecl['typename'] == typename):
+                outmess('updatevars: attempt to change the typename of "%s" ("%s") to "%s". Ignoring.\n' % (
+                    ename, edecl['typename'], typename))
+            if 'attrspec' not in edecl:
+                edecl['attrspec'] = copy.copy(attrspec)
             elif attrspec:
                 for a in attrspec:
                     if a not in edecl['attrspec']:
                         edecl['attrspec'].append(a)
         else:
-            edecl['typespec']=copy.copy(typespec)
-            edecl['kindselector']=copy.copy(kindselect)
-            edecl['charselector']=copy.copy(charselect)
-            edecl['typename']=typename
-            edecl['attrspec']=copy.copy(attrspec)
+            edecl['typespec'] = copy.copy(typespec)
+            edecl['kindselector'] = copy.copy(kindselect)
+            edecl['charselector'] = copy.copy(charselect)
+            edecl['typename'] = typename
+            edecl['attrspec'] = copy.copy(attrspec)
+        if 'external' in (edecl.get('attrspec') or []) and e in groupcache[groupcounter]['args']:
+            if 'externals' not in groupcache[groupcounter]:
+                groupcache[groupcounter]['externals'] = []
+            groupcache[groupcounter]['externals'].append(e)
         if m.group('after'):
-            m1=lenarraypattern.match(markouterparen(m.group('after')))
+            m1 = lenarraypattern.match(markouterparen(m.group('after')))
             if m1:
-                d1=m1.groupdict()
-                for lk in ['len','array','init']:
-                    if d1[lk+'2'] is not None: d1[lk]=d1[lk+'2']; del d1[lk+'2']
-                for k in d1.keys():
-                    if d1[k] is not None: d1[k]=unmarkouterparen(d1[k])
-                    else: del d1[k]
-                if d1.has_key('len') and d1.has_key('array'):
-                    if d1['len']=='':
-                        d1['len']=d1['array']
+                d1 = m1.groupdict()
+                for lk in ['len', 'array', 'init']:
+                    if d1[lk + '2'] is not None:
+                        d1[lk] = d1[lk + '2']
+                        del d1[lk + '2']
+                for k in list(d1.keys()):
+                    if d1[k] is not None:
+                        d1[k] = unmarkouterparen(d1[k])
+                    else:
+                        del d1[k]
+                if 'len' in d1 and 'array' in d1:
+                    if d1['len'] == '':
+                        d1['len'] = d1['array']
                         del d1['array']
                     else:
-                        d1['array']=d1['array']+','+d1['len']
+                        d1['array'] = d1['array'] + ',' + d1['len']
                         del d1['len']
-                        errmess('updatevars: "%s %s" is mapped to "%s %s(%s)"\n'%(typespec,e,typespec,ename,d1['array']))
-                if d1.has_key('array'):
-                    dm = 'dimension(%s)'%d1['array']
-                    if not edecl.has_key('attrspec') or (not edecl['attrspec']):
-                        edecl['attrspec']=[dm]
+                        errmess('updatevars: "%s %s" is mapped to "%s %s(%s)"\n' % (
+                            typespec, e, typespec, ename, d1['array']))
+                if 'array' in d1:
+                    dm = 'dimension(%s)' % d1['array']
+                    if 'attrspec' not in edecl or (not edecl['attrspec']):
+                        edecl['attrspec'] = [dm]
                     else:
                         edecl['attrspec'].append(dm)
                         for dm1 in edecl['attrspec']:
-                            if dm1[:9]=='dimension' and dm1!=dm:
+                            if dm1[:9] == 'dimension' and dm1 != dm:
                                 del edecl['attrspec'][-1]
-                                errmess('updatevars:%s: attempt to change %r to %r. Ignoring.\n' \
-                                        % (ename,dm1,dm))
+                                errmess('updatevars:%s: attempt to change %r to %r. Ignoring.\n'
+                                        % (ename, dm1, dm))
                                 break
 
-                if d1.has_key('len'):
-                    if typespec in ['complex','integer','logical','real']:
-                        if (not edecl.has_key('kindselector')) or (not edecl['kindselector']):
-                            edecl['kindselector']={}
-                        edecl['kindselector']['*']=d1['len']
+                if 'len' in d1:
+                    if typespec in ['complex', 'integer', 'logical', 'real']:
+                        if ('kindselector' not in edecl) or (not edecl['kindselector']):
+                            edecl['kindselector'] = {}
+                        edecl['kindselector']['*'] = d1['len']
                     elif typespec == 'character':
-                        if (not edecl.has_key('charselector')) or (not edecl['charselector']): edecl['charselector']={}
-                        if edecl['charselector'].has_key('len'): del edecl['charselector']['len']
-                        edecl['charselector']['*']=d1['len']
-                if d1.has_key('init'):
-                    if edecl.has_key('=') and (not edecl['=']==d1['init']):
-                        outmess('updatevars: attempt to change the init expression of "%s" ("%s") to "%s". Ignoring.\n' % (ename,edecl['='],d1['init']))
+                        if ('charselector' not in edecl) or (not edecl['charselector']):
+                            edecl['charselector'] = {}
+                        if 'len' in edecl['charselector']:
+                            del edecl['charselector']['len']
+                        edecl['charselector']['*'] = d1['len']
+                if 'init' in d1:
+                    if '=' in edecl and (not edecl['='] == d1['init']):
+                        outmess('updatevars: attempt to change the init expression of "%s" ("%s") to "%s". Ignoring.\n' % (
+                            ename, edecl['='], d1['init']))
                     else:
-                        edecl['=']=d1['init']
+                        edecl['='] = d1['init']
             else:
-                outmess('updatevars: could not crack entity declaration "%s". Ignoring.\n'%(ename+m.group('after')))
-        for k in edecl.keys():
-            if not edecl[k]: del edecl[k]
-        groupcache[groupcounter]['vars'][ename]=edecl
-        if groupcache[groupcounter].has_key('varnames'):
+                outmess('updatevars: could not crack entity declaration "%s". Ignoring.\n' % (
+                    ename + m.group('after')))
+        for k in list(edecl.keys()):
+            if not edecl[k]:
+                del edecl[k]
+        groupcache[groupcounter]['vars'][ename] = edecl
+        if 'varnames' in groupcache[groupcounter]:
             groupcache[groupcounter]['varnames'].append(ename)
         last_name = ename
     return last_name
 
-def cracktypespec(typespec,selector):
-    kindselect=None
-    charselect=None
-    typename=None
+
+def cracktypespec(typespec, selector):
+    kindselect = None
+    charselect = None
+    typename = None
     if selector:
-        if typespec in ['complex','integer','logical','real']:
-            kindselect=kindselector.match(selector)
+        if typespec in ['complex', 'integer', 'logical', 'real']:
+            kindselect = kindselector.match(selector)
             if not kindselect:
-                outmess('cracktypespec: no kindselector pattern found for %s\n'%(`selector`))
+                outmess(
+                    'cracktypespec: no kindselector pattern found for %s\n' % (repr(selector)))
                 return
-            kindselect=kindselect.groupdict()
-            kindselect['*']=kindselect['kind2']
+            kindselect = kindselect.groupdict()
+            kindselect['*'] = kindselect['kind2']
             del kindselect['kind2']
-            for k in kindselect.keys():
-                if not kindselect[k]: del kindselect[k]
-            for k,i in kindselect.items():
+            for k in list(kindselect.keys()):
+                if not kindselect[k]:
+                    del kindselect[k]
+            for k, i in list(kindselect.items()):
                 kindselect[k] = rmbadname1(i)
-        elif typespec=='character':
-            charselect=charselector.match(selector)
+        elif typespec == 'character':
+            charselect = charselector.match(selector)
             if not charselect:
-                outmess('cracktypespec: no charselector pattern found for %s\n'%(`selector`))
+                outmess(
+                    'cracktypespec: no charselector pattern found for %s\n' % (repr(selector)))
                 return
-            charselect=charselect.groupdict()
-            charselect['*']=charselect['charlen']
+            charselect = charselect.groupdict()
+            charselect['*'] = charselect['charlen']
             del charselect['charlen']
             if charselect['lenkind']:
-                lenkind=lenkindpattern.match(markoutercomma(charselect['lenkind']))
-                lenkind=lenkind.groupdict()
-                for lk in ['len','kind']:
-                    if lenkind[lk+'2']:
-                        lenkind[lk]=lenkind[lk+'2']
-                    charselect[lk]=lenkind[lk]
-                    del lenkind[lk+'2']
+                lenkind = lenkindpattern.match(
+                    markoutercomma(charselect['lenkind']))
+                lenkind = lenkind.groupdict()
+                for lk in ['len', 'kind']:
+                    if lenkind[lk + '2']:
+                        lenkind[lk] = lenkind[lk + '2']
+                    charselect[lk] = lenkind[lk]
+                    del lenkind[lk + '2']
             del charselect['lenkind']
-            for k in charselect.keys():
-                if not charselect[k]: del charselect[k]
-            for k,i in charselect.items():
+            for k in list(charselect.keys()):
+                if not charselect[k]:
+                    del charselect[k]
+            for k, i in list(charselect.items()):
                 charselect[k] = rmbadname1(i)
-        elif typespec=='type':
-            typename=re.match(r'\s*\(\s*(?P<name>\w+)\s*\)',selector,re.I)
-            if typename: typename=typename.group('name')
-            else: outmess('cracktypespec: no typename found in %s\n'%(`typespec+selector`))
+        elif typespec == 'type':
+            typename = re.match(r'\s*\(\s*(?P<name>\w+)\s*\)', selector, re.I)
+            if typename:
+                typename = typename.group('name')
+            else:
+                outmess('cracktypespec: no typename found in %s\n' %
+                        (repr(typespec + selector)))
         else:
-            outmess('cracktypespec: no selector used for %s\n'%(`selector`))
-    return kindselect,charselect,typename
+            outmess('cracktypespec: no selector used for %s\n' %
+                    (repr(selector)))
+    return kindselect, charselect, typename
 ######
-def setattrspec(decl,attr,force=0):
-    if not decl: decl={}
-    if not attr: return decl
-    if not decl.has_key('attrspec'):
-        decl['attrspec']=[attr]
+
+
+def setattrspec(decl, attr, force=0):
+    if not decl:
+        decl = {}
+    if not attr:
         return decl
-    if force: decl['attrspec'].append(attr)
-    if attr in decl['attrspec']: return decl
-    if attr=='static' and 'automatic' not in decl['attrspec']:
+    if 'attrspec' not in decl:
+        decl['attrspec'] = [attr]
+        return decl
+    if force:
         decl['attrspec'].append(attr)
-    elif attr=='automatic' and 'static' not in decl['attrspec']:
+    if attr in decl['attrspec']:
+        return decl
+    if attr == 'static' and 'automatic' not in decl['attrspec']:
         decl['attrspec'].append(attr)
-    elif attr=='public' and 'private' not in decl['attrspec']:
+    elif attr == 'automatic' and 'static' not in decl['attrspec']:
         decl['attrspec'].append(attr)
-    elif attr=='private' and 'public' not in decl['attrspec']:
-        decl['attrspec'].append(attr)
+    elif attr == 'public':
+        if 'private' not in decl['attrspec']:
+            decl['attrspec'].append(attr)
+    elif attr == 'private':
+        if 'public' not in decl['attrspec']:
+            decl['attrspec'].append(attr)
     else:
         decl['attrspec'].append(attr)
     return decl
-def setkindselector(decl,sel,force=0):
-    if not decl: decl={}
-    if not sel: return decl
-    if not decl.has_key('kindselector'):
-        decl['kindselector']=sel
+
+
+def setkindselector(decl, sel, force=0):
+    if not decl:
+        decl = {}
+    if not sel:
         return decl
-    for k in sel.keys():
-        if force or not decl['kindselector'].has_key(k):
-            decl['kindselector'][k]=sel[k]
+    if 'kindselector' not in decl:
+        decl['kindselector'] = sel
+        return decl
+    for k in list(sel.keys()):
+        if force or k not in decl['kindselector']:
+            decl['kindselector'][k] = sel[k]
     return decl
-def setcharselector(decl,sel,force=0):
-    if not decl: decl={}
-    if not sel: return decl
-    if not decl.has_key('charselector'):
-        decl['charselector']=sel
+
+
+def setcharselector(decl, sel, force=0):
+    if not decl:
+        decl = {}
+    if not sel:
         return decl
-    for k in sel.keys():
-        if force or not decl['charselector'].has_key(k):
-            decl['charselector'][k]=sel[k]
+    if 'charselector' not in decl:
+        decl['charselector'] = sel
+        return decl
+    for k in list(sel.keys()):
+        if force or k not in decl['charselector']:
+            decl['charselector'][k] = sel[k]
     return decl
-def getblockname(block,unknown='unknown'):
-    if block.has_key('name'): return block['name']
+
+
+def getblockname(block, unknown='unknown'):
+    if 'name' in block:
+        return block['name']
     return unknown
-###### post processing
+
+# post processing
+
+
 def setmesstext(block):
     global filepositiontext
-    try: filepositiontext='In: %s:%s\n'%(block['from'],block['name'])
-    except: pass
+
+    try:
+        filepositiontext = 'In: %s:%s\n' % (block['from'], block['name'])
+    except Exception:
+        pass
+
 
 def get_usedict(block):
     usedict = {}
-    if block.has_key('parent_block'):
+    if 'parent_block' in block:
         usedict = get_usedict(block['parent_block'])
-    if block.has_key('use'):
+    if 'use' in block:
         usedict.update(block['use'])
     return usedict
 
+
 def get_useparameters(block, param_map=None):
     global f90modulevars
+
     if param_map is None:
         param_map = {}
     usedict = get_usedict(block)
     if not usedict:
         return param_map
-    for usename,mapping in usedict.items():
-        usename = string.lower(usename)
-        if not f90modulevars.has_key(usename):
+    for usename, mapping in list(usedict.items()):
+        usename = usename.lower()
+        if usename not in f90modulevars:
+            outmess('get_useparameters: no module %s info used by %s\n' %
+                    (usename, block.get('name')))
             continue
         mvars = f90modulevars[usename]
         params = get_parameters(mvars)
@@ -1436,149 +1909,155 @@
             continue
         # XXX: apply mapping
         if mapping:
-            errmess('get_useparameters: mapping for %s not impl.' % (mapping))
-        for k,v in params.items():
-            if param_map.has_key(k):
-                outmess('get_useparameters: overriding parameter %s with'\
-                        ' value from module %s' % (`k`,`usename`))
+            errmess('get_useparameters: mapping for %s not impl.\n' % (mapping))
+        for k, v in list(params.items()):
+            if k in param_map:
+                outmess('get_useparameters: overriding parameter %s with'
+                        ' value from module %s\n' % (repr(k), repr(usename)))
             param_map[k] = v
+
     return param_map
 
-def postcrack2(block,tab='',param_map=None):
+
+def postcrack2(block, tab='', param_map=None):
     global f90modulevars
+
     if not f90modulevars:
         return block
-    if type(block)==types.ListType:
-        ret = []
-        for g in block:
-            g = postcrack2(g,tab=tab+'\t',param_map=param_map)
-            ret.append(g)
+    if isinstance(block, list):
+        ret = [postcrack2(g, tab=tab + '\t', param_map=param_map)
+               for g in block]
         return ret
     setmesstext(block)
-    outmess('%sBlock: %s\n'%(tab,block['name']),0)
+    outmess('%sBlock: %s\n' % (tab, block['name']), 0)
 
     if param_map is None:
         param_map = get_useparameters(block)
 
-    if param_map is not None and block.has_key('vars'):
+    if param_map is not None and 'vars' in block:
         vars = block['vars']
-        for n in vars.keys():
+        for n in list(vars.keys()):
             var = vars[n]
-            if var.has_key('kindselector'):
+            if 'kindselector' in var:
                 kind = var['kindselector']
-                if kind.has_key('kind'):
+                if 'kind' in kind:
                     val = kind['kind']
-                    if param_map.has_key(val):
+                    if val in param_map:
                         kind['kind'] = param_map[val]
-    new_body = []
-    for b in block['body']:
-        b = postcrack2(b,tab=tab+'\t',param_map=param_map)
-        new_body.append(b)
+    new_body = [postcrack2(b, tab=tab + '\t', param_map=param_map)
+                for b in block['body']]
     block['body'] = new_body
 
     return block
 
-def postcrack(block,args=None,tab=''):
+
+def postcrack(block, args=None, tab=''):
     """
     TODO:
           function return values
           determine expression types if in argument list
     """
-    global usermodules,onlyfunctions
-    if type(block)==types.ListType:
-        gret=[]
-        uret=[]
+    global usermodules, onlyfunctions
+
+    if isinstance(block, list):
+        gret = []
+        uret = []
         for g in block:
             setmesstext(g)
-            g=postcrack(g,tab=tab+'\t')
-            if g.has_key('name') and string.find(g['name'],'__user__')>=0: # sort user routines to appear first
+            g = postcrack(g, tab=tab + '\t')
+            # sort user routines to appear first
+            if 'name' in g and '__user__' in g['name']:
                 uret.append(g)
             else:
                 gret.append(g)
-        return uret+gret
+        return uret + gret
     setmesstext(block)
-    if (not type(block)==types.DictType) and not block.has_key('block'):
-        raise 'postcrack: Expected block dictionary instead of ',block
-    if block.has_key('name') and not block['name']=='unknown_interface':
-        outmess('%sBlock: %s\n'%(tab,block['name']),0)
-    blocktype=block['block']
-    block=analyzeargs(block)
-    block=analyzecommon(block)
-    block['vars']=analyzevars(block)
-    block['sortvars']=sortvarnames(block['vars'])
-    if block.has_key('args') and block['args']:
-        args=block['args']
-    block['body']=analyzebody(block,args,tab=tab)
-
-    userisdefined=[]
-##     fromuser = []
-    if block.has_key('use'):
-        useblock=block['use']
-        for k in useblock.keys():
-            if string.find(k,'__user__')>=0:
+    if not isinstance(block, dict) and 'block' not in block:
+        raise Exception('postcrack: Expected block dictionary instead of ' +
+                        str(block))
+    if 'name' in block and not block['name'] == 'unknown_interface':
+        outmess('%sBlock: %s\n' % (tab, block['name']), 0)
+    block = analyzeargs(block)
+    block = analyzecommon(block)
+    block['vars'] = analyzevars(block)
+    block['sortvars'] = sortvarnames(block['vars'])
+    if 'args' in block and block['args']:
+        args = block['args']
+    block['body'] = analyzebody(block, args, tab=tab)
+
+    userisdefined = []
+    if 'use' in block:
+        useblock = block['use']
+        for k in list(useblock.keys()):
+            if '__user__' in k:
                 userisdefined.append(k)
-##                 if useblock[k].has_key('map'):
-##                     for n in useblock[k]['map'].values():
-##                         if n not in fromuser: fromuser.append(n)
-    else: useblock={}
-    name=''
-    if block.has_key('name'):name=block['name']
-    if block.has_key('externals') and block['externals']:# and not userisdefined: # Build a __user__ module
-        interfaced=[]
-        if block.has_key('interfaced'): interfaced=block['interfaced']
-        mvars=copy.copy(block['vars'])
-        if name: mname=name+'__user__routines'
-        else: mname='unknown__user__routines'
+    else:
+        useblock = {}
+    name = ''
+    if 'name' in block:
+        name = block['name']
+    # and not userisdefined: # Build a __user__ module
+    if 'externals' in block and block['externals']:
+        interfaced = []
+        if 'interfaced' in block:
+            interfaced = block['interfaced']
+        mvars = copy.copy(block['vars'])
+        if name:
+            mname = name + '__user__routines'
+        else:
+            mname = 'unknown__user__routines'
         if mname in userisdefined:
-            i=1
-            while '%s_%i'%(mname,i) in userisdefined: i=i+1
-            mname='%s_%i'%(mname,i)
-        interface={'block':'interface','body':[],'vars':{},'name':name+'_user_interface'}
+            i = 1
+            while '%s_%i' % (mname, i) in userisdefined:
+                i = i + 1
+            mname = '%s_%i' % (mname, i)
+        interface = {'block': 'interface', 'body': [],
+                     'vars': {}, 'name': name + '_user_interface'}
         for e in block['externals']:
-##             if e in fromuser:
-##                 outmess('  Skipping %s that is defined explicitly in another use statement\n'%(`e`))
-##                 continue
             if e in interfaced:
-                edef=[]
-                j=-1
+                edef = []
+                j = -1
                 for b in block['body']:
-                    j=j+1
-                    if b['block']=='interface':
-                        i=-1
+                    j = j + 1
+                    if b['block'] == 'interface':
+                        i = -1
                         for bb in b['body']:
-                            i=i+1
-                            if bb.has_key('name') and bb['name']==e:
-                                edef=copy.copy(bb)
+                            i = i + 1
+                            if 'name' in bb and bb['name'] == e:
+                                edef = copy.copy(bb)
                                 del b['body'][i]
                                 break
                         if edef:
-                            if not b['body']: del block['body'][j]
+                            if not b['body']:
+                                del block['body'][j]
                             del interfaced[interfaced.index(e)]
                             break
                 interface['body'].append(edef)
             else:
-                if mvars.has_key(e) and not isexternal(mvars[e]):
-                    interface['vars'][e]=mvars[e]
+                if e in mvars and not isexternal(mvars[e]):
+                    interface['vars'][e] = mvars[e]
         if interface['vars'] or interface['body']:
-            block['interfaced']=interfaced
-            mblock={'block':'python module','body':[interface],'vars':{},'name':mname,'interfaced':block['externals']}
-            useblock[mname]={}
+            block['interfaced'] = interfaced
+            mblock = {'block': 'python module', 'body': [
+                interface], 'vars': {}, 'name': mname, 'interfaced': block['externals']}
+            useblock[mname] = {}
             usermodules.append(mblock)
     if useblock:
-        block['use']=useblock
+        block['use'] = useblock
     return block
+
 
 def sortvarnames(vars):
     indep = []
     dep = []
-    for v in vars.keys():
-        if vars[v].has_key('depend') and vars[v]['depend']:
+    for v in list(vars.keys()):
+        if 'depend' in vars[v] and vars[v]['depend']:
             dep.append(v)
-            #print '%s depends on %s'%(v,vars[v]['depend'])
-        else: indep.append(v)
+        else:
+            indep.append(v)
     n = len(dep)
     i = 0
-    while dep: #XXX: How to catch dependence cycles correctly?
+    while dep:  # XXX: How to catch dependence cycles correctly?
         v = dep[0]
         fl = 0
         for w in dep[1:]:
@@ -1586,12 +2065,12 @@
                 fl = 1
                 break
         if fl:
-            dep = dep[1:]+[v]
+            dep = dep[1:] + [v]
             i = i + 1
-            if i>n:
+            if i > n:
                 errmess('sortvarnames: failed to compute dependencies because'
                         ' of cyclic dependencies between '
-                        +string.join(dep,', ')+'\n')
+                        + ', '.join(dep) + '\n')
                 indep = indep + dep
                 break
         else:
@@ -1599,387 +2078,421 @@
             dep = dep[1:]
             n = len(dep)
             i = 0
-    #print indep
     return indep
 
+
 def analyzecommon(block):
-    if not hascommon(block): return block
-    commonvars=[]
-    for k in block['common'].keys():
-        comvars=[]
+    if not hascommon(block):
+        return block
+    commonvars = []
+    for k in list(block['common'].keys()):
+        comvars = []
         for e in block['common'][k]:
-            m=re.match(r'\A\s*\b(?P<name>.*?)\b\s*(\((?P<dims>.*?)\)|)\s*\Z',e,re.I)
+            m = re.match(
+                r'\A\s*\b(?P<name>.*?)\b\s*(\((?P<dims>.*?)\)|)\s*\Z', e, re.I)
             if m:
-                dims=[]
+                dims = []
                 if m.group('dims'):
-                    dims=map(string.strip,string.split(markoutercomma(m.group('dims')),'@,@'))
-                n=string.strip(m.group('name'))
-                if block['vars'].has_key(n):
-                    if block['vars'][n].has_key('attrspec'):
-                        block['vars'][n]['attrspec'].append('dimension(%s)'%(string.join(dims,',')))
+                    dims = [x.strip()
+                            for x in markoutercomma(m.group('dims')).split('@,@')]
+                n = rmbadname1(m.group('name').strip())
+                if n in block['vars']:
+                    if 'attrspec' in block['vars'][n]:
+                        block['vars'][n]['attrspec'].append(
+                            'dimension(%s)' % (','.join(dims)))
                     else:
-                        block['vars'][n]['attrspec']=['dimension(%s)'%(string.join(dims,','))]
+                        block['vars'][n]['attrspec'] = [
+                            'dimension(%s)' % (','.join(dims))]
                 else:
                     if dims:
-                        block['vars'][n]={'attrspec':['dimension(%s)'%(string.join(dims,','))]}
-                    else: block['vars'][n]={}
-                if n not in commonvars: commonvars.append(n)
+                        block['vars'][n] = {
+                            'attrspec': ['dimension(%s)' % (','.join(dims))]}
+                    else:
+                        block['vars'][n] = {}
+                if n not in commonvars:
+                    commonvars.append(n)
             else:
-                n=e
-                errmess('analyzecommon: failed to extract "<name>[(<dims>)]" from "%s" in common /%s/.\n'%(e,k))
+                n = e
+                errmess(
+                    'analyzecommon: failed to extract "<name>[(<dims>)]" from "%s" in common /%s/.\n' % (e, k))
             comvars.append(n)
-        block['common'][k]=comvars
-    if not block.has_key('commonvars'):
-        block['commonvars']=commonvars
+        block['common'][k] = comvars
+    if 'commonvars' not in block:
+        block['commonvars'] = commonvars
     else:
-        block['commonvars']=block['commonvars']+commonvars
+        block['commonvars'] = block['commonvars'] + commonvars
     return block
-def analyzebody(block,args,tab=''):
-    global usermodules,skipfuncs,onlyfuncs,f90modulevars
+
+
+def analyzebody(block, args, tab=''):
+    global usermodules, skipfuncs, onlyfuncs, f90modulevars
+
     setmesstext(block)
-    body=[]
+    body = []
     for b in block['body']:
         b['parent_block'] = block
-        if b['block'] in ['function','subroutine']:
+        if b['block'] in ['function', 'subroutine']:
             if args is not None and b['name'] not in args:
                 continue
             else:
-                as=b['args']
+                as_ = b['args']
             if b['name'] in skipfuncs:
                 continue
             if onlyfuncs and b['name'] not in onlyfuncs:
                 continue
-        else: as=args
-        b=postcrack(b,as,tab=tab+'\t')
-        if b['block']=='interface' and not b['body']:
-            if not b.has_key('f2pyenhancements'):
+            b['saved_interface'] = crack2fortrangen(
+                b, '\n' + ' ' * 6, as_interface=True)
+
+        else:
+            as_ = args
+        b = postcrack(b, as_, tab=tab + '\t')
+        if b['block'] in ['interface', 'abstract interface'] and \
+           not b['body'] and not b['implementedby']:
+            if 'f2pyenhancements' not in b:
                 continue
-        if string.replace(b['block'],' ','')=='pythonmodule':
+        if b['block'].replace(' ', '') == 'pythonmodule':
             usermodules.append(b)
         else:
-            if b['block']=='module':
+            if b['block'] == 'module':
                 f90modulevars[b['name']] = b['vars']
             body.append(b)
     return body
+
+
 def buildimplicitrules(block):
     setmesstext(block)
-    implicitrules=defaultimplicitrules
-    attrrules={}
-    if block.has_key('implicit'):
+    implicitrules = defaultimplicitrules
+    attrrules = {}
+    if 'implicit' in block:
         if block['implicit'] is None:
-            implicitrules=None
-            if verbose>1:
-                outmess('buildimplicitrules: no implicit rules for routine %s.\n'%`block['name']`)
+            implicitrules = None
+            if verbose > 1:
+                outmess(
+                    'buildimplicitrules: no implicit rules for routine %s.\n' % repr(block['name']))
         else:
-            for k in block['implicit'].keys():
-                if block['implicit'][k].get('typespec') not in ['static','automatic']:
-                    implicitrules[k]=block['implicit'][k]
+            for k in list(block['implicit'].keys()):
+                if block['implicit'][k].get('typespec') not in ['static', 'automatic']:
+                    implicitrules[k] = block['implicit'][k]
                 else:
-                    attrrules[k]=block['implicit'][k]['typespec']
-    return implicitrules,attrrules
-
-def myeval(e,g=None,l=None):
-    r = eval(e,g,l)
-    if type(r) in [type(0),type(0.0)]:
+                    attrrules[k] = block['implicit'][k]['typespec']
+    return implicitrules, attrrules
+
+
+def myeval(e, g=None, l=None):
+    """ Like `eval` but returns only integers and floats """
+    r = eval(e, g, l)
+    if type(r) in [int, float]:
         return r
-    raise ValueError,'r=%r' % (r)
-
-getlincoef_re_1 = re.compile(r'\A\b\w+\b\Z',re.I)
-def getlincoef(e,xset): # e = a*x+b ; x in xset
+    raise ValueError('r=%r' % (r))
+
+getlincoef_re_1 = re.compile(r'\A\b\w+\b\Z', re.I)
+
+
+def getlincoef(e, xset):  # e = a*x+b ; x in xset
+    """
+    Obtain ``a`` and ``b`` when ``e == "a*x+b"``, where ``x`` is a symbol in
+    xset.
+
+    >>> getlincoef('2*x + 1', {'x'})
+    (2, 1, 'x')
+    >>> getlincoef('3*x + x*2 + 2 + 1', {'x'})
+    (5, 3, 'x')
+    >>> getlincoef('0', {'x'})
+    (0, 0, None)
+    >>> getlincoef('0*x', {'x'})
+    (0, 0, 'x')
+    >>> getlincoef('x*x', {'x'})
+    (None, None, None)
+
+    This can be tricked by sufficiently complex expressions
+
+    >>> getlincoef('(x - 0.5)*(x - 1.5)*(x - 1)*x + 2*x + 3', {'x'})
+    (2.0, 3.0, 'x')
+    """
     try:
-        c = int(myeval(e,{},{}))
-        return 0,c,None
-    except: pass
+        c = int(myeval(e, {}, {}))
+        return 0, c, None
+    except Exception:
+        pass
     if getlincoef_re_1.match(e):
-        return 1,0,e
+        return 1, 0, e
     len_e = len(e)
     for x in xset:
-        if len(x)>len_e: continue
-        re_1 = re.compile(r'(?P<before>.*?)\b'+x+r'\b(?P<after>.*)',re.I)
+        if len(x) > len_e:
+            continue
+        if re.search(r'\w\s*\([^)]*\b' + x + r'\b', e):
+            # skip function calls having x as an argument, e.g max(1, x)
+            continue
+        re_1 = re.compile(r'(?P<before>.*?)\b' + x + r'\b(?P<after>.*)', re.I)
         m = re_1.match(e)
         if m:
             try:
                 m1 = re_1.match(e)
                 while m1:
-                    ee = '%s(%s)%s'%(m1.group('before'),0,m1.group('after'))
+                    ee = '%s(%s)%s' % (
+                        m1.group('before'), 0, m1.group('after'))
                     m1 = re_1.match(ee)
-                b = myeval(ee,{},{})
+                b = myeval(ee, {}, {})
                 m1 = re_1.match(e)
                 while m1:
-                    ee = '%s(%s)%s'%(m1.group('before'),1,m1.group('after'))
+                    ee = '%s(%s)%s' % (
+                        m1.group('before'), 1, m1.group('after'))
                     m1 = re_1.match(ee)
-                a = myeval(ee,{},{}) - b
+                a = myeval(ee, {}, {}) - b
                 m1 = re_1.match(e)
                 while m1:
-                    ee = '%s(%s)%s'%(m1.group('before'),0.5,m1.group('after'))
+                    ee = '%s(%s)%s' % (
+                        m1.group('before'), 0.5, m1.group('after'))
                     m1 = re_1.match(ee)
-                c = myeval(ee,{},{})
-                if (a*0.5+b==c):
-                    return a,b,x
-            except: pass
+                c = myeval(ee, {}, {})
+                # computing another point to be sure that expression is linear
+                m1 = re_1.match(e)
+                while m1:
+                    ee = '%s(%s)%s' % (
+                        m1.group('before'), 1.5, m1.group('after'))
+                    m1 = re_1.match(ee)
+                c2 = myeval(ee, {}, {})
+                if (a * 0.5 + b == c and a * 1.5 + b == c2):
+                    return a, b, x
+            except Exception:
+                pass
             break
-    return None,None,None
-
-_varname_match = re.compile(r'\A[a-z]\w*\Z').match
-def getarrlen(dl,args,star='*'):
-    edl = []
-    try: edl.append(myeval(dl[0],{},{}))
-    except: edl.append(dl[0])
-    try: edl.append(myeval(dl[1],{},{}))
-    except: edl.append(dl[1])
-    if type(edl[0]) is type(0):
-        p1 = 1-edl[0]
-        if p1==0: d = str(dl[1])
-        elif p1<0: d = '%s-%s'%(dl[1],-p1)
-        else: d = '%s+%s'%(dl[1],p1)
-    elif type(edl[1]) is type(0):
-        p1 = 1+edl[1]
-        if p1==0: d='-(%s)' % (dl[0])
-        else: d='%s-(%s)' % (p1,dl[0])
-    else: d = '%s-(%s)+1'%(dl[1],dl[0])
-    try: return `myeval(d,{},{})`,None,None
-    except: pass
-    d1,d2=getlincoef(dl[0],args),getlincoef(dl[1],args)
-    if None not in [d1[0],d2[0]]:
-        if (d1[0],d2[0])==(0,0):
-            return `d2[1]-d1[1]+1`,None,None
-        b = d2[1] - d1[1] + 1
-        d1 = (d1[0],0,d1[2])
-        d2 = (d2[0],b,d2[2])
-        if d1[0]==0 and d2[2] in args:
-            if b<0: return '%s * %s - %s'%(d2[0],d2[2],-b),d2[2],'+%s)/(%s)'%(-b,d2[0])
-            elif b: return '%s * %s + %s'%(d2[0],d2[2],b),d2[2],'-%s)/(%s)'%(b,d2[0])
-            else: return '%s * %s'%(d2[0],d2[2]),d2[2],')/(%s)'%(d2[0])
-        if d2[0]==0 and d1[2] in args:
-
-            if b<0: return '%s * %s - %s'%(-d1[0],d1[2],-b),d1[2],'+%s)/(%s)'%(-b,-d1[0])
-            elif b: return '%s * %s + %s'%(-d1[0],d1[2],b),d1[2],'-%s)/(%s)'%(b,-d1[0])
-            else: return '%s * %s'%(-d1[0],d1[2]),d1[2],')/(%s)'%(-d1[0])
-        if d1[2]==d2[2] and d1[2] in args:
-            a = d2[0] - d1[0]
-            if not a: return `b`,None,None
-            if b<0: return '%s * %s - %s'%(a,d1[2],-b),d2[2],'+%s)/(%s)'%(-b,a)
-            elif b: return '%s * %s + %s'%(a,d1[2],b),d2[2],'-%s)/(%s)'%(b,a)
-            else: return '%s * %s'%(a,d1[2]),d2[2],')/(%s)'%(a)
-        if d1[0]==d2[0]==1:
-            c = str(d1[2])
-            if c not in args:
-                if _varname_match(c):
-                    outmess('\tgetarrlen:variable "%s" undefined\n' % (c))
-                c = '(%s)'%c
-            if b==0: d='%s-%s' % (d2[2],c)
-            elif b<0: d='%s-%s-%s' % (d2[2],c,-b)
-            else: d='%s-%s+%s' % (d2[2],c,b)
-        elif d1[0]==0:
-            c2 = str(d2[2])
-            if c2 not in args:
-                if _varname_match(c2):
-                    outmess('\tgetarrlen:variable "%s" undefined\n' % (c2))
-                c2 = '(%s)'%c2
-            if d2[0]==1: pass
-            elif d2[0]==-1: c2='-%s' %c2
-            else: c2='%s*%s'%(d2[0],c2)
-
-            if b==0: d=c2
-            elif b<0: d='%s-%s' % (c2,-b)
-            else: d='%s+%s' % (c2,b)
-        elif d2[0]==0:
-            c1 = str(d1[2])
-            if c1 not in args:
-                if _varname_match(c1):
-                    outmess('\tgetarrlen:variable "%s" undefined\n' % (c1))
-                c1 = '(%s)'%c1
-            if d1[0]==1: c1='-%s'%c1
-            elif d1[0]==-1: c1='+%s'%c1
-            elif d1[0]<0: c1='+%s*%s'%(-d1[0],c1)
-            else: c1 = '-%s*%s' % (d1[0],c1)
-
-            if b==0: d=c1
-            elif b<0: d='%s-%s' % (c1,-b)
-            else: d='%s+%s' % (c1,b)
-        else:
-            c1 = str(d1[2])
-            if c1 not in args:
-                if _varname_match(c1):
-                    outmess('\tgetarrlen:variable "%s" undefined\n' % (c1))
-                c1 = '(%s)'%c1
-            if d1[0]==1: c1='-%s'%c1
-            elif d1[0]==-1: c1='+%s'%c1
-            elif d1[0]<0: c1='+%s*%s'%(-d1[0],c1)
-            else: c1 = '-%s*%s' % (d1[0],c1)
-
-            c2 = str(d2[2])
-            if c2 not in args:
-                if _varname_match(c2):
-                    outmess('\tgetarrlen:variable "%s" undefined\n' % (c2))
-                c2 = '(%s)'%c2
-            if d2[0]==1: pass
-            elif d2[0]==-1: c2='-%s' %c2
-            else: c2='%s*%s'%(d2[0],c2)
-
-            if b==0: d='%s%s' % (c2,c1)
-            elif b<0: d='%s%s-%s' % (c2,c1,-b)
-            else: d='%s%s+%s' % (c2,c1,b)
-    return d,None,None
-
-word_pattern = re.compile(r'\b[a-z][\w$]*\b',re.I)
+    return None, None, None
+
+
+word_pattern = re.compile(r'\b[a-z][\w$]*\b', re.I)
+
 
 def _get_depend_dict(name, vars, deps):
-    if vars.has_key(name):
-        words = vars[name].get('depend',[])
-
-        if vars[name].has_key('=') and not isstring(vars[name]):
+    if name in vars:
+        words = vars[name].get('depend', [])
+
+        if '=' in vars[name] and not isstring(vars[name]):
             for word in word_pattern.findall(vars[name]['=']):
-                if word not in words and vars.has_key(word):
+                # The word_pattern may return values that are not
+                # only variables, they can be string content for instance
+                if word not in words and word in vars and word != name:
                     words.append(word)
         for word in words[:]:
-            for w in deps.get(word,[]) \
+            for w in deps.get(word, []) \
                     or _get_depend_dict(word, vars, deps):
                 if w not in words:
                     words.append(w)
     else:
-        outmess('_get_depend_dict: no dependence info for %s\n' % (`name`))
+        outmess('_get_depend_dict: no dependence info for %s\n' % (repr(name)))
         words = []
     deps[name] = words
     return words
 
+
 def _calc_depend_dict(vars):
-    names = vars.keys()
+    names = list(vars.keys())
     depend_dict = {}
     for n in names:
         _get_depend_dict(n, vars, depend_dict)
     return depend_dict
+
 
 def get_sorted_names(vars):
     """
     """
     depend_dict = _calc_depend_dict(vars)
     names = []
-    for name in depend_dict.keys():
+    for name in list(depend_dict.keys()):
         if not depend_dict[name]:
             names.append(name)
             del depend_dict[name]
     while depend_dict:
-        for name, lst in depend_dict.items():
-            new_lst = [n for n in lst if depend_dict.has_key(n)]
+        for name, lst in list(depend_dict.items()):
+            new_lst = [n for n in lst if n in depend_dict]
             if not new_lst:
                 names.append(name)
                 del depend_dict[name]
             else:
                 depend_dict[name] = new_lst
-    return [name for name in names if vars.has_key(name)]
+    return [name for name in names if name in vars]
+
 
 def _kind_func(string):
-    #XXX: return something sensible.
+    # XXX: return something sensible.
     if string[0] in "'\"":
         string = string[1:-1]
     if real16pattern.match(string):
+        return 8
+    elif real8pattern.match(string):
+        return 4
+    return 'kind(' + string + ')'
+
+
+def _selected_int_kind_func(r):
+    # XXX: This should be processor dependent
+    m = 10 ** r
+    if m <= 2 ** 8:
+        return 1
+    if m <= 2 ** 16:
+        return 2
+    if m <= 2 ** 32:
+        return 4
+    if m <= 2 ** 63:
+        return 8
+    if m <= 2 ** 128:
         return 16
-    elif real8pattern.match(string):
+    return -1
+
+
+def _selected_real_kind_func(p, r=0, radix=0):
+    # XXX: This should be processor dependent
+    # This is only good for 0 <= p <= 20
+    if p < 7:
+        return 4
+    if p < 16:
         return 8
-    return 'kind('+string+')'
-
-def _selected_int_kind_func(r):
-    #XXX: This should be processor dependent
-    m = 10**r
-    if m<=2**8: return 1
-    if m<=2**16: return 2
-    if m<=2**32: return 4
-    if m<=2**64: return 8
-    if m<=2**128: return 16
+    machine = platform.machine().lower()
+    if machine.startswith(('aarch64', 'power', 'ppc', 'riscv', 's390x', 'sparc')):
+        if p <= 20:
+            return 16
+    else:
+        if p < 19:
+            return 10
+        elif p <= 20:
+            return 16
     return -1
+
 
 def get_parameters(vars, global_params={}):
     params = copy.copy(global_params)
     g_params = copy.copy(global_params)
-    for name,func in [('kind',_kind_func),
-                      ('selected_int_kind',_selected_int_kind_func),
-                      ]:
-        if not g_params.has_key(name):
+    for name, func in [('kind', _kind_func),
+                       ('selected_int_kind', _selected_int_kind_func),
+                       ('selected_real_kind', _selected_real_kind_func), ]:
+        if name not in g_params:
             g_params[name] = func
     param_names = []
     for n in get_sorted_names(vars):
-        if vars[n].has_key('attrspec') and 'parameter' in vars[n]['attrspec']:
+        if 'attrspec' in vars[n] and 'parameter' in vars[n]['attrspec']:
             param_names.append(n)
-    kind_re = re.compile(r'\bkind\s*\(\s*(?P<value>.*)\s*\)',re.I)
-    selected_int_kind_re = re.compile(r'\bselected_int_kind\s*\(\s*(?P<value>.*)\s*\)',re.I)
+    kind_re = re.compile(r'\bkind\s*\(\s*(?P<value>.*)\s*\)', re.I)
+    selected_int_kind_re = re.compile(
+        r'\bselected_int_kind\s*\(\s*(?P<value>.*)\s*\)', re.I)
+    selected_kind_re = re.compile(
+        r'\bselected_(int|real)_kind\s*\(\s*(?P<value>.*)\s*\)', re.I)
     for n in param_names:
-        if vars[n].has_key('='):
+        if '=' in vars[n]:
             v = vars[n]['=']
             if islogical(vars[n]):
                 v = v.lower()
                 for repl in [
-                    ('.false.','False'),
-                    ('.true.','True'),
-                    #TODO: test .eq., .neq., etc replacements.
-                    ]:
+                    ('.false.', 'False'),
+                    ('.true.', 'True'),
+                    # TODO: test .eq., .neq., etc replacements.
+                ]:
                     v = v.replace(*repl)
-            v = kind_re.sub(r'kind("\1")',v)
-            v = selected_int_kind_re.sub(r'selected_int_kind(\1)',v)
-            if isinteger(vars[n]) and not selected_int_kind_re.match(v):
-                v = v.split('_')[0]
+            v = kind_re.sub(r'kind("\1")', v)
+            v = selected_int_kind_re.sub(r'selected_int_kind(\1)', v)
+
+            # We need to act according to the data.
+            # The easy case is if the data has a kind-specifier,
+            # then we may easily remove those specifiers.
+            # However, it may be that the user uses other specifiers...(!)
+            is_replaced = False
+            if 'kindselector' in vars[n]:
+                if 'kind' in vars[n]['kindselector']:
+                    orig_v_len = len(v)
+                    v = v.replace('_' + vars[n]['kindselector']['kind'], '')
+                    # Again, this will be true if even a single specifier
+                    # has been replaced, see comment above.
+                    is_replaced = len(v) < orig_v_len
+                    
+            if not is_replaced:
+                if not selected_kind_re.match(v):
+                    v_ = v.split('_')
+                    # In case there are additive parameters
+                    if len(v_) > 1: 
+                        v = ''.join(v_[:-1]).lower().replace(v_[-1].lower(), '')
+
+            # Currently this will not work for complex numbers.
+            # There is missing code for extracting a complex number,
+            # which may be defined in either of these:
+            #  a) (Re, Im)
+            #  b) cmplx(Re, Im)
+            #  c) dcmplx(Re, Im)
+            #  d) cmplx(Re, Im, <prec>)
+
             if isdouble(vars[n]):
                 tt = list(v)
                 for m in real16pattern.finditer(v):
-                    tt[m.start():m.end()] = list(\
-                            v[m.start():m.end()].lower().replace('d', 'e'))
-                v = string.join(tt,'')
-            if iscomplex(vars[n]):
-                if v[0]=='(' and v[-1]==')':
-                    l = markoutercomma(v[1:-1]).split('@,@')
-                    print n,params
+                    tt[m.start():m.end()] = list(
+                        v[m.start():m.end()].lower().replace('d', 'e'))
+                v = ''.join(tt)
+
+            elif iscomplex(vars[n]):
+                outmess(f'get_parameters[TODO]: '
+                        f'implement evaluation of complex expression {v}\n')
+
+            # Handle _dp for gh-6624
+            # Also fixes gh-20460
+            if real16pattern.search(v):
+                v = 8
+            elif real8pattern.search(v):
+                v = 4
             try:
-                params[n] = eval(v,g_params,params)
-            except Exception,msg:
+                params[n] = eval(v, g_params, params)
+
+            except Exception as msg:
                 params[n] = v
-                #print params
-                outmess('get_parameters: got "%s" on %s\n' % (msg,`v`))
-            if isstring(vars[n]) and type(params[n]) is type(0):
+                outmess('get_parameters: got "%s" on %s\n' % (msg, repr(v)))
+            if isstring(vars[n]) and isinstance(params[n], int):
                 params[n] = chr(params[n])
-            nl = string.lower(n)
-            if nl!=n:
+            nl = n.lower()
+            if nl != n:
                 params[nl] = params[n]
         else:
-            print vars[n]
-            outmess('get_parameters:parameter %s does not have value?!\n'%(`n`))
+            print(vars[n])
+            outmess(
+                'get_parameters:parameter %s does not have value?!\n' % (repr(n)))
     return params
 
-def _eval_length(length,params):
-    if length in ['(:)','(*)','*']:
+
+def _eval_length(length, params):
+    if length in ['(:)', '(*)', '*']:
         return '(*)'
-    return _eval_scalar(length,params)
-
-_is_kind_number = re.compile('\d+_').match
-
-def _eval_scalar(value,params):
+    return _eval_scalar(length, params)
+
+_is_kind_number = re.compile(r'\d+_').match
+
+
+def _eval_scalar(value, params):
     if _is_kind_number(value):
         value = value.split('_')[0]
     try:
-        value = str(eval(value,{},params))
-    except (NameError, SyntaxError):
+        value = eval(value, {}, params)
+        value = (repr if isinstance(value, str) else str)(value)
+    except (NameError, SyntaxError, TypeError):
         return value
-    except Exception,msg:
-        errmess('"%s" in evaluating %r '\
-                '(available names: %s)\n' \
-                % (msg,value,params.keys()))
+    except Exception as msg:
+        errmess('"%s" in evaluating %r '
+                '(available names: %s)\n'
+                % (msg, value, list(params.keys())))
     return value
+
 
 def analyzevars(block):
     global f90modulevars
+
     setmesstext(block)
-    implicitrules,attrrules=buildimplicitrules(block)
-    vars=copy.copy(block['vars'])
-    if block['block']=='function' and not vars.has_key(block['name']):
-        vars[block['name']]={}
-    if block['vars'].has_key(''):
+    implicitrules, attrrules = buildimplicitrules(block)
+    vars = copy.copy(block['vars'])
+    if block['block'] == 'function' and block['name'] not in vars:
+        vars[block['name']] = {}
+    if '' in block['vars']:
         del vars['']
-        if block['vars'][''].has_key('attrspec'):
-            gen=block['vars']['']['attrspec']
-            for n in vars.keys():
-                for k in ['public','private']:
+        if 'attrspec' in block['vars']['']:
+            gen = block['vars']['']['attrspec']
+            for n in list(vars.keys()):
+                for k in ['public', 'private']:
                     if k in gen:
-                        vars[n]=setattrspec(vars[n],k)
-    svars=[]
+                        vars[n] = setattrspec(vars[n], k)
+    svars = []
     args = block['args']
     for a in args:
         try:
@@ -1987,684 +2500,858 @@
             svars.append(a)
         except KeyError:
             pass
-    for n in vars.keys():
-        if n not in args: svars.append(n)
+    for n in list(vars.keys()):
+        if n not in args:
+            svars.append(n)
 
     params = get_parameters(vars, get_useparameters(block))
 
     dep_matches = {}
-    name_match = re.compile(r'\w[\w\d_$]*').match
-    for v in vars.keys():
+    name_match = re.compile(r'[A-Za-z][\w$]*').match
+    for v in list(vars.keys()):
         m = name_match(v)
         if m:
             n = v[m.start():m.end()]
             try:
                 dep_matches[n]
             except KeyError:
-                dep_matches[n] = re.compile(r'.*\b%s\b'%(v),re.I).match
+                dep_matches[n] = re.compile(r'.*\b%s\b' % (v), re.I).match
     for n in svars:
-        if n[0] in attrrules.keys():
-            vars[n]=setattrspec(vars[n],attrrules[n[0]])
-        if not vars[n].has_key('typespec'):
-            if not(vars[n].has_key('attrspec') and 'external' in vars[n]['attrspec']):
+        if n[0] in list(attrrules.keys()):
+            vars[n] = setattrspec(vars[n], attrrules[n[0]])
+        if 'typespec' not in vars[n]:
+            if not('attrspec' in vars[n] and 'external' in vars[n]['attrspec']):
                 if implicitrules:
-                    ln0 = string.lower(n[0])
-                    for k in implicitrules[ln0].keys():
-                        if k=='typespec' and implicitrules[ln0][k]=='undefined':
+                    ln0 = n[0].lower()
+                    for k in list(implicitrules[ln0].keys()):
+                        if k == 'typespec' and implicitrules[ln0][k] == 'undefined':
                             continue
-                        if not vars[n].has_key(k):
-                            vars[n][k]=implicitrules[ln0][k]
-                        elif k=='attrspec':
+                        if k not in vars[n]:
+                            vars[n][k] = implicitrules[ln0][k]
+                        elif k == 'attrspec':
                             for l in implicitrules[ln0][k]:
-                                vars[n]=setattrspec(vars[n],l)
+                                vars[n] = setattrspec(vars[n], l)
                 elif n in block['args']:
-                    outmess('analyzevars: typespec of variable %s is not defined in routine %s.\n'%(`n`,block['name']))
-
-        if vars[n].has_key('charselector'):
-            if vars[n]['charselector'].has_key('len'):
+                    outmess('analyzevars: typespec of variable %s is not defined in routine %s.\n' % (
+                        repr(n), block['name']))
+
+        if 'charselector' in vars[n]:
+            if 'len' in vars[n]['charselector']:
                 l = vars[n]['charselector']['len']
                 try:
-                    l = str(eval(l,{},params))
-                except:
+                    l = str(eval(l, {}, params))
+                except Exception:
                     pass
                 vars[n]['charselector']['len'] = l
 
-        if vars[n].has_key('kindselector'):
-            if vars[n]['kindselector'].has_key('kind'):
+        if 'kindselector' in vars[n]:
+            if 'kind' in vars[n]['kindselector']:
                 l = vars[n]['kindselector']['kind']
                 try:
-                    l = str(eval(l,{},params))
-                except:
+                    l = str(eval(l, {}, params))
+                except Exception:
                     pass
                 vars[n]['kindselector']['kind'] = l
 
-        savelindims = {}
-        if vars[n].has_key('attrspec'):
-            attr=vars[n]['attrspec']
+        dimension_exprs = {}
+        if 'attrspec' in vars[n]:
+            attr = vars[n]['attrspec']
             attr.reverse()
-            vars[n]['attrspec']=[]
-            dim,intent,depend,check,note=None,None,None,None,None
+            vars[n]['attrspec'] = []
+            dim, intent, depend, check, note = None, None, None, None, None
             for a in attr:
-                if a[:9]=='dimension': dim=(string.strip(a[9:]))[1:-1]
-                elif a[:6]=='intent': intent=(string.strip(a[6:]))[1:-1]
-                elif a[:6]=='depend': depend=(string.strip(a[6:]))[1:-1]
-                elif a[:5]=='check': check=(string.strip(a[5:]))[1:-1]
-                elif a[:4]=='note': note=(string.strip(a[4:]))[1:-1]
-                else: vars[n]=setattrspec(vars[n],a)
+                if a[:9] == 'dimension':
+                    dim = (a[9:].strip())[1:-1]
+                elif a[:6] == 'intent':
+                    intent = (a[6:].strip())[1:-1]
+                elif a[:6] == 'depend':
+                    depend = (a[6:].strip())[1:-1]
+                elif a[:5] == 'check':
+                    check = (a[5:].strip())[1:-1]
+                elif a[:4] == 'note':
+                    note = (a[4:].strip())[1:-1]
+                else:
+                    vars[n] = setattrspec(vars[n], a)
                 if intent:
-                    if not vars[n].has_key('intent'): vars[n]['intent']=[]
-                    for c in map(string.strip,string.split(markoutercomma(intent),'@,@')):
-                        if not c in vars[n]['intent']:
-                            vars[n]['intent'].append(c)
-                    intent=None
+                    if 'intent' not in vars[n]:
+                        vars[n]['intent'] = []
+                    for c in [x.strip() for x in markoutercomma(intent).split('@,@')]:
+                        # Remove spaces so that 'in out' becomes 'inout'
+                        tmp = c.replace(' ', '')
+                        if tmp not in vars[n]['intent']:
+                            vars[n]['intent'].append(tmp)
+                    intent = None
                 if note:
-                    note=string.replace(note,'\\n\\n','\n\n')
-                    note=string.replace(note,'\\n ','\n')
-                    if not vars[n].has_key('note'): vars[n]['note']=[note]
-                    else: vars[n]['note'].append(note)
-                    note=None
+                    note = note.replace('\\n\\n', '\n\n')
+                    note = note.replace('\\n ', '\n')
+                    if 'note' not in vars[n]:
+                        vars[n]['note'] = [note]
+                    else:
+                        vars[n]['note'].append(note)
+                    note = None
                 if depend is not None:
-                    if not vars[n].has_key('depend'): vars[n]['depend']=[]
-                    for c in rmbadname(map(string.strip,string.split(markoutercomma(depend),'@,@'))):
+                    if 'depend' not in vars[n]:
+                        vars[n]['depend'] = []
+                    for c in rmbadname([x.strip() for x in markoutercomma(depend).split('@,@')]):
                         if c not in vars[n]['depend']:
                             vars[n]['depend'].append(c)
-                    depend=None
+                    depend = None
                 if check is not None:
-                    if not vars[n].has_key('check'): vars[n]['check']=[]
-                    for c in map(string.strip,string.split(markoutercomma(check),'@,@')):
-                        if not c in vars[n]['check']:
+                    if 'check' not in vars[n]:
+                        vars[n]['check'] = []
+                    for c in [x.strip() for x in markoutercomma(check).split('@,@')]:
+                        if c not in vars[n]['check']:
                             vars[n]['check'].append(c)
-                    check=None
-            if dim and not vars[n].has_key('dimension'):
-                vars[n]['dimension']=[]
-                for d in rmbadname(map(string.strip,string.split(markoutercomma(dim),'@,@'))):
-                    star = '*'
-                    if d==':': star=':'
-                    if params.has_key(d):
+                    check = None
+            if dim and 'dimension' not in vars[n]:
+                vars[n]['dimension'] = []
+                for d in rmbadname([x.strip() for x in markoutercomma(dim).split('@,@')]):
+                    star = ':' if d == ':' else '*'
+                    # Evaluate `d` with respect to params
+                    if d in params:
                         d = str(params[d])
-                    for p in params.keys():
-                        m = re.match(r'(?P<before>.*?)\b'+p+r'\b(?P<after>.*)',d,re.I)
-                        if m:
-                            #outmess('analyzevars:replacing parameter %s in %s (dimension of %s) with %s\n'%(`p`,`d`,`n`,`params[p]`))
-                            d = m.group('before')+str(params[p])+m.group('after')
-                    if d==star:
+                    for p in params:
+                        re_1 = re.compile(r'(?P<before>.*?)\b' + p + r'\b(?P<after>.*)', re.I)
+                        m = re_1.match(d)
+                        while m:
+                            d = m.group('before') + \
+                                str(params[p]) + m.group('after')
+                            m = re_1.match(d)
+
+                    if d == star:
                         dl = [star]
                     else:
-                        dl=string.split(markoutercomma(d,':'),'@:@')
-                    if len(dl)==2 and '*' in dl: # e.g. dimension(5:*)
+                        dl = markoutercomma(d, ':').split('@:@')
+                    if len(dl) == 2 and '*' in dl:  # e.g. dimension(5:*)
                         dl = ['*']
                         d = '*'
-                    if len(dl)==1 and not dl[0]==star: dl = ['1',dl[0]]
-                    if len(dl)==2:
-                        d,v,di = getarrlen(dl,block['vars'].keys())
-                        if d[:4] == '1 * ': d = d[4:]
-                        if di and di[-4:] == '/(1)': di = di[:-4]
-                        if v: savelindims[d] = v,di
+                    if len(dl) == 1 and dl[0] != star:
+                        dl = ['1', dl[0]]
+                    if len(dl) == 2:
+                        d1, d2 = map(symbolic.Expr.parse, dl)
+                        dsize = d2 - d1 + 1
+                        d = dsize.tostring(language=symbolic.Language.C)
+                        # find variables v that define d as a linear
+                        # function, `d == a * v + b`, and store
+                        # coefficients a and b for further analysis.
+                        solver_and_deps = {}
+                        for v in block['vars']:
+                            s = symbolic.as_symbol(v)
+                            if dsize.contains(s):
+                                try:
+                                    a, b = dsize.linear_solve(s)
+
+                                    def solve_v(s, a=a, b=b):
+                                        return (s - b) / a
+
+                                    all_symbols = set(a.symbols())
+                                    all_symbols.update(b.symbols())
+                                except RuntimeError as msg:
+                                    # d is not a linear function of v,
+                                    # however, if v can be determined
+                                    # from d using other means,
+                                    # implement the corresponding
+                                    # solve_v function here.
+                                    solve_v = None
+                                    all_symbols = set(dsize.symbols())
+                                v_deps = set(
+                                    s.data for s in all_symbols
+                                    if s.data in vars)
+                                solver_and_deps[v] = solve_v, list(v_deps)
+                        # Note that dsize may contain symbols that are
+                        # not defined in block['vars']. Here we assume
+                        # these correspond to Fortran/C intrinsic
+                        # functions or that are defined by other
+                        # means. We'll let the compiler validate the
+                        # definiteness of such symbols.
+                        dimension_exprs[d] = solver_and_deps
                     vars[n]['dimension'].append(d)
-        if vars[n].has_key('dimension'):
-            if isintent_c(vars[n]):
-                shape_macro = 'shape'
-            else:
-                shape_macro = 'shape'#'fshape'
+
+        if 'dimension' in vars[n]:
             if isstringarray(vars[n]):
-                if vars[n].has_key('charselector'):
+                if 'charselector' in vars[n]:
                     d = vars[n]['charselector']
-                    if d.has_key('*'):
+                    if '*' in d:
                         d = d['*']
-                        errmess('analyzevars: character array "character*%s %s(%s)" is considered as "character %s(%s)"; "intent(c)" is forced.\n'\
-                                %(d,n,
-                                  ','.join(vars[n]['dimension']),
-                                  n,','.join(vars[n]['dimension']+[d])))
+                        errmess('analyzevars: character array "character*%s %s(%s)" is considered as "character %s(%s)"; "intent(c)" is forced.\n'
+                                % (d, n,
+                                   ','.join(vars[n]['dimension']),
+                                   n, ','.join(vars[n]['dimension'] + [d])))
                         vars[n]['dimension'].append(d)
                         del vars[n]['charselector']
-                        if not vars[n].has_key('intent'):
+                        if 'intent' not in vars[n]:
                             vars[n]['intent'] = []
                         if 'c' not in vars[n]['intent']:
                             vars[n]['intent'].append('c')
                     else:
-                        errmess("analyzevars: charselector=%r unhandled." % (d))
-        if not vars[n].has_key('check') and block.has_key('args') and n in block['args']:
-            flag=not vars[n].has_key('depend')
-            if flag: vars[n]['depend']=[]
-            vars[n]['check']=[]
-            if vars[n].has_key('dimension'):
-                #/----< no check
-                #vars[n]['check'].append('rank(%s)==%s'%(n,len(vars[n]['dimension'])))
-                i=-1; ni=len(vars[n]['dimension'])
-                for d in vars[n]['dimension']:
-                    ddeps=[] # dependecies of 'd'
-                    ad=''
-                    pd=''
-                    #origd = d
-                    if not vars.has_key(d):
-                        if savelindims.has_key(d):
-                            pd,ad='(',savelindims[d][1]
-                            d = savelindims[d][0]
-                        else:
-                            for r in block['args']:
-                            #for r in block['vars'].keys():
-                                if not vars.has_key(r): continue
-                                if re.match(r'.*?\b'+r+r'\b',d,re.I):
-                                    ddeps.append(r)
-                    if vars.has_key(d):
-                        if vars[d].has_key('attrspec'):
-                            for aa in vars[d]['attrspec']:
-                                if aa[:6]=='depend':
-                                    ddeps=ddeps+string.split((string.strip(aa[6:]))[1:-1],',')
-                        if vars[d].has_key('depend'):
-                            ddeps=ddeps+vars[d]['depend']
-                    i=i+1
-                    if vars.has_key(d) and (not vars[d].has_key('depend')) \
-                       and (not vars[d].has_key('=')) and (d not in vars[n]['depend']) \
-                       and l_or(isintent_in,isintent_inout,isintent_inplace)(vars[n]):
-                        vars[d]['depend']=[n]
-                        if ni>1:
-                            vars[d]['=']='%s%s(%s,%s)%s'% (pd,shape_macro,n,i,ad)
-                        else:
-                            vars[d]['=']='%slen(%s)%s'% (pd,n,ad)
-                        #  /---< no check
-                        if 1 and not vars[d].has_key('check'):
-                            if ni>1:
-                                vars[d]['check']=['%s%s(%s,%i)%s==%s'\
-                                                  %(pd,shape_macro,n,i,ad,d)]
+                        errmess(
+                            "analyzevars: charselector=%r unhandled.\n" % (d))
+
+        if 'check' not in vars[n] and 'args' in block and n in block['args']:
+            # n is an argument that has no checks defined. Here we
+            # generate some consistency checks for n, and when n is an
+            # array, generate checks for its dimensions and construct
+            # initialization expressions.
+            n_deps = vars[n].get('depend', [])
+            n_checks = []
+            n_is_input = l_or(isintent_in, isintent_inout,
+                              isintent_inplace)(vars[n])
+            if isarray(vars[n]):  # n is array
+                for i, d in enumerate(vars[n]['dimension']):
+                    coeffs_and_deps = dimension_exprs.get(d)
+                    if coeffs_and_deps is None:
+                        # d is `:` or `*` or a constant expression
+                        pass
+                    elif n_is_input:
+                        # n is an input array argument and its shape
+                        # may define variables used in dimension
+                        # specifications.
+                        for v, (solver, deps) in coeffs_and_deps.items():
+                            def compute_deps(v, deps):
+                                for v1 in coeffs_and_deps.get(v, [None, []])[1]:
+                                    if v1 not in deps:
+                                        deps.add(v1)
+                                        compute_deps(v1, deps)
+                            all_deps = set()
+                            compute_deps(v, all_deps)
+                            if ((v in n_deps
+                                 or '=' in vars[v]
+                                 or 'depend' in vars[v])):
+                                # Skip a variable that
+                                # - n depends on
+                                # - has user-defined initialization expression
+                                # - has user-defined dependencies
+                                continue
+                            if solver is not None and v not in all_deps:
+                                # v can be solved from d, hence, we
+                                # make it an optional argument with
+                                # initialization expression:
+                                is_required = False
+                                init = solver(symbolic.as_symbol(
+                                    f'shape({n}, {i})'))
+                                init = init.tostring(
+                                    language=symbolic.Language.C)
+                                vars[v]['='] = init
+                                # n needs to be initialized before v. So,
+                                # making v dependent on n and on any
+                                # variables in solver or d.
+                                vars[v]['depend'] = [n] + deps
+                                if 'check' not in vars[v]:
+                                    # add check only when no
+                                    # user-specified checks exist
+                                    vars[v]['check'] = [
+                                        f'shape({n}, {i}) == {d}']
                             else:
-                                vars[d]['check']=['%slen(%s)%s>=%s'%(pd,n,ad,d)]
-                        if not vars[d].has_key('attrspec'): vars[d]['attrspec']=['optional']
-                        if ('optional' not in vars[d]['attrspec']) and\
-                           ('required' not in vars[d]['attrspec']):
-                            vars[d]['attrspec'].append('optional')
-                    elif d not in ['*',':']:
-                        #/----< no check
-                        #if ni>1: vars[n]['check'].append('shape(%s,%i)==%s'%(n,i,d))
-                        #else: vars[n]['check'].append('len(%s)>=%s'%(n,d))
-                        if flag:
-                            if vars.has_key(d):
-                                if n not in ddeps:
-                                    vars[n]['depend'].append(d)
-                            else:
-                                vars[n]['depend'] = vars[n]['depend'] + ddeps
+                                # d is a non-linear function on v,
+                                # hence, v must be a required input
+                                # argument that n will depend on
+                                is_required = True
+                                if 'intent' not in vars[v]:
+                                    vars[v]['intent'] = []
+                                if 'in' not in vars[v]['intent']:
+                                    vars[v]['intent'].append('in')
+                                # v needs to be initialized before n
+                                n_deps.append(v)
+                                n_checks.append(
+                                    f'shape({n}, {i}) == {d}')
+                            v_attr = vars[v].get('attrspec', [])
+                            if not ('optional' in v_attr
+                                    or 'required' in v_attr):
+                                v_attr.append(
+                                    'required' if is_required else 'optional')
+                            if v_attr:
+                                vars[v]['attrspec'] = v_attr
+                    if coeffs_and_deps is not None:
+                        # extend v dependencies with ones specified in attrspec
+                        for v, (solver, deps) in coeffs_and_deps.items():
+                            v_deps = vars[v].get('depend', [])
+                            for aa in vars[v].get('attrspec', []):
+                                if aa.startswith('depend'):
+                                    aa = ''.join(aa.split())
+                                    v_deps.extend(aa[7:-1].split(','))
+                            if v_deps:
+                                vars[v]['depend'] = list(set(v_deps))
+                            if n not in v_deps:
+                                n_deps.append(v)
             elif isstring(vars[n]):
-                length='1'
-                if vars[n].has_key('charselector'):
-                    if vars[n]['charselector'].has_key('*'):
+                if 'charselector' in vars[n]:
+                    if '*' in vars[n]['charselector']:
                         length = _eval_length(vars[n]['charselector']['*'],
                                               params)
-                        vars[n]['charselector']['*']=length
-                    elif vars[n]['charselector'].has_key('len'):
+                        vars[n]['charselector']['*'] = length
+                    elif 'len' in vars[n]['charselector']:
                         length = _eval_length(vars[n]['charselector']['len'],
                                               params)
                         del vars[n]['charselector']['len']
-                        vars[n]['charselector']['*']=length
-
-            if not vars[n]['check']: del vars[n]['check']
-            if flag and not vars[n]['depend']: del vars[n]['depend']
-        if vars[n].has_key('='):
-            if not vars[n].has_key('attrspec'): vars[n]['attrspec']=[]
+                        vars[n]['charselector']['*'] = length
+            if n_checks:
+                vars[n]['check'] = n_checks
+            if n_deps:
+                vars[n]['depend'] = list(set(n_deps))
+
+        if '=' in vars[n]:
+            if 'attrspec' not in vars[n]:
+                vars[n]['attrspec'] = []
             if ('optional' not in vars[n]['attrspec']) and \
                ('required' not in vars[n]['attrspec']):
                 vars[n]['attrspec'].append('optional')
-            if not vars[n].has_key('depend'):
-                vars[n]['depend']=[]
-                for v,m in dep_matches.items():
-                    if m(vars[n]['=']): vars[n]['depend'].append(v)
-                if not vars[n]['depend']: del vars[n]['depend']
+            if 'depend' not in vars[n]:
+                vars[n]['depend'] = []
+                for v, m in list(dep_matches.items()):
+                    if m(vars[n]['=']):
+                        vars[n]['depend'].append(v)
+                if not vars[n]['depend']:
+                    del vars[n]['depend']
             if isscalar(vars[n]):
-                vars[n]['='] = _eval_scalar(vars[n]['='],params)
-
-    for n in vars.keys():
-        if n==block['name']: # n is block name
-            if vars[n].has_key('note'):
-                block['note']=vars[n]['note']
-            if block['block']=='function':
-                if block.has_key('result') and vars.has_key(block['result']):
-                    vars[n]=appenddecl(vars[n],vars[block['result']])
-                if block.has_key('prefix'):
-                    pr=block['prefix']; ispure=0; isrec=1
-                    pr1=string.replace(pr,'pure','')
-                    ispure=(not pr==pr1)
-                    pr=string.replace(pr1,'recursive','')
-                    isrec=(not pr==pr1)
-                    m=typespattern[0].match(pr)
+                vars[n]['='] = _eval_scalar(vars[n]['='], params)
+
+    for n in list(vars.keys()):
+        if n == block['name']:  # n is block name
+            if 'note' in vars[n]:
+                block['note'] = vars[n]['note']
+            if block['block'] == 'function':
+                if 'result' in block and block['result'] in vars:
+                    vars[n] = appenddecl(vars[n], vars[block['result']])
+                if 'prefix' in block:
+                    pr = block['prefix']
+                    pr1 = pr.replace('pure', '')
+                    ispure = (not pr == pr1)
+                    pr = pr1.replace('recursive', '')
+                    isrec = (not pr == pr1)
+                    m = typespattern[0].match(pr)
                     if m:
-                        typespec,selector,attr,edecl=cracktypespec0(m.group('this'),m.group('after'))
-                        kindselect,charselect,typename=cracktypespec(typespec,selector)
-                        vars[n]['typespec']=typespec
+                        typespec, selector, attr, edecl = cracktypespec0(
+                            m.group('this'), m.group('after'))
+                        kindselect, charselect, typename = cracktypespec(
+                            typespec, selector)
+                        vars[n]['typespec'] = typespec
                         if kindselect:
-                            if kindselect.has_key('kind'):
+                            if 'kind' in kindselect:
                                 try:
-                                    kindselect['kind'] = eval(kindselect['kind'],{},params)
-                                except:
+                                    kindselect['kind'] = eval(
+                                        kindselect['kind'], {}, params)
+                                except Exception:
                                     pass
-                            vars[n]['kindselector']=kindselect
-                        if charselect: vars[n]['charselector']=charselect
-                        if typename: vars[n]['typename']=typename
-                        if ispure: vars[n]=setattrspec(vars[n],'pure')
-                        if isrec: vars[n]=setattrspec(vars[n],'recursive')
+                            vars[n]['kindselector'] = kindselect
+                        if charselect:
+                            vars[n]['charselector'] = charselect
+                        if typename:
+                            vars[n]['typename'] = typename
+                        if ispure:
+                            vars[n] = setattrspec(vars[n], 'pure')
+                        if isrec:
+                            vars[n] = setattrspec(vars[n], 'recursive')
                     else:
-                        outmess('analyzevars: prefix (%s) were not used\n'%`block['prefix']`)
-    if not block['block'] in ['module','pythonmodule','python module','block data']:
-        if block.has_key('commonvars'):
-            neededvars=copy.copy(block['args']+block['commonvars'])
+                        outmess(
+                            'analyzevars: prefix (%s) were not used\n' % repr(block['prefix']))
+    if not block['block'] in ['module', 'pythonmodule', 'python module', 'block data']:
+        if 'commonvars' in block:
+            neededvars = copy.copy(block['args'] + block['commonvars'])
         else:
-            neededvars=copy.copy(block['args'])
-        for n in vars.keys():
-            if l_or(isintent_callback,isintent_aux)(vars[n]):
+            neededvars = copy.copy(block['args'])
+        for n in list(vars.keys()):
+            if l_or(isintent_callback, isintent_aux)(vars[n]):
                 neededvars.append(n)
-        if block.has_key('entry'):
-            neededvars.extend(block['entry'].keys())
-            for k in block['entry'].keys():
+        if 'entry' in block:
+            neededvars.extend(list(block['entry'].keys()))
+            for k in list(block['entry'].keys()):
                 for n in block['entry'][k]:
                     if n not in neededvars:
                         neededvars.append(n)
-        if block['block']=='function':
-            if block.has_key('result'):
+        if block['block'] == 'function':
+            if 'result' in block:
                 neededvars.append(block['result'])
             else:
                 neededvars.append(block['name'])
-        if block['block'] in ['subroutine','function']:
+        if block['block'] in ['subroutine', 'function']:
             name = block['name']
-            if vars.has_key(name) and vars[name].has_key('intent'):
+            if name in vars and 'intent' in vars[name]:
                 block['intent'] = vars[name]['intent']
         if block['block'] == 'type':
-            neededvars.extend(vars.keys())
-        for n in vars.keys():
+            neededvars.extend(list(vars.keys()))
+        for n in list(vars.keys()):
             if n not in neededvars:
                 del vars[n]
     return vars
-analyzeargs_re_1 = re.compile(r'\A[a-z]+[\w$]*\Z',re.I)
+
+analyzeargs_re_1 = re.compile(r'\A[a-z]+[\w$]*\Z', re.I)
+
+
+def expr2name(a, block, args=[]):
+    orig_a = a
+    a_is_expr = not analyzeargs_re_1.match(a)
+    if a_is_expr:  # `a` is an expression
+        implicitrules, attrrules = buildimplicitrules(block)
+        at = determineexprtype(a, block['vars'], implicitrules)
+        na = 'e_'
+        for c in a:
+            c = c.lower()
+            if c not in string.ascii_lowercase + string.digits:
+                c = '_'
+            na = na + c
+        if na[-1] == '_':
+            na = na + 'e'
+        else:
+            na = na + '_e'
+        a = na
+        while a in block['vars'] or a in block['args']:
+            a = a + 'r'
+    if a in args:
+        k = 1
+        while a + str(k) in args:
+            k = k + 1
+        a = a + str(k)
+    if a_is_expr:
+        block['vars'][a] = at
+    else:
+        if a not in block['vars']:
+            if orig_a in block['vars']:
+                block['vars'][a] = block['vars'][orig_a]
+            else:
+                block['vars'][a] = {}
+        if 'externals' in block and orig_a in block['externals'] + block['interfaced']:
+            block['vars'][a] = setattrspec(block['vars'][a], 'external')
+    return a
+
+
 def analyzeargs(block):
     setmesstext(block)
-    implicitrules,attrrules=buildimplicitrules(block)
-    if not block.has_key('args'): block['args']=[]
-    args=[]
-    re_1 = analyzeargs_re_1
+    implicitrules, _ = buildimplicitrules(block)
+    if 'args' not in block:
+        block['args'] = []
+    args = []
     for a in block['args']:
-        if not re_1.match(a): # `a` is an expression
-            at=determineexprtype(a,block['vars'],implicitrules)
-            na='e_'
-            for c in a:
-                if c not in string.lowercase+string.digits: c='_'
-                na=na+c
-            if na[-1]=='_': na=na+'e'
-            else: na=na+'_e'
-            a=na
-            while block['vars'].has_key(a) or a in block['args']: a=a+'r'
-            block['vars'][a]=at
+        a = expr2name(a, block, args)
         args.append(a)
-        if not block['vars'].has_key(a):
-            block['vars'][a]={}
-        if block.has_key('externals') and a in block['externals']+block['interfaced']:
-            block['vars'][a]=setattrspec(block['vars'][a],'external')
-    block['args']=args
-
-    if block.has_key('entry'):
-        for k,args1 in block['entry'].items():
+    block['args'] = args
+    if 'entry' in block:
+        for k, args1 in list(block['entry'].items()):
             for a in args1:
-                if not block['vars'].has_key(a):
-                    block['vars'][a]={}
+                if a not in block['vars']:
+                    block['vars'][a] = {}
 
     for b in block['body']:
         if b['name'] in args:
-            if not block.has_key('externals'): block['externals']=[]
+            if 'externals' not in block:
+                block['externals'] = []
             if b['name'] not in block['externals']:
                 block['externals'].append(b['name'])
-    if block.has_key('result') and not block['vars'].has_key(block['result']):
-        block['vars'][block['result']]={}
+    if 'result' in block and block['result'] not in block['vars']:
+        block['vars'][block['result']] = {}
     return block
-determineexprtype_re_1 = re.compile(r'\A\(.+?[,].+?\)\Z',re.I)
-determineexprtype_re_2 = re.compile(r'\A[+-]?\d+(_(P<name>[\w]+)|)\Z',re.I)
-determineexprtype_re_3 = re.compile(r'\A[+-]?[\d.]+[\d+-de.]*(_(P<name>[\w]+)|)\Z',re.I)
-determineexprtype_re_4 = re.compile(r'\A\(.*\)\Z',re.I)
-determineexprtype_re_5 = re.compile(r'\A(?P<name>\w+)\s*\(.*?\)\s*\Z',re.I)
+
+determineexprtype_re_1 = re.compile(r'\A\(.+?,.+?\)\Z', re.I)
+determineexprtype_re_2 = re.compile(r'\A[+-]?\d+(_(?P<name>\w+)|)\Z', re.I)
+determineexprtype_re_3 = re.compile(
+    r'\A[+-]?[\d.]+[-\d+de.]*(_(?P<name>\w+)|)\Z', re.I)
+determineexprtype_re_4 = re.compile(r'\A\(.*\)\Z', re.I)
+determineexprtype_re_5 = re.compile(r'\A(?P<name>\w+)\s*\(.*?\)\s*\Z', re.I)
+
+
 def _ensure_exprdict(r):
-    if type(r) is type(0):
-        return {'typespec':'integer'}
-    if type(r) is type(0.0):
-        return {'typespec':'real'}
-    if type(r) is type(0j):
-        return {'typespec':'complex'}
-    assert type(r) is type({}),`r`
-    return r
-
-def determineexprtype(expr,vars,rules={}):
-    if vars.has_key(expr):
+    if isinstance(r, int):
+        return {'typespec': 'integer'}
+    if isinstance(r, float):
+        return {'typespec': 'real'}
+    if isinstance(r, complex):
+        return {'typespec': 'complex'}
+    if isinstance(r, dict):
+        return r
+    raise AssertionError(repr(r))
+
+
+def determineexprtype(expr, vars, rules={}):
+    if expr in vars:
         return _ensure_exprdict(vars[expr])
-    expr=string.strip(expr)
+    expr = expr.strip()
     if determineexprtype_re_1.match(expr):
-        return {'typespec':'complex'}
-    m=determineexprtype_re_2.match(expr)
+        return {'typespec': 'complex'}
+    m = determineexprtype_re_2.match(expr)
     if m:
-        if m.groupdict().has_key('name') and m.group('name'):
-            outmess('determineexprtype: selected kind types not supported (%s)\n'%`expr`)
-        return {'typespec':'integer'}
+        if 'name' in m.groupdict() and m.group('name'):
+            outmess(
+                'determineexprtype: selected kind types not supported (%s)\n' % repr(expr))
+        return {'typespec': 'integer'}
     m = determineexprtype_re_3.match(expr)
     if m:
-        if m.groupdict().has_key('name') and m.group('name'):
-            outmess('determineexprtype: selected kind types not supported (%s)\n'%`expr`)
-        return {'typespec':'real'}
-    for op in ['+','-','*','/']:
-        for e in map(string.strip,string.split(markoutercomma(expr,comma=op),'@'+op+'@')):
-            if vars.has_key(e):
+        if 'name' in m.groupdict() and m.group('name'):
+            outmess(
+                'determineexprtype: selected kind types not supported (%s)\n' % repr(expr))
+        return {'typespec': 'real'}
+    for op in ['+', '-', '*', '/']:
+        for e in [x.strip() for x in markoutercomma(expr, comma=op).split('@' + op + '@')]:
+            if e in vars:
                 return _ensure_exprdict(vars[e])
-    t={}
-    if determineexprtype_re_4.match(expr): # in parenthesis
-        t=determineexprtype(expr[1:-1],vars,rules)
+    t = {}
+    if determineexprtype_re_4.match(expr):  # in parenthesis
+        t = determineexprtype(expr[1:-1], vars, rules)
     else:
         m = determineexprtype_re_5.match(expr)
         if m:
-            rn=m.group('name')
-            t=determineexprtype(m.group('name'),vars,rules)
-            if t and t.has_key('attrspec'): del t['attrspec']
+            rn = m.group('name')
+            t = determineexprtype(m.group('name'), vars, rules)
+            if t and 'attrspec' in t:
+                del t['attrspec']
             if not t:
-                if rules.has_key(rn[0]):
+                if rn[0] in rules:
                     return _ensure_exprdict(rules[rn[0]])
     if expr[0] in '\'"':
-        return {'typespec':'character','charselector':{'*':'*'}}
+        return {'typespec': 'character', 'charselector': {'*': '*'}}
     if not t:
-        outmess('determineexprtype: could not determine expressions (%s) type.\n'%(`expr`))
+        outmess(
+            'determineexprtype: could not determine expressions (%s) type.\n' % (repr(expr)))
     return t
+
 ######
-def crack2fortrangen(block,tab='\n'):
+
+
+def crack2fortrangen(block, tab='\n', as_interface=False):
     global skipfuncs, onlyfuncs
+
     setmesstext(block)
-    ret=''
-    if type(block) is type([]):
+    ret = ''
+    if isinstance(block, list):
         for g in block:
-            if g['block'] in ['function','subroutine']:
+            if g and g['block'] in ['function', 'subroutine']:
                 if g['name'] in skipfuncs:
                     continue
                 if onlyfuncs and g['name'] not in onlyfuncs:
                     continue
-            ret=ret+crack2fortrangen(g,tab)
+            ret = ret + crack2fortrangen(g, tab, as_interface=as_interface)
         return ret
-    prefix=''
-    name=''
-    args=''
-    blocktype=block['block']
-    if blocktype=='program': return ''
-    al=[]
-    if block.has_key('name'): name=block['name']
-    if block.has_key('args'):
+    prefix = ''
+    name = ''
+    args = ''
+    blocktype = block['block']
+    if blocktype == 'program':
+        return ''
+    argsl = []
+    if 'name' in block:
+        name = block['name']
+    if 'args' in block:
         vars = block['vars']
-        al = [a for a in block['args'] if not isintent_callback(vars[a])]
-        if block['block']=='function' or al:
-            args='(%s)'%string.join(al,',')
+        for a in block['args']:
+            a = expr2name(a, block, argsl)
+            if not isintent_callback(vars[a]):
+                argsl.append(a)
+        if block['block'] == 'function' or argsl:
+            args = '(%s)' % ','.join(argsl)
     f2pyenhancements = ''
-    if block.has_key('f2pyenhancements'):
-        for k in block['f2pyenhancements'].keys():
-            f2pyenhancements = '%s%s%s %s'%(f2pyenhancements,tab+tabchar,k,block['f2pyenhancements'][k])
-    intent_lst = block.get('intent',[])[:]
-    if blocktype=='function' and 'callback' in intent_lst:
+    if 'f2pyenhancements' in block:
+        for k in list(block['f2pyenhancements'].keys()):
+            f2pyenhancements = '%s%s%s %s' % (
+                f2pyenhancements, tab + tabchar, k, block['f2pyenhancements'][k])
+    intent_lst = block.get('intent', [])[:]
+    if blocktype == 'function' and 'callback' in intent_lst:
         intent_lst.remove('callback')
     if intent_lst:
-        f2pyenhancements = '%s%sintent(%s) %s'%\
-                           (f2pyenhancements,tab+tabchar,
-                            string.join(intent_lst,','),name)
-    use=''
-    if block.has_key('use'):
-        use=use2fortran(block['use'],tab+tabchar)
-    common=''
-    if block.has_key('common'):
-        common=common2fortran(block['common'],tab+tabchar)
-    if name=='unknown_interface': name=''
-    result=''
-    if block.has_key('result'):
-        result=' result (%s)'%block['result']
-        if block['result'] not in al:
-            al.append(block['result'])
-    #if block.has_key('prefix'): prefix=block['prefix']+' '
-    body=crack2fortrangen(block['body'],tab+tabchar)
-    vars=vars2fortran(block,block['vars'],al,tab+tabchar)
-    mess=''
-    if block.has_key('from'):
-        mess='! in %s'%block['from']
-    if block.has_key('entry'):
+        f2pyenhancements = '%s%sintent(%s) %s' %\
+                           (f2pyenhancements, tab + tabchar,
+                            ','.join(intent_lst), name)
+    use = ''
+    if 'use' in block:
+        use = use2fortran(block['use'], tab + tabchar)
+    common = ''
+    if 'common' in block:
+        common = common2fortran(block['common'], tab + tabchar)
+    if name == 'unknown_interface':
+        name = ''
+    result = ''
+    if 'result' in block:
+        result = ' result (%s)' % block['result']
+        if block['result'] not in argsl:
+            argsl.append(block['result'])
+    body = crack2fortrangen(block['body'], tab + tabchar, as_interface=as_interface)
+    vars = vars2fortran(
+        block, block['vars'], argsl, tab + tabchar, as_interface=as_interface)
+    mess = ''
+    if 'from' in block and not as_interface:
+        mess = '! in %s' % block['from']
+    if 'entry' in block:
         entry_stmts = ''
-        for k,i in block['entry'].items():
+        for k, i in list(block['entry'].items()):
             entry_stmts = '%s%sentry %s(%s)' \
-                          % (entry_stmts,tab+tabchar,k,string.join(i,','))
+                          % (entry_stmts, tab + tabchar, k, ','.join(i))
         body = body + entry_stmts
-    if blocktype=='block data' and name=='_BLOCK_DATA_':
+    if blocktype == 'block data' and name == '_BLOCK_DATA_':
         name = ''
-    ret='%s%s%s %s%s%s %s%s%s%s%s%s%send %s %s'%(tab,prefix,blocktype,name,args,result,mess,f2pyenhancements,use,vars,common,body,tab,blocktype,name)
+    ret = '%s%s%s %s%s%s %s%s%s%s%s%s%send %s %s' % (
+        tab, prefix, blocktype, name, args, result, mess, f2pyenhancements, use, vars, common, body, tab, blocktype, name)
     return ret
-def common2fortran(common,tab=''):
-    ret=''
-    for k in common.keys():
-        if k=='_BLNK_':
-            ret='%s%scommon %s'%(ret,tab,string.join(common[k],','))
+
+
+def common2fortran(common, tab=''):
+    ret = ''
+    for k in list(common.keys()):
+        if k == '_BLNK_':
+            ret = '%s%scommon %s' % (ret, tab, ','.join(common[k]))
         else:
-            ret='%s%scommon /%s/ %s'%(ret,tab,k,string.join(common[k],','))
+            ret = '%s%scommon /%s/ %s' % (ret, tab, k, ','.join(common[k]))
     return ret
-def use2fortran(use,tab=''):
-    ret=''
-    for m in use.keys():
-        ret='%s%suse %s,'%(ret,tab,m)
-        if use[m]=={}:
-            if ret and ret[-1]==',': ret=ret[:-1]
+
+
+def use2fortran(use, tab=''):
+    ret = ''
+    for m in list(use.keys()):
+        ret = '%s%suse %s,' % (ret, tab, m)
+        if use[m] == {}:
+            if ret and ret[-1] == ',':
+                ret = ret[:-1]
             continue
-        if use[m].has_key('only') and use[m]['only']:
-            ret='%s,only:'%(ret)
-        if use[m].has_key('map') and use[m]['map']:
-            c=' '
-            for k in use[m]['map'].keys():
-                if k==use[m]['map'][k]:
-                    ret='%s%s%s'%(ret,c,k); c=','
+        if 'only' in use[m] and use[m]['only']:
+            ret = '%s only:' % (ret)
+        if 'map' in use[m] and use[m]['map']:
+            c = ' '
+            for k in list(use[m]['map'].keys()):
+                if k == use[m]['map'][k]:
+                    ret = '%s%s%s' % (ret, c, k)
+                    c = ','
                 else:
-                    ret='%s%s%s=>%s'%(ret,c,k,use[m]['map'][k]); c=','
-        if ret and ret[-1]==',': ret=ret[:-1]
+                    ret = '%s%s%s=>%s' % (ret, c, k, use[m]['map'][k])
+                    c = ','
+        if ret and ret[-1] == ',':
+            ret = ret[:-1]
     return ret
+
+
 def true_intent_list(var):
     lst = var['intent']
     ret = []
     for intent in lst:
         try:
-            exec('c = isintent_%s(var)' % intent)
-        except NameError:
-            c = 0
-        if c:
-            ret.append(intent)
+            f = globals()['isintent_%s' % intent]
+        except KeyError:
+            pass
+        else:
+            if f(var):
+                ret.append(intent)
     return ret
-def vars2fortran(block,vars,args,tab=''):
+
+
+def vars2fortran(block, vars, args, tab='', as_interface=False):
     """
     TODO:
     public sub
     ...
     """
     setmesstext(block)
-    ret=''
-    nout=[]
+    ret = ''
+    nout = []
     for a in args:
-        if block['vars'].has_key(a): nout.append(a)
-    if block.has_key('commonvars'):
+        if a in block['vars']:
+            nout.append(a)
+    if 'commonvars' in block:
         for a in block['commonvars']:
-            if vars.has_key(a):
-                if a not in nout: nout.append(a)
-            else: errmess('vars2fortran: Confused?!: "%s" is not defined in vars.\n'%a)
-    if block.has_key('varnames'):
+            if a in vars:
+                if a not in nout:
+                    nout.append(a)
+            else:
+                errmess(
+                    'vars2fortran: Confused?!: "%s" is not defined in vars.\n' % a)
+    if 'varnames' in block:
         nout.extend(block['varnames'])
-    for a in vars.keys():
-        if a not in nout: nout.append(a)
+    if not as_interface:
+        for a in list(vars.keys()):
+            if a not in nout:
+                nout.append(a)
     for a in nout:
-        if vars[a].has_key('depend'):
+        if 'depend' in vars[a]:
             for d in vars[a]['depend']:
-                if vars.has_key(d) and vars[d].has_key('depend') and a in vars[d]['depend']:
-                    errmess('vars2fortran: Warning: cross-dependence between variables "%s" and "%s"\n'%(a,d))
-        if block.has_key('externals') and a in block['externals']:
+                if d in vars and 'depend' in vars[d] and a in vars[d]['depend']:
+                    errmess(
+                        'vars2fortran: Warning: cross-dependence between variables "%s" and "%s"\n' % (a, d))
+        if 'externals' in block and a in block['externals']:
             if isintent_callback(vars[a]):
-                ret='%s%sintent(callback) %s'%(ret,tab,a)
-            ret='%s%sexternal %s'%(ret,tab,a)
+                ret = '%s%sintent(callback) %s' % (ret, tab, a)
+            ret = '%s%sexternal %s' % (ret, tab, a)
             if isoptional(vars[a]):
-                ret='%s%soptional %s'%(ret,tab,a)
-            if vars.has_key(a) and not vars[a].has_key('typespec'):
+                ret = '%s%soptional %s' % (ret, tab, a)
+            if a in vars and 'typespec' not in vars[a]:
                 continue
-            cont=1
+            cont = 1
             for b in block['body']:
-                if a==b['name'] and b['block']=='function': cont=0;break
-            if cont: continue
-        if not vars.has_key(a):
+                if a == b['name'] and b['block'] == 'function':
+                    cont = 0
+                    break
+            if cont:
+                continue
+        if a not in vars:
             show(vars)
-            outmess('vars2fortran: No definition for argument "%s".\n'%a)
+            outmess('vars2fortran: No definition for argument "%s".\n' % a)
             continue
-        if a==block['name'] and not block['block']=='function':
-            continue
-        if not vars[a].has_key('typespec'):
-            if vars[a].has_key('attrspec') and 'external' in vars[a]['attrspec']:
+        if a == block['name']:
+            if block['block'] != 'function' or block.get('result'):
+                # 1) skip declaring a variable that name matches with
+                #    subroutine name
+                # 2) skip declaring function when its type is
+                #    declared via `result` construction
+                continue
+        if 'typespec' not in vars[a]:
+            if 'attrspec' in vars[a] and 'external' in vars[a]['attrspec']:
                 if a in args:
-                    ret='%s%sexternal %s'%(ret,tab,a)
+                    ret = '%s%sexternal %s' % (ret, tab, a)
                 continue
             show(vars[a])
-            outmess('vars2fortran: No typespec for argument "%s".\n'%a)
+            outmess('vars2fortran: No typespec for argument "%s".\n' % a)
             continue
-        vardef=vars[a]['typespec']
-        if vardef=='type' and vars[a].has_key('typename'):
-            vardef='%s(%s)'%(vardef,vars[a]['typename'])
-        selector={}
-        if vars[a].has_key('kindselector'): selector=vars[a]['kindselector']
-        elif vars[a].has_key('charselector'): selector=vars[a]['charselector']
-        if selector.has_key('*'):
-            if selector['*'] in ['*',':']:
-                vardef='%s*(%s)'%(vardef,selector['*'])
+        vardef = vars[a]['typespec']
+        if vardef == 'type' and 'typename' in vars[a]:
+            vardef = '%s(%s)' % (vardef, vars[a]['typename'])
+        selector = {}
+        if 'kindselector' in vars[a]:
+            selector = vars[a]['kindselector']
+        elif 'charselector' in vars[a]:
+            selector = vars[a]['charselector']
+        if '*' in selector:
+            if selector['*'] in ['*', ':']:
+                vardef = '%s*(%s)' % (vardef, selector['*'])
             else:
-                vardef='%s*%s'%(vardef,selector['*'])
+                vardef = '%s*%s' % (vardef, selector['*'])
         else:
-            if selector.has_key('len'):
-                vardef='%s(len=%s'%(vardef,selector['len'])
-                if selector.has_key('kind'):
-                    vardef='%s,kind=%s)'%(vardef,selector['kind'])
+            if 'len' in selector:
+                vardef = '%s(len=%s' % (vardef, selector['len'])
+                if 'kind' in selector:
+                    vardef = '%s,kind=%s)' % (vardef, selector['kind'])
                 else:
-                    vardef='%s)'%(vardef)
-            elif selector.has_key('kind'):
-                vardef='%s(kind=%s)'%(vardef,selector['kind'])
-        c=' '
-        if vars[a].has_key('attrspec'):
-            attr=[]
-            for l in vars[a]['attrspec']:
-                if l not in ['external']:
-                    attr.append(l)
+                    vardef = '%s)' % (vardef)
+            elif 'kind' in selector:
+                vardef = '%s(kind=%s)' % (vardef, selector['kind'])
+        c = ' '
+        if 'attrspec' in vars[a]:
+            attr = [l for l in vars[a]['attrspec']
+                    if l not in ['external']]
             if attr:
-                vardef='%s %s'%(vardef,string.join(attr,','))
-                c=','
-        if vars[a].has_key('dimension'):
-#             if not isintent_c(vars[a]):
-#                 vars[a]['dimension'].reverse()
-            vardef='%s%sdimension(%s)'%(vardef,c,string.join(vars[a]['dimension'],','))
-            c=','
-        if vars[a].has_key('intent'):
+                vardef = '%s, %s' % (vardef, ','.join(attr))
+                c = ','
+        if 'dimension' in vars[a]:
+            vardef = '%s%sdimension(%s)' % (
+                vardef, c, ','.join(vars[a]['dimension']))
+            c = ','
+        if 'intent' in vars[a]:
             lst = true_intent_list(vars[a])
             if lst:
-                vardef='%s%sintent(%s)'%(vardef,c,string.join(lst,','))
-            c=','
-        if vars[a].has_key('check'):
-            vardef='%s%scheck(%s)'%(vardef,c,string.join(vars[a]['check'],','))
-            c=','
-        if vars[a].has_key('depend'):
-            vardef='%s%sdepend(%s)'%(vardef,c,string.join(vars[a]['depend'],','))
-            c=','
-        if vars[a].has_key('='):
+                vardef = '%s%sintent(%s)' % (vardef, c, ','.join(lst))
+            c = ','
+        if 'check' in vars[a]:
+            vardef = '%s%scheck(%s)' % (vardef, c, ','.join(vars[a]['check']))
+            c = ','
+        if 'depend' in vars[a]:
+            vardef = '%s%sdepend(%s)' % (
+                vardef, c, ','.join(vars[a]['depend']))
+            c = ','
+        if '=' in vars[a]:
             v = vars[a]['=']
-            if vars[a]['typespec'] in ['complex','double complex']:
+            if vars[a]['typespec'] in ['complex', 'double complex']:
                 try:
                     v = eval(v)
-                    v = '(%s,%s)' % (v.real,v.imag)
-                except:
+                    v = '(%s,%s)' % (v.real, v.imag)
+                except Exception:
                     pass
-            vardef='%s :: %s=%s'%(vardef,a,v)
+            vardef = '%s :: %s=%s' % (vardef, a, v)
         else:
-            vardef='%s :: %s'%(vardef,a)
-        ret='%s%s%s'%(ret,tab,vardef)
+            vardef = '%s :: %s' % (vardef, a)
+        ret = '%s%s%s' % (ret, tab, vardef)
     return ret
 ######
 
+
 def crackfortran(files):
     global usermodules
-    outmess('Reading fortran codes...\n',0)
-    readfortrancode(files,crackline)
-    outmess('Post-processing...\n',0)
-    usermodules=[]
-    postlist=postcrack(grouplist[0])
-    outmess('Post-processing (stage 2)...\n',0)
-    postlist=postcrack2(postlist)
-    return usermodules+postlist
+
+    outmess('Reading fortran codes...\n', 0)
+    readfortrancode(files, crackline)
+    outmess('Post-processing...\n', 0)
+    usermodules = []
+    postlist = postcrack(grouplist[0])
+    outmess('Post-processing (stage 2)...\n', 0)
+    postlist = postcrack2(postlist)
+    return usermodules + postlist
+
+
 def crack2fortran(block):
     global f2py_version
-    pyf=crack2fortrangen(block)+'\n'
-    header="""!    -*- f90 -*-
+
+    pyf = crack2fortrangen(block) + '\n'
+    header = """!    -*- f90 -*-
 ! Note: the context of this file is case sensitive.
 """
-    footer="""
+    footer = """
 ! This file was auto-generated with f2py (version:%s).
-! See http://cens.ioc.ee/projects/f2py2e/
-"""%(f2py_version)
-    return header+pyf+footer
+! See:
+! https://web.archive.org/web/20140822061353/http://cens.ioc.ee/projects/f2py2e
+""" % (f2py_version)
+    return header + pyf + footer
 
 if __name__ == "__main__":
-    files=[]
-    funcs=[]
-    f=1;f2=0;f3=0
-    showblocklist=0
+    files = []
+    funcs = []
+    f = 1
+    f2 = 0
+    f3 = 0
+    showblocklist = 0
     for l in sys.argv[1:]:
-        if l=='': pass
-        elif l[0]==':':
-            f=0
-        elif l=='-quiet':
-            quiet=1
-            verbose=0
-        elif l=='-verbose':
-            verbose=2
-            quiet=0
-        elif l=='-fix':
+        if l == '':
+            pass
+        elif l[0] == ':':
+            f = 0
+        elif l == '-quiet':
+            quiet = 1
+            verbose = 0
+        elif l == '-verbose':
+            verbose = 2
+            quiet = 0
+        elif l == '-fix':
             if strictf77:
-                outmess('Use option -f90 before -fix if Fortran 90 code is in fix form.\n',0)
-            skipemptyends=1
-            sourcecodeform='fix'
-        elif l=='-skipemptyends':
-            skipemptyends=1
-        elif l=='--ignore-contains':
-            ignorecontains=1
-        elif l=='-f77':
-            strictf77=1
-            sourcecodeform='fix'
-        elif l=='-f90':
-            strictf77=0
-            sourcecodeform='free'
-            skipemptyends=1
-        elif l=='-h':
-            f2=1
-        elif l=='-show':
-            showblocklist=1
-        elif l=='-m':
-            f3=1
-        elif l[0]=='-':
-            errmess('Unknown option %s\n'%`l`)
+                outmess(
+                    'Use option -f90 before -fix if Fortran 90 code is in fix form.\n', 0)
+            skipemptyends = 1
+            sourcecodeform = 'fix'
+        elif l == '-skipemptyends':
+            skipemptyends = 1
+        elif l == '--ignore-contains':
+            ignorecontains = 1
+        elif l == '-f77':
+            strictf77 = 1
+            sourcecodeform = 'fix'
+        elif l == '-f90':
+            strictf77 = 0
+            sourcecodeform = 'free'
+            skipemptyends = 1
+        elif l == '-h':
+            f2 = 1
+        elif l == '-show':
+            showblocklist = 1
+        elif l == '-m':
+            f3 = 1
+        elif l[0] == '-':
+            errmess('Unknown option %s\n' % repr(l))
         elif f2:
-            f2=0
-            pyffilename=l
+            f2 = 0
+            pyffilename = l
         elif f3:
-            f3=0
-            f77modulename=l
+            f3 = 0
+            f77modulename = l
         elif f:
             try:
                 open(l).close()
                 files.append(l)
-            except IOError,detail:
-                errmess('IOError: %s\n'%str(detail))
+            except OSError as detail:
+                errmess(f'OSError: {detail!s}\n')
         else:
             funcs.append(l)
     if not strictf77 and f77modulename and not skipemptyends:
         outmess("""\
-  Warning: You have specifyied module name for non Fortran 77 code
+  Warning: You have specified module name for non Fortran 77 code
   that should not need one (expect if you are scanning F90 code
   for non module blocks but then you should use flag -skipemptyends
   and also be sure that the files do not contain programs without program statement).
-""",0)
-
-    postlist=crackfortran(files,funcs)
+""", 0)
+
+    postlist = crackfortran(files)
     if pyffilename:
-        outmess('Writing fortran code to file %s\n'%`pyffilename`,0)
-        pyf=crack2fortran(postlist)
-        f=open(pyffilename,'w')
-        f.write(pyf)
-        f.close()
+        outmess('Writing fortran code to file %s\n' % repr(pyffilename), 0)
+        pyf = crack2fortran(postlist)
+        with open(pyffilename, 'w') as f: 
+            f.write(pyf)
     if showblocklist:
         show(postlist)
('numpy/f2py', 'cb_rules.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,4 +1,4 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 """
 
 Build call-back mechanism for f2py2e.
@@ -11,519 +11,630 @@
 NO WARRANTY IS EXPRESSED OR IMPLIED.  USE AT YOUR OWN RISK.
 $Date: 2005/07/20 11:27:58 $
 Pearu Peterson
+
 """
-
-__version__ = "$Revision: 1.53 $"[10:-1]
-
-import __version__
+from . import __version__
+from .auxfuncs import (
+    applyrules, debugcapi, dictappend, errmess, getargs, hasnote, isarray,
+    iscomplex, iscomplexarray, iscomplexfunction, isfunction, isintent_c,
+    isintent_hide, isintent_in, isintent_inout, isintent_nothide,
+    isintent_out, isoptional, isrequired, isscalar, isstring,
+    isstringfunction, issubroutine, l_and, l_not, l_or, outmess, replace,
+    stripcomma, throw_error
+)
+from . import cfuncs
+
 f2py_version = __version__.version
 
 
-import pprint
-import sys,string,time,types,copy
-errmess=sys.stderr.write
-outmess=sys.stdout.write
-show=pprint.pprint
-
-from auxfuncs import *
-import capi_maps
-#from capi_maps import *
-import cfuncs
-
 ################## Rules for callback function ##############
 
-cb_routine_rules={
-    'cbtypedefs':'typedef #rctype#(*#name#_typedef)(#optargs_td##args_td##strarglens_td##noargs#);',
-    'body':"""
+cb_routine_rules = {
+    'cbtypedefs': 'typedef #rctype#(*#name#_typedef)(#optargs_td##args_td##strarglens_td##noargs#);',
+    'body': """
 #begintitle#
-PyObject *#name#_capi = NULL;/*was Py_None*/
-PyTupleObject *#name#_args_capi = NULL;
-int #name#_nofargs = 0;
-jmp_buf #name#_jmpbuf;
+typedef struct {
+    PyObject *capi;
+    PyTupleObject *args_capi;
+    int nofargs;
+    jmp_buf jmpbuf;
+} #name#_t;
+
+#if defined(F2PY_THREAD_LOCAL_DECL) && !defined(F2PY_USE_PYTHON_TLS)
+
+static F2PY_THREAD_LOCAL_DECL #name#_t *_active_#name# = NULL;
+
+static #name#_t *swap_active_#name#(#name#_t *ptr) {
+    #name#_t *prev = _active_#name#;
+    _active_#name# = ptr;
+    return prev;
+}
+
+static #name#_t *get_active_#name#(void) {
+    return _active_#name#;
+}
+
+#else
+
+static #name#_t *swap_active_#name#(#name#_t *ptr) {
+    char *key = "__f2py_cb_#name#";
+    return (#name#_t *)F2PySwapThreadLocalCallbackPtr(key, ptr);
+}
+
+static #name#_t *get_active_#name#(void) {
+    char *key = "__f2py_cb_#name#";
+    return (#name#_t *)F2PyGetThreadLocalCallbackPtr(key);
+}
+
+#endif
+
 /*typedef #rctype#(*#name#_typedef)(#optargs_td##args_td##strarglens_td##noargs#);*/
 #static# #rctype# #callbackname# (#optargs##args##strarglens##noargs#) {
-\tPyTupleObject *capi_arglist = #name#_args_capi;
-\tPyObject *capi_return = NULL;
-\tPyObject *capi_tmp = NULL;
-\tint capi_j,capi_i = 0;
-\tint capi_longjmp_ok = 1;
+    #name#_t cb_local = { NULL, NULL, 0 };
+    #name#_t *cb = NULL;
+    PyTupleObject *capi_arglist = NULL;
+    PyObject *capi_return = NULL;
+    PyObject *capi_tmp = NULL;
+    PyObject *capi_arglist_list = NULL;
+    int capi_j,capi_i = 0;
+    int capi_longjmp_ok = 1;
 #decl#
 #ifdef F2PY_REPORT_ATEXIT
 f2py_cb_start_clock();
 #endif
-\tCFUNCSMESS(\"cb:Call-back function #name# (maxnofargs=#maxnofargs#(-#nofoptargs#))\\n\");
-\tCFUNCSMESSPY(\"cb:#name#_capi=\",#name#_capi);
-\tif (#name#_capi==NULL) {
-\t\tcapi_longjmp_ok = 0;
-\t\t#name#_capi = PyObject_GetAttrString(#modulename#_module,\"#argname#\");
-\t}
-\tif (#name#_capi==NULL) {
-\t\tPyErr_SetString(#modulename#_error,\"cb: Callback #argname# not defined (as an argument or module #modulename# attribute).\\n\");
-\t\tgoto capi_fail;
-\t}
-\tif (PyCObject_Check(#name#_capi)) {
-\t#name#_typedef #name#_cptr;
-\t#name#_cptr = PyCObject_AsVoidPtr(#name#_capi);
-\t#returncptr#(*#name#_cptr)(#optargs_nm##args_nm#);
-\t#return#
-\t}
-\tif (capi_arglist==NULL) {
-\t\tcapi_longjmp_ok = 0;
-\t\tcapi_tmp = PyObject_GetAttrString(#modulename#_module,\"#argname#_extra_args\");
-\t\tif (capi_tmp) {
-\t\t\tcapi_arglist = (PyTupleObject *)PySequence_Tuple(capi_tmp);
-\t\t\tif (capi_arglist==NULL) {
-\t\t\t\tPyErr_SetString(#modulename#_error,\"Failed to convert #modulename#.#argname#_extra_args to tuple.\\n\");
-\t\t\t\tgoto capi_fail;
-\t\t\t}
-\t\t} else {
-\t\t\tPyErr_Clear();
-\t\t\tcapi_arglist = (PyTupleObject *)Py_BuildValue(\"()\");
-\t\t}
-\t}
-\tif (capi_arglist == NULL) {
-\t\tPyErr_SetString(#modulename#_error,\"Callback #argname# argument list is not set.\\n\");
-\t\tgoto capi_fail;
-\t}
+    cb = get_active_#name#();
+    if (cb == NULL) {
+        capi_longjmp_ok = 0;
+        cb = &cb_local;
+    }
+    capi_arglist = cb->args_capi;
+    CFUNCSMESS(\"cb:Call-back function #name# (maxnofargs=#maxnofargs#(-#nofoptargs#))\\n\");
+    CFUNCSMESSPY(\"cb:#name#_capi=\",cb->capi);
+    if (cb->capi==NULL) {
+        capi_longjmp_ok = 0;
+        cb->capi = PyObject_GetAttrString(#modulename#_module,\"#argname#\");
+        CFUNCSMESSPY(\"cb:#name#_capi=\",cb->capi);
+    }
+    if (cb->capi==NULL) {
+        PyErr_SetString(#modulename#_error,\"cb: Callback #argname# not defined (as an argument or module #modulename# attribute).\\n\");
+        goto capi_fail;
+    }
+    if (F2PyCapsule_Check(cb->capi)) {
+    #name#_typedef #name#_cptr;
+    #name#_cptr = F2PyCapsule_AsVoidPtr(cb->capi);
+    #returncptr#(*#name#_cptr)(#optargs_nm##args_nm##strarglens_nm#);
+    #return#
+    }
+    if (capi_arglist==NULL) {
+        capi_longjmp_ok = 0;
+        capi_tmp = PyObject_GetAttrString(#modulename#_module,\"#argname#_extra_args\");
+        if (capi_tmp) {
+            capi_arglist = (PyTupleObject *)PySequence_Tuple(capi_tmp);
+            Py_DECREF(capi_tmp);
+            if (capi_arglist==NULL) {
+                PyErr_SetString(#modulename#_error,\"Failed to convert #modulename#.#argname#_extra_args to tuple.\\n\");
+                goto capi_fail;
+            }
+        } else {
+            PyErr_Clear();
+            capi_arglist = (PyTupleObject *)Py_BuildValue(\"()\");
+        }
+    }
+    if (capi_arglist == NULL) {
+        PyErr_SetString(#modulename#_error,\"Callback #argname# argument list is not set.\\n\");
+        goto capi_fail;
+    }
 #setdims#
+#ifdef PYPY_VERSION
+#define CAPI_ARGLIST_SETITEM(idx, value) PyList_SetItem((PyObject *)capi_arglist_list, idx, value)
+    capi_arglist_list = PySequence_List(capi_arglist);
+    if (capi_arglist_list == NULL) goto capi_fail;
+#else
+#define CAPI_ARGLIST_SETITEM(idx, value) PyTuple_SetItem((PyObject *)capi_arglist, idx, value)
+#endif
 #pyobjfrom#
-\tCFUNCSMESSPY(\"cb:capi_arglist=\",capi_arglist);
-\tCFUNCSMESS(\"cb:Call-back calling Python function #argname#.\\n\");
+#undef CAPI_ARGLIST_SETITEM
+#ifdef PYPY_VERSION
+    CFUNCSMESSPY(\"cb:capi_arglist=\",capi_arglist_list);
+#else
+    CFUNCSMESSPY(\"cb:capi_arglist=\",capi_arglist);
+#endif
+    CFUNCSMESS(\"cb:Call-back calling Python function #argname#.\\n\");
 #ifdef F2PY_REPORT_ATEXIT
 f2py_cb_start_call_clock();
 #endif
-\tcapi_return = PyObject_CallObject(#name#_capi,(PyObject *)capi_arglist);
+#ifdef PYPY_VERSION
+    capi_return = PyObject_CallObject(cb->capi,(PyObject *)capi_arglist_list);
+    Py_DECREF(capi_arglist_list);
+    capi_arglist_list = NULL;
+#else
+    capi_return = PyObject_CallObject(cb->capi,(PyObject *)capi_arglist);
+#endif
 #ifdef F2PY_REPORT_ATEXIT
 f2py_cb_stop_call_clock();
 #endif
-\tCFUNCSMESSPY(\"cb:capi_return=\",capi_return);
-\tif (capi_return == NULL) {
-\t\tfprintf(stderr,\"capi_return is NULL\\n\");
-\t\tgoto capi_fail;
-\t}
-\tif (capi_return == Py_None) {
-\t\tPy_DECREF(capi_return);
-\t\tcapi_return = Py_BuildValue(\"()\");
-\t}
-\telse if (!PyTuple_Check(capi_return)) {
-\t\tcapi_return = Py_BuildValue(\"(N)\",capi_return);
-\t}
-\tcapi_j = PyTuple_Size(capi_return);
-\tcapi_i = 0;
+    CFUNCSMESSPY(\"cb:capi_return=\",capi_return);
+    if (capi_return == NULL) {
+        fprintf(stderr,\"capi_return is NULL\\n\");
+        goto capi_fail;
+    }
+    if (capi_return == Py_None) {
+        Py_DECREF(capi_return);
+        capi_return = Py_BuildValue(\"()\");
+    }
+    else if (!PyTuple_Check(capi_return)) {
+        capi_return = Py_BuildValue(\"(N)\",capi_return);
+    }
+    capi_j = PyTuple_Size(capi_return);
+    capi_i = 0;
 #frompyobj#
-\tCFUNCSMESS(\"cb:#name#:successful\\n\");
-\tPy_DECREF(capi_return);
+    CFUNCSMESS(\"cb:#name#:successful\\n\");
+    Py_DECREF(capi_return);
 #ifdef F2PY_REPORT_ATEXIT
 f2py_cb_stop_clock();
 #endif
-\tgoto capi_return_pt;
+    goto capi_return_pt;
 capi_fail:
-\tfprintf(stderr,\"Call-back #name# failed.\\n\");
-\tPy_XDECREF(capi_return);
-\tif (capi_longjmp_ok)
-\t\tlongjmp(#name#_jmpbuf,-1);
+    fprintf(stderr,\"Call-back #name# failed.\\n\");
+    Py_XDECREF(capi_return);
+    Py_XDECREF(capi_arglist_list);
+    if (capi_longjmp_ok) {
+        longjmp(cb->jmpbuf,-1);
+    }
 capi_return_pt:
-\t;
+    ;
 #return#
 }
 #endtitle#
 """,
-    'need':['setjmp.h','CFUNCSMESS'],
-    'maxnofargs':'#maxnofargs#',
-    'nofoptargs':'#nofoptargs#',
-    'docstr':"""\
-\tdef #argname#(#docsignature#): return #docreturn#\\n\\
+    'need': ['setjmp.h', 'CFUNCSMESS', 'F2PY_THREAD_LOCAL_DECL'],
+    'maxnofargs': '#maxnofargs#',
+    'nofoptargs': '#nofoptargs#',
+    'docstr': """\
+    def #argname#(#docsignature#): return #docreturn#\\n\\
 #docstrsigns#""",
-    'latexdocstr':"""
+    'latexdocstr': """
 {{}\\verb@def #argname#(#latexdocsignature#): return #docreturn#@{}}
 #routnote#
 
 #latexdocstrsigns#""",
-    'docstrshort':'def #argname#(#docsignature#): return #docreturn#'
-    }
-cb_rout_rules=[
-    {# Init
-    'separatorsfor':{'decl':'\n',
-                     'args':',','optargs':'','pyobjfrom':'\n','freemem':'\n',
-                     'args_td':',','optargs_td':'',
-                     'args_nm':',','optargs_nm':'',
-                     'frompyobj':'\n','setdims':'\n',
-                     'docstrsigns':'\\n"\n"',
-                     'latexdocstrsigns':'\n',
-                     'latexdocstrreq':'\n','latexdocstropt':'\n',
-                     'latexdocstrout':'\n','latexdocstrcbs':'\n',
-                     },
-    'decl':'/*decl*/','pyobjfrom':'/*pyobjfrom*/','frompyobj':'/*frompyobj*/',
-    'args':[],'optargs':'','return':'','strarglens':'','freemem':'/*freemem*/',
-    'args_td':[],'optargs_td':'','strarglens_td':'',
-    'args_nm':[],'optargs_nm':'','strarglens_nm':'',
-    'noargs':'',
-    'setdims':'/*setdims*/',
-    'docstrsigns':'','latexdocstrsigns':'',
-    'docstrreq':'\tRequired arguments:',
-    'docstropt':'\tOptional arguments:',
-    'docstrout':'\tReturn objects:',
-    'docstrcbs':'\tCall-back functions:',
-    'docreturn':'','docsign':'','docsignopt':'',
-    'latexdocstrreq':'\\noindent Required arguments:',
-    'latexdocstropt':'\\noindent Optional arguments:',
-    'latexdocstrout':'\\noindent Return objects:',
-    'latexdocstrcbs':'\\noindent Call-back functions:',
-    'routnote':{hasnote:'--- #note#',l_not(hasnote):''},
-    },{ # Function
-    'decl':'\t#ctype# return_value;',
-    'frompyobj':[{debugcapi:'\tCFUNCSMESS("cb:Getting return_value->");'},
-                 '\tif (capi_j>capi_i)\n\t\tGETSCALARFROMPYTUPLE(capi_return,capi_i++,&return_value,#ctype#,"#ctype#_from_pyobj failed in converting return_value of call-back function #name# to C #ctype#\\n");',
-                 {debugcapi:'\tfprintf(stderr,"#showvalueformat#.\\n",return_value);'}
-                 ],
-    'need':['#ctype#_from_pyobj',{debugcapi:'CFUNCSMESS'},'GETSCALARFROMPYTUPLE'],
-    'return':'\treturn return_value;',
-    '_check':l_and(isfunction,l_not(isstringfunction),l_not(iscomplexfunction))
+    'docstrshort': 'def #argname#(#docsignature#): return #docreturn#'
+}
+cb_rout_rules = [
+    {  # Init
+        'separatorsfor': {'decl': '\n',
+                          'args': ',', 'optargs': '', 'pyobjfrom': '\n', 'freemem': '\n',
+                          'args_td': ',', 'optargs_td': '',
+                          'args_nm': ',', 'optargs_nm': '',
+                          'frompyobj': '\n', 'setdims': '\n',
+                          'docstrsigns': '\\n"\n"',
+                          'latexdocstrsigns': '\n',
+                          'latexdocstrreq': '\n', 'latexdocstropt': '\n',
+                          'latexdocstrout': '\n', 'latexdocstrcbs': '\n',
+                          },
+        'decl': '/*decl*/', 'pyobjfrom': '/*pyobjfrom*/', 'frompyobj': '/*frompyobj*/',
+        'args': [], 'optargs': '', 'return': '', 'strarglens': '', 'freemem': '/*freemem*/',
+        'args_td': [], 'optargs_td': '', 'strarglens_td': '',
+        'args_nm': [], 'optargs_nm': '', 'strarglens_nm': '',
+        'noargs': '',
+        'setdims': '/*setdims*/',
+        'docstrsigns': '', 'latexdocstrsigns': '',
+        'docstrreq': '    Required arguments:',
+        'docstropt': '    Optional arguments:',
+        'docstrout': '    Return objects:',
+        'docstrcbs': '    Call-back functions:',
+        'docreturn': '', 'docsign': '', 'docsignopt': '',
+        'latexdocstrreq': '\\noindent Required arguments:',
+        'latexdocstropt': '\\noindent Optional arguments:',
+        'latexdocstrout': '\\noindent Return objects:',
+        'latexdocstrcbs': '\\noindent Call-back functions:',
+        'routnote': {hasnote: '--- #note#', l_not(hasnote): ''},
+    }, {  # Function
+        'decl': '    #ctype# return_value = 0;',
+        'frompyobj': [
+            {debugcapi: '    CFUNCSMESS("cb:Getting return_value->");'},
+            '''\
+    if (capi_j>capi_i) {
+        GETSCALARFROMPYTUPLE(capi_return,capi_i++,&return_value,#ctype#,
+          "#ctype#_from_pyobj failed in converting return_value of"
+          " call-back function #name# to C #ctype#\\n");
+    } else {
+        fprintf(stderr,"Warning: call-back function #name# did not provide"
+                       " return value (index=%d, type=#ctype#)\\n",capi_i);
+    }''',
+            {debugcapi:
+             '    fprintf(stderr,"#showvalueformat#.\\n",return_value);'}
+        ],
+        'need': ['#ctype#_from_pyobj', {debugcapi: 'CFUNCSMESS'}, 'GETSCALARFROMPYTUPLE'],
+        'return': '    return return_value;',
+        '_check': l_and(isfunction, l_not(isstringfunction), l_not(iscomplexfunction))
     },
-    {# String function
-    'pyobjfrom':{debugcapi:'\tfprintf(stderr,"debug-capi:cb:#name#:%d:\\n",return_value_len);'},
-    'args':'#ctype# return_value,int return_value_len',
-    'args_nm':'return_value,&return_value_len',
-    'args_td':'#ctype# ,int',
-    'frompyobj':[{debugcapi:'\tCFUNCSMESS("cb:Getting return_value->\\"");'},
-                 """\tif (capi_j>capi_i)
-\t\tGETSTRFROMPYTUPLE(capi_return,capi_i++,return_value,return_value_len);""",
-                 {debugcapi:'\tfprintf(stderr,"#showvalueformat#\\".\\n",return_value);'}
-                 ],
-    'need':['#ctype#_from_pyobj',{debugcapi:'CFUNCSMESS'},
-            'string.h','GETSTRFROMPYTUPLE'],
-    'return':'return;',
-    '_check':isstringfunction
+    {  # String function
+        'pyobjfrom': {debugcapi: '    fprintf(stderr,"debug-capi:cb:#name#:%d:\\n",return_value_len);'},
+        'args': '#ctype# return_value,int return_value_len',
+        'args_nm': 'return_value,&return_value_len',
+        'args_td': '#ctype# ,int',
+        'frompyobj': [
+            {debugcapi: '    CFUNCSMESS("cb:Getting return_value->\\"");'},
+            """\
+    if (capi_j>capi_i) {
+        GETSTRFROMPYTUPLE(capi_return,capi_i++,return_value,return_value_len);
+    } else {
+        fprintf(stderr,"Warning: call-back function #name# did not provide"
+                       " return value (index=%d, type=#ctype#)\\n",capi_i);
+    }""",
+            {debugcapi:
+             '    fprintf(stderr,"#showvalueformat#\\".\\n",return_value);'}
+        ],
+        'need': ['#ctype#_from_pyobj', {debugcapi: 'CFUNCSMESS'},
+                 'string.h', 'GETSTRFROMPYTUPLE'],
+        'return': 'return;',
+        '_check': isstringfunction
     },
-    {# Complex function
-    'optargs':"""
+    {  # Complex function
+        'optargs': """
 #ifndef F2PY_CB_RETURNCOMPLEX
 #ctype# *return_value
 #endif
 """,
-    'optargs_nm':"""
+        'optargs_nm': """
 #ifndef F2PY_CB_RETURNCOMPLEX
 return_value
 #endif
 """,
-    'optargs_td':"""
+        'optargs_td': """
 #ifndef F2PY_CB_RETURNCOMPLEX
 #ctype# *
 #endif
 """,
-    'decl':"""
+        'decl': """
 #ifdef F2PY_CB_RETURNCOMPLEX
-\t#ctype# return_value;
-#endif
-""",
-    'frompyobj':[{debugcapi:'\tCFUNCSMESS("cb:Getting return_value->");'},
-                     """\
-\tif (capi_j>capi_i)
+    #ctype# return_value = {0, 0};
+#endif
+""",
+        'frompyobj': [
+            {debugcapi: '    CFUNCSMESS("cb:Getting return_value->");'},
+            """\
+    if (capi_j>capi_i) {
 #ifdef F2PY_CB_RETURNCOMPLEX
-\t\tGETSCALARFROMPYTUPLE(capi_return,capi_i++,&return_value,#ctype#,\"#ctype#_from_pyobj failed in converting return_value of call-back function #name# to C #ctype#\\n\");
+        GETSCALARFROMPYTUPLE(capi_return,capi_i++,&return_value,#ctype#,
+          \"#ctype#_from_pyobj failed in converting return_value of call-back\"
+          \" function #name# to C #ctype#\\n\");
 #else
-\t\tGETSCALARFROMPYTUPLE(capi_return,capi_i++,return_value,#ctype#,\"#ctype#_from_pyobj failed in converting return_value of call-back function #name# to C #ctype#\\n\");
-#endif
-""",
-                 {debugcapi:"""
+        GETSCALARFROMPYTUPLE(capi_return,capi_i++,return_value,#ctype#,
+          \"#ctype#_from_pyobj failed in converting return_value of call-back\"
+          \" function #name# to C #ctype#\\n\");
+#endif
+    } else {
+        fprintf(stderr,
+                \"Warning: call-back function #name# did not provide\"
+                \" return value (index=%d, type=#ctype#)\\n\",capi_i);
+    }""",
+            {debugcapi: """\
 #ifdef F2PY_CB_RETURNCOMPLEX
-\tfprintf(stderr,\"#showvalueformat#.\\n\",(return_value).r,(return_value).i);
+    fprintf(stderr,\"#showvalueformat#.\\n\",(return_value).r,(return_value).i);
 #else
-\tfprintf(stderr,\"#showvalueformat#.\\n\",(*return_value).r,(*return_value).i);
-#endif
-
+    fprintf(stderr,\"#showvalueformat#.\\n\",(*return_value).r,(*return_value).i);
+#endif
 """}
-                 ],
-    'return':"""
+        ],
+        'return': """
 #ifdef F2PY_CB_RETURNCOMPLEX
-\treturn return_value;
+    return return_value;
 #else
-\treturn;
-#endif
-""",
-    'need':['#ctype#_from_pyobj',{debugcapi:'CFUNCSMESS'},
-            'string.h','GETSCALARFROMPYTUPLE','#ctype#'],
-    '_check':iscomplexfunction
+    return;
+#endif
+""",
+        'need': ['#ctype#_from_pyobj', {debugcapi: 'CFUNCSMESS'},
+                 'string.h', 'GETSCALARFROMPYTUPLE', '#ctype#'],
+        '_check': iscomplexfunction
     },
-    {'docstrout':'\t\t#pydocsignout#',
-     'latexdocstrout':['\\item[]{{}\\verb@#pydocsignout#@{}}',
-                       {hasnote:'--- #note#'}],
-     'docreturn':'#rname#,',
-     '_check':isfunction},
-    {'_check':issubroutine,'return':'return;'}
-    ]
-
-cb_arg_rules=[
-    { # Doc
-    'docstropt':{l_and(isoptional,isintent_nothide):'\t\t#pydocsign#'},
-    'docstrreq':{l_and(isrequired,isintent_nothide):'\t\t#pydocsign#'},
-    'docstrout':{isintent_out:'\t\t#pydocsignout#'},
-    'latexdocstropt':{l_and(isoptional,isintent_nothide):['\\item[]{{}\\verb@#pydocsign#@{}}',
-                                                          {hasnote:'--- #note#'}]},
-    'latexdocstrreq':{l_and(isrequired,isintent_nothide):['\\item[]{{}\\verb@#pydocsign#@{}}',
-                                                          {hasnote:'--- #note#'}]},
-    'latexdocstrout':{isintent_out:['\\item[]{{}\\verb@#pydocsignout#@{}}',
-                                    {l_and(hasnote,isintent_hide):'--- #note#',
-                                     l_and(hasnote,isintent_nothide):'--- See above.'}]},
-    'docsign':{l_and(isrequired,isintent_nothide):'#varname#,'},
-    'docsignopt':{l_and(isoptional,isintent_nothide):'#varname#,'},
-    'depend':''
+    {'docstrout': '        #pydocsignout#',
+     'latexdocstrout': ['\\item[]{{}\\verb@#pydocsignout#@{}}',
+                        {hasnote: '--- #note#'}],
+     'docreturn': '#rname#,',
+     '_check': isfunction},
+    {'_check': issubroutine, 'return': 'return;'}
+]
+
+cb_arg_rules = [
+    {  # Doc
+        'docstropt': {l_and(isoptional, isintent_nothide): '        #pydocsign#'},
+        'docstrreq': {l_and(isrequired, isintent_nothide): '        #pydocsign#'},
+        'docstrout': {isintent_out: '        #pydocsignout#'},
+        'latexdocstropt': {l_and(isoptional, isintent_nothide): ['\\item[]{{}\\verb@#pydocsign#@{}}',
+                                                                 {hasnote: '--- #note#'}]},
+        'latexdocstrreq': {l_and(isrequired, isintent_nothide): ['\\item[]{{}\\verb@#pydocsign#@{}}',
+                                                                 {hasnote: '--- #note#'}]},
+        'latexdocstrout': {isintent_out: ['\\item[]{{}\\verb@#pydocsignout#@{}}',
+                                          {l_and(hasnote, isintent_hide): '--- #note#',
+                                           l_and(hasnote, isintent_nothide): '--- See above.'}]},
+        'docsign': {l_and(isrequired, isintent_nothide): '#varname#,'},
+        'docsignopt': {l_and(isoptional, isintent_nothide): '#varname#,'},
+        'depend': ''
     },
     {
-    'args':{
-    l_and (isscalar,isintent_c):'#ctype# #varname#',
-    l_and (isscalar,l_not(isintent_c)):'#ctype# *#varname#_cb_capi',
-    isarray:'#ctype# *#varname#',
-    isstring:'#ctype# #varname#'
+        'args': {
+            l_and(isscalar, isintent_c): '#ctype# #varname_i#',
+            l_and(isscalar, l_not(isintent_c)): '#ctype# *#varname_i#_cb_capi',
+            isarray: '#ctype# *#varname_i#',
+            isstring: '#ctype# #varname_i#'
+        },
+        'args_nm': {
+            l_and(isscalar, isintent_c): '#varname_i#',
+            l_and(isscalar, l_not(isintent_c)): '#varname_i#_cb_capi',
+            isarray: '#varname_i#',
+            isstring: '#varname_i#'
+        },
+        'args_td': {
+            l_and(isscalar, isintent_c): '#ctype#',
+            l_and(isscalar, l_not(isintent_c)): '#ctype# *',
+            isarray: '#ctype# *',
+            isstring: '#ctype#'
+        },
+        'need': {l_or(isscalar, isarray, isstring): '#ctype#'},
+        # untested with multiple args
+        'strarglens': {isstring: ',int #varname_i#_cb_len'},
+        'strarglens_td': {isstring: ',int'},  # untested with multiple args
+        # untested with multiple args
+        'strarglens_nm': {isstring: ',#varname_i#_cb_len'},
     },
-    'args_nm':{
-    l_and (isscalar,isintent_c):'#varname#',
-    l_and (isscalar,l_not(isintent_c)):'#varname#_cb_capi',
-    isarray:'#varname#',
-    isstring:'#varname#'
+    {  # Scalars
+        'decl': {l_not(isintent_c): '    #ctype# #varname_i#=(*#varname_i#_cb_capi);'},
+        'error': {l_and(isintent_c, isintent_out,
+                        throw_error('intent(c,out) is forbidden for callback scalar arguments')):
+                  ''},
+        'frompyobj': [{debugcapi: '    CFUNCSMESS("cb:Getting #varname#->");'},
+                      {isintent_out:
+                       '    if (capi_j>capi_i)\n        GETSCALARFROMPYTUPLE(capi_return,capi_i++,#varname_i#_cb_capi,#ctype#,"#ctype#_from_pyobj failed in converting argument #varname# of call-back function #name# to C #ctype#\\n");'},
+                      {l_and(debugcapi, l_and(l_not(iscomplex), isintent_c)):
+                          '    fprintf(stderr,"#showvalueformat#.\\n",#varname_i#);'},
+                      {l_and(debugcapi, l_and(l_not(iscomplex), l_not( isintent_c))):
+                          '    fprintf(stderr,"#showvalueformat#.\\n",*#varname_i#_cb_capi);'},
+                      {l_and(debugcapi, l_and(iscomplex, isintent_c)):
+                          '    fprintf(stderr,"#showvalueformat#.\\n",(#varname_i#).r,(#varname_i#).i);'},
+                      {l_and(debugcapi, l_and(iscomplex, l_not( isintent_c))):
+                          '    fprintf(stderr,"#showvalueformat#.\\n",(*#varname_i#_cb_capi).r,(*#varname_i#_cb_capi).i);'},
+                      ],
+        'need': [{isintent_out: ['#ctype#_from_pyobj', 'GETSCALARFROMPYTUPLE']},
+                 {debugcapi: 'CFUNCSMESS'}],
+        '_check': isscalar
+    }, {
+        'pyobjfrom': [{isintent_in: """\
+    if (cb->nofargs>capi_i)
+        if (CAPI_ARGLIST_SETITEM(capi_i++,pyobj_from_#ctype#1(#varname_i#)))
+            goto capi_fail;"""},
+                      {isintent_inout: """\
+    if (cb->nofargs>capi_i)
+        if (CAPI_ARGLIST_SETITEM(capi_i++,pyarr_from_p_#ctype#1(#varname_i#_cb_capi)))
+            goto capi_fail;"""}],
+        'need': [{isintent_in: 'pyobj_from_#ctype#1'},
+                 {isintent_inout: 'pyarr_from_p_#ctype#1'},
+                 {iscomplex: '#ctype#'}],
+        '_check': l_and(isscalar, isintent_nothide),
+        '_optional': ''
+    }, {  # String
+        'frompyobj': [{debugcapi: '    CFUNCSMESS("cb:Getting #varname#->\\"");'},
+                      """    if (capi_j>capi_i)
+        GETSTRFROMPYTUPLE(capi_return,capi_i++,#varname_i#,#varname_i#_cb_len);""",
+                      {debugcapi:
+                       '    fprintf(stderr,"#showvalueformat#\\":%d:.\\n",#varname_i#,#varname_i#_cb_len);'},
+                      ],
+        'need': ['#ctype#', 'GETSTRFROMPYTUPLE',
+                 {debugcapi: 'CFUNCSMESS'}, 'string.h'],
+        '_check': l_and(isstring, isintent_out)
+    }, {
+        'pyobjfrom': [{debugcapi: '    fprintf(stderr,"debug-capi:cb:#varname#=\\"#showvalueformat#\\":%d:\\n",#varname_i#,#varname_i#_cb_len);'},
+                      {isintent_in: """\
+    if (cb->nofargs>capi_i)
+        if (CAPI_ARGLIST_SETITEM(capi_i++,pyobj_from_#ctype#1size(#varname_i#,#varname_i#_cb_len)))
+            goto capi_fail;"""},
+                      {isintent_inout: """\
+    if (cb->nofargs>capi_i) {
+        int #varname_i#_cb_dims[] = {#varname_i#_cb_len};
+        if (CAPI_ARGLIST_SETITEM(capi_i++,pyarr_from_p_#ctype#1(#varname_i#,#varname_i#_cb_dims)))
+            goto capi_fail;
+    }"""}],
+        'need': [{isintent_in: 'pyobj_from_#ctype#1size'},
+                 {isintent_inout: 'pyarr_from_p_#ctype#1'}],
+        '_check': l_and(isstring, isintent_nothide),
+        '_optional': ''
     },
-    'args_td':{
-    l_and (isscalar,isintent_c):'#ctype#',
-    l_and (isscalar,l_not(isintent_c)):'#ctype# *',
-    isarray:'#ctype# *',
-    isstring:'#ctype#'
-    },
-     'strarglens':{isstring:',int #varname#_cb_len'}, # untested with multiple args
-     'strarglens_td':{isstring:',int'}, # untested with multiple args
-
-     },
-    { # Scalars
-    'decl':{l_not(isintent_c):'\t#ctype# #varname#=(*#varname#_cb_capi);'},
-    'error': {l_and(isintent_c,isintent_out,
-                    throw_error('intent(c,out) is forbidden for callback scalar arguments')):\
-               ''},
-    'frompyobj':[{debugcapi:'\tCFUNCSMESS("cb:Getting #varname#->");'},
-                 {isintent_out:'\tif (capi_j>capi_i)\n\t\tGETSCALARFROMPYTUPLE(capi_return,capi_i++,#varname#_cb_capi,#ctype#,"#ctype#_from_pyobj failed in converting argument #varname# of call-back function #name# to C #ctype#\\n");'},
-                 {l_and(debugcapi,l_and(l_not(iscomplex),isintent_c)):'\tfprintf(stderr,"#showvalueformat#.\\n",#varname#);'},
-                 {l_and(debugcapi,l_and(l_not(iscomplex),l_not(isintent_c))):'\tfprintf(stderr,"#showvalueformat#.\\n",*#varname#_cb_capi);'},
-                 {l_and(debugcapi,l_and(iscomplex,isintent_c)):'\tfprintf(stderr,"#showvalueformat#.\\n",(#varname#).r,(#varname#).i);'},
-                 {l_and(debugcapi,l_and(iscomplex,l_not(isintent_c))):'\tfprintf(stderr,"#showvalueformat#.\\n",(*#varname#_cb_capi).r,(*#varname#_cb_capi).i);'},
-                 ],
-    'need':[{isintent_out:['#ctype#_from_pyobj','GETSCALARFROMPYTUPLE']},
-            {debugcapi:'CFUNCSMESS'}],
-     '_check':isscalar
-     },{
-    'pyobjfrom':[{isintent_in:"""\
-\tif (#name#_nofargs>capi_i)
-\t\tif (PyTuple_SetItem((PyObject *)capi_arglist,capi_i++,pyobj_from_#ctype#1(#varname#)))
-\t\t\tgoto capi_fail;"""},
-                 {isintent_inout:"""\
-\tif (#name#_nofargs>capi_i)
-\t\tif (PyTuple_SetItem((PyObject *)capi_arglist,capi_i++,pyarr_from_p_#ctype#1(#varname#_cb_capi)))
-\t\t\tgoto capi_fail;"""}],
-    'need':[{isintent_in:'pyobj_from_#ctype#1'},
-            {isintent_inout:'pyarr_from_p_#ctype#1'},
-            {iscomplex:'#ctype#'}],
-    '_check':l_and(isscalar,isintent_nothide),
-    '_optional':''
-    },{# String
-    'frompyobj':[{debugcapi:'\tCFUNCSMESS("cb:Getting #varname#->\\"");'},
-                 """\tif (capi_j>capi_i)
-\t\tGETSTRFROMPYTUPLE(capi_return,capi_i++,#varname#,#varname#_cb_len);""",
-                 {debugcapi:'\tfprintf(stderr,"#showvalueformat#\\":%d:.\\n",#varname#,#varname#_cb_len);'},
-                 ],
-    'need':['#ctype#','GETSTRFROMPYTUPLE',
-            {debugcapi:'CFUNCSMESS'},'string.h'],
-    '_check':l_and(isstring,isintent_out)
-    },{
-    'pyobjfrom':[{debugcapi:'\tfprintf(stderr,"debug-capi:cb:#varname#=\\"#showvalueformat#\\":%d:\\n",#varname#,#varname#_cb_len);'},
-                 {isintent_in:"""\
-\tif (#name#_nofargs>capi_i)
-\t\tif (PyTuple_SetItem((PyObject *)capi_arglist,capi_i++,pyobj_from_#ctype#1(#varname#)))
-\t\t\tgoto capi_fail;"""},
-                 {isintent_inout:"""\
-\tif (#name#_nofargs>capi_i) {
-\t\tint #varname#_cb_dims[] = {#varname#_cb_len};
-\t\tif (PyTuple_SetItem((PyObject *)capi_arglist,capi_i++,pyarr_from_p_#ctype#1(#varname#,#varname#_cb_dims)))
-\t\t\tgoto capi_fail;
-\t}"""}],
-    'need':[{isintent_in:'pyobj_from_#ctype#1'},
-            {isintent_inout:'pyarr_from_p_#ctype#1'}],
-    '_check':l_and(isstring,isintent_nothide),
-    '_optional':''
-    },
-# Array ...
+    # Array ...
     {
-    'decl':'\tintp #varname#_Dims[#rank#] = {#rank*[-1]#};',
-    'setdims':'\t#cbsetdims#;',
-    '_check':isarray,
-    '_depend':''
+        'decl': '    npy_intp #varname_i#_Dims[#rank#] = {#rank*[-1]#};',
+        'setdims': '    #cbsetdims#;',
+        '_check': isarray,
+        '_depend': ''
     },
     {
-    'pyobjfrom':[{debugcapi:'\tfprintf(stderr,"debug-capi:cb:#varname#\\n");'},
-                 {isintent_c:"""\
-\tif (#name#_nofargs>capi_i) {
-\t\tPyArrayObject *tmp_arr = (PyArrayObject *)PyArray_New(&PyArray_Type,#rank#,#varname#_Dims,#atype#,NULL,(char*)#varname#,0,CARRAY_FLAGS,NULL); /*XXX: Hmm, what will destroy this array??? */
-""",
-                  l_not(isintent_c):"""\
-\tif (#name#_nofargs>capi_i) {
-\t\tPyArrayObject *tmp_arr = (PyArrayObject *)PyArray_New(&PyArray_Type,#rank#,#varname#_Dims,#atype#,NULL,(char*)#varname#,0,FARRAY_FLAGS,NULL); /*XXX: Hmm, what will destroy this array??? */
-""",
-                  },
-                 """
-\t\tif (tmp_arr==NULL)
-\t\t\tgoto capi_fail;
-\t\tif (PyTuple_SetItem((PyObject *)capi_arglist,capi_i++,(PyObject *)tmp_arr))
-\t\t\tgoto capi_fail;
+        'pyobjfrom': [{debugcapi: '    fprintf(stderr,"debug-capi:cb:#varname#\\n");'},
+                      {isintent_c: """\
+    if (cb->nofargs>capi_i) {
+        int itemsize_ = #atype# == NPY_STRING ? 1 : 0;
+        /*XXX: Hmm, what will destroy this array??? */
+        PyArrayObject *tmp_arr = (PyArrayObject *)PyArray_New(&PyArray_Type,#rank#,#varname_i#_Dims,#atype#,NULL,(char*)#varname_i#,itemsize_,NPY_ARRAY_CARRAY,NULL);
+""",
+                       l_not(isintent_c): """\
+    if (cb->nofargs>capi_i) {
+        int itemsize_ = #atype# == NPY_STRING ? 1 : 0;
+        /*XXX: Hmm, what will destroy this array??? */
+        PyArrayObject *tmp_arr = (PyArrayObject *)PyArray_New(&PyArray_Type,#rank#,#varname_i#_Dims,#atype#,NULL,(char*)#varname_i#,itemsize_,NPY_ARRAY_FARRAY,NULL);
+""",
+                       },
+                      """
+        if (tmp_arr==NULL)
+            goto capi_fail;
+        if (CAPI_ARGLIST_SETITEM(capi_i++,(PyObject *)tmp_arr))
+            goto capi_fail;
 }"""],
-    '_check':l_and(isarray,isintent_nothide,l_or(isintent_in,isintent_inout)),
-    '_optional':'',
-    },{
-    'frompyobj':[{debugcapi:'\tCFUNCSMESS("cb:Getting #varname#->");'},
-                 """\tif (capi_j>capi_i) {
-\t\tPyArrayObject *rv_cb_arr = NULL;
-\t\tif ((capi_tmp = PyTuple_GetItem(capi_return,capi_i++))==NULL) goto capi_fail;
-\t\trv_cb_arr =  array_from_pyobj(#atype#,#varname#_Dims,#rank#,F2PY_INTENT_IN""",
-    {isintent_c:'|F2PY_INTENT_C'},
-    """,capi_tmp);
-\t\tif (rv_cb_arr == NULL) {
-\t\t\tfprintf(stderr,\"rv_cb_arr is NULL\\n\");
-\t\t\tgoto capi_fail;
-\t\t}
-\t\tMEMCOPY(#varname#,rv_cb_arr->data,PyArray_NBYTES(rv_cb_arr));
-\t\tif (capi_tmp != (PyObject *)rv_cb_arr) {
-\t\t\tPy_DECREF(rv_cb_arr);
-\t\t}
-\t}""",
-                 {debugcapi:'\tfprintf(stderr,"<-.\\n");'},
-                 ],
-    'need':['MEMCOPY',{iscomplexarray:'#ctype#'}],
-    '_check':l_and(isarray,isintent_out)
-    },{
-    'docreturn':'#varname#,',
-    '_check':isintent_out
-    }
-    ]
+        '_check': l_and(isarray, isintent_nothide, l_or(isintent_in, isintent_inout)),
+        '_optional': '',
+    }, {
+        'frompyobj': [{debugcapi: '    CFUNCSMESS("cb:Getting #varname#->");'},
+                      """    if (capi_j>capi_i) {
+        PyArrayObject *rv_cb_arr = NULL;
+        if ((capi_tmp = PyTuple_GetItem(capi_return,capi_i++))==NULL) goto capi_fail;
+        rv_cb_arr =  array_from_pyobj(#atype#,#varname_i#_Dims,#rank#,F2PY_INTENT_IN""",
+                      {isintent_c: '|F2PY_INTENT_C'},
+                      """,capi_tmp);
+        if (rv_cb_arr == NULL) {
+            fprintf(stderr,\"rv_cb_arr is NULL\\n\");
+            goto capi_fail;
+        }
+        MEMCOPY(#varname_i#,PyArray_DATA(rv_cb_arr),PyArray_NBYTES(rv_cb_arr));
+        if (capi_tmp != (PyObject *)rv_cb_arr) {
+            Py_DECREF(rv_cb_arr);
+        }
+    }""",
+                      {debugcapi: '    fprintf(stderr,"<-.\\n");'},
+                      ],
+        'need': ['MEMCOPY', {iscomplexarray: '#ctype#'}],
+        '_check': l_and(isarray, isintent_out)
+    }, {
+        'docreturn': '#varname#,',
+        '_check': isintent_out
+    }
+]
 
 ################## Build call-back module #############
-cb_map={}
+cb_map = {}
+
+
 def buildcallbacks(m):
-    global cb_map
-    cb_map[m['name']]=[]
+    cb_map[m['name']] = []
     for bi in m['body']:
-        if bi['block']=='interface':
+        if bi['block'] == 'interface':
             for b in bi['body']:
                 if b:
-                    buildcallback(b,m['name'])
+                    buildcallback(b, m['name'])
                 else:
                     errmess('warning: empty body for %s\n' % (m['name']))
 
-def buildcallback(rout,um):
-    global cb_map
-    outmess('\tConstructing call-back function "cb_%s_in_%s"\n'%(rout['name'],um))
-    args,depargs=getargs(rout)
-    capi_maps.depargs=depargs
-    var=rout['vars']
-    vrd=capi_maps.cb_routsign2map(rout,um)
-    rd=dictappend({},vrd)
-    cb_map[um].append([rout['name'],rd['name']])
+
+def buildcallback(rout, um):
+    from . import capi_maps
+
+    outmess('    Constructing call-back function "cb_%s_in_%s"\n' %
+            (rout['name'], um))
+    args, depargs = getargs(rout)
+    capi_maps.depargs = depargs
+    var = rout['vars']
+    vrd = capi_maps.cb_routsign2map(rout, um)
+    rd = dictappend({}, vrd)
+    cb_map[um].append([rout['name'], rd['name']])
     for r in cb_rout_rules:
-        if (r.has_key('_check') and r['_check'](rout)) or (not r.has_key('_check')):
-            ar=applyrules(r,vrd,rout)
-            rd=dictappend(rd,ar)
-    savevrd={}
+        if ('_check' in r and r['_check'](rout)) or ('_check' not in r):
+            ar = applyrules(r, vrd, rout)
+            rd = dictappend(rd, ar)
+    savevrd = {}
+    for i, a in enumerate(args):
+        vrd = capi_maps.cb_sign2map(a, var[a], index=i)
+        savevrd[a] = vrd
+        for r in cb_arg_rules:
+            if '_depend' in r:
+                continue
+            if '_optional' in r and isoptional(var[a]):
+                continue
+            if ('_check' in r and r['_check'](var[a])) or ('_check' not in r):
+                ar = applyrules(r, vrd, var[a])
+                rd = dictappend(rd, ar)
+                if '_break' in r:
+                    break
     for a in args:
-        vrd=capi_maps.cb_sign2map(a,var[a])
-        savevrd[a]=vrd
+        vrd = savevrd[a]
         for r in cb_arg_rules:
-            if r.has_key('_depend'): continue
-            if r.has_key('_optional') and isoptional(var[a]): continue
-            if (r.has_key('_check') and r['_check'](var[a])) or (not r.has_key('_check')):
-                ar=applyrules(r,vrd,var[a])
-                rd=dictappend(rd,ar)
-                if r.has_key('_break'): break
-    for a in args:
-        vrd=savevrd[a]
+            if '_depend' in r:
+                continue
+            if ('_optional' not in r) or ('_optional' in r and isrequired(var[a])):
+                continue
+            if ('_check' in r and r['_check'](var[a])) or ('_check' not in r):
+                ar = applyrules(r, vrd, var[a])
+                rd = dictappend(rd, ar)
+                if '_break' in r:
+                    break
+    for a in depargs:
+        vrd = savevrd[a]
         for r in cb_arg_rules:
-            if r.has_key('_depend'): continue
-            if (not r.has_key('_optional')) or (r.has_key('_optional') and isrequired(var[a])): continue
-            if (r.has_key('_check') and r['_check'](var[a])) or (not r.has_key('_check')):
-                ar=applyrules(r,vrd,var[a])
-                rd=dictappend(rd,ar)
-                if r.has_key('_break'): break
-    for a in depargs:
-        vrd=savevrd[a]
-        for r in cb_arg_rules:
-            if not r.has_key('_depend'): continue
-            if r.has_key('_optional'): continue
-            if (r.has_key('_check') and r['_check'](var[a])) or (not r.has_key('_check')):
-                ar=applyrules(r,vrd,var[a])
-                rd=dictappend(rd,ar)
-                if r.has_key('_break'): break
-    if rd.has_key('args') and rd.has_key('optargs'):
-        if type(rd['optargs'])==type([]):
-            rd['optargs']=rd['optargs']+["""
+            if '_depend' not in r:
+                continue
+            if '_optional' in r:
+                continue
+            if ('_check' in r and r['_check'](var[a])) or ('_check' not in r):
+                ar = applyrules(r, vrd, var[a])
+                rd = dictappend(rd, ar)
+                if '_break' in r:
+                    break
+    if 'args' in rd and 'optargs' in rd:
+        if isinstance(rd['optargs'], list):
+            rd['optargs'] = rd['optargs'] + ["""
 #ifndef F2PY_CB_RETURNCOMPLEX
 ,
 #endif
 """]
-            rd['optargs_nm']=rd['optargs_nm']+["""
+            rd['optargs_nm'] = rd['optargs_nm'] + ["""
 #ifndef F2PY_CB_RETURNCOMPLEX
 ,
 #endif
 """]
-            rd['optargs_td']=rd['optargs_td']+["""
+            rd['optargs_td'] = rd['optargs_td'] + ["""
 #ifndef F2PY_CB_RETURNCOMPLEX
 ,
 #endif
 """]
-    if type(rd['docreturn'])==types.ListType:
-        rd['docreturn']=stripcomma(replace('#docreturn#',{'docreturn':rd['docreturn']}))
-    optargs=stripcomma(replace('#docsignopt#',
-                                {'docsignopt':rd['docsignopt']}
-                               ))
-    if optargs=='':
-        rd['docsignature']=stripcomma(replace('#docsign#',{'docsign':rd['docsign']}))
+    if isinstance(rd['docreturn'], list):
+        rd['docreturn'] = stripcomma(
+            replace('#docreturn#', {'docreturn': rd['docreturn']}))
+    optargs = stripcomma(replace('#docsignopt#',
+                                 {'docsignopt': rd['docsignopt']}
+                                 ))
+    if optargs == '':
+        rd['docsignature'] = stripcomma(
+            replace('#docsign#', {'docsign': rd['docsign']}))
     else:
-        rd['docsignature']=replace('#docsign#[#docsignopt#]',
-                                   {'docsign':rd['docsign'],
-                                    'docsignopt':optargs,
-                                    })
-    rd['latexdocsignature']=string.replace(rd['docsignature'],'_','\\_')
-    rd['latexdocsignature']=string.replace(rd['latexdocsignature'],',',', ')
-    rd['docstrsigns']=[]
-    rd['latexdocstrsigns']=[]
-    for k in ['docstrreq','docstropt','docstrout','docstrcbs']:
-        if rd.has_key(k) and type(rd[k])==types.ListType:
-            rd['docstrsigns']=rd['docstrsigns']+rd[k]
-        k='latex'+k
-        if rd.has_key(k) and type(rd[k])==types.ListType:
-            rd['latexdocstrsigns']=rd['latexdocstrsigns']+rd[k][0:1]+\
-                                    ['\\begin{description}']+rd[k][1:]+\
-                                    ['\\end{description}']
-    if not rd.has_key('args'):
-        rd['args']=''
-        rd['args_td']=''
-        rd['args_nm']=''
+        rd['docsignature'] = replace('#docsign#[#docsignopt#]',
+                                     {'docsign': rd['docsign'],
+                                      'docsignopt': optargs,
+                                      })
+    rd['latexdocsignature'] = rd['docsignature'].replace('_', '\\_')
+    rd['latexdocsignature'] = rd['latexdocsignature'].replace(',', ', ')
+    rd['docstrsigns'] = []
+    rd['latexdocstrsigns'] = []
+    for k in ['docstrreq', 'docstropt', 'docstrout', 'docstrcbs']:
+        if k in rd and isinstance(rd[k], list):
+            rd['docstrsigns'] = rd['docstrsigns'] + rd[k]
+        k = 'latex' + k
+        if k in rd and isinstance(rd[k], list):
+            rd['latexdocstrsigns'] = rd['latexdocstrsigns'] + rd[k][0:1] +\
+                ['\\begin{description}'] + rd[k][1:] +\
+                ['\\end{description}']
+    if 'args' not in rd:
+        rd['args'] = ''
+        rd['args_td'] = ''
+        rd['args_nm'] = ''
     if not (rd.get('args') or rd.get('optargs') or rd.get('strarglens')):
         rd['noargs'] = 'void'
 
-    ar=applyrules(cb_routine_rules,rd)
-    cfuncs.callbacks[rd['name']]=ar['body']
-    if type(ar['need'])==types.StringType:
-        ar['need']=[ar['need']]
-
-    if rd.has_key('need'):
+    ar = applyrules(cb_routine_rules, rd)
+    cfuncs.callbacks[rd['name']] = ar['body']
+    if isinstance(ar['need'], str):
+        ar['need'] = [ar['need']]
+
+    if 'need' in rd:
         for t in cfuncs.typedefs.keys():
             if t in rd['need']:
                 ar['need'].append(t)
 
-    cfuncs.typedefs_generated[rd['name']+'_typedef'] = ar['cbtypedefs']
-    ar['need'].append(rd['name']+'_typedef')
-    cfuncs.needs[rd['name']]=ar['need']
-
-    capi_maps.lcb2_map[rd['name']]={'maxnofargs':ar['maxnofargs'],
-                                    'nofoptargs':ar['nofoptargs'],
-                                    'docstr':ar['docstr'],
-                                    'latexdocstr':ar['latexdocstr'],
-                                    'argname':rd['argname']
-                                    }
-    outmess('\t  %s\n'%(ar['docstrshort']))
-    #print ar['body']
+    cfuncs.typedefs_generated[rd['name'] + '_typedef'] = ar['cbtypedefs']
+    ar['need'].append(rd['name'] + '_typedef')
+    cfuncs.needs[rd['name']] = ar['need']
+
+    capi_maps.lcb2_map[rd['name']] = {'maxnofargs': ar['maxnofargs'],
+                                      'nofoptargs': ar['nofoptargs'],
+                                      'docstr': ar['docstr'],
+                                      'latexdocstr': ar['latexdocstr'],
+                                      'argname': rd['argname']
+                                      }
+    outmess('      %s\n' % (ar['docstrshort']))
     return
 ################## Build call-back function #############
('numpy/f2py', '__init__.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,42 +1,187 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
+"""Fortran to Python Interface Generator.
 
-__all__ = ['run_main','compile','f2py_testing']
+"""
+__all__ = ['run_main', 'compile', 'get_include']
 
+import sys
+import subprocess
 import os
-import sys
-import commands
 
-from info import __doc__
+from . import f2py2e
+from . import diagnose
 
-import f2py2e
 run_main = f2py2e.run_main
 main = f2py2e.main
-import f2py_testing
+
 
 def compile(source,
-            modulename = 'untitled',
-            extra_args = '',
-            verbose = 1,
-            source_fn = None
-            ):
-    ''' Build extension module from processing source with f2py.
-    Read the source of this function for more information.
-    '''
-    from numpy.distutils.exec_command import exec_command
+            modulename='untitled',
+            extra_args='',
+            verbose=True,
+            source_fn=None,
+            extension='.f',
+            full_output=False
+           ):
+    """
+    Build extension module from a Fortran 77 source string with f2py.
+
+    Parameters
+    ----------
+    source : str or bytes
+        Fortran source of module / subroutine to compile
+
+        .. versionchanged:: 1.16.0
+           Accept str as well as bytes
+
+    modulename : str, optional
+        The name of the compiled python module
+    extra_args : str or list, optional
+        Additional parameters passed to f2py
+
+        .. versionchanged:: 1.16.0
+            A list of args may also be provided.
+
+    verbose : bool, optional
+        Print f2py output to screen
+    source_fn : str, optional
+        Name of the file where the fortran source is written.
+        The default is to use a temporary file with the extension
+        provided by the ``extension`` parameter
+    extension : ``{'.f', '.f90'}``, optional
+        Filename extension if `source_fn` is not provided.
+        The extension tells which fortran standard is used.
+        The default is ``.f``, which implies F77 standard.
+
+        .. versionadded:: 1.11.0
+
+    full_output : bool, optional
+        If True, return a `subprocess.CompletedProcess` containing
+        the stdout and stderr of the compile process, instead of just
+        the status code.
+
+        .. versionadded:: 1.20.0
+
+
+    Returns
+    -------
+    result : int or `subprocess.CompletedProcess`
+        0 on success, or a `subprocess.CompletedProcess` if
+        ``full_output=True``
+
+    Examples
+    --------
+    .. literalinclude:: ../../source/f2py/code/results/compile_session.dat
+        :language: python
+
+    """
     import tempfile
+    import shlex
+
     if source_fn is None:
-        fname = os.path.join(tempfile.mktemp()+'.f')
+        f, fname = tempfile.mkstemp(suffix=extension)
+        # f is a file descriptor so need to close it
+        # carefully -- not with .close() directly
+        os.close(f)
     else:
         fname = source_fn
 
-    f = open(fname,'w')
-    f.write(source)
-    f.close()
+    if not isinstance(source, str):
+        source = str(source, 'utf-8')
+    try:
+        with open(fname, 'w') as f:
+            f.write(source)
 
-    args = ' -c -m %s %s %s'%(modulename,fname,extra_args)
-    c = '%s -c "import numpy.f2py as f2py2e;f2py2e.main()" %s' %(sys.executable,args)
-    s,o = exec_command(c)
-    if source_fn is None:
-        try: os.remove(fname)
-        except OSError: pass
-    return s
+        args = ['-c', '-m', modulename, f.name]
+
+        if isinstance(extra_args, str):
+            is_posix = (os.name == 'posix')
+            extra_args = shlex.split(extra_args, posix=is_posix)
+
+        args.extend(extra_args)
+
+        c = [sys.executable,
+             '-c',
+             'import numpy.f2py as f2py2e;f2py2e.main()'] + args
+        try:
+            cp = subprocess.run(c, stdout=subprocess.PIPE,
+                                   stderr=subprocess.PIPE)
+        except OSError:
+            # preserve historic status code used by exec_command()
+            cp = subprocess.CompletedProcess(c, 127, stdout=b'', stderr=b'')
+        else:
+            if verbose:
+                print(cp.stdout.decode())
+    finally:
+        if source_fn is None:
+            os.remove(fname)
+
+    if full_output:
+        return cp
+    else:
+        return cp.returncode
+
+
+def get_include():
+    """
+    Return the directory that contains the ``fortranobject.c`` and ``.h`` files.
+
+    .. note::
+
+        This function is not needed when building an extension with
+        `numpy.distutils` directly from ``.f`` and/or ``.pyf`` files
+        in one go.
+
+    Python extension modules built with f2py-generated code need to use
+    ``fortranobject.c`` as a source file, and include the ``fortranobject.h``
+    header. This function can be used to obtain the directory containing
+    both of these files.
+
+    Returns
+    -------
+    include_path : str
+        Absolute path to the directory containing ``fortranobject.c`` and
+        ``fortranobject.h``.
+
+    Notes
+    -----
+    .. versionadded:: 1.21.1
+
+    Unless the build system you are using has specific support for f2py,
+    building a Python extension using a ``.pyf`` signature file is a two-step
+    process. For a module ``mymod``:
+
+    * Step 1: run ``python -m numpy.f2py mymod.pyf --quiet``. This
+      generates ``_mymodmodule.c`` and (if needed)
+      ``_fblas-f2pywrappers.f`` files next to ``mymod.pyf``.
+    * Step 2: build your Python extension module. This requires the
+      following source files:
+
+      * ``_mymodmodule.c``
+      * ``_mymod-f2pywrappers.f`` (if it was generated in Step 1)
+      * ``fortranobject.c``
+
+    See Also
+    --------
+    numpy.get_include : function that returns the numpy include directory
+
+    """
+    return os.path.join(os.path.dirname(__file__), 'src')
+
+
+def __getattr__(attr):
+
+    # Avoid importing things that aren't needed for building
+    # which might import the main numpy module
+    if attr == "test":
+        from numpy._pytesttester import PytestTester
+        test = PytestTester(__name__)
+        return test
+
+    else:
+        raise AttributeError("module {!r} has no attribute "
+                              "{!r}".format(__name__, attr))
+
+
+def __dir__():
+    return list(globals().keys() | {"test"})
('numpy/f2py', 'rules.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,4 +1,4 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 """
 
 Rules for building C/API module with f2py2e.
@@ -16,13 +16,13 @@
     if (successful) {
 
       callfortran
-      if (succesful) {
+      if (successful) {
 
         put_a_to_python
-        if (succesful) {
+        if (successful) {
 
           put_b_to_python
-          if (succesful) {
+          if (successful) {
 
             buildvalue = ...
 
@@ -39,8 +39,7 @@
   cleanup_a
 
   return buildvalue
-"""
-"""
+
 Copyright 1999,2000 Pearu Peterson all rights reserved,
 Pearu Peterson <pearu@ioc.ee>
 Permission to use, modify, and distribute this software is given under the
@@ -49,201 +48,248 @@
 NO WARRANTY IS EXPRESSED OR IMPLIED.  USE AT YOUR OWN RISK.
 $Date: 2005/08/30 08:58:42 $
 Pearu Peterson
+
 """
-
-__version__ = "$Revision: 1.129 $"[10:-1]
-
-import __version__
+import os, sys
+import time
+import copy
+from pathlib import Path
+
+# __version__.version is now the same as the NumPy version
+from . import __version__
 f2py_version = __version__.version
-
-import pprint
-import sys,string,time,types,copy
-errmess=sys.stderr.write
-outmess=sys.stdout.write
-show=pprint.pprint
-
-from auxfuncs import *
-import capi_maps
-from capi_maps import *
-import cfuncs
-import common_rules
-import use_rules
-import f90mod_rules
-import func2subr
-options={}
-
-sepdict={}
+numpy_version = __version__.version
+
+from .auxfuncs import (
+    applyrules, debugcapi, dictappend, errmess, gentitle, getargs2,
+    hascallstatement, hasexternals, hasinitvalue, hasnote, hasresultnote,
+    isarray, isarrayofstrings, iscomplex, iscomplexarray,
+    iscomplexfunction, iscomplexfunction_warn, isdummyroutine, isexternal,
+    isfunction, isfunction_wrap, isint1array, isintent_aux, isintent_c,
+    isintent_callback, isintent_copy, isintent_hide, isintent_inout,
+    isintent_nothide, isintent_out, isintent_overwrite, islogical,
+    islong_complex, islong_double, islong_doublefunction, islong_long,
+    islong_longfunction, ismoduleroutine, isoptional, isrequired, isscalar,
+    issigned_long_longarray, isstring, isstringarray, isstringfunction,
+    issubroutine, issubroutine_wrap, isthreadsafe, isunsigned,
+    isunsigned_char, isunsigned_chararray, isunsigned_long_long,
+    isunsigned_long_longarray, isunsigned_short, isunsigned_shortarray,
+    l_and, l_not, l_or, outmess, replace, stripcomma, requiresf90wrapper
+)
+
+from . import capi_maps
+from . import cfuncs
+from . import common_rules
+from . import use_rules
+from . import f90mod_rules
+from . import func2subr
+
+options = {}
+sepdict = {}
 #for k in ['need_cfuncs']: sepdict[k]=','
 for k in ['decl',
           'frompyobj',
           'cleanupfrompyobj',
-          'topyarr','method',
-          'pyobjfrom','closepyobjfrom',
+          'topyarr', 'method',
+          'pyobjfrom', 'closepyobjfrom',
           'freemem',
           'userincludes',
-          'includes0','includes','typedefs','typedefs_generated',
-          'cppmacros','cfuncs','callbacks',
+          'includes0', 'includes', 'typedefs', 'typedefs_generated',
+          'cppmacros', 'cfuncs', 'callbacks',
           'latexdoc',
           'restdoc',
-          'routine_defs','externroutines',
+          'routine_defs', 'externroutines',
           'initf2pywraphooks',
-          'commonhooks','initcommonhooks',
-          'f90modhooks','initf90modhooks']:
-    sepdict[k]='\n'
+          'commonhooks', 'initcommonhooks',
+          'f90modhooks', 'initf90modhooks']:
+    sepdict[k] = '\n'
 
 #################### Rules for C/API module #################
 
-module_rules={
-    'modulebody':"""\
+generationtime = int(os.environ.get('SOURCE_DATE_EPOCH', time.time()))
+module_rules = {
+    'modulebody': """\
 /* File: #modulename#module.c
  * This file is auto-generated with f2py (version:#f2py_version#).
  * f2py is a Fortran to Python Interface Generator (FPIG), Second Edition,
  * written by Pearu Peterson <pearu@cens.ioc.ee>.
- * See http://cens.ioc.ee/projects/f2py2e/
- * Generation date: """+time.asctime(time.localtime(time.time()))+"""
- * $R"""+"""evision:$
- * $D"""+"""ate:$
+ * Generation date: """ + time.asctime(time.gmtime(generationtime)) + """
  * Do not edit this file directly unless you know what you are doing!!!
  */
+
 #ifdef __cplusplus
 extern \"C\" {
 #endif
 
-"""+gentitle("See f2py2e/cfuncs.py: includes")+"""
+#ifndef PY_SSIZE_T_CLEAN
+#define PY_SSIZE_T_CLEAN
+#endif /* PY_SSIZE_T_CLEAN */
+
+/* Unconditionally included */
+#include <Python.h>
+#include <numpy/npy_os.h>
+
+""" + gentitle("See f2py2e/cfuncs.py: includes") + """
 #includes#
 #includes0#
 
-"""+gentitle("See f2py2e/rules.py: mod_rules['modulebody']")+"""
+""" + gentitle("See f2py2e/rules.py: mod_rules['modulebody']") + """
 static PyObject *#modulename#_error;
 static PyObject *#modulename#_module;
 
-"""+gentitle("See f2py2e/cfuncs.py: typedefs")+"""
+""" + gentitle("See f2py2e/cfuncs.py: typedefs") + """
 #typedefs#
 
-"""+gentitle("See f2py2e/cfuncs.py: typedefs_generated")+"""
+""" + gentitle("See f2py2e/cfuncs.py: typedefs_generated") + """
 #typedefs_generated#
 
-"""+gentitle("See f2py2e/cfuncs.py: cppmacros")+"""
+""" + gentitle("See f2py2e/cfuncs.py: cppmacros") + """
 #cppmacros#
 
-"""+gentitle("See f2py2e/cfuncs.py: cfuncs")+"""
+""" + gentitle("See f2py2e/cfuncs.py: cfuncs") + """
 #cfuncs#
 
-"""+gentitle("See f2py2e/cfuncs.py: userincludes")+"""
+""" + gentitle("See f2py2e/cfuncs.py: userincludes") + """
 #userincludes#
 
-"""+gentitle("See f2py2e/capi_rules.py: usercode")+"""
+""" + gentitle("See f2py2e/capi_rules.py: usercode") + """
 #usercode#
 
 /* See f2py2e/rules.py */
 #externroutines#
 
-"""+gentitle("See f2py2e/capi_rules.py: usercode1")+"""
+""" + gentitle("See f2py2e/capi_rules.py: usercode1") + """
 #usercode1#
 
-"""+gentitle("See f2py2e/cb_rules.py: buildcallback")+"""
+""" + gentitle("See f2py2e/cb_rules.py: buildcallback") + """
 #callbacks#
 
-"""+gentitle("See f2py2e/rules.py: buildapi")+"""
+""" + gentitle("See f2py2e/rules.py: buildapi") + """
 #body#
 
-"""+gentitle("See f2py2e/f90mod_rules.py: buildhooks")+"""
+""" + gentitle("See f2py2e/f90mod_rules.py: buildhooks") + """
 #f90modhooks#
 
-"""+gentitle("See f2py2e/rules.py: module_rules['modulebody']")+"""
-
-"""+gentitle("See f2py2e/common_rules.py: buildhooks")+"""
+""" + gentitle("See f2py2e/rules.py: module_rules['modulebody']") + """
+
+""" + gentitle("See f2py2e/common_rules.py: buildhooks") + """
 #commonhooks#
 
-"""+gentitle("See f2py2e/rules.py")+"""
+""" + gentitle("See f2py2e/rules.py") + """
 
 static FortranDataDef f2py_routine_defs[] = {
 #routine_defs#
-\t{NULL}
+    {NULL}
 };
 
 static PyMethodDef f2py_module_methods[] = {
 #pymethoddef#
-\t{NULL,NULL}
+    {NULL,NULL}
 };
 
-PyMODINIT_FUNC init#modulename#(void) {
-\tint i;
-\tPyObject *m,*d, *s;
-\tm = #modulename#_module = Py_InitModule(\"#modulename#\", f2py_module_methods);
-\tPyFortran_Type.ob_type = &PyType_Type;
-\timport_array();
-\tif (PyErr_Occurred())
-\t\tPy_FatalError(\"can't initialize module #modulename# (failed to import numpy)\");
-\td = PyModule_GetDict(m);
-\ts = PyString_FromString(\"$R"""+"""evision: $\");
-\tPyDict_SetItemString(d, \"__version__\", s);
-\ts = PyString_FromString(\"This module '#modulename#' is auto-generated with f2py (version:#f2py_version#).\\nFunctions:\\n\"\n#docs#\".\");
-\tPyDict_SetItemString(d, \"__doc__\", s);
-\t#modulename#_error = PyErr_NewException (\"#modulename#.error\", NULL, NULL);
-\tPy_DECREF(s);
-\tfor(i=0;f2py_routine_defs[i].name!=NULL;i++)
-\t\tPyDict_SetItemString(d, f2py_routine_defs[i].name,PyFortranObject_NewAsAttr(&f2py_routine_defs[i]));
+static struct PyModuleDef moduledef = {
+    PyModuleDef_HEAD_INIT,
+    "#modulename#",
+    NULL,
+    -1,
+    f2py_module_methods,
+    NULL,
+    NULL,
+    NULL,
+    NULL
+};
+
+PyMODINIT_FUNC PyInit_#modulename#(void) {
+    int i;
+    PyObject *m,*d, *s, *tmp;
+    m = #modulename#_module = PyModule_Create(&moduledef);
+    Py_SET_TYPE(&PyFortran_Type, &PyType_Type);
+    import_array();
+    if (PyErr_Occurred())
+        {PyErr_SetString(PyExc_ImportError, \"can't initialize module #modulename# (failed to import numpy)\"); return m;}
+    d = PyModule_GetDict(m);
+    s = PyUnicode_FromString(\"#f2py_version#\");
+    PyDict_SetItemString(d, \"__version__\", s);
+    Py_DECREF(s);
+    s = PyUnicode_FromString(
+        \"This module '#modulename#' is auto-generated with f2py (version:#f2py_version#).\\nFunctions:\\n\"\n#docs#\".\");
+    PyDict_SetItemString(d, \"__doc__\", s);
+    Py_DECREF(s);
+    s = PyUnicode_FromString(\"""" + numpy_version + """\");
+    PyDict_SetItemString(d, \"__f2py_numpy_version__\", s);
+    Py_DECREF(s);
+    #modulename#_error = PyErr_NewException (\"#modulename#.error\", NULL, NULL);
+    /*
+     * Store the error object inside the dict, so that it could get deallocated.
+     * (in practice, this is a module, so it likely will not and cannot.)
+     */
+    PyDict_SetItemString(d, \"_#modulename#_error\", #modulename#_error);
+    Py_DECREF(#modulename#_error);
+    for(i=0;f2py_routine_defs[i].name!=NULL;i++) {
+        tmp = PyFortranObject_NewAsAttr(&f2py_routine_defs[i]);
+        PyDict_SetItemString(d, f2py_routine_defs[i].name, tmp);
+        Py_DECREF(tmp);
+    }
 #initf2pywraphooks#
 #initf90modhooks#
 #initcommonhooks#
 #interface_usercode#
 
 #ifdef F2PY_REPORT_ATEXIT
-\tif (! PyErr_Occurred())
-\t\ton_exit(f2py_report_on_exit,(void*)\"#modulename#\");
+    if (! PyErr_Occurred())
+        on_exit(f2py_report_on_exit,(void*)\"#modulename#\");
 #endif
-
+    return m;
 }
 #ifdef __cplusplus
 }
 #endif
 """,
-    'separatorsfor':{'latexdoc':'\n\n',
-                     'restdoc':'\n\n'},
-    'latexdoc':['\\section{Module \\texttt{#texmodulename#}}\n',
-                '#modnote#\n',
-                '#latexdoc#'],
-    'restdoc':['Module #modulename#\n'+'='*80,
-               '\n#restdoc#']
-    }
-
-defmod_rules=[
-    {'body':'/*eof body*/',
-     'method':'/*eof method*/',
-     'externroutines':'/*eof externroutines*/',
-     'routine_defs':'/*eof routine_defs*/',
-     'initf90modhooks':'/*eof initf90modhooks*/',
-     'initf2pywraphooks':'/*eof initf2pywraphooks*/',
-     'initcommonhooks':'/*eof initcommonhooks*/',
-     'latexdoc':'',
-     'restdoc':'',
-     'modnote':{hasnote:'#note#',l_not(hasnote):''},
+    'separatorsfor': {'latexdoc': '\n\n',
+                      'restdoc': '\n\n'},
+    'latexdoc': ['\\section{Module \\texttt{#texmodulename#}}\n',
+                 '#modnote#\n',
+                 '#latexdoc#'],
+    'restdoc': ['Module #modulename#\n' + '=' * 80,
+                '\n#restdoc#']
+}
+
+defmod_rules = [
+    {'body': '/*eof body*/',
+     'method': '/*eof method*/',
+     'externroutines': '/*eof externroutines*/',
+     'routine_defs': '/*eof routine_defs*/',
+     'initf90modhooks': '/*eof initf90modhooks*/',
+     'initf2pywraphooks': '/*eof initf2pywraphooks*/',
+     'initcommonhooks': '/*eof initcommonhooks*/',
+     'latexdoc': '',
+     'restdoc': '',
+     'modnote': {hasnote: '#note#', l_not(hasnote): ''},
      }
-    ]
-
-routine_rules={
-    'separatorsfor':sepdict,
-    'body':"""
+]
+
+routine_rules = {
+    'separatorsfor': sepdict,
+    'body': """
 #begintitle#
-static char doc_#apiname#[] = \"\\\nFunction signature:\\n\\\n\t#docreturn##name#(#docsignatureshort#)\\n\\\n#docstrsigns#\";
+static char doc_#apiname#[] = \"\\\n#docreturn##name#(#docsignatureshort#)\\n\\nWrapper for ``#name#``.\\\n\\n#docstrsigns#\";
 /* #declfortranroutine# */
 static PyObject *#apiname#(const PyObject *capi_self,
                            PyObject *capi_args,
                            PyObject *capi_keywds,
                            #functype# (*f2py_func)(#callprotoargument#)) {
-\tPyObject * volatile capi_buildvalue = NULL;
-\tvolatile int f2py_success = 1;
+    PyObject * volatile capi_buildvalue = NULL;
+    volatile int f2py_success = 1;
 #decl#
-\tstatic char *capi_kwlist[] = {#kwlist##kwlistopt##kwlistxa#NULL};
+    static char *capi_kwlist[] = {#kwlist##kwlistopt##kwlistxa#NULL};
 #usercode#
 #routdebugenter#
 #ifdef F2PY_REPORT_ATEXIT
 f2py_start_clock();
 #endif
-\tif (!PyArg_ParseTupleAndKeywords(capi_args,capi_keywds,\\
-\t\t\"#argformat#|#keyformat##xaformat#:#pyname#\",\\
-\t\tcapi_kwlist#args_capi##keys_capi##keys_xa#))\n\t\treturn NULL;
+    if (!PyArg_ParseTupleAndKeywords(capi_args,capi_keywds,\\
+        \"#argformat#|#keyformat##xaformat#:#pyname#\",\\
+        capi_kwlist#args_capi##keys_capi##keys_xa#))\n        return NULL;
 #frompyobj#
 /*end of frompyobj*/
 #ifdef F2PY_REPORT_ATEXIT
@@ -256,444 +302,505 @@
 f2py_stop_call_clock();
 #endif
 /*end of callfortranroutine*/
-\t\tif (f2py_success) {
+        if (f2py_success) {
 #pyobjfrom#
 /*end of pyobjfrom*/
-\t\tCFUNCSMESS(\"Building return value.\\n\");
-\t\tcapi_buildvalue = Py_BuildValue(\"#returnformat#\"#return#);
+        CFUNCSMESS(\"Building return value.\\n\");
+        capi_buildvalue = Py_BuildValue(\"#returnformat#\"#return#);
 /*closepyobjfrom*/
 #closepyobjfrom#
-\t\t} /*if (f2py_success) after callfortranroutine*/
+        } /*if (f2py_success) after callfortranroutine*/
 /*cleanupfrompyobj*/
 #cleanupfrompyobj#
-\tif (capi_buildvalue == NULL) {
+    if (capi_buildvalue == NULL) {
 #routdebugfailure#
-\t} else {
+    } else {
 #routdebugleave#
-\t}
-\tCFUNCSMESS(\"Freeing memory.\\n\");
+    }
+    CFUNCSMESS(\"Freeing memory.\\n\");
 #freemem#
 #ifdef F2PY_REPORT_ATEXIT
 f2py_stop_clock();
 #endif
-\treturn capi_buildvalue;
+    return capi_buildvalue;
 }
 #endtitle#
 """,
-    'routine_defs':'#routine_def#',
-    'initf2pywraphooks':'#initf2pywraphook#',
-    'externroutines':'#declfortranroutine#',
-    'doc':'#docreturn##name#(#docsignature#)',
-    'docshort':'#docreturn##name#(#docsignatureshort#)',
-    'docs':'"\t#docreturn##name#(#docsignature#)\\n"\n',
-    'need':['arrayobject.h','CFUNCSMESS','MINMAX'],
-    'cppmacros':{debugcapi:'#define DEBUGCFUNCS'},
-    'latexdoc':['\\subsection{Wrapper function \\texttt{#texname#}}\n',
-                """
+    'routine_defs': '#routine_def#',
+    'initf2pywraphooks': '#initf2pywraphook#',
+    'externroutines': '#declfortranroutine#',
+    'doc': '#docreturn##name#(#docsignature#)',
+    'docshort': '#docreturn##name#(#docsignatureshort#)',
+    'docs': '"    #docreturn##name#(#docsignature#)\\n"\n',
+    'need': ['arrayobject.h', 'CFUNCSMESS', 'MINMAX'],
+    'cppmacros': {debugcapi: '#define DEBUGCFUNCS'},
+    'latexdoc': ['\\subsection{Wrapper function \\texttt{#texname#}}\n',
+                 """
 \\noindent{{}\\verb@#docreturn##name#@{}}\\texttt{(#latexdocsignatureshort#)}
 #routnote#
 
 #latexdocstrsigns#
 """],
-    'restdoc':['Wrapped function ``#name#``\n'+'-'*80,
-
-               ]
-    }
+    'restdoc': ['Wrapped function ``#name#``\n' + '-' * 80,
+
+                ]
+}
 
 ################## Rules for C/API function ##############
 
-rout_rules=[
-    { # Init
-    'separatorsfor': {'callfortranroutine':'\n','routdebugenter':'\n','decl':'\n',
-                      'routdebugleave':'\n','routdebugfailure':'\n',
-                      'setjmpbuf':' || ',
-                      'docstrreq':'\n','docstropt':'\n','docstrout':'\n',
-                      'docstrcbs':'\n','docstrsigns':'\\n"\n"',
-                      'latexdocstrsigns':'\n',
-                      'latexdocstrreq':'\n','latexdocstropt':'\n',
-                      'latexdocstrout':'\n','latexdocstrcbs':'\n',
-                      },
-    'kwlist':'','kwlistopt':'','callfortran':'','callfortranappend':'',
-    'docsign':'','docsignopt':'','decl':'/*decl*/',
-    'freemem':'/*freemem*/',
-    'docsignshort':'','docsignoptshort':'',
-    'docstrsigns':'','latexdocstrsigns':'',
-    'docstrreq':'Required arguments:',
-    'docstropt':'Optional arguments:',
-    'docstrout':'Return objects:',
-    'docstrcbs':'Call-back functions:',
-    'latexdocstrreq':'\\noindent Required arguments:',
-    'latexdocstropt':'\\noindent Optional arguments:',
-    'latexdocstrout':'\\noindent Return objects:',
-    'latexdocstrcbs':'\\noindent Call-back functions:',
-    'args_capi':'','keys_capi':'','functype':'',
-    'frompyobj':'/*frompyobj*/',
-    'cleanupfrompyobj':['/*end of cleanupfrompyobj*/'], #this list will be reversed
-    'pyobjfrom':'/*pyobjfrom*/',
-    'closepyobjfrom':['/*end of closepyobjfrom*/'], #this list will be reversed
-    'topyarr':'/*topyarr*/','routdebugleave':'/*routdebugleave*/',
-    'routdebugenter':'/*routdebugenter*/',
-    'routdebugfailure':'/*routdebugfailure*/',
-    'callfortranroutine':'/*callfortranroutine*/',
-    'argformat':'','keyformat':'','need_cfuncs':'',
-    'docreturn':'','return':'','returnformat':'','rformat':'',
-    'kwlistxa':'','keys_xa':'','xaformat':'','docsignxa':'','docsignxashort':'',
-    'initf2pywraphook':'',
-    'routnote':{hasnote:'--- #note#',l_not(hasnote):''},
-    },{
-        'apiname':'f2py_rout_#modulename#_#name#',
-        'pyname':'#modulename#.#name#',
-        'decl':'',
-        '_check':l_not(ismoduleroutine)
-    },{
-        'apiname':'f2py_rout_#modulename#_#f90modulename#_#name#',
-        'pyname':'#modulename#.#f90modulename#.#name#',
-        'decl':'',
-        '_check':ismoduleroutine
-    },{ # Subroutine
-    'functype':'void',
-    'declfortranroutine':{l_and(l_not(l_or(ismoduleroutine,isintent_c)),l_not(isdummyroutine)):'extern void #F_FUNC#(#fortranname#,#FORTRANNAME#)(#callprotoargument#);',
-                          l_and(l_not(ismoduleroutine),isintent_c,l_not(isdummyroutine)):'extern void #fortranname#(#callprotoargument#);',
-                          ismoduleroutine:'',
-                          isdummyroutine:''
+rout_rules = [
+    {  # Init
+        'separatorsfor': {'callfortranroutine': '\n', 'routdebugenter': '\n', 'decl': '\n',
+                          'routdebugleave': '\n', 'routdebugfailure': '\n',
+                          'setjmpbuf': ' || ',
+                          'docstrreq': '\n', 'docstropt': '\n', 'docstrout': '\n',
+                          'docstrcbs': '\n', 'docstrsigns': '\\n"\n"',
+                          'latexdocstrsigns': '\n',
+                          'latexdocstrreq': '\n', 'latexdocstropt': '\n',
+                          'latexdocstrout': '\n', 'latexdocstrcbs': '\n',
                           },
-    'routine_def':{l_not(l_or(ismoduleroutine,isintent_c,isdummyroutine)):'\t{\"#name#\",-1,{{-1}},0,(char *)#F_FUNC#(#fortranname#,#FORTRANNAME#),(f2py_init_func)#apiname#,doc_#apiname#},',
-                   l_and(l_not(ismoduleroutine),isintent_c,l_not(isdummyroutine)):'\t{\"#name#\",-1,{{-1}},0,(char *)#fortranname#,(f2py_init_func)#apiname#,doc_#apiname#},',
-                   l_and(l_not(ismoduleroutine),isdummyroutine):'\t{\"#name#\",-1,{{-1}},0,NULL,(f2py_init_func)#apiname#,doc_#apiname#},',
-                   },
-    'need':{l_and(l_not(l_or(ismoduleroutine,isintent_c)),l_not(isdummyroutine)):'F_FUNC'},
-    'callfortranroutine':[
-        {debugcapi:["""\tfprintf(stderr,\"debug-capi:Fortran subroutine `#fortranname#(#callfortran#)\'\\n\");"""]},
-        {hasexternals:"""\
-\t\tif (#setjmpbuf#) {
-\t\t\tf2py_success = 0;
-\t\t} else {"""},
-        {isthreadsafe:'\t\t\tPy_BEGIN_ALLOW_THREADS'},
-        {hascallstatement:'''\t\t\t\t#callstatement#;
-\t\t\t\t/*(*f2py_func)(#callfortran#);*/'''},
-        {l_not(l_or(hascallstatement,isdummyroutine)):'\t\t\t\t(*f2py_func)(#callfortran#);'},
-        {isthreadsafe:'\t\t\tPy_END_ALLOW_THREADS'},
-        {hasexternals:"""\t\t}"""}
-         ],
-    '_check':issubroutine,
-    },{ # Wrapped function
-    'functype':'void',
-    'declfortranroutine':{l_not(l_or(ismoduleroutine,isdummyroutine)):'extern void #F_WRAPPEDFUNC#(#name_lower#,#NAME#)(#callprotoargument#);',
-                          isdummyroutine:'',
-                          },
-
-    'routine_def':{l_not(l_or(ismoduleroutine,isdummyroutine)):'\t{\"#name#\",-1,{{-1}},0,(char *)#F_WRAPPEDFUNC#(#name_lower#,#NAME#),(f2py_init_func)#apiname#,doc_#apiname#},',
-                   isdummyroutine:'\t{\"#name#\",-1,{{-1}},0,NULL,(f2py_init_func)#apiname#,doc_#apiname#},',
-                   },
-    'initf2pywraphook':{l_not(l_or(ismoduleroutine,isdummyroutine)):'''
+        'kwlist': '', 'kwlistopt': '', 'callfortran': '', 'callfortranappend': '',
+        'docsign': '', 'docsignopt': '', 'decl': '/*decl*/',
+        'freemem': '/*freemem*/',
+        'docsignshort': '', 'docsignoptshort': '',
+        'docstrsigns': '', 'latexdocstrsigns': '',
+        'docstrreq': '\\nParameters\\n----------',
+        'docstropt': '\\nOther Parameters\\n----------------',
+        'docstrout': '\\nReturns\\n-------',
+        'docstrcbs': '\\nNotes\\n-----\\nCall-back functions::\\n',
+        'latexdocstrreq': '\\noindent Required arguments:',
+        'latexdocstropt': '\\noindent Optional arguments:',
+        'latexdocstrout': '\\noindent Return objects:',
+        'latexdocstrcbs': '\\noindent Call-back functions:',
+        'args_capi': '', 'keys_capi': '', 'functype': '',
+        'frompyobj': '/*frompyobj*/',
+        # this list will be reversed
+        'cleanupfrompyobj': ['/*end of cleanupfrompyobj*/'],
+        'pyobjfrom': '/*pyobjfrom*/',
+        # this list will be reversed
+        'closepyobjfrom': ['/*end of closepyobjfrom*/'],
+        'topyarr': '/*topyarr*/', 'routdebugleave': '/*routdebugleave*/',
+        'routdebugenter': '/*routdebugenter*/',
+        'routdebugfailure': '/*routdebugfailure*/',
+        'callfortranroutine': '/*callfortranroutine*/',
+        'argformat': '', 'keyformat': '', 'need_cfuncs': '',
+        'docreturn': '', 'return': '', 'returnformat': '', 'rformat': '',
+        'kwlistxa': '', 'keys_xa': '', 'xaformat': '', 'docsignxa': '', 'docsignxashort': '',
+        'initf2pywraphook': '',
+        'routnote': {hasnote: '--- #note#', l_not(hasnote): ''},
+    }, {
+        'apiname': 'f2py_rout_#modulename#_#name#',
+        'pyname': '#modulename#.#name#',
+        'decl': '',
+        '_check': l_not(ismoduleroutine)
+    }, {
+        'apiname': 'f2py_rout_#modulename#_#f90modulename#_#name#',
+        'pyname': '#modulename#.#f90modulename#.#name#',
+        'decl': '',
+        '_check': ismoduleroutine
+    }, {  # Subroutine
+        'functype': 'void',
+        'declfortranroutine': {l_and(l_not(l_or(ismoduleroutine, isintent_c)), l_not(isdummyroutine)): 'extern void #F_FUNC#(#fortranname#,#FORTRANNAME#)(#callprotoargument#);',
+                               l_and(l_not(ismoduleroutine), isintent_c, l_not(isdummyroutine)): 'extern void #fortranname#(#callprotoargument#);',
+                               ismoduleroutine: '',
+                               isdummyroutine: ''
+                               },
+        'routine_def': {l_not(l_or(ismoduleroutine, isintent_c, isdummyroutine)): '    {\"#name#\",-1,{{-1}},0,(char *)#F_FUNC#(#fortranname#,#FORTRANNAME#),(f2py_init_func)#apiname#,doc_#apiname#},',
+                        l_and(l_not(ismoduleroutine), isintent_c, l_not(isdummyroutine)): '    {\"#name#\",-1,{{-1}},0,(char *)#fortranname#,(f2py_init_func)#apiname#,doc_#apiname#},',
+                        l_and(l_not(ismoduleroutine), isdummyroutine): '    {\"#name#\",-1,{{-1}},0,NULL,(f2py_init_func)#apiname#,doc_#apiname#},',
+                        },
+        'need': {l_and(l_not(l_or(ismoduleroutine, isintent_c)), l_not(isdummyroutine)): 'F_FUNC'},
+        'callfortranroutine': [
+            {debugcapi: [
+                """    fprintf(stderr,\"debug-capi:Fortran subroutine `#fortranname#(#callfortran#)\'\\n\");"""]},
+            {hasexternals: """\
+        if (#setjmpbuf#) {
+            f2py_success = 0;
+        } else {"""},
+            {isthreadsafe: '            Py_BEGIN_ALLOW_THREADS'},
+            {hascallstatement: '''                #callstatement#;
+                /*(*f2py_func)(#callfortran#);*/'''},
+            {l_not(l_or(hascallstatement, isdummyroutine))
+                   : '                (*f2py_func)(#callfortran#);'},
+            {isthreadsafe: '            Py_END_ALLOW_THREADS'},
+            {hasexternals: """        }"""}
+        ],
+        '_check': l_and(issubroutine, l_not(issubroutine_wrap)),
+    }, {  # Wrapped function
+        'functype': 'void',
+        'declfortranroutine': {l_not(l_or(ismoduleroutine, isdummyroutine)): 'extern void #F_WRAPPEDFUNC#(#name_lower#,#NAME#)(#callprotoargument#);',
+                               isdummyroutine: '',
+                               },
+
+        'routine_def': {l_not(l_or(ismoduleroutine, isdummyroutine)): '    {\"#name#\",-1,{{-1}},0,(char *)#F_WRAPPEDFUNC#(#name_lower#,#NAME#),(f2py_init_func)#apiname#,doc_#apiname#},',
+                        isdummyroutine: '    {\"#name#\",-1,{{-1}},0,NULL,(f2py_init_func)#apiname#,doc_#apiname#},',
+                        },
+        'initf2pywraphook': {l_not(l_or(ismoduleroutine, isdummyroutine)): '''
     {
       extern #ctype# #F_FUNC#(#name_lower#,#NAME#)(void);
       PyObject* o = PyDict_GetItemString(d,"#name#");
-      PyObject_SetAttrString(o,"_cpointer", PyCObject_FromVoidPtr((void*)#F_FUNC#(#name_lower#,#NAME#),NULL));
+      tmp = F2PyCapsule_FromVoidPtr((void*)#F_FUNC#(#name_lower#,#NAME#),NULL);
+      PyObject_SetAttrString(o,"_cpointer", tmp);
+      Py_DECREF(tmp);
+      s = PyUnicode_FromString("#name#");
+      PyObject_SetAttrString(o,"__name__", s);
+      Py_DECREF(s);
     }
     '''},
-    'need':{l_not(l_or(ismoduleroutine,isdummyroutine)):['F_WRAPPEDFUNC','F_FUNC']},
-    'callfortranroutine':[
-    {debugcapi:["""\tfprintf(stderr,\"debug-capi:Fortran subroutine `f2pywrap#name_lower#(#callfortran#)\'\\n\");"""]},
-    {hasexternals:"""\
-\tif (#setjmpbuf#) {
-\t\tf2py_success = 0;
-\t} else {"""},
-    {isthreadsafe:'\tPy_BEGIN_ALLOW_THREADS'},
-    {l_not(l_or(hascallstatement,isdummyroutine)):'\t(*f2py_func)(#callfortran#);'},
-    {hascallstatement:'\t#callstatement#;\n\t/*(*f2py_func)(#callfortran#);*/'},
-    {isthreadsafe:'\tPy_END_ALLOW_THREADS'},
-    {hasexternals:'\t}'}
-    ],
-    '_check':isfunction_wrap,
-    },{ # Function
-    'functype':'#ctype#',
-    'docreturn':{l_not(isintent_hide):'#rname#,'},
-    'docstrout':'\t#pydocsignout#',
-    'latexdocstrout':['\\item[]{{}\\verb@#pydocsignout#@{}}',
-                      {hasresultnote:'--- #resultnote#'}],
-    'callfortranroutine':[{l_and(debugcapi,isstringfunction):"""\
+        'need': {l_not(l_or(ismoduleroutine, isdummyroutine)): ['F_WRAPPEDFUNC', 'F_FUNC']},
+        'callfortranroutine': [
+            {debugcapi: [
+                """    fprintf(stderr,\"debug-capi:Fortran subroutine `f2pywrap#name_lower#(#callfortran#)\'\\n\");"""]},
+            {hasexternals: """\
+    if (#setjmpbuf#) {
+        f2py_success = 0;
+    } else {"""},
+            {isthreadsafe: '    Py_BEGIN_ALLOW_THREADS'},
+            {l_not(l_or(hascallstatement, isdummyroutine))
+                   : '    (*f2py_func)(#callfortran#);'},
+            {hascallstatement:
+                '    #callstatement#;\n    /*(*f2py_func)(#callfortran#);*/'},
+            {isthreadsafe: '    Py_END_ALLOW_THREADS'},
+            {hasexternals: '    }'}
+        ],
+        '_check': isfunction_wrap,
+    }, {  # Wrapped subroutine
+        'functype': 'void',
+        'declfortranroutine': {l_not(l_or(ismoduleroutine, isdummyroutine)): 'extern void #F_WRAPPEDFUNC#(#name_lower#,#NAME#)(#callprotoargument#);',
+                               isdummyroutine: '',
+                               },
+
+        'routine_def': {l_not(l_or(ismoduleroutine, isdummyroutine)): '    {\"#name#\",-1,{{-1}},0,(char *)#F_WRAPPEDFUNC#(#name_lower#,#NAME#),(f2py_init_func)#apiname#,doc_#apiname#},',
+                        isdummyroutine: '    {\"#name#\",-1,{{-1}},0,NULL,(f2py_init_func)#apiname#,doc_#apiname#},',
+                        },
+        'initf2pywraphook': {l_not(l_or(ismoduleroutine, isdummyroutine)): '''
+    {
+      extern void #F_FUNC#(#name_lower#,#NAME#)(void);
+      PyObject* o = PyDict_GetItemString(d,"#name#");
+      tmp = F2PyCapsule_FromVoidPtr((void*)#F_FUNC#(#name_lower#,#NAME#),NULL);
+      PyObject_SetAttrString(o,"_cpointer", tmp);
+      Py_DECREF(tmp);
+      s = PyUnicode_FromString("#name#");
+      PyObject_SetAttrString(o,"__name__", s);
+      Py_DECREF(s);
+    }
+    '''},
+        'need': {l_not(l_or(ismoduleroutine, isdummyroutine)): ['F_WRAPPEDFUNC', 'F_FUNC']},
+        'callfortranroutine': [
+            {debugcapi: [
+                """    fprintf(stderr,\"debug-capi:Fortran subroutine `f2pywrap#name_lower#(#callfortran#)\'\\n\");"""]},
+            {hasexternals: """\
+    if (#setjmpbuf#) {
+        f2py_success = 0;
+    } else {"""},
+            {isthreadsafe: '    Py_BEGIN_ALLOW_THREADS'},
+            {l_not(l_or(hascallstatement, isdummyroutine))
+                   : '    (*f2py_func)(#callfortran#);'},
+            {hascallstatement:
+                '    #callstatement#;\n    /*(*f2py_func)(#callfortran#);*/'},
+            {isthreadsafe: '    Py_END_ALLOW_THREADS'},
+            {hasexternals: '    }'}
+        ],
+        '_check': issubroutine_wrap,
+    }, {  # Function
+        'functype': '#ctype#',
+        'docreturn': {l_not(isintent_hide): '#rname#,'},
+        'docstrout': '#pydocsignout#',
+        'latexdocstrout': ['\\item[]{{}\\verb@#pydocsignout#@{}}',
+                           {hasresultnote: '--- #resultnote#'}],
+        'callfortranroutine': [{l_and(debugcapi, isstringfunction): """\
 #ifdef USESCOMPAQFORTRAN
-\tfprintf(stderr,\"debug-capi:Fortran function #ctype# #fortranname#(#callcompaqfortran#)\\n\");
+    fprintf(stderr,\"debug-capi:Fortran function #ctype# #fortranname#(#callcompaqfortran#)\\n\");
 #else
-\tfprintf(stderr,\"debug-capi:Fortran function #ctype# #fortranname#(#callfortran#)\\n\");
+    fprintf(stderr,\"debug-capi:Fortran function #ctype# #fortranname#(#callfortran#)\\n\");
 #endif
 """},
-                          {l_and(debugcapi,l_not(isstringfunction)):"""\
-\tfprintf(stderr,\"debug-capi:Fortran function #ctype# #fortranname#(#callfortran#)\\n\");
+                               {l_and(debugcapi, l_not(isstringfunction)): """\
+    fprintf(stderr,\"debug-capi:Fortran function #ctype# #fortranname#(#callfortran#)\\n\");
 """}
-                          ],
-    '_check':l_and(isfunction,l_not(isfunction_wrap))
-    },{ # Scalar function
-    'declfortranroutine':{l_and(l_not(l_or(ismoduleroutine,isintent_c)),l_not(isdummyroutine)):'extern #ctype# #F_FUNC#(#fortranname#,#FORTRANNAME#)(#callprotoargument#);',
-                          l_and(l_not(ismoduleroutine),isintent_c,l_not(isdummyroutine)):'extern #ctype# #fortranname#(#callprotoargument#);',
-                          isdummyroutine:''
-                          },
-    'routine_def':{l_and(l_not(l_or(ismoduleroutine,isintent_c)),l_not(isdummyroutine)):'\t{\"#name#\",-1,{{-1}},0,(char *)#F_FUNC#(#fortranname#,#FORTRANNAME#),(f2py_init_func)#apiname#,doc_#apiname#},',
-                   l_and(l_not(ismoduleroutine),isintent_c,l_not(isdummyroutine)):'\t{\"#name#\",-1,{{-1}},0,(char *)#fortranname#,(f2py_init_func)#apiname#,doc_#apiname#},',
-                   isdummyroutine:'\t{\"#name#\",-1,{{-1}},0,NULL,(f2py_init_func)#apiname#,doc_#apiname#},',
-                   },
-    'decl':[{iscomplexfunction_warn:'\t#ctype# #name#_return_value={0,0};',
-             l_not(iscomplexfunction):'\t#ctype# #name#_return_value=0;'},
-            {iscomplexfunction:'\tPyObject *#name#_return_value_capi = Py_None;'}
-            ],
-    'callfortranroutine':[
-        {hasexternals:"""\
-\tif (#setjmpbuf#) {
-\t\tf2py_success = 0;
-\t} else {"""},
-        {isthreadsafe:'\tPy_BEGIN_ALLOW_THREADS'},
-        {hascallstatement:'''\t#callstatement#;
-/*\t#name#_return_value = (*f2py_func)(#callfortran#);*/
+                               ],
+        '_check': l_and(isfunction, l_not(isfunction_wrap))
+    }, {  # Scalar function
+        'declfortranroutine': {l_and(l_not(l_or(ismoduleroutine, isintent_c)), l_not(isdummyroutine)): 'extern #ctype# #F_FUNC#(#fortranname#,#FORTRANNAME#)(#callprotoargument#);',
+                               l_and(l_not(ismoduleroutine), isintent_c, l_not(isdummyroutine)): 'extern #ctype# #fortranname#(#callprotoargument#);',
+                               isdummyroutine: ''
+                               },
+        'routine_def': {l_and(l_not(l_or(ismoduleroutine, isintent_c)), l_not(isdummyroutine)): '    {\"#name#\",-1,{{-1}},0,(char *)#F_FUNC#(#fortranname#,#FORTRANNAME#),(f2py_init_func)#apiname#,doc_#apiname#},',
+                        l_and(l_not(ismoduleroutine), isintent_c, l_not(isdummyroutine)): '    {\"#name#\",-1,{{-1}},0,(char *)#fortranname#,(f2py_init_func)#apiname#,doc_#apiname#},',
+                        isdummyroutine: '    {\"#name#\",-1,{{-1}},0,NULL,(f2py_init_func)#apiname#,doc_#apiname#},',
+                        },
+        'decl': [{iscomplexfunction_warn: '    #ctype# #name#_return_value={0,0};',
+                  l_not(iscomplexfunction): '    #ctype# #name#_return_value=0;'},
+                 {iscomplexfunction:
+                  '    PyObject *#name#_return_value_capi = Py_None;'}
+                 ],
+        'callfortranroutine': [
+            {hasexternals: """\
+    if (#setjmpbuf#) {
+        f2py_success = 0;
+    } else {"""},
+            {isthreadsafe: '    Py_BEGIN_ALLOW_THREADS'},
+            {hascallstatement: '''    #callstatement#;
+/*    #name#_return_value = (*f2py_func)(#callfortran#);*/
 '''},
-        {l_not(l_or(hascallstatement,isdummyroutine)):'\t#name#_return_value = (*f2py_func)(#callfortran#);'},
-        {isthreadsafe:'\tPy_END_ALLOW_THREADS'},
-        {hasexternals:'\t}'},
-        {l_and(debugcapi,iscomplexfunction):'\tfprintf(stderr,"#routdebugshowvalue#\\n",#name#_return_value.r,#name#_return_value.i);'},
-        {l_and(debugcapi,l_not(iscomplexfunction)):'\tfprintf(stderr,"#routdebugshowvalue#\\n",#name#_return_value);'}],
-    'pyobjfrom':{iscomplexfunction:'\t#name#_return_value_capi = pyobj_from_#ctype#1(#name#_return_value);'},
-    'need':[{l_not(isdummyroutine):'F_FUNC'},
-    {iscomplexfunction:'pyobj_from_#ctype#1'},
-    {islong_longfunction:'long_long'},
-    {islong_doublefunction:'long_double'}],
-    'returnformat':{l_not(isintent_hide):'#rformat#'},
-    'return':{iscomplexfunction:',#name#_return_value_capi',
-    l_not(l_or(iscomplexfunction,isintent_hide)):',#name#_return_value'},
-    '_check':l_and(isfunction,l_not(isstringfunction),l_not(isfunction_wrap))
-    },{ # String function # in use for --no-wrap
-    'declfortranroutine':'extern void #F_FUNC#(#fortranname#,#FORTRANNAME#)(#callprotoargument#);',
-    'routine_def':{l_not(l_or(ismoduleroutine,isintent_c)):
-#        '\t{\"#name#\",-1,{{-1}},0,(char *)F_FUNC(#fortranname#,#FORTRANNAME#),(void *)#apiname#,doc_#apiname#},',
-        '\t{\"#name#\",-1,{{-1}},0,(char *)#F_FUNC#(#fortranname#,#FORTRANNAME#),(f2py_init_func)#apiname#,doc_#apiname#},',
-        l_and(l_not(ismoduleroutine),isintent_c):
-#            '\t{\"#name#\",-1,{{-1}},0,(char *)#fortranname#,(void *)#apiname#,doc_#apiname#},'
-            '\t{\"#name#\",-1,{{-1}},0,(char *)#fortranname#,(f2py_init_func)#apiname#,doc_#apiname#},'
-                   },
-    'decl':['\t#ctype# #name#_return_value = NULL;',
-            '\tint #name#_return_value_len = 0;'],
-    'callfortran':'#name#_return_value,#name#_return_value_len,',
-    'callfortranroutine':['\t#name#_return_value_len = #rlength#;',
-                          '\tif ((#name#_return_value = (string)malloc(sizeof(char)*(#name#_return_value_len+1))) == NULL) {',
-                          '\t\tPyErr_SetString(PyExc_MemoryError, \"out of memory\");',
-                          '\t\tf2py_success = 0;',
-                          '\t} else {',
-                          "\t\t(#name#_return_value)[#name#_return_value_len] = '\\0';",
-                          '\t}',
-                          '\tif (f2py_success) {',
-                          {hasexternals:"""\
-\t\tif (#setjmpbuf#) {
-\t\t\tf2py_success = 0;
-\t\t} else {"""},
-                          {isthreadsafe:'\t\tPy_BEGIN_ALLOW_THREADS'},
-                          """\
+            {l_not(l_or(hascallstatement, isdummyroutine))
+                   : '    #name#_return_value = (*f2py_func)(#callfortran#);'},
+            {isthreadsafe: '    Py_END_ALLOW_THREADS'},
+            {hasexternals: '    }'},
+            {l_and(debugcapi, iscomplexfunction)
+                   : '    fprintf(stderr,"#routdebugshowvalue#\\n",#name#_return_value.r,#name#_return_value.i);'},
+            {l_and(debugcapi, l_not(iscomplexfunction)): '    fprintf(stderr,"#routdebugshowvalue#\\n",#name#_return_value);'}],
+        'pyobjfrom': {iscomplexfunction: '    #name#_return_value_capi = pyobj_from_#ctype#1(#name#_return_value);'},
+        'need': [{l_not(isdummyroutine): 'F_FUNC'},
+                 {iscomplexfunction: 'pyobj_from_#ctype#1'},
+                 {islong_longfunction: 'long_long'},
+                 {islong_doublefunction: 'long_double'}],
+        'returnformat': {l_not(isintent_hide): '#rformat#'},
+        'return': {iscomplexfunction: ',#name#_return_value_capi',
+                   l_not(l_or(iscomplexfunction, isintent_hide)): ',#name#_return_value'},
+        '_check': l_and(isfunction, l_not(isstringfunction), l_not(isfunction_wrap))
+    }, {  # String function # in use for --no-wrap
+        'declfortranroutine': 'extern void #F_FUNC#(#fortranname#,#FORTRANNAME#)(#callprotoargument#);',
+        'routine_def': {l_not(l_or(ismoduleroutine, isintent_c)):
+                        '    {\"#name#\",-1,{{-1}},0,(char *)#F_FUNC#(#fortranname#,#FORTRANNAME#),(f2py_init_func)#apiname#,doc_#apiname#},',
+                        l_and(l_not(ismoduleroutine), isintent_c):
+                        '    {\"#name#\",-1,{{-1}},0,(char *)#fortranname#,(f2py_init_func)#apiname#,doc_#apiname#},'
+                        },
+        'decl': ['    #ctype# #name#_return_value = NULL;',
+                 '    int #name#_return_value_len = 0;'],
+        'callfortran':'#name#_return_value,#name#_return_value_len,',
+        'callfortranroutine':['    #name#_return_value_len = #rlength#;',
+                              '    if ((#name#_return_value = (string)malloc('
+                              + '#name#_return_value_len+1) == NULL) {',
+                              '        PyErr_SetString(PyExc_MemoryError, \"out of memory\");',
+                              '        f2py_success = 0;',
+                              '    } else {',
+                              "        (#name#_return_value)[#name#_return_value_len] = '\\0';",
+                              '    }',
+                              '    if (f2py_success) {',
+                              {hasexternals: """\
+        if (#setjmpbuf#) {
+            f2py_success = 0;
+        } else {"""},
+                              {isthreadsafe: '        Py_BEGIN_ALLOW_THREADS'},
+                              """\
 #ifdef USESCOMPAQFORTRAN
-\t\t(*f2py_func)(#callcompaqfortran#);
+        (*f2py_func)(#callcompaqfortran#);
 #else
-\t\t(*f2py_func)(#callfortran#);
+        (*f2py_func)(#callfortran#);
 #endif
 """,
-                          {isthreadsafe:'\t\tPy_END_ALLOW_THREADS'},
-                          {hasexternals:'\t\t}'},
-                          {debugcapi:'\t\tfprintf(stderr,"#routdebugshowvalue#\\n",#name#_return_value_len,#name#_return_value);'},
-                          '\t} /* if (f2py_success) after (string)malloc */',
-                          ],
-    'returnformat':'#rformat#',
-    'return':',#name#_return_value',
-    'freemem':'\tSTRINGFREE(#name#_return_value);',
-    'need':['F_FUNC','#ctype#','STRINGFREE'],
-    '_check':l_and(isstringfunction,l_not(isfunction_wrap)) # ???obsolete
-    },
-    { # Debugging
-    'routdebugenter':'\tfprintf(stderr,"debug-capi:Python C/API function #modulename#.#name#(#docsignature#)\\n");',
-    'routdebugleave':'\tfprintf(stderr,"debug-capi:Python C/API function #modulename#.#name#: successful.\\n");',
-    'routdebugfailure':'\tfprintf(stderr,"debug-capi:Python C/API function #modulename#.#name#: failure.\\n");',
-    '_check':debugcapi
+                              {isthreadsafe: '        Py_END_ALLOW_THREADS'},
+                              {hasexternals: '        }'},
+                              {debugcapi:
+                                  '        fprintf(stderr,"#routdebugshowvalue#\\n",#name#_return_value_len,#name#_return_value);'},
+                              '    } /* if (f2py_success) after (string)malloc */',
+                              ],
+        'returnformat': '#rformat#',
+        'return': ',#name#_return_value',
+        'freemem': '    STRINGFREE(#name#_return_value);',
+        'need': ['F_FUNC', '#ctype#', 'STRINGFREE'],
+        '_check':l_and(isstringfunction, l_not(isfunction_wrap))  # ???obsolete
+    },
+    {  # Debugging
+        'routdebugenter': '    fprintf(stderr,"debug-capi:Python C/API function #modulename#.#name#(#docsignature#)\\n");',
+        'routdebugleave': '    fprintf(stderr,"debug-capi:Python C/API function #modulename#.#name#: successful.\\n");',
+        'routdebugfailure': '    fprintf(stderr,"debug-capi:Python C/API function #modulename#.#name#: failure.\\n");',
+        '_check': debugcapi
     }
-    ]
+]
 
 ################ Rules for arguments ##################
 
-typedef_need_dict = {islong_long:'long_long',
-            islong_double:'long_double',
-            islong_complex:'complex_long_double',
-            isunsigned_char:'unsigned_char',
-            isunsigned_short:'unsigned_short',
-            isunsigned:'unsigned',
-            isunsigned_long_long:'unsigned_long_long'}
-
-aux_rules=[
+typedef_need_dict = {islong_long: 'long_long',
+                     islong_double: 'long_double',
+                     islong_complex: 'complex_long_double',
+                     isunsigned_char: 'unsigned_char',
+                     isunsigned_short: 'unsigned_short',
+                     isunsigned: 'unsigned',
+                     isunsigned_long_long: 'unsigned_long_long',
+                     isunsigned_chararray: 'unsigned_char',
+                     isunsigned_shortarray: 'unsigned_short',
+                     isunsigned_long_longarray: 'unsigned_long_long',
+                     issigned_long_longarray: 'long_long',
+                     }
+
+aux_rules = [
     {
-    'separatorsfor':sepdict
-    },
-    { # Common
-    'frompyobj':['\t/* Processing auxiliary variable #varname# */',
-                 {debugcapi:'\tfprintf(stderr,"#vardebuginfo#\\n");'},],
-    'cleanupfrompyobj':'\t/* End of cleaning variable #varname# */',
-    'need':typedef_need_dict,
-    },
-# Scalars (not complex)
-    { # Common
-    'decl':'\t#ctype# #varname# = 0;',
-    'need':{hasinitvalue:'math.h'},
-    'frompyobj':{hasinitvalue:'\t#varname# = #init#;'},
-    '_check':l_and(isscalar,l_not(iscomplex)),
+        'separatorsfor': sepdict
+    },
+    {  # Common
+        'frompyobj': ['    /* Processing auxiliary variable #varname# */',
+                      {debugcapi: '    fprintf(stderr,"#vardebuginfo#\\n");'}, ],
+        'cleanupfrompyobj': '    /* End of cleaning variable #varname# */',
+        'need': typedef_need_dict,
+    },
+    # Scalars (not complex)
+    {  # Common
+        'decl': '    #ctype# #varname# = 0;',
+        'need': {hasinitvalue: 'math.h'},
+        'frompyobj': {hasinitvalue: '    #varname# = #init#;'},
+        '_check': l_and(isscalar, l_not(iscomplex)),
     },
     {
-    'return':',#varname#',
-    'docstrout':'\t#pydocsignout#',
-    'docreturn':'#outvarname#,',
-    'returnformat':'#varrformat#',
-    '_check':l_and(isscalar,l_not(iscomplex),isintent_out),
-    },
-# Complex scalars
-    { # Common
-    'decl':'\t#ctype# #varname#;',
-    'frompyobj': {hasinitvalue:'\t#varname#.r = #init.r#, #varname#.i = #init.i#;'},
-    '_check':iscomplex
-    },
-# String
-    { # Common
-    'decl':['\t#ctype# #varname# = NULL;',
-            '\tint slen(#varname#);',
-            ],
-    'need':['len..'],
-    '_check':isstring
-    },
-# Array
-    { # Common
-    'decl':['\t#ctype# *#varname# = NULL;',
-            '\tintp #varname#_Dims[#rank#] = {#rank*[-1]#};',
-            '\tconst int #varname#_Rank = #rank#;',
-            ],
-    'need':['len..',{hasinitvalue:'forcomb'},{hasinitvalue:'CFUNCSMESS'}],
-    '_check':isarray
-    },
-# Scalararray
-    { # Common
-    '_check':l_and(isarray,l_not(iscomplexarray))
-    },{ # Not hidden
-    '_check':l_and(isarray,l_not(iscomplexarray),isintent_nothide)
-    },
-# Integer*1 array
-    {'need':'#ctype#',
-     '_check':isint1array,
-     '_depend':''
+        'return': ',#varname#',
+        'docstrout': '#pydocsignout#',
+        'docreturn': '#outvarname#,',
+        'returnformat': '#varrformat#',
+        '_check': l_and(isscalar, l_not(iscomplex), isintent_out),
+    },
+    # Complex scalars
+    {  # Common
+        'decl': '    #ctype# #varname#;',
+        'frompyobj': {hasinitvalue: '    #varname#.r = #init.r#, #varname#.i = #init.i#;'},
+        '_check': iscomplex
+    },
+    # String
+    {  # Common
+        'decl': ['    #ctype# #varname# = NULL;',
+                 '    int slen(#varname#);',
+                 ],
+        'need':['len..'],
+        '_check':isstring
+    },
+    # Array
+    {  # Common
+        'decl': ['    #ctype# *#varname# = NULL;',
+                 '    npy_intp #varname#_Dims[#rank#] = {#rank*[-1]#};',
+                 '    const int #varname#_Rank = #rank#;',
+                 ],
+        'need':['len..', {hasinitvalue: 'forcomb'}, {hasinitvalue: 'CFUNCSMESS'}],
+        '_check': isarray
+    },
+    # Scalararray
+    {  # Common
+        '_check': l_and(isarray, l_not(iscomplexarray))
+    }, {  # Not hidden
+        '_check': l_and(isarray, l_not(iscomplexarray), isintent_nothide)
+    },
+    # Integer*1 array
+    {'need': '#ctype#',
+     '_check': isint1array,
+     '_depend': ''
      },
-# Integer*-1 array
-    {'need':'#ctype#',
-     '_check':isunsigned_chararray,
-     '_depend':''
+    # Integer*-1 array
+    {'need': '#ctype#',
+     '_check': isunsigned_chararray,
+     '_depend': ''
      },
-# Integer*-2 array
-    {'need':'#ctype#',
-     '_check':isunsigned_shortarray,
-     '_depend':''
+    # Integer*-2 array
+    {'need': '#ctype#',
+     '_check': isunsigned_shortarray,
+     '_depend': ''
      },
-# Integer*-8 array
-    {'need':'#ctype#',
-     '_check':isunsigned_long_longarray,
-     '_depend':''
+    # Integer*-8 array
+    {'need': '#ctype#',
+     '_check': isunsigned_long_longarray,
+     '_depend': ''
      },
-# Complexarray
-    {'need':'#ctype#',
-     '_check':iscomplexarray,
-     '_depend':''
+    # Complexarray
+    {'need': '#ctype#',
+     '_check': iscomplexarray,
+     '_depend': ''
      },
-# Stringarray
-     {
-     'callfortranappend':{isarrayofstrings:'flen(#varname#),'},
-     'need':'string',
-     '_check':isstringarray
-     }
-    ]
-
-arg_rules=[
+    # Stringarray
     {
-    'separatorsfor':sepdict
-    },
-    { # Common
-    'frompyobj':['\t/* Processing variable #varname# */',
-                 {debugcapi:'\tfprintf(stderr,"#vardebuginfo#\\n");'},],
-    'cleanupfrompyobj':'\t/* End of cleaning variable #varname# */',
-    '_depend':'',
-    'need':typedef_need_dict,
-    },
-# Doc signatures
+        'callfortranappend': {isarrayofstrings: 'flen(#varname#),'},
+        'need': 'string',
+        '_check': isstringarray
+    }
+]
+
+arg_rules = [
     {
-    'docstropt':{l_and(isoptional,isintent_nothide):'\t#pydocsign#'},
-    'docstrreq':{l_and(isrequired,isintent_nothide):'\t#pydocsign#'},
-    'docstrout':{isintent_out:'\t#pydocsignout#'},
-    'latexdocstropt':{l_and(isoptional,isintent_nothide):['\\item[]{{}\\verb@#pydocsign#@{}}',
-                                                          {hasnote:'--- #note#'}]},
-    'latexdocstrreq':{l_and(isrequired,isintent_nothide):['\\item[]{{}\\verb@#pydocsign#@{}}',
-                                                          {hasnote:'--- #note#'}]},
-    'latexdocstrout':{isintent_out:['\\item[]{{}\\verb@#pydocsignout#@{}}',
-                                    {l_and(hasnote,isintent_hide):'--- #note#',
-                                     l_and(hasnote,isintent_nothide):'--- See above.'}]},
-    'depend':''
-    },
-# Required/Optional arguments
+        'separatorsfor': sepdict
+    },
+    {  # Common
+        'frompyobj': ['    /* Processing variable #varname# */',
+                      {debugcapi: '    fprintf(stderr,"#vardebuginfo#\\n");'}, ],
+        'cleanupfrompyobj': '    /* End of cleaning variable #varname# */',
+        '_depend': '',
+        'need': typedef_need_dict,
+    },
+    # Doc signatures
     {
-    'kwlist':'"#varname#",',
-    'docsign':'#varname#,',
-    '_check':l_and(isintent_nothide,l_not(isoptional))
-    },
+        'docstropt': {l_and(isoptional, isintent_nothide): '#pydocsign#'},
+        'docstrreq': {l_and(isrequired, isintent_nothide): '#pydocsign#'},
+        'docstrout': {isintent_out: '#pydocsignout#'},
+        'latexdocstropt': {l_and(isoptional, isintent_nothide): ['\\item[]{{}\\verb@#pydocsign#@{}}',
+                                                                 {hasnote: '--- #note#'}]},
+        'latexdocstrreq': {l_and(isrequired, isintent_nothide): ['\\item[]{{}\\verb@#pydocsign#@{}}',
+                                                                 {hasnote: '--- #note#'}]},
+        'latexdocstrout': {isintent_out: ['\\item[]{{}\\verb@#pydocsignout#@{}}',
+                                          {l_and(hasnote, isintent_hide): '--- #note#',
+                                           l_and(hasnote, isintent_nothide): '--- See above.'}]},
+        'depend': ''
+    },
+    # Required/Optional arguments
     {
-    'kwlistopt':'"#varname#",',
-    'docsignopt':'#varname#=#showinit#,',
-    'docsignoptshort':'#varname#,',
-    '_check':l_and(isintent_nothide,isoptional)
-    },
-# Docstring/BuildValue
+        'kwlist': '"#varname#",',
+        'docsign': '#varname#,',
+        '_check': l_and(isintent_nothide, l_not(isoptional))
+    },
     {
-    'docreturn':'#outvarname#,',
-    'returnformat':'#varrformat#',
-    '_check':isintent_out
-    },
-# Externals (call-back functions)
-    { # Common
-    'docsignxa':{isintent_nothide:'#varname#_extra_args=(),'},
-    'docsignxashort':{isintent_nothide:'#varname#_extra_args,'},
-    'docstropt':{isintent_nothide:'\t#varname#_extra_args := () input tuple'},
-    'docstrcbs':'#cbdocstr#',
-    'latexdocstrcbs':'\\item[] #cblatexdocstr#',
-    'latexdocstropt':{isintent_nothide:'\\item[]{{}\\verb@#varname#_extra_args := () input tuple@{}} --- Extra arguments for call-back function {{}\\verb@#varname#@{}}.'},
-    'decl':['\tPyObject *#varname#_capi = Py_None;',
-            '\tPyTupleObject *#varname#_xa_capi = NULL;',
-            '\tPyTupleObject *#varname#_args_capi = NULL;',
-            '\tint #varname#_nofargs_capi = 0;',
-            {l_not(isintent_callback):'\t#cbname#_typedef #varname#_cptr;'}
-            ],
-    'kwlistxa':{isintent_nothide:'"#varname#_extra_args",'},
-    'argformat':{isrequired:'O'},
-    'keyformat':{isoptional:'O'},
-    'xaformat':{isintent_nothide:'O!'},
-    'args_capi':{isrequired:',&#varname#_capi'},
-    'keys_capi':{isoptional:',&#varname#_capi'},
-    'keys_xa':',&PyTuple_Type,&#varname#_xa_capi',
-    'setjmpbuf':'(setjmp(#cbname#_jmpbuf))',
-    'callfortran':{l_not(isintent_callback):'#varname#_cptr,'},
-    'need':['#cbname#','setjmp.h'],
-    '_check':isexternal
-    },
+        'kwlistopt': '"#varname#",',
+        'docsignopt': '#varname#=#showinit#,',
+        'docsignoptshort': '#varname#,',
+        '_check': l_and(isintent_nothide, isoptional)
+    },
+    # Docstring/BuildValue
     {
-    'frompyobj':[{l_not(isintent_callback):"""\
-if(PyCObject_Check(#varname#_capi)) {
-  #varname#_cptr = PyCObject_AsVoidPtr(#varname#_capi);
+        'docreturn': '#outvarname#,',
+        'returnformat': '#varrformat#',
+        '_check': isintent_out
+    },
+    # Externals (call-back functions)
+    {  # Common
+        'docsignxa': {isintent_nothide: '#varname#_extra_args=(),'},
+        'docsignxashort': {isintent_nothide: '#varname#_extra_args,'},
+        'docstropt': {isintent_nothide: '#varname#_extra_args : input tuple, optional\\n    Default: ()'},
+        'docstrcbs': '#cbdocstr#',
+        'latexdocstrcbs': '\\item[] #cblatexdocstr#',
+        'latexdocstropt': {isintent_nothide: '\\item[]{{}\\verb@#varname#_extra_args := () input tuple@{}} --- Extra arguments for call-back function {{}\\verb@#varname#@{}}.'},
+        'decl': ['    #cbname#_t #varname#_cb = { Py_None, NULL, 0 };',
+                 '    #cbname#_t *#varname#_cb_ptr = &#varname#_cb;',
+                 '    PyTupleObject *#varname#_xa_capi = NULL;',
+                 {l_not(isintent_callback):
+                  '    #cbname#_typedef #varname#_cptr;'}
+                 ],
+        'kwlistxa': {isintent_nothide: '"#varname#_extra_args",'},
+        'argformat': {isrequired: 'O'},
+        'keyformat': {isoptional: 'O'},
+        'xaformat': {isintent_nothide: 'O!'},
+        'args_capi': {isrequired: ',&#varname#_cb.capi'},
+        'keys_capi': {isoptional: ',&#varname#_cb.capi'},
+        'keys_xa': ',&PyTuple_Type,&#varname#_xa_capi',
+        'setjmpbuf': '(setjmp(#varname#_cb.jmpbuf))',
+        'callfortran': {l_not(isintent_callback): '#varname#_cptr,'},
+        'need': ['#cbname#', 'setjmp.h'],
+        '_check':isexternal
+    },
+    {
+        'frompyobj': [{l_not(isintent_callback): """\
+if(F2PyCapsule_Check(#varname#_cb.capi)) {
+  #varname#_cptr = F2PyCapsule_AsVoidPtr(#varname#_cb.capi);
 } else {
   #varname#_cptr = #cbname#;
 }
-"""},{isintent_callback:"""\
-if (#varname#_capi==Py_None) {
-  #varname#_capi = PyObject_GetAttrString(#modulename#_module,\"#varname#\");
-  if (#varname#_capi) {
+"""}, {isintent_callback: """\
+if (#varname#_cb.capi==Py_None) {
+  #varname#_cb.capi = PyObject_GetAttrString(#modulename#_module,\"#varname#\");
+  if (#varname#_cb.capi) {
     if (#varname#_xa_capi==NULL) {
       if (PyObject_HasAttrString(#modulename#_module,\"#varname#_extra_args\")) {
         PyObject* capi_tmp = PyObject_GetAttrString(#modulename#_module,\"#varname#_extra_args\");
-        if (capi_tmp)
+        if (capi_tmp) {
           #varname#_xa_capi = (PyTupleObject *)PySequence_Tuple(capi_tmp);
-        else
+          Py_DECREF(capi_tmp);
+        }
+        else {
           #varname#_xa_capi = (PyTupleObject *)Py_BuildValue(\"()\");
+        }
         if (#varname#_xa_capi==NULL) {
           PyErr_SetString(#modulename#_error,\"Failed to convert #modulename#.#varname#_extra_args to tuple.\\n\");
           return NULL;
@@ -701,644 +808,703 @@
       }
     }
   }
-  if (#varname#_capi==NULL) {
+  if (#varname#_cb.capi==NULL) {
     PyErr_SetString(#modulename#_error,\"Callback #varname# not defined (as an argument or module #modulename# attribute).\\n\");
     return NULL;
   }
 }
 """},
-##    {l_not(isintent_callback):"""\
-## if (#varname#_capi==Py_None) {
-## printf(\"hoi\\n\");
-## }
-## """},
-"""\
-\t#varname#_nofargs_capi = #cbname#_nofargs;
-\tif (create_cb_arglist(#varname#_capi,#varname#_xa_capi,#maxnofargs#,#nofoptargs#,&#cbname#_nofargs,&#varname#_args_capi,\"failed in processing argument list for call-back #varname#.\")) {
-\t\tjmp_buf #varname#_jmpbuf;""",
-{debugcapi:["""\
-\t\tfprintf(stderr,\"debug-capi:Assuming %d arguments; at most #maxnofargs#(-#nofoptargs#) is expected.\\n\",#cbname#_nofargs);
-\t\tCFUNCSMESSPY(\"for #varname#=\",#cbname#_capi);""",
-{l_not(isintent_callback):"""\t\tfprintf(stderr,\"#vardebugshowvalue# (call-back in C).\\n\",#cbname#);"""}]},
-          """\
-\t\tCFUNCSMESS(\"Saving jmpbuf for `#varname#`.\\n\");
-\t\tSWAP(#varname#_capi,#cbname#_capi,PyObject);
-\t\tSWAP(#varname#_args_capi,#cbname#_args_capi,PyTupleObject);
-\t\tmemcpy(&#varname#_jmpbuf,&#cbname#_jmpbuf,sizeof(jmp_buf));""",
-          ],
-'cleanupfrompyobj':
-"""\
-\t\tCFUNCSMESS(\"Restoring jmpbuf for `#varname#`.\\n\");
-\t\t#cbname#_capi = #varname#_capi;
-\t\tPy_DECREF(#cbname#_args_capi);
-\t\t#cbname#_args_capi = #varname#_args_capi;
-\t\t#cbname#_nofargs = #varname#_nofargs_capi;
-\t\tmemcpy(&#cbname#_jmpbuf,&#varname#_jmpbuf,sizeof(jmp_buf));
-\t}""",
-    'need':['SWAP','create_cb_arglist'],
-    '_check':isexternal,
-    '_depend':''
-    },
-# Scalars (not complex)
-    { # Common
-    'decl':'\t#ctype# #varname# = 0;',
-    'pyobjfrom':{debugcapi:'\tfprintf(stderr,"#vardebugshowvalue#\\n",#varname#);'},
-    'callfortran':{isintent_c:'#varname#,',l_not(isintent_c):'&#varname#,'},
-    'return':{isintent_out:',#varname#'},
-    '_check':l_and(isscalar,l_not(iscomplex))
-    },{
-    'need':{hasinitvalue:'math.h'},
-    '_check':l_and(isscalar,l_not(iscomplex)),
-    #'_depend':''
-    },{ # Not hidden
-    'decl':'\tPyObject *#varname#_capi = Py_None;',
-    'argformat':{isrequired:'O'},
-    'keyformat':{isoptional:'O'},
-    'args_capi':{isrequired:',&#varname#_capi'},
-    'keys_capi':{isoptional:',&#varname#_capi'},
-    'pyobjfrom':{isintent_inout:"""\
-\tf2py_success = try_pyarr_from_#ctype#(#varname#_capi,&#varname#);
-\tif (f2py_success) {"""},
-    'closepyobjfrom':{isintent_inout:"\t} /*if (f2py_success) of #varname# pyobjfrom*/"},
-    'need':{isintent_inout:'try_pyarr_from_#ctype#'},
-    '_check':l_and(isscalar,l_not(iscomplex),isintent_nothide)
-    },{
-    'frompyobj':[
-# hasinitvalue...
-#   if pyobj is None:
-#     varname = init
-#   else
-#     from_pyobj(varname)
-#
-# isoptional and noinitvalue...
-#   if pyobj is not None:
-#     from_pyobj(varname)
-#   else:
-#     varname is uninitialized
-#
-# ...
-#   from_pyobj(varname)
-#
-    {hasinitvalue:'\tif (#varname#_capi == Py_None) #varname# = #init#; else',
-     '_depend':''},
-    {l_and(isoptional,l_not(hasinitvalue)):'\tif (#varname#_capi != Py_None)',
-     '_depend':''},
-    {l_not(islogical):'''\
-\t\tf2py_success = #ctype#_from_pyobj(&#varname#,#varname#_capi,"#pyname#() #nth# (#varname#) can\'t be converted to #ctype#");
-\tif (f2py_success) {'''},
-    {islogical:'''\
-\t\t#varname# = (#ctype#)PyObject_IsTrue(#varname#_capi);
-\t\tf2py_success = 1;
-\tif (f2py_success) {'''},
-     ],
-    'cleanupfrompyobj':'\t} /*if (f2py_success) of #varname#*/',
-    'need':{l_not(islogical):'#ctype#_from_pyobj'},
-    '_check':l_and(isscalar,l_not(iscomplex),isintent_nothide),
-    '_depend':''
-#    },{ # Hidden
-#    '_check':l_and(isscalar,l_not(iscomplex),isintent_hide)
-    },{ # Hidden
-    'frompyobj':{hasinitvalue:'\t#varname# = #init#;'},
-    'need':typedef_need_dict,
-    '_check':l_and(isscalar,l_not(iscomplex),isintent_hide),
-    '_depend':''
-    },{ # Common
-    'frompyobj':{debugcapi:'\tfprintf(stderr,"#vardebugshowvalue#\\n",#varname#);'},
-    '_check':l_and(isscalar,l_not(iscomplex)),
-    '_depend':''
-    },
-# Complex scalars
-    { # Common
-    'decl':'\t#ctype# #varname#;',
-    'callfortran':{isintent_c:'#varname#,',l_not(isintent_c):'&#varname#,'},
-    'pyobjfrom':{debugcapi:'\tfprintf(stderr,"#vardebugshowvalue#\\n",#varname#.r,#varname#.i);'},
-    'return':{isintent_out:',#varname#_capi'},
-    '_check':iscomplex
-    },{ # Not hidden
-    'decl':'\tPyObject *#varname#_capi = Py_None;',
-    'argformat':{isrequired:'O'},
-    'keyformat':{isoptional:'O'},
-    'args_capi':{isrequired:',&#varname#_capi'},
-    'keys_capi':{isoptional:',&#varname#_capi'},
-    'need':{isintent_inout:'try_pyarr_from_#ctype#'},
-    'pyobjfrom':{isintent_inout:"""\
-\t\tf2py_success = try_pyarr_from_#ctype#(#varname#_capi,&#varname#);
-\t\tif (f2py_success) {"""},
-    'closepyobjfrom':{isintent_inout:"\t\t} /*if (f2py_success) of #varname# pyobjfrom*/"},
-    '_check':l_and(iscomplex,isintent_nothide)
-    },{
-    'frompyobj':[{hasinitvalue:'\tif (#varname#_capi==Py_None) {#varname#.r = #init.r#, #varname#.i = #init.i#;} else'},
-                 {l_and(isoptional,l_not(hasinitvalue)):'\tif (#varname#_capi != Py_None)'},
-#                 '\t\tf2py_success = #ctype#_from_pyobj(&#varname#,#varname#_capi,"#ctype#_from_pyobj failed in converting #nth# `#varname#\' of #pyname# to C #ctype#\\n");'
-                 '\t\tf2py_success = #ctype#_from_pyobj(&#varname#,#varname#_capi,"#pyname#() #nth# (#varname#) can\'t be converted to #ctype#");'
-                 '\n\tif (f2py_success) {'],
-    'cleanupfrompyobj':'\t}  /*if (f2py_success) of #varname# frompyobj*/',
-    'need':['#ctype#_from_pyobj'],
-    '_check':l_and(iscomplex,isintent_nothide),
-    '_depend':''
-    },{ # Hidden
-    'decl':{isintent_out:'\tPyObject *#varname#_capi = Py_None;'},
-    '_check':l_and(iscomplex,isintent_hide)
-    },{
-    'frompyobj': {hasinitvalue:'\t#varname#.r = #init.r#, #varname#.i = #init.i#;'},
-    '_check':l_and(iscomplex,isintent_hide),
-    '_depend':''
-    },{ # Common
-    'pyobjfrom':{isintent_out:'\t#varname#_capi = pyobj_from_#ctype#1(#varname#);'},
-    'need':['pyobj_from_#ctype#1'],
-    '_check':iscomplex
-    },{
-    'frompyobj':{debugcapi:'\tfprintf(stderr,"#vardebugshowvalue#\\n",#varname#.r,#varname#.i);'},
-    '_check':iscomplex,
-    '_depend':''
-    },
-# String
-    { # Common
-    'decl':['\t#ctype# #varname# = NULL;',
-            '\tint slen(#varname#);',
-            '\tPyObject *#varname#_capi = Py_None;'],
-    'callfortran':'#varname#,',
-    'callfortranappend':'slen(#varname#),',
-    'pyobjfrom':{debugcapi:'\tfprintf(stderr,"#vardebugshowvalue#\\n",slen(#varname#),#varname#);'},
-#    'freemem':'\tSTRINGFREE(#varname#);',
-    'return':{isintent_out:',#varname#'},
-    'need':['len..'],#'STRINGFREE'],
-    '_check':isstring
-    },{ # Common
-    'frompyobj':"""\
-\tslen(#varname#) = #length#;
-\tf2py_success = #ctype#_from_pyobj(&#varname#,&slen(#varname#),#init#,#varname#_capi,\"#ctype#_from_pyobj failed in converting #nth# `#varname#\' of #pyname# to C #ctype#\");
-\tif (f2py_success) {""",
-    'cleanupfrompyobj':"""\
-\t\tSTRINGFREE(#varname#);
-\t}  /*if (f2py_success) of #varname#*/""",
-    'need':['#ctype#_from_pyobj','len..','STRINGFREE'],
-    '_check':isstring,
-    '_depend':''
-    },{ # Not hidden
-    'argformat':{isrequired:'O'},
-    'keyformat':{isoptional:'O'},
-    'args_capi':{isrequired:',&#varname#_capi'},
-    'keys_capi':{isoptional:',&#varname#_capi'},
-    'pyobjfrom':{isintent_inout:'''\
-\tf2py_success = try_pyarr_from_#ctype#(#varname#_capi,#varname#);
-\tif (f2py_success) {'''},
-    'closepyobjfrom':{isintent_inout:'\t} /*if (f2py_success) of #varname# pyobjfrom*/'},
-    'need':{isintent_inout:'try_pyarr_from_#ctype#'},
-    '_check':l_and(isstring,isintent_nothide)
-    },{ # Hidden
-    '_check':l_and(isstring,isintent_hide)
-    },{
-    'frompyobj':{debugcapi:'\tfprintf(stderr,"#vardebugshowvalue#\\n",slen(#varname#),#varname#);'},
-    '_check':isstring,
-    '_depend':''
-    },
-# Array
-    { # Common
-    'decl':['\t#ctype# *#varname# = NULL;',
-            '\tintp #varname#_Dims[#rank#] = {#rank*[-1]#};',
-            '\tconst int #varname#_Rank = #rank#;',
-            '\tPyArrayObject *capi_#varname#_tmp = NULL;',
-            '\tint capi_#varname#_intent = 0;',
-            ],
-    'callfortran':'#varname#,',
-    'return':{isintent_out:',capi_#varname#_tmp'},
-    'need':'len..',
-    '_check':isarray
-    },{ # intent(overwrite) array
-    'decl':'\tint capi_overwrite_#varname# = 1;',
-    'kwlistxa':'"overwrite_#varname#",',
-    'xaformat':'i',
-    'keys_xa':',&capi_overwrite_#varname#',
-    'docsignxa':'overwrite_#varname#=1,',
-    'docsignxashort':'overwrite_#varname#,',
-    'docstropt':'\toverwrite_#varname# := 1 input int',
-    '_check':l_and(isarray,isintent_overwrite),
-    },{
-    'frompyobj':'\tcapi_#varname#_intent |= (capi_overwrite_#varname#?0:F2PY_INTENT_COPY);',
-    '_check':l_and(isarray,isintent_overwrite),
-    '_depend':'',
-    },
-    { # intent(copy) array
-    'decl':'\tint capi_overwrite_#varname# = 0;',
-     'kwlistxa':'"overwrite_#varname#",',
-     'xaformat':'i',
-     'keys_xa':',&capi_overwrite_#varname#',
-     'docsignxa':'overwrite_#varname#=0,',
-     'docsignxashort':'overwrite_#varname#,',
-     'docstropt':'\toverwrite_#varname# := 0 input int',
-     '_check':l_and(isarray,isintent_copy),
-     },{
-     'frompyobj':'\tcapi_#varname#_intent |= (capi_overwrite_#varname#?0:F2PY_INTENT_COPY);',
-     '_check':l_and(isarray,isintent_copy),
-     '_depend':'',
-    },{
-    'need':[{hasinitvalue:'forcomb'},{hasinitvalue:'CFUNCSMESS'}],
-    '_check':isarray,
-    '_depend':''
-    },{ # Not hidden
-    'decl':'\tPyObject *#varname#_capi = Py_None;',
-    'argformat':{isrequired:'O'},
-    'keyformat':{isoptional:'O'},
-    'args_capi':{isrequired:',&#varname#_capi'},
-    'keys_capi':{isoptional:',&#varname#_capi'},
-#     'pyobjfrom':{isintent_inout:"""\
-# /* Partly because of the following hack, intent(inout) is depreciated,
-#    Use intent(in,out) instead.
-
-# \tif ((#varname#_capi != Py_None) && PyArray_Check(#varname#_capi) \\
-# \t\t&& (#varname#_capi != (PyObject *)capi_#varname#_tmp)) {
-# \t\tif (((PyArrayObject *)#varname#_capi)->nd != capi_#varname#_tmp->nd) {
-# \t\t\tif (#varname#_capi != capi_#varname#_tmp->base)
-# \t\t\t\tcopy_ND_array((PyArrayObject *)capi_#varname#_tmp->base,(PyArrayObject *)#varname#_capi);
-# \t\t} else
-# \t\t\tcopy_ND_array(capi_#varname#_tmp,(PyArrayObject *)#varname#_capi);
-# \t}
-# */
-# """},
-#     'need':{isintent_inout:'copy_ND_array'},
-    '_check':l_and(isarray,isintent_nothide)
-    },{
-    'frompyobj':['\t#setdims#;',
-                 '\tcapi_#varname#_intent |= #intent#;',
-                 {isintent_hide:'\tcapi_#varname#_tmp = array_from_pyobj(#atype#,#varname#_Dims,#varname#_Rank,capi_#varname#_intent,Py_None);'},
-                 {isintent_nothide:'\tcapi_#varname#_tmp = array_from_pyobj(#atype#,#varname#_Dims,#varname#_Rank,capi_#varname#_intent,#varname#_capi);'},
-                 """\
-\tif (capi_#varname#_tmp == NULL) {
-\t\tif (!PyErr_Occurred())
-\t\t\tPyErr_SetString(#modulename#_error,\"failed in converting #nth# `#varname#\' of #pyname# to C/Fortran array\" );
-\t} else {
-\t\t#varname# = (#ctype# *)(capi_#varname#_tmp->data);
+            """\
+    if (create_cb_arglist(#varname#_cb.capi,#varname#_xa_capi,#maxnofargs#,#nofoptargs#,&#varname#_cb.nofargs,&#varname#_cb.args_capi,\"failed in processing argument list for call-back #varname#.\")) {
 """,
-{hasinitvalue:[
-    {isintent_nothide:'\tif (#varname#_capi == Py_None) {'},
-    {isintent_hide:'\t{'},
-    {iscomplexarray:'\t\t#ctype# capi_c;'},
-    """\
-\t\tint *_i,capi_i=0;
-\t\tCFUNCSMESS(\"#name#: Initializing #varname#=#init#\\n\");
-\t\tif (initforcomb(capi_#varname#_tmp->dimensions,capi_#varname#_tmp->nd,1)) {
-\t\t\twhile ((_i = nextforcomb()))
-\t\t\t\t#varname#[capi_i++] = #init#; /* fortran way */
-\t\t} else {
-\t\t\tif (!PyErr_Occurred())
-\t\t\t\tPyErr_SetString(#modulename#_error,\"Initialization of #nth# #varname# failed (initforcomb).\");
-\t\t\tf2py_success = 0;
-\t\t}
-\t}
-\tif (f2py_success) {"""]},
+            {debugcapi: ["""\
+        fprintf(stderr,\"debug-capi:Assuming %d arguments; at most #maxnofargs#(-#nofoptargs#) is expected.\\n\",#varname#_cb.nofargs);
+        CFUNCSMESSPY(\"for #varname#=\",#varname#_cb.capi);""",
+                         {l_not(isintent_callback): """        fprintf(stderr,\"#vardebugshowvalue# (call-back in C).\\n\",#cbname#);"""}]},
+            """\
+        CFUNCSMESS(\"Saving callback variables for `#varname#`.\\n\");
+        #varname#_cb_ptr = swap_active_#cbname#(#varname#_cb_ptr);""",
+        ],
+        'cleanupfrompyobj':
+        """\
+        CFUNCSMESS(\"Restoring callback variables for `#varname#`.\\n\");
+        #varname#_cb_ptr = swap_active_#cbname#(#varname#_cb_ptr);
+        Py_DECREF(#varname#_cb.args_capi);
+    }""",
+        'need': ['SWAP', 'create_cb_arglist'],
+        '_check':isexternal,
+        '_depend':''
+    },
+    # Scalars (not complex)
+    {  # Common
+        'decl': '    #ctype# #varname# = 0;',
+        'pyobjfrom': {debugcapi: '    fprintf(stderr,"#vardebugshowvalue#\\n",#varname#);'},
+        'callfortran': {isintent_c: '#varname#,', l_not(isintent_c): '&#varname#,'},
+        'return': {isintent_out: ',#varname#'},
+        '_check': l_and(isscalar, l_not(iscomplex))
+    }, {
+        'need': {hasinitvalue: 'math.h'},
+        '_check': l_and(isscalar, l_not(iscomplex)),
+    }, {  # Not hidden
+        'decl': '    PyObject *#varname#_capi = Py_None;',
+        'argformat': {isrequired: 'O'},
+        'keyformat': {isoptional: 'O'},
+        'args_capi': {isrequired: ',&#varname#_capi'},
+        'keys_capi': {isoptional: ',&#varname#_capi'},
+        'pyobjfrom': {isintent_inout: """\
+    f2py_success = try_pyarr_from_#ctype#(#varname#_capi,&#varname#);
+    if (f2py_success) {"""},
+        'closepyobjfrom': {isintent_inout: "    } /*if (f2py_success) of #varname# pyobjfrom*/"},
+        'need': {isintent_inout: 'try_pyarr_from_#ctype#'},
+        '_check': l_and(isscalar, l_not(iscomplex), isintent_nothide)
+    }, {
+        'frompyobj': [
+            # hasinitvalue...
+            #   if pyobj is None:
+            #     varname = init
+            #   else
+            #     from_pyobj(varname)
+            #
+            # isoptional and noinitvalue...
+            #   if pyobj is not None:
+            #     from_pyobj(varname)
+            #   else:
+            #     varname is uninitialized
+            #
+            # ...
+            #   from_pyobj(varname)
+            #
+            {hasinitvalue: '    if (#varname#_capi == Py_None) #varname# = #init#; else',
+             '_depend': ''},
+            {l_and(isoptional, l_not(hasinitvalue)): '    if (#varname#_capi != Py_None)',
+             '_depend': ''},
+            {l_not(islogical): '''\
+        f2py_success = #ctype#_from_pyobj(&#varname#,#varname#_capi,"#pyname#() #nth# (#varname#) can\'t be converted to #ctype#");
+    if (f2py_success) {'''},
+            {islogical: '''\
+        #varname# = (#ctype#)PyObject_IsTrue(#varname#_capi);
+        f2py_success = 1;
+    if (f2py_success) {'''},
+        ],
+        'cleanupfrompyobj': '    } /*if (f2py_success) of #varname#*/',
+        'need': {l_not(islogical): '#ctype#_from_pyobj'},
+        '_check': l_and(isscalar, l_not(iscomplex), isintent_nothide),
+        '_depend': ''
+    }, {  # Hidden
+        'frompyobj': {hasinitvalue: '    #varname# = #init#;'},
+        'need': typedef_need_dict,
+        '_check': l_and(isscalar, l_not(iscomplex), isintent_hide),
+        '_depend': ''
+    }, {  # Common
+        'frompyobj': {debugcapi: '    fprintf(stderr,"#vardebugshowvalue#\\n",#varname#);'},
+        '_check': l_and(isscalar, l_not(iscomplex)),
+        '_depend': ''
+    },
+    # Complex scalars
+    {  # Common
+        'decl': '    #ctype# #varname#;',
+        'callfortran': {isintent_c: '#varname#,', l_not(isintent_c): '&#varname#,'},
+        'pyobjfrom': {debugcapi: '    fprintf(stderr,"#vardebugshowvalue#\\n",#varname#.r,#varname#.i);'},
+        'return': {isintent_out: ',#varname#_capi'},
+        '_check': iscomplex
+    }, {  # Not hidden
+        'decl': '    PyObject *#varname#_capi = Py_None;',
+        'argformat': {isrequired: 'O'},
+        'keyformat': {isoptional: 'O'},
+        'args_capi': {isrequired: ',&#varname#_capi'},
+        'keys_capi': {isoptional: ',&#varname#_capi'},
+        'need': {isintent_inout: 'try_pyarr_from_#ctype#'},
+        'pyobjfrom': {isintent_inout: """\
+        f2py_success = try_pyarr_from_#ctype#(#varname#_capi,&#varname#);
+        if (f2py_success) {"""},
+        'closepyobjfrom': {isintent_inout: "        } /*if (f2py_success) of #varname# pyobjfrom*/"},
+        '_check': l_and(iscomplex, isintent_nothide)
+    }, {
+        'frompyobj': [{hasinitvalue: '    if (#varname#_capi==Py_None) {#varname#.r = #init.r#, #varname#.i = #init.i#;} else'},
+                      {l_and(isoptional, l_not(hasinitvalue))
+                             : '    if (#varname#_capi != Py_None)'},
+                      '        f2py_success = #ctype#_from_pyobj(&#varname#,#varname#_capi,"#pyname#() #nth# (#varname#) can\'t be converted to #ctype#");'
+                      '\n    if (f2py_success) {'],
+        'cleanupfrompyobj': '    }  /*if (f2py_success) of #varname# frompyobj*/',
+        'need': ['#ctype#_from_pyobj'],
+        '_check': l_and(iscomplex, isintent_nothide),
+        '_depend': ''
+    }, {  # Hidden
+        'decl': {isintent_out: '    PyObject *#varname#_capi = Py_None;'},
+        '_check': l_and(iscomplex, isintent_hide)
+    }, {
+        'frompyobj': {hasinitvalue: '    #varname#.r = #init.r#, #varname#.i = #init.i#;'},
+        '_check': l_and(iscomplex, isintent_hide),
+        '_depend': ''
+    }, {  # Common
+        'pyobjfrom': {isintent_out: '    #varname#_capi = pyobj_from_#ctype#1(#varname#);'},
+        'need': ['pyobj_from_#ctype#1'],
+        '_check': iscomplex
+    }, {
+        'frompyobj': {debugcapi: '    fprintf(stderr,"#vardebugshowvalue#\\n",#varname#.r,#varname#.i);'},
+        '_check': iscomplex,
+        '_depend': ''
+    },
+    # String
+    {  # Common
+        'decl': ['    #ctype# #varname# = NULL;',
+                 '    int slen(#varname#);',
+                 '    PyObject *#varname#_capi = Py_None;'],
+        'callfortran':'#varname#,',
+        'callfortranappend':'slen(#varname#),',
+        'pyobjfrom':[
+            {debugcapi:
+             '    fprintf(stderr,'
+             '"#vardebugshowvalue#\\n",slen(#varname#),#varname#);'},
+            # The trailing null value for Fortran is blank.
+            {l_and(isintent_out, l_not(isintent_c)):
+             "        STRINGPADN(#varname#, slen(#varname#), ' ', '\\0');"},
+        ],
+        'return': {isintent_out: ',#varname#'},
+        'need': ['len..',
+                 {l_and(isintent_out, l_not(isintent_c)): 'STRINGPADN'}],
+        '_check':isstring
+    }, {  # Common
+        'frompyobj': [
+            """\
+    slen(#varname#) = #length#;
+    f2py_success = #ctype#_from_pyobj(&#varname#,&slen(#varname#),#init#,"""
+"""#varname#_capi,\"#ctype#_from_pyobj failed in converting #nth#"""
+"""`#varname#\' of #pyname# to C #ctype#\");
+    if (f2py_success) {""",
+            # The trailing null value for Fortran is blank.
+            {l_not(isintent_c):
+             "        STRINGPADN(#varname#, slen(#varname#), '\\0', ' ');"},
+        ],
+        'cleanupfrompyobj': """\
+        STRINGFREE(#varname#);
+    }  /*if (f2py_success) of #varname#*/""",
+        'need': ['#ctype#_from_pyobj', 'len..', 'STRINGFREE',
+                 {l_not(isintent_c): 'STRINGPADN'}],
+        '_check':isstring,
+        '_depend':''
+    }, {  # Not hidden
+        'argformat': {isrequired: 'O'},
+        'keyformat': {isoptional: 'O'},
+        'args_capi': {isrequired: ',&#varname#_capi'},
+        'keys_capi': {isoptional: ',&#varname#_capi'},
+        'pyobjfrom': [
+            {l_and(isintent_inout, l_not(isintent_c)):
+             "        STRINGPADN(#varname#, slen(#varname#), ' ', '\\0');"},
+            {isintent_inout: '''\
+    f2py_success = try_pyarr_from_#ctype#(#varname#_capi, #varname#,
+                                          slen(#varname#));
+    if (f2py_success) {'''}],
+        'closepyobjfrom': {isintent_inout: '    } /*if (f2py_success) of #varname# pyobjfrom*/'},
+        'need': {isintent_inout: 'try_pyarr_from_#ctype#',
+                 l_and(isintent_inout, l_not(isintent_c)): 'STRINGPADN'},
+        '_check': l_and(isstring, isintent_nothide)
+    }, {  # Hidden
+        '_check': l_and(isstring, isintent_hide)
+    }, {
+        'frompyobj': {debugcapi: '    fprintf(stderr,"#vardebugshowvalue#\\n",slen(#varname#),#varname#);'},
+        '_check': isstring,
+        '_depend': ''
+    },
+    # Array
+    {  # Common
+        'decl': ['    #ctype# *#varname# = NULL;',
+                 '    npy_intp #varname#_Dims[#rank#] = {#rank*[-1]#};',
+                 '    const int #varname#_Rank = #rank#;',
+                 '    PyArrayObject *capi_#varname#_tmp = NULL;',
+                 '    int capi_#varname#_intent = 0;',
                  ],
-    'cleanupfrompyobj':[ # note that this list will be reversed
-    '\t}  /*if (capi_#varname#_tmp == NULL) ... else of #varname#*/',
-    {l_not(l_or(isintent_out,isintent_hide)):"""\
-\tif((PyObject *)capi_#varname#_tmp!=#varname#_capi) {
-\t\tPy_XDECREF(capi_#varname#_tmp); }"""},
-    {l_and(isintent_hide,l_not(isintent_out)):"""\t\tPy_XDECREF(capi_#varname#_tmp);"""},
-    {hasinitvalue:'\t}  /*if (f2py_success) of #varname# init*/'},
-    ],
-    '_check':isarray,
-    '_depend':''
-    },
-#    { # Hidden
-#    'freemem':{l_not(isintent_out):'\tPy_XDECREF(capi_#varname#_tmp);'},
-#    '_check':l_and(isarray,isintent_hide)
-#    },
-# Scalararray
-    { # Common
-    '_check':l_and(isarray,l_not(iscomplexarray))
-    },{ # Not hidden
-    '_check':l_and(isarray,l_not(iscomplexarray),isintent_nothide)
-    },
-# Integer*1 array
-    {'need':'#ctype#',
-     '_check':isint1array,
-     '_depend':''
+        'callfortran':'#varname#,',
+        'return':{isintent_out: ',capi_#varname#_tmp'},
+        'need': 'len..',
+        '_check': isarray
+    }, {  # intent(overwrite) array
+        'decl': '    int capi_overwrite_#varname# = 1;',
+        'kwlistxa': '"overwrite_#varname#",',
+        'xaformat': 'i',
+        'keys_xa': ',&capi_overwrite_#varname#',
+        'docsignxa': 'overwrite_#varname#=1,',
+        'docsignxashort': 'overwrite_#varname#,',
+        'docstropt': 'overwrite_#varname# : input int, optional\\n    Default: 1',
+        '_check': l_and(isarray, isintent_overwrite),
+    }, {
+        'frompyobj': '    capi_#varname#_intent |= (capi_overwrite_#varname#?0:F2PY_INTENT_COPY);',
+        '_check': l_and(isarray, isintent_overwrite),
+        '_depend': '',
+    },
+    {  # intent(copy) array
+        'decl': '    int capi_overwrite_#varname# = 0;',
+        'kwlistxa': '"overwrite_#varname#",',
+        'xaformat': 'i',
+        'keys_xa': ',&capi_overwrite_#varname#',
+        'docsignxa': 'overwrite_#varname#=0,',
+        'docsignxashort': 'overwrite_#varname#,',
+        'docstropt': 'overwrite_#varname# : input int, optional\\n    Default: 0',
+        '_check': l_and(isarray, isintent_copy),
+    }, {
+        'frompyobj': '    capi_#varname#_intent |= (capi_overwrite_#varname#?0:F2PY_INTENT_COPY);',
+        '_check': l_and(isarray, isintent_copy),
+        '_depend': '',
+    }, {
+        'need': [{hasinitvalue: 'forcomb'}, {hasinitvalue: 'CFUNCSMESS'}],
+        '_check': isarray,
+        '_depend': ''
+    }, {  # Not hidden
+        'decl': '    PyObject *#varname#_capi = Py_None;',
+        'argformat': {isrequired: 'O'},
+        'keyformat': {isoptional: 'O'},
+        'args_capi': {isrequired: ',&#varname#_capi'},
+        'keys_capi': {isoptional: ',&#varname#_capi'},
+        '_check': l_and(isarray, isintent_nothide)
+    }, {
+        'frompyobj': ['    #setdims#;',
+                      '    capi_#varname#_intent |= #intent#;',
+                      {isintent_hide:
+                       '    capi_#varname#_tmp = array_from_pyobj(#atype#,#varname#_Dims,#varname#_Rank,capi_#varname#_intent,Py_None);'},
+                      {isintent_nothide:
+                       '    capi_#varname#_tmp = array_from_pyobj(#atype#,#varname#_Dims,#varname#_Rank,capi_#varname#_intent,#varname#_capi);'},
+                      """\
+    if (capi_#varname#_tmp == NULL) {
+        PyObject *exc, *val, *tb;
+        PyErr_Fetch(&exc, &val, &tb);
+        PyErr_SetString(exc ? exc : #modulename#_error,\"failed in converting #nth# `#varname#\' of #pyname# to C/Fortran array\" );
+        npy_PyErr_ChainExceptionsCause(exc, val, tb);
+    } else {
+        #varname# = (#ctype# *)(PyArray_DATA(capi_#varname#_tmp));
+""",
+                      {hasinitvalue: [
+                          {isintent_nothide:
+                              '    if (#varname#_capi == Py_None) {'},
+                          {isintent_hide: '    {'},
+                          {iscomplexarray: '        #ctype# capi_c;'},
+                          """\
+        int *_i,capi_i=0;
+        CFUNCSMESS(\"#name#: Initializing #varname#=#init#\\n\");
+        if (initforcomb(PyArray_DIMS(capi_#varname#_tmp),PyArray_NDIM(capi_#varname#_tmp),1)) {
+            while ((_i = nextforcomb()))
+                #varname#[capi_i++] = #init#; /* fortran way */
+        } else {
+            PyObject *exc, *val, *tb;
+            PyErr_Fetch(&exc, &val, &tb);
+            PyErr_SetString(exc ? exc : #modulename#_error,\"Initialization of #nth# #varname# failed (initforcomb).\");
+            npy_PyErr_ChainExceptionsCause(exc, val, tb);
+            f2py_success = 0;
+        }
+    }
+    if (f2py_success) {"""]},
+                      ],
+        'cleanupfrompyobj': [  # note that this list will be reversed
+            '    }  /*if (capi_#varname#_tmp == NULL) ... else of #varname#*/',
+            {l_not(l_or(isintent_out, isintent_hide)): """\
+    if((PyObject *)capi_#varname#_tmp!=#varname#_capi) {
+        Py_XDECREF(capi_#varname#_tmp); }"""},
+            {l_and(isintent_hide, l_not(isintent_out))
+                   : """        Py_XDECREF(capi_#varname#_tmp);"""},
+            {hasinitvalue: '    }  /*if (f2py_success) of #varname# init*/'},
+        ],
+        '_check': isarray,
+        '_depend': ''
+    },
+    # Scalararray
+    {  # Common
+        '_check': l_and(isarray, l_not(iscomplexarray))
+    }, {  # Not hidden
+        '_check': l_and(isarray, l_not(iscomplexarray), isintent_nothide)
+    },
+    # Integer*1 array
+    {'need': '#ctype#',
+     '_check': isint1array,
+     '_depend': ''
      },
-# Integer*-1 array
-    {'need':'#ctype#',
-     '_check':isunsigned_chararray,
-     '_depend':''
+    # Integer*-1 array
+    {'need': '#ctype#',
+     '_check': isunsigned_chararray,
+     '_depend': ''
      },
-# Integer*-2 array
-    {'need':'#ctype#',
-     '_check':isunsigned_shortarray,
-     '_depend':''
+    # Integer*-2 array
+    {'need': '#ctype#',
+     '_check': isunsigned_shortarray,
+     '_depend': ''
      },
-# Integer*-8 array
-    {'need':'#ctype#',
-     '_check':isunsigned_long_longarray,
-     '_depend':''
+    # Integer*-8 array
+    {'need': '#ctype#',
+     '_check': isunsigned_long_longarray,
+     '_depend': ''
      },
-# Complexarray
-    {'need':'#ctype#',
-     '_check':iscomplexarray,
-     '_depend':''
+    # Complexarray
+    {'need': '#ctype#',
+     '_check': iscomplexarray,
+     '_depend': ''
      },
-# Stringarray
-     {
-     'callfortranappend':{isarrayofstrings:'flen(#varname#),'},
-     'need':'string',
-     '_check':isstringarray
-     }
-    ]
-
-################# Rules for checking ###############
-
-check_rules=[
+    # Stringarray
     {
-    'frompyobj':{debugcapi:'\tfprintf(stderr,\"debug-capi:Checking `#check#\'\\n\");'},
-    'need':'len..'
-    },{
-    'frompyobj':'\tCHECKSCALAR(#check#,\"#check#\",\"#nth# #varname#\",\"#varshowvalue#\",#varname#) {',
-    'cleanupfrompyobj':'\t} /*CHECKSCALAR(#check#)*/',
-    'need':'CHECKSCALAR',
-    '_check':l_and(isscalar,l_not(iscomplex)),
-    '_break':''
-    },{
-    'frompyobj':'\tCHECKSTRING(#check#,\"#check#\",\"#nth# #varname#\",\"#varshowvalue#\",#varname#) {',
-    'cleanupfrompyobj':'\t} /*CHECKSTRING(#check#)*/',
-    'need':'CHECKSTRING',
-    '_check':isstring,
-    '_break':''
-    },{
-    'need':'CHECKARRAY',
-    'frompyobj':'\tCHECKARRAY(#check#,\"#check#\",\"#nth# #varname#\") {',
-    'cleanupfrompyobj':'\t} /*CHECKARRAY(#check#)*/',
-    '_check':isarray,
-    '_break':''
-    },{
-    'need':'CHECKGENERIC',
-    'frompyobj':'\tCHECKGENERIC(#check#,\"#check#\",\"#nth# #varname#\") {',
-    'cleanupfrompyobj':'\t} /*CHECKGENERIC(#check#)*/',
+        'callfortranappend': {isarrayofstrings: 'flen(#varname#),'},
+        'need': 'string',
+        '_check': isstringarray
     }
 ]
 
+################# Rules for checking ###############
+
+check_rules = [
+    {
+        'frompyobj': {debugcapi: '    fprintf(stderr,\"debug-capi:Checking `#check#\'\\n\");'},
+        'need': 'len..'
+    }, {
+        'frompyobj': '    CHECKSCALAR(#check#,\"#check#\",\"#nth# #varname#\",\"#varshowvalue#\",#varname#) {',
+        'cleanupfrompyobj': '    } /*CHECKSCALAR(#check#)*/',
+        'need': 'CHECKSCALAR',
+        '_check': l_and(isscalar, l_not(iscomplex)),
+        '_break': ''
+    }, {
+        'frompyobj': '    CHECKSTRING(#check#,\"#check#\",\"#nth# #varname#\",\"#varshowvalue#\",#varname#) {',
+        'cleanupfrompyobj': '    } /*CHECKSTRING(#check#)*/',
+        'need': 'CHECKSTRING',
+        '_check': isstring,
+        '_break': ''
+    }, {
+        'need': 'CHECKARRAY',
+        'frompyobj': '    CHECKARRAY(#check#,\"#check#\",\"#nth# #varname#\") {',
+        'cleanupfrompyobj': '    } /*CHECKARRAY(#check#)*/',
+        '_check': isarray,
+        '_break': ''
+    }, {
+        'need': 'CHECKGENERIC',
+        'frompyobj': '    CHECKGENERIC(#check#,\"#check#\",\"#nth# #varname#\") {',
+        'cleanupfrompyobj': '    } /*CHECKGENERIC(#check#)*/',
+    }
+]
+
 ########## Applying the rules. No need to modify what follows #############
 
 #################### Build C/API module #######################
 
-def buildmodule(m,um):
+
+def buildmodule(m, um):
     """
     Return
     """
-    global f2py_version,options
-    outmess('\tBuilding module "%s"...\n'%(m['name']))
+    outmess('    Building module "%s"...\n' % (m['name']))
     ret = {}
-    mod_rules=defmod_rules[:]
-    vrd=modsign2map(m)
-    rd=dictappend({'f2py_version':f2py_version},vrd)
+    mod_rules = defmod_rules[:]
+    vrd = capi_maps.modsign2map(m)
+    rd = dictappend({'f2py_version': f2py_version}, vrd)
     funcwrappers = []
-    funcwrappers2 = [] # F90 codes
+    funcwrappers2 = []  # F90 codes
     for n in m['interfaced']:
-        nb=None
+        nb = None
         for bi in m['body']:
-            if not bi['block']=='interface':
+            if bi['block'] not in ['interface', 'abstract interface']:
                 errmess('buildmodule: Expected interface block. Skipping.\n')
                 continue
             for b in bi['body']:
-                if b['name']==n: nb=b;break
+                if b['name'] == n:
+                    nb = b
+                    break
 
         if not nb:
-            errmess('buildmodule: Could not found the body of interfaced routine "%s". Skipping.\n'%(n))
+            print(
+                'buildmodule: Could not find the body of interfaced routine "%s". Skipping.\n' % (n), file=sys.stderr)
             continue
         nb_list = [nb]
-        if nb.has_key('entry'):
-            for k,a in nb['entry'].items():
+        if 'entry' in nb:
+            for k, a in nb['entry'].items():
                 nb1 = copy.deepcopy(nb)
                 del nb1['entry']
                 nb1['name'] = k
                 nb1['args'] = a
                 nb_list.append(nb1)
         for nb in nb_list:
-            api,wrap=buildapi(nb)
+            # requiresf90wrapper must be called before buildapi as it
+            # rewrites assumed shape arrays as automatic arrays.
+            isf90 = requiresf90wrapper(nb)
+            # options is in scope here
+            if options['emptygen']:
+                b_path = options['buildpath']
+                m_name = vrd['modulename']
+                outmess('    Generating possibly empty wrappers"\n')
+                Path(f"{b_path}/{vrd['coutput']}").touch()
+                if isf90:
+                    # f77 + f90 wrappers
+                    outmess(f'    Maybe empty "{m_name}-f2pywrappers2.f90"\n')
+                    Path(f'{b_path}/{m_name}-f2pywrappers2.f90').touch()
+                    outmess(f'    Maybe empty "{m_name}-f2pywrappers.f"\n')
+                    Path(f'{b_path}/{m_name}-f2pywrappers.f').touch()
+                else:
+                    # only f77 wrappers
+                    outmess(f'    Maybe empty "{m_name}-f2pywrappers.f"\n')
+                    Path(f'{b_path}/{m_name}-f2pywrappers.f').touch()
+            api, wrap = buildapi(nb)
             if wrap:
-                if ismoduleroutine(nb):
+                if isf90:
                     funcwrappers2.append(wrap)
                 else:
                     funcwrappers.append(wrap)
-            ar=applyrules(api,vrd)
-            rd=dictappend(rd,ar)
+            ar = applyrules(api, vrd)
+            rd = dictappend(rd, ar)
 
     # Construct COMMON block support
-    cr,wrap = common_rules.buildhooks(m)
+    cr, wrap = common_rules.buildhooks(m)
     if wrap:
         funcwrappers.append(wrap)
-    ar=applyrules(cr,vrd)
-    rd=dictappend(rd,ar)
+    ar = applyrules(cr, vrd)
+    rd = dictappend(rd, ar)
 
     # Construct F90 module support
-    mr,wrap = f90mod_rules.buildhooks(m)
+    mr, wrap = f90mod_rules.buildhooks(m)
     if wrap:
         funcwrappers2.append(wrap)
-    ar=applyrules(mr,vrd)
-    rd=dictappend(rd,ar)
+    ar = applyrules(mr, vrd)
+    rd = dictappend(rd, ar)
 
     for u in um:
-        ar=use_rules.buildusevars(u,m['use'][u['name']])
-        rd=dictappend(rd,ar)
-
-    needs=cfuncs.get_needs()
-    code={}
+        ar = use_rules.buildusevars(u, m['use'][u['name']])
+        rd = dictappend(rd, ar)
+
+    needs = cfuncs.get_needs()
+    # Add mapped definitions
+    needs['typedefs'] += [cvar for cvar in capi_maps.f2cmap_mapped #
+                          if cvar in typedef_need_dict.values()]
+    code = {}
     for n in needs.keys():
-        code[n]=[]
+        code[n] = []
         for k in needs[n]:
-            c=''
-            if cfuncs.includes0.has_key(k): c=cfuncs.includes0[k]
-            elif cfuncs.includes.has_key(k): c=cfuncs.includes[k]
-            elif cfuncs.userincludes.has_key(k): c=cfuncs.userincludes[k]
-            elif cfuncs.typedefs.has_key(k): c=cfuncs.typedefs[k]
-            elif cfuncs.typedefs_generated.has_key(k):
-                c=cfuncs.typedefs_generated[k]
-            elif cfuncs.cppmacros.has_key(k): c=cfuncs.cppmacros[k]
-            elif cfuncs.cfuncs.has_key(k): c=cfuncs.cfuncs[k]
-            elif cfuncs.callbacks.has_key(k): c=cfuncs.callbacks[k]
-            elif cfuncs.f90modhooks.has_key(k): c=cfuncs.f90modhooks[k]
-            elif cfuncs.commonhooks.has_key(k): c=cfuncs.commonhooks[k]
-            else: errmess('buildmodule: unknown need %s.\n'%(`k`));continue
+            c = ''
+            if k in cfuncs.includes0:
+                c = cfuncs.includes0[k]
+            elif k in cfuncs.includes:
+                c = cfuncs.includes[k]
+            elif k in cfuncs.userincludes:
+                c = cfuncs.userincludes[k]
+            elif k in cfuncs.typedefs:
+                c = cfuncs.typedefs[k]
+            elif k in cfuncs.typedefs_generated:
+                c = cfuncs.typedefs_generated[k]
+            elif k in cfuncs.cppmacros:
+                c = cfuncs.cppmacros[k]
+            elif k in cfuncs.cfuncs:
+                c = cfuncs.cfuncs[k]
+            elif k in cfuncs.callbacks:
+                c = cfuncs.callbacks[k]
+            elif k in cfuncs.f90modhooks:
+                c = cfuncs.f90modhooks[k]
+            elif k in cfuncs.commonhooks:
+                c = cfuncs.commonhooks[k]
+            else:
+                errmess('buildmodule: unknown need %s.\n' % (repr(k)))
+                continue
             code[n].append(c)
     mod_rules.append(code)
     for r in mod_rules:
-        if (r.has_key('_check') and r['_check'](m)) or (not r.has_key('_check')):
-            ar=applyrules(r,vrd,m)
-            rd=dictappend(rd,ar)
-    ar=applyrules(module_rules,rd)
-
-    fn = os.path.join(options['buildpath'],vrd['modulename']+'module.c')
+        if ('_check' in r and r['_check'](m)) or ('_check' not in r):
+            ar = applyrules(r, vrd, m)
+            rd = dictappend(rd, ar)
+    ar = applyrules(module_rules, rd)
+
+    fn = os.path.join(options['buildpath'], vrd['coutput'])
     ret['csrc'] = fn
-    f=open(fn,'w')
-    f.write(string.replace(ar['modulebody'],'\t',2*' '))
-    f.close()
-    outmess('\tWrote C/API module "%s" to file "%s/%smodule.c"\n'%(m['name'],options['buildpath'],vrd['modulename']))
+    with open(fn, 'w') as f:
+        f.write(ar['modulebody'].replace('\t', 2 * ' '))
+    outmess('    Wrote C/API module "%s" to file "%s"\n' % (m['name'], fn))
 
     if options['dorestdoc']:
-        fn = os.path.join(options['buildpath'],vrd['modulename']+'module.rest')
-        f=open(fn,'w')
-        f.write('.. -*- rest -*-\n')
-        f.write(string.join(ar['restdoc'],'\n'))
-        f.close()
-        outmess('\tReST Documentation is saved to file "%s/%smodule.rest"\n'%(options['buildpath'],vrd['modulename']))
+        fn = os.path.join(
+            options['buildpath'], vrd['modulename'] + 'module.rest')
+        with open(fn, 'w') as f:
+            f.write('.. -*- rest -*-\n')
+            f.write('\n'.join(ar['restdoc']))
+        outmess('    ReST Documentation is saved to file "%s/%smodule.rest"\n' %
+                (options['buildpath'], vrd['modulename']))
     if options['dolatexdoc']:
-        fn = os.path.join(options['buildpath'],vrd['modulename']+'module.tex')
+        fn = os.path.join(
+            options['buildpath'], vrd['modulename'] + 'module.tex')
         ret['ltx'] = fn
-        f=open(fn,'w')
-        f.write('%% This file is auto-generated with f2py (version:%s)\n'%(f2py_version))
-        if not options.has_key('shortlatex'):
-            f.write('\\documentclass{article}\n\\usepackage{a4wide}\n\\begin{document}\n\\tableofcontents\n\n')
-        f.write(string.join(ar['latexdoc'],'\n'))
-        if not options.has_key('shortlatex'):
-            f.write('\\end{document}')
-        f.close()
-        outmess('\tDocumentation is saved to file "%s/%smodule.tex"\n'%(options['buildpath'],vrd['modulename']))
+        with open(fn, 'w') as f:
+            f.write(
+                '%% This file is auto-generated with f2py (version:%s)\n' % (f2py_version))
+            if 'shortlatex' not in options:
+                f.write(
+                    '\\documentclass{article}\n\\usepackage{a4wide}\n\\begin{document}\n\\tableofcontents\n\n')
+                f.write('\n'.join(ar['latexdoc']))
+            if 'shortlatex' not in options:
+                f.write('\\end{document}')
+        outmess('    Documentation is saved to file "%s/%smodule.tex"\n' %
+                (options['buildpath'], vrd['modulename']))
     if funcwrappers:
-        wn = os.path.join(options['buildpath'],'%s-f2pywrappers.f'%(vrd['modulename']))
+        wn = os.path.join(options['buildpath'], vrd['f2py_wrapper_output'])
         ret['fsrc'] = wn
-        f=open(wn,'w')
-        f.write('C     -*- fortran -*-\n')
-        f.write('C     This file is autogenerated with f2py (version:%s)\n'%(f2py_version))
-        f.write('C     It contains Fortran 77 wrappers to fortran functions.\n')
-        lines = []
-        for l in string.split(string.join(funcwrappers,'\n\n')+'\n','\n'):
-            if l and l[0]==' ':
-                while len(l)>=66:
-                    lines.append(l[:66]+'\n     &')
-                    l = l[66:]
-                lines.append(l+'\n')
-            else: lines.append(l+'\n')
-        lines = string.join(lines,'').replace('\n     &\n','\n')
-        f.write(lines)
-        f.close()
-        outmess('\tFortran 77 wrappers are saved to "%s"\n'%(wn))
+        with open(wn, 'w') as f:
+            f.write('C     -*- fortran -*-\n')
+            f.write(
+                'C     This file is autogenerated with f2py (version:%s)\n' % (f2py_version))
+            f.write(
+                'C     It contains Fortran 77 wrappers to fortran functions.\n')
+            lines = []
+            for l in ('\n\n'.join(funcwrappers) + '\n').split('\n'):
+                if 0 <= l.find('!') < 66:
+                    # don't split comment lines
+                    lines.append(l + '\n')
+                elif l and l[0] == ' ':
+                    while len(l) >= 66:
+                        lines.append(l[:66] + '\n     &')
+                        l = l[66:]
+                    lines.append(l + '\n')
+                else:
+                    lines.append(l + '\n')
+            lines = ''.join(lines).replace('\n     &\n', '\n')
+            f.write(lines)
+        outmess('    Fortran 77 wrappers are saved to "%s"\n' % (wn))
     if funcwrappers2:
-        wn = os.path.join(options['buildpath'],'%s-f2pywrappers2.f90'%(vrd['modulename']))
+        wn = os.path.join(
+            options['buildpath'], '%s-f2pywrappers2.f90' % (vrd['modulename']))
         ret['fsrc'] = wn
-        f=open(wn,'w')
-        f.write('!     -*- f90 -*-\n')
-        f.write('!     This file is autogenerated with f2py (version:%s)\n'%(f2py_version))
-        f.write('!     It contains Fortran 90 wrappers to fortran functions.\n')
-        lines = []
-        for l in string.split(string.join(funcwrappers2,'\n\n')+'\n','\n'):
-            if len(l)>72 and l[0]==' ':
-                lines.append(l[:72]+'&\n     &')
-                l = l[72:]
-                while len(l)>66:
-                    lines.append(l[:66]+'&\n     &')
-                    l = l[66:]
-                lines.append(l+'\n')
-            else: lines.append(l+'\n')
-        lines = string.join(lines,'').replace('\n     &\n','\n')
-        f.write(lines)
-        f.close()
-        outmess('\tFortran 90 wrappers are saved to "%s"\n'%(wn))
+        with open(wn, 'w') as f:
+            f.write('!     -*- f90 -*-\n')
+            f.write(
+                '!     This file is autogenerated with f2py (version:%s)\n' % (f2py_version))
+            f.write(
+                '!     It contains Fortran 90 wrappers to fortran functions.\n')
+            lines = []
+            for l in ('\n\n'.join(funcwrappers2) + '\n').split('\n'):
+                if 0 <= l.find('!') < 72:
+                    # don't split comment lines
+                    lines.append(l + '\n')
+                elif len(l) > 72 and l[0] == ' ':
+                    lines.append(l[:72] + '&\n     &')
+                    l = l[72:]
+                    while len(l) > 66:
+                        lines.append(l[:66] + '&\n     &')
+                        l = l[66:]
+                    lines.append(l + '\n')
+                else:
+                    lines.append(l + '\n')
+            lines = ''.join(lines).replace('\n     &\n', '\n')
+            f.write(lines)
+        outmess('    Fortran 90 wrappers are saved to "%s"\n' % (wn))
     return ret
 
 ################## Build C/API function #############
 
-stnd={1:'st',2:'nd',3:'rd',4:'th',5:'th',6:'th',7:'th',8:'th',9:'th',0:'th'}
+stnd = {1: 'st', 2: 'nd', 3: 'rd', 4: 'th', 5: 'th',
+        6: 'th', 7: 'th', 8: 'th', 9: 'th', 0: 'th'}
+
+
 def buildapi(rout):
-    rout,wrap = func2subr.assubr(rout)
-    args,depargs=getargs2(rout)
-    capi_maps.depargs=depargs
-    var=rout['vars']
-    auxvars = [a for a in var.keys() if isintent_aux(var[a])]
+    rout, wrap = func2subr.assubr(rout)
+    args, depargs = getargs2(rout)
+    capi_maps.depargs = depargs
+    var = rout['vars']
 
     if ismoduleroutine(rout):
-        outmess('\t\t\tConstructing wrapper function "%s.%s"...\n'%(rout['modulename'],rout['name']))
+        outmess('            Constructing wrapper function "%s.%s"...\n' %
+                (rout['modulename'], rout['name']))
     else:
-        outmess('\t\tConstructing wrapper function "%s"...\n'%(rout['name']))
+        outmess('        Constructing wrapper function "%s"...\n' % (rout['name']))
     # Routine
-    vrd=routsign2map(rout)
-    rd=dictappend({},vrd)
+    vrd = capi_maps.routsign2map(rout)
+    rd = dictappend({}, vrd)
     for r in rout_rules:
-        if (r.has_key('_check') and r['_check'](rout)) or (not r.has_key('_check')):
-            ar=applyrules(r,vrd,rout)
-            rd=dictappend(rd,ar)
+        if ('_check' in r and r['_check'](rout)) or ('_check' not in r):
+            ar = applyrules(r, vrd, rout)
+            rd = dictappend(rd, ar)
 
     # Args
-    nth,nthk=0,0
-    savevrd={}
+    nth, nthk = 0, 0
+    savevrd = {}
     for a in args:
-        vrd=sign2map(a,var[a])
+        vrd = capi_maps.sign2map(a, var[a])
         if isintent_aux(var[a]):
             _rules = aux_rules
         else:
             _rules = arg_rules
             if not isintent_hide(var[a]):
                 if not isoptional(var[a]):
-                    nth=nth+1
-                    vrd['nth']=`nth`+stnd[nth%10]+' argument'
+                    nth = nth + 1
+                    vrd['nth'] = repr(nth) + stnd[nth % 10] + ' argument'
                 else:
-                    nthk=nthk+1
-                    vrd['nth']=`nthk`+stnd[nthk%10]+' keyword'
-            else: vrd['nth']='hidden'
-        savevrd[a]=vrd
+                    nthk = nthk + 1
+                    vrd['nth'] = repr(nthk) + stnd[nthk % 10] + ' keyword'
+            else:
+                vrd['nth'] = 'hidden'
+        savevrd[a] = vrd
         for r in _rules:
-            if r.has_key('_depend'): continue
-            if (r.has_key('_check') and r['_check'](var[a])) or (not r.has_key('_check')):
-                ar=applyrules(r,vrd,var[a])
-                rd=dictappend(rd,ar)
-                if r.has_key('_break'): break
+            if '_depend' in r:
+                continue
+            if ('_check' in r and r['_check'](var[a])) or ('_check' not in r):
+                ar = applyrules(r, vrd, var[a])
+                rd = dictappend(rd, ar)
+                if '_break' in r:
+                    break
     for a in depargs:
         if isintent_aux(var[a]):
             _rules = aux_rules
         else:
             _rules = arg_rules
-        vrd=savevrd[a]
+        vrd = savevrd[a]
         for r in _rules:
-            if not r.has_key('_depend'): continue
-            if (r.has_key('_check') and r['_check'](var[a])) or (not r.has_key('_check')):
-                ar=applyrules(r,vrd,var[a])
-                rd=dictappend(rd,ar)
-                if r.has_key('_break'): break
-        if var[a].has_key('check'):
+            if '_depend' not in r:
+                continue
+            if ('_check' in r and r['_check'](var[a])) or ('_check' not in r):
+                ar = applyrules(r, vrd, var[a])
+                rd = dictappend(rd, ar)
+                if '_break' in r:
+                    break
+        if 'check' in var[a]:
             for c in var[a]['check']:
-                vrd['check']=c
-                ar=applyrules(check_rules,vrd,var[a])
-                rd=dictappend(rd,ar)
-    if type(rd['cleanupfrompyobj']) is types.ListType:
+                vrd['check'] = c
+                ar = applyrules(check_rules, vrd, var[a])
+                rd = dictappend(rd, ar)
+    if isinstance(rd['cleanupfrompyobj'], list):
         rd['cleanupfrompyobj'].reverse()
-    if type(rd['closepyobjfrom']) is types.ListType:
+    if isinstance(rd['closepyobjfrom'], list):
         rd['closepyobjfrom'].reverse()
-    rd['docsignature']=stripcomma(replace('#docsign##docsignopt##docsignxa#',
-                                          {'docsign':rd['docsign'],
-                                           'docsignopt':rd['docsignopt'],
-                                           'docsignxa':rd['docsignxa']}))
-    optargs=stripcomma(replace('#docsignopt##docsignxa#',
-                               {'docsignxa':rd['docsignxashort'],
-                                'docsignopt':rd['docsignoptshort']}
-                               ))
-    if optargs=='':
-        rd['docsignatureshort']=stripcomma(replace('#docsign#',{'docsign':rd['docsign']}))
+    rd['docsignature'] = stripcomma(replace('#docsign##docsignopt##docsignxa#',
+                                            {'docsign': rd['docsign'],
+                                             'docsignopt': rd['docsignopt'],
+                                             'docsignxa': rd['docsignxa']}))
+    optargs = stripcomma(replace('#docsignopt##docsignxa#',
+                                 {'docsignxa': rd['docsignxashort'],
+                                  'docsignopt': rd['docsignoptshort']}
+                                 ))
+    if optargs == '':
+        rd['docsignatureshort'] = stripcomma(
+            replace('#docsign#', {'docsign': rd['docsign']}))
     else:
-        rd['docsignatureshort']=replace('#docsign#[#docsignopt#]',
-                                        {'docsign':rd['docsign'],
-                                         'docsignopt':optargs,
-                                         })
-    rd['latexdocsignatureshort']=string.replace(rd['docsignatureshort'],'_','\\_')
-    rd['latexdocsignatureshort']=string.replace(rd['latexdocsignatureshort'],',',', ')
-    cfs=stripcomma(replace('#callfortran##callfortranappend#',{'callfortran':rd['callfortran'],'callfortranappend':rd['callfortranappend']}))
-    if len(rd['callfortranappend'])>1:
-        rd['callcompaqfortran']=stripcomma(replace('#callfortran# 0,#callfortranappend#',{'callfortran':rd['callfortran'],'callfortranappend':rd['callfortranappend']}))
+        rd['docsignatureshort'] = replace('#docsign#[#docsignopt#]',
+                                          {'docsign': rd['docsign'],
+                                           'docsignopt': optargs,
+                                           })
+    rd['latexdocsignatureshort'] = rd['docsignatureshort'].replace('_', '\\_')
+    rd['latexdocsignatureshort'] = rd[
+        'latexdocsignatureshort'].replace(',', ', ')
+    cfs = stripcomma(replace('#callfortran##callfortranappend#', {
+                     'callfortran': rd['callfortran'], 'callfortranappend': rd['callfortranappend']}))
+    if len(rd['callfortranappend']) > 1:
+        rd['callcompaqfortran'] = stripcomma(replace('#callfortran# 0,#callfortranappend#', {
+                                             'callfortran': rd['callfortran'], 'callfortranappend': rd['callfortranappend']}))
     else:
-        rd['callcompaqfortran']=cfs
-    rd['callfortran']=cfs
-    if type(rd['docreturn'])==types.ListType:
-        rd['docreturn']=stripcomma(replace('#docreturn#',{'docreturn':rd['docreturn']}))+' = '
-    rd['docstrsigns']=[]
-    rd['latexdocstrsigns']=[]
-    for k in ['docstrreq','docstropt','docstrout','docstrcbs']:
-        if rd.has_key(k) and type(rd[k])==types.ListType:
-            rd['docstrsigns']=rd['docstrsigns']+rd[k]
-        k='latex'+k
-        if rd.has_key(k) and type(rd[k])==types.ListType:
-            rd['latexdocstrsigns']=rd['latexdocstrsigns']+rd[k][0:1]+\
-                                    ['\\begin{description}']+rd[k][1:]+\
-                                    ['\\end{description}']
-    ar=applyrules(routine_rules,rd)
+        rd['callcompaqfortran'] = cfs
+    rd['callfortran'] = cfs
+    if isinstance(rd['docreturn'], list):
+        rd['docreturn'] = stripcomma(
+            replace('#docreturn#', {'docreturn': rd['docreturn']})) + ' = '
+    rd['docstrsigns'] = []
+    rd['latexdocstrsigns'] = []
+    for k in ['docstrreq', 'docstropt', 'docstrout', 'docstrcbs']:
+        if k in rd and isinstance(rd[k], list):
+            rd['docstrsigns'] = rd['docstrsigns'] + rd[k]
+        k = 'latex' + k
+        if k in rd and isinstance(rd[k], list):
+            rd['latexdocstrsigns'] = rd['latexdocstrsigns'] + rd[k][0:1] +\
+                ['\\begin{description}'] + rd[k][1:] +\
+                ['\\end{description}']
+
+    ar = applyrules(routine_rules, rd)
     if ismoduleroutine(rout):
-        outmess('\t\t\t  %s\n'%(ar['docshort']))
+        outmess('              %s\n' % (ar['docshort']))
     else:
-        outmess('\t\t  %s\n'%(ar['docshort']))
-    return ar,wrap
+        outmess('          %s\n' % (ar['docshort']))
+    return ar, wrap
 
 
 #################### EOF rules.py #######################
('numpy/f2py', 'f2py2e.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,10 +1,10 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 """
 
 f2py2e - Fortran to Python C/API generator. 2nd Edition.
          See __usage__ below.
 
-Copyright 1999--2005 Pearu Peterson all rights reserved,
+Copyright 1999--2011 Pearu Peterson all rights reserved,
 Pearu Peterson <pearu@cens.ioc.ee>
 Permission to use, modify, and distribute this software is given under the
 terms of the NumPy License.
@@ -12,36 +12,32 @@
 NO WARRANTY IS EXPRESSED OR IMPLIED.  USE AT YOUR OWN RISK.
 $Date: 2005/05/06 08:31:19 $
 Pearu Peterson
+
 """
-__version__ = "$Revision: 1.90 $"[10:-1]
-
-import __version__
+import sys
+import os
+import pprint
+import re
+from pathlib import Path
+
+from . import crackfortran
+from . import rules
+from . import cb_rules
+from . import auxfuncs
+from . import cfuncs
+from . import f90mod_rules
+from . import __version__
+from . import capi_maps
+
 f2py_version = __version__.version
-
-import sys,os,string,pprint,shutil,types,re
-errmess=sys.stderr.write
-#outmess=sys.stdout.write
-show=pprint.pprint
-
-import crackfortran
-import rules
-import cb_rules
-import common_rules
-import auxfuncs
-import cfuncs
-import capi_maps
-import func2subr
-import f90mod_rules
-
+numpy_version = __version__.version
+errmess = sys.stderr.write
+# outmess=sys.stdout.write
+show = pprint.pprint
 outmess = auxfuncs.outmess
 
-try:
-    from numpy import __version__ as numpy_version
-except ImportError:
-    numpy_version = 'N/A'
-
-__usage__ = """\
-Usage:
+__usage__ =\
+f"""Usage:
 
 1) To construct extension module sources:
 
@@ -63,6 +59,12 @@
              extension modules are built.
 
 Options:
+
+  --2d-numpy       Use numpy.f2py tool with NumPy support. [DEFAULT]
+  --2d-numeric     Use f2py2e tool with Numeric support.
+  --2d-numarray    Use f2py2e tool with Numarray support.
+  --g3-numpy       Use 3rd generation f2py from the separate f2py package.
+                   [NOT AVAILABLE YET]
 
   -h <filename>    Write signatures of the fortran routines to file <filename>
                    and exit. You can then edit <filename> and use it instead
@@ -81,19 +83,22 @@
                    file <modulename>module.c or extension module <modulename>.
                    Default is 'untitled'.
 
+  '-include<header>'  Writes additional headers in the C wrapper, can be passed
+                      multiple times, generates #include <header> each time.
+
   --[no-]lower     Do [not] lower the cases in <fortran files>. By default,
                    --lower is assumed with -h key, and --no-lower without -h key.
 
   --build-dir <dirname>  All f2py generated files are created in <dirname>.
-                   Default is tempfile.mktemp().
+                   Default is tempfile.mkdtemp().
 
   --overwrite-signature  Overwrite existing signature file.
 
   --[no-]latex-doc Create (or not) <modulename>module.tex.
                    Default is --no-latex-doc.
   --short-latex    Create 'incomplete' LaTeX document (without commands
-                   \\documentclass, \\tableofcontents, and \\begin{document},
-                   \\end{document}).
+                   \\documentclass, \\tableofcontents, and \\begin{{document}},
+                   \\end{{document}}).
 
   --[no-]rest-doc Create (or not) <modulename>module.rst.
                    Default is --no-rest-doc.
@@ -105,15 +110,19 @@
                    functions. --wrap-functions is default because it ensures
                    maximum portability/compiler independence.
 
-  --include_paths <path1>:<path2>:...   Search include files from the given
+  --include-paths <path1>:<path2>:...   Search include files from the given
                    directories.
 
   --help-link [..] List system resources found by system_info.py. See also
                    --link-<resource> switch below. [..] is optional list
                    of resources names. E.g. try 'f2py --help-link lapack_opt'.
 
+  --f2cmap <filename>  Load Fortran-to-Python KIND specification from the given
+                   file. Default: .f2py_f2cmap in current directory.
+
   --quiet          Run quietly.
   --verbose        Run with extra verbosity.
+  --skip-empty-wrappers   Only generate wrapper files when needed.
   -v               Print f2py version ID and exit.
 
 
@@ -159,348 +168,473 @@
   array. Integer <int> sets the threshold for array sizes when
   a message should be shown.
 
-Version:     %s
-numpy Version: %s
-Requires:    Python 2.3 or higher.
+Version:     {f2py_version}
+numpy Version: {numpy_version}
+Requires:    Python 3.5 or higher.
 License:     NumPy license (see LICENSE.txt in the NumPy source code)
-Copyright 1999 - 2005 Pearu Peterson all rights reserved.
-http://cens.ioc.ee/projects/f2py2e/"""%(f2py_version, numpy_version)
+Copyright 1999 - 2011 Pearu Peterson all rights reserved.
+https://web.archive.org/web/20140822061353/http://cens.ioc.ee/projects/f2py2e"""
 
 
 def scaninputline(inputline):
-    files,funcs,skipfuncs,onlyfuncs,debug=[],[],[],[],[]
-    f,f2,f3,f4,f5,f6,f7=1,0,0,0,0,0,0
+    files, skipfuncs, onlyfuncs, debug = [], [], [], []
+    f, f2, f3, f5, f6, f7, f8, f9, f10 = 1, 0, 0, 0, 0, 0, 0, 0, 0
     verbose = 1
-    dolc=-1
+    emptygen = True
+    dolc = -1
     dolatexdoc = 0
     dorestdoc = 0
     wrapfuncs = 1
     buildpath = '.'
     include_paths = []
-    signsfile,modulename=None,None
-    options = {'buildpath':buildpath}
+    signsfile, modulename = None, None
+    options = {'buildpath': buildpath,
+               'coutput': None,
+               'f2py_wrapper_output': None}
     for l in inputline:
-        if l=='': pass
-        elif l=='only:': f=0
-        elif l=='skip:': f=-1
-        elif l==':': f=1;f4=0
-        elif l[:8]=='--debug-': debug.append(l[8:])
-        elif l=='--lower': dolc=1
-        elif l=='--build-dir': f6=1
-        elif l=='--no-lower': dolc=0
-        elif l=='--quiet': verbose = 0
-        elif l=='--verbose': verbose += 1
-        elif l=='--latex-doc': dolatexdoc=1
-        elif l=='--no-latex-doc': dolatexdoc=0
-        elif l=='--rest-doc': dorestdoc=1
-        elif l=='--no-rest-doc': dorestdoc=0
-        elif l=='--wrap-functions': wrapfuncs=1
-        elif l=='--no-wrap-functions': wrapfuncs=0
-        elif l=='--short-latex': options['shortlatex']=1
-        elif l=='--overwrite-signature': options['h-overwrite']=1
-        elif l=='-h': f2=1
-        elif l=='-m': f3=1
-        elif l[:2]=='-v':
-            print f2py_version
+        if l == '':
+            pass
+        elif l == 'only:':
+            f = 0
+        elif l == 'skip:':
+            f = -1
+        elif l == ':':
+            f = 1
+        elif l[:8] == '--debug-':
+            debug.append(l[8:])
+        elif l == '--lower':
+            dolc = 1
+        elif l == '--build-dir':
+            f6 = 1
+        elif l == '--no-lower':
+            dolc = 0
+        elif l == '--quiet':
+            verbose = 0
+        elif l == '--verbose':
+            verbose += 1
+        elif l == '--latex-doc':
+            dolatexdoc = 1
+        elif l == '--no-latex-doc':
+            dolatexdoc = 0
+        elif l == '--rest-doc':
+            dorestdoc = 1
+        elif l == '--no-rest-doc':
+            dorestdoc = 0
+        elif l == '--wrap-functions':
+            wrapfuncs = 1
+        elif l == '--no-wrap-functions':
+            wrapfuncs = 0
+        elif l == '--short-latex':
+            options['shortlatex'] = 1
+        elif l == '--coutput':
+            f8 = 1
+        elif l == '--f2py-wrapper-output':
+            f9 = 1
+        elif l == '--f2cmap':
+            f10 = 1
+        elif l == '--overwrite-signature':
+            options['h-overwrite'] = 1
+        elif l == '-h':
+            f2 = 1
+        elif l == '-m':
+            f3 = 1
+        elif l[:2] == '-v':
+            print(f2py_version)
             sys.exit()
-        elif l=='--show-compilers':
-            f5=1
-        elif l[:8]=='-include':
+        elif l == '--show-compilers':
+            f5 = 1
+        elif l[:8] == '-include':
             cfuncs.outneeds['userincludes'].append(l[9:-1])
-            cfuncs.userincludes[l[9:-1]]='#include '+l[8:]
-        elif l[:15]=='--include_paths':
-            f7=1
-        elif l[0]=='-':
-            errmess('Unknown option %s\n'%`l`)
+            cfuncs.userincludes[l[9:-1]] = '#include ' + l[8:]
+        elif l[:15] in '--include_paths':
+            outmess(
+                'f2py option --include_paths is deprecated, use --include-paths instead.\n')
+            f7 = 1
+        elif l[:15] in '--include-paths':
+            f7 = 1
+        elif l == '--skip-empty-wrappers':
+            emptygen = False
+        elif l[0] == '-':
+            errmess('Unknown option %s\n' % repr(l))
             sys.exit()
-        elif f2: f2=0;signsfile=l
-        elif f3: f3=0;modulename=l
-        elif f6: f6=0;buildpath=l
-        elif f7: f7=0;include_paths.extend(l.split(os.pathsep))
-        elif f==1:
+        elif f2:
+            f2 = 0
+            signsfile = l
+        elif f3:
+            f3 = 0
+            modulename = l
+        elif f6:
+            f6 = 0
+            buildpath = l
+        elif f7:
+            f7 = 0
+            include_paths.extend(l.split(os.pathsep))
+        elif f8:
+            f8 = 0
+            options["coutput"] = l
+        elif f9:
+            f9 = 0
+            options["f2py_wrapper_output"] = l
+        elif f10:
+            f10 = 0
+            options["f2cmap_file"] = l
+        elif f == 1:
             try:
-                open(l).close()
+                with open(l):
+                    pass
                 files.append(l)
-            except IOError,detail:
-                errmess('IOError: %s. Skipping file "%s".\n'%(str(detail),l))
-        elif f==-1: skipfuncs.append(l)
-        elif f==0: onlyfuncs.append(l)
+            except OSError as detail:
+                errmess(f'OSError: {detail!s}. Skipping file "{l!s}".\n')
+        elif f == -1:
+            skipfuncs.append(l)
+        elif f == 0:
+            onlyfuncs.append(l)
     if not f5 and not files and not modulename:
-        print __usage__
+        print(__usage__)
         sys.exit()
     if not os.path.isdir(buildpath):
         if not verbose:
-            outmess('Creating build directory %s'%(buildpath))
+            outmess('Creating build directory %s\n' % (buildpath))
         os.mkdir(buildpath)
     if signsfile:
-        signsfile = os.path.join(buildpath,signsfile)
-    if signsfile and os.path.isfile(signsfile) and not options.has_key('h-overwrite'):
-        errmess('Signature file "%s" exists!!! Use --overwrite-signature to overwrite.\n'%(signsfile))
+        signsfile = os.path.join(buildpath, signsfile)
+    if signsfile and os.path.isfile(signsfile) and 'h-overwrite' not in options:
+        errmess(
+            'Signature file "%s" exists!!! Use --overwrite-signature to overwrite.\n' % (signsfile))
         sys.exit()
 
-    options['debug']=debug
-    options['verbose']=verbose
-    if dolc==-1 and not signsfile: options['do-lower']=0
-    else: options['do-lower']=dolc
-    if modulename: options['module']=modulename
-    if signsfile: options['signsfile']=signsfile
-    if onlyfuncs: options['onlyfuncs']=onlyfuncs
-    if skipfuncs: options['skipfuncs']=skipfuncs
+    options['emptygen'] = emptygen
+    options['debug'] = debug
+    options['verbose'] = verbose
+    if dolc == -1 and not signsfile:
+        options['do-lower'] = 0
+    else:
+        options['do-lower'] = dolc
+    if modulename:
+        options['module'] = modulename
+    if signsfile:
+        options['signsfile'] = signsfile
+    if onlyfuncs:
+        options['onlyfuncs'] = onlyfuncs
+    if skipfuncs:
+        options['skipfuncs'] = skipfuncs
     options['dolatexdoc'] = dolatexdoc
     options['dorestdoc'] = dorestdoc
     options['wrapfuncs'] = wrapfuncs
-    options['buildpath']=buildpath
-    options['include_paths']=include_paths
-    return files,options
-
-def callcrackfortran(files,options):
-    rules.options=options
-    funcs=[]
-    crackfortran.debug=options['debug']
-    crackfortran.verbose=options['verbose']
-    if options.has_key('module'):
-        crackfortran.f77modulename=options['module']
-    if options.has_key('skipfuncs'):
-        crackfortran.skipfuncs=options['skipfuncs']
-    if options.has_key('onlyfuncs'):
-        crackfortran.onlyfuncs=options['onlyfuncs']
-    crackfortran.include_paths[:]=options['include_paths']
-    crackfortran.dolowercase=options['do-lower']
-    postlist=crackfortran.crackfortran(files)
-    if options.has_key('signsfile'):
-        outmess('Saving signatures to file "%s"\n'%(options['signsfile']))
-        pyf=crackfortran.crack2fortran(postlist)
-        if options['signsfile'][-6:]=='stdout':
+    options['buildpath'] = buildpath
+    options['include_paths'] = include_paths
+    options.setdefault('f2cmap_file', None)
+    return files, options
+
+
+def callcrackfortran(files, options):
+    rules.options = options
+    crackfortran.debug = options['debug']
+    crackfortran.verbose = options['verbose']
+    if 'module' in options:
+        crackfortran.f77modulename = options['module']
+    if 'skipfuncs' in options:
+        crackfortran.skipfuncs = options['skipfuncs']
+    if 'onlyfuncs' in options:
+        crackfortran.onlyfuncs = options['onlyfuncs']
+    crackfortran.include_paths[:] = options['include_paths']
+    crackfortran.dolowercase = options['do-lower']
+    postlist = crackfortran.crackfortran(files)
+    if 'signsfile' in options:
+        outmess('Saving signatures to file "%s"\n' % (options['signsfile']))
+        pyf = crackfortran.crack2fortran(postlist)
+        if options['signsfile'][-6:] == 'stdout':
             sys.stdout.write(pyf)
         else:
-            f=open(options['signsfile'],'w')
-            f.write(pyf)
-            f.close()
+            with open(options['signsfile'], 'w') as f:
+                f.write(pyf)
+    if options["coutput"] is None:
+        for mod in postlist:
+            mod["coutput"] = "%smodule.c" % mod["name"]
+    else:
+        for mod in postlist:
+            mod["coutput"] = options["coutput"]
+    if options["f2py_wrapper_output"] is None:
+        for mod in postlist:
+            mod["f2py_wrapper_output"] = "%s-f2pywrappers.f" % mod["name"]
+    else:
+        for mod in postlist:
+            mod["f2py_wrapper_output"] = options["f2py_wrapper_output"]
     return postlist
 
-def buildmodules(list):
+
+def buildmodules(lst):
     cfuncs.buildcfuncs()
     outmess('Building modules...\n')
-    modules,mnames,isusedby=[],[],{}
-    for i in range(len(list)):
-        if string.find(list[i]['name'],'__user__')>=0:
-            cb_rules.buildcallbacks(list[i])
+    modules, mnames, isusedby = [], [], {}
+    for item in lst:
+        if '__user__' in item['name']:
+            cb_rules.buildcallbacks(item)
         else:
-            if list[i].has_key('use'):
-                for u in list[i]['use'].keys():
-                    if not isusedby.has_key(u): isusedby[u]=[]
-                    isusedby[u].append(list[i]['name'])
-            modules.append(list[i])
-            mnames.append(list[i]['name'])
+            if 'use' in item:
+                for u in item['use'].keys():
+                    if u not in isusedby:
+                        isusedby[u] = []
+                    isusedby[u].append(item['name'])
+            modules.append(item)
+            mnames.append(item['name'])
     ret = {}
-    for i in range(len(mnames)):
-        if isusedby.has_key(mnames[i]):
-            outmess('\tSkipping module "%s" which is used by %s.\n'%(mnames[i],string.join(map(lambda s:'"%s"'%s,isusedby[mnames[i]]),',')))
+    for module, name in zip(modules, mnames):
+        if name in isusedby:
+            outmess('\tSkipping module "%s" which is used by %s.\n' % (
+                name, ','.join('"%s"' % s for s in isusedby[name])))
         else:
-            um=[]
-            if modules[i].has_key('use'):
-                for u in modules[i]['use'].keys():
-                    if isusedby.has_key(u) and u in mnames:
+            um = []
+            if 'use' in module:
+                for u in module['use'].keys():
+                    if u in isusedby and u in mnames:
                         um.append(modules[mnames.index(u)])
                     else:
-                        outmess('\tModule "%s" uses nonexisting "%s" which will be ignored.\n'%(mnames[i],u))
-            ret[mnames[i]] = {}
-            dict_append(ret[mnames[i]],rules.buildmodule(modules[i],um))
+                        outmess(
+                            f'\tModule "{name}" uses nonexisting "{u}" '
+                            'which will be ignored.\n')
+            ret[name] = {}
+            dict_append(ret[name], rules.buildmodule(module, um))
     return ret
 
-def dict_append(d_out,d_in):
-    for (k,v) in d_in.items():
-        if not d_out.has_key(k):
+
+def dict_append(d_out, d_in):
+    for (k, v) in d_in.items():
+        if k not in d_out:
             d_out[k] = []
-        if type(v) is types.ListType:
+        if isinstance(v, list):
             d_out[k] = d_out[k] + v
         else:
             d_out[k].append(v)
 
+
 def run_main(comline_list):
-    """Run f2py as if string.join(comline_list,' ') is used as a command line.
-    In case of using -h flag, return None.
     """
-    reload(crackfortran)
-    f2pydir=os.path.dirname(os.path.abspath(cfuncs.__file__))
-    fobjhsrc = os.path.join(f2pydir,'src','fortranobject.h')
-    fobjcsrc = os.path.join(f2pydir,'src','fortranobject.c')
-    files,options=scaninputline(comline_list)
-    auxfuncs.options=options
-    postlist=callcrackfortran(files,options)
-    isusedby={}
-    for i in range(len(postlist)):
-        if postlist[i].has_key('use'):
-            for u in postlist[i]['use'].keys():
-                if not isusedby.has_key(u): isusedby[u]=[]
-                isusedby[u].append(postlist[i]['name'])
-    for i in range(len(postlist)):
-        if postlist[i]['block']=='python module' and string.find(postlist[i]['name'],'__user__')<0:
-            if isusedby.has_key(postlist[i]['name']):
-                #if not quiet:
-                outmess('Skipping Makefile build for module "%s" which is used by %s\n'%(postlist[i]['name'],string.join(map(lambda s:'"%s"'%s,isusedby[postlist[i]['name']]),',')))
-    if options.has_key('signsfile'):
-        if options['verbose']>1:
-            outmess('Stopping. Edit the signature file and then run f2py on the signature file: ')
-            outmess('%s %s\n'%(os.path.basename(sys.argv[0]),options['signsfile']))
+    Equivalent to running::
+
+        f2py <args>
+
+    where ``<args>=string.join(<list>,' ')``, but in Python.  Unless
+    ``-h`` is used, this function returns a dictionary containing
+    information on generated modules and their dependencies on source
+    files.
+
+    You cannot build extension modules with this function, that is,
+    using ``-c`` is not allowed. Use the ``compile`` command instead.
+
+    Examples
+    --------
+    The command ``f2py -m scalar scalar.f`` can be executed from Python as
+    follows.
+
+    .. literalinclude:: ../../source/f2py/code/results/run_main_session.dat
+        :language: python
+
+    """
+    crackfortran.reset_global_f2py_vars()
+    f2pydir = os.path.dirname(os.path.abspath(cfuncs.__file__))
+    fobjhsrc = os.path.join(f2pydir, 'src', 'fortranobject.h')
+    fobjcsrc = os.path.join(f2pydir, 'src', 'fortranobject.c')
+    files, options = scaninputline(comline_list)
+    auxfuncs.options = options
+    capi_maps.load_f2cmap_file(options['f2cmap_file'])
+    postlist = callcrackfortran(files, options)
+    isusedby = {}
+    for plist in postlist:
+        if 'use' in plist:
+            for u in plist['use'].keys():
+                if u not in isusedby:
+                    isusedby[u] = []
+                isusedby[u].append(plist['name'])
+    for plist in postlist:
+        if plist['block'] == 'python module' and '__user__' in plist['name']:
+            if plist['name'] in isusedby:
+                # if not quiet:
+                outmess(
+                    f'Skipping Makefile build for module "{plist["name"]}" '
+                    'which is used by {}\n'.format(
+                        ','.join(f'"{s}"' for s in isusedby[plist['name']])))
+    if 'signsfile' in options:
+        if options['verbose'] > 1:
+            outmess(
+                'Stopping. Edit the signature file and then run f2py on the signature file: ')
+            outmess('%s %s\n' %
+                    (os.path.basename(sys.argv[0]), options['signsfile']))
         return
-    for i in range(len(postlist)):
-        if postlist[i]['block']!='python module':
-            if not options.has_key('python module'):
-                errmess('Tip: If your original code is Fortran 77 then you must use -m option.\n')
-            raise TypeError,'All blocks must be module blocks but got %s'%(`postlist[i]['block']`)
-    auxfuncs.debugoptions=options['debug']
-    f90mod_rules.options=options
-    auxfuncs.wrapfuncs=options['wrapfuncs']
-
-    ret=buildmodules(postlist)
+    for plist in postlist:
+        if plist['block'] != 'python module':
+            if 'python module' not in options:
+                errmess(
+                    'Tip: If your original code is Fortran source then you must use -m option.\n')
+            raise TypeError('All blocks must be python module blocks but got %s' % (
+                repr(plist['block'])))
+    auxfuncs.debugoptions = options['debug']
+    f90mod_rules.options = options
+    auxfuncs.wrapfuncs = options['wrapfuncs']
+
+    ret = buildmodules(postlist)
 
     for mn in ret.keys():
-        dict_append(ret[mn],{'csrc':fobjcsrc,'h':fobjhsrc})
+        dict_append(ret[mn], {'csrc': fobjcsrc, 'h': fobjhsrc})
     return ret
 
-def filter_files(prefix,suffix,files,remove_prefix=None):
+
+def filter_files(prefix, suffix, files, remove_prefix=None):
     """
     Filter files by prefix and suffix.
     """
-    filtered,rest = [],[]
-    match = re.compile(prefix+r'.*'+suffix+r'\Z').match
+    filtered, rest = [], []
+    match = re.compile(prefix + r'.*' + suffix + r'\Z').match
     if remove_prefix:
         ind = len(prefix)
     else:
         ind = 0
-    for file in map(string.strip,files):
-        if match(file): filtered.append(file[ind:])
-        else: rest.append(file)
-    return filtered,rest
+    for file in [x.strip() for x in files]:
+        if match(file):
+            filtered.append(file[ind:])
+        else:
+            rest.append(file)
+    return filtered, rest
+
 
 def get_prefix(module):
     p = os.path.dirname(os.path.dirname(module.__file__))
     return p
+
 
 def run_compile():
     """
     Do it all in one call!
     """
-    import tempfile,os,shutil
+    import tempfile
 
     i = sys.argv.index('-c')
     del sys.argv[i]
 
     remove_build_dir = 0
-    try: i = sys.argv.index('--build-dir')
-    except ValueError: i=None
+    try:
+        i = sys.argv.index('--build-dir')
+    except ValueError:
+        i = None
     if i is not None:
-        build_dir = sys.argv[i+1]
-        del sys.argv[i+1]
+        build_dir = sys.argv[i + 1]
+        del sys.argv[i + 1]
         del sys.argv[i]
     else:
         remove_build_dir = 1
-        build_dir = os.path.join(tempfile.mktemp())
-
-    sysinfo_flags = filter(re.compile(r'[-][-]link[-]').match,sys.argv[1:])
-    sys.argv = filter(lambda a,flags=sysinfo_flags:a not in flags,sys.argv)
+        build_dir = tempfile.mkdtemp()
+
+    _reg1 = re.compile(r'--link-')
+    sysinfo_flags = [_m for _m in sys.argv[1:] if _reg1.match(_m)]
+    sys.argv = [_m for _m in sys.argv if _m not in sysinfo_flags]
     if sysinfo_flags:
         sysinfo_flags = [f[7:] for f in sysinfo_flags]
 
-    f2py_flags = filter(re.compile(r'[-][-]((no[-]|)(wrap[-]functions|lower)|debug[-]capi|quiet)|[-]include').match,sys.argv[1:])
-    sys.argv = filter(lambda a,flags=f2py_flags:a not in flags,sys.argv)
+    _reg2 = re.compile(
+        r'--((no-|)(wrap-functions|lower)|debug-capi|quiet|skip-empty-wrappers)|-include')
+    f2py_flags = [_m for _m in sys.argv[1:] if _reg2.match(_m)]
+    sys.argv = [_m for _m in sys.argv if _m not in f2py_flags]
     f2py_flags2 = []
     fl = 0
     for a in sys.argv[1:]:
-        if a in ['only:','skip:']:
+        if a in ['only:', 'skip:']:
             fl = 1
-        elif a==':':
+        elif a == ':':
             fl = 0
-        if fl or a==':':
+        if fl or a == ':':
             f2py_flags2.append(a)
-    if f2py_flags2 and f2py_flags2[-1]!=':':
+    if f2py_flags2 and f2py_flags2[-1] != ':':
         f2py_flags2.append(':')
     f2py_flags.extend(f2py_flags2)
 
-    sys.argv = filter(lambda a,flags=f2py_flags2:a not in flags,sys.argv)
-
-    flib_flags = filter(re.compile(r'[-][-]((f(90)?compiler([-]exec|)|compiler)=|help[-]compiler)').match,sys.argv[1:])
-    sys.argv = filter(lambda a,flags=flib_flags:a not in flags,sys.argv)
-    fc_flags = filter(re.compile(r'[-][-]((f(77|90)(flags|exec)|opt|arch)=|(debug|noopt|noarch|help[-]fcompiler))').match,sys.argv[1:])
-    sys.argv = filter(lambda a,flags=fc_flags:a not in flags,sys.argv)
-
-    if 1:
-        del_list = []
-        for s in flib_flags:
-            v = '--fcompiler='
-            if s[:len(v)]==v:
-                from numpy.distutils import fcompiler
-                allowed_keys = fcompiler.fcompiler_class.keys()
-                nv = ov = s[len(v):].lower()
-                if ov not in allowed_keys:
-                    vmap = {} # XXX
-                    try:
-                        nv = vmap[ov]
-                    except KeyError:
-                        if ov not in vmap.values():
-                            print 'Unknown vendor: "%s"' % (s[len(v):])
-                    nv = ov
-                i = flib_flags.index(s)
-                flib_flags[i] = '--fcompiler=' + nv
-                continue
-        for s in del_list:
+    sys.argv = [_m for _m in sys.argv if _m not in f2py_flags2]
+    _reg3 = re.compile(
+        r'--((f(90)?compiler(-exec|)|compiler)=|help-compiler)')
+    flib_flags = [_m for _m in sys.argv[1:] if _reg3.match(_m)]
+    sys.argv = [_m for _m in sys.argv if _m not in flib_flags]
+    _reg4 = re.compile(
+        r'--((f(77|90)(flags|exec)|opt|arch)=|(debug|noopt|noarch|help-fcompiler))')
+    fc_flags = [_m for _m in sys.argv[1:] if _reg4.match(_m)]
+    sys.argv = [_m for _m in sys.argv if _m not in fc_flags]
+
+    del_list = []
+    for s in flib_flags:
+        v = '--fcompiler='
+        if s[:len(v)] == v:
+            from numpy.distutils import fcompiler
+            fcompiler.load_all_fcompiler_classes()
+            allowed_keys = list(fcompiler.fcompiler_class.keys())
+            nv = ov = s[len(v):].lower()
+            if ov not in allowed_keys:
+                vmap = {}  # XXX
+                try:
+                    nv = vmap[ov]
+                except KeyError:
+                    if ov not in vmap.values():
+                        print('Unknown vendor: "%s"' % (s[len(v):]))
+                nv = ov
             i = flib_flags.index(s)
-            del flib_flags[i]
-        assert len(flib_flags)<=2,`flib_flags`
-    setup_flags = filter(re.compile(r'[-][-](verbose)').match,sys.argv[1:])
-    sys.argv = filter(lambda a,flags=setup_flags:a not in flags,sys.argv)
+            flib_flags[i] = '--fcompiler=' + nv
+            continue
+    for s in del_list:
+        i = flib_flags.index(s)
+        del flib_flags[i]
+    assert len(flib_flags) <= 2, repr(flib_flags)
+
+    _reg5 = re.compile(r'--(verbose)')
+    setup_flags = [_m for _m in sys.argv[1:] if _reg5.match(_m)]
+    sys.argv = [_m for _m in sys.argv if _m not in setup_flags]
+
     if '--quiet' in f2py_flags:
         setup_flags.append('--quiet')
 
     modulename = 'untitled'
     sources = sys.argv[1:]
+
+    for optname in ['--include_paths', '--include-paths', '--f2cmap']:
+        if optname in sys.argv:
+            i = sys.argv.index(optname)
+            f2py_flags.extend(sys.argv[i:i + 2])
+            del sys.argv[i + 1], sys.argv[i]
+            sources = sys.argv[1:]
+
     if '-m' in sys.argv:
         i = sys.argv.index('-m')
-        modulename = sys.argv[i+1]
-        del sys.argv[i+1],sys.argv[i]
+        modulename = sys.argv[i + 1]
+        del sys.argv[i + 1], sys.argv[i]
         sources = sys.argv[1:]
     else:
         from numpy.distutils.command.build_src import get_f2py_modulename
-        pyf_files,sources = filter_files('','[.]pyf([.]src|)',sources)
+        pyf_files, sources = filter_files('', '[.]pyf([.]src|)', sources)
         sources = pyf_files + sources
         for f in pyf_files:
             modulename = get_f2py_modulename(f)
             if modulename:
                 break
 
-    extra_objects, sources = filter_files('','[.](o|a|so)',sources)
-    include_dirs, sources = filter_files('-I','',sources,remove_prefix=1)
-    library_dirs, sources = filter_files('-L','',sources,remove_prefix=1)
-    libraries, sources = filter_files('-l','',sources,remove_prefix=1)
-    undef_macros, sources = filter_files('-U','',sources,remove_prefix=1)
-    define_macros, sources = filter_files('-D','',sources,remove_prefix=1)
-    using_numarray = 0
-    using_numeric = 0
+    extra_objects, sources = filter_files('', '[.](o|a|so|dylib)', sources)
+    include_dirs, sources = filter_files('-I', '', sources, remove_prefix=1)
+    library_dirs, sources = filter_files('-L', '', sources, remove_prefix=1)
+    libraries, sources = filter_files('-l', '', sources, remove_prefix=1)
+    undef_macros, sources = filter_files('-U', '', sources, remove_prefix=1)
+    define_macros, sources = filter_files('-D', '', sources, remove_prefix=1)
     for i in range(len(define_macros)):
-        name_value = string.split(define_macros[i],'=',1)
-        if len(name_value)==1:
+        name_value = define_macros[i].split('=', 1)
+        if len(name_value) == 1:
             name_value.append(None)
-        if len(name_value)==2:
+        if len(name_value) == 2:
             define_macros[i] = tuple(name_value)
         else:
-            print 'Invalid use of -D:',name_value
+            print('Invalid use of -D:', name_value)
 
     from numpy.distutils.system_info import get_info
 
-    num_include_dir = None
     num_info = {}
-    #import numpy
-    #n = 'numpy'
-    #p = get_prefix(numpy)
-    #from numpy.distutils.misc_util import get_numpy_include_dirs
-    #num_info = {'include_dirs': get_numpy_include_dirs()}
-
     if num_info:
-        include_dirs.extend(num_info.get('include_dirs',[]))
-
-    from numpy.distutils.core import setup,Extension
-    ext_args = {'name':modulename,'sources':sources,
+        include_dirs.extend(num_info.get('include_dirs', []))
+
+    from numpy.distutils.core import setup, Extension
+    ext_args = {'name': modulename, 'sources': sources,
                 'include_dirs': include_dirs,
                 'library_dirs': library_dirs,
                 'libraries': libraries,
@@ -515,26 +649,30 @@
         for n in sysinfo_flags:
             i = get_info(n)
             if not i:
-                outmess('No %s resources found in system'\
-                        ' (try `f2py --help-link`)\n' % (`n`))
-            dict_append(ext_args,**i)
+                outmess('No %s resources found in system'
+                        ' (try `f2py --help-link`)\n' % (repr(n)))
+            dict_append(ext_args, **i)
 
     ext = Extension(**ext_args)
     sys.argv = [sys.argv[0]] + setup_flags
     sys.argv.extend(['build',
-                     '--build-temp',build_dir,
-                     '--build-base',build_dir,
-                     '--build-platlib','.'])
+                     '--build-temp', build_dir,
+                     '--build-base', build_dir,
+                     '--build-platlib', '.',
+                     # disable CCompilerOpt
+                     '--disable-optimization'])
     if fc_flags:
-        sys.argv.extend(['config_fc']+fc_flags)
+        sys.argv.extend(['config_fc'] + fc_flags)
     if flib_flags:
-        sys.argv.extend(['build_ext']+flib_flags)
-
-    setup(ext_modules = [ext])
+        sys.argv.extend(['build_ext'] + flib_flags)
+
+    setup(ext_modules=[ext])
 
     if remove_build_dir and os.path.exists(build_dir):
-        outmess('Removing build directory %s\n'%(build_dir))
+        import shutil
+        outmess('Removing build directory %s\n' % (build_dir))
         shutil.rmtree(build_dir)
+
 
 def main():
     if '--help-link' in sys.argv[1:]:
@@ -542,13 +680,25 @@
         from numpy.distutils.system_info import show_all
         show_all()
         return
+
+    # Probably outdated options that were not working before 1.16
+    if '--g3-numpy' in sys.argv[1:]:
+        sys.stderr.write("G3 f2py support is not implemented, yet.\\n")
+        sys.exit(1)
+    elif '--2e-numeric' in sys.argv[1:]:
+        sys.argv.remove('--2e-numeric')
+    elif '--2e-numarray' in sys.argv[1:]:
+        # Note that this errors becaust the -DNUMARRAY argument is
+        # not recognized. Just here for back compatibility and the
+        # error message.
+        sys.argv.append("-DNUMARRAY")
+        sys.argv.remove('--2e-numarray')
+    elif '--2e-numpy' in sys.argv[1:]:
+        sys.argv.remove('--2e-numpy')
+    else:
+        pass
+
     if '-c' in sys.argv[1:]:
         run_compile()
     else:
         run_main(sys.argv[1:])
-
-#if __name__ == "__main__":
-#    main()
-
-
-# EOF
('numpy/f2py', 'func2subr.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,4 +1,4 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 """
 
 Rules for building C/API module with f2py2e.
@@ -11,155 +11,290 @@
 NO WARRANTY IS EXPRESSED OR IMPLIED.  USE AT YOUR OWN RISK.
 $Date: 2004/11/26 11:13:06 $
 Pearu Peterson
+
 """
-
 __version__ = "$Revision: 1.16 $"[10:-1]
 
-f2py_version='See `f2py -v`'
-
-import pprint,copy
-import sys,string,time,types,copy
-errmess=sys.stderr.write
-outmess=sys.stdout.write
-show=pprint.pprint
-
-from auxfuncs import *
-def var2fixfortran(vars,a,fa=None,f90mode=None):
+f2py_version = 'See `f2py -v`'
+
+import copy
+
+from .auxfuncs import (
+    getfortranname, isexternal, isfunction, isfunction_wrap, isintent_in,
+    isintent_out, islogicalfunction, ismoduleroutine, isscalar,
+    issubroutine, issubroutine_wrap, outmess, show
+)
+
+
+def var2fixfortran(vars, a, fa=None, f90mode=None):
     if fa is None:
         fa = a
-    if not vars.has_key(a):
+    if a not in vars:
         show(vars)
-        outmess('var2fixfortran: No definition for argument "%s".\n'%a)
+        outmess('var2fixfortran: No definition for argument "%s".\n' % a)
         return ''
-    if not vars[a].has_key('typespec'):
+    if 'typespec' not in vars[a]:
         show(vars[a])
-        outmess('var2fixfortran: No typespec for argument "%s".\n'%a)
+        outmess('var2fixfortran: No typespec for argument "%s".\n' % a)
         return ''
-    vardef=vars[a]['typespec']
-    if vardef=='type' and vars[a].has_key('typename'):
-        vardef='%s(%s)'%(vardef,vars[a]['typename'])
-    selector={}
+    vardef = vars[a]['typespec']
+    if vardef == 'type' and 'typename' in vars[a]:
+        vardef = '%s(%s)' % (vardef, vars[a]['typename'])
+    selector = {}
     lk = ''
-    if vars[a].has_key('kindselector'):
-        selector=vars[a]['kindselector']
+    if 'kindselector' in vars[a]:
+        selector = vars[a]['kindselector']
         lk = 'kind'
-    elif vars[a].has_key('charselector'):
-        selector=vars[a]['charselector']
+    elif 'charselector' in vars[a]:
+        selector = vars[a]['charselector']
         lk = 'len'
-    if selector.has_key('*'):
+    if '*' in selector:
         if f90mode:
-            if selector['*'] in ['*',':','(*)']:
-                vardef='%s(len=*)'%(vardef)
+            if selector['*'] in ['*', ':', '(*)']:
+                vardef = '%s(len=*)' % (vardef)
             else:
-                vardef='%s(%s=%s)'%(vardef,lk,selector['*'])
-        else:
-            if selector['*'] in ['*',':']:
-                vardef='%s*(%s)'%(vardef,selector['*'])
+                vardef = '%s(%s=%s)' % (vardef, lk, selector['*'])
+        else:
+            if selector['*'] in ['*', ':']:
+                vardef = '%s*(%s)' % (vardef, selector['*'])
             else:
-                vardef='%s*%s'%(vardef,selector['*'])
-    else:
-        if selector.has_key('len'):
-            vardef='%s(len=%s'%(vardef,selector['len'])
-            if selector.has_key('kind'):
-                vardef='%s,kind=%s)'%(vardef,selector['kind'])
+                vardef = '%s*%s' % (vardef, selector['*'])
+    else:
+        if 'len' in selector:
+            vardef = '%s(len=%s' % (vardef, selector['len'])
+            if 'kind' in selector:
+                vardef = '%s,kind=%s)' % (vardef, selector['kind'])
             else:
-                vardef='%s)'%(vardef)
-        elif selector.has_key('kind'):
-            vardef='%s(kind=%s)'%(vardef,selector['kind'])
-
-    vardef='%s %s'%(vardef,fa)
-    if vars[a].has_key('dimension'):
-        vardef='%s(%s)'%(vardef,string.join(vars[a]['dimension'],','))
+                vardef = '%s)' % (vardef)
+        elif 'kind' in selector:
+            vardef = '%s(kind=%s)' % (vardef, selector['kind'])
+
+    vardef = '%s %s' % (vardef, fa)
+    if 'dimension' in vars[a]:
+        vardef = '%s(%s)' % (vardef, ','.join(vars[a]['dimension']))
     return vardef
 
-def createfuncwrapper(rout,signature=0):
+
+def createfuncwrapper(rout, signature=0):
     assert isfunction(rout)
+
+    extra_args = []
+    vars = rout['vars']
+    for a in rout['args']:
+        v = rout['vars'][a]
+        for i, d in enumerate(v.get('dimension', [])):
+            if d == ':':
+                dn = 'f2py_%s_d%s' % (a, i)
+                dv = dict(typespec='integer', intent=['hide'])
+                dv['='] = 'shape(%s, %s)' % (a, i)
+                extra_args.append(dn)
+                vars[dn] = dv
+                v['dimension'][i] = dn
+    rout['args'].extend(extra_args)
+    need_interface = bool(extra_args)
+
     ret = ['']
-    def add(line,ret=ret):
-        ret[0] = '%s\n      %s'%(ret[0],line)
+
+    def add(line, ret=ret):
+        ret[0] = '%s\n      %s' % (ret[0], line)
     name = rout['name']
     fortranname = getfortranname(rout)
     f90mode = ismoduleroutine(rout)
-    newname = '%sf2pywrap'%(name)
-    vars = rout['vars']
-    if not vars.has_key(newname):
+    newname = '%sf2pywrap' % (name)
+
+    if newname not in vars:
         vars[newname] = vars[name]
-        args = [newname]+rout['args'][1:]
-    else:
-        args = [newname]+rout['args']
-
-    l = var2fixfortran(vars,name,newname,f90mode)
-    return_char_star = 0
-    if l[:13]=='character*(*)':
-        return_char_star = 1
-        if f90mode: l = 'character(len=10)'+l[13:]
-        else: l = 'character*10'+l[13:]
+        args = [newname] + rout['args'][1:]
+    else:
+        args = [newname] + rout['args']
+
+    l = var2fixfortran(vars, name, newname, f90mode)
+    if l[:13] == 'character*(*)':
+        if f90mode:
+            l = 'character(len=10)' + l[13:]
+        else:
+            l = 'character*10' + l[13:]
         charselect = vars[name]['charselector']
-        if charselect.get('*','')=='(*)':
+        if charselect.get('*', '') == '(*)':
             charselect['*'] = '10'
+    sargs = ', '.join(args)
     if f90mode:
-        sargs = string.join(args,', ')
-        add('subroutine f2pywrap_%s_%s (%s)'%(rout['modulename'],name,sargs))
+        add('subroutine f2pywrap_%s_%s (%s)' %
+            (rout['modulename'], name, sargs))
         if not signature:
-            add('use %s, only : %s'%(rout['modulename'],fortranname))
-    else:
-        add('subroutine f2pywrap%s (%s)'%(name,string.join(args,', ')))
-        add('external %s'%(fortranname))
-        #if not return_char_star:
-        l = l + ', '+fortranname
+            add('use %s, only : %s' % (rout['modulename'], fortranname))
+    else:
+        add('subroutine f2pywrap%s (%s)' % (name, sargs))
+        if not need_interface:
+            add('external %s' % (fortranname))
+            l = l + ', ' + fortranname
+    if need_interface:
+        for line in rout['saved_interface'].split('\n'):
+            if line.lstrip().startswith('use ') and '__user__' not in line:
+                add(line)
+
     args = args[1:]
     dumped_args = []
     for a in args:
         if isexternal(vars[a]):
-            add('external %s'%(a))
-            dumped_args.append(a)
-    for a in args:
-        if a in dumped_args: continue
+            add('external %s' % (a))
+            dumped_args.append(a)
+    for a in args:
+        if a in dumped_args:
+            continue
         if isscalar(vars[a]):
-            add(var2fixfortran(vars,a,f90mode=f90mode))
-            dumped_args.append(a)
-    for a in args:
-        if a in dumped_args: continue
-        add(var2fixfortran(vars,a,f90mode=f90mode))
+            add(var2fixfortran(vars, a, f90mode=f90mode))
+            dumped_args.append(a)
+    for a in args:
+        if a in dumped_args:
+            continue
+        if isintent_in(vars[a]):
+            add(var2fixfortran(vars, a, f90mode=f90mode))
+            dumped_args.append(a)
+    for a in args:
+        if a in dumped_args:
+            continue
+        add(var2fixfortran(vars, a, f90mode=f90mode))
 
     add(l)
+
+    if need_interface:
+        if f90mode:
+            # f90 module already defines needed interface
+            pass
+        else:
+            add('interface')
+            add(rout['saved_interface'].lstrip())
+            add('end interface')
+
+    sargs = ', '.join([a for a in args if a not in extra_args])
 
     if not signature:
         if islogicalfunction(rout):
-            add('%s = .not.(.not.%s(%s))'%(newname,fortranname,string.join(args,', ')))
-        else:
-            add('%s = %s(%s)'%(newname,fortranname,string.join(args,', ')))
+            add('%s = .not.(.not.%s(%s))' % (newname, fortranname, sargs))
+        else:
+            add('%s = %s(%s)' % (newname, fortranname, sargs))
     if f90mode:
-        add('end subroutine f2pywrap_%s_%s'%(rout['modulename'],name))
+        add('end subroutine f2pywrap_%s_%s' % (rout['modulename'], name))
     else:
         add('end')
-    #print '**'*10
-    #print ret[0]
-    #print '**'*10
     return ret[0]
 
+
+def createsubrwrapper(rout, signature=0):
+    assert issubroutine(rout)
+
+    extra_args = []
+    vars = rout['vars']
+    for a in rout['args']:
+        v = rout['vars'][a]
+        for i, d in enumerate(v.get('dimension', [])):
+            if d == ':':
+                dn = 'f2py_%s_d%s' % (a, i)
+                dv = dict(typespec='integer', intent=['hide'])
+                dv['='] = 'shape(%s, %s)' % (a, i)
+                extra_args.append(dn)
+                vars[dn] = dv
+                v['dimension'][i] = dn
+    rout['args'].extend(extra_args)
+    need_interface = bool(extra_args)
+
+    ret = ['']
+
+    def add(line, ret=ret):
+        ret[0] = '%s\n      %s' % (ret[0], line)
+    name = rout['name']
+    fortranname = getfortranname(rout)
+    f90mode = ismoduleroutine(rout)
+
+    args = rout['args']
+
+    sargs = ', '.join(args)
+    if f90mode:
+        add('subroutine f2pywrap_%s_%s (%s)' %
+            (rout['modulename'], name, sargs))
+        if not signature:
+            add('use %s, only : %s' % (rout['modulename'], fortranname))
+    else:
+        add('subroutine f2pywrap%s (%s)' % (name, sargs))
+        if not need_interface:
+            add('external %s' % (fortranname))
+
+    if need_interface:
+        for line in rout['saved_interface'].split('\n'):
+            if line.lstrip().startswith('use ') and '__user__' not in line:
+                add(line)
+
+    dumped_args = []
+    for a in args:
+        if isexternal(vars[a]):
+            add('external %s' % (a))
+            dumped_args.append(a)
+    for a in args:
+        if a in dumped_args:
+            continue
+        if isscalar(vars[a]):
+            add(var2fixfortran(vars, a, f90mode=f90mode))
+            dumped_args.append(a)
+    for a in args:
+        if a in dumped_args:
+            continue
+        add(var2fixfortran(vars, a, f90mode=f90mode))
+
+    if need_interface:
+        if f90mode:
+            # f90 module already defines needed interface
+            pass
+        else:
+            add('interface')
+            for line in rout['saved_interface'].split('\n'):
+                if line.lstrip().startswith('use ') and '__user__' in line:
+                    continue
+                add(line)
+            add('end interface')
+
+    sargs = ', '.join([a for a in args if a not in extra_args])
+
+    if not signature:
+        add('call %s(%s)' % (fortranname, sargs))
+    if f90mode:
+        add('end subroutine f2pywrap_%s_%s' % (rout['modulename'], name))
+    else:
+        add('end')
+    return ret[0]
+
+
 def assubr(rout):
-    if not isfunction_wrap(rout): return rout,''
-    fortranname = getfortranname(rout)
-    name = rout['name']
-    outmess('\t\tCreating wrapper for Fortran function "%s"("%s")...\n'%(name,fortranname))
-    rout = copy.copy(rout)
-    fname = name
-    rname = fname
-    if rout.has_key('result'):
-        rname = rout['result']
-        rout['vars'][fname]=rout['vars'][rname]
-    fvar = rout['vars'][fname]
-    if not isintent_out(fvar):
-        if not fvar.has_key('intent'): fvar['intent']=[]
-        fvar['intent'].append('out')
-        flag=1
-        for i in fvar['intent']:
-            if i.startswith('out='):
-                flag = 0
-                break
-        if flag:
-            fvar['intent'].append('out=%s' % (rname))
-
-    rout['args'] = [fname] + rout['args']
-    return rout,createfuncwrapper(rout)
+    if isfunction_wrap(rout):
+        fortranname = getfortranname(rout)
+        name = rout['name']
+        outmess('\t\tCreating wrapper for Fortran function "%s"("%s")...\n' % (
+            name, fortranname))
+        rout = copy.copy(rout)
+        fname = name
+        rname = fname
+        if 'result' in rout:
+            rname = rout['result']
+            rout['vars'][fname] = rout['vars'][rname]
+        fvar = rout['vars'][fname]
+        if not isintent_out(fvar):
+            if 'intent' not in fvar:
+                fvar['intent'] = []
+            fvar['intent'].append('out')
+            flag = 1
+            for i in fvar['intent']:
+                if i.startswith('out='):
+                    flag = 0
+                    break
+            if flag:
+                fvar['intent'].append('out=%s' % (rname))
+        rout['args'][:] = [fname] + rout['args']
+        return rout, createfuncwrapper(rout)
+    if issubroutine_wrap(rout):
+        fortranname = getfortranname(rout)
+        name = rout['name']
+        outmess('\t\tCreating wrapper for Fortran subroutine "%s"("%s")...\n' % (
+            name, fortranname))
+        rout = copy.copy(rout)
+        return rout, createsubrwrapper(rout)
+    return rout, ''
('numpy/f2py', '__version__.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,9 +1 @@
-major = 2
-
-try:
-    from __svn_version__ import version
-    version_info = (major,version)
-    version = '%s_%s' % version_info
-except Exception,msg:
-    print msg
-    version = '%s_?' % (major)
+from numpy.version import version
('numpy/f2py', 'diagnose.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,51 +1,46 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
+import os
+import sys
+import tempfile
 
-import os,sys,tempfile
 
 def run_command(cmd):
-    print 'Running %r:' % (cmd)
-    s = os.system(cmd)
-    print '------'
+    print('Running %r:' % (cmd))
+    os.system(cmd)
+    print('------')
+
+
 def run():
     _path = os.getcwd()
     os.chdir(tempfile.gettempdir())
-    print '------'
-    print 'os.name=%r' % (os.name)
-    print '------'
-    print 'sys.platform=%r' % (sys.platform)
-    print '------'
-    print 'sys.version:'
-    print sys.version
-    print '------'
-    print 'sys.prefix:'
-    print sys.prefix
-    print '------'
-    print 'sys.path=%r' % (':'.join(sys.path))
-    print '------'
-    try:
-        import Numeric
-        has_Numeric = 1
-    except ImportError:
-        print 'Failed to import Numeric:',sys.exc_value
-        has_Numeric = 0
-    try:
-        import numarray
-        has_numarray = 1
-    except ImportError:
-        print 'Failed to import numarray:',sys.exc_value
-        has_numarray = 0
+    print('------')
+    print('os.name=%r' % (os.name))
+    print('------')
+    print('sys.platform=%r' % (sys.platform))
+    print('------')
+    print('sys.version:')
+    print(sys.version)
+    print('------')
+    print('sys.prefix:')
+    print(sys.prefix)
+    print('------')
+    print('sys.path=%r' % (':'.join(sys.path)))
+    print('------')
+
     try:
         import numpy
         has_newnumpy = 1
     except ImportError:
-        print 'Failed to import new numpy:', sys.exc_value
+        print('Failed to import new numpy:', sys.exc_info()[1])
         has_newnumpy = 0
+
     try:
-        import f2py2e
+        from numpy.f2py import f2py2e
         has_f2py2e = 1
     except ImportError:
-        print 'Failed to import f2py2e:',sys.exc_value
+        print('Failed to import f2py2e:', sys.exc_info()[1])
         has_f2py2e = 0
+
     try:
         import numpy.distutils
         has_numpy_distutils = 2
@@ -54,113 +49,106 @@
             import numpy_distutils
             has_numpy_distutils = 1
         except ImportError:
-            print 'Failed to import numpy_distutils:',sys.exc_value
+            print('Failed to import numpy_distutils:', sys.exc_info()[1])
             has_numpy_distutils = 0
-    if has_Numeric:
-        try:
-            print 'Found Numeric version %r in %s' % \
-                  (Numeric.__version__,Numeric.__file__)
-        except Exception,msg:
-            print 'error:',msg
-            print '------'
-    if has_numarray:
-        try:
-            print 'Found numarray version %r in %s' % \
-                  (numarray.__version__,numarray.__file__)
-        except Exception,msg:
-            print 'error:',msg
-            print '------'
+
     if has_newnumpy:
         try:
-            print 'Found new numpy version %r in %s' % \
-                  (numpy.__version__, numpy.__file__)
-        except Exception,msg:
-            print 'error:', msg
-            print '------'
+            print('Found new numpy version %r in %s' %
+                  (numpy.__version__, numpy.__file__))
+        except Exception as msg:
+            print('error:', msg)
+            print('------')
+
     if has_f2py2e:
         try:
-            print 'Found f2py2e version %r in %s' % \
-                  (f2py2e.__version__.version,f2py2e.__file__)
-        except Exception,msg:
-            print 'error:',msg
-            print '------'
+            print('Found f2py2e version %r in %s' %
+                  (f2py2e.__version__.version, f2py2e.__file__))
+        except Exception as msg:
+            print('error:', msg)
+            print('------')
+
     if has_numpy_distutils:
         try:
-            if has_numpy_distutils==2:
-                print 'Found numpy.distutils version %r in %r' % (\
-            numpy.distutils.__version__,
-            numpy.distutils.__file__)
+            if has_numpy_distutils == 2:
+                print('Found numpy.distutils version %r in %r' % (
+                    numpy.distutils.__version__,
+                    numpy.distutils.__file__))
             else:
-                print 'Found numpy_distutils version %r in %r' % (\
-            numpy_distutils.numpy_distutils_version.numpy_distutils_version,
-            numpy_distutils.__file__)
-            print '------'
-        except Exception,msg:
-            print 'error:',msg
-            print '------'
+                print('Found numpy_distutils version %r in %r' % (
+                    numpy_distutils.numpy_distutils_version.numpy_distutils_version,
+                    numpy_distutils.__file__))
+            print('------')
+        except Exception as msg:
+            print('error:', msg)
+            print('------')
         try:
-            if has_numpy_distutils==1:
-                print 'Importing numpy_distutils.command.build_flib ...',
+            if has_numpy_distutils == 1:
+                print(
+                    'Importing numpy_distutils.command.build_flib ...', end=' ')
                 import numpy_distutils.command.build_flib as build_flib
-                print 'ok'
-                print '------'
+                print('ok')
+                print('------')
                 try:
-                    print 'Checking availability of supported Fortran compilers:'
+                    print(
+                        'Checking availability of supported Fortran compilers:')
                     for compiler_class in build_flib.all_compilers:
                         compiler_class(verbose=1).is_available()
-                        print '------'
-                except Exception,msg:
-                    print 'error:',msg
-                    print '------'
-        except Exception,msg:
-            print 'error:',msg,'(ignore it, build_flib is obsolute for numpy.distutils 0.2.2 and up)'
-            print '------'
+                        print('------')
+                except Exception as msg:
+                    print('error:', msg)
+                    print('------')
+        except Exception as msg:
+            print(
+                'error:', msg, '(ignore it, build_flib is obsolute for numpy.distutils 0.2.2 and up)')
+            print('------')
         try:
-            if has_numpy_distutils==2:
-                print 'Importing numpy.distutils.fcompiler ...',
+            if has_numpy_distutils == 2:
+                print('Importing numpy.distutils.fcompiler ...', end=' ')
                 import numpy.distutils.fcompiler as fcompiler
             else:
-                print 'Importing numpy_distutils.fcompiler ...',
+                print('Importing numpy_distutils.fcompiler ...', end=' ')
                 import numpy_distutils.fcompiler as fcompiler
-            print 'ok'
-            print '------'
+            print('ok')
+            print('------')
             try:
-                print 'Checking availability of supported Fortran compilers:'
+                print('Checking availability of supported Fortran compilers:')
                 fcompiler.show_fcompilers()
-                print '------'
-            except Exception,msg:
-                print 'error:',msg
-                print '------'
-        except Exception,msg:
-            print 'error:',msg
-            print '------'
+                print('------')
+            except Exception as msg:
+                print('error:', msg)
+                print('------')
+        except Exception as msg:
+            print('error:', msg)
+            print('------')
         try:
-            if has_numpy_distutils==2:
-                print 'Importing numpy.distutils.cpuinfo ...',
+            if has_numpy_distutils == 2:
+                print('Importing numpy.distutils.cpuinfo ...', end=' ')
                 from numpy.distutils.cpuinfo import cpuinfo
-                print 'ok'
-                print '------'
+                print('ok')
+                print('------')
             else:
                 try:
-                    print 'Importing numpy_distutils.command.cpuinfo ...',
+                    print(
+                        'Importing numpy_distutils.command.cpuinfo ...', end=' ')
                     from numpy_distutils.command.cpuinfo import cpuinfo
-                    print 'ok'
-                    print '------'
-                except Exception,msg:
-                    print 'error:',msg,'(ignore it)'
-                    print 'Importing numpy_distutils.cpuinfo ...',
+                    print('ok')
+                    print('------')
+                except Exception as msg:
+                    print('error:', msg, '(ignore it)')
+                    print('Importing numpy_distutils.cpuinfo ...', end=' ')
                     from numpy_distutils.cpuinfo import cpuinfo
-                    print 'ok'
-                    print '------'
+                    print('ok')
+                    print('------')
             cpu = cpuinfo()
-            print 'CPU information:',
+            print('CPU information:', end=' ')
             for name in dir(cpuinfo):
-                if name[0]=='_' and name[1]!='_' and getattr(cpu,name[1:])():
-                    print name[1:],
-            print '------'
-        except Exception,msg:
-            print 'error:',msg
-            print '------'
+                if name[0] == '_' and name[1] != '_' and getattr(cpu, name[1:])():
+                    print(name[1:], end=' ')
+            print('------')
+        except Exception as msg:
+            print('error:', msg)
+            print('------')
     os.chdir(_path)
 if __name__ == "__main__":
     run()
('numpy/f2py', 'setup.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,9 +1,9 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 """
 setup.py for installing F2PY
 
 Usage:
-   python setup.py install
+   pip install .
 
 Copyright 2001-2005 Pearu Peterson all rights reserved,
 Pearu Peterson <pearu@cens.ioc.ee>
@@ -14,94 +14,58 @@
 $Revision: 1.32 $
 $Date: 2005/01/30 17:22:14 $
 Pearu Peterson
+
 """
-
-__version__ = "$Id: setup.py,v 1.32 2005/01/30 17:22:14 pearu Exp $"
-
-import os
-import sys
-from distutils.dep_util import newer
 from numpy.distutils.core import setup
 from numpy.distutils.misc_util import Configuration
 
+
 from __version__ import version
 
-def configuration(parent_package='',top_path=None):
+
+def configuration(parent_package='', top_path=None):
     config = Configuration('f2py', parent_package, top_path)
+    config.add_subpackage('tests')
+    config.add_data_dir('tests/src')
+    config.add_data_files(
+        'src/fortranobject.c',
+        'src/fortranobject.h')
+    config.add_data_files('*.pyi')
+    return config
 
-    config.add_data_dir('docs')
-
-    config.add_data_files('src/fortranobject.c',
-                          'src/fortranobject.h',
-                          'f2py.1'
-                          )
-
-    config.make_svn_version_py()
-
-    def generate_f2py_py(build_dir):
-        f2py_exe = 'f2py'+os.path.basename(sys.executable)[6:]
-        if f2py_exe[-4:]=='.exe':
-            f2py_exe = f2py_exe[:-4] + '.py'
-        if 'bdist_wininst' in sys.argv and f2py_exe[-3:] != '.py':
-            f2py_exe = f2py_exe + '.py'
-        target = os.path.join(build_dir,f2py_exe)
-        if newer(__file__,target):
-            print 'Creating',target
-            f = open(target,'w')
-            f.write('''\
-#!/usr/bin/env %s
-# See http://cens.ioc.ee/projects/f2py2e/
-import os
-os.environ["NO_SCIPY_IMPORT"]="f2py"
-import numpy.f2py as f2py
-f2py.main()
-'''%(os.path.basename(sys.executable)))
-            f.close()
-        return target
-
-    config.add_scripts(generate_f2py_py)
-
-    print 'F2PY Version',config.get_version()
-
-    return config
 
 if __name__ == "__main__":
 
     config = configuration(top_path='')
-    version = config.get_version()
-    print 'F2PY Version',version
     config = config.todict()
 
-    if sys.version[:3]>='2.3':
-        config['download_url'] = "http://cens.ioc.ee/projects/f2py2e/2.x"\
-                                 "/F2PY-2-latest.tar.gz"
-        config['classifiers'] = [
-            'Development Status :: 5 - Production/Stable',
-            'Intended Audience :: Developers',
-            'Intended Audience :: Science/Research',
-            'License :: OSI Approved :: NumPy License',
-            'Natural Language :: English',
-            'Operating System :: OS Independent',
-            'Programming Language :: C',
-            'Programming Language :: Fortran',
-            'Programming Language :: Python',
-            'Topic :: Scientific/Engineering',
-            'Topic :: Software Development :: Code Generators',
-            ]
+    config['classifiers'] = [
+        'Development Status :: 5 - Production/Stable',
+        'Intended Audience :: Developers',
+        'Intended Audience :: Science/Research',
+        'License :: OSI Approved :: NumPy License',
+        'Natural Language :: English',
+        'Operating System :: OS Independent',
+        'Programming Language :: C',
+        'Programming Language :: Fortran',
+        'Programming Language :: Python',
+        'Topic :: Scientific/Engineering',
+        'Topic :: Software Development :: Code Generators',
+    ]
     setup(version=version,
-          description       = "F2PY - Fortran to Python Interface Generaton",
-          author            = "Pearu Peterson",
-          author_email      = "pearu@cens.ioc.ee",
-          maintainer        = "Pearu Peterson",
-          maintainer_email  = "pearu@cens.ioc.ee",
-          license           = "BSD",
-          platforms         = "Unix, Windows (mingw|cygwin), Mac OSX",
-          long_description  = """\
+          description="F2PY - Fortran to Python Interface Generator",
+          author="Pearu Peterson",
+          author_email="pearu@cens.ioc.ee",
+          maintainer="Pearu Peterson",
+          maintainer_email="pearu@cens.ioc.ee",
+          license="BSD",
+          platforms="Unix, Windows (mingw|cygwin), Mac OSX",
+          long_description="""\
 The Fortran to Python Interface Generator, or F2PY for short, is a
 command line tool (f2py) for generating Python C/API modules for
 wrapping Fortran 77/90/95 subroutines, accessing common blocks from
 Python, and calling Python functions from Fortran (call-backs).
 Interfacing subroutines/data from Fortran 90/95 modules is supported.""",
-          url               = "http://cens.ioc.ee/projects/f2py2e/",
-          keywords          = ['Fortran','f2py'],
+          url="https://numpy.org/doc/stable/f2py/",
+          keywords=['Fortran', 'f2py'],
           **config)
('numpy/f2py', 'capi_maps.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,4 +1,4 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 """
 
 Copyright 1999,2000 Pearu Peterson all rights reserved,
@@ -9,505 +9,602 @@
 NO WARRANTY IS EXPRESSED OR IMPLIED.  USE AT YOUR OWN RISK.
 $Date: 2005/05/06 10:57:33 $
 Pearu Peterson
+
 """
-
-__version__ = "$Revision: 1.60 $"[10:-1]
-
-import __version__
+from . import __version__
 f2py_version = __version__.version
 
-import string,copy,re,os
-from auxfuncs import *
-from crackfortran import markoutercomma
-import cb_rules
+import copy
+import re
+import os
+from .crackfortran import markoutercomma
+from . import cb_rules
+
+# The environment provided by auxfuncs.py is needed for some calls to eval.
+# As the needed functions cannot be determined by static inspection of the
+# code, it is safest to use import * pending a major refactoring of f2py.
+from .auxfuncs import *
+
+__all__ = [
+    'getctype', 'getstrlength', 'getarrdims', 'getpydocsign',
+    'getarrdocsign', 'getinit', 'sign2map', 'routsign2map', 'modsign2map',
+    'cb_sign2map', 'cb_routsign2map', 'common_sign2map'
+]
+
 
 # Numarray and Numeric users should set this False
 using_newcore = True
 
-depargs=[]
-lcb_map={}
-lcb2_map={}
+depargs = []
+lcb_map = {}
+lcb2_map = {}
 # forced casting: mainly caused by the fact that Python or Numeric
 #                 C/APIs do not support the corresponding C types.
-c2py_map={'double':'float',
-          'float':'float',                          # forced casting
-          'long_double':'float',                    # forced casting
-          'char':'int',                             # forced casting
-          'signed_char':'int',                      # forced casting
-          'unsigned_char':'int',                    # forced casting
-          'short':'int',                            # forced casting
-          'unsigned_short':'int',                   # forced casting
-          'int':'int',                              # (forced casting)
-          'long':'int',
-          'long_long':'long',
-          'unsigned':'int',                         # forced casting
-          'complex_float':'complex',                # forced casting
-          'complex_double':'complex',
-          'complex_long_double':'complex',          # forced casting
-          'string':'string',
-          }
-c2capi_map={'double':'PyArray_DOUBLE',
-            'float':'PyArray_FLOAT',
-            'long_double':'PyArray_DOUBLE',           # forced casting
-            'char':'PyArray_CHAR',
-            'unsigned_char':'PyArray_UBYTE',
-            'signed_char':'PyArray_SBYTE',
-            'short':'PyArray_SHORT',
-            'unsigned_short':'PyArray_USHORT',
-            'int':'PyArray_INT',
-            'unsigned':'PyArray_UINT',
-            'long':'PyArray_LONG',
-            'long_long':'PyArray_LONG',                # forced casting
-            'complex_float':'PyArray_CFLOAT',
-            'complex_double':'PyArray_CDOUBLE',
-            'complex_long_double':'PyArray_CDOUBLE',   # forced casting
-            'string':'PyArray_CHAR'}
-
-#These new maps aren't used anyhere yet, but should be by default
+c2py_map = {'double': 'float',
+            'float': 'float',                          # forced casting
+            'long_double': 'float',                    # forced casting
+            'char': 'int',                             # forced casting
+            'signed_char': 'int',                      # forced casting
+            'unsigned_char': 'int',                    # forced casting
+            'short': 'int',                            # forced casting
+            'unsigned_short': 'int',                   # forced casting
+            'int': 'int',                              # forced casting
+            'long': 'int',
+            'long_long': 'long',
+            'unsigned': 'int',                         # forced casting
+            'complex_float': 'complex',                # forced casting
+            'complex_double': 'complex',
+            'complex_long_double': 'complex',          # forced casting
+            'string': 'string',
+            }
+c2capi_map = {'double': 'NPY_DOUBLE',
+              'float': 'NPY_FLOAT',
+              'long_double': 'NPY_DOUBLE',           # forced casting
+              'char': 'NPY_STRING',
+              'unsigned_char': 'NPY_UBYTE',
+              'signed_char': 'NPY_BYTE',
+              'short': 'NPY_SHORT',
+              'unsigned_short': 'NPY_USHORT',
+              'int': 'NPY_INT',
+              'unsigned': 'NPY_UINT',
+              'long': 'NPY_LONG',
+              'long_long': 'NPY_LONG',                # forced casting
+              'complex_float': 'NPY_CFLOAT',
+              'complex_double': 'NPY_CDOUBLE',
+              'complex_long_double': 'NPY_CDOUBLE',   # forced casting
+              'string': 'NPY_STRING'}
+
+# These new maps aren't used anywhere yet, but should be by default
 #  unless building numeric or numarray extensions.
 if using_newcore:
-    c2capi_map={'double':'PyArray_DOUBLE',
-            'float':'PyArray_FLOAT',
-            'long_double':'PyArray_LONGDOUBLE',
-            'char':'PyArray_BYTE',
-            'unsigned_char':'PyArray_UBYTE',
-            'signed_char':'PyArray_BYTE',
-            'short':'PyArray_SHORT',
-            'unsigned_short':'PyArray_USHORT',
-            'int':'PyArray_INT',
-            'unsigned':'PyArray_UINT',
-            'long':'PyArray_LONG',
-            'unsigned_long':'PyArray_ULONG',
-            'long_long':'PyArray_LONGLONG',
-            'unsigned_long_long':'Pyarray_ULONGLONG',
-            'complex_float':'PyArray_CFLOAT',
-            'complex_double':'PyArray_CDOUBLE',
-            'complex_long_double':'PyArray_CDOUBLE',
-            'string':'PyArray_STRING'}
-c2pycode_map={'double':'d',
-              'float':'f',
-              'long_double':'d',                       # forced casting
-              'char':'1',
-              'signed_char':'1',
-              'unsigned_char':'b',
-              'short':'s',
-              'unsigned_short':'w',
-              'int':'i',
-              'unsigned':'u',
-              'long':'l',
-              'long_long':'L',
-              'complex_float':'F',
-              'complex_double':'D',
-              'complex_long_double':'D',               # forced casting
-              'string':'c'
+    c2capi_map = {'double': 'NPY_DOUBLE',
+                  'float': 'NPY_FLOAT',
+                  'long_double': 'NPY_LONGDOUBLE',
+                  'char': 'NPY_BYTE',
+                  'unsigned_char': 'NPY_UBYTE',
+                  'signed_char': 'NPY_BYTE',
+                  'short': 'NPY_SHORT',
+                  'unsigned_short': 'NPY_USHORT',
+                  'int': 'NPY_INT',
+                  'unsigned': 'NPY_UINT',
+                  'long': 'NPY_LONG',
+                  'unsigned_long': 'NPY_ULONG',
+                  'long_long': 'NPY_LONGLONG',
+                  'unsigned_long_long': 'NPY_ULONGLONG',
+                  'complex_float': 'NPY_CFLOAT',
+                  'complex_double': 'NPY_CDOUBLE',
+                  'complex_long_double': 'NPY_CDOUBLE',
+                  'string':'NPY_STRING'
+                  }
+
+c2pycode_map = {'double': 'd',
+                'float': 'f',
+                'long_double': 'd',                       # forced casting
+                'char': '1',
+                'signed_char': '1',
+                'unsigned_char': 'b',
+                'short': 's',
+                'unsigned_short': 'w',
+                'int': 'i',
+                'unsigned': 'u',
+                'long': 'l',
+                'long_long': 'L',
+                'complex_float': 'F',
+                'complex_double': 'D',
+                'complex_long_double': 'D',               # forced casting
+                'string': 'c'
+                }
+
+if using_newcore:
+    c2pycode_map = {'double': 'd',
+                    'float': 'f',
+                    'long_double': 'g',
+                    'char': 'b',
+                    'unsigned_char': 'B',
+                    'signed_char': 'b',
+                    'short': 'h',
+                    'unsigned_short': 'H',
+                    'int': 'i',
+                    'unsigned': 'I',
+                    'long': 'l',
+                    'unsigned_long': 'L',
+                    'long_long': 'q',
+                    'unsigned_long_long': 'Q',
+                    'complex_float': 'F',
+                    'complex_double': 'D',
+                    'complex_long_double': 'G',
+                    'string': 'S'}
+
+c2buildvalue_map = {'double': 'd',
+                    'float': 'f',
+                    'char': 'b',
+                    'signed_char': 'b',
+                    'short': 'h',
+                    'int': 'i',
+                    'long': 'l',
+                    'long_long': 'L',
+                    'complex_float': 'N',
+                    'complex_double': 'N',
+                    'complex_long_double': 'N',
+                    'string': 'y'}
+
+f2cmap_all = {'real': {'': 'float', '4': 'float', '8': 'double',
+                       '12': 'long_double', '16': 'long_double'},
+              'integer': {'': 'int', '1': 'signed_char', '2': 'short',
+                          '4': 'int', '8': 'long_long',
+                          '-1': 'unsigned_char', '-2': 'unsigned_short',
+                          '-4': 'unsigned', '-8': 'unsigned_long_long'},
+              'complex': {'': 'complex_float', '8': 'complex_float',
+                          '16': 'complex_double', '24': 'complex_long_double',
+                          '32': 'complex_long_double'},
+              'complexkind': {'': 'complex_float', '4': 'complex_float',
+                              '8': 'complex_double', '12': 'complex_long_double',
+                              '16': 'complex_long_double'},
+              'logical': {'': 'int', '1': 'char', '2': 'short', '4': 'int',
+                          '8': 'long_long'},
+              'double complex': {'': 'complex_double'},
+              'double precision': {'': 'double'},
+              'byte': {'': 'char'},
+              'character': {'': 'string'}
               }
-if using_newcore:
-    c2pycode_map={'double':'d',
-                 'float':'f',
-                 'long_double':'g',
-                 'char':'b',
-                 'unsigned_char':'B',
-                 'signed_char':'b',
-                 'short':'h',
-                 'unsigned_short':'H',
-                 'int':'i',
-                 'unsigned':'I',
-                 'long':'l',
-                 'unsigned_long':'L',
-                 'long_long':'q',
-                 'unsigned_long_long':'Q',
-                 'complex_float':'F',
-                 'complex_double':'D',
-                 'complex_long_double':'G',
-                 'string':'S'}
-c2buildvalue_map={'double':'d',
-                  'float':'f',
-                  'char':'b',
-                  'signed_char':'b',
-                  'short':'h',
-                  'int':'i',
-                  'long':'l',
-                  'long_long':'L',
-                  'complex_float':'N',
-                  'complex_double':'N',
-                  'complex_long_double':'N',
-                  'string':'z'}
-if using_newcore:
-    #c2buildvalue_map=???
-    pass
-
-f2cmap_all={'real':{'':'float','4':'float','8':'double','12':'long_double','16':'long_double'},
-            'integer':{'':'int','1':'signed_char','2':'short','4':'int','8':'long_long',
-                       '-1':'unsigned_char','-2':'unsigned_short','-4':'unsigned',
-                       '-8':'unsigned_long_long'},
-            'complex':{'':'complex_float','8':'complex_float',
-                       '16':'complex_double','24':'complex_long_double',
-                       '32':'complex_long_double'},
-            'complexkind':{'':'complex_float','4':'complex_float',
-                           '8':'complex_double','12':'complex_long_double',
-                           '16':'complex_long_double'},
-            'logical':{'':'int','1':'char','2':'short','4':'int','8':'long_long'},
-            'double complex':{'':'complex_double'},
-            'double precision':{'':'double'},
-            'byte':{'':'char'},
-            'character':{'':'string'}
-            }
-
-if os.path.isfile('.f2py_f2cmap'):
+
+f2cmap_default = copy.deepcopy(f2cmap_all)
+
+f2cmap_mapped = []
+
+def load_f2cmap_file(f2cmap_file):
+    global f2cmap_all
+
+    f2cmap_all = copy.deepcopy(f2cmap_default)
+
+    if f2cmap_file is None:
+        # Default value
+        f2cmap_file = '.f2py_f2cmap'
+        if not os.path.isfile(f2cmap_file):
+            return
+
     # User defined additions to f2cmap_all.
-    # .f2py_f2cmap must contain a dictionary of dictionaries, only.
-    # For example, {'real':{'low':'float'}} means that Fortran 'real(low)' is
-    # interpreted as C 'float'.
-    # This feature is useful for F90/95 users if they use PARAMETERSs
-    # in type specifications.
+    # f2cmap_file must contain a dictionary of dictionaries, only. For
+    # example, {'real':{'low':'float'}} means that Fortran 'real(low)' is
+    # interpreted as C 'float'. This feature is useful for F90/95 users if
+    # they use PARAMETERS in type specifications.
     try:
-        outmess('Reading .f2py_f2cmap ...\n')
-        f = open('.f2py_f2cmap','r')
-        d = eval(f.read(),{},{})
-        f.close()
-        for k,d1 in d.items():
+        outmess('Reading f2cmap from {!r} ...\n'.format(f2cmap_file))
+        with open(f2cmap_file, 'r') as f:
+            d = eval(f.read().lower(), {}, {})
+        for k, d1 in d.items():
             for k1 in d1.keys():
-                d1[string.lower(k1)] = d1[k1]
-            d[string.lower(k)] = d[k]
+                d1[k1.lower()] = d1[k1]
+            d[k.lower()] = d[k]
         for k in d.keys():
-            if not f2cmap_all.has_key(k): f2cmap_all[k]={}
+            if k not in f2cmap_all:
+                f2cmap_all[k] = {}
             for k1 in d[k].keys():
-                if c2py_map.has_key(d[k][k1]):
-                    if f2cmap_all[k].has_key(k1):
-                        outmess("\tWarning: redefinition of {'%s':{'%s':'%s'->'%s'}}\n"%(k,k1,f2cmap_all[k][k1],d[k][k1]))
+                if d[k][k1] in c2py_map:
+                    if k1 in f2cmap_all[k]:
+                        outmess(
+                            "\tWarning: redefinition of {'%s':{'%s':'%s'->'%s'}}\n" % (k, k1, f2cmap_all[k][k1], d[k][k1]))
                     f2cmap_all[k][k1] = d[k][k1]
-                    outmess('\tMapping "%s(kind=%s)" to "%s"\n' % (k,k1,d[k][k1]))
+                    outmess('\tMapping "%s(kind=%s)" to "%s"\n' %
+                            (k, k1, d[k][k1]))
+                    f2cmap_mapped.append(d[k][k1])
                 else:
-                    errmess("\tIgnoring map {'%s':{'%s':'%s'}}: '%s' must be in %s\n"%(k,k1,d[k][k1],d[k][k1],c2py_map.keys()))
-        outmess('Succesfully applied user defined changes from .f2py_f2cmap\n')
-    except:
-        errmess('Failed to apply user defined changes from .f2py_f2cmap. Skipping.\n')
-cformat_map={'double':'%g',
-             'float':'%g',
-             'long_double':'%Lg',
-             'char':'%d',
-             'signed_char':'%d',
-             'unsigned_char':'%hhu',
-             'short':'%hd',
-             'unsigned_short':'%hu',
-             'int':'%d',
-             'unsigned':'%u',
-             'long':'%ld',
-             'unsigned_long':'%lu',
-             'long_long':'%ld',
-             'complex_float':'(%g,%g)',
-             'complex_double':'(%g,%g)',
-             'complex_long_double':'(%Lg,%Lg)',
-             'string':'%s',
-             }
-
-############### Auxiliary functions
+                    errmess("\tIgnoring map {'%s':{'%s':'%s'}}: '%s' must be in %s\n" % (
+                        k, k1, d[k][k1], d[k][k1], list(c2py_map.keys())))
+        outmess('Successfully applied user defined f2cmap changes\n')
+    except Exception as msg:
+        errmess(
+            'Failed to apply user defined f2cmap changes: %s. Skipping.\n' % (msg))
+
+cformat_map = {'double': '%g',
+               'float': '%g',
+               'long_double': '%Lg',
+               'char': '%d',
+               'signed_char': '%d',
+               'unsigned_char': '%hhu',
+               'short': '%hd',
+               'unsigned_short': '%hu',
+               'int': '%d',
+               'unsigned': '%u',
+               'long': '%ld',
+               'unsigned_long': '%lu',
+               'long_long': '%ld',
+               'complex_float': '(%g,%g)',
+               'complex_double': '(%g,%g)',
+               'complex_long_double': '(%Lg,%Lg)',
+               'string': '%s',
+               }
+
+# Auxiliary functions
+
+
 def getctype(var):
     """
     Determines C type
     """
-    ctype='void'
+    ctype = 'void'
     if isfunction(var):
-        if var.has_key('result'): a=var['result']
-        else: a=var['name']
-        if var['vars'].has_key(a): return getctype(var['vars'][a])
-        else: errmess('getctype: function %s has no return value?!\n'%a)
+        if 'result' in var:
+            a = var['result']
+        else:
+            a = var['name']
+        if a in var['vars']:
+            return getctype(var['vars'][a])
+        else:
+            errmess('getctype: function %s has no return value?!\n' % a)
     elif issubroutine(var):
         return ctype
-    elif var.has_key('typespec') and f2cmap_all.has_key(string.lower(var['typespec'])):
-        typespec = string.lower(var['typespec'])
-        f2cmap=f2cmap_all[typespec]
-        ctype=f2cmap[''] # default type
-        if var.has_key('kindselector'):
-            if var['kindselector'].has_key('*'):
+    elif 'typespec' in var and var['typespec'].lower() in f2cmap_all:
+        typespec = var['typespec'].lower()
+        f2cmap = f2cmap_all[typespec]
+        ctype = f2cmap['']  # default type
+        if 'kindselector' in var:
+            if '*' in var['kindselector']:
                 try:
-                    ctype=f2cmap[var['kindselector']['*']]
+                    ctype = f2cmap[var['kindselector']['*']]
                 except KeyError:
-                    errmess('getctype: "%s %s %s" not supported.\n'%(var['typespec'],'*',var['kindselector']['*']))
-            elif var['kindselector'].has_key('kind'):
-                if f2cmap_all.has_key(typespec+'kind'):
-                    f2cmap=f2cmap_all[typespec+'kind']
+                    errmess('getctype: "%s %s %s" not supported.\n' %
+                            (var['typespec'], '*', var['kindselector']['*']))
+            elif 'kind' in var['kindselector']:
+                if typespec + 'kind' in f2cmap_all:
+                    f2cmap = f2cmap_all[typespec + 'kind']
                 try:
-                    ctype=f2cmap[var['kindselector']['kind']]
+                    ctype = f2cmap[var['kindselector']['kind']]
                 except KeyError:
-                    if f2cmap_all.has_key(typespec):
-                        f2cmap=f2cmap_all[typespec]
+                    if typespec in f2cmap_all:
+                        f2cmap = f2cmap_all[typespec]
                     try:
-                        ctype=f2cmap[str(var['kindselector']['kind'])]
+                        ctype = f2cmap[str(var['kindselector']['kind'])]
                     except KeyError:
-                        errmess('getctype: "%s(kind=%s)" not supported (use .f2py_f2cmap).\n'\
-                                %(typespec,var['kindselector']['kind']))
-
+                        errmess('getctype: "%s(kind=%s)" is mapped to C "%s" (to override define dict(%s = dict(%s="<C typespec>")) in %s/.f2py_f2cmap file).\n'
+                                % (typespec, var['kindselector']['kind'], ctype,
+                                   typespec, var['kindselector']['kind'], os.getcwd()))
     else:
         if not isexternal(var):
-            errmess('getctype: No C-type found in "%s", assuming void.\n'%var)
+            errmess('getctype: No C-type found in "%s", assuming void.\n' % var)
     return ctype
+
+
 def getstrlength(var):
     if isstringfunction(var):
-        if var.has_key('result'): a=var['result']
-        else: a=var['name']
-        if var['vars'].has_key(a): return getstrlength(var['vars'][a])
-        else: errmess('getstrlength: function %s has no return value?!\n'%a)
+        if 'result' in var:
+            a = var['result']
+        else:
+            a = var['name']
+        if a in var['vars']:
+            return getstrlength(var['vars'][a])
+        else:
+            errmess('getstrlength: function %s has no return value?!\n' % a)
     if not isstring(var):
-        errmess('getstrlength: expected a signature of a string but got: %s\n'%(`var`))
-    len='1'
-    if var.has_key('charselector'):
-        a=var['charselector']
-        if a.has_key('*'): len=a['*']
-        elif a.has_key('len'): len=a['len']
-    if re.match(r'\(\s*([*]|[:])\s*\)',len) or re.match(r'([*]|[:])',len):
-    #if len in ['(*)','*','(:)',':']:
+        errmess(
+            'getstrlength: expected a signature of a string but got: %s\n' % (repr(var)))
+    len = '1'
+    if 'charselector' in var:
+        a = var['charselector']
+        if '*' in a:
+            len = a['*']
+        elif 'len' in a:
+            len = a['len']
+    if re.match(r'\(\s*(\*|:)\s*\)', len) or re.match(r'(\*|:)', len):
         if isintent_hide(var):
-            errmess('getstrlength:intent(hide): expected a string with defined length but got: %s\n'%(`var`))
-        len='-1'
+            errmess('getstrlength:intent(hide): expected a string with defined length but got: %s\n' % (
+                repr(var)))
+        len = '-1'
     return len
-def getarrdims(a,var,verbose=0):
-    global depargs
-    ret={}
+
+
+def getarrdims(a, var, verbose=0):
+    ret = {}
     if isstring(var) and not isarray(var):
-        ret['dims']=getstrlength(var)
-        ret['size']=ret['dims']
-        ret['rank']='1'
+        ret['dims'] = getstrlength(var)
+        ret['size'] = ret['dims']
+        ret['rank'] = '1'
     elif isscalar(var):
-        ret['size']='1'
-        ret['rank']='0'
-        ret['dims']=''
+        ret['size'] = '1'
+        ret['rank'] = '0'
+        ret['dims'] = ''
     elif isarray(var):
-#         if not isintent_c(var):
-#             var['dimension'].reverse()
-        dim=copy.copy(var['dimension'])
-        ret['size']=string.join(dim,'*')
-        try: ret['size']=`eval(ret['size'])`
-        except: pass
-        ret['dims']=string.join(dim,',')
-        ret['rank']=`len(dim)`
-        ret['rank*[-1]']=`len(dim)*[-1]`[1:-1]
-        for i in range(len(dim)): # solve dim for dependecies
-            v=[]
-            if dim[i] in depargs: v=[dim[i]]
+        dim = copy.copy(var['dimension'])
+        ret['size'] = '*'.join(dim)
+        try:
+            ret['size'] = repr(eval(ret['size']))
+        except Exception:
+            pass
+        ret['dims'] = ','.join(dim)
+        ret['rank'] = repr(len(dim))
+        ret['rank*[-1]'] = repr(len(dim) * [-1])[1:-1]
+        for i in range(len(dim)):  # solve dim for dependencies
+            v = []
+            if dim[i] in depargs:
+                v = [dim[i]]
             else:
                 for va in depargs:
-                    if re.match(r'.*?\b%s\b.*'%va,dim[i]):
+                    if re.match(r'.*?\b%s\b.*' % va, dim[i]):
                         v.append(va)
             for va in v:
-                if depargs.index(va)>depargs.index(a):
-                    dim[i]='*'
+                if depargs.index(va) > depargs.index(a):
+                    dim[i] = '*'
                     break
-        ret['setdims'],i='',-1
+        ret['setdims'], i = '', -1
         for d in dim:
-            i=i+1
-            if d not in ['*',':','(*)','(:)']:
-                ret['setdims']='%s#varname#_Dims[%d]=%s,'%(ret['setdims'],i,d)
-        if ret['setdims']: ret['setdims']=ret['setdims'][:-1]
-        ret['cbsetdims'],i='',-1
+            i = i + 1
+            if d not in ['*', ':', '(*)', '(:)']:
+                ret['setdims'] = '%s#varname#_Dims[%d]=%s,' % (
+                    ret['setdims'], i, d)
+        if ret['setdims']:
+            ret['setdims'] = ret['setdims'][:-1]
+        ret['cbsetdims'], i = '', -1
         for d in var['dimension']:
-            i=i+1
-            if d not in ['*',':','(*)','(:)']:
-                ret['cbsetdims']='%s#varname#_Dims[%d]=%s,'%(ret['cbsetdims'],i,d)
-            elif verbose :
-                errmess('getarrdims: If in call-back function: array argument %s must have bounded dimensions: got %s\n'%(`a`,`d`))
-        if ret['cbsetdims']: ret['cbsetdims']=ret['cbsetdims'][:-1]
+            i = i + 1
+            if d not in ['*', ':', '(*)', '(:)']:
+                ret['cbsetdims'] = '%s#varname#_Dims[%d]=%s,' % (
+                    ret['cbsetdims'], i, d)
+            elif isintent_in(var):
+                outmess('getarrdims:warning: assumed shape array, using 0 instead of %r\n'
+                        % (d))
+                ret['cbsetdims'] = '%s#varname#_Dims[%d]=%s,' % (
+                    ret['cbsetdims'], i, 0)
+            elif verbose:
+                errmess(
+                    'getarrdims: If in call-back function: array argument %s must have bounded dimensions: got %s\n' % (repr(a), repr(d)))
+        if ret['cbsetdims']:
+            ret['cbsetdims'] = ret['cbsetdims'][:-1]
 #         if not isintent_c(var):
 #             var['dimension'].reverse()
     return ret
-def getpydocsign(a,var):
+
+
+def getpydocsign(a, var):
     global lcb_map
     if isfunction(var):
-        if var.has_key('result'): af=var['result']
-        else: af=var['name']
-        if var['vars'].has_key(af): return getpydocsign(af,var['vars'][af])
-        else: errmess('getctype: function %s has no return value?!\n'%af)
-        return '',''
-    sig,sigout=a,a
-    opt=''
-    if isintent_in(var): opt='input'
-    elif isintent_inout(var): opt='in/output'
+        if 'result' in var:
+            af = var['result']
+        else:
+            af = var['name']
+        if af in var['vars']:
+            return getpydocsign(af, var['vars'][af])
+        else:
+            errmess('getctype: function %s has no return value?!\n' % af)
+        return '', ''
+    sig, sigout = a, a
+    opt = ''
+    if isintent_in(var):
+        opt = 'input'
+    elif isintent_inout(var):
+        opt = 'in/output'
     out_a = a
     if isintent_out(var):
         for k in var['intent']:
-            if k[:4]=='out=':
+            if k[:4] == 'out=':
                 out_a = k[4:]
                 break
-    init=''
-    ctype=getctype(var)
+    init = ''
+    ctype = getctype(var)
 
     if hasinitvalue(var):
-        init,showinit=getinit(a,var)
-        init='= %s'%(showinit)
+        init, showinit = getinit(a, var)
+        init = ', optional\\n    Default: %s' % showinit
     if isscalar(var):
         if isintent_inout(var):
-            sig='%s :%s %s rank-0 array(%s,\'%s\')'%(a,init,opt,c2py_map[ctype],
-                              c2pycode_map[ctype],)
-        else:
-            sig='%s :%s %s %s'%(a,init,opt,c2py_map[ctype])
-        sigout='%s : %s'%(out_a,c2py_map[ctype])
+            sig = '%s : %s rank-0 array(%s,\'%s\')%s' % (a, opt, c2py_map[ctype],
+                                                         c2pycode_map[ctype], init)
+        else:
+            sig = '%s : %s %s%s' % (a, opt, c2py_map[ctype], init)
+        sigout = '%s : %s' % (out_a, c2py_map[ctype])
     elif isstring(var):
         if isintent_inout(var):
-            sig='%s :%s %s rank-0 array(string(len=%s),\'c\')'%(a,init,opt,getstrlength(var))
-        else:
-            sig='%s :%s %s string(len=%s)'%(a,init,opt,getstrlength(var))
-        sigout='%s : string(len=%s)'%(out_a,getstrlength(var))
+            sig = '%s : %s rank-0 array(string(len=%s),\'c\')%s' % (
+                a, opt, getstrlength(var), init)
+        else:
+            sig = '%s : %s string(len=%s)%s' % (
+                a, opt, getstrlength(var), init)
+        sigout = '%s : string(len=%s)' % (out_a, getstrlength(var))
     elif isarray(var):
-        dim=var['dimension']
-        rank=`len(dim)`
-        sig='%s :%s %s rank-%s array(\'%s\') with bounds (%s)'%(a,init,opt,rank,
-                                             c2pycode_map[ctype],
-                                             string.join(dim,','))
-        if a==out_a:
-            sigout='%s : rank-%s array(\'%s\') with bounds (%s)'\
-                    %(a,rank,c2pycode_map[ctype],string.join(dim,','))
-        else:
-            sigout='%s : rank-%s array(\'%s\') with bounds (%s) and %s storage'\
-                    %(out_a,rank,c2pycode_map[ctype],string.join(dim,','),a)
+        dim = var['dimension']
+        rank = repr(len(dim))
+        sig = '%s : %s rank-%s array(\'%s\') with bounds (%s)%s' % (a, opt, rank,
+                                                                    c2pycode_map[
+                                                                        ctype],
+                                                                    ','.join(dim), init)
+        if a == out_a:
+            sigout = '%s : rank-%s array(\'%s\') with bounds (%s)'\
+                % (a, rank, c2pycode_map[ctype], ','.join(dim))
+        else:
+            sigout = '%s : rank-%s array(\'%s\') with bounds (%s) and %s storage'\
+                % (out_a, rank, c2pycode_map[ctype], ','.join(dim), a)
     elif isexternal(var):
-        ua=''
-        if lcb_map.has_key(a) and lcb2_map.has_key(lcb_map[a]) and lcb2_map[lcb_map[a]].has_key('argname'):
-            ua=lcb2_map[lcb_map[a]]['argname']
-            if not ua==a: ua=' => %s'%ua
-            else: ua=''
-        sig='%s : call-back function%s'%(a,ua)
-        sigout=sig
-    else:
-        errmess('getpydocsign: Could not resolve docsignature for "%s".\\n'%a)
-    return sig,sigout
-def getarrdocsign(a,var):
-    ctype=getctype(var)
+        ua = ''
+        if a in lcb_map and lcb_map[a] in lcb2_map and 'argname' in lcb2_map[lcb_map[a]]:
+            ua = lcb2_map[lcb_map[a]]['argname']
+            if not ua == a:
+                ua = ' => %s' % ua
+            else:
+                ua = ''
+        sig = '%s : call-back function%s' % (a, ua)
+        sigout = sig
+    else:
+        errmess(
+            'getpydocsign: Could not resolve docsignature for "%s".\n' % a)
+    return sig, sigout
+
+
+def getarrdocsign(a, var):
+    ctype = getctype(var)
     if isstring(var) and (not isarray(var)):
-        sig='%s : rank-0 array(string(len=%s),\'c\')'%(a,getstrlength(var))
+        sig = '%s : rank-0 array(string(len=%s),\'c\')' % (a,
+                                                           getstrlength(var))
     elif isscalar(var):
-        sig='%s : rank-0 array(%s,\'%s\')'%(a,c2py_map[ctype],
-                                            c2pycode_map[ctype],)
+        sig = '%s : rank-0 array(%s,\'%s\')' % (a, c2py_map[ctype],
+                                                c2pycode_map[ctype],)
     elif isarray(var):
-        dim=var['dimension']
-        rank=`len(dim)`
-        sig='%s : rank-%s array(\'%s\') with bounds (%s)'%(a,rank,
-                                                           c2pycode_map[ctype],
-                                                           string.join(dim,','))
+        dim = var['dimension']
+        rank = repr(len(dim))
+        sig = '%s : rank-%s array(\'%s\') with bounds (%s)' % (a, rank,
+                                                               c2pycode_map[
+                                                                   ctype],
+                                                               ','.join(dim))
     return sig
 
-def getinit(a,var):
-    if isstring(var): init,showinit='""',"''"
-    else: init,showinit='',''
+
+def getinit(a, var):
+    if isstring(var):
+        init, showinit = '""', "''"
+    else:
+        init, showinit = '', ''
     if hasinitvalue(var):
-        init=var['=']
-        showinit=init
+        init = var['=']
+        showinit = init
         if iscomplex(var) or iscomplexarray(var):
-            ret={}
+            ret = {}
 
             try:
                 v = var["="]
                 if ',' in v:
-                    ret['init.r'],ret['init.i']=string.split(markoutercomma(v[1:-1]),'@,@')
+                    ret['init.r'], ret['init.i'] = markoutercomma(
+                        v[1:-1]).split('@,@')
                 else:
-                    v = eval(v,{},{})
-                    ret['init.r'],ret['init.i']=str(v.real),str(v.imag)
-            except: raise 'sign2map: expected complex number `(r,i)\' but got `%s\' as initial value of %s.'%(init,`a`)
+                    v = eval(v, {}, {})
+                    ret['init.r'], ret['init.i'] = str(v.real), str(v.imag)
+            except Exception:
+                raise ValueError(
+                    'getinit: expected complex number `(r,i)\' but got `%s\' as initial value of %r.' % (init, a))
             if isarray(var):
-                init='(capi_c.r=%s,capi_c.i=%s,capi_c)'%(ret['init.r'],ret['init.i'])
+                init = '(capi_c.r=%s,capi_c.i=%s,capi_c)' % (
+                    ret['init.r'], ret['init.i'])
         elif isstring(var):
-            if not init: init,showinit='""',"''"
-            if init[0]=="'":
-                init='"%s"'%(string.replace(init[1:-1],'"','\\"'))
-            if init[0]=='"': showinit="'%s'"%(init[1:-1])
-    return init,showinit
-
-def sign2map(a,var):
+            if not init:
+                init, showinit = '""', "''"
+            if init[0] == "'":
+                init = '"%s"' % (init[1:-1].replace('"', '\\"'))
+            if init[0] == '"':
+                showinit = "'%s'" % (init[1:-1])
+    return init, showinit
+
+
+def sign2map(a, var):
     """
     varname,ctype,atype
     init,init.r,init.i,pytype
     vardebuginfo,vardebugshowvalue,varshowvalue
-    varrfromat
+    varrformat
+
     intent
     """
-    global lcb_map,cb_map
     out_a = a
     if isintent_out(var):
         for k in var['intent']:
-            if k[:4]=='out=':
+            if k[:4] == 'out=':
                 out_a = k[4:]
                 break
-    ret={'varname':a,'outvarname':out_a}
-    ret['ctype']=getctype(var)
+    ret = {'varname': a, 'outvarname': out_a, 'ctype': getctype(var)}
     intent_flags = []
-    for f,s in isintent_dict.items():
-        if f(var): intent_flags.append('F2PY_%s'%s)
+    for f, s in isintent_dict.items():
+        if f(var):
+            intent_flags.append('F2PY_%s' % s)
     if intent_flags:
-        #XXX: Evaluate intent_flags here.
-        ret['intent'] = string.join(intent_flags,'|')
+        # TODO: Evaluate intent_flags here.
+        ret['intent'] = '|'.join(intent_flags)
     else:
         ret['intent'] = 'F2PY_INTENT_IN'
-    if isarray(var): ret['varrformat']='N'
-    elif c2buildvalue_map.has_key(ret['ctype']):
-        ret['varrformat']=c2buildvalue_map[ret['ctype']]
-    else: ret['varrformat']='O'
-    ret['init'],ret['showinit']=getinit(a,var)
+    if isarray(var):
+        ret['varrformat'] = 'N'
+    elif ret['ctype'] in c2buildvalue_map:
+        ret['varrformat'] = c2buildvalue_map[ret['ctype']]
+    else:
+        ret['varrformat'] = 'O'
+    ret['init'], ret['showinit'] = getinit(a, var)
     if hasinitvalue(var) and iscomplex(var) and not isarray(var):
-        ret['init.r'],ret['init.i'] = string.split(markoutercomma(ret['init'][1:-1]),'@,@')
+        ret['init.r'], ret['init.i'] = markoutercomma(
+            ret['init'][1:-1]).split('@,@')
     if isexternal(var):
-        ret['cbnamekey']=a
-        if lcb_map.has_key(a):
-            ret['cbname']=lcb_map[a]
-            ret['maxnofargs']=lcb2_map[lcb_map[a]]['maxnofargs']
-            ret['nofoptargs']=lcb2_map[lcb_map[a]]['nofoptargs']
-            ret['cbdocstr']=lcb2_map[lcb_map[a]]['docstr']
-            ret['cblatexdocstr']=lcb2_map[lcb_map[a]]['latexdocstr']
-        else:
-            ret['cbname']=a
-            errmess('sign2map: Confused: external %s is not in lcb_map%s.\n'%(a,lcb_map.keys()))
+        ret['cbnamekey'] = a
+        if a in lcb_map:
+            ret['cbname'] = lcb_map[a]
+            ret['maxnofargs'] = lcb2_map[lcb_map[a]]['maxnofargs']
+            ret['nofoptargs'] = lcb2_map[lcb_map[a]]['nofoptargs']
+            ret['cbdocstr'] = lcb2_map[lcb_map[a]]['docstr']
+            ret['cblatexdocstr'] = lcb2_map[lcb_map[a]]['latexdocstr']
+        else:
+            ret['cbname'] = a
+            errmess('sign2map: Confused: external %s is not in lcb_map%s.\n' % (
+                a, list(lcb_map.keys())))
     if isstring(var):
-        ret['length']=getstrlength(var)
+        ret['length'] = getstrlength(var)
     if isarray(var):
-        ret=dictappend(ret,getarrdims(a,var))
-        dim=copy.copy(var['dimension'])
-    if c2capi_map.has_key(ret['ctype']): ret['atype']=c2capi_map[ret['ctype']]
+        ret = dictappend(ret, getarrdims(a, var))
+        dim = copy.copy(var['dimension'])
+    if ret['ctype'] in c2capi_map:
+        ret['atype'] = c2capi_map[ret['ctype']]
     # Debug info
     if debugcapi(var):
-        il=[isintent_in,'input',isintent_out,'output',
-            isintent_inout,'inoutput',isrequired,'required',
-            isoptional,'optional',isintent_hide,'hidden',
-            iscomplex,'complex scalar',
-            l_and(isscalar,l_not(iscomplex)),'scalar',
-            isstring,'string',isarray,'array',
-            iscomplexarray,'complex array',isstringarray,'string array',
-            iscomplexfunction,'complex function',
-            l_and(isfunction,l_not(iscomplexfunction)),'function',
-            isexternal,'callback',
-            isintent_callback,'callback',
-            isintent_aux,'auxiliary',
-            #ismutable,'mutable',l_not(ismutable),'immutable',
-            ]
-        rl=[]
-        for i in range(0,len(il),2):
-            if il[i](var): rl.append(il[i+1])
+        il = [isintent_in, 'input', isintent_out, 'output',
+              isintent_inout, 'inoutput', isrequired, 'required',
+              isoptional, 'optional', isintent_hide, 'hidden',
+              iscomplex, 'complex scalar',
+              l_and(isscalar, l_not(iscomplex)), 'scalar',
+              isstring, 'string', isarray, 'array',
+              iscomplexarray, 'complex array', isstringarray, 'string array',
+              iscomplexfunction, 'complex function',
+              l_and(isfunction, l_not(iscomplexfunction)), 'function',
+              isexternal, 'callback',
+              isintent_callback, 'callback',
+              isintent_aux, 'auxiliary',
+              ]
+        rl = []
+        for i in range(0, len(il), 2):
+            if il[i](var):
+                rl.append(il[i + 1])
         if isstring(var):
-            rl.append('slen(%s)=%s'%(a,ret['length']))
+            rl.append('slen(%s)=%s' % (a, ret['length']))
         if isarray(var):
-#             if not isintent_c(var):
-#                 var['dimension'].reverse()
-            ddim=string.join(map(lambda x,y:'%s|%s'%(x,y),var['dimension'],dim),',')
-            rl.append('dims(%s)'%ddim)
-#             if not isintent_c(var):
-#                 var['dimension'].reverse()
+            ddim = ','.join(
+                map(lambda x, y: '%s|%s' % (x, y), var['dimension'], dim))
+            rl.append('dims(%s)' % ddim)
         if isexternal(var):
-            ret['vardebuginfo']='debug-capi:%s=>%s:%s'%(a,ret['cbname'],string.join(rl,','))
-        else:
-            ret['vardebuginfo']='debug-capi:%s %s=%s:%s'%(ret['ctype'],a,ret['showinit'],string.join(rl,','))
+            ret['vardebuginfo'] = 'debug-capi:%s=>%s:%s' % (
+                a, ret['cbname'], ','.join(rl))
+        else:
+            ret['vardebuginfo'] = 'debug-capi:%s %s=%s:%s' % (
+                ret['ctype'], a, ret['showinit'], ','.join(rl))
         if isscalar(var):
-            if cformat_map.has_key(ret['ctype']):
-                ret['vardebugshowvalue']='debug-capi:%s=%s'%(a,cformat_map[ret['ctype']])
+            if ret['ctype'] in cformat_map:
+                ret['vardebugshowvalue'] = 'debug-capi:%s=%s' % (
+                    a, cformat_map[ret['ctype']])
         if isstring(var):
-            ret['vardebugshowvalue']='debug-capi:slen(%s)=%%d %s=\\"%%s\\"'%(a,a)
+            ret['vardebugshowvalue'] = 'debug-capi:slen(%s)=%%d %s=\\"%%s\\"' % (
+                a, a)
         if isexternal(var):
-            ret['vardebugshowvalue']='debug-capi:%s=%%p'%(a)
-    if cformat_map.has_key(ret['ctype']):
-        ret['varshowvalue']='#name#:%s=%s'%(a,cformat_map[ret['ctype']])
-        ret['showvalueformat']='%s'%(cformat_map[ret['ctype']])
+            ret['vardebugshowvalue'] = 'debug-capi:%s=%%p' % (a)
+    if ret['ctype'] in cformat_map:
+        ret['varshowvalue'] = '#name#:%s=%s' % (a, cformat_map[ret['ctype']])
+        ret['showvalueformat'] = '%s' % (cformat_map[ret['ctype']])
     if isstring(var):
-        ret['varshowvalue']='#name#:slen(%s)=%%d %s=\\"%%s\\"'%(a,a)
-    ret['pydocsign'],ret['pydocsignout']=getpydocsign(a,var)
+        ret['varshowvalue'] = '#name#:slen(%s)=%%d %s=\\"%%s\\"' % (a, a)
+    ret['pydocsign'], ret['pydocsignout'] = getpydocsign(a, var)
     if hasnote(var):
-        ret['note']=var['note']
+        ret['note'] = var['note']
     return ret
+
 
 def routsign2map(rout):
     """
@@ -518,18 +615,18 @@
     global lcb_map
     name = rout['name']
     fname = getfortranname(rout)
-    ret={'name':name,
-         'texname':string.replace(name,'_','\\_'),
-         'name_lower':string.lower(name),
-         'NAME':string.upper(name),
-         'begintitle':gentitle(name),
-         'endtitle':gentitle('end of %s'%name),
-         'fortranname':fname,
-         'FORTRANNAME':string.upper(fname),
-         'callstatement':getcallstatement(rout) or '',
-         'usercode':getusercode(rout) or '',
-         'usercode1':getusercode1(rout) or '',
-         }
+    ret = {'name': name,
+           'texname': name.replace('_', '\\_'),
+           'name_lower': name.lower(),
+           'NAME': name.upper(),
+           'begintitle': gentitle(name),
+           'endtitle': gentitle('end of %s' % name),
+           'fortranname': fname,
+           'FORTRANNAME': fname.upper(),
+           'callstatement': getcallstatement(rout) or '',
+           'usercode': getusercode(rout) or '',
+           'usercode1': getusercode1(rout) or '',
+           }
     if '_' in fname:
         ret['F_FUNC'] = 'F_FUNC_US'
     else:
@@ -538,66 +635,73 @@
         ret['F_WRAPPEDFUNC'] = 'F_WRAPPEDFUNC_US'
     else:
         ret['F_WRAPPEDFUNC'] = 'F_WRAPPEDFUNC'
-    lcb_map={}
-    if rout.has_key('use'):
+    lcb_map = {}
+    if 'use' in rout:
         for u in rout['use'].keys():
-            if cb_rules.cb_map.has_key(u):
+            if u in cb_rules.cb_map:
                 for un in cb_rules.cb_map[u]:
-                    ln=un[0]
-                    if rout['use'][u].has_key('map'):
+                    ln = un[0]
+                    if 'map' in rout['use'][u]:
                         for k in rout['use'][u]['map'].keys():
-                            if rout['use'][u]['map'][k]==un[0]: ln=k;break
-                    lcb_map[ln]=un[1]
-            #else:
-            #    errmess('routsign2map: cb_map does not contain module "%s" used in "use" statement.\n'%(u))
-    elif rout.has_key('externals') and rout['externals']:
-        errmess('routsign2map: Confused: function %s has externals %s but no "use" statement.\n'%(ret['name'],`rout['externals']`))
-    ret['callprotoargument'] = getcallprotoargument(rout,lcb_map) or ''
+                            if rout['use'][u]['map'][k] == un[0]:
+                                ln = k
+                                break
+                    lcb_map[ln] = un[1]
+    elif 'externals' in rout and rout['externals']:
+        errmess('routsign2map: Confused: function %s has externals %s but no "use" statement.\n' % (
+            ret['name'], repr(rout['externals'])))
+    ret['callprotoargument'] = getcallprotoargument(rout, lcb_map) or ''
     if isfunction(rout):
-        if rout.has_key('result'): a=rout['result']
-        else: a=rout['name']
-        ret['rname']=a
-        ret['pydocsign'],ret['pydocsignout']=getpydocsign(a,rout)
-        ret['ctype']=getctype(rout['vars'][a])
+        if 'result' in rout:
+            a = rout['result']
+        else:
+            a = rout['name']
+        ret['rname'] = a
+        ret['pydocsign'], ret['pydocsignout'] = getpydocsign(a, rout)
+        ret['ctype'] = getctype(rout['vars'][a])
         if hasresultnote(rout):
-            ret['resultnote']=rout['vars'][a]['note']
-            rout['vars'][a]['note']=['See elsewhere.']
-        if c2buildvalue_map.has_key(ret['ctype']):
-            ret['rformat']=c2buildvalue_map[ret['ctype']]
-        else:
-            ret['rformat']='O'
-            errmess('routsign2map: no c2buildvalue key for type %s\n'%(`ret['ctype']`))
+            ret['resultnote'] = rout['vars'][a]['note']
+            rout['vars'][a]['note'] = ['See elsewhere.']
+        if ret['ctype'] in c2buildvalue_map:
+            ret['rformat'] = c2buildvalue_map[ret['ctype']]
+        else:
+            ret['rformat'] = 'O'
+            errmess('routsign2map: no c2buildvalue key for type %s\n' %
+                    (repr(ret['ctype'])))
         if debugcapi(rout):
-            if cformat_map.has_key(ret['ctype']):
-                ret['routdebugshowvalue']='debug-capi:%s=%s'%(a,cformat_map[ret['ctype']])
+            if ret['ctype'] in cformat_map:
+                ret['routdebugshowvalue'] = 'debug-capi:%s=%s' % (
+                    a, cformat_map[ret['ctype']])
             if isstringfunction(rout):
-                ret['routdebugshowvalue']='debug-capi:slen(%s)=%%d %s=\\"%%s\\"'%(a,a)
+                ret['routdebugshowvalue'] = 'debug-capi:slen(%s)=%%d %s=\\"%%s\\"' % (
+                    a, a)
         if isstringfunction(rout):
-            ret['rlength']=getstrlength(rout['vars'][a])
-            if ret['rlength']=='-1':
-                errmess('routsign2map: expected explicit specification of the length of the string returned by the fortran function %s; taking 10.\n'%(`rout['name']`))
-                ret['rlength']='10'
+            ret['rlength'] = getstrlength(rout['vars'][a])
+            if ret['rlength'] == '-1':
+                errmess('routsign2map: expected explicit specification of the length of the string returned by the fortran function %s; taking 10.\n' % (
+                    repr(rout['name'])))
+                ret['rlength'] = '10'
     if hasnote(rout):
-        ret['note']=rout['note']
-        rout['note']=['See elsewhere.']
+        ret['note'] = rout['note']
+        rout['note'] = ['See elsewhere.']
     return ret
 
+
 def modsign2map(m):
     """
     modulename
     """
     if ismodule(m):
-        ret={'f90modulename':m['name'],
-             'F90MODULENAME':string.upper(m['name']),
-             'texf90modulename':string.replace(m['name'],'_','\\_')}
-    else:
-        ret={'modulename':m['name'],
-             'MODULENAME':string.upper(m['name']),
-             'texmodulename':string.replace(m['name'],'_','\\_')}
+        ret = {'f90modulename': m['name'],
+               'F90MODULENAME': m['name'].upper(),
+               'texf90modulename': m['name'].replace('_', '\\_')}
+    else:
+        ret = {'modulename': m['name'],
+               'MODULENAME': m['name'].upper(),
+               'texmodulename': m['name'].replace('_', '\\_')}
     ret['restdoc'] = getrestdoc(m) or []
     if hasnote(m):
-        ret['note']=m['note']
-        #m['note']=['See elsewhere.']
+        ret['note'] = m['note']
     ret['usercode'] = getusercode(m) or ''
     ret['usercode1'] = getusercode1(m) or ''
     if m['body']:
@@ -605,35 +709,42 @@
     else:
         ret['interface_usercode'] = ''
     ret['pymethoddef'] = getpymethoddef(m) or ''
+    if 'coutput' in m:
+        ret['coutput'] = m['coutput']
+    if 'f2py_wrapper_output' in m:
+        ret['f2py_wrapper_output'] = m['f2py_wrapper_output']
     return ret
 
-def cb_sign2map(a,var):
-    ret={'varname':a}
-    ret['ctype']=getctype(var)
-    if c2capi_map.has_key(ret['ctype']):
-        ret['atype']=c2capi_map[ret['ctype']]
-    if cformat_map.has_key(ret['ctype']):
-        ret['showvalueformat']='%s'%(cformat_map[ret['ctype']])
+
+def cb_sign2map(a, var, index=None):
+    ret = {'varname': a}
+    ret['varname_i'] = ret['varname']
+    ret['ctype'] = getctype(var)
+    if ret['ctype'] in c2capi_map:
+        ret['atype'] = c2capi_map[ret['ctype']]
+    if ret['ctype'] in cformat_map:
+        ret['showvalueformat'] = '%s' % (cformat_map[ret['ctype']])
     if isarray(var):
-        ret=dictappend(ret,getarrdims(a,var))
-    ret['pydocsign'],ret['pydocsignout']=getpydocsign(a,var)
+        ret = dictappend(ret, getarrdims(a, var))
+    ret['pydocsign'], ret['pydocsignout'] = getpydocsign(a, var)
     if hasnote(var):
-        ret['note']=var['note']
-        var['note']=['See elsewhere.']
+        ret['note'] = var['note']
+        var['note'] = ['See elsewhere.']
     return ret
 
-def cb_routsign2map(rout,um):
+
+def cb_routsign2map(rout, um):
     """
     name,begintitle,endtitle,argname
     ctype,rctype,maxnofargs,nofoptargs,returncptr
     """
-    ret={'name':'cb_%s_in_%s'%(rout['name'],um),
-         'returncptr':''}
+    ret = {'name': 'cb_%s_in_%s' % (rout['name'], um),
+           'returncptr': ''}
     if isintent_callback(rout):
         if '_' in rout['name']:
-            F_FUNC='F_FUNC_US'
-        else:
-            F_FUNC='F_FUNC'
+            F_FUNC = 'F_FUNC_US'
+        else:
+            F_FUNC = 'F_FUNC'
         ret['callbackname'] = '%s(%s,%s)' \
                               % (F_FUNC,
                                  rout['name'].lower(),
@@ -643,15 +754,16 @@
     else:
         ret['callbackname'] = ret['name']
         ret['static'] = 'static'
-    ret['argname']=rout['name']
-    ret['begintitle']=gentitle(ret['name'])
-    ret['endtitle']=gentitle('end of %s'%ret['name'])
-    ret['ctype']=getctype(rout)
-    ret['rctype']='void'
-    if ret['ctype']=='string': ret['rctype']='void'
-    else:
-        ret['rctype']=ret['ctype']
-    if ret['rctype']!='void':
+    ret['argname'] = rout['name']
+    ret['begintitle'] = gentitle(ret['name'])
+    ret['endtitle'] = gentitle('end of %s' % ret['name'])
+    ret['ctype'] = getctype(rout)
+    ret['rctype'] = 'void'
+    if ret['ctype'] == 'string':
+        ret['rctype'] = 'void'
+    else:
+        ret['rctype'] = ret['ctype']
+    if ret['rctype'] != 'void':
         if iscomplexfunction(rout):
             ret['returncptr'] = """
 #ifdef F2PY_CB_RETURNCOMPLEX
@@ -660,20 +772,22 @@
 """
         else:
             ret['returncptr'] = 'return_value='
-    if cformat_map.has_key(ret['ctype']):
-        ret['showvalueformat']='%s'%(cformat_map[ret['ctype']])
+    if ret['ctype'] in cformat_map:
+        ret['showvalueformat'] = '%s' % (cformat_map[ret['ctype']])
     if isstringfunction(rout):
-        ret['strlength']=getstrlength(rout)
+        ret['strlength'] = getstrlength(rout)
     if isfunction(rout):
-        if rout.has_key('result'): a=rout['result']
-        else: a=rout['name']
+        if 'result' in rout:
+            a = rout['result']
+        else:
+            a = rout['name']
         if hasnote(rout['vars'][a]):
-            ret['note']=rout['vars'][a]['note']
-            rout['vars'][a]['note']=['See elsewhere.']
-        ret['rname']=a
-        ret['pydocsign'],ret['pydocsignout']=getpydocsign(a,rout)
+            ret['note'] = rout['vars'][a]['note']
+            rout['vars'][a]['note'] = ['See elsewhere.']
+        ret['rname'] = a
+        ret['pydocsign'], ret['pydocsignout'] = getpydocsign(a, rout)
         if iscomplexfunction(rout):
-            ret['rctype']="""
+            ret['rctype'] = """
 #ifdef F2PY_CB_RETURNCOMPLEX
 #ctype#
 #else
@@ -682,40 +796,42 @@
 """
     else:
         if hasnote(rout):
-            ret['note']=rout['note']
-            rout['note']=['See elsewhere.']
-    nofargs=0
-    nofoptargs=0
-    if rout.has_key('args') and rout.has_key('vars'):
+            ret['note'] = rout['note']
+            rout['note'] = ['See elsewhere.']
+    nofargs = 0
+    nofoptargs = 0
+    if 'args' in rout and 'vars' in rout:
         for a in rout['args']:
-            var=rout['vars'][a]
-            if l_or(isintent_in,isintent_inout)(var):
-                nofargs=nofargs+1
+            var = rout['vars'][a]
+            if l_or(isintent_in, isintent_inout)(var):
+                nofargs = nofargs + 1
                 if isoptional(var):
-                    nofoptargs=nofoptargs+1
-    ret['maxnofargs']=`nofargs`
-    ret['nofoptargs']=`nofoptargs`
-    if hasnote(rout) and isfunction(rout) and rout.has_key('result'):
-        ret['routnote']=rout['note']
-        rout['note']=['See elsewhere.']
+                    nofoptargs = nofoptargs + 1
+    ret['maxnofargs'] = repr(nofargs)
+    ret['nofoptargs'] = repr(nofoptargs)
+    if hasnote(rout) and isfunction(rout) and 'result' in rout:
+        ret['routnote'] = rout['note']
+        rout['note'] = ['See elsewhere.']
     return ret
 
-def common_sign2map(a,var): # obsolute
-    ret={'varname':a}
-    ret['ctype']=getctype(var)
-    if isstringarray(var): ret['ctype']='char'
-    if c2capi_map.has_key(ret['ctype']):
-        ret['atype']=c2capi_map[ret['ctype']]
-    if cformat_map.has_key(ret['ctype']):
-        ret['showvalueformat']='%s'%(cformat_map[ret['ctype']])
+
+def common_sign2map(a, var):  # obsolute
+    ret = {'varname': a, 'ctype': getctype(var)}
+    if isstringarray(var):
+        ret['ctype'] = 'char'
+    if ret['ctype'] in c2capi_map:
+        ret['atype'] = c2capi_map[ret['ctype']]
+    if ret['ctype'] in cformat_map:
+        ret['showvalueformat'] = '%s' % (cformat_map[ret['ctype']])
     if isarray(var):
-        ret=dictappend(ret,getarrdims(a,var))
+        ret = dictappend(ret, getarrdims(a, var))
     elif isstring(var):
-        ret['size']=getstrlength(var)
-        ret['rank']='1'
-    ret['pydocsign'],ret['pydocsignout']=getpydocsign(a,var)
+        ret['size'] = getstrlength(var)
+        ret['rank'] = '1'
+    ret['pydocsign'], ret['pydocsignout'] = getpydocsign(a, var)
     if hasnote(var):
-        ret['note']=var['note']
-        var['note']=['See elsewhere.']
-    ret['arrdocstr']=getarrdocsign(a,var) # for strings this returns 0-rank but actually is 1-rank
+        ret['note'] = var['note']
+        var['note'] = ['See elsewhere.']
+    # for strings this returns 0-rank but actually is 1-rank
+    ret['arrdocstr'] = getarrdocsign(a, var)
     return ret
('numpy/f2py', 'f90mod_rules.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,4 +1,4 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 """
 
 Build F90 module support for f2py2e.
@@ -11,40 +11,44 @@
 NO WARRANTY IS EXPRESSED OR IMPLIED.  USE AT YOUR OWN RISK.
 $Date: 2005/02/03 19:30:23 $
 Pearu Peterson
+
 """
-
 __version__ = "$Revision: 1.27 $"[10:-1]
 
-f2py_version='See `f2py -v`'
-
-import pprint
-import sys,string,time,types,copy
-errmess=sys.stderr.write
-outmess=sys.stdout.write
-show=pprint.pprint
-
-from auxfuncs import *
-import capi_maps
-import cfuncs
-import rules
-import func2subr
-from crackfortran import undo_rmbadname, undo_rmbadname1
-
-options={}
+f2py_version = 'See `f2py -v`'
+
+import numpy as np
+
+from . import capi_maps
+from . import func2subr
+from .crackfortran import undo_rmbadname, undo_rmbadname1
+
+# The environment provided by auxfuncs.py is needed for some calls to eval.
+# As the needed functions cannot be determined by static inspection of the
+# code, it is safest to use import * pending a major refactoring of f2py.
+from .auxfuncs import *
+
+options = {}
+
 
 def findf90modules(m):
-    if ismodule(m): return [m]
-    if not hasbody(m): return []
+    if ismodule(m):
+        return [m]
+    if not hasbody(m):
+        return []
     ret = []
     for b in m['body']:
-        if ismodule(b): ret.append(b)
-        else: ret=ret+findf90modules(b)
+        if ismodule(b):
+            ret.append(b)
+        else:
+            ret = ret + findf90modules(b)
     return ret
 
 fgetdims1 = """\
       external f2pysetdata
       logical ns
-      integer s(*),r,i,j
+      integer r,i
+      integer(%d) s(*)
       ns = .FALSE.
       if (allocated(d)) then
          do i=1,r
@@ -56,9 +60,9 @@
             deallocate(d)
          end if
       end if
-      if ((.not.allocated(d)).and.(s(1).ge.1)) then"""
-
-fgetdims2="""\
+      if ((.not.allocated(d)).and.(s(1).ge.1)) then""" % np.intp().itemsize
+
+fgetdims2 = """\
       end if
       if (allocated(d)) then
          do i=1,r
@@ -68,7 +72,7 @@
       flag = 1
       call f2pysetdata(d,allocated(d))"""
 
-fgetdims2_sa="""\
+fgetdims2_sa = """\
       end if
       if (allocated(d)) then
          do i=1,r
@@ -81,42 +85,54 @@
 
 
 def buildhooks(pymod):
-    global fgetdims1,fgetdims2
-    ret = {'f90modhooks':[],'initf90modhooks':[],'body':[],
-           'need':['F_FUNC','arrayobject.h'],
-           'separatorsfor':{'includes0':'\n','includes':'\n'},
-           'docs':['"Fortran 90/95 modules:\\n"'],
-           'latexdoc':[]}
-    fhooks=['']
-    def fadd(line,s=fhooks): s[0] = '%s\n      %s'%(s[0],line)
+    from . import rules
+    ret = {'f90modhooks': [], 'initf90modhooks': [], 'body': [],
+           'need': ['F_FUNC', 'arrayobject.h'],
+           'separatorsfor': {'includes0': '\n', 'includes': '\n'},
+           'docs': ['"Fortran 90/95 modules:\\n"'],
+           'latexdoc': []}
+    fhooks = ['']
+
+    def fadd(line, s=fhooks):
+        s[0] = '%s\n      %s' % (s[0], line)
     doc = ['']
-    def dadd(line,s=doc): s[0] = '%s\n%s'%(s[0],line)
+
+    def dadd(line, s=doc):
+        s[0] = '%s\n%s' % (s[0], line)
     for m in findf90modules(pymod):
-        sargs,fargs,efargs,modobjs,notvars,onlyvars=[],[],[],[],[m['name']],[]
+        sargs, fargs, efargs, modobjs, notvars, onlyvars = [], [], [], [], [
+            m['name']], []
         sargsp = []
         ifargs = []
         mfargs = []
         if hasbody(m):
-            for b in m['body']: notvars.append(b['name'])
+            for b in m['body']:
+                notvars.append(b['name'])
         for n in m['vars'].keys():
             var = m['vars'][n]
-            if (n not in notvars) and (not l_or(isintent_hide,isprivate)(var)):
+            if (n not in notvars) and (not l_or(isintent_hide, isprivate)(var)):
                 onlyvars.append(n)
                 mfargs.append(n)
-        outmess('\t\tConstructing F90 module support for "%s"...\n'%(m['name']))
+        outmess('\t\tConstructing F90 module support for "%s"...\n' %
+                (m['name']))
         if onlyvars:
-            outmess('\t\t  Variables: %s\n'%(string.join(onlyvars)))
-        chooks=['']
-        def cadd(line,s=chooks): s[0] = '%s\n%s'%(s[0],line)
-        ihooks=['']
-        def iadd(line,s=ihooks): s[0] = '%s\n%s'%(s[0],line)
-
-        vrd=capi_maps.modsign2map(m)
-        cadd('static FortranDataDef f2py_%s_def[] = {'%(m['name']))
-        dadd('\\subsection{Fortran 90/95 module \\texttt{%s}}\n'%(m['name']))
+            outmess('\t\t  Variables: %s\n' % (' '.join(onlyvars)))
+        chooks = ['']
+
+        def cadd(line, s=chooks):
+            s[0] = '%s\n%s' % (s[0], line)
+        ihooks = ['']
+
+        def iadd(line, s=ihooks):
+            s[0] = '%s\n%s' % (s[0], line)
+
+        vrd = capi_maps.modsign2map(m)
+        cadd('static FortranDataDef f2py_%s_def[] = {' % (m['name']))
+        dadd('\\subsection{Fortran 90/95 module \\texttt{%s}}\n' % (m['name']))
         if hasnote(m):
             note = m['note']
-            if type(note) is type([]): note=string.join(note,'\n')
+            if isinstance(note, list):
+                note = '\n'.join(note)
             dadd(note)
         if onlyvars:
             dadd('\\begin{description}')
@@ -125,114 +141,130 @@
             modobjs.append(n)
             ct = capi_maps.getctype(var)
             at = capi_maps.c2capi_map[ct]
-            dm = capi_maps.getarrdims(n,var)
-            dms = string.strip(string.replace(dm['dims'],'*','-1'))
-            dms = string.strip(string.replace(dms,':','-1'))
-            if not dms: dms='-1'
+            dm = capi_maps.getarrdims(n, var)
+            dms = dm['dims'].replace('*', '-1').strip()
+            dms = dms.replace(':', '-1').strip()
+            if not dms:
+                dms = '-1'
             use_fgetdims2 = fgetdims2
             if isstringarray(var):
-                if var.has_key('charselector') and var['charselector'].has_key('len'):
-                    cadd('\t{"%s",%s,{{%s,%s}},%s},'\
-                         %(undo_rmbadname1(n),dm['rank'],dms,var['charselector']['len'],at))
+                if 'charselector' in var and 'len' in var['charselector']:
+                    cadd('\t{"%s",%s,{{%s,%s}},%s},'
+                         % (undo_rmbadname1(n), dm['rank'], dms, var['charselector']['len'], at))
                     use_fgetdims2 = fgetdims2_sa
                 else:
-                    cadd('\t{"%s",%s,{{%s}},%s},'%(undo_rmbadname1(n),dm['rank'],dms,at))
+                    cadd('\t{"%s",%s,{{%s}},%s},' %
+                         (undo_rmbadname1(n), dm['rank'], dms, at))
             else:
-                cadd('\t{"%s",%s,{{%s}},%s},'%(undo_rmbadname1(n),dm['rank'],dms,at))
-            dadd('\\item[]{{}\\verb@%s@{}}'%(capi_maps.getarrdocsign(n,var)))
+                cadd('\t{"%s",%s,{{%s}},%s},' %
+                     (undo_rmbadname1(n), dm['rank'], dms, at))
+            dadd('\\item[]{{}\\verb@%s@{}}' %
+                 (capi_maps.getarrdocsign(n, var)))
             if hasnote(var):
                 note = var['note']
-                if type(note) is type([]): note=string.join(note,'\n')
-                dadd('--- %s'%(note))
+                if isinstance(note, list):
+                    note = '\n'.join(note)
+                dadd('--- %s' % (note))
             if isallocatable(var):
-                fargs.append('f2py_%s_getdims_%s'%(m['name'],n))
+                fargs.append('f2py_%s_getdims_%s' % (m['name'], n))
                 efargs.append(fargs[-1])
-                sargs.append('void (*%s)(int*,int*,void(*)(char*,int*),int*)'%(n))
+                sargs.append(
+                    'void (*%s)(int*,int*,void(*)(char*,int*),int*)' % (n))
                 sargsp.append('void (*)(int*,int*,void(*)(char*,int*),int*)')
-                iadd('\tf2py_%s_def[i_f2py++].func = %s;'%(m['name'],n))
-                fadd('subroutine %s(r,s,f2pysetdata,flag)'%(fargs[-1]))
-                fadd('use %s, only: d => %s\n'%(m['name'],undo_rmbadname1(n)))
+                iadd('\tf2py_%s_def[i_f2py++].func = %s;' % (m['name'], n))
+                fadd('subroutine %s(r,s,f2pysetdata,flag)' % (fargs[-1]))
+                fadd('use %s, only: d => %s\n' %
+                     (m['name'], undo_rmbadname1(n)))
                 fadd('integer flag\n')
-                fhooks[0]=fhooks[0]+fgetdims1
-                dms = eval('range(1,%s+1)'%(dm['rank']))
-                fadd(' allocate(d(%s))\n'%(string.join(map(lambda i:'s(%s)'%i,dms),',')))
-                fhooks[0]=fhooks[0]+use_fgetdims2
-                fadd('end subroutine %s'%(fargs[-1]))
+                fhooks[0] = fhooks[0] + fgetdims1
+                dms = range(1, int(dm['rank']) + 1)
+                fadd(' allocate(d(%s))\n' %
+                     (','.join(['s(%s)' % i for i in dms])))
+                fhooks[0] = fhooks[0] + use_fgetdims2
+                fadd('end subroutine %s' % (fargs[-1]))
             else:
                 fargs.append(n)
-                sargs.append('char *%s'%(n))
+                sargs.append('char *%s' % (n))
                 sargsp.append('char*')
-                iadd('\tf2py_%s_def[i_f2py++].data = %s;'%(m['name'],n))
+                iadd('\tf2py_%s_def[i_f2py++].data = %s;' % (m['name'], n))
         if onlyvars:
             dadd('\\end{description}')
         if hasbody(m):
             for b in m['body']:
                 if not isroutine(b):
-                    print 'Skipping',b['block'],b['name']
+                    outmess("f90mod_rules.buildhooks:"
+                            f" skipping {b['block']} {b['name']}\n")
                     continue
-                modobjs.append('%s()'%(b['name']))
+                modobjs.append('%s()' % (b['name']))
                 b['modulename'] = m['name']
-                api,wrap=rules.buildapi(b)
+                api, wrap = rules.buildapi(b)
                 if isfunction(b):
-                    fhooks[0]=fhooks[0]+wrap
-                    fargs.append('f2pywrap_%s_%s'%(m['name'],b['name']))
-                    #efargs.append(fargs[-1])
-                    ifargs.append(func2subr.createfuncwrapper(b,signature=1))
+                    fhooks[0] = fhooks[0] + wrap
+                    fargs.append('f2pywrap_%s_%s' % (m['name'], b['name']))
+                    ifargs.append(func2subr.createfuncwrapper(b, signature=1))
                 else:
-                    fargs.append(b['name'])
-                    mfargs.append(fargs[-1])
-                    #if options.has_key('--external-modroutines') and options['--external-modroutines']:
-                    #    outmess('\t\t\tapplying --external-modroutines for %s\n'%(b['name']))
-                    #     efargs.append(fargs[-1])
-                api['externroutines']=[]
-                ar=applyrules(api,vrd)
-                ar['docs']=[]
-                ar['docshort']=[]
-                ret=dictappend(ret,ar)
-                cadd('\t{"%s",-1,{{-1}},0,NULL,(void *)f2py_rout_#modulename#_%s_%s,doc_f2py_rout_#modulename#_%s_%s},'%(b['name'],m['name'],b['name'],m['name'],b['name']))
-                sargs.append('char *%s'%(b['name']))
+                    if wrap:
+                        fhooks[0] = fhooks[0] + wrap
+                        fargs.append('f2pywrap_%s_%s' % (m['name'], b['name']))
+                        ifargs.append(
+                            func2subr.createsubrwrapper(b, signature=1))
+                    else:
+                        fargs.append(b['name'])
+                        mfargs.append(fargs[-1])
+                api['externroutines'] = []
+                ar = applyrules(api, vrd)
+                ar['docs'] = []
+                ar['docshort'] = []
+                ret = dictappend(ret, ar)
+                cadd('\t{"%s",-1,{{-1}},0,NULL,(void *)f2py_rout_#modulename#_%s_%s,doc_f2py_rout_#modulename#_%s_%s},' %
+                     (b['name'], m['name'], b['name'], m['name'], b['name']))
+                sargs.append('char *%s' % (b['name']))
                 sargsp.append('char *')
-                iadd('\tf2py_%s_def[i_f2py++].data = %s;'%(m['name'],b['name']))
+                iadd('\tf2py_%s_def[i_f2py++].data = %s;' %
+                     (m['name'], b['name']))
         cadd('\t{NULL}\n};\n')
         iadd('}')
-        ihooks[0]='static void f2py_setup_%s(%s) {\n\tint i_f2py=0;%s'%(m['name'],string.join(sargs,','),ihooks[0])
+        ihooks[0] = 'static void f2py_setup_%s(%s) {\n\tint i_f2py=0;%s' % (
+            m['name'], ','.join(sargs), ihooks[0])
         if '_' in m['name']:
-            F_FUNC='F_FUNC_US'
+            F_FUNC = 'F_FUNC_US'
         else:
-            F_FUNC='F_FUNC'
-        iadd('extern void %s(f2pyinit%s,F2PYINIT%s)(void (*)(%s));'\
-             %(F_FUNC,m['name'],string.upper(m['name']),string.join(sargsp,',')))
-        iadd('static void f2py_init_%s(void) {'%(m['name']))
-        iadd('\t%s(f2pyinit%s,F2PYINIT%s)(f2py_setup_%s);'\
-             %(F_FUNC,m['name'],string.upper(m['name']),m['name']))
+            F_FUNC = 'F_FUNC'
+        iadd('extern void %s(f2pyinit%s,F2PYINIT%s)(void (*)(%s));'
+             % (F_FUNC, m['name'], m['name'].upper(), ','.join(sargsp)))
+        iadd('static void f2py_init_%s(void) {' % (m['name']))
+        iadd('\t%s(f2pyinit%s,F2PYINIT%s)(f2py_setup_%s);'
+             % (F_FUNC, m['name'], m['name'].upper(), m['name']))
         iadd('}\n')
-        ret['f90modhooks']=ret['f90modhooks']+chooks+ihooks
-        ret['initf90modhooks']=['\tPyDict_SetItemString(d, "%s", PyFortranObject_New(f2py_%s_def,f2py_init_%s));'%(m['name'],m['name'],m['name'])]+ret['initf90modhooks']
+        ret['f90modhooks'] = ret['f90modhooks'] + chooks + ihooks
+        ret['initf90modhooks'] = ['\tPyDict_SetItemString(d, "%s", PyFortranObject_New(f2py_%s_def,f2py_init_%s));' % (
+            m['name'], m['name'], m['name'])] + ret['initf90modhooks']
         fadd('')
-        fadd('subroutine f2pyinit%s(f2pysetupfunc)'%(m['name']))
-        #fadd('use %s'%(m['name']))
+        fadd('subroutine f2pyinit%s(f2pysetupfunc)' % (m['name']))
         if mfargs:
             for a in undo_rmbadname(mfargs):
-                fadd('use %s, only : %s'%(m['name'],a))
+                fadd('use %s, only : %s' % (m['name'], a))
         if ifargs:
-            fadd(string.join(['interface']+ifargs))
+            fadd(' '.join(['interface'] + ifargs))
             fadd('end interface')
         fadd('external f2pysetupfunc')
         if efargs:
             for a in undo_rmbadname(efargs):
-                fadd('external %s'%(a))
-        fadd('call f2pysetupfunc(%s)'%(string.join(undo_rmbadname(fargs),',')))
-        fadd('end subroutine f2pyinit%s\n'%(m['name']))
-
-        dadd(string.replace(string.join(ret['latexdoc'],'\n'),r'\subsection{',r'\subsubsection{'))
-
-        ret['latexdoc']=[]
-        ret['docs'].append('"\t%s --- %s"'%(m['name'],
-                                            string.join(undo_rmbadname(modobjs),',')))
-
-    ret['routine_defs']=''
-    ret['doc']=[]
-    ret['docshort']=[]
-    ret['latexdoc']=doc[0]
-    if len(ret['docs'])<=1: ret['docs']=''
-    return ret,fhooks[0]
+                fadd('external %s' % (a))
+        fadd('call f2pysetupfunc(%s)' % (','.join(undo_rmbadname(fargs))))
+        fadd('end subroutine f2pyinit%s\n' % (m['name']))
+
+        dadd('\n'.join(ret['latexdoc']).replace(
+            r'\subsection{', r'\subsubsection{'))
+
+        ret['latexdoc'] = []
+        ret['docs'].append('"\t%s --- %s"' % (m['name'],
+                                              ','.join(undo_rmbadname(modobjs))))
+
+    ret['routine_defs'] = ''
+    ret['doc'] = []
+    ret['docshort'] = []
+    ret['latexdoc'] = doc[0]
+    if len(ret['docs']) <= 1:
+        ret['docs'] = ''
+    return ret, fhooks[0]
('numpy/f2py', 'use_rules.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,4 +1,4 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 """
 
 Build 'use others module data' mechanism for f2py2e.
@@ -13,25 +13,20 @@
 NO WARRANTY IS EXPRESSED OR IMPLIED.  USE AT YOUR OWN RISK.
 $Date: 2000/09/10 12:35:43 $
 Pearu Peterson
+
 """
-
 __version__ = "$Revision: 1.3 $"[10:-1]
 
-f2py_version='See `f2py -v`'
+f2py_version = 'See `f2py -v`'
 
-import pprint
-import sys,string,time,types,copy
-errmess=sys.stderr.write
-outmess=sys.stdout.write
-show=pprint.pprint
 
-from auxfuncs import *
-import capi_maps
-import cfuncs
-##############
+from .auxfuncs import (
+    applyrules, dictappend, gentitle, hasnote, outmess
+)
 
-usemodule_rules={
-    'body':"""
+
+usemodule_rules = {
+    'body': """
 #begintitle#
 static char doc_#apiname#[] = \"\\\nVariable wrapper signature:\\n\\
 \t #name# = get_#name#()\\n\\
@@ -47,63 +42,72 @@
 \treturn NULL;
 }
 """,
-    'method':'\t{\"get_#name#\",#apiname#,METH_VARARGS|METH_KEYWORDS,doc_#apiname#},',
-    'need':['F_MODFUNC']
-    }
+    'method': '\t{\"get_#name#\",#apiname#,METH_VARARGS|METH_KEYWORDS,doc_#apiname#},',
+    'need': ['F_MODFUNC']
+}
 
 ################
 
-def buildusevars(m,r):
-    ret={}
-    outmess('\t\tBuilding use variable hooks for module "%s" (feature only for F90/F95)...\n'%(m['name']))
-    varsmap={}
-    revmap={}
-    if r.has_key('map'):
+
+def buildusevars(m, r):
+    ret = {}
+    outmess(
+        '\t\tBuilding use variable hooks for module "%s" (feature only for F90/F95)...\n' % (m['name']))
+    varsmap = {}
+    revmap = {}
+    if 'map' in r:
         for k in r['map'].keys():
-            if revmap.has_key(r['map'][k]):
-                outmess('\t\t\tVariable "%s<=%s" is already mapped by "%s". Skipping.\n'%(r['map'][k],k,revmap[r['map'][k]]))
+            if r['map'][k] in revmap:
+                outmess('\t\t\tVariable "%s<=%s" is already mapped by "%s". Skipping.\n' % (
+                    r['map'][k], k, revmap[r['map'][k]]))
             else:
-                revmap[r['map'][k]]=k
-    if r.has_key('only') and r['only']:
+                revmap[r['map'][k]] = k
+    if 'only' in r and r['only']:
         for v in r['map'].keys():
-            if m['vars'].has_key(r['map'][v]):
+            if r['map'][v] in m['vars']:
 
-                if revmap[r['map'][v]]==v:
-                    varsmap[v]=r['map'][v]
+                if revmap[r['map'][v]] == v:
+                    varsmap[v] = r['map'][v]
                 else:
-                    outmess('\t\t\tIgnoring map "%s=>%s". See above.\n'%(v,r['map'][v]))
+                    outmess('\t\t\tIgnoring map "%s=>%s". See above.\n' %
+                            (v, r['map'][v]))
             else:
-                outmess('\t\t\tNo definition for variable "%s=>%s". Skipping.\n'%(v,r['map'][v]))
+                outmess(
+                    '\t\t\tNo definition for variable "%s=>%s". Skipping.\n' % (v, r['map'][v]))
     else:
         for v in m['vars'].keys():
-            if revmap.has_key(v):
-                varsmap[v]=revmap[v]
+            if v in revmap:
+                varsmap[v] = revmap[v]
             else:
-                varsmap[v]=v
+                varsmap[v] = v
     for v in varsmap.keys():
-        ret=dictappend(ret,buildusevar(v,varsmap[v],m['vars'],m['name']))
+        ret = dictappend(ret, buildusevar(v, varsmap[v], m['vars'], m['name']))
     return ret
-def buildusevar(name,realname,vars,usemodulename):
-    outmess('\t\t\tConstructing wrapper function for variable "%s=>%s"...\n'%(name,realname))
-    ret={}
-    vrd={'name':name,
-         'realname':realname,
-         'REALNAME':string.upper(realname),
-         'usemodulename':usemodulename,
-         'USEMODULENAME':string.upper(usemodulename),
-         'texname':string.replace(name,'_','\\_'),
-         'begintitle':gentitle('%s=>%s'%(name,realname)),
-         'endtitle':gentitle('end of %s=>%s'%(name,realname)),
-         'apiname':'#modulename#_use_%s_from_%s'%(realname,usemodulename)
-         }
-    nummap={0:'Ro',1:'Ri',2:'Rii',3:'Riii',4:'Riv',5:'Rv',6:'Rvi',7:'Rvii',8:'Rviii',9:'Rix'}
-    vrd['texnamename']=name
+
+
+def buildusevar(name, realname, vars, usemodulename):
+    outmess('\t\t\tConstructing wrapper function for variable "%s=>%s"...\n' % (
+        name, realname))
+    ret = {}
+    vrd = {'name': name,
+           'realname': realname,
+           'REALNAME': realname.upper(),
+           'usemodulename': usemodulename,
+           'USEMODULENAME': usemodulename.upper(),
+           'texname': name.replace('_', '\\_'),
+           'begintitle': gentitle('%s=>%s' % (name, realname)),
+           'endtitle': gentitle('end of %s=>%s' % (name, realname)),
+           'apiname': '#modulename#_use_%s_from_%s' % (realname, usemodulename)
+           }
+    nummap = {0: 'Ro', 1: 'Ri', 2: 'Rii', 3: 'Riii', 4: 'Riv',
+              5: 'Rv', 6: 'Rvi', 7: 'Rvii', 8: 'Rviii', 9: 'Rix'}
+    vrd['texnamename'] = name
     for i in nummap.keys():
-        vrd['texnamename']=string.replace(vrd['texnamename'],`i`,nummap[i])
-    if hasnote(vars[realname]): vrd['note']=vars[realname]['note']
-    rd=dictappend({},vrd)
-    var=vars[realname]
+        vrd['texnamename'] = vrd['texnamename'].replace(repr(i), nummap[i])
+    if hasnote(vars[realname]):
+        vrd['note'] = vars[realname]['note']
+    rd = dictappend({}, vrd)
 
-    print name,realname,vars[realname]
-    ret=applyrules(usemodule_rules,rd)
+    print(name, realname, vars[realname])
+    ret = applyrules(usemodule_rules, rd)
     return ret
('numpy/f2py', 'auxfuncs.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,4 +1,4 @@
-#!/usr/bin/env python
+#!/usr/bin/env python3
 """
 
 Auxiliary functions for f2py2e.
@@ -12,477 +12,846 @@
 NO WARRANTY IS EXPRESSED OR IMPLIED.  USE AT YOUR OWN RISK.
 $Date: 2005/07/24 19:01:55 $
 Pearu Peterson
+
 """
-__version__ = "$Revision: 1.65 $"[10:-1]
-
-import __version__
+import pprint
+import sys
+import types
+from functools import reduce
+
+from . import __version__
+from . import cfuncs
+
+__all__ = [
+    'applyrules', 'debugcapi', 'dictappend', 'errmess', 'gentitle',
+    'getargs2', 'getcallprotoargument', 'getcallstatement',
+    'getfortranname', 'getpymethoddef', 'getrestdoc', 'getusercode',
+    'getusercode1', 'hasbody', 'hascallstatement', 'hascommon',
+    'hasexternals', 'hasinitvalue', 'hasnote', 'hasresultnote',
+    'isallocatable', 'isarray', 'isarrayofstrings', 'iscomplex',
+    'iscomplexarray', 'iscomplexfunction', 'iscomplexfunction_warn',
+    'isdouble', 'isdummyroutine', 'isexternal', 'isfunction',
+    'isfunction_wrap', 'isint1array', 'isinteger', 'isintent_aux',
+    'isintent_c', 'isintent_callback', 'isintent_copy', 'isintent_dict',
+    'isintent_hide', 'isintent_in', 'isintent_inout', 'isintent_inplace',
+    'isintent_nothide', 'isintent_out', 'isintent_overwrite', 'islogical',
+    'islogicalfunction', 'islong_complex', 'islong_double',
+    'islong_doublefunction', 'islong_long', 'islong_longfunction',
+    'ismodule', 'ismoduleroutine', 'isoptional', 'isprivate', 'isrequired',
+    'isroutine', 'isscalar', 'issigned_long_longarray', 'isstring',
+    'isstringarray', 'isstringfunction', 'issubroutine',
+    'issubroutine_wrap', 'isthreadsafe', 'isunsigned', 'isunsigned_char',
+    'isunsigned_chararray', 'isunsigned_long_long',
+    'isunsigned_long_longarray', 'isunsigned_short',
+    'isunsigned_shortarray', 'l_and', 'l_not', 'l_or', 'outmess',
+    'replace', 'show', 'stripcomma', 'throw_error',
+]
+
+
 f2py_version = __version__.version
 
-import pprint
-import sys,string,time,types,os
-import cfuncs
-
-
-errmess=sys.stderr.write
-#outmess=sys.stdout.write
-show=pprint.pprint
-
-options={}
-debugoptions=[]
+
+errmess = sys.stderr.write
+show = pprint.pprint
+
+options = {}
+debugoptions = []
 wrapfuncs = 1
 
+
 def outmess(t):
-    if options.get('verbose',1):
+    if options.get('verbose', 1):
         sys.stdout.write(t)
 
-def debugcapi(var): return 'capi' in debugoptions
+
+def debugcapi(var):
+    return 'capi' in debugoptions
+
+
 def _isstring(var):
-    return var.has_key('typespec') and var['typespec']=='character' and (not isexternal(var))
+    return 'typespec' in var and var['typespec'] == 'character' and \
+           not isexternal(var)
+
+
 def isstring(var):
     return _isstring(var) and not isarray(var)
+
+
 def ischaracter(var):
-    return isstring(var) and not (var.has_key('charselector'))
+    return isstring(var) and 'charselector' not in var
+
+
 def isstringarray(var):
     return isarray(var) and _isstring(var)
+
+
 def isarrayofstrings(var):
-    # leaving out '*' for now so that
-    # `character*(*) a(m)` and `character a(m,*)`
-    # are treated differently. Luckily `character**` is illegal.
-    return isstringarray(var) and var['dimension'][-1]=='(*)'
-def isarray(var): return var.has_key('dimension') and (not isexternal(var))
-def isscalar(var): return not (isarray(var) or isstring(var) or isexternal(var))
+    # leaving out '*' for now so that `character*(*) a(m)` and `character
+    # a(m,*)` are treated differently. Luckily `character**` is illegal.
+    return isstringarray(var) and var['dimension'][-1] == '(*)'
+
+
+def isarray(var):
+    return 'dimension' in var and not isexternal(var)
+
+
+def isscalar(var):
+    return not (isarray(var) or isstring(var) or isexternal(var))
+
+
 def iscomplex(var):
-    return isscalar(var) and var.get('typespec') in ['complex','double complex']
+    return isscalar(var) and \
+           var.get('typespec') in ['complex', 'double complex']
+
+
 def islogical(var):
-    return isscalar(var) and var.get('typespec')=='logical'
+    return isscalar(var) and var.get('typespec') == 'logical'
+
+
 def isinteger(var):
-    return isscalar(var) and var.get('typespec')=='integer'
+    return isscalar(var) and var.get('typespec') == 'integer'
+
+
 def isreal(var):
-    return isscalar(var) and var.get('typespec')=='real'
+    return isscalar(var) and var.get('typespec') == 'real'
+
+
 def get_kind(var):
-    try: return var['kindselector']['*']
+    try:
+        return var['kindselector']['*']
     except KeyError:
-        try: return var['kindselector']['kind']
-        except KeyError: pass
+        try:
+            return var['kindselector']['kind']
+        except KeyError:
+            pass
+
+
 def islong_long(var):
-    if not isscalar(var): return 0
-    if var.get('typespec') not in ['integer','logical']: return 0
-    return get_kind(var)=='8'
+    if not isscalar(var):
+        return 0
+    if var.get('typespec') not in ['integer', 'logical']:
+        return 0
+    return get_kind(var) == '8'
+
+
 def isunsigned_char(var):
-    if not isscalar(var): return 0
-    if var.get('typespec') != 'integer': return 0
-    return get_kind(var)=='-1'
+    if not isscalar(var):
+        return 0
+    if var.get('typespec') != 'integer':
+        return 0
+    return get_kind(var) == '-1'
+
+
 def isunsigned_short(var):
-    if not isscalar(var): return 0
-    if var.get('typespec') != 'integer': return 0
-    return get_kind(var)=='-2'
+    if not isscalar(var):
+        return 0
+    if var.get('typespec') != 'integer':
+        return 0
+    return get_kind(var) == '-2'
+
+
 def isunsigned(var):
-    if not isscalar(var): return 0
-    if var.get('typespec') != 'integer': return 0
-    return get_kind(var)=='-4'
+    if not isscalar(var):
+        return 0
+    if var.get('typespec') != 'integer':
+        return 0
+    return get_kind(var) == '-4'
+
+
 def isunsigned_long_long(var):
-    if not isscalar(var): return 0
-    if var.get('typespec') != 'integer': return 0
-    return get_kind(var)=='-8'
+    if not isscalar(var):
+        return 0
+    if var.get('typespec') != 'integer':
+        return 0
+    return get_kind(var) == '-8'
+
+
 def isdouble(var):
-    if not isscalar(var): return 0
-    if not var.get('typespec')=='real': return 0
-    return get_kind(var)=='8'
+    if not isscalar(var):
+        return 0
+    if not var.get('typespec') == 'real':
+        return 0
+    return get_kind(var) == '8'
+
+
 def islong_double(var):
-    if not isscalar(var): return 0
-    if not var.get('typespec')=='real': return 0
-    return get_kind(var)=='16'
+    if not isscalar(var):
+        return 0
+    if not var.get('typespec') == 'real':
+        return 0
+    return get_kind(var) == '16'
+
+
 def islong_complex(var):
-    if not iscomplex(var): return 0
-    return get_kind(var)=='32'
-
-def iscomplexarray(var): return isarray(var) and var.get('typespec') in ['complex','double complex']
-def isint1array(var): return isarray(var) and var.get('typespec')=='integer' \
-    and get_kind(var)=='1'
-def isunsigned_chararray(var): return isarray(var) and var.get('typespec')=='integer' and get_kind(var)=='-1'
-def isunsigned_shortarray(var): return isarray(var) and var.get('typespec')=='integer' and get_kind(var)=='-2'
-def isunsignedarray(var): return isarray(var) and var.get('typespec')=='integer' and get_kind(var)=='-4'
-def isunsigned_long_longarray(var): return isarray(var) and var.get('typespec')=='integer' and get_kind(var)=='-8'
+    if not iscomplex(var):
+        return 0
+    return get_kind(var) == '32'
+
+
+def iscomplexarray(var):
+    return isarray(var) and \
+           var.get('typespec') in ['complex', 'double complex']
+
+
+def isint1array(var):
+    return isarray(var) and var.get('typespec') == 'integer' \
+        and get_kind(var) == '1'
+
+
+def isunsigned_chararray(var):
+    return isarray(var) and var.get('typespec') in ['integer', 'logical']\
+        and get_kind(var) == '-1'
+
+
+def isunsigned_shortarray(var):
+    return isarray(var) and var.get('typespec') in ['integer', 'logical']\
+        and get_kind(var) == '-2'
+
+
+def isunsignedarray(var):
+    return isarray(var) and var.get('typespec') in ['integer', 'logical']\
+        and get_kind(var) == '-4'
+
+
+def isunsigned_long_longarray(var):
+    return isarray(var) and var.get('typespec') in ['integer', 'logical']\
+        and get_kind(var) == '-8'
+
+
+def issigned_chararray(var):
+    return isarray(var) and var.get('typespec') in ['integer', 'logical']\
+        and get_kind(var) == '1'
+
+
+def issigned_shortarray(var):
+    return isarray(var) and var.get('typespec') in ['integer', 'logical']\
+        and get_kind(var) == '2'
+
+
+def issigned_array(var):
+    return isarray(var) and var.get('typespec') in ['integer', 'logical']\
+        and get_kind(var) == '4'
+
+
+def issigned_long_longarray(var):
+    return isarray(var) and var.get('typespec') in ['integer', 'logical']\
+        and get_kind(var) == '8'
+
+
 def isallocatable(var):
-    return var.has_key('attrspec') and 'allocatable' in var['attrspec']
-def ismutable(var): return not (not var.has_key('dimension') or isstring(var))
-def ismoduleroutine(rout): return rout.has_key('modulename')
-def ismodule(rout): return (rout.has_key('block') and 'module'==rout['block'])
-def isfunction(rout): return (rout.has_key('block') and 'function'==rout['block'])
-#def isfunction_wrap(rout):
-#    return wrapfuncs and (iscomplexfunction(rout) or isstringfunction(rout)) and (not isexternal(rout))
+    return 'attrspec' in var and 'allocatable' in var['attrspec']
+
+
+def ismutable(var):
+    return not ('dimension' not in var or isstring(var))
+
+
+def ismoduleroutine(rout):
+    return 'modulename' in rout
+
+
+def ismodule(rout):
+    return 'block' in rout and 'module' == rout['block']
+
+
+def isfunction(rout):
+    return 'block' in rout and 'function' == rout['block']
+
+
 def isfunction_wrap(rout):
-    if isintent_c(rout): return 0
+    if isintent_c(rout):
+        return 0
     return wrapfuncs and isfunction(rout) and (not isexternal(rout))
-def issubroutine(rout): return (rout.has_key('block') and 'subroutine'==rout['block'])
-def isroutine(rout): return isfunction(rout) or issubroutine(rout)
+
+
+def issubroutine(rout):
+    return 'block' in rout and 'subroutine' == rout['block']
+
+
+def issubroutine_wrap(rout):
+    if isintent_c(rout):
+        return 0
+    return issubroutine(rout) and hasassumedshape(rout)
+
+
+def hasassumedshape(rout):
+    if rout.get('hasassumedshape'):
+        return True
+    for a in rout['args']:
+        for d in rout['vars'].get(a, {}).get('dimension', []):
+            if d == ':':
+                rout['hasassumedshape'] = True
+                return True
+    return False
+
+
+def requiresf90wrapper(rout):
+    return ismoduleroutine(rout) or hasassumedshape(rout)
+
+
+def isroutine(rout):
+    return isfunction(rout) or issubroutine(rout)
+
+
 def islogicalfunction(rout):
-    if not isfunction(rout): return 0
-    if rout.has_key('result'): a=rout['result']
-    else: a=rout['name']
-    if rout['vars'].has_key(a): return islogical(rout['vars'][a])
-    return 0
+    if not isfunction(rout):
+        return 0
+    if 'result' in rout:
+        a = rout['result']
+    else:
+        a = rout['name']
+    if a in rout['vars']:
+        return islogical(rout['vars'][a])
+    return 0
+
+
 def islong_longfunction(rout):
-    if not isfunction(rout): return 0
-    if rout.has_key('result'): a=rout['result']
-    else: a=rout['name']
-    if rout['vars'].has_key(a): return islong_long(rout['vars'][a])
-    return 0
+    if not isfunction(rout):
+        return 0
+    if 'result' in rout:
+        a = rout['result']
+    else:
+        a = rout['name']
+    if a in rout['vars']:
+        return islong_long(rout['vars'][a])
+    return 0
+
+
 def islong_doublefunction(rout):
-    if not isfunction(rout): return 0
-    if rout.has_key('result'): a=rout['result']
-    else: a=rout['name']
-    if rout['vars'].has_key(a): return islong_double(rout['vars'][a])
-    return 0
+    if not isfunction(rout):
+        return 0
+    if 'result' in rout:
+        a = rout['result']
+    else:
+        a = rout['name']
+    if a in rout['vars']:
+        return islong_double(rout['vars'][a])
+    return 0
+
+
 def iscomplexfunction(rout):
-    if not isfunction(rout): return 0
-    if rout.has_key('result'): a=rout['result']
-    else: a=rout['name']
-    if rout['vars'].has_key(a): return iscomplex(rout['vars'][a])
-    return 0
+    if not isfunction(rout):
+        return 0
+    if 'result' in rout:
+        a = rout['result']
+    else:
+        a = rout['name']
+    if a in rout['vars']:
+        return iscomplex(rout['vars'][a])
+    return 0
+
+
 def iscomplexfunction_warn(rout):
     if iscomplexfunction(rout):
         outmess("""\
     **************************************************************
         Warning: code with a function returning complex value
         may not work correctly with your Fortran compiler.
-        Run the following test before using it in your applications:
-        $(f2py install dir)/test-site/{b/runme_scalar,e/runme}
-        When using GNU gcc/g77 compilers, codes should work correctly.
+        When using GNU gcc/g77 compilers, codes should work
+        correctly for callbacks with:
+        f2py -c -DF2PY_CB_RETURNCOMPLEX
     **************************************************************\n""")
         return 1
     return 0
+
+
 def isstringfunction(rout):
-    if not isfunction(rout): return 0
-    if rout.has_key('result'): a=rout['result']
-    else: a=rout['name']
-    if rout['vars'].has_key(a): return isstring(rout['vars'][a])
-    return 0
-def hasexternals(rout): return rout.has_key('externals') and rout['externals']
-def isthreadsafe(rout): return rout.has_key('f2pyenhancements') and rout['f2pyenhancements'].has_key('threadsafe')
-def hasvariables(rout): return rout.has_key('vars') and rout['vars']
-def isoptional(var): return (var.has_key('attrspec') and 'optional' in var['attrspec'] and 'required' not in var['attrspec']) and isintent_nothide(var)
-def isexternal(var): return (var.has_key('attrspec') and 'external' in var['attrspec'])
-def isrequired(var): return not isoptional(var) and isintent_nothide(var)
+    if not isfunction(rout):
+        return 0
+    if 'result' in rout:
+        a = rout['result']
+    else:
+        a = rout['name']
+    if a in rout['vars']:
+        return isstring(rout['vars'][a])
+    return 0
+
+
+def hasexternals(rout):
+    return 'externals' in rout and rout['externals']
+
+
+def isthreadsafe(rout):
+    return 'f2pyenhancements' in rout and \
+           'threadsafe' in rout['f2pyenhancements']
+
+
+def hasvariables(rout):
+    return 'vars' in rout and rout['vars']
+
+
+def isoptional(var):
+    return ('attrspec' in var and 'optional' in var['attrspec'] and
+            'required' not in var['attrspec']) and isintent_nothide(var)
+
+
+def isexternal(var):
+    return 'attrspec' in var and 'external' in var['attrspec']
+
+
+def isrequired(var):
+    return not isoptional(var) and isintent_nothide(var)
+
+
 def isintent_in(var):
-    if not var.has_key('intent'): return 1
-    if 'hide' in var['intent']: return 0
-    if 'inplace' in var['intent']: return 0
-    if 'in' in var['intent']: return 1
-    if 'out' in var['intent']: return 0
-    if 'inout' in var['intent']: return 0
-    if 'outin' in var['intent']: return 0
+    if 'intent' not in var:
+        return 1
+    if 'hide' in var['intent']:
+        return 0
+    if 'inplace' in var['intent']:
+        return 0
+    if 'in' in var['intent']:
+        return 1
+    if 'out' in var['intent']:
+        return 0
+    if 'inout' in var['intent']:
+        return 0
+    if 'outin' in var['intent']:
+        return 0
     return 1
-def isintent_inout(var): return var.has_key('intent') and ('inout' in var['intent'] or 'outin' in var['intent']) and 'in' not in var['intent'] and 'hide' not in var['intent'] and 'inplace' not in var['intent']
+
+
+def isintent_inout(var):
+    return ('intent' in var and ('inout' in var['intent'] or
+            'outin' in var['intent']) and 'in' not in var['intent'] and
+            'hide' not in var['intent'] and 'inplace' not in var['intent'])
+
+
 def isintent_out(var):
-    return 'out' in var.get('intent',[])
-def isintent_hide(var): return (var.has_key('intent') and ('hide' in var['intent'] or ('out' in var['intent'] and 'in' not in var['intent'] and (not l_or(isintent_inout,isintent_inplace)(var)))))
-def isintent_nothide(var): return not isintent_hide(var)
+    return 'out' in var.get('intent', [])
+
+
+def isintent_hide(var):
+    return ('intent' in var and ('hide' in var['intent'] or
+            ('out' in var['intent'] and 'in' not in var['intent'] and
+                (not l_or(isintent_inout, isintent_inplace)(var)))))
+
+def isintent_nothide(var):
+    return not isintent_hide(var)
+
+
 def isintent_c(var):
-    return 'c' in var.get('intent',[])
-# def isintent_f(var):
-#     return not isintent_c(var)
+    return 'c' in var.get('intent', [])
+
+
 def isintent_cache(var):
-    return 'cache' in var.get('intent',[])
+    return 'cache' in var.get('intent', [])
+
+
 def isintent_copy(var):
-    return 'copy' in var.get('intent',[])
+    return 'copy' in var.get('intent', [])
+
+
 def isintent_overwrite(var):
-    return 'overwrite' in var.get('intent',[])
+    return 'overwrite' in var.get('intent', [])
+
+
 def isintent_callback(var):
-    return 'callback' in var.get('intent',[])
+    return 'callback' in var.get('intent', [])
+
+
 def isintent_inplace(var):
-    return 'inplace' in var.get('intent',[])
+    return 'inplace' in var.get('intent', [])
+
+
 def isintent_aux(var):
-    return 'aux' in var.get('intent',[])
-
-isintent_dict = {isintent_in:'INTENT_IN',isintent_inout:'INTENT_INOUT',
-                 isintent_out:'INTENT_OUT',isintent_hide:'INTENT_HIDE',
-                 isintent_cache:'INTENT_CACHE',
-                 isintent_c:'INTENT_C',isoptional:'OPTIONAL',
-                 isintent_inplace:'INTENT_INPLACE'
+    return 'aux' in var.get('intent', [])
+
+
+def isintent_aligned4(var):
+    return 'aligned4' in var.get('intent', [])
+
+
+def isintent_aligned8(var):
+    return 'aligned8' in var.get('intent', [])
+
+
+def isintent_aligned16(var):
+    return 'aligned16' in var.get('intent', [])
+
+isintent_dict = {isintent_in: 'INTENT_IN', isintent_inout: 'INTENT_INOUT',
+                 isintent_out: 'INTENT_OUT', isintent_hide: 'INTENT_HIDE',
+                 isintent_cache: 'INTENT_CACHE',
+                 isintent_c: 'INTENT_C', isoptional: 'OPTIONAL',
+                 isintent_inplace: 'INTENT_INPLACE',
+                 isintent_aligned4: 'INTENT_ALIGNED4',
+                 isintent_aligned8: 'INTENT_ALIGNED8',
+                 isintent_aligned16: 'INTENT_ALIGNED16',
                  }
 
+
 def isprivate(var):
-    return var.has_key('attrspec') and 'private' in var['attrspec']
-
-def hasinitvalue(var): return var.has_key('=')
+    return 'attrspec' in var and 'private' in var['attrspec']
+
+
+def hasinitvalue(var):
+    return '=' in var
+
+
 def hasinitvalueasstring(var):
-    if not hasinitvalue(var): return 0
-    return var['='][0] in ['"',"'"]
+    if not hasinitvalue(var):
+        return 0
+    return var['='][0] in ['"', "'"]
+
+
 def hasnote(var):
-    return var.has_key('note')
+    return 'note' in var
+
+
 def hasresultnote(rout):
-    if not isfunction(rout): return 0
-    if rout.has_key('result'): a=rout['result']
-    else: a=rout['name']
-    if rout['vars'].has_key(a): return hasnote(rout['vars'][a])
-    return 0
+    if not isfunction(rout):
+        return 0
+    if 'result' in rout:
+        a = rout['result']
+    else:
+        a = rout['name']
+    if a in rout['vars']:
+        return hasnote(rout['vars'][a])
+    return 0
+
+
 def hascommon(rout):
-    return rout.has_key('common')
+    return 'common' in rout
+
+
 def containscommon(rout):
-    if hascommon(rout): return 1
+    if hascommon(rout):
+        return 1
     if hasbody(rout):
         for b in rout['body']:
-            if containscommon(b): return 1
-    return 0
+            if containscommon(b):
+                return 1
+    return 0
+
+
 def containsmodule(block):
-    if ismodule(block): return 1
-    if not hasbody(block): return 0
-    ret = []
+    if ismodule(block):
+        return 1
+    if not hasbody(block):
+        return 0
     for b in block['body']:
-        if containsmodule(b): return 1
-    return 0
+        if containsmodule(b):
+            return 1
+    return 0
+
+
 def hasbody(rout):
-    return rout.has_key('body')
+    return 'body' in rout
+
+
 def hascallstatement(rout):
     return getcallstatement(rout) is not None
 
-def istrue(var): return 1
-def isfalse(var): return 0
+
+def istrue(var):
+    return 1
+
+
+def isfalse(var):
+    return 0
+
 
 class F2PYError(Exception):
     pass
 
+
 class throw_error:
-    def __init__(self,mess):
+
+    def __init__(self, mess):
         self.mess = mess
-    def __call__(self,var):
-        mess = '\n\n  var = %s\n  Message: %s\n' % (var,self.mess)
-        raise F2PYError,mess
+
+    def __call__(self, var):
+        mess = '\n\n  var = %s\n  Message: %s\n' % (var, self.mess)
+        raise F2PYError(mess)
+
 
 def l_and(*f):
-    l,l2='lambda v',[]
+    l, l2 = 'lambda v', []
     for i in range(len(f)):
-        l='%s,f%d=f[%d]'%(l,i,i)
-        l2.append('f%d(v)'%(i))
-    return eval('%s:%s'%(l,string.join(l2,' and ')))
+        l = '%s,f%d=f[%d]' % (l, i, i)
+        l2.append('f%d(v)' % (i))
+    return eval('%s:%s' % (l, ' and '.join(l2)))
+
+
 def l_or(*f):
-    l,l2='lambda v',[]
+    l, l2 = 'lambda v', []
     for i in range(len(f)):
-        l='%s,f%d=f[%d]'%(l,i,i)
-        l2.append('f%d(v)'%(i))
-    return eval('%s:%s'%(l,string.join(l2,' or ')))
+        l = '%s,f%d=f[%d]' % (l, i, i)
+        l2.append('f%d(v)' % (i))
+    return eval('%s:%s' % (l, ' or '.join(l2)))
+
+
 def l_not(f):
     return eval('lambda v,f=f:not f(v)')
 
+
 def isdummyroutine(rout):
     try:
-        return rout['f2pyenhancements']['fortranname']==''
+        return rout['f2pyenhancements']['fortranname'] == ''
     except KeyError:
         return 0
+
 
 def getfortranname(rout):
     try:
         name = rout['f2pyenhancements']['fortranname']
-        if name=='':
+        if name == '':
             raise KeyError
         if not name:
-            errmess('Failed to use fortranname from %s\n'%(rout['f2pyenhancements']))
+            errmess('Failed to use fortranname from %s\n' %
+                    (rout['f2pyenhancements']))
             raise KeyError
     except KeyError:
         name = rout['name']
     return name
 
-def getmultilineblock(rout,blockname,comment=1,counter=0):
+
+def getmultilineblock(rout, blockname, comment=1, counter=0):
     try:
         r = rout['f2pyenhancements'].get(blockname)
     except KeyError:
         return
-    if not r: return
-    if counter>0 and type(r) is type(''):
+    if not r:
         return
-    if type(r) is type([]):
-        if counter>=len(r): return
+    if counter > 0 and isinstance(r, str):
+        return
+    if isinstance(r, list):
+        if counter >= len(r):
+            return
         r = r[counter]
-    if r[:3]=="'''":
+    if r[:3] == "'''":
         if comment:
-            r = '\t/* start ' + blockname + ' multiline ('+`counter`+') */\n' + r[3:]
+            r = '\t/* start ' + blockname + \
+                ' multiline (' + repr(counter) + ') */\n' + r[3:]
         else:
             r = r[3:]
-        if r[-3:]=="'''":
+        if r[-3:] == "'''":
             if comment:
-                r = r[:-3] + '\n\t/* end multiline ('+`counter`+')*/'
+                r = r[:-3] + '\n\t/* end multiline (' + repr(counter) + ')*/'
             else:
                 r = r[:-3]
         else:
-            errmess("%s multiline block should end with `'''`: %s\n" \
-                    % (blockname,repr(r)))
+            errmess("%s multiline block should end with `'''`: %s\n"
+                    % (blockname, repr(r)))
     return r
 
+
 def getcallstatement(rout):
-    return getmultilineblock(rout,'callstatement')
-
-def getcallprotoargument(rout,cb_map={}):
-    r = getmultilineblock(rout,'callprotoargument',comment=0)
-    if r: return r
+    return getmultilineblock(rout, 'callstatement')
+
+
+def getcallprotoargument(rout, cb_map={}):
+    r = getmultilineblock(rout, 'callprotoargument', comment=0)
+    if r:
+        return r
     if hascallstatement(rout):
-        outmess('warning: callstatement is defined without callprotoargument\n')
+        outmess(
+            'warning: callstatement is defined without callprotoargument\n')
         return
-    from capi_maps import getctype
-    arg_types,arg_types2 = [],[]
-    if l_and(isstringfunction,l_not(isfunction_wrap))(rout):
-        arg_types.extend(['char*','size_t'])
+    from .capi_maps import getctype
+    arg_types, arg_types2 = [], []
+    if l_and(isstringfunction, l_not(isfunction_wrap))(rout):
+        arg_types.extend(['char*', 'size_t'])
     for n in rout['args']:
         var = rout['vars'][n]
         if isintent_callback(var):
             continue
-        if cb_map.has_key(n):
-            ctype = cb_map[n]+'_typedef'
+        if n in cb_map:
+            ctype = cb_map[n] + '_typedef'
         else:
             ctype = getctype(var)
-            if l_and(isintent_c,l_or(isscalar,iscomplex))(var):
+            if l_and(isintent_c, l_or(isscalar, iscomplex))(var):
                 pass
             elif isstring(var):
                 pass
-                #ctype = 'void*'
             else:
-                ctype = ctype+'*'
+                ctype = ctype + '*'
             if isstring(var) or isarrayofstrings(var):
                 arg_types2.append('size_t')
         arg_types.append(ctype)
 
-    proto_args = string.join(arg_types+arg_types2,',')
+    proto_args = ','.join(arg_types + arg_types2)
     if not proto_args:
         proto_args = 'void'
-    #print proto_args
     return proto_args
 
+
 def getusercode(rout):
-    return getmultilineblock(rout,'usercode')
+    return getmultilineblock(rout, 'usercode')
+
+
 def getusercode1(rout):
-    return getmultilineblock(rout,'usercode',counter=1)
+    return getmultilineblock(rout, 'usercode', counter=1)
+
 
 def getpymethoddef(rout):
-    return getmultilineblock(rout,'pymethoddef')
+    return getmultilineblock(rout, 'pymethoddef')
+
 
 def getargs(rout):
-    sortargs,args=[],[]
-    if rout.has_key('args'):
-        args=rout['args']
-        if rout.has_key('sortvars'):
+    sortargs, args = [], []
+    if 'args' in rout:
+        args = rout['args']
+        if 'sortvars' in rout:
             for a in rout['sortvars']:
-                if a in args: sortargs.append(a)
+                if a in args:
+                    sortargs.append(a)
             for a in args:
                 if a not in sortargs:
                     sortargs.append(a)
-        else: sortargs=rout['args']
-    return args,sortargs
+        else:
+            sortargs = rout['args']
+    return args, sortargs
+
 
 def getargs2(rout):
-    sortargs,args=[],rout.get('args',[])
-    auxvars = [a for a in rout['vars'].keys() if isintent_aux(rout['vars'][a])\
+    sortargs, args = [], rout.get('args', [])
+    auxvars = [a for a in rout['vars'].keys() if isintent_aux(rout['vars'][a])
                and a not in args]
     args = auxvars + args
-    if rout.has_key('sortvars'):
+    if 'sortvars' in rout:
         for a in rout['sortvars']:
-            if a in args: sortargs.append(a)
+            if a in args:
+                sortargs.append(a)
         for a in args:
             if a not in sortargs:
                 sortargs.append(a)
-    else: sortargs=auxvars + rout['args']
-    return args,sortargs
+    else:
+        sortargs = auxvars + rout['args']
+    return args, sortargs
+
 
 def getrestdoc(rout):
-    if not rout.has_key('f2pymultilines'):
+    if 'f2pymultilines' not in rout:
         return None
     k = None
-    if rout['block']=='python module':
-        k = rout['block'],rout['name']
-    return rout['f2pymultilines'].get(k,None)
+    if rout['block'] == 'python module':
+        k = rout['block'], rout['name']
+    return rout['f2pymultilines'].get(k, None)
+
 
 def gentitle(name):
-    l=(80-len(name)-6)/2
-    return '/*%s %s %s*/'%(l*'*',name,l*'*')
+    l = (80 - len(name) - 6) // 2
+    return '/*%s %s %s*/' % (l * '*', name, l * '*')
+
+
 def flatlist(l):
-    if type(l)==types.ListType:
-        return reduce(lambda x,y,f=flatlist:x+f(y),l,[])
+    if isinstance(l, list):
+        return reduce(lambda x, y, f=flatlist: x + f(y), l, [])
     return [l]
+
+
 def stripcomma(s):
-    if s and s[-1]==',': return s[:-1]
+    if s and s[-1] == ',':
+        return s[:-1]
     return s
-def replace(str,dict,defaultsep=''):
-    if type(dict)==types.ListType:
-        return map(lambda d,f=replace,sep=defaultsep,s=str:f(s,d,sep),dict)
-    if type(str)==types.ListType:
-        return map(lambda s,f=replace,sep=defaultsep,d=dict:f(s,d,sep),str)
-    for k in 2*dict.keys():
-        if k=='separatorsfor': continue
-        if dict.has_key('separatorsfor') and dict['separatorsfor'].has_key(k):
-            sep=dict['separatorsfor'][k]
+
+
+def replace(str, d, defaultsep=''):
+    if isinstance(d, list):
+        return [replace(str, _m, defaultsep) for _m in d]
+    if isinstance(str, list):
+        return [replace(_m, d, defaultsep) for _m in str]
+    for k in 2 * list(d.keys()):
+        if k == 'separatorsfor':
+            continue
+        if 'separatorsfor' in d and k in d['separatorsfor']:
+            sep = d['separatorsfor'][k]
         else:
-            sep=defaultsep
-        if type(dict[k])==types.ListType:
-            str=string.replace(str,'#%s#'%(k),string.join(flatlist(dict[k]),sep))
+            sep = defaultsep
+        if isinstance(d[k], list):
+            str = str.replace('#%s#' % (k), sep.join(flatlist(d[k])))
         else:
-            str=string.replace(str,'#%s#'%(k),dict[k])
+            str = str.replace('#%s#' % (k), d[k])
     return str
 
-def dictappend(rd,ar):
-    if type(ar)==types.ListType:
-        for a in ar: rd=dictappend(rd,a)
+
+def dictappend(rd, ar):
+    if isinstance(ar, list):
+        for a in ar:
+            rd = dictappend(rd, a)
         return rd
     for k in ar.keys():
-        if k[0]=='_': continue
-        if rd.has_key(k):
-            if type(rd[k])==types.StringType: rd[k]=[rd[k]]
-            if type(rd[k])==types.ListType:
-                if type(ar[k])==types.ListType: rd[k]=rd[k]+ar[k]
-                else: rd[k].append(ar[k])
-            elif type(rd[k])==types.DictType:
-                if type(ar[k])==types.DictType:
-                    if k=='separatorsfor':
+        if k[0] == '_':
+            continue
+        if k in rd:
+            if isinstance(rd[k], str):
+                rd[k] = [rd[k]]
+            if isinstance(rd[k], list):
+                if isinstance(ar[k], list):
+                    rd[k] = rd[k] + ar[k]
+                else:
+                    rd[k].append(ar[k])
+            elif isinstance(rd[k], dict):
+                if isinstance(ar[k], dict):
+                    if k == 'separatorsfor':
                         for k1 in ar[k].keys():
-                            if not rd[k].has_key(k1): rd[k][k1]=ar[k][k1]
-                    else: rd[k]=dictappend(rd[k],ar[k])
-        else: rd[k]=ar[k]
+                            if k1 not in rd[k]:
+                                rd[k][k1] = ar[k][k1]
+                    else:
+                        rd[k] = dictappend(rd[k], ar[k])
+        else:
+            rd[k] = ar[k]
     return rd
 
-def applyrules(rules,dict,var={}):
-    ret={}
-    if type(rules)==types.ListType:
+
+def applyrules(rules, d, var={}):
+    ret = {}
+    if isinstance(rules, list):
         for r in rules:
-            rr=applyrules(r,dict,var)
-            ret=dictappend(ret,rr)
-            if rr.has_key('_break'): break
+            rr = applyrules(r, d, var)
+            ret = dictappend(ret, rr)
+            if '_break' in rr:
+                break
         return ret
-    if rules.has_key('_check') and (not rules['_check'](var)): return ret
-    if rules.has_key('need'):
-        res = applyrules({'needs':rules['need']},dict,var)
-        if res.has_key('needs'):
+    if '_check' in rules and (not rules['_check'](var)):
+        return ret
+    if 'need' in rules:
+        res = applyrules({'needs': rules['need']}, d, var)
+        if 'needs' in res:
             cfuncs.append_needs(res['needs'])
 
     for k in rules.keys():
-        if k=='separatorsfor': ret[k]=rules[k]; continue
-        if type(rules[k])==types.StringType:
-            ret[k]=replace(rules[k],dict)
-        elif type(rules[k])==types.ListType:
-            ret[k]=[]
+        if k == 'separatorsfor':
+            ret[k] = rules[k]
+            continue
+        if isinstance(rules[k], str):
+            ret[k] = replace(rules[k], d)
+        elif isinstance(rules[k], list):
+            ret[k] = []
             for i in rules[k]:
-                ar=applyrules({k:i},dict,var)
-                if ar.has_key(k): ret[k].append(ar[k])
-        elif k[0]=='_':
+                ar = applyrules({k: i}, d, var)
+                if k in ar:
+                    ret[k].append(ar[k])
+        elif k[0] == '_':
             continue
-        elif type(rules[k])==types.DictType:
-            ret[k]=[]
+        elif isinstance(rules[k], dict):
+            ret[k] = []
             for k1 in rules[k].keys():
-                if type(k1)==types.FunctionType and k1(var):
-                    if type(rules[k][k1])==types.ListType:
+                if isinstance(k1, types.FunctionType) and k1(var):
+                    if isinstance(rules[k][k1], list):
                         for i in rules[k][k1]:
-                            if type(i)==types.DictType:
-                                res=applyrules({'supertext':i},dict,var)
-                                if res.has_key('supertext'): i=res['supertext']
-                                else: i=''
-                            ret[k].append(replace(i,dict))
+                            if isinstance(i, dict):
+                                res = applyrules({'supertext': i}, d, var)
+                                if 'supertext' in res:
+                                    i = res['supertext']
+                                else:
+                                    i = ''
+                            ret[k].append(replace(i, d))
                     else:
-                        i=rules[k][k1]
-                        if type(i)==types.DictType:
-                            res=applyrules({'supertext':i},dict)
-                            if res.has_key('supertext'): i=res['supertext']
-                            else: i=''
-                        ret[k].append(replace(i,dict))
+                        i = rules[k][k1]
+                        if isinstance(i, dict):
+                            res = applyrules({'supertext': i}, d)
+                            if 'supertext' in res:
+                                i = res['supertext']
+                            else:
+                                i = ''
+                        ret[k].append(replace(i, d))
         else:
-            errmess('applyrules: ignoring rule %s.\n'%`rules[k]`)
-        if type(ret[k])==types.ListType:
-            if len(ret[k])==1: ret[k]=ret[k][0]
-            if ret[k]==[]: del ret[k]
+            errmess('applyrules: ignoring rule %s.\n' % repr(rules[k]))
+        if isinstance(ret[k], list):
+            if len(ret[k]) == 1:
+                ret[k] = ret[k][0]
+            if ret[k] == []:
+                del ret[k]
     return ret
('numpy/lib', 'scimath.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -2,78 +2,624 @@
 Wrapper functions to more user-friendly calling of certain math functions
 whose output data-type is different than the input data-type in certain
 domains of the input.
+
+For example, for functions like `log` with branch cuts, the versions in this
+module provide the mathematically valid answers in the complex plane::
+
+  >>> import math
+  >>> np.emath.log(-math.exp(1)) == (1+1j*math.pi)
+  True
+
+Similarly, `sqrt`, other base logarithms, `power` and trig functions are
+correctly handled.  See their respective docstrings for specific examples.
+
+Functions
+---------
+
+.. autosummary::
+   :toctree: generated/
+
+   sqrt
+   log
+   log2
+   logn
+   log10
+   power
+   arccos
+   arcsin
+   arctanh
+
 """
-
-__all__ = ['sqrt', 'log', 'log2', 'logn','log10', 'power', 'arccos',
-           'arcsin', 'arctanh']
-
 import numpy.core.numeric as nx
 import numpy.core.numerictypes as nt
 from numpy.core.numeric import asarray, any
+from numpy.core.overrides import array_function_dispatch
 from numpy.lib.type_check import isreal
 
 
-#__all__.extend([key for key in dir(nx.umath)
-#                if key[0] != '_' and key not in __all__])
+__all__ = [
+    'sqrt', 'log', 'log2', 'logn', 'log10', 'power', 'arccos', 'arcsin',
+    'arctanh'
+    ]
+
 
 _ln2 = nx.log(2.0)
 
+
 def _tocomplex(arr):
-    if isinstance(arr.dtype, (nt.single, nt.byte, nt.short, nt.ubyte,
-                              nt.ushort)):
+    """Convert its input `arr` to a complex array.
+
+    The input is returned as a complex array of the smallest type that will fit
+    the original data: types like single, byte, short, etc. become csingle,
+    while others become cdouble.
+
+    A copy of the input is always made.
+
+    Parameters
+    ----------
+    arr : array
+
+    Returns
+    -------
+    array
+        An array with the same input data as the input but in complex form.
+
+    Examples
+    --------
+
+    First, consider an input of type short:
+
+    >>> a = np.array([1,2,3],np.short)
+
+    >>> ac = np.lib.scimath._tocomplex(a); ac
+    array([1.+0.j, 2.+0.j, 3.+0.j], dtype=complex64)
+
+    >>> ac.dtype
+    dtype('complex64')
+
+    If the input is of type double, the output is correspondingly of the
+    complex double type as well:
+
+    >>> b = np.array([1,2,3],np.double)
+
+    >>> bc = np.lib.scimath._tocomplex(b); bc
+    array([1.+0.j, 2.+0.j, 3.+0.j])
+
+    >>> bc.dtype
+    dtype('complex128')
+
+    Note that even if the input was complex to begin with, a copy is still
+    made, since the astype() method always copies:
+
+    >>> c = np.array([1,2,3],np.csingle)
+
+    >>> cc = np.lib.scimath._tocomplex(c); cc
+    array([1.+0.j,  2.+0.j,  3.+0.j], dtype=complex64)
+
+    >>> c *= 2; c
+    array([2.+0.j,  4.+0.j,  6.+0.j], dtype=complex64)
+
+    >>> cc
+    array([1.+0.j,  2.+0.j,  3.+0.j], dtype=complex64)
+    """
+    if issubclass(arr.dtype.type, (nt.single, nt.byte, nt.short, nt.ubyte,
+                                   nt.ushort, nt.csingle)):
         return arr.astype(nt.csingle)
     else:
         return arr.astype(nt.cdouble)
 
+
 def _fix_real_lt_zero(x):
+    """Convert `x` to complex if it has real, negative components.
+
+    Otherwise, output is just the array version of the input (via asarray).
+
+    Parameters
+    ----------
+    x : array_like
+
+    Returns
+    -------
+    array
+
+    Examples
+    --------
+    >>> np.lib.scimath._fix_real_lt_zero([1,2])
+    array([1, 2])
+
+    >>> np.lib.scimath._fix_real_lt_zero([-1,2])
+    array([-1.+0.j,  2.+0.j])
+
+    """
     x = asarray(x)
-    if any(isreal(x) & (x<0)):
+    if any(isreal(x) & (x < 0)):
         x = _tocomplex(x)
     return x
 
+
+def _fix_int_lt_zero(x):
+    """Convert `x` to double if it has real, negative components.
+
+    Otherwise, output is just the array version of the input (via asarray).
+
+    Parameters
+    ----------
+    x : array_like
+
+    Returns
+    -------
+    array
+
+    Examples
+    --------
+    >>> np.lib.scimath._fix_int_lt_zero([1,2])
+    array([1, 2])
+
+    >>> np.lib.scimath._fix_int_lt_zero([-1,2])
+    array([-1.,  2.])
+    """
+    x = asarray(x)
+    if any(isreal(x) & (x < 0)):
+        x = x * 1.0
+    return x
+
+
 def _fix_real_abs_gt_1(x):
+    """Convert `x` to complex if it has real components x_i with abs(x_i)>1.
+
+    Otherwise, output is just the array version of the input (via asarray).
+
+    Parameters
+    ----------
+    x : array_like
+
+    Returns
+    -------
+    array
+
+    Examples
+    --------
+    >>> np.lib.scimath._fix_real_abs_gt_1([0,1])
+    array([0, 1])
+
+    >>> np.lib.scimath._fix_real_abs_gt_1([0,2])
+    array([0.+0.j, 2.+0.j])
+    """
     x = asarray(x)
-    if any(isreal(x) & (abs(x)>1)):
+    if any(isreal(x) & (abs(x) > 1)):
         x = _tocomplex(x)
     return x
 
+
+def _unary_dispatcher(x):
+    return (x,)
+
+
+@array_function_dispatch(_unary_dispatcher)
 def sqrt(x):
+    """
+    Compute the square root of x.
+
+    For negative input elements, a complex value is returned
+    (unlike `numpy.sqrt` which returns NaN).
+
+    Parameters
+    ----------
+    x : array_like
+       The input value(s).
+
+    Returns
+    -------
+    out : ndarray or scalar
+       The square root of `x`. If `x` was a scalar, so is `out`,
+       otherwise an array is returned.
+
+    See Also
+    --------
+    numpy.sqrt
+
+    Examples
+    --------
+    For real, non-negative inputs this works just like `numpy.sqrt`:
+
+    >>> np.emath.sqrt(1)
+    1.0
+    >>> np.emath.sqrt([1, 4])
+    array([1.,  2.])
+
+    But it automatically handles negative inputs:
+
+    >>> np.emath.sqrt(-1)
+    1j
+    >>> np.emath.sqrt([-1,4])
+    array([0.+1.j, 2.+0.j])
+
+    Different results are expected because:
+    floating point 0.0 and -0.0 are distinct.
+
+    For more control, explicitly use complex() as follows:
+
+    >>> np.emath.sqrt(complex(-4.0, 0.0))
+    2j
+    >>> np.emath.sqrt(complex(-4.0, -0.0))
+    -2j
+    """
     x = _fix_real_lt_zero(x)
     return nx.sqrt(x)
 
+
+@array_function_dispatch(_unary_dispatcher)
 def log(x):
+    """
+    Compute the natural logarithm of `x`.
+
+    Return the "principal value" (for a description of this, see `numpy.log`)
+    of :math:`log_e(x)`. For real `x > 0`, this is a real number (``log(0)``
+    returns ``-inf`` and ``log(np.inf)`` returns ``inf``). Otherwise, the
+    complex principle value is returned.
+
+    Parameters
+    ----------
+    x : array_like
+       The value(s) whose log is (are) required.
+
+    Returns
+    -------
+    out : ndarray or scalar
+       The log of the `x` value(s). If `x` was a scalar, so is `out`,
+       otherwise an array is returned.
+
+    See Also
+    --------
+    numpy.log
+
+    Notes
+    -----
+    For a log() that returns ``NAN`` when real `x < 0`, use `numpy.log`
+    (note, however, that otherwise `numpy.log` and this `log` are identical,
+    i.e., both return ``-inf`` for `x = 0`, ``inf`` for `x = inf`, and,
+    notably, the complex principle value if ``x.imag != 0``).
+
+    Examples
+    --------
+    >>> np.emath.log(np.exp(1))
+    1.0
+
+    Negative arguments are handled "correctly" (recall that
+    ``exp(log(x)) == x`` does *not* hold for real ``x < 0``):
+
+    >>> np.emath.log(-np.exp(1)) == (1 + np.pi * 1j)
+    True
+
+    """
     x = _fix_real_lt_zero(x)
     return nx.log(x)
 
+
+@array_function_dispatch(_unary_dispatcher)
 def log10(x):
+    """
+    Compute the logarithm base 10 of `x`.
+
+    Return the "principal value" (for a description of this, see
+    `numpy.log10`) of :math:`log_{10}(x)`. For real `x > 0`, this
+    is a real number (``log10(0)`` returns ``-inf`` and ``log10(np.inf)``
+    returns ``inf``). Otherwise, the complex principle value is returned.
+
+    Parameters
+    ----------
+    x : array_like or scalar
+       The value(s) whose log base 10 is (are) required.
+
+    Returns
+    -------
+    out : ndarray or scalar
+       The log base 10 of the `x` value(s). If `x` was a scalar, so is `out`,
+       otherwise an array object is returned.
+
+    See Also
+    --------
+    numpy.log10
+
+    Notes
+    -----
+    For a log10() that returns ``NAN`` when real `x < 0`, use `numpy.log10`
+    (note, however, that otherwise `numpy.log10` and this `log10` are
+    identical, i.e., both return ``-inf`` for `x = 0`, ``inf`` for `x = inf`,
+    and, notably, the complex principle value if ``x.imag != 0``).
+
+    Examples
+    --------
+
+    (We set the printing precision so the example can be auto-tested)
+
+    >>> np.set_printoptions(precision=4)
+
+    >>> np.emath.log10(10**1)
+    1.0
+
+    >>> np.emath.log10([-10**1, -10**2, 10**2])
+    array([1.+1.3644j, 2.+1.3644j, 2.+0.j    ])
+
+    """
     x = _fix_real_lt_zero(x)
     return nx.log10(x)
 
+
+def _logn_dispatcher(n, x):
+    return (n, x,)
+
+
+@array_function_dispatch(_logn_dispatcher)
 def logn(n, x):
-    """ Take log base n of x.
+    """
+    Take log base n of x.
+
+    If `x` contains negative inputs, the answer is computed and returned in the
+    complex domain.
+
+    Parameters
+    ----------
+    n : array_like
+       The integer base(s) in which the log is taken.
+    x : array_like
+       The value(s) whose log base `n` is (are) required.
+
+    Returns
+    -------
+    out : ndarray or scalar
+       The log base `n` of the `x` value(s). If `x` was a scalar, so is
+       `out`, otherwise an array is returned.
+
+    Examples
+    --------
+    >>> np.set_printoptions(precision=4)
+
+    >>> np.emath.logn(2, [4, 8])
+    array([2., 3.])
+    >>> np.emath.logn(2, [-4, -8, 8])
+    array([2.+4.5324j, 3.+4.5324j, 3.+0.j    ])
+
     """
     x = _fix_real_lt_zero(x)
     n = _fix_real_lt_zero(n)
-    return log(x)/log(n)
-
+    return nx.log(x)/nx.log(n)
+
+
+@array_function_dispatch(_unary_dispatcher)
 def log2(x):
-    """ Take log base 2 of x.
+    """
+    Compute the logarithm base 2 of `x`.
+
+    Return the "principal value" (for a description of this, see
+    `numpy.log2`) of :math:`log_2(x)`. For real `x > 0`, this is
+    a real number (``log2(0)`` returns ``-inf`` and ``log2(np.inf)`` returns
+    ``inf``). Otherwise, the complex principle value is returned.
+
+    Parameters
+    ----------
+    x : array_like
+       The value(s) whose log base 2 is (are) required.
+
+    Returns
+    -------
+    out : ndarray or scalar
+       The log base 2 of the `x` value(s). If `x` was a scalar, so is `out`,
+       otherwise an array is returned.
+
+    See Also
+    --------
+    numpy.log2
+
+    Notes
+    -----
+    For a log2() that returns ``NAN`` when real `x < 0`, use `numpy.log2`
+    (note, however, that otherwise `numpy.log2` and this `log2` are
+    identical, i.e., both return ``-inf`` for `x = 0`, ``inf`` for `x = inf`,
+    and, notably, the complex principle value if ``x.imag != 0``).
+
+    Examples
+    --------
+    We set the printing precision so the example can be auto-tested:
+
+    >>> np.set_printoptions(precision=4)
+
+    >>> np.emath.log2(8)
+    3.0
+    >>> np.emath.log2([-4, -8, 8])
+    array([2.+4.5324j, 3.+4.5324j, 3.+0.j    ])
+
     """
     x = _fix_real_lt_zero(x)
-    return log(x)/_ln2
-
+    return nx.log2(x)
+
+
+def _power_dispatcher(x, p):
+    return (x, p)
+
+
+@array_function_dispatch(_power_dispatcher)
 def power(x, p):
+    """
+    Return x to the power p, (x**p).
+
+    If `x` contains negative values, the output is converted to the
+    complex domain.
+
+    Parameters
+    ----------
+    x : array_like
+        The input value(s).
+    p : array_like of ints
+        The power(s) to which `x` is raised. If `x` contains multiple values,
+        `p` has to either be a scalar, or contain the same number of values
+        as `x`. In the latter case, the result is
+        ``x[0]**p[0], x[1]**p[1], ...``.
+
+    Returns
+    -------
+    out : ndarray or scalar
+        The result of ``x**p``. If `x` and `p` are scalars, so is `out`,
+        otherwise an array is returned.
+
+    See Also
+    --------
+    numpy.power
+
+    Examples
+    --------
+    >>> np.set_printoptions(precision=4)
+
+    >>> np.emath.power([2, 4], 2)
+    array([ 4, 16])
+    >>> np.emath.power([2, 4], -2)
+    array([0.25  ,  0.0625])
+    >>> np.emath.power([-2, 4], 2)
+    array([ 4.-0.j, 16.+0.j])
+
+    """
     x = _fix_real_lt_zero(x)
+    p = _fix_int_lt_zero(p)
     return nx.power(x, p)
 
+
+@array_function_dispatch(_unary_dispatcher)
 def arccos(x):
+    """
+    Compute the inverse cosine of x.
+
+    Return the "principal value" (for a description of this, see
+    `numpy.arccos`) of the inverse cosine of `x`. For real `x` such that
+    `abs(x) <= 1`, this is a real number in the closed interval
+    :math:`[0, \\pi]`.  Otherwise, the complex principle value is returned.
+
+    Parameters
+    ----------
+    x : array_like or scalar
+       The value(s) whose arccos is (are) required.
+
+    Returns
+    -------
+    out : ndarray or scalar
+       The inverse cosine(s) of the `x` value(s). If `x` was a scalar, so
+       is `out`, otherwise an array object is returned.
+
+    See Also
+    --------
+    numpy.arccos
+
+    Notes
+    -----
+    For an arccos() that returns ``NAN`` when real `x` is not in the
+    interval ``[-1,1]``, use `numpy.arccos`.
+
+    Examples
+    --------
+    >>> np.set_printoptions(precision=4)
+
+    >>> np.emath.arccos(1) # a scalar is returned
+    0.0
+
+    >>> np.emath.arccos([1,2])
+    array([0.-0.j   , 0.-1.317j])
+
+    """
     x = _fix_real_abs_gt_1(x)
-    return arccos(x)
-
+    return nx.arccos(x)
+
+
+@array_function_dispatch(_unary_dispatcher)
 def arcsin(x):
+    """
+    Compute the inverse sine of x.
+
+    Return the "principal value" (for a description of this, see
+    `numpy.arcsin`) of the inverse sine of `x`. For real `x` such that
+    `abs(x) <= 1`, this is a real number in the closed interval
+    :math:`[-\\pi/2, \\pi/2]`.  Otherwise, the complex principle value is
+    returned.
+
+    Parameters
+    ----------
+    x : array_like or scalar
+       The value(s) whose arcsin is (are) required.
+
+    Returns
+    -------
+    out : ndarray or scalar
+       The inverse sine(s) of the `x` value(s). If `x` was a scalar, so
+       is `out`, otherwise an array object is returned.
+
+    See Also
+    --------
+    numpy.arcsin
+
+    Notes
+    -----
+    For an arcsin() that returns ``NAN`` when real `x` is not in the
+    interval ``[-1,1]``, use `numpy.arcsin`.
+
+    Examples
+    --------
+    >>> np.set_printoptions(precision=4)
+
+    >>> np.emath.arcsin(0)
+    0.0
+
+    >>> np.emath.arcsin([0,1])
+    array([0.    , 1.5708])
+
+    """
     x = _fix_real_abs_gt_1(x)
-    return arcsin(x)
-
+    return nx.arcsin(x)
+
+
+@array_function_dispatch(_unary_dispatcher)
 def arctanh(x):
+    """
+    Compute the inverse hyperbolic tangent of `x`.
+
+    Return the "principal value" (for a description of this, see
+    `numpy.arctanh`) of ``arctanh(x)``. For real `x` such that
+    ``abs(x) < 1``, this is a real number.  If `abs(x) > 1`, or if `x` is
+    complex, the result is complex. Finally, `x = 1` returns``inf`` and
+    ``x=-1`` returns ``-inf``.
+
+    Parameters
+    ----------
+    x : array_like
+       The value(s) whose arctanh is (are) required.
+
+    Returns
+    -------
+    out : ndarray or scalar
+       The inverse hyperbolic tangent(s) of the `x` value(s). If `x` was
+       a scalar so is `out`, otherwise an array is returned.
+
+
+    See Also
+    --------
+    numpy.arctanh
+
+    Notes
+    -----
+    For an arctanh() that returns ``NAN`` when real `x` is not in the
+    interval ``(-1,1)``, use `numpy.arctanh` (this latter, however, does
+    return +/-inf for ``x = +/-1``).
+
+    Examples
+    --------
+    >>> np.set_printoptions(precision=4)
+
+    >>> from numpy.testing import suppress_warnings
+    >>> with suppress_warnings() as sup:
+    ...     sup.filter(RuntimeWarning)
+    ...     np.emath.arctanh(np.eye(2))
+    array([[inf,  0.],
+           [ 0., inf]])
+    >>> np.emath.arctanh([1j])
+    array([0.+0.7854j])
+
+    """
     x = _fix_real_abs_gt_1(x)
-    return arctanh(x)
+    return nx.arctanh(x)
('numpy/lib', '__init__.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,36 +1,61 @@
+"""
+**Note:** almost all functions in the ``numpy.lib`` namespace
+are also present in the main ``numpy`` namespace.  Please use the
+functions as ``np.<funcname>`` where possible.
 
-from info import __doc__
+``numpy.lib`` is mostly a space for implementing functions that don't
+belong in core or in another NumPy submodule with a clear purpose
+(e.g. ``random``, ``fft``, ``linalg``, ``ma``).
+
+Most contains basic functions that are used by several submodules and are
+useful to have in the main name-space.
+
+"""
+import math
+
 from numpy.version import version as __version__
 
-from type_check import *
-from index_tricks import *
-from function_base import *
-from shape_base import *
-from twodim_base import *
-from ufunclike import *
+# Public submodules
+# Note: recfunctions and (maybe) format are public too, but not imported
+from . import mixins
+from . import scimath as emath
 
-import scimath as emath
-from polynomial import *
-from machar import *
-from getlimits import *
-#import convertcode
-from utils import *
-from arraysetops import *
-import math
+# Private submodules
+from .type_check import *
+from .index_tricks import *
+from .function_base import *
+from .nanfunctions import *
+from .shape_base import *
+from .stride_tricks import *
+from .twodim_base import *
+from .ufunclike import *
+from .histograms import *
 
-__all__ = ['emath','math']
+from .polynomial import *
+from .utils import *
+from .arraysetops import *
+from .npyio import *
+from .arrayterator import Arrayterator
+from .arraypad import *
+from ._version import *
+from numpy.core._multiarray_umath import tracemalloc_domain
+
+__all__ = ['emath', 'math', 'tracemalloc_domain', 'Arrayterator']
 __all__ += type_check.__all__
 __all__ += index_tricks.__all__
 __all__ += function_base.__all__
 __all__ += shape_base.__all__
+__all__ += stride_tricks.__all__
 __all__ += twodim_base.__all__
 __all__ += ufunclike.__all__
+__all__ += arraypad.__all__
 __all__ += polynomial.__all__
-__all__ += machar.__all__
-__all__ += getlimits.__all__
 __all__ += utils.__all__
 __all__ += arraysetops.__all__
+__all__ += npyio.__all__
+__all__ += nanfunctions.__all__
+__all__ += histograms.__all__
 
-def test(level=1, verbosity=1):
-    from numpy.testing import NumpyTest
-    return NumpyTest().test(level, verbosity)
+from numpy._pytesttester import PytestTester
+test = PytestTester(__name__)
+del PytestTester
('numpy/lib', 'twodim_base.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,122 +1,1133 @@
 """ Basic functions for manipulating 2d arrays
 
 """
-
-__all__ = ['diag','eye','fliplr','flipud','rot90','tri','triu','tril',
-           'vander']
-
-from numpy.core.numeric import asanyarray, int_, equal, subtract, arange, \
-     zeros, arange, greater_equal, multiply, ones, asarray
-
+import functools
+import operator
+
+from numpy.core.numeric import (
+    asanyarray, arange, zeros, greater_equal, multiply, ones,
+    asarray, where, int8, int16, int32, int64, intp, empty, promote_types,
+    diagonal, nonzero, indices
+    )
+from numpy.core.overrides import set_array_function_like_doc, set_module
+from numpy.core import overrides
+from numpy.core import iinfo
+from numpy.lib.stride_tricks import broadcast_to
+
+
+__all__ = [
+    'diag', 'diagflat', 'eye', 'fliplr', 'flipud', 'tri', 'triu',
+    'tril', 'vander', 'histogram2d', 'mask_indices', 'tril_indices',
+    'tril_indices_from', 'triu_indices', 'triu_indices_from', ]
+
+
+array_function_dispatch = functools.partial(
+    overrides.array_function_dispatch, module='numpy')
+
+
+i1 = iinfo(int8)
+i2 = iinfo(int16)
+i4 = iinfo(int32)
+
+
+def _min_int(low, high):
+    """ get small int that fits the range """
+    if high <= i1.max and low >= i1.min:
+        return int8
+    if high <= i2.max and low >= i2.min:
+        return int16
+    if high <= i4.max and low >= i4.min:
+        return int32
+    return int64
+
+
+def _flip_dispatcher(m):
+    return (m,)
+
+
+@array_function_dispatch(_flip_dispatcher)
 def fliplr(m):
-    """ returns an array m with the rows preserved and columns flipped
-        in the left/right direction.  Works on the first two dimensions of m.
+    """
+    Reverse the order of elements along axis 1 (left/right).
+
+    For a 2-D array, this flips the entries in each row in the left/right
+    direction. Columns are preserved, but appear in a different order than
+    before.
+
+    Parameters
+    ----------
+    m : array_like
+        Input array, must be at least 2-D.
+
+    Returns
+    -------
+    f : ndarray
+        A view of `m` with the columns reversed.  Since a view
+        is returned, this operation is :math:`\\mathcal O(1)`.
+
+    See Also
+    --------
+    flipud : Flip array in the up/down direction.
+    flip : Flip array in one or more dimensions.
+    rot90 : Rotate array counterclockwise.
+
+    Notes
+    -----
+    Equivalent to ``m[:,::-1]`` or ``np.flip(m, axis=1)``.
+    Requires the array to be at least 2-D.
+
+    Examples
+    --------
+    >>> A = np.diag([1.,2.,3.])
+    >>> A
+    array([[1.,  0.,  0.],
+           [0.,  2.,  0.],
+           [0.,  0.,  3.]])
+    >>> np.fliplr(A)
+    array([[0.,  0.,  1.],
+           [0.,  2.,  0.],
+           [3.,  0.,  0.]])
+
+    >>> A = np.random.randn(2,3,5)
+    >>> np.all(np.fliplr(A) == A[:,::-1,...])
+    True
+
     """
     m = asanyarray(m)
     if m.ndim < 2:
-        raise ValueError, "Input must be >= 2-d."
+        raise ValueError("Input must be >= 2-d.")
     return m[:, ::-1]
 
+
+@array_function_dispatch(_flip_dispatcher)
 def flipud(m):
-    """ returns an array with the columns preserved and rows flipped in
-        the up/down direction.  Works on the first dimension of m.
+    """
+    Reverse the order of elements along axis 0 (up/down).
+
+    For a 2-D array, this flips the entries in each column in the up/down
+    direction. Rows are preserved, but appear in a different order than before.
+
+    Parameters
+    ----------
+    m : array_like
+        Input array.
+
+    Returns
+    -------
+    out : array_like
+        A view of `m` with the rows reversed.  Since a view is
+        returned, this operation is :math:`\\mathcal O(1)`.
+
+    See Also
+    --------
+    fliplr : Flip array in the left/right direction.
+    flip : Flip array in one or more dimensions.
+    rot90 : Rotate array counterclockwise.
+
+    Notes
+    -----
+    Equivalent to ``m[::-1, ...]`` or ``np.flip(m, axis=0)``.
+    Requires the array to be at least 1-D.
+
+    Examples
+    --------
+    >>> A = np.diag([1.0, 2, 3])
+    >>> A
+    array([[1.,  0.,  0.],
+           [0.,  2.,  0.],
+           [0.,  0.,  3.]])
+    >>> np.flipud(A)
+    array([[0.,  0.,  3.],
+           [0.,  2.,  0.],
+           [1.,  0.,  0.]])
+
+    >>> A = np.random.randn(2,3,5)
+    >>> np.all(np.flipud(A) == A[::-1,...])
+    True
+
+    >>> np.flipud([1,2])
+    array([2, 1])
+
     """
     m = asanyarray(m)
     if m.ndim < 1:
-        raise ValueError, "Input must be >= 1-d."
-    return m[::-1,...]
-
-def rot90(m, k=1):
-    """ returns the array found by rotating m by k*90
-    degrees in the counterclockwise direction.  Works on the first two
-    dimensions of m.
+        raise ValueError("Input must be >= 1-d.")
+    return m[::-1, ...]
+
+
+def _eye_dispatcher(N, M=None, k=None, dtype=None, order=None, *, like=None):
+    return (like,)
+
+
+@set_array_function_like_doc
+@set_module('numpy')
+def eye(N, M=None, k=0, dtype=float, order='C', *, like=None):
+    """
+    Return a 2-D array with ones on the diagonal and zeros elsewhere.
+
+    Parameters
+    ----------
+    N : int
+      Number of rows in the output.
+    M : int, optional
+      Number of columns in the output. If None, defaults to `N`.
+    k : int, optional
+      Index of the diagonal: 0 (the default) refers to the main diagonal,
+      a positive value refers to an upper diagonal, and a negative value
+      to a lower diagonal.
+    dtype : data-type, optional
+      Data-type of the returned array.
+    order : {'C', 'F'}, optional
+        Whether the output should be stored in row-major (C-style) or
+        column-major (Fortran-style) order in memory.
+
+        .. versionadded:: 1.14.0
+    ${ARRAY_FUNCTION_LIKE}
+
+        .. versionadded:: 1.20.0
+
+    Returns
+    -------
+    I : ndarray of shape (N,M)
+      An array where all elements are equal to zero, except for the `k`-th
+      diagonal, whose values are equal to one.
+
+    See Also
+    --------
+    identity : (almost) equivalent function
+    diag : diagonal 2-D array from a 1-D array specified by the user.
+
+    Examples
+    --------
+    >>> np.eye(2, dtype=int)
+    array([[1, 0],
+           [0, 1]])
+    >>> np.eye(3, k=1)
+    array([[0.,  1.,  0.],
+           [0.,  0.,  1.],
+           [0.,  0.,  0.]])
+
+    """
+    if like is not None:
+        return _eye_with_like(N, M=M, k=k, dtype=dtype, order=order, like=like)
+    if M is None:
+        M = N
+    m = zeros((N, M), dtype=dtype, order=order)
+    if k >= M:
+        return m
+    # Ensure M and k are integers, so we don't get any surprise casting
+    # results in the expressions `M-k` and `M+1` used below.  This avoids
+    # a problem with inputs with type (for example) np.uint64.
+    M = operator.index(M)
+    k = operator.index(k)
+    if k >= 0:
+        i = k
+    else:
+        i = (-k) * M
+    m[:M-k].flat[i::M+1] = 1
+    return m
+
+
+_eye_with_like = array_function_dispatch(
+    _eye_dispatcher
+)(eye)
+
+
+def _diag_dispatcher(v, k=None):
+    return (v,)
+
+
+@array_function_dispatch(_diag_dispatcher)
+def diag(v, k=0):
+    """
+    Extract a diagonal or construct a diagonal array.
+
+    See the more detailed documentation for ``numpy.diagonal`` if you use this
+    function to extract a diagonal and wish to write to the resulting array;
+    whether it returns a copy or a view depends on what version of numpy you
+    are using.
+
+    Parameters
+    ----------
+    v : array_like
+        If `v` is a 2-D array, return a copy of its `k`-th diagonal.
+        If `v` is a 1-D array, return a 2-D array with `v` on the `k`-th
+        diagonal.
+    k : int, optional
+        Diagonal in question. The default is 0. Use `k>0` for diagonals
+        above the main diagonal, and `k<0` for diagonals below the main
+        diagonal.
+
+    Returns
+    -------
+    out : ndarray
+        The extracted diagonal or constructed diagonal array.
+
+    See Also
+    --------
+    diagonal : Return specified diagonals.
+    diagflat : Create a 2-D array with the flattened input as a diagonal.
+    trace : Sum along diagonals.
+    triu : Upper triangle of an array.
+    tril : Lower triangle of an array.
+
+    Examples
+    --------
+    >>> x = np.arange(9).reshape((3,3))
+    >>> x
+    array([[0, 1, 2],
+           [3, 4, 5],
+           [6, 7, 8]])
+
+    >>> np.diag(x)
+    array([0, 4, 8])
+    >>> np.diag(x, k=1)
+    array([1, 5])
+    >>> np.diag(x, k=-1)
+    array([3, 7])
+
+    >>> np.diag(np.diag(x))
+    array([[0, 0, 0],
+           [0, 4, 0],
+           [0, 0, 8]])
+
+    """
+    v = asanyarray(v)
+    s = v.shape
+    if len(s) == 1:
+        n = s[0]+abs(k)
+        res = zeros((n, n), v.dtype)
+        if k >= 0:
+            i = k
+        else:
+            i = (-k) * n
+        res[:n-k].flat[i::n+1] = v
+        return res
+    elif len(s) == 2:
+        return diagonal(v, k)
+    else:
+        raise ValueError("Input must be 1- or 2-d.")
+
+
+@array_function_dispatch(_diag_dispatcher)
+def diagflat(v, k=0):
+    """
+    Create a two-dimensional array with the flattened input as a diagonal.
+
+    Parameters
+    ----------
+    v : array_like
+        Input data, which is flattened and set as the `k`-th
+        diagonal of the output.
+    k : int, optional
+        Diagonal to set; 0, the default, corresponds to the "main" diagonal,
+        a positive (negative) `k` giving the number of the diagonal above
+        (below) the main.
+
+    Returns
+    -------
+    out : ndarray
+        The 2-D output array.
+
+    See Also
+    --------
+    diag : MATLAB work-alike for 1-D and 2-D arrays.
+    diagonal : Return specified diagonals.
+    trace : Sum along diagonals.
+
+    Examples
+    --------
+    >>> np.diagflat([[1,2], [3,4]])
+    array([[1, 0, 0, 0],
+           [0, 2, 0, 0],
+           [0, 0, 3, 0],
+           [0, 0, 0, 4]])
+
+    >>> np.diagflat([1,2], 1)
+    array([[0, 1, 0],
+           [0, 0, 2],
+           [0, 0, 0]])
+
+    """
+    try:
+        wrap = v.__array_wrap__
+    except AttributeError:
+        wrap = None
+    v = asarray(v).ravel()
+    s = len(v)
+    n = s + abs(k)
+    res = zeros((n, n), v.dtype)
+    if (k >= 0):
+        i = arange(0, n-k, dtype=intp)
+        fi = i+k+i*n
+    else:
+        i = arange(0, n+k, dtype=intp)
+        fi = i+(i-k)*n
+    res.flat[fi] = v
+    if not wrap:
+        return res
+    return wrap(res)
+
+
+def _tri_dispatcher(N, M=None, k=None, dtype=None, *, like=None):
+    return (like,)
+
+
+@set_array_function_like_doc
+@set_module('numpy')
+def tri(N, M=None, k=0, dtype=float, *, like=None):
+    """
+    An array with ones at and below the given diagonal and zeros elsewhere.
+
+    Parameters
+    ----------
+    N : int
+        Number of rows in the array.
+    M : int, optional
+        Number of columns in the array.
+        By default, `M` is taken equal to `N`.
+    k : int, optional
+        The sub-diagonal at and below which the array is filled.
+        `k` = 0 is the main diagonal, while `k` < 0 is below it,
+        and `k` > 0 is above.  The default is 0.
+    dtype : dtype, optional
+        Data type of the returned array.  The default is float.
+    ${ARRAY_FUNCTION_LIKE}
+
+        .. versionadded:: 1.20.0
+
+    Returns
+    -------
+    tri : ndarray of shape (N, M)
+        Array with its lower triangle filled with ones and zero elsewhere;
+        in other words ``T[i,j] == 1`` for ``j <= i + k``, 0 otherwise.
+
+    Examples
+    --------
+    >>> np.tri(3, 5, 2, dtype=int)
+    array([[1, 1, 1, 0, 0],
+           [1, 1, 1, 1, 0],
+           [1, 1, 1, 1, 1]])
+
+    >>> np.tri(3, 5, -1)
+    array([[0.,  0.,  0.,  0.,  0.],
+           [1.,  0.,  0.,  0.,  0.],
+           [1.,  1.,  0.,  0.,  0.]])
+
+    """
+    if like is not None:
+        return _tri_with_like(N, M=M, k=k, dtype=dtype, like=like)
+
+    if M is None:
+        M = N
+
+    m = greater_equal.outer(arange(N, dtype=_min_int(0, N)),
+                            arange(-k, M-k, dtype=_min_int(-k, M - k)))
+
+    # Avoid making a copy if the requested type is already bool
+    m = m.astype(dtype, copy=False)
+
+    return m
+
+
+_tri_with_like = array_function_dispatch(
+    _tri_dispatcher
+)(tri)
+
+
+def _trilu_dispatcher(m, k=None):
+    return (m,)
+
+
+@array_function_dispatch(_trilu_dispatcher)
+def tril(m, k=0):
+    """
+    Lower triangle of an array.
+
+    Return a copy of an array with elements above the `k`-th diagonal zeroed.
+    For arrays with ``ndim`` exceeding 2, `tril` will apply to the final two
+    axes.
+
+    Parameters
+    ----------
+    m : array_like, shape (..., M, N)
+        Input array.
+    k : int, optional
+        Diagonal above which to zero elements.  `k = 0` (the default) is the
+        main diagonal, `k < 0` is below it and `k > 0` is above.
+
+    Returns
+    -------
+    tril : ndarray, shape (..., M, N)
+        Lower triangle of `m`, of same shape and data-type as `m`.
+
+    See Also
+    --------
+    triu : same thing, only for the upper triangle
+
+    Examples
+    --------
+    >>> np.tril([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], -1)
+    array([[ 0,  0,  0],
+           [ 4,  0,  0],
+           [ 7,  8,  0],
+           [10, 11, 12]])
+
+    >>> np.tril(np.arange(3*4*5).reshape(3, 4, 5))
+    array([[[ 0,  0,  0,  0,  0],
+            [ 5,  6,  0,  0,  0],
+            [10, 11, 12,  0,  0],
+            [15, 16, 17, 18,  0]],
+           [[20,  0,  0,  0,  0],
+            [25, 26,  0,  0,  0],
+            [30, 31, 32,  0,  0],
+            [35, 36, 37, 38,  0]],
+           [[40,  0,  0,  0,  0],
+            [45, 46,  0,  0,  0],
+            [50, 51, 52,  0,  0],
+            [55, 56, 57, 58,  0]]])
+
     """
     m = asanyarray(m)
-    if m.ndim < 2:
-        raise ValueError, "Input must >= 2-d."
-    k = k % 4
-    if k == 0: return m
-    elif k == 1: return fliplr(m).transpose()
-    elif k == 2: return fliplr(flipud(m))
-    else: return fliplr(m.transpose())  # k==3
-
-def eye(N, M=None, k=0, dtype=int_):
-    """ eye returns a N-by-M 2-d array where the  k-th diagonal is all ones,
-        and everything else is zeros.
-    """
-    if M is None: M = N
-    m = equal(subtract.outer(arange(N), arange(M)),-k)
-    return m.astype(dtype)
-
-def diag(v, k=0):
-    """ returns the k-th diagonal if v is a array or returns a array
-        with v as the k-th diagonal if v is a vector.
-    """
-    v = asarray(v)
-    s = v.shape
-    if len(s)==1:
-        n = s[0]+abs(k)
-        res = zeros((n,n), v.dtype)
-        if (k>=0):
-            i = arange(0,n-k)
-            fi = i+k+i*n
-        else:
-            i = arange(0,n+k)
-            fi = i+(i-k)*n
-        res.flat[fi] = v
-        return res
-    elif len(s)==2:
-        N1,N2 = s
-        if k >= 0:
-            M = min(N1,N2-k)
-            i = arange(0,M)
-            fi = i+k+i*N2
-        else:
-            M = min(N1+k,N2)
-            i = arange(0,M)
-            fi = i + (i-k)*N2
-        return v.flat[fi]
+    mask = tri(*m.shape[-2:], k=k, dtype=bool)
+
+    return where(mask, m, zeros(1, m.dtype))
+
+
+@array_function_dispatch(_trilu_dispatcher)
+def triu(m, k=0):
+    """
+    Upper triangle of an array.
+
+    Return a copy of an array with the elements below the `k`-th diagonal
+    zeroed. For arrays with ``ndim`` exceeding 2, `triu` will apply to the
+    final two axes.
+
+    Please refer to the documentation for `tril` for further details.
+
+    See Also
+    --------
+    tril : lower triangle of an array
+
+    Examples
+    --------
+    >>> np.triu([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], -1)
+    array([[ 1,  2,  3],
+           [ 4,  5,  6],
+           [ 0,  8,  9],
+           [ 0,  0, 12]])
+
+    >>> np.triu(np.arange(3*4*5).reshape(3, 4, 5))
+    array([[[ 0,  1,  2,  3,  4],
+            [ 0,  6,  7,  8,  9],
+            [ 0,  0, 12, 13, 14],
+            [ 0,  0,  0, 18, 19]],
+           [[20, 21, 22, 23, 24],
+            [ 0, 26, 27, 28, 29],
+            [ 0,  0, 32, 33, 34],
+            [ 0,  0,  0, 38, 39]],
+           [[40, 41, 42, 43, 44],
+            [ 0, 46, 47, 48, 49],
+            [ 0,  0, 52, 53, 54],
+            [ 0,  0,  0, 58, 59]]])
+
+    """
+    m = asanyarray(m)
+    mask = tri(*m.shape[-2:], k=k-1, dtype=bool)
+
+    return where(mask, zeros(1, m.dtype), m)
+
+
+def _vander_dispatcher(x, N=None, increasing=None):
+    return (x,)
+
+
+# Originally borrowed from John Hunter and matplotlib
+@array_function_dispatch(_vander_dispatcher)
+def vander(x, N=None, increasing=False):
+    """
+    Generate a Vandermonde matrix.
+
+    The columns of the output matrix are powers of the input vector. The
+    order of the powers is determined by the `increasing` boolean argument.
+    Specifically, when `increasing` is False, the `i`-th output column is
+    the input vector raised element-wise to the power of ``N - i - 1``. Such
+    a matrix with a geometric progression in each row is named for Alexandre-
+    Theophile Vandermonde.
+
+    Parameters
+    ----------
+    x : array_like
+        1-D input array.
+    N : int, optional
+        Number of columns in the output.  If `N` is not specified, a square
+        array is returned (``N = len(x)``).
+    increasing : bool, optional
+        Order of the powers of the columns.  If True, the powers increase
+        from left to right, if False (the default) they are reversed.
+
+        .. versionadded:: 1.9.0
+
+    Returns
+    -------
+    out : ndarray
+        Vandermonde matrix.  If `increasing` is False, the first column is
+        ``x^(N-1)``, the second ``x^(N-2)`` and so forth. If `increasing` is
+        True, the columns are ``x^0, x^1, ..., x^(N-1)``.
+
+    See Also
+    --------
+    polynomial.polynomial.polyvander
+
+    Examples
+    --------
+    >>> x = np.array([1, 2, 3, 5])
+    >>> N = 3
+    >>> np.vander(x, N)
+    array([[ 1,  1,  1],
+           [ 4,  2,  1],
+           [ 9,  3,  1],
+           [25,  5,  1]])
+
+    >>> np.column_stack([x**(N-1-i) for i in range(N)])
+    array([[ 1,  1,  1],
+           [ 4,  2,  1],
+           [ 9,  3,  1],
+           [25,  5,  1]])
+
+    >>> x = np.array([1, 2, 3, 5])
+    >>> np.vander(x)
+    array([[  1,   1,   1,   1],
+           [  8,   4,   2,   1],
+           [ 27,   9,   3,   1],
+           [125,  25,   5,   1]])
+    >>> np.vander(x, increasing=True)
+    array([[  1,   1,   1,   1],
+           [  1,   2,   4,   8],
+           [  1,   3,   9,  27],
+           [  1,   5,  25, 125]])
+
+    The determinant of a square Vandermonde matrix is the product
+    of the differences between the values of the input vector:
+
+    >>> np.linalg.det(np.vander(x))
+    48.000000000000043 # may vary
+    >>> (5-3)*(5-2)*(5-1)*(3-2)*(3-1)*(2-1)
+    48
+
+    """
+    x = asarray(x)
+    if x.ndim != 1:
+        raise ValueError("x must be a one-dimensional array or sequence.")
+    if N is None:
+        N = len(x)
+
+    v = empty((len(x), N), dtype=promote_types(x.dtype, int))
+    tmp = v[:, ::-1] if not increasing else v
+
+    if N > 0:
+        tmp[:, 0] = 1
+    if N > 1:
+        tmp[:, 1:] = x[:, None]
+        multiply.accumulate(tmp[:, 1:], out=tmp[:, 1:], axis=1)
+
+    return v
+
+
+def _histogram2d_dispatcher(x, y, bins=None, range=None, normed=None,
+                            weights=None, density=None):
+    yield x
+    yield y
+
+    # This terrible logic is adapted from the checks in histogram2d
+    try:
+        N = len(bins)
+    except TypeError:
+        N = 1
+    if N == 2:
+        yield from bins  # bins=[x, y]
     else:
-        raise ValueError, "Input must be 1- or 2-d."
-
-
-def tri(N, M=None, k=0, dtype=int_):
-    """ returns a N-by-M array where all the diagonals starting from
-        lower left corner up to the k-th are all ones.
-    """
-    if M is None: M = N
-    m = greater_equal(subtract.outer(arange(N), arange(M)),-k)
-    return m.astype(dtype)
-
-def tril(m, k=0):
-    """ returns the elements on and below the k-th diagonal of m.  k=0 is the
-        main diagonal, k > 0 is above and k < 0 is below the main diagonal.
-    """
-    m = asanyarray(m)
-    out = multiply(tri(m.shape[0], m.shape[1], k=k, dtype=m.dtype),m)
-    return out
-
-def triu(m, k=0):
-    """ returns the elements on and above the k-th diagonal of m.  k=0 is the
-        main diagonal, k > 0 is above and k < 0 is below the main diagonal.
-    """
-    m = asanyarray(m)
-    out = multiply((1-tri(m.shape[0], m.shape[1], k-1, m.dtype)),m)
-    return out
-
-# borrowed from John Hunter and matplotlib
-def vander(x, N=None):
-    """
-    X = vander(x,N=None)
-
-    The Vandermonde matrix of vector x.  The i-th column of X is the
-    the i-th power of x.  N is the maximum power to compute; if N is
-    None it defaults to len(x).
-
-    """
-    x = asarray(x)
-    if N is None: N=len(x)
-    X = ones( (len(x),N), x.dtype)
-    for i in range(N-1):
-        X[:,i] = x**(N-i-1)
-    return X
+        yield bins
+
+    yield weights
+
+
+@array_function_dispatch(_histogram2d_dispatcher)
+def histogram2d(x, y, bins=10, range=None, normed=None, weights=None,
+                density=None):
+    """
+    Compute the bi-dimensional histogram of two data samples.
+
+    Parameters
+    ----------
+    x : array_like, shape (N,)
+        An array containing the x coordinates of the points to be
+        histogrammed.
+    y : array_like, shape (N,)
+        An array containing the y coordinates of the points to be
+        histogrammed.
+    bins : int or array_like or [int, int] or [array, array], optional
+        The bin specification:
+
+          * If int, the number of bins for the two dimensions (nx=ny=bins).
+          * If array_like, the bin edges for the two dimensions
+            (x_edges=y_edges=bins).
+          * If [int, int], the number of bins in each dimension
+            (nx, ny = bins).
+          * If [array, array], the bin edges in each dimension
+            (x_edges, y_edges = bins).
+          * A combination [int, array] or [array, int], where int
+            is the number of bins and array is the bin edges.
+
+    range : array_like, shape(2,2), optional
+        The leftmost and rightmost edges of the bins along each dimension
+        (if not specified explicitly in the `bins` parameters):
+        ``[[xmin, xmax], [ymin, ymax]]``. All values outside of this range
+        will be considered outliers and not tallied in the histogram.
+    density : bool, optional
+        If False, the default, returns the number of samples in each bin.
+        If True, returns the probability *density* function at the bin,
+        ``bin_count / sample_count / bin_area``.
+    normed : bool, optional
+        An alias for the density argument that behaves identically. To avoid
+        confusion with the broken normed argument to `histogram`, `density`
+        should be preferred.
+    weights : array_like, shape(N,), optional
+        An array of values ``w_i`` weighing each sample ``(x_i, y_i)``.
+        Weights are normalized to 1 if `normed` is True. If `normed` is
+        False, the values of the returned histogram are equal to the sum of
+        the weights belonging to the samples falling into each bin.
+
+    Returns
+    -------
+    H : ndarray, shape(nx, ny)
+        The bi-dimensional histogram of samples `x` and `y`. Values in `x`
+        are histogrammed along the first dimension and values in `y` are
+        histogrammed along the second dimension.
+    xedges : ndarray, shape(nx+1,)
+        The bin edges along the first dimension.
+    yedges : ndarray, shape(ny+1,)
+        The bin edges along the second dimension.
+
+    See Also
+    --------
+    histogram : 1D histogram
+    histogramdd : Multidimensional histogram
+
+    Notes
+    -----
+    When `normed` is True, then the returned histogram is the sample
+    density, defined such that the sum over bins of the product
+    ``bin_value * bin_area`` is 1.
+
+    Please note that the histogram does not follow the Cartesian convention
+    where `x` values are on the abscissa and `y` values on the ordinate
+    axis.  Rather, `x` is histogrammed along the first dimension of the
+    array (vertical), and `y` along the second dimension of the array
+    (horizontal).  This ensures compatibility with `histogramdd`.
+
+    Examples
+    --------
+    >>> from matplotlib.image import NonUniformImage
+    >>> import matplotlib.pyplot as plt
+
+    Construct a 2-D histogram with variable bin width. First define the bin
+    edges:
+
+    >>> xedges = [0, 1, 3, 5]
+    >>> yedges = [0, 2, 3, 4, 6]
+
+    Next we create a histogram H with random bin content:
+
+    >>> x = np.random.normal(2, 1, 100)
+    >>> y = np.random.normal(1, 1, 100)
+    >>> H, xedges, yedges = np.histogram2d(x, y, bins=(xedges, yedges))
+    >>> # Histogram does not follow Cartesian convention (see Notes),
+    >>> # therefore transpose H for visualization purposes.
+    >>> H = H.T
+
+    :func:`imshow <matplotlib.pyplot.imshow>` can only display square bins:
+
+    >>> fig = plt.figure(figsize=(7, 3))
+    >>> ax = fig.add_subplot(131, title='imshow: square bins')
+    >>> plt.imshow(H, interpolation='nearest', origin='lower',
+    ...         extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])
+    <matplotlib.image.AxesImage object at 0x...>
+
+    :func:`pcolormesh <matplotlib.pyplot.pcolormesh>` can display actual edges:
+
+    >>> ax = fig.add_subplot(132, title='pcolormesh: actual edges',
+    ...         aspect='equal')
+    >>> X, Y = np.meshgrid(xedges, yedges)
+    >>> ax.pcolormesh(X, Y, H)
+    <matplotlib.collections.QuadMesh object at 0x...>
+
+    :class:`NonUniformImage <matplotlib.image.NonUniformImage>` can be used to
+    display actual bin edges with interpolation:
+
+    >>> ax = fig.add_subplot(133, title='NonUniformImage: interpolated',
+    ...         aspect='equal', xlim=xedges[[0, -1]], ylim=yedges[[0, -1]])
+    >>> im = NonUniformImage(ax, interpolation='bilinear')
+    >>> xcenters = (xedges[:-1] + xedges[1:]) / 2
+    >>> ycenters = (yedges[:-1] + yedges[1:]) / 2
+    >>> im.set_data(xcenters, ycenters, H)
+    >>> ax.images.append(im)
+    >>> plt.show()
+
+    It is also possible to construct a 2-D histogram without specifying bin
+    edges:
+
+    >>> # Generate non-symmetric test data
+    >>> n = 10000
+    >>> x = np.linspace(1, 100, n)
+    >>> y = 2*np.log(x) + np.random.rand(n) - 0.5
+    >>> # Compute 2d histogram. Note the order of x/y and xedges/yedges
+    >>> H, yedges, xedges = np.histogram2d(y, x, bins=20)
+
+    Now we can plot the histogram using
+    :func:`pcolormesh <matplotlib.pyplot.pcolormesh>`, and a
+    :func:`hexbin <matplotlib.pyplot.hexbin>` for comparison.
+
+    >>> # Plot histogram using pcolormesh
+    >>> fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)
+    >>> ax1.pcolormesh(xedges, yedges, H, cmap='rainbow')
+    >>> ax1.plot(x, 2*np.log(x), 'k-')
+    >>> ax1.set_xlim(x.min(), x.max())
+    >>> ax1.set_ylim(y.min(), y.max())
+    >>> ax1.set_xlabel('x')
+    >>> ax1.set_ylabel('y')
+    >>> ax1.set_title('histogram2d')
+    >>> ax1.grid()
+
+    >>> # Create hexbin plot for comparison
+    >>> ax2.hexbin(x, y, gridsize=20, cmap='rainbow')
+    >>> ax2.plot(x, 2*np.log(x), 'k-')
+    >>> ax2.set_title('hexbin')
+    >>> ax2.set_xlim(x.min(), x.max())
+    >>> ax2.set_xlabel('x')
+    >>> ax2.grid()
+
+    >>> plt.show()
+    """
+    from numpy import histogramdd
+
+    if len(x) != len(y):
+        raise ValueError('x and y must have the same length.')
+
+    try:
+        N = len(bins)
+    except TypeError:
+        N = 1
+
+    if N != 1 and N != 2:
+        xedges = yedges = asarray(bins)
+        bins = [xedges, yedges]
+    hist, edges = histogramdd([x, y], bins, range, normed, weights, density)
+    return hist, edges[0], edges[1]
+
+
+@set_module('numpy')
+def mask_indices(n, mask_func, k=0):
+    """
+    Return the indices to access (n, n) arrays, given a masking function.
+
+    Assume `mask_func` is a function that, for a square array a of size
+    ``(n, n)`` with a possible offset argument `k`, when called as
+    ``mask_func(a, k)`` returns a new array with zeros in certain locations
+    (functions like `triu` or `tril` do precisely this). Then this function
+    returns the indices where the non-zero values would be located.
+
+    Parameters
+    ----------
+    n : int
+        The returned indices will be valid to access arrays of shape (n, n).
+    mask_func : callable
+        A function whose call signature is similar to that of `triu`, `tril`.
+        That is, ``mask_func(x, k)`` returns a boolean array, shaped like `x`.
+        `k` is an optional argument to the function.
+    k : scalar
+        An optional argument which is passed through to `mask_func`. Functions
+        like `triu`, `tril` take a second argument that is interpreted as an
+        offset.
+
+    Returns
+    -------
+    indices : tuple of arrays.
+        The `n` arrays of indices corresponding to the locations where
+        ``mask_func(np.ones((n, n)), k)`` is True.
+
+    See Also
+    --------
+    triu, tril, triu_indices, tril_indices
+
+    Notes
+    -----
+    .. versionadded:: 1.4.0
+
+    Examples
+    --------
+    These are the indices that would allow you to access the upper triangular
+    part of any 3x3 array:
+
+    >>> iu = np.mask_indices(3, np.triu)
+
+    For example, if `a` is a 3x3 array:
+
+    >>> a = np.arange(9).reshape(3, 3)
+    >>> a
+    array([[0, 1, 2],
+           [3, 4, 5],
+           [6, 7, 8]])
+    >>> a[iu]
+    array([0, 1, 2, 4, 5, 8])
+
+    An offset can be passed also to the masking function.  This gets us the
+    indices starting on the first diagonal right of the main one:
+
+    >>> iu1 = np.mask_indices(3, np.triu, 1)
+
+    with which we now extract only three elements:
+
+    >>> a[iu1]
+    array([1, 2, 5])
+
+    """
+    m = ones((n, n), int)
+    a = mask_func(m, k)
+    return nonzero(a != 0)
+
+
+@set_module('numpy')
+def tril_indices(n, k=0, m=None):
+    """
+    Return the indices for the lower-triangle of an (n, m) array.
+
+    Parameters
+    ----------
+    n : int
+        The row dimension of the arrays for which the returned
+        indices will be valid.
+    k : int, optional
+        Diagonal offset (see `tril` for details).
+    m : int, optional
+        .. versionadded:: 1.9.0
+
+        The column dimension of the arrays for which the returned
+        arrays will be valid.
+        By default `m` is taken equal to `n`.
+
+
+    Returns
+    -------
+    inds : tuple of arrays
+        The indices for the triangle. The returned tuple contains two arrays,
+        each with the indices along one dimension of the array.
+
+    See also
+    --------
+    triu_indices : similar function, for upper-triangular.
+    mask_indices : generic function accepting an arbitrary mask function.
+    tril, triu
+
+    Notes
+    -----
+    .. versionadded:: 1.4.0
+
+    Examples
+    --------
+    Compute two different sets of indices to access 4x4 arrays, one for the
+    lower triangular part starting at the main diagonal, and one starting two
+    diagonals further right:
+
+    >>> il1 = np.tril_indices(4)
+    >>> il2 = np.tril_indices(4, 2)
+
+    Here is how they can be used with a sample array:
+
+    >>> a = np.arange(16).reshape(4, 4)
+    >>> a
+    array([[ 0,  1,  2,  3],
+           [ 4,  5,  6,  7],
+           [ 8,  9, 10, 11],
+           [12, 13, 14, 15]])
+
+    Both for indexing:
+
+    >>> a[il1]
+    array([ 0,  4,  5, ..., 13, 14, 15])
+
+    And for assigning values:
+
+    >>> a[il1] = -1
+    >>> a
+    array([[-1,  1,  2,  3],
+           [-1, -1,  6,  7],
+           [-1, -1, -1, 11],
+           [-1, -1, -1, -1]])
+
+    These cover almost the whole array (two diagonals right of the main one):
+
+    >>> a[il2] = -10
+    >>> a
+    array([[-10, -10, -10,   3],
+           [-10, -10, -10, -10],
+           [-10, -10, -10, -10],
+           [-10, -10, -10, -10]])
+
+    """
+    tri_ = tri(n, m, k=k, dtype=bool)
+
+    return tuple(broadcast_to(inds, tri_.shape)[tri_]
+                 for inds in indices(tri_.shape, sparse=True))
+
+
+def _trilu_indices_form_dispatcher(arr, k=None):
+    return (arr,)
+
+
+@array_function_dispatch(_trilu_indices_form_dispatcher)
+def tril_indices_from(arr, k=0):
+    """
+    Return the indices for the lower-triangle of arr.
+
+    See `tril_indices` for full details.
+
+    Parameters
+    ----------
+    arr : array_like
+        The indices will be valid for square arrays whose dimensions are
+        the same as arr.
+    k : int, optional
+        Diagonal offset (see `tril` for details).
+
+    See Also
+    --------
+    tril_indices, tril
+
+    Notes
+    -----
+    .. versionadded:: 1.4.0
+
+    """
+    if arr.ndim != 2:
+        raise ValueError("input array must be 2-d")
+    return tril_indices(arr.shape[-2], k=k, m=arr.shape[-1])
+
+
+@set_module('numpy')
+def triu_indices(n, k=0, m=None):
+    """
+    Return the indices for the upper-triangle of an (n, m) array.
+
+    Parameters
+    ----------
+    n : int
+        The size of the arrays for which the returned indices will
+        be valid.
+    k : int, optional
+        Diagonal offset (see `triu` for details).
+    m : int, optional
+        .. versionadded:: 1.9.0
+
+        The column dimension of the arrays for which the returned
+        arrays will be valid.
+        By default `m` is taken equal to `n`.
+
+
+    Returns
+    -------
+    inds : tuple, shape(2) of ndarrays, shape(`n`)
+        The indices for the triangle. The returned tuple contains two arrays,
+        each with the indices along one dimension of the array.  Can be used
+        to slice a ndarray of shape(`n`, `n`).
+
+    See also
+    --------
+    tril_indices : similar function, for lower-triangular.
+    mask_indices : generic function accepting an arbitrary mask function.
+    triu, tril
+
+    Notes
+    -----
+    .. versionadded:: 1.4.0
+
+    Examples
+    --------
+    Compute two different sets of indices to access 4x4 arrays, one for the
+    upper triangular part starting at the main diagonal, and one starting two
+    diagonals further right:
+
+    >>> iu1 = np.triu_indices(4)
+    >>> iu2 = np.triu_indices(4, 2)
+
+    Here is how they can be used with a sample array:
+
+    >>> a = np.arange(16).reshape(4, 4)
+    >>> a
+    array([[ 0,  1,  2,  3],
+           [ 4,  5,  6,  7],
+           [ 8,  9, 10, 11],
+           [12, 13, 14, 15]])
+
+    Both for indexing:
+
+    >>> a[iu1]
+    array([ 0,  1,  2, ..., 10, 11, 15])
+
+    And for assigning values:
+
+    >>> a[iu1] = -1
+    >>> a
+    array([[-1, -1, -1, -1],
+           [ 4, -1, -1, -1],
+           [ 8,  9, -1, -1],
+           [12, 13, 14, -1]])
+
+    These cover only a small part of the whole array (two diagonals right
+    of the main one):
+
+    >>> a[iu2] = -10
+    >>> a
+    array([[ -1,  -1, -10, -10],
+           [  4,  -1,  -1, -10],
+           [  8,   9,  -1,  -1],
+           [ 12,  13,  14,  -1]])
+
+    """
+    tri_ = ~tri(n, m, k=k - 1, dtype=bool)
+
+    return tuple(broadcast_to(inds, tri_.shape)[tri_]
+                 for inds in indices(tri_.shape, sparse=True))
+
+
+@array_function_dispatch(_trilu_indices_form_dispatcher)
+def triu_indices_from(arr, k=0):
+    """
+    Return the indices for the upper-triangle of arr.
+
+    See `triu_indices` for full details.
+
+    Parameters
+    ----------
+    arr : ndarray, shape(N, N)
+        The indices will be valid for square arrays.
+    k : int, optional
+        Diagonal offset (see `triu` for details).
+
+    Returns
+    -------
+    triu_indices_from : tuple, shape(2) of ndarray, shape(N)
+        Indices for the upper-triangle of `arr`.
+
+    See Also
+    --------
+    triu_indices, triu
+
+    Notes
+    -----
+    .. versionadded:: 1.4.0
+
+    """
+    if arr.ndim != 2:
+        raise ValueError("input array must be 2-d")
+    return triu_indices(arr.shape[-2], k=k, m=arr.shape[-1])
('numpy/lib', 'index_tricks.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,263 +1,422 @@
-## Automatically adapted for numpy Sep 19, 2005 by convertcode.py
-
-__all__ = ['unravel_index',
-           'mgrid',
-           'ogrid',
-           'r_', 'c_', 's_',
-           'index_exp', 'ix_',
-           'ndenumerate','ndindex']
-
+import functools
 import sys
-import types
+import math
+import warnings
+
 import numpy.core.numeric as _nx
-from numpy.core.numeric import asarray, ScalarType
-
-import function_base
-import numpy.core.defmatrix as matrix
-makemat = matrix.matrix
-
-# contributed by Stefan van der Walt
-def unravel_index(x,dims):
-    """Convert a flat index into an index tuple for a matrix of given shape.
-
-    e.g. for a 2x2 matrix, unravel_index(2,(2,2)) returns (1,0).
-
-    Example usage:
-      p = x.argmax()
-      idx = unravel_index(p)
-      x[idx] == x.max()
-
-    Note:  x.flat[p] == x.max()
-
-      Thus, it may be easier to use flattened indexing than to re-map
-      the index to a tuple.
-    """
-    if x > _nx.prod(dims)-1 or x < 0:
-        raise ValueError("Invalid index, must be 0 <= x <= number of elements.")
-        
-    idx = _nx.empty_like(dims)
-
-    # Take dimensions
-    # [a,b,c,d]
-    # Reverse and drop first element
-    # [d,c,b]
-    # Prepend [1]
-    # [1,d,c,b]
-    # Calculate cumulative product
-    # [1,d,dc,dcb]
-    # Reverse
-    # [dcb,dc,d,1]
-    dim_prod = _nx.cumprod([1] + list(dims)[:0:-1])[::-1]
-    # Indeces become [x/dcb % a, x/dc % b, x/d % c, x/1 % d]
-    return tuple(x/dim_prod % dims)
-
+from numpy.core.numeric import (
+    asarray, ScalarType, array, alltrue, cumprod, arange, ndim
+)
+from numpy.core.numerictypes import find_common_type, issubdtype
+
+import numpy.matrixlib as matrixlib
+from .function_base import diff
+from numpy.core.multiarray import ravel_multi_index, unravel_index
+from numpy.core.overrides import set_module
+from numpy.core import overrides, linspace
+from numpy.lib.stride_tricks import as_strided
+
+
+array_function_dispatch = functools.partial(
+    overrides.array_function_dispatch, module='numpy')
+
+
+__all__ = [
+    'ravel_multi_index', 'unravel_index', 'mgrid', 'ogrid', 'r_', 'c_',
+    's_', 'index_exp', 'ix_', 'ndenumerate', 'ndindex', 'fill_diagonal',
+    'diag_indices', 'diag_indices_from'
+]
+
+
+def _ix__dispatcher(*args):
+    return args
+
+
+@array_function_dispatch(_ix__dispatcher)
 def ix_(*args):
-    """ Construct an open mesh from multiple sequences.
-
-    This function takes n 1-d sequences and returns n outputs with n
-    dimensions each such that the shape is 1 in all but one dimension and
-    the dimension with the non-unit shape value cycles through all n
-    dimensions.
-
-    Using ix_() one can quickly construct index arrays that will index
-    the cross product.
-
-    a[ix_([1,3,7],[2,5,8])]  returns the array
-
-    a[1,2]  a[1,5]  a[1,8]
-    a[3,2]  a[3,5]  a[3,8]
-    a[7,2]  a[7,5]  a[7,8]
+    """
+    Construct an open mesh from multiple sequences.
+
+    This function takes N 1-D sequences and returns N outputs with N
+    dimensions each, such that the shape is 1 in all but one dimension
+    and the dimension with the non-unit shape value cycles through all
+    N dimensions.
+
+    Using `ix_` one can quickly construct index arrays that will index
+    the cross product. ``a[np.ix_([1,3],[2,5])]`` returns the array
+    ``[[a[1,2] a[1,5]], [a[3,2] a[3,5]]]``.
+
+    Parameters
+    ----------
+    args : 1-D sequences
+        Each sequence should be of integer or boolean type.
+        Boolean sequences will be interpreted as boolean masks for the
+        corresponding dimension (equivalent to passing in
+        ``np.nonzero(boolean_sequence)``).
+
+    Returns
+    -------
+    out : tuple of ndarrays
+        N arrays with N dimensions each, with N the number of input
+        sequences. Together these arrays form an open mesh.
+
+    See Also
+    --------
+    ogrid, mgrid, meshgrid
+
+    Examples
+    --------
+    >>> a = np.arange(10).reshape(2, 5)
+    >>> a
+    array([[0, 1, 2, 3, 4],
+           [5, 6, 7, 8, 9]])
+    >>> ixgrid = np.ix_([0, 1], [2, 4])
+    >>> ixgrid
+    (array([[0],
+           [1]]), array([[2, 4]]))
+    >>> ixgrid[0].shape, ixgrid[1].shape
+    ((2, 1), (1, 2))
+    >>> a[ixgrid]
+    array([[2, 4],
+           [7, 9]])
+
+    >>> ixgrid = np.ix_([True, True], [2, 4])
+    >>> a[ixgrid]
+    array([[2, 4],
+           [7, 9]])
+    >>> ixgrid = np.ix_([True, True], [False, False, True, False, True])
+    >>> a[ixgrid]
+    array([[2, 4],
+           [7, 9]])
+
     """
     out = []
     nd = len(args)
-    baseshape = [1]*nd
-    for k in range(nd):
-        new = _nx.array(args[k])
-        if (new.ndim != 1):
-            raise ValueError, "Cross index must be 1 dimensional"
-        baseshape[k] = len(new)
-        new.shape = tuple(baseshape)
+    for k, new in enumerate(args):
+        if not isinstance(new, _nx.ndarray):
+            new = asarray(new)
+            if new.size == 0:
+                # Explicitly type empty arrays to avoid float default
+                new = new.astype(_nx.intp)
+        if new.ndim != 1:
+            raise ValueError("Cross index must be 1 dimensional")
+        if issubdtype(new.dtype, _nx.bool_):
+            new, = new.nonzero()
+        new = new.reshape((1,)*k + (new.size,) + (1,)*(nd-k-1))
         out.append(new)
-        baseshape[k] = 1
     return tuple(out)
 
-class nd_grid(object):
-    """ Construct a "meshgrid" in N-dimensions.
-
-        grid = nd_grid() creates an instance which will return a mesh-grid
-        when indexed.  The dimension and number of the output arrays are equal
-        to the number of indexing dimensions.  If the step length is not a
-        complex number, then the stop is not inclusive.
-
-        However, if the step length is a COMPLEX NUMBER (e.g. 5j), then the
-        integer part of it's magnitude is interpreted as specifying the
-        number of points to create between the start and stop values, where
-        the stop value IS INCLUSIVE.
-
-        If instantiated with an argument of sparse=True, the mesh-grid is
-        open (or not fleshed out) so that only one-dimension of each returned
-        argument is greater than 1
-
-        Example:
-
-           >>> mgrid = nd_grid()
-           >>> mgrid[0:5,0:5]
-           array([[[0, 0, 0, 0, 0],
-                   [1, 1, 1, 1, 1],
-                   [2, 2, 2, 2, 2],
-                   [3, 3, 3, 3, 3],
-                   [4, 4, 4, 4, 4]],
-                  [[0, 1, 2, 3, 4],
-                   [0, 1, 2, 3, 4],
-                   [0, 1, 2, 3, 4],
-                   [0, 1, 2, 3, 4],
-                   [0, 1, 2, 3, 4]]])
-           >>> mgrid[-1:1:5j]
-           array([-1. , -0.5,  0. ,  0.5,  1. ])
-
-           >>> ogrid = nd_grid(sparse=True)
-           >>> ogrid[0:5,0:5]
-           [array([[0],[1],[2],[3],[4]]), array([[0, 1, 2, 3, 4]])]
-    """
+
+class nd_grid:
+    """
+    Construct a multi-dimensional "meshgrid".
+
+    ``grid = nd_grid()`` creates an instance which will return a mesh-grid
+    when indexed.  The dimension and number of the output arrays are equal
+    to the number of indexing dimensions.  If the step length is not a
+    complex number, then the stop is not inclusive.
+
+    However, if the step length is a **complex number** (e.g. 5j), then the
+    integer part of its magnitude is interpreted as specifying the
+    number of points to create between the start and stop values, where
+    the stop value **is inclusive**.
+
+    If instantiated with an argument of ``sparse=True``, the mesh-grid is
+    open (or not fleshed out) so that only one-dimension of each returned
+    argument is greater than 1.
+
+    Parameters
+    ----------
+    sparse : bool, optional
+        Whether the grid is sparse or not. Default is False.
+
+    Notes
+    -----
+    Two instances of `nd_grid` are made available in the NumPy namespace,
+    `mgrid` and `ogrid`, approximately defined as::
+
+        mgrid = nd_grid(sparse=False)
+        ogrid = nd_grid(sparse=True)
+
+    Users should use these pre-defined instances instead of using `nd_grid`
+    directly.
+    """
+
     def __init__(self, sparse=False):
         self.sparse = sparse
-    def __getitem__(self,key):
+
+    def __getitem__(self, key):
         try:
             size = []
-            typecode = _nx.Int
-            for k in range(len(key)):
-                step = key[k].step
-                start = key[k].start
-                if start is None: start=0
-                if step is None: step=1
-                if type(step) is type(1j):
+            typ = int
+            for kk in key:
+                step = kk.step
+                start = kk.start
+                if start is None:
+                    start = 0
+                if step is None:
+                    step = 1
+                if isinstance(step, (_nx.complexfloating, complex)):
                     size.append(int(abs(step)))
-                    typecode = _nx.Float
+                    typ = float
                 else:
-                    size.append(int((key[k].stop - start)/(step*1.0)))
-                if isinstance(step,types.FloatType) or \
-                   isinstance(start, types.FloatType) or \
-                   isinstance(key[k].stop, types.FloatType):
-                    typecode = _nx.Float
+                    size.append(
+                        int(math.ceil((kk.stop - start) / (step * 1.0))))
+                if (isinstance(step, (_nx.floating, float)) or
+                        isinstance(start, (_nx.floating, float)) or
+                        isinstance(kk.stop, (_nx.floating, float))):
+                    typ = float
             if self.sparse:
-                nn = map(lambda x,t: _nx.arange(x,dtype=t),size,(typecode,)*len(size))
+                nn = [_nx.arange(_x, dtype=_t)
+                      for _x, _t in zip(size, (typ,)*len(size))]
             else:
-                nn = _nx.indices(size,typecode)
-            for k in range(len(size)):
-                step = key[k].step
-                start = key[k].start
-                if start is None: start=0
-                if step is None: step=1
-                if type(step) is type(1j):
+                nn = _nx.indices(size, typ)
+            for k, kk in enumerate(key):
+                step = kk.step
+                start = kk.start
+                if start is None:
+                    start = 0
+                if step is None:
+                    step = 1
+                if isinstance(step, (_nx.complexfloating, complex)):
                     step = int(abs(step))
-                    step = (key[k].stop - start)/float(step-1)
+                    if step != 1:
+                        step = (kk.stop - start) / float(step - 1)
                 nn[k] = (nn[k]*step+start)
             if self.sparse:
                 slobj = [_nx.newaxis]*len(size)
                 for k in range(len(size)):
-                    slobj[k] = slice(None,None)
-                    nn[k] = nn[k][slobj]
+                    slobj[k] = slice(None, None)
+                    nn[k] = nn[k][tuple(slobj)]
                     slobj[k] = _nx.newaxis
             return nn
         except (IndexError, TypeError):
             step = key.step
             stop = key.stop
             start = key.start
-            if start is None: start = 0
-            if type(step) is type(1j):
+            if start is None:
+                start = 0
+            if isinstance(step, (_nx.complexfloating, complex)):
                 step = abs(step)
                 length = int(step)
-                step = (key.stop-start)/float(step-1)
-                stop = key.stop+step
-                return _nx.arange(0,length,1,_nx.Float)*step + start
+                if step != 1:
+                    step = (key.stop-start)/float(step-1)
+                return _nx.arange(0, length, 1, float)*step + start
             else:
                 return _nx.arange(start, stop, step)
 
-    def __getslice__(self,i,j):
-        return _nx.arange(i,j)
-
-    def __len__(self):
-        return 0
-
-mgrid = nd_grid(sparse=False)
-ogrid = nd_grid(sparse=True)
-
-class concatenator(object):
-    """Translates slice objects to concatenation along an axis.
-    """
-    def _retval(self, res):
-        if self.matrix:
-            oldndim = res.ndim
-            res = makemat(res)
-            if oldndim == 1 and self.col:
-                res = res.T
-        self.axis = self._axis
-        self.matrix = self._matrix
-        self.col = 0
-        return res
-
-    def __init__(self, axis=0, matrix=False):
-        self._axis = axis
-        self._matrix = matrix
+
+class MGridClass(nd_grid):
+    """
+    `nd_grid` instance which returns a dense multi-dimensional "meshgrid".
+
+    An instance of `numpy.lib.index_tricks.nd_grid` which returns an dense
+    (or fleshed out) mesh-grid when indexed, so that each returned argument
+    has the same shape.  The dimensions and number of the output arrays are
+    equal to the number of indexing dimensions.  If the step length is not a
+    complex number, then the stop is not inclusive.
+
+    However, if the step length is a **complex number** (e.g. 5j), then
+    the integer part of its magnitude is interpreted as specifying the
+    number of points to create between the start and stop values, where
+    the stop value **is inclusive**.
+
+    Returns
+    -------
+    mesh-grid `ndarrays` all of the same dimensions
+
+    See Also
+    --------
+    lib.index_tricks.nd_grid : class of `ogrid` and `mgrid` objects
+    ogrid : like mgrid but returns open (not fleshed out) mesh grids
+    r_ : array concatenator
+
+    Examples
+    --------
+    >>> np.mgrid[0:5, 0:5]
+    array([[[0, 0, 0, 0, 0],
+            [1, 1, 1, 1, 1],
+            [2, 2, 2, 2, 2],
+            [3, 3, 3, 3, 3],
+            [4, 4, 4, 4, 4]],
+           [[0, 1, 2, 3, 4],
+            [0, 1, 2, 3, 4],
+            [0, 1, 2, 3, 4],
+            [0, 1, 2, 3, 4],
+            [0, 1, 2, 3, 4]]])
+    >>> np.mgrid[-1:1:5j]
+    array([-1. , -0.5,  0. ,  0.5,  1. ])
+
+    """
+
+    def __init__(self):
+        super().__init__(sparse=False)
+
+
+mgrid = MGridClass()
+
+
+class OGridClass(nd_grid):
+    """
+    `nd_grid` instance which returns an open multi-dimensional "meshgrid".
+
+    An instance of `numpy.lib.index_tricks.nd_grid` which returns an open
+    (i.e. not fleshed out) mesh-grid when indexed, so that only one dimension
+    of each returned array is greater than 1.  The dimension and number of the
+    output arrays are equal to the number of indexing dimensions.  If the step
+    length is not a complex number, then the stop is not inclusive.
+
+    However, if the step length is a **complex number** (e.g. 5j), then
+    the integer part of its magnitude is interpreted as specifying the
+    number of points to create between the start and stop values, where
+    the stop value **is inclusive**.
+
+    Returns
+    -------
+    mesh-grid
+        `ndarrays` with only one dimension not equal to 1
+
+    See Also
+    --------
+    np.lib.index_tricks.nd_grid : class of `ogrid` and `mgrid` objects
+    mgrid : like `ogrid` but returns dense (or fleshed out) mesh grids
+    r_ : array concatenator
+
+    Examples
+    --------
+    >>> from numpy import ogrid
+    >>> ogrid[-1:1:5j]
+    array([-1. , -0.5,  0. ,  0.5,  1. ])
+    >>> ogrid[0:5,0:5]
+    [array([[0],
+            [1],
+            [2],
+            [3],
+            [4]]), array([[0, 1, 2, 3, 4]])]
+
+    """
+
+    def __init__(self):
+        super().__init__(sparse=True)
+
+
+ogrid = OGridClass()
+
+
+class AxisConcatenator:
+    """
+    Translates slice objects to concatenation along an axis.
+
+    For detailed documentation on usage, see `r_`.
+    """
+    # allow ma.mr_ to override this
+    concatenate = staticmethod(_nx.concatenate)
+    makemat = staticmethod(matrixlib.matrix)
+
+    def __init__(self, axis=0, matrix=False, ndmin=1, trans1d=-1):
         self.axis = axis
         self.matrix = matrix
-        self.col = 0
-
-    def __getitem__(self,key):
-        if isinstance(key,types.StringType):
+        self.trans1d = trans1d
+        self.ndmin = ndmin
+
+    def __getitem__(self, key):
+        # handle matrix builder syntax
+        if isinstance(key, str):
             frame = sys._getframe().f_back
-            mymat = matrix.bmat(key,frame.f_globals,frame.f_locals)
+            mymat = matrixlib.bmat(key, frame.f_globals, frame.f_locals)
             return mymat
-        if type(key) is not types.TupleType:
+
+        if not isinstance(key, tuple):
             key = (key,)
+
+        # copy attributes, since they can be overridden in the first argument
+        trans1d = self.trans1d
+        ndmin = self.ndmin
+        matrix = self.matrix
+        axis = self.axis
+
         objs = []
         scalars = []
-        final_dtypedescr = None
-        for k in range(len(key)):
+        arraytypes = []
+        scalartypes = []
+
+        for k, item in enumerate(key):
             scalar = False
-            if type(key[k]) is types.SliceType:
-                step = key[k].step
-                start = key[k].start
-                stop = key[k].stop
-                if start is None: start = 0
+            if isinstance(item, slice):
+                step = item.step
+                start = item.start
+                stop = item.stop
+                if start is None:
+                    start = 0
                 if step is None:
                     step = 1
-                if type(step) is type(1j):
+                if isinstance(step, (_nx.complexfloating, complex)):
                     size = int(abs(step))
-                    newobj = function_base.linspace(start, stop, num=size)
+                    newobj = linspace(start, stop, num=size)
                 else:
                     newobj = _nx.arange(start, stop, step)
-            elif type(key[k]) is types.StringType:
-                if (key[k] in 'rc'):
-                    self.matrix = True
-                    self.col = (key[k] == 'c')
+                if ndmin > 1:
+                    newobj = array(newobj, copy=False, ndmin=ndmin)
+                    if trans1d != -1:
+                        newobj = newobj.swapaxes(-1, trans1d)
+            elif isinstance(item, str):
+                if k != 0:
+                    raise ValueError("special directives must be the "
+                                     "first entry.")
+                if item in ('r', 'c'):
+                    matrix = True
+                    col = (item == 'c')
                     continue
+                if ',' in item:
+                    vec = item.split(',')
+                    try:
+                        axis, ndmin = [int(x) for x in vec[:2]]
+                        if len(vec) == 3:
+                            trans1d = int(vec[2])
+                        continue
+                    except Exception as e:
+                        raise ValueError(
+                            "unknown special directive {!r}".format(item)
+                        ) from e
                 try:
-                    self.axis = int(key[k])
+                    axis = int(item)
                     continue
-                except:
-                    raise ValueError, "Unknown special directive."
-            elif type(key[k]) in ScalarType:
-                newobj = asarray([key[k]])
-                scalars.append(k)
+                except (ValueError, TypeError) as e:
+                    raise ValueError("unknown special directive") from e
+            elif type(item) in ScalarType:
+                newobj = array(item, ndmin=ndmin)
+                scalars.append(len(objs))
                 scalar = True
+                scalartypes.append(newobj.dtype)
             else:
-                newobj = key[k]
+                item_ndim = ndim(item)
+                newobj = array(item, copy=False, subok=True, ndmin=ndmin)
+                if trans1d != -1 and item_ndim < ndmin:
+                    k2 = ndmin - item_ndim
+                    k1 = trans1d
+                    if k1 < 0:
+                        k1 += k2 + 1
+                    defaxes = list(range(ndmin))
+                    axes = defaxes[:k1] + defaxes[k2:] + defaxes[k1:k2]
+                    newobj = newobj.transpose(axes)
             objs.append(newobj)
-            if isinstance(newobj, _nx.ndarray) and not scalar:
-                if final_dtypedescr is None:
-                    final_dtypedescr = newobj.dtype
-                elif newobj.dtype > final_dtypedescr:
-                    final_dtypedescr = newobj.dtype
-        if final_dtypedescr is not None:
+            if not scalar and isinstance(newobj, _nx.ndarray):
+                arraytypes.append(newobj.dtype)
+
+        # Ensure that scalars won't up-cast unless warranted
+        final_dtype = find_common_type(arraytypes, scalartypes)
+        if final_dtype is not None:
             for k in scalars:
-                objs[k] = objs[k].astype(final_dtypedescr)
-        res = _nx.concatenate(tuple(objs),axis=self.axis)
-        return self._retval(res)
-
-    def __getslice__(self,i,j):
-        res = _nx.arange(i,j)
-        return self._retval(res)
+                objs[k] = objs[k].astype(final_dtype)
+
+        res = self.concatenate(tuple(objs), axis=axis)
+
+        if matrix:
+            oldndim = res.ndim
+            res = self.makemat(res)
+            if oldndim == 1 and col:
+                res = res.T
+        return res
 
     def __len__(self):
         return 0
@@ -266,105 +425,276 @@
 # etc. because otherwise we couldn't get the doc string to come out right
 # in help(r_)
 
-class r_class(concatenator):
-    """Translates slice objects to concatenation along the first axis.
-
-        For example:
-        >>> r_[array([1,2,3]), 0, 0, array([4,5,6])]
-        array([1, 2, 3, 0, 0, 4, 5, 6])
-    """
+
+class RClass(AxisConcatenator):
+    """
+    Translates slice objects to concatenation along the first axis.
+
+    This is a simple way to build up arrays quickly. There are two use cases.
+
+    1. If the index expression contains comma separated arrays, then stack
+       them along their first axis.
+    2. If the index expression contains slice notation or scalars then create
+       a 1-D array with a range indicated by the slice notation.
+
+    If slice notation is used, the syntax ``start:stop:step`` is equivalent
+    to ``np.arange(start, stop, step)`` inside of the brackets. However, if
+    ``step`` is an imaginary number (i.e. 100j) then its integer portion is
+    interpreted as a number-of-points desired and the start and stop are
+    inclusive. In other words ``start:stop:stepj`` is interpreted as
+    ``np.linspace(start, stop, step, endpoint=1)`` inside of the brackets.
+    After expansion of slice notation, all comma separated sequences are
+    concatenated together.
+
+    Optional character strings placed as the first element of the index
+    expression can be used to change the output. The strings 'r' or 'c' result
+    in matrix output. If the result is 1-D and 'r' is specified a 1 x N (row)
+    matrix is produced. If the result is 1-D and 'c' is specified, then a N x 1
+    (column) matrix is produced. If the result is 2-D then both provide the
+    same matrix result.
+
+    A string integer specifies which axis to stack multiple comma separated
+    arrays along. A string of two comma-separated integers allows indication
+    of the minimum number of dimensions to force each entry into as the
+    second integer (the axis to concatenate along is still the first integer).
+
+    A string with three comma-separated integers allows specification of the
+    axis to concatenate along, the minimum number of dimensions to force the
+    entries to, and which axis should contain the start of the arrays which
+    are less than the specified number of dimensions. In other words the third
+    integer allows you to specify where the 1's should be placed in the shape
+    of the arrays that have their shapes upgraded. By default, they are placed
+    in the front of the shape tuple. The third argument allows you to specify
+    where the start of the array should be instead. Thus, a third argument of
+    '0' would place the 1's at the end of the array shape. Negative integers
+    specify where in the new shape tuple the last dimension of upgraded arrays
+    should be placed, so the default is '-1'.
+
+    Parameters
+    ----------
+    Not a function, so takes no parameters
+
+
+    Returns
+    -------
+    A concatenated ndarray or matrix.
+
+    See Also
+    --------
+    concatenate : Join a sequence of arrays along an existing axis.
+    c_ : Translates slice objects to concatenation along the second axis.
+
+    Examples
+    --------
+    >>> np.r_[np.array([1,2,3]), 0, 0, np.array([4,5,6])]
+    array([1, 2, 3, ..., 4, 5, 6])
+    >>> np.r_[-1:1:6j, [0]*3, 5, 6]
+    array([-1. , -0.6, -0.2,  0.2,  0.6,  1. ,  0. ,  0. ,  0. ,  5. ,  6. ])
+
+    String integers specify the axis to concatenate along or the minimum
+    number of dimensions to force entries into.
+
+    >>> a = np.array([[0, 1, 2], [3, 4, 5]])
+    >>> np.r_['-1', a, a] # concatenate along last axis
+    array([[0, 1, 2, 0, 1, 2],
+           [3, 4, 5, 3, 4, 5]])
+    >>> np.r_['0,2', [1,2,3], [4,5,6]] # concatenate along first axis, dim>=2
+    array([[1, 2, 3],
+           [4, 5, 6]])
+
+    >>> np.r_['0,2,0', [1,2,3], [4,5,6]]
+    array([[1],
+           [2],
+           [3],
+           [4],
+           [5],
+           [6]])
+    >>> np.r_['1,2,0', [1,2,3], [4,5,6]]
+    array([[1, 4],
+           [2, 5],
+           [3, 6]])
+
+    Using 'r' or 'c' as a first string argument creates a matrix.
+
+    >>> np.r_['r',[1,2,3], [4,5,6]]
+    matrix([[1, 2, 3, 4, 5, 6]])
+
+    """
+
     def __init__(self):
-        concatenator.__init__(self, 0)
-
-r_ = r_class()
-
-class c_class(concatenator):
-    """Translates slice objects to concatenation along the second axis.
-
-        For example:
-        >>> c_[array([[1],[2],[3]]), array([[4],[5],[6]])]
-        array([[1, 4],
-               [2, 5],
-               [3, 6]])
-    """
+        AxisConcatenator.__init__(self, 0)
+
+
+r_ = RClass()
+
+
+class CClass(AxisConcatenator):
+    """
+    Translates slice objects to concatenation along the second axis.
+
+    This is short-hand for ``np.r_['-1,2,0', index expression]``, which is
+    useful because of its common occurrence. In particular, arrays will be
+    stacked along their last axis after being upgraded to at least 2-D with
+    1's post-pended to the shape (column vectors made out of 1-D arrays).
+
+    See Also
+    --------
+    column_stack : Stack 1-D arrays as columns into a 2-D array.
+    r_ : For more detailed documentation.
+
+    Examples
+    --------
+    >>> np.c_[np.array([1,2,3]), np.array([4,5,6])]
+    array([[1, 4],
+           [2, 5],
+           [3, 6]])
+    >>> np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]
+    array([[1, 2, 3, ..., 4, 5, 6]])
+
+    """
+
     def __init__(self):
-        concatenator.__init__(self, -1)
-
-c_ = c_class()
-
-class ndenumerate(object):
-    """
-    A simple nd index iterator over an array.
-
-    Example:
-    >>> a = array([[1,2],[3,4]])
-    >>> for index, x in ndenumerate(a):
-    ...     print index, x
+        AxisConcatenator.__init__(self, -1, ndmin=2, trans1d=0)
+
+
+c_ = CClass()
+
+
+@set_module('numpy')
+class ndenumerate:
+    """
+    Multidimensional index iterator.
+
+    Return an iterator yielding pairs of array coordinates and values.
+
+    Parameters
+    ----------
+    arr : ndarray
+      Input array.
+
+    See Also
+    --------
+    ndindex, flatiter
+
+    Examples
+    --------
+    >>> a = np.array([[1, 2], [3, 4]])
+    >>> for index, x in np.ndenumerate(a):
+    ...     print(index, x)
     (0, 0) 1
     (0, 1) 2
     (1, 0) 3
     (1, 1) 4
-    """
+
+    """
+
     def __init__(self, arr):
         self.iter = asarray(arr).flat
 
-    def next(self):
-        return self.iter.coords, self.iter.next()
+    def __next__(self):
+        """
+        Standard iterator method, returns the index tuple and array value.
+
+        Returns
+        -------
+        coords : tuple of ints
+            The indices of the current iteration.
+        val : scalar
+            The array element of the current iteration.
+
+        """
+        return self.iter.coords, next(self.iter)
 
     def __iter__(self):
         return self
 
-class ndindex(object):
-    """Pass in a sequence of integers corresponding
-    to the number of dimensions in the counter.  This iterator
-    will then return an N-dimensional counter.
-
-    Example:
-    >>> for index in ndindex(4,3,2):
-            print index
-    (0,0,0)
-    (0,0,1)
-    (0,1,0)
-    ...
-    (3,1,1)
-    (3,2,0)
-    (3,2,1)
-    """
-
-    def __init__(self, *args):
-        self.nd = len(args)
-        self.ind = [0]*self.nd
-        self.index = 0
-        self.maxvals = args
-        tot = 1
-        for k in range(self.nd):
-            tot *= args[k]
-        self.total = tot
-
-    def _incrementone(self, axis):
-        if (axis < 0):  # base case
-            return
-        if (self.ind[axis] < self.maxvals[axis]-1):
-            self.ind[axis] += 1
-        else:
-            self.ind[axis] = 0
-            self._incrementone(axis-1)
-
-    def ndincr(self):
-        self._incrementone(self.nd-1)
-
-    def next(self):
-        if (self.index >= self.total):
-            raise StopIteration
-        val = tuple(self.ind)
-        self.index += 1
-        self.ndincr()
-        return val
+
+@set_module('numpy')
+class ndindex:
+    """
+    An N-dimensional iterator object to index arrays.
+
+    Given the shape of an array, an `ndindex` instance iterates over
+    the N-dimensional index of the array. At each iteration a tuple
+    of indices is returned, the last dimension is iterated over first.
+
+    Parameters
+    ----------
+    shape : ints, or a single tuple of ints
+        The size of each dimension of the array can be passed as 
+        individual parameters or as the elements of a tuple.
+
+    See Also
+    --------
+    ndenumerate, flatiter
+
+    Examples
+    --------
+    Dimensions as individual arguments
+    
+    >>> for index in np.ndindex(3, 2, 1):
+    ...     print(index)
+    (0, 0, 0)
+    (0, 1, 0)
+    (1, 0, 0)
+    (1, 1, 0)
+    (2, 0, 0)
+    (2, 1, 0)
+
+    Same dimensions - but in a tuple ``(3, 2, 1)``
+    
+    >>> for index in np.ndindex((3, 2, 1)):
+    ...     print(index)
+    (0, 0, 0)
+    (0, 1, 0)
+    (1, 0, 0)
+    (1, 1, 0)
+    (2, 0, 0)
+    (2, 1, 0)
+
+    """
+
+    def __init__(self, *shape):
+        if len(shape) == 1 and isinstance(shape[0], tuple):
+            shape = shape[0]
+        x = as_strided(_nx.zeros(1), shape=shape,
+                       strides=_nx.zeros_like(shape))
+        self._it = _nx.nditer(x, flags=['multi_index', 'zerosize_ok'],
+                              order='C')
 
     def __iter__(self):
         return self
 
-
+    def ndincr(self):
+        """
+        Increment the multi-dimensional index by one.
+
+        This method is for backward compatibility only: do not use.
+
+        .. deprecated:: 1.20.0
+            This method has been advised against since numpy 1.8.0, but only
+            started emitting DeprecationWarning as of this version.
+        """
+        # NumPy 1.20.0, 2020-09-08
+        warnings.warn(
+            "`ndindex.ndincr()` is deprecated, use `next(ndindex)` instead",
+            DeprecationWarning, stacklevel=2)
+        next(self)
+
+    def __next__(self):
+        """
+        Standard iterator method, updates the index and returns the index
+        tuple.
+
+        Returns
+        -------
+        val : tuple of ints
+            Returns a tuple containing the indices of the current
+            iteration.
+
+        """
+        next(self._it)
+        return self._it.multi_index
 
 
 # You can do all this with slice() plus a few special objects,
@@ -377,39 +707,308 @@
 # Cosmetic changes by T. Oliphant 2001
 #
 #
-# This module provides a convenient method for constructing
-# array indices algorithmically. It provides one importable object,
-# 'index_expression'.
-
-class _index_expression_class(object):
+
+class IndexExpression:
     """
     A nicer way to build up index tuples for arrays.
 
+    .. note::
+       Use one of the two predefined instances `index_exp` or `s_`
+       rather than directly using `IndexExpression`.
+
     For any index combination, including slicing and axis insertion,
-    'a[indices]' is the same as 'a[index_exp[indices]]' for any
-    array 'a'. However, 'index_exp[indices]' can be used anywhere
+    ``a[indices]`` is the same as ``a[np.index_exp[indices]]`` for any
+    array `a`. However, ``np.index_exp[indices]`` can be used anywhere
     in Python code and returns a tuple of slice objects that can be
     used in the construction of complex index expressions.
-    """
-    maxint = sys.maxint
+
+    Parameters
+    ----------
+    maketuple : bool
+        If True, always returns a tuple.
+
+    See Also
+    --------
+    index_exp : Predefined instance that always returns a tuple:
+       `index_exp = IndexExpression(maketuple=True)`.
+    s_ : Predefined instance without tuple conversion:
+       `s_ = IndexExpression(maketuple=False)`.
+
+    Notes
+    -----
+    You can do all this with `slice()` plus a few special objects,
+    but there's a lot to remember and this version is simpler because
+    it uses the standard array indexing syntax.
+
+    Examples
+    --------
+    >>> np.s_[2::2]
+    slice(2, None, 2)
+    >>> np.index_exp[2::2]
+    (slice(2, None, 2),)
+
+    >>> np.array([0, 1, 2, 3, 4])[np.s_[2::2]]
+    array([2, 4])
+
+    """
+
     def __init__(self, maketuple):
         self.maketuple = maketuple
 
     def __getitem__(self, item):
-        if self.maketuple and type(item) != type(()):
+        if self.maketuple and not isinstance(item, tuple):
             return (item,)
         else:
             return item
 
-    def __len__(self):
-        return self.maxint
-
-    def __getslice__(self, start, stop):
-        if stop == self.maxint:
-            stop = None
-        return self[start:stop:None]
-
-index_exp = _index_expression_class(1)
-s_ = _index_expression_class(0)
+
+index_exp = IndexExpression(maketuple=True)
+s_ = IndexExpression(maketuple=False)
 
 # End contribution from Konrad.
+
+
+# The following functions complement those in twodim_base, but are
+# applicable to N-dimensions.
+
+
+def _fill_diagonal_dispatcher(a, val, wrap=None):
+    return (a,)
+
+
+@array_function_dispatch(_fill_diagonal_dispatcher)
+def fill_diagonal(a, val, wrap=False):
+    """Fill the main diagonal of the given array of any dimensionality.
+
+    For an array `a` with ``a.ndim >= 2``, the diagonal is the list of
+    locations with indices ``a[i, ..., i]`` all identical. This function
+    modifies the input array in-place, it does not return a value.
+
+    Parameters
+    ----------
+    a : array, at least 2-D.
+      Array whose diagonal is to be filled, it gets modified in-place.
+
+    val : scalar or array_like
+      Value(s) to write on the diagonal. If `val` is scalar, the value is
+      written along the diagonal. If array-like, the flattened `val` is
+      written along the diagonal, repeating if necessary to fill all
+      diagonal entries.
+
+    wrap : bool
+      For tall matrices in NumPy version up to 1.6.2, the
+      diagonal "wrapped" after N columns. You can have this behavior
+      with this option. This affects only tall matrices.
+
+    See also
+    --------
+    diag_indices, diag_indices_from
+
+    Notes
+    -----
+    .. versionadded:: 1.4.0
+
+    This functionality can be obtained via `diag_indices`, but internally
+    this version uses a much faster implementation that never constructs the
+    indices and uses simple slicing.
+
+    Examples
+    --------
+    >>> a = np.zeros((3, 3), int)
+    >>> np.fill_diagonal(a, 5)
+    >>> a
+    array([[5, 0, 0],
+           [0, 5, 0],
+           [0, 0, 5]])
+
+    The same function can operate on a 4-D array:
+
+    >>> a = np.zeros((3, 3, 3, 3), int)
+    >>> np.fill_diagonal(a, 4)
+
+    We only show a few blocks for clarity:
+
+    >>> a[0, 0]
+    array([[4, 0, 0],
+           [0, 0, 0],
+           [0, 0, 0]])
+    >>> a[1, 1]
+    array([[0, 0, 0],
+           [0, 4, 0],
+           [0, 0, 0]])
+    >>> a[2, 2]
+    array([[0, 0, 0],
+           [0, 0, 0],
+           [0, 0, 4]])
+
+    The wrap option affects only tall matrices:
+
+    >>> # tall matrices no wrap
+    >>> a = np.zeros((5, 3), int)
+    >>> np.fill_diagonal(a, 4)
+    >>> a
+    array([[4, 0, 0],
+           [0, 4, 0],
+           [0, 0, 4],
+           [0, 0, 0],
+           [0, 0, 0]])
+
+    >>> # tall matrices wrap
+    >>> a = np.zeros((5, 3), int)
+    >>> np.fill_diagonal(a, 4, wrap=True)
+    >>> a
+    array([[4, 0, 0],
+           [0, 4, 0],
+           [0, 0, 4],
+           [0, 0, 0],
+           [4, 0, 0]])
+
+    >>> # wide matrices
+    >>> a = np.zeros((3, 5), int)
+    >>> np.fill_diagonal(a, 4, wrap=True)
+    >>> a
+    array([[4, 0, 0, 0, 0],
+           [0, 4, 0, 0, 0],
+           [0, 0, 4, 0, 0]])
+
+    The anti-diagonal can be filled by reversing the order of elements
+    using either `numpy.flipud` or `numpy.fliplr`.
+
+    >>> a = np.zeros((3, 3), int);
+    >>> np.fill_diagonal(np.fliplr(a), [1,2,3])  # Horizontal flip
+    >>> a
+    array([[0, 0, 1],
+           [0, 2, 0],
+           [3, 0, 0]])
+    >>> np.fill_diagonal(np.flipud(a), [1,2,3])  # Vertical flip
+    >>> a
+    array([[0, 0, 3],
+           [0, 2, 0],
+           [1, 0, 0]])
+
+    Note that the order in which the diagonal is filled varies depending
+    on the flip function.
+    """
+    if a.ndim < 2:
+        raise ValueError("array must be at least 2-d")
+    end = None
+    if a.ndim == 2:
+        # Explicit, fast formula for the common case.  For 2-d arrays, we
+        # accept rectangular ones.
+        step = a.shape[1] + 1
+        # This is needed to don't have tall matrix have the diagonal wrap.
+        if not wrap:
+            end = a.shape[1] * a.shape[1]
+    else:
+        # For more than d=2, the strided formula is only valid for arrays with
+        # all dimensions equal, so we check first.
+        if not alltrue(diff(a.shape) == 0):
+            raise ValueError("All dimensions of input must be of equal length")
+        step = 1 + (cumprod(a.shape[:-1])).sum()
+
+    # Write the value out into the diagonal.
+    a.flat[:end:step] = val
+
+
+@set_module('numpy')
+def diag_indices(n, ndim=2):
+    """
+    Return the indices to access the main diagonal of an array.
+
+    This returns a tuple of indices that can be used to access the main
+    diagonal of an array `a` with ``a.ndim >= 2`` dimensions and shape
+    (n, n, ..., n). For ``a.ndim = 2`` this is the usual diagonal, for
+    ``a.ndim > 2`` this is the set of indices to access ``a[i, i, ..., i]``
+    for ``i = [0..n-1]``.
+
+    Parameters
+    ----------
+    n : int
+      The size, along each dimension, of the arrays for which the returned
+      indices can be used.
+
+    ndim : int, optional
+      The number of dimensions.
+
+    See Also
+    --------
+    diag_indices_from
+
+    Notes
+    -----
+    .. versionadded:: 1.4.0
+
+    Examples
+    --------
+    Create a set of indices to access the diagonal of a (4, 4) array:
+
+    >>> di = np.diag_indices(4)
+    >>> di
+    (array([0, 1, 2, 3]), array([0, 1, 2, 3]))
+    >>> a = np.arange(16).reshape(4, 4)
+    >>> a
+    array([[ 0,  1,  2,  3],
+           [ 4,  5,  6,  7],
+           [ 8,  9, 10, 11],
+           [12, 13, 14, 15]])
+    >>> a[di] = 100
+    >>> a
+    array([[100,   1,   2,   3],
+           [  4, 100,   6,   7],
+           [  8,   9, 100,  11],
+           [ 12,  13,  14, 100]])
+
+    Now, we create indices to manipulate a 3-D array:
+
+    >>> d3 = np.diag_indices(2, 3)
+    >>> d3
+    (array([0, 1]), array([0, 1]), array([0, 1]))
+
+    And use it to set the diagonal of an array of zeros to 1:
+
+    >>> a = np.zeros((2, 2, 2), dtype=int)
+    >>> a[d3] = 1
+    >>> a
+    array([[[1, 0],
+            [0, 0]],
+           [[0, 0],
+            [0, 1]]])
+
+    """
+    idx = arange(n)
+    return (idx,) * ndim
+
+
+def _diag_indices_from(arr):
+    return (arr,)
+
+
+@array_function_dispatch(_diag_indices_from)
+def diag_indices_from(arr):
+    """
+    Return the indices to access the main diagonal of an n-dimensional array.
+
+    See `diag_indices` for full details.
+
+    Parameters
+    ----------
+    arr : array, at least 2-D
+
+    See Also
+    --------
+    diag_indices
+
+    Notes
+    -----
+    .. versionadded:: 1.4.0
+
+    """
+
+    if not arr.ndim >= 2:
+        raise ValueError("input array must be at least 2-d")
+    # For more than d=2, the strided formula is only valid for arrays with
+    # all dimensions equal, so we check first.
+    if not alltrue(diff(arr.shape) == 0):
+        raise ValueError("All dimensions of input must be of equal length")
+
+    return diag_indices(arr.shape[0], arr.ndim)
('numpy/lib', 'shape_base.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,582 +1,1280 @@
-__all__ = ['atleast_1d','atleast_2d','atleast_3d','vstack','hstack',
-           'column_stack','dstack','array_split','split','hsplit',
-           'vsplit','dsplit','apply_over_axes','expand_dims',
-           'apply_along_axis', 'repmat', 'kron']
+import functools
 
 import numpy.core.numeric as _nx
-from numpy.core.numeric import asarray, zeros, newaxis, outerproduct, \
-     concatenate, isscalar, array, asanyarray
-from numpy.core.oldnumeric import product, reshape
-
-def apply_along_axis(func1d,axis,arr,*args):
-    """ Execute func1d(arr[i],*args) where func1d takes 1-D arrays
-        and arr is an N-d array.  i varies so as to apply the function
-        along the given axis for each 1-d subarray in arr.
-    """
-    arr = asarray(arr)
+from numpy.core.numeric import (
+    asarray, zeros, outer, concatenate, array, asanyarray
+    )
+from numpy.core.fromnumeric import reshape, transpose
+from numpy.core.multiarray import normalize_axis_index
+from numpy.core import overrides
+from numpy.core import vstack, atleast_3d
+from numpy.core.numeric import normalize_axis_tuple
+from numpy.core.shape_base import _arrays_for_stack_dispatcher
+from numpy.lib.index_tricks import ndindex
+from numpy.matrixlib.defmatrix import matrix  # this raises all the right alarm bells
+
+
+__all__ = [
+    'column_stack', 'row_stack', 'dstack', 'array_split', 'split',
+    'hsplit', 'vsplit', 'dsplit', 'apply_over_axes', 'expand_dims',
+    'apply_along_axis', 'kron', 'tile', 'get_array_wrap', 'take_along_axis',
+    'put_along_axis'
+    ]
+
+
+array_function_dispatch = functools.partial(
+    overrides.array_function_dispatch, module='numpy')
+
+
+def _make_along_axis_idx(arr_shape, indices, axis):
+    # compute dimensions to iterate over
+    if not _nx.issubdtype(indices.dtype, _nx.integer):
+        raise IndexError('`indices` must be an integer array')
+    if len(arr_shape) != indices.ndim:
+        raise ValueError(
+            "`indices` and `arr` must have the same number of dimensions")
+    shape_ones = (1,) * indices.ndim
+    dest_dims = list(range(axis)) + [None] + list(range(axis+1, indices.ndim))
+
+    # build a fancy index, consisting of orthogonal aranges, with the
+    # requested index inserted at the right location
+    fancy_index = []
+    for dim, n in zip(dest_dims, arr_shape):
+        if dim is None:
+            fancy_index.append(indices)
+        else:
+            ind_shape = shape_ones[:dim] + (-1,) + shape_ones[dim+1:]
+            fancy_index.append(_nx.arange(n).reshape(ind_shape))
+
+    return tuple(fancy_index)
+
+
+def _take_along_axis_dispatcher(arr, indices, axis):
+    return (arr, indices)
+
+
+@array_function_dispatch(_take_along_axis_dispatcher)
+def take_along_axis(arr, indices, axis):
+    """
+    Take values from the input array by matching 1d index and data slices.
+
+    This iterates over matching 1d slices oriented along the specified axis in
+    the index and data arrays, and uses the former to look up values in the
+    latter. These slices can be different lengths.
+
+    Functions returning an index along an axis, like `argsort` and
+    `argpartition`, produce suitable indices for this function.
+
+    .. versionadded:: 1.15.0
+
+    Parameters
+    ----------
+    arr : ndarray (Ni..., M, Nk...)
+        Source array
+    indices : ndarray (Ni..., J, Nk...)
+        Indices to take along each 1d slice of `arr`. This must match the
+        dimension of arr, but dimensions Ni and Nj only need to broadcast
+        against `arr`.
+    axis : int
+        The axis to take 1d slices along. If axis is None, the input array is
+        treated as if it had first been flattened to 1d, for consistency with
+        `sort` and `argsort`.
+
+    Returns
+    -------
+    out: ndarray (Ni..., J, Nk...)
+        The indexed result.
+
+    Notes
+    -----
+    This is equivalent to (but faster than) the following use of `ndindex` and
+    `s_`, which sets each of ``ii`` and ``kk`` to a tuple of indices::
+
+        Ni, M, Nk = a.shape[:axis], a.shape[axis], a.shape[axis+1:]
+        J = indices.shape[axis]  # Need not equal M
+        out = np.empty(Ni + (J,) + Nk)
+
+        for ii in ndindex(Ni):
+            for kk in ndindex(Nk):
+                a_1d       = a      [ii + s_[:,] + kk]
+                indices_1d = indices[ii + s_[:,] + kk]
+                out_1d     = out    [ii + s_[:,] + kk]
+                for j in range(J):
+                    out_1d[j] = a_1d[indices_1d[j]]
+
+    Equivalently, eliminating the inner loop, the last two lines would be::
+
+                out_1d[:] = a_1d[indices_1d]
+
+    See Also
+    --------
+    take : Take along an axis, using the same indices for every 1d slice
+    put_along_axis :
+        Put values into the destination array by matching 1d index and data slices
+
+    Examples
+    --------
+
+    For this sample array
+
+    >>> a = np.array([[10, 30, 20], [60, 40, 50]])
+
+    We can sort either by using sort directly, or argsort and this function
+
+    >>> np.sort(a, axis=1)
+    array([[10, 20, 30],
+           [40, 50, 60]])
+    >>> ai = np.argsort(a, axis=1); ai
+    array([[0, 2, 1],
+           [1, 2, 0]])
+    >>> np.take_along_axis(a, ai, axis=1)
+    array([[10, 20, 30],
+           [40, 50, 60]])
+
+    The same works for max and min, if you expand the dimensions:
+
+    >>> np.expand_dims(np.max(a, axis=1), axis=1)
+    array([[30],
+           [60]])
+    >>> ai = np.expand_dims(np.argmax(a, axis=1), axis=1)
+    >>> ai
+    array([[1],
+           [0]])
+    >>> np.take_along_axis(a, ai, axis=1)
+    array([[30],
+           [60]])
+
+    If we want to get the max and min at the same time, we can stack the
+    indices first
+
+    >>> ai_min = np.expand_dims(np.argmin(a, axis=1), axis=1)
+    >>> ai_max = np.expand_dims(np.argmax(a, axis=1), axis=1)
+    >>> ai = np.concatenate([ai_min, ai_max], axis=1)
+    >>> ai
+    array([[0, 1],
+           [1, 0]])
+    >>> np.take_along_axis(a, ai, axis=1)
+    array([[10, 30],
+           [40, 60]])
+    """
+    # normalize inputs
+    if axis is None:
+        arr = arr.flat
+        arr_shape = (len(arr),)  # flatiter has no .shape
+        axis = 0
+    else:
+        axis = normalize_axis_index(axis, arr.ndim)
+        arr_shape = arr.shape
+
+    # use the fancy index
+    return arr[_make_along_axis_idx(arr_shape, indices, axis)]
+
+
+def _put_along_axis_dispatcher(arr, indices, values, axis):
+    return (arr, indices, values)
+
+
+@array_function_dispatch(_put_along_axis_dispatcher)
+def put_along_axis(arr, indices, values, axis):
+    """
+    Put values into the destination array by matching 1d index and data slices.
+
+    This iterates over matching 1d slices oriented along the specified axis in
+    the index and data arrays, and uses the former to place values into the
+    latter. These slices can be different lengths.
+
+    Functions returning an index along an axis, like `argsort` and
+    `argpartition`, produce suitable indices for this function.
+
+    .. versionadded:: 1.15.0
+
+    Parameters
+    ----------
+    arr : ndarray (Ni..., M, Nk...)
+        Destination array.
+    indices : ndarray (Ni..., J, Nk...)
+        Indices to change along each 1d slice of `arr`. This must match the
+        dimension of arr, but dimensions in Ni and Nj may be 1 to broadcast
+        against `arr`.
+    values : array_like (Ni..., J, Nk...)
+        values to insert at those indices. Its shape and dimension are
+        broadcast to match that of `indices`.
+    axis : int
+        The axis to take 1d slices along. If axis is None, the destination
+        array is treated as if a flattened 1d view had been created of it.
+
+    Notes
+    -----
+    This is equivalent to (but faster than) the following use of `ndindex` and
+    `s_`, which sets each of ``ii`` and ``kk`` to a tuple of indices::
+
+        Ni, M, Nk = a.shape[:axis], a.shape[axis], a.shape[axis+1:]
+        J = indices.shape[axis]  # Need not equal M
+
+        for ii in ndindex(Ni):
+            for kk in ndindex(Nk):
+                a_1d       = a      [ii + s_[:,] + kk]
+                indices_1d = indices[ii + s_[:,] + kk]
+                values_1d  = values [ii + s_[:,] + kk]
+                for j in range(J):
+                    a_1d[indices_1d[j]] = values_1d[j]
+
+    Equivalently, eliminating the inner loop, the last two lines would be::
+
+                a_1d[indices_1d] = values_1d
+
+    See Also
+    --------
+    take_along_axis :
+        Take values from the input array by matching 1d index and data slices
+
+    Examples
+    --------
+
+    For this sample array
+
+    >>> a = np.array([[10, 30, 20], [60, 40, 50]])
+
+    We can replace the maximum values with:
+
+    >>> ai = np.expand_dims(np.argmax(a, axis=1), axis=1)
+    >>> ai
+    array([[1],
+           [0]])
+    >>> np.put_along_axis(a, ai, 99, axis=1)
+    >>> a
+    array([[10, 99, 20],
+           [99, 40, 50]])
+
+    """
+    # normalize inputs
+    if axis is None:
+        arr = arr.flat
+        axis = 0
+        arr_shape = (len(arr),)  # flatiter has no .shape
+    else:
+        axis = normalize_axis_index(axis, arr.ndim)
+        arr_shape = arr.shape
+
+    # use the fancy index
+    arr[_make_along_axis_idx(arr_shape, indices, axis)] = values
+
+
+def _apply_along_axis_dispatcher(func1d, axis, arr, *args, **kwargs):
+    return (arr,)
+
+
+@array_function_dispatch(_apply_along_axis_dispatcher)
+def apply_along_axis(func1d, axis, arr, *args, **kwargs):
+    """
+    Apply a function to 1-D slices along the given axis.
+
+    Execute `func1d(a, *args, **kwargs)` where `func1d` operates on 1-D arrays
+    and `a` is a 1-D slice of `arr` along `axis`.
+
+    This is equivalent to (but faster than) the following use of `ndindex` and
+    `s_`, which sets each of ``ii``, ``jj``, and ``kk`` to a tuple of indices::
+
+        Ni, Nk = a.shape[:axis], a.shape[axis+1:]
+        for ii in ndindex(Ni):
+            for kk in ndindex(Nk):
+                f = func1d(arr[ii + s_[:,] + kk])
+                Nj = f.shape
+                for jj in ndindex(Nj):
+                    out[ii + jj + kk] = f[jj]
+
+    Equivalently, eliminating the inner loop, this can be expressed as::
+
+        Ni, Nk = a.shape[:axis], a.shape[axis+1:]
+        for ii in ndindex(Ni):
+            for kk in ndindex(Nk):
+                out[ii + s_[...,] + kk] = func1d(arr[ii + s_[:,] + kk])
+
+    Parameters
+    ----------
+    func1d : function (M,) -> (Nj...)
+        This function should accept 1-D arrays. It is applied to 1-D
+        slices of `arr` along the specified axis.
+    axis : integer
+        Axis along which `arr` is sliced.
+    arr : ndarray (Ni..., M, Nk...)
+        Input array.
+    args : any
+        Additional arguments to `func1d`.
+    kwargs : any
+        Additional named arguments to `func1d`.
+
+        .. versionadded:: 1.9.0
+
+
+    Returns
+    -------
+    out : ndarray  (Ni..., Nj..., Nk...)
+        The output array. The shape of `out` is identical to the shape of
+        `arr`, except along the `axis` dimension. This axis is removed, and
+        replaced with new dimensions equal to the shape of the return value
+        of `func1d`. So if `func1d` returns a scalar `out` will have one
+        fewer dimensions than `arr`.
+
+    See Also
+    --------
+    apply_over_axes : Apply a function repeatedly over multiple axes.
+
+    Examples
+    --------
+    >>> def my_func(a):
+    ...     \"\"\"Average first and last element of a 1-D array\"\"\"
+    ...     return (a[0] + a[-1]) * 0.5
+    >>> b = np.array([[1,2,3], [4,5,6], [7,8,9]])
+    >>> np.apply_along_axis(my_func, 0, b)
+    array([4., 5., 6.])
+    >>> np.apply_along_axis(my_func, 1, b)
+    array([2.,  5.,  8.])
+
+    For a function that returns a 1D array, the number of dimensions in
+    `outarr` is the same as `arr`.
+
+    >>> b = np.array([[8,1,7], [4,3,9], [5,2,6]])
+    >>> np.apply_along_axis(sorted, 1, b)
+    array([[1, 7, 8],
+           [3, 4, 9],
+           [2, 5, 6]])
+
+    For a function that returns a higher dimensional array, those dimensions
+    are inserted in place of the `axis` dimension.
+
+    >>> b = np.array([[1,2,3], [4,5,6], [7,8,9]])
+    >>> np.apply_along_axis(np.diag, -1, b)
+    array([[[1, 0, 0],
+            [0, 2, 0],
+            [0, 0, 3]],
+           [[4, 0, 0],
+            [0, 5, 0],
+            [0, 0, 6]],
+           [[7, 0, 0],
+            [0, 8, 0],
+            [0, 0, 9]]])
+    """
+    # handle negative axes
+    arr = asanyarray(arr)
     nd = arr.ndim
-    if axis < 0:
-        axis += nd
-    if (axis >= nd):
-        raise ValueError("axis must be less than arr.ndim; axis=%d, rank=%d."
-            % (axis,nd))
-    ind = [0]*(nd-1)
-    i = zeros(nd,'O')
-    indlist = range(nd)
-    indlist.remove(axis)
-    i[axis] = slice(None,None)
-    outshape = asarray(arr.shape).take(indlist)
-    i.put(ind, indlist)
-    res = func1d(arr[tuple(i.tolist())],*args)
-    #  if res is a number, then we have a smaller output array
-    if isscalar(res):
-        outarr = zeros(outshape,asarray(res).dtype)
-        outarr[ind] = res
-        Ntot = product(outshape)
-        k = 1
-        while k < Ntot:
-            # increment the index
-            ind[-1] += 1
-            n = -1
-            while (ind[n] >= outshape[n]) and (n > (1-nd)):
-                ind[n-1] += 1
-                ind[n] = 0
-                n -= 1
-            i.put(ind,indlist)
-            res = func1d(arr[tuple(i.tolist())],*args)
-            outarr[ind] = res
-            k += 1
-        return outarr
+    axis = normalize_axis_index(axis, nd)
+
+    # arr, with the iteration axis at the end
+    in_dims = list(range(nd))
+    inarr_view = transpose(arr, in_dims[:axis] + in_dims[axis+1:] + [axis])
+
+    # compute indices for the iteration axes, and append a trailing ellipsis to
+    # prevent 0d arrays decaying to scalars, which fixes gh-8642
+    inds = ndindex(inarr_view.shape[:-1])
+    inds = (ind + (Ellipsis,) for ind in inds)
+
+    # invoke the function on the first item
+    try:
+        ind0 = next(inds)
+    except StopIteration as e:
+        raise ValueError(
+            'Cannot apply_along_axis when any iteration dimensions are 0'
+        ) from None
+    res = asanyarray(func1d(inarr_view[ind0], *args, **kwargs))
+
+    # build a buffer for storing evaluations of func1d.
+    # remove the requested axis, and add the new ones on the end.
+    # laid out so that each write is contiguous.
+    # for a tuple index inds, buff[inds] = func1d(inarr_view[inds])
+    buff = zeros(inarr_view.shape[:-1] + res.shape, res.dtype)
+
+    # permutation of axes such that out = buff.transpose(buff_permute)
+    buff_dims = list(range(buff.ndim))
+    buff_permute = (
+        buff_dims[0 : axis] +
+        buff_dims[buff.ndim-res.ndim : buff.ndim] +
+        buff_dims[axis : buff.ndim-res.ndim]
+    )
+
+    # matrices have a nasty __array_prepare__ and __array_wrap__
+    if not isinstance(res, matrix):
+        buff = res.__array_prepare__(buff)
+
+    # save the first result, then compute and save all remaining results
+    buff[ind0] = res
+    for ind in inds:
+        buff[ind] = asanyarray(func1d(inarr_view[ind], *args, **kwargs))
+
+    if not isinstance(res, matrix):
+        # wrap the array, to preserve subclasses
+        buff = res.__array_wrap__(buff)
+
+        # finally, rotate the inserted axes back to where they belong
+        return transpose(buff, buff_permute)
+
     else:
-        Ntot = product(outshape)
-        holdshape = outshape
-        outshape = list(arr.shape)
-        outshape[axis] = len(res)
-        outarr = zeros(outshape,asarray(res).dtype)
-        outarr[tuple(i.tolist())] = res
-        k = 1
-        while k < Ntot:
-            # increment the index
-            ind[-1] += 1
-            n = -1
-            while (ind[n] >= holdshape[n]) and (n > (1-nd)):
-                ind[n-1] += 1
-                ind[n] = 0
-                n -= 1
-            i.put(ind, indlist)
-            res = func1d(arr[tuple(i.tolist())],*args)
-            outarr[tuple(i.tolist())] = res
-            k += 1
-        return outarr
-
-
+        # matrices have to be transposed first, because they collapse dimensions!
+        out_arr = transpose(buff, buff_permute)
+        return res.__array_wrap__(out_arr)
+
+
+def _apply_over_axes_dispatcher(func, a, axes):
+    return (a,)
+
+
+@array_function_dispatch(_apply_over_axes_dispatcher)
 def apply_over_axes(func, a, axes):
-    """Apply a function repeatedly over multiple axes, keeping the same shape
-    for the resulting array.
-
-    func is called as res = func(a, axis).  The result is assumed
-    to be either the same shape as a or have one less dimension.
-    This call is repeated for each axis in the axes sequence.
+    """
+    Apply a function repeatedly over multiple axes.
+
+    `func` is called as `res = func(a, axis)`, where `axis` is the first
+    element of `axes`.  The result `res` of the function call must have
+    either the same dimensions as `a` or one less dimension.  If `res`
+    has one less dimension than `a`, a dimension is inserted before
+    `axis`.  The call to `func` is then repeated for each axis in `axes`,
+    with `res` as the first argument.
+
+    Parameters
+    ----------
+    func : function
+        This function must take two arguments, `func(a, axis)`.
+    a : array_like
+        Input array.
+    axes : array_like
+        Axes over which `func` is applied; the elements must be integers.
+
+    Returns
+    -------
+    apply_over_axis : ndarray
+        The output array.  The number of dimensions is the same as `a`,
+        but the shape can be different.  This depends on whether `func`
+        changes the shape of its output with respect to its input.
+
+    See Also
+    --------
+    apply_along_axis :
+        Apply a function to 1-D slices of an array along the given axis.
+
+    Notes
+    -----
+    This function is equivalent to tuple axis arguments to reorderable ufuncs
+    with keepdims=True. Tuple axis arguments to ufuncs have been available since
+    version 1.7.0.
+
+    Examples
+    --------
+    >>> a = np.arange(24).reshape(2,3,4)
+    >>> a
+    array([[[ 0,  1,  2,  3],
+            [ 4,  5,  6,  7],
+            [ 8,  9, 10, 11]],
+           [[12, 13, 14, 15],
+            [16, 17, 18, 19],
+            [20, 21, 22, 23]]])
+
+    Sum over axes 0 and 2. The result has same number of dimensions
+    as the original array:
+
+    >>> np.apply_over_axes(np.sum, a, [0,2])
+    array([[[ 60],
+            [ 92],
+            [124]]])
+
+    Tuple axis arguments to ufuncs are equivalent:
+
+    >>> np.sum(a, axis=(0,2), keepdims=True)
+    array([[[ 60],
+            [ 92],
+            [124]]])
+
     """
     val = asarray(a)
     N = a.ndim
     if array(axes).ndim == 0:
         axes = (axes,)
     for axis in axes:
-        if axis < 0: axis = N + axis
+        if axis < 0:
+            axis = N + axis
         args = (val, axis)
         res = func(*args)
         if res.ndim == val.ndim:
             val = res
         else:
-            res = expand_dims(res,axis)
+            res = expand_dims(res, axis)
             if res.ndim == val.ndim:
                 val = res
             else:
-                raise ValueError, "function is not returning"\
-                      " an array of correct shape"
+                raise ValueError("function is not returning "
+                                 "an array of the correct shape")
     return val
 
+
+def _expand_dims_dispatcher(a, axis):
+    return (a,)
+
+
+@array_function_dispatch(_expand_dims_dispatcher)
 def expand_dims(a, axis):
-    """Expand the shape of a by including newaxis before given axis.
-    """
-    a = asarray(a)
-    shape = a.shape
-    if axis < 0:
-        axis = axis + len(shape) + 1
-    return a.reshape(shape[:axis] + (1,) + shape[axis:])
-
-
-def atleast_1d(*arys):
-    """ Force a sequence of arrays to each be at least 1D.
-
-         Description:
-            Force an array to be at least 1D.  If an array is 0D, the
-            array is converted to a single row of values.  Otherwise,
-            the array is unaltered.
-         Arguments:
-            *arys -- arrays to be converted to 1 or more dimensional array.
-         Returns:
-            input array converted to at least 1D array.
-    """
-    res = []
-    for ary in arys:
-        res.append(array(ary,copy=False,ndmin=1))
-    if len(res) == 1:
-        return res[0]
+    """
+    Expand the shape of an array.
+
+    Insert a new axis that will appear at the `axis` position in the expanded
+    array shape.
+
+    Parameters
+    ----------
+    a : array_like
+        Input array.
+    axis : int or tuple of ints
+        Position in the expanded axes where the new axis (or axes) is placed.
+
+        .. deprecated:: 1.13.0
+            Passing an axis where ``axis > a.ndim`` will be treated as
+            ``axis == a.ndim``, and passing ``axis < -a.ndim - 1`` will
+            be treated as ``axis == 0``. This behavior is deprecated.
+
+        .. versionchanged:: 1.18.0
+            A tuple of axes is now supported.  Out of range axes as
+            described above are now forbidden and raise an `AxisError`.
+
+    Returns
+    -------
+    result : ndarray
+        View of `a` with the number of dimensions increased.
+
+    See Also
+    --------
+    squeeze : The inverse operation, removing singleton dimensions
+    reshape : Insert, remove, and combine dimensions, and resize existing ones
+    doc.indexing, atleast_1d, atleast_2d, atleast_3d
+
+    Examples
+    --------
+    >>> x = np.array([1, 2])
+    >>> x.shape
+    (2,)
+
+    The following is equivalent to ``x[np.newaxis, :]`` or ``x[np.newaxis]``:
+
+    >>> y = np.expand_dims(x, axis=0)
+    >>> y
+    array([[1, 2]])
+    >>> y.shape
+    (1, 2)
+
+    The following is equivalent to ``x[:, np.newaxis]``:
+
+    >>> y = np.expand_dims(x, axis=1)
+    >>> y
+    array([[1],
+           [2]])
+    >>> y.shape
+    (2, 1)
+
+    ``axis`` may also be a tuple:
+
+    >>> y = np.expand_dims(x, axis=(0, 1))
+    >>> y
+    array([[[1, 2]]])
+
+    >>> y = np.expand_dims(x, axis=(2, 0))
+    >>> y
+    array([[[1],
+            [2]]])
+
+    Note that some examples may use ``None`` instead of ``np.newaxis``.  These
+    are the same objects:
+
+    >>> np.newaxis is None
+    True
+
+    """
+    if isinstance(a, matrix):
+        a = asarray(a)
     else:
-        return res
-
-def atleast_2d(*arys):
-    """ Force a sequence of arrays to each be at least 2D.
-
-         Description:
-            Force an array to each be at least 2D.  If the array
-            is 0D or 1D, the array is converted to a single
-            row of values.  Otherwise, the array is unaltered.
-         Arguments:
-            arys -- arrays to be converted to 2 or more dimensional array.
-         Returns:
-            input array converted to at least 2D array.
-    """
-    res = []
-    for ary in arys:
-        res.append(array(ary,copy=False,ndmin=2))
-    if len(res) == 1:
-        return res[0]
-    else:
-        return res
-
-def atleast_3d(*arys):
-    """ Force a sequence of arrays to each be at least 3D.
-
-         Description:
-            Force an array each be at least 3D.  If the array is 0D or 1D,
-            the array is converted to a single 1xNx1 array of values where
-            N is the orginal length of the array. If the array is 2D, the
-            array is converted to a single MxNx1 array of values where MxN
-            is the orginal shape of the array. Otherwise, the array is
-            unaltered.
-         Arguments:
-            arys -- arrays to be converted to 3 or more dimensional array.
-         Returns:
-            input array converted to at least 3D array.
-    """
-    res = []
-    for ary in arys:
-        ary = asarray(ary)
-        if len(ary.shape) == 0:
-            result = ary.reshape(1,1,1)
-        elif len(ary.shape) == 1:
-            result = ary[newaxis,:,newaxis]
-        elif len(ary.shape) == 2:
-            result = ary[:,:,newaxis]
-        else:
-            result = ary
-        res.append(result)
-    if len(res) == 1:
-        return res[0]
-    else:
-        return res
-
-
-def vstack(tup):
-    """ Stack arrays in sequence vertically (row wise)
-
-        Description:
-            Take a sequence of arrays and stack them veritcally
-            to make a single array.  All arrays in the sequence
-            must have the same shape along all but the first axis.
-            vstack will rebuild arrays divided by vsplit.
-        Arguments:
-            tup -- sequence of arrays.  All arrays must have the same
-                   shape.
-        Examples:
-            >>> import numpy
-            >>> a = array((1,2,3))
-            >>> b = array((2,3,4))
-            >>> numpy.vstack((a,b))
-            array([[1, 2, 3],
-                   [2, 3, 4]])
-            >>> a = array([[1],[2],[3]])
-            >>> b = array([[2],[3],[4]])
-            >>> numpy.vstack((a,b))
-            array([[1],
-                   [2],
-                   [3],
-                   [2],
-                   [3],
-                   [4]])
-
-    """
-    return _nx.concatenate(map(atleast_2d,tup),0)
-
-def hstack(tup):
-    """ Stack arrays in sequence horizontally (column wise)
-
-        Description:
-            Take a sequence of arrays and stack them horizontally
-            to make a single array.  All arrays in the sequence
-            must have the same shape along all but the second axis.
-            hstack will rebuild arrays divided by hsplit.
-        Arguments:
-            tup -- sequence of arrays.  All arrays must have the same
-                   shape.
-        Examples:
-            >>> import numpy
-            >>> a = array((1,2,3))
-            >>> b = array((2,3,4))
-            >>> numpy.hstack((a,b))
-            array([1, 2, 3, 2, 3, 4])
-            >>> a = array([[1],[2],[3]])
-            >>> b = array([[2],[3],[4]])
-            >>> numpy.hstack((a,b))
-            array([[1, 2],
-                   [2, 3],
-                   [3, 4]])
-
-    """
-    return _nx.concatenate(map(atleast_1d,tup),1)
-
+        a = asanyarray(a)
+
+    if type(axis) not in (tuple, list):
+        axis = (axis,)
+
+    out_ndim = len(axis) + a.ndim
+    axis = normalize_axis_tuple(axis, out_ndim)
+
+    shape_it = iter(a.shape)
+    shape = [1 if ax in axis else next(shape_it) for ax in range(out_ndim)]
+
+    return a.reshape(shape)
+
+
+row_stack = vstack
+
+
+def _column_stack_dispatcher(tup):
+    return _arrays_for_stack_dispatcher(tup)
+
+
+@array_function_dispatch(_column_stack_dispatcher)
 def column_stack(tup):
-    """ Stack 1D arrays as columns into a 2D array
-
-        Description:
-            Take a sequence of 1D arrays and stack them as columns
-            to make a single 2D array.  All arrays in the sequence
-            must have the same length.
-        Arguments:
-            tup -- sequence of 1D arrays.  All arrays must have the same
-                   length.
-        Examples:
-            >>> import numpy
-            >>> a = array((1,2,3))
-            >>> b = array((2,3,4))
-            >>> numpy.column_stack((a,b))
-            array([[1, 2],
-                   [2, 3],
-                   [3, 4]])
-
-    """
-    arrays = map(_nx.transpose,map(atleast_2d,tup))
-    return _nx.concatenate(arrays,1)
-
+    """
+    Stack 1-D arrays as columns into a 2-D array.
+
+    Take a sequence of 1-D arrays and stack them as columns
+    to make a single 2-D array. 2-D arrays are stacked as-is,
+    just like with `hstack`.  1-D arrays are turned into 2-D columns
+    first.
+
+    Parameters
+    ----------
+    tup : sequence of 1-D or 2-D arrays.
+        Arrays to stack. All of them must have the same first dimension.
+
+    Returns
+    -------
+    stacked : 2-D array
+        The array formed by stacking the given arrays.
+
+    See Also
+    --------
+    stack, hstack, vstack, concatenate
+
+    Examples
+    --------
+    >>> a = np.array((1,2,3))
+    >>> b = np.array((2,3,4))
+    >>> np.column_stack((a,b))
+    array([[1, 2],
+           [2, 3],
+           [3, 4]])
+
+    """
+    if not overrides.ARRAY_FUNCTION_ENABLED:
+        # raise warning if necessary
+        _arrays_for_stack_dispatcher(tup, stacklevel=2)
+
+    arrays = []
+    for v in tup:
+        arr = asanyarray(v)
+        if arr.ndim < 2:
+            arr = array(arr, copy=False, subok=True, ndmin=2).T
+        arrays.append(arr)
+    return _nx.concatenate(arrays, 1)
+
+
+def _dstack_dispatcher(tup):
+    return _arrays_for_stack_dispatcher(tup)
+
+
+@array_function_dispatch(_dstack_dispatcher)
 def dstack(tup):
-    """ Stack arrays in sequence depth wise (along third dimension)
-
-        Description:
-            Take a sequence of arrays and stack them along the third axis.
-            All arrays in the sequence must have the same shape along all
-            but the third axis.  This is a simple way to stack 2D arrays
-            (images) into a single 3D array for processing.
-            dstack will rebuild arrays divided by dsplit.
-        Arguments:
-            tup -- sequence of arrays.  All arrays must have the same
-                   shape.
-        Examples:
-            >>> import numpy
-            >>> a = array((1,2,3))
-            >>> b = array((2,3,4))
-            >>> numpy.dstack((a,b))
-            array([       [[1, 2],
-                    [2, 3],
-                    [3, 4]]])
-            >>> a = array([[1],[2],[3]])
-            >>> b = array([[2],[3],[4]])
-            >>> numpy.dstack((a,b))
-            array([[        [1, 2]],
-                   [        [2, 3]],
-                   [        [3, 4]]])
-    """
-    return _nx.concatenate(map(atleast_3d,tup),2)
+    """
+    Stack arrays in sequence depth wise (along third axis).
+
+    This is equivalent to concatenation along the third axis after 2-D arrays
+    of shape `(M,N)` have been reshaped to `(M,N,1)` and 1-D arrays of shape
+    `(N,)` have been reshaped to `(1,N,1)`. Rebuilds arrays divided by
+    `dsplit`.
+
+    This function makes most sense for arrays with up to 3 dimensions. For
+    instance, for pixel-data with a height (first axis), width (second axis),
+    and r/g/b channels (third axis). The functions `concatenate`, `stack` and
+    `block` provide more general stacking and concatenation operations.
+
+    Parameters
+    ----------
+    tup : sequence of arrays
+        The arrays must have the same shape along all but the third axis.
+        1-D or 2-D arrays must have the same shape.
+
+    Returns
+    -------
+    stacked : ndarray
+        The array formed by stacking the given arrays, will be at least 3-D.
+
+    See Also
+    --------
+    concatenate : Join a sequence of arrays along an existing axis.
+    stack : Join a sequence of arrays along a new axis.
+    block : Assemble an nd-array from nested lists of blocks.
+    vstack : Stack arrays in sequence vertically (row wise).
+    hstack : Stack arrays in sequence horizontally (column wise).
+    column_stack : Stack 1-D arrays as columns into a 2-D array.
+    dsplit : Split array along third axis.
+
+    Examples
+    --------
+    >>> a = np.array((1,2,3))
+    >>> b = np.array((2,3,4))
+    >>> np.dstack((a,b))
+    array([[[1, 2],
+            [2, 3],
+            [3, 4]]])
+
+    >>> a = np.array([[1],[2],[3]])
+    >>> b = np.array([[2],[3],[4]])
+    >>> np.dstack((a,b))
+    array([[[1, 2]],
+           [[2, 3]],
+           [[3, 4]]])
+
+    """
+    if not overrides.ARRAY_FUNCTION_ENABLED:
+        # raise warning if necessary
+        _arrays_for_stack_dispatcher(tup, stacklevel=2)
+
+    arrs = atleast_3d(*tup)
+    if not isinstance(arrs, list):
+        arrs = [arrs]
+    return _nx.concatenate(arrs, 2)
+
 
 def _replace_zero_by_x_arrays(sub_arys):
     for i in range(len(sub_arys)):
-        if len(_nx.shape(sub_arys[i])) == 0:
-            sub_arys[i] = _nx.array([])
-        elif _nx.sometrue(_nx.equal(_nx.shape(sub_arys[i]),0)):
-            sub_arys[i] = _nx.array([])
+        if _nx.ndim(sub_arys[i]) == 0:
+            sub_arys[i] = _nx.empty(0, dtype=sub_arys[i].dtype)
+        elif _nx.sometrue(_nx.equal(_nx.shape(sub_arys[i]), 0)):
+            sub_arys[i] = _nx.empty(0, dtype=sub_arys[i].dtype)
     return sub_arys
 
-def array_split(ary,indices_or_sections,axis = 0):
-    """ Divide an array into a list of sub-arrays.
-
-        Description:
-           Divide ary into a list of sub-arrays along the
-           specified axis.  If indices_or_sections is an integer,
-           ary is divided into that many equally sized arrays.
-           If it is impossible to make an equal split, each of the
-           leading arrays in the list have one additional member.  If
-           indices_or_sections is a list of sorted integers, its
-           entries define the indexes where ary is split.
-
-        Arguments:
-           ary -- N-D array.
-              Array to be divided into sub-arrays.
-           indices_or_sections -- integer or 1D array.
-              If integer, defines the number of (close to) equal sized
-              sub-arrays.  If it is a 1D array of sorted indices, it
-              defines the indexes at which ary is divided.  Any empty
-              list results in a single sub-array equal to the original
-              array.
-           axis -- integer. default=0.
-              Specifies the axis along which to split ary.
-        Caveats:
-           Currently, the default for axis is 0.  This
-           means a 2D array is divided into multiple groups
-           of rows.  This seems like the appropriate default, but
-           we've agreed most other functions should default to
-           axis=-1.  Perhaps we should use axis=-1 for consistency.
-           However, we could also make the argument that NumPy
-           works on "rows" by default.  sum() sums up rows of
-           values.  split() will split data into rows.  Opinions?
+
+def _array_split_dispatcher(ary, indices_or_sections, axis=None):
+    return (ary, indices_or_sections)
+
+
+@array_function_dispatch(_array_split_dispatcher)
+def array_split(ary, indices_or_sections, axis=0):
+    """
+    Split an array into multiple sub-arrays.
+
+    Please refer to the ``split`` documentation.  The only difference
+    between these functions is that ``array_split`` allows
+    `indices_or_sections` to be an integer that does *not* equally
+    divide the axis. For an array of length l that should be split
+    into n sections, it returns l % n sub-arrays of size l//n + 1
+    and the rest of size l//n.
+
+    See Also
+    --------
+    split : Split array into multiple sub-arrays of equal size.
+
+    Examples
+    --------
+    >>> x = np.arange(8.0)
+    >>> np.array_split(x, 3)
+    [array([0.,  1.,  2.]), array([3.,  4.,  5.]), array([6.,  7.])]
+
+    >>> x = np.arange(9)
+    >>> np.array_split(x, 4)
+    [array([0, 1, 2]), array([3, 4]), array([5, 6]), array([7, 8])]
+
     """
     try:
         Ntotal = ary.shape[axis]
     except AttributeError:
         Ntotal = len(ary)
-    try: # handle scalar case.
+    try:
+        # handle array case.
         Nsections = len(indices_or_sections) + 1
         div_points = [0] + list(indices_or_sections) + [Ntotal]
-    except TypeError: #indices_or_sections is a scalar, not an array.
+    except TypeError:
+        # indices_or_sections is a scalar, not an array.
         Nsections = int(indices_or_sections)
         if Nsections <= 0:
-            raise ValueError, 'number sections must be larger than 0.'
-        Neach_section,extras = divmod(Ntotal,Nsections)
-        section_sizes = [0] + \
-                        extras * [Neach_section+1] + \
-                        (Nsections-extras) * [Neach_section]
-        div_points = _nx.array(section_sizes).cumsum()
+            raise ValueError('number sections must be larger than 0.') from None
+        Neach_section, extras = divmod(Ntotal, Nsections)
+        section_sizes = ([0] +
+                         extras * [Neach_section+1] +
+                         (Nsections-extras) * [Neach_section])
+        div_points = _nx.array(section_sizes, dtype=_nx.intp).cumsum()
 
     sub_arys = []
-    sary = _nx.swapaxes(ary,axis,0)
+    sary = _nx.swapaxes(ary, axis, 0)
     for i in range(Nsections):
-        st = div_points[i]; end = div_points[i+1]
-        sub_arys.append(_nx.swapaxes(sary[st:end],axis,0))
-
-    # there is a wierd issue with array slicing that allows
-    # 0x10 arrays and other such things.  The following cluge is needed
-    # to get around this issue.
-    sub_arys = _replace_zero_by_x_arrays(sub_arys)
-    # end cluge.
+        st = div_points[i]
+        end = div_points[i + 1]
+        sub_arys.append(_nx.swapaxes(sary[st:end], axis, 0))
 
     return sub_arys
 
-def split(ary,indices_or_sections,axis=0):
-    """ Divide an array into a list of sub-arrays.
-
-        Description:
-           Divide ary into a list of sub-arrays along the
-           specified axis.  If indices_or_sections is an integer,
-           ary is divided into that many equally sized arrays.
-           If it is impossible to make an equal split, an error is
-           raised.  This is the only way this function differs from
-           the array_split() function. If indices_or_sections is a
-           list of sorted integers, its entries define the indexes
-           where ary is split.
-
-        Arguments:
-           ary -- N-D array.
-              Array to be divided into sub-arrays.
-           indices_or_sections -- integer or 1D array.
-              If integer, defines the number of (close to) equal sized
-              sub-arrays.  If it is a 1D array of sorted indices, it
-              defines the indexes at which ary is divided.  Any empty
-              list results in a single sub-array equal to the original
-              array.
-           axis -- integer. default=0.
-              Specifies the axis along which to split ary.
-        Caveats:
-           Currently, the default for axis is 0.  This
-           means a 2D array is divided into multiple groups
-           of rows.  This seems like the appropriate default, but
-           we've agreed most other functions should default to
-           axis=-1.  Perhaps we should use axis=-1 for consistency.
-           However, we could also make the argument that NumPy
-           works on "rows" by default.  sum() sums up rows of
-           values.  split() will split data into rows.  Opinions?
-    """
-    try: len(indices_or_sections)
+
+def _split_dispatcher(ary, indices_or_sections, axis=None):
+    return (ary, indices_or_sections)
+
+
+@array_function_dispatch(_split_dispatcher)
+def split(ary, indices_or_sections, axis=0):
+    """
+    Split an array into multiple sub-arrays as views into `ary`.
+
+    Parameters
+    ----------
+    ary : ndarray
+        Array to be divided into sub-arrays.
+    indices_or_sections : int or 1-D array
+        If `indices_or_sections` is an integer, N, the array will be divided
+        into N equal arrays along `axis`.  If such a split is not possible,
+        an error is raised.
+
+        If `indices_or_sections` is a 1-D array of sorted integers, the entries
+        indicate where along `axis` the array is split.  For example,
+        ``[2, 3]`` would, for ``axis=0``, result in
+
+          - ary[:2]
+          - ary[2:3]
+          - ary[3:]
+
+        If an index exceeds the dimension of the array along `axis`,
+        an empty sub-array is returned correspondingly.
+    axis : int, optional
+        The axis along which to split, default is 0.
+
+    Returns
+    -------
+    sub-arrays : list of ndarrays
+        A list of sub-arrays as views into `ary`.
+
+    Raises
+    ------
+    ValueError
+        If `indices_or_sections` is given as an integer, but
+        a split does not result in equal division.
+
+    See Also
+    --------
+    array_split : Split an array into multiple sub-arrays of equal or
+                  near-equal size.  Does not raise an exception if
+                  an equal division cannot be made.
+    hsplit : Split array into multiple sub-arrays horizontally (column-wise).
+    vsplit : Split array into multiple sub-arrays vertically (row wise).
+    dsplit : Split array into multiple sub-arrays along the 3rd axis (depth).
+    concatenate : Join a sequence of arrays along an existing axis.
+    stack : Join a sequence of arrays along a new axis.
+    hstack : Stack arrays in sequence horizontally (column wise).
+    vstack : Stack arrays in sequence vertically (row wise).
+    dstack : Stack arrays in sequence depth wise (along third dimension).
+
+    Examples
+    --------
+    >>> x = np.arange(9.0)
+    >>> np.split(x, 3)
+    [array([0.,  1.,  2.]), array([3.,  4.,  5.]), array([6.,  7.,  8.])]
+
+    >>> x = np.arange(8.0)
+    >>> np.split(x, [3, 5, 6, 10])
+    [array([0.,  1.,  2.]),
+     array([3.,  4.]),
+     array([5.]),
+     array([6.,  7.]),
+     array([], dtype=float64)]
+
+    """
+    try:
+        len(indices_or_sections)
     except TypeError:
         sections = indices_or_sections
         N = ary.shape[axis]
         if N % sections:
-            raise ValueError, 'array split does not result in an equal division'
-    res = array_split(ary,indices_or_sections,axis)
-    return res
-
-def hsplit(ary,indices_or_sections):
-    """ Split ary into multiple columns of sub-arrays
-
-        Description:
-            Split a single array into multiple sub arrays.  The array is
-            divided into groups of columns.  If indices_or_sections is
-            an integer, ary is divided into that many equally sized sub arrays.
-            If it is impossible to make the sub-arrays equally sized, the
-            operation throws a ValueError exception. See array_split and
-            split for other options on indices_or_sections.
-        Arguments:
-           ary -- N-D array.
-              Array to be divided into sub-arrays.
-           indices_or_sections -- integer or 1D array.
-              If integer, defines the number of (close to) equal sized
-              sub-arrays.  If it is a 1D array of sorted indices, it
-              defines the indexes at which ary is divided.  Any empty
-              list results in a single sub-array equal to the original
-              array.
-        Returns:
-            sequence of sub-arrays.  The returned arrays have the same
-            number of dimensions as the input array.
-        Related:
-            hstack, split, array_split, vsplit, dsplit.
-        Examples:
-            >>> import numpy
-            >>> a= array((1,2,3,4))
-            >>> numpy.hsplit(a,2)
-            [array([1, 2]), array([3, 4])]
-            >>> a = array([[1,2,3,4],[1,2,3,4]])
-            [array([[1, 2],
-                   [1, 2]]), array([[3, 4],
-                   [3, 4]])]
-
-    """
-    if len(_nx.shape(ary)) == 0:
-        raise ValueError, 'hsplit only works on arrays of 1 or more dimensions'
-    if len(ary.shape) > 1:
-        return split(ary,indices_or_sections,1)
+            raise ValueError(
+                'array split does not result in an equal division') from None
+    return array_split(ary, indices_or_sections, axis)
+
+
+def _hvdsplit_dispatcher(ary, indices_or_sections):
+    return (ary, indices_or_sections)
+
+
+@array_function_dispatch(_hvdsplit_dispatcher)
+def hsplit(ary, indices_or_sections):
+    """
+    Split an array into multiple sub-arrays horizontally (column-wise).
+
+    Please refer to the `split` documentation.  `hsplit` is equivalent
+    to `split` with ``axis=1``, the array is always split along the second
+    axis except for 1-D arrays, where it is split at ``axis=0``.
+
+    See Also
+    --------
+    split : Split an array into multiple sub-arrays of equal size.
+
+    Examples
+    --------
+    >>> x = np.arange(16.0).reshape(4, 4)
+    >>> x
+    array([[ 0.,   1.,   2.,   3.],
+           [ 4.,   5.,   6.,   7.],
+           [ 8.,   9.,  10.,  11.],
+           [12.,  13.,  14.,  15.]])
+    >>> np.hsplit(x, 2)
+    [array([[  0.,   1.],
+           [  4.,   5.],
+           [  8.,   9.],
+           [12.,  13.]]),
+     array([[  2.,   3.],
+           [  6.,   7.],
+           [10.,  11.],
+           [14.,  15.]])]
+    >>> np.hsplit(x, np.array([3, 6]))
+    [array([[ 0.,   1.,   2.],
+           [ 4.,   5.,   6.],
+           [ 8.,   9.,  10.],
+           [12.,  13.,  14.]]),
+     array([[ 3.],
+           [ 7.],
+           [11.],
+           [15.]]),
+     array([], shape=(4, 0), dtype=float64)]
+
+    With a higher dimensional array the split is still along the second axis.
+
+    >>> x = np.arange(8.0).reshape(2, 2, 2)
+    >>> x
+    array([[[0.,  1.],
+            [2.,  3.]],
+           [[4.,  5.],
+            [6.,  7.]]])
+    >>> np.hsplit(x, 2)
+    [array([[[0.,  1.]],
+           [[4.,  5.]]]),
+     array([[[2.,  3.]],
+           [[6.,  7.]]])]
+
+    With a 1-D array, the split is along axis 0.
+
+    >>> x = np.array([0, 1, 2, 3, 4, 5])
+    >>> np.hsplit(x, 2)
+    [array([0, 1, 2]), array([3, 4, 5])]
+
+    """
+    if _nx.ndim(ary) == 0:
+        raise ValueError('hsplit only works on arrays of 1 or more dimensions')
+    if ary.ndim > 1:
+        return split(ary, indices_or_sections, 1)
     else:
-        return split(ary,indices_or_sections,0)
-
-def vsplit(ary,indices_or_sections):
-    """ Split ary into multiple rows of sub-arrays
-
-        Description:
-            Split a single array into multiple sub arrays.  The array is
-            divided into groups of rows.  If indices_or_sections is
-            an integer, ary is divided into that many equally sized sub arrays.
-            If it is impossible to make the sub-arrays equally sized, the
-            operation throws a ValueError exception. See array_split and
-            split for other options on indices_or_sections.
-        Arguments:
-           ary -- N-D array.
-              Array to be divided into sub-arrays.
-           indices_or_sections -- integer or 1D array.
-              If integer, defines the number of (close to) equal sized
-              sub-arrays.  If it is a 1D array of sorted indices, it
-              defines the indexes at which ary is divided.  Any empty
-              list results in a single sub-array equal to the original
-              array.
-        Returns:
-            sequence of sub-arrays.  The returned arrays have the same
-            number of dimensions as the input array.
-        Caveats:
-           How should we handle 1D arrays here?  I am currently raising
-           an error when I encounter them.  Any better approach?
-
-           Should we reduce the returned array to their minium dimensions
-           by getting rid of any dimensions that are 1?
-        Related:
-            vstack, split, array_split, hsplit, dsplit.
-        Examples:
-            import numpy
-            >>> a = array([[1,2,3,4],
-            ...            [1,2,3,4]])
-            >>> numpy.vsplit(a)
-            [array([       [1, 2, 3, 4]]), array([       [1, 2, 3, 4]])]
-
-    """
-    if len(_nx.shape(ary)) < 2:
-        raise ValueError, 'vsplit only works on arrays of 2 or more dimensions'
-    return split(ary,indices_or_sections,0)
-
-def dsplit(ary,indices_or_sections):
-    """ Split ary into multiple sub-arrays along the 3rd axis (depth)
-
-        Description:
-            Split a single array into multiple sub arrays.  The array is
-            divided into groups along the 3rd axis.  If indices_or_sections is
-            an integer, ary is divided into that many equally sized sub arrays.
-            If it is impossible to make the sub-arrays equally sized, the
-            operation throws a ValueError exception. See array_split and
-            split for other options on indices_or_sections.
-        Arguments:
-           ary -- N-D array.
-              Array to be divided into sub-arrays.
-           indices_or_sections -- integer or 1D array.
-              If integer, defines the number of (close to) equal sized
-              sub-arrays.  If it is a 1D array of sorted indices, it
-              defines the indexes at which ary is divided.  Any empty
-              list results in a single sub-array equal to the original
-              array.
-        Returns:
-            sequence of sub-arrays.  The returned arrays have the same
-            number of dimensions as the input array.
-        Caveats:
-           See vsplit caveats.
-        Related:
-            dstack, split, array_split, hsplit, vsplit.
-        Examples:
-            >>> a = array([[[1,2,3,4],[1,2,3,4]]])
-            [array([       [[1, 2],
-                    [1, 2]]]), array([       [[3, 4],
-                    [3, 4]]])]
-
-    """
-    if len(_nx.shape(ary)) < 3:
-        raise ValueError, 'vsplit only works on arrays of 3 or more dimensions'
-    return split(ary,indices_or_sections,2)
-
-# should figure out how to generalize this one.
-def repmat(a, m, n):
-    """Repeat a 0-d to 2-d array mxn times
-    """
-    a = asarray(a)
-    ndim = a.ndim
-    if ndim == 0:
-        origrows, origcols = (1,1)
-    elif ndim == 1:
-        origrows, origcols = (1, a.shape[0])
-    else:
-        origrows, origcols = a.shape
-    rows = origrows * m
-    cols = origcols * n
-    c = a.reshape(1,a.size).repeat(m, 0).reshape(rows, origcols).repeat(n,0)
-    return c.reshape(rows, cols)
-
-
-def _getwrapper(*args):
+        return split(ary, indices_or_sections, 0)
+
+
+@array_function_dispatch(_hvdsplit_dispatcher)
+def vsplit(ary, indices_or_sections):
+    """
+    Split an array into multiple sub-arrays vertically (row-wise).
+
+    Please refer to the ``split`` documentation.  ``vsplit`` is equivalent
+    to ``split`` with `axis=0` (default), the array is always split along the
+    first axis regardless of the array dimension.
+
+    See Also
+    --------
+    split : Split an array into multiple sub-arrays of equal size.
+
+    Examples
+    --------
+    >>> x = np.arange(16.0).reshape(4, 4)
+    >>> x
+    array([[ 0.,   1.,   2.,   3.],
+           [ 4.,   5.,   6.,   7.],
+           [ 8.,   9.,  10.,  11.],
+           [12.,  13.,  14.,  15.]])
+    >>> np.vsplit(x, 2)
+    [array([[0., 1., 2., 3.],
+           [4., 5., 6., 7.]]), array([[ 8.,  9., 10., 11.],
+           [12., 13., 14., 15.]])]
+    >>> np.vsplit(x, np.array([3, 6]))
+    [array([[ 0.,  1.,  2.,  3.],
+           [ 4.,  5.,  6.,  7.],
+           [ 8.,  9., 10., 11.]]), array([[12., 13., 14., 15.]]), array([], shape=(0, 4), dtype=float64)]
+
+    With a higher dimensional array the split is still along the first axis.
+
+    >>> x = np.arange(8.0).reshape(2, 2, 2)
+    >>> x
+    array([[[0.,  1.],
+            [2.,  3.]],
+           [[4.,  5.],
+            [6.,  7.]]])
+    >>> np.vsplit(x, 2)
+    [array([[[0., 1.],
+            [2., 3.]]]), array([[[4., 5.],
+            [6., 7.]]])]
+
+    """
+    if _nx.ndim(ary) < 2:
+        raise ValueError('vsplit only works on arrays of 2 or more dimensions')
+    return split(ary, indices_or_sections, 0)
+
+
+@array_function_dispatch(_hvdsplit_dispatcher)
+def dsplit(ary, indices_or_sections):
+    """
+    Split array into multiple sub-arrays along the 3rd axis (depth).
+
+    Please refer to the `split` documentation.  `dsplit` is equivalent
+    to `split` with ``axis=2``, the array is always split along the third
+    axis provided the array dimension is greater than or equal to 3.
+
+    See Also
+    --------
+    split : Split an array into multiple sub-arrays of equal size.
+
+    Examples
+    --------
+    >>> x = np.arange(16.0).reshape(2, 2, 4)
+    >>> x
+    array([[[ 0.,   1.,   2.,   3.],
+            [ 4.,   5.,   6.,   7.]],
+           [[ 8.,   9.,  10.,  11.],
+            [12.,  13.,  14.,  15.]]])
+    >>> np.dsplit(x, 2)
+    [array([[[ 0.,  1.],
+            [ 4.,  5.]],
+           [[ 8.,  9.],
+            [12., 13.]]]), array([[[ 2.,  3.],
+            [ 6.,  7.]],
+           [[10., 11.],
+            [14., 15.]]])]
+    >>> np.dsplit(x, np.array([3, 6]))
+    [array([[[ 0.,   1.,   2.],
+            [ 4.,   5.,   6.]],
+           [[ 8.,   9.,  10.],
+            [12.,  13.,  14.]]]),
+     array([[[ 3.],
+            [ 7.]],
+           [[11.],
+            [15.]]]),
+    array([], shape=(2, 2, 0), dtype=float64)]
+    """
+    if _nx.ndim(ary) < 3:
+        raise ValueError('dsplit only works on arrays of 3 or more dimensions')
+    return split(ary, indices_or_sections, 2)
+
+def get_array_prepare(*args):
     """Find the wrapper for the array with the highest priority.
 
     In case of ties, leftmost wins. If no wrapper is found, return None
     """
-    wrappers = [(getattr(x, '__array_priority__', 0), -i,
-                 x.__array_wrap__) for i, x in enumerate(args) 
-                                   if hasattr(x, '__array_wrap__')]
-    wrappers.sort()
+    wrappers = sorted((getattr(x, '__array_priority__', 0), -i,
+                 x.__array_prepare__) for i, x in enumerate(args)
+                                   if hasattr(x, '__array_prepare__'))
     if wrappers:
         return wrappers[-1][-1]
-    return None 
-
-def kron(a,b):
-    """kronecker product of a and b
-
-    Kronecker product of two matrices is block matrix
-    [[ a[ 0 ,0]*b, a[ 0 ,1]*b, ... , a[ 0 ,n-1]*b  ],
-     [ ...                                   ...   ],
-     [ a[m-1,0]*b, a[m-1,1]*b, ... , a[m-1,n-1]*b  ]]
-    """
-    wrapper = _getwrapper(a, b)
-    a = asanyarray(a)
+    return None
+
+def get_array_wrap(*args):
+    """Find the wrapper for the array with the highest priority.
+
+    In case of ties, leftmost wins. If no wrapper is found, return None
+    """
+    wrappers = sorted((getattr(x, '__array_priority__', 0), -i,
+                 x.__array_wrap__) for i, x in enumerate(args)
+                                   if hasattr(x, '__array_wrap__'))
+    if wrappers:
+        return wrappers[-1][-1]
+    return None
+
+
+def _kron_dispatcher(a, b):
+    return (a, b)
+
+
+@array_function_dispatch(_kron_dispatcher)
+def kron(a, b):
+    """
+    Kronecker product of two arrays.
+
+    Computes the Kronecker product, a composite array made of blocks of the
+    second array scaled by the first.
+
+    Parameters
+    ----------
+    a, b : array_like
+
+    Returns
+    -------
+    out : ndarray
+
+    See Also
+    --------
+    outer : The outer product
+
+    Notes
+    -----
+    The function assumes that the number of dimensions of `a` and `b`
+    are the same, if necessary prepending the smallest with ones.
+    If ``a.shape = (r0,r1,..,rN)`` and ``b.shape = (s0,s1,...,sN)``,
+    the Kronecker product has shape ``(r0*s0, r1*s1, ..., rN*SN)``.
+    The elements are products of elements from `a` and `b`, organized
+    explicitly by::
+
+        kron(a,b)[k0,k1,...,kN] = a[i0,i1,...,iN] * b[j0,j1,...,jN]
+
+    where::
+
+        kt = it * st + jt,  t = 0,...,N
+
+    In the common 2-D case (N=1), the block structure can be visualized::
+
+        [[ a[0,0]*b,   a[0,1]*b,  ... , a[0,-1]*b  ],
+         [  ...                              ...   ],
+         [ a[-1,0]*b,  a[-1,1]*b, ... , a[-1,-1]*b ]]
+
+
+    Examples
+    --------
+    >>> np.kron([1,10,100], [5,6,7])
+    array([  5,   6,   7, ..., 500, 600, 700])
+    >>> np.kron([5,6,7], [1,10,100])
+    array([  5,  50, 500, ...,   7,  70, 700])
+
+    >>> np.kron(np.eye(2), np.ones((2,2)))
+    array([[1.,  1.,  0.,  0.],
+           [1.,  1.,  0.,  0.],
+           [0.,  0.,  1.,  1.],
+           [0.,  0.,  1.,  1.]])
+
+    >>> a = np.arange(100).reshape((2,5,2,5))
+    >>> b = np.arange(24).reshape((2,3,4))
+    >>> c = np.kron(a,b)
+    >>> c.shape
+    (2, 10, 6, 20)
+    >>> I = (1,3,0,2)
+    >>> J = (0,2,1)
+    >>> J1 = (0,) + J             # extend to ndim=4
+    >>> S1 = (1,) + b.shape
+    >>> K = tuple(np.array(I) * np.array(S1) + np.array(J1))
+    >>> c[K] == a[I]*b[J]
+    True
+
+    """
+    # Working:
+    # 1. Equalise the shapes by prepending smaller array with 1s
+    # 2. Expand shapes of both the arrays by adding new axes at
+    #    odd positions for 1st array and even positions for 2nd
+    # 3. Compute the product of the modified array
+    # 4. The inner most array elements now contain the rows of
+    #    the Kronecker product
+    # 5. Reshape the result to kron's shape, which is same as
+    #    product of shapes of the two arrays.
     b = asanyarray(b)
-    if not (len(a.shape) == len(b.shape) == 2):
-        raise ValueError("a and b must both be two dimensional")
+    a = array(a, copy=False, subok=True, ndmin=b.ndim)
+    is_any_mat = isinstance(a, matrix) or isinstance(b, matrix)
+    ndb, nda = b.ndim, a.ndim
+    nd = max(ndb, nda)
+
+    if (nda == 0 or ndb == 0):
+        return _nx.multiply(a, b)
+
+    as_ = a.shape
+    bs = b.shape
     if not a.flags.contiguous:
-        a = reshape(a, a.shape)
+        a = reshape(a, as_)
     if not b.flags.contiguous:
-        b = reshape(b, b.shape)
-    o = outerproduct(a,b)
-    o=o.reshape(a.shape + b.shape)
-    result = concatenate(concatenate(o, axis=1), axis=1)
-    if wrapper is not None:
-        result = wrapper(result)
-    return result
+        b = reshape(b, bs)
+
+    # Equalise the shapes by prepending smaller one with 1s
+    as_ = (1,)*max(0, ndb-nda) + as_
+    bs = (1,)*max(0, nda-ndb) + bs
+
+    # Insert empty dimensions
+    a_arr = expand_dims(a, axis=tuple(range(ndb-nda)))
+    b_arr = expand_dims(b, axis=tuple(range(nda-ndb)))
+
+    # Compute the product
+    a_arr = expand_dims(a_arr, axis=tuple(range(1, nd*2, 2)))
+    b_arr = expand_dims(b_arr, axis=tuple(range(0, nd*2, 2)))
+    # In case of `mat`, convert result to `array`
+    result = _nx.multiply(a_arr, b_arr, subok=(not is_any_mat))
+
+    # Reshape back
+    result = result.reshape(_nx.multiply(as_, bs))
+
+    return result if not is_any_mat else matrix(result, copy=False)
+
+
+def _tile_dispatcher(A, reps):
+    return (A, reps)
+
+
+@array_function_dispatch(_tile_dispatcher)
+def tile(A, reps):
+    """
+    Construct an array by repeating A the number of times given by reps.
+
+    If `reps` has length ``d``, the result will have dimension of
+    ``max(d, A.ndim)``.
+
+    If ``A.ndim < d``, `A` is promoted to be d-dimensional by prepending new
+    axes. So a shape (3,) array is promoted to (1, 3) for 2-D replication,
+    or shape (1, 1, 3) for 3-D replication. If this is not the desired
+    behavior, promote `A` to d-dimensions manually before calling this
+    function.
+
+    If ``A.ndim > d``, `reps` is promoted to `A`.ndim by pre-pending 1's to it.
+    Thus for an `A` of shape (2, 3, 4, 5), a `reps` of (2, 2) is treated as
+    (1, 1, 2, 2).
+
+    Note : Although tile may be used for broadcasting, it is strongly
+    recommended to use numpy's broadcasting operations and functions.
+
+    Parameters
+    ----------
+    A : array_like
+        The input array.
+    reps : array_like
+        The number of repetitions of `A` along each axis.
+
+    Returns
+    -------
+    c : ndarray
+        The tiled output array.
+
+    See Also
+    --------
+    repeat : Repeat elements of an array.
+    broadcast_to : Broadcast an array to a new shape
+
+    Examples
+    --------
+    >>> a = np.array([0, 1, 2])
+    >>> np.tile(a, 2)
+    array([0, 1, 2, 0, 1, 2])
+    >>> np.tile(a, (2, 2))
+    array([[0, 1, 2, 0, 1, 2],
+           [0, 1, 2, 0, 1, 2]])
+    >>> np.tile(a, (2, 1, 2))
+    array([[[0, 1, 2, 0, 1, 2]],
+           [[0, 1, 2, 0, 1, 2]]])
+
+    >>> b = np.array([[1, 2], [3, 4]])
+    >>> np.tile(b, 2)
+    array([[1, 2, 1, 2],
+           [3, 4, 3, 4]])
+    >>> np.tile(b, (2, 1))
+    array([[1, 2],
+           [3, 4],
+           [1, 2],
+           [3, 4]])
+
+    >>> c = np.array([1,2,3,4])
+    >>> np.tile(c,(4,1))
+    array([[1, 2, 3, 4],
+           [1, 2, 3, 4],
+           [1, 2, 3, 4],
+           [1, 2, 3, 4]])
+    """
+    try:
+        tup = tuple(reps)
+    except TypeError:
+        tup = (reps,)
+    d = len(tup)
+    if all(x == 1 for x in tup) and isinstance(A, _nx.ndarray):
+        # Fixes the problem that the function does not make a copy if A is a
+        # numpy array and the repetitions are 1 in all dimensions
+        return _nx.array(A, copy=True, subok=True, ndmin=d)
+    else:
+        # Note that no copy of zero-sized arrays is made. However since they
+        # have no data there is no risk of an inadvertent overwrite.
+        c = _nx.array(A, copy=False, subok=True, ndmin=d)
+    if (d < c.ndim):
+        tup = (1,)*(c.ndim-d) + tup
+    shape_out = tuple(s*t for s, t in zip(c.shape, tup))
+    n = c.size
+    if n > 0:
+        for dim_in, nrep in zip(c.shape, tup):
+            if nrep != 1:
+                c = c.reshape(-1, n).repeat(nrep, 0)
+            n //= dim_in
+    return c.reshape(shape_out)
('numpy/lib', 'setup.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,28 +1,12 @@
+def configuration(parent_package='',top_path=None):
+    from numpy.distutils.misc_util import Configuration
 
-import imp
-import os
-from os.path import join
-from glob import glob
-from distutils.dep_util import newer,newer_group
-
-def configuration(parent_package='',top_path=None):
-    from numpy.distutils.misc_util import Configuration,dot_join
-    from numpy.distutils.system_info import get_info
-
-    config = Configuration('lib',parent_package,top_path)
-    local_dir = config.local_path
-
-    config.add_include_dirs(join('..','core','include'))
-
-
-    config.add_extension('_compiled_base',
-                         sources=[join('src','_compiled_base.c')]
-                         )
-
-    config.add_data_dir('tests')
-
+    config = Configuration('lib', parent_package, top_path)
+    config.add_subpackage('tests')
+    config.add_data_dir('tests/data')
+    config.add_data_files('*.pyi')
     return config
 
-if __name__=='__main__':
+if __name__ == '__main__':
     from numpy.distutils.core import setup
-    setup(**configuration(top_path='').todict())
+    setup(configuration=configuration)
('numpy/lib', 'utils.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,28 +1,1073 @@
-from numpy.core.numerictypes import obj2sctype
-
-__all__ = ['issubclass_', 'get_numpy_include', 'issubsctype']
-
-def issubclass_(arg1, arg2):
+import os
+import sys
+import textwrap
+import types
+import re
+import warnings
+
+from numpy.core.numerictypes import issubclass_, issubsctype, issubdtype
+from numpy.core.overrides import set_module
+from numpy.core import ndarray, ufunc, asarray
+import numpy as np
+
+__all__ = [
+    'issubclass_', 'issubsctype', 'issubdtype', 'deprecate',
+    'deprecate_with_doc', 'get_include', 'info', 'source', 'who',
+    'lookfor', 'byte_bounds', 'safe_eval'
+    ]
+
+def get_include():
+    """
+    Return the directory that contains the NumPy \\*.h header files.
+
+    Extension modules that need to compile against NumPy should use this
+    function to locate the appropriate include directory.
+
+    Notes
+    -----
+    When using ``distutils``, for example in ``setup.py``::
+
+        import numpy as np
+        ...
+        Extension('extension_name', ...
+                include_dirs=[np.get_include()])
+        ...
+
+    """
+    import numpy
+    if numpy.show_config is None:
+        # running from numpy source directory
+        d = os.path.join(os.path.dirname(numpy.__file__), 'core', 'include')
+    else:
+        # using installed numpy core headers
+        import numpy.core as core
+        d = os.path.join(os.path.dirname(core.__file__), 'include')
+    return d
+
+
+def _set_function_name(func, name):
+    func.__name__ = name
+    return func
+
+
+class _Deprecate:
+    """
+    Decorator class to deprecate old functions.
+
+    Refer to `deprecate` for details.
+
+    See Also
+    --------
+    deprecate
+
+    """
+
+    def __init__(self, old_name=None, new_name=None, message=None):
+        self.old_name = old_name
+        self.new_name = new_name
+        self.message = message
+
+    def __call__(self, func, *args, **kwargs):
+        """
+        Decorator call.  Refer to ``decorate``.
+
+        """
+        old_name = self.old_name
+        new_name = self.new_name
+        message = self.message
+
+        if old_name is None:
+            try:
+                old_name = func.__name__
+            except AttributeError:
+                old_name = func.__name__
+        if new_name is None:
+            depdoc = "`%s` is deprecated!" % old_name
+        else:
+            depdoc = "`%s` is deprecated, use `%s` instead!" % \
+                     (old_name, new_name)
+
+        if message is not None:
+            depdoc += "\n" + message
+
+        def newfunc(*args,**kwds):
+            """`arrayrange` is deprecated, use `arange` instead!"""
+            warnings.warn(depdoc, DeprecationWarning, stacklevel=2)
+            return func(*args, **kwds)
+
+        newfunc = _set_function_name(newfunc, old_name)
+        doc = func.__doc__
+        if doc is None:
+            doc = depdoc
+        else:
+            lines = doc.expandtabs().split('\n')
+            indent = _get_indent(lines[1:])
+            if lines[0].lstrip():
+                # Indent the original first line to let inspect.cleandoc()
+                # dedent the docstring despite the deprecation notice.
+                doc = indent * ' ' + doc
+            else:
+                # Remove the same leading blank lines as cleandoc() would.
+                skip = len(lines[0]) + 1
+                for line in lines[1:]:
+                    if len(line) > indent:
+                        break
+                    skip += len(line) + 1
+                doc = doc[skip:]
+            depdoc = textwrap.indent(depdoc, ' ' * indent)
+            doc = '\n\n'.join([depdoc, doc])
+        newfunc.__doc__ = doc
+        try:
+            d = func.__dict__
+        except AttributeError:
+            pass
+        else:
+            newfunc.__dict__.update(d)
+        return newfunc
+
+
+def _get_indent(lines):
+    """
+    Determines the leading whitespace that could be removed from all the lines.
+    """
+    indent = sys.maxsize
+    for line in lines:
+        content = len(line.lstrip())
+        if content:
+            indent = min(indent, len(line) - content)
+    if indent == sys.maxsize:
+        indent = 0
+    return indent
+
+
+def deprecate(*args, **kwargs):
+    """
+    Issues a DeprecationWarning, adds warning to `old_name`'s
+    docstring, rebinds ``old_name.__name__`` and returns the new
+    function object.
+
+    This function may also be used as a decorator.
+
+    Parameters
+    ----------
+    func : function
+        The function to be deprecated.
+    old_name : str, optional
+        The name of the function to be deprecated. Default is None, in
+        which case the name of `func` is used.
+    new_name : str, optional
+        The new name for the function. Default is None, in which case the
+        deprecation message is that `old_name` is deprecated. If given, the
+        deprecation message is that `old_name` is deprecated and `new_name`
+        should be used instead.
+    message : str, optional
+        Additional explanation of the deprecation.  Displayed in the
+        docstring after the warning.
+
+    Returns
+    -------
+    old_func : function
+        The deprecated function.
+
+    Examples
+    --------
+    Note that ``olduint`` returns a value after printing Deprecation
+    Warning:
+
+    >>> olduint = np.deprecate(np.uint)
+    DeprecationWarning: `uint64` is deprecated! # may vary
+    >>> olduint(6)
+    6
+
+    """
+    # Deprecate may be run as a function or as a decorator
+    # If run as a function, we initialise the decorator class
+    # and execute its __call__ method.
+
+    if args:
+        fn = args[0]
+        args = args[1:]
+
+        return _Deprecate(*args, **kwargs)(fn)
+    else:
+        return _Deprecate(*args, **kwargs)
+
+
+def deprecate_with_doc(msg):
+    """
+    Deprecates a function and includes the deprecation in its docstring.
+
+    This function is used as a decorator. It returns an object that can be
+    used to issue a DeprecationWarning, by passing the to-be decorated
+    function as argument, this adds warning to the to-be decorated function's
+    docstring and returns the new function object.
+
+    See Also
+    --------
+    deprecate : Decorate a function such that it issues a `DeprecationWarning`
+
+    Parameters
+    ----------
+    msg : str
+        Additional explanation of the deprecation. Displayed in the
+        docstring after the warning.
+
+    Returns
+    -------
+    obj : object
+
+    """
+    return _Deprecate(message=msg)
+
+
+#--------------------------------------------
+# Determine if two arrays can share memory
+#--------------------------------------------
+
+def byte_bounds(a):
+    """
+    Returns pointers to the end-points of an array.
+
+    Parameters
+    ----------
+    a : ndarray
+        Input array. It must conform to the Python-side of the array
+        interface.
+
+    Returns
+    -------
+    (low, high) : tuple of 2 integers
+        The first integer is the first byte of the array, the second
+        integer is just past the last byte of the array.  If `a` is not
+        contiguous it will not use every byte between the (`low`, `high`)
+        values.
+
+    Examples
+    --------
+    >>> I = np.eye(2, dtype='f'); I.dtype
+    dtype('float32')
+    >>> low, high = np.byte_bounds(I)
+    >>> high - low == I.size*I.itemsize
+    True
+    >>> I = np.eye(2); I.dtype
+    dtype('float64')
+    >>> low, high = np.byte_bounds(I)
+    >>> high - low == I.size*I.itemsize
+    True
+
+    """
+    ai = a.__array_interface__
+    a_data = ai['data'][0]
+    astrides = ai['strides']
+    ashape = ai['shape']
+    bytes_a = asarray(a).dtype.itemsize
+
+    a_low = a_high = a_data
+    if astrides is None:
+        # contiguous case
+        a_high += a.size * bytes_a
+    else:
+        for shape, stride in zip(ashape, astrides):
+            if stride < 0:
+                a_low += (shape-1)*stride
+            else:
+                a_high += (shape-1)*stride
+        a_high += bytes_a
+    return a_low, a_high
+
+
+#-----------------------------------------------------------------------------
+# Function for output and information on the variables used.
+#-----------------------------------------------------------------------------
+
+
+def who(vardict=None):
+    """
+    Print the NumPy arrays in the given dictionary.
+
+    If there is no dictionary passed in or `vardict` is None then returns
+    NumPy arrays in the globals() dictionary (all NumPy arrays in the
+    namespace).
+
+    Parameters
+    ----------
+    vardict : dict, optional
+        A dictionary possibly containing ndarrays.  Default is globals().
+
+    Returns
+    -------
+    out : None
+        Returns 'None'.
+
+    Notes
+    -----
+    Prints out the name, shape, bytes and type of all of the ndarrays
+    present in `vardict`.
+
+    Examples
+    --------
+    >>> a = np.arange(10)
+    >>> b = np.ones(20)
+    >>> np.who()
+    Name            Shape            Bytes            Type
+    ===========================================================
+    a               10               80               int64
+    b               20               160              float64
+    Upper bound on total bytes  =       240
+
+    >>> d = {'x': np.arange(2.0), 'y': np.arange(3.0), 'txt': 'Some str',
+    ... 'idx':5}
+    >>> np.who(d)
+    Name            Shape            Bytes            Type
+    ===========================================================
+    x               2                16               float64
+    y               3                24               float64
+    Upper bound on total bytes  =       40
+
+    """
+    if vardict is None:
+        frame = sys._getframe().f_back
+        vardict = frame.f_globals
+    sta = []
+    cache = {}
+    for name in vardict.keys():
+        if isinstance(vardict[name], ndarray):
+            var = vardict[name]
+            idv = id(var)
+            if idv in cache.keys():
+                namestr = name + " (%s)" % cache[idv]
+                original = 0
+            else:
+                cache[idv] = name
+                namestr = name
+                original = 1
+            shapestr = " x ".join(map(str, var.shape))
+            bytestr = str(var.nbytes)
+            sta.append([namestr, shapestr, bytestr, var.dtype.name,
+                        original])
+
+    maxname = 0
+    maxshape = 0
+    maxbyte = 0
+    totalbytes = 0
+    for val in sta:
+        if maxname < len(val[0]):
+            maxname = len(val[0])
+        if maxshape < len(val[1]):
+            maxshape = len(val[1])
+        if maxbyte < len(val[2]):
+            maxbyte = len(val[2])
+        if val[4]:
+            totalbytes += int(val[2])
+
+    if len(sta) > 0:
+        sp1 = max(10, maxname)
+        sp2 = max(10, maxshape)
+        sp3 = max(10, maxbyte)
+        prval = "Name %s Shape %s Bytes %s Type" % (sp1*' ', sp2*' ', sp3*' ')
+        print(prval + "\n" + "="*(len(prval)+5) + "\n")
+
+    for val in sta:
+        print("%s %s %s %s %s %s %s" % (val[0], ' '*(sp1-len(val[0])+4),
+                                        val[1], ' '*(sp2-len(val[1])+5),
+                                        val[2], ' '*(sp3-len(val[2])+5),
+                                        val[3]))
+    print("\nUpper bound on total bytes  =       %d" % totalbytes)
+    return
+
+#-----------------------------------------------------------------------------
+
+
+# NOTE:  pydoc defines a help function which works similarly to this
+#  except it uses a pager to take over the screen.
+
+# combine name and arguments and split to multiple lines of width
+# characters.  End lines on a comma and begin argument list indented with
+# the rest of the arguments.
+def _split_line(name, arguments, width):
+    firstwidth = len(name)
+    k = firstwidth
+    newstr = name
+    sepstr = ", "
+    arglist = arguments.split(sepstr)
+    for argument in arglist:
+        if k == firstwidth:
+            addstr = ""
+        else:
+            addstr = sepstr
+        k = k + len(argument) + len(addstr)
+        if k > width:
+            k = firstwidth + 1 + len(argument)
+            newstr = newstr + ",\n" + " "*(firstwidth+2) + argument
+        else:
+            newstr = newstr + addstr + argument
+    return newstr
+
+_namedict = None
+_dictlist = None
+
+# Traverse all module directories underneath globals
+# to see if something is defined
+def _makenamedict(module='numpy'):
+    module = __import__(module, globals(), locals(), [])
+    thedict = {module.__name__:module.__dict__}
+    dictlist = [module.__name__]
+    totraverse = [module.__dict__]
+    while True:
+        if len(totraverse) == 0:
+            break
+        thisdict = totraverse.pop(0)
+        for x in thisdict.keys():
+            if isinstance(thisdict[x], types.ModuleType):
+                modname = thisdict[x].__name__
+                if modname not in dictlist:
+                    moddict = thisdict[x].__dict__
+                    dictlist.append(modname)
+                    totraverse.append(moddict)
+                    thedict[modname] = moddict
+    return thedict, dictlist
+
+
+def _info(obj, output=None):
+    """Provide information about ndarray obj.
+
+    Parameters
+    ----------
+    obj : ndarray
+        Must be ndarray, not checked.
+    output
+        Where printed output goes.
+
+    Notes
+    -----
+    Copied over from the numarray module prior to its removal.
+    Adapted somewhat as only numpy is an option now.
+
+    Called by info.
+
+    """
+    extra = ""
+    tic = ""
+    bp = lambda x: x
+    cls = getattr(obj, '__class__', type(obj))
+    nm = getattr(cls, '__name__', cls)
+    strides = obj.strides
+    endian = obj.dtype.byteorder
+
+    if output is None:
+        output = sys.stdout
+
+    print("class: ", nm, file=output)
+    print("shape: ", obj.shape, file=output)
+    print("strides: ", strides, file=output)
+    print("itemsize: ", obj.itemsize, file=output)
+    print("aligned: ", bp(obj.flags.aligned), file=output)
+    print("contiguous: ", bp(obj.flags.contiguous), file=output)
+    print("fortran: ", obj.flags.fortran, file=output)
+    print(
+        "data pointer: %s%s" % (hex(obj.ctypes._as_parameter_.value), extra),
+        file=output
+        )
+    print("byteorder: ", end=' ', file=output)
+    if endian in ['|', '=']:
+        print("%s%s%s" % (tic, sys.byteorder, tic), file=output)
+        byteswap = False
+    elif endian == '>':
+        print("%sbig%s" % (tic, tic), file=output)
+        byteswap = sys.byteorder != "big"
+    else:
+        print("%slittle%s" % (tic, tic), file=output)
+        byteswap = sys.byteorder != "little"
+    print("byteswap: ", bp(byteswap), file=output)
+    print("type: %s" % obj.dtype, file=output)
+
+
+@set_module('numpy')
+def info(object=None, maxwidth=76, output=None, toplevel='numpy'):
+    """
+    Get help information for a function, class, or module.
+
+    Parameters
+    ----------
+    object : object or str, optional
+        Input object or name to get information about. If `object` is a
+        numpy object, its docstring is given. If it is a string, available
+        modules are searched for matching objects.  If None, information
+        about `info` itself is returned.
+    maxwidth : int, optional
+        Printing width.
+    output : file like object, optional
+        File like object that the output is written to, default is
+        ``None``, in which case ``sys.stdout`` will be used.
+        The object has to be opened in 'w' or 'a' mode.
+    toplevel : str, optional
+        Start search at this level.
+
+    See Also
+    --------
+    source, lookfor
+
+    Notes
+    -----
+    When used interactively with an object, ``np.info(obj)`` is equivalent
+    to ``help(obj)`` on the Python prompt or ``obj?`` on the IPython
+    prompt.
+
+    Examples
+    --------
+    >>> np.info(np.polyval) # doctest: +SKIP
+       polyval(p, x)
+         Evaluate the polynomial p at x.
+         ...
+
+    When using a string for `object` it is possible to get multiple results.
+
+    >>> np.info('fft') # doctest: +SKIP
+         *** Found in numpy ***
+    Core FFT routines
+    ...
+         *** Found in numpy.fft ***
+     fft(a, n=None, axis=-1)
+    ...
+         *** Repeat reference found in numpy.fft.fftpack ***
+         *** Total of 3 references found. ***
+
+    """
+    global _namedict, _dictlist
+    # Local import to speed up numpy's import time.
+    import pydoc
+    import inspect
+
+    if (hasattr(object, '_ppimport_importer') or
+           hasattr(object, '_ppimport_module')):
+        object = object._ppimport_module
+    elif hasattr(object, '_ppimport_attr'):
+        object = object._ppimport_attr
+
+    if output is None:
+        output = sys.stdout
+
+    if object is None:
+        info(info)
+    elif isinstance(object, ndarray):
+        _info(object, output=output)
+    elif isinstance(object, str):
+        if _namedict is None:
+            _namedict, _dictlist = _makenamedict(toplevel)
+        numfound = 0
+        objlist = []
+        for namestr in _dictlist:
+            try:
+                obj = _namedict[namestr][object]
+                if id(obj) in objlist:
+                    print("\n     "
+                          "*** Repeat reference found in %s *** " % namestr,
+                          file=output
+                          )
+                else:
+                    objlist.append(id(obj))
+                    print("     *** Found in %s ***" % namestr, file=output)
+                    info(obj)
+                    print("-"*maxwidth, file=output)
+                numfound += 1
+            except KeyError:
+                pass
+        if numfound == 0:
+            print("Help for %s not found." % object, file=output)
+        else:
+            print("\n     "
+                  "*** Total of %d references found. ***" % numfound,
+                  file=output
+                  )
+
+    elif inspect.isfunction(object) or inspect.ismethod(object):
+        name = object.__name__
+        try:
+            arguments = str(inspect.signature(object))
+        except Exception:
+            arguments = "()"
+
+        if len(name+arguments) > maxwidth:
+            argstr = _split_line(name, arguments, maxwidth)
+        else:
+            argstr = name + arguments
+
+        print(" " + argstr + "\n", file=output)
+        print(inspect.getdoc(object), file=output)
+
+    elif inspect.isclass(object):
+        name = object.__name__
+        try:
+            arguments = str(inspect.signature(object))
+        except Exception:
+            arguments = "()"
+
+        if len(name+arguments) > maxwidth:
+            argstr = _split_line(name, arguments, maxwidth)
+        else:
+            argstr = name + arguments
+
+        print(" " + argstr + "\n", file=output)
+        doc1 = inspect.getdoc(object)
+        if doc1 is None:
+            if hasattr(object, '__init__'):
+                print(inspect.getdoc(object.__init__), file=output)
+        else:
+            print(inspect.getdoc(object), file=output)
+
+        methods = pydoc.allmethods(object)
+
+        public_methods = [meth for meth in methods if meth[0] != '_']
+        if public_methods:
+            print("\n\nMethods:\n", file=output)
+            for meth in public_methods:
+                thisobj = getattr(object, meth, None)
+                if thisobj is not None:
+                    methstr, other = pydoc.splitdoc(
+                            inspect.getdoc(thisobj) or "None"
+                            )
+                print("  %s  --  %s" % (meth, methstr), file=output)
+
+    elif hasattr(object, '__doc__'):
+        print(inspect.getdoc(object), file=output)
+
+
+@set_module('numpy')
+def source(object, output=sys.stdout):
+    """
+    Print or write to a file the source code for a NumPy object.
+
+    The source code is only returned for objects written in Python. Many
+    functions and classes are defined in C and will therefore not return
+    useful information.
+
+    Parameters
+    ----------
+    object : numpy object
+        Input object. This can be any object (function, class, module,
+        ...).
+    output : file object, optional
+        If `output` not supplied then source code is printed to screen
+        (sys.stdout).  File object must be created with either write 'w' or
+        append 'a' modes.
+
+    See Also
+    --------
+    lookfor, info
+
+    Examples
+    --------
+    >>> np.source(np.interp)                        #doctest: +SKIP
+    In file: /usr/lib/python2.6/dist-packages/numpy/lib/function_base.py
+    def interp(x, xp, fp, left=None, right=None):
+        \"\"\".... (full docstring printed)\"\"\"
+        if isinstance(x, (float, int, number)):
+            return compiled_interp([x], xp, fp, left, right).item()
+        else:
+            return compiled_interp(x, xp, fp, left, right)
+
+    The source code is only returned for objects written in Python.
+
+    >>> np.source(np.array)                         #doctest: +SKIP
+    Not available for this object.
+
+    """
+    # Local import to speed up numpy's import time.
+    import inspect
     try:
-        return issubclass(arg1, arg2)
-    except TypeError:
-        return False
-
-def issubsctype(arg1, arg2):
-    return issubclass(obj2sctype(arg1), obj2sctype(arg2))
-
-def get_numpy_include():
-    """Return the directory in the package that contains the numpy/*.h header
-    files.
-
-    Extension modules that need to compile against numpy should use this
-    function to locate the appropriate include directory. Using distutils:
-
-      import numpy
-      Extension('extension_name', ...
-                include_dirs=[numpy.get_numpy_include()])
-    """
-    from numpy.distutils.misc_util import get_numpy_include_dirs
-    include_dirs = get_numpy_include_dirs()
-    assert len(include_dirs)==1,`include_dirs`
-    return include_dirs[0]
+        print("In file: %s\n" % inspect.getsourcefile(object), file=output)
+        print(inspect.getsource(object), file=output)
+    except Exception:
+        print("Not available for this object.", file=output)
+
+
+# Cache for lookfor: {id(module): {name: (docstring, kind, index), ...}...}
+# where kind: "func", "class", "module", "object"
+# and index: index in breadth-first namespace traversal
+_lookfor_caches = {}
+
+# regexp whose match indicates that the string may contain a function
+# signature
+_function_signature_re = re.compile(r"[a-z0-9_]+\(.*[,=].*\)", re.I)
+
+
+@set_module('numpy')
+def lookfor(what, module=None, import_modules=True, regenerate=False,
+            output=None):
+    """
+    Do a keyword search on docstrings.
+
+    A list of objects that matched the search is displayed,
+    sorted by relevance. All given keywords need to be found in the
+    docstring for it to be returned as a result, but the order does
+    not matter.
+
+    Parameters
+    ----------
+    what : str
+        String containing words to look for.
+    module : str or list, optional
+        Name of module(s) whose docstrings to go through.
+    import_modules : bool, optional
+        Whether to import sub-modules in packages. Default is True.
+    regenerate : bool, optional
+        Whether to re-generate the docstring cache. Default is False.
+    output : file-like, optional
+        File-like object to write the output to. If omitted, use a pager.
+
+    See Also
+    --------
+    source, info
+
+    Notes
+    -----
+    Relevance is determined only roughly, by checking if the keywords occur
+    in the function name, at the start of a docstring, etc.
+
+    Examples
+    --------
+    >>> np.lookfor('binary representation') # doctest: +SKIP
+    Search results for 'binary representation'
+    ------------------------------------------
+    numpy.binary_repr
+        Return the binary representation of the input number as a string.
+    numpy.core.setup_common.long_double_representation
+        Given a binary dump as given by GNU od -b, look for long double
+    numpy.base_repr
+        Return a string representation of a number in the given base system.
+    ...
+
+    """
+    import pydoc
+
+    # Cache
+    cache = _lookfor_generate_cache(module, import_modules, regenerate)
+
+    # Search
+    # XXX: maybe using a real stemming search engine would be better?
+    found = []
+    whats = str(what).lower().split()
+    if not whats:
+        return
+
+    for name, (docstring, kind, index) in cache.items():
+        if kind in ('module', 'object'):
+            # don't show modules or objects
+            continue
+        doc = docstring.lower()
+        if all(w in doc for w in whats):
+            found.append(name)
+
+    # Relevance sort
+    # XXX: this is full Harrison-Stetson heuristics now,
+    # XXX: it probably could be improved
+
+    kind_relevance = {'func': 1000, 'class': 1000,
+                      'module': -1000, 'object': -1000}
+
+    def relevance(name, docstr, kind, index):
+        r = 0
+        # do the keywords occur within the start of the docstring?
+        first_doc = "\n".join(docstr.lower().strip().split("\n")[:3])
+        r += sum([200 for w in whats if w in first_doc])
+        # do the keywords occur in the function name?
+        r += sum([30 for w in whats if w in name])
+        # is the full name long?
+        r += -len(name) * 5
+        # is the object of bad type?
+        r += kind_relevance.get(kind, -1000)
+        # is the object deep in namespace hierarchy?
+        r += -name.count('.') * 10
+        r += max(-index / 100, -100)
+        return r
+
+    def relevance_value(a):
+        return relevance(a, *cache[a])
+    found.sort(key=relevance_value)
+
+    # Pretty-print
+    s = "Search results for '%s'" % (' '.join(whats))
+    help_text = [s, "-"*len(s)]
+    for name in found[::-1]:
+        doc, kind, ix = cache[name]
+
+        doclines = [line.strip() for line in doc.strip().split("\n")
+                    if line.strip()]
+
+        # find a suitable short description
+        try:
+            first_doc = doclines[0].strip()
+            if _function_signature_re.search(first_doc):
+                first_doc = doclines[1].strip()
+        except IndexError:
+            first_doc = ""
+        help_text.append("%s\n    %s" % (name, first_doc))
+
+    if not found:
+        help_text.append("Nothing found.")
+
+    # Output
+    if output is not None:
+        output.write("\n".join(help_text))
+    elif len(help_text) > 10:
+        pager = pydoc.getpager()
+        pager("\n".join(help_text))
+    else:
+        print("\n".join(help_text))
+
+def _lookfor_generate_cache(module, import_modules, regenerate):
+    """
+    Generate docstring cache for given module.
+
+    Parameters
+    ----------
+    module : str, None, module
+        Module for which to generate docstring cache
+    import_modules : bool
+        Whether to import sub-modules in packages.
+    regenerate : bool
+        Re-generate the docstring cache
+
+    Returns
+    -------
+    cache : dict {obj_full_name: (docstring, kind, index), ...}
+        Docstring cache for the module, either cached one (regenerate=False)
+        or newly generated.
+
+    """
+    # Local import to speed up numpy's import time.
+    import inspect
+
+    from io import StringIO
+
+    if module is None:
+        module = "numpy"
+
+    if isinstance(module, str):
+        try:
+            __import__(module)
+        except ImportError:
+            return {}
+        module = sys.modules[module]
+    elif isinstance(module, list) or isinstance(module, tuple):
+        cache = {}
+        for mod in module:
+            cache.update(_lookfor_generate_cache(mod, import_modules,
+                                                 regenerate))
+        return cache
+
+    if id(module) in _lookfor_caches and not regenerate:
+        return _lookfor_caches[id(module)]
+
+    # walk items and collect docstrings
+    cache = {}
+    _lookfor_caches[id(module)] = cache
+    seen = {}
+    index = 0
+    stack = [(module.__name__, module)]
+    while stack:
+        name, item = stack.pop(0)
+        if id(item) in seen:
+            continue
+        seen[id(item)] = True
+
+        index += 1
+        kind = "object"
+
+        if inspect.ismodule(item):
+            kind = "module"
+            try:
+                _all = item.__all__
+            except AttributeError:
+                _all = None
+
+            # import sub-packages
+            if import_modules and hasattr(item, '__path__'):
+                for pth in item.__path__:
+                    for mod_path in os.listdir(pth):
+                        this_py = os.path.join(pth, mod_path)
+                        init_py = os.path.join(pth, mod_path, '__init__.py')
+                        if (os.path.isfile(this_py) and
+                                mod_path.endswith('.py')):
+                            to_import = mod_path[:-3]
+                        elif os.path.isfile(init_py):
+                            to_import = mod_path
+                        else:
+                            continue
+                        if to_import == '__init__':
+                            continue
+
+                        try:
+                            old_stdout = sys.stdout
+                            old_stderr = sys.stderr
+                            try:
+                                sys.stdout = StringIO()
+                                sys.stderr = StringIO()
+                                __import__("%s.%s" % (name, to_import))
+                            finally:
+                                sys.stdout = old_stdout
+                                sys.stderr = old_stderr
+                        # Catch SystemExit, too
+                        except (Exception, SystemExit):
+                            continue
+
+            for n, v in _getmembers(item):
+                try:
+                    item_name = getattr(v, '__name__', "%s.%s" % (name, n))
+                    mod_name = getattr(v, '__module__', None)
+                except NameError:
+                    # ref. SWIG's global cvars
+                    #    NameError: Unknown C global variable
+                    item_name = "%s.%s" % (name, n)
+                    mod_name = None
+                if '.' not in item_name and mod_name:
+                    item_name = "%s.%s" % (mod_name, item_name)
+
+                if not item_name.startswith(name + '.'):
+                    # don't crawl "foreign" objects
+                    if isinstance(v, ufunc):
+                        # ... unless they are ufuncs
+                        pass
+                    else:
+                        continue
+                elif not (inspect.ismodule(v) or _all is None or n in _all):
+                    continue
+                stack.append(("%s.%s" % (name, n), v))
+        elif inspect.isclass(item):
+            kind = "class"
+            for n, v in _getmembers(item):
+                stack.append(("%s.%s" % (name, n), v))
+        elif hasattr(item, "__call__"):
+            kind = "func"
+
+        try:
+            doc = inspect.getdoc(item)
+        except NameError:
+            # ref SWIG's NameError: Unknown C global variable
+            doc = None
+        if doc is not None:
+            cache[name] = (doc, kind, index)
+
+    return cache
+
+def _getmembers(item):
+    import inspect
+    try:
+        members = inspect.getmembers(item)
+    except Exception:
+        members = [(x, getattr(item, x)) for x in dir(item)
+                   if hasattr(item, x)]
+    return members
+
+
+def safe_eval(source):
+    """
+    Protected string evaluation.
+
+    Evaluate a string containing a Python literal expression without
+    allowing the execution of arbitrary non-literal code.
+
+    Parameters
+    ----------
+    source : str
+        The string to evaluate.
+
+    Returns
+    -------
+    obj : object
+       The result of evaluating `source`.
+
+    Raises
+    ------
+    SyntaxError
+        If the code has invalid Python syntax, or if it contains
+        non-literal code.
+
+    Examples
+    --------
+    >>> np.safe_eval('1')
+    1
+    >>> np.safe_eval('[1, 2, 3]')
+    [1, 2, 3]
+    >>> np.safe_eval('{"foo": ("bar", 10.0)}')
+    {'foo': ('bar', 10.0)}
+
+    >>> np.safe_eval('import os')
+    Traceback (most recent call last):
+      ...
+    SyntaxError: invalid syntax
+
+    >>> np.safe_eval('open("/home/user/.ssh/id_dsa").read()')
+    Traceback (most recent call last):
+      ...
+    ValueError: malformed node or string: <_ast.Call object at 0x...>
+
+    """
+    # Local import to speed up numpy's import time.
+    import ast
+    return ast.literal_eval(source)
+
+
+def _median_nancheck(data, result, axis):
+    """
+    Utility function to check median result from data for NaN values at the end
+    and return NaN in that case. Input result can also be a MaskedArray.
+
+    Parameters
+    ----------
+    data : array
+        Sorted input data to median function
+    result : Array or MaskedArray
+        Result of median function.
+    axis : int
+        Axis along which the median was computed.
+
+    Returns
+    -------
+    result : scalar or ndarray
+        Median or NaN in axes which contained NaN in the input.  If the input
+        was an array, NaN will be inserted in-place.  If a scalar, either the
+        input itself or a scalar NaN.
+    """
+    if data.size == 0:
+        return result
+    n = np.isnan(data.take(-1, axis=axis))
+    # masked NaN values are ok
+    if np.ma.isMaskedArray(n):
+        n = n.filled(False)
+    if np.count_nonzero(n.ravel()) > 0:
+        # Without given output, it is possible that the current result is a
+        # numpy scalar, which is not writeable.  If so, just return nan.
+        if isinstance(result, np.generic):
+            return data.dtype.type(np.nan)
+
+        result[n] = np.nan
+    return result
+
+def _opt_info():
+    """
+    Returns a string contains the supported CPU features by the current build.
+
+    The string format can be explained as follows:
+        - dispatched features that are supported by the running machine
+          end with `*`.
+        - dispatched features that are "not" supported by the running machine
+          end with `?`.
+        - remained features are representing the baseline.
+    """
+    from numpy.core._multiarray_umath import (
+        __cpu_features__, __cpu_baseline__, __cpu_dispatch__
+    )
+
+    if len(__cpu_baseline__) == 0 and len(__cpu_dispatch__) == 0:
+        return ''
+
+    enabled_features = ' '.join(__cpu_baseline__)
+    for feature in __cpu_dispatch__:
+        if __cpu_features__[feature]:
+            enabled_features += f" {feature}*"
+        else:
+            enabled_features += f" {feature}?"
+
+    return enabled_features
+#-----------------------------------------------------------------------------
('numpy/lib', 'function_base.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,424 +1,1785 @@
-
-__all__ = ['logspace', 'linspace',
-           'select', 'piecewise', 'trim_zeros',
-           'copy', 'iterable', #'base_repr', 'binary_repr',
-           'diff', 'gradient', 'angle', 'unwrap', 'sort_complex', 'disp',
-           'unique', 'extract', 'insert', 'nansum', 'nanmax', 'nanargmax',
-           'nanargmin', 'nanmin', 'vectorize', 'asarray_chkfinite', 'average',
-           'histogram', 'bincount', 'digitize', 'cov', 'corrcoef', 'msort',
-           'median', 'sinc', 'hamming', 'hanning', 'bartlett', 'blackman',
-           'kaiser', 'trapz', 'i0', 'add_newdoc', 'add_docstring',
-           ]
-
-import types
+import collections.abc
+import functools
+import re
+import sys
+import warnings
+
+import numpy as np
 import numpy.core.numeric as _nx
-from numpy.core.numeric import ones, zeros, arange, concatenate, array, \
-     asarray, empty, empty_like, asanyarray
-from numpy.core.numeric import ScalarType, dot, where, newaxis
-from numpy.core.umath import pi, multiply, add, arctan2,  \
-     frompyfunc, isnan, cos, less_equal, sqrt, sin, mod, exp
-from numpy.core.oldnumeric import ravel, nonzero, choose, \
-     typecodes, ArrayType, sort
-from numpy.lib.shape_base import atleast_1d
+from numpy.core import transpose
+from numpy.core.numeric import (
+    ones, zeros_like, arange, concatenate, array, asarray, asanyarray, empty,
+    ndarray, take, dot, where, intp, integer, isscalar, absolute
+    )
+from numpy.core.umath import (
+    pi, add, arctan2, frompyfunc, cos, less_equal, sqrt, sin,
+    mod, exp, not_equal, subtract
+    )
+from numpy.core.fromnumeric import (
+    ravel, nonzero, partition, mean, any, sum
+    )
+from numpy.core.numerictypes import typecodes
+from numpy.core.overrides import set_module
+from numpy.core import overrides
+from numpy.core.function_base import add_newdoc
 from numpy.lib.twodim_base import diag
-from _compiled_base import digitize, bincount, _insert, add_docstring
-
-#end Fernando's utilities
-
-
-def linspace(start, stop, num=50, endpoint=True, retstep=False):
-    """Return evenly spaced numbers.
-
-    Return 'num' evenly spaced samples from 'start' to 'stop'.  If
-    'endpoint' is True, the last sample is 'stop'. If 'retstep' is
-    True then return the step value used.
-    """
-    num = int(num)
-    if num <= 0:
-        return array([], float)
-    if endpoint:
-        if num == 1:
-            return array([float(start)])
-        step = (stop-start)/float((num-1))
+from numpy.core.multiarray import (
+    _insert, add_docstring, bincount, normalize_axis_index, _monotonicity,
+    interp as compiled_interp, interp_complex as compiled_interp_complex
+    )
+from numpy.core.umath import _add_newdoc_ufunc as add_newdoc_ufunc
+
+import builtins
+
+# needed in this module for compatibility
+from numpy.lib.histograms import histogram, histogramdd  # noqa: F401
+
+
+array_function_dispatch = functools.partial(
+    overrides.array_function_dispatch, module='numpy')
+
+
+__all__ = [
+    'select', 'piecewise', 'trim_zeros', 'copy', 'iterable', 'percentile',
+    'diff', 'gradient', 'angle', 'unwrap', 'sort_complex', 'disp', 'flip',
+    'rot90', 'extract', 'place', 'vectorize', 'asarray_chkfinite', 'average',
+    'bincount', 'digitize', 'cov', 'corrcoef',
+    'msort', 'median', 'sinc', 'hamming', 'hanning', 'bartlett',
+    'blackman', 'kaiser', 'trapz', 'i0', 'add_newdoc', 'add_docstring',
+    'meshgrid', 'delete', 'insert', 'append', 'interp', 'add_newdoc_ufunc',
+    'quantile'
+    ]
+
+# _QuantileMethods is a dictionary listing all the supported methods to
+# compute quantile/percentile.
+#
+# Below virtual_index refer to the index of the element where the percentile
+# would be found in the sorted sample.
+# When the sample contains exactly the percentile wanted, the virtual_index is
+# an integer to the index of this element.
+# When the percentile wanted is in between two elements, the virtual_index
+# is made of a integer part (a.k.a 'i' or 'left') and a fractional part
+# (a.k.a 'g' or 'gamma')
+#
+# Each method in _QuantileMethods has two properties
+# get_virtual_index : Callable
+#   The function used to compute the virtual_index.
+# fix_gamma : Callable
+#   A function used for discret methods to force the index to a specific value.
+_QuantileMethods = dict(
+    # --- HYNDMAN and FAN METHODS
+    # Discrete methods
+    inverted_cdf=dict(
+        get_virtual_index=lambda n, quantiles: _inverted_cdf(n, quantiles),
+        fix_gamma=lambda gamma, _: gamma,  # should never be called
+    ),
+    averaged_inverted_cdf=dict(
+        get_virtual_index=lambda n, quantiles: (n * quantiles) - 1,
+        fix_gamma=lambda gamma, _: _get_gamma_mask(
+            shape=gamma.shape,
+            default_value=1.,
+            conditioned_value=0.5,
+            where=gamma == 0),
+    ),
+    closest_observation=dict(
+        get_virtual_index=lambda n, quantiles: _closest_observation(n,
+                                                                    quantiles),
+        fix_gamma=lambda gamma, _: gamma,  # should never be called
+    ),
+    # Continuous methods
+    interpolated_inverted_cdf=dict(
+        get_virtual_index=lambda n, quantiles:
+        _compute_virtual_index(n, quantiles, 0, 1),
+        fix_gamma=lambda gamma, _: gamma,
+    ),
+    hazen=dict(
+        get_virtual_index=lambda n, quantiles:
+        _compute_virtual_index(n, quantiles, 0.5, 0.5),
+        fix_gamma=lambda gamma, _: gamma,
+    ),
+    weibull=dict(
+        get_virtual_index=lambda n, quantiles:
+        _compute_virtual_index(n, quantiles, 0, 0),
+        fix_gamma=lambda gamma, _: gamma,
+    ),
+    # Default method.
+    # To avoid some rounding issues, `(n-1) * quantiles` is preferred to
+    # `_compute_virtual_index(n, quantiles, 1, 1)`.
+    # They are mathematically equivalent.
+    linear=dict(
+        get_virtual_index=lambda n, quantiles: (n - 1) * quantiles,
+        fix_gamma=lambda gamma, _: gamma,
+    ),
+    median_unbiased=dict(
+        get_virtual_index=lambda n, quantiles:
+        _compute_virtual_index(n, quantiles, 1 / 3.0, 1 / 3.0),
+        fix_gamma=lambda gamma, _: gamma,
+    ),
+    normal_unbiased=dict(
+        get_virtual_index=lambda n, quantiles:
+        _compute_virtual_index(n, quantiles, 3 / 8.0, 3 / 8.0),
+        fix_gamma=lambda gamma, _: gamma,
+    ),
+    # --- OTHER METHODS
+    lower=dict(
+        get_virtual_index=lambda n, quantiles: np.floor(
+            (n - 1) * quantiles).astype(np.intp),
+        fix_gamma=lambda gamma, _: gamma,
+        # should never be called, index dtype is int
+    ),
+    higher=dict(
+        get_virtual_index=lambda n, quantiles: np.ceil(
+            (n - 1) * quantiles).astype(np.intp),
+        fix_gamma=lambda gamma, _: gamma,
+        # should never be called, index dtype is int
+    ),
+    midpoint=dict(
+        get_virtual_index=lambda n, quantiles: 0.5 * (
+                np.floor((n - 1) * quantiles)
+                + np.ceil((n - 1) * quantiles)),
+        fix_gamma=lambda gamma, index: _get_gamma_mask(
+            shape=gamma.shape,
+            default_value=0.5,
+            conditioned_value=0.,
+            where=index % 1 == 0),
+    ),
+    nearest=dict(
+        get_virtual_index=lambda n, quantiles: np.around(
+            (n - 1) * quantiles).astype(np.intp),
+        fix_gamma=lambda gamma, _: gamma,
+        # should never be called, index dtype is int
+    ))
+
+
+def _rot90_dispatcher(m, k=None, axes=None):
+    return (m,)
+
+
+@array_function_dispatch(_rot90_dispatcher)
+def rot90(m, k=1, axes=(0, 1)):
+    """
+    Rotate an array by 90 degrees in the plane specified by axes.
+
+    Rotation direction is from the first towards the second axis.
+
+    Parameters
+    ----------
+    m : array_like
+        Array of two or more dimensions.
+    k : integer
+        Number of times the array is rotated by 90 degrees.
+    axes : (2,) array_like
+        The array is rotated in the plane defined by the axes.
+        Axes must be different.
+
+        .. versionadded:: 1.12.0
+
+    Returns
+    -------
+    y : ndarray
+        A rotated view of `m`.
+
+    See Also
+    --------
+    flip : Reverse the order of elements in an array along the given axis.
+    fliplr : Flip an array horizontally.
+    flipud : Flip an array vertically.
+
+    Notes
+    -----
+    ``rot90(m, k=1, axes=(1,0))``  is the reverse of
+    ``rot90(m, k=1, axes=(0,1))``
+
+    ``rot90(m, k=1, axes=(1,0))`` is equivalent to
+    ``rot90(m, k=-1, axes=(0,1))``
+
+    Examples
+    --------
+    >>> m = np.array([[1,2],[3,4]], int)
+    >>> m
+    array([[1, 2],
+           [3, 4]])
+    >>> np.rot90(m)
+    array([[2, 4],
+           [1, 3]])
+    >>> np.rot90(m, 2)
+    array([[4, 3],
+           [2, 1]])
+    >>> m = np.arange(8).reshape((2,2,2))
+    >>> np.rot90(m, 1, (1,2))
+    array([[[1, 3],
+            [0, 2]],
+           [[5, 7],
+            [4, 6]]])
+
+    """
+    axes = tuple(axes)
+    if len(axes) != 2:
+        raise ValueError("len(axes) must be 2.")
+
+    m = asanyarray(m)
+
+    if axes[0] == axes[1] or absolute(axes[0] - axes[1]) == m.ndim:
+        raise ValueError("Axes must be different.")
+
+    if (axes[0] >= m.ndim or axes[0] < -m.ndim
+        or axes[1] >= m.ndim or axes[1] < -m.ndim):
+        raise ValueError("Axes={} out of range for array of ndim={}."
+            .format(axes, m.ndim))
+
+    k %= 4
+
+    if k == 0:
+        return m[:]
+    if k == 2:
+        return flip(flip(m, axes[0]), axes[1])
+
+    axes_list = arange(0, m.ndim)
+    (axes_list[axes[0]], axes_list[axes[1]]) = (axes_list[axes[1]],
+                                                axes_list[axes[0]])
+
+    if k == 1:
+        return transpose(flip(m, axes[1]), axes_list)
     else:
-        step = (stop-start)/float(num)
-    y = _nx.arange(0, num) * step + start
-    if retstep:
-        return y, step
+        # k == 3
+        return flip(transpose(m, axes_list), axes[1])
+
+
+def _flip_dispatcher(m, axis=None):
+    return (m,)
+
+
+@array_function_dispatch(_flip_dispatcher)
+def flip(m, axis=None):
+    """
+    Reverse the order of elements in an array along the given axis.
+
+    The shape of the array is preserved, but the elements are reordered.
+
+    .. versionadded:: 1.12.0
+
+    Parameters
+    ----------
+    m : array_like
+        Input array.
+    axis : None or int or tuple of ints, optional
+         Axis or axes along which to flip over. The default,
+         axis=None, will flip over all of the axes of the input array.
+         If axis is negative it counts from the last to the first axis.
+
+         If axis is a tuple of ints, flipping is performed on all of the axes
+         specified in the tuple.
+
+         .. versionchanged:: 1.15.0
+            None and tuples of axes are supported
+
+    Returns
+    -------
+    out : array_like
+        A view of `m` with the entries of axis reversed.  Since a view is
+        returned, this operation is done in constant time.
+
+    See Also
+    --------
+    flipud : Flip an array vertically (axis=0).
+    fliplr : Flip an array horizontally (axis=1).
+
+    Notes
+    -----
+    flip(m, 0) is equivalent to flipud(m).
+
+    flip(m, 1) is equivalent to fliplr(m).
+
+    flip(m, n) corresponds to ``m[...,::-1,...]`` with ``::-1`` at position n.
+
+    flip(m) corresponds to ``m[::-1,::-1,...,::-1]`` with ``::-1`` at all
+    positions.
+
+    flip(m, (0, 1)) corresponds to ``m[::-1,::-1,...]`` with ``::-1`` at
+    position 0 and position 1.
+
+    Examples
+    --------
+    >>> A = np.arange(8).reshape((2,2,2))
+    >>> A
+    array([[[0, 1],
+            [2, 3]],
+           [[4, 5],
+            [6, 7]]])
+    >>> np.flip(A, 0)
+    array([[[4, 5],
+            [6, 7]],
+           [[0, 1],
+            [2, 3]]])
+    >>> np.flip(A, 1)
+    array([[[2, 3],
+            [0, 1]],
+           [[6, 7],
+            [4, 5]]])
+    >>> np.flip(A)
+    array([[[7, 6],
+            [5, 4]],
+           [[3, 2],
+            [1, 0]]])
+    >>> np.flip(A, (0, 2))
+    array([[[5, 4],
+            [7, 6]],
+           [[1, 0],
+            [3, 2]]])
+    >>> A = np.random.randn(3,4,5)
+    >>> np.all(np.flip(A,2) == A[:,:,::-1,...])
+    True
+    """
+    if not hasattr(m, 'ndim'):
+        m = asarray(m)
+    if axis is None:
+        indexer = (np.s_[::-1],) * m.ndim
     else:
-        return y
-
-def logspace(start,stop,num=50,endpoint=True,base=10.0):
-    """Evenly spaced numbers on a logarithmic scale.
-
-    Computes int(num) evenly spaced exponents from start to stop.
-    If endpoint=True, then last exponent is stop.
-    Returns base**exponents.
-    """
-    y = linspace(start,stop,num=num,endpoint=endpoint)
-    return _nx.power(base,y)
-
+        axis = _nx.normalize_axis_tuple(axis, m.ndim)
+        indexer = [np.s_[:]] * m.ndim
+        for ax in axis:
+            indexer[ax] = np.s_[::-1]
+        indexer = tuple(indexer)
+    return m[indexer]
+
+
+@set_module('numpy')
 def iterable(y):
-    try: iter(y)
-    except: return 0
-    return 1
-
-def histogram(a, bins=10, range=None, normed=False):
-    a = asarray(a).ravel()
-    if not iterable(bins):
-        if range is None:
-            range = (a.min(), a.max())
-        mn, mx = [mi+0.0 for mi in range]
-        if mn == mx:
-            mn -= 0.5
-            mx += 0.5
-        bins = linspace(mn, mx, bins, endpoint=False)
-
-    n = sort(a).searchsorted(bins)
-    n = concatenate([n, [len(a)]])
-    n = n[1:]-n[:-1]
-
-    if normed:
-        db = bins[1] - bins[0]
-        return 1.0/(a.size*db) * n, bins
+    """
+    Check whether or not an object can be iterated over.
+
+    Parameters
+    ----------
+    y : object
+      Input object.
+
+    Returns
+    -------
+    b : bool
+      Return ``True`` if the object has an iterator method or is a
+      sequence and ``False`` otherwise.
+
+
+    Examples
+    --------
+    >>> np.iterable([1, 2, 3])
+    True
+    >>> np.iterable(2)
+    False
+
+    Notes
+    -----
+    In most cases, the results of ``np.iterable(obj)`` are consistent with
+    ``isinstance(obj, collections.abc.Iterable)``. One notable exception is
+    the treatment of 0-dimensional arrays::
+
+        >>> from collections.abc import Iterable
+        >>> a = np.array(1.0)  # 0-dimensional numpy array
+        >>> isinstance(a, Iterable)
+        True
+        >>> np.iterable(a)
+        False
+
+    """
+    try:
+        iter(y)
+    except TypeError:
+        return False
+    return True
+
+
+def _average_dispatcher(a, axis=None, weights=None, returned=None, *,
+                        keepdims=None):
+    return (a, weights)
+
+
+@array_function_dispatch(_average_dispatcher)
+def average(a, axis=None, weights=None, returned=False, *,
+            keepdims=np._NoValue):
+    """
+    Compute the weighted average along the specified axis.
+
+    Parameters
+    ----------
+    a : array_like
+        Array containing data to be averaged. If `a` is not an array, a
+        conversion is attempted.
+    axis : None or int or tuple of ints, optional
+        Axis or axes along which to average `a`.  The default,
+        axis=None, will average over all of the elements of the input array.
+        If axis is negative it counts from the last to the first axis.
+
+        .. versionadded:: 1.7.0
+
+        If axis is a tuple of ints, averaging is performed on all of the axes
+        specified in the tuple instead of a single axis or all the axes as
+        before.
+    weights : array_like, optional
+        An array of weights associated with the values in `a`. Each value in
+        `a` contributes to the average according to its associated weight.
+        The weights array can either be 1-D (in which case its length must be
+        the size of `a` along the given axis) or of the same shape as `a`.
+        If `weights=None`, then all data in `a` are assumed to have a
+        weight equal to one.  The 1-D calculation is::
+
+            avg = sum(a * weights) / sum(weights)
+
+        The only constraint on `weights` is that `sum(weights)` must not be 0.
+    returned : bool, optional
+        Default is `False`. If `True`, the tuple (`average`, `sum_of_weights`)
+        is returned, otherwise only the average is returned.
+        If `weights=None`, `sum_of_weights` is equivalent to the number of
+        elements over which the average is taken.
+    keepdims : bool, optional
+        If this is set to True, the axes which are reduced are left
+        in the result as dimensions with size one. With this option,
+        the result will broadcast correctly against the original `a`.
+        *Note:* `keepdims` will not work with instances of `numpy.matrix`
+        or other classes whose methods do not support `keepdims`.
+
+        .. versionadded:: 1.23.0
+
+    Returns
+    -------
+    retval, [sum_of_weights] : array_type or double
+        Return the average along the specified axis. When `returned` is `True`,
+        return a tuple with the average as the first element and the sum
+        of the weights as the second element. `sum_of_weights` is of the
+        same type as `retval`. The result dtype follows a genereal pattern.
+        If `weights` is None, the result dtype will be that of `a` , or ``float64``
+        if `a` is integral. Otherwise, if `weights` is not None and `a` is non-
+        integral, the result type will be the type of lowest precision capable of
+        representing values of both `a` and `weights`. If `a` happens to be
+        integral, the previous rules still applies but the result dtype will
+        at least be ``float64``.
+
+    Raises
+    ------
+    ZeroDivisionError
+        When all weights along axis are zero. See `numpy.ma.average` for a
+        version robust to this type of error.
+    TypeError
+        When the length of 1D `weights` is not the same as the shape of `a`
+        along axis.
+
+    See Also
+    --------
+    mean
+
+    ma.average : average for masked arrays -- useful if your data contains
+                 "missing" values
+    numpy.result_type : Returns the type that results from applying the
+                        numpy type promotion rules to the arguments.
+
+    Examples
+    --------
+    >>> data = np.arange(1, 5)
+    >>> data
+    array([1, 2, 3, 4])
+    >>> np.average(data)
+    2.5
+    >>> np.average(np.arange(1, 11), weights=np.arange(10, 0, -1))
+    4.0
+
+    >>> data = np.arange(6).reshape((3, 2))
+    >>> data
+    array([[0, 1],
+           [2, 3],
+           [4, 5]])
+    >>> np.average(data, axis=1, weights=[1./4, 3./4])
+    array([0.75, 2.75, 4.75])
+    >>> np.average(data, weights=[1./4, 3./4])
+    Traceback (most recent call last):
+        ...
+    TypeError: Axis must be specified when shapes of a and weights differ.
+
+    >>> a = np.ones(5, dtype=np.float128)
+    >>> w = np.ones(5, dtype=np.complex64)
+    >>> avg = np.average(a, weights=w)
+    >>> print(avg.dtype)
+    complex256
+
+    With ``keepdims=True``, the following result has shape (3, 1).
+
+    >>> np.average(data, axis=1, keepdims=True)
+    array([[0.5],
+           [2.5],
+           [4.5]])
+    """
+    a = np.asanyarray(a)
+
+    if keepdims is np._NoValue:
+        # Don't pass on the keepdims argument if one wasn't given.
+        keepdims_kw = {}
     else:
-        return n, bins
-
-def average(a, axis=0, weights=None, returned=False):
-    """average(a, axis=0, weights=None, returned=False)
-
-    Average the array over the given axis.  If the axis is None, average
-    over all dimensions of the array.  Equivalent to a.mean(axis), but
-    with a default axis of 0 instead of None.
-
-    If an integer axis is given, this equals:
-        a.sum(axis) * 1.0 / len(a)
-
-    If axis is None, this equals:
-        a.sum(axis) * 1.0 / product(a.shape)
-
-    If weights are given, result is:
-        sum(a * weights) / sum(weights),
-    where the weights must have a's shape or be 1D with length the
-    size of a in the given axis. Integer weights are converted to
-    Float.  Not specifying weights is equivalent to specifying
-    weights that are all 1.
-
-    If 'returned' is True, return a tuple: the result and the sum of
-    the weights or count of values. The shape of these two results
-    will be the same.
-
-    Raises ZeroDivisionError if appropriate.  (The version in MA does
-    not -- it returns masked values).
-    """
-    if axis is None:
-        a = array(a).ravel()
-        if weights is None:
-            n = add.reduce(a)
-            d = len(a) * 1.0
+        keepdims_kw = {'keepdims': keepdims}
+
+    if weights is None:
+        avg = a.mean(axis, **keepdims_kw)
+        scl = avg.dtype.type(a.size/avg.size)
+    else:
+        wgt = np.asanyarray(weights)
+
+        if issubclass(a.dtype.type, (np.integer, np.bool_)):
+            result_dtype = np.result_type(a.dtype, wgt.dtype, 'f8')
         else:
-            w = array(weights).ravel() * 1.0
-            n = add.reduce(multiply(a, w))
-            d = add.reduce(w)
+            result_dtype = np.result_type(a.dtype, wgt.dtype)
+
+        # Sanity checks
+        if a.shape != wgt.shape:
+            if axis is None:
+                raise TypeError(
+                    "Axis must be specified when shapes of a and weights "
+                    "differ.")
+            if wgt.ndim != 1:
+                raise TypeError(
+                    "1D weights expected when shapes of a and weights differ.")
+            if wgt.shape[0] != a.shape[axis]:
+                raise ValueError(
+                    "Length of weights not compatible with specified axis.")
+
+            # setup wgt to broadcast along axis
+            wgt = np.broadcast_to(wgt, (a.ndim-1)*(1,) + wgt.shape)
+            wgt = wgt.swapaxes(-1, axis)
+
+        scl = wgt.sum(axis=axis, dtype=result_dtype)
+        if np.any(scl == 0.0):
+            raise ZeroDivisionError(
+                "Weights sum to zero, can't be normalized")
+
+        avg = np.multiply(a, wgt,
+                          dtype=result_dtype).sum(axis, **keepdims_kw) / scl
+
+    if returned:
+        if scl.shape != avg.shape:
+            scl = np.broadcast_to(scl, avg.shape).copy()
+        return avg, scl
     else:
-        a = array(a)
-        ash = a.shape
-        if ash == ():
-            a.shape = (1,)
-        if weights is None:
-            n = add.reduce(a, axis)
-            d = ash[axis] * 1.0
-            if returned:
-                d = ones(n.shape) * d
-        else:
-            w = array(weights, copy=False) * 1.0
-            wsh = w.shape
-            if wsh == ():
-                wsh = (1,)
-            if wsh == ash:
-                n = add.reduce(a*w, axis)
-                d = add.reduce(w, axis)
-            elif wsh == (ash[axis],):
-                ni = ash[axis]
-                r = [newaxis]*ni
-                r[axis] = slice(None, None, 1)
-                w1 = eval("w["+repr(tuple(r))+"]*ones(ash, Float)")
-                n = add.reduce(a*w1, axis)
-                d = add.reduce(w1, axis)
-            else:
-                raise ValueError, 'averaging weights have wrong shape'
-
-    if not isinstance(d, ArrayType):
-        if d == 0.0:
-            raise ZeroDivisionError, 'zero denominator in average()'
-    if returned:
-        return n/d, d
-    else:
-        return n/d
-
-def asarray_chkfinite(a):
-    """Like asarray, but check that no NaNs or Infs are present.
-    """
-    a = asarray(a)
-    if (a.dtype.char in _nx.typecodes['AllFloat']) \
-           and (_nx.isnan(a).any() or _nx.isinf(a).any()):
-        raise ValueError, "array must not contain infs or NaNs"
+        return avg
+
+
+@set_module('numpy')
+def asarray_chkfinite(a, dtype=None, order=None):
+    """Convert the input to an array, checking for NaNs or Infs.
+
+    Parameters
+    ----------
+    a : array_like
+        Input data, in any form that can be converted to an array.  This
+        includes lists, lists of tuples, tuples, tuples of tuples, tuples
+        of lists and ndarrays.  Success requires no NaNs or Infs.
+    dtype : data-type, optional
+        By default, the data-type is inferred from the input data.
+    order : {'C', 'F', 'A', 'K'}, optional
+        Memory layout.  'A' and 'K' depend on the order of input array a.
+        'C' row-major (C-style),
+        'F' column-major (Fortran-style) memory representation.
+        'A' (any) means 'F' if `a` is Fortran contiguous, 'C' otherwise
+        'K' (keep) preserve input order
+        Defaults to 'C'.
+
+    Returns
+    -------
+    out : ndarray
+        Array interpretation of `a`.  No copy is performed if the input
+        is already an ndarray.  If `a` is a subclass of ndarray, a base
+        class ndarray is returned.
+
+    Raises
+    ------
+    ValueError
+        Raises ValueError if `a` contains NaN (Not a Number) or Inf (Infinity).
+
+    See Also
+    --------
+    asarray : Create and array.
+    asanyarray : Similar function which passes through subclasses.
+    ascontiguousarray : Convert input to a contiguous array.
+    asfarray : Convert input to a floating point ndarray.
+    asfortranarray : Convert input to an ndarray with column-major
+                     memory order.
+    fromiter : Create an array from an iterator.
+    fromfunction : Construct an array by executing a function on grid
+                   positions.
+
+    Examples
+    --------
+    Convert a list into an array.  If all elements are finite
+    ``asarray_chkfinite`` is identical to ``asarray``.
+
+    >>> a = [1, 2]
+    >>> np.asarray_chkfinite(a, dtype=float)
+    array([1., 2.])
+
+    Raises ValueError if array_like contains Nans or Infs.
+
+    >>> a = [1, 2, np.inf]
+    >>> try:
+    ...     np.asarray_chkfinite(a)
+    ... except ValueError:
+    ...     print('ValueError')
+    ...
+    ValueError
+
+    """
+    a = asarray(a, dtype=dtype, order=order)
+    if a.dtype.char in typecodes['AllFloat'] and not np.isfinite(a).all():
+        raise ValueError(
+            "array must not contain infs or NaNs")
     return a
 
+
+def _piecewise_dispatcher(x, condlist, funclist, *args, **kw):
+    yield x
+    # support the undocumented behavior of allowing scalars
+    if np.iterable(condlist):
+        yield from condlist
+
+
+@array_function_dispatch(_piecewise_dispatcher)
 def piecewise(x, condlist, funclist, *args, **kw):
-    """Return a piecewise-defined function.
-
-    x is the domain
-
-    condlist is a list of boolean arrays or a single boolean array
-      The length of the condition list must be n2 or n2-1 where n2
-      is the length of the function list.  If len(condlist)==n2-1, then
-      an 'otherwise' condition is formed by |'ing all the conditions
-      and inverting.
-
-    funclist is a list of functions to call of length (n2).
-      Each function should return an array output for an array input
-      Each function can take (the same set) of extra arguments and
-      keyword arguments which are passed in after the function list.
-      A constant may be used in funclist for a function that returns a
-      constant (e.g. val  and lambda x: val are equivalent in a funclist).
-
-    The output is the same shape and type as x and is found by
-      calling the functions on the appropriate portions of x.
-
-    Note: This is similar to choose or select, except
-          the the functions are only evaluated on elements of x
-          that satisfy the corresponding condition.
-
-    The result is
-           |--
-           |  f1(x)  for condition1
-     y = --|  f2(x)  for condition2
-           |   ...
-           |  fn(x)  for conditionn
-           |--
+    """
+    Evaluate a piecewise-defined function.
+
+    Given a set of conditions and corresponding functions, evaluate each
+    function on the input data wherever its condition is true.
+
+    Parameters
+    ----------
+    x : ndarray or scalar
+        The input domain.
+    condlist : list of bool arrays or bool scalars
+        Each boolean array corresponds to a function in `funclist`.  Wherever
+        `condlist[i]` is True, `funclist[i](x)` is used as the output value.
+
+        Each boolean array in `condlist` selects a piece of `x`,
+        and should therefore be of the same shape as `x`.
+
+        The length of `condlist` must correspond to that of `funclist`.
+        If one extra function is given, i.e. if
+        ``len(funclist) == len(condlist) + 1``, then that extra function
+        is the default value, used wherever all conditions are false.
+    funclist : list of callables, f(x,*args,**kw), or scalars
+        Each function is evaluated over `x` wherever its corresponding
+        condition is True.  It should take a 1d array as input and give an 1d
+        array or a scalar value as output.  If, instead of a callable,
+        a scalar is provided then a constant function (``lambda x: scalar``) is
+        assumed.
+    args : tuple, optional
+        Any further arguments given to `piecewise` are passed to the functions
+        upon execution, i.e., if called ``piecewise(..., ..., 1, 'a')``, then
+        each function is called as ``f(x, 1, 'a')``.
+    kw : dict, optional
+        Keyword arguments used in calling `piecewise` are passed to the
+        functions upon execution, i.e., if called
+        ``piecewise(..., ..., alpha=1)``, then each function is called as
+        ``f(x, alpha=1)``.
+
+    Returns
+    -------
+    out : ndarray
+        The output is the same shape and type as x and is found by
+        calling the functions in `funclist` on the appropriate portions of `x`,
+        as defined by the boolean arrays in `condlist`.  Portions not covered
+        by any condition have a default value of 0.
+
+
+    See Also
+    --------
+    choose, select, where
+
+    Notes
+    -----
+    This is similar to choose or select, except that functions are
+    evaluated on elements of `x` that satisfy the corresponding condition from
+    `condlist`.
+
+    The result is::
+
+            |--
+            |funclist[0](x[condlist[0]])
+      out = |funclist[1](x[condlist[1]])
+            |...
+            |funclist[n2](x[condlist[n2]])
+            |--
+
+    Examples
+    --------
+    Define the sigma function, which is -1 for ``x < 0`` and +1 for ``x >= 0``.
+
+    >>> x = np.linspace(-2.5, 2.5, 6)
+    >>> np.piecewise(x, [x < 0, x >= 0], [-1, 1])
+    array([-1., -1., -1.,  1.,  1.,  1.])
+
+    Define the absolute value, which is ``-x`` for ``x <0`` and ``x`` for
+    ``x >= 0``.
+
+    >>> np.piecewise(x, [x < 0, x >= 0], [lambda x: -x, lambda x: x])
+    array([2.5,  1.5,  0.5,  0.5,  1.5,  2.5])
+
+    Apply the same function to a scalar value.
+
+    >>> y = -2
+    >>> np.piecewise(y, [y < 0, y >= 0], [lambda x: -x, lambda x: x])
+    array(2)
 
     """
     x = asanyarray(x)
     n2 = len(funclist)
-    if not isinstance(condlist, type([])):
+
+    # undocumented: single condition is promoted to a list of one condition
+    if isscalar(condlist) or (
+            not isinstance(condlist[0], (list, ndarray)) and x.ndim != 0):
         condlist = [condlist]
+
+    condlist = asarray(condlist, dtype=bool)
     n = len(condlist)
-    if n == n2-1:  # compute the "otherwise" condition.
-        totlist = condlist[0]
-        for k in range(1, n):
-            totlist |= condlist[k]
-        condlist.append(~totlist)
+
+    if n == n2 - 1:  # compute the "otherwise" condition.
+        condelse = ~np.any(condlist, axis=0, keepdims=True)
+        condlist = np.concatenate([condlist, condelse], axis=0)
         n += 1
-    if (n != n2):
-        raise ValueError, "function list and condition list must be the same"
-    y = empty(x.shape, x.dtype)
-    for k in range(n):
-        item = funclist[k]
-        if not callable(item):
-            y[condlist[k]] = item
+    elif n != n2:
+        raise ValueError(
+            "with {} condition(s), either {} or {} functions are expected"
+            .format(n, n, n+1)
+        )
+
+    y = zeros_like(x)
+    for cond, func in zip(condlist, funclist):
+        if not isinstance(func, collections.abc.Callable):
+            y[cond] = func
         else:
-            y[condlist[k]] = item(x[condlist[k]], *args, **kw)
+            vals = x[cond]
+            if vals.size > 0:
+                y[cond] = func(vals, *args, **kw)
+
     return y
 
+
+def _select_dispatcher(condlist, choicelist, default=None):
+    yield from condlist
+    yield from choicelist
+
+
+@array_function_dispatch(_select_dispatcher)
 def select(condlist, choicelist, default=0):
-    """ Return an array composed of different elements of choicelist
-        depending on the list of conditions.
-
-        condlist is a list of condition arrays containing ones or zeros
-
-        choicelist is a list of choice arrays (of the "same" size as the
-        arrays in condlist).  The result array has the "same" size as the
-        arrays in choicelist.  If condlist is [c0, ..., cN-1] then choicelist
-        must be of length N.  The elements of the choicelist can then be
-        represented as [v0, ..., vN-1]. The default choice if none of the
-        conditions are met is given as the default argument.
-
-        The conditions are tested in order and the first one statisfied is
-        used to select the choice. In other words, the elements of the
-        output array are found from the following tree (notice the order of
-        the conditions matters):
-
-        if c0: v0
-        elif c1: v1
-        elif c2: v2
-        ...
-        elif cN-1: vN-1
-        else: default
-
-        Note that one of the condition arrays must be large enough to handle
-        the largest array in the choice list.
-    """
-    n = len(condlist)
-    n2 = len(choicelist)
-    if n2 != n:
-        raise ValueError, "list of cases must be same length as list of conditions"
-    choicelist.insert(0, default)
-    S = 0
-    pfac = 1
-    for k in range(1, n+1):
-        S += k * pfac * asarray(condlist[k-1])
-        if k < n:
-            pfac *= (1-asarray(condlist[k-1]))
-    # handle special case of a 1-element condition but
-    #  a multi-element choice
-    if type(S) in ScalarType or max(asarray(S).shape)==1:
-        pfac = asarray(1)
-        for k in range(n2+1):
-            pfac = pfac + asarray(choicelist[k])
-        S = S*ones(asarray(pfac).shape)
-    return choose(S, tuple(choicelist))
-
-def _asarray1d(arr, copy=False):
-    """Ensure 1D array for one array.
-    """
-    if copy:
-        return asarray(arr).flatten()
+    """
+    Return an array drawn from elements in choicelist, depending on conditions.
+
+    Parameters
+    ----------
+    condlist : list of bool ndarrays
+        The list of conditions which determine from which array in `choicelist`
+        the output elements are taken. When multiple conditions are satisfied,
+        the first one encountered in `condlist` is used.
+    choicelist : list of ndarrays
+        The list of arrays from which the output elements are taken. It has
+        to be of the same length as `condlist`.
+    default : scalar, optional
+        The element inserted in `output` when all conditions evaluate to False.
+
+    Returns
+    -------
+    output : ndarray
+        The output at position m is the m-th element of the array in
+        `choicelist` where the m-th element of the corresponding array in
+        `condlist` is True.
+
+    See Also
+    --------
+    where : Return elements from one of two arrays depending on condition.
+    take, choose, compress, diag, diagonal
+
+    Examples
+    --------
+    >>> x = np.arange(6)
+    >>> condlist = [x<3, x>3]
+    >>> choicelist = [x, x**2]
+    >>> np.select(condlist, choicelist, 42)
+    array([ 0,  1,  2, 42, 16, 25])
+
+    >>> condlist = [x<=4, x>3]
+    >>> choicelist = [x, x**2]
+    >>> np.select(condlist, choicelist, 55)
+    array([ 0,  1,  2,  3,  4, 25])
+
+    """
+    # Check the size of condlist and choicelist are the same, or abort.
+    if len(condlist) != len(choicelist):
+        raise ValueError(
+            'list of cases must be same length as list of conditions')
+
+    # Now that the dtype is known, handle the deprecated select([], []) case
+    if len(condlist) == 0:
+        raise ValueError("select with an empty condition list is not possible")
+
+    choicelist = [np.asarray(choice) for choice in choicelist]
+
+    try:
+        intermediate_dtype = np.result_type(*choicelist)
+    except TypeError as e:
+        msg = f'Choicelist elements do not have a common dtype: {e}'
+        raise TypeError(msg) from None
+    default_array = np.asarray(default)
+    choicelist.append(default_array)
+
+    # need to get the result type before broadcasting for correct scalar
+    # behaviour
+    try:
+        dtype = np.result_type(intermediate_dtype, default_array)
+    except TypeError as e:
+        msg = f'Choicelists and default value do not have a common dtype: {e}'
+        raise TypeError(msg) from None
+
+    # Convert conditions to arrays and broadcast conditions and choices
+    # as the shape is needed for the result. Doing it separately optimizes
+    # for example when all choices are scalars.
+    condlist = np.broadcast_arrays(*condlist)
+    choicelist = np.broadcast_arrays(*choicelist)
+
+    # If cond array is not an ndarray in boolean format or scalar bool, abort.
+    for i, cond in enumerate(condlist):
+        if cond.dtype.type is not np.bool_:
+            raise TypeError(
+                'invalid entry {} in condlist: should be boolean ndarray'.format(i))
+
+    if choicelist[0].ndim == 0:
+        # This may be common, so avoid the call.
+        result_shape = condlist[0].shape
     else:
-        return asarray(arr).ravel()
-
-def copy(a):
-    """Return an array copy of the given object.
-    """
-    return array(a, copy=True)
+        result_shape = np.broadcast_arrays(condlist[0], choicelist[0])[0].shape
+
+    result = np.full(result_shape, choicelist[-1], dtype)
+
+    # Use np.copyto to burn each choicelist array onto result, using the
+    # corresponding condlist as a boolean mask. This is done in reverse
+    # order since the first choice should take precedence.
+    choicelist = choicelist[-2::-1]
+    condlist = condlist[::-1]
+    for choice, cond in zip(choicelist, condlist):
+        np.copyto(result, choice, where=cond)
+
+    return result
+
+
+def _copy_dispatcher(a, order=None, subok=None):
+    return (a,)
+
+
+@array_function_dispatch(_copy_dispatcher)
+def copy(a, order='K', subok=False):
+    """
+    Return an array copy of the given object.
+
+    Parameters
+    ----------
+    a : array_like
+        Input data.
+    order : {'C', 'F', 'A', 'K'}, optional
+        Controls the memory layout of the copy. 'C' means C-order,
+        'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,
+        'C' otherwise. 'K' means match the layout of `a` as closely
+        as possible. (Note that this function and :meth:`ndarray.copy` are very
+        similar, but have different default values for their order=
+        arguments.)
+    subok : bool, optional
+        If True, then sub-classes will be passed-through, otherwise the
+        returned array will be forced to be a base-class array (defaults to False).
+
+        .. versionadded:: 1.19.0
+
+    Returns
+    -------
+    arr : ndarray
+        Array interpretation of `a`.
+
+    See Also
+    --------
+    ndarray.copy : Preferred method for creating an array copy
+
+    Notes
+    -----
+    This is equivalent to:
+
+    >>> np.array(a, copy=True)  #doctest: +SKIP
+
+    Examples
+    --------
+    Create an array x, with a reference y and a copy z:
+
+    >>> x = np.array([1, 2, 3])
+    >>> y = x
+    >>> z = np.copy(x)
+
+    Note that, when we modify x, y changes, but not z:
+
+    >>> x[0] = 10
+    >>> x[0] == y[0]
+    True
+    >>> x[0] == z[0]
+    False
+
+    Note that, np.copy clears previously set WRITEABLE=False flag.
+
+    >>> a = np.array([1, 2, 3])
+    >>> a.flags["WRITEABLE"] = False
+    >>> b = np.copy(a)
+    >>> b.flags["WRITEABLE"]
+    True
+    >>> b[0] = 3
+    >>> b
+    array([3, 2, 3])
+
+    Note that np.copy is a shallow copy and will not copy object
+    elements within arrays. This is mainly important for arrays
+    containing Python objects. The new array will contain the
+    same object which may lead to surprises if that object can
+    be modified (is mutable):
+
+    >>> a = np.array([1, 'm', [2, 3, 4]], dtype=object)
+    >>> b = np.copy(a)
+    >>> b[2][0] = 10
+    >>> a
+    array([1, 'm', list([10, 3, 4])], dtype=object)
+
+    To ensure all elements within an ``object`` array are copied,
+    use `copy.deepcopy`:
+
+    >>> import copy
+    >>> a = np.array([1, 'm', [2, 3, 4]], dtype=object)
+    >>> c = copy.deepcopy(a)
+    >>> c[2][0] = 10
+    >>> c
+    array([1, 'm', list([10, 3, 4])], dtype=object)
+    >>> a
+    array([1, 'm', list([2, 3, 4])], dtype=object)
+
+    """
+    return array(a, order=order, subok=subok, copy=True)
 
 # Basic operations
 
-def gradient(f, *varargs):
-    """Calculate the gradient of an N-dimensional scalar function.
-
-    Uses central differences on the interior and first differences on boundaries
-    to give the same shape.
-
-    Inputs:
-
-      f -- An N-dimensional array giving samples of a scalar function
-
-      varargs -- 0, 1, or N scalars giving the sample distances in each direction
-
-    Outputs:
-
-      N arrays of the same shape as f giving the derivative of f with respect
-       to each dimension.
-    """
-    N = len(f.shape)  # number of dimensions
+
+def _gradient_dispatcher(f, *varargs, axis=None, edge_order=None):
+    yield f
+    yield from varargs
+
+
+@array_function_dispatch(_gradient_dispatcher)
+def gradient(f, *varargs, axis=None, edge_order=1):
+    """
+    Return the gradient of an N-dimensional array.
+
+    The gradient is computed using second order accurate central differences
+    in the interior points and either first or second order accurate one-sides
+    (forward or backwards) differences at the boundaries.
+    The returned gradient hence has the same shape as the input array.
+
+    Parameters
+    ----------
+    f : array_like
+        An N-dimensional array containing samples of a scalar function.
+    varargs : list of scalar or array, optional
+        Spacing between f values. Default unitary spacing for all dimensions.
+        Spacing can be specified using:
+
+        1. single scalar to specify a sample distance for all dimensions.
+        2. N scalars to specify a constant sample distance for each dimension.
+           i.e. `dx`, `dy`, `dz`, ...
+        3. N arrays to specify the coordinates of the values along each
+           dimension of F. The length of the array must match the size of
+           the corresponding dimension
+        4. Any combination of N scalars/arrays with the meaning of 2. and 3.
+
+        If `axis` is given, the number of varargs must equal the number of axes.
+        Default: 1.
+
+    edge_order : {1, 2}, optional
+        Gradient is calculated using N-th order accurate differences
+        at the boundaries. Default: 1.
+
+        .. versionadded:: 1.9.1
+
+    axis : None or int or tuple of ints, optional
+        Gradient is calculated only along the given axis or axes
+        The default (axis = None) is to calculate the gradient for all the axes
+        of the input array. axis may be negative, in which case it counts from
+        the last to the first axis.
+
+        .. versionadded:: 1.11.0
+
+    Returns
+    -------
+    gradient : ndarray or list of ndarray
+        A list of ndarrays (or a single ndarray if there is only one dimension)
+        corresponding to the derivatives of f with respect to each dimension.
+        Each derivative has the same shape as f.
+
+    Examples
+    --------
+    >>> f = np.array([1, 2, 4, 7, 11, 16], dtype=float)
+    >>> np.gradient(f)
+    array([1. , 1.5, 2.5, 3.5, 4.5, 5. ])
+    >>> np.gradient(f, 2)
+    array([0.5 ,  0.75,  1.25,  1.75,  2.25,  2.5 ])
+
+    Spacing can be also specified with an array that represents the coordinates
+    of the values F along the dimensions.
+    For instance a uniform spacing:
+
+    >>> x = np.arange(f.size)
+    >>> np.gradient(f, x)
+    array([1. ,  1.5,  2.5,  3.5,  4.5,  5. ])
+
+    Or a non uniform one:
+
+    >>> x = np.array([0., 1., 1.5, 3.5, 4., 6.], dtype=float)
+    >>> np.gradient(f, x)
+    array([1. ,  3. ,  3.5,  6.7,  6.9,  2.5])
+
+    For two dimensional arrays, the return will be two arrays ordered by
+    axis. In this example the first array stands for the gradient in
+    rows and the second one in columns direction:
+
+    >>> np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=float))
+    [array([[ 2.,  2., -1.],
+           [ 2.,  2., -1.]]), array([[1. , 2.5, 4. ],
+           [1. , 1. , 1. ]])]
+
+    In this example the spacing is also specified:
+    uniform for axis=0 and non uniform for axis=1
+
+    >>> dx = 2.
+    >>> y = [1., 1.5, 3.5]
+    >>> np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=float), dx, y)
+    [array([[ 1. ,  1. , -0.5],
+           [ 1. ,  1. , -0.5]]), array([[2. , 2. , 2. ],
+           [2. , 1.7, 0.5]])]
+
+    It is possible to specify how boundaries are treated using `edge_order`
+
+    >>> x = np.array([0, 1, 2, 3, 4])
+    >>> f = x**2
+    >>> np.gradient(f, edge_order=1)
+    array([1.,  2.,  4.,  6.,  7.])
+    >>> np.gradient(f, edge_order=2)
+    array([0., 2., 4., 6., 8.])
+
+    The `axis` keyword can be used to specify a subset of axes of which the
+    gradient is calculated
+
+    >>> np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=float), axis=0)
+    array([[ 2.,  2., -1.],
+           [ 2.,  2., -1.]])
+
+    Notes
+    -----
+    Assuming that :math:`f\\in C^{3}` (i.e., :math:`f` has at least 3 continuous
+    derivatives) and let :math:`h_{*}` be a non-homogeneous stepsize, we
+    minimize the "consistency error" :math:`\\eta_{i}` between the true gradient
+    and its estimate from a linear combination of the neighboring grid-points:
+
+    .. math::
+
+        \\eta_{i} = f_{i}^{\\left(1\\right)} -
+                    \\left[ \\alpha f\\left(x_{i}\\right) +
+                            \\beta f\\left(x_{i} + h_{d}\\right) +
+                            \\gamma f\\left(x_{i}-h_{s}\\right)
+                    \\right]
+
+    By substituting :math:`f(x_{i} + h_{d})` and :math:`f(x_{i} - h_{s})`
+    with their Taylor series expansion, this translates into solving
+    the following the linear system:
+
+    .. math::
+
+        \\left\\{
+            \\begin{array}{r}
+                \\alpha+\\beta+\\gamma=0 \\\\
+                \\beta h_{d}-\\gamma h_{s}=1 \\\\
+                \\beta h_{d}^{2}+\\gamma h_{s}^{2}=0
+            \\end{array}
+        \\right.
+
+    The resulting approximation of :math:`f_{i}^{(1)}` is the following:
+
+    .. math::
+
+        \\hat f_{i}^{(1)} =
+            \\frac{
+                h_{s}^{2}f\\left(x_{i} + h_{d}\\right)
+                + \\left(h_{d}^{2} - h_{s}^{2}\\right)f\\left(x_{i}\\right)
+                - h_{d}^{2}f\\left(x_{i}-h_{s}\\right)}
+                { h_{s}h_{d}\\left(h_{d} + h_{s}\\right)}
+            + \\mathcal{O}\\left(\\frac{h_{d}h_{s}^{2}
+                                + h_{s}h_{d}^{2}}{h_{d}
+                                + h_{s}}\\right)
+
+    It is worth noting that if :math:`h_{s}=h_{d}`
+    (i.e., data are evenly spaced)
+    we find the standard second order approximation:
+
+    .. math::
+
+        \\hat f_{i}^{(1)}=
+            \\frac{f\\left(x_{i+1}\\right) - f\\left(x_{i-1}\\right)}{2h}
+            + \\mathcal{O}\\left(h^{2}\\right)
+
+    With a similar procedure the forward/backward approximations used for
+    boundaries can be derived.
+
+    References
+    ----------
+    .. [1]  Quarteroni A., Sacco R., Saleri F. (2007) Numerical Mathematics
+            (Texts in Applied Mathematics). New York: Springer.
+    .. [2]  Durran D. R. (1999) Numerical Methods for Wave Equations
+            in Geophysical Fluid Dynamics. New York: Springer.
+    .. [3]  Fornberg B. (1988) Generation of Finite Difference Formulas on
+            Arbitrarily Spaced Grids,
+            Mathematics of Computation 51, no. 184 : 699-706.
+            `PDF <http://www.ams.org/journals/mcom/1988-51-184/
+            S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf>`_.
+    """
+    f = np.asanyarray(f)
+    N = f.ndim  # number of dimensions
+
+    if axis is None:
+        axes = tuple(range(N))
+    else:
+        axes = _nx.normalize_axis_tuple(axis, N)
+
+    len_axes = len(axes)
     n = len(varargs)
     if n == 0:
-        dx = [1.0]*N
-    elif n == 1:
-        dx = [varargs[0]]*N
-    elif n == N:
+        # no spacing argument - use 1 in all axes
+        dx = [1.0] * len_axes
+    elif n == 1 and np.ndim(varargs[0]) == 0:
+        # single scalar for all axes
+        dx = varargs * len_axes
+    elif n == len_axes:
+        # scalar or 1d array for each axis
         dx = list(varargs)
+        for i, distances in enumerate(dx):
+            distances = np.asanyarray(distances)
+            if distances.ndim == 0:
+                continue
+            elif distances.ndim != 1:
+                raise ValueError("distances must be either scalars or 1d")
+            if len(distances) != f.shape[axes[i]]:
+                raise ValueError("when 1d, distances must match "
+                                 "the length of the corresponding dimension")
+            if np.issubdtype(distances.dtype, np.integer):
+                # Convert numpy integer types to float64 to avoid modular
+                # arithmetic in np.diff(distances).
+                distances = distances.astype(np.float64)
+            diffx = np.diff(distances)
+            # if distances are constant reduce to the scalar case
+            # since it brings a consistent speedup
+            if (diffx == diffx[0]).all():
+                diffx = diffx[0]
+            dx[i] = diffx
     else:
-        raise SyntaxError, "invalid number of arguments"
-
-    # use central differences on interior and first differences on endpoints
-
-    print dx
+        raise TypeError("invalid number of arguments")
+
+    if edge_order > 2:
+        raise ValueError("'edge_order' greater than 2 not supported")
+
+    # use central differences on interior and one-sided differences on the
+    # endpoints. This preserves second order-accuracy over the full domain.
+
     outvals = []
 
     # create slice objects --- initially all are [:, :, ..., :]
     slice1 = [slice(None)]*N
     slice2 = [slice(None)]*N
     slice3 = [slice(None)]*N
-
-    otype = f.dtype.char
-    if otype not in ['f', 'd', 'F', 'D']:
-        otype = 'd'
-
-    for axis in range(N):
-        # select out appropriate parts for this dimension
-        out = zeros(f.shape, f.dtype.char)
+    slice4 = [slice(None)]*N
+
+    otype = f.dtype
+    if otype.type is np.datetime64:
+        # the timedelta dtype with the same unit information
+        otype = np.dtype(otype.name.replace('datetime', 'timedelta'))
+        # view as timedelta to allow addition
+        f = f.view(otype)
+    elif otype.type is np.timedelta64:
+        pass
+    elif np.issubdtype(otype, np.inexact):
+        pass
+    else:
+        # All other types convert to floating point.
+        # First check if f is a numpy integer type; if so, convert f to float64
+        # to avoid modular arithmetic when computing the changes in f.
+        if np.issubdtype(otype, np.integer):
+            f = f.astype(np.float64)
+        otype = np.float64
+
+    for axis, ax_dx in zip(axes, dx):
+        if f.shape[axis] < edge_order + 1:
+            raise ValueError(
+                "Shape of array too small to calculate a numerical gradient, "
+                "at least (edge_order + 1) elements are required.")
+        # result allocation
+        out = np.empty_like(f, dtype=otype)
+
+        # spacing for the current axis
+        uniform_spacing = np.ndim(ax_dx) == 0
+
+        # Numerical differentiation: 2nd order interior
         slice1[axis] = slice(1, -1)
-        slice2[axis] = slice(2, None)
-        slice3[axis] = slice(None, -2)
-        # 1D equivalent -- out[1:-1] = (f[2:] - f[:-2])/2.0
-        out[slice1] = (f[slice2] - f[slice3])/2.0
-        slice1[axis] = 0
-        slice2[axis] = 1
-        slice3[axis] = 0
-        # 1D equivalent -- out[0] = (f[1] - f[0])
-        out[slice1] = (f[slice2] - f[slice3])
-        slice1[axis] = -1
-        slice2[axis] = -1
-        slice3[axis] = -2
-        # 1D equivalent -- out[-1] = (f[-1] - f[-2])
-        out[slice1] = (f[slice2] - f[slice3])
-
-        # divide by step size
-        outvals.append(out / dx[axis])
+        slice2[axis] = slice(None, -2)
+        slice3[axis] = slice(1, -1)
+        slice4[axis] = slice(2, None)
+
+        if uniform_spacing:
+            out[tuple(slice1)] = (f[tuple(slice4)] - f[tuple(slice2)]) / (2. * ax_dx)
+        else:
+            dx1 = ax_dx[0:-1]
+            dx2 = ax_dx[1:]
+            a = -(dx2)/(dx1 * (dx1 + dx2))
+            b = (dx2 - dx1) / (dx1 * dx2)
+            c = dx1 / (dx2 * (dx1 + dx2))
+            # fix the shape for broadcasting
+            shape = np.ones(N, dtype=int)
+            shape[axis] = -1
+            a.shape = b.shape = c.shape = shape
+            # 1D equivalent -- out[1:-1] = a * f[:-2] + b * f[1:-1] + c * f[2:]
+            out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] + c * f[tuple(slice4)]
+
+        # Numerical differentiation: 1st order edges
+        if edge_order == 1:
+            slice1[axis] = 0
+            slice2[axis] = 1
+            slice3[axis] = 0
+            dx_0 = ax_dx if uniform_spacing else ax_dx[0]
+            # 1D equivalent -- out[0] = (f[1] - f[0]) / (x[1] - x[0])
+            out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_0
+
+            slice1[axis] = -1
+            slice2[axis] = -1
+            slice3[axis] = -2
+            dx_n = ax_dx if uniform_spacing else ax_dx[-1]
+            # 1D equivalent -- out[-1] = (f[-1] - f[-2]) / (x[-1] - x[-2])
+            out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_n
+
+        # Numerical differentiation: 2nd order edges
+        else:
+            slice1[axis] = 0
+            slice2[axis] = 0
+            slice3[axis] = 1
+            slice4[axis] = 2
+            if uniform_spacing:
+                a = -1.5 / ax_dx
+                b = 2. / ax_dx
+                c = -0.5 / ax_dx
+            else:
+                dx1 = ax_dx[0]
+                dx2 = ax_dx[1]
+                a = -(2. * dx1 + dx2)/(dx1 * (dx1 + dx2))
+                b = (dx1 + dx2) / (dx1 * dx2)
+                c = - dx1 / (dx2 * (dx1 + dx2))
+            # 1D equivalent -- out[0] = a * f[0] + b * f[1] + c * f[2]
+            out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] + c * f[tuple(slice4)]
+
+            slice1[axis] = -1
+            slice2[axis] = -3
+            slice3[axis] = -2
+            slice4[axis] = -1
+            if uniform_spacing:
+                a = 0.5 / ax_dx
+                b = -2. / ax_dx
+                c = 1.5 / ax_dx
+            else:
+                dx1 = ax_dx[-2]
+                dx2 = ax_dx[-1]
+                a = (dx2) / (dx1 * (dx1 + dx2))
+                b = - (dx2 + dx1) / (dx1 * dx2)
+                c = (2. * dx2 + dx1) / (dx2 * (dx1 + dx2))
+            # 1D equivalent -- out[-1] = a * f[-3] + b * f[-2] + c * f[-1]
+            out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] + c * f[tuple(slice4)]
+
+        outvals.append(out)
 
         # reset the slice object in this dimension to ":"
         slice1[axis] = slice(None)
         slice2[axis] = slice(None)
         slice3[axis] = slice(None)
-
-    if N == 1:
+        slice4[axis] = slice(None)
+
+    if len_axes == 1:
         return outvals[0]
     else:
         return outvals
 
 
-def diff(a, n=1, axis=-1):
-    """Calculate the nth order discrete difference along given axis.
+def _diff_dispatcher(a, n=None, axis=None, prepend=None, append=None):
+    return (a, prepend, append)
+
+
+@array_function_dispatch(_diff_dispatcher)
+def diff(a, n=1, axis=-1, prepend=np._NoValue, append=np._NoValue):
+    """
+    Calculate the n-th discrete difference along the given axis.
+
+    The first difference is given by ``out[i] = a[i+1] - a[i]`` along
+    the given axis, higher differences are calculated by using `diff`
+    recursively.
+
+    Parameters
+    ----------
+    a : array_like
+        Input array
+    n : int, optional
+        The number of times values are differenced. If zero, the input
+        is returned as-is.
+    axis : int, optional
+        The axis along which the difference is taken, default is the
+        last axis.
+    prepend, append : array_like, optional
+        Values to prepend or append to `a` along axis prior to
+        performing the difference.  Scalar values are expanded to
+        arrays with length 1 in the direction of axis and the shape
+        of the input array in along all other axes.  Otherwise the
+        dimension and shape must match `a` except along axis.
+
+        .. versionadded:: 1.16.0
+
+    Returns
+    -------
+    diff : ndarray
+        The n-th differences. The shape of the output is the same as `a`
+        except along `axis` where the dimension is smaller by `n`. The
+        type of the output is the same as the type of the difference
+        between any two elements of `a`. This is the same as the type of
+        `a` in most cases. A notable exception is `datetime64`, which
+        results in a `timedelta64` output array.
+
+    See Also
+    --------
+    gradient, ediff1d, cumsum
+
+    Notes
+    -----
+    Type is preserved for boolean arrays, so the result will contain
+    `False` when consecutive elements are the same and `True` when they
+    differ.
+
+    For unsigned integer arrays, the results will also be unsigned. This
+    should not be surprising, as the result is consistent with
+    calculating the difference directly:
+
+    >>> u8_arr = np.array([1, 0], dtype=np.uint8)
+    >>> np.diff(u8_arr)
+    array([255], dtype=uint8)
+    >>> u8_arr[1,...] - u8_arr[0,...]
+    255
+
+    If this is not desirable, then the array should be cast to a larger
+    integer type first:
+
+    >>> i16_arr = u8_arr.astype(np.int16)
+    >>> np.diff(i16_arr)
+    array([-1], dtype=int16)
+
+    Examples
+    --------
+    >>> x = np.array([1, 2, 4, 7, 0])
+    >>> np.diff(x)
+    array([ 1,  2,  3, -7])
+    >>> np.diff(x, n=2)
+    array([  1,   1, -10])
+
+    >>> x = np.array([[1, 3, 6, 10], [0, 5, 6, 8]])
+    >>> np.diff(x)
+    array([[2, 3, 4],
+           [5, 1, 2]])
+    >>> np.diff(x, axis=0)
+    array([[-1,  2,  0, -2]])
+
+    >>> x = np.arange('1066-10-13', '1066-10-16', dtype=np.datetime64)
+    >>> np.diff(x)
+    array([1, 1], dtype='timedelta64[D]')
+
     """
     if n == 0:
         return a
     if n < 0:
-        raise ValueError, 'order must be non-negative but got ' + `n`
-    a = asarray(a)
-    nd = len(a.shape)
-    slice1 = [slice(None)]*nd
-    slice2 = [slice(None)]*nd
+        raise ValueError(
+            "order must be non-negative but got " + repr(n))
+
+    a = asanyarray(a)
+    nd = a.ndim
+    if nd == 0:
+        raise ValueError("diff requires input that is at least one dimensional")
+    axis = normalize_axis_index(axis, nd)
+
+    combined = []
+    if prepend is not np._NoValue:
+        prepend = np.asanyarray(prepend)
+        if prepend.ndim == 0:
+            shape = list(a.shape)
+            shape[axis] = 1
+            prepend = np.broadcast_to(prepend, tuple(shape))
+        combined.append(prepend)
+
+    combined.append(a)
+
+    if append is not np._NoValue:
+        append = np.asanyarray(append)
+        if append.ndim == 0:
+            shape = list(a.shape)
+            shape[axis] = 1
+            append = np.broadcast_to(append, tuple(shape))
+        combined.append(append)
+
+    if len(combined) > 1:
+        a = np.concatenate(combined, axis)
+
+    slice1 = [slice(None)] * nd
+    slice2 = [slice(None)] * nd
     slice1[axis] = slice(1, None)
     slice2[axis] = slice(None, -1)
     slice1 = tuple(slice1)
     slice2 = tuple(slice2)
-    if n > 1:
-        return diff(a[slice1]-a[slice2], n-1, axis=axis)
+
+    op = not_equal if a.dtype == np.bool_ else subtract
+    for _ in range(n):
+        a = op(a[slice1], a[slice2])
+
+    return a
+
+
+def _interp_dispatcher(x, xp, fp, left=None, right=None, period=None):
+    return (x, xp, fp)
+
+
+@array_function_dispatch(_interp_dispatcher)
+def interp(x, xp, fp, left=None, right=None, period=None):
+    """
+    One-dimensional linear interpolation for monotonically increasing sample points.
+
+    Returns the one-dimensional piecewise linear interpolant to a function
+    with given discrete data points (`xp`, `fp`), evaluated at `x`.
+
+    Parameters
+    ----------
+    x : array_like
+        The x-coordinates at which to evaluate the interpolated values.
+
+    xp : 1-D sequence of floats
+        The x-coordinates of the data points, must be increasing if argument
+        `period` is not specified. Otherwise, `xp` is internally sorted after
+        normalizing the periodic boundaries with ``xp = xp % period``.
+
+    fp : 1-D sequence of float or complex
+        The y-coordinates of the data points, same length as `xp`.
+
+    left : optional float or complex corresponding to fp
+        Value to return for `x < xp[0]`, default is `fp[0]`.
+
+    right : optional float or complex corresponding to fp
+        Value to return for `x > xp[-1]`, default is `fp[-1]`.
+
+    period : None or float, optional
+        A period for the x-coordinates. This parameter allows the proper
+        interpolation of angular x-coordinates. Parameters `left` and `right`
+        are ignored if `period` is specified.
+
+        .. versionadded:: 1.10.0
+
+    Returns
+    -------
+    y : float or complex (corresponding to fp) or ndarray
+        The interpolated values, same shape as `x`.
+
+    Raises
+    ------
+    ValueError
+        If `xp` and `fp` have different length
+        If `xp` or `fp` are not 1-D sequences
+        If `period == 0`
+
+    See Also
+    --------
+    scipy.interpolate
+
+    Warnings
+    --------
+    The x-coordinate sequence is expected to be increasing, but this is not
+    explicitly enforced.  However, if the sequence `xp` is non-increasing,
+    interpolation results are meaningless.
+
+    Note that, since NaN is unsortable, `xp` also cannot contain NaNs.
+
+    A simple check for `xp` being strictly increasing is::
+
+        np.all(np.diff(xp) > 0)
+
+    Examples
+    --------
+    >>> xp = [1, 2, 3]
+    >>> fp = [3, 2, 0]
+    >>> np.interp(2.5, xp, fp)
+    1.0
+    >>> np.interp([0, 1, 1.5, 2.72, 3.14], xp, fp)
+    array([3.  , 3.  , 2.5 , 0.56, 0.  ])
+    >>> UNDEF = -99.0
+    >>> np.interp(3.14, xp, fp, right=UNDEF)
+    -99.0
+
+    Plot an interpolant to the sine function:
+
+    >>> x = np.linspace(0, 2*np.pi, 10)
+    >>> y = np.sin(x)
+    >>> xvals = np.linspace(0, 2*np.pi, 50)
+    >>> yinterp = np.interp(xvals, x, y)
+    >>> import matplotlib.pyplot as plt
+    >>> plt.plot(x, y, 'o')
+    [<matplotlib.lines.Line2D object at 0x...>]
+    >>> plt.plot(xvals, yinterp, '-x')
+    [<matplotlib.lines.Line2D object at 0x...>]
+    >>> plt.show()
+
+    Interpolation with periodic x-coordinates:
+
+    >>> x = [-180, -170, -185, 185, -10, -5, 0, 365]
+    >>> xp = [190, -190, 350, -350]
+    >>> fp = [5, 10, 3, 4]
+    >>> np.interp(x, xp, fp, period=360)
+    array([7.5 , 5.  , 8.75, 6.25, 3.  , 3.25, 3.5 , 3.75])
+
+    Complex interpolation:
+
+    >>> x = [1.5, 4.0]
+    >>> xp = [2,3,5]
+    >>> fp = [1.0j, 0, 2+3j]
+    >>> np.interp(x, xp, fp)
+    array([0.+1.j , 1.+1.5j])
+
+    """
+
+    fp = np.asarray(fp)
+
+    if np.iscomplexobj(fp):
+        interp_func = compiled_interp_complex
+        input_dtype = np.complex128
     else:
-        return a[slice1]-a[slice2]
-
-def angle(z, deg=0):
-    """Return the angle of the complex argument z.
-    """
-    if deg:
-        fact = 180/pi
-    else:
-        fact = 1.0
-    z = asarray(z)
-    if (issubclass(z.dtype.type, _nx.complexfloating)):
+        interp_func = compiled_interp
+        input_dtype = np.float64
+
+    if period is not None:
+        if period == 0:
+            raise ValueError("period must be a non-zero value")
+        period = abs(period)
+        left = None
+        right = None
+
+        x = np.asarray(x, dtype=np.float64)
+        xp = np.asarray(xp, dtype=np.float64)
+        fp = np.asarray(fp, dtype=input_dtype)
+
+        if xp.ndim != 1 or fp.ndim != 1:
+            raise ValueError("Data points must be 1-D sequences")
+        if xp.shape[0] != fp.shape[0]:
+            raise ValueError("fp and xp are not of the same length")
+        # normalizing periodic boundaries
+        x = x % period
+        xp = xp % period
+        asort_xp = np.argsort(xp)
+        xp = xp[asort_xp]
+        fp = fp[asort_xp]
+        xp = np.concatenate((xp[-1:]-period, xp, xp[0:1]+period))
+        fp = np.concatenate((fp[-1:], fp, fp[0:1]))
+
+    return interp_func(x, xp, fp, left, right)
+
+
+def _angle_dispatcher(z, deg=None):
+    return (z,)
+
+
+@array_function_dispatch(_angle_dispatcher)
+def angle(z, deg=False):
+    """
+    Return the angle of the complex argument.
+
+    Parameters
+    ----------
+    z : array_like
+        A complex number or sequence of complex numbers.
+    deg : bool, optional
+        Return angle in degrees if True, radians if False (default).
+
+    Returns
+    -------
+    angle : ndarray or scalar
+        The counterclockwise angle from the positive real axis on the complex
+        plane in the range ``(-pi, pi]``, with dtype as numpy.float64.
+
+        .. versionchanged:: 1.16.0
+            This function works on subclasses of ndarray like `ma.array`.
+
+    See Also
+    --------
+    arctan2
+    absolute
+
+    Notes
+    -----
+    Although the angle of the complex number 0 is undefined, ``numpy.angle(0)``
+    returns the value 0.
+
+    Examples
+    --------
+    >>> np.angle([1.0, 1.0j, 1+1j])               # in radians
+    array([ 0.        ,  1.57079633,  0.78539816]) # may vary
+    >>> np.angle(1+1j, deg=True)                  # in degrees
+    45.0
+
+    """
+    z = asanyarray(z)
+    if issubclass(z.dtype.type, _nx.complexfloating):
         zimag = z.imag
         zreal = z.real
     else:
         zimag = 0
         zreal = z
-    return arctan2(zimag, zreal) * fact
-
-def unwrap(p, discont=pi, axis=-1):
-    """Unwrap radian phase p by changing absolute jumps greater than
-       'discont' to their 2*pi complement along the given axis.
+
+    a = arctan2(zimag, zreal)
+    if deg:
+        a *= 180/pi
+    return a
+
+
+def _unwrap_dispatcher(p, discont=None, axis=None, *, period=None):
+    return (p,)
+
+
+@array_function_dispatch(_unwrap_dispatcher)
+def unwrap(p, discont=None, axis=-1, *, period=2*pi):
+    r"""
+    Unwrap by taking the complement of large deltas with respect to the period.
+
+    This unwraps a signal `p` by changing elements which have an absolute
+    difference from their predecessor of more than ``max(discont, period/2)``
+    to their `period`-complementary values.
+
+    For the default case where `period` is :math:`2\pi` and `discont` is
+    :math:`\pi`, this unwraps a radian phase `p` such that adjacent differences
+    are never greater than :math:`\pi` by adding :math:`2k\pi` for some
+    integer :math:`k`.
+
+    Parameters
+    ----------
+    p : array_like
+        Input array.
+    discont : float, optional
+        Maximum discontinuity between values, default is ``period/2``.
+        Values below ``period/2`` are treated as if they were ``period/2``.
+        To have an effect different from the default, `discont` should be
+        larger than ``period/2``.
+    axis : int, optional
+        Axis along which unwrap will operate, default is the last axis.
+    period : float, optional
+        Size of the range over which the input wraps. By default, it is
+        ``2 pi``.
+
+        .. versionadded:: 1.21.0
+
+    Returns
+    -------
+    out : ndarray
+        Output array.
+
+    See Also
+    --------
+    rad2deg, deg2rad
+
+    Notes
+    -----
+    If the discontinuity in `p` is smaller than ``period/2``,
+    but larger than `discont`, no unwrapping is done because taking
+    the complement would only make the discontinuity larger.
+
+    Examples
+    --------
+    >>> phase = np.linspace(0, np.pi, num=5)
+    >>> phase[3:] += np.pi
+    >>> phase
+    array([ 0.        ,  0.78539816,  1.57079633,  5.49778714,  6.28318531]) # may vary
+    >>> np.unwrap(phase)
+    array([ 0.        ,  0.78539816,  1.57079633, -0.78539816,  0.        ]) # may vary
+    >>> np.unwrap([0, 1, 2, -1, 0], period=4)
+    array([0, 1, 2, 3, 4])
+    >>> np.unwrap([ 1, 2, 3, 4, 5, 6, 1, 2, 3], period=6)
+    array([1, 2, 3, 4, 5, 6, 7, 8, 9])
+    >>> np.unwrap([2, 3, 4, 5, 2, 3, 4, 5], period=4)
+    array([2, 3, 4, 5, 6, 7, 8, 9])
+    >>> phase_deg = np.mod(np.linspace(0 ,720, 19), 360) - 180
+    >>> np.unwrap(phase_deg, period=360)
+    array([-180., -140., -100.,  -60.,  -20.,   20.,   60.,  100.,  140.,
+            180.,  220.,  260.,  300.,  340.,  380.,  420.,  460.,  500.,
+            540.])
     """
     p = asarray(p)
-    nd = len(p.shape)
+    nd = p.ndim
     dd = diff(p, axis=axis)
+    if discont is None:
+        discont = period/2
     slice1 = [slice(None, None)]*nd     # full slices
     slice1[axis] = slice(1, None)
-    ddmod = mod(dd+pi, 2*pi)-pi
-    _nx.putmask(ddmod, (ddmod==-pi) & (dd > 0), pi)
-    ph_correct = ddmod - dd;
-    _nx.putmask(ph_correct, abs(dd)<discont, 0)
-    up = array(p, copy=True, dtype='d')
+    slice1 = tuple(slice1)
+    dtype = np.result_type(dd, period)
+    if _nx.issubdtype(dtype, _nx.integer):
+        interval_high, rem = divmod(period, 2)
+        boundary_ambiguous = rem == 0
+    else:
+        interval_high = period / 2
+        boundary_ambiguous = True
+    interval_low = -interval_high
+    ddmod = mod(dd - interval_low, period) + interval_low
+    if boundary_ambiguous:
+        # for `mask = (abs(dd) == period/2)`, the above line made
+        # `ddmod[mask] == -period/2`. correct these such that
+        # `ddmod[mask] == sign(dd[mask])*period/2`.
+        _nx.copyto(ddmod, interval_high,
+                   where=(ddmod == interval_low) & (dd > 0))
+    ph_correct = ddmod - dd
+    _nx.copyto(ph_correct, 0, where=abs(dd) < discont)
+    up = array(p, copy=True, dtype=dtype)
     up[slice1] = p[slice1] + ph_correct.cumsum(axis)
     return up
 
+
+def _sort_complex(a):
+    return (a,)
+
+
+@array_function_dispatch(_sort_complex)
 def sort_complex(a):
-    """ Sort 'a' as a complex array using the real part first and then
-    the imaginary part if the real part is equal (the default sort order
-    for complex arrays).  This function is a wrapper ensuring a complex
-    return type.
-    """
-    b = array(a,copy=True)
+    """
+    Sort a complex array using the real part first, then the imaginary part.
+
+    Parameters
+    ----------
+    a : array_like
+        Input array
+
+    Returns
+    -------
+    out : complex ndarray
+        Always returns a sorted complex array.
+
+    Examples
+    --------
+    >>> np.sort_complex([5, 3, 6, 2, 1])
+    array([1.+0.j, 2.+0.j, 3.+0.j, 5.+0.j, 6.+0.j])
+
+    >>> np.sort_complex([1 + 2j, 2 - 1j, 3 - 2j, 3 - 3j, 3 + 5j])
+    array([1.+2.j,  2.-1.j,  3.-3.j,  3.-2.j,  3.+5.j])
+
+    """
+    b = array(a, copy=True)
     b.sort()
     if not issubclass(b.dtype.type, _nx.complexfloating):
         if b.dtype.char in 'bhBH':
@@ -430,98 +1791,201 @@
     else:
         return b
 
+
+def _trim_zeros(filt, trim=None):
+    return (filt,)
+
+
+@array_function_dispatch(_trim_zeros)
 def trim_zeros(filt, trim='fb'):
-    """ Trim the leading and trailing zeros from a 1D array.
-
-    Example:
-        >>> import numpy
-        >>> a = array((0, 0, 0, 1, 2, 3, 2, 1, 0))
-        >>> numpy.trim_zeros(a)
-        array([1, 2, 3, 2, 1])
-    """
+    """
+    Trim the leading and/or trailing zeros from a 1-D array or sequence.
+
+    Parameters
+    ----------
+    filt : 1-D array or sequence
+        Input array.
+    trim : str, optional
+        A string with 'f' representing trim from front and 'b' to trim from
+        back. Default is 'fb', trim zeros from both front and back of the
+        array.
+
+    Returns
+    -------
+    trimmed : 1-D array or sequence
+        The result of trimming the input. The input data type is preserved.
+
+    Examples
+    --------
+    >>> a = np.array((0, 0, 0, 1, 2, 3, 0, 2, 1, 0))
+    >>> np.trim_zeros(a)
+    array([1, 2, 3, 0, 2, 1])
+
+    >>> np.trim_zeros(a, 'b')
+    array([0, 0, 0, ..., 0, 2, 1])
+
+    The input data type is preserved, list/tuple in means list/tuple out.
+
+    >>> np.trim_zeros([0, 1, 2, 0])
+    [1, 2]
+
+    """
+
     first = 0
     trim = trim.upper()
     if 'F' in trim:
         for i in filt:
-            if i != 0.: break
-            else: first = first + 1
+            if i != 0.:
+                break
+            else:
+                first = first + 1
     last = len(filt)
     if 'B' in trim:
         for i in filt[::-1]:
-            if i != 0.: break
-            else: last = last - 1
+            if i != 0.:
+                break
+            else:
+                last = last - 1
     return filt[first:last]
 
-def unique(inseq):
-    """Return unique items from a 1-dimensional sequence.
-    """
-    # Dictionary setting is quite fast.
-    set = {}
-    for item in inseq:
-        set[item] = None
-    return asarray(set.keys())
-
+
+def _extract_dispatcher(condition, arr):
+    return (condition, arr)
+
+
+@array_function_dispatch(_extract_dispatcher)
 def extract(condition, arr):
-    """Return the elements of ravel(arr) where ravel(condition) is True
-    (in 1D).
-
-    Equivalent to compress(ravel(condition), ravel(arr)).
-    """
-    return _nx.take(ravel(arr), nonzero(ravel(condition)))
-
-def insert(arr, mask, vals):
-    """Similar to putmask arr[mask] = vals but the 1D array vals has the
-    same number of elements as the non-zero values of mask. Inverse of
-    extract.
-    """
+    """
+    Return the elements of an array that satisfy some condition.
+
+    This is equivalent to ``np.compress(ravel(condition), ravel(arr))``.  If
+    `condition` is boolean ``np.extract`` is equivalent to ``arr[condition]``.
+
+    Note that `place` does the exact opposite of `extract`.
+
+    Parameters
+    ----------
+    condition : array_like
+        An array whose nonzero or True entries indicate the elements of `arr`
+        to extract.
+    arr : array_like
+        Input array of the same size as `condition`.
+
+    Returns
+    -------
+    extract : ndarray
+        Rank 1 array of values from `arr` where `condition` is True.
+
+    See Also
+    --------
+    take, put, copyto, compress, place
+
+    Examples
+    --------
+    >>> arr = np.arange(12).reshape((3, 4))
+    >>> arr
+    array([[ 0,  1,  2,  3],
+           [ 4,  5,  6,  7],
+           [ 8,  9, 10, 11]])
+    >>> condition = np.mod(arr, 3)==0
+    >>> condition
+    array([[ True, False, False,  True],
+           [False, False,  True, False],
+           [False,  True, False, False]])
+    >>> np.extract(condition, arr)
+    array([0, 3, 6, 9])
+
+
+    If `condition` is boolean:
+
+    >>> arr[condition]
+    array([0, 3, 6, 9])
+
+    """
+    return _nx.take(ravel(arr), nonzero(ravel(condition))[0])
+
+
+def _place_dispatcher(arr, mask, vals):
+    return (arr, mask, vals)
+
+
+@array_function_dispatch(_place_dispatcher)
+def place(arr, mask, vals):
+    """
+    Change elements of an array based on conditional and input values.
+
+    Similar to ``np.copyto(arr, vals, where=mask)``, the difference is that
+    `place` uses the first N elements of `vals`, where N is the number of
+    True values in `mask`, while `copyto` uses the elements where `mask`
+    is True.
+
+    Note that `extract` does the exact opposite of `place`.
+
+    Parameters
+    ----------
+    arr : ndarray
+        Array to put data into.
+    mask : array_like
+        Boolean mask array. Must have the same size as `a`.
+    vals : 1-D sequence
+        Values to put into `a`. Only the first N elements are used, where
+        N is the number of True values in `mask`. If `vals` is smaller
+        than N, it will be repeated, and if elements of `a` are to be masked,
+        this sequence must be non-empty.
+
+    See Also
+    --------
+    copyto, put, take, extract
+
+    Examples
+    --------
+    >>> arr = np.arange(6).reshape(2, 3)
+    >>> np.place(arr, arr>2, [44, 55])
+    >>> arr
+    array([[ 0,  1,  2],
+           [44, 55, 44]])
+
+    """
+    if not isinstance(arr, np.ndarray):
+        raise TypeError("argument 1 must be numpy.ndarray, "
+                        "not {name}".format(name=type(arr).__name__))
+
     return _insert(arr, mask, vals)
 
-def nansum(a, axis=-1):
-    """Sum the array over the given axis, treating NaNs as 0.
-    """
-    y = array(a)
-    if not issubclass(y.dtype.type, _nx.integer):
-        y[isnan(a)] = 0
-    return y.sum(axis)
-
-def nanmin(a, axis=-1):
-    """Find the minimium over the given axis, ignoring NaNs.
-    """
-    y = array(a)
-    if not issubclass(y.dtype.type, _nx.integer):
-        y[isnan(a)] = _nx.inf
-    return y.min(axis)
-
-def nanargmin(a, axis=-1):
-    """Find the indices of the minimium over the given axis ignoring NaNs.
-    """
-    y = array(a)
-    if not issubclass(y.dtype.type, _nx.integer):
-        y[isnan(a)] = _nx.inf
-    return y.argmin(axis)
-
-def nanmax(a, axis=-1):
-    """Find the maximum over the given axis ignoring NaNs.
-    """
-    y = array(a)
-    if not issubclass(y.dtype.type, _nx.integer):
-        y[isnan(a)] = -_nx.inf
-    return y.max(axis)
-
-def nanargmax(a, axis=-1):
-    """Find the maximum over the given axis ignoring NaNs.
-    """
-    y = array(a)
-    if not issubclass(y.dtype.type, _nx.integer):
-        y[isnan(a)] = -_nx.inf
-    return y.argmax(axis)
 
 def disp(mesg, device=None, linefeed=True):
-    """Display a message to the given device (default is sys.stdout)
-    with or without a linefeed.
+    """
+    Display a message on a device.
+
+    Parameters
+    ----------
+    mesg : str
+        Message to display.
+    device : object
+        Device to write message. If None, defaults to ``sys.stdout`` which is
+        very similar to ``print``. `device` needs to have ``write()`` and
+        ``flush()`` methods.
+    linefeed : bool, optional
+        Option whether to print a line feed or not. Defaults to True.
+
+    Raises
+    ------
+    AttributeError
+        If `device` does not have a ``write()`` or ``flush()`` method.
+
+    Examples
+    --------
+    Besides ``sys.stdout``, a file-like object can also be used as it has
+    both required methods:
+
+    >>> from io import StringIO
+    >>> buf = StringIO()
+    >>> np.disp(u'"Display" in a file', device=buf)
+    >>> buf.getvalue()
+    '"Display" in a file\\n'
+
     """
     if device is None:
-        import sys
         device = sys.stdout
     if linefeed:
         device.write('%s\n' % mesg)
@@ -530,353 +1994,3562 @@
     device.flush()
     return
 
-# return number of input arguments and
-#  number of default arguments
-import re
-def _get_nargs(obj):
-    if not callable(obj):
-        raise TypeError, "Object is not callable."
-    if hasattr(obj,'func_code'):
-        fcode = obj.func_code
-        nargs = fcode.co_argcount
-        if obj.func_defaults is not None:
-            ndefaults = len(obj.func_defaults)
+
+# See https://docs.scipy.org/doc/numpy/reference/c-api.generalized-ufuncs.html
+_DIMENSION_NAME = r'\w+'
+_CORE_DIMENSION_LIST = '(?:{0:}(?:,{0:})*)?'.format(_DIMENSION_NAME)
+_ARGUMENT = r'\({}\)'.format(_CORE_DIMENSION_LIST)
+_ARGUMENT_LIST = '{0:}(?:,{0:})*'.format(_ARGUMENT)
+_SIGNATURE = '^{0:}->{0:}$'.format(_ARGUMENT_LIST)
+
+
+def _parse_gufunc_signature(signature):
+    """
+    Parse string signatures for a generalized universal function.
+
+    Arguments
+    ---------
+    signature : string
+        Generalized universal function signature, e.g., ``(m,n),(n,p)->(m,p)``
+        for ``np.matmul``.
+
+    Returns
+    -------
+    Tuple of input and output core dimensions parsed from the signature, each
+    of the form List[Tuple[str, ...]].
+    """
+    signature = re.sub(r'\s+', '', signature)
+
+    if not re.match(_SIGNATURE, signature):
+        raise ValueError(
+            'not a valid gufunc signature: {}'.format(signature))
+    return tuple([tuple(re.findall(_DIMENSION_NAME, arg))
+                  for arg in re.findall(_ARGUMENT, arg_list)]
+                 for arg_list in signature.split('->'))
+
+
+def _update_dim_sizes(dim_sizes, arg, core_dims):
+    """
+    Incrementally check and update core dimension sizes for a single argument.
+
+    Arguments
+    ---------
+    dim_sizes : Dict[str, int]
+        Sizes of existing core dimensions. Will be updated in-place.
+    arg : ndarray
+        Argument to examine.
+    core_dims : Tuple[str, ...]
+        Core dimensions for this argument.
+    """
+    if not core_dims:
+        return
+
+    num_core_dims = len(core_dims)
+    if arg.ndim < num_core_dims:
+        raise ValueError(
+            '%d-dimensional argument does not have enough '
+            'dimensions for all core dimensions %r'
+            % (arg.ndim, core_dims))
+
+    core_shape = arg.shape[-num_core_dims:]
+    for dim, size in zip(core_dims, core_shape):
+        if dim in dim_sizes:
+            if size != dim_sizes[dim]:
+                raise ValueError(
+                    'inconsistent size for core dimension %r: %r vs %r'
+                    % (dim, size, dim_sizes[dim]))
         else:
-            ndefaults = 0
-        if isinstance(obj, types.MethodType):
-            nargs -= 1
-        return nargs, ndefaults
-    terr = re.compile(r'.*? takes exactly (?P<exargs>\d+) argument(s|) \((?P<gargs>\d+) given\)')
-    try:
-        obj()
-        return 0, 0
-    except TypeError, msg:
-        m = terr.match(str(msg))
-        if m:
-            nargs = int(m.group('exargs'))
-            ndefaults = int(m.group('gargs'))
-            if isinstance(obj, types.MethodType):
-                nargs -= 1
-            return nargs, ndefaults
-    raise ValueError, 'failed to determine the number of arguments for %s' % (obj)
-
-
-class vectorize(object):
-    """
- vectorize(somefunction, otypes=None, doc=None)
- Generalized Function class.
-
-  Description:
-
-    Define a vectorized function which takes nested sequence
-    objects or numpy arrays as inputs and returns a
-    numpy array as output, evaluating the function over successive
-    tuples of the input arrays like the python map function except it uses
-    the broadcasting rules of numpy.
-
-  Input:
-
-    somefunction -- a Python function or method
-
-  Example:
-
-    def myfunc(a, b):
-        if a > b:
-            return a-b
-        else
-            return a+b
-
-    vfunc = vectorize(myfunc)
-
+            dim_sizes[dim] = size
+
+
+def _parse_input_dimensions(args, input_core_dims):
+    """
+    Parse broadcast and core dimensions for vectorize with a signature.
+
+    Arguments
+    ---------
+    args : Tuple[ndarray, ...]
+        Tuple of input arguments to examine.
+    input_core_dims : List[Tuple[str, ...]]
+        List of core dimensions corresponding to each input.
+
+    Returns
+    -------
+    broadcast_shape : Tuple[int, ...]
+        Common shape to broadcast all non-core dimensions to.
+    dim_sizes : Dict[str, int]
+        Common sizes for named core dimensions.
+    """
+    broadcast_args = []
+    dim_sizes = {}
+    for arg, core_dims in zip(args, input_core_dims):
+        _update_dim_sizes(dim_sizes, arg, core_dims)
+        ndim = arg.ndim - len(core_dims)
+        dummy_array = np.lib.stride_tricks.as_strided(0, arg.shape[:ndim])
+        broadcast_args.append(dummy_array)
+    broadcast_shape = np.lib.stride_tricks._broadcast_shape(*broadcast_args)
+    return broadcast_shape, dim_sizes
+
+
+def _calculate_shapes(broadcast_shape, dim_sizes, list_of_core_dims):
+    """Helper for calculating broadcast shapes with core dimensions."""
+    return [broadcast_shape + tuple(dim_sizes[dim] for dim in core_dims)
+            for core_dims in list_of_core_dims]
+
+
+def _create_arrays(broadcast_shape, dim_sizes, list_of_core_dims, dtypes,
+                   results=None):
+    """Helper for creating output arrays in vectorize."""
+    shapes = _calculate_shapes(broadcast_shape, dim_sizes, list_of_core_dims)
+    if dtypes is None:
+        dtypes = [None] * len(shapes)
+    if results is None:
+        arrays = tuple(np.empty(shape=shape, dtype=dtype)
+                       for shape, dtype in zip(shapes, dtypes))
+    else:
+        arrays = tuple(np.empty_like(result, shape=shape, dtype=dtype)
+                       for result, shape, dtype
+                       in zip(results, shapes, dtypes))
+    return arrays
+
+
+@set_module('numpy')
+class vectorize:
+    """
+    vectorize(pyfunc, otypes=None, doc=None, excluded=None, cache=False,
+              signature=None)
+
+    Generalized function class.
+
+    Define a vectorized function which takes a nested sequence of objects or
+    numpy arrays as inputs and returns a single numpy array or a tuple of numpy
+    arrays. The vectorized function evaluates `pyfunc` over successive tuples
+    of the input arrays like the python map function, except it uses the
+    broadcasting rules of numpy.
+
+    The data type of the output of `vectorized` is determined by calling
+    the function with the first element of the input.  This can be avoided
+    by specifying the `otypes` argument.
+
+    Parameters
+    ----------
+    pyfunc : callable
+        A python function or method.
+    otypes : str or list of dtypes, optional
+        The output data type. It must be specified as either a string of
+        typecode characters or a list of data type specifiers. There should
+        be one data type specifier for each output.
+    doc : str, optional
+        The docstring for the function. If None, the docstring will be the
+        ``pyfunc.__doc__``.
+    excluded : set, optional
+        Set of strings or integers representing the positional or keyword
+        arguments for which the function will not be vectorized.  These will be
+        passed directly to `pyfunc` unmodified.
+
+        .. versionadded:: 1.7.0
+
+    cache : bool, optional
+        If `True`, then cache the first function call that determines the number
+        of outputs if `otypes` is not provided.
+
+        .. versionadded:: 1.7.0
+
+    signature : string, optional
+        Generalized universal function signature, e.g., ``(m,n),(n)->(m)`` for
+        vectorized matrix-vector multiplication. If provided, ``pyfunc`` will
+        be called with (and expected to return) arrays with shapes given by the
+        size of corresponding core dimensions. By default, ``pyfunc`` is
+        assumed to take scalars as input and output.
+
+        .. versionadded:: 1.12.0
+
+    Returns
+    -------
+    vectorized : callable
+        Vectorized function.
+
+    See Also
+    --------
+    frompyfunc : Takes an arbitrary Python function and returns a ufunc
+
+    Notes
+    -----
+    The `vectorize` function is provided primarily for convenience, not for
+    performance. The implementation is essentially a for loop.
+
+    If `otypes` is not specified, then a call to the function with the
+    first argument will be used to determine the number of outputs.  The
+    results of this call will be cached if `cache` is `True` to prevent
+    calling the function twice.  However, to implement the cache, the
+    original function must be wrapped which will slow down subsequent
+    calls, so only do this if your function is expensive.
+
+    The new keyword argument interface and `excluded` argument support
+    further degrades performance.
+
+    References
+    ----------
+    .. [1] :doc:`/reference/c-api/generalized-ufuncs`
+
+    Examples
+    --------
+    >>> def myfunc(a, b):
+    ...     "Return a-b if a>b, otherwise return a+b"
+    ...     if a > b:
+    ...         return a - b
+    ...     else:
+    ...         return a + b
+
+    >>> vfunc = np.vectorize(myfunc)
     >>> vfunc([1, 2, 3, 4], 2)
     array([3, 4, 1, 2])
 
-    """
-    def __init__(self, pyfunc, otypes='', doc=None):
-        nin, ndefault = _get_nargs(pyfunc)
-        self.thefunc = pyfunc
-        self.ufunc = None
-        self.nin = nin
-        self.nin_wo_defaults = nin - ndefault
-        self.nout = None
+    The docstring is taken from the input function to `vectorize` unless it
+    is specified:
+
+    >>> vfunc.__doc__
+    'Return a-b if a>b, otherwise return a+b'
+    >>> vfunc = np.vectorize(myfunc, doc='Vectorized `myfunc`')
+    >>> vfunc.__doc__
+    'Vectorized `myfunc`'
+
+    The output type is determined by evaluating the first element of the input,
+    unless it is specified:
+
+    >>> out = vfunc([1, 2, 3, 4], 2)
+    >>> type(out[0])
+    <class 'numpy.int64'>
+    >>> vfunc = np.vectorize(myfunc, otypes=[float])
+    >>> out = vfunc([1, 2, 3, 4], 2)
+    >>> type(out[0])
+    <class 'numpy.float64'>
+
+    The `excluded` argument can be used to prevent vectorizing over certain
+    arguments.  This can be useful for array-like arguments of a fixed length
+    such as the coefficients for a polynomial as in `polyval`:
+
+    >>> def mypolyval(p, x):
+    ...     _p = list(p)
+    ...     res = _p.pop(0)
+    ...     while _p:
+    ...         res = res*x + _p.pop(0)
+    ...     return res
+    >>> vpolyval = np.vectorize(mypolyval, excluded=['p'])
+    >>> vpolyval(p=[1, 2, 3], x=[0, 1])
+    array([3, 6])
+
+    Positional arguments may also be excluded by specifying their position:
+
+    >>> vpolyval.excluded.add(0)
+    >>> vpolyval([1, 2, 3], x=[0, 1])
+    array([3, 6])
+
+    The `signature` argument allows for vectorizing functions that act on
+    non-scalar arrays of fixed length. For example, you can use it for a
+    vectorized calculation of Pearson correlation coefficient and its p-value:
+
+    >>> import scipy.stats
+    >>> pearsonr = np.vectorize(scipy.stats.pearsonr,
+    ...                 signature='(n),(n)->(),()')
+    >>> pearsonr([[0, 1, 2, 3]], [[1, 2, 3, 4], [4, 3, 2, 1]])
+    (array([ 1., -1.]), array([ 0.,  0.]))
+
+    Or for a vectorized convolution:
+
+    >>> convolve = np.vectorize(np.convolve, signature='(n),(m)->(k)')
+    >>> convolve(np.eye(4), [1, 2, 1])
+    array([[1., 2., 1., 0., 0., 0.],
+           [0., 1., 2., 1., 0., 0.],
+           [0., 0., 1., 2., 1., 0.],
+           [0., 0., 0., 1., 2., 1.]])
+
+    """
+    def __init__(self, pyfunc, otypes=None, doc=None, excluded=None,
+                 cache=False, signature=None):
+        self.pyfunc = pyfunc
+        self.cache = cache
+        self.signature = signature
+        self._ufunc = {}    # Caching to improve default performance
+
         if doc is None:
             self.__doc__ = pyfunc.__doc__
         else:
             self.__doc__ = doc
-        if isinstance(otypes, types.StringType):
-            self.otypes = otypes
+
+        if isinstance(otypes, str):
+            for char in otypes:
+                if char not in typecodes['All']:
+                    raise ValueError("Invalid otype specified: %s" % (char,))
+        elif iterable(otypes):
+            otypes = ''.join([_nx.dtype(x).char for x in otypes])
+        elif otypes is not None:
+            raise ValueError("Invalid otype specification")
+        self.otypes = otypes
+
+        # Excluded variable support
+        if excluded is None:
+            excluded = set()
+        self.excluded = set(excluded)
+
+        if signature is not None:
+            self._in_and_out_core_dims = _parse_gufunc_signature(signature)
         else:
-            raise ValueError, "output types must be a string"
-        for char in self.otypes:
-            if char not in typecodes['All']:
-                raise ValueError, "invalid typecode specified"
-        self.lastcallargs = 0
-
-    def __call__(self, *args):
-        # get number of outputs and output types by calling
-        #  the function on the first entries of args
-        nargs = len(args)
-        if (nargs > self.nin) or (nargs < self.nin_wo_defaults):
-            raise ValueError, "mismatch between python function inputs"\
-                  " and received arguments"
-        if self.nout is None or self.otypes == '':
-            newargs = []
-            for arg in args:
-                newargs.append(asarray(arg).flat[0])
-            theout = self.thefunc(*newargs)
-            if isinstance(theout, types.TupleType):
-                self.nout = len(theout)
+            self._in_and_out_core_dims = None
+
+    def __call__(self, *args, **kwargs):
+        """
+        Return arrays with the results of `pyfunc` broadcast (vectorized) over
+        `args` and `kwargs` not in `excluded`.
+        """
+        excluded = self.excluded
+        if not kwargs and not excluded:
+            func = self.pyfunc
+            vargs = args
+        else:
+            # The wrapper accepts only positional arguments: we use `names` and
+            # `inds` to mutate `the_args` and `kwargs` to pass to the original
+            # function.
+            nargs = len(args)
+
+            names = [_n for _n in kwargs if _n not in excluded]
+            inds = [_i for _i in range(nargs) if _i not in excluded]
+            the_args = list(args)
+
+            def func(*vargs):
+                for _n, _i in enumerate(inds):
+                    the_args[_i] = vargs[_n]
+                kwargs.update(zip(names, vargs[len(inds):]))
+                return self.pyfunc(*the_args, **kwargs)
+
+            vargs = [args[_i] for _i in inds]
+            vargs.extend([kwargs[_n] for _n in names])
+
+        return self._vectorize_call(func=func, args=vargs)
+
+    def _get_ufunc_and_otypes(self, func, args):
+        """Return (ufunc, otypes)."""
+        # frompyfunc will fail if args is empty
+        if not args:
+            raise ValueError('args can not be empty')
+
+        if self.otypes is not None:
+            otypes = self.otypes
+
+            # self._ufunc is a dictionary whose keys are the number of
+            # arguments (i.e. len(args)) and whose values are ufuncs created
+            # by frompyfunc. len(args) can be different for different calls if
+            # self.pyfunc has parameters with default values.  We only use the
+            # cache when func is self.pyfunc, which occurs when the call uses
+            # only positional arguments and no arguments are excluded.
+
+            nin = len(args)
+            nout = len(self.otypes)
+            if func is not self.pyfunc or nin not in self._ufunc:
+                ufunc = frompyfunc(func, nin, nout)
             else:
-                self.nout = 1
-                theout = (theout,)
-            if self.otypes == '':
-                otypes = []
-                for k in range(self.nout):
-                    otypes.append(asarray(theout[k]).dtype.char)
-                self.otypes = ''.join(otypes)
-
-        if (self.ufunc is None) or (self.lastcallargs != nargs):
-            self.ufunc = frompyfunc(self.thefunc, nargs, self.nout)
-            self.lastcallargs = nargs
-
-        if self.nout == 1:
-            return array(self.ufunc(*args),copy=False).astype(self.otypes[0])
+                ufunc = None  # We'll get it from self._ufunc
+            if func is self.pyfunc:
+                ufunc = self._ufunc.setdefault(nin, ufunc)
         else:
-            return tuple([array(x,copy=False).astype(c) \
-                          for x, c in zip(self.ufunc(*args), self.otypes)])
-
-def cov(m,y=None, rowvar=1, bias=0):
-    """Estimate the covariance matrix.
-
-    If m is a vector, return the variance.  For matrices return the
-    covariance matrix.
-
-    If y is given it is treated as an additional (set of)
-    variable(s). 
-
-    Normalization is by (N-1) where N is the number of observations
-    (unbiased estimate).  If bias is 1 then normalization is by N.
-
-    If rowvar is non-zero (default), then each row is a variable with
-    observations in the columns, otherwise each column
-    is a variable and the observations are in the rows.
-    """
-
-    X = array(m,ndmin=2)
-    if X.shape[0] == 1:
-        rowvar = 1
-    if rowvar:
-        axis = 0
-        tup = (slice(None),newaxis)
+            # Get number of outputs and output types by calling the function on
+            # the first entries of args.  We also cache the result to prevent
+            # the subsequent call when the ufunc is evaluated.
+            # Assumes that ufunc first evaluates the 0th elements in the input
+            # arrays (the input values are not checked to ensure this)
+            args = [asarray(arg) for arg in args]
+            if builtins.any(arg.size == 0 for arg in args):
+                raise ValueError('cannot call `vectorize` on size 0 inputs '
+                                 'unless `otypes` is set')
+
+            inputs = [arg.flat[0] for arg in args]
+            outputs = func(*inputs)
+
+            # Performance note: profiling indicates that -- for simple
+            # functions at least -- this wrapping can almost double the
+            # execution time.
+            # Hence we make it optional.
+            if self.cache:
+                _cache = [outputs]
+
+                def _func(*vargs):
+                    if _cache:
+                        return _cache.pop()
+                    else:
+                        return func(*vargs)
+            else:
+                _func = func
+
+            if isinstance(outputs, tuple):
+                nout = len(outputs)
+            else:
+                nout = 1
+                outputs = (outputs,)
+
+            otypes = ''.join([asarray(outputs[_k]).dtype.char
+                              for _k in range(nout)])
+
+            # Performance note: profiling indicates that creating the ufunc is
+            # not a significant cost compared with wrapping so it seems not
+            # worth trying to cache this.
+            ufunc = frompyfunc(_func, len(args), nout)
+
+        return ufunc, otypes
+
+    def _vectorize_call(self, func, args):
+        """Vectorized call to `func` over positional `args`."""
+        if self.signature is not None:
+            res = self._vectorize_call_with_signature(func, args)
+        elif not args:
+            res = func()
+        else:
+            ufunc, otypes = self._get_ufunc_and_otypes(func=func, args=args)
+
+            # Convert args to object arrays first
+            inputs = [asanyarray(a, dtype=object) for a in args]
+
+            outputs = ufunc(*inputs)
+
+            if ufunc.nout == 1:
+                res = asanyarray(outputs, dtype=otypes[0])
+            else:
+                res = tuple([asanyarray(x, dtype=t)
+                             for x, t in zip(outputs, otypes)])
+        return res
+
+    def _vectorize_call_with_signature(self, func, args):
+        """Vectorized call over positional arguments with a signature."""
+        input_core_dims, output_core_dims = self._in_and_out_core_dims
+
+        if len(args) != len(input_core_dims):
+            raise TypeError('wrong number of positional arguments: '
+                            'expected %r, got %r'
+                            % (len(input_core_dims), len(args)))
+        args = tuple(asanyarray(arg) for arg in args)
+
+        broadcast_shape, dim_sizes = _parse_input_dimensions(
+            args, input_core_dims)
+        input_shapes = _calculate_shapes(broadcast_shape, dim_sizes,
+                                         input_core_dims)
+        args = [np.broadcast_to(arg, shape, subok=True)
+                for arg, shape in zip(args, input_shapes)]
+
+        outputs = None
+        otypes = self.otypes
+        nout = len(output_core_dims)
+
+        for index in np.ndindex(*broadcast_shape):
+            results = func(*(arg[index] for arg in args))
+
+            n_results = len(results) if isinstance(results, tuple) else 1
+
+            if nout != n_results:
+                raise ValueError(
+                    'wrong number of outputs from pyfunc: expected %r, got %r'
+                    % (nout, n_results))
+
+            if nout == 1:
+                results = (results,)
+
+            if outputs is None:
+                for result, core_dims in zip(results, output_core_dims):
+                    _update_dim_sizes(dim_sizes, result, core_dims)
+
+                outputs = _create_arrays(broadcast_shape, dim_sizes,
+                                         output_core_dims, otypes, results)
+
+            for output, result in zip(outputs, results):
+                output[index] = result
+
+        if outputs is None:
+            # did not call the function even once
+            if otypes is None:
+                raise ValueError('cannot call `vectorize` on size 0 inputs '
+                                 'unless `otypes` is set')
+            if builtins.any(dim not in dim_sizes
+                            for dims in output_core_dims
+                            for dim in dims):
+                raise ValueError('cannot call `vectorize` with a signature '
+                                 'including new output dimensions on size 0 '
+                                 'inputs')
+            outputs = _create_arrays(broadcast_shape, dim_sizes,
+                                     output_core_dims, otypes)
+
+        return outputs[0] if nout == 1 else outputs
+
+
+def _cov_dispatcher(m, y=None, rowvar=None, bias=None, ddof=None,
+                    fweights=None, aweights=None, *, dtype=None):
+    return (m, y, fweights, aweights)
+
+
+@array_function_dispatch(_cov_dispatcher)
+def cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None,
+        aweights=None, *, dtype=None):
+    """
+    Estimate a covariance matrix, given data and weights.
+
+    Covariance indicates the level to which two variables vary together.
+    If we examine N-dimensional samples, :math:`X = [x_1, x_2, ... x_N]^T`,
+    then the covariance matrix element :math:`C_{ij}` is the covariance of
+    :math:`x_i` and :math:`x_j`. The element :math:`C_{ii}` is the variance
+    of :math:`x_i`.
+
+    See the notes for an outline of the algorithm.
+
+    Parameters
+    ----------
+    m : array_like
+        A 1-D or 2-D array containing multiple variables and observations.
+        Each row of `m` represents a variable, and each column a single
+        observation of all those variables. Also see `rowvar` below.
+    y : array_like, optional
+        An additional set of variables and observations. `y` has the same form
+        as that of `m`.
+    rowvar : bool, optional
+        If `rowvar` is True (default), then each row represents a
+        variable, with observations in the columns. Otherwise, the relationship
+        is transposed: each column represents a variable, while the rows
+        contain observations.
+    bias : bool, optional
+        Default normalization (False) is by ``(N - 1)``, where ``N`` is the
+        number of observations given (unbiased estimate). If `bias` is True,
+        then normalization is by ``N``. These values can be overridden by using
+        the keyword ``ddof`` in numpy versions >= 1.5.
+    ddof : int, optional
+        If not ``None`` the default value implied by `bias` is overridden.
+        Note that ``ddof=1`` will return the unbiased estimate, even if both
+        `fweights` and `aweights` are specified, and ``ddof=0`` will return
+        the simple average. See the notes for the details. The default value
+        is ``None``.
+
+        .. versionadded:: 1.5
+    fweights : array_like, int, optional
+        1-D array of integer frequency weights; the number of times each
+        observation vector should be repeated.
+
+        .. versionadded:: 1.10
+    aweights : array_like, optional
+        1-D array of observation vector weights. These relative weights are
+        typically large for observations considered "important" and smaller for
+        observations considered less "important". If ``ddof=0`` the array of
+        weights can be used to assign probabilities to observation vectors.
+
+        .. versionadded:: 1.10
+    dtype : data-type, optional
+        Data-type of the result. By default, the return data-type will have
+        at least `numpy.float64` precision.
+
+        .. versionadded:: 1.20
+
+    Returns
+    -------
+    out : ndarray
+        The covariance matrix of the variables.
+
+    See Also
+    --------
+    corrcoef : Normalized covariance matrix
+
+    Notes
+    -----
+    Assume that the observations are in the columns of the observation
+    array `m` and let ``f = fweights`` and ``a = aweights`` for brevity. The
+    steps to compute the weighted covariance are as follows::
+
+        >>> m = np.arange(10, dtype=np.float64)
+        >>> f = np.arange(10) * 2
+        >>> a = np.arange(10) ** 2.
+        >>> ddof = 1
+        >>> w = f * a
+        >>> v1 = np.sum(w)
+        >>> v2 = np.sum(w * a)
+        >>> m -= np.sum(m * w, axis=None, keepdims=True) / v1
+        >>> cov = np.dot(m * w, m.T) * v1 / (v1**2 - ddof * v2)
+
+    Note that when ``a == 1``, the normalization factor
+    ``v1 / (v1**2 - ddof * v2)`` goes over to ``1 / (np.sum(f) - ddof)``
+    as it should.
+
+    Examples
+    --------
+    Consider two variables, :math:`x_0` and :math:`x_1`, which
+    correlate perfectly, but in opposite directions:
+
+    >>> x = np.array([[0, 2], [1, 1], [2, 0]]).T
+    >>> x
+    array([[0, 1, 2],
+           [2, 1, 0]])
+
+    Note how :math:`x_0` increases while :math:`x_1` decreases. The covariance
+    matrix shows this clearly:
+
+    >>> np.cov(x)
+    array([[ 1., -1.],
+           [-1.,  1.]])
+
+    Note that element :math:`C_{0,1}`, which shows the correlation between
+    :math:`x_0` and :math:`x_1`, is negative.
+
+    Further, note how `x` and `y` are combined:
+
+    >>> x = [-2.1, -1,  4.3]
+    >>> y = [3,  1.1,  0.12]
+    >>> X = np.stack((x, y), axis=0)
+    >>> np.cov(X)
+    array([[11.71      , -4.286     ], # may vary
+           [-4.286     ,  2.144133]])
+    >>> np.cov(x, y)
+    array([[11.71      , -4.286     ], # may vary
+           [-4.286     ,  2.144133]])
+    >>> np.cov(x)
+    array(11.71)
+
+    """
+    # Check inputs
+    if ddof is not None and ddof != int(ddof):
+        raise ValueError(
+            "ddof must be integer")
+
+    # Handles complex arrays too
+    m = np.asarray(m)
+    if m.ndim > 2:
+        raise ValueError("m has more than 2 dimensions")
+
+    if y is not None:
+        y = np.asarray(y)
+        if y.ndim > 2:
+            raise ValueError("y has more than 2 dimensions")
+
+    if dtype is None:
+        if y is None:
+            dtype = np.result_type(m, np.float64)
+        else:
+            dtype = np.result_type(m, y, np.float64)
+
+    X = array(m, ndmin=2, dtype=dtype)
+    if not rowvar and X.shape[0] != 1:
+        X = X.T
+    if X.shape[0] == 0:
+        return np.array([]).reshape(0, 0)
+    if y is not None:
+        y = array(y, copy=False, ndmin=2, dtype=dtype)
+        if not rowvar and y.shape[0] != 1:
+            y = y.T
+        X = np.concatenate((X, y), axis=0)
+
+    if ddof is None:
+        if bias == 0:
+            ddof = 1
+        else:
+            ddof = 0
+
+    # Get the product of frequencies and weights
+    w = None
+    if fweights is not None:
+        fweights = np.asarray(fweights, dtype=float)
+        if not np.all(fweights == np.around(fweights)):
+            raise TypeError(
+                "fweights must be integer")
+        if fweights.ndim > 1:
+            raise RuntimeError(
+                "cannot handle multidimensional fweights")
+        if fweights.shape[0] != X.shape[1]:
+            raise RuntimeError(
+                "incompatible numbers of samples and fweights")
+        if any(fweights < 0):
+            raise ValueError(
+                "fweights cannot be negative")
+        w = fweights
+    if aweights is not None:
+        aweights = np.asarray(aweights, dtype=float)
+        if aweights.ndim > 1:
+            raise RuntimeError(
+                "cannot handle multidimensional aweights")
+        if aweights.shape[0] != X.shape[1]:
+            raise RuntimeError(
+                "incompatible numbers of samples and aweights")
+        if any(aweights < 0):
+            raise ValueError(
+                "aweights cannot be negative")
+        if w is None:
+            w = aweights
+        else:
+            w *= aweights
+
+    avg, w_sum = average(X, axis=1, weights=w, returned=True)
+    w_sum = w_sum[0]
+
+    # Determine the normalization
+    if w is None:
+        fact = X.shape[1] - ddof
+    elif ddof == 0:
+        fact = w_sum
+    elif aweights is None:
+        fact = w_sum - ddof
     else:
-        axis = 1
-        tup = (newaxis, slice(None))
-
-        
-    if y is not None:
-        y = array(y,copy=False,ndmin=2)
-        X = concatenate((X,y),axis)
-
-    X -= X.mean(axis=1-axis)[tup]
-    if rowvar:
-        N = X.shape[1]
+        fact = w_sum - ddof*sum(w*aweights)/w_sum
+
+    if fact <= 0:
+        warnings.warn("Degrees of freedom <= 0 for slice",
+                      RuntimeWarning, stacklevel=3)
+        fact = 0.0
+
+    X -= avg[:, None]
+    if w is None:
+        X_T = X.T
     else:
-        N = X.shape[0]
-
-    if bias:
-        fact = N*1.0
-    else:
-        fact = N-1.0
-
-    if not rowvar:
-        return (dot(X.transpose(), X.conj()) / fact).squeeze()
-    else:
-        return (dot(X,X.transpose().conj())/fact).squeeze()
-    
-def corrcoef(x, y=None, rowvar=1, bias=0):
-    """The correlation coefficients
-    """
-    c = cov(x, y, rowvar, bias)
+        X_T = (X*w).T
+    c = dot(X, X_T.conj())
+    c *= np.true_divide(1, fact)
+    return c.squeeze()
+
+
+def _corrcoef_dispatcher(x, y=None, rowvar=None, bias=None, ddof=None, *,
+                         dtype=None):
+    return (x, y)
+
+
+@array_function_dispatch(_corrcoef_dispatcher)
+def corrcoef(x, y=None, rowvar=True, bias=np._NoValue, ddof=np._NoValue, *,
+             dtype=None):
+    """
+    Return Pearson product-moment correlation coefficients.
+
+    Please refer to the documentation for `cov` for more detail.  The
+    relationship between the correlation coefficient matrix, `R`, and the
+    covariance matrix, `C`, is
+
+    .. math:: R_{ij} = \\frac{ C_{ij} } { \\sqrt{ C_{ii} C_{jj} } }
+
+    The values of `R` are between -1 and 1, inclusive.
+
+    Parameters
+    ----------
+    x : array_like
+        A 1-D or 2-D array containing multiple variables and observations.
+        Each row of `x` represents a variable, and each column a single
+        observation of all those variables. Also see `rowvar` below.
+    y : array_like, optional
+        An additional set of variables and observations. `y` has the same
+        shape as `x`.
+    rowvar : bool, optional
+        If `rowvar` is True (default), then each row represents a
+        variable, with observations in the columns. Otherwise, the relationship
+        is transposed: each column represents a variable, while the rows
+        contain observations.
+    bias : _NoValue, optional
+        Has no effect, do not use.
+
+        .. deprecated:: 1.10.0
+    ddof : _NoValue, optional
+        Has no effect, do not use.
+
+        .. deprecated:: 1.10.0
+    dtype : data-type, optional
+        Data-type of the result. By default, the return data-type will have
+        at least `numpy.float64` precision.
+
+        .. versionadded:: 1.20
+
+    Returns
+    -------
+    R : ndarray
+        The correlation coefficient matrix of the variables.
+
+    See Also
+    --------
+    cov : Covariance matrix
+
+    Notes
+    -----
+    Due to floating point rounding the resulting array may not be Hermitian,
+    the diagonal elements may not be 1, and the elements may not satisfy the
+    inequality abs(a) <= 1. The real and imaginary parts are clipped to the
+    interval [-1,  1] in an attempt to improve on that situation but is not
+    much help in the complex case.
+
+    This function accepts but discards arguments `bias` and `ddof`.  This is
+    for backwards compatibility with previous versions of this function.  These
+    arguments had no effect on the return values of the function and can be
+    safely ignored in this and previous versions of numpy.
+
+    Examples
+    --------
+    In this example we generate two random arrays, ``xarr`` and ``yarr``, and
+    compute the row-wise and column-wise Pearson correlation coefficients,
+    ``R``. Since ``rowvar`` is  true by  default, we first find the row-wise
+    Pearson correlation coefficients between the variables of ``xarr``.
+
+    >>> import numpy as np
+    >>> rng = np.random.default_rng(seed=42)
+    >>> xarr = rng.random((3, 3))
+    >>> xarr
+    array([[0.77395605, 0.43887844, 0.85859792],
+           [0.69736803, 0.09417735, 0.97562235],
+           [0.7611397 , 0.78606431, 0.12811363]])
+    >>> R1 = np.corrcoef(xarr)
+    >>> R1
+    array([[ 1.        ,  0.99256089, -0.68080986],
+           [ 0.99256089,  1.        , -0.76492172],
+           [-0.68080986, -0.76492172,  1.        ]])
+
+    If we add another set of variables and observations ``yarr``, we can
+    compute the row-wise Pearson correlation coefficients between the
+    variables in ``xarr`` and ``yarr``.
+
+    >>> yarr = rng.random((3, 3))
+    >>> yarr
+    array([[0.45038594, 0.37079802, 0.92676499],
+           [0.64386512, 0.82276161, 0.4434142 ],
+           [0.22723872, 0.55458479, 0.06381726]])
+    >>> R2 = np.corrcoef(xarr, yarr)
+    >>> R2
+    array([[ 1.        ,  0.99256089, -0.68080986,  0.75008178, -0.934284  ,
+            -0.99004057],
+           [ 0.99256089,  1.        , -0.76492172,  0.82502011, -0.97074098,
+            -0.99981569],
+           [-0.68080986, -0.76492172,  1.        , -0.99507202,  0.89721355,
+             0.77714685],
+           [ 0.75008178,  0.82502011, -0.99507202,  1.        , -0.93657855,
+            -0.83571711],
+           [-0.934284  , -0.97074098,  0.89721355, -0.93657855,  1.        ,
+             0.97517215],
+           [-0.99004057, -0.99981569,  0.77714685, -0.83571711,  0.97517215,
+             1.        ]])
+
+    Finally if we use the option ``rowvar=False``, the columns are now
+    being treated as the variables and we will find the column-wise Pearson
+    correlation coefficients between variables in ``xarr`` and ``yarr``.
+
+    >>> R3 = np.corrcoef(xarr, yarr, rowvar=False)
+    >>> R3
+    array([[ 1.        ,  0.77598074, -0.47458546, -0.75078643, -0.9665554 ,
+             0.22423734],
+           [ 0.77598074,  1.        , -0.92346708, -0.99923895, -0.58826587,
+            -0.44069024],
+           [-0.47458546, -0.92346708,  1.        ,  0.93773029,  0.23297648,
+             0.75137473],
+           [-0.75078643, -0.99923895,  0.93773029,  1.        ,  0.55627469,
+             0.47536961],
+           [-0.9665554 , -0.58826587,  0.23297648,  0.55627469,  1.        ,
+            -0.46666491],
+           [ 0.22423734, -0.44069024,  0.75137473,  0.47536961, -0.46666491,
+             1.        ]])
+
+    """
+    if bias is not np._NoValue or ddof is not np._NoValue:
+        # 2015-03-15, 1.10
+        warnings.warn('bias and ddof have no effect and are deprecated',
+                      DeprecationWarning, stacklevel=3)
+    c = cov(x, y, rowvar, dtype=dtype)
     try:
         d = diag(c)
-    except ValueError: # scalar covariance
-        return 1
-    return c/sqrt(multiply.outer(d,d))
-
+    except ValueError:
+        # scalar covariance
+        # nan if incorrect value (nan, inf, 0), 1 otherwise
+        return c / c
+    stddev = sqrt(d.real)
+    c /= stddev[:, None]
+    c /= stddev[None, :]
+
+    # Clip real and imaginary parts to [-1, 1].  This does not guarantee
+    # abs(a[i,j]) <= 1 for complex arrays, but is the best we can do without
+    # excessive work.
+    np.clip(c.real, -1, 1, out=c.real)
+    if np.iscomplexobj(c):
+        np.clip(c.imag, -1, 1, out=c.imag)
+
+    return c
+
+
+@set_module('numpy')
 def blackman(M):
-    """blackman(M) returns the M-point Blackman window.
-    """
-    n = arange(0,M)
-    return 0.42-0.5*cos(2.0*pi*n/(M-1)) + 0.08*cos(4.0*pi*n/(M-1))
-
+    """
+    Return the Blackman window.
+
+    The Blackman window is a taper formed by using the first three
+    terms of a summation of cosines. It was designed to have close to the
+    minimal leakage possible.  It is close to optimal, only slightly worse
+    than a Kaiser window.
+
+    Parameters
+    ----------
+    M : int
+        Number of points in the output window. If zero or less, an empty
+        array is returned.
+
+    Returns
+    -------
+    out : ndarray
+        The window, with the maximum value normalized to one (the value one
+        appears only if the number of samples is odd).
+
+    See Also
+    --------
+    bartlett, hamming, hanning, kaiser
+
+    Notes
+    -----
+    The Blackman window is defined as
+
+    .. math::  w(n) = 0.42 - 0.5 \\cos(2\\pi n/M) + 0.08 \\cos(4\\pi n/M)
+
+    Most references to the Blackman window come from the signal processing
+    literature, where it is used as one of many windowing functions for
+    smoothing values.  It is also known as an apodization (which means
+    "removing the foot", i.e. smoothing discontinuities at the beginning
+    and end of the sampled signal) or tapering function. It is known as a
+    "near optimal" tapering function, almost as good (by some measures)
+    as the kaiser window.
+
+    References
+    ----------
+    Blackman, R.B. and Tukey, J.W., (1958) The measurement of power spectra,
+    Dover Publications, New York.
+
+    Oppenheim, A.V., and R.W. Schafer. Discrete-Time Signal Processing.
+    Upper Saddle River, NJ: Prentice-Hall, 1999, pp. 468-471.
+
+    Examples
+    --------
+    >>> import matplotlib.pyplot as plt
+    >>> np.blackman(12)
+    array([-1.38777878e-17,   3.26064346e-02,   1.59903635e-01, # may vary
+            4.14397981e-01,   7.36045180e-01,   9.67046769e-01,
+            9.67046769e-01,   7.36045180e-01,   4.14397981e-01,
+            1.59903635e-01,   3.26064346e-02,  -1.38777878e-17])
+
+    Plot the window and the frequency response:
+
+    >>> from numpy.fft import fft, fftshift
+    >>> window = np.blackman(51)
+    >>> plt.plot(window)
+    [<matplotlib.lines.Line2D object at 0x...>]
+    >>> plt.title("Blackman window")
+    Text(0.5, 1.0, 'Blackman window')
+    >>> plt.ylabel("Amplitude")
+    Text(0, 0.5, 'Amplitude')
+    >>> plt.xlabel("Sample")
+    Text(0.5, 0, 'Sample')
+    >>> plt.show()
+
+    >>> plt.figure()
+    <Figure size 640x480 with 0 Axes>
+    >>> A = fft(window, 2048) / 25.5
+    >>> mag = np.abs(fftshift(A))
+    >>> freq = np.linspace(-0.5, 0.5, len(A))
+    >>> with np.errstate(divide='ignore', invalid='ignore'):
+    ...     response = 20 * np.log10(mag)
+    ...
+    >>> response = np.clip(response, -100, 100)
+    >>> plt.plot(freq, response)
+    [<matplotlib.lines.Line2D object at 0x...>]
+    >>> plt.title("Frequency response of Blackman window")
+    Text(0.5, 1.0, 'Frequency response of Blackman window')
+    >>> plt.ylabel("Magnitude [dB]")
+    Text(0, 0.5, 'Magnitude [dB]')
+    >>> plt.xlabel("Normalized frequency [cycles per sample]")
+    Text(0.5, 0, 'Normalized frequency [cycles per sample]')
+    >>> _ = plt.axis('tight')
+    >>> plt.show()
+
+    """
+    if M < 1:
+        return array([], dtype=np.result_type(M, 0.0))
+    if M == 1:
+        return ones(1, dtype=np.result_type(M, 0.0))
+    n = arange(1-M, M, 2)
+    return 0.42 + 0.5*cos(pi*n/(M-1)) + 0.08*cos(2.0*pi*n/(M-1))
+
+
+@set_module('numpy')
 def bartlett(M):
-    """bartlett(M) returns the M-point Bartlett window.
-    """
-    n = arange(0,M)
-    return where(less_equal(n,(M-1)/2.0),2.0*n/(M-1),2.0-2.0*n/(M-1))
-
+    """
+    Return the Bartlett window.
+
+    The Bartlett window is very similar to a triangular window, except
+    that the end points are at zero.  It is often used in signal
+    processing for tapering a signal, without generating too much
+    ripple in the frequency domain.
+
+    Parameters
+    ----------
+    M : int
+        Number of points in the output window. If zero or less, an
+        empty array is returned.
+
+    Returns
+    -------
+    out : array
+        The triangular window, with the maximum value normalized to one
+        (the value one appears only if the number of samples is odd), with
+        the first and last samples equal to zero.
+
+    See Also
+    --------
+    blackman, hamming, hanning, kaiser
+
+    Notes
+    -----
+    The Bartlett window is defined as
+
+    .. math:: w(n) = \\frac{2}{M-1} \\left(
+              \\frac{M-1}{2} - \\left|n - \\frac{M-1}{2}\\right|
+              \\right)
+
+    Most references to the Bartlett window come from the signal processing
+    literature, where it is used as one of many windowing functions for
+    smoothing values.  Note that convolution with this window produces linear
+    interpolation.  It is also known as an apodization (which means "removing
+    the foot", i.e. smoothing discontinuities at the beginning and end of the
+    sampled signal) or tapering function. The Fourier transform of the
+    Bartlett window is the product of two sinc functions. Note the excellent
+    discussion in Kanasewich [2]_.
+
+    References
+    ----------
+    .. [1] M.S. Bartlett, "Periodogram Analysis and Continuous Spectra",
+           Biometrika 37, 1-16, 1950.
+    .. [2] E.R. Kanasewich, "Time Sequence Analysis in Geophysics",
+           The University of Alberta Press, 1975, pp. 109-110.
+    .. [3] A.V. Oppenheim and R.W. Schafer, "Discrete-Time Signal
+           Processing", Prentice-Hall, 1999, pp. 468-471.
+    .. [4] Wikipedia, "Window function",
+           https://en.wikipedia.org/wiki/Window_function
+    .. [5] W.H. Press,  B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling,
+           "Numerical Recipes", Cambridge University Press, 1986, page 429.
+
+    Examples
+    --------
+    >>> import matplotlib.pyplot as plt
+    >>> np.bartlett(12)
+    array([ 0.        ,  0.18181818,  0.36363636,  0.54545455,  0.72727273, # may vary
+            0.90909091,  0.90909091,  0.72727273,  0.54545455,  0.36363636,
+            0.18181818,  0.        ])
+
+    Plot the window and its frequency response (requires SciPy and matplotlib):
+
+    >>> from numpy.fft import fft, fftshift
+    >>> window = np.bartlett(51)
+    >>> plt.plot(window)
+    [<matplotlib.lines.Line2D object at 0x...>]
+    >>> plt.title("Bartlett window")
+    Text(0.5, 1.0, 'Bartlett window')
+    >>> plt.ylabel("Amplitude")
+    Text(0, 0.5, 'Amplitude')
+    >>> plt.xlabel("Sample")
+    Text(0.5, 0, 'Sample')
+    >>> plt.show()
+
+    >>> plt.figure()
+    <Figure size 640x480 with 0 Axes>
+    >>> A = fft(window, 2048) / 25.5
+    >>> mag = np.abs(fftshift(A))
+    >>> freq = np.linspace(-0.5, 0.5, len(A))
+    >>> with np.errstate(divide='ignore', invalid='ignore'):
+    ...     response = 20 * np.log10(mag)
+    ...
+    >>> response = np.clip(response, -100, 100)
+    >>> plt.plot(freq, response)
+    [<matplotlib.lines.Line2D object at 0x...>]
+    >>> plt.title("Frequency response of Bartlett window")
+    Text(0.5, 1.0, 'Frequency response of Bartlett window')
+    >>> plt.ylabel("Magnitude [dB]")
+    Text(0, 0.5, 'Magnitude [dB]')
+    >>> plt.xlabel("Normalized frequency [cycles per sample]")
+    Text(0.5, 0, 'Normalized frequency [cycles per sample]')
+    >>> _ = plt.axis('tight')
+    >>> plt.show()
+
+    """
+    if M < 1:
+        return array([], dtype=np.result_type(M, 0.0))
+    if M == 1:
+        return ones(1, dtype=np.result_type(M, 0.0))
+    n = arange(1-M, M, 2)
+    return where(less_equal(n, 0), 1 + n/(M-1), 1 - n/(M-1))
+
+
+@set_module('numpy')
 def hanning(M):
-    """hanning(M) returns the M-point Hanning window.
-    """
-    n = arange(0,M)
-    return 0.5-0.5*cos(2.0*pi*n/(M-1))
-
+    """
+    Return the Hanning window.
+
+    The Hanning window is a taper formed by using a weighted cosine.
+
+    Parameters
+    ----------
+    M : int
+        Number of points in the output window. If zero or less, an
+        empty array is returned.
+
+    Returns
+    -------
+    out : ndarray, shape(M,)
+        The window, with the maximum value normalized to one (the value
+        one appears only if `M` is odd).
+
+    See Also
+    --------
+    bartlett, blackman, hamming, kaiser
+
+    Notes
+    -----
+    The Hanning window is defined as
+
+    .. math::  w(n) = 0.5 - 0.5\\cos\\left(\\frac{2\\pi{n}}{M-1}\\right)
+               \\qquad 0 \\leq n \\leq M-1
+
+    The Hanning was named for Julius von Hann, an Austrian meteorologist.
+    It is also known as the Cosine Bell. Some authors prefer that it be
+    called a Hann window, to help avoid confusion with the very similar
+    Hamming window.
+
+    Most references to the Hanning window come from the signal processing
+    literature, where it is used as one of many windowing functions for
+    smoothing values.  It is also known as an apodization (which means
+    "removing the foot", i.e. smoothing discontinuities at the beginning
+    and end of the sampled signal) or tapering function.
+
+    References
+    ----------
+    .. [1] Blackman, R.B. and Tukey, J.W., (1958) The measurement of power
+           spectra, Dover Publications, New York.
+    .. [2] E.R. Kanasewich, "Time Sequence Analysis in Geophysics",
+           The University of Alberta Press, 1975, pp. 106-108.
+    .. [3] Wikipedia, "Window function",
+           https://en.wikipedia.org/wiki/Window_function
+    .. [4] W.H. Press,  B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling,
+           "Numerical Recipes", Cambridge University Press, 1986, page 425.
+
+    Examples
+    --------
+    >>> np.hanning(12)
+    array([0.        , 0.07937323, 0.29229249, 0.57115742, 0.82743037,
+           0.97974649, 0.97974649, 0.82743037, 0.57115742, 0.29229249,
+           0.07937323, 0.        ])
+
+    Plot the window and its frequency response:
+
+    >>> import matplotlib.pyplot as plt
+    >>> from numpy.fft import fft, fftshift
+    >>> window = np.hanning(51)
+    >>> plt.plot(window)
+    [<matplotlib.lines.Line2D object at 0x...>]
+    >>> plt.title("Hann window")
+    Text(0.5, 1.0, 'Hann window')
+    >>> plt.ylabel("Amplitude")
+    Text(0, 0.5, 'Amplitude')
+    >>> plt.xlabel("Sample")
+    Text(0.5, 0, 'Sample')
+    >>> plt.show()
+
+    >>> plt.figure()
+    <Figure size 640x480 with 0 Axes>
+    >>> A = fft(window, 2048) / 25.5
+    >>> mag = np.abs(fftshift(A))
+    >>> freq = np.linspace(-0.5, 0.5, len(A))
+    >>> with np.errstate(divide='ignore', invalid='ignore'):
+    ...     response = 20 * np.log10(mag)
+    ...
+    >>> response = np.clip(response, -100, 100)
+    >>> plt.plot(freq, response)
+    [<matplotlib.lines.Line2D object at 0x...>]
+    >>> plt.title("Frequency response of the Hann window")
+    Text(0.5, 1.0, 'Frequency response of the Hann window')
+    >>> plt.ylabel("Magnitude [dB]")
+    Text(0, 0.5, 'Magnitude [dB]')
+    >>> plt.xlabel("Normalized frequency [cycles per sample]")
+    Text(0.5, 0, 'Normalized frequency [cycles per sample]')
+    >>> plt.axis('tight')
+    ...
+    >>> plt.show()
+
+    """
+    if M < 1:
+        return array([], dtype=np.result_type(M, 0.0))
+    if M == 1:
+        return ones(1, dtype=np.result_type(M, 0.0))
+    n = arange(1-M, M, 2)
+    return 0.5 + 0.5*cos(pi*n/(M-1))
+
+
+@set_module('numpy')
 def hamming(M):
-    """hamming(M) returns the M-point Hamming window.
-    """
-    n = arange(0,M)
-    return 0.54-0.46*cos(2.0*pi*n/(M-1))
+    """
+    Return the Hamming window.
+
+    The Hamming window is a taper formed by using a weighted cosine.
+
+    Parameters
+    ----------
+    M : int
+        Number of points in the output window. If zero or less, an
+        empty array is returned.
+
+    Returns
+    -------
+    out : ndarray
+        The window, with the maximum value normalized to one (the value
+        one appears only if the number of samples is odd).
+
+    See Also
+    --------
+    bartlett, blackman, hanning, kaiser
+
+    Notes
+    -----
+    The Hamming window is defined as
+
+    .. math::  w(n) = 0.54 - 0.46\\cos\\left(\\frac{2\\pi{n}}{M-1}\\right)
+               \\qquad 0 \\leq n \\leq M-1
+
+    The Hamming was named for R. W. Hamming, an associate of J. W. Tukey
+    and is described in Blackman and Tukey. It was recommended for
+    smoothing the truncated autocovariance function in the time domain.
+    Most references to the Hamming window come from the signal processing
+    literature, where it is used as one of many windowing functions for
+    smoothing values.  It is also known as an apodization (which means
+    "removing the foot", i.e. smoothing discontinuities at the beginning
+    and end of the sampled signal) or tapering function.
+
+    References
+    ----------
+    .. [1] Blackman, R.B. and Tukey, J.W., (1958) The measurement of power
+           spectra, Dover Publications, New York.
+    .. [2] E.R. Kanasewich, "Time Sequence Analysis in Geophysics", The
+           University of Alberta Press, 1975, pp. 109-110.
+    .. [3] Wikipedia, "Window function",
+           https://en.wikipedia.org/wiki/Window_function
+    .. [4] W.H. Press,  B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling,
+           "Numerical Recipes", Cambridge University Press, 1986, page 425.
+
+    Examples
+    --------
+    >>> np.hamming(12)
+    array([ 0.08      ,  0.15302337,  0.34890909,  0.60546483,  0.84123594, # may vary
+            0.98136677,  0.98136677,  0.84123594,  0.60546483,  0.34890909,
+            0.15302337,  0.08      ])
+
+    Plot the window and the frequency response:
+
+    >>> import matplotlib.pyplot as plt
+    >>> from numpy.fft import fft, fftshift
+    >>> window = np.hamming(51)
+    >>> plt.plot(window)
+    [<matplotlib.lines.Line2D object at 0x...>]
+    >>> plt.title("Hamming window")
+    Text(0.5, 1.0, 'Hamming window')
+    >>> plt.ylabel("Amplitude")
+    Text(0, 0.5, 'Amplitude')
+    >>> plt.xlabel("Sample")
+    Text(0.5, 0, 'Sample')
+    >>> plt.show()
+
+    >>> plt.figure()
+    <Figure size 640x480 with 0 Axes>
+    >>> A = fft(window, 2048) / 25.5
+    >>> mag = np.abs(fftshift(A))
+    >>> freq = np.linspace(-0.5, 0.5, len(A))
+    >>> response = 20 * np.log10(mag)
+    >>> response = np.clip(response, -100, 100)
+    >>> plt.plot(freq, response)
+    [<matplotlib.lines.Line2D object at 0x...>]
+    >>> plt.title("Frequency response of Hamming window")
+    Text(0.5, 1.0, 'Frequency response of Hamming window')
+    >>> plt.ylabel("Magnitude [dB]")
+    Text(0, 0.5, 'Magnitude [dB]')
+    >>> plt.xlabel("Normalized frequency [cycles per sample]")
+    Text(0.5, 0, 'Normalized frequency [cycles per sample]')
+    >>> plt.axis('tight')
+    ...
+    >>> plt.show()
+
+    """
+    if M < 1:
+        return array([], dtype=np.result_type(M, 0.0))
+    if M == 1:
+        return ones(1, dtype=np.result_type(M, 0.0))
+    n = arange(1-M, M, 2)
+    return 0.54 + 0.46*cos(pi*n/(M-1))
+
 
 ## Code from cephes for i0
 
 _i0A = [
--4.41534164647933937950E-18,
- 3.33079451882223809783E-17,
--2.43127984654795469359E-16,
- 1.71539128555513303061E-15,
--1.16853328779934516808E-14,
- 7.67618549860493561688E-14,
--4.85644678311192946090E-13,
- 2.95505266312963983461E-12,
--1.72682629144155570723E-11,
- 9.67580903537323691224E-11,
--5.18979560163526290666E-10,
- 2.65982372468238665035E-9,
--1.30002500998624804212E-8,
- 6.04699502254191894932E-8,
--2.67079385394061173391E-7,
- 1.11738753912010371815E-6,
--4.41673835845875056359E-6,
- 1.64484480707288970893E-5,
--5.75419501008210370398E-5,
- 1.88502885095841655729E-4,
--5.76375574538582365885E-4,
- 1.63947561694133579842E-3,
--4.32430999505057594430E-3,
- 1.05464603945949983183E-2,
--2.37374148058994688156E-2,
- 4.93052842396707084878E-2,
--9.49010970480476444210E-2,
- 1.71620901522208775349E-1,
--3.04682672343198398683E-1,
- 6.76795274409476084995E-1]
+    -4.41534164647933937950E-18,
+    3.33079451882223809783E-17,
+    -2.43127984654795469359E-16,
+    1.71539128555513303061E-15,
+    -1.16853328779934516808E-14,
+    7.67618549860493561688E-14,
+    -4.85644678311192946090E-13,
+    2.95505266312963983461E-12,
+    -1.72682629144155570723E-11,
+    9.67580903537323691224E-11,
+    -5.18979560163526290666E-10,
+    2.65982372468238665035E-9,
+    -1.30002500998624804212E-8,
+    6.04699502254191894932E-8,
+    -2.67079385394061173391E-7,
+    1.11738753912010371815E-6,
+    -4.41673835845875056359E-6,
+    1.64484480707288970893E-5,
+    -5.75419501008210370398E-5,
+    1.88502885095841655729E-4,
+    -5.76375574538582365885E-4,
+    1.63947561694133579842E-3,
+    -4.32430999505057594430E-3,
+    1.05464603945949983183E-2,
+    -2.37374148058994688156E-2,
+    4.93052842396707084878E-2,
+    -9.49010970480476444210E-2,
+    1.71620901522208775349E-1,
+    -3.04682672343198398683E-1,
+    6.76795274409476084995E-1
+    ]
 
 _i0B = [
--7.23318048787475395456E-18,
--4.83050448594418207126E-18,
- 4.46562142029675999901E-17,
- 3.46122286769746109310E-17,
--2.82762398051658348494E-16,
--3.42548561967721913462E-16,
- 1.77256013305652638360E-15,
- 3.81168066935262242075E-15,
--9.55484669882830764870E-15,
--4.15056934728722208663E-14,
- 1.54008621752140982691E-14,
- 3.85277838274214270114E-13,
- 7.18012445138366623367E-13,
--1.79417853150680611778E-12,
--1.32158118404477131188E-11,
--3.14991652796324136454E-11,
- 1.18891471078464383424E-11,
- 4.94060238822496958910E-10,
- 3.39623202570838634515E-9,
- 2.26666899049817806459E-8,
- 2.04891858946906374183E-7,
- 2.89137052083475648297E-6,
- 6.88975834691682398426E-5,
- 3.36911647825569408990E-3,
- 8.04490411014108831608E-1]
+    -7.23318048787475395456E-18,
+    -4.83050448594418207126E-18,
+    4.46562142029675999901E-17,
+    3.46122286769746109310E-17,
+    -2.82762398051658348494E-16,
+    -3.42548561967721913462E-16,
+    1.77256013305652638360E-15,
+    3.81168066935262242075E-15,
+    -9.55484669882830764870E-15,
+    -4.15056934728722208663E-14,
+    1.54008621752140982691E-14,
+    3.85277838274214270114E-13,
+    7.18012445138366623367E-13,
+    -1.79417853150680611778E-12,
+    -1.32158118404477131188E-11,
+    -3.14991652796324136454E-11,
+    1.18891471078464383424E-11,
+    4.94060238822496958910E-10,
+    3.39623202570838634515E-9,
+    2.26666899049817806459E-8,
+    2.04891858946906374183E-7,
+    2.89137052083475648297E-6,
+    6.88975834691682398426E-5,
+    3.36911647825569408990E-3,
+    8.04490411014108831608E-1
+    ]
+
 
 def _chbevl(x, vals):
     b0 = vals[0]
     b1 = 0.0
 
-    for i in xrange(1,len(vals)):
+    for i in range(1, len(vals)):
         b2 = b1
         b1 = b0
         b0 = x*b1 - b2 + vals[i]
 
     return 0.5*(b0 - b2)
 
+
 def _i0_1(x):
     return exp(x) * _chbevl(x/2.0-2, _i0A)
 
+
 def _i0_2(x):
     return exp(x) * _chbevl(32.0/x - 2.0, _i0B) / sqrt(x)
 
+
+def _i0_dispatcher(x):
+    return (x,)
+
+
+@array_function_dispatch(_i0_dispatcher)
 def i0(x):
-    x = atleast_1d(x).copy()
-    y = empty_like(x)
-    ind = (x<0)
-    x[ind] = -x[ind]
-    ind = (x<=8.0)
-    y[ind] = _i0_1(x[ind])
-    ind2 = ~ind
-    y[ind2] = _i0_2(x[ind2])
-    return y.squeeze()
+    """
+    Modified Bessel function of the first kind, order 0.
+
+    Usually denoted :math:`I_0`.
+
+    Parameters
+    ----------
+    x : array_like of float
+        Argument of the Bessel function.
+
+    Returns
+    -------
+    out : ndarray, shape = x.shape, dtype = float
+        The modified Bessel function evaluated at each of the elements of `x`.
+
+    See Also
+    --------
+    scipy.special.i0, scipy.special.iv, scipy.special.ive
+
+    Notes
+    -----
+    The scipy implementation is recommended over this function: it is a
+    proper ufunc written in C, and more than an order of magnitude faster.
+
+    We use the algorithm published by Clenshaw [1]_ and referenced by
+    Abramowitz and Stegun [2]_, for which the function domain is
+    partitioned into the two intervals [0,8] and (8,inf), and Chebyshev
+    polynomial expansions are employed in each interval. Relative error on
+    the domain [0,30] using IEEE arithmetic is documented [3]_ as having a
+    peak of 5.8e-16 with an rms of 1.4e-16 (n = 30000).
+
+    References
+    ----------
+    .. [1] C. W. Clenshaw, "Chebyshev series for mathematical functions", in
+           *National Physical Laboratory Mathematical Tables*, vol. 5, London:
+           Her Majesty's Stationery Office, 1962.
+    .. [2] M. Abramowitz and I. A. Stegun, *Handbook of Mathematical
+           Functions*, 10th printing, New York: Dover, 1964, pp. 379.
+           https://personal.math.ubc.ca/~cbm/aands/page_379.htm
+    .. [3] https://metacpan.org/pod/distribution/Math-Cephes/lib/Math/Cephes.pod#i0:-Modified-Bessel-function-of-order-zero
+
+    Examples
+    --------
+    >>> np.i0(0.)
+    array(1.0)
+    >>> np.i0([0, 1, 2, 3])
+    array([1.        , 1.26606588, 2.2795853 , 4.88079259])
+
+    """
+    x = np.asanyarray(x)
+    if x.dtype.kind == 'c':
+        raise TypeError("i0 not supported for complex values")
+    if x.dtype.kind != 'f':
+        x = x.astype(float)
+    x = np.abs(x)
+    return piecewise(x, [x <= 8.0], [_i0_1, _i0_2])
 
 ## End of cephes code for i0
 
-def kaiser(M,beta):
-    """kaiser(M, beta) returns a Kaiser window of length M with shape parameter
-    beta. It depends on numpy.special (in full numpy) for the modified bessel
-    function i0.
-    """
-    from numpy.dual import i0
-    n = arange(0,M)
+
+@set_module('numpy')
+def kaiser(M, beta):
+    """
+    Return the Kaiser window.
+
+    The Kaiser window is a taper formed by using a Bessel function.
+
+    Parameters
+    ----------
+    M : int
+        Number of points in the output window. If zero or less, an
+        empty array is returned.
+    beta : float
+        Shape parameter for window.
+
+    Returns
+    -------
+    out : array
+        The window, with the maximum value normalized to one (the value
+        one appears only if the number of samples is odd).
+
+    See Also
+    --------
+    bartlett, blackman, hamming, hanning
+
+    Notes
+    -----
+    The Kaiser window is defined as
+
+    .. math::  w(n) = I_0\\left( \\beta \\sqrt{1-\\frac{4n^2}{(M-1)^2}}
+               \\right)/I_0(\\beta)
+
+    with
+
+    .. math:: \\quad -\\frac{M-1}{2} \\leq n \\leq \\frac{M-1}{2},
+
+    where :math:`I_0` is the modified zeroth-order Bessel function.
+
+    The Kaiser was named for Jim Kaiser, who discovered a simple
+    approximation to the DPSS window based on Bessel functions.  The Kaiser
+    window is a very good approximation to the Digital Prolate Spheroidal
+    Sequence, or Slepian window, which is the transform which maximizes the
+    energy in the main lobe of the window relative to total energy.
+
+    The Kaiser can approximate many other windows by varying the beta
+    parameter.
+
+    ====  =======================
+    beta  Window shape
+    ====  =======================
+    0     Rectangular
+    5     Similar to a Hamming
+    6     Similar to a Hanning
+    8.6   Similar to a Blackman
+    ====  =======================
+
+    A beta value of 14 is probably a good starting point. Note that as beta
+    gets large, the window narrows, and so the number of samples needs to be
+    large enough to sample the increasingly narrow spike, otherwise NaNs will
+    get returned.
+
+    Most references to the Kaiser window come from the signal processing
+    literature, where it is used as one of many windowing functions for
+    smoothing values.  It is also known as an apodization (which means
+    "removing the foot", i.e. smoothing discontinuities at the beginning
+    and end of the sampled signal) or tapering function.
+
+    References
+    ----------
+    .. [1] J. F. Kaiser, "Digital Filters" - Ch 7 in "Systems analysis by
+           digital computer", Editors: F.F. Kuo and J.F. Kaiser, p 218-285.
+           John Wiley and Sons, New York, (1966).
+    .. [2] E.R. Kanasewich, "Time Sequence Analysis in Geophysics", The
+           University of Alberta Press, 1975, pp. 177-178.
+    .. [3] Wikipedia, "Window function",
+           https://en.wikipedia.org/wiki/Window_function
+
+    Examples
+    --------
+    >>> import matplotlib.pyplot as plt
+    >>> np.kaiser(12, 14)
+     array([7.72686684e-06, 3.46009194e-03, 4.65200189e-02, # may vary
+            2.29737120e-01, 5.99885316e-01, 9.45674898e-01,
+            9.45674898e-01, 5.99885316e-01, 2.29737120e-01,
+            4.65200189e-02, 3.46009194e-03, 7.72686684e-06])
+
+
+    Plot the window and the frequency response:
+
+    >>> from numpy.fft import fft, fftshift
+    >>> window = np.kaiser(51, 14)
+    >>> plt.plot(window)
+    [<matplotlib.lines.Line2D object at 0x...>]
+    >>> plt.title("Kaiser window")
+    Text(0.5, 1.0, 'Kaiser window')
+    >>> plt.ylabel("Amplitude")
+    Text(0, 0.5, 'Amplitude')
+    >>> plt.xlabel("Sample")
+    Text(0.5, 0, 'Sample')
+    >>> plt.show()
+
+    >>> plt.figure()
+    <Figure size 640x480 with 0 Axes>
+    >>> A = fft(window, 2048) / 25.5
+    >>> mag = np.abs(fftshift(A))
+    >>> freq = np.linspace(-0.5, 0.5, len(A))
+    >>> response = 20 * np.log10(mag)
+    >>> response = np.clip(response, -100, 100)
+    >>> plt.plot(freq, response)
+    [<matplotlib.lines.Line2D object at 0x...>]
+    >>> plt.title("Frequency response of Kaiser window")
+    Text(0.5, 1.0, 'Frequency response of Kaiser window')
+    >>> plt.ylabel("Magnitude [dB]")
+    Text(0, 0.5, 'Magnitude [dB]')
+    >>> plt.xlabel("Normalized frequency [cycles per sample]")
+    Text(0.5, 0, 'Normalized frequency [cycles per sample]')
+    >>> plt.axis('tight')
+    (-0.5, 0.5, -100.0, ...) # may vary
+    >>> plt.show()
+
+    """
+    if M == 1:
+        return np.ones(1, dtype=np.result_type(M, 0.0))
+    n = arange(0, M)
     alpha = (M-1)/2.0
-    return i0(beta * sqrt(1-((n-alpha)/alpha)**2.0))/i0(beta)
-
+    return i0(beta * sqrt(1-((n-alpha)/alpha)**2.0))/i0(float(beta))
+
+
+def _sinc_dispatcher(x):
+    return (x,)
+
+
+@array_function_dispatch(_sinc_dispatcher)
 def sinc(x):
-    """sinc(x) returns sin(pi*x)/(pi*x) at all points of array x.
-    """
-    y = pi* where(x == 0, 1.0e-20, x)
+    r"""
+    Return the normalized sinc function.
+
+    The sinc function is equal to :math:`\sin(\pi x)/(\pi x)` for any argument
+    :math:`x\ne 0`. ``sinc(0)`` takes the limit value 1, making ``sinc`` not
+    only everywhere continuous but also infinitely differentiable.
+
+    .. note::
+
+        Note the normalization factor of ``pi`` used in the definition.
+        This is the most commonly used definition in signal processing.
+        Use ``sinc(x / np.pi)`` to obtain the unnormalized sinc function
+        :math:`\sin(x)/x` that is more common in mathematics.
+
+    Parameters
+    ----------
+    x : ndarray
+        Array (possibly multi-dimensional) of values for which to calculate
+        ``sinc(x)``.
+
+    Returns
+    -------
+    out : ndarray
+        ``sinc(x)``, which has the same shape as the input.
+
+    Notes
+    -----
+    The name sinc is short for "sine cardinal" or "sinus cardinalis".
+
+    The sinc function is used in various signal processing applications,
+    including in anti-aliasing, in the construction of a Lanczos resampling
+    filter, and in interpolation.
+
+    For bandlimited interpolation of discrete-time signals, the ideal
+    interpolation kernel is proportional to the sinc function.
+
+    References
+    ----------
+    .. [1] Weisstein, Eric W. "Sinc Function." From MathWorld--A Wolfram Web
+           Resource. http://mathworld.wolfram.com/SincFunction.html
+    .. [2] Wikipedia, "Sinc function",
+           https://en.wikipedia.org/wiki/Sinc_function
+
+    Examples
+    --------
+    >>> import matplotlib.pyplot as plt
+    >>> x = np.linspace(-4, 4, 41)
+    >>> np.sinc(x)
+     array([-3.89804309e-17,  -4.92362781e-02,  -8.40918587e-02, # may vary
+            -8.90384387e-02,  -5.84680802e-02,   3.89804309e-17,
+            6.68206631e-02,   1.16434881e-01,   1.26137788e-01,
+            8.50444803e-02,  -3.89804309e-17,  -1.03943254e-01,
+            -1.89206682e-01,  -2.16236208e-01,  -1.55914881e-01,
+            3.89804309e-17,   2.33872321e-01,   5.04551152e-01,
+            7.56826729e-01,   9.35489284e-01,   1.00000000e+00,
+            9.35489284e-01,   7.56826729e-01,   5.04551152e-01,
+            2.33872321e-01,   3.89804309e-17,  -1.55914881e-01,
+           -2.16236208e-01,  -1.89206682e-01,  -1.03943254e-01,
+           -3.89804309e-17,   8.50444803e-02,   1.26137788e-01,
+            1.16434881e-01,   6.68206631e-02,   3.89804309e-17,
+            -5.84680802e-02,  -8.90384387e-02,  -8.40918587e-02,
+            -4.92362781e-02,  -3.89804309e-17])
+
+    >>> plt.plot(x, np.sinc(x))
+    [<matplotlib.lines.Line2D object at 0x...>]
+    >>> plt.title("Sinc Function")
+    Text(0.5, 1.0, 'Sinc Function')
+    >>> plt.ylabel("Amplitude")
+    Text(0, 0.5, 'Amplitude')
+    >>> plt.xlabel("X")
+    Text(0.5, 0, 'X')
+    >>> plt.show()
+
+    """
+    x = np.asanyarray(x)
+    y = pi * where(x == 0, 1.0e-20, x)
     return sin(y)/y
 
+
+def _msort_dispatcher(a):
+    return (a,)
+
+
+@array_function_dispatch(_msort_dispatcher)
 def msort(a):
-    b = array(a,copy=True)
+    """
+    Return a copy of an array sorted along the first axis.
+
+    Parameters
+    ----------
+    a : array_like
+        Array to be sorted.
+
+    Returns
+    -------
+    sorted_array : ndarray
+        Array of the same type and shape as `a`.
+
+    See Also
+    --------
+    sort
+
+    Notes
+    -----
+    ``np.msort(a)`` is equivalent to  ``np.sort(a, axis=0)``.
+
+    """
+    b = array(a, subok=True, copy=True)
     b.sort(0)
     return b
 
-def median(m):
-    """median(m) returns a median of m along the first dimension of m.
-    """
-    sorted = msort(m)
-    if sorted.shape[0] % 2 == 1:
-        return sorted[int(sorted.shape[0]/2)]
+
+def _ureduce(a, func, **kwargs):
+    """
+    Internal Function.
+    Call `func` with `a` as first argument swapping the axes to use extended
+    axis on functions that don't support it natively.
+
+    Returns result and a.shape with axis dims set to 1.
+
+    Parameters
+    ----------
+    a : array_like
+        Input array or object that can be converted to an array.
+    func : callable
+        Reduction function capable of receiving a single axis argument.
+        It is called with `a` as first argument followed by `kwargs`.
+    kwargs : keyword arguments
+        additional keyword arguments to pass to `func`.
+
+    Returns
+    -------
+    result : tuple
+        Result of func(a, **kwargs) and a.shape with axis dims set to 1
+        which can be used to reshape the result to the same shape a ufunc with
+        keepdims=True would produce.
+
+    """
+    a = np.asanyarray(a)
+    axis = kwargs.get('axis', None)
+    if axis is not None:
+        keepdim = list(a.shape)
+        nd = a.ndim
+        axis = _nx.normalize_axis_tuple(axis, nd)
+
+        for ax in axis:
+            keepdim[ax] = 1
+
+        if len(axis) == 1:
+            kwargs['axis'] = axis[0]
+        else:
+            keep = set(range(nd)) - set(axis)
+            nkeep = len(keep)
+            # swap axis that should not be reduced to front
+            for i, s in enumerate(sorted(keep)):
+                a = a.swapaxes(i, s)
+            # merge reduced axis
+            a = a.reshape(a.shape[:nkeep] + (-1,))
+            kwargs['axis'] = -1
+        keepdim = tuple(keepdim)
     else:
-        sorted = msort(m)
-        index = sorted.shape[0]/2
-        return (sorted[index-1]+sorted[index])/2.0
-
+        keepdim = (1,) * a.ndim
+
+    r = func(a, **kwargs)
+    return r, keepdim
+
+
+def _median_dispatcher(
+        a, axis=None, out=None, overwrite_input=None, keepdims=None):
+    return (a, out)
+
+
+@array_function_dispatch(_median_dispatcher)
+def median(a, axis=None, out=None, overwrite_input=False, keepdims=False):
+    """
+    Compute the median along the specified axis.
+
+    Returns the median of the array elements.
+
+    Parameters
+    ----------
+    a : array_like
+        Input array or object that can be converted to an array.
+    axis : {int, sequence of int, None}, optional
+        Axis or axes along which the medians are computed. The default
+        is to compute the median along a flattened version of the array.
+        A sequence of axes is supported since version 1.9.0.
+    out : ndarray, optional
+        Alternative output array in which to place the result. It must
+        have the same shape and buffer length as the expected output,
+        but the type (of the output) will be cast if necessary.
+    overwrite_input : bool, optional
+       If True, then allow use of memory of input array `a` for
+       calculations. The input array will be modified by the call to
+       `median`. This will save memory when you do not need to preserve
+       the contents of the input array. Treat the input as undefined,
+       but it will probably be fully or partially sorted. Default is
+       False. If `overwrite_input` is ``True`` and `a` is not already an
+       `ndarray`, an error will be raised.
+    keepdims : bool, optional
+        If this is set to True, the axes which are reduced are left
+        in the result as dimensions with size one. With this option,
+        the result will broadcast correctly against the original `arr`.
+
+        .. versionadded:: 1.9.0
+
+    Returns
+    -------
+    median : ndarray
+        A new array holding the result. If the input contains integers
+        or floats smaller than ``float64``, then the output data-type is
+        ``np.float64``.  Otherwise, the data-type of the output is the
+        same as that of the input. If `out` is specified, that array is
+        returned instead.
+
+    See Also
+    --------
+    mean, percentile
+
+    Notes
+    -----
+    Given a vector ``V`` of length ``N``, the median of ``V`` is the
+    middle value of a sorted copy of ``V``, ``V_sorted`` - i
+    e., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the
+    two middle values of ``V_sorted`` when ``N`` is even.
+
+    Examples
+    --------
+    >>> a = np.array([[10, 7, 4], [3, 2, 1]])
+    >>> a
+    array([[10,  7,  4],
+           [ 3,  2,  1]])
+    >>> np.median(a)
+    3.5
+    >>> np.median(a, axis=0)
+    array([6.5, 4.5, 2.5])
+    >>> np.median(a, axis=1)
+    array([7.,  2.])
+    >>> m = np.median(a, axis=0)
+    >>> out = np.zeros_like(m)
+    >>> np.median(a, axis=0, out=m)
+    array([6.5,  4.5,  2.5])
+    >>> m
+    array([6.5,  4.5,  2.5])
+    >>> b = a.copy()
+    >>> np.median(b, axis=1, overwrite_input=True)
+    array([7.,  2.])
+    >>> assert not np.all(a==b)
+    >>> b = a.copy()
+    >>> np.median(b, axis=None, overwrite_input=True)
+    3.5
+    >>> assert not np.all(a==b)
+
+    """
+    r, k = _ureduce(a, func=_median, axis=axis, out=out,
+                    overwrite_input=overwrite_input)
+    if keepdims:
+        return r.reshape(k)
+    else:
+        return r
+
+
+def _median(a, axis=None, out=None, overwrite_input=False):
+    # can't be reasonably be implemented in terms of percentile as we have to
+    # call mean to not break astropy
+    a = np.asanyarray(a)
+
+    # Set the partition indexes
+    if axis is None:
+        sz = a.size
+    else:
+        sz = a.shape[axis]
+    if sz % 2 == 0:
+        szh = sz // 2
+        kth = [szh - 1, szh]
+    else:
+        kth = [(sz - 1) // 2]
+    # Check if the array contains any nan's
+    if np.issubdtype(a.dtype, np.inexact):
+        kth.append(-1)
+
+    if overwrite_input:
+        if axis is None:
+            part = a.ravel()
+            part.partition(kth)
+        else:
+            a.partition(kth, axis=axis)
+            part = a
+    else:
+        part = partition(a, kth, axis=axis)
+
+    if part.shape == ():
+        # make 0-D arrays work
+        return part.item()
+    if axis is None:
+        axis = 0
+
+    indexer = [slice(None)] * part.ndim
+    index = part.shape[axis] // 2
+    if part.shape[axis] % 2 == 1:
+        # index with slice to allow mean (below) to work
+        indexer[axis] = slice(index, index+1)
+    else:
+        indexer[axis] = slice(index-1, index+1)
+    indexer = tuple(indexer)
+
+    # Use mean in both odd and even case to coerce data type,
+    # using out array if needed.
+    rout = mean(part[indexer], axis=axis, out=out)
+    # Check if the array contains any nan's
+    if np.issubdtype(a.dtype, np.inexact) and sz > 0:
+        # If nans are possible, warn and replace by nans like mean would.
+        rout = np.lib.utils._median_nancheck(part, rout, axis)
+
+    return rout
+
+
+def _percentile_dispatcher(a, q, axis=None, out=None, overwrite_input=None,
+                           method=None, keepdims=None, *, interpolation=None):
+    return (a, q, out)
+
+
+@array_function_dispatch(_percentile_dispatcher)
+def percentile(a,
+               q,
+               axis=None,
+               out=None,
+               overwrite_input=False,
+               method="linear",
+               keepdims=False,
+               *,
+               interpolation=None):
+    """
+    Compute the q-th percentile of the data along the specified axis.
+
+    Returns the q-th percentile(s) of the array elements.
+
+    Parameters
+    ----------
+    a : array_like
+        Input array or object that can be converted to an array.
+    q : array_like of float
+        Percentile or sequence of percentiles to compute, which must be between
+        0 and 100 inclusive.
+    axis : {int, tuple of int, None}, optional
+        Axis or axes along which the percentiles are computed. The
+        default is to compute the percentile(s) along a flattened
+        version of the array.
+
+        .. versionchanged:: 1.9.0
+            A tuple of axes is supported
+    out : ndarray, optional
+        Alternative output array in which to place the result. It must
+        have the same shape and buffer length as the expected output,
+        but the type (of the output) will be cast if necessary.
+    overwrite_input : bool, optional
+        If True, then allow the input array `a` to be modified by intermediate
+        calculations, to save memory. In this case, the contents of the input
+        `a` after this function completes is undefined.
+    method : str, optional
+        This parameter specifies the method to use for estimating the
+        percentile.  There are many different methods, some unique to NumPy.
+        See the notes for explanation.  The options sorted by their R type
+        as summarized in the H&F paper [1]_ are:
+
+        1. 'inverted_cdf'
+        2. 'averaged_inverted_cdf'
+        3. 'closest_observation'
+        4. 'interpolated_inverted_cdf'
+        5. 'hazen'
+        6. 'weibull'
+        7. 'linear'  (default)
+        8. 'median_unbiased'
+        9. 'normal_unbiased'
+
+        The first three methods are discontiuous.  NumPy further defines the
+        following discontinuous variations of the default 'linear' (7.) option:
+
+        * 'lower'
+        * 'higher',
+        * 'midpoint'
+        * 'nearest'
+
+        .. versionchanged:: 1.22.0
+            This argument was previously called "interpolation" and only
+            offered the "linear" default and last four options.
+
+    keepdims : bool, optional
+        If this is set to True, the axes which are reduced are left in
+        the result as dimensions with size one. With this option, the
+        result will broadcast correctly against the original array `a`.
+
+        .. versionadded:: 1.9.0
+
+    interpolation : str, optional
+        Deprecated name for the method keyword argument.
+
+        .. deprecated:: 1.22.0
+
+    Returns
+    -------
+    percentile : scalar or ndarray
+        If `q` is a single percentile and `axis=None`, then the result
+        is a scalar. If multiple percentiles are given, first axis of
+        the result corresponds to the percentiles. The other axes are
+        the axes that remain after the reduction of `a`. If the input
+        contains integers or floats smaller than ``float64``, the output
+        data-type is ``float64``. Otherwise, the output data-type is the
+        same as that of the input. If `out` is specified, that array is
+        returned instead.
+
+    See Also
+    --------
+    mean
+    median : equivalent to ``percentile(..., 50)``
+    nanpercentile
+    quantile : equivalent to percentile, except q in the range [0, 1].
+
+    Notes
+    -----
+    Given a vector ``V`` of length ``N``, the q-th percentile of ``V`` is
+    the value ``q/100`` of the way from the minimum to the maximum in a
+    sorted copy of ``V``. The values and distances of the two nearest
+    neighbors as well as the `method` parameter will determine the
+    percentile if the normalized ranking does not match the location of
+    ``q`` exactly. This function is the same as the median if ``q=50``, the
+    same as the minimum if ``q=0`` and the same as the maximum if
+    ``q=100``.
+
+    This optional `method` parameter specifies the method to use when the
+    desired quantile lies between two data points ``i < j``.
+    If ``g`` is the fractional part of the index surrounded by ``i`` and
+    alpha and beta are correction constants modifying i and j.
+
+    Below, 'q' is the quantile value, 'n' is the sample size and
+    alpha and beta are constants.
+    The following formula gives an interpolation "i + g" of where the quantile
+    would be in the sorted sample.
+    With 'i' being the floor and 'g' the fractional part of the result.
+
+    .. math::
+        i + g = (q - alpha) / ( n - alpha - beta + 1 )
+
+    The different methods then work as follows
+
+    inverted_cdf:
+        method 1 of H&F [1]_.
+        This method gives discontinuous results:
+
+        * if g > 0 ; then take j
+        * if g = 0 ; then take i
+
+    averaged_inverted_cdf:
+        method 2 of H&F [1]_.
+        This method give discontinuous results:
+
+        * if g > 0 ; then take j
+        * if g = 0 ; then average between bounds
+
+    closest_observation:
+        method 3 of H&F [1]_.
+        This method give discontinuous results:
+
+        * if g > 0 ; then take j
+        * if g = 0 and index is odd ; then take j
+        * if g = 0 and index is even ; then take i
+
+    interpolated_inverted_cdf:
+        method 4 of H&F [1]_.
+        This method give continuous results using:
+
+        * alpha = 0
+        * beta = 1
+
+    hazen:
+        method 5 of H&F [1]_.
+        This method give continuous results using:
+
+        * alpha = 1/2
+        * beta = 1/2
+
+    weibull:
+        method 6 of H&F [1]_.
+        This method give continuous results using:
+
+        * alpha = 0
+        * beta = 0
+
+    linear:
+        method 7 of H&F [1]_.
+        This method give continuous results using:
+
+        * alpha = 1
+        * beta = 1
+
+    median_unbiased:
+        method 8 of H&F [1]_.
+        This method is probably the best method if the sample
+        distribution function is unknown (see reference).
+        This method give continuous results using:
+
+        * alpha = 1/3
+        * beta = 1/3
+
+    normal_unbiased:
+        method 9 of H&F [1]_.
+        This method is probably the best method if the sample
+        distribution function is known to be normal.
+        This method give continuous results using:
+
+        * alpha = 3/8
+        * beta = 3/8
+
+    lower:
+        NumPy method kept for backwards compatibility.
+        Takes ``i`` as the interpolation point.
+
+    higher:
+        NumPy method kept for backwards compatibility.
+        Takes ``j`` as the interpolation point.
+
+    nearest:
+        NumPy method kept for backwards compatibility.
+        Takes ``i`` or ``j``, whichever is nearest.
+
+    midpoint:
+        NumPy method kept for backwards compatibility.
+        Uses ``(i + j) / 2``.
+
+    Examples
+    --------
+    >>> a = np.array([[10, 7, 4], [3, 2, 1]])
+    >>> a
+    array([[10,  7,  4],
+           [ 3,  2,  1]])
+    >>> np.percentile(a, 50)
+    3.5
+    >>> np.percentile(a, 50, axis=0)
+    array([6.5, 4.5, 2.5])
+    >>> np.percentile(a, 50, axis=1)
+    array([7.,  2.])
+    >>> np.percentile(a, 50, axis=1, keepdims=True)
+    array([[7.],
+           [2.]])
+
+    >>> m = np.percentile(a, 50, axis=0)
+    >>> out = np.zeros_like(m)
+    >>> np.percentile(a, 50, axis=0, out=out)
+    array([6.5, 4.5, 2.5])
+    >>> m
+    array([6.5, 4.5, 2.5])
+
+    >>> b = a.copy()
+    >>> np.percentile(b, 50, axis=1, overwrite_input=True)
+    array([7.,  2.])
+    >>> assert not np.all(a == b)
+
+    The different methods can be visualized graphically:
+
+    .. plot::
+
+        import matplotlib.pyplot as plt
+
+        a = np.arange(4)
+        p = np.linspace(0, 100, 6001)
+        ax = plt.gca()
+        lines = [
+            ('linear', '-', 'C0'),
+            ('inverted_cdf', ':', 'C1'),
+            # Almost the same as `inverted_cdf`:
+            ('averaged_inverted_cdf', '-.', 'C1'),
+            ('closest_observation', ':', 'C2'),
+            ('interpolated_inverted_cdf', '--', 'C1'),
+            ('hazen', '--', 'C3'),
+            ('weibull', '-.', 'C4'),
+            ('median_unbiased', '--', 'C5'),
+            ('normal_unbiased', '-.', 'C6'),
+            ]
+        for method, style, color in lines:
+            ax.plot(
+                p, np.percentile(a, p, method=method),
+                label=method, linestyle=style, color=color)
+        ax.set(
+            title='Percentiles for different methods and data: ' + str(a),
+            xlabel='Percentile',
+            ylabel='Estimated percentile value',
+            yticks=a)
+        ax.legend()
+        plt.show()
+
+    References
+    ----------
+    .. [1] R. J. Hyndman and Y. Fan,
+       "Sample quantiles in statistical packages,"
+       The American Statistician, 50(4), pp. 361-365, 1996
+
+    """
+    if interpolation is not None:
+        method = _check_interpolation_as_method(
+            method, interpolation, "percentile")
+    q = np.true_divide(q, 100)
+    q = asanyarray(q)  # undo any decay that the ufunc performed (see gh-13105)
+    if not _quantile_is_valid(q):
+        raise ValueError("Percentiles must be in the range [0, 100]")
+    return _quantile_unchecked(
+        a, q, axis, out, overwrite_input, method, keepdims)
+
+
+def _quantile_dispatcher(a, q, axis=None, out=None, overwrite_input=None,
+                         method=None, keepdims=None, *, interpolation=None):
+    return (a, q, out)
+
+
+@array_function_dispatch(_quantile_dispatcher)
+def quantile(a,
+             q,
+             axis=None,
+             out=None,
+             overwrite_input=False,
+             method="linear",
+             keepdims=False,
+             *,
+             interpolation=None):
+    """
+    Compute the q-th quantile of the data along the specified axis.
+
+    .. versionadded:: 1.15.0
+
+    Parameters
+    ----------
+    a : array_like
+        Input array or object that can be converted to an array.
+    q : array_like of float
+        Quantile or sequence of quantiles to compute, which must be between
+        0 and 1 inclusive.
+    axis : {int, tuple of int, None}, optional
+        Axis or axes along which the quantiles are computed. The default is
+        to compute the quantile(s) along a flattened version of the array.
+    out : ndarray, optional
+        Alternative output array in which to place the result. It must have
+        the same shape and buffer length as the expected output, but the
+        type (of the output) will be cast if necessary.
+    overwrite_input : bool, optional
+        If True, then allow the input array `a` to be modified by
+        intermediate calculations, to save memory. In this case, the
+        contents of the input `a` after this function completes is
+        undefined.
+    method : str, optional
+        This parameter specifies the method to use for estimating the
+        quantile.  There are many different methods, some unique to NumPy.
+        See the notes for explanation.  The options sorted by their R type
+        as summarized in the H&F paper [1]_ are:
+
+        1. 'inverted_cdf'
+        2. 'averaged_inverted_cdf'
+        3. 'closest_observation'
+        4. 'interpolated_inverted_cdf'
+        5. 'hazen'
+        6. 'weibull'
+        7. 'linear'  (default)
+        8. 'median_unbiased'
+        9. 'normal_unbiased'
+
+        The first three methods are discontinuous.  NumPy further defines the
+        following discontinuous variations of the default 'linear' (7.) option:
+
+        * 'lower'
+        * 'higher',
+        * 'midpoint'
+        * 'nearest'
+
+        .. versionchanged:: 1.22.0
+            This argument was previously called "interpolation" and only
+            offered the "linear" default and last four options.
+
+    keepdims : bool, optional
+        If this is set to True, the axes which are reduced are left in
+        the result as dimensions with size one. With this option, the
+        result will broadcast correctly against the original array `a`.
+
+    interpolation : str, optional
+        Deprecated name for the method keyword argument.
+
+        .. deprecated:: 1.22.0
+
+    Returns
+    -------
+    quantile : scalar or ndarray
+        If `q` is a single quantile and `axis=None`, then the result
+        is a scalar. If multiple quantiles are given, first axis of
+        the result corresponds to the quantiles. The other axes are
+        the axes that remain after the reduction of `a`. If the input
+        contains integers or floats smaller than ``float64``, the output
+        data-type is ``float64``. Otherwise, the output data-type is the
+        same as that of the input. If `out` is specified, that array is
+        returned instead.
+
+    See Also
+    --------
+    mean
+    percentile : equivalent to quantile, but with q in the range [0, 100].
+    median : equivalent to ``quantile(..., 0.5)``
+    nanquantile
+
+    Notes
+    -----
+    Given a vector ``V`` of length ``N``, the q-th quantile of ``V`` is the
+    value ``q`` of the way from the minimum to the maximum in a sorted copy of
+    ``V``. The values and distances of the two nearest neighbors as well as the
+    `method` parameter will determine the quantile if the normalized
+    ranking does not match the location of ``q`` exactly. This function is the
+    same as the median if ``q=0.5``, the same as the minimum if ``q=0.0`` and
+    the same as the maximum if ``q=1.0``.
+
+    The optional `method` parameter specifies the method to use when the
+    desired quantile lies between two data points ``i < j``.
+    If ``g`` is the fractional part of the index surrounded by ``i`` and ``j``,
+    and alpha and beta are correction constants modifying i and j:
+
+    .. math::
+        i + g = (q - alpha) / ( n - alpha - beta + 1 )
+
+    The different methods then work as follows
+
+    inverted_cdf:
+        method 1 of H&F [1]_.
+        This method gives discontinuous results:
+
+        * if g > 0 ; then take j
+        * if g = 0 ; then take i
+
+    averaged_inverted_cdf:
+        method 2 of H&F [1]_.
+        This method gives discontinuous results:
+
+        * if g > 0 ; then take j
+        * if g = 0 ; then average between bounds
+
+    closest_observation:
+        method 3 of H&F [1]_.
+        This method gives discontinuous results:
+
+        * if g > 0 ; then take j
+        * if g = 0 and index is odd ; then take j
+        * if g = 0 and index is even ; then take i
+
+    interpolated_inverted_cdf:
+        method 4 of H&F [1]_.
+        This method gives continuous results using:
+
+        * alpha = 0
+        * beta = 1
+
+    hazen:
+        method 5 of H&F [1]_.
+        This method gives continuous results using:
+
+        * alpha = 1/2
+        * beta = 1/2
+
+    weibull:
+        method 6 of H&F [1]_.
+        This method gives continuous results using:
+
+        * alpha = 0
+        * beta = 0
+
+    linear:
+        method 7 of H&F [1]_.
+        This method gives continuous results using:
+
+        * alpha = 1
+        * beta = 1
+
+    median_unbiased:
+        method 8 of H&F [1]_.
+        This method is probably the best method if the sample
+        distribution function is unknown (see reference).
+        This method gives continuous results using:
+
+        * alpha = 1/3
+        * beta = 1/3
+
+    normal_unbiased:
+        method 9 of H&F [1]_.
+        This method is probably the best method if the sample
+        distribution function is known to be normal.
+        This method gives continuous results using:
+
+        * alpha = 3/8
+        * beta = 3/8
+
+    lower:
+        NumPy method kept for backwards compatibility.
+        Takes ``i`` as the interpolation point.
+
+    higher:
+        NumPy method kept for backwards compatibility.
+        Takes ``j`` as the interpolation point.
+
+    nearest:
+        NumPy method kept for backwards compatibility.
+        Takes ``i`` or ``j``, whichever is nearest.
+
+    midpoint:
+        NumPy method kept for backwards compatibility.
+        Uses ``(i + j) / 2``.
+
+    Examples
+    --------
+    >>> a = np.array([[10, 7, 4], [3, 2, 1]])
+    >>> a
+    array([[10,  7,  4],
+           [ 3,  2,  1]])
+    >>> np.quantile(a, 0.5)
+    3.5
+    >>> np.quantile(a, 0.5, axis=0)
+    array([6.5, 4.5, 2.5])
+    >>> np.quantile(a, 0.5, axis=1)
+    array([7.,  2.])
+    >>> np.quantile(a, 0.5, axis=1, keepdims=True)
+    array([[7.],
+           [2.]])
+    >>> m = np.quantile(a, 0.5, axis=0)
+    >>> out = np.zeros_like(m)
+    >>> np.quantile(a, 0.5, axis=0, out=out)
+    array([6.5, 4.5, 2.5])
+    >>> m
+    array([6.5, 4.5, 2.5])
+    >>> b = a.copy()
+    >>> np.quantile(b, 0.5, axis=1, overwrite_input=True)
+    array([7.,  2.])
+    >>> assert not np.all(a == b)
+
+    See also `numpy.percentile` for a visualization of most methods.
+
+    References
+    ----------
+    .. [1] R. J. Hyndman and Y. Fan,
+       "Sample quantiles in statistical packages,"
+       The American Statistician, 50(4), pp. 361-365, 1996
+
+    """
+    if interpolation is not None:
+        method = _check_interpolation_as_method(
+            method, interpolation, "quantile")
+
+    q = np.asanyarray(q)
+    if not _quantile_is_valid(q):
+        raise ValueError("Quantiles must be in the range [0, 1]")
+    return _quantile_unchecked(
+        a, q, axis, out, overwrite_input, method, keepdims)
+
+
+def _quantile_unchecked(a,
+                        q,
+                        axis=None,
+                        out=None,
+                        overwrite_input=False,
+                        method="linear",
+                        keepdims=False):
+    """Assumes that q is in [0, 1], and is an ndarray"""
+    r, k = _ureduce(a,
+                    func=_quantile_ureduce_func,
+                    q=q,
+                    axis=axis,
+                    out=out,
+                    overwrite_input=overwrite_input,
+                    method=method)
+    if keepdims:
+        return r.reshape(q.shape + k)
+    else:
+        return r
+
+
+def _quantile_is_valid(q):
+    # avoid expensive reductions, relevant for arrays with < O(1000) elements
+    if q.ndim == 1 and q.size < 10:
+        for i in range(q.size):
+            if not (0.0 <= q[i] <= 1.0):
+                return False
+    else:
+        if not (np.all(0 <= q) and np.all(q <= 1)):
+            return False
+    return True
+
+
+def _check_interpolation_as_method(method, interpolation, fname):
+    # Deprecated NumPy 1.22, 2021-11-08
+    warnings.warn(
+        f"the `interpolation=` argument to {fname} was renamed to "
+        "`method=`, which has additional options.\n"
+        "Users of the modes 'nearest', 'lower', 'higher', or "
+        "'midpoint' are encouraged to review the method they used. "
+        "(Deprecated NumPy 1.22)",
+        DeprecationWarning, stacklevel=4)
+    if method != "linear":
+        # sanity check, we assume this basically never happens
+        raise TypeError(
+            "You shall not pass both `method` and `interpolation`!\n"
+            "(`interpolation` is Deprecated in favor of `method`)")
+    return interpolation
+
+
+def _compute_virtual_index(n, quantiles, alpha: float, beta: float):
+    """
+    Compute the floating point indexes of an array for the linear
+    interpolation of quantiles.
+    n : array_like
+        The sample sizes.
+    quantiles : array_like
+        The quantiles values.
+    alpha : float
+        A constant used to correct the index computed.
+    beta : float
+        A constant used to correct the index computed.
+
+    alpha and beta values depend on the chosen method
+    (see quantile documentation)
+
+    Reference:
+    Hyndman&Fan paper "Sample Quantiles in Statistical Packages",
+    DOI: 10.1080/00031305.1996.10473566
+    """
+    return n * quantiles + (
+            alpha + quantiles * (1 - alpha - beta)
+    ) - 1
+
+
+def _get_gamma(virtual_indexes, previous_indexes, method):
+    """
+    Compute gamma (a.k.a 'm' or 'weight') for the linear interpolation
+    of quantiles.
+
+    virtual_indexes : array_like
+        The indexes where the percentile is supposed to be found in the sorted
+        sample.
+    previous_indexes : array_like
+        The floor values of virtual_indexes.
+    interpolation : dict
+        The interpolation method chosen, which may have a specific rule
+        modifying gamma.
+
+    gamma is usually the fractional part of virtual_indexes but can be modified
+    by the interpolation method.
+    """
+    gamma = np.asanyarray(virtual_indexes - previous_indexes)
+    gamma = method["fix_gamma"](gamma, virtual_indexes)
+    return np.asanyarray(gamma)
+
+
+def _lerp(a, b, t, out=None):
+    """
+    Compute the linear interpolation weighted by gamma on each point of
+    two same shape array.
+
+    a : array_like
+        Left bound.
+    b : array_like
+        Right bound.
+    t : array_like
+        The interpolation weight.
+    out : array_like
+        Output array.
+    """
+    diff_b_a = subtract(b, a)
+    # asanyarray is a stop-gap until gh-13105
+    lerp_interpolation = asanyarray(add(a, diff_b_a * t, out=out))
+    subtract(b, diff_b_a * (1 - t), out=lerp_interpolation, where=t >= 0.5)
+    if lerp_interpolation.ndim == 0 and out is None:
+        lerp_interpolation = lerp_interpolation[()]  # unpack 0d arrays
+    return lerp_interpolation
+
+
+def _get_gamma_mask(shape, default_value, conditioned_value, where):
+    out = np.full(shape, default_value)
+    np.copyto(out, conditioned_value, where=where, casting="unsafe")
+    return out
+
+
+def _discret_interpolation_to_boundaries(index, gamma_condition_fun):
+    previous = np.floor(index)
+    next = previous + 1
+    gamma = index - previous
+    res = _get_gamma_mask(shape=index.shape,
+                          default_value=next,
+                          conditioned_value=previous,
+                          where=gamma_condition_fun(gamma, index)
+                          ).astype(np.intp)
+    # Some methods can lead to out-of-bound integers, clip them:
+    res[res < 0] = 0
+    return res
+
+
+def _closest_observation(n, quantiles):
+    gamma_fun = lambda gamma, index: (gamma == 0) & (np.floor(index) % 2 == 0)
+    return _discret_interpolation_to_boundaries((n * quantiles) - 1 - 0.5,
+                                                gamma_fun)
+
+
+def _inverted_cdf(n, quantiles):
+    gamma_fun = lambda gamma, _: (gamma == 0)
+    return _discret_interpolation_to_boundaries((n * quantiles) - 1,
+                                                gamma_fun)
+
+
+def _quantile_ureduce_func(
+        a: np.array,
+        q: np.array,
+        axis: int = None,
+        out=None,
+        overwrite_input: bool = False,
+        method="linear",
+) -> np.array:
+    if q.ndim > 2:
+        # The code below works fine for nd, but it might not have useful
+        # semantics. For now, keep the supported dimensions the same as it was
+        # before.
+        raise ValueError("q must be a scalar or 1d")
+    if overwrite_input:
+        if axis is None:
+            axis = 0
+            arr = a.ravel()
+        else:
+            arr = a
+    else:
+        if axis is None:
+            axis = 0
+            arr = a.flatten()
+        else:
+            arr = a.copy()
+    result = _quantile(arr,
+                       quantiles=q,
+                       axis=axis,
+                       method=method,
+                       out=out)
+    return result
+
+
+def _get_indexes(arr, virtual_indexes, valid_values_count):
+    """
+    Get the valid indexes of arr neighbouring virtual_indexes.
+    Note
+    This is a companion function to linear interpolation of
+    Quantiles
+
+    Returns
+    -------
+    (previous_indexes, next_indexes): Tuple
+        A Tuple of virtual_indexes neighbouring indexes
+    """
+    previous_indexes = np.asanyarray(np.floor(virtual_indexes))
+    next_indexes = np.asanyarray(previous_indexes + 1)
+    indexes_above_bounds = virtual_indexes >= valid_values_count - 1
+    # When indexes is above max index, take the max value of the array
+    if indexes_above_bounds.any():
+        previous_indexes[indexes_above_bounds] = -1
+        next_indexes[indexes_above_bounds] = -1
+    # When indexes is below min index, take the min value of the array
+    indexes_below_bounds = virtual_indexes < 0
+    if indexes_below_bounds.any():
+        previous_indexes[indexes_below_bounds] = 0
+        next_indexes[indexes_below_bounds] = 0
+    if np.issubdtype(arr.dtype, np.inexact):
+        # After the sort, slices having NaNs will have for last element a NaN
+        virtual_indexes_nans = np.isnan(virtual_indexes)
+        if virtual_indexes_nans.any():
+            previous_indexes[virtual_indexes_nans] = -1
+            next_indexes[virtual_indexes_nans] = -1
+    previous_indexes = previous_indexes.astype(np.intp)
+    next_indexes = next_indexes.astype(np.intp)
+    return previous_indexes, next_indexes
+
+
+def _quantile(
+        arr: np.array,
+        quantiles: np.array,
+        axis: int = -1,
+        method="linear",
+        out=None,
+):
+    """
+    Private function that doesn't support extended axis or keepdims.
+    These methods are extended to this function using _ureduce
+    See nanpercentile for parameter usage
+    It computes the quantiles of the array for the given axis.
+    A linear interpolation is performed based on the `interpolation`.
+
+    By default, the method is "linear" where alpha == beta == 1 which
+    performs the 7th method of Hyndman&Fan.
+    With "median_unbiased" we get alpha == beta == 1/3
+    thus the 8th method of Hyndman&Fan.
+    """
+    # --- Setup
+    arr = np.asanyarray(arr)
+    values_count = arr.shape[axis]
+    # The dimensions of `q` are prepended to the output shape, so we need the
+    # axis being sampled from `arr` to be last.
+    DATA_AXIS = 0
+    if axis != DATA_AXIS:  # But moveaxis is slow, so only call it if axis!=0.
+        arr = np.moveaxis(arr, axis, destination=DATA_AXIS)
+    # --- Computation of indexes
+    # Index where to find the value in the sorted array.
+    # Virtual because it is a floating point value, not an valid index.
+    # The nearest neighbours are used for interpolation
+    try:
+        method = _QuantileMethods[method]
+    except KeyError:
+        raise ValueError(
+            f"{method!r} is not a valid method. Use one of: "
+            f"{_QuantileMethods.keys()}") from None
+    virtual_indexes = method["get_virtual_index"](values_count, quantiles)
+    virtual_indexes = np.asanyarray(virtual_indexes)
+    if np.issubdtype(virtual_indexes.dtype, np.integer):
+        # No interpolation needed, take the points along axis
+        if np.issubdtype(arr.dtype, np.inexact):
+            # may contain nan, which would sort to the end
+            arr.partition(concatenate((virtual_indexes.ravel(), [-1])), axis=0)
+            slices_having_nans = np.isnan(arr[-1])
+        else:
+            # cannot contain nan
+            arr.partition(virtual_indexes.ravel(), axis=0)
+            slices_having_nans = np.array(False, dtype=bool)
+        result = take(arr, virtual_indexes, axis=0, out=out)
+    else:
+        previous_indexes, next_indexes = _get_indexes(arr,
+                                                      virtual_indexes,
+                                                      values_count)
+        # --- Sorting
+        arr.partition(
+            np.unique(np.concatenate(([0, -1],
+                                      previous_indexes.ravel(),
+                                      next_indexes.ravel(),
+                                      ))),
+            axis=DATA_AXIS)
+        if np.issubdtype(arr.dtype, np.inexact):
+            slices_having_nans = np.isnan(
+                take(arr, indices=-1, axis=DATA_AXIS)
+            )
+        else:
+            slices_having_nans = None
+        # --- Get values from indexes
+        previous = np.take(arr, previous_indexes, axis=DATA_AXIS)
+        next = np.take(arr, next_indexes, axis=DATA_AXIS)
+        # --- Linear interpolation
+        gamma = _get_gamma(virtual_indexes, previous_indexes, method)
+        result_shape = virtual_indexes.shape + (1,) * (arr.ndim - 1)
+        gamma = gamma.reshape(result_shape)
+        result = _lerp(previous,
+                       next,
+                       gamma,
+                       out=out)
+    if np.any(slices_having_nans):
+        if result.ndim == 0 and out is None:
+            # can't write to a scalar
+            result = arr.dtype.type(np.nan)
+        else:
+            result[..., slices_having_nans] = np.nan
+    return result
+
+
+def _trapz_dispatcher(y, x=None, dx=None, axis=None):
+    return (y, x)
+
+
+@array_function_dispatch(_trapz_dispatcher)
 def trapz(y, x=None, dx=1.0, axis=-1):
-    """Integrate y(x) using samples along the given axis and the composite
-    trapezoidal rule.  If x is None, spacing given by dx is assumed.
-    """
-    y = asarray(y)
+    r"""
+    Integrate along the given axis using the composite trapezoidal rule.
+
+    If `x` is provided, the integration happens in sequence along its
+    elements - they are not sorted.
+
+    Integrate `y` (`x`) along each 1d slice on the given axis, compute
+    :math:`\int y(x) dx`.
+    When `x` is specified, this integrates along the parametric curve,
+    computing :math:`\int_t y(t) dt =
+    \int_t y(t) \left.\frac{dx}{dt}\right|_{x=x(t)} dt`.
+
+    Parameters
+    ----------
+    y : array_like
+        Input array to integrate.
+    x : array_like, optional
+        The sample points corresponding to the `y` values. If `x` is None,
+        the sample points are assumed to be evenly spaced `dx` apart. The
+        default is None.
+    dx : scalar, optional
+        The spacing between sample points when `x` is None. The default is 1.
+    axis : int, optional
+        The axis along which to integrate.
+
+    Returns
+    -------
+    trapz : float or ndarray
+        Definite integral of `y` = n-dimensional array as approximated along
+        a single axis by the trapezoidal rule. If `y` is a 1-dimensional array,
+        then the result is a float. If `n` is greater than 1, then the result
+        is an `n`-1 dimensional array.
+
+    See Also
+    --------
+    sum, cumsum
+
+    Notes
+    -----
+    Image [2]_ illustrates trapezoidal rule -- y-axis locations of points
+    will be taken from `y` array, by default x-axis distances between
+    points will be 1.0, alternatively they can be provided with `x` array
+    or with `dx` scalar.  Return value will be equal to combined area under
+    the red lines.
+
+
+    References
+    ----------
+    .. [1] Wikipedia page: https://en.wikipedia.org/wiki/Trapezoidal_rule
+
+    .. [2] Illustration image:
+           https://en.wikipedia.org/wiki/File:Composite_trapezoidal_rule_illustration.png
+
+    Examples
+    --------
+    >>> np.trapz([1,2,3])
+    4.0
+    >>> np.trapz([1,2,3], x=[4,6,8])
+    8.0
+    >>> np.trapz([1,2,3], dx=2)
+    8.0
+
+    Using a decreasing `x` corresponds to integrating in reverse:
+
+    >>> np.trapz([1,2,3], x=[8,6,4])
+    -8.0
+
+    More generally `x` is used to integrate along a parametric curve.
+    This finds the area of a circle, noting we repeat the sample which closes
+    the curve:
+
+    >>> theta = np.linspace(0, 2 * np.pi, num=1000, endpoint=True)
+    >>> np.trapz(np.cos(theta), x=np.sin(theta))
+    3.141571941375841
+
+    >>> a = np.arange(6).reshape(2, 3)
+    >>> a
+    array([[0, 1, 2],
+           [3, 4, 5]])
+    >>> np.trapz(a, axis=0)
+    array([1.5, 2.5, 3.5])
+    >>> np.trapz(a, axis=1)
+    array([2.,  8.])
+    """
+    y = asanyarray(y)
     if x is None:
         d = dx
     else:
-        d = diff(x,axis=axis)
-    nd = len(y.shape)
+        x = asanyarray(x)
+        if x.ndim == 1:
+            d = diff(x)
+            # reshape to correct shape
+            shape = [1]*y.ndim
+            shape[axis] = d.shape[0]
+            d = d.reshape(shape)
+        else:
+            d = diff(x, axis=axis)
+    nd = y.ndim
     slice1 = [slice(None)]*nd
     slice2 = [slice(None)]*nd
-    slice1[axis] = slice(1,None)
-    slice2[axis] = slice(None,-1)
-    return add.reduce(d * (y[slice1]+y[slice2])/2.0,axis)
-
-#always succeed
-def add_newdoc(place, obj, doc):
+    slice1[axis] = slice(1, None)
+    slice2[axis] = slice(None, -1)
     try:
-        new = {}
-        exec 'from %s import %s' % (place, obj) in new
-        if isinstance(doc, str):
-            add_docstring(new[obj], doc.strip())
-        elif isinstance(doc, tuple):
-            add_docstring(getattr(new[obj], doc[0]), doc[1].strip())
-        elif isinstance(doc, list):
-            for val in doc:
-                add_docstring(getattr(new[obj], val[0]), val[1].strip())
-    except:
-        pass
+        ret = (d * (y[tuple(slice1)] + y[tuple(slice2)]) / 2.0).sum(axis)
+    except ValueError:
+        # Operations didn't work, cast to ndarray
+        d = np.asarray(d)
+        y = np.asarray(y)
+        ret = add.reduce(d * (y[tuple(slice1)]+y[tuple(slice2)])/2.0, axis)
+    return ret
+
+
+def _meshgrid_dispatcher(*xi, copy=None, sparse=None, indexing=None):
+    return xi
+
+
+# Based on scitools meshgrid
+@array_function_dispatch(_meshgrid_dispatcher)
+def meshgrid(*xi, copy=True, sparse=False, indexing='xy'):
+    """
+    Return coordinate matrices from coordinate vectors.
+
+    Make N-D coordinate arrays for vectorized evaluations of
+    N-D scalar/vector fields over N-D grids, given
+    one-dimensional coordinate arrays x1, x2,..., xn.
+
+    .. versionchanged:: 1.9
+       1-D and 0-D cases are allowed.
+
+    Parameters
+    ----------
+    x1, x2,..., xn : array_like
+        1-D arrays representing the coordinates of a grid.
+    indexing : {'xy', 'ij'}, optional
+        Cartesian ('xy', default) or matrix ('ij') indexing of output.
+        See Notes for more details.
+
+        .. versionadded:: 1.7.0
+    sparse : bool, optional
+        If True the shape of the returned coordinate array for dimension *i*
+        is reduced from ``(N1, ..., Ni, ... Nn)`` to
+        ``(1, ..., 1, Ni, 1, ..., 1)``.  These sparse coordinate grids are
+        intended to be use with :ref:`basics.broadcasting`.  When all
+        coordinates are used in an expression, broadcasting still leads to a
+        fully-dimensonal result array.
+
+        Default is False.
+
+        .. versionadded:: 1.7.0
+    copy : bool, optional
+        If False, a view into the original arrays are returned in order to
+        conserve memory.  Default is True.  Please note that
+        ``sparse=False, copy=False`` will likely return non-contiguous
+        arrays.  Furthermore, more than one element of a broadcast array
+        may refer to a single memory location.  If you need to write to the
+        arrays, make copies first.
+
+        .. versionadded:: 1.7.0
+
+    Returns
+    -------
+    X1, X2,..., XN : ndarray
+        For vectors `x1`, `x2`,..., `xn` with lengths ``Ni=len(xi)``,
+        returns ``(N1, N2, N3,..., Nn)`` shaped arrays if indexing='ij'
+        or ``(N2, N1, N3,..., Nn)`` shaped arrays if indexing='xy'
+        with the elements of `xi` repeated to fill the matrix along
+        the first dimension for `x1`, the second for `x2` and so on.
+
+    Notes
+    -----
+    This function supports both indexing conventions through the indexing
+    keyword argument.  Giving the string 'ij' returns a meshgrid with
+    matrix indexing, while 'xy' returns a meshgrid with Cartesian indexing.
+    In the 2-D case with inputs of length M and N, the outputs are of shape
+    (N, M) for 'xy' indexing and (M, N) for 'ij' indexing.  In the 3-D case
+    with inputs of length M, N and P, outputs are of shape (N, M, P) for
+    'xy' indexing and (M, N, P) for 'ij' indexing.  The difference is
+    illustrated by the following code snippet::
+
+        xv, yv = np.meshgrid(x, y, indexing='ij')
+        for i in range(nx):
+            for j in range(ny):
+                # treat xv[i,j], yv[i,j]
+
+        xv, yv = np.meshgrid(x, y, indexing='xy')
+        for i in range(nx):
+            for j in range(ny):
+                # treat xv[j,i], yv[j,i]
+
+    In the 1-D and 0-D case, the indexing and sparse keywords have no effect.
+
+    See Also
+    --------
+    mgrid : Construct a multi-dimensional "meshgrid" using indexing notation.
+    ogrid : Construct an open multi-dimensional "meshgrid" using indexing
+            notation.
+
+    Examples
+    --------
+    >>> nx, ny = (3, 2)
+    >>> x = np.linspace(0, 1, nx)
+    >>> y = np.linspace(0, 1, ny)
+    >>> xv, yv = np.meshgrid(x, y)
+    >>> xv
+    array([[0. , 0.5, 1. ],
+           [0. , 0.5, 1. ]])
+    >>> yv
+    array([[0.,  0.,  0.],
+           [1.,  1.,  1.]])
+    >>> xv, yv = np.meshgrid(x, y, sparse=True)  # make sparse output arrays
+    >>> xv
+    array([[0. ,  0.5,  1. ]])
+    >>> yv
+    array([[0.],
+           [1.]])
+
+    `meshgrid` is very useful to evaluate functions on a grid.  If the
+    function depends on all coordinates, you can use the parameter
+    ``sparse=True`` to save memory and computation time.
+
+    >>> x = np.linspace(-5, 5, 101)
+    >>> y = np.linspace(-5, 5, 101)
+    >>> # full coordinate arrays
+    >>> xx, yy = np.meshgrid(x, y)
+    >>> zz = np.sqrt(xx**2 + yy**2)
+    >>> xx.shape, yy.shape, zz.shape
+    ((101, 101), (101, 101), (101, 101))
+    >>> # sparse coordinate arrays
+    >>> xs, ys = np.meshgrid(x, y, sparse=True)
+    >>> zs = np.sqrt(xs**2 + ys**2)
+    >>> xs.shape, ys.shape, zs.shape
+    ((1, 101), (101, 1), (101, 101))
+    >>> np.array_equal(zz, zs)
+    True
+
+    >>> import matplotlib.pyplot as plt
+    >>> h = plt.contourf(x, y, zs)
+    >>> plt.axis('scaled')
+    >>> plt.colorbar()
+    >>> plt.show()
+    """
+    ndim = len(xi)
+
+    if indexing not in ['xy', 'ij']:
+        raise ValueError(
+            "Valid values for `indexing` are 'xy' and 'ij'.")
+
+    s0 = (1,) * ndim
+    output = [np.asanyarray(x).reshape(s0[:i] + (-1,) + s0[i + 1:])
+              for i, x in enumerate(xi)]
+
+    if indexing == 'xy' and ndim > 1:
+        # switch first and second axis
+        output[0].shape = (1, -1) + s0[2:]
+        output[1].shape = (-1, 1) + s0[2:]
+
+    if not sparse:
+        # Return the full N-D matrix (not only the 1-D vector)
+        output = np.broadcast_arrays(*output, subok=True)
+
+    if copy:
+        output = [x.copy() for x in output]
+
+    return output
+
+
+def _delete_dispatcher(arr, obj, axis=None):
+    return (arr, obj)
+
+
+@array_function_dispatch(_delete_dispatcher)
+def delete(arr, obj, axis=None):
+    """
+    Return a new array with sub-arrays along an axis deleted. For a one
+    dimensional array, this returns those entries not returned by
+    `arr[obj]`.
+
+    Parameters
+    ----------
+    arr : array_like
+        Input array.
+    obj : slice, int or array of ints
+        Indicate indices of sub-arrays to remove along the specified axis.
+
+        .. versionchanged:: 1.19.0
+            Boolean indices are now treated as a mask of elements to remove,
+            rather than being cast to the integers 0 and 1.
+
+    axis : int, optional
+        The axis along which to delete the subarray defined by `obj`.
+        If `axis` is None, `obj` is applied to the flattened array.
+
+    Returns
+    -------
+    out : ndarray
+        A copy of `arr` with the elements specified by `obj` removed. Note
+        that `delete` does not occur in-place. If `axis` is None, `out` is
+        a flattened array.
+
+    See Also
+    --------
+    insert : Insert elements into an array.
+    append : Append elements at the end of an array.
+
+    Notes
+    -----
+    Often it is preferable to use a boolean mask. For example:
+
+    >>> arr = np.arange(12) + 1
+    >>> mask = np.ones(len(arr), dtype=bool)
+    >>> mask[[0,2,4]] = False
+    >>> result = arr[mask,...]
+
+    Is equivalent to ``np.delete(arr, [0,2,4], axis=0)``, but allows further
+    use of `mask`.
+
+    Examples
+    --------
+    >>> arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])
+    >>> arr
+    array([[ 1,  2,  3,  4],
+           [ 5,  6,  7,  8],
+           [ 9, 10, 11, 12]])
+    >>> np.delete(arr, 1, 0)
+    array([[ 1,  2,  3,  4],
+           [ 9, 10, 11, 12]])
+
+    >>> np.delete(arr, np.s_[::2], 1)
+    array([[ 2,  4],
+           [ 6,  8],
+           [10, 12]])
+    >>> np.delete(arr, [1,3,5], None)
+    array([ 1,  3,  5,  7,  8,  9, 10, 11, 12])
+
+    """
+    wrap = None
+    if type(arr) is not ndarray:
+        try:
+            wrap = arr.__array_wrap__
+        except AttributeError:
+            pass
+
+    arr = asarray(arr)
+    ndim = arr.ndim
+    arrorder = 'F' if arr.flags.fnc else 'C'
+    if axis is None:
+        if ndim != 1:
+            arr = arr.ravel()
+        # needed for np.matrix, which is still not 1d after being ravelled
+        ndim = arr.ndim
+        axis = ndim - 1
+    else:
+        axis = normalize_axis_index(axis, ndim)
+
+    slobj = [slice(None)]*ndim
+    N = arr.shape[axis]
+    newshape = list(arr.shape)
+
+    if isinstance(obj, slice):
+        start, stop, step = obj.indices(N)
+        xr = range(start, stop, step)
+        numtodel = len(xr)
+
+        if numtodel <= 0:
+            if wrap:
+                return wrap(arr.copy(order=arrorder))
+            else:
+                return arr.copy(order=arrorder)
+
+        # Invert if step is negative:
+        if step < 0:
+            step = -step
+            start = xr[-1]
+            stop = xr[0] + 1
+
+        newshape[axis] -= numtodel
+        new = empty(newshape, arr.dtype, arrorder)
+        # copy initial chunk
+        if start == 0:
+            pass
+        else:
+            slobj[axis] = slice(None, start)
+            new[tuple(slobj)] = arr[tuple(slobj)]
+        # copy end chunk
+        if stop == N:
+            pass
+        else:
+            slobj[axis] = slice(stop-numtodel, None)
+            slobj2 = [slice(None)]*ndim
+            slobj2[axis] = slice(stop, None)
+            new[tuple(slobj)] = arr[tuple(slobj2)]
+        # copy middle pieces
+        if step == 1:
+            pass
+        else:  # use array indexing.
+            keep = ones(stop-start, dtype=bool)
+            keep[:stop-start:step] = False
+            slobj[axis] = slice(start, stop-numtodel)
+            slobj2 = [slice(None)]*ndim
+            slobj2[axis] = slice(start, stop)
+            arr = arr[tuple(slobj2)]
+            slobj2[axis] = keep
+            new[tuple(slobj)] = arr[tuple(slobj2)]
+        if wrap:
+            return wrap(new)
+        else:
+            return new
+
+    if isinstance(obj, (int, integer)) and not isinstance(obj, bool):
+        single_value = True
+    else:
+        single_value = False
+        _obj = obj
+        obj = np.asarray(obj)
+        if obj.size == 0 and not isinstance(_obj, np.ndarray):
+            obj = obj.astype(intp)
+        elif obj.size == 1 and not isinstance(_obj, bool):
+            obj = obj.astype(intp).reshape(())
+            single_value = True
+
+    if single_value:
+        # optimization for a single value
+        if (obj < -N or obj >= N):
+            raise IndexError(
+                "index %i is out of bounds for axis %i with "
+                "size %i" % (obj, axis, N))
+        if (obj < 0):
+            obj += N
+        newshape[axis] -= 1
+        new = empty(newshape, arr.dtype, arrorder)
+        slobj[axis] = slice(None, obj)
+        new[tuple(slobj)] = arr[tuple(slobj)]
+        slobj[axis] = slice(obj, None)
+        slobj2 = [slice(None)]*ndim
+        slobj2[axis] = slice(obj+1, None)
+        new[tuple(slobj)] = arr[tuple(slobj2)]
+    else:
+        if obj.dtype == bool:
+            if obj.shape != (N,):
+                raise ValueError('boolean array argument obj to delete '
+                                 'must be one dimensional and match the axis '
+                                 'length of {}'.format(N))
+
+            # optimization, the other branch is slower
+            keep = ~obj
+        else:
+            keep = ones(N, dtype=bool)
+            keep[obj,] = False
+
+        slobj[axis] = keep
+        new = arr[tuple(slobj)]
+
+    if wrap:
+        return wrap(new)
+    else:
+        return new
+
+
+def _insert_dispatcher(arr, obj, values, axis=None):
+    return (arr, obj, values)
+
+
+@array_function_dispatch(_insert_dispatcher)
+def insert(arr, obj, values, axis=None):
+    """
+    Insert values along the given axis before the given indices.
+
+    Parameters
+    ----------
+    arr : array_like
+        Input array.
+    obj : int, slice or sequence of ints
+        Object that defines the index or indices before which `values` is
+        inserted.
+
+        .. versionadded:: 1.8.0
+
+        Support for multiple insertions when `obj` is a single scalar or a
+        sequence with one element (similar to calling insert multiple
+        times).
+    values : array_like
+        Values to insert into `arr`. If the type of `values` is different
+        from that of `arr`, `values` is converted to the type of `arr`.
+        `values` should be shaped so that ``arr[...,obj,...] = values``
+        is legal.
+    axis : int, optional
+        Axis along which to insert `values`.  If `axis` is None then `arr`
+        is flattened first.
+
+    Returns
+    -------
+    out : ndarray
+        A copy of `arr` with `values` inserted.  Note that `insert`
+        does not occur in-place: a new array is returned. If
+        `axis` is None, `out` is a flattened array.
+
+    See Also
+    --------
+    append : Append elements at the end of an array.
+    concatenate : Join a sequence of arrays along an existing axis.
+    delete : Delete elements from an array.
+
+    Notes
+    -----
+    Note that for higher dimensional inserts ``obj=0`` behaves very different
+    from ``obj=[0]`` just like ``arr[:,0,:] = values`` is different from
+    ``arr[:,[0],:] = values``.
+
+    Examples
+    --------
+    >>> a = np.array([[1, 1], [2, 2], [3, 3]])
+    >>> a
+    array([[1, 1],
+           [2, 2],
+           [3, 3]])
+    >>> np.insert(a, 1, 5)
+    array([1, 5, 1, ..., 2, 3, 3])
+    >>> np.insert(a, 1, 5, axis=1)
+    array([[1, 5, 1],
+           [2, 5, 2],
+           [3, 5, 3]])
+
+    Difference between sequence and scalars:
+
+    >>> np.insert(a, [1], [[1],[2],[3]], axis=1)
+    array([[1, 1, 1],
+           [2, 2, 2],
+           [3, 3, 3]])
+    >>> np.array_equal(np.insert(a, 1, [1, 2, 3], axis=1),
+    ...                np.insert(a, [1], [[1],[2],[3]], axis=1))
+    True
+
+    >>> b = a.flatten()
+    >>> b
+    array([1, 1, 2, 2, 3, 3])
+    >>> np.insert(b, [2, 2], [5, 6])
+    array([1, 1, 5, ..., 2, 3, 3])
+
+    >>> np.insert(b, slice(2, 4), [5, 6])
+    array([1, 1, 5, ..., 2, 3, 3])
+
+    >>> np.insert(b, [2, 2], [7.13, False]) # type casting
+    array([1, 1, 7, ..., 2, 3, 3])
+
+    >>> x = np.arange(8).reshape(2, 4)
+    >>> idx = (1, 3)
+    >>> np.insert(x, idx, 999, axis=1)
+    array([[  0, 999,   1,   2, 999,   3],
+           [  4, 999,   5,   6, 999,   7]])
+
+    """
+    wrap = None
+    if type(arr) is not ndarray:
+        try:
+            wrap = arr.__array_wrap__
+        except AttributeError:
+            pass
+
+    arr = asarray(arr)
+    ndim = arr.ndim
+    arrorder = 'F' if arr.flags.fnc else 'C'
+    if axis is None:
+        if ndim != 1:
+            arr = arr.ravel()
+        # needed for np.matrix, which is still not 1d after being ravelled
+        ndim = arr.ndim
+        axis = ndim - 1
+    else:
+        axis = normalize_axis_index(axis, ndim)
+    slobj = [slice(None)]*ndim
+    N = arr.shape[axis]
+    newshape = list(arr.shape)
+
+    if isinstance(obj, slice):
+        # turn it into a range object
+        indices = arange(*obj.indices(N), dtype=intp)
+    else:
+        # need to copy obj, because indices will be changed in-place
+        indices = np.array(obj)
+        if indices.dtype == bool:
+            # See also delete
+            # 2012-10-11, NumPy 1.8
+            warnings.warn(
+                "in the future insert will treat boolean arrays and "
+                "array-likes as a boolean index instead of casting it to "
+                "integer", FutureWarning, stacklevel=3)
+            indices = indices.astype(intp)
+            # Code after warning period:
+            #if obj.ndim != 1:
+            #    raise ValueError('boolean array argument obj to insert '
+            #                     'must be one dimensional')
+            #indices = np.flatnonzero(obj)
+        elif indices.ndim > 1:
+            raise ValueError(
+                "index array argument obj to insert must be one dimensional "
+                "or scalar")
+    if indices.size == 1:
+        index = indices.item()
+        if index < -N or index > N:
+            raise IndexError(f"index {obj} is out of bounds for axis {axis} "
+                             f"with size {N}")
+        if (index < 0):
+            index += N
+
+        # There are some object array corner cases here, but we cannot avoid
+        # that:
+        values = array(values, copy=False, ndmin=arr.ndim, dtype=arr.dtype)
+        if indices.ndim == 0:
+            # broadcasting is very different here, since a[:,0,:] = ... behaves
+            # very different from a[:,[0],:] = ...! This changes values so that
+            # it works likes the second case. (here a[:,0:1,:])
+            values = np.moveaxis(values, 0, axis)
+        numnew = values.shape[axis]
+        newshape[axis] += numnew
+        new = empty(newshape, arr.dtype, arrorder)
+        slobj[axis] = slice(None, index)
+        new[tuple(slobj)] = arr[tuple(slobj)]
+        slobj[axis] = slice(index, index+numnew)
+        new[tuple(slobj)] = values
+        slobj[axis] = slice(index+numnew, None)
+        slobj2 = [slice(None)] * ndim
+        slobj2[axis] = slice(index, None)
+        new[tuple(slobj)] = arr[tuple(slobj2)]
+        if wrap:
+            return wrap(new)
+        return new
+    elif indices.size == 0 and not isinstance(obj, np.ndarray):
+        # Can safely cast the empty list to intp
+        indices = indices.astype(intp)
+
+    indices[indices < 0] += N
+
+    numnew = len(indices)
+    order = indices.argsort(kind='mergesort')   # stable sort
+    indices[order] += np.arange(numnew)
+
+    newshape[axis] += numnew
+    old_mask = ones(newshape[axis], dtype=bool)
+    old_mask[indices] = False
+
+    new = empty(newshape, arr.dtype, arrorder)
+    slobj2 = [slice(None)]*ndim
+    slobj[axis] = indices
+    slobj2[axis] = old_mask
+    new[tuple(slobj)] = values
+    new[tuple(slobj2)] = arr
+
+    if wrap:
+        return wrap(new)
+    return new
+
+
+def _append_dispatcher(arr, values, axis=None):
+    return (arr, values)
+
+
+@array_function_dispatch(_append_dispatcher)
+def append(arr, values, axis=None):
+    """
+    Append values to the end of an array.
+
+    Parameters
+    ----------
+    arr : array_like
+        Values are appended to a copy of this array.
+    values : array_like
+        These values are appended to a copy of `arr`.  It must be of the
+        correct shape (the same shape as `arr`, excluding `axis`).  If
+        `axis` is not specified, `values` can be any shape and will be
+        flattened before use.
+    axis : int, optional
+        The axis along which `values` are appended.  If `axis` is not
+        given, both `arr` and `values` are flattened before use.
+
+    Returns
+    -------
+    append : ndarray
+        A copy of `arr` with `values` appended to `axis`.  Note that
+        `append` does not occur in-place: a new array is allocated and
+        filled.  If `axis` is None, `out` is a flattened array.
+
+    See Also
+    --------
+    insert : Insert elements into an array.
+    delete : Delete elements from an array.
+
+    Examples
+    --------
+    >>> np.append([1, 2, 3], [[4, 5, 6], [7, 8, 9]])
+    array([1, 2, 3, ..., 7, 8, 9])
+
+    When `axis` is specified, `values` must have the correct shape.
+
+    >>> np.append([[1, 2, 3], [4, 5, 6]], [[7, 8, 9]], axis=0)
+    array([[1, 2, 3],
+           [4, 5, 6],
+           [7, 8, 9]])
+    >>> np.append([[1, 2, 3], [4, 5, 6]], [7, 8, 9], axis=0)
+    Traceback (most recent call last):
+        ...
+    ValueError: all the input arrays must have same number of dimensions, but
+    the array at index 0 has 2 dimension(s) and the array at index 1 has 1
+    dimension(s)
+
+    """
+    arr = asanyarray(arr)
+    if axis is None:
+        if arr.ndim != 1:
+            arr = arr.ravel()
+        values = ravel(values)
+        axis = arr.ndim-1
+    return concatenate((arr, values), axis=axis)
+
+
+def _digitize_dispatcher(x, bins, right=None):
+    return (x, bins)
+
+
+@array_function_dispatch(_digitize_dispatcher)
+def digitize(x, bins, right=False):
+    """
+    Return the indices of the bins to which each value in input array belongs.
+
+    =========  =============  ============================
+    `right`    order of bins  returned index `i` satisfies
+    =========  =============  ============================
+    ``False``  increasing     ``bins[i-1] <= x < bins[i]``
+    ``True``   increasing     ``bins[i-1] < x <= bins[i]``
+    ``False``  decreasing     ``bins[i-1] > x >= bins[i]``
+    ``True``   decreasing     ``bins[i-1] >= x > bins[i]``
+    =========  =============  ============================
+
+    If values in `x` are beyond the bounds of `bins`, 0 or ``len(bins)`` is
+    returned as appropriate.
+
+    Parameters
+    ----------
+    x : array_like
+        Input array to be binned. Prior to NumPy 1.10.0, this array had to
+        be 1-dimensional, but can now have any shape.
+    bins : array_like
+        Array of bins. It has to be 1-dimensional and monotonic.
+    right : bool, optional
+        Indicating whether the intervals include the right or the left bin
+        edge. Default behavior is (right==False) indicating that the interval
+        does not include the right edge. The left bin end is open in this
+        case, i.e., bins[i-1] <= x < bins[i] is the default behavior for
+        monotonically increasing bins.
+
+    Returns
+    -------
+    indices : ndarray of ints
+        Output array of indices, of same shape as `x`.
+
+    Raises
+    ------
+    ValueError
+        If `bins` is not monotonic.
+    TypeError
+        If the type of the input is complex.
+
+    See Also
+    --------
+    bincount, histogram, unique, searchsorted
+
+    Notes
+    -----
+    If values in `x` are such that they fall outside the bin range,
+    attempting to index `bins` with the indices that `digitize` returns
+    will result in an IndexError.
+
+    .. versionadded:: 1.10.0
+
+    `np.digitize` is  implemented in terms of `np.searchsorted`. This means
+    that a binary search is used to bin the values, which scales much better
+    for larger number of bins than the previous linear search. It also removes
+    the requirement for the input array to be 1-dimensional.
+
+    For monotonically _increasing_ `bins`, the following are equivalent::
+
+        np.digitize(x, bins, right=True)
+        np.searchsorted(bins, x, side='left')
+
+    Note that as the order of the arguments are reversed, the side must be too.
+    The `searchsorted` call is marginally faster, as it does not do any
+    monotonicity checks. Perhaps more importantly, it supports all dtypes.
+
+    Examples
+    --------
+    >>> x = np.array([0.2, 6.4, 3.0, 1.6])
+    >>> bins = np.array([0.0, 1.0, 2.5, 4.0, 10.0])
+    >>> inds = np.digitize(x, bins)
+    >>> inds
+    array([1, 4, 3, 2])
+    >>> for n in range(x.size):
+    ...   print(bins[inds[n]-1], "<=", x[n], "<", bins[inds[n]])
+    ...
+    0.0 <= 0.2 < 1.0
+    4.0 <= 6.4 < 10.0
+    2.5 <= 3.0 < 4.0
+    1.0 <= 1.6 < 2.5
+
+    >>> x = np.array([1.2, 10.0, 12.4, 15.5, 20.])
+    >>> bins = np.array([0, 5, 10, 15, 20])
+    >>> np.digitize(x,bins,right=True)
+    array([1, 2, 3, 4, 4])
+    >>> np.digitize(x,bins,right=False)
+    array([1, 3, 3, 4, 5])
+    """
+    x = _nx.asarray(x)
+    bins = _nx.asarray(bins)
+
+    # here for compatibility, searchsorted below is happy to take this
+    if np.issubdtype(x.dtype, _nx.complexfloating):
+        raise TypeError("x may not be complex")
+
+    mono = _monotonicity(bins)
+    if mono == 0:
+        raise ValueError("bins must be monotonically increasing or decreasing")
+
+    # this is backwards because the arguments below are swapped
+    side = 'left' if right else 'right'
+    if mono == -1:
+        # reverse the bins, and invert the results
+        return len(bins) - _nx.searchsorted(bins[::-1], x, side=side)
+    else:
+        return _nx.searchsorted(bins, x, side=side)
('numpy/lib', 'arraysetops.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,206 +1,830 @@
 """
-Set operations for 1D numeric arrays based on sort() function.
-
-Contains:
-  ediff1d,
-  unique1d,
-  intersect1d,
-  intersect1d_nu,
-  setxor1d,
-  setmember1d,
-  union1d,
-  setdiff1d
-
-Concerning the speed, test_unique1d_speed() reveals that up to 10000000
-elements unique1d() is about 10 times faster than the standard dictionary-based
-numpy.unique().
-
-Limitations: Except unique1d, union1d and intersect1d_nu, all functions expect
-inputs with unique elements. Speed could be gained in some operations by an
-implementaion of sort(), that can provide directly the permutation vectors,
-avoiding thus calls to argsort().
-
-To do: Optionally return indices analogously to unique1d for all functions.
-
-Author: Robert Cimrman
+Set operations for arrays based on sorting.
+
+Notes
+-----
+
+For floating point arrays, inaccurate results may appear due to usual round-off
+and floating point comparison issues.
+
+Speed could be gained in some operations by an implementation of
+`numpy.sort`, that can provide directly the permutation vectors, thus avoiding
+calls to `numpy.argsort`.
+
+Original author: Robert Cimrman
+
 """
-__all__ = ['unique1d', 'intersect1d', 'intersect1d_nu', 'setxor1d',
-           'setmember1d', 'union1d', 'setdiff1d']
-
-
-# 02.11.2005, c
-import time
-import numpy
-
-##
-# 03.11.2005, c
-def ediff1d( ar1, to_end = None, to_begin = None ):
-    """Array difference with prefixed and/or appended value."""
-    dar1 = ar1[1:] - ar1[:-1]
-    if to_end and to_begin:
-        shape = (ar1.shape[0] + 1,) + ar1.shape[1:]
-        ed = numpy.empty( shape, dtype = ar1.dtype )
-        ed[0], ed[-1] = to_begin, to_end
-        ed[1:-1] = dar1
-    elif to_end:
-        ed = numpy.empty( ar1.shape, dtype = ar1.dtype )
-        ed[-1] = to_end
-        ed[:-1] = dar1
-    elif to_begin:
-        ed = numpy.empty( ar1.shape, dtype = ar1.dtype )
-        ed[0] = to_begin
-        ed[1:] = dar1
-    else:
-        ed = dar1
-
-    return ed
-
-
-##
-# 01.11.2005, c
-# 02.11.2005
-def unique1d( ar1, retindx = False ):
-    """Unique elements of 1D array. When ret_indx is True, return also the
-    indices indx such that ar1.flat[indx] is the resulting array of unique
-    elements."""
-    if retindx:
-        ar = numpy.array(ar1).ravel()
-        perm = ar.argsort()
-        aux = ar.take(perm)
-        flag = ediff1d( aux, 1 ) != 0
-        return perm.compress(flag), aux.compress(flag)
-    else:
-        ar = numpy.array( ar1 ).flatten()
+import functools
+
+import numpy as np
+from numpy.core import overrides
+
+
+array_function_dispatch = functools.partial(
+    overrides.array_function_dispatch, module='numpy')
+
+
+__all__ = [
+    'ediff1d', 'intersect1d', 'setxor1d', 'union1d', 'setdiff1d', 'unique',
+    'in1d', 'isin'
+    ]
+
+
+def _ediff1d_dispatcher(ary, to_end=None, to_begin=None):
+    return (ary, to_end, to_begin)
+
+
+@array_function_dispatch(_ediff1d_dispatcher)
+def ediff1d(ary, to_end=None, to_begin=None):
+    """
+    The differences between consecutive elements of an array.
+
+    Parameters
+    ----------
+    ary : array_like
+        If necessary, will be flattened before the differences are taken.
+    to_end : array_like, optional
+        Number(s) to append at the end of the returned differences.
+    to_begin : array_like, optional
+        Number(s) to prepend at the beginning of the returned differences.
+
+    Returns
+    -------
+    ediff1d : ndarray
+        The differences. Loosely, this is ``ary.flat[1:] - ary.flat[:-1]``.
+
+    See Also
+    --------
+    diff, gradient
+
+    Notes
+    -----
+    When applied to masked arrays, this function drops the mask information
+    if the `to_begin` and/or `to_end` parameters are used.
+
+    Examples
+    --------
+    >>> x = np.array([1, 2, 4, 7, 0])
+    >>> np.ediff1d(x)
+    array([ 1,  2,  3, -7])
+
+    >>> np.ediff1d(x, to_begin=-99, to_end=np.array([88, 99]))
+    array([-99,   1,   2, ...,  -7,  88,  99])
+
+    The returned array is always 1D.
+
+    >>> y = [[1, 2, 4], [1, 6, 24]]
+    >>> np.ediff1d(y)
+    array([ 1,  2, -3,  5, 18])
+
+    """
+    # force a 1d array
+    ary = np.asanyarray(ary).ravel()
+
+    # enforce that the dtype of `ary` is used for the output
+    dtype_req = ary.dtype
+
+    # fast track default case
+    if to_begin is None and to_end is None:
+        return ary[1:] - ary[:-1]
+
+    if to_begin is None:
+        l_begin = 0
+    else:
+        to_begin = np.asanyarray(to_begin)
+        if not np.can_cast(to_begin, dtype_req, casting="same_kind"):
+            raise TypeError("dtype of `to_begin` must be compatible "
+                            "with input `ary` under the `same_kind` rule.")
+
+        to_begin = to_begin.ravel()
+        l_begin = len(to_begin)
+
+    if to_end is None:
+        l_end = 0
+    else:
+        to_end = np.asanyarray(to_end)
+        if not np.can_cast(to_end, dtype_req, casting="same_kind"):
+            raise TypeError("dtype of `to_end` must be compatible "
+                            "with input `ary` under the `same_kind` rule.")
+
+        to_end = to_end.ravel()
+        l_end = len(to_end)
+
+    # do the calculation in place and copy to_begin and to_end
+    l_diff = max(len(ary) - 1, 0)
+    result = np.empty(l_diff + l_begin + l_end, dtype=ary.dtype)
+    result = ary.__array_wrap__(result)
+    if l_begin > 0:
+        result[:l_begin] = to_begin
+    if l_end > 0:
+        result[l_begin + l_diff:] = to_end
+    np.subtract(ary[1:], ary[:-1], result[l_begin:l_begin + l_diff])
+    return result
+
+
+def _unpack_tuple(x):
+    """ Unpacks one-element tuples for use as return values """
+    if len(x) == 1:
+        return x[0]
+    else:
+        return x
+
+
+def _unique_dispatcher(ar, return_index=None, return_inverse=None,
+                       return_counts=None, axis=None, *, equal_nan=None):
+    return (ar,)
+
+
+@array_function_dispatch(_unique_dispatcher)
+def unique(ar, return_index=False, return_inverse=False,
+           return_counts=False, axis=None, *, equal_nan=True):
+    """
+    Find the unique elements of an array.
+
+    Returns the sorted unique elements of an array. There are three optional
+    outputs in addition to the unique elements:
+
+    * the indices of the input array that give the unique values
+    * the indices of the unique array that reconstruct the input array
+    * the number of times each unique value comes up in the input array
+
+    Parameters
+    ----------
+    ar : array_like
+        Input array. Unless `axis` is specified, this will be flattened if it
+        is not already 1-D.
+    return_index : bool, optional
+        If True, also return the indices of `ar` (along the specified axis,
+        if provided, or in the flattened array) that result in the unique array.
+    return_inverse : bool, optional
+        If True, also return the indices of the unique array (for the specified
+        axis, if provided) that can be used to reconstruct `ar`.
+    return_counts : bool, optional
+        If True, also return the number of times each unique item appears
+        in `ar`.
+    axis : int or None, optional
+        The axis to operate on. If None, `ar` will be flattened. If an integer,
+        the subarrays indexed by the given axis will be flattened and treated
+        as the elements of a 1-D array with the dimension of the given axis,
+        see the notes for more details.  Object arrays or structured arrays
+        that contain objects are not supported if the `axis` kwarg is used. The
+        default is None.
+
+        .. versionadded:: 1.13.0
+
+    equal_nan : bool, optional
+        If True, collapses multiple NaN values in the return array into one.
+
+        .. versionadded:: 1.24
+
+    Returns
+    -------
+    unique : ndarray
+        The sorted unique values.
+    unique_indices : ndarray, optional
+        The indices of the first occurrences of the unique values in the
+        original array. Only provided if `return_index` is True.
+    unique_inverse : ndarray, optional
+        The indices to reconstruct the original array from the
+        unique array. Only provided if `return_inverse` is True.
+    unique_counts : ndarray, optional
+        The number of times each of the unique values comes up in the
+        original array. Only provided if `return_counts` is True.
+
+        .. versionadded:: 1.9.0
+
+    See Also
+    --------
+    numpy.lib.arraysetops : Module with a number of other functions for
+                            performing set operations on arrays.
+    repeat : Repeat elements of an array.
+
+    Notes
+    -----
+    When an axis is specified the subarrays indexed by the axis are sorted.
+    This is done by making the specified axis the first dimension of the array
+    (move the axis to the first dimension to keep the order of the other axes)
+    and then flattening the subarrays in C order. The flattened subarrays are
+    then viewed as a structured type with each element given a label, with the
+    effect that we end up with a 1-D array of structured types that can be
+    treated in the same way as any other 1-D array. The result is that the
+    flattened subarrays are sorted in lexicographic order starting with the
+    first element.
+
+    .. versionchanged: NumPy 1.21
+        If nan values are in the input array, a single nan is put
+        to the end of the sorted unique values.
+
+        Also for complex arrays all NaN values are considered equivalent
+        (no matter whether the NaN is in the real or imaginary part).
+        As the representant for the returned array the smallest one in the
+        lexicographical order is chosen - see np.sort for how the lexicographical
+        order is defined for complex arrays.
+
+    Examples
+    --------
+    >>> np.unique([1, 1, 2, 2, 3, 3])
+    array([1, 2, 3])
+    >>> a = np.array([[1, 1], [2, 3]])
+    >>> np.unique(a)
+    array([1, 2, 3])
+
+    Return the unique rows of a 2D array
+
+    >>> a = np.array([[1, 0, 0], [1, 0, 0], [2, 3, 4]])
+    >>> np.unique(a, axis=0)
+    array([[1, 0, 0], [2, 3, 4]])
+
+    Return the indices of the original array that give the unique values:
+
+    >>> a = np.array(['a', 'b', 'b', 'c', 'a'])
+    >>> u, indices = np.unique(a, return_index=True)
+    >>> u
+    array(['a', 'b', 'c'], dtype='<U1')
+    >>> indices
+    array([0, 1, 3])
+    >>> a[indices]
+    array(['a', 'b', 'c'], dtype='<U1')
+
+    Reconstruct the input array from the unique values and inverse:
+
+    >>> a = np.array([1, 2, 6, 4, 2, 3, 2])
+    >>> u, indices = np.unique(a, return_inverse=True)
+    >>> u
+    array([1, 2, 3, 4, 6])
+    >>> indices
+    array([0, 1, 4, 3, 1, 2, 1])
+    >>> u[indices]
+    array([1, 2, 6, 4, 2, 3, 2])
+
+    Reconstruct the input values from the unique values and counts:
+
+    >>> a = np.array([1, 2, 6, 4, 2, 3, 2])
+    >>> values, counts = np.unique(a, return_counts=True)
+    >>> values
+    array([1, 2, 3, 4, 6])
+    >>> counts
+    array([1, 3, 1, 1, 1])
+    >>> np.repeat(values, counts)
+    array([1, 2, 2, 2, 3, 4, 6])    # original order not preserved
+
+    """
+    ar = np.asanyarray(ar)
+    if axis is None:
+        ret = _unique1d(ar, return_index, return_inverse, return_counts, 
+                        equal_nan=equal_nan)
+        return _unpack_tuple(ret)
+
+    # axis was specified and not None
+    try:
+        ar = np.moveaxis(ar, axis, 0)
+    except np.AxisError:
+        # this removes the "axis1" or "axis2" prefix from the error message
+        raise np.AxisError(axis, ar.ndim) from None
+
+    # Must reshape to a contiguous 2D array for this to work...
+    orig_shape, orig_dtype = ar.shape, ar.dtype
+    ar = ar.reshape(orig_shape[0], np.prod(orig_shape[1:], dtype=np.intp))
+    ar = np.ascontiguousarray(ar)
+    dtype = [('f{i}'.format(i=i), ar.dtype) for i in range(ar.shape[1])]
+
+    # At this point, `ar` has shape `(n, m)`, and `dtype` is a structured
+    # data type with `m` fields where each field has the data type of `ar`.
+    # In the following, we create the array `consolidated`, which has
+    # shape `(n,)` with data type `dtype`.
+    try:
+        if ar.shape[1] > 0:
+            consolidated = ar.view(dtype)
+        else:
+            # If ar.shape[1] == 0, then dtype will be `np.dtype([])`, which is
+            # a data type with itemsize 0, and the call `ar.view(dtype)` will
+            # fail.  Instead, we'll use `np.empty` to explicitly create the
+            # array with shape `(len(ar),)`.  Because `dtype` in this case has
+            # itemsize 0, the total size of the result is still 0 bytes.
+            consolidated = np.empty(len(ar), dtype=dtype)
+    except TypeError as e:
+        # There's no good way to do this for object arrays, etc...
+        msg = 'The axis argument to unique is not supported for dtype {dt}'
+        raise TypeError(msg.format(dt=ar.dtype)) from e
+
+    def reshape_uniq(uniq):
+        n = len(uniq)
+        uniq = uniq.view(orig_dtype)
+        uniq = uniq.reshape(n, *orig_shape[1:])
+        uniq = np.moveaxis(uniq, 0, axis)
+        return uniq
+
+    output = _unique1d(consolidated, return_index,
+                       return_inverse, return_counts, equal_nan=equal_nan)
+    output = (reshape_uniq(output[0]),) + output[1:]
+    return _unpack_tuple(output)
+
+
+def _unique1d(ar, return_index=False, return_inverse=False,
+              return_counts=False, *, equal_nan=True):
+    """
+    Find the unique elements of an array, ignoring shape.
+    """
+    ar = np.asanyarray(ar).flatten()
+
+    optional_indices = return_index or return_inverse
+
+    if optional_indices:
+        perm = ar.argsort(kind='mergesort' if return_index else 'quicksort')
+        aux = ar[perm]
+    else:
         ar.sort()
-        return ar.compress( ediff1d( ar, 1 ) != 0)
-
-##
-# 01.11.2005, c
-def intersect1d( ar1, ar2 ):
-    """Intersection of 1D arrays with unique elements."""
-    aux = numpy.concatenate((ar1,ar2))
+        aux = ar
+    mask = np.empty(aux.shape, dtype=np.bool_)
+    mask[:1] = True
+    if (equal_nan and aux.shape[0] > 0 and aux.dtype.kind in "cfmM" and
+            np.isnan(aux[-1])):
+        if aux.dtype.kind == "c":  # for complex all NaNs are considered equivalent
+            aux_firstnan = np.searchsorted(np.isnan(aux), True, side='left')
+        else:
+            aux_firstnan = np.searchsorted(aux, aux[-1], side='left')
+        if aux_firstnan > 0:
+            mask[1:aux_firstnan] = (
+                aux[1:aux_firstnan] != aux[:aux_firstnan - 1])
+        mask[aux_firstnan] = True
+        mask[aux_firstnan + 1:] = False
+    else:
+        mask[1:] = aux[1:] != aux[:-1]
+
+    ret = (aux[mask],)
+    if return_index:
+        ret += (perm[mask],)
+    if return_inverse:
+        imask = np.cumsum(mask) - 1
+        inv_idx = np.empty(mask.shape, dtype=np.intp)
+        inv_idx[perm] = imask
+        ret += (inv_idx,)
+    if return_counts:
+        idx = np.concatenate(np.nonzero(mask) + ([mask.size],))
+        ret += (np.diff(idx),)
+    return ret
+
+
+def _intersect1d_dispatcher(
+        ar1, ar2, assume_unique=None, return_indices=None):
+    return (ar1, ar2)
+
+
+@array_function_dispatch(_intersect1d_dispatcher)
+def intersect1d(ar1, ar2, assume_unique=False, return_indices=False):
+    """
+    Find the intersection of two arrays.
+
+    Return the sorted, unique values that are in both of the input arrays.
+
+    Parameters
+    ----------
+    ar1, ar2 : array_like
+        Input arrays. Will be flattened if not already 1D.
+    assume_unique : bool
+        If True, the input arrays are both assumed to be unique, which
+        can speed up the calculation.  If True but ``ar1`` or ``ar2`` are not
+        unique, incorrect results and out-of-bounds indices could result.
+        Default is False.
+    return_indices : bool
+        If True, the indices which correspond to the intersection of the two
+        arrays are returned. The first instance of a value is used if there are
+        multiple. Default is False.
+
+        .. versionadded:: 1.15.0
+
+    Returns
+    -------
+    intersect1d : ndarray
+        Sorted 1D array of common and unique elements.
+    comm1 : ndarray
+        The indices of the first occurrences of the common values in `ar1`.
+        Only provided if `return_indices` is True.
+    comm2 : ndarray
+        The indices of the first occurrences of the common values in `ar2`.
+        Only provided if `return_indices` is True.
+
+
+    See Also
+    --------
+    numpy.lib.arraysetops : Module with a number of other functions for
+                            performing set operations on arrays.
+
+    Examples
+    --------
+    >>> np.intersect1d([1, 3, 4, 3], [3, 1, 2, 1])
+    array([1, 3])
+
+    To intersect more than two arrays, use functools.reduce:
+
+    >>> from functools import reduce
+    >>> reduce(np.intersect1d, ([1, 3, 4, 3], [3, 1, 2, 1], [6, 3, 4, 2]))
+    array([3])
+
+    To return the indices of the values common to the input arrays
+    along with the intersected values:
+
+    >>> x = np.array([1, 1, 2, 3, 4])
+    >>> y = np.array([2, 1, 4, 6])
+    >>> xy, x_ind, y_ind = np.intersect1d(x, y, return_indices=True)
+    >>> x_ind, y_ind
+    (array([0, 2, 4]), array([1, 0, 2]))
+    >>> xy, x[x_ind], y[y_ind]
+    (array([1, 2, 4]), array([1, 2, 4]), array([1, 2, 4]))
+
+    """
+    ar1 = np.asanyarray(ar1)
+    ar2 = np.asanyarray(ar2)
+
+    if not assume_unique:
+        if return_indices:
+            ar1, ind1 = unique(ar1, return_index=True)
+            ar2, ind2 = unique(ar2, return_index=True)
+        else:
+            ar1 = unique(ar1)
+            ar2 = unique(ar2)
+    else:
+        ar1 = ar1.ravel()
+        ar2 = ar2.ravel()
+
+    aux = np.concatenate((ar1, ar2))
+    if return_indices:
+        aux_sort_indices = np.argsort(aux, kind='mergesort')
+        aux = aux[aux_sort_indices]
+    else:
+        aux.sort()
+
+    mask = aux[1:] == aux[:-1]
+    int1d = aux[:-1][mask]
+
+    if return_indices:
+        ar1_indices = aux_sort_indices[:-1][mask]
+        ar2_indices = aux_sort_indices[1:][mask] - ar1.size
+        if not assume_unique:
+            ar1_indices = ind1[ar1_indices]
+            ar2_indices = ind2[ar2_indices]
+
+        return int1d, ar1_indices, ar2_indices
+    else:
+        return int1d
+
+
+def _setxor1d_dispatcher(ar1, ar2, assume_unique=None):
+    return (ar1, ar2)
+
+
+@array_function_dispatch(_setxor1d_dispatcher)
+def setxor1d(ar1, ar2, assume_unique=False):
+    """
+    Find the set exclusive-or of two arrays.
+
+    Return the sorted, unique values that are in only one (not both) of the
+    input arrays.
+
+    Parameters
+    ----------
+    ar1, ar2 : array_like
+        Input arrays.
+    assume_unique : bool
+        If True, the input arrays are both assumed to be unique, which
+        can speed up the calculation.  Default is False.
+
+    Returns
+    -------
+    setxor1d : ndarray
+        Sorted 1D array of unique values that are in only one of the input
+        arrays.
+
+    Examples
+    --------
+    >>> a = np.array([1, 2, 3, 2, 4])
+    >>> b = np.array([2, 3, 5, 7, 5])
+    >>> np.setxor1d(a,b)
+    array([1, 4, 5, 7])
+
+    """
+    if not assume_unique:
+        ar1 = unique(ar1)
+        ar2 = unique(ar2)
+
+    aux = np.concatenate((ar1, ar2))
+    if aux.size == 0:
+        return aux
+
     aux.sort()
-    return aux.compress( (aux[1:] - aux[:-1]) == 0)
-
-##
-# 01.11.2005, c
-def intersect1d_nu( ar1, ar2 ):
-    """Intersection of 1D arrays with any elements."""
-    # Might be faster then unique1d( intersect1d( ar1, ar2 ) )?
-    aux = numpy.concatenate((unique1d(ar1), unique1d(ar2)))
-    aux.sort()
-    return aux.compress( (aux[1:] - aux[:-1]) == 0)
-
-##
-# 01.11.2005, c
-def setxor1d( ar1, ar2 ):
-    """Set exclusive-or of 1D arrays with unique elements."""
-    aux = numpy.concatenate( (ar1, ar2 ) )
-    aux.sort()
-    flag = ediff1d( aux, to_end = 1, to_begin = 1 ) == 0
-    flag2 = ediff1d( flag, 0 ) == 0
-    return aux.compress( flag2 )
-
-##
-# 03.11.2005, c
-# 05.01.2006
-def setmember1d( ar1, ar2 ):
-    """Return an array of shape of ar1 containing 1 where the elements of
-    ar1 are in ar2 and 0 otherwise."""
-    concat = numpy.concatenate
-    zlike = numpy.zeros_like
-    ar = concat( (ar1, ar2 ) )
-    tt = concat( (zlike( ar1 ),
-                  zlike( ar2 ) + 1) )
-    perm = ar.argsort()
-    aux = ar.take(perm)
-    aux2 = tt.take(perm)
-    flag = ediff1d( aux, 1 ) == 0
-
-    ii = numpy.where( flag * aux2 )[0]
-    aux = perm[ii+1]
-    perm[ii+1] = perm[ii]
-    perm[ii] = aux
-
-    indx = perm.argsort()[:len( ar1 )]
-
-    return flag.take( indx )
-
-##
-# 03.11.2005, c
-def union1d( ar1, ar2 ):
-    """Union of 1D arrays with unique elements."""
-    return unique1d( numpy.concatenate( (ar1, ar2) ) )
-
-##
-# 03.11.2005, c
-def setdiff1d( ar1, ar2 ):
-    """Set difference of 1D arrays with unique elements."""
-    aux = setmember1d( ar1, ar2 )
-    return ar1.compress(aux == 0)
-
-##
-# 02.11.2005, c
-def test_unique1d_speed( plot_results = False ):
-#    exponents = numpy.linspace( 2, 7, 9 )
-    exponents = numpy.linspace( 2, 6, 9 )
-    ratios = []
-    nItems = []
-    dt1s = []
-    dt2s = []
-    for ii in exponents:
-
-        nItem = 10 ** ii
-        print 'using %d items:' % nItem
-        a = numpy.fix( nItem / 10 * numpy.random.random( nItem ) )
-
-        print 'dictionary:'
-        tt = time.clock()
-        b = numpy.unique( a )
-        dt1 = time.clock() - tt
-        print dt1
-
-        print 'array:'
-        tt = time.clock()
-        c = unique1d( a )
-        dt2 = time.clock() - tt
-        print dt2
-
-
-        if dt1 < 1e-8:
-            ratio = 'ND'
+    flag = np.concatenate(([True], aux[1:] != aux[:-1], [True]))
+    return aux[flag[1:] & flag[:-1]]
+
+
+def _in1d_dispatcher(ar1, ar2, assume_unique=None, invert=None):
+    return (ar1, ar2)
+
+
+@array_function_dispatch(_in1d_dispatcher)
+def in1d(ar1, ar2, assume_unique=False, invert=False):
+    """
+    Test whether each element of a 1-D array is also present in a second array.
+
+    Returns a boolean array the same length as `ar1` that is True
+    where an element of `ar1` is in `ar2` and False otherwise.
+
+    We recommend using :func:`isin` instead of `in1d` for new code.
+
+    Parameters
+    ----------
+    ar1 : (M,) array_like
+        Input array.
+    ar2 : array_like
+        The values against which to test each value of `ar1`.
+    assume_unique : bool, optional
+        If True, the input arrays are both assumed to be unique, which
+        can speed up the calculation.  Default is False.
+    invert : bool, optional
+        If True, the values in the returned array are inverted (that is,
+        False where an element of `ar1` is in `ar2` and True otherwise).
+        Default is False. ``np.in1d(a, b, invert=True)`` is equivalent
+        to (but is faster than) ``np.invert(in1d(a, b))``.
+
+        .. versionadded:: 1.8.0
+
+    Returns
+    -------
+    in1d : (M,) ndarray, bool
+        The values `ar1[in1d]` are in `ar2`.
+
+    See Also
+    --------
+    isin                  : Version of this function that preserves the
+                            shape of ar1.
+    numpy.lib.arraysetops : Module with a number of other functions for
+                            performing set operations on arrays.
+
+    Notes
+    -----
+    `in1d` can be considered as an element-wise function version of the
+    python keyword `in`, for 1-D sequences. ``in1d(a, b)`` is roughly
+    equivalent to ``np.array([item in b for item in a])``.
+    However, this idea fails if `ar2` is a set, or similar (non-sequence)
+    container:  As ``ar2`` is converted to an array, in those cases
+    ``asarray(ar2)`` is an object array rather than the expected array of
+    contained values.
+
+    .. versionadded:: 1.4.0
+
+    Examples
+    --------
+    >>> test = np.array([0, 1, 2, 5, 0])
+    >>> states = [0, 2]
+    >>> mask = np.in1d(test, states)
+    >>> mask
+    array([ True, False,  True, False,  True])
+    >>> test[mask]
+    array([0, 2, 0])
+    >>> mask = np.in1d(test, states, invert=True)
+    >>> mask
+    array([False,  True, False,  True, False])
+    >>> test[mask]
+    array([1, 5])
+    """
+    # Ravel both arrays, behavior for the first array could be different
+    ar1 = np.asarray(ar1).ravel()
+    ar2 = np.asarray(ar2).ravel()
+
+    # Ensure that iteration through object arrays yields size-1 arrays
+    if ar2.dtype == object:
+        ar2 = ar2.reshape(-1, 1)
+
+    # Check if one of the arrays may contain arbitrary objects
+    contains_object = ar1.dtype.hasobject or ar2.dtype.hasobject
+
+    # This code is run when
+    # a) the first condition is true, making the code significantly faster
+    # b) the second condition is true (i.e. `ar1` or `ar2` may contain
+    #    arbitrary objects), since then sorting is not guaranteed to work
+    if len(ar2) < 10 * len(ar1) ** 0.145 or contains_object:
+        if invert:
+            mask = np.ones(len(ar1), dtype=bool)
+            for a in ar2:
+                mask &= (ar1 != a)
         else:
-            ratio = dt2 / dt1
-        print 'ratio:', ratio
-        print 'nUnique: %d == %d\n' % (len( b ), len( c ))
-
-        nItems.append( nItem )
-        ratios.append( ratio )
-        dt1s.append( dt1 )
-        dt2s.append( dt2 )
-
-        assert numpy.alltrue( b == c )
-
-
-    print nItems
-    print dt1s
-    print dt2s
-    print ratios
-
-    if plot_results:
-        import pylab
-
-        def plotMe( fig, fun, nItems, dt1s, dt2s ):
-            pylab.figure( fig )
-            fun( nItems, dt1s, 'g-o', linewidth = 2, markersize = 8 )
-            fun( nItems, dt2s, 'b-x', linewidth = 2, markersize = 8 )
-            pylab.legend( ('dictionary', 'array' ) )
-            pylab.xlabel( 'nItem' )
-            pylab.ylabel( 'time [s]' )
-
-        plotMe( 1, pylab.loglog, nItems, dt1s, dt2s )
-        plotMe( 2, pylab.plot, nItems, dt1s, dt2s )
-        pylab.show()
-
-if (__name__ == '__main__'):
-    test_unique1d_speed( plot_results = True )
+            mask = np.zeros(len(ar1), dtype=bool)
+            for a in ar2:
+                mask |= (ar1 == a)
+        return mask
+
+    # Otherwise use sorting
+    if not assume_unique:
+        ar1, rev_idx = np.unique(ar1, return_inverse=True)
+        ar2 = np.unique(ar2)
+
+    ar = np.concatenate((ar1, ar2))
+    # We need this to be a stable sort, so always use 'mergesort'
+    # here. The values from the first array should always come before
+    # the values from the second array.
+    order = ar.argsort(kind='mergesort')
+    sar = ar[order]
+    if invert:
+        bool_ar = (sar[1:] != sar[:-1])
+    else:
+        bool_ar = (sar[1:] == sar[:-1])
+    flag = np.concatenate((bool_ar, [invert]))
+    ret = np.empty(ar.shape, dtype=bool)
+    ret[order] = flag
+
+    if assume_unique:
+        return ret[:len(ar1)]
+    else:
+        return ret[rev_idx]
+
+
+def _isin_dispatcher(element, test_elements, assume_unique=None, invert=None):
+    return (element, test_elements)
+
+
+@array_function_dispatch(_isin_dispatcher)
+def isin(element, test_elements, assume_unique=False, invert=False):
+    """
+    Calculates ``element in test_elements``, broadcasting over `element` only.
+    Returns a boolean array of the same shape as `element` that is True
+    where an element of `element` is in `test_elements` and False otherwise.
+
+    Parameters
+    ----------
+    element : array_like
+        Input array.
+    test_elements : array_like
+        The values against which to test each value of `element`.
+        This argument is flattened if it is an array or array_like.
+        See notes for behavior with non-array-like parameters.
+    assume_unique : bool, optional
+        If True, the input arrays are both assumed to be unique, which
+        can speed up the calculation.  Default is False.
+    invert : bool, optional
+        If True, the values in the returned array are inverted, as if
+        calculating `element not in test_elements`. Default is False.
+        ``np.isin(a, b, invert=True)`` is equivalent to (but faster
+        than) ``np.invert(np.isin(a, b))``.
+
+    Returns
+    -------
+    isin : ndarray, bool
+        Has the same shape as `element`. The values `element[isin]`
+        are in `test_elements`.
+
+    See Also
+    --------
+    in1d                  : Flattened version of this function.
+    numpy.lib.arraysetops : Module with a number of other functions for
+                            performing set operations on arrays.
+
+    Notes
+    -----
+
+    `isin` is an element-wise function version of the python keyword `in`.
+    ``isin(a, b)`` is roughly equivalent to
+    ``np.array([item in b for item in a])`` if `a` and `b` are 1-D sequences.
+
+    `element` and `test_elements` are converted to arrays if they are not
+    already. If `test_elements` is a set (or other non-sequence collection)
+    it will be converted to an object array with one element, rather than an
+    array of the values contained in `test_elements`. This is a consequence
+    of the `array` constructor's way of handling non-sequence collections.
+    Converting the set to a list usually gives the desired behavior.
+
+    .. versionadded:: 1.13.0
+
+    Examples
+    --------
+    >>> element = 2*np.arange(4).reshape((2, 2))
+    >>> element
+    array([[0, 2],
+           [4, 6]])
+    >>> test_elements = [1, 2, 4, 8]
+    >>> mask = np.isin(element, test_elements)
+    >>> mask
+    array([[False,  True],
+           [ True, False]])
+    >>> element[mask]
+    array([2, 4])
+
+    The indices of the matched values can be obtained with `nonzero`:
+
+    >>> np.nonzero(mask)
+    (array([0, 1]), array([1, 0]))
+
+    The test can also be inverted:
+
+    >>> mask = np.isin(element, test_elements, invert=True)
+    >>> mask
+    array([[ True, False],
+           [False,  True]])
+    >>> element[mask]
+    array([0, 6])
+
+    Because of how `array` handles sets, the following does not
+    work as expected:
+
+    >>> test_set = {1, 2, 4, 8}
+    >>> np.isin(element, test_set)
+    array([[False, False],
+           [False, False]])
+
+    Casting the set to a list gives the expected result:
+
+    >>> np.isin(element, list(test_set))
+    array([[False,  True],
+           [ True, False]])
+    """
+    element = np.asarray(element)
+    return in1d(element, test_elements, assume_unique=assume_unique,
+                invert=invert).reshape(element.shape)
+
+
+def _union1d_dispatcher(ar1, ar2):
+    return (ar1, ar2)
+
+
+@array_function_dispatch(_union1d_dispatcher)
+def union1d(ar1, ar2):
+    """
+    Find the union of two arrays.
+
+    Return the unique, sorted array of values that are in either of the two
+    input arrays.
+
+    Parameters
+    ----------
+    ar1, ar2 : array_like
+        Input arrays. They are flattened if they are not already 1D.
+
+    Returns
+    -------
+    union1d : ndarray
+        Unique, sorted union of the input arrays.
+
+    See Also
+    --------
+    numpy.lib.arraysetops : Module with a number of other functions for
+                            performing set operations on arrays.
+
+    Examples
+    --------
+    >>> np.union1d([-1, 0, 1], [-2, 0, 2])
+    array([-2, -1,  0,  1,  2])
+
+    To find the union of more than two arrays, use functools.reduce:
+
+    >>> from functools import reduce
+    >>> reduce(np.union1d, ([1, 3, 4, 3], [3, 1, 2, 1], [6, 3, 4, 2]))
+    array([1, 2, 3, 4, 6])
+    """
+    return unique(np.concatenate((ar1, ar2), axis=None))
+
+
+def _setdiff1d_dispatcher(ar1, ar2, assume_unique=None):
+    return (ar1, ar2)
+
+
+@array_function_dispatch(_setdiff1d_dispatcher)
+def setdiff1d(ar1, ar2, assume_unique=False):
+    """
+    Find the set difference of two arrays.
+
+    Return the unique values in `ar1` that are not in `ar2`.
+
+    Parameters
+    ----------
+    ar1 : array_like
+        Input array.
+    ar2 : array_like
+        Input comparison array.
+    assume_unique : bool
+        If True, the input arrays are both assumed to be unique, which
+        can speed up the calculation.  Default is False.
+
+    Returns
+    -------
+    setdiff1d : ndarray
+        1D array of values in `ar1` that are not in `ar2`. The result
+        is sorted when `assume_unique=False`, but otherwise only sorted
+        if the input is sorted.
+
+    See Also
+    --------
+    numpy.lib.arraysetops : Module with a number of other functions for
+                            performing set operations on arrays.
+
+    Examples
+    --------
+    >>> a = np.array([1, 2, 3, 2, 4, 1])
+    >>> b = np.array([3, 4, 5, 6])
+    >>> np.setdiff1d(a, b)
+    array([1, 2])
+
+    """
+    if assume_unique:
+        ar1 = np.asarray(ar1).ravel()
+    else:
+        ar1 = unique(ar1)
+        ar2 = unique(ar2)
+    return ar1[in1d(ar1, ar2, assume_unique=True, invert=True)]
('numpy/lib', 'type_check.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,171 +1,735 @@
-## Automatically adapted for numpy Sep 19, 2005 by convertcode.py
-
-__all__ = ['iscomplexobj','isrealobj','imag','iscomplex',
-           'isreal','nan_to_num','real','real_if_close',
-           'typename','asfarray','mintypecode','asscalar',
+"""Automatically adapted for numpy Sep 19, 2005 by convertcode.py
+
+"""
+import functools
+import warnings
+
+__all__ = ['iscomplexobj', 'isrealobj', 'imag', 'iscomplex',
+           'isreal', 'nan_to_num', 'real', 'real_if_close',
+           'typename', 'asfarray', 'mintypecode',
            'common_type']
 
 import numpy.core.numeric as _nx
-from numpy.core.numeric import asarray, array, isnan, obj2sctype
-from ufunclike import isneginf, isposinf
+from numpy.core.numeric import asarray, asanyarray, isnan, zeros
+from numpy.core.overrides import set_module
+from numpy.core import overrides
+from .ufunclike import isneginf, isposinf
+
+
+array_function_dispatch = functools.partial(
+    overrides.array_function_dispatch, module='numpy')
+
 
 _typecodes_by_elsize = 'GDFgdfQqLlIiHhBb?'
 
-def mintypecode(typechars,typeset='GDFgdf',default='d'):
-    """ Return a minimum data type character from typeset that
-    handles all typechars given
-
-    The returned type character must be the smallest size such that
-    an array of the returned type can handle the data from an array of
-    type t for each t in typechars (or if typechars is an array,
-    then its dtype.char).
-
-    If the typechars does not intersect with the typeset, then default
-    is returned.
-
-    If t in typechars is not a string then t=asarray(t).dtype.char is
-    applied.
-    """
-    typecodes = [(type(t) is type('') and t) or asarray(t).dtype.char\
-                 for t in typechars]
-    intersection = [t for t in typecodes if t in typeset]
+
+@set_module('numpy')
+def mintypecode(typechars, typeset='GDFgdf', default='d'):
+    """
+    Return the character for the minimum-size type to which given types can
+    be safely cast.
+
+    The returned type character must represent the smallest size dtype such
+    that an array of the returned type can handle the data from an array of
+    all types in `typechars` (or if `typechars` is an array, then its
+    dtype.char).
+
+    Parameters
+    ----------
+    typechars : list of str or array_like
+        If a list of strings, each string should represent a dtype.
+        If array_like, the character representation of the array dtype is used.
+    typeset : str or list of str, optional
+        The set of characters that the returned character is chosen from.
+        The default set is 'GDFgdf'.
+    default : str, optional
+        The default character, this is returned if none of the characters in
+        `typechars` matches a character in `typeset`.
+
+    Returns
+    -------
+    typechar : str
+        The character representing the minimum-size type that was found.
+
+    See Also
+    --------
+    dtype, sctype2char, maximum_sctype
+
+    Examples
+    --------
+    >>> np.mintypecode(['d', 'f', 'S'])
+    'd'
+    >>> x = np.array([1.1, 2-3.j])
+    >>> np.mintypecode(x)
+    'D'
+
+    >>> np.mintypecode('abceh', default='G')
+    'G'
+
+    """
+    typecodes = ((isinstance(t, str) and t) or asarray(t).dtype.char
+                 for t in typechars)
+    intersection = set(t for t in typecodes if t in typeset)
     if not intersection:
         return default
     if 'F' in intersection and 'd' in intersection:
         return 'D'
-    l = []
-    for t in intersection:
-        i = _typecodes_by_elsize.index(t)
-        l.append((i,t))
-    l.sort()
-    return l[0][1]
-
+    return min(intersection, key=_typecodes_by_elsize.index)
+
+
+def _asfarray_dispatcher(a, dtype=None):
+    return (a,)
+
+
+@array_function_dispatch(_asfarray_dispatcher)
 def asfarray(a, dtype=_nx.float_):
-    """asfarray(a,dtype=None) returns a as a float array."""
-    dtype = _nx.obj2sctype(dtype)
-    if not issubclass(dtype, _nx.inexact):
+    """
+    Return an array converted to a float type.
+
+    Parameters
+    ----------
+    a : array_like
+        The input array.
+    dtype : str or dtype object, optional
+        Float type code to coerce input array `a`.  If `dtype` is one of the
+        'int' dtypes, it is replaced with float64.
+
+    Returns
+    -------
+    out : ndarray
+        The input `a` as a float ndarray.
+
+    Examples
+    --------
+    >>> np.asfarray([2, 3])
+    array([2.,  3.])
+    >>> np.asfarray([2, 3], dtype='float')
+    array([2.,  3.])
+    >>> np.asfarray([2, 3], dtype='int8')
+    array([2.,  3.])
+
+    """
+    if not _nx.issubdtype(dtype, _nx.inexact):
         dtype = _nx.float_
-    a = asarray(a,dtype=dtype)
-    return a
-
+    return asarray(a, dtype=dtype)
+
+
+def _real_dispatcher(val):
+    return (val,)
+
+
+@array_function_dispatch(_real_dispatcher)
 def real(val):
-    return asarray(val).real
-
+    """
+    Return the real part of the complex argument.
+
+    Parameters
+    ----------
+    val : array_like
+        Input array.
+
+    Returns
+    -------
+    out : ndarray or scalar
+        The real component of the complex argument. If `val` is real, the type
+        of `val` is used for the output.  If `val` has complex elements, the
+        returned type is float.
+
+    See Also
+    --------
+    real_if_close, imag, angle
+
+    Examples
+    --------
+    >>> a = np.array([1+2j, 3+4j, 5+6j])
+    >>> a.real
+    array([1.,  3.,  5.])
+    >>> a.real = 9
+    >>> a
+    array([9.+2.j,  9.+4.j,  9.+6.j])
+    >>> a.real = np.array([9, 8, 7])
+    >>> a
+    array([9.+2.j,  8.+4.j,  7.+6.j])
+    >>> np.real(1 + 1j)
+    1.0
+
+    """
+    try:
+        return val.real
+    except AttributeError:
+        return asanyarray(val).real
+
+
+def _imag_dispatcher(val):
+    return (val,)
+
+
+@array_function_dispatch(_imag_dispatcher)
 def imag(val):
-    return asarray(val).imag
-
+    """
+    Return the imaginary part of the complex argument.
+
+    Parameters
+    ----------
+    val : array_like
+        Input array.
+
+    Returns
+    -------
+    out : ndarray or scalar
+        The imaginary component of the complex argument. If `val` is real,
+        the type of `val` is used for the output.  If `val` has complex
+        elements, the returned type is float.
+
+    See Also
+    --------
+    real, angle, real_if_close
+
+    Examples
+    --------
+    >>> a = np.array([1+2j, 3+4j, 5+6j])
+    >>> a.imag
+    array([2.,  4.,  6.])
+    >>> a.imag = np.array([8, 10, 12])
+    >>> a
+    array([1. +8.j,  3.+10.j,  5.+12.j])
+    >>> np.imag(1 + 1j)
+    1.0
+
+    """
+    try:
+        return val.imag
+    except AttributeError:
+        return asanyarray(val).imag
+
+
+def _is_type_dispatcher(x):
+    return (x,)
+
+
+@array_function_dispatch(_is_type_dispatcher)
 def iscomplex(x):
-    return imag(x) != _nx.zeros_like(x)
-
+    """
+    Returns a bool array, where True if input element is complex.
+
+    What is tested is whether the input has a non-zero imaginary part, not if
+    the input type is complex.
+
+    Parameters
+    ----------
+    x : array_like
+        Input array.
+
+    Returns
+    -------
+    out : ndarray of bools
+        Output array.
+
+    See Also
+    --------
+    isreal
+    iscomplexobj : Return True if x is a complex type or an array of complex
+                   numbers.
+
+    Examples
+    --------
+    >>> np.iscomplex([1+1j, 1+0j, 4.5, 3, 2, 2j])
+    array([ True, False, False, False, False,  True])
+
+    """
+    ax = asanyarray(x)
+    if issubclass(ax.dtype.type, _nx.complexfloating):
+        return ax.imag != 0
+    res = zeros(ax.shape, bool)
+    return res[()]   # convert to scalar if needed
+
+
+@array_function_dispatch(_is_type_dispatcher)
 def isreal(x):
-    return imag(x) == _nx.zeros_like(x)
-
+    """
+    Returns a bool array, where True if input element is real.
+
+    If element has complex type with zero complex part, the return value
+    for that element is True.
+
+    Parameters
+    ----------
+    x : array_like
+        Input array.
+
+    Returns
+    -------
+    out : ndarray, bool
+        Boolean array of same shape as `x`.
+
+    Notes
+    -----
+    `isreal` may behave unexpectedly for string or object arrays (see examples)
+
+    See Also
+    --------
+    iscomplex
+    isrealobj : Return True if x is not a complex type.
+
+    Examples
+    --------
+    >>> a = np.array([1+1j, 1+0j, 4.5, 3, 2, 2j], dtype=complex)
+    >>> np.isreal(a)
+    array([False,  True,  True,  True,  True, False])
+
+    The function does not work on string arrays.
+
+    >>> a = np.array([2j, "a"], dtype="U")
+    >>> np.isreal(a)  # Warns about non-elementwise comparison
+    False
+
+    Returns True for all elements in input array of ``dtype=object`` even if
+    any of the elements is complex.
+
+    >>> a = np.array([1, "2", 3+4j], dtype=object)
+    >>> np.isreal(a)
+    array([ True,  True,  True])
+
+    isreal should not be used with object arrays
+
+    >>> a = np.array([1+2j, 2+1j], dtype=object)
+    >>> np.isreal(a)
+    array([ True,  True])
+
+    """
+    return imag(x) == 0
+
+
+@array_function_dispatch(_is_type_dispatcher)
 def iscomplexobj(x):
-    return issubclass( asarray(x).dtype.type, _nx.complexfloating)
-
+    """
+    Check for a complex type or an array of complex numbers.
+
+    The type of the input is checked, not the value. Even if the input
+    has an imaginary part equal to zero, `iscomplexobj` evaluates to True.
+
+    Parameters
+    ----------
+    x : any
+        The input can be of any type and shape.
+
+    Returns
+    -------
+    iscomplexobj : bool
+        The return value, True if `x` is of a complex type or has at least
+        one complex element.
+
+    See Also
+    --------
+    isrealobj, iscomplex
+
+    Examples
+    --------
+    >>> np.iscomplexobj(1)
+    False
+    >>> np.iscomplexobj(1+0j)
+    True
+    >>> np.iscomplexobj([3, 1+0j, True])
+    True
+
+    """
+    try:
+        dtype = x.dtype
+        type_ = dtype.type
+    except AttributeError:
+        type_ = asarray(x).dtype.type
+    return issubclass(type_, _nx.complexfloating)
+
+
+@array_function_dispatch(_is_type_dispatcher)
 def isrealobj(x):
-    return not issubclass( asarray(x).dtype.type, _nx.complexfloating)
+    """
+    Return True if x is a not complex type or an array of complex numbers.
+
+    The type of the input is checked, not the value. So even if the input
+    has an imaginary part equal to zero, `isrealobj` evaluates to False
+    if the data type is complex.
+
+    Parameters
+    ----------
+    x : any
+        The input can be of any type and shape.
+
+    Returns
+    -------
+    y : bool
+        The return value, False if `x` is of a complex type.
+
+    See Also
+    --------
+    iscomplexobj, isreal
+
+    Notes
+    -----
+    The function is only meant for arrays with numerical values but it
+    accepts all other objects. Since it assumes array input, the return
+    value of other objects may be True.
+
+    >>> np.isrealobj('A string')
+    True
+    >>> np.isrealobj(False)
+    True
+    >>> np.isrealobj(None)
+    True
+
+    Examples
+    --------
+    >>> np.isrealobj(1)
+    True
+    >>> np.isrealobj(1+0j)
+    False
+    >>> np.isrealobj([3, 1+0j, True])
+    False
+
+    """
+    return not iscomplexobj(x)
 
 #-----------------------------------------------------------------------------
 
 def _getmaxmin(t):
-    import getlimits
+    from numpy.core import getlimits
     f = getlimits.finfo(t)
     return f.max, f.min
 
-def nan_to_num(x):
-    # mapping:
-    #    NaN -> 0
-    #    Inf -> limits.double_max
-    #   -Inf -> limits.double_min
-    try:
-        t = x.dtype.type
-    except AttributeError:
-        t = obj2sctype(type(x))
-    if issubclass(t, _nx.complexfloating):
-        y = nan_to_num(x.real) + 1j * nan_to_num(x.imag)
-    elif issubclass(t, _nx.integer):
-        y = array(x)
-    else:
-        y = array(x)
-        if not y.shape:
-            y = array([x])
-            scalar = True
-        else:
-            scalar = False
-        are_inf = isposinf(y)
-        are_neg_inf = isneginf(y)
-        are_nan = isnan(y)
-        maxf, minf = _getmaxmin(y.dtype.type)
-        y[are_nan] = 0
-        y[are_inf] = maxf
-        y[are_neg_inf] = minf
-        if scalar:
-            y = y[0]
-    return y
+
+def _nan_to_num_dispatcher(x, copy=None, nan=None, posinf=None, neginf=None):
+    return (x,)
+
+
+@array_function_dispatch(_nan_to_num_dispatcher)
+def nan_to_num(x, copy=True, nan=0.0, posinf=None, neginf=None):
+    """
+    Replace NaN with zero and infinity with large finite numbers (default
+    behaviour) or with the numbers defined by the user using the `nan`,
+    `posinf` and/or `neginf` keywords.
+
+    If `x` is inexact, NaN is replaced by zero or by the user defined value in
+    `nan` keyword, infinity is replaced by the largest finite floating point
+    values representable by ``x.dtype`` or by the user defined value in
+    `posinf` keyword and -infinity is replaced by the most negative finite
+    floating point values representable by ``x.dtype`` or by the user defined
+    value in `neginf` keyword.
+
+    For complex dtypes, the above is applied to each of the real and
+    imaginary components of `x` separately.
+
+    If `x` is not inexact, then no replacements are made.
+
+    Parameters
+    ----------
+    x : scalar or array_like
+        Input data.
+    copy : bool, optional
+        Whether to create a copy of `x` (True) or to replace values
+        in-place (False). The in-place operation only occurs if
+        casting to an array does not require a copy.
+        Default is True.
+
+        .. versionadded:: 1.13
+    nan : int, float, optional
+        Value to be used to fill NaN values. If no value is passed
+        then NaN values will be replaced with 0.0.
+
+        .. versionadded:: 1.17
+    posinf : int, float, optional
+        Value to be used to fill positive infinity values. If no value is
+        passed then positive infinity values will be replaced with a very
+        large number.
+
+        .. versionadded:: 1.17
+    neginf : int, float, optional
+        Value to be used to fill negative infinity values. If no value is
+        passed then negative infinity values will be replaced with a very
+        small (or negative) number.
+
+        .. versionadded:: 1.17
+
+
+
+    Returns
+    -------
+    out : ndarray
+        `x`, with the non-finite values replaced. If `copy` is False, this may
+        be `x` itself.
+
+    See Also
+    --------
+    isinf : Shows which elements are positive or negative infinity.
+    isneginf : Shows which elements are negative infinity.
+    isposinf : Shows which elements are positive infinity.
+    isnan : Shows which elements are Not a Number (NaN).
+    isfinite : Shows which elements are finite (not NaN, not infinity)
+
+    Notes
+    -----
+    NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic
+    (IEEE 754). This means that Not a Number is not equivalent to infinity.
+
+    Examples
+    --------
+    >>> np.nan_to_num(np.inf)
+    1.7976931348623157e+308
+    >>> np.nan_to_num(-np.inf)
+    -1.7976931348623157e+308
+    >>> np.nan_to_num(np.nan)
+    0.0
+    >>> x = np.array([np.inf, -np.inf, np.nan, -128, 128])
+    >>> np.nan_to_num(x)
+    array([ 1.79769313e+308, -1.79769313e+308,  0.00000000e+000, # may vary
+           -1.28000000e+002,  1.28000000e+002])
+    >>> np.nan_to_num(x, nan=-9999, posinf=33333333, neginf=33333333)
+    array([ 3.3333333e+07,  3.3333333e+07, -9.9990000e+03,
+           -1.2800000e+02,  1.2800000e+02])
+    >>> y = np.array([complex(np.inf, np.nan), np.nan, complex(np.nan, np.inf)])
+    array([  1.79769313e+308,  -1.79769313e+308,   0.00000000e+000, # may vary
+         -1.28000000e+002,   1.28000000e+002])
+    >>> np.nan_to_num(y)
+    array([  1.79769313e+308 +0.00000000e+000j, # may vary
+             0.00000000e+000 +0.00000000e+000j,
+             0.00000000e+000 +1.79769313e+308j])
+    >>> np.nan_to_num(y, nan=111111, posinf=222222)
+    array([222222.+111111.j, 111111.     +0.j, 111111.+222222.j])
+    """
+    x = _nx.array(x, subok=True, copy=copy)
+    xtype = x.dtype.type
+
+    isscalar = (x.ndim == 0)
+
+    if not issubclass(xtype, _nx.inexact):
+        return x[()] if isscalar else x
+
+    iscomplex = issubclass(xtype, _nx.complexfloating)
+
+    dest = (x.real, x.imag) if iscomplex else (x,)
+    maxf, minf = _getmaxmin(x.real.dtype)
+    if posinf is not None:
+        maxf = posinf
+    if neginf is not None:
+        minf = neginf
+    for d in dest:
+        idx_nan = isnan(d)
+        idx_posinf = isposinf(d)
+        idx_neginf = isneginf(d)
+        _nx.copyto(d, nan, where=idx_nan)
+        _nx.copyto(d, maxf, where=idx_posinf)
+        _nx.copyto(d, minf, where=idx_neginf)
+    return x[()] if isscalar else x
 
 #-----------------------------------------------------------------------------
 
-def real_if_close(a,tol=100):
-    a = asarray(a)
-    if a.dtype.char not in 'FDG':
+def _real_if_close_dispatcher(a, tol=None):
+    return (a,)
+
+
+@array_function_dispatch(_real_if_close_dispatcher)
+def real_if_close(a, tol=100):
+    """
+    If input is complex with all imaginary parts close to zero, return
+    real parts.
+
+    "Close to zero" is defined as `tol` * (machine epsilon of the type for
+    `a`).
+
+    Parameters
+    ----------
+    a : array_like
+        Input array.
+    tol : float
+        Tolerance in machine epsilons for the complex part of the elements
+        in the array.
+
+    Returns
+    -------
+    out : ndarray
+        If `a` is real, the type of `a` is used for the output.  If `a`
+        has complex elements, the returned type is float.
+
+    See Also
+    --------
+    real, imag, angle
+
+    Notes
+    -----
+    Machine epsilon varies from machine to machine and between data types
+    but Python floats on most platforms have a machine epsilon equal to
+    2.2204460492503131e-16.  You can use 'np.finfo(float).eps' to print
+    out the machine epsilon for floats.
+
+    Examples
+    --------
+    >>> np.finfo(float).eps
+    2.2204460492503131e-16 # may vary
+
+    >>> np.real_if_close([2.1 + 4e-14j, 5.2 + 3e-15j], tol=1000)
+    array([2.1, 5.2])
+    >>> np.real_if_close([2.1 + 4e-13j, 5.2 + 3e-15j], tol=1000)
+    array([2.1+4.e-13j, 5.2 + 3e-15j])
+
+    """
+    a = asanyarray(a)
+    if not issubclass(a.dtype.type, _nx.complexfloating):
         return a
     if tol > 1:
-        import getlimits
+        from numpy.core import getlimits
         f = getlimits.finfo(a.dtype.type)
         tol = f.eps * tol
-    if _nx.allclose(a.imag, 0, atol=tol):
+    if _nx.all(_nx.absolute(a.imag) < tol):
         a = a.real
     return a
 
 
-def asscalar(a):
-    return a.item()
-
 #-----------------------------------------------------------------------------
 
-_namefromtype = {'S1' : 'character',
-                 '?' : 'bool',
-                 'b' : 'signed char',
-                 'B' : 'unsigned char',
-                 'h' : 'short',
-                 'H' : 'unsigned short',
-                 'i' : 'integer',
-                 'I' : 'unsigned integer',
-                 'l' : 'long integer',
-                 'L' : 'unsigned long integer',
-                 'q' : 'long long integer',
-                 'Q' : 'unsigned long long integer',
-                 'f' : 'single precision',
-                 'd' : 'double precision',
-                 'g' : 'long precision',
-                 'F' : 'complex single precision',
-                 'D' : 'complex double precision',
-                 'G' : 'complex long double precision',
-                 'S' : 'string',
-                 'U' : 'unicode',
-                 'V' : 'void',
-                 'O' : 'object'
+_namefromtype = {'S1': 'character',
+                 '?': 'bool',
+                 'b': 'signed char',
+                 'B': 'unsigned char',
+                 'h': 'short',
+                 'H': 'unsigned short',
+                 'i': 'integer',
+                 'I': 'unsigned integer',
+                 'l': 'long integer',
+                 'L': 'unsigned long integer',
+                 'q': 'long long integer',
+                 'Q': 'unsigned long long integer',
+                 'f': 'single precision',
+                 'd': 'double precision',
+                 'g': 'long precision',
+                 'F': 'complex single precision',
+                 'D': 'complex double precision',
+                 'G': 'complex long double precision',
+                 'S': 'string',
+                 'U': 'unicode',
+                 'V': 'void',
+                 'O': 'object'
                  }
 
+@set_module('numpy')
 def typename(char):
-    """Return an english description for the given data type character.
+    """
+    Return a description for the given data type code.
+
+    Parameters
+    ----------
+    char : str
+        Data type code.
+
+    Returns
+    -------
+    out : str
+        Description of the input data type code.
+
+    See Also
+    --------
+    dtype, typecodes
+
+    Examples
+    --------
+    >>> typechars = ['S1', '?', 'B', 'D', 'G', 'F', 'I', 'H', 'L', 'O', 'Q',
+    ...              'S', 'U', 'V', 'b', 'd', 'g', 'f', 'i', 'h', 'l', 'q']
+    >>> for typechar in typechars:
+    ...     print(typechar, ' : ', np.typename(typechar))
+    ...
+    S1  :  character
+    ?  :  bool
+    B  :  unsigned char
+    D  :  complex double precision
+    G  :  complex long double precision
+    F  :  complex single precision
+    I  :  unsigned integer
+    H  :  unsigned short
+    L  :  unsigned long integer
+    O  :  object
+    Q  :  unsigned long long integer
+    S  :  string
+    U  :  unicode
+    V  :  void
+    b  :  signed char
+    d  :  double precision
+    g  :  long precision
+    f  :  single precision
+    i  :  integer
+    h  :  short
+    l  :  long integer
+    q  :  long long integer
+
     """
     return _namefromtype[char]
 
 #-----------------------------------------------------------------------------
 
-#determine the "minimum common type code" for a group of arrays.
-array_kind = {'i':0, 'l': 0, 'f': 0, 'd': 0, 'g':0, 'F': 1, 'D': 1, 'G':1}
-array_precision = {'i': 1, 'l': 1,
-                   'f': 0, 'd': 1, 'g':2,
-                   'F': 0, 'D': 1, 'G':2}
-array_type = [['f', 'd', 'g'], ['F', 'D', 'G']]
+#determine the "minimum common type" for a group of arrays.
+array_type = [[_nx.half, _nx.single, _nx.double, _nx.longdouble],
+              [None, _nx.csingle, _nx.cdouble, _nx.clongdouble]]
+array_precision = {_nx.half: 0,
+                   _nx.single: 1,
+                   _nx.double: 2,
+                   _nx.longdouble: 3,
+                   _nx.csingle: 1,
+                   _nx.cdouble: 2,
+                   _nx.clongdouble: 3}
+
+
+def _common_type_dispatcher(*arrays):
+    return arrays
+
+
+@array_function_dispatch(_common_type_dispatcher)
 def common_type(*arrays):
-    kind = 0
+    """
+    Return a scalar type which is common to the input arrays.
+
+    The return type will always be an inexact (i.e. floating point) scalar
+    type, even if all the arrays are integer arrays. If one of the inputs is
+    an integer array, the minimum precision type that is returned is a
+    64-bit floating point dtype.
+
+    All input arrays except int64 and uint64 can be safely cast to the
+    returned dtype without loss of information.
+
+    Parameters
+    ----------
+    array1, array2, ... : ndarrays
+        Input arrays.
+
+    Returns
+    -------
+    out : data type code
+        Data type code.
+
+    See Also
+    --------
+    dtype, mintypecode
+
+    Examples
+    --------
+    >>> np.common_type(np.arange(2, dtype=np.float32))
+    <class 'numpy.float32'>
+    >>> np.common_type(np.arange(2, dtype=np.float32), np.arange(2))
+    <class 'numpy.float64'>
+    >>> np.common_type(np.arange(4), np.array([45, 6.j]), np.array([45.0]))
+    <class 'numpy.complex128'>
+
+    """
+    is_complex = False
     precision = 0
     for a in arrays:
-        t = a.dtype.char
-        kind = max(kind, array_kind[t])
-        precision = max(precision, array_precision[t])
-    return array_type[kind][precision]
+        t = a.dtype.type
+        if iscomplexobj(a):
+            is_complex = True
+        if issubclass(t, _nx.integer):
+            p = 2  # array_precision[_nx.double]
+        else:
+            p = array_precision.get(t, None)
+            if p is None:
+                raise TypeError("can't get common type for non-numeric array")
+        precision = max(precision, p)
+    if is_complex:
+        return array_type[1][precision]
+    else:
+        return array_type[0][precision]
('numpy/lib', 'polynomial.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,96 +1,240 @@
 """
 Functions to operate on polynomials.
+
 """
-
 __all__ = ['poly', 'roots', 'polyint', 'polyder', 'polyadd',
            'polysub', 'polymul', 'polydiv', 'polyval', 'poly1d',
-           'polyfit']
-
+           'polyfit', 'RankWarning']
+
+import functools
 import re
+import warnings
 import numpy.core.numeric as NX
 
-from numpy.core import isscalar
+from numpy.core import (isscalar, abs, finfo, atleast_1d, hstack, dot, array,
+                        ones)
+from numpy.core import overrides
+from numpy.core.overrides import set_module
 from numpy.lib.twodim_base import diag, vander
-from numpy.lib.shape_base import hstack, atleast_1d
-from numpy.lib.function_base import trim_zeros, sort_complex
-eigvals = None
-lstsq = None
-
-def get_linalg_funcs():
-    "Look for linear algebra functions in numpy"
-    global eigvals, lstsq
-    from numpy.dual import eigvals, lstsq
-    return
-
-def _eigvals(arg):
-    "Return the eigenvalues of the argument"
-    try:
-        return eigvals(arg)
-    except TypeError:
-        get_linalg_funcs()
-        return eigvals(arg)
-
-def _lstsq(X, y):
-    "Do least squares on the arguments"
-    try:
-        return lstsq(X, y)
-    except TypeError:
-        get_linalg_funcs()
-        return lstsq(X, y)
-
+from numpy.lib.function_base import trim_zeros
+from numpy.lib.type_check import iscomplex, real, imag, mintypecode
+from numpy.linalg import eigvals, lstsq, inv
+
+
+array_function_dispatch = functools.partial(
+    overrides.array_function_dispatch, module='numpy')
+
+
+@set_module('numpy')
+class RankWarning(UserWarning):
+    """
+    Issued by `polyfit` when the Vandermonde matrix is rank deficient.
+
+    For more information, a way to suppress the warning, and an example of
+    `RankWarning` being issued, see `polyfit`.
+
+    """
+    pass
+
+
+def _poly_dispatcher(seq_of_zeros):
+    return seq_of_zeros
+
+
+@array_function_dispatch(_poly_dispatcher)
 def poly(seq_of_zeros):
-    """ Return a sequence representing a polynomial given a sequence of roots.
-
-        If the input is a matrix, return the characteristic polynomial.
-
-        Example:
-
-         >>> b = roots([1,3,1,5,6])
-         >>> poly(b)
-         array([1., 3., 1., 5., 6.])
+    """
+    Find the coefficients of a polynomial with the given sequence of roots.
+
+    .. note::
+       This forms part of the old polynomial API. Since version 1.4, the
+       new polynomial API defined in `numpy.polynomial` is preferred.
+       A summary of the differences can be found in the
+       :doc:`transition guide </reference/routines.polynomials>`.
+
+    Returns the coefficients of the polynomial whose leading coefficient
+    is one for the given sequence of zeros (multiple roots must be included
+    in the sequence as many times as their multiplicity; see Examples).
+    A square matrix (or array, which will be treated as a matrix) can also
+    be given, in which case the coefficients of the characteristic polynomial
+    of the matrix are returned.
+
+    Parameters
+    ----------
+    seq_of_zeros : array_like, shape (N,) or (N, N)
+        A sequence of polynomial roots, or a square array or matrix object.
+
+    Returns
+    -------
+    c : ndarray
+        1D array of polynomial coefficients from highest to lowest degree:
+
+        ``c[0] * x**(N) + c[1] * x**(N-1) + ... + c[N-1] * x + c[N]``
+        where c[0] always equals 1.
+
+    Raises
+    ------
+    ValueError
+        If input is the wrong shape (the input must be a 1-D or square
+        2-D array).
+
+    See Also
+    --------
+    polyval : Compute polynomial values.
+    roots : Return the roots of a polynomial.
+    polyfit : Least squares polynomial fit.
+    poly1d : A one-dimensional polynomial class.
+
+    Notes
+    -----
+    Specifying the roots of a polynomial still leaves one degree of
+    freedom, typically represented by an undetermined leading
+    coefficient. [1]_ In the case of this function, that coefficient -
+    the first one in the returned array - is always taken as one. (If
+    for some reason you have one other point, the only automatic way
+    presently to leverage that information is to use ``polyfit``.)
+
+    The characteristic polynomial, :math:`p_a(t)`, of an `n`-by-`n`
+    matrix **A** is given by
+
+        :math:`p_a(t) = \\mathrm{det}(t\\, \\mathbf{I} - \\mathbf{A})`,
+
+    where **I** is the `n`-by-`n` identity matrix. [2]_
+
+    References
+    ----------
+    .. [1] M. Sullivan and M. Sullivan, III, "Algebra and Trignometry,
+       Enhanced With Graphing Utilities," Prentice-Hall, pg. 318, 1996.
+
+    .. [2] G. Strang, "Linear Algebra and Its Applications, 2nd Edition,"
+       Academic Press, pg. 182, 1980.
+
+    Examples
+    --------
+    Given a sequence of a polynomial's zeros:
+
+    >>> np.poly((0, 0, 0)) # Multiple root example
+    array([1., 0., 0., 0.])
+
+    The line above represents z**3 + 0*z**2 + 0*z + 0.
+
+    >>> np.poly((-1./2, 0, 1./2))
+    array([ 1.  ,  0.  , -0.25,  0.  ])
+
+    The line above represents z**3 - z/4
+
+    >>> np.poly((np.random.random(1)[0], 0, np.random.random(1)[0]))
+    array([ 1.        , -0.77086955,  0.08618131,  0.        ]) # random
+
+    Given a square array object:
+
+    >>> P = np.array([[0, 1./3], [-1./2, 0]])
+    >>> np.poly(P)
+    array([1.        , 0.        , 0.16666667])
+
+    Note how in all cases the leading coefficient is always 1.
+
     """
     seq_of_zeros = atleast_1d(seq_of_zeros)
     sh = seq_of_zeros.shape
-    if len(sh) == 2 and sh[0] == sh[1]:
-        seq_of_zeros = _eigvals(seq_of_zeros)
-    elif len(sh) ==1:
-        pass
+
+    if len(sh) == 2 and sh[0] == sh[1] and sh[0] != 0:
+        seq_of_zeros = eigvals(seq_of_zeros)
+    elif len(sh) == 1:
+        dt = seq_of_zeros.dtype
+        # Let object arrays slip through, e.g. for arbitrary precision
+        if dt != object:
+            seq_of_zeros = seq_of_zeros.astype(mintypecode(dt.char))
     else:
-        raise ValueError, "input must be 1d or square 2d array."
+        raise ValueError("input must be 1d or non-empty square 2d array.")
 
     if len(seq_of_zeros) == 0:
         return 1.0
-
-    a = [1]
-    for k in range(len(seq_of_zeros)):
-        a = NX.convolve(a, [1, -seq_of_zeros[k]], mode='full')
+    dt = seq_of_zeros.dtype
+    a = ones((1,), dtype=dt)
+    for zero in seq_of_zeros:
+        a = NX.convolve(a, array([1, -zero], dtype=dt), mode='full')
 
     if issubclass(a.dtype.type, NX.complexfloating):
         # if complex roots are all complex conjugates, the roots are real.
         roots = NX.asarray(seq_of_zeros, complex)
-        pos_roots = sort_complex(NX.compress(roots.imag > 0, roots))
-        neg_roots = NX.conjugate(sort_complex(
-                                        NX.compress(roots.imag < 0,roots)))
-        if (len(pos_roots) == len(neg_roots) and
-            NX.alltrue(neg_roots == pos_roots)):
+        if NX.all(NX.sort(roots) == NX.sort(roots.conjugate())):
             a = a.real.copy()
 
     return a
 
+
+def _roots_dispatcher(p):
+    return p
+
+
+@array_function_dispatch(_roots_dispatcher)
 def roots(p):
-    """ Return the roots of the polynomial coefficients in p.
-
-        The values in the rank-1 array p are coefficients of a polynomial.
-        If the length of p is n+1 then the polynomial is
-        p[0] * x**n + p[1] * x**(n-1) + ... + p[n-1]*x + p[n]
+    """
+    Return the roots of a polynomial with coefficients given in p.
+
+    .. note::
+       This forms part of the old polynomial API. Since version 1.4, the
+       new polynomial API defined in `numpy.polynomial` is preferred.
+       A summary of the differences can be found in the
+       :doc:`transition guide </reference/routines.polynomials>`.
+
+    The values in the rank-1 array `p` are coefficients of a polynomial.
+    If the length of `p` is n+1 then the polynomial is described by::
+
+      p[0] * x**n + p[1] * x**(n-1) + ... + p[n-1]*x + p[n]
+
+    Parameters
+    ----------
+    p : array_like
+        Rank-1 array of polynomial coefficients.
+
+    Returns
+    -------
+    out : ndarray
+        An array containing the roots of the polynomial.
+
+    Raises
+    ------
+    ValueError
+        When `p` cannot be converted to a rank-1 array.
+
+    See also
+    --------
+    poly : Find the coefficients of a polynomial with a given sequence
+           of roots.
+    polyval : Compute polynomial values.
+    polyfit : Least squares polynomial fit.
+    poly1d : A one-dimensional polynomial class.
+
+    Notes
+    -----
+    The algorithm relies on computing the eigenvalues of the
+    companion matrix [1]_.
+
+    References
+    ----------
+    .. [1] R. A. Horn & C. R. Johnson, *Matrix Analysis*.  Cambridge, UK:
+        Cambridge University Press, 1999, pp. 146-7.
+
+    Examples
+    --------
+    >>> coeff = [3.2, 2, 1]
+    >>> np.roots(coeff)
+    array([-0.3125+0.46351241j, -0.3125-0.46351241j])
+
     """
     # If input is scalar, this makes it an array
     p = atleast_1d(p)
-    if len(p.shape) != 1:
-        raise ValueError,"Input must be a rank-1 array."
+    if p.ndim != 1:
+        raise ValueError("Input must be a rank-1 array.")
 
     # find non-zero array entries
-    non_zero = NX.nonzero(NX.ravel(p))
+    non_zero = NX.nonzero(NX.ravel(p))[0]
+
+    # Return an empty array if polynomial is all zeros
+    if len(non_zero) == 0:
+        return NX.array([])
 
     # find the number of trailing zeros -- this is the number of roots at 0.
     trailing_zeros = len(p) - non_zero[-1] - 1
@@ -106,145 +250,597 @@
     if N > 1:
         # build companion matrix and find its eigenvalues (the roots)
         A = diag(NX.ones((N-2,), p.dtype), -1)
-        A[0, :] = -p[1:] / p[0]
-        roots = _eigvals(A)
+        A[0,:] = -p[1:] / p[0]
+        roots = eigvals(A)
     else:
-        return NX.array([])
+        roots = NX.array([])
 
     # tack any zeros onto the back of the array
     roots = hstack((roots, NX.zeros(trailing_zeros, roots.dtype)))
     return roots
 
+
+def _polyint_dispatcher(p, m=None, k=None):
+    return (p,)
+
+
+@array_function_dispatch(_polyint_dispatcher)
 def polyint(p, m=1, k=None):
-    """Return the mth analytical integral of the polynomial p.
-
-    If k is None, then zero-valued constants of integration are used.
-    otherwise, k should be a list of length m (or a scalar if m=1) to
-    represent the constants of integration to use for each integration
-    (starting with k[0])
+    """
+    Return an antiderivative (indefinite integral) of a polynomial.
+
+    .. note::
+       This forms part of the old polynomial API. Since version 1.4, the
+       new polynomial API defined in `numpy.polynomial` is preferred.
+       A summary of the differences can be found in the
+       :doc:`transition guide </reference/routines.polynomials>`.
+
+    The returned order `m` antiderivative `P` of polynomial `p` satisfies
+    :math:`\\frac{d^m}{dx^m}P(x) = p(x)` and is defined up to `m - 1`
+    integration constants `k`. The constants determine the low-order
+    polynomial part
+
+    .. math:: \\frac{k_{m-1}}{0!} x^0 + \\ldots + \\frac{k_0}{(m-1)!}x^{m-1}
+
+    of `P` so that :math:`P^{(j)}(0) = k_{m-j-1}`.
+
+    Parameters
+    ----------
+    p : array_like or poly1d
+        Polynomial to integrate.
+        A sequence is interpreted as polynomial coefficients, see `poly1d`.
+    m : int, optional
+        Order of the antiderivative. (Default: 1)
+    k : list of `m` scalars or scalar, optional
+        Integration constants. They are given in the order of integration:
+        those corresponding to highest-order terms come first.
+
+        If ``None`` (default), all constants are assumed to be zero.
+        If `m = 1`, a single scalar can be given instead of a list.
+
+    See Also
+    --------
+    polyder : derivative of a polynomial
+    poly1d.integ : equivalent method
+
+    Examples
+    --------
+    The defining property of the antiderivative:
+
+    >>> p = np.poly1d([1,1,1])
+    >>> P = np.polyint(p)
+    >>> P
+     poly1d([ 0.33333333,  0.5       ,  1.        ,  0.        ]) # may vary
+    >>> np.polyder(P) == p
+    True
+
+    The integration constants default to zero, but can be specified:
+
+    >>> P = np.polyint(p, 3)
+    >>> P(0)
+    0.0
+    >>> np.polyder(P)(0)
+    0.0
+    >>> np.polyder(P, 2)(0)
+    0.0
+    >>> P = np.polyint(p, 3, k=[6,5,3])
+    >>> P
+    poly1d([ 0.01666667,  0.04166667,  0.16666667,  3. ,  5. ,  3. ]) # may vary
+
+    Note that 3 = 6 / 2!, and that the constants are given in the order of
+    integrations. Constant of the highest-order polynomial term comes first:
+
+    >>> np.polyder(P, 2)(0)
+    6.0
+    >>> np.polyder(P, 1)(0)
+    5.0
+    >>> P(0)
+    3.0
+
     """
     m = int(m)
     if m < 0:
-        raise ValueError, "Order of integral must be positive (see polyder)"
+        raise ValueError("Order of integral must be positive (see polyder)")
     if k is None:
         k = NX.zeros(m, float)
     k = atleast_1d(k)
     if len(k) == 1 and m > 1:
         k = k[0]*NX.ones(m, float)
     if len(k) < m:
-        raise ValueError, \
-              "k must be a scalar or a rank-1 array of length 1 or >m."
+        raise ValueError(
+              "k must be a scalar or a rank-1 array of length 1 or >m.")
+
+    truepoly = isinstance(p, poly1d)
+    p = NX.asarray(p)
     if m == 0:
+        if truepoly:
+            return poly1d(p)
         return p
     else:
-        truepoly = isinstance(p, poly1d)
-        p = NX.asarray(p)
-        y = NX.zeros(len(p)+1, float)
-        y[:-1] = p*1.0/NX.arange(len(p), 0, -1)
-        y[-1] = k[0]
-        val = polyint(y, m-1, k=k[1:])
+        # Note: this must work also with object and integer arrays
+        y = NX.concatenate((p.__truediv__(NX.arange(len(p), 0, -1)), [k[0]]))
+        val = polyint(y, m - 1, k=k[1:])
         if truepoly:
-            val = poly1d(val)
+            return poly1d(val)
         return val
 
+
+def _polyder_dispatcher(p, m=None):
+    return (p,)
+
+
+@array_function_dispatch(_polyder_dispatcher)
 def polyder(p, m=1):
-    """Return the mth derivative of the polynomial p.
+    """
+    Return the derivative of the specified order of a polynomial.
+
+    .. note::
+       This forms part of the old polynomial API. Since version 1.4, the
+       new polynomial API defined in `numpy.polynomial` is preferred.
+       A summary of the differences can be found in the
+       :doc:`transition guide </reference/routines.polynomials>`.
+
+    Parameters
+    ----------
+    p : poly1d or sequence
+        Polynomial to differentiate.
+        A sequence is interpreted as polynomial coefficients, see `poly1d`.
+    m : int, optional
+        Order of differentiation (default: 1)
+
+    Returns
+    -------
+    der : poly1d
+        A new polynomial representing the derivative.
+
+    See Also
+    --------
+    polyint : Anti-derivative of a polynomial.
+    poly1d : Class for one-dimensional polynomials.
+
+    Examples
+    --------
+    The derivative of the polynomial :math:`x^3 + x^2 + x^1 + 1` is:
+
+    >>> p = np.poly1d([1,1,1,1])
+    >>> p2 = np.polyder(p)
+    >>> p2
+    poly1d([3, 2, 1])
+
+    which evaluates to:
+
+    >>> p2(2.)
+    17.0
+
+    We can verify this, approximating the derivative with
+    ``(f(x + h) - f(x))/h``:
+
+    >>> (p(2. + 0.001) - p(2.)) / 0.001
+    17.007000999997857
+
+    The fourth-order derivative of a 3rd-order polynomial is zero:
+
+    >>> np.polyder(p, 2)
+    poly1d([6, 2])
+    >>> np.polyder(p, 3)
+    poly1d([6])
+    >>> np.polyder(p, 4)
+    poly1d([0])
+
     """
     m = int(m)
+    if m < 0:
+        raise ValueError("Order of derivative must be positive (see polyint)")
+
     truepoly = isinstance(p, poly1d)
     p = NX.asarray(p)
-    n = len(p)-1
+    n = len(p) - 1
     y = p[:-1] * NX.arange(n, 0, -1)
-    if m < 0:
-        raise ValueError, "Order of derivative must be positive (see polyint)"
     if m == 0:
-        return p
+        val = p
     else:
-        val = polyder(y, m-1)
-        if truepoly:
-            val = poly1d(val)
-        return val
-
-def polyfit(x, y, N):
-    """
-
-    Do a best fit polynomial of order N of y to x.  Return value is a
-    vector of polynomial coefficients [pk ... p1 p0].  Eg, for N=2
-
-      p2*x0^2 +  p1*x0 + p0 = y1
-      p2*x1^2 +  p1*x1 + p0 = y1
-      p2*x2^2 +  p1*x2 + p0 = y2
-      .....
-      p2*xk^2 +  p1*xk + p0 = yk
-
-
-    Method: if X is a the Vandermonde Matrix computed from x (see
-    http://mathworld.wolfram.com/VandermondeMatrix.html), then the
-    polynomial least squares solution is given by the 'p' in
-
-      X*p = y
-
-    where X is a len(x) x N+1 matrix, p is a N+1 length vector, and y
-    is a len(x) x 1 vector
-
-    This equation can be solved as
-
-      p = (XT*X)^-1 * XT * y
-
-    where XT is the transpose of X and -1 denotes the inverse.
-
-    For more info, see
-    http://mathworld.wolfram.com/LeastSquaresFittingPolynomial.html,
-    but note that the k's and n's in the superscripts and subscripts
-    on that page.  The linear algebra is correct, however.
-
-    See also polyval
-
-    """
-    x = NX.asarray(x)+0.
-    y = NX.asarray(y)+0.
-    y = NX.reshape(y, (len(y), 1))
-    X = vander(x, N+1)
-    c, resids, rank, s = _lstsq(X, y)
-    c.shape = (N+1,)
-    return c
-
-
-
+        val = polyder(y, m - 1)
+    if truepoly:
+        val = poly1d(val)
+    return val
+
+
+def _polyfit_dispatcher(x, y, deg, rcond=None, full=None, w=None, cov=None):
+    return (x, y, w)
+
+
+@array_function_dispatch(_polyfit_dispatcher)
+def polyfit(x, y, deg, rcond=None, full=False, w=None, cov=False):
+    """
+    Least squares polynomial fit.
+
+    .. note::
+       This forms part of the old polynomial API. Since version 1.4, the
+       new polynomial API defined in `numpy.polynomial` is preferred.
+       A summary of the differences can be found in the
+       :doc:`transition guide </reference/routines.polynomials>`.
+
+    Fit a polynomial ``p(x) = p[0] * x**deg + ... + p[deg]`` of degree `deg`
+    to points `(x, y)`. Returns a vector of coefficients `p` that minimises
+    the squared error in the order `deg`, `deg-1`, ... `0`.
+
+    The `Polynomial.fit <numpy.polynomial.polynomial.Polynomial.fit>` class
+    method is recommended for new code as it is more stable numerically. See
+    the documentation of the method for more information.
+
+    Parameters
+    ----------
+    x : array_like, shape (M,)
+        x-coordinates of the M sample points ``(x[i], y[i])``.
+    y : array_like, shape (M,) or (M, K)
+        y-coordinates of the sample points. Several data sets of sample
+        points sharing the same x-coordinates can be fitted at once by
+        passing in a 2D-array that contains one dataset per column.
+    deg : int
+        Degree of the fitting polynomial
+    rcond : float, optional
+        Relative condition number of the fit. Singular values smaller than
+        this relative to the largest singular value will be ignored. The
+        default value is len(x)*eps, where eps is the relative precision of
+        the float type, about 2e-16 in most cases.
+    full : bool, optional
+        Switch determining nature of return value. When it is False (the
+        default) just the coefficients are returned, when True diagnostic
+        information from the singular value decomposition is also returned.
+    w : array_like, shape (M,), optional
+        Weights. If not None, the weight ``w[i]`` applies to the unsquared
+        residual ``y[i] - y_hat[i]`` at ``x[i]``. Ideally the weights are
+        chosen so that the errors of the products ``w[i]*y[i]`` all have the
+        same variance.  When using inverse-variance weighting, use
+        ``w[i] = 1/sigma(y[i])``.  The default value is None.
+    cov : bool or str, optional
+        If given and not `False`, return not just the estimate but also its
+        covariance matrix. By default, the covariance are scaled by
+        chi2/dof, where dof = M - (deg + 1), i.e., the weights are presumed
+        to be unreliable except in a relative sense and everything is scaled
+        such that the reduced chi2 is unity. This scaling is omitted if
+        ``cov='unscaled'``, as is relevant for the case that the weights are
+        w = 1/sigma, with sigma known to be a reliable estimate of the
+        uncertainty.
+
+    Returns
+    -------
+    p : ndarray, shape (deg + 1,) or (deg + 1, K)
+        Polynomial coefficients, highest power first.  If `y` was 2-D, the
+        coefficients for `k`-th data set are in ``p[:,k]``.
+
+    residuals, rank, singular_values, rcond
+        These values are only returned if ``full == True``
+
+        - residuals -- sum of squared residuals of the least squares fit
+        - rank -- the effective rank of the scaled Vandermonde
+           coefficient matrix
+        - singular_values -- singular values of the scaled Vandermonde
+           coefficient matrix
+        - rcond -- value of `rcond`.
+
+        For more details, see `numpy.linalg.lstsq`.
+
+    V : ndarray, shape (M,M) or (M,M,K)
+        Present only if ``full == False`` and ``cov == True``.  The covariance
+        matrix of the polynomial coefficient estimates.  The diagonal of
+        this matrix are the variance estimates for each coefficient.  If y
+        is a 2-D array, then the covariance matrix for the `k`-th data set
+        are in ``V[:,:,k]``
+
+
+    Warns
+    -----
+    RankWarning
+        The rank of the coefficient matrix in the least-squares fit is
+        deficient. The warning is only raised if ``full == False``.
+
+        The warnings can be turned off by
+
+        >>> import warnings
+        >>> warnings.simplefilter('ignore', np.RankWarning)
+
+    See Also
+    --------
+    polyval : Compute polynomial values.
+    linalg.lstsq : Computes a least-squares fit.
+    scipy.interpolate.UnivariateSpline : Computes spline fits.
+
+    Notes
+    -----
+    The solution minimizes the squared error
+
+    .. math::
+        E = \\sum_{j=0}^k |p(x_j) - y_j|^2
+
+    in the equations::
+
+        x[0]**n * p[0] + ... + x[0] * p[n-1] + p[n] = y[0]
+        x[1]**n * p[0] + ... + x[1] * p[n-1] + p[n] = y[1]
+        ...
+        x[k]**n * p[0] + ... + x[k] * p[n-1] + p[n] = y[k]
+
+    The coefficient matrix of the coefficients `p` is a Vandermonde matrix.
+
+    `polyfit` issues a `RankWarning` when the least-squares fit is badly
+    conditioned. This implies that the best fit is not well-defined due
+    to numerical error. The results may be improved by lowering the polynomial
+    degree or by replacing `x` by `x` - `x`.mean(). The `rcond` parameter
+    can also be set to a value smaller than its default, but the resulting
+    fit may be spurious: including contributions from the small singular
+    values can add numerical noise to the result.
+
+    Note that fitting polynomial coefficients is inherently badly conditioned
+    when the degree of the polynomial is large or the interval of sample points
+    is badly centered. The quality of the fit should always be checked in these
+    cases. When polynomial fits are not satisfactory, splines may be a good
+    alternative.
+
+    References
+    ----------
+    .. [1] Wikipedia, "Curve fitting",
+           https://en.wikipedia.org/wiki/Curve_fitting
+    .. [2] Wikipedia, "Polynomial interpolation",
+           https://en.wikipedia.org/wiki/Polynomial_interpolation
+
+    Examples
+    --------
+    >>> import warnings
+    >>> x = np.array([0.0, 1.0, 2.0, 3.0,  4.0,  5.0])
+    >>> y = np.array([0.0, 0.8, 0.9, 0.1, -0.8, -1.0])
+    >>> z = np.polyfit(x, y, 3)
+    >>> z
+    array([ 0.08703704, -0.81349206,  1.69312169, -0.03968254]) # may vary
+
+    It is convenient to use `poly1d` objects for dealing with polynomials:
+
+    >>> p = np.poly1d(z)
+    >>> p(0.5)
+    0.6143849206349179 # may vary
+    >>> p(3.5)
+    -0.34732142857143039 # may vary
+    >>> p(10)
+    22.579365079365115 # may vary
+
+    High-order polynomials may oscillate wildly:
+
+    >>> with warnings.catch_warnings():
+    ...     warnings.simplefilter('ignore', np.RankWarning)
+    ...     p30 = np.poly1d(np.polyfit(x, y, 30))
+    ...
+    >>> p30(4)
+    -0.80000000000000204 # may vary
+    >>> p30(5)
+    -0.99999999999999445 # may vary
+    >>> p30(4.5)
+    -0.10547061179440398 # may vary
+
+    Illustration:
+
+    >>> import matplotlib.pyplot as plt
+    >>> xp = np.linspace(-2, 6, 100)
+    >>> _ = plt.plot(x, y, '.', xp, p(xp), '-', xp, p30(xp), '--')
+    >>> plt.ylim(-2,2)
+    (-2, 2)
+    >>> plt.show()
+
+    """
+    order = int(deg) + 1
+    x = NX.asarray(x) + 0.0
+    y = NX.asarray(y) + 0.0
+
+    # check arguments.
+    if deg < 0:
+        raise ValueError("expected deg >= 0")
+    if x.ndim != 1:
+        raise TypeError("expected 1D vector for x")
+    if x.size == 0:
+        raise TypeError("expected non-empty vector for x")
+    if y.ndim < 1 or y.ndim > 2:
+        raise TypeError("expected 1D or 2D array for y")
+    if x.shape[0] != y.shape[0]:
+        raise TypeError("expected x and y to have same length")
+
+    # set rcond
+    if rcond is None:
+        rcond = len(x)*finfo(x.dtype).eps
+
+    # set up least squares equation for powers of x
+    lhs = vander(x, order)
+    rhs = y
+
+    # apply weighting
+    if w is not None:
+        w = NX.asarray(w) + 0.0
+        if w.ndim != 1:
+            raise TypeError("expected a 1-d array for weights")
+        if w.shape[0] != y.shape[0]:
+            raise TypeError("expected w and y to have the same length")
+        lhs *= w[:, NX.newaxis]
+        if rhs.ndim == 2:
+            rhs *= w[:, NX.newaxis]
+        else:
+            rhs *= w
+
+    # scale lhs to improve condition number and solve
+    scale = NX.sqrt((lhs*lhs).sum(axis=0))
+    lhs /= scale
+    c, resids, rank, s = lstsq(lhs, rhs, rcond)
+    c = (c.T/scale).T  # broadcast scale coefficients
+
+    # warn on rank reduction, which indicates an ill conditioned matrix
+    if rank != order and not full:
+        msg = "Polyfit may be poorly conditioned"
+        warnings.warn(msg, RankWarning, stacklevel=4)
+
+    if full:
+        return c, resids, rank, s, rcond
+    elif cov:
+        Vbase = inv(dot(lhs.T, lhs))
+        Vbase /= NX.outer(scale, scale)
+        if cov == "unscaled":
+            fac = 1
+        else:
+            if len(x) <= order:
+                raise ValueError("the number of data points must exceed order "
+                                 "to scale the covariance matrix")
+            # note, this used to be: fac = resids / (len(x) - order - 2.0)
+            # it was deciced that the "- 2" (originally justified by "Bayesian
+            # uncertainty analysis") is not what the user expects
+            # (see gh-11196 and gh-11197)
+            fac = resids / (len(x) - order)
+        if y.ndim == 1:
+            return c, Vbase * fac
+        else:
+            return c, Vbase[:,:, NX.newaxis] * fac
+    else:
+        return c
+
+
+def _polyval_dispatcher(p, x):
+    return (p, x)
+
+
+@array_function_dispatch(_polyval_dispatcher)
 def polyval(p, x):
-    """Evaluate the polynomial p at x.  If x is a polynomial then composition.
-
-    Description:
-
-      If p is of length N, this function returns the value:
-      p[0]*(x**N-1) + p[1]*(x**N-2) + ... + p[N-2]*x + p[N-1]
-
-      x can be a sequence and p(x) will be returned for all elements of x.
-      or x can be another polynomial and the composite polynomial p(x) will be
-      returned.
-
-      Notice:  This can produce inaccurate results for polynomials with
-      significant variability. Use carefully.
+    """
+    Evaluate a polynomial at specific values.
+
+    .. note::
+       This forms part of the old polynomial API. Since version 1.4, the
+       new polynomial API defined in `numpy.polynomial` is preferred.
+       A summary of the differences can be found in the
+       :doc:`transition guide </reference/routines.polynomials>`.
+
+    If `p` is of length N, this function returns the value:
+
+        ``p[0]*x**(N-1) + p[1]*x**(N-2) + ... + p[N-2]*x + p[N-1]``
+
+    If `x` is a sequence, then ``p(x)`` is returned for each element of ``x``.
+    If `x` is another polynomial then the composite polynomial ``p(x(t))``
+    is returned.
+
+    Parameters
+    ----------
+    p : array_like or poly1d object
+       1D array of polynomial coefficients (including coefficients equal
+       to zero) from highest degree to the constant term, or an
+       instance of poly1d.
+    x : array_like or poly1d object
+       A number, an array of numbers, or an instance of poly1d, at
+       which to evaluate `p`.
+
+    Returns
+    -------
+    values : ndarray or poly1d
+       If `x` is a poly1d instance, the result is the composition of the two
+       polynomials, i.e., `x` is "substituted" in `p` and the simplified
+       result is returned. In addition, the type of `x` - array_like or
+       poly1d - governs the type of the output: `x` array_like => `values`
+       array_like, `x` a poly1d object => `values` is also.
+
+    See Also
+    --------
+    poly1d: A polynomial class.
+
+    Notes
+    -----
+    Horner's scheme [1]_ is used to evaluate the polynomial. Even so,
+    for polynomials of high degree the values may be inaccurate due to
+    rounding errors. Use carefully.
+
+    If `x` is a subtype of `ndarray` the return value will be of the same type.
+
+    References
+    ----------
+    .. [1] I. N. Bronshtein, K. A. Semendyayev, and K. A. Hirsch (Eng.
+       trans. Ed.), *Handbook of Mathematics*, New York, Van Nostrand
+       Reinhold Co., 1985, pg. 720.
+
+    Examples
+    --------
+    >>> np.polyval([3,0,1], 5)  # 3 * 5**2 + 0 * 5**1 + 1
+    76
+    >>> np.polyval([3,0,1], np.poly1d(5))
+    poly1d([76])
+    >>> np.polyval(np.poly1d([3,0,1]), 5)
+    76
+    >>> np.polyval(np.poly1d([3,0,1]), np.poly1d(5))
+    poly1d([76])
+
     """
     p = NX.asarray(p)
     if isinstance(x, poly1d):
         y = 0
     else:
-        x = NX.asarray(x)
+        x = NX.asanyarray(x)
         y = NX.zeros_like(x)
-    for i in range(len(p)):
-        y = x * y + p[i]
+    for pv in p:
+        y = y * x + pv
     return y
 
+
+def _binary_op_dispatcher(a1, a2):
+    return (a1, a2)
+
+
+@array_function_dispatch(_binary_op_dispatcher)
 def polyadd(a1, a2):
-    """Adds two polynomials represented as sequences
+    """
+    Find the sum of two polynomials.
+
+    .. note::
+       This forms part of the old polynomial API. Since version 1.4, the
+       new polynomial API defined in `numpy.polynomial` is preferred.
+       A summary of the differences can be found in the
+       :doc:`transition guide </reference/routines.polynomials>`.
+
+    Returns the polynomial resulting from the sum of two input polynomials.
+    Each input must be either a poly1d object or a 1D sequence of polynomial
+    coefficients, from highest to lowest degree.
+
+    Parameters
+    ----------
+    a1, a2 : array_like or poly1d object
+        Input polynomials.
+
+    Returns
+    -------
+    out : ndarray or poly1d object
+        The sum of the inputs. If either input is a poly1d object, then the
+        output is also a poly1d object. Otherwise, it is a 1D array of
+        polynomial coefficients from highest to lowest degree.
+
+    See Also
+    --------
+    poly1d : A one-dimensional polynomial class.
+    poly, polyadd, polyder, polydiv, polyfit, polyint, polysub, polyval
+
+    Examples
+    --------
+    >>> np.polyadd([1, 2], [9, 5, 4])
+    array([9, 6, 6])
+
+    Using poly1d objects:
+
+    >>> p1 = np.poly1d([1, 2])
+    >>> p2 = np.poly1d([9, 5, 4])
+    >>> print(p1)
+    1 x + 2
+    >>> print(p2)
+       2
+    9 x + 5 x + 4
+    >>> print(np.polyadd(p1, p2))
+       2
+    9 x + 6 x + 6
+
     """
     truepoly = (isinstance(a1, poly1d) or isinstance(a2, poly1d))
     a1 = atleast_1d(a1)
     a2 = atleast_1d(a2)
     diff = len(a2) - len(a1)
     if diff == 0:
-        return a1 + a2
+        val = a1 + a2
     elif diff > 0:
         zr = NX.zeros(diff, a1.dtype)
         val = NX.concatenate((zr, a1)) + a2
@@ -255,15 +851,50 @@
         val = poly1d(val)
     return val
 
+
+@array_function_dispatch(_binary_op_dispatcher)
 def polysub(a1, a2):
-    """Subtracts two polynomials represented as sequences
+    """
+    Difference (subtraction) of two polynomials.
+
+    .. note::
+       This forms part of the old polynomial API. Since version 1.4, the
+       new polynomial API defined in `numpy.polynomial` is preferred.
+       A summary of the differences can be found in the
+       :doc:`transition guide </reference/routines.polynomials>`.
+
+    Given two polynomials `a1` and `a2`, returns ``a1 - a2``.
+    `a1` and `a2` can be either array_like sequences of the polynomials'
+    coefficients (including coefficients equal to zero), or `poly1d` objects.
+
+    Parameters
+    ----------
+    a1, a2 : array_like or poly1d
+        Minuend and subtrahend polynomials, respectively.
+
+    Returns
+    -------
+    out : ndarray or poly1d
+        Array or `poly1d` object of the difference polynomial's coefficients.
+
+    See Also
+    --------
+    polyval, polydiv, polymul, polyadd
+
+    Examples
+    --------
+    .. math:: (2 x^2 + 10 x - 2) - (3 x^2 + 10 x -4) = (-x^2 + 2)
+
+    >>> np.polysub([2, 10, -2], [3, 10, -4])
+    array([-1,  0,  2])
+
     """
     truepoly = (isinstance(a1, poly1d) or isinstance(a2, poly1d))
     a1 = atleast_1d(a1)
     a2 = atleast_1d(a2)
     diff = len(a2) - len(a1)
     if diff == 0:
-        return a1 - a2
+        val = a1 - a2
     elif diff > 0:
         zr = NX.zeros(diff, a1.dtype)
         val = NX.concatenate((zr, a1)) - a2
@@ -275,27 +906,135 @@
     return val
 
 
+@array_function_dispatch(_binary_op_dispatcher)
 def polymul(a1, a2):
-    """Multiplies two polynomials represented as sequences.
+    """
+    Find the product of two polynomials.
+
+    .. note::
+       This forms part of the old polynomial API. Since version 1.4, the
+       new polynomial API defined in `numpy.polynomial` is preferred.
+       A summary of the differences can be found in the
+       :doc:`transition guide </reference/routines.polynomials>`.
+
+    Finds the polynomial resulting from the multiplication of the two input
+    polynomials. Each input must be either a poly1d object or a 1D sequence
+    of polynomial coefficients, from highest to lowest degree.
+
+    Parameters
+    ----------
+    a1, a2 : array_like or poly1d object
+        Input polynomials.
+
+    Returns
+    -------
+    out : ndarray or poly1d object
+        The polynomial resulting from the multiplication of the inputs. If
+        either inputs is a poly1d object, then the output is also a poly1d
+        object. Otherwise, it is a 1D array of polynomial coefficients from
+        highest to lowest degree.
+
+    See Also
+    --------
+    poly1d : A one-dimensional polynomial class.
+    poly, polyadd, polyder, polydiv, polyfit, polyint, polysub, polyval
+    convolve : Array convolution. Same output as polymul, but has parameter
+               for overlap mode.
+
+    Examples
+    --------
+    >>> np.polymul([1, 2, 3], [9, 5, 1])
+    array([ 9, 23, 38, 17,  3])
+
+    Using poly1d objects:
+
+    >>> p1 = np.poly1d([1, 2, 3])
+    >>> p2 = np.poly1d([9, 5, 1])
+    >>> print(p1)
+       2
+    1 x + 2 x + 3
+    >>> print(p2)
+       2
+    9 x + 5 x + 1
+    >>> print(np.polymul(p1, p2))
+       4      3      2
+    9 x + 23 x + 38 x + 17 x + 3
+
     """
     truepoly = (isinstance(a1, poly1d) or isinstance(a2, poly1d))
+    a1, a2 = poly1d(a1), poly1d(a2)
     val = NX.convolve(a1, a2)
     if truepoly:
         val = poly1d(val)
     return val
 
+
+def _polydiv_dispatcher(u, v):
+    return (u, v)
+
+
+@array_function_dispatch(_polydiv_dispatcher)
 def polydiv(u, v):
-    """Computes q and r polynomials so that u(s) = q(s)*v(s) + r(s)
-    and deg r < deg v.
-    """
-    truepoly = (isinstance(u, poly1d) or isinstance(u, poly1d))
-    u = atleast_1d(u)
-    v = atleast_1d(v)
+    """
+    Returns the quotient and remainder of polynomial division.
+
+    .. note::
+       This forms part of the old polynomial API. Since version 1.4, the
+       new polynomial API defined in `numpy.polynomial` is preferred.
+       A summary of the differences can be found in the
+       :doc:`transition guide </reference/routines.polynomials>`.
+
+    The input arrays are the coefficients (including any coefficients
+    equal to zero) of the "numerator" (dividend) and "denominator"
+    (divisor) polynomials, respectively.
+
+    Parameters
+    ----------
+    u : array_like or poly1d
+        Dividend polynomial's coefficients.
+
+    v : array_like or poly1d
+        Divisor polynomial's coefficients.
+
+    Returns
+    -------
+    q : ndarray
+        Coefficients, including those equal to zero, of the quotient.
+    r : ndarray
+        Coefficients, including those equal to zero, of the remainder.
+
+    See Also
+    --------
+    poly, polyadd, polyder, polydiv, polyfit, polyint, polymul, polysub
+    polyval
+
+    Notes
+    -----
+    Both `u` and `v` must be 0-d or 1-d (ndim = 0 or 1), but `u.ndim` need
+    not equal `v.ndim`. In other words, all four possible combinations -
+    ``u.ndim = v.ndim = 0``, ``u.ndim = v.ndim = 1``,
+    ``u.ndim = 1, v.ndim = 0``, and ``u.ndim = 0, v.ndim = 1`` - work.
+
+    Examples
+    --------
+    .. math:: \\frac{3x^2 + 5x + 2}{2x + 1} = 1.5x + 1.75, remainder 0.25
+
+    >>> x = np.array([3.0, 5.0, 2.0])
+    >>> y = np.array([2.0, 1.0])
+    >>> np.polydiv(x, y)
+    (array([1.5 , 1.75]), array([0.25]))
+
+    """
+    truepoly = (isinstance(u, poly1d) or isinstance(v, poly1d))
+    u = atleast_1d(u) + 0.0
+    v = atleast_1d(v) + 0.0
+    # w has the common type
+    w = u[0] + v[0]
     m = len(u) - 1
     n = len(v) - 1
     scale = 1. / v[0]
-    q = NX.zeros((m-n+1,), float)
-    r = u.copy()
+    q = NX.zeros((max(m - n + 1, 1),), w.dtype)
+    r = u.astype(w.dtype)
     for k in range(0, m-n+1):
         d = scale * r[k]
         q[k] = d
@@ -303,17 +1042,16 @@
     while NX.allclose(r[0], 0, rtol=1e-14) and (r.shape[-1] > 1):
         r = r[1:]
     if truepoly:
-        q = poly1d(q)
-        r = poly1d(r)
+        return poly1d(q), poly1d(r)
     return q, r
 
-_poly_mat = re.compile(r"[*][*]([0-9]*)")
+_poly_mat = re.compile(r"\*\*([0-9]*)")
 def _raise_power(astr, wrap=70):
     n = 0
     line1 = ''
     line2 = ''
     output = ' '
-    while 1:
+    while True:
         mat = _poly_mat.search(astr, n)
         if mat is None:
             break
@@ -323,8 +1061,8 @@
         n = span[1]
         toadd2 = partstr + ' '*(len(power)-1)
         toadd1 = ' '*(len(partstr)-1) + power
-        if ((len(line2)+len(toadd2) > wrap) or \
-            (len(line1)+len(toadd1) > wrap)):
+        if ((len(line2) + len(toadd2) > wrap) or
+                (len(line1) + len(toadd1) > wrap)):
             output += line1 + "\n" + line2 + "\n "
             line1 = toadd1
             line2 = toadd2
@@ -335,49 +1073,182 @@
     return output + astr[n:]
 
 
-class poly1d(object):
-    """A one-dimensional polynomial class.
-
-    p = poly1d([1,2,3]) constructs the polynomial x**2 + 2 x + 3
-
-    p(0.5) evaluates the polynomial at the location
-    p.r  is a list of roots
-    p.c  is the coefficient array [1,2,3]
-    p.order is the polynomial order (after leading zeros in p.c are removed)
-    p[k] is the coefficient on the kth power of x (backwards from
-         sequencing the coefficient array.
-
-    polynomials can be added, substracted, multplied and divided (returns
-         quotient and remainder).
-    asarray(p) will also give the coefficient array, so polynomials can
-         be used in all functions that accept arrays.
-
-    p = poly1d([1,2,3], variable='lambda') will use lambda in the
-    string representation of p.
-    """
-    coeffs = None
-    order = None
-    variable = None
-    def __init__(self, c_or_r, r=0, variable=None):
+@set_module('numpy')
+class poly1d:
+    """
+    A one-dimensional polynomial class.
+
+    .. note::
+       This forms part of the old polynomial API. Since version 1.4, the
+       new polynomial API defined in `numpy.polynomial` is preferred.
+       A summary of the differences can be found in the
+       :doc:`transition guide </reference/routines.polynomials>`.
+
+    A convenience class, used to encapsulate "natural" operations on
+    polynomials so that said operations may take on their customary
+    form in code (see Examples).
+
+    Parameters
+    ----------
+    c_or_r : array_like
+        The polynomial's coefficients, in decreasing powers, or if
+        the value of the second parameter is True, the polynomial's
+        roots (values where the polynomial evaluates to 0).  For example,
+        ``poly1d([1, 2, 3])`` returns an object that represents
+        :math:`x^2 + 2x + 3`, whereas ``poly1d([1, 2, 3], True)`` returns
+        one that represents :math:`(x-1)(x-2)(x-3) = x^3 - 6x^2 + 11x -6`.
+    r : bool, optional
+        If True, `c_or_r` specifies the polynomial's roots; the default
+        is False.
+    variable : str, optional
+        Changes the variable used when printing `p` from `x` to `variable`
+        (see Examples).
+
+    Examples
+    --------
+    Construct the polynomial :math:`x^2 + 2x + 3`:
+
+    >>> p = np.poly1d([1, 2, 3])
+    >>> print(np.poly1d(p))
+       2
+    1 x + 2 x + 3
+
+    Evaluate the polynomial at :math:`x = 0.5`:
+
+    >>> p(0.5)
+    4.25
+
+    Find the roots:
+
+    >>> p.r
+    array([-1.+1.41421356j, -1.-1.41421356j])
+    >>> p(p.r)
+    array([ -4.44089210e-16+0.j,  -4.44089210e-16+0.j]) # may vary
+
+    These numbers in the previous line represent (0, 0) to machine precision
+
+    Show the coefficients:
+
+    >>> p.c
+    array([1, 2, 3])
+
+    Display the order (the leading zero-coefficients are removed):
+
+    >>> p.order
+    2
+
+    Show the coefficient of the k-th power in the polynomial
+    (which is equivalent to ``p.c[-(i+1)]``):
+
+    >>> p[1]
+    2
+
+    Polynomials can be added, subtracted, multiplied, and divided
+    (returns quotient and remainder):
+
+    >>> p * p
+    poly1d([ 1,  4, 10, 12,  9])
+
+    >>> (p**3 + 4) / p
+    (poly1d([ 1.,  4., 10., 12.,  9.]), poly1d([4.]))
+
+    ``asarray(p)`` gives the coefficient array, so polynomials can be
+    used in all functions that accept arrays:
+
+    >>> p**2 # square of polynomial
+    poly1d([ 1,  4, 10, 12,  9])
+
+    >>> np.square(p) # square of individual coefficients
+    array([1, 4, 9])
+
+    The variable used in the string representation of `p` can be modified,
+    using the `variable` parameter:
+
+    >>> p = np.poly1d([1,2,3], variable='z')
+    >>> print(p)
+       2
+    1 z + 2 z + 3
+
+    Construct a polynomial from its roots:
+
+    >>> np.poly1d([1, 2], True)
+    poly1d([ 1., -3.,  2.])
+
+    This is the same polynomial as obtained by:
+
+    >>> np.poly1d([1, -1]) * np.poly1d([1, -2])
+    poly1d([ 1, -3,  2])
+
+    """
+    __hash__ = None
+
+    @property
+    def coeffs(self):
+        """ The polynomial coefficients """
+        return self._coeffs
+
+    @coeffs.setter
+    def coeffs(self, value):
+        # allowing this makes p.coeffs *= 2 legal
+        if value is not self._coeffs:
+            raise AttributeError("Cannot set attribute")
+
+    @property
+    def variable(self):
+        """ The name of the polynomial variable """
+        return self._variable
+
+    # calculated attributes
+    @property
+    def order(self):
+        """ The order or degree of the polynomial """
+        return len(self._coeffs) - 1
+
+    @property
+    def roots(self):
+        """ The roots of the polynomial, where self(x) == 0 """
+        return roots(self._coeffs)
+
+    # our internal _coeffs property need to be backed by __dict__['coeffs'] for
+    # scipy to work correctly.
+    @property
+    def _coeffs(self):
+        return self.__dict__['coeffs']
+    @_coeffs.setter
+    def _coeffs(self, coeffs):
+        self.__dict__['coeffs'] = coeffs
+
+    # alias attributes
+    r = roots
+    c = coef = coefficients = coeffs
+    o = order
+
+    def __init__(self, c_or_r, r=False, variable=None):
         if isinstance(c_or_r, poly1d):
-            for key in c_or_r.__dict__.keys():
-                self.__dict__[key] = c_or_r.__dict__[key]
+            self._variable = c_or_r._variable
+            self._coeffs = c_or_r._coeffs
+
+            if set(c_or_r.__dict__) - set(self.__dict__):
+                msg = ("In the future extra properties will not be copied "
+                       "across when constructing one poly1d from another")
+                warnings.warn(msg, FutureWarning, stacklevel=2)
+                self.__dict__.update(c_or_r.__dict__)
+
             if variable is not None:
-                self.__dict__['variable'] = variable
+                self._variable = variable
             return
         if r:
             c_or_r = poly(c_or_r)
         c_or_r = atleast_1d(c_or_r)
-        if len(c_or_r.shape) > 1:
-            raise ValueError, "Polynomial must be 1d only."
+        if c_or_r.ndim > 1:
+            raise ValueError("Polynomial must be 1d only.")
         c_or_r = trim_zeros(c_or_r, trim='f')
         if len(c_or_r) == 0:
-            c_or_r = NX.array([0.])
-        self.__dict__['coeffs'] = c_or_r
-        self.__dict__['order'] = len(c_or_r) - 1
+            c_or_r = NX.array([0], dtype=c_or_r.dtype)
+        self._coeffs = c_or_r
         if variable is None:
             variable = 'x'
-        self.__dict__['variable'] = variable
+        self._variable = variable
 
     def __array__(self, t=None):
         if t:
@@ -394,13 +1265,28 @@
         return self.order
 
     def __str__(self):
-        N = self.order
         thestr = "0"
         var = self.variable
-        for k in range(len(self.coeffs)):
-            coefstr ='%.4g' % abs(self.coeffs[k])
-            if coefstr[-4:] == '0000':
-                coefstr = coefstr[:-5]
+
+        # Remove leading zeros
+        coeffs = self.coeffs[NX.logical_or.accumulate(self.coeffs != 0)]
+        N = len(coeffs)-1
+
+        def fmt_float(q):
+            s = '%.4g' % q
+            if s.endswith('.0000'):
+                s = s[:-5]
+            return s
+
+        for k, coeff in enumerate(coeffs):
+            if not iscomplex(coeff):
+                coefstr = fmt_float(real(coeff))
+            elif real(coeff) == 0:
+                coefstr = '%sj' % fmt_float(imag(coeff))
+            else:
+                coefstr = '(%s + %sj)' % (fmt_float(real(coeff)),
+                                          fmt_float(imag(coeff)))
+
             power = (N-k)
             if power == 0:
                 if coefstr != '0':
@@ -427,19 +1313,22 @@
 
             if k > 0:
                 if newstr != '':
-                    if self.coeffs[k] < 0:
-                        thestr = "%s - %s" % (thestr, newstr)
+                    if newstr.startswith('-'):
+                        thestr = "%s - %s" % (thestr, newstr[1:])
                     else:
                         thestr = "%s + %s" % (thestr, newstr)
-            elif (k == 0) and (newstr != '') and (self.coeffs[k] < 0):
-                thestr = "-%s" % (newstr,)
             else:
                 thestr = newstr
         return _raise_power(thestr)
 
-
     def __call__(self, val):
         return polyval(self.coeffs, val)
+
+    def __neg__(self):
+        return poly1d(-self.coeffs)
+
+    def __pos__(self):
+        return self
 
     def __mul__(self, other):
         if isscalar(other):
@@ -465,7 +1354,7 @@
 
     def __pow__(self, val):
         if not isscalar(val) or int(val) != val or val < 0:
-            raise ValueError, "Power to non-negative integers only."
+            raise ValueError("Power to non-negative integers only.")
         res = [1]
         for _ in range(val):
             res = polymul(self.coeffs, res)
@@ -486,6 +1375,8 @@
             other = poly1d(other)
             return polydiv(self, other)
 
+    __truediv__ = __div__
+
     def __rdiv__(self, other):
         if isscalar(other):
             return poly1d(other/self.coeffs)
@@ -493,52 +1384,69 @@
             other = poly1d(other)
             return polydiv(other, self)
 
+    __rtruediv__ = __rdiv__
+
     def __eq__(self, other):
+        if not isinstance(other, poly1d):
+            return NotImplemented
+        if self.coeffs.shape != other.coeffs.shape:
+            return False
         return (self.coeffs == other.coeffs).all()
 
     def __ne__(self, other):
-        return (self.coeffs != other.coeffs).any()
-
-    def __setattr__(self, key, val):
-        raise ValueError, "Attributes cannot be changed this way."
-
-    def __getattr__(self, key):
-        if key in ['r', 'roots']:
-            return roots(self.coeffs)
-        elif key in ['c','coef','coefficients']:
-            return self.coeffs
-        elif key in ['o']:
-            return self.order
-        else:
-            return self.__dict__[key]
+        if not isinstance(other, poly1d):
+            return NotImplemented
+        return not self.__eq__(other)
+
 
     def __getitem__(self, val):
         ind = self.order - val
         if val > self.order:
-            return 0
+            return self.coeffs.dtype.type(0)
         if val < 0:
-            return 0
+            return self.coeffs.dtype.type(0)
         return self.coeffs[ind]
 
     def __setitem__(self, key, val):
         ind = self.order - key
         if key < 0:
-            raise ValueError, "Does not support negative powers."
+            raise ValueError("Does not support negative powers.")
         if key > self.order:
             zr = NX.zeros(key-self.order, self.coeffs.dtype)
-            self.__dict__['coeffs'] = NX.concatenate((zr, self.coeffs))
-            self.__dict__['order'] = key
+            self._coeffs = NX.concatenate((zr, self.coeffs))
             ind = 0
-        self.__dict__['coeffs'][ind] = val
+        self._coeffs[ind] = val
         return
 
+    def __iter__(self):
+        return iter(self.coeffs)
+
     def integ(self, m=1, k=0):
-        """Return the mth analytical integral of this polynomial.
-        See the documentation for polyint.
+        """
+        Return an antiderivative (indefinite integral) of this polynomial.
+
+        Refer to `polyint` for full documentation.
+
+        See Also
+        --------
+        polyint : equivalent function
+
         """
         return poly1d(polyint(self.coeffs, m=m, k=k))
 
     def deriv(self, m=1):
-        """Return the mth derivative of this polynomial.
+        """
+        Return a derivative of this polynomial.
+
+        Refer to `polyder` for full documentation.
+
+        See Also
+        --------
+        polyder : equivalent function
+
         """
         return poly1d(polyder(self.coeffs, m=m))
+
+# Stuff to do on module import
+
+warnings.simplefilter('always', RankWarning)
('numpy/lib', 'ufunclike.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,58 +1,268 @@
 """
 Module of functions that are like ufuncs in acting on arrays and optionally
 storing results in an output array.
+
 """
-__all__ = ['fix', 'isneginf', 'isposinf', 'log2']
+__all__ = ['fix', 'isneginf', 'isposinf']
 
 import numpy.core.numeric as nx
-from numpy.core.numeric import asarray, empty, isinf, signbit
-import numpy.core.umath as umath
-
-def fix(x, y=None):
-    """ Round x to nearest integer towards zero.
-    """
-    x = asarray(x)
-    if y is None:
-        y = nx.floor(x)
+from numpy.core.overrides import (
+    array_function_dispatch, ARRAY_FUNCTION_ENABLED,
+)
+import warnings
+import functools
+
+
+def _deprecate_out_named_y(f):
+    """
+    Allow the out argument to be passed as the name `y` (deprecated)
+
+    In future, this decorator should be removed.
+    """
+    @functools.wraps(f)
+    def func(x, out=None, **kwargs):
+        if 'y' in kwargs:
+            if 'out' in kwargs:
+                raise TypeError(
+                    "{} got multiple values for argument 'out'/'y'"
+                    .format(f.__name__)
+                )
+            out = kwargs.pop('y')
+            # NumPy 1.13.0, 2017-04-26
+            warnings.warn(
+                "The name of the out argument to {} has changed from `y` to "
+                "`out`, to match other ufuncs.".format(f.__name__),
+                DeprecationWarning, stacklevel=3)
+        return f(x, out=out, **kwargs)
+
+    return func
+
+
+def _fix_out_named_y(f):
+    """
+    Allow the out argument to be passed as the name `y` (deprecated)
+
+    This decorator should only be used if _deprecate_out_named_y is used on
+    a corresponding dispatcher function.
+    """
+    @functools.wraps(f)
+    def func(x, out=None, **kwargs):
+        if 'y' in kwargs:
+            # we already did error checking in _deprecate_out_named_y
+            out = kwargs.pop('y')
+        return f(x, out=out, **kwargs)
+
+    return func
+
+
+def _fix_and_maybe_deprecate_out_named_y(f):
+    """
+    Use the appropriate decorator, depending upon if dispatching is being used.
+    """
+    if ARRAY_FUNCTION_ENABLED:
+        return _fix_out_named_y(f)
     else:
-        nx.floor(x, y)
-    if x.ndim == 0:
-        if (x<0):
-            y += 1
+        return _deprecate_out_named_y(f)
+
+
+@_deprecate_out_named_y
+def _dispatcher(x, out=None):
+    return (x, out)
+
+
+@array_function_dispatch(_dispatcher, verify=False, module='numpy')
+@_fix_and_maybe_deprecate_out_named_y
+def fix(x, out=None):
+    """
+    Round to nearest integer towards zero.
+
+    Round an array of floats element-wise to nearest integer towards zero.
+    The rounded values are returned as floats.
+
+    Parameters
+    ----------
+    x : array_like
+        An array of floats to be rounded
+    out : ndarray, optional
+        A location into which the result is stored. If provided, it must have
+        a shape that the input broadcasts to. If not provided or None, a
+        freshly-allocated array is returned.
+
+    Returns
+    -------
+    out : ndarray of floats
+        A float array with the same dimensions as the input.
+        If second argument is not supplied then a float array is returned
+        with the rounded values.
+
+        If a second argument is supplied the result is stored there.
+        The return value `out` is then a reference to that array.
+
+    See Also
+    --------
+    rint, trunc, floor, ceil
+    around : Round to given number of decimals
+
+    Examples
+    --------
+    >>> np.fix(3.14)
+    3.0
+    >>> np.fix(3)
+    3.0
+    >>> np.fix([2.1, 2.9, -2.1, -2.9])
+    array([ 2.,  2., -2., -2.])
+
+    """
+    # promote back to an array if flattened
+    res = nx.asanyarray(nx.ceil(x, out=out))
+    res = nx.floor(x, out=res, where=nx.greater_equal(x, 0))
+
+    # when no out argument is passed and no subclasses are involved, flatten
+    # scalars
+    if out is None and type(res) is nx.ndarray:
+        res = res[()]
+    return res
+
+
+@array_function_dispatch(_dispatcher, verify=False, module='numpy')
+@_fix_and_maybe_deprecate_out_named_y
+def isposinf(x, out=None):
+    """
+    Test element-wise for positive infinity, return result as bool array.
+
+    Parameters
+    ----------
+    x : array_like
+        The input array.
+    out : array_like, optional
+        A location into which the result is stored. If provided, it must have a
+        shape that the input broadcasts to. If not provided or None, a
+        freshly-allocated boolean array is returned.
+
+    Returns
+    -------
+    out : ndarray
+        A boolean array with the same dimensions as the input.
+        If second argument is not supplied then a boolean array is returned
+        with values True where the corresponding element of the input is
+        positive infinity and values False where the element of the input is
+        not positive infinity.
+
+        If a second argument is supplied the result is stored there. If the
+        type of that array is a numeric type the result is represented as zeros
+        and ones, if the type is boolean then as False and True.
+        The return value `out` is then a reference to that array.
+
+    See Also
+    --------
+    isinf, isneginf, isfinite, isnan
+
+    Notes
+    -----
+    NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic
+    (IEEE 754).
+
+    Errors result if the second argument is also supplied when x is a scalar
+    input, if first and second arguments have different shapes, or if the
+    first argument has complex values
+
+    Examples
+    --------
+    >>> np.isposinf(np.PINF)
+    True
+    >>> np.isposinf(np.inf)
+    True
+    >>> np.isposinf(np.NINF)
+    False
+    >>> np.isposinf([-np.inf, 0., np.inf])
+    array([False, False,  True])
+
+    >>> x = np.array([-np.inf, 0., np.inf])
+    >>> y = np.array([2, 2, 2])
+    >>> np.isposinf(x, y)
+    array([0, 0, 1])
+    >>> y
+    array([0, 0, 1])
+
+    """
+    is_inf = nx.isinf(x)
+    try:
+        signbit = ~nx.signbit(x)
+    except TypeError as e:
+        dtype = nx.asanyarray(x).dtype
+        raise TypeError(f'This operation is not supported for {dtype} values '
+                        'because it would be ambiguous.') from e
     else:
-        y[x<0] = y[x<0]+1
-    return y
-
-def isposinf(x, y=None):
-    """Return a boolean array y with y[i] True for x[i] = +Inf.
-
-    If y is an array, the result replaces the contents of y.
-    """
-    if y is None:
-        y = empty(x.shape, dtype=nx.bool_)
-    umath.logical_and(isinf(x), ~signbit(x), y)
-    return y
-
-def isneginf(x, y=None):
-    """Return a boolean array y with y[i] True for x[i] = -Inf.
-
-    If y is an array, the result replaces the contents of y.
-    """
-    if y is None:
-        y = empty(x.shape, dtype=nx.bool_)
-    umath.logical_and(isinf(x), signbit(x), y)
-    return y
-
-_log2 = umath.log(2)
-def log2(x, y=None):
-    """Returns the base 2 logarithm of x
-
-    If y is an array, the result replaces the contents of y.
-    """
-    x = asarray(x)
-    if y is None:
-        y = umath.log(x)
+        return nx.logical_and(is_inf, signbit, out)
+
+
+@array_function_dispatch(_dispatcher, verify=False, module='numpy')
+@_fix_and_maybe_deprecate_out_named_y
+def isneginf(x, out=None):
+    """
+    Test element-wise for negative infinity, return result as bool array.
+
+    Parameters
+    ----------
+    x : array_like
+        The input array.
+    out : array_like, optional
+        A location into which the result is stored. If provided, it must have a
+        shape that the input broadcasts to. If not provided or None, a
+        freshly-allocated boolean array is returned.
+
+    Returns
+    -------
+    out : ndarray
+        A boolean array with the same dimensions as the input.
+        If second argument is not supplied then a numpy boolean array is
+        returned with values True where the corresponding element of the
+        input is negative infinity and values False where the element of
+        the input is not negative infinity.
+
+        If a second argument is supplied the result is stored there. If the
+        type of that array is a numeric type the result is represented as
+        zeros and ones, if the type is boolean then as False and True. The
+        return value `out` is then a reference to that array.
+
+    See Also
+    --------
+    isinf, isposinf, isnan, isfinite
+
+    Notes
+    -----
+    NumPy uses the IEEE Standard for Binary Floating-Point for Arithmetic
+    (IEEE 754).
+
+    Errors result if the second argument is also supplied when x is a scalar
+    input, if first and second arguments have different shapes, or if the
+    first argument has complex values.
+
+    Examples
+    --------
+    >>> np.isneginf(np.NINF)
+    True
+    >>> np.isneginf(np.inf)
+    False
+    >>> np.isneginf(np.PINF)
+    False
+    >>> np.isneginf([-np.inf, 0., np.inf])
+    array([ True, False, False])
+
+    >>> x = np.array([-np.inf, 0., np.inf])
+    >>> y = np.array([2, 2, 2])
+    >>> np.isneginf(x, y)
+    array([1, 0, 0])
+    >>> y
+    array([1, 0, 0])
+
+    """
+    is_inf = nx.isinf(x)
+    try:
+        signbit = nx.signbit(x)
+    except TypeError as e:
+        dtype = nx.asanyarray(x).dtype
+        raise TypeError(f'This operation is not supported for {dtype} values '
+                        'because it would be ambiguous.') from e
     else:
-        umath.log(x, y)
-    y /= _log2
-    return y
+        return nx.logical_and(is_inf, signbit, out)
('numpy/random', '__init__.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,18 +1,215 @@
-# To get sub-modules
-from info import __doc__, __all__
-from mtrand import *
-
-# Some aliases:
-ranf = random = sample = random_sample
-__all__.extend(['ranf','random','sample'])
+"""
+========================
+Random Number Generation
+========================
+
+Use ``default_rng()`` to create a `Generator` and call its methods.
+
+=============== =========================================================
+Generator
+--------------- ---------------------------------------------------------
+Generator       Class implementing all of the random number distributions
+default_rng     Default constructor for ``Generator``
+=============== =========================================================
+
+============================================= ===
+BitGenerator Streams that work with Generator
+--------------------------------------------- ---
+MT19937
+PCG64
+PCG64DXSM
+Philox
+SFC64
+============================================= ===
+
+============================================= ===
+Getting entropy to initialize a BitGenerator
+--------------------------------------------- ---
+SeedSequence
+============================================= ===
+
+
+Legacy
+------
+
+For backwards compatibility with previous versions of numpy before 1.17, the
+various aliases to the global `RandomState` methods are left alone and do not
+use the new `Generator` API.
+
+==================== =========================================================
+Utility functions
+-------------------- ---------------------------------------------------------
+random               Uniformly distributed floats over ``[0, 1)``
+bytes                Uniformly distributed random bytes.
+permutation          Randomly permute a sequence / generate a random sequence.
+shuffle              Randomly permute a sequence in place.
+choice               Random sample from 1-D array.
+==================== =========================================================
+
+==================== =========================================================
+Compatibility
+functions - removed
+in the new API
+-------------------- ---------------------------------------------------------
+rand                 Uniformly distributed values.
+randn                Normally distributed values.
+ranf                 Uniformly distributed floating point numbers.
+random_integers      Uniformly distributed integers in a given range.
+                     (deprecated, use ``integers(..., closed=True)`` instead)
+random_sample        Alias for `random_sample`
+randint              Uniformly distributed integers in a given range
+seed                 Seed the legacy random number generator.
+==================== =========================================================
+
+==================== =========================================================
+Univariate
+distributions
+-------------------- ---------------------------------------------------------
+beta                 Beta distribution over ``[0, 1]``.
+binomial             Binomial distribution.
+chisquare            :math:`\\chi^2` distribution.
+exponential          Exponential distribution.
+f                    F (Fisher-Snedecor) distribution.
+gamma                Gamma distribution.
+geometric            Geometric distribution.
+gumbel               Gumbel distribution.
+hypergeometric       Hypergeometric distribution.
+laplace              Laplace distribution.
+logistic             Logistic distribution.
+lognormal            Log-normal distribution.
+logseries            Logarithmic series distribution.
+negative_binomial    Negative binomial distribution.
+noncentral_chisquare Non-central chi-square distribution.
+noncentral_f         Non-central F distribution.
+normal               Normal / Gaussian distribution.
+pareto               Pareto distribution.
+poisson              Poisson distribution.
+power                Power distribution.
+rayleigh             Rayleigh distribution.
+triangular           Triangular distribution.
+uniform              Uniform distribution.
+vonmises             Von Mises circular distribution.
+wald                 Wald (inverse Gaussian) distribution.
+weibull              Weibull distribution.
+zipf                 Zipf's distribution over ranked data.
+==================== =========================================================
+
+==================== ==========================================================
+Multivariate
+distributions
+-------------------- ----------------------------------------------------------
+dirichlet            Multivariate generalization of Beta distribution.
+multinomial          Multivariate generalization of the binomial distribution.
+multivariate_normal  Multivariate generalization of the normal distribution.
+==================== ==========================================================
+
+==================== =========================================================
+Standard
+distributions
+-------------------- ---------------------------------------------------------
+standard_cauchy      Standard Cauchy-Lorentz distribution.
+standard_exponential Standard exponential distribution.
+standard_gamma       Standard Gamma distribution.
+standard_normal      Standard normal distribution.
+standard_t           Standard Student's t-distribution.
+==================== =========================================================
+
+==================== =========================================================
+Internal functions
+-------------------- ---------------------------------------------------------
+get_state            Get tuple representing internal state of generator.
+set_state            Set state of generator.
+==================== =========================================================
+
+
+"""
+__all__ = [
+    'beta',
+    'binomial',
+    'bytes',
+    'chisquare',
+    'choice',
+    'dirichlet',
+    'exponential',
+    'f',
+    'gamma',
+    'geometric',
+    'get_state',
+    'gumbel',
+    'hypergeometric',
+    'laplace',
+    'logistic',
+    'lognormal',
+    'logseries',
+    'multinomial',
+    'multivariate_normal',
+    'negative_binomial',
+    'noncentral_chisquare',
+    'noncentral_f',
+    'normal',
+    'pareto',
+    'permutation',
+    'poisson',
+    'power',
+    'rand',
+    'randint',
+    'randn',
+    'random',
+    'random_integers',
+    'random_sample',
+    'ranf',
+    'rayleigh',
+    'sample',
+    'seed',
+    'set_state',
+    'shuffle',
+    'standard_cauchy',
+    'standard_exponential',
+    'standard_gamma',
+    'standard_normal',
+    'standard_t',
+    'triangular',
+    'uniform',
+    'vonmises',
+    'wald',
+    'weibull',
+    'zipf',
+]
+
+# add these for module-freeze analysis (like PyInstaller)
+from . import _pickle
+from . import _common
+from . import _bounded_integers
+
+from ._generator import Generator, default_rng
+from .bit_generator import SeedSequence, BitGenerator
+from ._mt19937 import MT19937
+from ._pcg64 import PCG64, PCG64DXSM
+from ._philox import Philox
+from ._sfc64 import SFC64
+from .mtrand import *
+
+__all__ += ['Generator', 'RandomState', 'SeedSequence', 'MT19937',
+            'Philox', 'PCG64', 'PCG64DXSM', 'SFC64', 'default_rng',
+            'BitGenerator']
+
 
 def __RandomState_ctor():
     """Return a RandomState instance.
 
     This function exists solely to assist (un)pickling.
+
+    Note that the state of the RandomState returned here is irrelevant, as this
+    function's entire purpose is to return a newly allocated RandomState whose
+    state pickle can set.  Consequently the RandomState returned by this function
+    is a freshly allocated copy with a seed=0.
+
+    See https://github.com/numpy/numpy/issues/4763 for a detailed discussion
+
     """
-    return RandomState()
-
-def test(level=1, verbosity=1):
-    from numpy.testing import NumpyTest
-    return NumpyTest().test(level, verbosity)
+    return RandomState(seed=0)
+
+
+from numpy._pytesttester import PytestTester
+test = PytestTester(__name__)
+del PytestTester
('numpy/random', 'setup.py')
--- /Users/tshi/researchProjs/numpy/numpy-0.9.8/
+++ /Users/tshi/researchProjs/numpy/numpy-1.23.0/
@@ -1,54 +1,169 @@
+import os
+import platform
+import sys
+from os.path import join
 
-from os.path import join, split
+from numpy.distutils.system_info import platform_bits
 
-def configuration(parent_package='',top_path=None):
+is_msvc = (platform.platform().startswith('Windows') and
+           platform.python_compiler().startswith('MS'))
+
+
+def configuration(parent_package='', top_path=None):
     from numpy.distutils.misc_util import Configuration, get_mathlibs
-    config = Configuration('random',parent_package,top_path)
+    config = Configuration('random', parent_package, top_path)
 
     def generate_libraries(ext, build_dir):
         config_cmd = config.get_config_cmd()
-        if top_path is None:
-            libs = get_mathlibs()
-        else:
-            path = join(split(build_dir)[0],'core')
-            libs = get_mathlibs(path)
-        tc = testcode_wincrypt()
-        if config_cmd.try_run(tc):
-            libs.append('Advapi32')
+        libs = get_mathlibs()
+        if sys.platform == 'win32':
+            libs.extend(['Advapi32', 'Kernel32'])
         ext.libraries.extend(libs)
         return None
 
-    libs = []
-    # Configure mtrand
+    # enable unix large file support on 32 bit systems
+    # (64 bit off_t, lseek -> lseek64 etc.)
+    if sys.platform[:3] == 'aix':
+        defs = [('_LARGE_FILES', None)]
+    else:
+        defs = [('_FILE_OFFSET_BITS', '64'),
+                ('_LARGEFILE_SOURCE', '1'),
+                ('_LARGEFILE64_SOURCE', '1')]
+
+    defs.append(('NPY_NO_DEPRECATED_API', 0))
+    config.add_subpackage('tests')
+    config.add_data_dir('tests/data')
+    config.add_data_dir('_examples')
+
+    EXTRA_LINK_ARGS = []
+    EXTRA_LIBRARIES = ['npyrandom']
+    if os.name != 'nt':
+        # Math lib
+        EXTRA_LIBRARIES.append('m')
+    # Some bit generators exclude GCC inlining
+    EXTRA_COMPILE_ARGS = ['-U__GNUC_GNU_INLINE__']
+
+    if is_msvc and platform_bits == 32:
+        # 32-bit windows requires explicit sse2 option
+        EXTRA_COMPILE_ARGS += ['/arch:SSE2']
+    elif not is_msvc:
+        # Some bit generators require c99
+        EXTRA_COMPILE_ARGS += ['-std=c99']
+
+    if sys.platform == 'cygwin':
+        # Export symbols without __declspec(dllexport) for using by cython.
+        # Using __declspec(dllexport) does not export other necessary symbols
+        # in Cygwin package's Cython environment, making it impossible to
+        # import modules.
+        EXTRA_LINK_ARGS += ['-Wl,--export-all-symbols']
+
+    # Use legacy integer variable sizes
+    LEGACY_DEFS = [('NP_RANDOM_LEGACY', '1')]
+    PCG64_DEFS = []
+    # One can force emulated 128-bit arithmetic if one wants.
+    #PCG64_DEFS += [('PCG_FORCE_EMULATED_128BIT_MATH', '1')]
+    depends = ['__init__.pxd', 'c_distributions.pxd', 'bit_generator.pxd']
+
+    # npyrandom - a library like npymath
+    npyrandom_sources = [
+        'src/distributions/logfactorial.c',
+        'src/distributions/distributions.c',
+        'src/distributions/random_mvhg_count.c',
+        'src/distributions/random_mvhg_marginals.c',
+        'src/distributions/random_hypergeometric.c',
+    ]
+
+    def gl_if_msvc(build_cmd):
+        """ Add flag if we are using MSVC compiler
+
+        We can't see this in our scope, because we have not initialized the
+        distutils build command, so use this deferred calculation to run when
+        we are building the library.
+        """
+        # Keep in sync with numpy/core/setup.py
+        if build_cmd.compiler.compiler_type == 'msvc':
+            # explicitly disable whole-program optimization
+            return ['/GL-']
+        return []
+
+    config.add_installed_library('npyrandom',
+        sources=npyrandom_sources,
+        install_dir='lib',
+        build_info={
+            'include_dirs' : [],  # empty list required for creating npyrandom.h
+            'extra_compiler_args': [gl_if_msvc],
+        })
+
+    for gen in ['mt19937']:
+        # gen.pyx, src/gen/gen.c, src/gen/gen-jump.c
+        config.add_extension(f'_{gen}',
+                             sources=[f'_{gen}.c',
+                                      f'src/{gen}/{gen}.c',
+                                      f'src/{gen}/{gen}-jump.c'],
+                             include_dirs=['.', 'src', join('src', gen)],
+                             libraries=EXTRA_LIBRARIES,
+                             extra_compile_args=EXTRA_COMPILE_ARGS,
+                             extra_link_args=EXTRA_LINK_ARGS,
+                             depends=depends + [f'_{gen}.pyx'],
+                             define_macros=defs,
+                             )
+    for gen in ['philox', 'pcg64', 'sfc64']:
+        # gen.pyx, src/gen/gen.c
+        _defs = defs + PCG64_DEFS if gen == 'pcg64' else defs
+        config.add_extension(f'_{gen}',
+                             sources=[f'_{gen}.c',
+                                      f'src/{gen}/{gen}.c'],
+                             include_dirs=['.', 'src', join('src', gen)],
+                             libraries=EXTRA_LIBRARIES,
+                             extra_compile_args=EXTRA_COMPILE_ARGS,
+                             extra_link_args=EXTRA_LINK_ARGS,
+                             depends=depends + [f'_{gen}.pyx',
+                                   'bit_generator.pyx', 'bit_generator.pxd'],
+                             define_macros=_defs,
+                             )
+    for gen in ['_common', 'bit_generator']:
+        # gen.pyx
+        config.add_extension(gen,
+                             sources=[f'{gen}.c'],
+                             libraries=EXTRA_LIBRARIES,
+                             extra_compile_args=EXTRA_COMPILE_ARGS,
+                             extra_link_args=EXTRA_LINK_ARGS,
+                             include_dirs=['.', 'src'],
+                             depends=depends + [f'{gen}.pyx', f'{gen}.pxd',],
+                             define_macros=defs,
+                             )
+        config.add_data_files(f'{gen}.pxd')
+    for gen in ['_generator', '_bounded_integers']:
+        # gen.pyx, src/distributions/distributions.c
+        config.add_extension(gen,
+                             sources=[f'{gen}.c'],
+                             libraries=EXTRA_LIBRARIES + ['npymath'],
+                             extra_compile_args=EXTRA_COMPILE_ARGS,
+                             include_dirs=['.', 'src'],
+                             extra_link_args=EXTRA_LINK_ARGS,
+                             depends=depends + [f'{gen}.pyx'],
+                             define_macros=defs,
+                             )
+    config.add_data_files('_bounded_integers.pxd')
+    mtrand_libs = ['m', 'npymath'] if os.name != 'nt' else ['npymath']
     config.add_extension('mtrand',
-                         sources=[join('mtrand', x) for x in
-                                  ['mtrand.c', 'randomkit.c', 'initarray.c',
-                                   'distributions.c']]+[generate_libraries],
-                         libraries=libs,
-                         depends = [join('mtrand','*.h'),
-                                    join('mtrand','*.pyx'),
-                                    join('mtrand','*.pxi'),
-                                    ]
-                        )
-
-    config.add_data_files(('.', join('mtrand', 'randomkit.h')))
-    
+                         sources=['mtrand.c',
+                                  'src/legacy/legacy-distributions.c',
+                                  'src/distributions/distributions.c',
+                                 ],
+                         include_dirs=['.', 'src', 'src/legacy'],
+                         libraries=mtrand_libs,
+                         extra_compile_args=EXTRA_COMPILE_ARGS,
+                         extra_link_args=EXTRA_LINK_ARGS,
+                         depends=depends + ['mtrand.pyx'],
+                         define_macros=defs + LEGACY_DEFS,
+                         )
+    config.add_data_files(*depends)
+    config.add_data_files('*.pyi')
     return config
 
-def testcode_wincrypt():
-    return """\
-/* check to see if _WIN32 is defined */
-int main(int argc, char *argv[])
-{
-#ifdef _WIN32
-    return 0;
-#else
-#error No _WIN32
-#endif
-    return -1;
-}
-"""
 
 if __name__ == '__main__':
     from numpy.distutils.core import setup
-    setup(**configuration(top_path='').todict())
+
+    setup(configuration=configuration)
