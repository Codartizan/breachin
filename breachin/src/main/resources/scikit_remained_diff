('', 'conftest.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,73 +1,6 @@
 # Even if empty this file is useful so that when running from the root folder
 # ./sklearn is added to sys.path by pytest. See
-# https://docs.pytest.org/en/latest/pythonpath.html for more details.  For
-# example, this allows to build extensions in place and run pytest
+# https://docs.pytest.org/en/latest/explanation/pythonpath.html for more
+# details. For example, this allows to build extensions in place and run pytest
 # doc/modules/clustering.rst and use sklearn from the local folder rather than
 # the one from site-packages.
-
-import platform
-from distutils.version import LooseVersion
-
-import pytest
-from _pytest.doctest import DoctestItem
-
-PYTEST_MIN_VERSION = '3.3.0'
-
-if LooseVersion(pytest.__version__) < PYTEST_MIN_VERSION:
-    raise ImportError('Your version of pytest is too old, you should have '
-                      'at least pytest >= {} installed.'
-                      .format(PYTEST_MIN_VERSION))
-
-
-def pytest_addoption(parser):
-    parser.addoption("--skip-network", action="store_true", default=False,
-                     help="skip network tests")
-
-
-def pytest_collection_modifyitems(config, items):
-
-    # FeatureHasher is not compatible with PyPy
-    if platform.python_implementation() == 'PyPy':
-        skip_marker = pytest.mark.skip(
-            reason='FeatureHasher is not compatible with PyPy')
-        for item in items:
-            if item.name in (
-                    'sklearn.feature_extraction.hashing.FeatureHasher',
-                    'sklearn.feature_extraction.text.HashingVectorizer'):
-                item.add_marker(skip_marker)
-
-    # Skip tests which require internet if the flag is provided
-    if config.getoption("--skip-network"):
-        skip_network = pytest.mark.skip(
-            reason="test requires internet connectivity")
-        for item in items:
-            if "network" in item.keywords:
-                item.add_marker(skip_network)
-
-    # numpy changed the str/repr formatting of numpy arrays in 1.14. We want to
-    # run doctests only for numpy >= 1.14.
-    skip_doctests = False
-    try:
-        import numpy as np
-        if LooseVersion(np.__version__) < LooseVersion('1.14'):
-            skip_doctests = True
-    except ImportError:
-        pass
-
-    if skip_doctests:
-        skip_marker = pytest.mark.skip(
-            reason='doctests are only run for numpy >= 1.14 and python >= 3')
-
-        for item in items:
-            if isinstance(item, DoctestItem):
-                item.add_marker(skip_marker)
-
-
-def pytest_configure(config):
-    import sys
-    sys._is_pytest_session = True
-
-
-def pytest_unconfigure(config):
-    import sys
-    del sys._is_pytest_session
('', '.codecov.yml')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,7 +4,7 @@
   status:
     project:
       default:
-        # Commits pushed to master should not make the overall
+        # Commits pushed to main should not make the overall
         # project coverage decrease by more than 1%:
         target: auto
         threshold: 1%
@@ -12,13 +12,20 @@
       default:
         # Be tolerant on slight code coverage diff on PRs to limit
         # noisy red coverage status on github PRs.
-        # Note The coverage stats are still uploaded
+        # Note: The coverage stats are still uploaded
         # to codecov so that PR reviewers can see uncovered lines
-        # in the github diff if they install the codecov browser
-        # extension:
-        # https://github.com/codecov/browser-extension
         target: auto
         threshold: 1%

+codecov:
+  notify:
+    # Prevent coverage status to upload multiple times for parallel and long
+    # running CI pipelines. This configuration is particularly useful on PRs
+    # to avoid confusion. Note that this value is set to the number of Azure
+    # Pipeline jobs uploading coverage reports.
+    after_n_builds: 6
+
 ignore:
 - "sklearn/externals"
+- "sklearn/_build_utils"
+- "**/setup.py"
('', 'Makefile')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -31,10 +31,15 @@
 ifeq ($(BITS),64)
 	$(PYTEST) $(shell find doc -name '*.rst' | sort)
 endif
+test-code-parallel: in
+	$(PYTEST) -n auto --showlocals -v sklearn --durations=20

 test-coverage:
 	rm -rf coverage .coverage
 	$(PYTEST) sklearn --showlocals -v --cov=sklearn --cov-report=html:coverage
+test-coverage-parallel:
+	rm -rf coverage .coverage .coverage.*
+	$(PYTEST) sklearn -n auto --showlocals -v --cov=sklearn --cov-report=html:coverage

 test: test-code test-sphinxext test-doc

@@ -60,4 +65,4 @@
 	pylint -E -i y sklearn/ -d E1103,E0611,E1101

 flake8-diff:
-	./build_tools/circle/flake8_diff.sh
+	git diff upstream/main -u -- "*.py" | flake8 --diff
('', '.coveragerc')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,5 +4,6 @@
 parallel = True
 omit =
     */sklearn/externals/*
+    */sklearn/_build_utils/*
     */benchmarks/*
-    */setup.py
+    **/setup.py
('', 'azure-pipelines.yml')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,76 +1,273 @@
 # Adapted from https://github.com/pandas-dev/pandas/blob/master/azure-pipelines.yml
+schedules:
+- cron: "30 2 * * *"
+  displayName: Run nightly build
+  branches:
+    include:
+    - main
+  always: true
+
 jobs:
+- job: git_commit
+  displayName: Get Git Commit
+  pool:
+    vmImage: ubuntu-20.04
+  steps:
+    - bash: python build_tools/azure/get_commit_message.py
+      name: commit
+      displayName: Get source version message
+
+- job: linting
+  dependsOn: [git_commit]
+  condition: |
+    and(
+      succeeded(),
+      not(contains(dependencies['git_commit']['outputs']['commit.message'], '[lint skip]')),
+      not(contains(dependencies['git_commit']['outputs']['commit.message'], '[ci skip]'))
+    )
+  displayName: Linting
+  pool:
+    vmImage: ubuntu-20.04
+  steps:
+    - task: UsePythonVersion@0
+      inputs:
+        versionSpec: '3.9'
+    - bash: |
+        # Include pytest compatibility with mypy
+        pip install pytest flake8 mypy==0.782 black==22.3.0
+      displayName: Install linters
+    - bash: |
+        black --check --diff .
+      displayName: Run black
+    - bash: |
+        ./build_tools/circle/linting.sh
+      displayName: Run linting
+    - bash: |
+        mypy sklearn/
+      displayName: Run mypy
+
+- template: build_tools/azure/posix.yml
+  parameters:
+    name: Linux_Nightly
+    vmImage: ubuntu-20.04
+    dependsOn: [git_commit, linting]
+    condition: |
+      and(
+        succeeded(),
+        not(contains(dependencies['git_commit']['outputs']['commit.message'], '[ci skip]')),
+        or(eq(variables['Build.Reason'], 'Schedule'),
+           contains(dependencies['git_commit']['outputs']['commit.message'], '[scipy-dev]'
+          )
+        )
+      )
+    matrix:
+      pylatest_pip_scipy_dev:
+        DISTRIB: 'conda-pip-scipy-dev'
+        PYTHON_VERSION: '*'
+        CHECK_WARNINGS: 'true'
+        CHECK_PYTEST_SOFT_DEPENDENCY: 'true'
+        TEST_DOCSTRINGS: 'true'
+        # Tests that require large downloads over the networks are skipped in CI.
+        # Here we make sure, that they are still run on a regular basis.
+        SKLEARN_SKIP_NETWORK_TESTS: '0'
+
+# Check compilation with intel C++ compiler (ICC)
+- template: build_tools/azure/posix.yml
+  parameters:
+    name: Linux_Nightly_ICC
+    vmImage: ubuntu-20.04
+    dependsOn: [git_commit, linting]
+    condition: |
+      and(
+        succeeded(),
+        not(contains(dependencies['git_commit']['outputs']['commit.message'], '[ci skip]')),
+        or(eq(variables['Build.Reason'], 'Schedule'),
+           contains(dependencies['git_commit']['outputs']['commit.message'], '[icc-build]')
+        )
+      )
+    matrix:
+      pylatest_conda_forge_mkl:
+        DISTRIB: 'conda'
+        CONDA_CHANNEL: 'conda-forge'
+        PYTHON_VERSION: '*'
+        BLAS: 'mkl'
+        COVERAGE: 'false'
+        BUILD_WITH_ICC: 'true'
+
+- template: build_tools/azure/posix-docker.yml
+  parameters:
+    name: Linux_Nightly_PyPy
+    vmImage: ubuntu-20.04
+    dependsOn: [linting, git_commit]
+    condition: |
+      and(
+        succeeded(),
+        not(contains(dependencies['git_commit']['outputs']['commit.message'], '[ci skip]')),
+        or(
+          eq(variables['Build.Reason'], 'Schedule'),
+          contains(dependencies['git_commit']['outputs']['commit.message'], '[pypy]')
+        )
+      )
+    matrix:
+      pypy3:
+        DISTRIB: 'conda-mamba-pypy3'
+        DOCKER_CONTAINER: 'condaforge/mambaforge-pypy3:4.10.3-5'
+        PILLOW_VERSION: 'none'
+        PANDAS_VERSION: 'none'
+
+# Will run all the time regardless of linting outcome.
+- template: build_tools/azure/posix.yml
+  parameters:
+    name: Linux_Runs
+    vmImage: ubuntu-20.04
+    dependsOn: [git_commit]
+    condition: |
+      and(
+        succeeded(),
+        not(contains(dependencies['git_commit']['outputs']['commit.message'], '[ci skip]'))
+      )
+    matrix:
+      pylatest_conda_forge_mkl:
+        DISTRIB: 'conda'
+        CONDA_CHANNEL: 'conda-forge'
+        PYTHON_VERSION: '*'
+        BLAS: 'mkl'
+        COVERAGE: 'true'
+        SHOW_SHORT_SUMMARY: 'true'
+        SKLEARN_TESTS_GLOBAL_RANDOM_SEED: '42'  # default global random seed
+
+# Check compilation with Ubuntu bionic 18.04 LTS and scipy from conda-forge
+- template: build_tools/azure/posix.yml
+  parameters:
+    name: Ubuntu_Bionic
+    vmImage: ubuntu-18.04
+    dependsOn: [git_commit, linting]
+    condition: |
+      and(
+        succeeded(),
+        not(contains(dependencies['git_commit']['outputs']['commit.message'], '[ci skip]'))
+      )
+    matrix:
+      py38_conda_forge_openblas_ubuntu_1804:
+        DISTRIB: 'conda'
+        CONDA_CHANNEL: 'conda-forge'
+        PYTHON_VERSION: '3.8'
+        BLAS: 'openblas'
+        COVERAGE: 'false'
+        BUILD_WITH_ICC: 'false'
+        SKLEARN_TESTS_GLOBAL_RANDOM_SEED: '0'  # non-default seed
+
 - template: build_tools/azure/posix.yml
   parameters:
     name: Linux
-    vmImage: ubuntu-16.04
+    vmImage: ubuntu-20.04
+    dependsOn: [linting, git_commit]
+    condition: |
+      and(
+        succeeded(),
+        not(contains(dependencies['git_commit']['outputs']['commit.message'], '[ci skip]'))
+      )
     matrix:
       # Linux environment to test that scikit-learn can be built against
-      # versions of numpy, scipy with ATLAS that comes with Ubuntu Xenial 16.04
-      # i.e. numpy 1.11 and scipy 0.17
-      py35_np_atlas:
+      # versions of numpy, scipy with ATLAS that comes with Ubuntu Focal 20.04
+      # i.e. numpy 1.17.4 and scipy 1.3.3
+      ubuntu_atlas:
         DISTRIB: 'ubuntu'
-        PYTHON_VERSION: '3.5'
-        JOBLIB_VERSION: '0.11'
-        SKLEARN_NO_OPENMP: 'True'
-      # Linux + Python 3.5 build with OpenBLAS and without SITE_JOBLIB
-      py35_conda_openblas:
-        DISTRIB: 'conda'
-        PYTHON_VERSION: '3.5'
-        INSTALL_MKL: 'false'
-        NUMPY_VERSION: '1.11.0'
-        SCIPY_VERSION: '0.17.0'
-        CYTHON_VERSION: '*'
-        PILLOW_VERSION: '4.0.0'
-        MATPLOTLIB_VERSION: '1.5.1'
-        # later version of joblib are not packaged in conda for Python 3.5
-        JOBLIB_VERSION: '0.12.3'
-        COVERAGE: 'true'
-      # Linux environment to test the latest available dependencies and MKL.
-      # It runs tests requiring pandas and PyAMG.
-      pylatest_conda:
-        DISTRIB: 'conda'
-        PYTHON_VERSION: '*'
-        INSTALL_MKL: 'true'
-        NUMPY_VERSION: '*'
-        SCIPY_VERSION: '*'
-        PANDAS_VERSION: '*'
-        CYTHON_VERSION: '*'
-        PYAMG_VERSION: '*'
-        PILLOW_VERSION: '*'
-        JOBLIB_VERSION: '*'
-        MATPLOTLIB_VERSION: '*'
-        COVERAGE: 'true'
+        JOBLIB_VERSION: 'min'
+        PANDAS_VERSION: 'none'
+        THREADPOOLCTL_VERSION: 'min'
+        COVERAGE: 'false'
+        SKLEARN_TESTS_GLOBAL_RANDOM_SEED: '1'  # non-default seed
+      # Linux + Python 3.8 build with OpenBLAS
+      py38_conda_defaults_openblas:
+        DISTRIB: 'conda'
+        CONDA_CHANNEL: 'defaults'  # Anaconda main channel
+        PYTHON_VERSION: '3.8'
+        BLAS: 'openblas'
+        NUMPY_VERSION: 'min'
+        SCIPY_VERSION: 'min'
+        MATPLOTLIB_VERSION: 'min'
+        THREADPOOLCTL_VERSION: '2.2.0'
+        SKLEARN_ENABLE_DEBUG_CYTHON_DIRECTIVES: '1'
+        SKLEARN_RUN_FLOAT32_TESTS: '1'
+        SKLEARN_TESTS_GLOBAL_RANDOM_SEED: '2'  # non-default seed
+      # Linux environment to test the latest available dependencies.
+      # It runs tests requiring lightgbm, pandas and PyAMG.
+      pylatest_pip_openblas_pandas:
+        DISTRIB: 'conda-pip-latest'
+        PYTHON_VERSION: '3.9'
+        PYTEST_VERSION: '6.2.5'
         CHECK_PYTEST_SOFT_DEPENDENCY: 'true'
         TEST_DOCSTRINGS: 'true'
         CHECK_WARNINGS: 'true'
+        SKLEARN_TESTS_GLOBAL_RANDOM_SEED: '3'  # non-default seed
+
+- template: build_tools/azure/posix-docker.yml
+  parameters:
+    name: Linux_Docker
+    vmImage: ubuntu-20.04
+    dependsOn: [linting, git_commit]
+    condition: |
+      and(
+        succeeded(),
+        not(contains(dependencies['git_commit']['outputs']['commit.message'], '[ci skip]'))
+      )
+    matrix:
+      debian_atlas_32bit:
+        DISTRIB: 'debian-32'
+        DOCKER_CONTAINER: 'i386/debian:11.2'
+        JOBLIB_VERSION: 'min'
+        # disable pytest xdist due to unknown bug with 32-bit container
+        PYTEST_XDIST_VERSION: 'none'
+        PYTEST_VERSION: 'min'
+        THREADPOOLCTL_VERSION: '2.2.0'
+        SKLEARN_TESTS_GLOBAL_RANDOM_SEED: '4'  # non-default seed

 - template: build_tools/azure/posix.yml
   parameters:
     name: macOS
-    vmImage: xcode9-macos10.13
-    matrix:
-      pylatest_conda:
-        DISTRIB: 'conda'
-        PYTHON_VERSION: '*'
-        INSTALL_MKL: 'true'
-        NUMPY_VERSION: '*'
-        SCIPY_VERSION: '*'
-        CYTHON_VERSION: '*'
-        PILLOW_VERSION: '*'
-        JOBLIB_VERSION: '*'
-        COVERAGE: 'true'
+    vmImage: macOS-10.15
+    dependsOn: [linting, git_commit]
+    condition: |
+      and(
+        succeeded(),
+        not(contains(dependencies['git_commit']['outputs']['commit.message'], '[ci skip]'))
+      )
+    matrix:
+      pylatest_conda_forge_mkl:
+        DISTRIB: 'conda'
+        BLAS: 'mkl'
+        CONDA_CHANNEL: 'conda-forge'
+        SKLEARN_TESTS_GLOBAL_RANDOM_SEED: '5'  # non-default seed
+      pylatest_conda_mkl_no_openmp:
+        DISTRIB: 'conda'
+        BLAS: 'mkl'
+        SKLEARN_TEST_NO_OPENMP: 'true'
+        SKLEARN_SKIP_OPENMP_TEST: 'true'
+        SKLEARN_TESTS_GLOBAL_RANDOM_SEED: '6'  # non-default seed

 - template: build_tools/azure/windows.yml
   parameters:
     name: Windows
-    vmImage: vs2017-win2016
-    matrix:
-      py37_64:
-        PYTHON_VERSION: '3.7'
+    vmImage: windows-latest
+    dependsOn: [linting, git_commit]
+    condition: |
+      and(
+        succeeded(),
+        not(contains(dependencies['git_commit']['outputs']['commit.message'], '[ci skip]'))
+      )
+    matrix:
+      py38_conda_forge_mkl:
+        DISTRIB: 'conda'
+        CONDA_CHANNEL: 'conda-forge'
+        PYTHON_VERSION: '3.8'
         CHECK_WARNINGS: 'true'
         PYTHON_ARCH: '64'
+        # Unpin when pytest stalling issue is fixed
+        PYTEST_VERSION: '6.2.5'
         COVERAGE: 'true'
-      py35_32:
-        PYTHON_VERSION: '3.5'
+        SKLEARN_TESTS_GLOBAL_RANDOM_SEED: '7'  # non-default seed
+      py38_pip_openblas_32bit:
+        PYTHON_VERSION: '3.8'
         PYTHON_ARCH: '32'
+        SKLEARN_TESTS_GLOBAL_RANDOM_SEED: '8'  # non-default seed
('', 'COPYING')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,32 +1,29 @@
-New BSD License
+BSD 3-Clause License

-Copyright (c) 2007–2019 The scikit-learn developers.
+Copyright (c) 2007-2021 The scikit-learn developers.
 All rights reserved.
-

 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions are met:

-  a. Redistributions of source code must retain the above copyright notice,
-     this list of conditions and the following disclaimer.
-  b. Redistributions in binary form must reproduce the above copyright
-     notice, this list of conditions and the following disclaimer in the
-     documentation and/or other materials provided with the distribution.
-  c. Neither the name of the Scikit-learn Developers  nor the names of
-     its contributors may be used to endorse or promote products
-     derived from this software without specific prior written
-     permission.
+* Redistributions of source code must retain the above copyright notice, this
+  list of conditions and the following disclaimer.

+* Redistributions in binary form must reproduce the above copyright notice,
+  this list of conditions and the following disclaimer in the documentation
+  and/or other materials provided with the distribution.
+
+* Neither the name of the copyright holder nor the names of its
+  contributors may be used to endorse or promote products derived from
+  this software without specific prior written permission.

 THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
-IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
-ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR
-ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
-CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
-LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
-OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
-DAMAGE.
-
+CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
('', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,78 +8,77 @@
 import os
 import platform
 import shutil
+
+# We need to import setuptools before because it monkey-patches distutils
+import setuptools  # noqa
 from distutils.command.clean import clean as Clean
-from pkg_resources import parse_version
+from distutils.command.sdist import sdist
+
 import traceback
+import importlib
+
 try:
     import builtins
-    # This is a bit (!) hackish: we are setting a global variable so that the
-    # main sklearn __init__ can detect if it is being loaded by the setup
-    # routine, to avoid attempting to load components that aren't built yet:
-    # the numpy distutils extensions that are used by scikit-learn to
-    # recursively build the compiled extensions in sub-packages is based on the
-    # Python import machinery.
-    builtins.__SKLEARN_SETUP__ = True
 except ImportError:
-    # Python 2 is not support but we will raise an explicit error message next.
-    pass
-
-if sys.version_info < (3, 5):
-    raise RuntimeError("Scikit-learn requires Python 3.5 or later. The current"
-                       " Python version is %s installed in %s."
-                       % (platform.python_version(), sys.executable))
-
-DISTNAME = 'scikit-learn'
-DESCRIPTION = 'A set of python modules for machine learning and data mining'
-with open('README.rst') as f:
+    # Python 2 compat: just to be able to declare that Python >=3.8 is needed.
+    import __builtin__ as builtins
+
+# This is a bit (!) hackish: we are setting a global variable so that the
+# main sklearn __init__ can detect if it is being loaded by the setup
+# routine, to avoid attempting to load components that aren't built yet:
+# the numpy distutils extensions that are used by scikit-learn to
+# recursively build the compiled extensions in sub-packages is based on the
+# Python import machinery.
+builtins.__SKLEARN_SETUP__ = True
+
+
+DISTNAME = "scikit-learn"
+DESCRIPTION = "A set of python modules for machine learning and data mining"
+with open("README.rst") as f:
     LONG_DESCRIPTION = f.read()
-MAINTAINER = 'Andreas Mueller'
-MAINTAINER_EMAIL = 'amueller@ais.uni-bonn.de'
-URL = 'http://scikit-learn.org'
-DOWNLOAD_URL = 'https://pypi.org/project/scikit-learn/#files'
-LICENSE = 'new BSD'
+MAINTAINER = "Andreas Mueller"
+MAINTAINER_EMAIL = "amueller@ais.uni-bonn.de"
+URL = "http://scikit-learn.org"
+DOWNLOAD_URL = "https://pypi.org/project/scikit-learn/#files"
+LICENSE = "new BSD"
 PROJECT_URLS = {
-    'Bug Tracker': 'https://github.com/scikit-learn/scikit-learn/issues',
-    'Documentation': 'https://scikit-learn.org/stable/documentation.html',
-    'Source Code': 'https://github.com/scikit-learn/scikit-learn'
+    "Bug Tracker": "https://github.com/scikit-learn/scikit-learn/issues",
+    "Documentation": "https://scikit-learn.org/stable/documentation.html",
+    "Source Code": "https://github.com/scikit-learn/scikit-learn",
 }

 # We can actually import a restricted version of sklearn that
 # does not need the compiled code
-import sklearn
+import sklearn  # noqa
+import sklearn._min_dependencies as min_deps  # noqa
+from sklearn.externals._packaging.version import parse as parse_version  # noqa
+

 VERSION = sklearn.__version__

-if platform.python_implementation() == 'PyPy':
-    SCIPY_MIN_VERSION = '1.1.0'
-    NUMPY_MIN_VERSION = '1.14.0'
-else:
-    SCIPY_MIN_VERSION = '0.17.0'
-    NUMPY_MIN_VERSION = '1.11.0'
-
-JOBLIB_MIN_VERSION = '0.11'
-
-# Optional setuptools features
-# We need to import setuptools early, if we want setuptools features,
-# as it monkey-patches the 'setup' function
+
 # For some commands, use setuptools
 SETUPTOOLS_COMMANDS = {
-    'develop', 'release', 'bdist_egg', 'bdist_rpm',
-    'bdist_wininst', 'install_egg_info', 'build_sphinx',
-    'egg_info', 'easy_install', 'upload', 'bdist_wheel',
-    '--single-version-externally-managed',
+    "develop",
+    "release",
+    "bdist_egg",
+    "bdist_rpm",
+    "bdist_wininst",
+    "install_egg_info",
+    "build_sphinx",
+    "egg_info",
+    "easy_install",
+    "upload",
+    "bdist_wheel",
+    "--single-version-externally-managed",
 }
 if SETUPTOOLS_COMMANDS.intersection(sys.argv):
-    import setuptools
-
     extra_setuptools_args = dict(
         zip_safe=False,  # the package can run out of an .egg file
         include_package_data=True,
         extras_require={
-            'alldeps': (
-                'numpy >= {}'.format(NUMPY_MIN_VERSION),
-                'scipy >= {}'.format(SCIPY_MIN_VERSION),
-            ),
+            key: min_deps.tag_to_packages[key]
+            for key in ["examples", "docs", "tests", "benchmark"]
         },
     )
 else:
@@ -88,6 +87,7 @@

 # Custom clean command to remove build artifacts

+
 class CleanCommand(Clean):
     description = "Remove build artifacts from the source tree"

@@ -95,183 +95,231 @@
         Clean.run(self)
         # Remove c files if we are not within a sdist package
         cwd = os.path.abspath(os.path.dirname(__file__))
-        remove_c_files = not os.path.exists(os.path.join(cwd, 'PKG-INFO'))
+        remove_c_files = not os.path.exists(os.path.join(cwd, "PKG-INFO"))
         if remove_c_files:
-            print('Will remove generated .c files')
-        if os.path.exists('build'):
-            shutil.rmtree('build')
-        for dirpath, dirnames, filenames in os.walk('sklearn'):
+            print("Will remove generated .c files")
+        if os.path.exists("build"):
+            shutil.rmtree("build")
+        for dirpath, dirnames, filenames in os.walk("sklearn"):
             for filename in filenames:
-                if any(filename.endswith(suffix) for suffix in
-                       (".so", ".pyd", ".dll", ".pyc")):
+                if any(
+                    filename.endswith(suffix)
+                    for suffix in (".so", ".pyd", ".dll", ".pyc")
+                ):
                     os.unlink(os.path.join(dirpath, filename))
                     continue
                 extension = os.path.splitext(filename)[1]
-                if remove_c_files and extension in ['.c', '.cpp']:
-                    pyx_file = str.replace(filename, extension, '.pyx')
+                if remove_c_files and extension in [".c", ".cpp"]:
+                    pyx_file = str.replace(filename, extension, ".pyx")
                     if os.path.exists(os.path.join(dirpath, pyx_file)):
                         os.unlink(os.path.join(dirpath, filename))
             for dirname in dirnames:
-                if dirname == '__pycache__':
+                if dirname == "__pycache__":
                     shutil.rmtree(os.path.join(dirpath, dirname))


-# custom build_ext command to set OpenMP compile flags depending on os and
-# compiler
+cmdclass = {"clean": CleanCommand, "sdist": sdist}
+
+# Custom build_ext command to set OpenMP compile flags depending on os and
+# compiler. Also makes it possible to set the parallelism level via
+# and environment variable (useful for the wheel building CI).
 # build_ext has to be imported after setuptools
-from numpy.distutils.command.build_ext import build_ext  # noqa
-
-
-class build_ext_subclass(build_ext):
-    def build_extensions(self):
-        from sklearn._build_utils.openmp_helpers import get_openmp_flag
-
-        if not os.getenv('SKLEARN_NO_OPENMP'):
-            openmp_flag = get_openmp_flag(self.compiler)
-
-            for e in self.extensions:
-                e.extra_compile_args += openmp_flag
-                e.extra_link_args += openmp_flag
-
-        build_ext.build_extensions(self)
-
-
-cmdclass = {'clean': CleanCommand, 'build_ext': build_ext_subclass}
-
-
-# Optional wheelhouse-uploader features
-# To automate release of binary packages for scikit-learn we need a tool
-# to download the packages generated by travis and appveyor workers (with
-# version number matching the current release) and upload them all at once
-# to PyPI at release time.
-# The URL of the artifact repositories are configured in the setup.cfg file.
-
-WHEELHOUSE_UPLOADER_COMMANDS = {'fetch_artifacts', 'upload_all'}
-if WHEELHOUSE_UPLOADER_COMMANDS.intersection(sys.argv):
-    import wheelhouse_uploader.cmd
-
-    cmdclass.update(vars(wheelhouse_uploader.cmd))
-
-
-def configuration(parent_package='', top_path=None):
-    if os.path.exists('MANIFEST'):
-        os.remove('MANIFEST')
+try:
+    from numpy.distutils.command.build_ext import build_ext  # noqa
+
+    class build_ext_subclass(build_ext):
+        def finalize_options(self):
+            super().finalize_options()
+            if self.parallel is None:
+                # Do not override self.parallel if already defined by
+                # command-line flag (--parallel or -j)
+
+                parallel = os.environ.get("SKLEARN_BUILD_PARALLEL")
+                if parallel:
+                    self.parallel = int(parallel)
+            if self.parallel:
+                print("setting parallel=%d " % self.parallel)
+
+        def build_extensions(self):
+            from sklearn._build_utils.openmp_helpers import get_openmp_flag
+
+            if sklearn._OPENMP_SUPPORTED:
+                openmp_flag = get_openmp_flag(self.compiler)
+
+                for e in self.extensions:
+                    e.extra_compile_args += openmp_flag
+                    e.extra_link_args += openmp_flag
+
+            build_ext.build_extensions(self)
+
+    cmdclass["build_ext"] = build_ext_subclass
+
+except ImportError:
+    # Numpy should not be a dependency just to be able to introspect
+    # that python 3.8 is required.
+    pass
+
+
+def configuration(parent_package="", top_path=None):
+    if os.path.exists("MANIFEST"):
+        os.remove("MANIFEST")

     from numpy.distutils.misc_util import Configuration
+    from sklearn._build_utils import _check_cython_version

     config = Configuration(None, parent_package, top_path)

-    # Avoid non-useful msg:
+    # Avoid useless msg:
     # "Ignoring attempt to set 'name' (from ... "
-    config.set_options(ignore_setup_xxx_py=True,
-                       assume_default_configuration=True,
-                       delegate_options_to_subpackages=True,
-                       quiet=True)
-
-    config.add_subpackage('sklearn')
+    config.set_options(
+        ignore_setup_xxx_py=True,
+        assume_default_configuration=True,
+        delegate_options_to_subpackages=True,
+        quiet=True,
+    )
+
+    # Cython is required by config.add_subpackage for templated extensions
+    # that need the tempita sub-submodule. So check that we have the correct
+    # version of Cython so as to be able to raise a more informative error
+    # message from the start if it's not the case.
+    _check_cython_version()
+
+    config.add_subpackage("sklearn")

     return config


-def get_numpy_status():
+def check_package_status(package, min_version):
     """
-    Returns a dictionary containing a boolean specifying whether NumPy
+    Returns a dictionary containing a boolean specifying whether given package
     is up-to-date, along with the version string (empty string if
     not installed).
     """
-    numpy_status = {}
+    package_status = {}
     try:
-        import numpy
-        numpy_version = numpy.__version__
-        numpy_status['up_to_date'] = parse_version(
-            numpy_version) >= parse_version(NUMPY_MIN_VERSION)
-        numpy_status['version'] = numpy_version
+        module = importlib.import_module(package)
+        package_version = module.__version__
+        package_status["up_to_date"] = parse_version(package_version) >= parse_version(
+            min_version
+        )
+        package_status["version"] = package_version
     except ImportError:
         traceback.print_exc()
-        numpy_status['up_to_date'] = False
-        numpy_status['version'] = ""
-    return numpy_status
+        package_status["up_to_date"] = False
+        package_status["version"] = ""
+
+    req_str = "scikit-learn requires {} >= {}.\n".format(package, min_version)
+
+    instructions = (
+        "Installation instructions are available on the "
+        "scikit-learn website: "
+        "http://scikit-learn.org/stable/install.html\n"
+    )
+
+    if package_status["up_to_date"] is False:
+        if package_status["version"]:
+            raise ImportError(
+                "Your installation of {} {} is out-of-date.\n{}{}".format(
+                    package, package_status["version"], req_str, instructions
+                )
+            )
+        else:
+            raise ImportError(
+                "{} is not installed.\n{}{}".format(package, req_str, instructions)
+            )


 def setup_package():
-    metadata = dict(name=DISTNAME,
-                    maintainer=MAINTAINER,
-                    maintainer_email=MAINTAINER_EMAIL,
-                    description=DESCRIPTION,
-                    license=LICENSE,
-                    url=URL,
-                    download_url=DOWNLOAD_URL,
-                    project_urls=PROJECT_URLS,
-                    version=VERSION,
-                    long_description=LONG_DESCRIPTION,
-                    classifiers=['Intended Audience :: Science/Research',
-                                 'Intended Audience :: Developers',
-                                 'License :: OSI Approved',
-                                 'Programming Language :: C',
-                                 'Programming Language :: Python',
-                                 'Topic :: Software Development',
-                                 'Topic :: Scientific/Engineering',
-                                 'Operating System :: Microsoft :: Windows',
-                                 'Operating System :: POSIX',
-                                 'Operating System :: Unix',
-                                 'Operating System :: MacOS',
-                                 'Programming Language :: Python :: 3',
-                                 'Programming Language :: Python :: 3.5',
-                                 'Programming Language :: Python :: 3.6',
-                                 'Programming Language :: Python :: 3.7',
-                                 ('Programming Language :: Python :: '
-                                  'Implementation :: CPython'),
-                                 ('Programming Language :: Python :: '
-                                  'Implementation :: PyPy')
-                                 ],
-                    cmdclass=cmdclass,
-                    install_requires=[
-                        'numpy>={}'.format(NUMPY_MIN_VERSION),
-                        'scipy>={}'.format(SCIPY_MIN_VERSION),
-                        'joblib>={}'.format(JOBLIB_MIN_VERSION)
-                    ],
-                    **extra_setuptools_args)
-
-    if len(sys.argv) == 1 or (
-            len(sys.argv) >= 2 and ('--help' in sys.argv[1:] or
-                                    sys.argv[1] in ('--help-commands',
-                                                    'egg_info',
-                                                    '--version',
-                                                    'clean'))):
-        # For these actions, NumPy is not required
-        #
-        # They are required to succeed without Numpy for example when
+
+    # TODO: Require Python 3.8 for PyPy when PyPy3.8 is ready
+    # https://github.com/conda-forge/conda-forge-pinning-feedstock/issues/2089
+    if platform.python_implementation() == "PyPy":
+        python_requires = ">=3.7"
+        required_python_version = (3, 7)
+    else:
+        python_requires = ">=3.8"
+        required_python_version = (3, 8)
+
+    metadata = dict(
+        name=DISTNAME,
+        maintainer=MAINTAINER,
+        maintainer_email=MAINTAINER_EMAIL,
+        description=DESCRIPTION,
+        license=LICENSE,
+        url=URL,
+        download_url=DOWNLOAD_URL,
+        project_urls=PROJECT_URLS,
+        version=VERSION,
+        long_description=LONG_DESCRIPTION,
+        classifiers=[
+            "Intended Audience :: Science/Research",
+            "Intended Audience :: Developers",
+            "License :: OSI Approved",
+            "Programming Language :: C",
+            "Programming Language :: Python",
+            "Topic :: Software Development",
+            "Topic :: Scientific/Engineering",
+            "Development Status :: 5 - Production/Stable",
+            "Operating System :: Microsoft :: Windows",
+            "Operating System :: POSIX",
+            "Operating System :: Unix",
+            "Operating System :: MacOS",
+            "Programming Language :: Python :: 3",
+            "Programming Language :: Python :: 3.8",
+            "Programming Language :: Python :: 3.9",
+            "Programming Language :: Python :: 3.10",
+            "Programming Language :: Python :: Implementation :: CPython",
+            "Programming Language :: Python :: Implementation :: PyPy",
+        ],
+        cmdclass=cmdclass,
+        python_requires=python_requires,
+        install_requires=min_deps.tag_to_packages["install"],
+        package_data={"": ["*.pxd"]},
+        **extra_setuptools_args,
+    )
+
+    commands = [arg for arg in sys.argv[1:] if not arg.startswith("-")]
+    if all(
+        command in ("egg_info", "dist_info", "clean", "check") for command in commands
+    ):
+        # These actions are required to succeed without Numpy for example when
         # pip is used to install Scikit-learn when Numpy is not yet present in
         # the system.
-        try:
-            from setuptools import setup
-        except ImportError:
-            from distutils.core import setup
-
-        metadata['version'] = VERSION
+
+        # These commands use setup from setuptools
+        from setuptools import setup
+
+        metadata["version"] = VERSION
     else:
-        numpy_status = get_numpy_status()
-        numpy_req_str = "scikit-learn requires NumPy >= {}.\n".format(
-            NUMPY_MIN_VERSION)
-
-        instructions = ("Installation instructions are available on the "
-                        "scikit-learn website: "
-                        "http://scikit-learn.org/stable/install.html\n")
-
-        if numpy_status['up_to_date'] is False:
-            if numpy_status['version']:
-                raise ImportError("Your installation of Numerical Python "
-                                  "(NumPy) {} is out-of-date.\n{}{}"
-                                  .format(numpy_status['version'],
-                                          numpy_req_str, instructions))
-            else:
-                raise ImportError("Numerical Python (NumPy) is not "
-                                  "installed.\n{}{}"
-                                  .format(numpy_req_str, instructions))
-
+        if sys.version_info < required_python_version:
+            required_version = "%d.%d" % required_python_version
+            raise RuntimeError(
+                "Scikit-learn requires Python %s or later. The current"
+                " Python version is %s installed in %s."
+                % (required_version, platform.python_version(), sys.executable)
+            )
+
+        check_package_status("numpy", min_deps.NUMPY_MIN_VERSION)
+
+        check_package_status("scipy", min_deps.SCIPY_MIN_VERSION)
+
+        # These commands require the setup from numpy.distutils because they
+        # may use numpy.distutils compiler classes.
         from numpy.distutils.core import setup

-        metadata['configuration'] = configuration
+        # Monkeypatches CCompiler.spawn to prevent random wheel build errors on Windows
+        # The build errors on Windows was because msvccompiler spawn was not threadsafe
+        # This fixed can be removed when we build with numpy >= 1.22.2 on Windows.
+        # https://github.com/pypa/distutils/issues/5
+        # https://github.com/scikit-learn/scikit-learn/issues/22310
+        # https://github.com/numpy/numpy/pull/20640
+        from numpy.distutils.ccompiler import replace_method
+        from distutils.ccompiler import CCompiler
+        from sklearn.externals._numpy_compiler_patch import CCompiler_spawn
+
+        replace_method(CCompiler, "spawn", CCompiler_spawn)
+
+        metadata["configuration"] = configuration

     setup(**metadata)

('', 'setup.cfg')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,25 +4,68 @@
 [tool:pytest]
 # disable-pytest-warnings should be removed once we rewrite tests
 # using yield with parametrize
+doctest_optionflags = NORMALIZE_WHITESPACE ELLIPSIS
+testpaths = sklearn
 addopts =
-    --ignore build_tools
-    --ignore benchmarks
-    --ignore doc
-    --ignore examples
-    --ignore maint_tools
     --doctest-modules
     --disable-pytest-warnings
-    -rs
+    --color=yes
+    # Activate the plugin explicitly to ensure that the seed is reported
+    # correctly on the CI when running `pytest --pyargs sklearn` from the
+    # source folder.
+    -p sklearn.tests.random_seed
+    -rN

 filterwarnings =
     ignore:the matrix subclass:PendingDeprecationWarning

-[wheelhouse_uploader]
-artifact_indexes=
-    # Wheels built by travis (only for specific tags):
-    # https://github.com/MacPython/scikit-learn-wheels
-    http://wheels.scipy.org
+[flake8]
+# max line length for black
+max-line-length = 88
+target-version = ['py37']
+# Default flake8 3.5 ignored flags
+ignore=
+    E24,   # check ignored by default in flake8. Meaning unclear.
+    E121,  # continuation line under-indented
+    E123,  # closing bracket does not match indentation
+    E126,  # continuation line over-indented for hanging indent
+    E203,  # space before : (needed for how black formats slicing)
+    E226,  # missing whitespace around arithmetic operator
+    E704,  # multiple statements on one line (def)
+    E731,  # do not assign a lambda expression, use a def
+    E741,  # do not use variables named ‘l’, ‘O’, or ‘I’
+    W503,  # line break before binary operator
+    W504   # line break after binary operator
+exclude=
+    .git,
+    __pycache__,
+    dist,
+    sklearn/externals,
+    doc/_build,
+    doc/auto_examples,
+    doc/tutorial,
+    build

-[flake8]
-# Default flake8 3.5 ignored flags
-ignore=E121,E123,E126,E226,E24,E704,W503,W504
+# It's fine not to put the import at the top of the file in the examples
+# folder.
+per-file-ignores =
+    examples/*: E402
+    doc/conf.py: E402
+
+[mypy]
+ignore_missing_imports = True
+allow_redefinition = True
+
+[check-manifest]
+# ignore files missing in VCS
+ignore =
+    sklearn/_loss/_loss.pyx
+    sklearn/linear_model/_sag_fast.pyx
+    sklearn/utils/_seq_dataset.pyx
+    sklearn/utils/_seq_dataset.pxd
+    sklearn/utils/_weight_vector.pyx
+    sklearn/utils/_weight_vector.pxd
+
+[codespell]
+skip = ./.git,./.mypy_cache,./doc/themes/scikit-learn-modern/static/js,./sklearn/feature_extraction/_stop_words.py,./doc/_build,./doc/auto_examples,./doc/modules/generated
+ignore-words = build_tools/codespell_ignore_words.txt
('', 'lgtm.yml')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,7 +1,8 @@
 extraction:
   cpp:
     before_index:
-      - pip3 install numpy scipy Cython
+      - pip3 install numpy==1.16.3
+      - pip3 install --no-deps scipy Cython
     index:
       build_command:
         - python3 setup.py build_ext -i
('', '.travis.yml')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,42 +1,83 @@
-# make it explicit that we favor the new container-based travis workers
-sudo: false
-
+# Make it explicit that we favor the
+# new container-based Travis workers
 language: python
+dist: xenial
+# Only used to install cibuildwheel, CIBW_BUILD determines the python version being
+# built in the docker image itself. Also: travis does not have 3.10 yet.
+python: 3.9

 cache:
   apt: true
   directories:
-  - $HOME/.cache/pip
-  - $HOME/.ccache
-
-dist: xenial
+    - $HOME/.cache/pip
+    - $HOME/.ccache

 env:
   global:
-    # Directory where tests are run from
-    - TEST_DIR=/tmp/sklearn
-    - OMP_NUM_THREADS=4
-    - OPENBLAS_NUM_THREADS=4
+    - CPU_COUNT=3
+    - TEST_DIR=/tmp/sklearn  # Test directory for continuous integration jobs
+    - PYTEST_VERSION=latest
+    - OMP_NUM_THREADS=2
+    - OPENBLAS_NUM_THREADS=2
+    - SKLEARN_BUILD_PARALLEL=3
+    - SKLEARN_SKIP_NETWORK_TESTS=1
+    - PYTHONUNBUFFERED=1
+    # Custom environment variables for the ARM wheel builder
+    - CIBW_BUILD_VERBOSITY=1
+    - CIBW_TEST_COMMAND="bash {project}/build_tools/travis/test_wheels.sh"
+    - CIBW_ENVIRONMENT="CPU_COUNT=4
+                        OMP_NUM_THREADS=2
+                        OPENBLAS_NUM_THREADS=2
+                        SKLEARN_BUILD_PARALLEL=10
+                        SKLEARN_SKIP_NETWORK_TESTS=1
+                        PYTHONUNBUFFERED=1"

-matrix:
+jobs:
   include:
-    # Linux environment to test scikit-learn against numpy and scipy master
-    # installed from their CI wheels in a virtualenv with the Python
-    # interpreter provided by travis.
-    -  python: 3.7
-       env: DISTRIB="scipy-dev" CHECK_WARNINGS="true"
-       if: type = cron OR commit_message =~ /\[scipy-dev\]/
+    # Linux environments to build the scikit-learn wheels for the ARM64
+    # architecture and Python 3.8 and newer. This is used both at release time
+    # with the manual trigger in the commit message in the release branch and as
+    # a scheduled task to build the weekly dev build on the main branch. The
+    # weekly frequency is meant to avoid depleting the Travis CI credits too
+    # fast.
+    - os: linux
+      arch: arm64-graviton2
+      dist: focal
+      virt: vm
+      group: edge
+      if: type = cron or commit_message =~ /\[cd build\]/
+      env:
+        - CIBW_BUILD=cp38-manylinux_aarch64
+        - BUILD_WHEEL=true

-install: source build_tools/travis/install.sh
-script:
-  - bash build_tools/travis/test_script.sh
-  - bash build_tools/travis/test_docs.sh
-  - bash build_tools/travis/test_pytest_soft_dependency.sh
-after_success: source build_tools/travis/after_success.sh
+    - os: linux
+      arch: arm64-graviton2
+      dist: focal
+      virt: vm
+      group: edge
+      if: type = cron or commit_message =~ /\[cd build\]/
+      env:
+        - CIBW_BUILD=cp39-manylinux_aarch64
+        - BUILD_WHEEL=true
+
+    - os: linux
+      arch: arm64-graviton2
+      dist: focal
+      virt: vm
+      group: edge
+      if: type = cron or commit_message =~ /\[cd build\]/
+      env:
+        - CIBW_BUILD=cp310-manylinux_aarch64
+        - BUILD_WHEEL=true
+
+install: source build_tools/travis/install.sh || travis_terminate 1
+script: source build_tools/travis/script.sh || travis_terminate 1
+after_success: source build_tools/travis/after_success.sh || travis_terminate 1
+
 notifications:
   webhooks:
     urls:
       - https://webhooks.gitter.im/e/4ffabb4df010b70cd624
-    on_success: change  # options: [always|never|change] default: always
-    on_failure: always  # options: [always|never|change] default: always
-    on_start: never     # options: [always|never|change] default: always
+    on_success: change
+    on_failure: always
+    on_start: never
('sklearn', 'conftest.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,21 +1,243 @@
+from os import environ
+from functools import wraps
+import platform
+import sys
+
 import pytest
-
-
-@pytest.fixture(scope='function')
+import numpy as np
+from threadpoolctl import threadpool_limits
+from _pytest.doctest import DoctestItem
+
+from sklearn.utils import _IS_32BIT
+from sklearn.utils._openmp_helpers import _openmp_effective_n_threads
+from sklearn._min_dependencies import PYTEST_MIN_VERSION
+from sklearn.utils.fixes import parse_version
+from sklearn.datasets import fetch_20newsgroups
+from sklearn.datasets import fetch_20newsgroups_vectorized
+from sklearn.datasets import fetch_california_housing
+from sklearn.datasets import fetch_covtype
+from sklearn.datasets import fetch_kddcup99
+from sklearn.datasets import fetch_olivetti_faces
+from sklearn.datasets import fetch_rcv1
+from sklearn.tests import random_seed
+
+
+if parse_version(pytest.__version__) < parse_version(PYTEST_MIN_VERSION):
+    raise ImportError(
+        "Your version of pytest is too old, you should have "
+        "at least pytest >= {} installed.".format(PYTEST_MIN_VERSION)
+    )
+
+dataset_fetchers = {
+    "fetch_20newsgroups_fxt": fetch_20newsgroups,
+    "fetch_20newsgroups_vectorized_fxt": fetch_20newsgroups_vectorized,
+    "fetch_california_housing_fxt": fetch_california_housing,
+    "fetch_covtype_fxt": fetch_covtype,
+    "fetch_kddcup99_fxt": fetch_kddcup99,
+    "fetch_olivetti_faces_fxt": fetch_olivetti_faces,
+    "fetch_rcv1_fxt": fetch_rcv1,
+}
+
+_SKIP32_MARK = pytest.mark.skipif(
+    environ.get("SKLEARN_RUN_FLOAT32_TESTS", "0") != "1",
+    reason="Set SKLEARN_RUN_FLOAT32_TESTS=1 to run float32 dtype tests",
+)
+
+
+# Global fixtures
+@pytest.fixture(params=[pytest.param(np.float32, marks=_SKIP32_MARK), np.float64])
+def global_dtype(request):
+    yield request.param
+
+
+def _fetch_fixture(f):
+    """Fetch dataset (download if missing and requested by environment)."""
+    download_if_missing = environ.get("SKLEARN_SKIP_NETWORK_TESTS", "1") == "0"
+
+    @wraps(f)
+    def wrapped(*args, **kwargs):
+        kwargs["download_if_missing"] = download_if_missing
+        try:
+            return f(*args, **kwargs)
+        except IOError as e:
+            if str(e) != "Data not found and `download_if_missing` is False":
+                raise
+            pytest.skip("test is enabled when SKLEARN_SKIP_NETWORK_TESTS=0")
+
+    return pytest.fixture(lambda: wrapped)
+
+
+# Adds fixtures for fetching data
+fetch_20newsgroups_fxt = _fetch_fixture(fetch_20newsgroups)
+fetch_20newsgroups_vectorized_fxt = _fetch_fixture(fetch_20newsgroups_vectorized)
+fetch_california_housing_fxt = _fetch_fixture(fetch_california_housing)
+fetch_covtype_fxt = _fetch_fixture(fetch_covtype)
+fetch_kddcup99_fxt = _fetch_fixture(fetch_kddcup99)
+fetch_olivetti_faces_fxt = _fetch_fixture(fetch_olivetti_faces)
+fetch_rcv1_fxt = _fetch_fixture(fetch_rcv1)
+
+
+def pytest_collection_modifyitems(config, items):
+    """Called after collect is completed.
+
+    Parameters
+    ----------
+    config : pytest config
+    items : list of collected items
+    """
+    run_network_tests = environ.get("SKLEARN_SKIP_NETWORK_TESTS", "1") == "0"
+    skip_network = pytest.mark.skip(
+        reason="test is enabled when SKLEARN_SKIP_NETWORK_TESTS=0"
+    )
+
+    # download datasets during collection to avoid thread unsafe behavior
+    # when running pytest in parallel with pytest-xdist
+    dataset_features_set = set(dataset_fetchers)
+    datasets_to_download = set()
+
+    for item in items:
+        if not hasattr(item, "fixturenames"):
+            continue
+        item_fixtures = set(item.fixturenames)
+        dataset_to_fetch = item_fixtures & dataset_features_set
+        if not dataset_to_fetch:
+            continue
+
+        if run_network_tests:
+            datasets_to_download |= dataset_to_fetch
+        else:
+            # network tests are skipped
+            item.add_marker(skip_network)
+
+    # Only download datasets on the first worker spawned by pytest-xdist
+    # to avoid thread unsafe behavior. If pytest-xdist is not used, we still
+    # download before tests run.
+    worker_id = environ.get("PYTEST_XDIST_WORKER", "gw0")
+    if worker_id == "gw0" and run_network_tests:
+        for name in datasets_to_download:
+            dataset_fetchers[name]()
+
+    for item in items:
+        # Known failure on with GradientBoostingClassifier on ARM64
+        if (
+            item.name.endswith("GradientBoostingClassifier")
+            and platform.machine() == "aarch64"
+        ):
+
+            marker = pytest.mark.xfail(
+                reason=(
+                    "know failure. See "
+                    "https://github.com/scikit-learn/scikit-learn/issues/17797"  # noqa
+                )
+            )
+            item.add_marker(marker)
+
+    # numpy changed the str/repr formatting of numpy arrays in 1.14. We want to
+    # run doctests only for numpy >= 1.14.
+    skip_doctests = False
+    try:
+        import matplotlib  # noqa
+    except ImportError:
+        skip_doctests = True
+        reason = "matplotlib is required to run the doctests"
+
+    try:
+        if _IS_32BIT:
+            reason = "doctest are only run when the default numpy int is 64 bits."
+            skip_doctests = True
+        elif sys.platform.startswith("win32"):
+            reason = (
+                "doctests are not run for Windows because numpy arrays "
+                "repr is inconsistent across platforms."
+            )
+            skip_doctests = True
+    except ImportError:
+        pass
+
+    # Normally doctest has the entire module's scope. Here we set globs to an empty dict
+    # to remove the module's scope:
+    # https://docs.python.org/3/library/doctest.html#what-s-the-execution-context
+    for item in items:
+        if isinstance(item, DoctestItem):
+            item.dtest.globs = {}
+
+    if skip_doctests:
+        skip_marker = pytest.mark.skip(reason=reason)
+
+        for item in items:
+            if isinstance(item, DoctestItem):
+                # work-around an internal error with pytest if adding a skip
+                # mark to a doctest in a contextmanager, see
+                # https://github.com/pytest-dev/pytest/issues/8796 for more
+                # details.
+                if item.name != "sklearn._config.config_context":
+                    item.add_marker(skip_marker)
+    try:
+        import PIL  # noqa
+
+        pillow_installed = True
+    except ImportError:
+        pillow_installed = False
+
+    if not pillow_installed:
+        skip_marker = pytest.mark.skip(reason="pillow (or PIL) not installed!")
+        for item in items:
+            if item.name in [
+                "sklearn.feature_extraction.image.PatchExtractor",
+                "sklearn.feature_extraction.image.extract_patches_2d",
+            ]:
+                item.add_marker(skip_marker)
+
+
+@pytest.fixture(scope="function")
 def pyplot():
     """Setup and teardown fixture for matplotlib.

     This fixture checks if we can import matplotlib. If not, the tests will be
-    skipped. Otherwise, we setup matplotlib backend and close the figures
-    after running the functions.
+    skipped. Otherwise, we close the figures before and after running the
+    functions.

     Returns
     -------
     pyplot : module
         The ``matplotlib.pyplot`` module.
     """
-    matplotlib = pytest.importorskip('matplotlib')
-    matplotlib.use('agg', warn=False, force=True)
-    pyplot = pytest.importorskip('matplotlib.pyplot')
+    pyplot = pytest.importorskip("matplotlib.pyplot")
+    pyplot.close("all")
     yield pyplot
-    pyplot.close('all')
+    pyplot.close("all")
+
+
+def pytest_runtest_setup(item):
+    """Set the number of openmp threads based on the number of workers
+    xdist is using to prevent oversubscription.
+
+    Parameters
+    ----------
+    item : pytest item
+        item to be processed
+    """
+    xdist_worker_count = environ.get("PYTEST_XDIST_WORKER_COUNT")
+    if xdist_worker_count is None:
+        # returns if pytest-xdist is not installed
+        return
+    else:
+        xdist_worker_count = int(xdist_worker_count)
+
+    openmp_threads = _openmp_effective_n_threads()
+    threads_per_worker = max(openmp_threads // xdist_worker_count, 1)
+    threadpool_limits(threads_per_worker, user_api="openmp")
+
+
+def pytest_configure(config):
+    # Use matplotlib agg backend during the tests including doctests
+    try:
+        import matplotlib
+
+        matplotlib.use("agg")
+    except ImportError:
+        pass
+
+    # Register global_random_seed plugin if it is not already registered
+    if not config.pluginmanager.hasplugin("sklearn.tests.random_seed"):
+        config.pluginmanager.register(random_seed)
('sklearn', 'multiclass.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,6 @@
 """
-Multiclass and multilabel classification strategies
-===================================================
+Multiclass classification strategies
+====================================

 This module implements multiclass learning algorithms:
     - one-vs-the-rest / one-vs-all
@@ -45,16 +45,18 @@
 from .preprocessing import LabelBinarizer
 from .metrics.pairwise import euclidean_distances
 from .utils import check_random_state
+from .utils._tags import _safe_tags
 from .utils.validation import _num_samples
 from .utils.validation import check_is_fitted
-from .utils.validation import check_X_y, check_array
-from .utils.multiclass import (_check_partial_fit_first_call,
-                               check_classification_targets,
-                               _ovr_decision_function)
-from .utils.metaestimators import _safe_split, if_delegate_has_method
-
-from .utils._joblib import Parallel
-from .utils._joblib import delayed
+from .utils.multiclass import (
+    _check_partial_fit_first_call,
+    check_classification_targets,
+    _ovr_decision_function,
+)
+from .utils.metaestimators import _safe_split, available_if
+from .utils.fixes import delayed
+
+from joblib import Parallel

 __all__ = [
     "OneVsRestClassifier",
@@ -72,8 +74,9 @@
                 c = 0
             else:
                 c = y[0]
-            warnings.warn("Label %s is present in all training examples." %
-                          str(classes[c]))
+            warnings.warn(
+                "Label %s is present in all training examples." % str(classes[c])
+            )
         estimator = _ConstantPredictor().fit(X, unique_y)
     else:
         estimator = clone(estimator)
@@ -99,40 +102,92 @@
     return score


+def _threshold_for_binary_predict(estimator):
+    """Threshold for predictions from binary estimator."""
+    if hasattr(estimator, "decision_function") and is_classifier(estimator):
+        return 0.0
+    else:
+        # predict_proba threshold
+        return 0.5
+
+
 def _check_estimator(estimator):
     """Make sure that an estimator implements the necessary methods."""
-    if (not hasattr(estimator, "decision_function") and
-            not hasattr(estimator, "predict_proba")):
-        raise ValueError("The base estimator should implement "
-                         "decision_function or predict_proba!")
+    if not hasattr(estimator, "decision_function") and not hasattr(
+        estimator, "predict_proba"
+    ):
+        raise ValueError(
+            "The base estimator should implement decision_function or predict_proba!"
+        )


 class _ConstantPredictor(BaseEstimator):
-
     def fit(self, X, y):
+        check_params = dict(
+            force_all_finite=False, dtype=None, ensure_2d=False, accept_sparse=True
+        )
+        self._validate_data(
+            X, y, reset=True, validate_separately=(check_params, check_params)
+        )
         self.y_ = y
         return self

     def predict(self, X):
-        check_is_fitted(self, 'y_')
-
-        return np.repeat(self.y_, X.shape[0])
+        check_is_fitted(self)
+        self._validate_data(
+            X,
+            force_all_finite=False,
+            dtype=None,
+            accept_sparse=True,
+            ensure_2d=False,
+            reset=False,
+        )
+
+        return np.repeat(self.y_, _num_samples(X))

     def decision_function(self, X):
-        check_is_fitted(self, 'y_')
-
-        return np.repeat(self.y_, X.shape[0])
+        check_is_fitted(self)
+        self._validate_data(
+            X,
+            force_all_finite=False,
+            dtype=None,
+            accept_sparse=True,
+            ensure_2d=False,
+            reset=False,
+        )
+
+        return np.repeat(self.y_, _num_samples(X))

     def predict_proba(self, X):
-        check_is_fitted(self, 'y_')
-
-        return np.repeat([np.hstack([1 - self.y_, self.y_])],
-                         X.shape[0], axis=0)
-
-
-class OneVsRestClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin,
-                          MultiOutputMixin):
-    """One-vs-the-rest (OvR) multiclass/multilabel strategy
+        check_is_fitted(self)
+        self._validate_data(
+            X,
+            force_all_finite=False,
+            dtype=None,
+            accept_sparse=True,
+            ensure_2d=False,
+            reset=False,
+        )
+        y_ = self.y_.astype(np.float64)
+        return np.repeat([np.hstack([1 - y_, y_])], _num_samples(X), axis=0)
+
+
+def _estimators_has(attr):
+    """Check if self.estimator or self.estimators_[0] has attr.
+
+    If `self.estimators_[0]` has the attr, then its safe to assume that other
+    values has it too. This function is used together with `avaliable_if`.
+    """
+    return lambda self: (
+        hasattr(self.estimator, attr)
+        or (hasattr(self, "estimators_") and hasattr(self.estimators_[0], attr))
+    )
+
+
+class OneVsRestClassifier(
+    MultiOutputMixin, ClassifierMixin, MetaEstimatorMixin, BaseEstimator
+):
+    """One-vs-the-rest (OvR) multiclass strategy.

     Also known as one-vs-all, this strategy consists in fitting one classifier
     per class. For each classifier, the class is fitted against all the other
@@ -143,26 +198,41 @@
     corresponding classifier. This is the most commonly used strategy for
     multiclass classification and is a fair default choice.

-    This strategy can also be used for multilabel learning, where a classifier
-    is used to predict multiple labels for instance, by fitting on a 2-d matrix
-    in which cell [i, j] is 1 if sample i has label j and 0 otherwise.
-
-    In the multilabel learning literature, OvR is also known as the binary
-    relevance method.
+    OneVsRestClassifier can also be used for multilabel classification. To use
+    this feature, provide an indicator matrix for the target `y` when calling
+    `.fit`. In other words, the target labels should be formatted as a 2D
+    binary (0/1) matrix, where [i, j] == 1 indicates the presence of label j
+    in sample i. This estimator uses the binary relevance method to perform
+    multilabel classification, which involves training one binary classifier
+    independently for each label.

     Read more in the :ref:`User Guide <ovr_classification>`.

     Parameters
     ----------
     estimator : estimator object
-        An estimator object implementing `fit` and one of `decision_function`
-        or `predict_proba`.
-
-    n_jobs : int or None, optional (default=None)
-        The number of jobs to use for the computation.
+        An estimator object implementing :term:`fit` and one of
+        :term:`decision_function` or :term:`predict_proba`.
+
+    n_jobs : int, default=None
+        The number of jobs to use for the computation: the `n_classes`
+        one-vs-rest problems are computed in parallel.
+
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.
+
+        .. versionchanged:: 0.20
+           `n_jobs` default changed from 1 to None
+
+    verbose : int, default=0
+        The verbosity level, if non zero, progress messages are printed.
+        Below 50, the output is sent to stderr. Otherwise, the output is sent
+        to stdout. The frequency of the messages increases with the verbosity
+        level, reporting all iterations at 10. See :class:`joblib.Parallel` for
+        more details.
+
+        .. versionadded:: 1.1

     Attributes
     ----------
@@ -171,31 +241,76 @@

     classes_ : array, shape = [`n_classes`]
         Class labels.
+
+    n_classes_ : int
+        Number of classes.
+
     label_binarizer_ : LabelBinarizer object
         Object used to transform multiclass labels to binary labels and
         vice-versa.
+
     multilabel_ : boolean
         Whether a OneVsRestClassifier is a multilabel classifier.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`. Only defined if the
+        underlying estimator exposes such an attribute when fit.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Only defined if the
+        underlying estimator exposes such an attribute when fit.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    MultiOutputClassifier : Alternate way of extending an estimator for
+        multilabel classification.
+    sklearn.preprocessing.MultiLabelBinarizer : Transform iterable of iterables
+        to binary indicator matrix.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.multiclass import OneVsRestClassifier
+    >>> from sklearn.svm import SVC
+    >>> X = np.array([
+    ...     [10, 10],
+    ...     [8, 10],
+    ...     [-5, 5.5],
+    ...     [-5.4, 5.5],
+    ...     [-20, -20],
+    ...     [-15, -20]
+    ... ])
+    >>> y = np.array([0, 0, 1, 1, 2, 2])
+    >>> clf = OneVsRestClassifier(SVC()).fit(X, y)
+    >>> clf.predict([[-19, -20], [9, 9], [-5, 5]])
+    array([2, 0, 1])
     """
-    def __init__(self, estimator, n_jobs=None):
+
+    def __init__(self, estimator, *, n_jobs=None, verbose=0):
         self.estimator = estimator
         self.n_jobs = n_jobs
+        self.verbose = verbose

     def fit(self, X, y):
         """Fit underlying estimators.

         Parameters
         ----------
-        X : (sparse) array-like, shape = [n_samples, n_features]
+        X : (sparse) array-like of shape (n_samples, n_features)
             Data.

-        y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]
+        y : (sparse) array-like of shape (n_samples,) or (n_samples, n_classes)
             Multi-class targets. An indicator matrix turns on multilabel
             classification.

         Returns
         -------
-        self
+        self : object
+            Instance of fitted estimator.
         """
         # A sparse LabelBinarizer, with sparse_output=True, has been shown to
         # outperform or match a dense label binarizer in all cases and has also
@@ -209,27 +324,39 @@
         # In cases where individual estimators are very fast to train setting
         # n_jobs > 1 in can results in slower performance due to the overhead
         # of spawning threads.  See joblib issue #112.
-        self.estimators_ = Parallel(n_jobs=self.n_jobs)(delayed(_fit_binary)(
-            self.estimator, X, column, classes=[
-                "not %s" % self.label_binarizer_.classes_[i],
-                self.label_binarizer_.classes_[i]])
-            for i, column in enumerate(columns))
+        self.estimators_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
+            delayed(_fit_binary)(
+                self.estimator,
+                X,
+                column,
+                classes=[
+                    "not %s" % self.label_binarizer_.classes_[i],
+                    self.label_binarizer_.classes_[i],
+                ],
+            )
+            for i, column in enumerate(columns)
+        )
+
+        if hasattr(self.estimators_[0], "n_features_in_"):
+            self.n_features_in_ = self.estimators_[0].n_features_in_
+        if hasattr(self.estimators_[0], "feature_names_in_"):
+            self.feature_names_in_ = self.estimators_[0].feature_names_in_

         return self

-    @if_delegate_has_method('estimator')
+    @available_if(_estimators_has("partial_fit"))
     def partial_fit(self, X, y, classes=None):
-        """Partially fit underlying estimators
+        """Partially fit underlying estimators.

         Should be used when memory is inefficient to train all data.
         Chunks of data can be passed in several iteration.

         Parameters
         ----------
-        X : (sparse) array-like, shape = [n_samples, n_features]
+        X : (sparse) array-like of shape (n_samples, n_features)
             Data.

-        y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]
+        y : (sparse) array-like of shape (n_samples,) or (n_samples, n_classes)
             Multi-class targets. An indicator matrix turns on multilabel
             classification.

@@ -242,14 +369,17 @@

         Returns
         -------
-        self
+        self : object
+            Instance of partially fitted estimator.
         """
         if _check_partial_fit_first_call(self, classes):
             if not hasattr(self.estimator, "partial_fit"):
-                raise ValueError(("Base estimator {0}, doesn't have "
-                                 "partial_fit method").format(self.estimator))
-            self.estimators_ = [clone(self.estimator) for _ in range
-                                (self.n_classes_)]
+                raise ValueError(
+                    ("Base estimator {0}, doesn't have partial_fit method").format(
+                        self.estimator
+                    )
+                )
+            self.estimators_ = [clone(self.estimator) for _ in range(self.n_classes_)]

             # A sparse LabelBinarizer, with sparse_output=True, has been
             # shown to outperform or match a dense label binarizer in all
@@ -259,9 +389,11 @@
             self.label_binarizer_.fit(self.classes_)

         if len(np.setdiff1d(y, self.classes_)):
-            raise ValueError(("Mini-batch contains {0} while classes " +
-                             "must be subset of {1}").format(np.unique(y),
-                                                             self.classes_))
+            raise ValueError(
+                (
+                    "Mini-batch contains {0} while classes " + "must be subset of {1}"
+                ).format(np.unique(y), self.classes_)
+            )

         Y = self.label_binarizer_.transform(y)
         Y = Y.tocsc()
@@ -269,7 +401,11 @@

         self.estimators_ = Parallel(n_jobs=self.n_jobs)(
             delayed(_partial_fit_binary)(estimator, X, column)
-            for estimator, column in zip(self.estimators_, columns))
+            for estimator, column in zip(self.estimators_, columns)
+        )
+
+        if hasattr(self.estimators_[0], "n_features_in_"):
+            self.n_features_in_ = self.estimators_[0].n_features_in_

         return self

@@ -278,20 +414,15 @@

         Parameters
         ----------
-        X : (sparse) array-like, shape = [n_samples, n_features]
+        X : (sparse) array-like of shape (n_samples, n_features)
             Data.

         Returns
         -------
-        y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes].
+        y : (sparse) array-like of shape (n_samples,) or (n_samples, n_classes)
             Predicted multi-class targets.
         """
-        check_is_fitted(self, 'estimators_')
-        if (hasattr(self.estimators_[0], "decision_function") and
-                is_classifier(self.estimators_[0])):
-            thresh = 0
-        else:
-            thresh = .5
+        check_is_fitted(self)

         n_samples = _num_samples(X)
         if self.label_binarizer_.y_type_ == "multiclass":
@@ -304,17 +435,19 @@
                 argmaxima[maxima == pred] = i
             return self.classes_[argmaxima]
         else:
-            indices = array.array('i')
-            indptr = array.array('i', [0])
+            thresh = _threshold_for_binary_predict(self.estimators_[0])
+            indices = array.array("i")
+            indptr = array.array("i", [0])
             for e in self.estimators_:
                 indices.extend(np.where(_predict_binary(e, X) > thresh)[0])
                 indptr.append(len(indices))
             data = np.ones(len(indices), dtype=int)
-            indicator = sp.csc_matrix((data, indices, indptr),
-                                      shape=(n_samples, len(self.estimators_)))
+            indicator = sp.csc_matrix(
+                (data, indices, indptr), shape=(n_samples, len(self.estimators_))
+            )
             return self.label_binarizer_.inverse_transform(indicator)

-    @if_delegate_has_method(['_first_estimator', 'estimator'])
+    @available_if(_estimators_has("predict_proba"))
     def predict_proba(self, X):
         """Probability estimates.

@@ -330,15 +463,16 @@

         Parameters
         ----------
-        X : array-like, shape = [n_samples, n_features]
+        X : array-like of shape (n_samples, n_features)
+            Input data.

         Returns
         -------
-        T : (sparse) array-like, shape = [n_samples, n_classes]
+        T : (sparse) array-like of shape (n_samples, n_classes)
             Returns the probability of the sample for each class in the model,
             where classes are ordered as they are in `self.classes_`.
         """
-        check_is_fitted(self, 'estimators_')
+        check_is_fitted(self)
         # Y[i, j] gives the probability that sample i has the label j.
         # In the multi-label case, these are not disjoint.
         Y = np.array([e.predict_proba(X)[:, 1] for e in self.estimators_]).T
@@ -353,75 +487,68 @@
             Y /= np.sum(Y, axis=1)[:, np.newaxis]
         return Y

-    @if_delegate_has_method(['_first_estimator', 'estimator'])
+    @available_if(_estimators_has("decision_function"))
     def decision_function(self, X):
-        """Returns the distance of each sample from the decision boundary for
-        each class. This can only be used with estimators which implement the
-        decision_function method.
+        """Decision function for the OneVsRestClassifier.
+
+        Return the distance of each sample from the decision boundary for each
+        class. This can only be used with estimators which implement the
+        `decision_function` method.

         Parameters
         ----------
-        X : array-like, shape = [n_samples, n_features]
+        X : array-like of shape (n_samples, n_features)
+            Input data.

         Returns
         -------
-        T : array-like, shape = [n_samples, n_classes]
+        T : array-like of shape (n_samples, n_classes) or (n_samples,) for \
+            binary classification.
+            Result of calling `decision_function` on the final estimator.
+
+            .. versionchanged:: 0.19
+                output shape changed to ``(n_samples,)`` to conform to
+                scikit-learn conventions for binary classification.
         """
-        check_is_fitted(self, 'estimators_')
+        check_is_fitted(self)
         if len(self.estimators_) == 1:
             return self.estimators_[0].decision_function(X)
-        return np.array([est.decision_function(X).ravel()
-                         for est in self.estimators_]).T
+        return np.array(
+            [est.decision_function(X).ravel() for est in self.estimators_]
+        ).T

     @property
     def multilabel_(self):
-        """Whether this is a multilabel classifier"""
-        return self.label_binarizer_.y_type_.startswith('multilabel')
+        """Whether this is a multilabel classifier."""
+        return self.label_binarizer_.y_type_.startswith("multilabel")

     @property
     def n_classes_(self):
+        """Number of classes."""
         return len(self.classes_)

-    @property
-    def coef_(self):
-        check_is_fitted(self, 'estimators_')
-        if not hasattr(self.estimators_[0], "coef_"):
-            raise AttributeError(
-                "Base estimator doesn't have a coef_ attribute.")
-        coefs = [e.coef_ for e in self.estimators_]
-        if sp.issparse(coefs[0]):
-            return sp.vstack(coefs)
-        return np.vstack(coefs)
-
-    @property
-    def intercept_(self):
-        check_is_fitted(self, 'estimators_')
-        if not hasattr(self.estimators_[0], "intercept_"):
-            raise AttributeError(
-                "Base estimator doesn't have an intercept_ attribute.")
-        return np.array([e.intercept_.ravel() for e in self.estimators_])
-
-    @property
-    def _pairwise(self):
+    def _more_tags(self):
         """Indicate if wrapped estimator is using a precomputed Gram matrix"""
-        return getattr(self.estimator, "_pairwise", False)
-
-    @property
-    def _first_estimator(self):
-        return self.estimators_[0]
+        return {"pairwise": _safe_tags(self.estimator, key="pairwise")}


 def _fit_ovo_binary(estimator, X, y, i, j):
     """Fit a single binary estimator (one-vs-one)."""
     cond = np.logical_or(y == i, y == j)
     y = y[cond]
-    y_binary = np.empty(y.shape, np.int)
+    y_binary = np.empty(y.shape, int)
     y_binary[y == i] = 0
     y_binary[y == j] = 1
-    indcond = np.arange(X.shape[0])[cond]
-    return _fit_binary(estimator,
-                       _safe_split(estimator, X, None, indices=indcond)[0],
-                       y_binary, classes=[i, j]), indcond
+    indcond = np.arange(_num_samples(X))[cond]
+    return (
+        _fit_binary(
+            estimator,
+            _safe_split(estimator, X, None, indices=indcond)[0],
+            y_binary,
+            classes=[i, j],
+        ),
+        indcond,
+    )


 def _partial_fit_ovo_binary(estimator, X, y, i, j):
@@ -436,8 +563,8 @@
     return estimator


-class OneVsOneClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
-    """One-vs-one multiclass strategy
+class OneVsOneClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):
+    """One-vs-one multiclass strategy.

     This strategy consists in fitting one classifier per class pair.
     At prediction time, the class which received the most votes is selected.
@@ -454,11 +581,13 @@
     Parameters
     ----------
     estimator : estimator object
-        An estimator object implementing `fit` and one of `decision_function`
-        or `predict_proba`.
-
-    n_jobs : int or None, optional (default=None)
-        The number of jobs to use for the computation.
+        An estimator object implementing :term:`fit` and one of
+        :term:`decision_function` or :term:`predict_proba`.
+
+    n_jobs : int, default=None
+        The number of jobs to use for the computation: the `n_classes * (
+        n_classes - 1) / 2` OVO problems are computed in parallel.
+
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.
@@ -471,12 +600,44 @@
     classes_ : numpy array of shape [n_classes]
         Array containing labels.

+    n_classes_ : int
+        Number of classes.
+
     pairwise_indices_ : list, length = ``len(estimators_)``, or ``None``
         Indices of samples used when training the estimators.
-        ``None`` when ``estimator`` does not have ``_pairwise`` attribute.
+        ``None`` when ``estimator``'s `pairwise` tag is False.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    OneVsRestClassifier : One-vs-all multiclass strategy.
+
+    Examples
+    --------
+    >>> from sklearn.datasets import load_iris
+    >>> from sklearn.model_selection import train_test_split
+    >>> from sklearn.multiclass import OneVsOneClassifier
+    >>> from sklearn.svm import LinearSVC
+    >>> X, y = load_iris(return_X_y=True)
+    >>> X_train, X_test, y_train, y_test = train_test_split(
+    ...     X, y, test_size=0.33, shuffle=True, random_state=0)
+    >>> clf = OneVsOneClassifier(
+    ...     LinearSVC(random_state=0)).fit(X_train, y_train)
+    >>> clf.predict(X_test[:10])
+    array([2, 1, 0, 2, 0, 2, 0, 1, 1, 1])
     """

-    def __init__(self, estimator, n_jobs=None):
+    def __init__(self, estimator, *, n_jobs=None):
         self.estimator = estimator
         self.n_jobs = n_jobs

@@ -485,50 +646,64 @@

         Parameters
         ----------
-        X : (sparse) array-like, shape = [n_samples, n_features]
+        X : (sparse) array-like of shape (n_samples, n_features)
             Data.

-        y : array-like, shape = [n_samples]
+        y : array-like of shape (n_samples,)
             Multi-class targets.

         Returns
         -------
-        self
+        self : object
+            The fitted underlying estimator.
         """
-        X, y = check_X_y(X, y, accept_sparse=['csr', 'csc'])
+        # We need to validate the data because we do a safe_indexing later.
+        X, y = self._validate_data(
+            X, y, accept_sparse=["csr", "csc"], force_all_finite=False
+        )
         check_classification_targets(y)

         self.classes_ = np.unique(y)
         if len(self.classes_) == 1:
-            raise ValueError("OneVsOneClassifier can not be fit when only one"
-                             " class is present.")
+            raise ValueError(
+                "OneVsOneClassifier can not be fit when only one class is present."
+            )
         n_classes = self.classes_.shape[0]
-        estimators_indices = list(zip(*(Parallel(n_jobs=self.n_jobs)(
-            delayed(_fit_ovo_binary)
-            (self.estimator, X, y, self.classes_[i], self.classes_[j])
-            for i in range(n_classes) for j in range(i + 1, n_classes)))))
+        estimators_indices = list(
+            zip(
+                *(
+                    Parallel(n_jobs=self.n_jobs)(
+                        delayed(_fit_ovo_binary)(
+                            self.estimator, X, y, self.classes_[i], self.classes_[j]
+                        )
+                        for i in range(n_classes)
+                        for j in range(i + 1, n_classes)
+                    )
+                )
+            )
+        )

         self.estimators_ = estimators_indices[0]
-        self.pairwise_indices_ = (
-            estimators_indices[1] if self._pairwise else None)
+
+        pairwise = self._get_tags()["pairwise"]
+        self.pairwise_indices_ = estimators_indices[1] if pairwise else None

         return self

-    @if_delegate_has_method(delegate='estimator')
+    @available_if(_estimators_has("partial_fit"))
     def partial_fit(self, X, y, classes=None):
-        """Partially fit underlying estimators
+        """Partially fit underlying estimators.

         Should be used when memory is inefficient to train all data. Chunks
         of data can be passed in several iteration, where the first call
         should have an array of all target variables.

-
         Parameters
         ----------
-        X : (sparse) array-like, shape = [n_samples, n_features]
+        X : (sparse) array-like of shape (n_samples, n_features)
             Data.

-        y : array-like, shape = [n_samples]
+        y : array-like of shape (n_samples,)
             Multi-class targets.

         classes : array, shape (n_classes, )
@@ -540,29 +715,43 @@

         Returns
         -------
-        self
+        self : object
+            The partially fitted underlying estimator.
         """
-        if _check_partial_fit_first_call(self, classes):
-            self.estimators_ = [clone(self.estimator) for _ in
-                                range(self.n_classes_ *
-                                      (self.n_classes_ - 1) // 2)]
+        first_call = _check_partial_fit_first_call(self, classes)
+        if first_call:
+            self.estimators_ = [
+                clone(self.estimator)
+                for _ in range(self.n_classes_ * (self.n_classes_ - 1) // 2)
+            ]

         if len(np.setdiff1d(y, self.classes_)):
-            raise ValueError("Mini-batch contains {0} while it "
-                             "must be subset of {1}".format(np.unique(y),
-                                                            self.classes_))
-
-        X, y = check_X_y(X, y, accept_sparse=['csr', 'csc'])
+            raise ValueError(
+                "Mini-batch contains {0} while it must be subset of {1}".format(
+                    np.unique(y), self.classes_
+                )
+            )
+
+        X, y = self._validate_data(
+            X,
+            y,
+            accept_sparse=["csr", "csc"],
+            force_all_finite=False,
+            reset=first_call,
+        )
         check_classification_targets(y)
         combinations = itertools.combinations(range(self.n_classes_), 2)
-        self.estimators_ = Parallel(
-            n_jobs=self.n_jobs)(
-                delayed(_partial_fit_ovo_binary)(
-                    estimator, X, y, self.classes_[i], self.classes_[j])
-                for estimator, (i, j) in zip(self.estimators_,
-                                              (combinations)))
+        self.estimators_ = Parallel(n_jobs=self.n_jobs)(
+            delayed(_partial_fit_ovo_binary)(
+                estimator, X, y, self.classes_[i], self.classes_[j]
+            )
+            for estimator, (i, j) in zip(self.estimators_, (combinations))
+        )

         self.pairwise_indices_ = None
+
+        if hasattr(self.estimators_[0], "n_features_in_"):
+            self.n_features_in_ = self.estimators_[0].n_features_in_

         return self

@@ -575,7 +764,7 @@

         Parameters
         ----------
-        X : (sparse) array-like, shape = [n_samples, n_features]
+        X : (sparse) array-like of shape (n_samples, n_features)
             Data.

         Returns
@@ -585,7 +774,8 @@
         """
         Y = self.decision_function(X)
         if self.n_classes_ == 2:
-            return self.classes_[(Y > 0).astype(np.int)]
+            thresh = _threshold_for_binary_predict(self.estimators_[0])
+            return self.classes_[(Y > thresh).astype(int)]
         return self.classes_[Y.argmax(axis=1)]

     def decision_function(self, X):
@@ -598,13 +788,25 @@

         Parameters
         ----------
-        X : array-like, shape = [n_samples, n_features]
+        X : array-like of shape (n_samples, n_features)
+            Input data.

         Returns
         -------
-        Y : array-like, shape = [n_samples, n_classes]
+        Y : array-like of shape (n_samples, n_classes) or (n_samples,)
+            Result of calling `decision_function` on the final estimator.
+
+            .. versionchanged:: 0.19
+                output shape changed to ``(n_samples,)`` to conform to
+                scikit-learn conventions for binary classification.
         """
-        check_is_fitted(self, 'estimators_')
+        check_is_fitted(self)
+        X = self._validate_data(
+            X,
+            accept_sparse=True,
+            force_all_finite=False,
+            reset=False,
+        )

         indices = self.pairwise_indices_
         if indices is None:
@@ -612,28 +814,29 @@
         else:
             Xs = [X[:, idx] for idx in indices]

-        predictions = np.vstack([est.predict(Xi)
-                                 for est, Xi in zip(self.estimators_, Xs)]).T
-        confidences = np.vstack([_predict_binary(est, Xi)
-                                 for est, Xi in zip(self.estimators_, Xs)]).T
-        Y = _ovr_decision_function(predictions,
-                                   confidences, len(self.classes_))
+        predictions = np.vstack(
+            [est.predict(Xi) for est, Xi in zip(self.estimators_, Xs)]
+        ).T
+        confidences = np.vstack(
+            [_predict_binary(est, Xi) for est, Xi in zip(self.estimators_, Xs)]
+        ).T
+        Y = _ovr_decision_function(predictions, confidences, len(self.classes_))
         if self.n_classes_ == 2:
             return Y[:, 1]
         return Y

     @property
     def n_classes_(self):
+        """Number of classes."""
         return len(self.classes_)

-    @property
-    def _pairwise(self):
+    def _more_tags(self):
         """Indicate if wrapped estimator is using a precomputed Gram matrix"""
-        return getattr(self.estimator, "_pairwise", False)
-
-
-class OutputCodeClassifier(BaseEstimator, ClassifierMixin, MetaEstimatorMixin):
-    """(Error-Correcting) Output-Code multiclass strategy
+        return {"pairwise": _safe_tags(self.estimator, key="pairwise")}
+
+
+class OutputCodeClassifier(MetaEstimatorMixin, ClassifierMixin, BaseEstimator):
+    """(Error-Correcting) Output-Code multiclass strategy.

     Output-code based strategies consist in representing each class with a
     binary code (an array of 0s and 1s). At fitting time, one binary
@@ -649,23 +852,24 @@
     Parameters
     ----------
     estimator : estimator object
-        An estimator object implementing `fit` and one of `decision_function`
-        or `predict_proba`.
-
-    code_size : float
+        An estimator object implementing :term:`fit` and one of
+        :term:`decision_function` or :term:`predict_proba`.
+
+    code_size : float, default=1.5
         Percentage of the number of classes to be used to create the code book.
         A number between 0 and 1 will require fewer classifiers than
         one-vs-the-rest. A number greater than 1 will require more classifiers
         than one-vs-the-rest.

-    random_state : int, RandomState instance or None, optional, default: None
-        The generator used to initialize the codebook.  If int, random_state is
-        the seed used by the random number generator; If RandomState instance,
-        random_state is the random number generator; If None, the random number
-        generator is the RandomState instance used by `np.random`.
-
-    n_jobs : int or None, optional (default=None)
-        The number of jobs to use for the computation.
+    random_state : int, RandomState instance, default=None
+        The generator used to initialize the codebook.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.
+
+    n_jobs : int, default=None
+        The number of jobs to use for the computation: the multiclass problems
+        are computed in parallel.
+
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.
@@ -675,11 +879,28 @@
     estimators_ : list of `int(n_classes * code_size)` estimators
         Estimators used for predictions.

-    classes_ : numpy array of shape [n_classes]
+    classes_ : ndarray of shape (n_classes,)
         Array containing labels.

-    code_book_ : numpy array of shape [n_classes, code_size]
+    code_book_ : ndarray of shape (n_classes, code_size)
         Binary array containing the code of each class.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`. Only defined if the
+        underlying estimator exposes such an attribute when fit.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Only defined if the
+        underlying estimator exposes such an attribute when fit.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    OneVsRestClassifier : One-vs-all multiclass strategy.
+    OneVsOneClassifier : One-vs-one multiclass strategy.

     References
     ----------
@@ -698,10 +919,23 @@
     .. [3] "The Elements of Statistical Learning",
        Hastie T., Tibshirani R., Friedman J., page 606 (second-edition)
        2008.
+
+    Examples
+    --------
+    >>> from sklearn.multiclass import OutputCodeClassifier
+    >>> from sklearn.ensemble import RandomForestClassifier
+    >>> from sklearn.datasets import make_classification
+    >>> X, y = make_classification(n_samples=100, n_features=4,
+    ...                            n_informative=2, n_redundant=0,
+    ...                            random_state=0, shuffle=False)
+    >>> clf = OutputCodeClassifier(
+    ...     estimator=RandomForestClassifier(random_state=0),
+    ...     random_state=0).fit(X, y)
+    >>> clf.predict([[0, 0, 0, 0]])
+    array([1])
     """

-    def __init__(self, estimator, code_size=1.5, random_state=None,
-                 n_jobs=None):
+    def __init__(self, estimator, *, code_size=1.5, random_state=None, n_jobs=None):
         self.estimator = estimator
         self.code_size = code_size
         self.random_state = random_state
@@ -712,20 +946,23 @@

         Parameters
         ----------
-        X : (sparse) array-like, shape = [n_samples, n_features]
+        X : (sparse) array-like of shape (n_samples, n_features)
             Data.

-        y : numpy array of shape [n_samples]
+        y : array-like of shape (n_samples,)
             Multi-class targets.

         Returns
         -------
-        self
+        self : object
+            Returns a fitted instance of self.
         """
-        X, y = check_X_y(X, y)
+        y = self._validate_data(X="no_validation", y=y)
+
         if self.code_size <= 0:
-            raise ValueError("code_size should be greater than 0, got {0}"
-                             "".format(self.code_size))
+            raise ValueError(
+                "code_size should be greater than 0, got {0}".format(self.code_size)
+            )

         _check_estimator(self.estimator)
         random_state = check_random_state(self.random_state)
@@ -733,11 +970,15 @@

         self.classes_ = np.unique(y)
         n_classes = self.classes_.shape[0]
+        if n_classes == 0:
+            raise ValueError(
+                "OutputCodeClassifier can not be fit when no class is present."
+            )
         code_size_ = int(n_classes * self.code_size)

         # FIXME: there are more elaborate methods than generating the codebook
         # randomly.
-        self.code_book_ = random_state.random_sample((n_classes, code_size_))
+        self.code_book_ = random_state.uniform(size=(n_classes, code_size_))
         self.code_book_[self.code_book_ > 0.5] = 1

         if hasattr(self.estimator, "decision_function"):
@@ -747,12 +988,19 @@

         classes_index = {c: i for i, c in enumerate(self.classes_)}

-        Y = np.array([self.code_book_[classes_index[y[i]]]
-                      for i in range(X.shape[0])], dtype=np.int)
+        Y = np.array(
+            [self.code_book_[classes_index[y[i]]] for i in range(_num_samples(y))],
+            dtype=int,
+        )

         self.estimators_ = Parallel(n_jobs=self.n_jobs)(
-            delayed(_fit_binary)(self.estimator, X, Y[:, i])
-            for i in range(Y.shape[1]))
+            delayed(_fit_binary)(self.estimator, X, Y[:, i]) for i in range(Y.shape[1])
+        )
+
+        if hasattr(self.estimators_[0], "n_features_in_"):
+            self.n_features_in_ = self.estimators_[0].n_features_in_
+        if hasattr(self.estimators_[0], "feature_names_in_"):
+            self.feature_names_in_ = self.estimators_[0].feature_names_in_

         return self

@@ -761,16 +1009,15 @@

         Parameters
         ----------
-        X : (sparse) array-like, shape = [n_samples, n_features]
+        X : (sparse) array-like of shape (n_samples, n_features)
             Data.

         Returns
         -------
-        y : numpy array of shape [n_samples]
+        y : ndarray of shape (n_samples,)
             Predicted multi-class targets.
         """
-        check_is_fitted(self, 'estimators_')
-        X = check_array(X)
+        check_is_fitted(self)
         Y = np.array([_predict_binary(e, X) for e in self.estimators_]).T
         pred = euclidean_distances(Y, self.code_book_).argmin(axis=1)
         return self.classes_[pred]
('sklearn', '_isotonic.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,13 +3,13 @@
 # Uses the pool adjacent violators algorithm (PAVA), with the
 # enhancement of searching for the longest decreasing subsequence to
 # pool at each step.
-#
-# cython: boundscheck=False, wraparound=False, cdivision=True

 import numpy as np
 cimport numpy as np
 cimport cython
 from cython cimport floating
+
+np.import_array()


 def _inplace_contiguous_isotonic_regression(floating[::1] y, floating[::1] w):
@@ -75,8 +75,6 @@
     Assumes that X is ordered, so that all duplicates follow each other.
     """
     unique_values = len(np.unique(X))
-    if unique_values == len(X):
-        return X, y, sample_weights

     cdef np.ndarray[dtype=floating] y_out = np.empty(unique_values,
                                                      dtype=X.dtype)
@@ -88,13 +86,14 @@
     cdef floating current_weight = 0
     cdef floating y_old = 0
     cdef int i = 0
-    cdef int current_count = 0
     cdef int j
     cdef floating x
     cdef int n_samples = len(X)
+    cdef floating eps = np.finfo(X.dtype).resolution
+
     for j in range(n_samples):
         x = X[j]
-        if x != current_x:
+        if x - current_x >= eps:
             # next unique value
             x_out[i] = current_x
             weights_out[i] = current_weight
@@ -103,13 +102,11 @@
             current_x = x
             current_weight = sample_weights[j]
             current_y = y[j] * sample_weights[j]
-            current_count = 1
         else:
             current_weight += sample_weights[j]
             current_y += y[j] * sample_weights[j]
-            current_count += 1

     x_out[i] = current_x
     weights_out[i] = current_weight
     y_out[i] = current_y / current_weight
-    return x_out, y_out, weights_out
+    return x_out[:i+1], y_out[:i+1], weights_out[:i+1]
('sklearn', 'kernel_approximation.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,10 +1,11 @@
 """
 The :mod:`sklearn.kernel_approximation` module implements several
-approximate kernel feature maps base on Fourier transforms.
+approximate kernel feature maps based on Fourier transforms and Count Sketches.
 """

 # Author: Andreas Mueller <amueller@ais.uni-bonn.de>
-#
+#         Daniel Lopez-Sanchez (TensorSketch) <lope@usal.es>
+
 # License: BSD 3 clause

 import warnings
@@ -13,17 +14,223 @@
 import scipy.sparse as sp
 from scipy.linalg import svd

+try:
+    from scipy.fft import fft, ifft
+except ImportError:  # scipy < 1.4
+    from scipy.fftpack import fft, ifft
+
 from .base import BaseEstimator
 from .base import TransformerMixin
-from .utils import check_array, check_random_state, as_float_array
+from .base import _ClassNamePrefixFeaturesOutMixin
+from .utils import check_random_state
 from .utils.extmath import safe_sparse_dot
 from .utils.validation import check_is_fitted
+from .utils.validation import _check_feature_names_in
 from .metrics.pairwise import pairwise_kernels, KERNEL_PARAMS
-
-
-class RBFSampler(BaseEstimator, TransformerMixin):
-    """Approximates feature map of an RBF kernel by Monte Carlo approximation
-    of its Fourier transform.
+from .utils.validation import check_non_negative
+
+
+class PolynomialCountSketch(
+    _ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator
+):
+    """Polynomial kernel approximation via Tensor Sketch.
+
+    Implements Tensor Sketch, which approximates the feature map
+    of the polynomial kernel::
+
+        K(X, Y) = (gamma * <X, Y> + coef0)^degree
+
+    by efficiently computing a Count Sketch of the outer product of a
+    vector with itself using Fast Fourier Transforms (FFT). Read more in the
+    :ref:`User Guide <polynomial_kernel_approx>`.
+
+    .. versionadded:: 0.24
+
+    Parameters
+    ----------
+    gamma : float, default=1.0
+        Parameter of the polynomial kernel whose feature map
+        will be approximated.
+
+    degree : int, default=2
+        Degree of the polynomial kernel whose feature map
+        will be approximated.
+
+    coef0 : int, default=0
+        Constant term of the polynomial kernel whose feature map
+        will be approximated.
+
+    n_components : int, default=100
+        Dimensionality of the output feature space. Usually, `n_components`
+        should be greater than the number of features in input samples in
+        order to achieve good performance. The optimal score / run time
+        balance is typically achieved around `n_components` = 10 * `n_features`,
+        but this depends on the specific dataset being used.
+
+    random_state : int, RandomState instance, default=None
+        Determines random number generation for indexHash and bitHash
+        initialization. Pass an int for reproducible results across multiple
+        function calls. See :term:`Glossary <random_state>`.
+
+    Attributes
+    ----------
+    indexHash_ : ndarray of shape (degree, n_features), dtype=int64
+        Array of indexes in range [0, n_components) used to represent
+        the 2-wise independent hash functions for Count Sketch computation.
+
+    bitHash_ : ndarray of shape (degree, n_features), dtype=float32
+        Array with random entries in {+1, -1}, used to represent
+        the 2-wise independent hash functions for Count Sketch computation.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.
+    Nystroem : Approximate a kernel map using a subset of the training data.
+    RBFSampler : Approximate a RBF kernel feature map using random Fourier
+        features.
+    SkewedChi2Sampler : Approximate feature map for "skewed chi-squared" kernel.
+    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.
+
+    Examples
+    --------
+    >>> from sklearn.kernel_approximation import PolynomialCountSketch
+    >>> from sklearn.linear_model import SGDClassifier
+    >>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]
+    >>> y = [0, 0, 1, 1]
+    >>> ps = PolynomialCountSketch(degree=3, random_state=1)
+    >>> X_features = ps.fit_transform(X)
+    >>> clf = SGDClassifier(max_iter=10, tol=1e-3)
+    >>> clf.fit(X_features, y)
+    SGDClassifier(max_iter=10)
+    >>> clf.score(X_features, y)
+    1.0
+    """
+
+    def __init__(
+        self, *, gamma=1.0, degree=2, coef0=0, n_components=100, random_state=None
+    ):
+        self.gamma = gamma
+        self.degree = degree
+        self.coef0 = coef0
+        self.n_components = n_components
+        self.random_state = random_state
+
+    def fit(self, X, y=None):
+        """Fit the model with X.
+
+        Initializes the internal variables. The method needs no information
+        about the distribution of data, so we only care about n_features in X.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,) or (n_samples, n_outputs), \
+                default=None
+            Target values (None for unsupervised transformations).
+
+        Returns
+        -------
+        self : object
+            Returns the instance itself.
+        """
+        if not self.degree >= 1:
+            raise ValueError(f"degree={self.degree} should be >=1.")
+
+        X = self._validate_data(X, accept_sparse="csc")
+        random_state = check_random_state(self.random_state)
+
+        n_features = X.shape[1]
+        if self.coef0 != 0:
+            n_features += 1
+
+        self.indexHash_ = random_state.randint(
+            0, high=self.n_components, size=(self.degree, n_features)
+        )
+
+        self.bitHash_ = random_state.choice(a=[-1, 1], size=(self.degree, n_features))
+        self._n_features_out = self.n_components
+        return self
+
+    def transform(self, X):
+        """Generate the feature map approximation for X.
+
+        Parameters
+        ----------
+        X : {array-like}, shape (n_samples, n_features)
+            New data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        Returns
+        -------
+        X_new : array-like, shape (n_samples, n_components)
+            Returns the instance itself.
+        """
+
+        check_is_fitted(self)
+        X = self._validate_data(X, accept_sparse="csc", reset=False)
+
+        X_gamma = np.sqrt(self.gamma) * X
+
+        if sp.issparse(X_gamma) and self.coef0 != 0:
+            X_gamma = sp.hstack(
+                [X_gamma, np.sqrt(self.coef0) * np.ones((X_gamma.shape[0], 1))],
+                format="csc",
+            )
+
+        elif not sp.issparse(X_gamma) and self.coef0 != 0:
+            X_gamma = np.hstack(
+                [X_gamma, np.sqrt(self.coef0) * np.ones((X_gamma.shape[0], 1))]
+            )
+
+        if X_gamma.shape[1] != self.indexHash_.shape[1]:
+            raise ValueError(
+                "Number of features of test samples does not"
+                " match that of training samples."
+            )
+
+        count_sketches = np.zeros((X_gamma.shape[0], self.degree, self.n_components))
+
+        if sp.issparse(X_gamma):
+            for j in range(X_gamma.shape[1]):
+                for d in range(self.degree):
+                    iHashIndex = self.indexHash_[d, j]
+                    iHashBit = self.bitHash_[d, j]
+                    count_sketches[:, d, iHashIndex] += (
+                        (iHashBit * X_gamma[:, j]).toarray().ravel()
+                    )
+
+        else:
+            for j in range(X_gamma.shape[1]):
+                for d in range(self.degree):
+                    iHashIndex = self.indexHash_[d, j]
+                    iHashBit = self.bitHash_[d, j]
+                    count_sketches[:, d, iHashIndex] += iHashBit * X_gamma[:, j]
+
+        # For each same, compute a count sketch of phi(x) using the polynomial
+        # multiplication (via FFT) of p count sketches of x.
+        count_sketches_fft = fft(count_sketches, axis=2, overwrite_x=True)
+        count_sketches_fft_prod = np.prod(count_sketches_fft, axis=1)
+        data_sketch = np.real(ifft(count_sketches_fft_prod, overwrite_x=True))
+
+        return data_sketch
+
+
+class RBFSampler(_ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator):
+    """Approximate a RBF kernel feature map using random Fourier features.

     It implements a variant of Random Kitchen Sinks.[1]

@@ -31,18 +238,59 @@

     Parameters
     ----------
-    gamma : float
-        Parameter of RBF kernel: exp(-gamma * x^2)
-
-    n_components : int
+    gamma : float, default=1.0
+        Parameter of RBF kernel: exp(-gamma * x^2).
+
+    n_components : int, default=100
         Number of Monte Carlo samples per original feature.
         Equals the dimensionality of the computed feature space.

-    random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None, default=None
+        Pseudo-random number generator to control the generation of the random
+        weights and random offset when fitting the training data.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.
+
+    Attributes
+    ----------
+    random_offset_ : ndarray of shape (n_components,), dtype=float64
+        Random offset used to compute the projection in the `n_components`
+        dimensions of the feature space.
+
+    random_weights_ : ndarray of shape (n_features, n_components),\
+        dtype=float64
+        Random projection directions drawn from the Fourier transform
+        of the RBF kernel.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.
+    Nystroem : Approximate a kernel map using a subset of the training data.
+    PolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch.
+    SkewedChi2Sampler : Approximate feature map for
+        "skewed chi-squared" kernel.
+    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.
+
+    Notes
+    -----
+    See "Random Features for Large-Scale Kernel Machines" by A. Rahimi and
+    Benjamin Recht.
+
+    [1] "Weighted Sums of Random Kitchen Sinks: Replacing
+    minimization with randomization in learning" by A. Rahimi and
+    Benjamin Recht.
+    (https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)

     Examples
     --------
@@ -54,28 +302,12 @@
     >>> X_features = rbf_feature.fit_transform(X)
     >>> clf = SGDClassifier(max_iter=5, tol=1e-3)
     >>> clf.fit(X_features, y)
-    ... # doctest: +NORMALIZE_WHITESPACE
-    SGDClassifier(alpha=0.0001, average=False, class_weight=None,
-           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
-           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,
-           n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,
-           random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1,
-           verbose=0, warm_start=False)
+    SGDClassifier(max_iter=5)
     >>> clf.score(X_features, y)
     1.0
-
-    Notes
-    -----
-    See "Random Features for Large-Scale Kernel Machines" by A. Rahimi and
-    Benjamin Recht.
-
-    [1] "Weighted Sums of Random Kitchen Sinks: Replacing
-    minimization with randomization in learning" by A. Rahimi and
-    Benjamin Recht.
-    (https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)
     """

-    def __init__(self, gamma=1., n_components=100, random_state=None):
+    def __init__(self, *, gamma=1.0, n_components=100, random_state=None):
         self.gamma = gamma
         self.n_components = n_components
         self.random_state = random_state
@@ -88,24 +320,29 @@
         Parameters
         ----------
         X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            Training data, where n_samples in the number of samples
-            and n_features is the number of features.
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        y : array-like, shape (n_samples,) or (n_samples, n_outputs), \
+                default=None
+            Target values (None for unsupervised transformations).

         Returns
         -------
         self : object
-            Returns the transformer.
+            Returns the instance itself.
         """

-        X = check_array(X, accept_sparse='csr')
+        X = self._validate_data(X, accept_sparse="csr")
         random_state = check_random_state(self.random_state)
         n_features = X.shape[1]

-        self.random_weights_ = (np.sqrt(2 * self.gamma) * random_state.normal(
-            size=(n_features, self.n_components)))
-
-        self.random_offset_ = random_state.uniform(0, 2 * np.pi,
-                                                   size=self.n_components)
+        self.random_weights_ = np.sqrt(2 * self.gamma) * random_state.normal(
+            size=(n_features, self.n_components)
+        )
+
+        self.random_offset_ = random_state.uniform(0, 2 * np.pi, size=self.n_components)
+        self._n_features_out = self.n_components
         return self

     def transform(self, X):
@@ -114,43 +351,81 @@
         Parameters
         ----------
         X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            New data, where n_samples in the number of samples
-            and n_features is the number of features.
+            New data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.

         Returns
         -------
         X_new : array-like, shape (n_samples, n_components)
+            Returns the instance itself.
         """
-        check_is_fitted(self, 'random_weights_')
-
-        X = check_array(X, accept_sparse='csr')
+        check_is_fitted(self)
+
+        X = self._validate_data(X, accept_sparse="csr", reset=False)
         projection = safe_sparse_dot(X, self.random_weights_)
         projection += self.random_offset_
         np.cos(projection, projection)
-        projection *= np.sqrt(2.) / np.sqrt(self.n_components)
+        projection *= np.sqrt(2.0) / np.sqrt(self.n_components)
         return projection


-class SkewedChi2Sampler(BaseEstimator, TransformerMixin):
-    """Approximates feature map of the "skewed chi-squared" kernel by Monte
-    Carlo approximation of its Fourier transform.
+class SkewedChi2Sampler(
+    _ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator
+):
+    """Approximate feature map for "skewed chi-squared" kernel.

     Read more in the :ref:`User Guide <skewed_chi_kernel_approx>`.

     Parameters
     ----------
-    skewedness : float
+    skewedness : float, default=1.0
         "skewedness" parameter of the kernel. Needs to be cross-validated.

-    n_components : int
-        number of Monte Carlo samples per original feature.
+    n_components : int, default=100
+        Number of Monte Carlo samples per original feature.
         Equals the dimensionality of the computed feature space.

-    random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None, default=None
+        Pseudo-random number generator to control the generation of the random
+        weights and random offset when fitting the training data.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.
+
+    Attributes
+    ----------
+    random_weights_ : ndarray of shape (n_features, n_components)
+        Weight array, sampled from a secant hyperbolic distribution, which will
+        be used to linearly transform the log of the data.
+
+    random_offset_ : ndarray of shape (n_features, n_components)
+        Bias term, which will be added to the data. It is uniformly distributed
+        between 0 and 2*pi.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.
+    Nystroem : Approximate a kernel map using a subset of the training data.
+    RBFSampler : Approximate a RBF kernel feature map using random Fourier
+        features.
+    SkewedChi2Sampler : Approximate feature map for "skewed chi-squared" kernel.
+    sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.
+    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.
+
+    References
+    ----------
+    See "Random Fourier Approximations for Skewed Multiplicative Histogram
+    Kernels" by Fuxin Li, Catalin Ionescu and Cristian Sminchisescu.

     Examples
     --------
@@ -163,30 +438,13 @@
     ...                                  random_state=0)
     >>> X_features = chi2_feature.fit_transform(X, y)
     >>> clf = SGDClassifier(max_iter=10, tol=1e-3)
-    >>> clf.fit(X_features, y)  # doctest: +NORMALIZE_WHITESPACE
-    SGDClassifier(alpha=0.0001, average=False, class_weight=None,
-           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
-           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=10,
-           n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,
-           random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1,
-           verbose=0, warm_start=False)
+    >>> clf.fit(X_features, y)
+    SGDClassifier(max_iter=10)
     >>> clf.score(X_features, y)
     1.0
-
-    References
-    ----------
-    See "Random Fourier Approximations for Skewed Multiplicative Histogram
-    Kernels" by Fuxin Li, Catalin Ionescu and Cristian Sminchisescu.
-
-    See also
-    --------
-    AdditiveChi2Sampler : A different approach for approximating an additive
-        variant of the chi squared kernel.
-
-    sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.
     """

-    def __init__(self, skewedness=1., n_components=100, random_state=None):
+    def __init__(self, *, skewedness=1.0, n_components=100, random_state=None):
         self.skewedness = skewedness
         self.n_components = n_components
         self.random_state = random_state
@@ -199,24 +457,27 @@
         Parameters
         ----------
         X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples in the number of samples
-            and n_features is the number of features.
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        y : array-like, shape (n_samples,) or (n_samples, n_outputs), \
+                default=None
+            Target values (None for unsupervised transformations).

         Returns
         -------
         self : object
-            Returns the transformer.
+            Returns the instance itself.
         """

-        X = check_array(X)
+        X = self._validate_data(X)
         random_state = check_random_state(self.random_state)
         n_features = X.shape[1]
         uniform = random_state.uniform(size=(n_features, self.n_components))
         # transform by inverse CDF of sech
-        self.random_weights_ = (1. / np.pi
-                                * np.log(np.tan(np.pi / 2. * uniform)))
-        self.random_offset_ = random_state.uniform(0, 2 * np.pi,
-                                                   size=self.n_components)
+        self.random_weights_ = 1.0 / np.pi * np.log(np.tan(np.pi / 2.0 * uniform))
+        self.random_offset_ = random_state.uniform(0, 2 * np.pi, size=self.n_components)
+        self._n_features_out = self.n_components
         return self

     def transform(self, X):
@@ -225,32 +486,32 @@
         Parameters
         ----------
         X : array-like, shape (n_samples, n_features)
-            New data, where n_samples in the number of samples
-            and n_features is the number of features. All values of X must be
+            New data, where `n_samples` is the number of samples
+            and `n_features` is the number of features. All values of X must be
             strictly greater than "-skewedness".

         Returns
         -------
         X_new : array-like, shape (n_samples, n_components)
+            Returns the instance itself.
         """
-        check_is_fitted(self, 'random_weights_')
-
-        X = as_float_array(X, copy=True)
-        X = check_array(X, copy=False)
+        check_is_fitted(self)
+        X = self._validate_data(
+            X, copy=True, dtype=[np.float64, np.float32], reset=False
+        )
         if (X <= -self.skewedness).any():
-            raise ValueError("X may not contain entries smaller than"
-                             " -skewedness.")
+            raise ValueError("X may not contain entries smaller than -skewedness.")

         X += self.skewedness
         np.log(X, X)
         projection = safe_sparse_dot(X, self.random_weights_)
         projection += self.random_offset_
         np.cos(projection, projection)
-        projection *= np.sqrt(2.) / np.sqrt(self.n_components)
+        projection *= np.sqrt(2.0) / np.sqrt(self.n_components)
         return projection


-class AdditiveChi2Sampler(BaseEstimator, TransformerMixin):
+class AdditiveChi2Sampler(TransformerMixin, BaseEstimator):
     """Approximate feature map for additive chi2 kernel.

     Uses sampling the fourier transform of the kernel characteristic
@@ -258,7 +519,7 @@

     Since the kernel that is to be approximated is additive, the components of
     the input vectors can be treated separately.  Each entry in the original
-    space is transformed into 2*sample_steps+1 features, where sample_steps is
+    space is transformed into 2*sample_steps-1 features, where sample_steps is
     a parameter of the method. Typical values of sample_steps include 1, 2 and
     3.

@@ -269,10 +530,50 @@

     Parameters
     ----------
-    sample_steps : int, optional
+    sample_steps : int, default=2
         Gives the number of (complex) sampling points.
-    sample_interval : float, optional
+
+    sample_interval : float, default=None
         Sampling interval. Must be specified when sample_steps not in {1,2,3}.
+
+    Attributes
+    ----------
+    sample_interval_ : float
+        Stored sampling interval. Specified as a parameter if `sample_steps`
+        not in {1,2,3}.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    SkewedChi2Sampler : A Fourier-approximation to a non-additive variant of
+        the chi squared kernel.
+
+    sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.
+
+    sklearn.metrics.pairwise.additive_chi2_kernel : The exact additive chi
+        squared kernel.
+
+    Notes
+    -----
+    This estimator approximates a slightly different version of the additive
+    chi squared kernel then ``metric.additive_chi2`` computes.
+
+    References
+    ----------
+    See `"Efficient additive kernels via explicit feature maps"
+    <http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf>`_
+    A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence,
+    2011

     Examples
     --------
@@ -283,58 +584,37 @@
     >>> chi2sampler = AdditiveChi2Sampler(sample_steps=2)
     >>> X_transformed = chi2sampler.fit_transform(X, y)
     >>> clf = SGDClassifier(max_iter=5, random_state=0, tol=1e-3)
-    >>> clf.fit(X_transformed, y)  # doctest: +NORMALIZE_WHITESPACE
-    SGDClassifier(alpha=0.0001, average=False, class_weight=None,
-           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
-           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,
-           n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,
-           random_state=0, shuffle=True, tol=0.001, validation_fraction=0.1,
-           verbose=0, warm_start=False)
-    >>> clf.score(X_transformed, y) # doctest: +ELLIPSIS
+    >>> clf.fit(X_transformed, y)
+    SGDClassifier(max_iter=5, random_state=0)
+    >>> clf.score(X_transformed, y)
     0.9499...
-
-    Notes
-    -----
-    This estimator approximates a slightly different version of the additive
-    chi squared kernel then ``metric.additive_chi2`` computes.
-
-    See also
-    --------
-    SkewedChi2Sampler : A Fourier-approximation to a non-additive variant of
-        the chi squared kernel.
-
-    sklearn.metrics.pairwise.chi2_kernel : The exact chi squared kernel.
-
-    sklearn.metrics.pairwise.additive_chi2_kernel : The exact additive chi
-        squared kernel.
-
-    References
-    ----------
-    See `"Efficient additive kernels via explicit feature maps"
-    <http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/vedaldi11efficient.pdf>`_
-    A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence,
-    2011
     """

-    def __init__(self, sample_steps=2, sample_interval=None):
+    def __init__(self, *, sample_steps=2, sample_interval=None):
         self.sample_steps = sample_steps
         self.sample_interval = sample_interval

     def fit(self, X, y=None):
-        """Set the parameters
+        """Set the parameters.

         Parameters
         ----------
         X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples in the number of samples
-            and n_features is the number of features.
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        y : array-like, shape (n_samples,) or (n_samples, n_outputs), \
+                default=None
+            Target values (None for unsupervised transformations).

         Returns
         -------
         self : object
             Returns the transformer.
         """
-        check_array(X, accept_sparse='csr')
+        X = self._validate_data(X, accept_sparse="csr")
+        check_non_negative(X, "X in AdditiveChi2Sampler.fit")
+
         if self.sample_interval is None:
             # See reference, figure 2 c)
             if self.sample_steps == 1:
@@ -344,8 +624,10 @@
             elif self.sample_steps == 3:
                 self.sample_interval_ = 0.4
             else:
-                raise ValueError("If sample_steps is not in [1, 2, 3],"
-                                 " you need to provide sample_interval")
+                raise ValueError(
+                    "If sample_steps is not in [1, 2, 3],"
+                    " you need to provide sample_interval"
+                )
         else:
             self.sample_interval_ = self.sample_interval
         return self
@@ -355,25 +637,27 @@

         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape = (n_samples, n_features)
+        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.

         Returns
         -------
-        X_new : {array, sparse matrix}, \
-               shape = (n_samples, n_features * (2*sample_steps + 1))
-            Whether the return value is an array of sparse matrix depends on
+        X_new : {ndarray, sparse matrix}, \
+               shape = (n_samples, n_features * (2*sample_steps - 1))
+            Whether the return value is an array or sparse matrix depends on
             the type of the input X.
         """
-        msg = ("%(name)s is not fitted. Call fit to set the parameters before"
-               " calling transform")
-        check_is_fitted(self, "sample_interval_", msg=msg)
-
-        X = check_array(X, accept_sparse='csr')
+        msg = (
+            "%(name)s is not fitted. Call fit to set the parameters before"
+            " calling transform"
+        )
+        check_is_fitted(self, msg=msg)
+
+        X = self._validate_data(X, accept_sparse="csr", reset=False)
+        check_non_negative(X, "X in AdditiveChi2Sampler.transform")
         sparse = sp.issparse(X)

-        # check if X has negative values. Doesn't play well with np.log.
-        if ((X.data if sparse else X) < 0).any():
-            raise ValueError("Entries of X must be non-negative.")
         # zeroth component
         # 1/cosh = sech
         # cosh(0) = 1.0
@@ -381,8 +665,35 @@
         transf = self._transform_sparse if sparse else self._transform_dense
         return transf(X)

+    def get_feature_names_out(self, input_features=None):
+        """Get output feature names for transformation.
+
+        Parameters
+        ----------
+        input_features : array-like of str or None, default=None
+            Only used to validate feature names with the names seen in :meth:`fit`.
+
+        Returns
+        -------
+        feature_names_out : ndarray of str objects
+            Transformed feature names.
+        """
+        input_features = _check_feature_names_in(
+            self, input_features, generate_names=True
+        )
+        est_name = self.__class__.__name__.lower()
+
+        names_list = [f"{est_name}_{name}_sqrt" for name in input_features]
+
+        for j in range(1, self.sample_steps):
+            cos_names = [f"{est_name}_{name}_cos{j}" for name in input_features]
+            sin_names = [f"{est_name}_{name}_sin{j}" for name in input_features]
+            names_list.extend(cos_names + sin_names)
+
+        return np.asarray(names_list, dtype=object)
+
     def _transform_dense(self, X):
-        non_zero = (X != 0.0)
+        non_zero = X != 0.0
         X_nz = X[non_zero]

         X_step = np.zeros_like(X)
@@ -394,8 +705,7 @@
         step_nz = 2 * X_nz * self.sample_interval_

         for j in range(1, self.sample_steps):
-            factor_nz = np.sqrt(step_nz /
-                                np.cosh(np.pi * j * self.sample_interval_))
+            factor_nz = np.sqrt(step_nz / np.cosh(np.pi * j * self.sample_interval_))

             X_step = np.zeros_like(X)
             X_step[non_zero] = factor_nz * np.cos(j * log_step_nz)
@@ -412,34 +722,36 @@
         indptr = X.indptr.copy()

         data_step = np.sqrt(X.data * self.sample_interval_)
-        X_step = sp.csr_matrix((data_step, indices, indptr),
-                               shape=X.shape, dtype=X.dtype, copy=False)
+        X_step = sp.csr_matrix(
+            (data_step, indices, indptr), shape=X.shape, dtype=X.dtype, copy=False
+        )
         X_new = [X_step]

         log_step_nz = self.sample_interval_ * np.log(X.data)
         step_nz = 2 * X.data * self.sample_interval_

         for j in range(1, self.sample_steps):
-            factor_nz = np.sqrt(step_nz /
-                                np.cosh(np.pi * j * self.sample_interval_))
+            factor_nz = np.sqrt(step_nz / np.cosh(np.pi * j * self.sample_interval_))

             data_step = factor_nz * np.cos(j * log_step_nz)
-            X_step = sp.csr_matrix((data_step, indices, indptr),
-                                   shape=X.shape, dtype=X.dtype, copy=False)
+            X_step = sp.csr_matrix(
+                (data_step, indices, indptr), shape=X.shape, dtype=X.dtype, copy=False
+            )
             X_new.append(X_step)

             data_step = factor_nz * np.sin(j * log_step_nz)
-            X_step = sp.csr_matrix((data_step, indices, indptr),
-                                   shape=X.shape, dtype=X.dtype, copy=False)
+            X_step = sp.csr_matrix(
+                (data_step, indices, indptr), shape=X.shape, dtype=X.dtype, copy=False
+            )
             X_new.append(X_step)

         return sp.hstack(X_new)

     def _more_tags(self):
-        return {'stateless': True}
-
-
-class Nystroem(BaseEstimator, TransformerMixin):
+        return {"stateless": True, "requires_positive_X": True}
+
+
+class Nystroem(_ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator):
     """Approximate a kernel map using a subset of the training data.

     Constructs an approximate feature map for an arbitrary kernel
@@ -447,11 +759,13 @@

     Read more in the :ref:`User Guide <nystroem_kernel_approx>`.

+    .. versionadded:: 0.13
+
     Parameters
     ----------
-    kernel : string or callable, default="rbf"
+    kernel : str or callable, default='rbf'
         Kernel map to be approximated. A callable should accept two arguments
-        and the keyword arguments passed to this object as kernel_params, and
+        and the keyword arguments passed to this object as `kernel_params`, and
         should return a floating point number.

     gamma : float, default=None
@@ -467,73 +781,105 @@
     degree : float, default=None
         Degree of the polynomial kernel. Ignored by other kernels.

-    kernel_params : mapping of string to any, optional
+    kernel_params : dict, default=None
         Additional parameters (keyword arguments) for kernel function passed
         as callable object.

-    n_components : int
+    n_components : int, default=100
         Number of features to construct.
         How many data points will be used to construct the mapping.

-    random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None, default=None
+        Pseudo-random number generator to control the uniform sampling without
+        replacement of `n_components` of the training data to construct the
+        basis kernel.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.
+
+    n_jobs : int, default=None
+        The number of jobs to use for the computation. This works by breaking
+        down the kernel matrix into `n_jobs` even slices and computing them in
+        parallel.
+
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
+        for more details.
+
+        .. versionadded:: 0.24

     Attributes
     ----------
-    components_ : array, shape (n_components, n_features)
+    components_ : ndarray of shape (n_components, n_features)
         Subset of training points used to construct the feature map.

-    component_indices_ : array, shape (n_components)
+    component_indices_ : ndarray of shape (n_components)
         Indices of ``components_`` in the training set.

-    normalization_ : array, shape (n_components, n_components)
+    normalization_ : ndarray of shape (n_components, n_components)
         Normalization matrix needed for embedding.
         Square root of the kernel matrix on ``components_``.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    AdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.
+    PolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch.
+    RBFSampler : Approximate a RBF kernel feature map using random Fourier
+        features.
+    SkewedChi2Sampler : Approximate feature map for "skewed chi-squared" kernel.
+    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.
+
+    References
+    ----------
+    * Williams, C.K.I. and Seeger, M.
+      "Using the Nystroem method to speed up kernel machines",
+      Advances in neural information processing systems 2001
+
+    * T. Yang, Y. Li, M. Mahdavi, R. Jin and Z. Zhou
+      "Nystroem Method vs Random Fourier Features: A Theoretical and Empirical
+      Comparison",
+      Advances in Neural Information Processing Systems 2012

     Examples
     --------
     >>> from sklearn import datasets, svm
     >>> from sklearn.kernel_approximation import Nystroem
-    >>> digits = datasets.load_digits(n_class=9)
-    >>> data = digits.data / 16.
+    >>> X, y = datasets.load_digits(n_class=9, return_X_y=True)
+    >>> data = X / 16.
     >>> clf = svm.LinearSVC()
     >>> feature_map_nystroem = Nystroem(gamma=.2,
     ...                                 random_state=1,
     ...                                 n_components=300)
     >>> data_transformed = feature_map_nystroem.fit_transform(data)
-    >>> clf.fit(data_transformed, digits.target)
-    ... # doctest: +NORMALIZE_WHITESPACE
-    LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
-         intercept_scaling=1, loss='squared_hinge', max_iter=1000,
-         multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
-         verbose=0)
-    >>> clf.score(data_transformed, digits.target) # doctest: +ELLIPSIS
+    >>> clf.fit(data_transformed, y)
+    LinearSVC()
+    >>> clf.score(data_transformed, y)
     0.9987...
-
-    References
-    ----------
-    * Williams, C.K.I. and Seeger, M.
-      "Using the Nystroem method to speed up kernel machines",
-      Advances in neural information processing systems 2001
-
-    * T. Yang, Y. Li, M. Mahdavi, R. Jin and Z. Zhou
-      "Nystroem Method vs Random Fourier Features: A Theoretical and Empirical
-      Comparison",
-      Advances in Neural Information Processing Systems 2012
-
-
-    See also
-    --------
-    RBFSampler : An approximation to the RBF kernel using random Fourier
-                 features.
-
-    sklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.
     """
-    def __init__(self, kernel="rbf", gamma=None, coef0=None, degree=None,
-                 kernel_params=None, n_components=100, random_state=None):
+
+    def __init__(
+        self,
+        kernel="rbf",
+        *,
+        gamma=None,
+        coef0=None,
+        degree=None,
+        kernel_params=None,
+        n_components=100,
+        random_state=None,
+        n_jobs=None,
+    ):
+
         self.kernel = kernel
         self.gamma = gamma
         self.coef0 = coef0
@@ -541,6 +887,7 @@
         self.kernel_params = kernel_params
         self.n_components = n_components
         self.random_state = random_state
+        self.n_jobs = n_jobs

     def fit(self, X, y=None):
         """Fit estimator to data.
@@ -550,10 +897,20 @@

         Parameters
         ----------
-        X : array-like, shape=(n_samples, n_feature)
-            Training data.
+        X : array-like, shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        y : array-like, shape (n_samples,) or (n_samples, n_outputs), \
+                default=None
+            Target values (None for unsupervised transformations).
+
+        Returns
+        -------
+        self : object
+            Returns the instance itself.
         """
-        X = check_array(X, accept_sparse='csr')
+        X = self._validate_data(X, accept_sparse="csr")
         rnd = check_random_state(self.random_state)
         n_samples = X.shape[0]

@@ -561,9 +918,11 @@
         if self.n_components > n_samples:
             # XXX should we just bail?
             n_components = n_samples
-            warnings.warn("n_components > n_samples. This is not possible.\n"
-                          "n_components was set to n_samples, which results"
-                          " in inefficient evaluation of the full kernel.")
+            warnings.warn(
+                "n_components > n_samples. This is not possible.\n"
+                "n_components was set to n_samples, which results"
+                " in inefficient evaluation of the full kernel."
+            )

         else:
             n_components = self.n_components
@@ -572,16 +931,21 @@
         basis_inds = inds[:n_components]
         basis = X[basis_inds]

-        basis_kernel = pairwise_kernels(basis, metric=self.kernel,
-                                        filter_params=True,
-                                        **self._get_kernel_params())
+        basis_kernel = pairwise_kernels(
+            basis,
+            metric=self.kernel,
+            filter_params=True,
+            n_jobs=self.n_jobs,
+            **self._get_kernel_params(),
+        )

         # sqrt of kernel matrix on basis vectors
         U, S, V = svd(basis_kernel)
         S = np.maximum(S, 1e-12)
         self.normalization_ = np.dot(U / np.sqrt(S), V)
         self.components_ = basis
-        self.component_indices_ = inds
+        self.component_indices_ = basis_inds
+        self._n_features_out = n_components
         return self

     def transform(self, X):
@@ -592,37 +956,56 @@

         Parameters
         ----------
-        X : array-like, shape=(n_samples, n_features)
+        X : array-like of shape (n_samples, n_features)
             Data to transform.

         Returns
         -------
-        X_transformed : array, shape=(n_samples, n_components)
+        X_transformed : ndarray of shape (n_samples, n_components)
             Transformed data.
         """
-        check_is_fitted(self, 'components_')
-        X = check_array(X, accept_sparse='csr')
+        check_is_fitted(self)
+        X = self._validate_data(X, accept_sparse="csr", reset=False)

         kernel_params = self._get_kernel_params()
-        embedded = pairwise_kernels(X, self.components_,
-                                    metric=self.kernel,
-                                    filter_params=True,
-                                    **kernel_params)
+        embedded = pairwise_kernels(
+            X,
+            self.components_,
+            metric=self.kernel,
+            filter_params=True,
+            n_jobs=self.n_jobs,
+            **kernel_params,
+        )
         return np.dot(embedded, self.normalization_.T)

     def _get_kernel_params(self):
         params = self.kernel_params
         if params is None:
             params = {}
-        if not callable(self.kernel):
-            for param in (KERNEL_PARAMS[self.kernel]):
+        if not callable(self.kernel) and self.kernel != "precomputed":
+            for param in KERNEL_PARAMS[self.kernel]:
                 if getattr(self, param) is not None:
                     params[param] = getattr(self, param)
         else:
-            if (self.gamma is not None or
-                    self.coef0 is not None or
-                    self.degree is not None):
-                raise ValueError("Don't pass gamma, coef0 or degree to "
-                                 "Nystroem if using a callable kernel.")
+            if (
+                self.gamma is not None
+                or self.coef0 is not None
+                or self.degree is not None
+            ):
+                raise ValueError(
+                    "Don't pass gamma, coef0 or degree to "
+                    "Nystroem if using a callable "
+                    "or precomputed kernel"
+                )

         return params
+
+    def _more_tags(self):
+        return {
+            "_xfail_checks": {
+                "check_transformer_preserve_dtypes": (
+                    "dtypes are preserved but not at a close enough precision"
+                )
+            },
+            "preserves_dtype": [np.float64, np.float32],
+        }
('sklearn', 'random_projection.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,5 +1,4 @@
-# -*- coding: utf8
-"""Random Projection transformers
+"""Random Projection transformers.

 Random Projections are a simple and computationally efficient way to
 reduce the dimensionality of the data by trading a controlled amount
@@ -31,10 +30,11 @@
 from abc import ABCMeta, abstractmethod

 import numpy as np
-from numpy.testing import assert_equal
+from scipy import linalg
 import scipy.sparse as sp

 from .base import BaseEstimator, TransformerMixin
+from .base import _ClassNamePrefixFeaturesOutMixin

 from .utils import check_random_state
 from .utils.extmath import safe_sparse_dot
@@ -42,14 +42,15 @@
 from .utils.validation import check_array, check_is_fitted
 from .exceptions import DataDimensionalityWarning

-
-__all__ = ["SparseRandomProjection",
-           "GaussianRandomProjection",
-           "johnson_lindenstrauss_min_dim"]
-
-
-def johnson_lindenstrauss_min_dim(n_samples, eps=0.1):
-    """Find a 'safe' number of components to randomly project to
+__all__ = [
+    "SparseRandomProjection",
+    "GaussianRandomProjection",
+    "johnson_lindenstrauss_min_dim",
+]
+
+
+def johnson_lindenstrauss_min_dim(n_samples, *, eps=0.1):
+    """Find a 'safe' number of components to randomly project to.

     The distortion introduced by a random projection `p` only changes the
     distance between two points by a factor (1 +- eps) in an euclidean space
@@ -58,9 +59,9 @@

       (1 - eps) ||u - v||^2 < ||p(u) - p(v)||^2 < (1 + eps) ||u - v||^2

-    Where u and v are any rows taken from a dataset of shape [n_samples,
-    n_features], eps is in ]0, 1[ and p is a projection by a random Gaussian
-    N(0, 1) matrix with shape [n_components, n_features] (or a sparse
+    Where u and v are any rows taken from a dataset of shape (n_samples,
+    n_features), eps is in ]0, 1[ and p is a projection by a random Gaussian
+    N(0, 1) matrix of shape (n_components, n_features) (or a sparse
     Achlioptas matrix).

     The minimum number of components to guarantee the eps-embedding is
@@ -77,24 +78,25 @@

     Parameters
     ----------
-    n_samples : int or numpy array of int greater than 0,
-        Number of samples. If an array is given, it will compute
-        a safe number of components array-wise.
-
-    eps : float or numpy array of float in ]0,1[, optional (default=0.1)
-        Maximum distortion rate as defined by the Johnson-Lindenstrauss lemma.
-        If an array is given, it will compute a safe number of components
-        array-wise.
+    n_samples : int or array-like of int
+        Number of samples that should be a integer greater than 0. If an array
+        is given, it will compute a safe number of components array-wise.
+
+    eps : float or ndarray of shape (n_components,), dtype=float, \
+            default=0.1
+        Maximum distortion rate in the range (0,1 ) as defined by the
+        Johnson-Lindenstrauss lemma. If an array is given, it will compute a
+        safe number of components array-wise.

     Returns
     -------
-    n_components : int or numpy array of int,
+    n_components : int or ndarray of int
         The minimal number of components to guarantee with good probability
         an eps-embedding with n_samples.

     Examples
     --------
-
+    >>> from sklearn.random_projection import johnson_lindenstrauss_min_dim
     >>> johnson_lindenstrauss_min_dim(1e6, eps=0.5)
     663

@@ -118,40 +120,39 @@
     n_samples = np.asarray(n_samples)

     if np.any(eps <= 0.0) or np.any(eps >= 1):
-        raise ValueError(
-            "The JL bound is defined for eps in ]0, 1[, got %r" % eps)
+        raise ValueError("The JL bound is defined for eps in ]0, 1[, got %r" % eps)

     if np.any(n_samples) <= 0:
         raise ValueError(
             "The JL bound is defined for n_samples greater than zero, got %r"
-            % n_samples)
-
-    denominator = (eps ** 2 / 2) - (eps ** 3 / 3)
-    return (4 * np.log(n_samples) / denominator).astype(np.int)
+            % n_samples
+        )
+
+    denominator = (eps**2 / 2) - (eps**3 / 3)
+    return (4 * np.log(n_samples) / denominator).astype(np.int64)


 def _check_density(density, n_features):
     """Factorize density check according to Li et al."""
-    if density == 'auto':
+    if density == "auto":
         density = 1 / np.sqrt(n_features)

     elif density <= 0 or density > 1:
-        raise ValueError("Expected density in range ]0, 1], got: %r"
-                         % density)
+        raise ValueError("Expected density in range ]0, 1], got: %r" % density)
     return density


 def _check_input_size(n_components, n_features):
-    """Factorize argument checking for random matrix generation"""
+    """Factorize argument checking for random matrix generation."""
     if n_components <= 0:
-        raise ValueError("n_components must be strictly positive, got %d" %
-                         n_components)
+        raise ValueError(
+            "n_components must be strictly positive, got %d" % n_components
+        )
     if n_features <= 0:
-        raise ValueError("n_features must be strictly positive, got %d" %
-                         n_features)
-
-
-def gaussian_random_matrix(n_components, n_features, random_state=None):
+        raise ValueError("n_features must be strictly positive, got %d" % n_features)
+
+
+def _gaussian_random_matrix(n_components, n_features, random_state=None):
     """Generate a dense Gaussian random matrix.

     The components of the random matrix are drawn from
@@ -168,34 +169,31 @@
     n_features : int,
         Dimensionality of the original source space.

-    random_state : int, RandomState instance or None, optional (default=None)
-        Control the pseudo random number generator used to generate the matrix
-        at fit time.  If int, random_state is the seed used by the random
-        number generator; If RandomState instance, random_state is the random
-        number generator; If None, the random number generator is the
-        RandomState instance used by `np.random`.
+    random_state : int, RandomState instance or None, default=None
+        Controls the pseudo random number generator used to generate the matrix
+        at fit time.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.

     Returns
     -------
-    components : numpy array of shape [n_components, n_features]
+    components : ndarray of shape (n_components, n_features)
         The generated Gaussian random matrix.

     See Also
     --------
     GaussianRandomProjection
-    sparse_random_matrix
     """
     _check_input_size(n_components, n_features)
     rng = check_random_state(random_state)
-    components = rng.normal(loc=0.0,
-                            scale=1.0 / np.sqrt(n_components),
-                            size=(n_components, n_features))
+    components = rng.normal(
+        loc=0.0, scale=1.0 / np.sqrt(n_components), size=(n_components, n_features)
+    )
     return components


-def sparse_random_matrix(n_components, n_features, density='auto',
-                         random_state=None):
-    """Generalized Achlioptas random sparse matrix for random projection
+def _sparse_random_matrix(n_components, n_features, density="auto", random_state=None):
+    """Generalized Achlioptas random sparse matrix for random projection.

     Setting density to 1 / 3 will yield the original matrix by Dimitris
     Achlioptas while setting a lower value will yield the generalization
@@ -218,8 +216,9 @@
     n_features : int,
         Dimensionality of the original source space.

-    density : float in range ]0, 1] or 'auto', optional (default='auto')
-        Ratio of non-zero component in the random projection matrix.
+    density : float or 'auto', default='auto'
+        Ratio of non-zero component in the random projection matrix in the
+        range `(0, 1]`

         If density = 'auto', the value is set to the minimum density
         as recommended by Ping Li et al.: 1 / sqrt(n_features).
@@ -227,22 +226,21 @@
         Use density = 1 / 3.0 if you want to reproduce the results from
         Achlioptas, 2001.

-    random_state : int, RandomState instance or None, optional (default=None)
-        Control the pseudo random number generator used to generate the matrix
-        at fit time.  If int, random_state is the seed used by the random
-        number generator; If RandomState instance, random_state is the random
-        number generator; If None, the random number generator is the
-        RandomState instance used by `np.random`.
+    random_state : int, RandomState instance or None, default=None
+        Controls the pseudo random number generator used to generate the matrix
+        at fit time.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.

     Returns
     -------
-    components : array or CSR matrix with shape [n_components, n_features]
-        The generated Gaussian random matrix.
+    components : {ndarray, sparse matrix} of shape (n_components, n_features)
+        The generated Gaussian random matrix. Sparse matrix will be of CSR
+        format.

     See Also
     --------
     SparseRandomProjection
-    gaussian_random_matrix

     References
     ----------
@@ -272,8 +270,9 @@
         for _ in range(n_components):
             # find the indices of the non-zero components for row i
             n_nonzero_i = rng.binomial(n_features, density)
-            indices_i = sample_without_replacement(n_features, n_nonzero_i,
-                                                   random_state=rng)
+            indices_i = sample_without_replacement(
+                n_features, n_nonzero_i, random_state=rng
+            )
             indices.append(indices_i)
             offset += n_nonzero_i
             indptr.append(offset)
@@ -284,13 +283,16 @@
         data = rng.binomial(1, 0.5, size=np.size(indices)) * 2 - 1

         # build the CSR structure by concatenating the rows
-        components = sp.csr_matrix((data, indices, indptr),
-                                   shape=(n_components, n_features))
+        components = sp.csr_matrix(
+            (data, indices, indptr), shape=(n_components, n_features)
+        )

         return np.sqrt(1 / density) / np.sqrt(n_components) * components


-class BaseRandomProjection(BaseEstimator, TransformerMixin, metaclass=ABCMeta):
+class BaseRandomProjection(
+    TransformerMixin, BaseEstimator, _ClassNamePrefixFeaturesOutMixin, metaclass=ABCMeta
+):
     """Base class for random projections.

     Warning: This class should not be used directly.
@@ -298,16 +300,22 @@
     """

     @abstractmethod
-    def __init__(self, n_components='auto', eps=0.1, dense_output=False,
-                 random_state=None):
+    def __init__(
+        self,
+        n_components="auto",
+        *,
+        eps=0.1,
+        compute_inverse_components=False,
+        random_state=None,
+    ):
         self.n_components = n_components
         self.eps = eps
-        self.dense_output = dense_output
+        self.compute_inverse_components = compute_inverse_components
         self.random_state = random_state

     @abstractmethod
     def _make_random_matrix(self, n_components, n_features):
-        """ Generate the random projection matrix
+        """Generate the random projection matrix.

         Parameters
         ----------
@@ -319,53 +327,65 @@

         Returns
         -------
-        components : numpy array or CSR matrix [n_components, n_features]
-            The generated random matrix.
+        components : {ndarray, sparse matrix} of shape (n_components, n_features)
+            The generated random matrix. Sparse matrix will be of CSR format.

         """

+    def _compute_inverse_components(self):
+        """Compute the pseudo-inverse of the (densified) components."""
+        components = self.components_
+        if sp.issparse(components):
+            components = components.toarray()
+        return linalg.pinv(components, check_finite=False)
+
     def fit(self, X, y=None):
-        """Generate a sparse random projection matrix
+        """Generate a sparse random projection matrix.

         Parameters
         ----------
-        X : numpy array or scipy.sparse of shape [n_samples, n_features]
+        X : {ndarray, sparse matrix} of shape (n_samples, n_features)
             Training set: only the shape is used to find optimal random
             matrix dimensions based on the theory referenced in the
             afore mentioned papers.

-        y
-            Ignored
+        y : Ignored
+            Not used, present here for API consistency by convention.

         Returns
         -------
-        self
-
+        self : object
+            BaseRandomProjection class instance.
         """
-        X = check_array(X, accept_sparse=['csr', 'csc'])
+        X = self._validate_data(
+            X, accept_sparse=["csr", "csc"], dtype=[np.float64, np.float32]
+        )

         n_samples, n_features = X.shape

-        if self.n_components == 'auto':
+        if self.n_components == "auto":
             self.n_components_ = johnson_lindenstrauss_min_dim(
-                n_samples=n_samples, eps=self.eps)
+                n_samples=n_samples, eps=self.eps
+            )

             if self.n_components_ <= 0:
                 raise ValueError(
-                    'eps=%f and n_samples=%d lead to a target dimension of '
-                    '%d which is invalid' % (
-                        self.eps, n_samples, self.n_components_))
+                    "eps=%f and n_samples=%d lead to a target dimension of "
+                    "%d which is invalid" % (self.eps, n_samples, self.n_components_)
+                )

             elif self.n_components_ > n_features:
                 raise ValueError(
-                    'eps=%f and n_samples=%d lead to a target dimension of '
-                    '%d which is larger than the original space with '
-                    'n_features=%d' % (self.eps, n_samples, self.n_components_,
-                                       n_features))
+                    "eps=%f and n_samples=%d lead to a target dimension of "
+                    "%d which is larger than the original space with "
+                    "n_features=%d"
+                    % (self.eps, n_samples, self.n_components_, n_features)
+                )
         else:
             if self.n_components <= 0:
-                raise ValueError("n_components must be greater than 0, got %s"
-                                 % self.n_components)
+                raise ValueError(
+                    "n_components must be greater than 0, got %s" % self.n_components
+                )

             elif self.n_components > n_features:
                 warnings.warn(
@@ -373,61 +393,76 @@
                     " features: n_features < n_components (%s < %s)."
                     "The dimensionality of the problem will not be reduced."
                     % (n_features, self.n_components),
-                    DataDimensionalityWarning)
+                    DataDimensionalityWarning,
+                )

             self.n_components_ = self.n_components

         # Generate a projection matrix of size [n_components, n_features]
-        self.components_ = self._make_random_matrix(self.n_components_,
-                                                    n_features)
-
-        # Check contract
-        assert_equal(
-            self.components_.shape,
-            (self.n_components_, n_features),
-            err_msg=('An error has occurred the self.components_ matrix has '
-                     ' not the proper shape.'))
+        self.components_ = self._make_random_matrix(
+            self.n_components_, n_features
+        ).astype(X.dtype, copy=False)
+
+        if self.compute_inverse_components:
+            self.inverse_components_ = self._compute_inverse_components()

         return self

-    def transform(self, X):
-        """Project the data by using matrix product with the random matrix
+    @property
+    def _n_features_out(self):
+        """Number of transformed output features.
+
+        Used by _ClassNamePrefixFeaturesOutMixin.get_feature_names_out.
+        """
+        return self.n_components
+
+    def inverse_transform(self, X):
+        """Project data back to its original space.
+
+        Returns an array X_original whose transform would be X. Note that even
+        if X is sparse, X_original is dense: this may use a lot of RAM.
+
+        If `compute_inverse_components` is False, the inverse of the components is
+        computed during each call to `inverse_transform` which can be costly.

         Parameters
         ----------
-        X : numpy array or scipy.sparse of shape [n_samples, n_features]
-            The input data to project into a smaller dimensional space.
+        X : {array-like, sparse matrix} of shape (n_samples, n_components)
+            Data to be transformed back.

         Returns
         -------
-        X_new : numpy array or scipy sparse of shape [n_samples, n_components]
-            Projected array.
+        X_original : ndarray of shape (n_samples, n_features)
+            Reconstructed data.
         """
-        X = check_array(X, accept_sparse=['csr', 'csc'])
-
-        check_is_fitted(self, 'components_')
-
-        if X.shape[1] != self.components_.shape[1]:
-            raise ValueError(
-                'Impossible to perform projection:'
-                'X at fit stage had a different number of features. '
-                '(%s != %s)' % (X.shape[1], self.components_.shape[1]))
-
-        X_new = safe_sparse_dot(X, self.components_.T,
-                                dense_output=self.dense_output)
-        return X_new
+        check_is_fitted(self)
+
+        X = check_array(X, dtype=[np.float64, np.float32], accept_sparse=("csr", "csc"))
+
+        if self.compute_inverse_components:
+            return X @ self.inverse_components_.T
+
+        inverse_components = self._compute_inverse_components()
+        return X @ inverse_components.T
+
+    def _more_tags(self):
+        return {
+            "preserves_dtype": [np.float64, np.float32],
+        }


 class GaussianRandomProjection(BaseRandomProjection):
-    """Reduce dimensionality through Gaussian random projection
+    """Reduce dimensionality through Gaussian random projection.

     The components of the random matrix are drawn from N(0, 1 / n_components).

     Read more in the :ref:`User Guide <gaussian_random_matrix>`.

+    .. versionadded:: 0.13
+
     Parameters
     ----------
-    n_components : int or 'auto', optional (default = 'auto')
+    n_components : int or 'auto', default='auto'
         Dimensionality of the target projection space.

         n_components can be automatically adjusted according to the
@@ -439,54 +474,84 @@
         very conservative estimated of the required number of components
         as it makes no assumption on the structure of the dataset.

-    eps : strictly positive float, optional (default=0.1)
+    eps : float, default=0.1
         Parameter to control the quality of the embedding according to
-        the Johnson-Lindenstrauss lemma when n_components is set to
-        'auto'.
+        the Johnson-Lindenstrauss lemma when `n_components` is set to
+        'auto'. The value should be strictly positive.

         Smaller values lead to better embedding and higher number of
         dimensions (n_components) in the target projection space.

-    random_state : int, RandomState instance or None, optional (default=None)
-        Control the pseudo random number generator used to generate the matrix
-        at fit time.  If int, random_state is the seed used by the random
-        number generator; If RandomState instance, random_state is the random
-        number generator; If None, the random number generator is the
-        RandomState instance used by `np.random`.
+    compute_inverse_components : bool, default=False
+        Learn the inverse transform by computing the pseudo-inverse of the
+        components during fit. Note that computing the pseudo-inverse does not
+        scale well to large matrices.
+
+    random_state : int, RandomState instance or None, default=None
+        Controls the pseudo random number generator used to generate the
+        projection matrix at fit time.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.

     Attributes
     ----------
-    n_component_ : int
+    n_components_ : int
         Concrete number of components computed when n_components="auto".

-    components_ : numpy array of shape [n_components, n_features]
+    components_ : ndarray of shape (n_components, n_features)
         Random matrix used for the projection.
+
+    inverse_components_ : ndarray of shape (n_features, n_components)
+        Pseudo-inverse of the components, only computed if
+        `compute_inverse_components` is True.
+
+        .. versionadded:: 1.1
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    SparseRandomProjection : Reduce dimensionality through sparse
+        random projection.

     Examples
     --------
     >>> import numpy as np
     >>> from sklearn.random_projection import GaussianRandomProjection
     >>> rng = np.random.RandomState(42)
-    >>> X = rng.rand(100, 10000)
+    >>> X = rng.rand(25, 3000)
     >>> transformer = GaussianRandomProjection(random_state=rng)
     >>> X_new = transformer.fit_transform(X)
     >>> X_new.shape
-    (100, 3947)
-
-    See Also
-    --------
-    SparseRandomProjection
-
+    (25, 2759)
     """
-    def __init__(self, n_components='auto', eps=0.1, random_state=None):
+
+    def __init__(
+        self,
+        n_components="auto",
+        *,
+        eps=0.1,
+        compute_inverse_components=False,
+        random_state=None,
+    ):
         super().__init__(
             n_components=n_components,
             eps=eps,
-            dense_output=True,
-            random_state=random_state)
+            compute_inverse_components=compute_inverse_components,
+            random_state=random_state,
+        )

     def _make_random_matrix(self, n_components, n_features):
-        """ Generate the random projection matrix
+        """Generate the random projection matrix.

         Parameters
         ----------
@@ -498,18 +563,37 @@

         Returns
         -------
-        components : numpy array or CSR matrix [n_components, n_features]
+        components : ndarray of shape (n_components, n_features)
             The generated random matrix.
-
         """
         random_state = check_random_state(self.random_state)
-        return gaussian_random_matrix(n_components,
-                                      n_features,
-                                      random_state=random_state)
+        return _gaussian_random_matrix(
+            n_components, n_features, random_state=random_state
+        )
+
+    def transform(self, X):
+        """Project the data by using matrix product with the random matrix.
+
+        Parameters
+        ----------
+        X : {ndarray, sparse matrix} of shape (n_samples, n_features)
+            The input data to project into a smaller dimensional space.
+
+        Returns
+        -------
+        X_new : ndarray of shape (n_samples, n_components)
+            Projected array.
+        """
+        check_is_fitted(self)
+        X = self._validate_data(
+            X, accept_sparse=["csr", "csc"], reset=False, dtype=[np.float64, np.float32]
+        )
+
+        return X @ self.components_.T


 class SparseRandomProjection(BaseRandomProjection):
-    """Reduce dimensionality through sparse random projection
+    """Reduce dimensionality through sparse random projection.

     Sparse random matrix is an alternative to dense random
     projection matrix that guarantees similar embedding quality while being
@@ -525,9 +609,11 @@

     Read more in the :ref:`User Guide <sparse_random_matrix>`.

+    .. versionadded:: 0.13
+
     Parameters
     ----------
-    n_components : int or 'auto', optional (default = 'auto')
+    n_components : int or 'auto', default='auto'
         Dimensionality of the target projection space.

         n_components can be automatically adjusted according to the
@@ -539,8 +625,9 @@
         very conservative estimated of the required number of components
         as it makes no assumption on the structure of the dataset.

-    density : float in range ]0, 1], optional (default='auto')
-        Ratio of non-zero component in the random projection matrix.
+    density : float or 'auto', default='auto'
+        Ratio in the range (0, 1] of non-zero component in the random
+        projection matrix.

         If density = 'auto', the value is set to the minimum density
         as recommended by Ping Li et al.: 1 / sqrt(n_features).
@@ -548,15 +635,15 @@
         Use density = 1 / 3.0 if you want to reproduce the results from
         Achlioptas, 2001.

-    eps : strictly positive float, optional, (default=0.1)
+    eps : float, default=0.1
         Parameter to control the quality of the embedding according to
         the Johnson-Lindenstrauss lemma when n_components is set to
-        'auto'.
+        'auto'. This value should be strictly positive.

         Smaller values lead to better embedding and higher number of
         dimensions (n_components) in the target projection space.

-    dense_output : boolean, optional (default=False)
+    dense_output : bool, default=False
         If True, ensure that the output of the random projection is a
         dense numpy array even if the input and random projection matrix
         are both sparse. In practice, if the number of components is
@@ -567,83 +654,139 @@
         If False, the projected data uses a sparse representation if
         the input is sparse.

-    random_state : int, RandomState instance or None, optional (default=None)
-        Control the pseudo random number generator used to generate the matrix
-        at fit time.  If int, random_state is the seed used by the random
-        number generator; If RandomState instance, random_state is the random
-        number generator; If None, the random number generator is the
-        RandomState instance used by `np.random`.
+    compute_inverse_components : bool, default=False
+        Learn the inverse transform by computing the pseudo-inverse of the
+        components during fit. Note that the pseudo-inverse is always a dense
+        array, even if the training data was sparse. This means that it might be
+        necessary to call `inverse_transform` on a small batch of samples at a
+        time to avoid exhausting the available memory on the host. Moreover,
+        computing the pseudo-inverse does not scale well to large matrices.
+
+    random_state : int, RandomState instance or None, default=None
+        Controls the pseudo random number generator used to generate the
+        projection matrix at fit time.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.

     Attributes
     ----------
-    n_component_ : int
+    n_components_ : int
         Concrete number of components computed when n_components="auto".

-    components_ : CSR matrix with shape [n_components, n_features]
-        Random matrix used for the projection.
+    components_ : sparse matrix of shape (n_components, n_features)
+        Random matrix used for the projection. Sparse matrix will be of CSR
+        format.
+
+    inverse_components_ : ndarray of shape (n_features, n_components)
+        Pseudo-inverse of the components, only computed if
+        `compute_inverse_components` is True.
+
+        .. versionadded:: 1.1

     density_ : float in range 0.0 - 1.0
         Concrete density computed from when density = "auto".
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    GaussianRandomProjection : Reduce dimensionality through Gaussian
+        random projection.
+
+    References
+    ----------
+
+    .. [1] Ping Li, T. Hastie and K. W. Church, 2006,
+           "Very Sparse Random Projections".
+           https://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf
+
+    .. [2] D. Achlioptas, 2001, "Database-friendly random projections",
+           https://users.soe.ucsc.edu/~optas/papers/jl.pdf

     Examples
     --------
     >>> import numpy as np
     >>> from sklearn.random_projection import SparseRandomProjection
     >>> rng = np.random.RandomState(42)
-    >>> X = rng.rand(100, 10000)
+    >>> X = rng.rand(25, 3000)
     >>> transformer = SparseRandomProjection(random_state=rng)
     >>> X_new = transformer.fit_transform(X)
     >>> X_new.shape
-    (100, 3947)
+    (25, 2759)
     >>> # very few components are non-zero
-    >>> np.mean(transformer.components_ != 0) # doctest: +ELLIPSIS
-    0.0100...
-
-    See Also
-    --------
-    GaussianRandomProjection
-
-    References
-    ----------
-
-    .. [1] Ping Li, T. Hastie and K. W. Church, 2006,
-           "Very Sparse Random Projections".
-           https://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf
-
-    .. [2] D. Achlioptas, 2001, "Database-friendly random projections",
-           https://users.soe.ucsc.edu/~optas/papers/jl.pdf
-
+    >>> np.mean(transformer.components_ != 0)
+    0.0182...
     """
-    def __init__(self, n_components='auto', density='auto', eps=0.1,
-                 dense_output=False, random_state=None):
+
+    def __init__(
+        self,
+        n_components="auto",
+        *,
+        density="auto",
+        eps=0.1,
+        dense_output=False,
+        compute_inverse_components=False,
+        random_state=None,
+    ):
         super().__init__(
             n_components=n_components,
             eps=eps,
-            dense_output=dense_output,
-            random_state=random_state)
-
+            compute_inverse_components=compute_inverse_components,
+            random_state=random_state,
+        )
+
+        self.dense_output = dense_output
         self.density = density

     def _make_random_matrix(self, n_components, n_features):
-        """ Generate the random projection matrix
+        """Generate the random projection matrix

         Parameters
         ----------
-        n_components : int,
+        n_components : int
             Dimensionality of the target projection space.

-        n_features : int,
+        n_features : int
             Dimensionality of the original source space.

         Returns
         -------
-        components : numpy array or CSR matrix [n_components, n_features]
-            The generated random matrix.
+        components : sparse matrix of shape (n_components, n_features)
+            The generated random matrix in CSR format.

         """
         random_state = check_random_state(self.random_state)
         self.density_ = _check_density(self.density, n_features)
-        return sparse_random_matrix(n_components,
-                                    n_features,
-                                    density=self.density_,
-                                    random_state=random_state)
+        return _sparse_random_matrix(
+            n_components, n_features, density=self.density_, random_state=random_state
+        )
+
+    def transform(self, X):
+        """Project the data by using matrix product with the random matrix.
+
+        Parameters
+        ----------
+        X : {ndarray, sparse matrix} of shape (n_samples, n_features)
+            The input data to project into a smaller dimensional space.
+
+        Returns
+        -------
+        X_new : {ndarray, sparse matrix} of shape (n_samples, n_components)
+            Projected array. It is a sparse matrix only when the input is sparse and
+            `dense_output = False`.
+        """
+        check_is_fitted(self)
+        X = self._validate_data(
+            X, accept_sparse=["csr", "csc"], reset=False, dtype=[np.float64, np.float32]
+        )
+
+        return safe_sparse_dot(X, self.components_.T, dense_output=self.dense_output)
('sklearn', 'isotonic.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,15 +6,16 @@
 import numpy as np
 from scipy import interpolate
 from scipy.stats import spearmanr
+import warnings
+import math
+
 from .base import BaseEstimator, TransformerMixin, RegressorMixin
 from .utils import check_array, check_consistent_length
+from .utils.validation import _check_sample_weight
 from ._isotonic import _inplace_contiguous_isotonic_regression, _make_unique
-import warnings
-import math
-
-
-__all__ = ['check_increasing', 'isotonic_regression',
-           'IsotonicRegression']
+
+
+__all__ = ["check_increasing", "isotonic_regression", "IsotonicRegression"]


 def check_increasing(x, y):
@@ -25,10 +26,10 @@

     Parameters
     ----------
-    x : array-like, shape=(n_samples,)
+    x : array-like of shape (n_samples,)
             Training data.

-    y : array-like, shape=(n_samples,)
+    y : array-like of shape (n_samples,)
         Training target.

     Returns
@@ -56,7 +57,7 @@

     # Run Fisher transform to get the rho CI, but handle rho=+/-1
     if rho not in [-1.0, 1.0] and len(x) > 3:
-        F = 0.5 * math.log((1. + rho) / (1. - rho))
+        F = 0.5 * math.log((1.0 + rho) / (1.0 - rho))
         F_se = 1 / math.sqrt(len(x) - 3)

         # Use a 95% CI, i.e., +/-1.96 S.E.
@@ -66,47 +67,43 @@

         # Warn if the CI spans zero.
         if np.sign(rho_0) != np.sign(rho_1):
-            warnings.warn("Confidence interval of the Spearman "
-                          "correlation coefficient spans zero. "
-                          "Determination of ``increasing`` may be "
-                          "suspect.")
+            warnings.warn(
+                "Confidence interval of the Spearman "
+                "correlation coefficient spans zero. "
+                "Determination of ``increasing`` may be "
+                "suspect."
+            )

     return increasing_bool


-def isotonic_regression(y, sample_weight=None, y_min=None, y_max=None,
-                        increasing=True):
-    """Solve the isotonic regression model::
-
-        min sum w[i] (y[i] - y_[i]) ** 2
-
-        subject to y_min = y_[1] <= y_[2] ... <= y_[n] = y_max
-
-    where:
-        - y[i] are inputs (real numbers)
-        - y_[i] are fitted
-        - w[i] are optional strictly positive weights (default to 1.0)
+def isotonic_regression(
+    y, *, sample_weight=None, y_min=None, y_max=None, increasing=True
+):
+    """Solve the isotonic regression model.

     Read more in the :ref:`User Guide <isotonic>`.

     Parameters
     ----------
-    y : iterable of floats
+    y : array-like of shape (n_samples,)
         The data.

-    sample_weight : iterable of floats, optional, default: None
+    sample_weight : array-like of shape (n_samples,), default=None
         Weights on each point of the regression.
         If None, weight is set to 1 (equal weights).

-    y_min : optional, default: None
-        If not None, set the lowest value of the fit to y_min.
-
-    y_max : optional, default: None
-        If not None, set the highest value of the fit to y_max.
-
-    increasing : boolean, optional, default: True
+    y_min : float, default=None
+        Lower bound on the lowest predicted value (the minimum value may
+        still be higher). If not set, defaults to -inf.
+
+    y_max : float, default=None
+        Upper bound on the highest predicted value (the maximum may still be
+        lower). If not set, defaults to +inf.
+
+    increasing : bool, default=True
         Whether to compute ``y_`` is increasing (if set to True) or decreasing
-        (if set to False)
+        (if set to False).

     Returns
     -------
@@ -119,12 +116,10 @@
     by Michael J. Best and Nilotpal Chakravarti, section 3.
     """
     order = np.s_[:] if increasing else np.s_[::-1]
-    y = check_array(y, ensure_2d=False, dtype=[np.float64, np.float32])
+    y = check_array(y, ensure_2d=False, input_name="y", dtype=[np.float64, np.float32])
     y = np.array(y[order], dtype=y.dtype)
-    if sample_weight is None:
-        sample_weight = np.ones(len(y), dtype=y.dtype)
-    else:
-        sample_weight = np.array(sample_weight[order], dtype=y.dtype)
+    sample_weight = _check_sample_weight(sample_weight, y, dtype=y.dtype, copy=True)
+    sample_weight = np.ascontiguousarray(sample_weight[order])

     _inplace_contiguous_isotonic_regression(y, sample_weight)
     if y_min is not None or y_max is not None:
@@ -137,48 +132,36 @@
     return y[order]


-class IsotonicRegression(BaseEstimator, TransformerMixin, RegressorMixin):
+class IsotonicRegression(RegressorMixin, TransformerMixin, BaseEstimator):
     """Isotonic regression model.

-    The isotonic regression optimization problem is defined by::
-
-        min sum w_i (y[i] - y_[i]) ** 2
-
-        subject to y_[i] <= y_[j] whenever X[i] <= X[j]
-        and min(y_) = y_min, max(y_) = y_max
-
-    where:
-        - ``y[i]`` are inputs (real numbers)
-        - ``y_[i]`` are fitted
-        - ``X`` specifies the order.
-          If ``X`` is non-decreasing then ``y_`` is non-decreasing.
-        - ``w[i]`` are optional strictly positive weights (default to 1.0)
-
     Read more in the :ref:`User Guide <isotonic>`.

+    .. versionadded:: 0.13
+
     Parameters
     ----------
-    y_min : optional, default: None
-        If not None, set the lowest value of the fit to y_min.
-
-    y_max : optional, default: None
-        If not None, set the highest value of the fit to y_max.
-
-    increasing : boolean or string, optional, default: True
-        If boolean, whether or not to fit the isotonic regression with y
-        increasing or decreasing.
-
-        The string value "auto" determines whether y should
-        increase or decrease based on the Spearman correlation estimate's
-        sign.
-
-    out_of_bounds : string, optional, default: "nan"
-        The ``out_of_bounds`` parameter handles how x-values outside of the
-        training domain are handled.  When set to "nan", predicted y-values
-        will be NaN.  When set to "clip", predicted y-values will be
-        set to the value corresponding to the nearest train interval endpoint.
-        When set to "raise", allow ``interp1d`` to throw ValueError.
-
+    y_min : float, default=None
+        Lower bound on the lowest predicted value (the minimum value may
+        still be higher). If not set, defaults to -inf.
+
+    y_max : float, default=None
+        Upper bound on the highest predicted value (the maximum may still be
+        lower). If not set, defaults to +inf.
+
+    increasing : bool or 'auto', default=True
+        Determines whether the predictions should be constrained to increase
+        or decrease with `X`. 'auto' will decide based on the Spearman
+        correlation estimate's sign.
+
+    out_of_bounds : {'nan', 'clip', 'raise'}, default='nan'
+        Handles how `X` values outside of the training domain are handled
+        during prediction.
+
+        - 'nan', predictions will be NaN.
+        - 'clip', predictions will be set to the value corresponding to
+          the nearest train interval endpoint.
+        - 'raise', a `ValueError` is raised.

     Attributes
     ----------
@@ -188,12 +171,35 @@
     X_max_ : float
         Maximum value of input array `X_` for right bound.

+    X_thresholds_ : ndarray of shape (n_thresholds,)
+        Unique ascending `X` values used to interpolate
+        the y = f(X) monotonic function.
+
+        .. versionadded:: 0.24
+
+    y_thresholds_ : ndarray of shape (n_thresholds,)
+        De-duplicated `y` values suitable to interpolate the y = f(X)
+        monotonic function.
+
+        .. versionadded:: 0.24
+
     f_ : function
         The stepwise interpolating function that covers the input domain ``X``.

+    increasing_ : bool
+        Inferred value for ``increasing``.
+
+    See Also
+    --------
+    sklearn.linear_model.LinearRegression : Ordinary least squares Linear
+        Regression.
+    sklearn.ensemble.HistGradientBoostingRegressor : Gradient boosting that
+        is a non-parametric model accepting monotonicity constraints.
+    isotonic_regression : Function to solve the isotonic regression model.
+
     Notes
     -----
-    Ties are broken using the secondary method from Leeuw, 1977.
+    Ties are broken using the secondary method from de Leeuw, 1977.

     References
     ----------
@@ -204,80 +210,84 @@

     Isotone Optimization in R : Pool-Adjacent-Violators
     Algorithm (PAVA) and Active Set Methods
-    Leeuw, Hornik, Mair
+    de Leeuw, Hornik, Mair
     Journal of Statistical Software 2009

     Correctness of Kruskal's algorithms for monotone regression with ties
-    Leeuw, Psychometrica, 1977
+    de Leeuw, Psychometrica, 1977

     Examples
     --------
     >>> from sklearn.datasets import make_regression
     >>> from sklearn.isotonic import IsotonicRegression
     >>> X, y = make_regression(n_samples=10, n_features=1, random_state=41)
-    >>> iso_reg = IsotonicRegression().fit(X.flatten(), y)
-    >>> iso_reg.predict([.1, .2])  # doctest: +ELLIPSIS
+    >>> iso_reg = IsotonicRegression().fit(X, y)
+    >>> iso_reg.predict([.1, .2])
     array([1.8628..., 3.7256...])
     """
-    def __init__(self, y_min=None, y_max=None, increasing=True,
-                 out_of_bounds='nan'):
+
+    def __init__(self, *, y_min=None, y_max=None, increasing=True, out_of_bounds="nan"):
         self.y_min = y_min
         self.y_max = y_max
         self.increasing = increasing
         self.out_of_bounds = out_of_bounds

-    def _check_fit_data(self, X, y, sample_weight=None):
-        if len(X.shape) != 1:
-            raise ValueError("X should be a 1d array")
+    def _check_input_data_shape(self, X):
+        if not (X.ndim == 1 or (X.ndim == 2 and X.shape[1] == 1)):
+            msg = (
+                "Isotonic regression input X should be a 1d array or "
+                "2d array with 1 feature"
+            )
+            raise ValueError(msg)

     def _build_f(self, X, y):
         """Build the f_ interp1d function."""

         # Handle the out_of_bounds argument by setting bounds_error
         if self.out_of_bounds not in ["raise", "nan", "clip"]:
-            raise ValueError("The argument ``out_of_bounds`` must be in "
-                             "'nan', 'clip', 'raise'; got {0}"
-                             .format(self.out_of_bounds))
+            raise ValueError(
+                "The argument ``out_of_bounds`` must be in "
+                "'nan', 'clip', 'raise'; got {0}".format(self.out_of_bounds)
+            )

         bounds_error = self.out_of_bounds == "raise"
         if len(y) == 1:
             # single y, constant prediction
             self.f_ = lambda x: y.repeat(x.shape)
         else:
-            self.f_ = interpolate.interp1d(X, y, kind='linear',
-                                           bounds_error=bounds_error)
+            self.f_ = interpolate.interp1d(
+                X, y, kind="linear", bounds_error=bounds_error
+            )

     def _build_y(self, X, y, sample_weight, trim_duplicates=True):
         """Build the y_ IsotonicRegression."""
-        self._check_fit_data(X, y, sample_weight)
+        self._check_input_data_shape(X)
+        X = X.reshape(-1)  # use 1d view

         # Determine increasing if auto-determination requested
-        if self.increasing == 'auto':
+        if self.increasing == "auto":
             self.increasing_ = check_increasing(X, y)
         else:
             self.increasing_ = self.increasing

         # If sample_weights is passed, removed zero-weight values and clean
         # order
-        if sample_weight is not None:
-            sample_weight = check_array(sample_weight, ensure_2d=False,
-                                        dtype=X.dtype)
-            mask = sample_weight > 0
-            X, y, sample_weight = X[mask], y[mask], sample_weight[mask]
-        else:
-            sample_weight = np.ones(len(y), dtype=X.dtype)
+        sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)
+        mask = sample_weight > 0
+        X, y, sample_weight = X[mask], y[mask], sample_weight[mask]

         order = np.lexsort((y, X))
         X, y, sample_weight = [array[order] for array in [X, y, sample_weight]]
-        unique_X, unique_y, unique_sample_weight = _make_unique(
-            X, y, sample_weight)
-
-        # Store _X_ and _y_ to maintain backward compat during the deprecation
-        # period of X_ and y_
-        self._X_ = X = unique_X
-        self._y_ = y = isotonic_regression(unique_y, unique_sample_weight,
-                                           self.y_min, self.y_max,
-                                           increasing=self.increasing_)
+        unique_X, unique_y, unique_sample_weight = _make_unique(X, y, sample_weight)
+
+        X = unique_X
+        y = isotonic_regression(
+            unique_y,
+            sample_weight=unique_sample_weight,
+            y_min=self.y_min,
+            y_max=self.y_max,
+            increasing=self.increasing_,
+        )

         # Handle the left and right bounds on X
         self.X_min_, self.X_max_ = np.min(X), np.max(X)
@@ -288,8 +298,7 @@
             # Aside from the 1st and last point, remove points whose y values
             # are equal to both the point before and the point after it.
             keep_data[1:-1] = np.logical_or(
-                np.not_equal(y[1:-1], y[:-2]),
-                np.not_equal(y[1:-1], y[2:])
+                np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:])
             )
             return X[keep_data], y[keep_data]
         else:
@@ -304,13 +313,16 @@

         Parameters
         ----------
-        X : array-like, shape=(n_samples,)
+        X : array-like of shape (n_samples,) or (n_samples, 1)
             Training data.

-        y : array-like, shape=(n_samples,)
+            .. versionchanged:: 0.24
+               Also accepts 2d array with 1 feature.
+
+        y : array-like of shape (n_samples,)
             Training target.

-        sample_weight : array-like, shape=(n_samples,), optional, default: None
+        sample_weight : array-like of shape (n_samples,), default=None
             Weights. If set to None, all weights will be set to 1 (equal
             weights).

@@ -321,13 +333,14 @@

         Notes
         -----
-        X is stored for future use, as `transform` needs X to interpolate
+        X is stored for future use, as :meth:`transform` needs X to interpolate
         new input data.
         """
-        check_params = dict(accept_sparse=False, ensure_2d=False,
-                            dtype=[np.float64, np.float32])
-        X = check_array(X, **check_params)
-        y = check_array(y, **check_params)
+        check_params = dict(accept_sparse=False, ensure_2d=False)
+        X = check_array(
+            X, input_name="X", dtype=[np.float64, np.float32], **check_params
+        )
+        y = check_array(y, input_name="y", dtype=X.dtype, **check_params)
         check_consistent_length(X, y, sample_weight)

         # Transform y by running the isotonic regression algorithm and
@@ -338,41 +351,45 @@
         # on the model to make it possible to support model persistence via
         # the pickle module as the object built by scipy.interp1d is not
         # picklable directly.
-        self._necessary_X_, self._necessary_y_ = X, y
+        self.X_thresholds_, self.y_thresholds_ = X, y

         # Build the interpolation function
         self._build_f(X, y)
         return self

     def transform(self, T):
-        """Transform new data by linear interpolation
+        """Transform new data by linear interpolation.

         Parameters
         ----------
-        T : array-like, shape=(n_samples,)
+        T : array-like of shape (n_samples,) or (n_samples, 1)
             Data to transform.
+
+            .. versionchanged:: 0.24
+               Also accepts 2d array with 1 feature.

         Returns
         -------
-        T_ : array, shape=(n_samples,)
-            The transformed data
+        y_pred : ndarray of shape (n_samples,)
+            The transformed data.
         """

-        if hasattr(self, '_necessary_X_'):
-            dtype = self._necessary_X_.dtype
+        if hasattr(self, "X_thresholds_"):
+            dtype = self.X_thresholds_.dtype
         else:
             dtype = np.float64

         T = check_array(T, dtype=dtype, ensure_2d=False)

-        if len(T.shape) != 1:
-            raise ValueError("Isotonic regression input should be a 1d array")
+        self._check_input_data_shape(T)
+        T = T.reshape(-1)  # use 1d view

         # Handle the out_of_bounds argument by clipping if needed
         if self.out_of_bounds not in ["raise", "nan", "clip"]:
-            raise ValueError("The argument ``out_of_bounds`` must be in "
-                             "'nan', 'clip', 'raise'; got {0}"
-                             .format(self.out_of_bounds))
+            raise ValueError(
+                "The argument ``out_of_bounds`` must be in "
+                "'nan', 'clip', 'raise'; got {0}".format(self.out_of_bounds)
+            )

         if self.out_of_bounds == "clip":
             T = np.clip(T, self.X_min_, self.X_max_)
@@ -389,21 +406,41 @@

         Parameters
         ----------
-        T : array-like, shape=(n_samples,)
+        T : array-like of shape (n_samples,) or (n_samples, 1)
             Data to transform.

         Returns
         -------
-        T_ : array, shape=(n_samples,)
+        y_pred : ndarray of shape (n_samples,)
             Transformed data.
         """
         return self.transform(T)

+    # We implement get_feature_names_out here instead of using
+    # `_ClassNamePrefixFeaturesOutMixin`` because `input_features` are ignored.
+    # `input_features` are ignored because `IsotonicRegression` accepts 1d
+    # arrays and the semantics of `feature_names_in_` are not clear for 1d arrays.
+    def get_feature_names_out(self, input_features=None):
+        """Get output feature names for transformation.
+
+        Parameters
+        ----------
+        input_features : array-like of str or None, default=None
+            Ignored.
+
+        Returns
+        -------
+        feature_names_out : ndarray of str objects
+            An ndarray with one string i.e. ["isotonicregression0"].
+        """
+        class_name = self.__class__.__name__.lower()
+        return np.asarray([f"{class_name}0"], dtype=object)
+
     def __getstate__(self):
-        """Pickle-protocol - return state of the estimator. """
+        """Pickle-protocol - return state of the estimator."""
         state = super().__getstate__()
         # remove interpolation method
-        state.pop('f_', None)
+        state.pop("f_", None)
         return state

     def __setstate__(self, state):
@@ -412,8 +449,8 @@
         We need to rebuild the interpolation function.
         """
         super().__setstate__(state)
-        if hasattr(self, '_necessary_X_') and hasattr(self, '_necessary_y_'):
-            self._build_f(self._necessary_X_, self._necessary_y_)
+        if hasattr(self, "X_thresholds_") and hasattr(self, "y_thresholds_"):
+            self._build_f(self.X_thresholds_, self.y_thresholds_)

     def _more_tags(self):
-        return {'X_types': ['1darray']}
+        return {"X_types": ["1darray"]}
('sklearn', 'multioutput.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -16,39 +16,44 @@

 import numpy as np
 import scipy.sparse as sp
+from joblib import Parallel
+
 from abc import ABCMeta, abstractmethod
 from .base import BaseEstimator, clone, MetaEstimatorMixin
 from .base import RegressorMixin, ClassifierMixin, is_classifier
 from .model_selection import cross_val_predict
-from .utils import check_array, check_X_y, check_random_state
-from .utils.fixes import parallel_helper
-from .utils.metaestimators import if_delegate_has_method
-from .utils.validation import check_is_fitted, has_fit_parameter
+from .utils.metaestimators import available_if
+from .utils import check_random_state
+from .utils.validation import check_is_fitted, has_fit_parameter, _check_fit_params
 from .utils.multiclass import check_classification_targets
-from .utils._joblib import Parallel, delayed
-
-__all__ = ["MultiOutputRegressor", "MultiOutputClassifier",
-           "ClassifierChain", "RegressorChain"]
-
-
-def _fit_estimator(estimator, X, y, sample_weight=None):
+from .utils.fixes import delayed
+
+__all__ = [
+    "MultiOutputRegressor",
+    "MultiOutputClassifier",
+    "ClassifierChain",
+    "RegressorChain",
+]
+
+
+def _fit_estimator(estimator, X, y, sample_weight=None, **fit_params):
     estimator = clone(estimator)
     if sample_weight is not None:
-        estimator.fit(X, y, sample_weight=sample_weight)
+        estimator.fit(X, y, sample_weight=sample_weight, **fit_params)
     else:
-        estimator.fit(X, y)
+        estimator.fit(X, y, **fit_params)
     return estimator


-def _partial_fit_estimator(estimator, X, y, classes=None, sample_weight=None,
-                           first_time=True):
+def _partial_fit_estimator(
+    estimator, X, y, classes=None, sample_weight=None, first_time=True
+):
     if first_time:
         estimator = clone(estimator)

     if sample_weight is not None:
         if classes is not None:
-            estimator.partial_fit(X, y, classes=classes,
-                                  sample_weight=sample_weight)
+            estimator.partial_fit(X, y, classes=classes, sample_weight=sample_weight)
         else:
             estimator.partial_fit(X, y, sample_weight=sample_weight)
     else:
@@ -59,382 +64,524 @@
     return estimator


-class MultiOutputEstimator(BaseEstimator, MetaEstimatorMixin,
-                           metaclass=ABCMeta):
+def _available_if_estimator_has(attr):
+    """Return a function to check if `estimator` or `estimators_` has `attr`.
+
+    Helper for Chain implementations.
+    """
+
+    def _check(self):
+        return hasattr(self.estimator, attr) or all(
+            hasattr(est, attr) for est in self.estimators_
+        )
+
+    return available_if(_check)
+
+
+class _MultiOutputEstimator(MetaEstimatorMixin, BaseEstimator, metaclass=ABCMeta):
     @abstractmethod
-    def __init__(self, estimator, n_jobs=None):
+    def __init__(self, estimator, *, n_jobs=None):
         self.estimator = estimator
         self.n_jobs = n_jobs

-    @if_delegate_has_method('estimator')
+    @_available_if_estimator_has("partial_fit")
     def partial_fit(self, X, y, classes=None, sample_weight=None):
-        """Incrementally fit the model to data.
-        Fit a separate model for each output variable.
-
-        Parameters
-        ----------
-        X : (sparse) array-like, shape (n_samples, n_features)
-            Data.
-
-        y : (sparse) array-like, shape (n_samples, n_outputs)
+        """Incrementally fit a separate model for each class output.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            The input data.
+
+        y : {array-like, sparse matrix} of shape (n_samples, n_outputs)
             Multi-output targets.

-        classes : list of numpy arrays, shape (n_outputs)
-            Each array is unique classes for one output in str/int
-            Can be obtained by via
-            ``[np.unique(y[:, i]) for i in range(y.shape[1])]``, where y is the
-            target matrix of the entire dataset.
+        classes : list of ndarray of shape (n_outputs,), default=None
+            Each array is unique classes for one output in str/int.
+            Can be obtained via
+            ``[np.unique(y[:, i]) for i in range(y.shape[1])]``, where `y`
+            is the target matrix of the entire dataset.
             This argument is required for the first call to partial_fit
             and can be omitted in the subsequent calls.
-            Note that y doesn't need to contain all labels in `classes`.
-
-        sample_weight : array-like, shape = (n_samples) or None
-            Sample weights. If None, then samples are equally weighted.
+            Note that `y` doesn't need to contain all labels in `classes`.
+
+        sample_weight : array-like of shape (n_samples,), default=None
+            Sample weights. If `None`, then samples are equally weighted.
             Only supported if the underlying regressor supports sample
             weights.

         Returns
         -------
         self : object
-        """
-        X, y = check_X_y(X, y,
-                         multi_output=True,
-                         accept_sparse=True)
+            Returns a fitted instance.
+        """
+        first_time = not hasattr(self, "estimators_")
+        y = self._validate_data(X="no_validation", y=y, multi_output=True)

         if y.ndim == 1:
-            raise ValueError("y must have at least two dimensions for "
-                             "multi-output regression but has only one.")
-
-        if (sample_weight is not None and
-                not has_fit_parameter(self.estimator, 'sample_weight')):
-            raise ValueError("Underlying estimator does not support"
-                             " sample weights.")
-
-        first_time = not hasattr(self, 'estimators_')
+            raise ValueError(
+                "y must have at least two dimensions for "
+                "multi-output regression but has only one."
+            )
+
+        if sample_weight is not None and not has_fit_parameter(
+            self.estimator, "sample_weight"
+        ):
+            raise ValueError("Underlying estimator does not support sample weights.")
+
+        first_time = not hasattr(self, "estimators_")

         self.estimators_ = Parallel(n_jobs=self.n_jobs)(
             delayed(_partial_fit_estimator)(
                 self.estimators_[i] if not first_time else self.estimator,
-                X, y[:, i],
+                X,
+                y[:, i],
                 classes[i] if classes is not None else None,
-                sample_weight, first_time) for i in range(y.shape[1]))
+                sample_weight,
+                first_time,
+            )
+            for i in range(y.shape[1])
+        )
+
+        if first_time and hasattr(self.estimators_[0], "n_features_in_"):
+            self.n_features_in_ = self.estimators_[0].n_features_in_
+        if first_time and hasattr(self.estimators_[0], "feature_names_in_"):
+            self.feature_names_in_ = self.estimators_[0].feature_names_in_
+
         return self

-    def fit(self, X, y, sample_weight=None):
-        """ Fit the model to data.
-        Fit a separate model for each output variable.
-
-        Parameters
-        ----------
-        X : (sparse) array-like, shape (n_samples, n_features)
-            Data.
-
-        y : (sparse) array-like, shape (n_samples, n_outputs)
+    def fit(self, X, y, sample_weight=None, **fit_params):
+        """Fit the model to data, separately for each output variable.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            The input data.
+
+        y : {array-like, sparse matrix} of shape (n_samples, n_outputs)
             Multi-output targets. An indicator matrix turns on multilabel
             estimation.

-        sample_weight : array-like, shape = (n_samples) or None
-            Sample weights. If None, then samples are equally weighted.
+        sample_weight : array-like of shape (n_samples,), default=None
+            Sample weights. If `None`, then samples are equally weighted.
             Only supported if the underlying regressor supports sample
             weights.

+        **fit_params : dict of string -> object
+            Parameters passed to the ``estimator.fit`` method of each step.
+
+            .. versionadded:: 0.23
+
         Returns
         -------
         self : object
+            Returns a fitted instance.
         """

         if not hasattr(self.estimator, "fit"):
-            raise ValueError("The base estimator should implement"
-                             " a fit method")
-
-        X, y = check_X_y(X, y,
-                         multi_output=True,
-                         accept_sparse=True)
+            raise ValueError("The base estimator should implement a fit method")
+
+        y = self._validate_data(X="no_validation", y=y, multi_output=True)

         if is_classifier(self):
             check_classification_targets(y)

         if y.ndim == 1:
-            raise ValueError("y must have at least two dimensions for "
-                             "multi-output regression but has only one.")
-
-        if (sample_weight is not None and
-                not has_fit_parameter(self.estimator, 'sample_weight')):
-            raise ValueError("Underlying estimator does not support"
-                             " sample weights.")
+            raise ValueError(
+                "y must have at least two dimensions for "
+                "multi-output regression but has only one."
+            )
+
+        if sample_weight is not None and not has_fit_parameter(
+            self.estimator, "sample_weight"
+        ):
+            raise ValueError("Underlying estimator does not support sample weights.")
+
+        fit_params_validated = _check_fit_params(X, fit_params)

         self.estimators_ = Parallel(n_jobs=self.n_jobs)(
             delayed(_fit_estimator)(
-                self.estimator, X, y[:, i], sample_weight)
-            for i in range(y.shape[1]))
+                self.estimator, X, y[:, i], sample_weight, **fit_params_validated
+            )
+            for i in range(y.shape[1])
+        )
+
+        if hasattr(self.estimators_[0], "n_features_in_"):
+            self.n_features_in_ = self.estimators_[0].n_features_in_
+        if hasattr(self.estimators_[0], "feature_names_in_"):
+            self.feature_names_in_ = self.estimators_[0].feature_names_in_
+
         return self

     def predict(self, X):
-        """Predict multi-output variable using a model
-         trained for each target variable.
-
-        Parameters
-        ----------
-        X : (sparse) array-like, shape (n_samples, n_features)
-            Data.
-
-        Returns
-        -------
-        y : (sparse) array-like, shape (n_samples, n_outputs)
+        """Predict multi-output variable using model for each target variable.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            The input data.
+
+        Returns
+        -------
+        y : {array-like, sparse matrix} of shape (n_samples, n_outputs)
             Multi-output targets predicted across multiple predictors.
             Note: Separate models are generated for each predictor.
         """
-        check_is_fitted(self, 'estimators_')
-        if not hasattr(self.estimator, "predict"):
-            raise ValueError("The base estimator should implement"
-                             " a predict method")
-
-        X = check_array(X, accept_sparse=True)
+        check_is_fitted(self)
+        if not hasattr(self.estimators_[0], "predict"):
+            raise ValueError("The base estimator should implement a predict method")

         y = Parallel(n_jobs=self.n_jobs)(
-            delayed(parallel_helper)(e, 'predict', X)
-            for e in self.estimators_)
+            delayed(e.predict)(X) for e in self.estimators_
+        )

         return np.asarray(y).T

     def _more_tags(self):
-        return {'multioutput_only': True}
-
-
-class MultiOutputRegressor(MultiOutputEstimator, RegressorMixin):
-    """Multi target regression
+        return {"multioutput_only": True}
+
+
+class MultiOutputRegressor(RegressorMixin, _MultiOutputEstimator):
+    """Multi target regression.

     This strategy consists of fitting one regressor per target. This is a
     simple strategy for extending regressors that do not natively support
     multi-target regression.

+    .. versionadded:: 0.18
+
     Parameters
     ----------
     estimator : estimator object
-        An estimator object implementing `fit` and `predict`.
+        An estimator object implementing :term:`fit` and :term:`predict`.

     n_jobs : int or None, optional (default=None)
-        The number of jobs to run in parallel for `fit`.
-        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
-        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
-        for more details.
-
-        When individual estimators are fast to train or predict
-        using `n_jobs>1` can result in slower performance due
-        to the overhead of spawning processes.
+        The number of jobs to run in parallel.
+        :meth:`fit`, :meth:`predict` and :meth:`partial_fit` (if supported
+        by the passed estimator) will be parallelized for each target.
+
+        When individual estimators are fast to train or predict,
+        using ``n_jobs > 1`` can result in slower performance due
+        to the parallelism overhead.
+
+        ``None`` means `1` unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all available processes / threads.
+        See :term:`Glossary <n_jobs>` for more details.
+
+        .. versionchanged:: 0.20
+            `n_jobs` default changed from `1` to `None`.

     Attributes
     ----------
     estimators_ : list of ``n_output`` estimators
         Estimators used for predictions.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`. Only defined if the
+        underlying `estimator` exposes such an attribute when fit.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Only defined if the
+        underlying estimators expose such an attribute when fit.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    RegressorChain : A multi-label model that arranges regressions into a
+        chain.
+    MultiOutputClassifier : Classifies each output independently rather than
+        chaining.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.datasets import load_linnerud
+    >>> from sklearn.multioutput import MultiOutputRegressor
+    >>> from sklearn.linear_model import Ridge
+    >>> X, y = load_linnerud(return_X_y=True)
+    >>> regr = MultiOutputRegressor(Ridge(random_state=123)).fit(X, y)
+    >>> regr.predict(X[[0]])
+    array([[176..., 35..., 57...]])
     """

-    def __init__(self, estimator, n_jobs=None):
-        super().__init__(estimator, n_jobs)
-
-    @if_delegate_has_method('estimator')
+    def __init__(self, estimator, *, n_jobs=None):
+        super().__init__(estimator, n_jobs=n_jobs)
+
+    @_available_if_estimator_has("partial_fit")
     def partial_fit(self, X, y, sample_weight=None):
-        """Incrementally fit the model to data.
-        Fit a separate model for each output variable.
-
-        Parameters
-        ----------
-        X : (sparse) array-like, shape (n_samples, n_features)
-            Data.
-
-        y : (sparse) array-like, shape (n_samples, n_outputs)
+        """Incrementally fit the model to data, for each output variable.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            The input data.
+
+        y : {array-like, sparse matrix} of shape (n_samples, n_outputs)
             Multi-output targets.

-        sample_weight : array-like, shape = (n_samples) or None
-            Sample weights. If None, then samples are equally weighted.
+        sample_weight : array-like of shape (n_samples,), default=None
+            Sample weights. If `None`, then samples are equally weighted.
             Only supported if the underlying regressor supports sample
             weights.

         Returns
         -------
         self : object
-        """
-        super().partial_fit(
-            X, y, sample_weight=sample_weight)
-
-    # XXX Remove this method in 0.23
-    def score(self, X, y, sample_weight=None):
-        """Returns the coefficient of determination R^2 of the prediction.
-
-        The coefficient R^2 is defined as (1 - u/v), where u is the residual
-        sum of squares ((y_true - y_pred) ** 2).sum() and v is the regression
-        sum of squares ((y_true - y_true.mean()) ** 2).sum().
-        Best possible score is 1.0 and it can be negative (because the
-        model can be arbitrarily worse). A constant model that always
-        predicts the expected value of y, disregarding the input features,
-        would get a R^2 score of 0.0.
-
-        Notes
-        -----
-        R^2 is calculated by weighting all the targets equally using
-        `multioutput='uniform_average'`.
-
-        Parameters
-        ----------
-        X : array-like, shape (n_samples, n_features)
-            Test samples.
-
-        y : array-like, shape (n_samples) or (n_samples, n_outputs)
-            True values for X.
-
-        sample_weight : array-like, shape [n_samples], optional
-            Sample weights.
-
-        Returns
-        -------
-        score : float
-            R^2 of self.predict(X) wrt. y.
-        """
-        # XXX remove in 0.19 when r2_score default for multioutput changes
-        from .metrics import r2_score
-        return r2_score(y, self.predict(X), sample_weight=sample_weight,
-                        multioutput='uniform_average')
-
-
-class MultiOutputClassifier(MultiOutputEstimator, ClassifierMixin):
-    """Multi target classification
+            Returns a fitted instance.
+        """
+        super().partial_fit(X, y, sample_weight=sample_weight)
+
+
+class MultiOutputClassifier(ClassifierMixin, _MultiOutputEstimator):
+    """Multi target classification.

     This strategy consists of fitting one classifier per target. This is a
     simple strategy for extending classifiers that do not natively support
-    multi-target classification
+    multi-target classification.

     Parameters
     ----------
     estimator : estimator object
-        An estimator object implementing `fit`, `score` and `predict_proba`.
+        An estimator object implementing :term:`fit`, :term:`score` and
+        :term:`predict_proba`.

     n_jobs : int or None, optional (default=None)
-        The number of jobs to use for the computation.
-        It does each target variable in y in parallel.
-        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
-        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
-        for more details.
+        The number of jobs to run in parallel.
+        :meth:`fit`, :meth:`predict` and :meth:`partial_fit` (if supported
+        by the passed estimator) will be parallelized for each target.
+
+        When individual estimators are fast to train or predict,
+        using ``n_jobs > 1`` can result in slower performance due
+        to the parallelism overhead.
+
+        ``None`` means `1` unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all available processes / threads.
+        See :term:`Glossary <n_jobs>` for more details.
+
+        .. versionchanged:: 0.20
+            `n_jobs` default changed from `1` to `None`.

     Attributes
     ----------
+    classes_ : ndarray of shape (n_classes,)
+        Class labels.
+
     estimators_ : list of ``n_output`` estimators
         Estimators used for predictions.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`. Only defined if the
+        underlying `estimator` exposes such an attribute when fit.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Only defined if the
+        underlying estimators expose such an attribute when fit.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    ClassifierChain : A multi-label model that arranges binary classifiers
+        into a chain.
+    MultiOutputRegressor : Fits one regressor per target variable.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.datasets import make_multilabel_classification
+    >>> from sklearn.multioutput import MultiOutputClassifier
+    >>> from sklearn.linear_model import LogisticRegression
+    >>> X, y = make_multilabel_classification(n_classes=3, random_state=0)
+    >>> clf = MultiOutputClassifier(LogisticRegression()).fit(X, y)
+    >>> clf.predict(X[-2:])
+    array([[1, 1, 1],
+           [1, 0, 1]])
     """

-    def __init__(self, estimator, n_jobs=None):
-        super().__init__(estimator, n_jobs)
-
+    def __init__(self, estimator, *, n_jobs=None):
+        super().__init__(estimator, n_jobs=n_jobs)
+
+    def fit(self, X, Y, sample_weight=None, **fit_params):
+        """Fit the model to data matrix X and targets Y.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            The input data.
+
+        Y : array-like of shape (n_samples, n_classes)
+            The target values.
+
+        sample_weight : array-like of shape (n_samples,), default=None
+            Sample weights. If `None`, then samples are equally weighted.
+            Only supported if the underlying classifier supports sample
+            weights.
+
+        **fit_params : dict of string -> object
+            Parameters passed to the ``estimator.fit`` method of each step.
+
+            .. versionadded:: 0.23
+
+        Returns
+        -------
+        self : object
+            Returns a fitted instance.
+        """
+        super().fit(X, Y, sample_weight, **fit_params)
+        self.classes_ = [estimator.classes_ for estimator in self.estimators_]
+        return self
+
+    def _check_predict_proba(self):
+        if hasattr(self, "estimators_"):
+            # raise an AttributeError if `predict_proba` does not exist for
+            # each estimator
+            [getattr(est, "predict_proba") for est in self.estimators_]
+            return True
+        # raise an AttributeError if `predict_proba` does not exist for the
+        # unfitted estimator
+        getattr(self.estimator, "predict_proba")
+        return True
+
+    @available_if(_check_predict_proba)
     def predict_proba(self, X):
-        """Probability estimates.
-        Returns prediction probabilities for each class of each output.
+        """Return prediction probabilities for each class of each output.

         This method will raise a ``ValueError`` if any of the
         estimators do not have ``predict_proba``.

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Data
-
-        Returns
-        -------
-        p : array of shape = [n_samples, n_classes], or a list of n_outputs \
-            such arrays if n_outputs > 1.
+        X : array-like of shape (n_samples, n_features)
+            The input data.
+
+        Returns
+        -------
+        p : array of shape (n_samples, n_classes), or a list of n_outputs \
+                such arrays if n_outputs > 1.
             The class probabilities of the input samples. The order of the
-            classes corresponds to that in the attribute `classes_`.
-        """
-        check_is_fitted(self, 'estimators_')
-        if not all([hasattr(estimator, "predict_proba")
-                    for estimator in self.estimators_]):
-            raise ValueError("The base estimator should implement "
-                             "predict_proba method")
-
-        results = [estimator.predict_proba(X) for estimator in
-                   self.estimators_]
+            classes corresponds to that in the attribute :term:`classes_`.
+
+            .. versionchanged:: 0.19
+                This function now returns a list of arrays where the length of
+                the list is ``n_outputs``, and each array is (``n_samples``,
+                ``n_classes``) for that particular output.
+        """
+        check_is_fitted(self)
+        results = [estimator.predict_proba(X) for estimator in self.estimators_]
         return results

     def score(self, X, y):
-        """Returns the mean accuracy on the given test data and labels.
-
-        Parameters
-        ----------
-        X : array-like, shape [n_samples, n_features]
-            Test samples
-
-        y : array-like, shape [n_samples, n_outputs]
-            True values for X
+        """Return the mean accuracy on the given test data and labels.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            Test samples.
+
+        y : array-like of shape (n_samples, n_outputs)
+            True values for X.

         Returns
         -------
         scores : float
-            accuracy_score of self.predict(X) versus y
-        """
-        check_is_fitted(self, 'estimators_')
+            Mean accuracy of predicted target versus true target.
+        """
+        check_is_fitted(self)
         n_outputs_ = len(self.estimators_)
         if y.ndim == 1:
-            raise ValueError("y must have at least two dimensions for "
-                             "multi target classification but has only one")
+            raise ValueError(
+                "y must have at least two dimensions for "
+                "multi target classification but has only one"
+            )
         if y.shape[1] != n_outputs_:
-            raise ValueError("The number of outputs of Y for fit {0} and"
-                             " score {1} should be same".
-                             format(n_outputs_, y.shape[1]))
+            raise ValueError(
+                "The number of outputs of Y for fit {0} and"
+                " score {1} should be same".format(n_outputs_, y.shape[1])
+            )
         y_pred = self.predict(X)
         return np.mean(np.all(y == y_pred, axis=1))

     def _more_tags(self):
         # FIXME
-        return {'_skip_test': True}
+        return {"_skip_test": True}
+
+
+def _available_if_base_estimator_has(attr):
+    """Return a function to check if `base_estimator` or `estimators_` has `attr`.
+
+    Helper for Chain implementations.
+    """
+
+    def _check(self):
+        return hasattr(self.base_estimator, attr) or all(
+            hasattr(est, attr) for est in self.estimators_
+        )
+
+    return available_if(_check)


 class _BaseChain(BaseEstimator, metaclass=ABCMeta):
-    def __init__(self, base_estimator, order=None, cv=None, random_state=None):
+    def __init__(self, base_estimator, *, order=None, cv=None, random_state=None):
         self.base_estimator = base_estimator
         self.order = order
         self.cv = cv
         self.random_state = random_state

     @abstractmethod
-    def fit(self, X, Y):
+    def fit(self, X, Y, **fit_params):
         """Fit the model to data matrix X and targets Y.

         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            The input data.
-        Y : array-like, shape (n_samples, n_classes)
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            The input data.
+
+        Y : array-like of shape (n_samples, n_classes)
             The target values.

+        **fit_params : dict of string -> object
+            Parameters passed to the `fit` method of each step.
+
+            .. versionadded:: 0.23
+
         Returns
         -------
         self : object
-        """
-        X, Y = check_X_y(X, Y, multi_output=True, accept_sparse=True)
+            Returns a fitted instance.
+        """
+        X, Y = self._validate_data(X, Y, multi_output=True, accept_sparse=True)

         random_state = check_random_state(self.random_state)
-        check_array(X, accept_sparse=True)
         self.order_ = self.order
+        if isinstance(self.order_, tuple):
+            self.order_ = np.array(self.order_)
+
         if self.order_ is None:
             self.order_ = np.array(range(Y.shape[1]))
         elif isinstance(self.order_, str):
-            if self.order_ == 'random':
+            if self.order_ == "random":
                 self.order_ = random_state.permutation(Y.shape[1])
         elif sorted(self.order_) != list(range(Y.shape[1])):
-                raise ValueError("invalid order")
-
-        self.estimators_ = [clone(self.base_estimator)
-                            for _ in range(Y.shape[1])]
+            raise ValueError("invalid order")
+
+        self.estimators_ = [clone(self.base_estimator) for _ in range(Y.shape[1])]

         if self.cv is None:
             Y_pred_chain = Y[:, self.order_]
             if sp.issparse(X):
-                X_aug = sp.hstack((X, Y_pred_chain), format='lil')
+                X_aug = sp.hstack((X, Y_pred_chain), format="lil")
                 X_aug = X_aug.tocsr()
             else:
                 X_aug = np.hstack((X, Y_pred_chain))

         elif sp.issparse(X):
             Y_pred_chain = sp.lil_matrix((X.shape[0], Y.shape[1]))
-            X_aug = sp.hstack((X, Y_pred_chain), format='lil')
+            X_aug = sp.hstack((X, Y_pred_chain), format="lil")

         else:
             Y_pred_chain = np.zeros((X.shape[0], Y.shape[1]))
@@ -444,12 +591,12 @@

         for chain_idx, estimator in enumerate(self.estimators_):
             y = Y[:, self.order_[chain_idx]]
-            estimator.fit(X_aug[:, :(X.shape[1] + chain_idx)], y)
+            estimator.fit(X_aug[:, : (X.shape[1] + chain_idx)], y, **fit_params)
             if self.cv is not None and chain_idx < len(self.estimators_) - 1:
                 col_idx = X.shape[1] + chain_idx
                 cv_result = cross_val_predict(
-                    self.base_estimator, X_aug[:, :col_idx],
-                    y=y, cv=self.cv)
+                    self.base_estimator, X_aug[:, :col_idx], y=y, cv=self.cv
+                )
                 if sp.issparse(X_aug):
                     X_aug[:, col_idx] = np.expand_dims(cv_result, 1)
                 else:
@@ -462,17 +609,16 @@

         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            The input data.
-
-        Returns
-        -------
-        Y_pred : array-like, shape (n_samples, n_classes)
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            The input data.
+
+        Returns
+        -------
+        Y_pred : array-like of shape (n_samples, n_classes)
             The predicted values.
-
-        """
-        check_is_fitted(self, 'estimators_')
-        X = check_array(X, accept_sparse=True)
+        """
+        check_is_fitted(self)
+        X = self._validate_data(X, accept_sparse=True, reset=False)
         Y_pred_chain = np.zeros((X.shape[0], len(self.estimators_)))
         for chain_idx, estimator in enumerate(self.estimators_):
             previous_predictions = Y_pred_chain[:, :chain_idx]
@@ -492,7 +638,7 @@
         return Y_pred


-class ClassifierChain(_BaseChain, ClassifierMixin, MetaEstimatorMixin):
+class ClassifierChain(MetaEstimatorMixin, ClassifierMixin, _BaseChain):
     """A multi-label model that arranges binary classifiers into a chain.

     Each model makes a prediction in the order specified by the chain using
@@ -500,14 +646,16 @@
     of models that are earlier in the chain.

     Read more in the :ref:`User Guide <classifierchain>`.
+
+    .. versionadded:: 0.19

     Parameters
     ----------
     base_estimator : estimator
         The base estimator from which the classifier chain is built.

-    order : array-like, shape=[n_outputs] or 'random', optional
-        By default the order will be determined by the order of columns in
+    order : array-like of shape (n_outputs,) or 'random', default=None
+        If `None`, the order will be determined by the order of columns in
         the label matrix Y.::

             order = [0, 1, 2, ..., Y.shape[1] - 1]
@@ -521,26 +669,26 @@
         column 1 in the Y matrix, the second model will make predictions
         for column 3, etc.

-        If order is 'random' a random ordering will be used.
-
-    cv : int, cross-validation generator or an iterable, optional \
-    (default=None)
+        If order is `random` a random ordering will be used.
+
+    cv : int, cross-validation generator or an iterable, default=None
         Determines whether to use cross validated predictions or true
         labels for the results of previous estimators in the chain.
-        If cv is None the true labels are used when fitting. Otherwise
-        possible inputs for cv are:
-
+        Possible inputs for cv are:
+
+        - None, to use true labels when fitting,
         - integer, to specify the number of folds in a (Stratified)KFold,
         - :term:`CV splitter`,
         - An iterable yielding (train, test) splits as arrays of indices.

     random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-        The random number generator is used to generate random chain orders.
+        If ``order='random'``, determines random number generation for the
+        chain order.
+        In addition, it controls the random seed given at each `base_estimator`
+        at each chaining iteration. Thus, it is only used when `base_estimator`
+        exposes a `random_state`.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.

     Attributes
     ----------
@@ -554,10 +702,22 @@
     order_ : list
         The order of labels in the classifier chain.

-    See also
+    n_features_in_ : int
+        Number of features seen during :term:`fit`. Only defined if the
+        underlying `base_estimator` exposes such an attribute when fit.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
     --------
-    RegressorChain: Equivalent for regression
-    MultioutputClassifier: Classifies each output independently rather than
+    RegressorChain : Equivalent for regression.
+    MultioutputClassifier : Classifies each output independently rather than
         chaining.

     References
@@ -565,6 +725,28 @@
     Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank, "Classifier
     Chains for Multi-label Classification", 2009.

+    Examples
+    --------
+    >>> from sklearn.datasets import make_multilabel_classification
+    >>> from sklearn.linear_model import LogisticRegression
+    >>> from sklearn.model_selection import train_test_split
+    >>> from sklearn.multioutput import ClassifierChain
+    >>> X, Y = make_multilabel_classification(
+    ...    n_samples=12, n_classes=3, random_state=0
+    ... )
+    >>> X_train, X_test, Y_train, Y_test = train_test_split(
+    ...    X, Y, random_state=0
+    ... )
+    >>> base_lr = LogisticRegression(solver='lbfgs', random_state=0)
+    >>> chain = ClassifierChain(base_lr, order='random', random_state=0)
+    >>> chain.fit(X_train, Y_train).predict(X_test)
+    array([[1., 1., 0.],
+           [1., 0., 0.],
+           [0., 1., 0.]])
+    >>> chain.predict_proba(X_test)
+    array([[0.8387..., 0.9431..., 0.4576...],
+           [0.8878..., 0.3684..., 0.2640...],
+           [0.0321..., 0.9935..., 0.0625...]])
     """

     def fit(self, X, Y):
@@ -572,34 +754,38 @@

         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            The input data.
-        Y : array-like, shape (n_samples, n_classes)
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            The input data.
+
+        Y : array-like of shape (n_samples, n_classes)
             The target values.

         Returns
         -------
         self : object
+            Class instance.
         """
         super().fit(X, Y)
-        self.classes_ = [estimator.classes_
-                         for chain_idx, estimator
-                         in enumerate(self.estimators_)]
+        self.classes_ = [
+            estimator.classes_ for chain_idx, estimator in enumerate(self.estimators_)
+        ]
         return self

-    @if_delegate_has_method('base_estimator')
+    @_available_if_base_estimator_has("predict_proba")
     def predict_proba(self, X):
         """Predict probability estimates.

         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
-
-        Returns
-        -------
-        Y_prob : array-like, shape (n_samples, n_classes)
-        """
-        X = check_array(X, accept_sparse=True)
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            The input data.
+
+        Returns
+        -------
+        Y_prob : array-like of shape (n_samples, n_classes)
+            The predicted probabilities.
+        """
+        X = self._validate_data(X, accept_sparse=True, reset=False)
         Y_prob_chain = np.zeros((X.shape[0], len(self.estimators_)))
         Y_pred_chain = np.zeros((X.shape[0], len(self.estimators_)))
         for chain_idx, estimator in enumerate(self.estimators_):
@@ -616,20 +802,22 @@

         return Y_prob

-    @if_delegate_has_method('base_estimator')
+    @_available_if_base_estimator_has("decision_function")
     def decision_function(self, X):
         """Evaluate the decision_function of the models in the chain.

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-
-        Returns
-        -------
-        Y_decision : array-like, shape (n_samples, n_classes )
+        X : array-like of shape (n_samples, n_features)
+            The input data.
+
+        Returns
+        -------
+        Y_decision : array-like of shape (n_samples, n_classes)
             Returns the decision function of the sample for each model
             in the chain.
         """
+        X = self._validate_data(X, accept_sparse=True, reset=False)
         Y_decision_chain = np.zeros((X.shape[0], len(self.estimators_)))
         Y_pred_chain = np.zeros((X.shape[0], len(self.estimators_)))
         for chain_idx, estimator in enumerate(self.estimators_):
@@ -648,11 +836,10 @@
         return Y_decision

     def _more_tags(self):
-        return {'_skip_test': True,
-                'multioutput_only': True}
-
-
-class RegressorChain(_BaseChain, RegressorMixin, MetaEstimatorMixin):
+        return {"_skip_test": True, "multioutput_only": True}
+
+
+class RegressorChain(MetaEstimatorMixin, RegressorMixin, _BaseChain):
     """A multi-label model that arranges regressions into a chain.

     Each model makes a prediction in the order specified by the chain using
@@ -660,14 +847,16 @@
     of models that are earlier in the chain.

     Read more in the :ref:`User Guide <regressorchain>`.
+
+    .. versionadded:: 0.20

     Parameters
     ----------
     base_estimator : estimator
         The base estimator from which the classifier chain is built.

-    order : array-like, shape=[n_outputs] or 'random', optional
-        By default the order will be determined by the order of columns in
+    order : array-like of shape (n_outputs,) or 'random', default=None
+        If `None`, the order will be determined by the order of columns in
         the label matrix Y.::

             order = [0, 1, 2, ..., Y.shape[1] - 1]
@@ -683,24 +872,24 @@

         If order is 'random' a random ordering will be used.

-    cv : int, cross-validation generator or an iterable, optional \
-    (default=None)
+    cv : int, cross-validation generator or an iterable, default=None
         Determines whether to use cross validated predictions or true
         labels for the results of previous estimators in the chain.
-        If cv is None the true labels are used when fitting. Otherwise
-        possible inputs for cv are:
-
+        Possible inputs for cv are:
+
+        - None, to use true labels when fitting,
         - integer, to specify the number of folds in a (Stratified)KFold,
         - :term:`CV splitter`,
         - An iterable yielding (train, test) splits as arrays of indices.

     random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-        The random number generator is used to generate random chain orders.
+        If ``order='random'``, determines random number generation for the
+        chain order.
+        In addition, it controls the random seed given at each `base_estimator`
+        at each chaining iteration. Thus, it is only used when `base_estimator`
+        exposes a `random_state`.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.

     Attributes
     ----------
@@ -710,29 +899,61 @@
     order_ : list
         The order of labels in the classifier chain.

-    See also
+    n_features_in_ : int
+        Number of features seen during :term:`fit`. Only defined if the
+        underlying `base_estimator` exposes such an attribute when fit.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
     --------
-    ClassifierChain: Equivalent for classification
-    MultioutputRegressor: Learns each output independently rather than
+    ClassifierChain : Equivalent for classification.
+    MultiOutputRegressor : Learns each output independently rather than
         chaining.

+    Examples
+    --------
+    >>> from sklearn.multioutput import RegressorChain
+    >>> from sklearn.linear_model import LogisticRegression
+    >>> logreg = LogisticRegression(solver='lbfgs',multi_class='multinomial')
+    >>> X, Y = [[1, 0], [0, 1], [1, 1]], [[0, 2], [1, 1], [2, 0]]
+    >>> chain = RegressorChain(base_estimator=logreg, order=[0, 1]).fit(X, Y)
+    >>> chain.predict(X)
+    array([[0., 2.],
+           [1., 1.],
+           [2., 0.]])
     """
-    def fit(self, X, Y):
+
+    def fit(self, X, Y, **fit_params):
         """Fit the model to data matrix X and targets Y.

         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            The input data.
-        Y : array-like, shape (n_samples, n_classes)
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            The input data.
+
+        Y : array-like of shape (n_samples, n_classes)
             The target values.

+        **fit_params : dict of string -> object
+            Parameters passed to the `fit` method at each step
+            of the regressor chain.
+
+            .. versionadded:: 0.23
+
         Returns
         -------
         self : object
-        """
-        super().fit(X, Y)
+            Returns a fitted instance.
+        """
+        super().fit(X, Y, **fit_params)
         return self

     def _more_tags(self):
-        return {'multioutput_only': True}
+        return {"multioutput_only": True}
('sklearn', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -13,39 +13,33 @@
 See http://scikit-learn.org for complete documentation.
 """
 import sys
-import re
-import warnings
 import logging
 import os
+import random
+

 from ._config import get_config, set_config, config_context

 logger = logging.getLogger(__name__)
-logger.addHandler(logging.StreamHandler())
-logger.setLevel(logging.INFO)

-
-# Make sure that DeprecationWarning within this package always gets printed
-warnings.filterwarnings('always', category=DeprecationWarning,
-                        module=r'^{0}\.'.format(re.escape(__name__)))

 # PEP0440 compatible formatted version, see:
 # https://www.python.org/dev/peps/pep-0440/
 #
 # Generic release markers:
-#   X.Y
+#   X.Y.0   # For first release after an increment in Y
 #   X.Y.Z   # For bugfix releases
 #
 # Admissible pre-release markers:
-#   X.YaN   # Alpha release
-#   X.YbN   # Beta release
-#   X.YrcN  # Release Candidate
-#   X.Y     # Final release
+#   X.Y.ZaN   # Alpha release
+#   X.Y.ZbN   # Beta release
+#   X.Y.ZrcN  # Release Candidate
+#   X.Y.Z     # Final release
 #
 # Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
 # 'X.Y.dev0' is the canonical version of 'X.Y.dev'
 #
-__version__ = '0.21.0'
+__version__ = "1.1.0"


 # On OSX, we can get a runtime error due to multiple OpenMP libraries loaded
@@ -58,51 +52,92 @@
 # the outer OpenMP parallel section.
 os.environ.setdefault("KMP_DUPLICATE_LIB_OK", "True")

+# Workaround issue discovered in intel-openmp 2019.5:
+# https://github.com/ContinuumIO/anaconda-issues/issues/11294
+os.environ.setdefault("KMP_INIT_AT_FORK", "FALSE")

 try:
     # This variable is injected in the __builtins__ by the build
     # process. It is used to enable importing subpackages of sklearn when
     # the binaries are not built
-    __SKLEARN_SETUP__
+    # mypy error: Cannot determine type of '__SKLEARN_SETUP__'
+    __SKLEARN_SETUP__  # type: ignore
 except NameError:
     __SKLEARN_SETUP__ = False

 if __SKLEARN_SETUP__:
-    sys.stderr.write('Partial import of sklearn during the build process.\n')
+    sys.stderr.write("Partial import of sklearn during the build process.\n")
     # We are not importing the rest of scikit-learn during the build
     # process, as it may not be compiled yet
 else:
-    from . import __check_build
+    # `_distributor_init` allows distributors to run custom init code.
+    # For instance, for the Windows wheel, this is used to pre-load the
+    # vcomp shared library runtime for OpenMP embedded in the sklearn/.libs
+    # sub-folder.
+    # It is necessary to do this prior to importing show_versions as the
+    # later is linked to the OpenMP runtime to make it possible to introspect
+    # it and importing it first would fail if the OpenMP dll cannot be found.
+    from . import _distributor_init  # noqa: F401
+    from . import __check_build  # noqa: F401
     from .base import clone
     from .utils._show_versions import show_versions

-    __check_build  # avoid flakes unused variable error
-
-    __all__ = ['calibration', 'cluster', 'covariance', 'cross_decomposition',
-               'datasets', 'decomposition', 'dummy', 'ensemble', 'exceptions',
-               'experimental', 'externals', 'feature_extraction',
-               'feature_selection', 'gaussian_process', 'inspection',
-               'isotonic', 'kernel_approximation', 'kernel_ridge',
-               'linear_model', 'manifold', 'metrics', 'mixture',
-               'model_selection', 'multiclass', 'multioutput',
-               'naive_bayes', 'neighbors', 'neural_network', 'pipeline',
-               'preprocessing', 'random_projection', 'semi_supervised',
-               'svm', 'tree', 'discriminant_analysis', 'impute', 'compose',
-               # Non-modules:
-               'clone', 'get_config', 'set_config', 'config_context',
-               'show_versions']
+    __all__ = [
+        "calibration",
+        "cluster",
+        "covariance",
+        "cross_decomposition",
+        "datasets",
+        "decomposition",
+        "dummy",
+        "ensemble",
+        "exceptions",
+        "experimental",
+        "externals",
+        "feature_extraction",
+        "feature_selection",
+        "gaussian_process",
+        "inspection",
+        "isotonic",
+        "kernel_approximation",
+        "kernel_ridge",
+        "linear_model",
+        "manifold",
+        "metrics",
+        "mixture",
+        "model_selection",
+        "multiclass",
+        "multioutput",
+        "naive_bayes",
+        "neighbors",
+        "neural_network",
+        "pipeline",
+        "preprocessing",
+        "random_projection",
+        "semi_supervised",
+        "svm",
+        "tree",
+        "discriminant_analysis",
+        "impute",
+        "compose",
+        # Non-modules:
+        "clone",
+        "get_config",
+        "set_config",
+        "config_context",
+        "show_versions",
+    ]


 def setup_module(module):
     """Fixture for the tests to assure globally controllable seeding of RNGs"""
-    import os
+
     import numpy as np
-    import random

     # Check if a random seed exists in the environment, if not create one.
-    _random_seed = os.environ.get('SKLEARN_SEED', None)
+    _random_seed = os.environ.get("SKLEARN_SEED", None)
     if _random_seed is None:
-        _random_seed = np.random.uniform() * (2 ** 31 - 1)
+        _random_seed = np.random.uniform() * np.iinfo(np.int32).max
     _random_seed = int(_random_seed)
     print("I: Seeding RNGs with %r" % _random_seed)
     np.random.seed(_random_seed)
('sklearn', 'kernel_ridge.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,12 +8,11 @@

 from .base import BaseEstimator, RegressorMixin, MultiOutputMixin
 from .metrics.pairwise import pairwise_kernels
-from .linear_model.ridge import _solve_cholesky_kernel
-from .utils import check_array, check_X_y
-from .utils.validation import check_is_fitted
-
-
-class KernelRidge(BaseEstimator, RegressorMixin, MultiOutputMixin):
+from .linear_model._ridge import _solve_cholesky_kernel
+from .utils.validation import check_is_fitted, _check_sample_weight
+
+
+class KernelRidge(MultiOutputMixin, RegressorMixin, BaseEstimator):
     """Kernel ridge regression.

     Kernel ridge regression (KRR) combines ridge regression (linear least
@@ -38,19 +37,29 @@

     Parameters
     ----------
-    alpha : {float, array-like}, shape = [n_targets]
-        Small positive values of alpha improve the conditioning of the problem
-        and reduce the variance of the estimates.  Alpha corresponds to
-        ``(2*C)^-1`` in other linear models such as LogisticRegression or
-        LinearSVC. If an array is passed, penalties are assumed to be specific
-        to the targets. Hence they must correspond in number.
-
-    kernel : string or callable, default="linear"
-        Kernel mapping used internally. A callable should accept two arguments
-        and the keyword arguments passed to this object as kernel_params, and
-        should return a floating point number. Set to "precomputed" in
-        order to pass a precomputed kernel matrix to the estimator
-        methods instead of samples.
+    alpha : float or array-like of shape (n_targets,), default=1.0
+        Regularization strength; must be a positive float. Regularization
+        improves the conditioning of the problem and reduces the variance of
+        the estimates. Larger values specify stronger regularization.
+        Alpha corresponds to ``1 / (2C)`` in other linear models such as
+        :class:`~sklearn.linear_model.LogisticRegression` or
+        :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are
+        assumed to be specific to the targets. Hence they must correspond in
+        number. See :ref:`ridge_regression` for formula.
+
+    kernel : str or callable, default="linear"
+        Kernel mapping used internally. This parameter is directly passed to
+        :class:`~sklearn.metrics.pairwise.pairwise_kernel`.
+        If `kernel` is a string, it must be one of the metrics
+        in `pairwise.PAIRWISE_KERNEL_FUNCTIONS` or "precomputed".
+        If `kernel` is "precomputed", X is assumed to be a kernel matrix.
+        Alternatively, if `kernel` is a callable function, it is called on
+        each pair of instances (rows) and the resulting value recorded. The
+        callable should take two rows from X as input and return the
+        corresponding kernel value as a single number. This means that
+        callables from :mod:`sklearn.metrics.pairwise` are not allowed, as
+        they operate on matrices, not single samples. Use the string
+        identifying the kernel instead.

     gamma : float, default=None
         Gamma parameter for the RBF, laplacian, polynomial, exponential chi2
@@ -65,32 +74,47 @@
         Zero coefficient for polynomial and sigmoid kernels.
         Ignored by other kernels.

-    kernel_params : mapping of string to any, optional
+    kernel_params : mapping of str to any, default=None
         Additional parameters (keyword arguments) for kernel function passed
         as callable object.

     Attributes
     ----------
-    dual_coef_ : array, shape = [n_samples] or [n_samples, n_targets]
+    dual_coef_ : ndarray of shape (n_samples,) or (n_samples, n_targets)
         Representation of weight vector(s) in kernel space

-    X_fit_ : {array-like, sparse matrix}, shape = [n_samples, n_features]
+    X_fit_ : {ndarray, sparse matrix} of shape (n_samples, n_features)
         Training data, which is also required for prediction. If
         kernel == "precomputed" this is instead the precomputed
-        training matrix, shape = [n_samples, n_samples].
+        training matrix, of shape (n_samples, n_samples).
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    sklearn.gaussian_process.GaussianProcessRegressor : Gaussian
+        Process regressor providing automatic kernel hyperparameters
+        tuning and predictions uncertainty.
+    sklearn.linear_model.Ridge : Linear ridge regression.
+    sklearn.linear_model.RidgeCV : Ridge regression with built-in
+        cross-validation.
+    sklearn.svm.SVR : Support Vector Regression accepting a large variety
+        of kernels.

     References
     ----------
     * Kevin P. Murphy
       "Machine Learning: A Probabilistic Perspective", The MIT Press
       chapter 14.4.3, pp. 492-493
-
-    See also
-    --------
-    sklearn.linear_model.Ridge:
-        Linear ridge regression.
-    sklearn.svm.SVR:
-        Support Vector Regression implemented using libsvm.

     Examples
     --------
@@ -100,13 +124,21 @@
     >>> rng = np.random.RandomState(0)
     >>> y = rng.randn(n_samples)
     >>> X = rng.randn(n_samples, n_features)
-    >>> clf = KernelRidge(alpha=1.0)
-    >>> clf.fit(X, y) # doctest: +NORMALIZE_WHITESPACE
-    KernelRidge(alpha=1.0, coef0=1, degree=3, gamma=None, kernel='linear',
-                kernel_params=None)
+    >>> krr = KernelRidge(alpha=1.0)
+    >>> krr.fit(X, y)
+    KernelRidge(alpha=1.0)
     """
-    def __init__(self, alpha=1, kernel="linear", gamma=None, degree=3, coef0=1,
-                 kernel_params=None):
+
+    def __init__(
+        self,
+        alpha=1,
+        *,
+        kernel="linear",
+        gamma=None,
+        degree=3,
+        coef0=1,
+        kernel_params=None,
+    ):
         self.alpha = alpha
         self.kernel = kernel
         self.gamma = gamma
@@ -118,41 +150,38 @@
         if callable(self.kernel):
             params = self.kernel_params or {}
         else:
-            params = {"gamma": self.gamma,
-                      "degree": self.degree,
-                      "coef0": self.coef0}
-        return pairwise_kernels(X, Y, metric=self.kernel,
-                                filter_params=True, **params)
-
-    @property
-    def _pairwise(self):
-        return self.kernel == "precomputed"
-
-    def fit(self, X, y=None, sample_weight=None):
-        """Fit Kernel Ridge regression model
+            params = {"gamma": self.gamma, "degree": self.degree, "coef0": self.coef0}
+        return pairwise_kernels(X, Y, metric=self.kernel, filter_params=True, **params)
+
+    def _more_tags(self):
+        return {"pairwise": self.kernel == "precomputed"}
+
+    def fit(self, X, y, sample_weight=None):
+        """Fit Kernel Ridge regression model.

         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape = [n_samples, n_features]
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
             Training data. If kernel == "precomputed" this is instead
-            a precomputed kernel matrix, shape = [n_samples,
-            n_samples].
-
-        y : array-like, shape = [n_samples] or [n_samples, n_targets]
-            Target values
-
-        sample_weight : float or array-like of shape [n_samples]
+            a precomputed kernel matrix, of shape (n_samples, n_samples).
+
+        y : array-like of shape (n_samples,) or (n_samples, n_targets)
+            Target values.
+
+        sample_weight : float or array-like of shape (n_samples,), default=None
             Individual weights for each sample, ignored if None is passed.

         Returns
         -------
-        self : returns an instance of self.
+        self : object
+            Returns the instance itself.
         """
         # Convert data
-        X, y = check_X_y(X, y, accept_sparse=("csr", "csc"), multi_output=True,
-                         y_numeric=True)
+        X, y = self._validate_data(
+            X, y, accept_sparse=("csr", "csc"), multi_output=True, y_numeric=True
+        )
         if sample_weight is not None and not isinstance(sample_weight, float):
-            sample_weight = check_array(sample_weight, ensure_2d=False)
+            sample_weight = _check_sample_weight(sample_weight, X)

         K = self._get_kernel(X)
         alpha = np.atleast_1d(self.alpha)
@@ -163,9 +192,7 @@
             ravel = True

         copy = self.kernel == "precomputed"
-        self.dual_coef_ = _solve_cholesky_kernel(K, y, alpha,
-                                                 sample_weight,
-                                                 copy)
+        self.dual_coef_ = _solve_cholesky_kernel(K, y, alpha, sample_weight, copy)
         if ravel:
             self.dual_coef_ = self.dual_coef_.ravel()

@@ -174,11 +201,11 @@
         return self

     def predict(self, X):
-        """Predict using the kernel ridge model
+        """Predict using the kernel ridge model.

         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape = [n_samples, n_features]
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
             Samples. If kernel == "precomputed" this is instead a
             precomputed kernel matrix, shape = [n_samples,
             n_samples_fitted], where n_samples_fitted is the number of
@@ -186,9 +213,10 @@

         Returns
         -------
-        C : array, shape = [n_samples] or [n_samples, n_targets]
+        C : ndarray of shape (n_samples,) or (n_samples, n_targets)
             Returns predicted values.
         """
-        check_is_fitted(self, ["X_fit_", "dual_coef_"])
+        check_is_fitted(self)
+        X = self._validate_data(X, accept_sparse=("csr", "csc"), reset=False)
         K = self._get_kernel(X, self.X_fit_)
         return np.dot(K, self.dual_coef_)
('sklearn', 'naive_bayes.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,5 +1,3 @@
-# -*- coding: utf-8 -*-
-
 """
 The :mod:`sklearn.naive_bayes` module implements Naive Bayes algorithms. These
 are supervised learning methods based on applying Bayes' theorem with strong
@@ -19,23 +17,31 @@

 from abc import ABCMeta, abstractmethod

+
 import numpy as np
-from scipy.sparse import issparse
+from scipy.special import logsumexp

 from .base import BaseEstimator, ClassifierMixin
 from .preprocessing import binarize
 from .preprocessing import LabelBinarizer
 from .preprocessing import label_binarize
-from .utils import check_X_y, check_array, check_consistent_length
+from .utils import deprecated
 from .utils.extmath import safe_sparse_dot
-from .utils.fixes import logsumexp
 from .utils.multiclass import _check_partial_fit_first_call
-from .utils.validation import check_is_fitted
-
-__all__ = ['BernoulliNB', 'GaussianNB', 'MultinomialNB', 'ComplementNB']
-
-
-class BaseNB(BaseEstimator, ClassifierMixin, metaclass=ABCMeta):
+from .utils.validation import check_is_fitted, check_non_negative
+from .utils.validation import _check_sample_weight
+
+
+__all__ = [
+    "BernoulliNB",
+    "GaussianNB",
+    "MultinomialNB",
+    "ComplementNB",
+    "CategoricalNB",
+]
+
+
+class _BaseNB(ClassifierMixin, BaseEstimator, metaclass=ABCMeta):
     """Abstract base class for naive Bayes estimators"""

     @abstractmethod
@@ -43,10 +49,17 @@
         """Compute the unnormalized posterior log probability of X

         I.e. ``log P(c) + log P(x|c)`` for all rows x of X, as an array-like of
-        shape [n_classes, n_samples].
-
-        Input is passed to _joint_log_likelihood as-is by predict,
-        predict_proba and predict_log_proba.
+        shape (n_samples, n_classes).
+
+        predict, predict_proba, and predict_log_proba pass the input through
+        _check_X and handle it over to _joint_log_likelihood.
+        """
+
+    @abstractmethod
+    def _check_X(self, X):
+        """To be overridden in subclasses with the actual checks.
+
+        Only used in predict* methods.
         """

     def predict(self, X):
@@ -55,13 +68,16 @@

         Parameters
         ----------
-        X : array-like, shape = [n_samples, n_features]
+        X : array-like of shape (n_samples, n_features)
+            The input samples.

         Returns
         -------
-        C : array, shape = [n_samples]
-            Predicted target values for X
-        """
+        C : ndarray of shape (n_samples,)
+            Predicted target values for X.
+        """
+        check_is_fitted(self)
+        X = self._check_X(X)
         jll = self._joint_log_likelihood(X)
         return self.classes_[np.argmax(jll, axis=1)]

@@ -71,15 +87,18 @@

         Parameters
         ----------
-        X : array-like, shape = [n_samples, n_features]
+        X : array-like of shape (n_samples, n_features)
+            The input samples.

         Returns
         -------
-        C : array-like, shape = [n_samples, n_classes]
+        C : array-like of shape (n_samples, n_classes)
             Returns the log-probability of the samples for each class in
             the model. The columns correspond to the classes in sorted
-            order, as they appear in the attribute `classes_`.
-        """
+            order, as they appear in the attribute :term:`classes_`.
+        """
+        check_is_fitted(self)
+        X = self._check_X(X)
         jll = self._joint_log_likelihood(X)
         # normalize by P(x) = P(f_1, ..., f_n)
         log_prob_x = logsumexp(jll, axis=1)
@@ -91,23 +110,24 @@

         Parameters
         ----------
-        X : array-like, shape = [n_samples, n_features]
+        X : array-like of shape (n_samples, n_features)
+            The input samples.

         Returns
         -------
-        C : array-like, shape = [n_samples, n_classes]
+        C : array-like of shape (n_samples, n_classes)
             Returns the probability of the samples for each class in
             the model. The columns correspond to the classes in sorted
-            order, as they appear in the attribute `classes_`.
+            order, as they appear in the attribute :term:`classes_`.
         """
         return np.exp(self.predict_log_proba(X))


-class GaussianNB(BaseNB):
+class GaussianNB(_BaseNB):
     """
-    Gaussian Naive Bayes (GaussianNB)
-
-    Can perform online updates to model parameters via `partial_fit` method.
+    Gaussian Naive Bayes (GaussianNB).
+
+    Can perform online updates to model parameters via :meth:`partial_fit`.
     For details on algorithm used to update feature means and variance online,
     see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:

@@ -117,30 +137,62 @@

     Parameters
     ----------
-    priors : array-like, shape (n_classes,)
-        Prior probabilities of the classes. If specified the priors are not
+    priors : array-like of shape (n_classes,)
+        Prior probabilities of the classes. If specified, the priors are not
         adjusted according to the data.

-    var_smoothing : float, optional (default=1e-9)
+    var_smoothing : float, default=1e-9
         Portion of the largest variance of all features that is added to
         variances for calculation stability.

+        .. versionadded:: 0.20
+
     Attributes
     ----------
-    class_prior_ : array, shape (n_classes,)
+    class_count_ : ndarray of shape (n_classes,)
+        number of training samples observed in each class.
+
+    class_prior_ : ndarray of shape (n_classes,)
         probability of each class.

-    class_count_ : array, shape (n_classes,)
-        number of training samples observed in each class.
-
-    theta_ : array, shape (n_classes, n_features)
-        mean of each feature per class
-
-    sigma_ : array, shape (n_classes, n_features)
-        variance of each feature per class
+    classes_ : ndarray of shape (n_classes,)
+        class labels known to the classifier.

     epsilon_ : float
-        absolute additive value to variances
+        absolute additive value to variances.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    sigma_ : ndarray of shape (n_classes, n_features)
+        Variance of each feature per class.
+
+        .. deprecated:: 1.0
+           `sigma_` is deprecated in 1.0 and will be removed in 1.2.
+           Use `var_` instead.
+
+    var_ : ndarray of shape (n_classes, n_features)
+        Variance of each feature per class.
+
+        .. versionadded:: 1.0
+
+    theta_ : ndarray of shape (n_classes, n_features)
+        mean of each feature per class.
+
+    See Also
+    --------
+    BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.
+    CategoricalNB : Naive Bayes classifier for categorical features.
+    ComplementNB : Complement Naive Bayes classifier.
+    MultinomialNB : Naive Bayes classifier for multinomial models.

     Examples
     --------
@@ -150,33 +202,33 @@
     >>> from sklearn.naive_bayes import GaussianNB
     >>> clf = GaussianNB()
     >>> clf.fit(X, Y)
-    GaussianNB(priors=None, var_smoothing=1e-09)
+    GaussianNB()
     >>> print(clf.predict([[-0.8, -1]]))
     [1]
     >>> clf_pf = GaussianNB()
     >>> clf_pf.partial_fit(X, Y, np.unique(Y))
-    GaussianNB(priors=None, var_smoothing=1e-09)
+    GaussianNB()
     >>> print(clf_pf.predict([[-0.8, -1]]))
     [1]
     """

-    def __init__(self, priors=None, var_smoothing=1e-9):
+    def __init__(self, *, priors=None, var_smoothing=1e-9):
         self.priors = priors
         self.var_smoothing = var_smoothing

     def fit(self, X, y, sample_weight=None):
-        """Fit Gaussian Naive Bayes according to X, y
+        """Fit Gaussian Naive Bayes according to X, y.

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training vectors, where n_samples is the number of samples
-            and n_features is the number of features.
-
-        y : array-like, shape (n_samples,)
+        X : array-like of shape (n_samples, n_features)
+            Training vectors, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,)
             Target values.

-        sample_weight : array-like, shape (n_samples,), optional (default=None)
+        sample_weight : array-like of shape (n_samples,), default=None
             Weights applied to individual samples (1. for unweighted).

             .. versionadded:: 0.17
@@ -185,10 +237,16 @@
         Returns
         -------
         self : object
-        """
-        X, y = check_X_y(X, y)
-        return self._partial_fit(X, y, np.unique(y), _refit=True,
-                                 sample_weight=sample_weight)
+            Returns the instance itself.
+        """
+        y = self._validate_data(y=y)
+        return self._partial_fit(
+            X, y, np.unique(y), _refit=True, sample_weight=sample_weight
+        )
+
+    def _check_X(self, X):
+        """Validate X, used only in predict* methods."""
+        return self._validate_data(X, reset=False)

     @staticmethod
     def _update_mean_variance(n_past, mu, var, X, sample_weight=None):
@@ -213,21 +271,21 @@
             weights were given, this should contain the sum of sample
             weights represented in old mean and variance.

-        mu : array-like, shape (number of Gaussians,)
+        mu : array-like of shape (number of Gaussians,)
             Means for Gaussians in original set.

-        var : array-like, shape (number of Gaussians,)
+        var : array-like of shape (number of Gaussians,)
             Variances for Gaussians in original set.

-        sample_weight : array-like, shape (n_samples,), optional (default=None)
+        sample_weight : array-like of shape (n_samples,), default=None
             Weights applied to individual samples (1. for unweighted).

         Returns
         -------
-        total_mu : array-like, shape (number of Gaussians,)
+        total_mu : array-like of shape (number of Gaussians,)
             Updated mean for each Gaussian over the combined set.

-        total_var : array-like, shape (number of Gaussians,)
+        total_var : array-like of shape (number of Gaussians,)
             Updated variance for each Gaussian over the combined set.
         """
         if X.shape[0] == 0:
@@ -237,8 +295,7 @@
         if sample_weight is not None:
             n_new = float(sample_weight.sum())
             new_mu = np.average(X, axis=0, weights=sample_weight)
-            new_var = np.average((X - new_mu) ** 2, axis=0,
-                                 weights=sample_weight)
+            new_var = np.average((X - new_mu) ** 2, axis=0, weights=sample_weight)
         else:
             n_new = X.shape[0]
             new_var = np.var(X, axis=0)
@@ -258,8 +315,7 @@
         # the sum-of-squared-differences (ssd)
         old_ssd = n_past * var
         new_ssd = n_new * new_var
-        total_ssd = (old_ssd + new_ssd +
-                     (n_new * n_past / n_total) * (mu - new_mu) ** 2)
+        total_ssd = old_ssd + new_ssd + (n_new * n_past / n_total) * (mu - new_mu) ** 2
         total_var = total_ssd / n_total

         return total_mu, total_var
@@ -281,20 +337,20 @@

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training vectors, where n_samples is the number of samples and
-            n_features is the number of features.
-
-        y : array-like, shape (n_samples,)
+        X : array-like of shape (n_samples, n_features)
+            Training vectors, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,)
             Target values.

-        classes : array-like, shape (n_classes,), optional (default=None)
+        classes : array-like of shape (n_classes,), default=None
             List of all the classes that can possibly appear in the y vector.

             Must be provided at the first call to partial_fit, can be omitted
             in subsequent calls.

-        sample_weight : array-like, shape (n_samples,), optional (default=None)
+        sample_weight : array-like of shape (n_samples,), default=None
             Weights applied to individual samples (1. for unweighted).

             .. versionadded:: 0.17
@@ -302,44 +358,48 @@
         Returns
         -------
         self : object
-        """
-        return self._partial_fit(X, y, classes, _refit=False,
-                                 sample_weight=sample_weight)
-
-    def _partial_fit(self, X, y, classes=None, _refit=False,
-                     sample_weight=None):
+            Returns the instance itself.
+        """
+        return self._partial_fit(
+            X, y, classes, _refit=False, sample_weight=sample_weight
+        )
+
+    def _partial_fit(self, X, y, classes=None, _refit=False, sample_weight=None):
         """Actual implementation of Gaussian NB fitting.

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training vectors, where n_samples is the number of samples and
-            n_features is the number of features.
-
-        y : array-like, shape (n_samples,)
+        X : array-like of shape (n_samples, n_features)
+            Training vectors, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,)
             Target values.

-        classes : array-like, shape (n_classes,), optional (default=None)
+        classes : array-like of shape (n_classes,), default=None
             List of all the classes that can possibly appear in the y vector.

             Must be provided at the first call to partial_fit, can be omitted
             in subsequent calls.

-        _refit : bool, optional (default=False)
+        _refit : bool, default=False
             If true, act as though this were the first time we called
             _partial_fit (ie, throw away any past fitting and start over).

-        sample_weight : array-like, shape (n_samples,), optional (default=None)
+        sample_weight : array-like of shape (n_samples,), default=None
             Weights applied to individual samples (1. for unweighted).

         Returns
         -------
         self : object
         """
-        X, y = check_X_y(X, y)
+        if _refit:
+            self.classes_ = None
+
+        first_call = _check_partial_fit_first_call(self, classes)
+        X, y = self._validate_data(X, y, reset=first_call)
         if sample_weight is not None:
-            sample_weight = check_array(sample_weight, ensure_2d=False)
-            check_consistent_length(y, sample_weight)
+            sample_weight = _check_sample_weight(sample_weight, X)

         # If the ratio of data variance between dimensions is too small, it
         # will cause numerical errors. To address this, we artificially
@@ -347,16 +407,13 @@
         # deviation of the largest dimension.
         self.epsilon_ = self.var_smoothing * np.var(X, axis=0).max()

-        if _refit:
-            self.classes_ = None
-
-        if _check_partial_fit_first_call(self, classes):
+        if first_call:
             # This is the first call to partial_fit:
             # initialize various cumulative counters
             n_features = X.shape[1]
             n_classes = len(self.classes_)
             self.theta_ = np.zeros((n_classes, n_features))
-            self.sigma_ = np.zeros((n_classes, n_features))
+            self.var_ = np.zeros((n_classes, n_features))

             self.class_count_ = np.zeros(n_classes, dtype=np.float64)

@@ -364,27 +421,25 @@
             # Take into account the priors
             if self.priors is not None:
                 priors = np.asarray(self.priors)
-                # Check that the provide prior match the number of classes
+                # Check that the provided prior matches the number of classes
                 if len(priors) != n_classes:
-                    raise ValueError('Number of priors must match number of'
-                                     ' classes.')
+                    raise ValueError("Number of priors must match number of classes.")
                 # Check that the sum is 1
                 if not np.isclose(priors.sum(), 1.0):
-                    raise ValueError('The sum of the priors should be 1.')
-                # Check that the prior are non-negative
+                    raise ValueError("The sum of the priors should be 1.")
+                # Check that the priors are non-negative
                 if (priors < 0).any():
-                    raise ValueError('Priors must be non-negative.')
+                    raise ValueError("Priors must be non-negative.")
                 self.class_prior_ = priors
             else:
                 # Initialize the priors to zeros for each class
-                self.class_prior_ = np.zeros(len(self.classes_),
-                                             dtype=np.float64)
+                self.class_prior_ = np.zeros(len(self.classes_), dtype=np.float64)
         else:
             if X.shape[1] != self.theta_.shape[1]:
                 msg = "Number of features %d does not match previous data %d."
                 raise ValueError(msg % (X.shape[1], self.theta_.shape[1]))
             # Put epsilon back in each time
-            self.sigma_[:, :] -= self.epsilon_
+            self.var_[:, :] -= self.epsilon_

         classes = self.classes_

@@ -392,9 +447,10 @@
         unique_y_in_classes = np.in1d(unique_y, classes)

         if not np.all(unique_y_in_classes):
-            raise ValueError("The target label(s) %s in y do not exist in the "
-                             "initial classes %s" %
-                             (unique_y[~unique_y_in_classes], classes))
+            raise ValueError(
+                "The target label(s) %s in y do not exist in the initial classes %s"
+                % (unique_y[~unique_y_in_classes], classes)
+            )

         for y_i in unique_y:
             i = classes.searchsorted(y_i)
@@ -408,14 +464,14 @@
                 N_i = X_i.shape[0]

             new_theta, new_sigma = self._update_mean_variance(
-                self.class_count_[i], self.theta_[i, :], self.sigma_[i, :],
-                X_i, sw_i)
+                self.class_count_[i], self.theta_[i, :], self.var_[i, :], X_i, sw_i
+            )

             self.theta_[i, :] = new_theta
-            self.sigma_[i, :] = new_sigma
+            self.var_[i, :] = new_sigma
             self.class_count_[i] += N_i

-        self.sigma_[:, :] += self.epsilon_
+        self.var_[:, :] += self.epsilon_

         # Update if only no priors is provided
         if self.priors is None:
@@ -425,39 +481,88 @@
         return self

     def _joint_log_likelihood(self, X):
-        check_is_fitted(self, "classes_")
-
-        X = check_array(X)
         joint_log_likelihood = []
         for i in range(np.size(self.classes_)):
             jointi = np.log(self.class_prior_[i])
-            n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))
-            n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) /
-                                 (self.sigma_[i, :]), 1)
+            n_ij = -0.5 * np.sum(np.log(2.0 * np.pi * self.var_[i, :]))
+            n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) / (self.var_[i, :]), 1)
             joint_log_likelihood.append(jointi + n_ij)

         joint_log_likelihood = np.array(joint_log_likelihood).T
         return joint_log_likelihood

+    @deprecated(  # type: ignore
+        "Attribute `sigma_` was deprecated in 1.0 and will be removed in"
+        "1.2. Use `var_` instead."
+    )
+    @property
+    def sigma_(self):
+        return self.var_
+

 _ALPHA_MIN = 1e-10


-class BaseDiscreteNB(BaseNB):
+class _BaseDiscreteNB(_BaseNB):
     """Abstract base class for naive Bayes on discrete/categorical data

     Any estimator based on this class should provide:

     __init__
-    _joint_log_likelihood(X) as per BaseNB
+    _joint_log_likelihood(X) as per _BaseNB
+    _update_feature_log_prob(alpha)
+    _count(X, Y)
     """

+    @abstractmethod
+    def _count(self, X, Y):
+        """Update counts that are used to calculate probabilities.
+
+        The counts make up a sufficient statistic extracted from the data.
+        Accordingly, this method is called each time `fit` or `partial_fit`
+        update the model. `class_count_` and `feature_count_` must be updated
+        here along with any model specific counts.
+
+        Parameters
+        ----------
+        X : {ndarray, sparse matrix} of shape (n_samples, n_features)
+            The input samples.
+        Y : ndarray of shape (n_samples, n_classes)
+            Binarized class labels.
+        """
+
+    @abstractmethod
+    def _update_feature_log_prob(self, alpha):
+        """Update feature log probabilities based on counts.
+
+        This method is called each time `fit` or `partial_fit` update the
+        model.
+
+        Parameters
+        ----------
+        alpha : float
+            smoothing parameter. See :meth:`_check_alpha`.
+        """
+
+    def _check_X(self, X):
+        """Validate X, used only in predict* methods."""
+        return self._validate_data(X, accept_sparse="csr", reset=False)
+
+    def _check_X_y(self, X, y, reset=True):
+        """Validate X and y in fit methods."""
+        return self._validate_data(X, y, accept_sparse="csr", reset=reset)
+
     def _update_class_log_prior(self, class_prior=None):
+        """Update class log priors.
+
+        The class log priors are based on `class_prior`, class count or the
+        number of classes. This method is called each time `fit` or
+        `partial_fit` update the model.
+        """
         n_classes = len(self.classes_)
         if class_prior is not None:
             if len(class_prior) != n_classes:
-                raise ValueError("Number of priors must match number of"
-                                 " classes.")
+                raise ValueError("Number of priors must match number of classes.")
             self.class_log_prior_ = np.log(class_prior)
         elif self.fit_prior:
             with warnings.catch_warnings():
@@ -467,22 +572,26 @@
                 log_class_count = np.log(self.class_count_)

             # empirical prior, with sample_weight taken into account
-            self.class_log_prior_ = (log_class_count -
-                                     np.log(self.class_count_.sum()))
+            self.class_log_prior_ = log_class_count - np.log(self.class_count_.sum())
         else:
             self.class_log_prior_ = np.full(n_classes, -np.log(n_classes))

     def _check_alpha(self):
         if np.min(self.alpha) < 0:
-            raise ValueError('Smoothing parameter alpha = %.1e. '
-                             'alpha should be > 0.' % np.min(self.alpha))
+            raise ValueError(
+                "Smoothing parameter alpha = %.1e. alpha should be > 0."
+                % np.min(self.alpha)
+            )
         if isinstance(self.alpha, np.ndarray):
-            if not self.alpha.shape[0] == self.feature_count_.shape[1]:
-                raise ValueError("alpha should be a scalar or a numpy array "
-                                 "with shape [n_features]")
+            if not self.alpha.shape[0] == self.n_features_in_:
+                raise ValueError(
+                    "alpha should be a scalar or a numpy array with shape [n_features]"
+                )
         if np.min(self.alpha) < _ALPHA_MIN:
-            warnings.warn('alpha too small will result in numeric errors, '
-                          'setting alpha = %.1e' % _ALPHA_MIN)
+            warnings.warn(
+                "alpha too small will result in numeric errors, setting alpha = %.1e"
+                % _ALPHA_MIN
+            )
             return np.maximum(self.alpha, _ALPHA_MIN)
         return self.alpha

@@ -502,43 +611,43 @@

         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape = [n_samples, n_features]
-            Training vectors, where n_samples is the number of samples and
-            n_features is the number of features.
-
-        y : array-like, shape = [n_samples]
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Training vectors, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,)
             Target values.

-        classes : array-like, shape = [n_classes] (default=None)
+        classes : array-like of shape (n_classes,), default=None
             List of all the classes that can possibly appear in the y vector.

             Must be provided at the first call to partial_fit, can be omitted
             in subsequent calls.

-        sample_weight : array-like, shape = [n_samples] (default=None)
+        sample_weight : array-like of shape (n_samples,), default=None
             Weights applied to individual samples (1. for unweighted).

         Returns
         -------
         self : object
-        """
-        X = check_array(X, accept_sparse='csr', dtype=np.float64)
+            Returns the instance itself.
+        """
+        first_call = not hasattr(self, "classes_")
+        X, y = self._check_X_y(X, y, reset=first_call)
         _, n_features = X.shape

         if _check_partial_fit_first_call(self, classes):
             # This is the first call to partial_fit:
             # initialize various cumulative counters
-            n_effective_classes = len(classes) if len(classes) > 1 else 2
-            self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)
-            self.feature_count_ = np.zeros((n_effective_classes, n_features),
-                                           dtype=np.float64)
-        elif n_features != self.coef_.shape[1]:
-            msg = "Number of features %d does not match previous data %d."
-            raise ValueError(msg % (n_features, self.coef_.shape[-1]))
+            n_classes = len(classes)
+            self._init_counters(n_classes, n_features)

         Y = label_binarize(y, classes=self.classes_)
         if Y.shape[1] == 1:
-            Y = np.concatenate((1 - Y, Y), axis=1)
+            if len(self.classes_) == 2:
+                Y = np.concatenate((1 - Y, Y), axis=1)
+            else:  # degenerate case: just one class
+                Y = np.ones_like(Y)

         if X.shape[0] != Y.shape[0]:
             msg = "X.shape[0]=%d and y.shape[0]=%d are incompatible."
@@ -548,8 +657,9 @@
         # We convert it to np.float64 to support sample_weight consistently
         Y = Y.astype(np.float64, copy=False)
         if sample_weight is not None:
+            sample_weight = _check_sample_weight(sample_weight, X)
             sample_weight = np.atleast_2d(sample_weight)
-            Y *= check_array(sample_weight).T
+            Y *= sample_weight.T

         class_prior = self.class_prior

@@ -567,75 +677,79 @@
         return self

     def fit(self, X, y, sample_weight=None):
-        """Fit Naive Bayes classifier according to X, y
+        """Fit Naive Bayes classifier according to X, y.

         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape = [n_samples, n_features]
-            Training vectors, where n_samples is the number of samples and
-            n_features is the number of features.
-
-        y : array-like, shape = [n_samples]
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Training vectors, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,)
             Target values.

-        sample_weight : array-like, shape = [n_samples], (default=None)
+        sample_weight : array-like of shape (n_samples,), default=None
             Weights applied to individual samples (1. for unweighted).

         Returns
         -------
         self : object
-        """
-        X, y = check_X_y(X, y, 'csr')
+            Returns the instance itself.
+        """
+        X, y = self._check_X_y(X, y)
         _, n_features = X.shape

         labelbin = LabelBinarizer()
         Y = labelbin.fit_transform(y)
         self.classes_ = labelbin.classes_
         if Y.shape[1] == 1:
-            Y = np.concatenate((1 - Y, Y), axis=1)
+            if len(self.classes_) == 2:
+                Y = np.concatenate((1 - Y, Y), axis=1)
+            else:  # degenerate case: just one class
+                Y = np.ones_like(Y)

         # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.
         # We convert it to np.float64 to support sample_weight consistently;
         # this means we also don't have to cast X to floating point
-        Y = Y.astype(np.float64, copy=False)
         if sample_weight is not None:
+            Y = Y.astype(np.float64, copy=False)
+            sample_weight = _check_sample_weight(sample_weight, X)
             sample_weight = np.atleast_2d(sample_weight)
-            Y *= check_array(sample_weight).T
+            Y *= sample_weight.T

         class_prior = self.class_prior

         # Count raw events from data before updating the class log prior
         # and feature log probas
-        n_effective_classes = Y.shape[1]
-        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)
-        self.feature_count_ = np.zeros((n_effective_classes, n_features),
-                                       dtype=np.float64)
+        n_classes = Y.shape[1]
+        self._init_counters(n_classes, n_features)
         self._count(X, Y)
         alpha = self._check_alpha()
         self._update_feature_log_prob(alpha)
         self._update_class_log_prior(class_prior=class_prior)
         return self

-    # XXX The following is a stopgap measure; we need to set the dimensions
-    # of class_log_prior_ and feature_log_prob_ correctly.
-    def _get_coef(self):
-        return (self.feature_log_prob_[1:]
-                if len(self.classes_) == 2 else self.feature_log_prob_)
-
-    def _get_intercept(self):
-        return (self.class_log_prior_[1:]
-                if len(self.classes_) == 2 else self.class_log_prior_)
-
-    coef_ = property(_get_coef)
-    intercept_ = property(_get_intercept)
+    def _init_counters(self, n_classes, n_features):
+        self.class_count_ = np.zeros(n_classes, dtype=np.float64)
+        self.feature_count_ = np.zeros((n_classes, n_features), dtype=np.float64)

     def _more_tags(self):
-        return {'poor_score': True}
-
-
-class MultinomialNB(BaseDiscreteNB):
+        return {"poor_score": True}
+
+    # TODO: Remove in 1.2
+    # mypy error: Decorated property not supported
+    @deprecated(  # type: ignore
+        "Attribute `n_features_` was deprecated in version 1.0 and will be "
+        "removed in 1.2. Use `n_features_in_` instead."
+    )
+    @property
+    def n_features_(self):
+        return self.n_features_in_
+
+
+class MultinomialNB(_BaseDiscreteNB):
     """
-    Naive Bayes classifier for multinomial models
+    Naive Bayes classifier for multinomial models.

     The multinomial Naive Bayes classifier is suitable for classification with
     discrete features (e.g., word counts for text classification). The
@@ -646,78 +760,95 @@

     Parameters
     ----------
-    alpha : float, optional (default=1.0)
+    alpha : float, default=1.0
         Additive (Laplace/Lidstone) smoothing parameter
         (0 for no smoothing).

-    fit_prior : boolean, optional (default=True)
+    fit_prior : bool, default=True
         Whether to learn class prior probabilities or not.
         If false, a uniform prior will be used.

-    class_prior : array-like, size (n_classes,), optional (default=None)
-        Prior probabilities of the classes. If specified the priors are not
+    class_prior : array-like of shape (n_classes,), default=None
+        Prior probabilities of the classes. If specified, the priors are not
         adjusted according to the data.

     Attributes
     ----------
-    class_log_prior_ : array, shape (n_classes, )
-        Smoothed empirical log probability for each class.
-
-    intercept_ : array, shape (n_classes, )
-        Mirrors ``class_log_prior_`` for interpreting MultinomialNB
-        as a linear model.
-
-    feature_log_prob_ : array, shape (n_classes, n_features)
-        Empirical log probability of features
-        given a class, ``P(x_i|y)``.
-
-    coef_ : array, shape (n_classes, n_features)
-        Mirrors ``feature_log_prob_`` for interpreting MultinomialNB
-        as a linear model.
-
-    class_count_ : array, shape (n_classes,)
+    class_count_ : ndarray of shape (n_classes,)
         Number of samples encountered for each class during fitting. This
         value is weighted by the sample weight when provided.

-    feature_count_ : array, shape (n_classes, n_features)
+    class_log_prior_ : ndarray of shape (n_classes,)
+        Smoothed empirical log probability for each class.
+
+    classes_ : ndarray of shape (n_classes,)
+        Class labels known to the classifier
+
+    feature_count_ : ndarray of shape (n_classes, n_features)
         Number of samples encountered for each (class, feature)
         during fitting. This value is weighted by the sample weight when
         provided.

-    Examples
+    feature_log_prob_ : ndarray of shape (n_classes, n_features)
+        Empirical log probability of features
+        given a class, ``P(x_i|y)``.
+
+    n_features_ : int
+        Number of features of each sample.
+
+        .. deprecated:: 1.0
+            Attribute `n_features_` was deprecated in version 1.0 and will be
+            removed in 1.2. Use `n_features_in_` instead.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
     --------
-    >>> import numpy as np
-    >>> X = np.random.randint(5, size=(6, 100))
-    >>> y = np.array([1, 2, 3, 4, 5, 6])
-    >>> from sklearn.naive_bayes import MultinomialNB
-    >>> clf = MultinomialNB()
-    >>> clf.fit(X, y)
-    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
-    >>> print(clf.predict(X[2:3]))
-    [3]
-
-    Notes
-    -----
-    For the rationale behind the names `coef_` and `intercept_`, i.e.
-    naive Bayes as a linear classifier, see J. Rennie et al. (2003),
-    Tackling the poor assumptions of naive Bayes text classifiers, ICML.
+    BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.
+    CategoricalNB : Naive Bayes classifier for categorical features.
+    ComplementNB : Complement Naive Bayes classifier.
+    GaussianNB : Gaussian Naive Bayes.

     References
     ----------
     C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to
     Information Retrieval. Cambridge University Press, pp. 234-265.
     https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> rng = np.random.RandomState(1)
+    >>> X = rng.randint(5, size=(6, 100))
+    >>> y = np.array([1, 2, 3, 4, 5, 6])
+    >>> from sklearn.naive_bayes import MultinomialNB
+    >>> clf = MultinomialNB()
+    >>> clf.fit(X, y)
+    MultinomialNB()
+    >>> print(clf.predict(X[2:3]))
+    [3]
     """

-    def __init__(self, alpha=1.0, fit_prior=True, class_prior=None):
+    def __init__(self, *, alpha=1.0, fit_prior=True, class_prior=None):
         self.alpha = alpha
         self.fit_prior = fit_prior
         self.class_prior = class_prior

+    def _more_tags(self):
+        return {"requires_positive_X": True}
+
     def _count(self, X, Y):
         """Count and smooth feature occurrences."""
-        if np.any((X.data if issparse(X) else X) < 0):
-            raise ValueError("Input X must be non-negative")
+        check_non_negative(X, "MultinomialNB (input X)")
         self.feature_count_ += safe_sparse_dot(Y.T, X)
         self.class_count_ += Y.sum(axis=0)

@@ -726,19 +857,16 @@
         smoothed_fc = self.feature_count_ + alpha
         smoothed_cc = smoothed_fc.sum(axis=1)

-        self.feature_log_prob_ = (np.log(smoothed_fc) -
-                                  np.log(smoothed_cc.reshape(-1, 1)))
+        self.feature_log_prob_ = np.log(smoothed_fc) - np.log(
+            smoothed_cc.reshape(-1, 1)
+        )

     def _joint_log_likelihood(self, X):
         """Calculate the posterior log probability of the samples X"""
-        check_is_fitted(self, "classes_")
-
-        X = check_array(X, accept_sparse='csr')
-        return (safe_sparse_dot(X, self.feature_log_prob_.T) +
-                self.class_log_prior_)
-
-
-class ComplementNB(BaseDiscreteNB):
+        return safe_sparse_dot(X, self.feature_log_prob_.T) + self.class_log_prior_
+
+
+class ComplementNB(_BaseDiscreteNB):
     """The Complement Naive Bayes classifier described in Rennie et al. (2003).

     The Complement Naive Bayes classifier was designed to correct the "severe
@@ -747,18 +875,20 @@

     Read more in the :ref:`User Guide <complement_naive_bayes>`.

+    .. versionadded:: 0.20
+
     Parameters
     ----------
-    alpha : float, optional (default=1.0)
+    alpha : float, default=1.0
         Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).

-    fit_prior : boolean, optional (default=True)
+    fit_prior : bool, default=True
         Only used in edge case with a single class in the training set.

-    class_prior : array-like, size (n_classes,), optional (default=None)
+    class_prior : array-like of shape (n_classes,), default=None
         Prior probabilities of the classes. Not used.

-    norm : boolean, optional (default=False)
+    norm : bool, default=False
         Whether or not a second normalization of the weights is performed. The
         default behavior mirrors the implementations found in Mahout and Weka,
         which do not follow the full algorithm described in Table 9 of the
@@ -766,36 +896,52 @@

     Attributes
     ----------
-    class_log_prior_ : array, shape (n_classes, )
+    class_count_ : ndarray of shape (n_classes,)
+        Number of samples encountered for each class during fitting. This
+        value is weighted by the sample weight when provided.
+
+    class_log_prior_ : ndarray of shape (n_classes,)
         Smoothed empirical log probability for each class. Only used in edge
         case with a single class in the training set.

-    feature_log_prob_ : array, shape (n_classes, n_features)
-        Empirical weights for class complements.
-
-    class_count_ : array, shape (n_classes,)
-        Number of samples encountered for each class during fitting. This
+    classes_ : ndarray of shape (n_classes,)
+        Class labels known to the classifier
+
+    feature_all_ : ndarray of shape (n_features,)
+        Number of samples encountered for each feature during fitting. This
         value is weighted by the sample weight when provided.

-    feature_count_ : array, shape (n_classes, n_features)
+    feature_count_ : ndarray of shape (n_classes, n_features)
         Number of samples encountered for each (class, feature) during fitting.
         This value is weighted by the sample weight when provided.

-    feature_all_ : array, shape (n_features,)
-        Number of samples encountered for each feature during fitting. This
-        value is weighted by the sample weight when provided.
-
-    Examples
+    feature_log_prob_ : ndarray of shape (n_classes, n_features)
+        Empirical weights for class complements.
+
+    n_features_ : int
+        Number of features of each sample.
+
+        .. deprecated:: 1.0
+            Attribute `n_features_` was deprecated in version 1.0 and will be
+            removed in 1.2. Use `n_features_in_` instead.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
     --------
-    >>> import numpy as np
-    >>> X = np.random.randint(5, size=(6, 100))
-    >>> y = np.array([1, 2, 3, 4, 5, 6])
-    >>> from sklearn.naive_bayes import ComplementNB
-    >>> clf = ComplementNB()
-    >>> clf.fit(X, y)
-    ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)
-    >>> print(clf.predict(X[2:3]))
-    [3]
+    BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.
+    CategoricalNB : Naive Bayes classifier for categorical features.
+    GaussianNB : Gaussian Naive Bayes.
+    MultinomialNB : Naive Bayes classifier for multinomial models.

     References
     ----------
@@ -803,19 +949,33 @@
     Tackling the poor assumptions of naive bayes text classifiers. In ICML
     (Vol. 3, pp. 616-623).
     https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> rng = np.random.RandomState(1)
+    >>> X = rng.randint(5, size=(6, 100))
+    >>> y = np.array([1, 2, 3, 4, 5, 6])
+    >>> from sklearn.naive_bayes import ComplementNB
+    >>> clf = ComplementNB()
+    >>> clf.fit(X, y)
+    ComplementNB()
+    >>> print(clf.predict(X[2:3]))
+    [3]
     """

-    def __init__(self, alpha=1.0, fit_prior=True, class_prior=None,
-                 norm=False):
+    def __init__(self, *, alpha=1.0, fit_prior=True, class_prior=None, norm=False):
         self.alpha = alpha
         self.fit_prior = fit_prior
         self.class_prior = class_prior
         self.norm = norm

+    def _more_tags(self):
+        return {"requires_positive_X": True}
+
     def _count(self, X, Y):
         """Count feature occurrences."""
-        if np.any((X.data if issparse(X) else X) < 0):
-            raise ValueError("Input X must be non-negative")
+        check_non_negative(X, "ComplementNB (input X)")
         self.feature_count_ += safe_sparse_dot(Y.T, X)
         self.class_count_ += Y.sum(axis=0)
         self.feature_all_ = self.feature_count_.sum(axis=0)
@@ -824,7 +984,7 @@
         """Apply smoothing to raw counts and compute the weights."""
         comp_count = self.feature_all_ + alpha - self.feature_count_
         logged = np.log(comp_count / comp_count.sum(axis=1, keepdims=True))
-        # BaseNB.predict uses argmax, but ComplementNB operates with argmin.
+        # _BaseNB.predict uses argmax, but ComplementNB operates with argmin.
         if self.norm:
             summed = logged.sum(axis=1, keepdims=True)
             feature_log_prob = logged / summed
@@ -834,16 +994,13 @@

     def _joint_log_likelihood(self, X):
         """Calculate the class scores for the samples in X."""
-        check_is_fitted(self, "classes_")
-
-        X = check_array(X, accept_sparse="csr")
         jll = safe_sparse_dot(X, self.feature_log_prob_.T)
         if len(self.classes_) == 1:
             jll += self.class_log_prior_
         return jll


-class BernoulliNB(BaseDiscreteNB):
+class BernoulliNB(_BaseDiscreteNB):
     """Naive Bayes classifier for multivariate Bernoulli models.

     Like MultinomialNB, this classifier is suitable for discrete data. The
@@ -854,77 +1011,116 @@

     Parameters
     ----------
-    alpha : float, optional (default=1.0)
+    alpha : float, default=1.0
         Additive (Laplace/Lidstone) smoothing parameter
         (0 for no smoothing).

-    binarize : float or None, optional (default=0.0)
+    binarize : float or None, default=0.0
         Threshold for binarizing (mapping to booleans) of sample features.
         If None, input is presumed to already consist of binary vectors.

-    fit_prior : boolean, optional (default=True)
+    fit_prior : bool, default=True
         Whether to learn class prior probabilities or not.
         If false, a uniform prior will be used.

-    class_prior : array-like, size=[n_classes,], optional (default=None)
-        Prior probabilities of the classes. If specified the priors are not
+    class_prior : array-like of shape (n_classes,), default=None
+        Prior probabilities of the classes. If specified, the priors are not
         adjusted according to the data.

     Attributes
     ----------
-    class_log_prior_ : array, shape = [n_classes]
-        Log probability of each class (smoothed).
-
-    feature_log_prob_ : array, shape = [n_classes, n_features]
-        Empirical log probability of features given a class, P(x_i|y).
-
-    class_count_ : array, shape = [n_classes]
+    class_count_ : ndarray of shape (n_classes,)
         Number of samples encountered for each class during fitting. This
         value is weighted by the sample weight when provided.

-    feature_count_ : array, shape = [n_classes, n_features]
+    class_log_prior_ : ndarray of shape (n_classes,)
+        Log probability of each class (smoothed).
+
+    classes_ : ndarray of shape (n_classes,)
+        Class labels known to the classifier
+
+    feature_count_ : ndarray of shape (n_classes, n_features)
         Number of samples encountered for each (class, feature)
         during fitting. This value is weighted by the sample weight when
         provided.

+    feature_log_prob_ : ndarray of shape (n_classes, n_features)
+        Empirical log probability of features given a class, P(x_i|y).
+
+    n_features_ : int
+        Number of features of each sample.
+
+        .. deprecated:: 1.0
+            Attribute `n_features_` was deprecated in version 1.0 and will be
+            removed in 1.2. Use `n_features_in_` instead.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    CategoricalNB : Naive Bayes classifier for categorical features.
+    ComplementNB : The Complement Naive Bayes classifier
+        described in Rennie et al. (2003).
+    GaussianNB : Gaussian Naive Bayes (GaussianNB).
+    MultinomialNB : Naive Bayes classifier for multinomial models.
+
+    References
+    ----------
+    C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to
+    Information Retrieval. Cambridge University Press, pp. 234-265.
+    https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html
+
+    A. McCallum and K. Nigam (1998). A comparison of event models for naive
+    Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for
+    Text Categorization, pp. 41-48.
+
+    V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with
+    naive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).
+
     Examples
     --------
     >>> import numpy as np
-    >>> X = np.random.randint(2, size=(6, 100))
+    >>> rng = np.random.RandomState(1)
+    >>> X = rng.randint(5, size=(6, 100))
     >>> Y = np.array([1, 2, 3, 4, 4, 5])
     >>> from sklearn.naive_bayes import BernoulliNB
     >>> clf = BernoulliNB()
     >>> clf.fit(X, Y)
-    BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)
+    BernoulliNB()
     >>> print(clf.predict(X[2:3]))
     [3]
-
-    References
-    ----------
-
-    C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to
-    Information Retrieval. Cambridge University Press, pp. 234-265.
-    https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html
-
-    A. McCallum and K. Nigam (1998). A comparison of event models for naive
-    Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for
-    Text Categorization, pp. 41-48.
-
-    V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with
-    naive Bayes -- Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).
     """

-    def __init__(self, alpha=1.0, binarize=.0, fit_prior=True,
-                 class_prior=None):
+    def __init__(self, *, alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None):
         self.alpha = alpha
         self.binarize = binarize
         self.fit_prior = fit_prior
         self.class_prior = class_prior

+    def _check_X(self, X):
+        """Validate X, used only in predict* methods."""
+        X = super()._check_X(X)
+        if self.binarize is not None:
+            X = binarize(X, threshold=self.binarize)
+        return X
+
+    def _check_X_y(self, X, y, reset=True):
+        X, y = super()._check_X_y(X, y, reset=reset)
+        if self.binarize is not None:
+            X = binarize(X, threshold=self.binarize)
+        return X, y
+
     def _count(self, X, Y):
         """Count and smooth feature occurrences."""
-        if self.binarize is not None:
-            X = binarize(X, threshold=self.binarize)
         self.feature_count_ += safe_sparse_dot(Y.T, X)
         self.class_count_ += Y.sum(axis=0)

@@ -933,24 +1129,20 @@
         smoothed_fc = self.feature_count_ + alpha
         smoothed_cc = self.class_count_ + alpha * 2

-        self.feature_log_prob_ = (np.log(smoothed_fc) -
-                                  np.log(smoothed_cc.reshape(-1, 1)))
+        self.feature_log_prob_ = np.log(smoothed_fc) - np.log(
+            smoothed_cc.reshape(-1, 1)
+        )

     def _joint_log_likelihood(self, X):
         """Calculate the posterior log probability of the samples X"""
-        check_is_fitted(self, "classes_")
-
-        X = check_array(X, accept_sparse='csr')
-
-        if self.binarize is not None:
-            X = binarize(X, threshold=self.binarize)
-
-        n_classes, n_features = self.feature_log_prob_.shape
-        n_samples, n_features_X = X.shape
+        n_features = self.feature_log_prob_.shape[1]
+        n_features_X = X.shape[1]

         if n_features_X != n_features:
-            raise ValueError("Expected input with %d features, got %d instead"
-                             % (n_features, n_features_X))
+            raise ValueError(
+                "Expected input with %d features, got %d instead"
+                % (n_features, n_features_X)
+            )

         neg_prob = np.log(1 - np.exp(self.feature_log_prob_))
         # Compute  neg_prob · (1 - X).T  as  ∑neg_prob - X · neg_prob
@@ -958,3 +1150,278 @@
         jll += self.class_log_prior_ + neg_prob.sum(axis=1)

         return jll
+
+
+class CategoricalNB(_BaseDiscreteNB):
+    """Naive Bayes classifier for categorical features.
+
+    The categorical Naive Bayes classifier is suitable for classification with
+    discrete features that are categorically distributed. The categories of
+    each feature are drawn from a categorical distribution.
+
+    Read more in the :ref:`User Guide <categorical_naive_bayes>`.
+
+    Parameters
+    ----------
+    alpha : float, default=1.0
+        Additive (Laplace/Lidstone) smoothing parameter
+        (0 for no smoothing).
+
+    fit_prior : bool, default=True
+        Whether to learn class prior probabilities or not.
+        If false, a uniform prior will be used.
+
+    class_prior : array-like of shape (n_classes,), default=None
+        Prior probabilities of the classes. If specified, the priors are not
+        adjusted according to the data.
+
+    min_categories : int or array-like of shape (n_features,), default=None
+        Minimum number of categories per feature.
+
+        - integer: Sets the minimum number of categories per feature to
+          `n_categories` for each features.
+        - array-like: shape (n_features,) where `n_categories[i]` holds the
+          minimum number of categories for the ith column of the input.
+        - None (default): Determines the number of categories automatically
+          from the training data.
+
+        .. versionadded:: 0.24
+
+    Attributes
+    ----------
+    category_count_ : list of arrays of shape (n_features,)
+        Holds arrays of shape (n_classes, n_categories of respective feature)
+        for each feature. Each array provides the number of samples
+        encountered for each class and category of the specific feature.
+
+    class_count_ : ndarray of shape (n_classes,)
+        Number of samples encountered for each class during fitting. This
+        value is weighted by the sample weight when provided.
+
+    class_log_prior_ : ndarray of shape (n_classes,)
+        Smoothed empirical log probability for each class.
+
+    classes_ : ndarray of shape (n_classes,)
+        Class labels known to the classifier
+
+    feature_log_prob_ : list of arrays of shape (n_features,)
+        Holds arrays of shape (n_classes, n_categories of respective feature)
+        for each feature. Each array provides the empirical log probability
+        of categories given the respective feature and class, ``P(x_i|y)``.
+
+    n_features_ : int
+        Number of features of each sample.
+
+        .. deprecated:: 1.0
+            Attribute `n_features_` was deprecated in version 1.0 and will be
+            removed in 1.2. Use `n_features_in_` instead.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    n_categories_ : ndarray of shape (n_features,), dtype=np.int64
+        Number of categories for each feature. This value is
+        inferred from the data or set by the minimum number of categories.
+
+        .. versionadded:: 0.24
+
+    See Also
+    --------
+    BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.
+    ComplementNB : Complement Naive Bayes classifier.
+    GaussianNB : Gaussian Naive Bayes.
+    MultinomialNB : Naive Bayes classifier for multinomial models.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> rng = np.random.RandomState(1)
+    >>> X = rng.randint(5, size=(6, 100))
+    >>> y = np.array([1, 2, 3, 4, 5, 6])
+    >>> from sklearn.naive_bayes import CategoricalNB
+    >>> clf = CategoricalNB()
+    >>> clf.fit(X, y)
+    CategoricalNB()
+    >>> print(clf.predict(X[2:3]))
+    [3]
+    """
+
+    def __init__(
+        self, *, alpha=1.0, fit_prior=True, class_prior=None, min_categories=None
+    ):
+        self.alpha = alpha
+        self.fit_prior = fit_prior
+        self.class_prior = class_prior
+        self.min_categories = min_categories
+
+    def fit(self, X, y, sample_weight=None):
+        """Fit Naive Bayes classifier according to X, y.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Training vectors, where `n_samples` is the number of samples and
+            `n_features` is the number of features. Here, each feature of X is
+            assumed to be from a different categorical distribution.
+            It is further assumed that all categories of each feature are
+            represented by the numbers 0, ..., n - 1, where n refers to the
+            total number of categories for the given feature. This can, for
+            instance, be achieved with the help of OrdinalEncoder.
+
+        y : array-like of shape (n_samples,)
+            Target values.
+
+        sample_weight : array-like of shape (n_samples,), default=None
+            Weights applied to individual samples (1. for unweighted).
+
+        Returns
+        -------
+        self : object
+            Returns the instance itself.
+        """
+        return super().fit(X, y, sample_weight=sample_weight)
+
+    def partial_fit(self, X, y, classes=None, sample_weight=None):
+        """Incremental fit on a batch of samples.
+
+        This method is expected to be called several times consecutively
+        on different chunks of a dataset so as to implement out-of-core
+        or online learning.
+
+        This is especially useful when the whole dataset is too big to fit in
+        memory at once.
+
+        This method has some performance overhead hence it is better to call
+        partial_fit on chunks of data that are as large as possible
+        (as long as fitting in the memory budget) to hide the overhead.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Training vectors, where `n_samples` is the number of samples and
+            `n_features` is the number of features. Here, each feature of X is
+            assumed to be from a different categorical distribution.
+            It is further assumed that all categories of each feature are
+            represented by the numbers 0, ..., n - 1, where n refers to the
+            total number of categories for the given feature. This can, for
+            instance, be achieved with the help of OrdinalEncoder.
+
+        y : array-like of shape (n_samples,)
+            Target values.
+
+        classes : array-like of shape (n_classes,), default=None
+            List of all the classes that can possibly appear in the y vector.
+
+            Must be provided at the first call to partial_fit, can be omitted
+            in subsequent calls.
+
+        sample_weight : array-like of shape (n_samples,), default=None
+            Weights applied to individual samples (1. for unweighted).
+
+        Returns
+        -------
+        self : object
+            Returns the instance itself.
+        """
+        return super().partial_fit(X, y, classes, sample_weight=sample_weight)
+
+    def _more_tags(self):
+        return {"requires_positive_X": True}
+
+    def _check_X(self, X):
+        """Validate X, used only in predict* methods."""
+        X = self._validate_data(
+            X, dtype="int", accept_sparse=False, force_all_finite=True, reset=False
+        )
+        check_non_negative(X, "CategoricalNB (input X)")
+        return X
+
+    def _check_X_y(self, X, y, reset=True):
+        X, y = self._validate_data(
+            X, y, dtype="int", accept_sparse=False, force_all_finite=True, reset=reset
+        )
+        check_non_negative(X, "CategoricalNB (input X)")
+        return X, y
+
+    def _init_counters(self, n_classes, n_features):
+        self.class_count_ = np.zeros(n_classes, dtype=np.float64)
+        self.category_count_ = [np.zeros((n_classes, 0)) for _ in range(n_features)]
+
+    @staticmethod
+    def _validate_n_categories(X, min_categories):
+        # rely on max for n_categories categories are encoded between 0...n-1
+        n_categories_X = X.max(axis=0) + 1
+        min_categories_ = np.array(min_categories)
+        if min_categories is not None:
+            if not np.issubdtype(min_categories_.dtype, np.signedinteger):
+                raise ValueError(
+                    "'min_categories' should have integral type. Got "
+                    f"{min_categories_.dtype} instead."
+                )
+            n_categories_ = np.maximum(n_categories_X, min_categories_, dtype=np.int64)
+            if n_categories_.shape != n_categories_X.shape:
+                raise ValueError(
+                    f"'min_categories' should have shape ({X.shape[1]},"
+                    ") when an array-like is provided. Got"
+                    f" {min_categories_.shape} instead."
+                )
+            return n_categories_
+        else:
+            return n_categories_X
+
+    def _count(self, X, Y):
+        def _update_cat_count_dims(cat_count, highest_feature):
+            diff = highest_feature + 1 - cat_count.shape[1]
+            if diff > 0:
+                # we append a column full of zeros for each new category
+                return np.pad(cat_count, [(0, 0), (0, diff)], "constant")
+            return cat_count
+
+        def _update_cat_count(X_feature, Y, cat_count, n_classes):
+            for j in range(n_classes):
+                mask = Y[:, j].astype(bool)
+                if Y.dtype.type == np.int64:
+                    weights = None
+                else:
+                    weights = Y[mask, j]
+                counts = np.bincount(X_feature[mask], weights=weights)
+                indices = np.nonzero(counts)[0]
+                cat_count[j, indices] += counts[indices]
+
+        self.class_count_ += Y.sum(axis=0)
+        self.n_categories_ = self._validate_n_categories(X, self.min_categories)
+        for i in range(self.n_features_in_):
+            X_feature = X[:, i]
+            self.category_count_[i] = _update_cat_count_dims(
+                self.category_count_[i], self.n_categories_[i] - 1
+            )
+            _update_cat_count(
+                X_feature, Y, self.category_count_[i], self.class_count_.shape[0]
+            )
+
+    def _update_feature_log_prob(self, alpha):
+        feature_log_prob = []
+        for i in range(self.n_features_in_):
+            smoothed_cat_count = self.category_count_[i] + alpha
+            smoothed_class_count = smoothed_cat_count.sum(axis=1)
+            feature_log_prob.append(
+                np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))
+            )
+        self.feature_log_prob_ = feature_log_prob
+
+    def _joint_log_likelihood(self, X):
+        self._check_n_features(X, reset=False)
+        jll = np.zeros((X.shape[0], self.class_count_.shape[0]))
+        for i in range(self.n_features_in_):
+            indices = X[:, i]
+            jll += self.feature_log_prob_[i][:, indices].T
+        total_ll = jll + self.class_log_prior_
+        return total_ll
('sklearn', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,87 +1,93 @@
+import sys
 import os

-from sklearn._build_utils import maybe_cythonize_extensions
+from sklearn._build_utils import cythonize_extensions


-def configuration(parent_package='', top_path=None):
+def configuration(parent_package="", top_path=None):
     from numpy.distutils.misc_util import Configuration
-    from numpy.distutils.system_info import get_info
     import numpy

-    # needs to be called during build otherwise show_version may fail sometimes
-    get_info('blas_opt', 0)
+    libraries = []
+    if os.name == "posix":
+        libraries.append("m")

-    libraries = []
-    if os.name == 'posix':
-        libraries.append('m')
-
-    config = Configuration('sklearn', parent_package, top_path)
+    config = Configuration("sklearn", parent_package, top_path)

     # submodules with build utilities
-    config.add_subpackage('__check_build')
-    config.add_subpackage('_build_utils')
+    config.add_subpackage("__check_build")
+    config.add_subpackage("_build_utils")

     # submodules which do not have their own setup.py
     # we must manually add sub-submodules & tests
-    config.add_subpackage('compose')
-    config.add_subpackage('compose/tests')
-    config.add_subpackage('covariance')
-    config.add_subpackage('covariance/tests')
-    config.add_subpackage('cross_decomposition')
-    config.add_subpackage('cross_decomposition/tests')
-    config.add_subpackage('feature_selection')
-    config.add_subpackage('feature_selection/tests')
-    config.add_subpackage('gaussian_process')
-    config.add_subpackage('gaussian_process/tests')
-    config.add_subpackage('impute')
-    config.add_subpackage('impute/tests')
-    config.add_subpackage('inspection')
-    config.add_subpackage('inspection/tests')
-    config.add_subpackage('mixture')
-    config.add_subpackage('mixture/tests')
-    config.add_subpackage('model_selection')
-    config.add_subpackage('model_selection/tests')
-    config.add_subpackage('neural_network')
-    config.add_subpackage('neural_network/tests')
-    config.add_subpackage('preprocessing')
-    config.add_subpackage('preprocessing/tests')
-    config.add_subpackage('semi_supervised')
-    config.add_subpackage('semi_supervised/tests')
-    config.add_subpackage('experimental')
-    config.add_subpackage('experimental/tests')
-    config.add_subpackage('ensemble/_hist_gradient_boosting')
-    config.add_subpackage('ensemble/_hist_gradient_boosting/tests')
+    config.add_subpackage("compose")
+    config.add_subpackage("compose/tests")
+    config.add_subpackage("covariance")
+    config.add_subpackage("covariance/tests")
+    config.add_subpackage("cross_decomposition")
+    config.add_subpackage("cross_decomposition/tests")
+    config.add_subpackage("feature_selection")
+    config.add_subpackage("feature_selection/tests")
+    config.add_subpackage("gaussian_process")
+    config.add_subpackage("gaussian_process/tests")
+    config.add_subpackage("impute")
+    config.add_subpackage("impute/tests")
+    config.add_subpackage("inspection")
+    config.add_subpackage("inspection/tests")
+    config.add_subpackage("mixture")
+    config.add_subpackage("mixture/tests")
+    config.add_subpackage("model_selection")
+    config.add_subpackage("model_selection/tests")
+    config.add_subpackage("neural_network")
+    config.add_subpackage("neural_network/tests")
+    config.add_subpackage("preprocessing")
+    config.add_subpackage("preprocessing/tests")
+    config.add_subpackage("semi_supervised")
+    config.add_subpackage("semi_supervised/tests")
+    config.add_subpackage("experimental")
+    config.add_subpackage("experimental/tests")
+    config.add_subpackage("ensemble/_hist_gradient_boosting")
+    config.add_subpackage("ensemble/_hist_gradient_boosting/tests")
+    config.add_subpackage("externals")
+    config.add_subpackage("externals/_packaging")

     # submodules which have their own setup.py
-    config.add_subpackage('cluster')
-    config.add_subpackage('datasets')
-    config.add_subpackage('decomposition')
-    config.add_subpackage('ensemble')
-    config.add_subpackage('externals')
-    config.add_subpackage('feature_extraction')
-    config.add_subpackage('manifold')
-    config.add_subpackage('metrics')
-    config.add_subpackage('neighbors')
-    config.add_subpackage('tree')
-    config.add_subpackage('utils')
-    config.add_subpackage('svm')
-    config.add_subpackage('linear_model')
+    config.add_subpackage("_loss")
+    config.add_subpackage("_loss/tests")
+    config.add_subpackage("cluster")
+    config.add_subpackage("datasets")
+    config.add_subpackage("decomposition")
+    config.add_subpackage("ensemble")
+    config.add_subpackage("feature_extraction")
+    config.add_subpackage("manifold")
+    config.add_subpackage("metrics")
+    config.add_subpackage("neighbors")
+    config.add_subpackage("tree")
+    config.add_subpackage("utils")
+    config.add_subpackage("svm")
+    config.add_subpackage("linear_model")

     # add cython extension module for isotonic regression
-    config.add_extension('_isotonic',
-                         sources=['_isotonic.pyx'],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries,
-                         )
+    config.add_extension(
+        "_isotonic",
+        sources=["_isotonic.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )

     # add the test directory
-    config.add_subpackage('tests')
+    config.add_subpackage("tests")

-    maybe_cythonize_extensions(top_path, config)
+    # Skip cythonization as we do not want to include the generated
+    # C/C++ files in the release tarballs as they are not necessarily
+    # forward compatible with future versions of Python for instance.
+    if "sdist" not in sys.argv:
+        cythonize_extensions(top_path, config)

     return config


-if __name__ == '__main__':
+if __name__ == "__main__":
     from numpy.distutils.core import setup
-    setup(**configuration(top_path='').todict())
+
+    setup(**configuration(top_path="").todict())
('sklearn', 'pipeline.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -14,45 +14,71 @@

 import numpy as np
 from scipy import sparse
+from joblib import Parallel

 from .base import clone, TransformerMixin
-from .utils._joblib import Parallel, delayed
-from .utils.metaestimators import if_delegate_has_method
-from .utils import Bunch, _print_elapsed_time
+from .preprocessing import FunctionTransformer
+from .utils._estimator_html_repr import _VisualBlock
+from .utils.metaestimators import available_if
+from .utils import (
+    Bunch,
+    _print_elapsed_time,
+)
+from .utils.deprecation import deprecated
+from .utils._tags import _safe_tags
 from .utils.validation import check_memory
+from .utils.validation import check_is_fitted
+from .utils.fixes import delayed
+from .exceptions import NotFittedError

 from .utils.metaestimators import _BaseComposition

-__all__ = ['Pipeline', 'FeatureUnion', 'make_pipeline', 'make_union']
+__all__ = ["Pipeline", "FeatureUnion", "make_pipeline", "make_union"]
+
+
+def _final_estimator_has(attr):
+    """Check that final_estimator has `attr`.
+
+    Used together with `avaliable_if` in `Pipeline`."""
+
+    def check(self):
+        # raise original `AttributeError` if `attr` does not exist
+        getattr(self._final_estimator, attr)
+        return True
+
+    return check


 class Pipeline(_BaseComposition):
-    """Pipeline of transforms with a final estimator.
+    """
+    Pipeline of transforms with a final estimator.

     Sequentially apply a list of transforms and a final estimator.
     Intermediate steps of the pipeline must be 'transforms', that is, they
-    must implement fit and transform methods.
-    The final estimator only needs to implement fit.
+    must implement `fit` and `transform` methods.
+    The final estimator only needs to implement `fit`.
     The transformers in the pipeline can be cached using ``memory`` argument.

     The purpose of the pipeline is to assemble several steps that can be
-    cross-validated together while setting different parameters.
-    For this, it enables setting parameters of the various steps using their
-    names and the parameter name separated by a '__', as in the example below.
-    A step's estimator may be replaced entirely by setting the parameter
-    with its name to another estimator, or a transformer removed by setting
-    it to 'passthrough' or ``None``.
+    cross-validated together while setting different parameters. For this, it
+    enables setting parameters of the various steps using their names and the
+    parameter name separated by a `'__'`, as in the example below. A step's
+    estimator may be replaced entirely by setting the parameter with its name
+    to another estimator, or a transformer removed by setting it to
+    `'passthrough'` or `None`.

     Read more in the :ref:`User Guide <pipeline>`.
+
+    .. versionadded:: 0.5

     Parameters
     ----------
-    steps : list
-        List of (name, transform) tuples (implementing fit/transform) that are
-        chained, in the order in which they are chained, with the last object
-        an estimator.
-
-    memory : None, str or object with the joblib.Memory interface, optional
+    steps : list of tuple
+        List of (name, transform) tuples (implementing `fit`/`transform`) that
+        are chained, in the order in which they are chained, with the last
+        object an estimator.
+
+    memory : str or object with the joblib.Memory interface, default=None
         Used to cache the fitted transformers of the pipeline. By default,
         no caching is performed. If a string is given, it is the path to
         the caching directory. Enabling caching triggers a clone of
@@ -62,86 +88,74 @@
         inspect estimators within the pipeline. Caching the
         transformers is advantageous when fitting is time consuming.

-    verbose : boolean, optional
+    verbose : bool, default=False
         If True, the time elapsed while fitting each step will be printed as it
         is completed.

     Attributes
     ----------
-    named_steps : bunch object, a dictionary with attribute access
+    named_steps : :class:`~sklearn.utils.Bunch`
+        Dictionary-like object, with the following attributes.
         Read-only attribute to access any step parameter by user given name.
         Keys are step names and values are steps parameters.

-    See also
+    classes_ : ndarray of shape (n_classes,)
+        The classes labels. Only exist if the last step of the pipeline is a
+        classifier.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`. Only defined if the
+        underlying first estimator in `steps` exposes such an attribute
+        when fit.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Only defined if the
+        underlying estimator exposes such an attribute when fit.
+
+        .. versionadded:: 1.0
+
+    See Also
     --------
-    sklearn.pipeline.make_pipeline : convenience function for simplified
-        pipeline construction.
+    make_pipeline : Convenience function for simplified pipeline construction.

     Examples
     --------
-    >>> from sklearn import svm
-    >>> from sklearn.datasets import samples_generator
-    >>> from sklearn.feature_selection import SelectKBest
-    >>> from sklearn.feature_selection import f_regression
+    >>> from sklearn.svm import SVC
+    >>> from sklearn.preprocessing import StandardScaler
+    >>> from sklearn.datasets import make_classification
+    >>> from sklearn.model_selection import train_test_split
     >>> from sklearn.pipeline import Pipeline
-    >>> # generate some data to play with
-    >>> X, y = samples_generator.make_classification(
-    ...     n_informative=5, n_redundant=0, random_state=42)
-    >>> # ANOVA SVM-C
-    >>> anova_filter = SelectKBest(f_regression, k=5)
-    >>> clf = svm.SVC(kernel='linear')
-    >>> anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])
-    >>> # You can set the parameters using the names issued
-    >>> # For instance, fit using a k of 10 in the SelectKBest
-    >>> # and a parameter 'C' of the svm
-    >>> anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)
-    ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
-    Pipeline(memory=None,
-             steps=[('anova', SelectKBest(...)),
-                    ('svc', SVC(...))], verbose=False)
-    >>> prediction = anova_svm.predict(X)
-    >>> anova_svm.score(X, y)  # doctest: +ELLIPSIS
-    0.83
-    >>> # getting the selected features chosen by anova_filter
-    >>> anova_svm['anova'].get_support()
-    ... # doctest: +NORMALIZE_WHITESPACE
-    array([False, False,  True,  True, False, False,  True,  True, False,
-           True, False,  True,  True, False,  True, False,  True,  True,
-           False, False])
-    >>> # Another way to get selected features chosen by anova_filter
-    >>> anova_svm.named_steps.anova.get_support()
-    ... # doctest: +NORMALIZE_WHITESPACE
-    array([False, False,  True,  True, False, False,  True,  True, False,
-           True, False,  True,  True, False,  True, False,  True,  True,
-           False, False])
-    >>> # Indexing can also be used to extract a sub-pipeline.
-    >>> sub_pipeline = anova_svm[:1]
-    >>> sub_pipeline  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
-    Pipeline(memory=None, steps=[('anova', ...)], verbose=False)
-    >>> coef = anova_svm[-1].coef_
-    >>> anova_svm['svc'] is anova_svm[-1]
-    True
-    >>> coef.shape
-    (1, 10)
-    >>> sub_pipeline.inverse_transform(coef).shape
-    (1, 20)
+    >>> X, y = make_classification(random_state=0)
+    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,
+    ...                                                     random_state=0)
+    >>> pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])
+    >>> # The pipeline can be used as any other estimator
+    >>> # and avoids leaking the test set into the train set
+    >>> pipe.fit(X_train, y_train)
+    Pipeline(steps=[('scaler', StandardScaler()), ('svc', SVC())])
+    >>> pipe.score(X_test, y_test)
+    0.88
     """

     # BaseEstimator interface
-    _required_parameters = ['steps']
-
-    def __init__(self, steps, memory=None, verbose=False):
+    _required_parameters = ["steps"]
+
+    def __init__(self, steps, *, memory=None, verbose=False):
         self.steps = steps
-        self._validate_steps()
         self.memory = memory
         self.verbose = verbose

     def get_params(self, deep=True):
         """Get parameters for this estimator.

-        Parameters
-        ----------
-        deep : boolean, optional
+        Returns the parameters given in the constructor as well as the
+        estimators contained within the `steps` of the `Pipeline`.
+
+        Parameters
+        ----------
+        deep : bool, default=True
             If True, will return the parameters for this estimator and
             contained subobjects that are estimators.

@@ -150,18 +164,28 @@
         params : mapping of string to any
             Parameter names mapped to their values.
         """
-        return self._get_params('steps', deep=deep)
+        return self._get_params("steps", deep=deep)

     def set_params(self, **kwargs):
         """Set the parameters of this estimator.

-        Valid parameter keys can be listed with ``get_params()``.
-
-        Returns
-        -------
-        self
-        """
-        self._set_params('steps', **kwargs)
+        Valid parameter keys can be listed with ``get_params()``. Note that
+        you can directly set the parameters of the estimators contained in
+        `steps`.
+
+        Parameters
+        ----------
+        **kwargs : dict
+            Parameters of this estimator or parameters of estimators contained
+            in `steps`. Parameters of the steps may be set using its name and
+            the parameter name separated by a '__'.
+
+        Returns
+        -------
+        self : object
+            Pipeline class instance.
+        """
+        self._set_params("steps", **kwargs)
         return self

     def _validate_steps(self):
@@ -175,22 +199,29 @@
         estimator = estimators[-1]

         for t in transformers:
-            if t is None or t == 'passthrough':
+            if t is None or t == "passthrough":
                 continue
-            if (not (hasattr(t, "fit") or hasattr(t, "fit_transform")) or not
-                    hasattr(t, "transform")):
-                raise TypeError("All intermediate steps should be "
-                                "transformers and implement fit and transform "
-                                "or be the string 'passthrough' "
-                                "'%s' (type %s) doesn't" % (t, type(t)))
+            if not (hasattr(t, "fit") or hasattr(t, "fit_transform")) or not hasattr(
+                t, "transform"
+            ):
+                raise TypeError(
+                    "All intermediate steps should be "
+                    "transformers and implement fit and transform "
+                    "or be the string 'passthrough' "
+                    "'%s' (type %s) doesn't" % (t, type(t))
+                )

         # We allow last estimator to be None as an identity transformation
-        if (estimator is not None and estimator != 'passthrough'
-                and not hasattr(estimator, "fit")):
+        if (
+            estimator is not None
+            and estimator != "passthrough"
+            and not hasattr(estimator, "fit")
+        ):
             raise TypeError(
                 "Last step of Pipeline should implement fit "
                 "or be the string 'passthrough'. "
-                "'%s' (type %s) doesn't" % (estimator, type(estimator)))
+                "'%s' (type %s) doesn't" % (estimator, type(estimator))
+            )

     def _iter(self, with_final=True, filter_passthrough=True):
         """
@@ -206,7 +237,7 @@
         for idx, (name, trans) in enumerate(islice(self.steps, 0, stop)):
             if not filter_passthrough:
                 yield idx, name, trans
-            elif trans is not None and trans != 'passthrough':
+            elif trans is not None and trans != "passthrough":
                 yield idx, name, trans

     def __len__(self):
@@ -216,7 +247,7 @@
         return len(self.steps)

     def __getitem__(self, ind):
-        """Returns a sub-pipeline or a single esimtator in the pipeline
+        """Returns a sub-pipeline or a single estimator in the pipeline

         Indexing with an integer will return an estimator; using a slice
         returns another Pipeline instance which copies a slice of this
@@ -226,8 +257,10 @@
         """
         if isinstance(ind, slice):
             if ind.step not in (1, None):
-                raise ValueError('Pipeline slicing only supports a step of 1')
-            return self.__class__(self.steps[ind])
+                raise ValueError("Pipeline slicing only supports a step of 1")
+            return self.__class__(
+                self.steps[ind], memory=self.memory, verbose=self.verbose
+            )
         try:
             name, est = self.steps[ind]
         except TypeError:
@@ -241,26 +274,43 @@

     @property
     def named_steps(self):
+        """Access the steps by name.
+
+        Read-only attribute to access any step by given name.
+        Keys are steps names and values are the steps objects."""
         # Use Bunch object to improve autocomplete
         return Bunch(**dict(self.steps))

     @property
     def _final_estimator(self):
         estimator = self.steps[-1][1]
-        return 'passthrough' if estimator is None else estimator
+        return "passthrough" if estimator is None else estimator

     def _log_message(self, step_idx):
         if not self.verbose:
             return None
-        name, step = self.steps[step_idx]
-
-        return '(step %d of %d) Processing %s' % (step_idx + 1,
-                                                  len(self.steps),
-                                                  name)
+        name, _ = self.steps[step_idx]
+
+        return "(step %d of %d) Processing %s" % (step_idx + 1, len(self.steps), name)
+
+    def _check_fit_params(self, **fit_params):
+        fit_params_steps = {name: {} for name, step in self.steps if step is not None}
+        for pname, pval in fit_params.items():
+            if "__" not in pname:
+                raise ValueError(
+                    "Pipeline.fit does not accept the {} parameter. "
+                    "You can pass parameters to specific steps of your "
+                    "pipeline using the stepname__parameter format, e.g. "
+                    "`Pipeline.fit(X, y, logisticregression__sample_weight"
+                    "=sample_weight)`.".format(pname)
+                )
+            step, param = pname.split("__", 1)
+            fit_params_steps[step][param] = pval
+        return fit_params_steps

     # Estimator interface

-    def _fit(self, X, y=None, **fit_params):
+    def _fit(self, X, y=None, **fit_params_steps):
         # shallow copy of steps - this should really be steps_
         self.steps = list(self.steps)
         self._validate_steps()
@@ -269,65 +319,40 @@

         fit_transform_one_cached = memory.cache(_fit_transform_one)

-        fit_params_steps = {name: {} for name, step in self.steps
-                            if step is not None}
-        for pname, pval in fit_params.items():
-            if '__' not in pname:
-                raise ValueError(
-                    "Pipeline.fit does not accept the {} parameter. "
-                    "You can pass parameters to specific steps of your "
-                    "pipeline using the stepname__parameter format, e.g. "
-                    "`Pipeline.fit(X, y, logisticregression__sample_weight"
-                    "=sample_weight)`.".format(pname))
-            step, param = pname.split('__', 1)
-            fit_params_steps[step][param] = pval
-        Xt = X
-        for (step_idx,
-             name,
-             transformer) in self._iter(with_final=False,
-                                        filter_passthrough=False):
-            if (transformer is None or transformer == 'passthrough'):
-                with _print_elapsed_time('Pipeline',
-                                         self._log_message(step_idx)):
+        for step_idx, name, transformer in self._iter(
+            with_final=False, filter_passthrough=False
+        ):
+            if transformer is None or transformer == "passthrough":
+                with _print_elapsed_time("Pipeline", self._log_message(step_idx)):
                     continue

-            if hasattr(memory, 'location'):
-                # joblib >= 0.12
-                if memory.location is None:
-                    # we do not clone when caching is disabled to
-                    # preserve backward compatibility
-                    cloned_transformer = transformer
-                else:
-                    cloned_transformer = clone(transformer)
-            elif hasattr(memory, 'cachedir'):
-                # joblib < 0.11
-                if memory.cachedir is None:
-                    # we do not clone when caching is disabled to
-                    # preserve backward compatibility
-                    cloned_transformer = transformer
-                else:
-                    cloned_transformer = clone(transformer)
+            if hasattr(memory, "location") and memory.location is None:
+                # we do not clone when caching is disabled to
+                # preserve backward compatibility
+                cloned_transformer = transformer
             else:
                 cloned_transformer = clone(transformer)
-            # Fit or load from cache the current transfomer
-            Xt, fitted_transformer = fit_transform_one_cached(
-                cloned_transformer, Xt, y, None,
-                message_clsname='Pipeline',
+            # Fit or load from cache the current transformer
+            X, fitted_transformer = fit_transform_one_cached(
+                cloned_transformer,
+                X,
+                y,
+                None,
+                message_clsname="Pipeline",
                 message=self._log_message(step_idx),
-                **fit_params_steps[name])
+                **fit_params_steps[name],
+            )
             # Replace the transformer of the step with the fitted
             # transformer. This is necessary when loading the transformer
             # from the cache.
             self.steps[step_idx] = (name, fitted_transformer)
-        if self._final_estimator == 'passthrough':
-            return Xt, {}
-        return Xt, fit_params_steps[self.steps[-1][0]]
+        return X

     def fit(self, X, y=None, **fit_params):
-        """Fit the model
-
-        Fit all the transforms one after the other and transform the
-        data, then fit the transformed data using the final estimator.
+        """Fit the model.
+
+        Fit all the transformers one after the other and transform the
+        data. Finally, fit the transformed data using the final estimator.

         Parameters
         ----------
@@ -346,21 +371,23 @@

         Returns
         -------
-        self : Pipeline
-            This estimator
-        """
-        Xt, fit_params = self._fit(X, y, **fit_params)
-        with _print_elapsed_time('Pipeline',
-                                 self._log_message(len(self.steps) - 1)):
-            if self._final_estimator != 'passthrough':
-                self._final_estimator.fit(Xt, y, **fit_params)
+        self : object
+            Pipeline with fitted steps.
+        """
+        fit_params_steps = self._check_fit_params(**fit_params)
+        Xt = self._fit(X, y, **fit_params_steps)
+        with _print_elapsed_time("Pipeline", self._log_message(len(self.steps) - 1)):
+            if self._final_estimator != "passthrough":
+                fit_params_last_step = fit_params_steps[self.steps[-1][0]]
+                self._final_estimator.fit(Xt, y, **fit_params_last_step)
+
         return self

     def fit_transform(self, X, y=None, **fit_params):
-        """Fit the model and transform with the final estimator
-
-        Fits all the transforms one after the other and transforms the
-        data, then uses fit_transform on transformed data with the final
+        """Fit the model and transform with the final estimator.
+
+        Fits all the transformers one after the other and transform the
+        data. Then uses `fit_transform` on transformed data with the final
         estimator.

         Parameters
@@ -380,23 +407,29 @@

         Returns
         -------
-        Xt : array-like, shape = [n_samples, n_transformed_features]
-            Transformed samples
-        """
+        Xt : ndarray of shape (n_samples, n_transformed_features)
+            Transformed samples.
+        """
+        fit_params_steps = self._check_fit_params(**fit_params)
+        Xt = self._fit(X, y, **fit_params_steps)
+
         last_step = self._final_estimator
-        Xt, fit_params = self._fit(X, y, **fit_params)
-        with _print_elapsed_time('Pipeline',
-                                 self._log_message(len(self.steps) - 1)):
-            if last_step == 'passthrough':
+        with _print_elapsed_time("Pipeline", self._log_message(len(self.steps) - 1)):
+            if last_step == "passthrough":
                 return Xt
-            if hasattr(last_step, 'fit_transform'):
-                return last_step.fit_transform(Xt, y, **fit_params)
+            fit_params_last_step = fit_params_steps[self.steps[-1][0]]
+            if hasattr(last_step, "fit_transform"):
+                return last_step.fit_transform(Xt, y, **fit_params_last_step)
             else:
-                return last_step.fit(Xt, y, **fit_params).transform(Xt)
-
-    @if_delegate_has_method(delegate='_final_estimator')
+                return last_step.fit(Xt, y, **fit_params_last_step).transform(Xt)
+
+    @available_if(_final_estimator_has("predict"))
     def predict(self, X, **predict_params):
-        """Apply transforms to the data, and predict with the final estimator
+        """Transform the data, and apply `predict` with the final estimator.
+
+        Call `transform` of each transformer in the pipeline. The transformed
+        data are finally passed to the final estimator that calls `predict`
+        method. Only valid if the final estimator implements `predict`.

         Parameters
         ----------
@@ -412,22 +445,26 @@
             transformations in the pipeline are not propagated to the
             final estimator.

-        Returns
-        -------
-        y_pred : array-like
+            .. versionadded:: 0.20
+
+        Returns
+        -------
+        y_pred : ndarray
+            Result of calling `predict` on the final estimator.
         """
         Xt = X
         for _, name, transform in self._iter(with_final=False):
             Xt = transform.transform(Xt)
-        return self.steps[-1][-1].predict(Xt, **predict_params)
-
-    @if_delegate_has_method(delegate='_final_estimator')
+        return self.steps[-1][1].predict(Xt, **predict_params)
+
+    @available_if(_final_estimator_has("fit_predict"))
     def fit_predict(self, X, y=None, **fit_params):
-        """Applies fit_predict of last step in pipeline after transforms.
-
-        Applies fit_transforms of a pipeline to the data, followed by the
-        fit_predict method of the final estimator in the pipeline. Valid
-        only if the final estimator implements fit_predict.
+        """Transform the data, and apply `fit_predict` with the final estimator.
+
+        Call `fit_transform` of each transformer in the pipeline. The
+        transformed data are finally passed to the final estimator that calls
+        `fit_predict` method. Only valid if the final estimator implements
+        `fit_predict`.

         Parameters
         ----------
@@ -446,17 +483,25 @@

         Returns
         -------
-        y_pred : array-like
-        """
-        Xt, fit_params = self._fit(X, y, **fit_params)
-        with _print_elapsed_time('Pipeline',
-                                 self._log_message(len(self.steps) - 1)):
-            y_pred = self.steps[-1][-1].fit_predict(Xt, y, **fit_params)
+        y_pred : ndarray
+            Result of calling `fit_predict` on the final estimator.
+        """
+        fit_params_steps = self._check_fit_params(**fit_params)
+        Xt = self._fit(X, y, **fit_params_steps)
+
+        fit_params_last_step = fit_params_steps[self.steps[-1][0]]
+        with _print_elapsed_time("Pipeline", self._log_message(len(self.steps) - 1)):
+            y_pred = self.steps[-1][1].fit_predict(Xt, y, **fit_params_last_step)
         return y_pred

-    @if_delegate_has_method(delegate='_final_estimator')
-    def predict_proba(self, X):
-        """Apply transforms, and predict_proba of the final estimator
+    @available_if(_final_estimator_has("predict_proba"))
+    def predict_proba(self, X, **predict_proba_params):
+        """Transform the data, and apply `predict_proba` with the final estimator.
+
+        Call `transform` of each transformer in the pipeline. The transformed
+        data are finally passed to the final estimator that calls
+        `predict_proba` method. Only valid if the final estimator implements
+        `predict_proba`.

         Parameters
         ----------
@@ -464,18 +509,28 @@
             Data to predict on. Must fulfill input requirements of first step
             of the pipeline.

-        Returns
-        -------
-        y_proba : array-like, shape = [n_samples, n_classes]
+        **predict_proba_params : dict of string -> object
+            Parameters to the `predict_proba` called at the end of all
+            transformations in the pipeline.
+
+        Returns
+        -------
+        y_proba : ndarray of shape (n_samples, n_classes)
+            Result of calling `predict_proba` on the final estimator.
         """
         Xt = X
         for _, name, transform in self._iter(with_final=False):
             Xt = transform.transform(Xt)
-        return self.steps[-1][-1].predict_proba(Xt)
-
-    @if_delegate_has_method(delegate='_final_estimator')
+        return self.steps[-1][1].predict_proba(Xt, **predict_proba_params)
+
+    @available_if(_final_estimator_has("decision_function"))
     def decision_function(self, X):
-        """Apply transforms, and decision_function of the final estimator
+        """Transform the data, and apply `decision_function` with the final estimator.
+
+        Call `transform` of each transformer in the pipeline. The transformed
+        data are finally passed to the final estimator that calls
+        `decision_function` method. Only valid if the final estimator
+        implements `decision_function`.

         Parameters
         ----------
@@ -485,16 +540,22 @@

         Returns
         -------
-        y_score : array-like, shape = [n_samples, n_classes]
+        y_score : ndarray of shape (n_samples, n_classes)
+            Result of calling `decision_function` on the final estimator.
         """
         Xt = X
         for _, name, transform in self._iter(with_final=False):
             Xt = transform.transform(Xt)
-        return self.steps[-1][-1].decision_function(Xt)
-
-    @if_delegate_has_method(delegate='_final_estimator')
-    def predict_log_proba(self, X):
-        """Apply transforms, and predict_log_proba of the final estimator
+        return self.steps[-1][1].decision_function(Xt)
+
+    @available_if(_final_estimator_has("score_samples"))
+    def score_samples(self, X):
+        """Transform the data, and apply `score_samples` with the final estimator.
+
+        Call `transform` of each transformer in the pipeline. The transformed
+        data are finally passed to the final estimator that calls
+        `score_samples` method. Only valid if the final estimator implements
+        `score_samples`.

         Parameters
         ----------
@@ -504,18 +565,58 @@

         Returns
         -------
-        y_score : array-like, shape = [n_samples, n_classes]
+        y_score : ndarray of shape (n_samples,)
+            Result of calling `score_samples` on the final estimator.
+        """
+        Xt = X
+        for _, _, transformer in self._iter(with_final=False):
+            Xt = transformer.transform(Xt)
+        return self.steps[-1][1].score_samples(Xt)
+
+    @available_if(_final_estimator_has("predict_log_proba"))
+    def predict_log_proba(self, X, **predict_log_proba_params):
+        """Transform the data, and apply `predict_log_proba` with the final estimator.
+
+        Call `transform` of each transformer in the pipeline. The transformed
+        data are finally passed to the final estimator that calls
+        `predict_log_proba` method. Only valid if the final estimator
+        implements `predict_log_proba`.
+
+        Parameters
+        ----------
+        X : iterable
+            Data to predict on. Must fulfill input requirements of first step
+            of the pipeline.
+
+        **predict_log_proba_params : dict of string -> object
+            Parameters to the ``predict_log_proba`` called at the end of all
+            transformations in the pipeline.
+
+        Returns
+        -------
+        y_log_proba : ndarray of shape (n_samples, n_classes)
+            Result of calling `predict_log_proba` on the final estimator.
         """
         Xt = X
         for _, name, transform in self._iter(with_final=False):
             Xt = transform.transform(Xt)
-        return self.steps[-1][-1].predict_log_proba(Xt)
-
-    @property
-    def transform(self):
-        """Apply transforms, and transform with the final estimator
-
-        This also works where final estimator is ``None``: all prior
+        return self.steps[-1][1].predict_log_proba(Xt, **predict_log_proba_params)
+
+    def _can_transform(self):
+        return self._final_estimator == "passthrough" or hasattr(
+            self._final_estimator, "transform"
+        )
+
+    @available_if(_can_transform)
+    def transform(self, X):
+        """Transform the data, and apply `transform` with the final estimator.
+
+        Call `transform` of each transformer in the pipeline. The transformed
+        data are finally passed to the final estimator that calls
+        `transform` method. Only valid if the final estimator
+        implements `transform`.
+
+        This also works where final estimator is `None` in which case all prior
         transformations are applied.

         Parameters
@@ -526,29 +627,26 @@

         Returns
         -------
-        Xt : array-like, shape = [n_samples, n_transformed_features]
-        """
-        # _final_estimator is None or has transform, otherwise attribute error
-        # XXX: Handling the None case means we can't use if_delegate_has_method
-        if self._final_estimator != 'passthrough':
-            self._final_estimator.transform
-        return self._transform
-
-    def _transform(self, X):
+        Xt : ndarray of shape (n_samples, n_transformed_features)
+            Transformed data.
+        """
         Xt = X
         for _, _, transform in self._iter():
             Xt = transform.transform(Xt)
         return Xt

-    @property
-    def inverse_transform(self):
-        """Apply inverse transformations in reverse order
-
-        All estimators in the pipeline must support ``inverse_transform``.
-
-        Parameters
-        ----------
-        Xt : array-like, shape = [n_samples, n_transformed_features]
+    def _can_inverse_transform(self):
+        return all(hasattr(t, "inverse_transform") for _, _, t in self._iter())
+
+    @available_if(_can_inverse_transform)
+    def inverse_transform(self, Xt):
+        """Apply `inverse_transform` for each step in a reverse order.
+
+        All estimators in the pipeline must support `inverse_transform`.
+
+        Parameters
+        ----------
+        Xt : array-like of shape (n_samples, n_transformed_features)
             Data samples, where ``n_samples`` is the number of samples and
             ``n_features`` is the number of features. Must fulfill
             input requirements of last step of pipeline's
@@ -556,24 +654,22 @@

         Returns
         -------
-        Xt : array-like, shape = [n_samples, n_features]
-        """
-        # raise AttributeError if necessary for hasattr behaviour
-        # XXX: Handling the None case means we can't use if_delegate_has_method
-        for _, _, transform in self._iter():
-            transform.inverse_transform
-        return self._inverse_transform
-
-    def _inverse_transform(self, X):
-        Xt = X
+        Xt : ndarray of shape (n_samples, n_features)
+            Inverse transformed data, that is, data in the original feature
+            space.
+        """
         reverse_iter = reversed(list(self._iter()))
         for _, _, transform in reverse_iter:
             Xt = transform.inverse_transform(Xt)
         return Xt

-    @if_delegate_has_method(delegate='_final_estimator')
+    @available_if(_final_estimator_has("score"))
     def score(self, X, y=None, sample_weight=None):
-        """Apply transforms, and score with the final estimator
+        """Transform the data, and apply `score` with the final estimator.
+
+        Call `transform` of each transformer in the pipeline. The transformed
+        data are finally passed to the final estimator that calls
+        `score` method. Only valid if the final estimator implements `score`.

         Parameters
         ----------
@@ -592,31 +688,100 @@
         Returns
         -------
         score : float
+            Result of calling `score` on the final estimator.
         """
         Xt = X
         for _, name, transform in self._iter(with_final=False):
             Xt = transform.transform(Xt)
         score_params = {}
         if sample_weight is not None:
-            score_params['sample_weight'] = sample_weight
-        return self.steps[-1][-1].score(Xt, y, **score_params)
+            score_params["sample_weight"] = sample_weight
+        return self.steps[-1][1].score(Xt, y, **score_params)

     @property
     def classes_(self):
-        return self.steps[-1][-1].classes_
+        """The classes labels. Only exist if the last step is a classifier."""
+        return self.steps[-1][1].classes_
+
+    def _more_tags(self):
+        # check if first estimator expects pairwise input
+        return {"pairwise": _safe_tags(self.steps[0][1], "pairwise")}
+
+    def get_feature_names_out(self, input_features=None):
+        """Get output feature names for transformation.
+
+        Transform input features using the pipeline.
+
+        Parameters
+        ----------
+        input_features : array-like of str or None, default=None
+            Input features.
+
+        Returns
+        -------
+        feature_names_out : ndarray of str objects
+            Transformed feature names.
+        """
+        feature_names_out = input_features
+        for _, name, transform in self._iter():
+            if not hasattr(transform, "get_feature_names_out"):
+                raise AttributeError(
+                    "Estimator {} does not provide get_feature_names_out. "
+                    "Did you mean to call pipeline[:-1].get_feature_names_out"
+                    "()?".format(name)
+                )
+            feature_names_out = transform.get_feature_names_out(feature_names_out)
+        return feature_names_out

     @property
-    def _pairwise(self):
-        # check if first estimator expects pairwise input
-        return getattr(self.steps[0][1], '_pairwise', False)
+    def n_features_in_(self):
+        """Number of features seen during first step `fit` method."""
+        # delegate to first step (which will call _check_is_fitted)
+        return self.steps[0][1].n_features_in_
+
+    @property
+    def feature_names_in_(self):
+        """Names of features seen during first step `fit` method."""
+        # delegate to first step (which will call _check_is_fitted)
+        return self.steps[0][1].feature_names_in_
+
+    def __sklearn_is_fitted__(self):
+        """Indicate whether pipeline has been fit."""
+        try:
+            # check if the last step of the pipeline is fitted
+            # we only check the last step since if the last step is fit, it
+            # means the previous steps should also be fit. This is faster than
+            # checking if every step of the pipeline is fit.
+            check_is_fitted(self.steps[-1][1])
+            return True
+        except NotFittedError:
+            return False
+
+    def _sk_visual_block_(self):
+        _, estimators = zip(*self.steps)
+
+        def _get_name(name, est):
+            if est is None or est == "passthrough":
+                return f"{name}: passthrough"
+            # Is an estimator
+            return f"{name}: {est.__class__.__name__}"
+
+        names = [_get_name(name, est) for name, est in self.steps]
+        name_details = [str(est) for est in estimators]
+        return _VisualBlock(
+            "serial",
+            estimators,
+            names=names,
+            name_details=name_details,
+            dash_wrapped=False,
+        )


 def _name_estimators(estimators):
     """Generate names for estimators."""

     names = [
-        estimator
-        if isinstance(estimator, str) else type(estimator).__name__.lower()
+        estimator if isinstance(estimator, str) else type(estimator).__name__.lower()
         for estimator in estimators
     ]
     namecount = defaultdict(int)
@@ -636,18 +801,19 @@
     return list(zip(names, estimators))


-def make_pipeline(*steps, **kwargs):
-    """Construct a Pipeline from the given estimators.
-
-    This is a shorthand for the Pipeline constructor; it does not require, and
-    does not permit, naming the estimators. Instead, their names will be set
-    to the lowercase of their types automatically.
+def make_pipeline(*steps, memory=None, verbose=False):
+    """Construct a :class:`Pipeline` from the given estimators.
+
+    This is a shorthand for the :class:`Pipeline` constructor; it does not
+    require, and does not permit, naming the estimators. Instead, their names
+    will be set to the lowercase of their types automatically.

     Parameters
     ----------
-    *steps : list of estimators.
-
-    memory : None, str or object with the joblib.Memory interface, optional
+    *steps : list of Estimator objects
+        List of the scikit-learn estimators that are chained together.
+
+    memory : str or object with the joblib.Memory interface, default=None
         Used to cache the fitted transformers of the pipeline. By default,
         no caching is performed. If a string is given, it is the path to
         the caching directory. Enabling caching triggers a clone of
@@ -657,37 +823,29 @@
         inspect estimators within the pipeline. Caching the
         transformers is advantageous when fitting is time consuming.

-    verbose : boolean, optional
+    verbose : bool, default=False
         If True, the time elapsed while fitting each step will be printed as it
         is completed.

-    See also
+    Returns
+    -------
+    p : Pipeline
+        Returns a scikit-learn :class:`Pipeline` object.
+
+    See Also
     --------
-    sklearn.pipeline.Pipeline : Class for creating a pipeline of
-        transforms with a final estimator.
+    Pipeline : Class for creating a pipeline of transforms with a final
+        estimator.

     Examples
     --------
     >>> from sklearn.naive_bayes import GaussianNB
     >>> from sklearn.preprocessing import StandardScaler
+    >>> from sklearn.pipeline import make_pipeline
     >>> make_pipeline(StandardScaler(), GaussianNB(priors=None))
-    ... # doctest: +NORMALIZE_WHITESPACE
-    Pipeline(memory=None,
-             steps=[('standardscaler',
-                     StandardScaler(copy=True, with_mean=True, with_std=True)),
-                    ('gaussiannb',
-                     GaussianNB(priors=None, var_smoothing=1e-09))],
-             verbose=False)
-
-    Returns
-    -------
-    p : Pipeline
+    Pipeline(steps=[('standardscaler', StandardScaler()),
+                    ('gaussiannb', GaussianNB())])
     """
-    memory = kwargs.pop('memory', None)
-    verbose = kwargs.pop('verbose', False)
-    if kwargs:
-        raise TypeError('Unknown keyword arguments: "{}"'
-                        .format(list(kwargs.keys())[0]))
     return Pipeline(_name_estimators(steps), memory=memory, verbose=verbose)


@@ -699,20 +857,16 @@
     return res * weight


-def _fit_transform_one(transformer,
-                       X,
-                       y,
-                       weight,
-                       message_clsname='',
-                       message=None,
-                       **fit_params):
+def _fit_transform_one(
+    transformer, X, y, weight, message_clsname="", message=None, **fit_params
+):
     """
     Fits ``transformer`` to ``X`` and ``y``. The transformed result is returned
     with the fitted transformer. If ``weight`` is not ``None``, the result will
     be multiplied by ``weight``.
     """
     with _print_elapsed_time(message_clsname, message):
-        if hasattr(transformer, 'fit_transform'):
+        if hasattr(transformer, "fit_transform"):
             res = transformer.fit_transform(X, y, **fit_params)
         else:
             res = transformer.fit(X, y, **fit_params).transform(X)
@@ -722,13 +876,7 @@
     return res * weight, transformer


-def _fit_one(transformer,
-             X,
-             y,
-             weight,
-             message_clsname='',
-             message=None,
-             **fit_params):
+def _fit_one(transformer, X, y, weight, message_clsname="", message=None, **fit_params):
     """
     Fits ``transformer`` to ``X`` and ``y``.
     """
@@ -736,7 +884,7 @@
         return transformer.fit(X, y, **fit_params)


-class FeatureUnion(_BaseComposition, TransformerMixin):
+class FeatureUnion(TransformerMixin, _BaseComposition):
     """Concatenates results of multiple transformer objects.

     This estimator applies a list of transformer objects in parallel to the
@@ -745,35 +893,59 @@

     Parameters of the transformers may be set using its name and the parameter
     name separated by a '__'. A transformer may be replaced entirely by
-    setting the parameter with its name to another transformer,
-    or removed by setting to 'drop' or ``None``.
+    setting the parameter with its name to another transformer, removed by
+    setting to 'drop' or disabled by setting to 'passthrough' (features are
+    passed without transformation).

     Read more in the :ref:`User Guide <feature_union>`.
+
+    .. versionadded:: 0.13

     Parameters
     ----------
-    transformer_list : list of (string, transformer) tuples
+    transformer_list : list of (str, transformer) tuples
         List of transformer objects to be applied to the data. The first
-        half of each tuple is the name of the transformer.
-
-    n_jobs : int or None, optional (default=None)
+        half of each tuple is the name of the transformer. The transformer can
+        be 'drop' for it to be ignored or can be 'passthrough' for features to
+        be passed unchanged.
+
+        .. versionadded:: 1.1
+           Added the option `"passthrough"`.
+
+        .. versionchanged:: 0.22
+           Deprecated `None` as a transformer in favor of 'drop'.
+
+    n_jobs : int, default=None
         Number of jobs to run in parallel.
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.

-    transformer_weights : dict, optional
+        .. versionchanged:: v0.20
+           `n_jobs` default changed from 1 to None
+
+    transformer_weights : dict, default=None
         Multiplicative weights for features per transformer.
         Keys are transformer names, values the weights.
-
-    verbose : boolean, optional(default=False)
+        Raises ValueError if key not present in ``transformer_list``.
+
+    verbose : bool, default=False
         If True, the time elapsed while fitting each transformer will be
         printed as it is completed.

-    See also
+    Attributes
+    ----------
+    n_features_in_ : int
+        Number of features seen during :term:`fit`. Only defined if the
+        underlying first transformer in `transformer_list` exposes such an
+        attribute when fit.
+
+        .. versionadded:: 0.24
+
+    See Also
     --------
-    sklearn.pipeline.make_union : convenience function for simplified
-        feature union construction.
+    make_union : Convenience function for simplified feature union
+        construction.

     Examples
     --------
@@ -782,26 +954,31 @@
     >>> union = FeatureUnion([("pca", PCA(n_components=1)),
     ...                       ("svd", TruncatedSVD(n_components=2))])
     >>> X = [[0., 1., 3], [2., 2., 5]]
-    >>> union.fit_transform(X)  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
+    >>> union.fit_transform(X)
     array([[ 1.5       ,  3.0...,  0.8...],
            [-1.5       ,  5.7..., -0.4...]])
     """
+
     _required_parameters = ["transformer_list"]

-    def __init__(self, transformer_list, n_jobs=None,
-                 transformer_weights=None, verbose=False):
+    def __init__(
+        self, transformer_list, *, n_jobs=None, transformer_weights=None, verbose=False
+    ):
         self.transformer_list = transformer_list
         self.n_jobs = n_jobs
         self.transformer_weights = transformer_weights
         self.verbose = verbose
-        self._validate_transformers()

     def get_params(self, deep=True):
         """Get parameters for this estimator.

-        Parameters
-        ----------
-        deep : boolean, optional
+        Returns the parameters given in the constructor as well as the
+        estimators contained within the `transformer_list` of the
+        `FeatureUnion`.
+
+        Parameters
+        ----------
+        deep : bool, default=True
             If True, will return the parameters for this estimator and
             contained subobjects that are estimators.

@@ -810,18 +987,28 @@
         params : mapping of string to any
             Parameter names mapped to their values.
         """
-        return self._get_params('transformer_list', deep=deep)
+        return self._get_params("transformer_list", deep=deep)

     def set_params(self, **kwargs):
         """Set the parameters of this estimator.

-        Valid parameter keys can be listed with ``get_params()``.
-
-        Returns
-        -------
-        self
-        """
-        self._set_params('transformer_list', **kwargs)
+        Valid parameter keys can be listed with ``get_params()``. Note that
+        you can directly set the parameters of the estimators contained in
+        `tranformer_list`.
+
+        Parameters
+        ----------
+        **kwargs : dict
+            Parameters of this estimator or parameters of estimators contained
+            in `transform_list`. Parameters of the transformers may be set
+            using its name and the parameter name separated by a '__'.
+
+        Returns
+        -------
+        self : object
+            FeatureUnion class instance.
+        """
+        self._set_params("transformer_list", **kwargs)
         return self

     def _validate_transformers(self):
@@ -832,24 +1019,47 @@

         # validate estimators
         for t in transformers:
-            if t is None or t == 'drop':
+            if t in ("drop", "passthrough"):
                 continue
-            if (not (hasattr(t, "fit") or hasattr(t, "fit_transform")) or not
-                    hasattr(t, "transform")):
-                raise TypeError("All estimators should implement fit and "
-                                "transform. '%s' (type %s) doesn't" %
-                                (t, type(t)))
+            if not (hasattr(t, "fit") or hasattr(t, "fit_transform")) or not hasattr(
+                t, "transform"
+            ):
+                raise TypeError(
+                    "All estimators should implement fit and "
+                    "transform. '%s' (type %s) doesn't" % (t, type(t))
+                )
+
+    def _validate_transformer_weights(self):
+        if not self.transformer_weights:
+            return
+
+        transformer_names = set(name for name, _ in self.transformer_list)
+        for name in self.transformer_weights:
+            if name not in transformer_names:
+                raise ValueError(
+                    f'Attempting to weight transformer "{name}", '
+                    "but it is not present in transformer_list."
+                )

     def _iter(self):
         """
         Generate (name, trans, weight) tuples excluding None and
         'drop' transformers.
         """
+
         get_weight = (self.transformer_weights or {}).get
-        return ((name, trans, get_weight(name))
-                for name, trans in self.transformer_list
-                if trans is not None and trans != 'drop')
-
+
+        for name, trans in self.transformer_list:
+            if trans == "drop":
+                continue
+            if trans == "passthrough":
+                trans = FunctionTransformer()
+            yield (name, trans, get_weight(name))
+
+    @deprecated(
+        "get_feature_names is deprecated in 1.0 and will be removed "
+        "in 1.2. Please use get_feature_names_out instead."
+    )
     def get_feature_names(self):
         """Get feature names from all transformers.

@@ -860,15 +1070,40 @@
         """
         feature_names = []
         for name, trans, weight in self._iter():
-            if not hasattr(trans, 'get_feature_names'):
-                raise AttributeError("Transformer %s (type %s) does not "
-                                     "provide get_feature_names."
-                                     % (str(name), type(trans).__name__))
-            feature_names.extend([name + "__" + f for f in
-                                  trans.get_feature_names()])
+            if not hasattr(trans, "get_feature_names"):
+                raise AttributeError(
+                    "Transformer %s (type %s) does not provide get_feature_names."
+                    % (str(name), type(trans).__name__)
+                )
+            feature_names.extend([name + "__" + f for f in trans.get_feature_names()])
         return feature_names

-    def fit(self, X, y=None):
+    def get_feature_names_out(self, input_features=None):
+        """Get output feature names for transformation.
+
+        Parameters
+        ----------
+        input_features : array-like of str or None, default=None
+            Input features.
+
+        Returns
+        -------
+        feature_names_out : ndarray of str objects
+            Transformed feature names.
+        """
+        feature_names = []
+        for name, trans, _ in self._iter():
+            if not hasattr(trans, "get_feature_names_out"):
+                raise AttributeError(
+                    "Transformer %s (type %s) does not provide get_feature_names_out."
+                    % (str(name), type(trans).__name__)
+                )
+            feature_names.extend(
+                [f"{name}__{f}" for f in trans.get_feature_names_out(input_features)]
+            )
+        return np.asarray(feature_names, dtype=object)
+
+    def fit(self, X, y=None, **fit_params):
         """Fit all transformers using X.

         Parameters
@@ -876,15 +1111,18 @@
         X : iterable or array-like, depending on transformers
             Input data, used to fit transformers.

-        y : array-like, shape (n_samples, ...), optional
+        y : array-like of shape (n_samples, n_outputs), default=None
             Targets for supervised learning.

-        Returns
-        -------
-        self : FeatureUnion
-            This estimator
-        """
-        transformers = self._parallel_func(X, y, {}, _fit_one)
+        **fit_params : dict, default=None
+            Parameters to pass to the fit method of the estimator.
+
+        Returns
+        -------
+        self : object
+            FeatureUnion class instance.
+        """
+        transformers = self._parallel_func(X, y, fit_params, _fit_one)
         if not transformers:
             # All transformers are None
             return self
@@ -900,14 +1138,18 @@
         X : iterable or array-like, depending on transformers
             Input data to be transformed.

-        y : array-like, shape (n_samples, ...), optional
+        y : array-like of shape (n_samples, n_outputs), default=None
             Targets for supervised learning.

-        Returns
-        -------
-        X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)
-            hstack of results of transformers. sum_n_components is the
-            sum of n_components (output dimension) over transformers.
+        **fit_params : dict, default=None
+            Parameters to pass to the fit method of the estimator.
+
+        Returns
+        -------
+        X_t : array-like or sparse matrix of \
+                shape (n_samples, sum_n_components)
+            The `hstack` of results of transformers. `sum_n_components` is the
+            sum of `n_components` (output dimension) over transformers.
         """
         results = self._parallel_func(X, y, fit_params, _fit_transform_one)
         if not results:
@@ -917,65 +1159,93 @@
         Xs, transformers = zip(*results)
         self._update_transformer_list(transformers)

+        return self._hstack(Xs)
+
+    def _log_message(self, name, idx, total):
+        if not self.verbose:
+            return None
+        return "(step %d of %d) Processing %s" % (idx, total, name)
+
+    def _parallel_func(self, X, y, fit_params, func):
+        """Runs func in parallel on X and y"""
+        self.transformer_list = list(self.transformer_list)
+        self._validate_transformers()
+        self._validate_transformer_weights()
+        transformers = list(self._iter())
+
+        return Parallel(n_jobs=self.n_jobs)(
+            delayed(func)(
+                transformer,
+                X,
+                y,
+                weight,
+                message_clsname="FeatureUnion",
+                message=self._log_message(name, idx, len(transformers)),
+                **fit_params,
+            )
+            for idx, (name, transformer, weight) in enumerate(transformers, 1)
+        )
+
+    def transform(self, X):
+        """Transform X separately by each transformer, concatenate results.
+
+        Parameters
+        ----------
+        X : iterable or array-like, depending on transformers
+            Input data to be transformed.
+
+        Returns
+        -------
+        X_t : array-like or sparse matrix of \
+                shape (n_samples, sum_n_components)
+            The `hstack` of results of transformers. `sum_n_components` is the
+            sum of `n_components` (output dimension) over transformers.
+        """
+        Xs = Parallel(n_jobs=self.n_jobs)(
+            delayed(_transform_one)(trans, X, None, weight)
+            for name, trans, weight in self._iter()
+        )
+        if not Xs:
+            # All transformers are None
+            return np.zeros((X.shape[0], 0))
+
+        return self._hstack(Xs)
+
+    def _hstack(self, Xs):
         if any(sparse.issparse(f) for f in Xs):
             Xs = sparse.hstack(Xs).tocsr()
         else:
             Xs = np.hstack(Xs)
         return Xs

-    def _log_message(self, name, idx, total):
-        if not self.verbose:
-            return None
-        return '(step %d of %d) Processing %s' % (idx, total, name)
-
-    def _parallel_func(self, X, y, fit_params, func):
-        """Runs func in parallel on X and y"""
-        self.transformer_list = list(self.transformer_list)
-        self._validate_transformers()
-        transformers = list(self._iter())
-
-        return Parallel(n_jobs=self.n_jobs)(delayed(func)(
-            transformer, X, y, weight,
-            message_clsname='FeatureUnion',
-            message=self._log_message(name, idx, len(transformers)),
-            **fit_params) for idx, (name, transformer,
-                                    weight) in enumerate(transformers, 1))
-
-    def transform(self, X):
-        """Transform X separately by each transformer, concatenate results.
-
-        Parameters
-        ----------
-        X : iterable or array-like, depending on transformers
-            Input data to be transformed.
-
-        Returns
-        -------
-        X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)
-            hstack of results of transformers. sum_n_components is the
-            sum of n_components (output dimension) over transformers.
-        """
-        Xs = Parallel(n_jobs=self.n_jobs)(
-            delayed(_transform_one)(trans, X, None, weight)
-            for name, trans, weight in self._iter())
-        if not Xs:
-            # All transformers are None
-            return np.zeros((X.shape[0], 0))
-        if any(sparse.issparse(f) for f in Xs):
-            Xs = sparse.hstack(Xs).tocsr()
-        else:
-            Xs = np.hstack(Xs)
-        return Xs
-
     def _update_transformer_list(self, transformers):
         transformers = iter(transformers)
-        self.transformer_list[:] = [(name, old if old is None or old == 'drop'
-                                     else next(transformers))
-                                    for name, old in self.transformer_list]
-
-
-def make_union(*transformers, **kwargs):
-    """Construct a FeatureUnion from the given transformers.
+        self.transformer_list[:] = [
+            (name, old if old == "drop" else next(transformers))
+            for name, old in self.transformer_list
+        ]
+
+    @property
+    def n_features_in_(self):
+        """Number of features seen during :term:`fit`."""
+
+        # X is passed to all transformers so we just delegate to the first one
+        return self.transformer_list[0][1].n_features_in_
+
+    def __sklearn_is_fitted__(self):
+        # Delegate whether feature union was fitted
+        for _, transformer, _ in self._iter():
+            check_is_fitted(transformer)
+        return True
+
+    def _sk_visual_block_(self):
+        names, transformers = zip(*self.transformer_list)
+        return _VisualBlock("parallel", transformers, names=names)
+
+
+def make_union(*transformers, n_jobs=None, verbose=False):
+    """
+    Construct a FeatureUnion from the given transformers.

     This is a shorthand for the FeatureUnion constructor; it does not require,
     and does not permit, naming the transformers. Instead, they will be given
@@ -985,13 +1255,16 @@
     ----------
     *transformers : list of estimators

-    n_jobs : int or None, optional (default=None)
+    n_jobs : int, default=None
         Number of jobs to run in parallel.
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.

-    verbose : boolean, optional(default=False)
+        .. versionchanged:: v0.20
+           `n_jobs` default changed from 1 to None
+
+    verbose : bool, default=False
         If True, the time elapsed while fitting each transformer will be
         printed as it is completed.

@@ -999,33 +1272,17 @@
     -------
     f : FeatureUnion

-    See also
+    See Also
     --------
-    sklearn.pipeline.FeatureUnion : Class for concatenating the results
-        of multiple transformer objects.
+    FeatureUnion : Class for concatenating the results of multiple transformer
+        objects.

     Examples
     --------
     >>> from sklearn.decomposition import PCA, TruncatedSVD
     >>> from sklearn.pipeline import make_union
-    >>> make_union(PCA(), TruncatedSVD())  # doctest: +NORMALIZE_WHITESPACE
-    FeatureUnion(n_jobs=None,
-           transformer_list=[('pca',
-                              PCA(copy=True, iterated_power='auto',
-                                  n_components=None, random_state=None,
-                                  svd_solver='auto', tol=0.0, whiten=False)),
-                             ('truncatedsvd',
-                              TruncatedSVD(algorithm='randomized',
-                              n_components=2, n_iter=5,
-                              random_state=None, tol=0.0))],
-           transformer_weights=None, verbose=False)
+    >>> make_union(PCA(), TruncatedSVD())
+     FeatureUnion(transformer_list=[('pca', PCA()),
+                                   ('truncatedsvd', TruncatedSVD())])
     """
-    n_jobs = kwargs.pop('n_jobs', None)
-    verbose = kwargs.pop('verbose', False)
-    if kwargs:
-        # We do not currently support `transformer_weights` as we may want to
-        # change its type spec in make_union
-        raise TypeError('Unknown keyword arguments: "{}"'
-                        .format(list(kwargs.keys())[0]))
-    return FeatureUnion(
-        _name_estimators(transformers), n_jobs=n_jobs, verbose=verbose)
+    return FeatureUnion(_name_estimators(transformers), n_jobs=n_jobs, verbose=verbose)
('sklearn', 'discriminant_analysis.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -11,61 +11,87 @@

 import warnings
 import numpy as np
-from .exceptions import ChangedBehaviorWarning
 from scipy import linalg
 from scipy.special import expit
+from numbers import Real

 from .base import BaseEstimator, TransformerMixin, ClassifierMixin
-from .linear_model.base import LinearClassifierMixin
+from .base import _ClassNamePrefixFeaturesOutMixin
+from .linear_model._base import LinearClassifierMixin
 from .covariance import ledoit_wolf, empirical_covariance, shrunk_covariance
 from .utils.multiclass import unique_labels
-from .utils import check_array, check_X_y
 from .utils.validation import check_is_fitted
 from .utils.multiclass import check_classification_targets
 from .utils.extmath import softmax
 from .preprocessing import StandardScaler


-__all__ = ['LinearDiscriminantAnalysis', 'QuadraticDiscriminantAnalysis']
-
-
-def _cov(X, shrinkage=None):
-    """Estimate covariance matrix (using optional shrinkage).
-
+__all__ = ["LinearDiscriminantAnalysis", "QuadraticDiscriminantAnalysis"]
+
+
+def _cov(X, shrinkage=None, covariance_estimator=None):
+    """Estimate covariance matrix (using optional covariance_estimator).
     Parameters
     ----------
-    X : array-like, shape (n_samples, n_features)
+    X : array-like of shape (n_samples, n_features)
         Input data.

-    shrinkage : string or float, optional
+    shrinkage : {'empirical', 'auto'} or float, default=None
         Shrinkage parameter, possible values:
           - None or 'empirical': no shrinkage (default).
           - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.
           - float between 0 and 1: fixed shrinkage parameter.

+        Shrinkage parameter is ignored if  `covariance_estimator`
+        is not None.
+
+    covariance_estimator : estimator, default=None
+        If not None, `covariance_estimator` is used to estimate
+        the covariance matrices instead of relying on the empirical
+        covariance estimator (with potential shrinkage).
+        The object should have a fit method and a ``covariance_`` attribute
+        like the estimators in :mod:`sklearn.covariance``.
+        if None the shrinkage parameter drives the estimate.
+
+        .. versionadded:: 0.24
+
     Returns
     -------
-    s : array, shape (n_features, n_features)
+    s : ndarray of shape (n_features, n_features)
         Estimated covariance matrix.
     """
-    shrinkage = "empirical" if shrinkage is None else shrinkage
-    if isinstance(shrinkage, str):
-        if shrinkage == 'auto':
-            sc = StandardScaler()  # standardize features
-            X = sc.fit_transform(X)
-            s = ledoit_wolf(X)[0]
-            # rescale
-            s = sc.scale_[:, np.newaxis] * s * sc.scale_[np.newaxis, :]
-        elif shrinkage == 'empirical':
-            s = empirical_covariance(X)
+    if covariance_estimator is None:
+        shrinkage = "empirical" if shrinkage is None else shrinkage
+        if isinstance(shrinkage, str):
+            if shrinkage == "auto":
+                sc = StandardScaler()  # standardize features
+                X = sc.fit_transform(X)
+                s = ledoit_wolf(X)[0]
+                # rescale
+                s = sc.scale_[:, np.newaxis] * s * sc.scale_[np.newaxis, :]
+            elif shrinkage == "empirical":
+                s = empirical_covariance(X)
+            else:
+                raise ValueError("unknown shrinkage parameter")
+        elif isinstance(shrinkage, Real):
+            if shrinkage < 0 or shrinkage > 1:
+                raise ValueError("shrinkage parameter must be between 0 and 1")
+            s = shrunk_covariance(empirical_covariance(X), shrinkage)
         else:
-            raise ValueError('unknown shrinkage parameter')
-    elif isinstance(shrinkage, float) or isinstance(shrinkage, int):
-        if shrinkage < 0 or shrinkage > 1:
-            raise ValueError('shrinkage parameter must be between 0 and 1')
-        s = shrunk_covariance(empirical_covariance(X), shrinkage)
+            raise TypeError("shrinkage must be a float or a string")
     else:
-        raise TypeError('shrinkage must be of string or int type')
+        if shrinkage is not None and shrinkage != 0:
+            raise ValueError(
+                "covariance_estimator and shrinkage parameters "
+                "are not None. Only one of the two can be set."
+            )
+        covariance_estimator.fit(X)
+        if not hasattr(covariance_estimator, "covariance_"):
+            raise ValueError(
+                "%s does not have a covariance_ attribute"
+                % covariance_estimator.__class__.__name__
+            )
+        s = covariance_estimator.covariance_
     return s


@@ -74,15 +100,15 @@

     Parameters
     ----------
-    X : array-like, shape (n_samples, n_features)
+    X : array-like of shape (n_samples, n_features)
         Input data.

-    y : array-like, shape (n_samples,) or (n_samples, n_targets)
+    y : array-like of shape (n_samples,) or (n_samples, n_targets)
         Target values.

     Returns
     -------
-    means : array-like, shape (n_classes, n_features)
+    means : array-like of shape (n_classes, n_features)
         Class means.
     """
     classes, y = np.unique(y, return_inverse=True)
@@ -93,42 +119,60 @@
     return means


-def _class_cov(X, y, priors, shrinkage=None):
-    """Compute class covariance matrix.
+def _class_cov(X, y, priors, shrinkage=None, covariance_estimator=None):
+    """Compute weighted within-class covariance matrix.
+
+    The per-class covariance are weighted by the class priors.

     Parameters
     ----------
-    X : array-like, shape (n_samples, n_features)
+    X : array-like of shape (n_samples, n_features)
         Input data.

-    y : array-like, shape (n_samples,) or (n_samples, n_targets)
+    y : array-like of shape (n_samples,) or (n_samples, n_targets)
         Target values.

-    priors : array-like, shape (n_classes,)
+    priors : array-like of shape (n_classes,)
         Class priors.

-    shrinkage : string or float, optional
+    shrinkage : 'auto' or float, default=None
         Shrinkage parameter, possible values:
           - None: no shrinkage (default).
           - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.
           - float between 0 and 1: fixed shrinkage parameter.

+        Shrinkage parameter is ignored if `covariance_estimator` is not None.
+
+    covariance_estimator : estimator, default=None
+        If not None, `covariance_estimator` is used to estimate
+        the covariance matrices instead of relying the empirical
+        covariance estimator (with potential shrinkage).
+        The object should have a fit method and a ``covariance_`` attribute
+        like the estimators in sklearn.covariance.
+        If None, the shrinkage parameter drives the estimate.
+
+        .. versionadded:: 0.24
+
     Returns
     -------
-    cov : array-like, shape (n_features, n_features)
-        Class covariance matrix.
+    cov : array-like of shape (n_features, n_features)
+        Weighted within-class covariance matrix
     """
     classes = np.unique(y)
     cov = np.zeros(shape=(X.shape[1], X.shape[1]))
     for idx, group in enumerate(classes):
         Xg = X[y == group, :]
-        cov += priors[idx] * np.atleast_2d(_cov(Xg, shrinkage))
+        cov += priors[idx] * np.atleast_2d(_cov(Xg, shrinkage, covariance_estimator))
     return cov


-class LinearDiscriminantAnalysis(BaseEstimator, LinearClassifierMixin,
-                                 TransformerMixin):
-    """Linear Discriminant Analysis
+class LinearDiscriminantAnalysis(
+    _ClassNamePrefixFeaturesOutMixin,
+    LinearClassifierMixin,
+    TransformerMixin,
+    BaseEstimator,
+):
+    """Linear Discriminant Analysis.

     A classifier with a linear decision boundary, generated by fitting class
     conditional densities to the data and using Bayes' rule.
@@ -137,7 +181,8 @@
     share the same covariance matrix.

     The fitted model can also be used to reduce the dimensionality of the input
-    by projecting it to the most discriminative directions.
+    by projecting it to the most discriminative directions, using the
+    `transform` method.

     .. versionadded:: 0.17
        *LinearDiscriminantAnalysis*.
@@ -146,93 +191,115 @@

     Parameters
     ----------
-    solver : string, optional
+    solver : {'svd', 'lsqr', 'eigen'}, default='svd'
         Solver to use, possible values:
           - 'svd': Singular value decomposition (default).
             Does not compute the covariance matrix, therefore this solver is
             recommended for data with a large number of features.
-          - 'lsqr': Least squares solution, can be combined with shrinkage.
-          - 'eigen': Eigenvalue decomposition, can be combined with shrinkage.
-
-    shrinkage : string or float, optional
+          - 'lsqr': Least squares solution.
+            Can be combined with shrinkage or custom covariance estimator.
+          - 'eigen': Eigenvalue decomposition.
+            Can be combined with shrinkage or custom covariance estimator.
+
+    shrinkage : 'auto' or float, default=None
         Shrinkage parameter, possible values:
           - None: no shrinkage (default).
           - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.
           - float between 0 and 1: fixed shrinkage parameter.

+        This should be left to None if `covariance_estimator` is used.
         Note that shrinkage works only with 'lsqr' and 'eigen' solvers.

-    priors : array, optional, shape (n_classes,)
-        Class priors.
-
-    n_components : int, optional (default=None)
+    priors : array-like of shape (n_classes,), default=None
+        The class prior probabilities. By default, the class proportions are
+        inferred from the training data.
+
+    n_components : int, default=None
         Number of components (<= min(n_classes - 1, n_features)) for
         dimensionality reduction. If None, will be set to
-        min(n_classes - 1, n_features).
-
-    store_covariance : bool, optional
-        Additionally compute class covariance matrix (default False), used
-        only in 'svd' solver.
+        min(n_classes - 1, n_features). This parameter only affects the
+        `transform` method.
+
+    store_covariance : bool, default=False
+        If True, explicitly compute the weighted within-class covariance
+        matrix when solver is 'svd'. The matrix is always computed
+        and stored for the other solvers.

         .. versionadded:: 0.17

-    tol : float, optional, (default 1.0e-4)
-        Threshold used for rank estimation in SVD solver.
+    tol : float, default=1.0e-4
+        Absolute threshold for a singular value of X to be considered
+        significant, used to estimate the rank of X. Dimensions whose
+        singular values are non-significant are discarded. Only used if
+        solver is 'svd'.

         .. versionadded:: 0.17
+
+    covariance_estimator : covariance estimator, default=None
+        If not None, `covariance_estimator` is used to estimate
+        the covariance matrices instead of relying on the empirical
+        covariance estimator (with potential shrinkage).
+        The object should have a fit method and a ``covariance_`` attribute
+        like the estimators in :mod:`sklearn.covariance`.
+        if None the shrinkage parameter drives the estimate.
+
+        This should be left to None if `shrinkage` is used.
+        Note that `covariance_estimator` works only with 'lsqr' and 'eigen'
+        solvers.
+
+        .. versionadded:: 0.24

     Attributes
     ----------
-    coef_ : array, shape (n_features,) or (n_classes, n_features)
+    coef_ : ndarray of shape (n_features,) or (n_classes, n_features)
         Weight vector(s).

-    intercept_ : array, shape (n_features,)
+    intercept_ : ndarray of shape (n_classes,)
         Intercept term.

-    covariance_ : array-like, shape (n_features, n_features)
-        Covariance matrix (shared by all classes).
-
-    explained_variance_ratio_ : array, shape (n_components,)
+    covariance_ : array-like of shape (n_features, n_features)
+        Weighted within-class covariance matrix. It corresponds to
+        `sum_k prior_k * C_k` where `C_k` is the covariance matrix of the
+        samples in class `k`. The `C_k` are estimated using the (potentially
+        shrunk) biased estimator of covariance. If solver is 'svd', only
+        exists when `store_covariance` is True.
+
+    explained_variance_ratio_ : ndarray of shape (n_components,)
         Percentage of variance explained by each of the selected components.
         If ``n_components`` is not set then all components are stored and the
         sum of explained variances is equal to 1.0. Only available when eigen
         or svd solver is used.

-    means_ : array-like, shape (n_classes, n_features)
-        Class means.
-
-    priors_ : array-like, shape (n_classes,)
+    means_ : array-like of shape (n_classes, n_features)
+        Class-wise means.
+
+    priors_ : array-like of shape (n_classes,)
         Class priors (sum to 1).

-    scalings_ : array-like, shape (rank, n_classes - 1)
+    scalings_ : array-like of shape (rank, n_classes - 1)
         Scaling of the features in the space spanned by the class centroids.
-
-    xbar_ : array-like, shape (n_features,)
-        Overall mean.
-
-    classes_ : array-like, shape (n_classes,)
+        Only available for 'svd' and 'eigen' solvers.
+
+    xbar_ : array-like of shape (n_features,)
+        Overall mean. Only present if solver is 'svd'.
+
+    classes_ : array-like of shape (n_classes,)
         Unique class labels.

-    See also
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
     --------
-    sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis: Quadratic
-        Discriminant Analysis
-
-    Notes
-    -----
-    The default solver is 'svd'. It can perform both classification and
-    transform, and it does not rely on the calculation of the covariance
-    matrix. This can be an advantage in situations where the number of features
-    is large. However, the 'svd' solver cannot be used with shrinkage.
-
-    The 'lsqr' solver is an efficient algorithm that only works for
-    classification. It supports shrinkage.
-
-    The 'eigen' solver is based on the optimization of the between class
-    scatter to within class scatter ratio. It can be used for both
-    classification and transform, and it supports shrinkage. However, the
-    'eigen' solver needs to compute the covariance matrix, so it might not be
-    suitable for situations with a high number of features.
+    QuadraticDiscriminantAnalysis : Quadratic Discriminant Analysis.

     Examples
     --------
@@ -241,44 +308,66 @@
     >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
     >>> y = np.array([1, 1, 1, 2, 2, 2])
     >>> clf = LinearDiscriminantAnalysis()
-    >>> clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
-    LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,
-                  solver='svd', store_covariance=False, tol=0.0001)
+    >>> clf.fit(X, y)
+    LinearDiscriminantAnalysis()
     >>> print(clf.predict([[-0.8, -1]]))
     [1]
     """

-    def __init__(self, solver='svd', shrinkage=None, priors=None,
-                 n_components=None, store_covariance=False, tol=1e-4):
+    def __init__(
+        self,
+        solver="svd",
+        shrinkage=None,
+        priors=None,
+        n_components=None,
+        store_covariance=False,
+        tol=1e-4,
+        covariance_estimator=None,
+    ):
         self.solver = solver
         self.shrinkage = shrinkage
         self.priors = priors
         self.n_components = n_components
         self.store_covariance = store_covariance  # used only in svd solver
         self.tol = tol  # used only in svd solver
-
-    def _solve_lsqr(self, X, y, shrinkage):
+        self.covariance_estimator = covariance_estimator
+
+    def _solve_lsqr(self, X, y, shrinkage, covariance_estimator):
         """Least squares solver.

         The least squares solver computes a straightforward solution of the
         optimal decision rule based directly on the discriminant functions. It
-        can only be used for classification (with optional shrinkage), because
+        can only be used for classification (with any covariance estimator),
+        because
         estimation of eigenvectors is not performed. Therefore, dimensionality
         reduction with the transform is not supported.

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
+        X : array-like of shape (n_samples, n_features)
             Training data.

-        y : array-like, shape (n_samples,) or (n_samples, n_classes)
+        y : array-like of shape (n_samples,) or (n_samples, n_classes)
             Target values.

-        shrinkage : string or float, optional
+        shrinkage : 'auto', float or None
             Shrinkage parameter, possible values:
-              - None: no shrinkage (default).
+              - None: no shrinkage.
               - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.
               - float between 0 and 1: fixed shrinkage parameter.
+
+            Shrinkage parameter is ignored if  `covariance_estimator` i
+            not None
+
+        covariance_estimator : estimator, default=None
+            If not None, `covariance_estimator` is used to estimate
+            the covariance matrices instead of relying the empirical
+            covariance estimator (with potential shrinkage).
+            The object should have a fit method and a ``covariance_`` attribute
+            like the estimators in sklearn.covariance.
+            if None the shrinkage parameter drives the estimate.
+
+            .. versionadded:: 0.24

         Notes
         -----
@@ -291,32 +380,48 @@
            0-471-05669-3.
         """
         self.means_ = _class_means(X, y)
-        self.covariance_ = _class_cov(X, y, self.priors_, shrinkage)
+        self.covariance_ = _class_cov(
+            X, y, self.priors_, shrinkage, covariance_estimator
+        )
         self.coef_ = linalg.lstsq(self.covariance_, self.means_.T)[0].T
-        self.intercept_ = (-0.5 * np.diag(np.dot(self.means_, self.coef_.T)) +
-                           np.log(self.priors_))
-
-    def _solve_eigen(self, X, y, shrinkage):
+        self.intercept_ = -0.5 * np.diag(np.dot(self.means_, self.coef_.T)) + np.log(
+            self.priors_
+        )
+
+    def _solve_eigen(self, X, y, shrinkage, covariance_estimator):
         """Eigenvalue solver.

         The eigenvalue solver computes the optimal solution of the Rayleigh
         coefficient (basically the ratio of between class scatter to within
         class scatter). This solver supports both classification and
-        dimensionality reduction (with optional shrinkage).
-
-        Parameters
-        ----------
-        X : array-like, shape (n_samples, n_features)
+        dimensionality reduction (with any covariance estimator).
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
             Training data.

-        y : array-like, shape (n_samples,) or (n_samples, n_targets)
+        y : array-like of shape (n_samples,) or (n_samples, n_targets)
             Target values.

-        shrinkage : string or float, optional
+        shrinkage : 'auto', float or None
             Shrinkage parameter, possible values:
-              - None: no shrinkage (default).
+              - None: no shrinkage.
               - 'auto': automatic shrinkage using the Ledoit-Wolf lemma.
               - float between 0 and 1: fixed shrinkage constant.
+
+            Shrinkage parameter is ignored if  `covariance_estimator` i
+            not None
+
+        covariance_estimator : estimator, default=None
+            If not None, `covariance_estimator` is used to estimate
+            the covariance matrices instead of relying the empirical
+            covariance estimator (with potential shrinkage).
+            The object should have a fit method and a ``covariance_`` attribute
+            like the estimators in sklearn.covariance.
+            if None the shrinkage parameter drives the estimate.
+
+            .. versionadded:: 0.24

         Notes
         -----
@@ -329,31 +434,35 @@
            0-471-05669-3.
         """
         self.means_ = _class_means(X, y)
-        self.covariance_ = _class_cov(X, y, self.priors_, shrinkage)
+        self.covariance_ = _class_cov(
+            X, y, self.priors_, shrinkage, covariance_estimator
+        )

         Sw = self.covariance_  # within scatter
-        St = _cov(X, shrinkage)  # total scatter
+        St = _cov(X, shrinkage, covariance_estimator)  # total scatter
         Sb = St - Sw  # between scatter

         evals, evecs = linalg.eigh(Sb, Sw)
-        self.explained_variance_ratio_ = np.sort(evals / np.sum(evals)
-                                                 )[::-1][:self._max_components]
+        self.explained_variance_ratio_ = np.sort(evals / np.sum(evals))[::-1][
+            : self._max_components
+        ]
         evecs = evecs[:, np.argsort(evals)[::-1]]  # sort eigenvectors

         self.scalings_ = evecs
         self.coef_ = np.dot(self.means_, evecs).dot(evecs.T)
-        self.intercept_ = (-0.5 * np.diag(np.dot(self.means_, self.coef_.T)) +
-                           np.log(self.priors_))
+        self.intercept_ = -0.5 * np.diag(np.dot(self.means_, self.coef_.T)) + np.log(
+            self.priors_
+        )

     def _solve_svd(self, X, y):
         """SVD solver.

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
+        X : array-like of shape (n_samples, n_features)
             Training data.

-        y : array-like, shape (n_samples,) or (n_samples, n_targets)
+        y : array-like of shape (n_samples,) or (n_samples, n_targets)
             Target values.
         """
         n_samples, n_features = X.shape
@@ -375,42 +484,49 @@
         # 1) within (univariate) scaling by with classes std-dev
         std = Xc.std(axis=0)
         # avoid division by zero in normalization
-        std[std == 0] = 1.
-        fac = 1. / (n_samples - n_classes)
+        std[std == 0] = 1.0
+        fac = 1.0 / (n_samples - n_classes)

         # 2) Within variance scaling
         X = np.sqrt(fac) * (Xc / std)
         # SVD of centered (within)scaled data
-        U, S, V = linalg.svd(X, full_matrices=False)
+        U, S, Vt = linalg.svd(X, full_matrices=False)

         rank = np.sum(S > self.tol)
-        if rank < n_features:
-            warnings.warn("Variables are collinear.")
         # Scaling of within covariance is: V' 1/S
-        scalings = (V[:rank] / std).T / S[:rank]
+        scalings = (Vt[:rank] / std).T / S[:rank]
+        fac = 1.0 if n_classes == 1 else 1.0 / (n_classes - 1)

         # 3) Between variance scaling
         # Scale weighted centers
-        X = np.dot(((np.sqrt((n_samples * self.priors_) * fac)) *
-                    (self.means_ - self.xbar_).T).T, scalings)
+        X = np.dot(
+            (
+                (np.sqrt((n_samples * self.priors_) * fac))
+                * (self.means_ - self.xbar_).T
+            ).T,
+            scalings,
+        )
         # Centers are living in a space with n_classes-1 dim (maximum)
         # Use SVD to find projection in the space spanned by the
         # (n_classes) centers
-        _, S, V = linalg.svd(X, full_matrices=0)
-
-        self.explained_variance_ratio_ = (S**2 / np.sum(
-            S**2))[:self._max_components]
+        _, S, Vt = linalg.svd(X, full_matrices=0)
+
+        if self._max_components == 0:
+            self.explained_variance_ratio_ = np.empty((0,), dtype=S.dtype)
+        else:
+            self.explained_variance_ratio_ = (S**2 / np.sum(S**2))[
+                : self._max_components
+            ]
+
         rank = np.sum(S > self.tol * S[0])
-        self.scalings_ = np.dot(scalings, V.T[:, :rank])
+        self.scalings_ = np.dot(scalings, Vt.T[:, :rank])
         coef = np.dot(self.means_ - self.xbar_, self.scalings_)
-        self.intercept_ = (-0.5 * np.sum(coef ** 2, axis=1) +
-                           np.log(self.priors_))
+        self.intercept_ = -0.5 * np.sum(coef**2, axis=1) + np.log(self.priors_)
         self.coef_ = np.dot(coef, self.scalings_.T)
         self.intercept_ -= np.dot(self.xbar_, self.coef_.T)

     def fit(self, X, y):
-        """Fit LinearDiscriminantAnalysis model according to the given
-           training data and parameters.
+        """Fit the Linear Discriminant Analysis model.

            .. versionchanged:: 0.19
               *store_covariance* has been moved to main constructor.
@@ -420,22 +536,28 @@

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
+        X : array-like of shape (n_samples, n_features)
             Training data.

-        y : array, shape (n_samples,)
+        y : array-like of shape (n_samples,)
             Target values.
-        """
-        # FIXME: Future warning to be removed in 0.23
-        X, y = check_X_y(X, y, ensure_min_samples=2, estimator=self,
-                         dtype=[np.float64, np.float32])
+
+        Returns
+        -------
+        self : object
+            Fitted estimator.
+        """
+        X, y = self._validate_data(
+            X, y, ensure_min_samples=2, dtype=[np.float64, np.float32]
+        )
         self.classes_ = unique_labels(y)
         n_samples, _ = X.shape
         n_classes = len(self.classes_)

         if n_samples == n_classes:
-            raise ValueError("The number of samples must be more "
-                             "than the number of classes.")
+            raise ValueError(
+                "The number of samples must be more than the number of classes."
+            )

         if self.priors is None:  # estimate priors from sample
             _, y_t = np.unique(y, return_inverse=True)  # non-negative ints
@@ -446,8 +568,7 @@
         if (self.priors_ < 0).any():
             raise ValueError("priors must be non-negative")
         if not np.isclose(self.priors_.sum(), 1.0):
-            warnings.warn("The priors do not sum to 1. Renormalizing",
-                          UserWarning)
+            warnings.warn("The priors do not sum to 1. Renormalizing", UserWarning)
             self.priors_ = self.priors_ / self.priors_.sum()

         # Maximum number of components no matter what n_components is
@@ -458,38 +579,48 @@
             self._max_components = max_components
         else:
             if self.n_components > max_components:
-                warnings.warn(
-                    "n_components cannot be larger than min(n_features, "
-                    "n_classes - 1). Using min(n_features, "
-                    "n_classes - 1) = min(%d, %d - 1) = %d components."
-                    % (X.shape[1], len(self.classes_), max_components),
-                    ChangedBehaviorWarning)
-                future_msg = ("In version 0.23, setting n_components > min("
-                              "n_features, n_classes - 1) will raise a "
-                              "ValueError. You should set n_components to None"
-                              " (default), or a value smaller or equal to "
-                              "min(n_features, n_classes - 1).")
-                warnings.warn(future_msg, FutureWarning)
-                self._max_components = max_components
-            else:
-                self._max_components = self.n_components
-
-        if self.solver == 'svd':
+                raise ValueError(
+                    "n_components cannot be larger than min(n_features, n_classes - 1)."
+                )
+            self._max_components = self.n_components
+
+        if self.solver == "svd":
             if self.shrinkage is not None:
-                raise NotImplementedError('shrinkage not supported')
+                raise NotImplementedError("shrinkage not supported")
+            if self.covariance_estimator is not None:
+                raise ValueError(
+                    "covariance estimator "
+                    "is not supported "
+                    "with svd solver. Try another solver"
+                )
             self._solve_svd(X, y)
-        elif self.solver == 'lsqr':
-            self._solve_lsqr(X, y, shrinkage=self.shrinkage)
-        elif self.solver == 'eigen':
-            self._solve_eigen(X, y, shrinkage=self.shrinkage)
+        elif self.solver == "lsqr":
+            self._solve_lsqr(
+                X,
+                y,
+                shrinkage=self.shrinkage,
+                covariance_estimator=self.covariance_estimator,
+            )
+        elif self.solver == "eigen":
+            self._solve_eigen(
+                X,
+                y,
+                shrinkage=self.shrinkage,
+                covariance_estimator=self.covariance_estimator,
+            )
         else:
-            raise ValueError("unknown solver {} (valid solvers are 'svd', "
-                             "'lsqr', and 'eigen').".format(self.solver))
+            raise ValueError(
+                "unknown solver {} (valid solvers are 'svd', "
+                "'lsqr', and 'eigen').".format(self.solver)
+            )
         if self.classes_.size == 2:  # treat binary case as a special case
-            self.coef_ = np.array(self.coef_[1, :] - self.coef_[0, :], ndmin=2,
-                                  dtype=X.dtype)
-            self.intercept_ = np.array(self.intercept_[1] - self.intercept_[0],
-                                       ndmin=1, dtype=X.dtype)
+            self.coef_ = np.array(
+                self.coef_[1, :] - self.coef_[0, :], ndmin=2, dtype=X.dtype
+            )
+            self.intercept_ = np.array(
+                self.intercept_[1] - self.intercept_[0], ndmin=1, dtype=X.dtype
+            )
+        self._n_features_out = self._max_components
         return self

     def transform(self, X):
@@ -497,46 +628,49 @@

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
+        X : array-like of shape (n_samples, n_features)
             Input data.

         Returns
         -------
-        X_new : array, shape (n_samples, n_components)
-            Transformed data.
-        """
-        if self.solver == 'lsqr':
-            raise NotImplementedError("transform not implemented for 'lsqr' "
-                                      "solver (use 'svd' or 'eigen').")
-        check_is_fitted(self, ['xbar_', 'scalings_'], all_or_any=any)
-
-        X = check_array(X)
-        if self.solver == 'svd':
+        X_new : ndarray of shape (n_samples, n_components) or \
+            (n_samples, min(rank, n_components))
+            Transformed data. In the case of the 'svd' solver, the shape
+            is (n_samples, min(rank, n_components)).
+        """
+        if self.solver == "lsqr":
+            raise NotImplementedError(
+                "transform not implemented for 'lsqr' solver (use 'svd' or 'eigen')."
+            )
+        check_is_fitted(self)
+
+        X = self._validate_data(X, reset=False)
+        if self.solver == "svd":
             X_new = np.dot(X - self.xbar_, self.scalings_)
-        elif self.solver == 'eigen':
+        elif self.solver == "eigen":
             X_new = np.dot(X, self.scalings_)

-        return X_new[:, :self._max_components]
+        return X_new[:, : self._max_components]

     def predict_proba(self, X):
         """Estimate probability.

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
+        X : array-like of shape (n_samples, n_features)
             Input data.

         Returns
         -------
-        C : array, shape (n_samples, n_classes)
+        C : ndarray of shape (n_samples, n_classes)
             Estimated probabilities.
         """
-        check_is_fitted(self, 'classes_')
+        check_is_fitted(self)

         decision = self.decision_function(X)
         if self.classes_.size == 2:
             proba = expit(decision)
-            return np.vstack([1-proba, proba]).T
+            return np.vstack([1 - proba, proba]).T
         else:
             return softmax(decision)

@@ -545,19 +679,44 @@

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
+        X : array-like of shape (n_samples, n_features)
             Input data.

         Returns
         -------
-        C : array, shape (n_samples, n_classes)
+        C : ndarray of shape (n_samples, n_classes)
             Estimated log probabilities.
         """
-        return np.log(self.predict_proba(X))
-
-
-class QuadraticDiscriminantAnalysis(BaseEstimator, ClassifierMixin):
-    """Quadratic Discriminant Analysis
+        prediction = self.predict_proba(X)
+        prediction[prediction == 0.0] += np.finfo(prediction.dtype).tiny
+        return np.log(prediction)
+
+    def decision_function(self, X):
+        """Apply decision function to an array of samples.
+
+        The decision function is equal (up to a constant factor) to the
+        log-posterior of the model, i.e. `log p(y = k | x)`. In a binary
+        classification setting this instead corresponds to the difference
+        `log p(y = 1 | x) - log p(y = 0 | x)`. See :ref:`lda_qda_math`.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            Array of samples (test vectors).
+
+        Returns
+        -------
+        C : ndarray of shape (n_samples,) or (n_samples, n_classes)
+            Decision function values related to each class, per sample.
+            In the two-class case, the shape is (n_samples,), giving the
+            log likelihood ratio of the positive class.
+        """
+        # Only override for the doc
+        return super().decision_function(X)
+
+
+class QuadraticDiscriminantAnalysis(ClassifierMixin, BaseEstimator):
+    """Quadratic Discriminant Analysis.

     A classifier with a quadratic decision boundary, generated
     by fitting class conditional densities to the data
@@ -572,45 +731,77 @@

     Parameters
     ----------
-    priors : array, optional, shape = [n_classes]
-        Priors on classes
-
-    reg_param : float, optional
-        Regularizes the covariance estimate as
-        ``(1-reg_param)*Sigma + reg_param*np.eye(n_features)``
-
-    store_covariance : boolean
-        If True the covariance matrices are computed and stored in the
-        `self.covariance_` attribute.
+    priors : ndarray of shape (n_classes,), default=None
+        Class priors. By default, the class proportions are inferred from the
+        training data.
+
+    reg_param : float, default=0.0
+        Regularizes the per-class covariance estimates by transforming S2 as
+        ``S2 = (1 - reg_param) * S2 + reg_param * np.eye(n_features)``,
+        where S2 corresponds to the `scaling_` attribute of a given class.
+
+    store_covariance : bool, default=False
+        If True, the class covariance matrices are explicitly computed and
+        stored in the `self.covariance_` attribute.

         .. versionadded:: 0.17

-    tol : float, optional, default 1.0e-4
-        Threshold used for rank estimation.
+    tol : float, default=1.0e-4
+        Absolute threshold for a singular value to be considered significant,
+        used to estimate the rank of `Xk` where `Xk` is the centered matrix
+        of samples in class k. This parameter does not affect the
+        predictions. It only controls a warning that is raised when features
+        are considered to be colinear.

         .. versionadded:: 0.17

     Attributes
     ----------
-    covariance_ : list of array-like, shape = [n_features, n_features]
-        Covariance matrices of each class.
-
-    means_ : array-like, shape = [n_classes, n_features]
-        Class means.
-
-    priors_ : array-like, shape = [n_classes]
+    covariance_ : list of len n_classes of ndarray \
+            of shape (n_features, n_features)
+        For each class, gives the covariance matrix estimated using the
+        samples of that class. The estimations are unbiased. Only present if
+        `store_covariance` is True.
+
+    means_ : array-like of shape (n_classes, n_features)
+        Class-wise means.
+
+    priors_ : array-like of shape (n_classes,)
         Class priors (sum to 1).

-    rotations_ : list of arrays
-        For each class k an array of shape [n_features, n_k], with
+    rotations_ : list of len n_classes of ndarray of shape (n_features, n_k)
+        For each class k an array of shape (n_features, n_k), where
         ``n_k = min(n_features, number of elements in class k)``
         It is the rotation of the Gaussian distribution, i.e. its
-        principal axis.
-
-    scalings_ : list of arrays
-        For each class k an array of shape [n_k]. It contains the scaling
-        of the Gaussian distributions along its principal axes, i.e. the
-        variance in the rotated coordinate system.
+        principal axis. It corresponds to `V`, the matrix of eigenvectors
+        coming from the SVD of `Xk = U S Vt` where `Xk` is the centered
+        matrix of samples from class k.
+
+    scalings_ : list of len n_classes of ndarray of shape (n_k,)
+        For each class, contains the scaling of
+        the Gaussian distributions along its principal axes, i.e. the
+        variance in the rotated coordinate system. It corresponds to `S^2 /
+        (n_samples - 1)`, where `S` is the diagonal matrix of singular values
+        from the SVD of `Xk`, where `Xk` is the centered matrix of samples
+        from class k.
+
+    classes_ : ndarray of shape (n_classes,)
+        Unique class labels.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    LinearDiscriminantAnalysis : Linear Discriminant Analysis.

     Examples
     --------
@@ -620,20 +811,14 @@
     >>> y = np.array([1, 1, 1, 2, 2, 2])
     >>> clf = QuadraticDiscriminantAnalysis()
     >>> clf.fit(X, y)
-    ... # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
-    QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
-                                  store_covariance=False, tol=0.0001)
+    QuadraticDiscriminantAnalysis()
     >>> print(clf.predict([[-0.8, -1]]))
     [1]
-
-    See also
-    --------
-    sklearn.discriminant_analysis.LinearDiscriminantAnalysis: Linear
-        Discriminant Analysis
     """

-    def __init__(self, priors=None, reg_param=0., store_covariance=False,
-                 tol=1.0e-4):
+    def __init__(
+        self, *, priors=None, reg_param=0.0, store_covariance=False, tol=1.0e-4
+    ):
         self.priors = np.asarray(priors) if priors is not None else None
         self.reg_param = reg_param
         self.store_covariance = store_covariance
@@ -651,21 +836,28 @@

         Parameters
         ----------
-        X : array-like, shape = [n_samples, n_features]
-            Training vector, where n_samples is the number of samples and
-            n_features is the number of features.
-
-        y : array, shape = [n_samples]
-            Target values (integers)
-        """
-        X, y = check_X_y(X, y)
+        X : array-like of shape (n_samples, n_features)
+            Training vector, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,)
+            Target values (integers).
+
+        Returns
+        -------
+        self : object
+            Fitted estimator.
+        """
+        X, y = self._validate_data(X, y)
         check_classification_targets(y)
         self.classes_, y = np.unique(y, return_inverse=True)
         n_samples, n_features = X.shape
         n_classes = len(self.classes_)
         if n_classes < 2:
-            raise ValueError('The number of classes has to be greater than'
-                             ' one; got %d class' % (n_classes))
+            raise ValueError(
+                "The number of classes has to be greater than one; got %d class"
+                % (n_classes)
+            )
         if self.priors is None:
             self.priors_ = np.bincount(y) / float(n_samples)
         else:
@@ -683,15 +875,17 @@
             meang = Xg.mean(0)
             means.append(meang)
             if len(Xg) == 1:
-                raise ValueError('y has only 1 sample in class %s, covariance '
-                                 'is ill defined.' % str(self.classes_[ind]))
+                raise ValueError(
+                    "y has only 1 sample in class %s, covariance is ill defined."
+                    % str(self.classes_[ind])
+                )
             Xgc = Xg - meang
             # Xgc = U * S * V.T
-            U, S, Vt = np.linalg.svd(Xgc, full_matrices=False)
+            _, S, Vt = np.linalg.svd(Xgc, full_matrices=False)
             rank = np.sum(S > self.tol)
             if rank < n_features:
                 warnings.warn("Variables are collinear")
-            S2 = (S ** 2) / (len(Xg) - 1)
+            S2 = (S**2) / (len(Xg) - 1)
             S2 = ((1 - self.reg_param) * S2) + self.reg_param
             if self.store_covariance or store_covariance:
                 # cov = V * (S^2 / (n-1)) * V.T
@@ -706,33 +900,39 @@
         return self

     def _decision_function(self, X):
-        check_is_fitted(self, 'classes_')
-
-        X = check_array(X)
+        # return log posterior, see eq (4.12) p. 110 of the ESL.
+        check_is_fitted(self)
+
+        X = self._validate_data(X, reset=False)
         norm2 = []
         for i in range(len(self.classes_)):
             R = self.rotations_[i]
             S = self.scalings_[i]
             Xm = X - self.means_[i]
             X2 = np.dot(Xm, R * (S ** (-0.5)))
-            norm2.append(np.sum(X2 ** 2, 1))
+            norm2.append(np.sum(X2**2, axis=1))
         norm2 = np.array(norm2).T  # shape = [len(X), n_classes]
         u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])
-        return (-0.5 * (norm2 + u) + np.log(self.priors_))
+        return -0.5 * (norm2 + u) + np.log(self.priors_)

     def decision_function(self, X):
         """Apply decision function to an array of samples.

-        Parameters
-        ----------
-        X : array-like, shape = [n_samples, n_features]
+        The decision function is equal (up to a constant factor) to the
+        log-posterior of the model, i.e. `log p(y = k | x)`. In a binary
+        classification setting this instead corresponds to the difference
+        `log p(y = 1 | x) - log p(y = 0 | x)`. See :ref:`lda_qda_math`.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
             Array of samples (test vectors).

         Returns
         -------
-        C : array, shape = [n_samples, n_classes] or [n_samples,]
+        C : ndarray of shape (n_samples,) or (n_samples, n_classes)
             Decision function values related to each class, per sample.
-            In the two-class case, the shape is [n_samples,], giving the
+            In the two-class case, the shape is (n_samples,), giving the
             log likelihood ratio of the positive class.
         """
         dec_func = self._decision_function(X)
@@ -748,11 +948,14 @@

         Parameters
         ----------
-        X : array-like, shape = [n_samples, n_features]
+        X : array-like of shape (n_samples, n_features)
+            Vector to be scored, where `n_samples` is the number of samples and
+            `n_features` is the number of features.

         Returns
         -------
-        C : array, shape = [n_samples]
+        C : ndarray of shape (n_samples,)
+            Estimated probabilities.
         """
         d = self._decision_function(X)
         y_pred = self.classes_.take(d.argmax(1))
@@ -763,12 +966,12 @@

         Parameters
         ----------
-        X : array-like, shape = [n_samples, n_features]
+        X : array-like of shape (n_samples, n_features)
             Array of samples/test vectors.

         Returns
         -------
-        C : array, shape = [n_samples, n_classes]
+        C : ndarray of shape (n_samples, n_classes)
             Posterior probabilities of classification per class.
         """
         values = self._decision_function(X)
@@ -779,16 +982,16 @@
         return likelihood / likelihood.sum(axis=1)[:, np.newaxis]

     def predict_log_proba(self, X):
-        """Return posterior probabilities of classification.
-
-        Parameters
-        ----------
-        X : array-like, shape = [n_samples, n_features]
+        """Return log of posterior probabilities of classification.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
             Array of samples/test vectors.

         Returns
         -------
-        C : array, shape = [n_samples, n_classes]
+        C : ndarray of shape (n_samples, n_classes)
             Posterior log-probabilities of classification per class.
         """
         # XXX : can do better to avoid precision overflows
('sklearn', 'exceptions.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,16 +3,17 @@
 classes used across scikit-learn.
 """

-__all__ = ['NotFittedError',
-           'ChangedBehaviorWarning',
-           'ConvergenceWarning',
-           'DataConversionWarning',
-           'DataDimensionalityWarning',
-           'EfficiencyWarning',
-           'FitFailedWarning',
-           'NonBLASDotWarning',
-           'SkipTestWarning',
-           'UndefinedMetricWarning']
+__all__ = [
+    "NotFittedError",
+    "ConvergenceWarning",
+    "DataConversionWarning",
+    "DataDimensionalityWarning",
+    "EfficiencyWarning",
+    "FitFailedWarning",
+    "SkipTestWarning",
+    "UndefinedMetricWarning",
+    "PositiveSpectrumWarning",
+]


 class NotFittedError(ValueError, AttributeError):
@@ -29,19 +30,11 @@
     ...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])
     ... except NotFittedError as e:
     ...     print(repr(e))
-    ... # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-    NotFittedError('This LinearSVC instance is not fitted yet'...)
+    NotFittedError("This LinearSVC instance is not fitted yet. Call 'fit' with
+    appropriate arguments before using this estimator."...)

     .. versionchanged:: 0.18
        Moved from sklearn.utils.validation.
-    """
-
-
-class ChangedBehaviorWarning(UserWarning):
-    """Warning class used to notify the user of any change in the behavior.
-
-    .. versionchanged:: 0.18
-       Moved from sklearn.base.
     """


@@ -103,39 +96,8 @@
     and the cross-validation helper function cross_val_score to warn when there
     is an error while fitting the estimator.

-    Examples
-    --------
-    >>> from sklearn.model_selection import GridSearchCV
-    >>> from sklearn.svm import LinearSVC
-    >>> from sklearn.exceptions import FitFailedWarning
-    >>> import warnings
-    >>> warnings.simplefilter('always', FitFailedWarning)
-    >>> gs = GridSearchCV(LinearSVC(), {'C': [-1, -2]}, error_score=0, cv=2)
-    >>> X, y = [[1, 2], [3, 4], [5, 6], [7, 8]], [0, 0, 1, 1]
-    >>> with warnings.catch_warnings(record=True) as w:
-    ...     try:
-    ...         gs.fit(X, y)  # This will raise a ValueError since C is < 0
-    ...     except ValueError:
-    ...         pass
-    ...     print(repr(w[-1].message))
-    ... # doctest: +NORMALIZE_WHITESPACE
-    FitFailedWarning('Estimator fit failed. The score on this train-test
-    partition for these parameters will be set to 0.000000.
-    Details: \\nValueError: Penalty term must be positive; got (C=-2)\\n'...)
-
     .. versionchanged:: 0.18
        Moved from sklearn.cross_validation.
-    """
-
-
-class NonBLASDotWarning(EfficiencyWarning):
-    """Warning used when the dot operation does not use BLAS.
-
-    This warning is used to notify the user that BLAS was not used for dot
-    operation and hence the efficiency may be affected.
-
-    .. versionchanged:: 0.18
-       Moved from sklearn.utils.validation, extends EfficiencyWarning.
     """


@@ -154,3 +116,15 @@
     .. versionchanged:: 0.18
        Moved from sklearn.base.
     """
+
+
+class PositiveSpectrumWarning(UserWarning):
+    """Warning raised when the eigenvalues of a PSD matrix have issues
+
+    This warning is typically raised by ``_check_psd_eigenvalues`` when the
+    eigenvalues of a positive semidefinite (PSD) matrix such as a gram matrix
+    (kernel) present significant negative eigenvalues, or bad conditioning i.e.
+    very small non-zero eigenvalues compared to the largest eigenvalue.
+
+    .. versionadded:: 0.22
+    """
('sklearn', '_config.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -2,34 +2,62 @@
 """
 import os
 from contextlib import contextmanager as contextmanager
+import threading

 _global_config = {
-    'assume_finite': bool(os.environ.get('SKLEARN_ASSUME_FINITE', False)),
-    'working_memory': int(os.environ.get('SKLEARN_WORKING_MEMORY', 1024)),
-    'print_changed_only': False,
+    "assume_finite": bool(os.environ.get("SKLEARN_ASSUME_FINITE", False)),
+    "working_memory": int(os.environ.get("SKLEARN_WORKING_MEMORY", 1024)),
+    "print_changed_only": True,
+    "display": "diagram",
+    "pairwise_dist_chunk_size": int(
+        os.environ.get("SKLEARN_PAIRWISE_DIST_CHUNK_SIZE", 256)
+    ),
+    "enable_cython_pairwise_dist": True,
 }
+_threadlocal = threading.local()
+
+
+def _get_threadlocal_config():
+    """Get a threadlocal **mutable** configuration. If the configuration
+    does not exist, copy the default global configuration."""
+    if not hasattr(_threadlocal, "global_config"):
+        _threadlocal.global_config = _global_config.copy()
+    return _threadlocal.global_config


 def get_config():
-    """Retrieve current values for configuration set by :func:`set_config`
+    """Retrieve current values for configuration set by :func:`set_config`.

     Returns
     -------
     config : dict
         Keys are parameter names that can be passed to :func:`set_config`.
+
+    See Also
+    --------
+    config_context : Context manager for global scikit-learn configuration.
+    set_config : Set global scikit-learn configuration.
     """
-    return _global_config.copy()
-
-
-def set_config(assume_finite=None, working_memory=None,
-               print_changed_only=None):
+    # Return a copy of the threadlocal configuration so that users will
+    # not be able to modify the configuration with the returned dict.
+    return _get_threadlocal_config().copy()
+
+
+def set_config(
+    assume_finite=None,
+    working_memory=None,
+    print_changed_only=None,
+    display=None,
+    pairwise_dist_chunk_size=None,
+    enable_cython_pairwise_dist=None,
+):
     """Set global scikit-learn configuration

     .. versionadded:: 0.19

     Parameters
     ----------
-    assume_finite : bool, optional
+    assume_finite : bool, default=None
         If True, validation for finiteness will be skipped,
         saving time, but leading to potential crashes. If
         False, validation for finiteness will be performed,
@@ -37,7 +65,7 @@

         .. versionadded:: 0.19

-    working_memory : int, optional
+    working_memory : int, default=None
         If set, scikit-learn will attempt to limit the size of temporary arrays
         to this number of MiB (per job when parallelised), often saving both
         computation time and memory on expensive operations that can be
@@ -45,7 +73,7 @@

         .. versionadded:: 0.20

-    print_changed_only : bool, optional
+    print_changed_only : bool, default=None
         If True, only the parameters that were set to non-default
         values will be printed when printing an estimator. For example,
         ``print(SVC())`` while True will only print 'SVC()' while the default
@@ -53,38 +81,135 @@
         all the non-changed parameters.

         .. versionadded:: 0.21
+
+    display : {'text', 'diagram'}, default=None
+        If 'diagram', estimators will be displayed as a diagram in a Jupyter
+        lab or notebook context. If 'text', estimators will be displayed as
+        text. Default is 'diagram'.
+
+        .. versionadded:: 0.23
+
+    pairwise_dist_chunk_size : int, default=None
+        The number of row vectors per chunk for PairwiseDistancesReduction.
+        Default is 256 (suitable for most of modern laptops' caches and architectures).
+
+        Intended for easier benchmarking and testing of scikit-learn internals.
+        End users are not expected to benefit from customizing this configuration
+        setting.
+
+        .. versionadded:: 1.1
+
+    enable_cython_pairwise_dist : bool, default=None
+        Use PairwiseDistancesReduction when possible.
+        Default is True.
+
+        Intended for easier benchmarking and testing of scikit-learn internals.
+        End users are not expected to benefit from customizing this configuration
+        setting.
+
+        .. versionadded:: 1.1
+
+    See Also
+    --------
+    config_context : Context manager for global scikit-learn configuration.
+    get_config : Retrieve current values of the global configuration.
     """
+    local_config = _get_threadlocal_config()
+
     if assume_finite is not None:
-        _global_config['assume_finite'] = assume_finite
+        local_config["assume_finite"] = assume_finite
     if working_memory is not None:
-        _global_config['working_memory'] = working_memory
+        local_config["working_memory"] = working_memory
     if print_changed_only is not None:
-        _global_config['print_changed_only'] = print_changed_only
+        local_config["print_changed_only"] = print_changed_only
+    if display is not None:
+        local_config["display"] = display
+    if pairwise_dist_chunk_size is not None:
+        local_config["pairwise_dist_chunk_size"] = pairwise_dist_chunk_size
+    if enable_cython_pairwise_dist is not None:
+        local_config["enable_cython_pairwise_dist"] = enable_cython_pairwise_dist


 @contextmanager
-def config_context(**new_config):
-    """Context manager for global scikit-learn configuration
+def config_context(
+    *,
+    assume_finite=None,
+    working_memory=None,
+    print_changed_only=None,
+    display=None,
+    pairwise_dist_chunk_size=None,
+    enable_cython_pairwise_dist=None,
+):
+    """Context manager for global scikit-learn configuration.

     Parameters
     ----------
-    assume_finite : bool, optional
+    assume_finite : bool, default=None
         If True, validation for finiteness will be skipped,
         saving time, but leading to potential crashes. If
         False, validation for finiteness will be performed,
-        avoiding error.  Global default: False.
-
-    working_memory : int, optional
+        avoiding error. If None, the existing value won't change.
+        The default value is False.
+
+    working_memory : int, default=None
         If set, scikit-learn will attempt to limit the size of temporary arrays
         to this number of MiB (per job when parallelised), often saving both
         computation time and memory on expensive operations that can be
-        performed in chunks. Global default: 1024.
+        performed in chunks. If None, the existing value won't change.
+        The default value is 1024.
+
+    print_changed_only : bool, default=None
+        If True, only the parameters that were set to non-default
+        values will be printed when printing an estimator. For example,
+        ``print(SVC())`` while True will only print 'SVC()', but would print
+        'SVC(C=1.0, cache_size=200, ...)' with all the non-changed parameters
+        when False. If None, the existing value won't change.
+        The default value is True.
+
+        .. versionchanged:: 0.23
+           Default changed from False to True.
+
+    display : {'text', 'diagram'}, default=None
+        If 'diagram', estimators will be displayed as a diagram in a Jupyter
+        lab or notebook context. If 'text', estimators will be displayed as
+        text. If None, the existing value won't change.
+        The default value is 'diagram'.
+
+        .. versionadded:: 0.23
+
+    pairwise_dist_chunk_size : int, default=None
+        The number of vectors per chunk for PairwiseDistancesReduction.
+        Default is 256 (suitable for most of modern laptops' caches and architectures).
+
+        Intended for easier benchmarking and testing of scikit-learn internals.
+        End users are not expected to benefit from customizing this configuration
+        setting.
+
+        .. versionadded:: 1.1
+
+    enable_cython_pairwise_dist : bool, default=None
+        Use PairwiseDistancesReduction when possible.
+        Default is True.
+
+        Intended for easier benchmarking and testing of scikit-learn internals.
+        End users are not expected to benefit from customizing this configuration
+        setting.
+
+        .. versionadded:: 1.1
+
+    Yields
+    ------
+    None.
+
+    See Also
+    --------
+    set_config : Set global scikit-learn configuration.
+    get_config : Retrieve current values of the global configuration.

     Notes
     -----
     All settings, not just those presently modified, will be returned to
-    their previous values when the context manager is exited. This is not
-    thread-safe.
+    their previous values when the context manager is exited.

     Examples
     --------
@@ -95,13 +220,19 @@
     >>> with sklearn.config_context(assume_finite=True):
     ...     with sklearn.config_context(assume_finite=False):
     ...         assert_all_finite([float('nan')])
-    ... # doctest: +ELLIPSIS
     Traceback (most recent call last):
     ...
-    ValueError: Input contains NaN, ...
+    ValueError: Input contains NaN...
     """
-    old_config = get_config().copy()
-    set_config(**new_config)
+    old_config = get_config()
+    set_config(
+        assume_finite=assume_finite,
+        working_memory=working_memory,
+        print_changed_only=print_changed_only,
+        display=display,
+        pairwise_dist_chunk_size=pairwise_dist_chunk_size,
+        enable_cython_pairwise_dist=enable_cython_pairwise_dist,
+    )

     try:
         yield
('sklearn', 'dummy.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -10,148 +10,229 @@
 from .base import BaseEstimator, ClassifierMixin, RegressorMixin
 from .base import MultiOutputMixin
 from .utils import check_random_state
+from .utils import deprecated
 from .utils.validation import _num_samples
 from .utils.validation import check_array
 from .utils.validation import check_consistent_length
-from .utils.validation import check_is_fitted
-from .utils.random import random_choice_csc
+from .utils.validation import check_is_fitted, _check_sample_weight
+from .utils.random import _random_choice_csc
 from .utils.stats import _weighted_percentile
 from .utils.multiclass import class_distribution


-class DummyClassifier(BaseEstimator, ClassifierMixin, MultiOutputMixin):
-    """
-    DummyClassifier is a classifier that makes predictions using simple rules.
-
-    This classifier is useful as a simple baseline to compare with other
-    (real) classifiers. Do not use it for real problems.
+class DummyClassifier(MultiOutputMixin, ClassifierMixin, BaseEstimator):
+    """DummyClassifier makes predictions that ignore the input features.
+
+    This classifier serves as a simple baseline to compare against other more
+    complex classifiers.
+
+    The specific behavior of the baseline is selected with the `strategy`
+    parameter.
+
+    All strategies make predictions that ignore the input feature values passed
+    as the `X` argument to `fit` and `predict`. The predictions, however,
+    typically depend on values observed in the `y` parameter passed to `fit`.
+
+    Note that the "stratified" and "uniform" strategies lead to
+    non-deterministic predictions that can be rendered deterministic by setting
+    the `random_state` parameter if needed. The other strategies are naturally
+    deterministic and, once fit, always return a the same constant prediction
+    for any value of `X`.

     Read more in the :ref:`User Guide <dummy_estimators>`.
+
+    .. versionadded:: 0.13

     Parameters
     ----------
-    strategy : str, default="stratified"
+    strategy : {"most_frequent", "prior", "stratified", "uniform", \
+            "constant"}, default="prior"
         Strategy to use to generate predictions.

-        * "stratified": generates predictions by respecting the training
-          set's class distribution.
-        * "most_frequent": always predicts the most frequent label in the
-          training set.
-        * "prior": always predicts the class that maximizes the class prior
-          (like "most_frequent") and ``predict_proba`` returns the class prior.
-        * "uniform": generates predictions uniformly at random.
+        * "most_frequent": the `predict` method always returns the most
+          frequent class label in the observed `y` argument passed to `fit`.
+          The `predict_proba` method returns the matching one-hot encoded
+          vector.
+        * "prior": the `predict` method always returns the most frequent
+          class label in the observed `y` argument passed to `fit` (like
+          "most_frequent"). ``predict_proba`` always returns the empirical
+          class distribution of `y` also known as the empirical class prior
+          distribution.
+        * "stratified": the `predict_proba` method randomly samples one-hot
+          vectors from a multinomial distribution parametrized by the empirical
+          class prior probabilities.
+          The `predict` method returns the class label which got probability
+          one in the one-hot vector of `predict_proba`.
+          Each sampled row of both methods is therefore independent and
+          identically distributed.
+        * "uniform": generates predictions uniformly at random from the list
+          of unique classes observed in `y`, i.e. each class has equal
+          probability.
         * "constant": always predicts a constant label that is provided by
           the user. This is useful for metrics that evaluate a non-majority
-          class
-
-          .. versionadded:: 0.17
-             Dummy Classifier now supports prior fitting strategy using
-             parameter *prior*.
-
-    random_state : int, RandomState instance or None, optional, default=None
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-    constant : int or str or array of shape = [n_outputs]
+          class.
+
+          .. versionchanged:: 0.24
+             The default value of `strategy` has changed to "prior" in version
+             0.24.
+
+    random_state : int, RandomState instance or None, default=None
+        Controls the randomness to generate the predictions when
+        ``strategy='stratified'`` or ``strategy='uniform'``.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.
+
+    constant : int or str or array-like of shape (n_outputs,), default=None
         The explicit constant as predicted by the "constant" strategy. This
         parameter is useful only for the "constant" strategy.

     Attributes
     ----------
-    classes_ : array or list of array of shape = [n_classes]
-        Class labels for each output.
-
-    n_classes_ : array or list of array of shape = [n_classes]
+    classes_ : ndarray of shape (n_classes,) or list of such arrays
+        Unique class labels observed in `y`. For multi-output classification
+        problems, this attribute is a list of arrays as each output has an
+        independent set of possible classes.
+
+    n_classes_ : int or list of int
         Number of label for each output.

-    class_prior_ : array or list of array of shape = [n_classes]
-        Probability of each class for each output.
-
-    n_outputs_ : int,
+    class_prior_ : ndarray of shape (n_classes,) or list of such arrays
+        Frequency of each class observed in `y`. For multioutput classification
+        problems, this is computed independently for each output.
+
+    n_outputs_ : int
         Number of outputs.

-    sparse_output_ : bool,
+    n_features_in_ : `None`
+        Always set to `None`.
+
+        .. versionadded:: 0.24
+        .. deprecated:: 1.0
+            Will be removed in 1.0
+
+    sparse_output_ : bool
         True if the array returned from predict is to be in sparse CSC format.
-        Is automatically set to True if the input y is passed in sparse format.
+        Is automatically set to True if the input `y` is passed in sparse
+        format.
+
+    See Also
+    --------
+    DummyRegressor : Regressor that makes predictions using simple rules.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.dummy import DummyClassifier
+    >>> X = np.array([-1, 1, 1, 1])
+    >>> y = np.array([0, 1, 1, 1])
+    >>> dummy_clf = DummyClassifier(strategy="most_frequent")
+    >>> dummy_clf.fit(X, y)
+    DummyClassifier(strategy='most_frequent')
+    >>> dummy_clf.predict(X)
+    array([1, 1, 1, 1])
+    >>> dummy_clf.score(X, y)
+    0.75
     """

-    def __init__(self, strategy="stratified", random_state=None,
-                 constant=None):
+    def __init__(self, *, strategy="prior", random_state=None, constant=None):
         self.strategy = strategy
         self.random_state = random_state
         self.constant = constant

     def fit(self, X, y, sample_weight=None):
-        """Fit the random classifier.
+        """Fit the baseline classifier.

         Parameters
         ----------
-        X : {array-like, object with finite length or shape}
-            Training data, requires length = n_samples
-
-        y : array-like, shape = [n_samples] or [n_samples, n_outputs]
+        X : array-like of shape (n_samples, n_features)
+            Training data.
+
+        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
             Target values.

-        sample_weight : array-like of shape = [n_samples], optional
+        sample_weight : array-like of shape (n_samples,), default=None
             Sample weights.

         Returns
         -------
         self : object
-        """
-        allowed_strategies = ("most_frequent", "stratified", "uniform",
-                              "constant", "prior")
+            Returns the instance itself.
+        """
+        allowed_strategies = (
+            "most_frequent",
+            "stratified",
+            "uniform",
+            "constant",
+            "prior",
+        )
+
         if self.strategy not in allowed_strategies:
-            raise ValueError("Unknown strategy type: %s, expected one of %s."
-                             % (self.strategy, allowed_strategies))
-
-        if self.strategy == "uniform" and sp.issparse(y):
+            raise ValueError(
+                "Unknown strategy type: %s, expected one of %s."
+                % (self.strategy, allowed_strategies)
+            )
+
+        self._strategy = self.strategy
+
+        if self._strategy == "uniform" and sp.issparse(y):
             y = y.toarray()
-            warnings.warn('A local copy of the target data has been converted '
-                          'to a numpy array. Predicting on sparse target data '
-                          'with the uniform strategy would not save memory '
-                          'and would be slower.',
-                          UserWarning)
+            warnings.warn(
+                "A local copy of the target data has been converted "
+                "to a numpy array. Predicting on sparse target data "
+                "with the uniform strategy would not save memory "
+                "and would be slower.",
+                UserWarning,
+            )

         self.sparse_output_ = sp.issparse(y)

         if not self.sparse_output_:
+            y = np.asarray(y)
             y = np.atleast_1d(y)
-
-        self.output_2d_ = y.ndim == 2 and y.shape[1] > 1

         if y.ndim == 1:
             y = np.reshape(y, (-1, 1))

         self.n_outputs_ = y.shape[1]

-        check_consistent_length(X, y, sample_weight)
-
-        if self.strategy == "constant":
+        check_consistent_length(X, y)
+
+        if sample_weight is not None:
+            sample_weight = _check_sample_weight(sample_weight, X)
+
+        if self._strategy == "constant":
             if self.constant is None:
-                raise ValueError("Constant target value has to be specified "
-                                 "when the constant strategy is used.")
+                raise ValueError(
+                    "Constant target value has to be specified "
+                    "when the constant strategy is used."
+                )
             else:
                 constant = np.reshape(np.atleast_1d(self.constant), (-1, 1))
                 if constant.shape[0] != self.n_outputs_:
-                    raise ValueError("Constant target value should have "
-                                     "shape (%d, 1)." % self.n_outputs_)
-
-        (self.classes_,
-         self.n_classes_,
-         self.class_prior_) = class_distribution(y, sample_weight)
-
-        if (self.strategy == "constant" and
-                any(constant[k] not in self.classes_[k]
-                    for k in range(self.n_outputs_))):
-            # Checking in case of constant strategy if the constant
-            # provided by the user is in y.
-            raise ValueError("The constant target value must be "
-                             "present in training data")
-
-        if self.n_outputs_ == 1 and not self.output_2d_:
+                    raise ValueError(
+                        "Constant target value should have shape (%d, 1)."
+                        % self.n_outputs_
+                    )
+
+        (self.classes_, self.n_classes_, self.class_prior_) = class_distribution(
+            y, sample_weight
+        )
+
+        if self._strategy == "constant":
+            for k in range(self.n_outputs_):
+                if not any(constant[k][0] == c for c in self.classes_[k]):
+                    # Checking in case of constant strategy if the constant
+                    # provided by the user is in y.
+                    err_msg = (
+                        "The constant target value must be present in "
+                        "the training data. You provided constant={}. "
+                        "Possible values are: {}.".format(
+                            self.constant, list(self.classes_[k])
+                        )
+                    )
+                    raise ValueError(err_msg)
+
+        if self.n_outputs_ == 1:
             self.n_classes_ = self.n_classes_[0]
             self.classes_ = self.classes_[0]
             self.class_prior_ = self.class_prior_[0]
@@ -163,15 +244,15 @@

         Parameters
         ----------
-        X : {array-like, object with finite length or shape}
-            Training data, requires length = n_samples
+        X : array-like of shape (n_samples, n_features)
+            Test data.

         Returns
         -------
-        y : array, shape = [n_samples] or [n_samples, n_outputs]
+        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
             Predicted target values for X.
         """
-        check_is_fitted(self, 'classes_')
+        check_is_fitted(self)

         # numpy random_state expects Python int and not long as size argument
         # under Windows
@@ -182,53 +263,65 @@
         classes_ = self.classes_
         class_prior_ = self.class_prior_
         constant = self.constant
-        if self.n_outputs_ == 1 and not self.output_2d_:
+        if self.n_outputs_ == 1:
             # Get same type even for self.n_outputs_ == 1
             n_classes_ = [n_classes_]
             classes_ = [classes_]
             class_prior_ = [class_prior_]
             constant = [constant]
         # Compute probability only once
-        if self.strategy == "stratified":
+        if self._strategy == "stratified":
             proba = self.predict_proba(X)
-            if self.n_outputs_ == 1 and not self.output_2d_:
+            if self.n_outputs_ == 1:
                 proba = [proba]

         if self.sparse_output_:
             class_prob = None
-            if self.strategy in ("most_frequent", "prior"):
+            if self._strategy in ("most_frequent", "prior"):
                 classes_ = [np.array([cp.argmax()]) for cp in class_prior_]

-            elif self.strategy == "stratified":
+            elif self._strategy == "stratified":
                 class_prob = class_prior_

-            elif self.strategy == "uniform":
-                raise ValueError("Sparse target prediction is not "
-                                 "supported with the uniform strategy")
-
-            elif self.strategy == "constant":
+            elif self._strategy == "uniform":
+                raise ValueError(
+                    "Sparse target prediction is not "
+                    "supported with the uniform strategy"
+                )
+
+            elif self._strategy == "constant":
                 classes_ = [np.array([c]) for c in constant]

-            y = random_choice_csc(n_samples, classes_, class_prob,
-                                  self.random_state)
+            y = _random_choice_csc(n_samples, classes_, class_prob, self.random_state)
         else:
-            if self.strategy in ("most_frequent", "prior"):
-                y = np.tile([classes_[k][class_prior_[k].argmax()] for
-                             k in range(self.n_outputs_)], [n_samples, 1])
-
-            elif self.strategy == "stratified":
-                y = np.vstack([classes_[k][proba[k].argmax(axis=1)] for
-                               k in range(self.n_outputs_)]).T
-
-            elif self.strategy == "uniform":
-                ret = [classes_[k][rs.randint(n_classes_[k], size=n_samples)]
-                       for k in range(self.n_outputs_)]
+            if self._strategy in ("most_frequent", "prior"):
+                y = np.tile(
+                    [
+                        classes_[k][class_prior_[k].argmax()]
+                        for k in range(self.n_outputs_)
+                    ],
+                    [n_samples, 1],
+                )
+
+            elif self._strategy == "stratified":
+                y = np.vstack(
+                    [
+                        classes_[k][proba[k].argmax(axis=1)]
+                        for k in range(self.n_outputs_)
+                    ]
+                ).T
+
+            elif self._strategy == "uniform":
+                ret = [
+                    classes_[k][rs.randint(n_classes_[k], size=n_samples)]
+                    for k in range(self.n_outputs_)
+                ]
                 y = np.vstack(ret).T

-            elif self.strategy == "constant":
+            elif self._strategy == "constant":
                 y = np.tile(self.constant, (n_samples, 1))

-            if self.n_outputs_ == 1 and not self.output_2d_:
+            if self.n_outputs_ == 1:
                 y = np.ravel(y)

         return y
@@ -239,17 +332,17 @@

         Parameters
         ----------
-        X : {array-like, object with finite length or shape}
-            Training data, requires length = n_samples
+        X : array-like of shape (n_samples, n_features)
+            Test data.

         Returns
         -------
-        P : array-like or list of array-lke of shape = [n_samples, n_classes]
+        P : ndarray of shape (n_samples, n_classes) or list of such arrays
             Returns the probability of the sample for each class in
             the model, where classes are ordered arithmetically, for each
             output.
         """
-        check_is_fitted(self, 'classes_')
+        check_is_fitted(self)

         # numpy random_state expects Python int and not long as size argument
         # under Windows
@@ -260,7 +353,7 @@
         classes_ = self.classes_
         class_prior_ = self.class_prior_
         constant = self.constant
-        if self.n_outputs_ == 1 and not self.output_2d_:
+        if self.n_outputs_ == 1:
             # Get same type even for self.n_outputs_ == 1
             n_classes_ = [n_classes_]
             classes_ = [classes_]
@@ -269,29 +362,29 @@

         P = []
         for k in range(self.n_outputs_):
-            if self.strategy == "most_frequent":
+            if self._strategy == "most_frequent":
                 ind = class_prior_[k].argmax()
                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
                 out[:, ind] = 1.0
-            elif self.strategy == "prior":
+            elif self._strategy == "prior":
                 out = np.ones((n_samples, 1)) * class_prior_[k]

-            elif self.strategy == "stratified":
+            elif self._strategy == "stratified":
                 out = rs.multinomial(1, class_prior_[k], size=n_samples)
                 out = out.astype(np.float64)

-            elif self.strategy == "uniform":
+            elif self._strategy == "uniform":
                 out = np.ones((n_samples, n_classes_[k]), dtype=np.float64)
                 out /= n_classes_[k]

-            elif self.strategy == "constant":
+            elif self._strategy == "constant":
                 ind = np.where(classes_[k] == constant[k])
                 out = np.zeros((n_samples, n_classes_[k]), dtype=np.float64)
                 out[:, ind] = 1.0

             P.append(out)

-        if self.n_outputs_ == 1 and not self.output_2d_:
+        if self.n_outputs_ == 1:
             P = P[0]

         return P
@@ -303,11 +396,11 @@
         Parameters
         ----------
         X : {array-like, object with finite length or shape}
-            Training data, requires length = n_samples
+            Training data.

         Returns
         -------
-        P : array-like or list of array-like of shape = [n_samples, n_classes]
+        P : ndarray of shape (n_samples, n_classes) or list of such arrays
             Returns the log probability of the sample for each class in
             the model, where classes are ordered arithmetically for each
             output.
@@ -319,10 +412,17 @@
             return [np.log(p) for p in proba]

     def _more_tags(self):
-        return {'poor_score': True, 'no_validation': True}
+        return {
+            "poor_score": True,
+            "no_validation": True,
+            "_xfail_checks": {
+                "check_methods_subset_invariance": "fails for the predict method",
+                "check_methods_sample_order_invariance": "fails for the predict method",
+            },
+        }

     def score(self, X, y, sample_weight=None):
-        """Returns the mean accuracy on the given test data and labels.
+        """Return the mean accuracy on the given test data and labels.

         In multi-label classification, this is the subset accuracy
         which is a harsh metric since you require for each sample that
@@ -330,42 +430,50 @@

         Parameters
         ----------
-        X : {array-like, None}
-            Test samples with shape = (n_samples, n_features) or
-            None. Passing None as test samples gives the same result
+        X : None or array-like of shape (n_samples, n_features)
+            Test samples. Passing None as test samples gives the same result
             as passing real test samples, since DummyClassifier
             operates independently of the sampled observations.

-        y : array-like, shape = (n_samples) or (n_samples, n_outputs)
+        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
             True labels for X.

-        sample_weight : array-like, shape = [n_samples], optional
+        sample_weight : array-like of shape (n_samples,), default=None
             Sample weights.

         Returns
         -------
         score : float
             Mean accuracy of self.predict(X) wrt. y.
-
         """
         if X is None:
             X = np.zeros(shape=(len(y), 1))
         return super().score(X, y, sample_weight)

-
-class DummyRegressor(BaseEstimator, RegressorMixin, MultiOutputMixin):
-    """
-    DummyRegressor is a regressor that makes predictions using
-    simple rules.
+    # TODO: Remove in 1.2
+    # mypy error: Decorated property not supported
+    @deprecated(  # type: ignore
+        "`n_features_in_` is deprecated in 1.0 and will be removed in 1.2."
+    )
+    @property
+    def n_features_in_(self):
+        check_is_fitted(self)
+        return None
+
+
+class DummyRegressor(MultiOutputMixin, RegressorMixin, BaseEstimator):
+    """Regressor that makes predictions using simple rules.

     This regressor is useful as a simple baseline to compare with other
     (real) regressors. Do not use it for real problems.

     Read more in the :ref:`User Guide <dummy_estimators>`.

+    .. versionadded:: 0.13
+
     Parameters
     ----------
-    strategy : str
+    strategy : {"mean", "median", "quantile", "constant"}, default="mean"
         Strategy to use to generate predictions.

         * "mean": always predicts the mean of the training set
@@ -375,26 +483,51 @@
         * "constant": always predicts a constant value that is provided by
           the user.

-    constant : int or float or array of shape = [n_outputs]
+    constant : int or float or array-like of shape (n_outputs,), default=None
         The explicit constant as predicted by the "constant" strategy. This
         parameter is useful only for the "constant" strategy.

-    quantile : float in [0.0, 1.0]
+    quantile : float in [0.0, 1.0], default=None
         The quantile to predict using the "quantile" strategy. A quantile of
         0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the
         maximum.

     Attributes
     ----------
-    constant_ : float or array of shape [n_outputs]
+    constant_ : ndarray of shape (1, n_outputs)
         Mean or median or quantile of the training targets or constant value
         given by the user.

-    n_outputs_ : int,
+    n_features_in_ : `None`
+        Always set to `None`.
+
+        .. versionadded:: 0.24
+        .. deprecated:: 1.0
+            Will be removed in 1.0
+
+    n_outputs_ : int
         Number of outputs.
+
+    See Also
+    --------
+    DummyClassifier: Classifier that makes predictions using simple rules.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.dummy import DummyRegressor
+    >>> X = np.array([1.0, 2.0, 3.0, 4.0])
+    >>> y = np.array([2.0, 3.0, 5.0, 10.0])
+    >>> dummy_regr = DummyRegressor(strategy="mean")
+    >>> dummy_regr.fit(X, y)
+    DummyRegressor()
+    >>> dummy_regr.predict(X)
+    array([5., 5., 5., 5.])
+    >>> dummy_regr.score(X, y)
+    0.0
     """

-    def __init__(self, strategy="mean", constant=None, quantile=None):
+    def __init__(self, *, strategy="mean", constant=None, quantile=None):
         self.strategy = strategy
         self.constant = constant
         self.quantile = quantile
@@ -404,35 +537,39 @@

         Parameters
         ----------
-        X : {array-like, object with finite length or shape}
-            Training data, requires length = n_samples
-
-        y : array-like, shape = [n_samples] or [n_samples, n_outputs]
+        X : array-like of shape (n_samples, n_features)
+            Training data.
+
+        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
             Target values.

-        sample_weight : array-like of shape = [n_samples], optional
+        sample_weight : array-like of shape (n_samples,), default=None
             Sample weights.

         Returns
         -------
         self : object
+            Fitted estimator.
         """
         allowed_strategies = ("mean", "median", "quantile", "constant")
         if self.strategy not in allowed_strategies:
-            raise ValueError("Unknown strategy type: %s, expected one of %s."
-                             % (self.strategy, allowed_strategies))
-
-        y = check_array(y, ensure_2d=False)
+            raise ValueError(
+                "Unknown strategy type: %s, expected one of %s."
+                % (self.strategy, allowed_strategies)
+            )
+
+        y = check_array(y, ensure_2d=False, input_name="y")
         if len(y) == 0:
             raise ValueError("y must not be empty.")
-
-        self.output_2d_ = y.ndim == 2 and y.shape[1] > 1

         if y.ndim == 1:
             y = np.reshape(y, (-1, 1))
         self.n_outputs_ = y.shape[1]

         check_consistent_length(X, y, sample_weight)
+
+        if sample_weight is not None:
+            sample_weight = _check_sample_weight(sample_weight, X)

         if self.strategy == "mean":
             self.constant_ = np.average(y, axis=0, weights=sample_weight)
@@ -441,113 +578,129 @@
             if sample_weight is None:
                 self.constant_ = np.median(y, axis=0)
             else:
-                self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
-                                                       percentile=50.)
-                                  for k in range(self.n_outputs_)]
+                self.constant_ = [
+                    _weighted_percentile(y[:, k], sample_weight, percentile=50.0)
+                    for k in range(self.n_outputs_)
+                ]

         elif self.strategy == "quantile":
             if self.quantile is None or not np.isscalar(self.quantile):
-                raise ValueError("Quantile must be a scalar in the range "
-                                 "[0.0, 1.0], but got %s." % self.quantile)
+                raise ValueError(
+                    "Quantile must be a scalar in the range [0.0, 1.0], but got %s."
+                    % self.quantile
+                )

             percentile = self.quantile * 100.0
             if sample_weight is None:
                 self.constant_ = np.percentile(y, axis=0, q=percentile)
             else:
-                self.constant_ = [_weighted_percentile(y[:, k], sample_weight,
-                                                       percentile=percentile)
-                                  for k in range(self.n_outputs_)]
+                self.constant_ = [
+                    _weighted_percentile(y[:, k], sample_weight, percentile=percentile)
+                    for k in range(self.n_outputs_)
+                ]

         elif self.strategy == "constant":
             if self.constant is None:
-                raise TypeError("Constant target value has to be specified "
-                                "when the constant strategy is used.")
-
-            self.constant = check_array(self.constant,
-                                        accept_sparse=['csr', 'csc', 'coo'],
-                                        ensure_2d=False, ensure_min_samples=0)
-
-            if self.output_2d_ and self.constant.shape[0] != y.shape[1]:
+                raise TypeError(
+                    "Constant target value has to be specified "
+                    "when the constant strategy is used."
+                )
+
+            self.constant_ = check_array(
+                self.constant,
+                accept_sparse=["csr", "csc", "coo"],
+                ensure_2d=False,
+                ensure_min_samples=0,
+            )
+
+            if self.n_outputs_ != 1 and self.constant_.shape[0] != y.shape[1]:
                 raise ValueError(
-                    "Constant target value should have "
-                    "shape (%d, 1)." % y.shape[1])
-
-            self.constant_ = self.constant
+                    "Constant target value should have shape (%d, 1)." % y.shape[1]
+                )

         self.constant_ = np.reshape(self.constant_, (1, -1))
         return self

     def predict(self, X, return_std=False):
-        """
-        Perform classification on test vectors X.
+        """Perform classification on test vectors X.

         Parameters
         ----------
-        X : {array-like, object with finite length or shape}
-            Training data, requires length = n_samples
-
-        return_std : boolean, optional
+        X : array-like of shape (n_samples, n_features)
+            Test data.
+
+        return_std : bool, default=False
             Whether to return the standard deviation of posterior prediction.
             All zeros in this case.

+            .. versionadded:: 0.20
+
         Returns
         -------
-        y : array, shape = [n_samples] or [n_samples, n_outputs]
+        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
             Predicted target values for X.

-        y_std : array, shape = [n_samples] or [n_samples, n_outputs]
+        y_std : array-like of shape (n_samples,) or (n_samples, n_outputs)
             Standard deviation of predictive distribution of query points.
         """
-        check_is_fitted(self, "constant_")
+        check_is_fitted(self)
         n_samples = _num_samples(X)

-        y = np.full((n_samples, self.n_outputs_), self.constant_,
-                    dtype=np.array(self.constant_).dtype)
+        y = np.full(
+            (n_samples, self.n_outputs_),
+            self.constant_,
+            dtype=np.array(self.constant_).dtype,
+        )
         y_std = np.zeros((n_samples, self.n_outputs_))

-        if self.n_outputs_ == 1 and not self.output_2d_:
+        if self.n_outputs_ == 1:
             y = np.ravel(y)
             y_std = np.ravel(y_std)

         return (y, y_std) if return_std else y

     def _more_tags(self):
-        return {'poor_score': True, 'no_validation': True}
+        return {"poor_score": True, "no_validation": True}

     def score(self, X, y, sample_weight=None):
-        """Returns the coefficient of determination R^2 of the prediction.
-
-        The coefficient R^2 is defined as (1 - u/v), where u is the residual
-        sum of squares ((y_true - y_pred) ** 2).sum() and v is the total
-        sum of squares ((y_true - y_true.mean()) ** 2).sum().
-        The best possible score is 1.0 and it can be negative (because the
-        model can be arbitrarily worse). A constant model that always
-        predicts the expected value of y, disregarding the input features,
-        would get a R^2 score of 0.0.
+        """Return the coefficient of determination R^2 of the prediction.
+
+        The coefficient R^2 is defined as `(1 - u/v)`, where `u` is the
+        residual sum of squares `((y_true - y_pred) ** 2).sum()` and `v` is the
+        total sum of squares `((y_true - y_true.mean()) ** 2).sum()`. The best
+        possible score is 1.0 and it can be negative (because the model can be
+        arbitrarily worse). A constant model that always predicts the expected
+        value of y, disregarding the input features, would get a R^2 score of
+        0.0.

         Parameters
         ----------
-        X : {array-like, None}
-            Test samples with shape = (n_samples, n_features) or None.
-            For some estimators this may be a
-            precomputed kernel matrix instead, shape = (n_samples,
-            n_samples_fitted], where n_samples_fitted is the number of
-            samples used in the fitting for the estimator.
-            Passing None as test samples gives the same result
-            as passing real test samples, since DummyRegressor
+        X : None or array-like of shape (n_samples, n_features)
+            Test samples. Passing None as test samples gives the same result
+            as passing real test samples, since `DummyRegressor`
             operates independently of the sampled observations.

-        y : array-like, shape = (n_samples) or (n_samples, n_outputs)
+        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
             True values for X.

-        sample_weight : array-like, shape = [n_samples], optional
+        sample_weight : array-like of shape (n_samples,), default=None
             Sample weights.

         Returns
         -------
         score : float
-            R^2 of self.predict(X) wrt. y.
+            R^2 of `self.predict(X)` wrt. y.
         """
         if X is None:
             X = np.zeros(shape=(len(y), 1))
         return super().score(X, y, sample_weight)
+
+    # TODO: Remove in 1.2
+    # mypy error: Decorated property not supported
+    @deprecated(  # type: ignore
+        "`n_features_in_` is deprecated in 1.0 and will be removed in 1.2."
+    )
+    @property
+    def n_features_in_(self):
+        check_is_fitted(self)
+        return None
('sklearn', 'base.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -13,51 +13,74 @@
 import numpy as np

 from . import __version__
+from ._config import get_config
 from .utils import _IS_32BIT
-
-_DEFAULT_TAGS = {
-    'non_deterministic': False,
-    'requires_positive_data': False,
-    'X_types': ['2darray'],
-    'poor_score': False,
-    'no_validation': False,
-    'multioutput': False,
-    "allow_nan": False,
-    'stateless': False,
-    'multilabel': False,
-    '_skip_test': False,
-    'multioutput_only': False}
-
-
-def clone(estimator, safe=True):
-    """Constructs a new estimator with the same parameters.
+from .utils._tags import (
+    _DEFAULT_TAGS,
+)
+from .utils.validation import check_X_y
+from .utils.validation import check_array
+from .utils.validation import _check_y
+from .utils.validation import _num_features
+from .utils.validation import _check_feature_names_in
+from .utils.validation import _generate_get_feature_names_out
+from .utils.validation import check_is_fitted
+from .utils._estimator_html_repr import estimator_html_repr
+from .utils.validation import _get_feature_names
+
+
+def clone(estimator, *, safe=True):
+    """Construct a new unfitted estimator with the same parameters.

     Clone does a deep copy of the model in an estimator
-    without actually copying attached data. It yields a new estimator
-    with the same parameters that has not been fit on any data.
+    without actually copying attached data. It returns a new estimator
+    with the same parameters that has not been fitted on any data.

     Parameters
     ----------
-    estimator : estimator object, or list, tuple or set of objects
-        The estimator or group of estimators to be cloned
-
-    safe : boolean, optional
-        If safe is false, clone will fall back to a deep copy on objects
+    estimator : {list, tuple, set} of estimator instance or a single \
+            estimator instance
+        The estimator or group of estimators to be cloned.
+    safe : bool, default=True
+        If safe is False, clone will fall back to a deep copy on objects
         that are not estimators.

+    Returns
+    -------
+    estimator : object
+        The deep copy of the input, an estimator if input is an estimator.
+
+    Notes
+    -----
+    If the estimator's `random_state` parameter is an integer (or if the
+    estimator doesn't have a `random_state` parameter), an *exact clone* is
+    returned: the clone and the original estimator will give the exact same
+    results. Otherwise, *statistical clone* is returned: the clone might
+    return different results from the original estimator. More details can be
+    found in :ref:`randomness`.
     """
     estimator_type = type(estimator)
     # XXX: not handling dictionaries
     if estimator_type in (list, tuple, set, frozenset):
         return estimator_type([clone(e, safe=safe) for e in estimator])
-    elif not hasattr(estimator, 'get_params') or isinstance(estimator, type):
+    elif not hasattr(estimator, "get_params") or isinstance(estimator, type):
         if not safe:
             return copy.deepcopy(estimator)
         else:
-            raise TypeError("Cannot clone object '%s' (type %s): "
-                            "it does not seem to be a scikit-learn estimator "
-                            "as it does not implement a 'get_params' methods."
-                            % (repr(estimator), type(estimator)))
+            if isinstance(estimator, type):
+                raise TypeError(
+                    "Cannot clone object. "
+                    + "You should provide an instance of "
+                    + "scikit-learn estimator instead of a class."
+                )
+            else:
+                raise TypeError(
+                    "Cannot clone object '%s' (type %s): "
+                    "it does not seem to be a scikit-learn "
+                    "estimator as it does not implement a "
+                    "'get_params' method." % (repr(estimator), type(estimator))
+                )
+
     klass = estimator.__class__
     new_object_params = estimator.get_params(deep=False)
     for name, param in new_object_params.items():
@@ -70,9 +93,10 @@
         param1 = new_object_params[name]
         param2 = params_set[name]
         if param1 is not param2:
-            raise RuntimeError('Cannot clone object %s, as the constructor '
-                               'either does not set or modifies parameter %s' %
-                               (estimator, name))
+            raise RuntimeError(
+                "Cannot clone object %s, as the constructor "
+                "either does not set or modifies parameter %s" % (estimator, name)
+            )
     return new_object


@@ -84,10 +108,10 @@
     params : dict
         The dictionary to pretty print

-    offset : int
+    offset : int, default=0
         The offset in characters to add at the begin of each line.

-    printer : callable
+    printer : callable, default=repr
         The function to convert entries to strings, typically
         the builtin str or repr

@@ -97,48 +121,37 @@
     np.set_printoptions(precision=5, threshold=64, edgeitems=2)
     params_list = list()
     this_line_length = offset
-    line_sep = ',\n' + (1 + offset // 2) * ' '
+    line_sep = ",\n" + (1 + offset // 2) * " "
     for i, (k, v) in enumerate(sorted(params.items())):
         if type(v) is float:
             # use str for representing floating point numbers
             # this way we get consistent representation across
             # architectures and versions.
-            this_repr = '%s=%s' % (k, str(v))
+            this_repr = "%s=%s" % (k, str(v))
         else:
             # use repr of the rest
-            this_repr = '%s=%s' % (k, printer(v))
+            this_repr = "%s=%s" % (k, printer(v))
         if len(this_repr) > 500:
-            this_repr = this_repr[:300] + '...' + this_repr[-100:]
+            this_repr = this_repr[:300] + "..." + this_repr[-100:]
         if i > 0:
-            if (this_line_length + len(this_repr) >= 75 or '\n' in this_repr):
+            if this_line_length + len(this_repr) >= 75 or "\n" in this_repr:
                 params_list.append(line_sep)
                 this_line_length = len(line_sep)
             else:
-                params_list.append(', ')
+                params_list.append(", ")
                 this_line_length += 2
         params_list.append(this_repr)
         this_line_length += len(this_repr)

     np.set_printoptions(**options)
-    lines = ''.join(params_list)
+    lines = "".join(params_list)
     # Strip trailing space to avoid nightmare in doctests
-    lines = '\n'.join(l.rstrip(' ') for l in lines.split('\n'))
+    lines = "\n".join(l.rstrip(" ") for l in lines.split("\n"))
     return lines


-def _update_if_consistent(dict1, dict2):
-    common_keys = set(dict1.keys()).intersection(dict2.keys())
-    for key in common_keys:
-        if dict1[key] != dict2[key]:
-            raise TypeError("Inconsistent values for tag {}: {} != {}".format(
-                key, dict1[key], dict2[key]
-            ))
-    dict1.update(dict2)
-    return dict1
-
-
 class BaseEstimator:
-    """Base class for all estimators in scikit-learn
+    """Base class for all estimators in scikit-learn.

     Notes
     -----
@@ -152,7 +165,7 @@
         """Get parameter names for the estimator"""
         # fetch the constructor or the original constructor before
         # deprecation wrapping if any
-        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)
+        init = getattr(cls.__init__, "deprecated_original", cls.__init__)
         if init is object.__init__:
             # No explicit constructor to introspect
             return []
@@ -161,39 +174,44 @@
         # to represent
         init_signature = inspect.signature(init)
         # Consider the constructor parameters excluding 'self'
-        parameters = [p for p in init_signature.parameters.values()
-                      if p.name != 'self' and p.kind != p.VAR_KEYWORD]
+        parameters = [
+            p
+            for p in init_signature.parameters.values()
+            if p.name != "self" and p.kind != p.VAR_KEYWORD
+        ]
         for p in parameters:
             if p.kind == p.VAR_POSITIONAL:
-                raise RuntimeError("scikit-learn estimators should always "
-                                   "specify their parameters in the signature"
-                                   " of their __init__ (no varargs)."
-                                   " %s with constructor %s doesn't "
-                                   " follow this convention."
-                                   % (cls, init_signature))
+                raise RuntimeError(
+                    "scikit-learn estimators should always "
+                    "specify their parameters in the signature"
+                    " of their __init__ (no varargs)."
+                    " %s with constructor %s doesn't "
+                    " follow this convention." % (cls, init_signature)
+                )
         # Extract and sort argument names excluding 'self'
         return sorted([p.name for p in parameters])

     def get_params(self, deep=True):
-        """Get parameters for this estimator.
-
-        Parameters
-        ----------
-        deep : boolean, optional
+        """
+        Get parameters for this estimator.
+
+        Parameters
+        ----------
+        deep : bool, default=True
             If True, will return the parameters for this estimator and
             contained subobjects that are estimators.

         Returns
         -------
-        params : mapping of string to any
+        params : dict
             Parameter names mapped to their values.
         """
         out = dict()
         for key in self._get_param_names():
-            value = getattr(self, key, None)
-            if deep and hasattr(value, 'get_params'):
+            value = getattr(self, key)
+            if deep and hasattr(value, "get_params"):
                 deep_items = value.get_params().items()
-                out.update((key + '__' + k, val) for k, val in deep_items)
+                out.update((key + "__" + k, val) for k, val in deep_items)
             out[key] = value
         return out

@@ -201,13 +219,19 @@
         """Set the parameters of this estimator.

         The method works on simple estimators as well as on nested objects
-        (such as pipelines). The latter have parameters of the form
-        ``<component>__<parameter>`` so that it's possible to update each
-        component of a nested object.
-
-        Returns
-        -------
-        self
+        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
+        parameters of the form ``<component>__<parameter>`` so that it's
+        possible to update each component of a nested object.
+
+        Parameters
+        ----------
+        **params : dict
+            Estimator parameters.
+
+        Returns
+        -------
+        self : estimator instance
+            Estimator instance.
         """
         if not params:
             # Simple optimization to gain speed (inspect is slow)
@@ -216,12 +240,13 @@

         nested_params = defaultdict(dict)  # grouped by prefix
         for key, value in params.items():
-            key, delim, sub_key = key.partition('__')
+            key, delim, sub_key = key.partition("__")
             if key not in valid_params:
-                raise ValueError('Invalid parameter %s for estimator %s. '
-                                 'Check the list of available parameters '
-                                 'with `estimator.get_params().keys()`.' %
-                                 (key, self))
+                local_valid_params = self._get_param_names()
+                raise ValueError(
+                    f"Invalid parameter {key!r} for estimator {self}. "
+                    f"Valid parameters are: {local_valid_params!r}."
+                )

             if delim:
                 nested_params[key][sub_key] = value
@@ -245,16 +270,19 @@

         # use ellipsis for sequences with a lot of elements
         pp = _EstimatorPrettyPrinter(
-            compact=True, indent=1, indent_at_name=True,
-            n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)
+            compact=True,
+            indent=1,
+            indent_at_name=True,
+            n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW,
+        )

         repr_ = pp.pformat(self)

         # Use bruteforce ellipsis when there are a lot of non-blank characters
-        n_nonblank = len(''.join(repr_.split()))
+        n_nonblank = len("".join(repr_.split()))
         if n_nonblank > N_CHAR_MAX:
             lim = N_CHAR_MAX // 2  # apprx number of chars to keep on both ends
-            regex = r'^(\s*\S){%d}' % lim
+            regex = r"^(\s*\S){%d}" % lim
             # The regex '^(\s*\S){%d}' % n
             # matches from the start of the string until the nth non-blank
             # character:
@@ -264,7 +292,7 @@
             left_lim = re.match(regex, repr_).end()
             right_lim = re.match(regex, repr_[::-1]).end()

-            if '\n' in repr_[left_lim:-right_lim]:
+            if "\n" in repr_[left_lim:-right_lim]:
                 # The left side and right side aren't on the same line.
                 # To avoid weird cuts, e.g.:
                 # categoric...ore',
@@ -273,13 +301,13 @@
                 # categoric...
                 # handle_unknown='ignore',
                 # so we add [^\n]*\n which matches until the next \n
-                regex += r'[^\n]*\n'
+                regex += r"[^\n]*\n"
                 right_lim = re.match(regex, repr_[::-1]).end()

-            ellipsis = '...'
+            ellipsis = "..."
             if left_lim + len(ellipsis) < len(repr_) - right_lim:
                 # Only add ellipsis if it results in a shorter repr
-                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]
+                repr_ = repr_[:left_lim] + "..." + repr_[-right_lim:]

         return repr_

@@ -289,48 +317,329 @@
         except AttributeError:
             state = self.__dict__.copy()

-        if type(self).__module__.startswith('sklearn.'):
+        if type(self).__module__.startswith("sklearn."):
             return dict(state.items(), _sklearn_version=__version__)
         else:
             return state

     def __setstate__(self, state):
-        if type(self).__module__.startswith('sklearn.'):
+        if type(self).__module__.startswith("sklearn."):
             pickle_version = state.pop("_sklearn_version", "pre-0.18")
             if pickle_version != __version__:
                 warnings.warn(
                     "Trying to unpickle estimator {0} from version {1} when "
                     "using version {2}. This might lead to breaking code or "
-                    "invalid results. Use at your own risk.".format(
-                        self.__class__.__name__, pickle_version, __version__),
-                    UserWarning)
+                    "invalid results. Use at your own risk. "
+                    "For more info please refer to:\n"
+                    "https://scikit-learn.org/stable/modules/model_persistence"
+                    ".html#security-maintainability-limitations".format(
+                        self.__class__.__name__, pickle_version, __version__
+                    ),
+                    UserWarning,
+                )
         try:
             super().__setstate__(state)
         except AttributeError:
             self.__dict__.update(state)

+    def _more_tags(self):
+        return _DEFAULT_TAGS
+
     def _get_tags(self):
         collected_tags = {}
-        for base_class in inspect.getmro(self.__class__):
-            if (hasattr(base_class, '_more_tags')
-                    and base_class != self.__class__):
+        for base_class in reversed(inspect.getmro(self.__class__)):
+            if hasattr(base_class, "_more_tags"):
+                # need the if because mixins might not have _more_tags
+                # but might do redundant work in estimators
+                # (i.e. calling more tags on BaseEstimator multiple times)
                 more_tags = base_class._more_tags(self)
-                collected_tags = _update_if_consistent(collected_tags,
-                                                       more_tags)
-        if hasattr(self, '_more_tags'):
-            more_tags = self._more_tags()
-            collected_tags = _update_if_consistent(collected_tags, more_tags)
-        tags = _DEFAULT_TAGS.copy()
-        tags.update(collected_tags)
-        return tags
+                collected_tags.update(more_tags)
+        return collected_tags
+
+    def _check_n_features(self, X, reset):
+        """Set the `n_features_in_` attribute, or check against it.
+
+        Parameters
+        ----------
+        X : {ndarray, sparse matrix} of shape (n_samples, n_features)
+            The input samples.
+        reset : bool
+            If True, the `n_features_in_` attribute is set to `X.shape[1]`.
+            If False and the attribute exists, then check that it is equal to
+            `X.shape[1]`. If False and the attribute does *not* exist, then
+            the check is skipped.
+            .. note::
+               It is recommended to call reset=True in `fit` and in the first
+               call to `partial_fit`. All other methods that validate `X`
+               should set `reset=False`.
+        """
+        try:
+            n_features = _num_features(X)
+        except TypeError as e:
+            if not reset and hasattr(self, "n_features_in_"):
+                raise ValueError(
+                    "X does not contain any features, but "
+                    f"{self.__class__.__name__} is expecting "
+                    f"{self.n_features_in_} features"
+                ) from e
+            # If the number of features is not defined and reset=True,
+            # then we skip this check
+            return
+
+        if reset:
+            self.n_features_in_ = n_features
+            return
+
+        if not hasattr(self, "n_features_in_"):
+            # Skip this check if the expected number of expected input features
+            # was not recorded by calling fit first. This is typically the case
+            # for stateless transformers.
+            return
+
+        if n_features != self.n_features_in_:
+            raise ValueError(
+                f"X has {n_features} features, but {self.__class__.__name__} "
+                f"is expecting {self.n_features_in_} features as input."
+            )
+
+    def _check_feature_names(self, X, *, reset):
+        """Set or check the `feature_names_in_` attribute.
+
+        .. versionadded:: 1.0
+
+        Parameters
+        ----------
+        X : {ndarray, dataframe} of shape (n_samples, n_features)
+            The input samples.
+
+        reset : bool
+            Whether to reset the `feature_names_in_` attribute.
+            If False, the input will be checked for consistency with
+            feature names of data provided when reset was last True.
+            .. note::
+               It is recommended to call `reset=True` in `fit` and in the first
+               call to `partial_fit`. All other methods that validate `X`
+               should set `reset=False`.
+        """
+
+        if reset:
+            feature_names_in = _get_feature_names(X)
+            if feature_names_in is not None:
+                self.feature_names_in_ = feature_names_in
+            elif hasattr(self, "feature_names_in_"):
+                # Delete the attribute when the estimator is fitted on a new dataset
+                # that has no feature names.
+                delattr(self, "feature_names_in_")
+            return
+
+        fitted_feature_names = getattr(self, "feature_names_in_", None)
+        X_feature_names = _get_feature_names(X)
+
+        if fitted_feature_names is None and X_feature_names is None:
+            # no feature names seen in fit and in X
+            return
+
+        if X_feature_names is not None and fitted_feature_names is None:
+            warnings.warn(
+                f"X has feature names, but {self.__class__.__name__} was fitted without"
+                " feature names"
+            )
+            return
+
+        if X_feature_names is None and fitted_feature_names is not None:
+            warnings.warn(
+                "X does not have valid feature names, but"
+                f" {self.__class__.__name__} was fitted with feature names"
+            )
+            return
+
+        # validate the feature names against the `feature_names_in_` attribute
+        if len(fitted_feature_names) != len(X_feature_names) or np.any(
+            fitted_feature_names != X_feature_names
+        ):
+            message = (
+                "The feature names should match those that were "
+                "passed during fit. Starting version 1.2, an error will be raised.\n"
+            )
+            fitted_feature_names_set = set(fitted_feature_names)
+            X_feature_names_set = set(X_feature_names)
+
+            unexpected_names = sorted(X_feature_names_set - fitted_feature_names_set)
+            missing_names = sorted(fitted_feature_names_set - X_feature_names_set)
+
+            def add_names(names):
+                output = ""
+                max_n_names = 5
+                for i, name in enumerate(names):
+                    if i >= max_n_names:
+                        output += "- ...\n"
+                        break
+                    output += f"- {name}\n"
+                return output
+
+            if unexpected_names:
+                message += "Feature names unseen at fit time:\n"
+                message += add_names(unexpected_names)
+
+            if missing_names:
+                message += "Feature names seen at fit time, yet now missing:\n"
+                message += add_names(missing_names)
+
+            if not missing_names and not unexpected_names:
+                message += (
+                    "Feature names must be in the same order as they were in fit.\n"
+                )
+
+            warnings.warn(message, FutureWarning)
+
+    def _validate_data(
+        self,
+        X="no_validation",
+        y="no_validation",
+        reset=True,
+        validate_separately=False,
+        **check_params,
+    ):
+        """Validate input data and set or check the `n_features_in_` attribute.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix, dataframe} of shape \
+                (n_samples, n_features), default='no validation'
+            The input samples.
+            If `'no_validation'`, no validation is performed on `X`. This is
+            useful for meta-estimator which can delegate input validation to
+            their underlying estimator(s). In that case `y` must be passed and
+            the only accepted `check_params` are `multi_output` and
+            `y_numeric`.
+
+        y : array-like of shape (n_samples,), default='no_validation'
+            The targets.
+
+            - If `None`, `check_array` is called on `X`. If the estimator's
+              requires_y tag is True, then an error will be raised.
+            - If `'no_validation'`, `check_array` is called on `X` and the
+              estimator's requires_y tag is ignored. This is a default
+              placeholder and is never meant to be explicitly set. In that case
+              `X` must be passed.
+            - Otherwise, only `y` with `_check_y` or both `X` and `y` are
+              checked with either `check_array` or `check_X_y` depending on
+              `validate_separately`.
+
+        reset : bool, default=True
+            Whether to reset the `n_features_in_` attribute.
+            If False, the input will be checked for consistency with data
+            provided when reset was last True.
+            .. note::
+               It is recommended to call reset=True in `fit` and in the first
+               call to `partial_fit`. All other methods that validate `X`
+               should set `reset=False`.
+
+        validate_separately : False or tuple of dicts, default=False
+            Only used if y is not None.
+            If False, call validate_X_y(). Else, it must be a tuple of kwargs
+            to be used for calling check_array() on X and y respectively.
+
+            `estimator=self` is automatically added to these dicts to generate
+            more informative error message in case of invalid input data.
+
+        **check_params : kwargs
+            Parameters passed to :func:`sklearn.utils.check_array` or
+            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately
+            is not False.
+
+            `estimator=self` is automatically added to these params to generate
+            more informative error message in case of invalid input data.
+
+        Returns
+        -------
+        out : {ndarray, sparse matrix} or tuple of these
+            The validated input. A tuple is returned if both `X` and `y` are
+            validated.
+        """
+        self._check_feature_names(X, reset=reset)
+
+        if y is None and self._get_tags()["requires_y"]:
+            raise ValueError(
+                f"This {self.__class__.__name__} estimator "
+                "requires y to be passed, but the target y is None."
+            )
+
+        no_val_X = isinstance(X, str) and X == "no_validation"
+        no_val_y = y is None or isinstance(y, str) and y == "no_validation"
+
+        default_check_params = {"estimator": self}
+        check_params = {**default_check_params, **check_params}
+
+        if no_val_X and no_val_y:
+            raise ValueError("Validation should be done on X, y or both.")
+        elif not no_val_X and no_val_y:
+            X = check_array(X, input_name="X", **check_params)
+            out = X
+        elif no_val_X and not no_val_y:
+            y = _check_y(y, **check_params)
+            out = y
+        else:
+            if validate_separately:
+                # We need this because some estimators validate X and y
+                # separately, and in general, separately calling check_array()
+                # on X and y isn't equivalent to just calling check_X_y()
+                # :(
+                check_X_params, check_y_params = validate_separately
+                if "estimator" not in check_X_params:
+                    check_X_params = {**default_check_params, **check_X_params}
+                X = check_array(X, input_name="X", **check_X_params)
+                if "estimator" not in check_y_params:
+                    check_y_params = {**default_check_params, **check_y_params}
+                y = check_array(y, input_name="y", **check_y_params)
+            else:
+                X, y = check_X_y(X, y, **check_params)
+            out = X, y
+
+        if not no_val_X and check_params.get("ensure_2d", True):
+            self._check_n_features(X, reset=reset)
+
+        return out
+
+    @property
+    def _repr_html_(self):
+        """HTML representation of estimator.
+
+        This is redundant with the logic of `_repr_mimebundle_`. The latter
+        should be favorted in the long term, `_repr_html_` is only
+        implemented for consumers who do not interpret `_repr_mimbundle_`.
+        """
+        if get_config()["display"] != "diagram":
+            raise AttributeError(
+                "_repr_html_ is only defined when the "
+                "'display' configuration option is set to "
+                "'diagram'"
+            )
+        return self._repr_html_inner
+
+    def _repr_html_inner(self):
+        """This function is returned by the @property `_repr_html_` to make
+        `hasattr(estimator, "_repr_html_") return `True` or `False` depending
+        on `get_config()["display"]`.
+        """
+        return estimator_html_repr(self)
+
+    def _repr_mimebundle_(self, **kwargs):
+        """Mime bundle used by jupyter kernels to display estimator"""
+        output = {"text/plain": repr(self)}
+        if get_config()["display"] == "diagram":
+            output["text/html"] = estimator_html_repr(self)
+        return output


 class ClassifierMixin:
     """Mixin class for all classifiers in scikit-learn."""
+
     _estimator_type = "classifier"

     def score(self, X, y, sample_weight=None):
-        """Returns the mean accuracy on the given test data and labels.
+        """
+        Return the mean accuracy on the given test data and labels.

         In multi-label classification, this is the subset accuracy
         which is a harsh metric since you require for each sample that
@@ -338,118 +647,116 @@

         Parameters
         ----------
-        X : array-like, shape = (n_samples, n_features)
+        X : array-like of shape (n_samples, n_features)
             Test samples.

-        y : array-like, shape = (n_samples) or (n_samples, n_outputs)
-            True labels for X.
-
-        sample_weight : array-like, shape = [n_samples], optional
+        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
+            True labels for `X`.
+
+        sample_weight : array-like of shape (n_samples,), default=None
             Sample weights.

         Returns
         -------
         score : float
-            Mean accuracy of self.predict(X) wrt. y.
-
+            Mean accuracy of ``self.predict(X)`` wrt. `y`.
         """
         from .metrics import accuracy_score
+
         return accuracy_score(y, self.predict(X), sample_weight=sample_weight)
+
+    def _more_tags(self):
+        return {"requires_y": True}


 class RegressorMixin:
     """Mixin class for all regression estimators in scikit-learn."""
+
     _estimator_type = "regressor"

     def score(self, X, y, sample_weight=None):
-        """Returns the coefficient of determination R^2 of the prediction.
-
-        The coefficient R^2 is defined as (1 - u/v), where u is the residual
-        sum of squares ((y_true - y_pred) ** 2).sum() and v is the total
-        sum of squares ((y_true - y_true.mean()) ** 2).sum().
+        """Return the coefficient of determination of the prediction.
+
+        The coefficient of determination :math:`R^2` is defined as
+        :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual
+        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`
+        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.
         The best possible score is 1.0 and it can be negative (because the
-        model can be arbitrarily worse). A constant model that always
-        predicts the expected value of y, disregarding the input features,
-        would get a R^2 score of 0.0.
-
-        Parameters
-        ----------
-        X : array-like, shape = (n_samples, n_features)
-            Test samples. For some estimators this may be a
-            precomputed kernel matrix instead, shape = (n_samples,
-            n_samples_fitted], where n_samples_fitted is the number of
-            samples used in the fitting for the estimator.
-
-        y : array-like, shape = (n_samples) or (n_samples, n_outputs)
-            True values for X.
-
-        sample_weight : array-like, shape = [n_samples], optional
+        model can be arbitrarily worse). A constant model that always predicts
+        the expected value of `y`, disregarding the input features, would get
+        a :math:`R^2` score of 0.0.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            Test samples. For some estimators this may be a precomputed
+            kernel matrix or a list of generic objects instead with shape
+            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``
+            is the number of samples used in the fitting for the estimator.
+
+        y : array-like of shape (n_samples,) or (n_samples, n_outputs)
+            True values for `X`.
+
+        sample_weight : array-like of shape (n_samples,), default=None
             Sample weights.

         Returns
         -------
         score : float
-            R^2 of self.predict(X) wrt. y.
+            :math:`R^2` of ``self.predict(X)`` wrt. `y`.

         Notes
         -----
-        The R2 score used when calling ``score`` on a regressor will use
+        The :math:`R^2` score used when calling ``score`` on a regressor uses
         ``multioutput='uniform_average'`` from version 0.23 to keep consistent
-        with `metrics.r2_score`. This will influence the ``score`` method of
-        all the multioutput regressors (except for
-        `multioutput.MultiOutputRegressor`). To specify the default value
-        manually and avoid the warning, please either call `metrics.r2_score`
-        directly or make a custom scorer with `metrics.make_scorer` (the
-        built-in scorer ``'r2'`` uses ``multioutput='uniform_average'``).
+        with default value of :func:`~sklearn.metrics.r2_score`.
+        This influences the ``score`` method of all the multioutput
+        regressors (except for
+        :class:`~sklearn.multioutput.MultiOutputRegressor`).
         """

         from .metrics import r2_score
-        from .metrics.regression import _check_reg_targets
+
         y_pred = self.predict(X)
-        # XXX: Remove the check in 0.23
-        y_type, _, _, _ = _check_reg_targets(y, y_pred, None)
-        if y_type == 'continuous-multioutput':
-            warnings.warn("The default value of multioutput (not exposed in "
-                          "score method) will change from 'variance_weighted' "
-                          "to 'uniform_average' in 0.23 to keep consistent "
-                          "with 'metrics.r2_score'. To specify the default "
-                          "value manually and avoid the warning, please "
-                          "either call 'metrics.r2_score' directly or make a "
-                          "custom scorer with 'metrics.make_scorer' (the "
-                          "built-in scorer 'r2' uses "
-                          "multioutput='uniform_average').", FutureWarning)
-        return r2_score(y, y_pred, sample_weight=sample_weight,
-                        multioutput='variance_weighted')
+        return r2_score(y, y_pred, sample_weight=sample_weight)
+
+    def _more_tags(self):
+        return {"requires_y": True}


 class ClusterMixin:
     """Mixin class for all cluster estimators in scikit-learn."""
+
     _estimator_type = "clusterer"

     def fit_predict(self, X, y=None):
-        """Performs clustering on X and returns cluster labels.
-
-        Parameters
-        ----------
-        X : ndarray, shape (n_samples, n_features)
+        """
+        Perform clustering on `X` and returns cluster labels.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
             Input data.

         y : Ignored
-            not used, present for API consistency by convention.
-
-        Returns
-        -------
-        labels : ndarray, shape (n_samples,)
-            cluster labels
+            Not used, present for API consistency by convention.
+
+        Returns
+        -------
+        labels : ndarray of shape (n_samples,), dtype=np.int64
+            Cluster labels.
         """
         # non-optimized default implementation; override when a better
         # method is possible for a given clustering algorithm
         self.fit(X)
         return self.labels_

+    def _more_tags(self):
+        return {"preserves_dtype": []}
+

 class BiclusterMixin:
-    """Mixin class for all bicluster estimators in scikit-learn"""
+    """Mixin class for all bicluster estimators in scikit-learn."""

     @property
     def biclusters_(self):
@@ -460,7 +767,7 @@
         return self.rows_, self.columns_

     def get_indices(self, i):
-        """Row and column indices of the i'th bicluster.
+        """Row and column indices of the `i`'th bicluster.

         Only works if ``rows_`` and ``columns_`` attributes exist.

@@ -471,18 +778,17 @@

         Returns
         -------
-        row_ind : np.array, dtype=np.intp
+        row_ind : ndarray, dtype=np.intp
             Indices of rows in the dataset that belong to the bicluster.
-        col_ind : np.array, dtype=np.intp
+        col_ind : ndarray, dtype=np.intp
             Indices of columns in the dataset that belong to the bicluster.
-
         """
         rows = self.rows_[i]
         columns = self.columns_[i]
         return np.nonzero(rows)[0], np.nonzero(columns)[0]

     def get_shape(self, i):
-        """Shape of the i'th bicluster.
+        """Shape of the `i`'th bicluster.

         Parameters
         ----------
@@ -491,26 +797,29 @@

         Returns
         -------
-        shape : (int, int)
-            Number of rows and columns (resp.) in the bicluster.
+        n_rows : int
+            Number of rows in the bicluster.
+
+        n_cols : int
+            Number of columns in the bicluster.
         """
         indices = self.get_indices(i)
         return tuple(len(i) for i in indices)

     def get_submatrix(self, i, data):
-        """Returns the submatrix corresponding to bicluster `i`.
+        """Return the submatrix corresponding to bicluster `i`.

         Parameters
         ----------
         i : int
             The index of the cluster.
-        data : array
+        data : array-like of shape (n_samples, n_features)
             The data.

         Returns
         -------
-        submatrix : array
-            The submatrix corresponding to bicluster i.
+        submatrix : ndarray of shape (n_rows, n_cols)
+            The submatrix corresponding to bicluster `i`.

         Notes
         -----
@@ -518,7 +827,8 @@
         ``columns_`` attributes exist.
         """
         from .utils.validation import check_array
-        data = check_array(data, accept_sparse='csr')
+
+        data = check_array(data, accept_sparse="csr")
         row_ind, col_ind = self.get_indices(i)
         return data[row_ind[:, np.newaxis], col_ind]

@@ -527,24 +837,28 @@
     """Mixin class for all transformers in scikit-learn."""

     def fit_transform(self, X, y=None, **fit_params):
-        """Fit to data, then transform it.
-
-        Fits transformer to X and y with optional parameters fit_params
-        and returns a transformed version of X.
-
-        Parameters
-        ----------
-        X : numpy array of shape [n_samples, n_features]
-            Training set.
-
-        y : numpy array of shape [n_samples]
-            Target values.
-
-        Returns
-        -------
-        X_new : numpy array of shape [n_samples, n_features_new]
+        """
+        Fit to data, then transform it.
+
+        Fits transformer to `X` and `y` with optional parameters `fit_params`
+        and returns a transformed version of `X`.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            Input samples.
+
+        y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \
+                default=None
+            Target values (None for unsupervised transformations).
+
+        **fit_params : dict
+            Additional fit parameters.
+
+        Returns
+        -------
+        X_new : ndarray array of shape (n_samples, n_features_new)
             Transformed array.
-
         """
         # non-optimized default implementation; override when a better
         # method is possible for a given clustering algorithm
@@ -556,16 +870,76 @@
             return self.fit(X, y, **fit_params).transform(X)


+class _OneToOneFeatureMixin:
+    """Provides `get_feature_names_out` for simple transformers.
+
+    Assumes there's a 1-to-1 correspondence between input features
+    and output features.
+    """
+
+    def get_feature_names_out(self, input_features=None):
+        """Get output feature names for transformation.
+
+        Parameters
+        ----------
+        input_features : array-like of str or None, default=None
+            Input features.
+
+            - If `input_features` is `None`, then `feature_names_in_` is
+              used as feature names in. If `feature_names_in_` is not defined,
+              then the following input feature names are generated:
+              `["x0", "x1", ..., "x(n_features_in_ - 1)"]`.
+            - If `input_features` is an array-like, then `input_features` must
+              match `feature_names_in_` if `feature_names_in_` is defined.
+
+        Returns
+        -------
+        feature_names_out : ndarray of str objects
+            Same as input features.
+        """
+        return _check_feature_names_in(self, input_features)
+
+
+class _ClassNamePrefixFeaturesOutMixin:
+    """Mixin class for transformers that generate their own names by prefixing.
+
+    Assumes that `_n_features_out` is defined for the estimator.
+    """
+
+    def get_feature_names_out(self, input_features=None):
+        """Get output feature names for transformation.
+
+        Parameters
+        ----------
+        input_features : array-like of str or None, default=None
+            Only used to validate feature names with the names seen in :meth:`fit`.
+
+        Returns
+        -------
+        feature_names_out : ndarray of str objects
+            Transformed feature names.
+        """
+        check_is_fitted(self, "_n_features_out")
+        return _generate_get_feature_names_out(
+            self, self._n_features_out, input_features=input_features
+        )
+
+
 class DensityMixin:
     """Mixin class for all density estimators in scikit-learn."""
+
     _estimator_type = "DensityEstimator"

     def score(self, X, y=None):
-        """Returns the score of the model on the data X
-
-        Parameters
-        ----------
-        X : array-like, shape = (n_samples, n_features)
+        """Return the score of the model on the data `X`.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            Test samples.
+
+        y : Ignored
+            Not used, present for API consistency by convention.

         Returns
         -------
@@ -576,24 +950,25 @@

 class OutlierMixin:
     """Mixin class for all outlier detection estimators in scikit-learn."""
+
     _estimator_type = "outlier_detector"

     def fit_predict(self, X, y=None):
-        """Performs fit on X and returns labels for X.
+        """Perform fit on X and returns labels for X.

         Returns -1 for outliers and 1 for inliers.

         Parameters
         ----------
-        X : ndarray, shape (n_samples, n_features)
-            Input data.
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            The input samples.

         y : Ignored
-            not used, present for API consistency by convention.
-
-        Returns
-        -------
-        y : ndarray, shape (n_samples,)
+            Not used, present for API consistency by convention.
+
+        Returns
+        -------
+        y : ndarray of shape (n_samples,)
             1 for inliers, -1 for outliers.
         """
         # override for transductive outlier detectors like LocalOulierFactor
@@ -605,21 +980,26 @@
     """Mixin class for all meta estimators in scikit-learn."""


-class MultiOutputMixin(object):
+class MultiOutputMixin:
     """Mixin to mark estimators that support multioutput."""
+
     def _more_tags(self):
-        return {'multioutput': True}
-
-
-class _UnstableArchMixin(object):
+        return {"multioutput": True}
+
+
+class _UnstableArchMixin:
     """Mark estimators that are non-determinstic on 32bit or PowerPC"""
+
     def _more_tags(self):
-        return {'non_deterministic': (
-            _IS_32BIT or platform.machine().startswith(('ppc', 'powerpc')))}
+        return {
+            "non_deterministic": (
+                _IS_32BIT or platform.machine().startswith(("ppc", "powerpc"))
+            )
+        }


 def is_classifier(estimator):
-    """Returns True if the given estimator is (probably) a classifier.
+    """Return True if the given estimator is (probably) a classifier.

     Parameters
     ----------
@@ -635,11 +1015,11 @@


 def is_regressor(estimator):
-    """Returns True if the given estimator is (probably) a regressor.
+    """Return True if the given estimator is (probably) a regressor.

     Parameters
     ----------
-    estimator : object
+    estimator : estimator instance
         Estimator object to test.

     Returns
@@ -651,11 +1031,11 @@


 def is_outlier_detector(estimator):
-    """Returns True if the given estimator is (probably) an outlier detector.
+    """Return True if the given estimator is (probably) an outlier detector.

     Parameters
     ----------
-    estimator : object
+    estimator : estimator instance
         Estimator object to test.

     Returns
('sklearn', 'calibration.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -9,87 +9,178 @@

 import warnings
 from inspect import signature
+from functools import partial

 from math import log
 import numpy as np
+from joblib import Parallel

 from scipy.special import expit
 from scipy.special import xlogy
 from scipy.optimize import fmin_bfgs
-from .preprocessing import LabelEncoder
-
-from .base import BaseEstimator, ClassifierMixin, RegressorMixin, clone
-from .preprocessing import label_binarize, LabelBinarizer
-from .utils import check_X_y, check_array, indexable, column_or_1d
-from .utils.validation import check_is_fitted, check_consistent_length
+
+from .base import (
+    BaseEstimator,
+    ClassifierMixin,
+    RegressorMixin,
+    clone,
+    MetaEstimatorMixin,
+    is_classifier,
+)
+from .preprocessing import label_binarize, LabelEncoder
+from .utils import (
+    column_or_1d,
+    indexable,
+    check_matplotlib_support,
+)
+
+from .utils.multiclass import check_classification_targets
+from .utils.fixes import delayed
+from .utils.validation import (
+    _check_fit_params,
+    _check_sample_weight,
+    _num_samples,
+    check_consistent_length,
+    check_is_fitted,
+)
+from .utils import _safe_indexing
 from .isotonic import IsotonicRegression
 from .svm import LinearSVC
-from .model_selection import check_cv
-
-
-class CalibratedClassifierCV(BaseEstimator, ClassifierMixin):
-    """Probability calibration with isotonic regression or sigmoid.
-
-    See glossary entry for :term:`cross-validation estimator`.
-
-    With this class, the base_estimator is fit on the train set of the
-    cross-validation generator and the test set is used for calibration.
-    The probabilities for each of the folds are then averaged
-    for prediction. In case that cv="prefit" is passed to __init__,
-    it is assumed that base_estimator has been fitted already and all
-    data is used for calibration. Note that data for fitting the
-    classifier and for calibrating it must be disjoint.
+from .model_selection import check_cv, cross_val_predict
+from .metrics._base import _check_pos_label_consistency
+from .metrics._plot.base import _get_response
+
+
+class CalibratedClassifierCV(ClassifierMixin, MetaEstimatorMixin, BaseEstimator):
+    """Probability calibration with isotonic regression or logistic regression.
+
+    This class uses cross-validation to both estimate the parameters of a
+    classifier and subsequently calibrate a classifier. With default
+    `ensemble=True`, for each cv split it
+    fits a copy of the base estimator to the training subset, and calibrates it
+    using the testing subset. For prediction, predicted probabilities are
+    averaged across these individual calibrated classifiers. When
+    `ensemble=False`, cross-validation is used to obtain unbiased predictions,
+    via :func:`~sklearn.model_selection.cross_val_predict`, which are then
+    used for calibration. For prediction, the base estimator, trained using all
+    the data, is used. This is the method implemented when `probabilities=True`
+    for :mod:`sklearn.svm` estimators.
+
+    Already fitted classifiers can be calibrated via the parameter
+    `cv="prefit"`. In this case, no cross-validation is used and all provided
+    data is used for calibration. The user has to take care manually that data
+    for model fitting and calibration are disjoint.
+
+    The calibration is based on the :term:`decision_function` method of the
+    `base_estimator` if it exists, else on :term:`predict_proba`.

     Read more in the :ref:`User Guide <calibration>`.

     Parameters
     ----------
-    base_estimator : instance BaseEstimator
-        The classifier whose output decision function needs to be calibrated
-        to offer more accurate predict_proba outputs. If cv=prefit, the
-        classifier must have been fit already on data.
-
-    method : 'sigmoid' or 'isotonic'
+    base_estimator : estimator instance, default=None
+        The classifier whose output need to be calibrated to provide more
+        accurate `predict_proba` outputs. The default classifier is
+        a :class:`~sklearn.svm.LinearSVC`.
+
+    method : {'sigmoid', 'isotonic'}, default='sigmoid'
         The method to use for calibration. Can be 'sigmoid' which
-        corresponds to Platt's method or 'isotonic' which is a
-        non-parametric approach. It is not advised to use isotonic calibration
-        with too few calibration samples ``(<<1000)`` since it tends to
-        overfit.
-        Use sigmoids (Platt's calibration) in this case.
-
-    cv : integer, cross-validation generator, iterable or "prefit", optional
+        corresponds to Platt's method (i.e. a logistic regression model) or
+        'isotonic' which is a non-parametric approach. It is not advised to
+        use isotonic calibration with too few calibration samples
+        ``(<<1000)`` since it tends to overfit.
+
+    cv : int, cross-validation generator, iterable or "prefit", \
+            default=None
         Determines the cross-validation splitting strategy.
         Possible inputs for cv are:

-        - None, to use the default 3-fold cross-validation,
+        - None, to use the default 5-fold cross-validation,
         - integer, to specify the number of folds.
         - :term:`CV splitter`,
         - An iterable yielding (train, test) splits as arrays of indices.

         For integer/None inputs, if ``y`` is binary or multiclass,
-        :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` is
-        neither binary nor multiclass, :class:`sklearn.model_selection.KFold`
+        :class:`~sklearn.model_selection.StratifiedKFold` is used. If ``y`` is
+        neither binary nor multiclass, :class:`~sklearn.model_selection.KFold`
         is used.

-        Refer :ref:`User Guide <cross_validation>` for the various
+        Refer to the :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.

-        If "prefit" is passed, it is assumed that base_estimator has been
+        If "prefit" is passed, it is assumed that `base_estimator` has been
         fitted already and all data is used for calibration.

-        .. versionchanged:: 0.20
-            ``cv`` default value if None will change from 3-fold to 5-fold
-            in v0.22.
+        .. versionchanged:: 0.22
+            ``cv`` default value if None changed from 3-fold to 5-fold.
+
+    n_jobs : int, default=None
+        Number of jobs to run in parallel.
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors.
+
+        Base estimator clones are fitted in parallel across cross-validation
+        iterations. Therefore parallelism happens only when `cv != "prefit"`.
+
+        See :term:`Glossary <n_jobs>` for more details.
+
+        .. versionadded:: 0.24
+
+    ensemble : bool, default=True
+        Determines how the calibrator is fitted when `cv` is not `'prefit'`.
+        Ignored if `cv='prefit'`.
+
+        If `True`, the `base_estimator` is fitted using training data, and
+        calibrated using testing data, for each `cv` fold. The final estimator
+        is an ensemble of `n_cv` fitted classifier and calibrator pairs, where
+        `n_cv` is the number of cross-validation folds. The output is the
+        average predicted probabilities of all pairs.
+
+        If `False`, `cv` is used to compute unbiased predictions, via
+        :func:`~sklearn.model_selection.cross_val_predict`, which are then
+        used for calibration. At prediction time, the classifier used is the
+        `base_estimator` trained on all the data.
+        Note that this method is also internally implemented  in
+        :mod:`sklearn.svm` estimators with the `probabilities=True` parameter.
+
+        .. versionadded:: 0.24

     Attributes
     ----------
-    classes_ : array, shape (n_classes)
+    classes_ : ndarray of shape (n_classes,)
         The class labels.

-    calibrated_classifiers_ : list (len() equal to cv or 1 if cv == "prefit")
-        The list of calibrated classifiers, one for each crossvalidation fold,
-        which has been fitted on all but the validation fold and calibrated
-        on the validation fold.
+    n_features_in_ : int
+        Number of features seen during :term:`fit`. Only defined if the
+        underlying base_estimator exposes such an attribute when fit.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Only defined if the
+        underlying base_estimator exposes such an attribute when fit.
+
+        .. versionadded:: 1.0
+
+    calibrated_classifiers_ : list (len() equal to cv or 1 if `cv="prefit"` \
+            or `ensemble=False`)
+        The list of classifier and calibrator pairs.
+
+        - When `cv="prefit"`, the fitted `base_estimator` and fitted
+          calibrator.
+        - When `cv` is not "prefit" and `ensemble=True`, `n_cv` fitted
+          `base_estimator` and calibrator pairs. `n_cv` is the number of
+          cross-validation folds.
+        - When `cv` is not "prefit" and `ensemble=False`, the `base_estimator`,
+          fitted on all the data, and fitted calibrator.
+
+        .. versionchanged:: 0.24
+            Single calibrated classifier case when `ensemble=False`.
+
+    See Also
+    --------
+    calibration_curve : Compute true and predicted probabilities
+        for a calibration curve.

     References
     ----------
@@ -104,49 +195,93 @@

     .. [4] Predicting Good Probabilities with Supervised Learning,
            A. Niculescu-Mizil & R. Caruana, ICML 2005
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_classification
+    >>> from sklearn.naive_bayes import GaussianNB
+    >>> from sklearn.calibration import CalibratedClassifierCV
+    >>> X, y = make_classification(n_samples=100, n_features=2,
+    ...                            n_redundant=0, random_state=42)
+    >>> base_clf = GaussianNB()
+    >>> calibrated_clf = CalibratedClassifierCV(base_estimator=base_clf, cv=3)
+    >>> calibrated_clf.fit(X, y)
+    CalibratedClassifierCV(base_estimator=GaussianNB(), cv=3)
+    >>> len(calibrated_clf.calibrated_classifiers_)
+    3
+    >>> calibrated_clf.predict_proba(X)[:5, :]
+    array([[0.110..., 0.889...],
+           [0.072..., 0.927...],
+           [0.928..., 0.071...],
+           [0.928..., 0.071...],
+           [0.071..., 0.928...]])
+    >>> from sklearn.model_selection import train_test_split
+    >>> X, y = make_classification(n_samples=100, n_features=2,
+    ...                            n_redundant=0, random_state=42)
+    >>> X_train, X_calib, y_train, y_calib = train_test_split(
+    ...        X, y, random_state=42
+    ... )
+    >>> base_clf = GaussianNB()
+    >>> base_clf.fit(X_train, y_train)
+    GaussianNB()
+    >>> calibrated_clf = CalibratedClassifierCV(
+    ...     base_estimator=base_clf,
+    ...     cv="prefit"
+    ... )
+    >>> calibrated_clf.fit(X_calib, y_calib)
+    CalibratedClassifierCV(base_estimator=GaussianNB(), cv='prefit')
+    >>> len(calibrated_clf.calibrated_classifiers_)
+    1
+    >>> calibrated_clf.predict_proba([[-0.5, 0.5]])
+    array([[0.936..., 0.063...]])
     """
-    def __init__(self, base_estimator=None, method='sigmoid', cv='warn'):
+
+    def __init__(
+        self,
+        base_estimator=None,
+        *,
+        method="sigmoid",
+        cv=None,
+        n_jobs=None,
+        ensemble=True,
+    ):
         self.base_estimator = base_estimator
         self.method = method
         self.cv = cv
-
-    def fit(self, X, y, sample_weight=None):
-        """Fit the calibrated model
+        self.n_jobs = n_jobs
+        self.ensemble = ensemble
+
+    def fit(self, X, y, sample_weight=None, **fit_params):
+        """Fit the calibrated model.

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
+        X : array-like of shape (n_samples, n_features)
             Training data.

-        y : array-like, shape (n_samples,)
+        y : array-like of shape (n_samples,)
             Target values.

-        sample_weight : array-like, shape = [n_samples] or None
+        sample_weight : array-like of shape (n_samples,), default=None
             Sample weights. If None, then samples are equally weighted.
+
+        **fit_params : dict
+            Parameters to pass to the `fit` method of the underlying
+            classifier.

         Returns
         -------
         self : object
             Returns an instance of self.
         """
-        X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],
-                         force_all_finite=False, allow_nd=True)
+        check_classification_targets(y)
         X, y = indexable(X, y)
-        le = LabelBinarizer().fit(y)
-        self.classes_ = le.classes_
-
-        # Check that each cross-validation fold can have at least one
-        # example per class
-        n_folds = self.cv if isinstance(self.cv, int) \
-            else self.cv.n_folds if hasattr(self.cv, "n_folds") else None
-        if n_folds and \
-                np.any([np.sum(y == class_) < n_folds for class_ in
-                        self.classes_]):
-            raise ValueError("Requesting %d-fold cross-validation but provided"
-                             " less than %d examples for at least one class."
-                             % (n_folds, n_folds))
-
-        self.calibrated_classifiers_ = []
+        if sample_weight is not None:
+            sample_weight = _check_sample_weight(sample_weight, X)
+
+        for sample_aligned_params in fit_params.values():
+            check_consistent_length(y, sample_aligned_params)
+
         if self.base_estimator is None:
             # we want all classifiers that don't expose a random_state
             # to be deterministic (and we don't want to expose this one).
@@ -154,72 +289,146 @@
         else:
             base_estimator = self.base_estimator

+        self.calibrated_classifiers_ = []
         if self.cv == "prefit":
-            calibrated_classifier = _CalibratedClassifier(
-                base_estimator, method=self.method)
-            if sample_weight is not None:
-                calibrated_classifier.fit(X, y, sample_weight)
-            else:
-                calibrated_classifier.fit(X, y)
+            # `classes_` should be consistent with that of base_estimator
+            check_is_fitted(self.base_estimator, attributes=["classes_"])
+            self.classes_ = self.base_estimator.classes_
+
+            pred_method, method_name = _get_prediction_method(base_estimator)
+            n_classes = len(self.classes_)
+            predictions = _compute_predictions(pred_method, method_name, X, n_classes)
+
+            calibrated_classifier = _fit_calibrator(
+                base_estimator,
+                predictions,
+                y,
+                self.classes_,
+                self.method,
+                sample_weight,
+            )
             self.calibrated_classifiers_.append(calibrated_classifier)
         else:
+            # Set `classes_` using all `y`
+            label_encoder_ = LabelEncoder().fit(y)
+            self.classes_ = label_encoder_.classes_
+            n_classes = len(self.classes_)
+
+            # sample_weight checks
+            fit_parameters = signature(base_estimator.fit).parameters
+            supports_sw = "sample_weight" in fit_parameters
+            if sample_weight is not None and not supports_sw:
+                estimator_name = type(base_estimator).__name__
+                warnings.warn(
+                    f"Since {estimator_name} does not appear to accept sample_weight, "
+                    "sample weights will only be used for the calibration itself. This "
+                    "can be caused by a limitation of the current scikit-learn API. "
+                    "See the following issue for more details: "
+                    "https://github.com/scikit-learn/scikit-learn/issues/21134. Be "
+                    "warned that the result of the calibration is likely to be "
+                    "incorrect."
+                )
+
+            # Check that each cross-validation fold can have at least one
+            # example per class
+            if isinstance(self.cv, int):
+                n_folds = self.cv
+            elif hasattr(self.cv, "n_splits"):
+                n_folds = self.cv.n_splits
+            else:
+                n_folds = None
+            if n_folds and np.any(
+                [np.sum(y == class_) < n_folds for class_ in self.classes_]
+            ):
+                raise ValueError(
+                    f"Requesting {n_folds}-fold "
+                    "cross-validation but provided less than "
+                    f"{n_folds} examples for at least one class."
+                )
             cv = check_cv(self.cv, y, classifier=True)
-            fit_parameters = signature(base_estimator.fit).parameters
-            estimator_name = type(base_estimator).__name__
-            if (sample_weight is not None
-                    and "sample_weight" not in fit_parameters):
-                warnings.warn("%s does not support sample_weight. Samples"
-                              " weights are only used for the calibration"
-                              " itself." % estimator_name)
-                base_estimator_sample_weight = None
+
+            if self.ensemble:
+                parallel = Parallel(n_jobs=self.n_jobs)
+                self.calibrated_classifiers_ = parallel(
+                    delayed(_fit_classifier_calibrator_pair)(
+                        clone(base_estimator),
+                        X,
+                        y,
+                        train=train,
+                        test=test,
+                        method=self.method,
+                        classes=self.classes_,
+                        supports_sw=supports_sw,
+                        sample_weight=sample_weight,
+                        **fit_params,
+                    )
+                    for train, test in cv.split(X, y)
+                )
             else:
-                if sample_weight is not None:
-                    sample_weight = check_array(sample_weight, ensure_2d=False)
-                    check_consistent_length(y, sample_weight)
-                base_estimator_sample_weight = sample_weight
-            for train, test in cv.split(X, y):
                 this_estimator = clone(base_estimator)
-                if base_estimator_sample_weight is not None:
-                    this_estimator.fit(
-                        X[train], y[train],
-                        sample_weight=base_estimator_sample_weight[train])
+                _, method_name = _get_prediction_method(this_estimator)
+                fit_params = (
+                    {"sample_weight": sample_weight}
+                    if sample_weight is not None and supports_sw
+                    else None
+                )
+                pred_method = partial(
+                    cross_val_predict,
+                    estimator=this_estimator,
+                    X=X,
+                    y=y,
+                    cv=cv,
+                    method=method_name,
+                    n_jobs=self.n_jobs,
+                    fit_params=fit_params,
+                )
+                predictions = _compute_predictions(
+                    pred_method, method_name, X, n_classes
+                )
+
+                if sample_weight is not None and supports_sw:
+                    this_estimator.fit(X, y, sample_weight=sample_weight)
                 else:
-                    this_estimator.fit(X[train], y[train])
-
-                calibrated_classifier = _CalibratedClassifier(
-                    this_estimator, method=self.method,
-                    classes=self.classes_)
-                if sample_weight is not None:
-                    calibrated_classifier.fit(X[test], y[test],
-                                              sample_weight[test])
-                else:
-                    calibrated_classifier.fit(X[test], y[test])
+                    this_estimator.fit(X, y)
+                # Note: Here we don't pass on fit_params because the supported
+                # calibrators don't support fit_params anyway
+                calibrated_classifier = _fit_calibrator(
+                    this_estimator,
+                    predictions,
+                    y,
+                    self.classes_,
+                    self.method,
+                    sample_weight,
+                )
                 self.calibrated_classifiers_.append(calibrated_classifier)

+        first_clf = self.calibrated_classifiers_[0].base_estimator
+        if hasattr(first_clf, "n_features_in_"):
+            self.n_features_in_ = first_clf.n_features_in_
+        if hasattr(first_clf, "feature_names_in_"):
+            self.feature_names_in_ = first_clf.feature_names_in_
         return self

     def predict_proba(self, X):
-        """Posterior probabilities of classification
-
-        This function returns posterior probabilities of classification
+        """Calibrated probabilities of classification.
+
+        This function returns calibrated probabilities of classification
         according to each class on an array of test vectors X.

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            The samples.
+        X : array-like of shape (n_samples, n_features)
+            The samples, as accepted by `base_estimator.predict_proba`.

         Returns
         -------
-        C : array, shape (n_samples, n_classes)
+        C : ndarray of shape (n_samples, n_classes)
             The predicted probas.
         """
-        check_is_fitted(self, ["classes_", "calibrated_classifiers_"])
-        X = check_array(X, accept_sparse=['csc', 'csr', 'coo'],
-                        force_all_finite=False)
+        check_is_fitted(self)
         # Compute the arithmetic mean of the predictions of the calibrated
         # classifiers
-        mean_proba = np.zeros((X.shape[0], len(self.classes_)))
+        mean_proba = np.zeros((_num_samples(X), len(self.classes_)))
         for calibrated_classifier in self.calibrated_classifiers_:
             proba = calibrated_classifier.predict_proba(X)
             mean_proba += proba
@@ -229,170 +438,306 @@
         return mean_proba

     def predict(self, X):
-        """Predict the target of new samples. Can be different from the
-        prediction of the uncalibrated classifier.
+        """Predict the target of new samples.
+
+        The predicted class is the class that has the highest probability,
+        and can thus be different from the prediction of the uncalibrated classifier.

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            The samples.
+        X : array-like of shape (n_samples, n_features)
+            The samples, as accepted by `base_estimator.predict`.

         Returns
         -------
-        C : array, shape (n_samples,)
+        C : ndarray of shape (n_samples,)
             The predicted class.
         """
-        check_is_fitted(self, ["classes_", "calibrated_classifiers_"])
+        check_is_fitted(self)
         return self.classes_[np.argmax(self.predict_proba(X), axis=1)]

+    def _more_tags(self):
+        return {
+            "_xfail_checks": {
+                "check_sample_weights_invariance": (
+                    "Due to the cross-validation and sample ordering, removing a sample"
+                    " is not strictly equal to putting is weight to zero. Specific unit"
+                    " tests are added for CalibratedClassifierCV specifically."
+                ),
+            }
+        }
+
+
+def _fit_classifier_calibrator_pair(
+    estimator,
+    X,
+    y,
+    train,
+    test,
+    supports_sw,
+    method,
+    classes,
+    sample_weight=None,
+    **fit_params,
+):
+    """Fit a classifier/calibration pair on a given train/test split.
+
+    Fit the classifier on the train set, compute its predictions on the test
+    set and use the predictions as input to fit the calibrator along with the
+    test labels.
+
+    Parameters
+    ----------
+    estimator : estimator instance
+        Cloned base estimator.
+
+    X : array-like, shape (n_samples, n_features)
+        Sample data.
+
+    y : array-like, shape (n_samples,)
+        Targets.
+
+    train : ndarray, shape (n_train_indices,)
+        Indices of the training subset.
+
+    test : ndarray, shape (n_test_indices,)
+        Indices of the testing subset.
+
+    supports_sw : bool
+        Whether or not the `estimator` supports sample weights.
+
+    method : {'sigmoid', 'isotonic'}
+        Method to use for calibration.
+
+    classes : ndarray, shape (n_classes,)
+        The target classes.
+
+    sample_weight : array-like, default=None
+        Sample weights for `X`.
+
+    **fit_params : dict
+        Parameters to pass to the `fit` method of the underlying
+        classifier.
+
+    Returns
+    -------
+    calibrated_classifier : _CalibratedClassifier instance
+    """
+    fit_params_train = _check_fit_params(X, fit_params, train)
+    X_train, y_train = _safe_indexing(X, train), _safe_indexing(y, train)
+    X_test, y_test = _safe_indexing(X, test), _safe_indexing(y, test)
+
+    if sample_weight is not None and supports_sw:
+        sw_train = _safe_indexing(sample_weight, train)
+        estimator.fit(X_train, y_train, sample_weight=sw_train, **fit_params_train)
+    else:
+        estimator.fit(X_train, y_train, **fit_params_train)
+
+    n_classes = len(classes)
+    pred_method, method_name = _get_prediction_method(estimator)
+    predictions = _compute_predictions(pred_method, method_name, X_test, n_classes)
+
+    sw_test = None if sample_weight is None else _safe_indexing(sample_weight, test)
+    calibrated_classifier = _fit_calibrator(
+        estimator, predictions, y_test, classes, method, sample_weight=sw_test
+    )
+    return calibrated_classifier
+
+
+def _get_prediction_method(clf):
+    """Return prediction method.
+
+    `decision_function` method of `clf` returned, if it
+    exists, otherwise `predict_proba` method returned.
+
+    Parameters
+    ----------
+    clf : Estimator instance
+        Fitted classifier to obtain the prediction method from.
+
+    Returns
+    -------
+    prediction_method : callable
+        The prediction method.
+    method_name : str
+        The name of the prediction method.
+    """
+    if hasattr(clf, "decision_function"):
+        method = getattr(clf, "decision_function")
+        return method, "decision_function"
+    elif hasattr(clf, "predict_proba"):
+        method = getattr(clf, "predict_proba")
+        return method, "predict_proba"
+    else:
+        raise RuntimeError(
+            "'base_estimator' has no 'decision_function' or 'predict_proba' method."
+        )
+
+
+def _compute_predictions(pred_method, method_name, X, n_classes):
+    """Return predictions for `X` and reshape binary outputs to shape
+    (n_samples, 1).
+
+    Parameters
+    ----------
+    pred_method : callable
+        Prediction method.
+
+    method_name: str
+        Name of the prediction method
+
+    X : array-like or None
+        Data used to obtain predictions.
+
+    n_classes : int
+        Number of classes present.
+
+    Returns
+    -------
+    predictions : array-like, shape (X.shape[0], len(clf.classes_))
+        The predictions. Note if there are 2 classes, array is of shape
+        (X.shape[0], 1).
+    """
+    predictions = pred_method(X=X)
+
+    if method_name == "decision_function":
+        if predictions.ndim == 1:
+            predictions = predictions[:, np.newaxis]
+    elif method_name == "predict_proba":
+        if n_classes == 2:
+            predictions = predictions[:, 1:]
+    else:  # pragma: no cover
+        # this branch should be unreachable.
+        raise ValueError(f"Invalid prediction method: {method_name}")
+    return predictions
+
+
+def _fit_calibrator(clf, predictions, y, classes, method, sample_weight=None):
+    """Fit calibrator(s) and return a `_CalibratedClassifier`
+    instance.
+
+    `n_classes` (i.e. `len(clf.classes_)`) calibrators are fitted.
+    However, if `n_classes` equals 2, one calibrator is fitted.
+
+    Parameters
+    ----------
+    clf : estimator instance
+        Fitted classifier.
+
+    predictions : array-like, shape (n_samples, n_classes) or (n_samples, 1) \
+                    when binary.
+        Raw predictions returned by the un-calibrated base classifier.
+
+    y : array-like, shape (n_samples,)
+        The targets.
+
+    classes : ndarray, shape (n_classes,)
+        All the prediction classes.
+
+    method : {'sigmoid', 'isotonic'}
+        The method to use for calibration.
+
+    sample_weight : ndarray, shape (n_samples,), default=None
+        Sample weights. If None, then samples are equally weighted.
+
+    Returns
+    -------
+    pipeline : _CalibratedClassifier instance
+    """
+    Y = label_binarize(y, classes=classes)
+    label_encoder = LabelEncoder().fit(classes)
+    pos_class_indices = label_encoder.transform(clf.classes_)
+    calibrators = []
+    for class_idx, this_pred in zip(pos_class_indices, predictions.T):
+        if method == "isotonic":
+            calibrator = IsotonicRegression(out_of_bounds="clip")
+        elif method == "sigmoid":
+            calibrator = _SigmoidCalibration()
+        else:
+            raise ValueError(
+                f"'method' should be one of: 'sigmoid' or 'isotonic'. Got {method}."
+            )
+        calibrator.fit(this_pred, Y[:, class_idx], sample_weight)
+        calibrators.append(calibrator)
+
+    pipeline = _CalibratedClassifier(clf, calibrators, method=method, classes=classes)
+    return pipeline
+

 class _CalibratedClassifier:
-    """Probability calibration with isotonic regression or sigmoid.
-
-    It assumes that base_estimator has already been fit, and trains the
-    calibration on the input set of the fit function. Note that this class
-    should not be used as an estimator directly. Use CalibratedClassifierCV
-    with cv="prefit" instead.
+    """Pipeline-like chaining a fitted classifier and its fitted calibrators.

     Parameters
     ----------
-    base_estimator : instance BaseEstimator
-        The classifier whose output decision function needs to be calibrated
-        to offer more accurate predict_proba outputs. No default value since
-        it has to be an already fitted estimator.
-
-    method : 'sigmoid' | 'isotonic'
+    base_estimator : estimator instance
+        Fitted classifier.
+
+    calibrators : list of fitted estimator instances
+        List of fitted calibrators (either 'IsotonicRegression' or
+        '_SigmoidCalibration'). The number of calibrators equals the number of
+        classes. However, if there are 2 classes, the list contains only one
+        fitted calibrator.
+
+    classes : array-like of shape (n_classes,)
+        All the prediction classes.
+
+    method : {'sigmoid', 'isotonic'}, default='sigmoid'
         The method to use for calibration. Can be 'sigmoid' which
         corresponds to Platt's method or 'isotonic' which is a
         non-parametric approach based on isotonic regression.
-
-    classes : array-like, shape (n_classes,), optional
-            Contains unique classes used to fit the base estimator.
-            if None, then classes is extracted from the given target values
-            in fit().
-
-    See also
-    --------
-    CalibratedClassifierCV
-
-    References
-    ----------
-    .. [1] Obtaining calibrated probability estimates from decision trees
-           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001
-
-    .. [2] Transforming Classifier Scores into Accurate Multiclass
-           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)
-
-    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to
-           Regularized Likelihood Methods, J. Platt, (1999)
-
-    .. [4] Predicting Good Probabilities with Supervised Learning,
-           A. Niculescu-Mizil & R. Caruana, ICML 2005
     """
-    def __init__(self, base_estimator, method='sigmoid', classes=None):
+
+    def __init__(self, base_estimator, calibrators, *, classes, method="sigmoid"):
         self.base_estimator = base_estimator
+        self.calibrators = calibrators
+        self.classes = classes
         self.method = method
-        self.classes = classes
-
-    def _preproc(self, X):
-        n_classes = len(self.classes_)
-        if hasattr(self.base_estimator, "decision_function"):
-            df = self.base_estimator.decision_function(X)
-            if df.ndim == 1:
-                df = df[:, np.newaxis]
-        elif hasattr(self.base_estimator, "predict_proba"):
-            df = self.base_estimator.predict_proba(X)
-            if n_classes == 2:
-                df = df[:, 1:]
-        else:
-            raise RuntimeError('classifier has no decision_function or '
-                               'predict_proba method.')
-
-        idx_pos_class = self.label_encoder_.\
-            transform(self.base_estimator.classes_)
-
-        return df, idx_pos_class
-
-    def fit(self, X, y, sample_weight=None):
-        """Calibrate the fitted model
+
+    def predict_proba(self, X):
+        """Calculate calibrated probabilities.
+
+        Calculates classification calibrated probabilities
+        for each class, in a one-vs-all manner, for `X`.

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training data.
-
-        y : array-like, shape (n_samples,)
-            Target values.
-
-        sample_weight : array-like, shape = [n_samples] or None
-            Sample weights. If None, then samples are equally weighted.
+        X : ndarray of shape (n_samples, n_features)
+            The sample data.

         Returns
         -------
-        self : object
-            Returns an instance of self.
+        proba : array, shape (n_samples, n_classes)
+            The predicted probabilities. Can be exact zeros.
         """
-
-        self.label_encoder_ = LabelEncoder()
-        if self.classes is None:
-            self.label_encoder_.fit(y)
-        else:
-            self.label_encoder_.fit(self.classes)
-
-        self.classes_ = self.label_encoder_.classes_
-        Y = label_binarize(y, self.classes_)
-
-        df, idx_pos_class = self._preproc(X)
-        self.calibrators_ = []
-
-        for k, this_df in zip(idx_pos_class, df.T):
-            if self.method == 'isotonic':
-                calibrator = IsotonicRegression(out_of_bounds='clip')
-            elif self.method == 'sigmoid':
-                calibrator = _SigmoidCalibration()
-            else:
-                raise ValueError('method should be "sigmoid" or '
-                                 '"isotonic". Got %s.' % self.method)
-            calibrator.fit(this_df, Y[:, k], sample_weight)
-            self.calibrators_.append(calibrator)
-
-        return self
-
-    def predict_proba(self, X):
-        """Posterior probabilities of classification
-
-        This function returns posterior probabilities of classification
-        according to each class on an array of test vectors X.
-
-        Parameters
-        ----------
-        X : array-like, shape (n_samples, n_features)
-            The samples.
-
-        Returns
-        -------
-        C : array, shape (n_samples, n_classes)
-            The predicted probas. Can be exact zeros.
-        """
-        n_classes = len(self.classes_)
-        proba = np.zeros((X.shape[0], n_classes))
-
-        df, idx_pos_class = self._preproc(X)
-
-        for k, this_df, calibrator in \
-                zip(idx_pos_class, df.T, self.calibrators_):
+        n_classes = len(self.classes)
+        pred_method, method_name = _get_prediction_method(self.base_estimator)
+        predictions = _compute_predictions(pred_method, method_name, X, n_classes)
+
+        label_encoder = LabelEncoder().fit(self.classes)
+        pos_class_indices = label_encoder.transform(self.base_estimator.classes_)
+
+        proba = np.zeros((_num_samples(X), n_classes))
+        for class_idx, this_pred, calibrator in zip(
+            pos_class_indices, predictions.T, self.calibrators
+        ):
             if n_classes == 2:
-                k += 1
-            proba[:, k] = calibrator.predict(this_df)
+                # When binary, `predictions` consists only of predictions for
+                # clf.classes_[1] but `pos_class_indices` = 0
+                class_idx += 1
+            proba[:, class_idx] = calibrator.predict(this_pred)

         # Normalize the probabilities
         if n_classes == 2:
-            proba[:, 0] = 1. - proba[:, 1]
+            proba[:, 0] = 1.0 - proba[:, 1]
         else:
-            proba /= np.sum(proba, axis=1)[:, np.newaxis]
-
-        # XXX : for some reason all probas can be 0
-        proba[np.isnan(proba)] = 1. / n_classes
+            denominator = np.sum(proba, axis=1)[:, np.newaxis]
+            # In the edge case where for each class calibrator returns a null
+            # probability for a given sample, use the uniform distribution
+            # instead.
+            uniform_proba = np.full_like(proba, 1 / n_classes)
+            proba = np.divide(
+                proba, denominator, out=uniform_proba, where=denominator != 0
+            )

         # Deal with cases where the predicted probability minimally exceeds 1.0
         proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0
@@ -400,18 +745,18 @@
         return proba


-def _sigmoid_calibration(df, y, sample_weight=None):
+def _sigmoid_calibration(predictions, y, sample_weight=None):
     """Probability Calibration with sigmoid method (Platt 2000)

     Parameters
     ----------
-    df : ndarray, shape (n_samples,)
+    predictions : ndarray of shape (n_samples,)
         The decision function or predict proba for the samples.

-    y : ndarray, shape (n_samples,)
+    y : ndarray of shape (n_samples,)
         The targets.

-    sample_weight : array-like, shape = [n_samples] or None
+    sample_weight : array-like of shape (n_samples,), default=None
         Sample weights. If None, then samples are equally weighted.

     Returns
@@ -426,23 +771,30 @@
     ----------
     Platt, "Probabilistic Outputs for Support Vector Machines"
     """
-    df = column_or_1d(df)
+    predictions = column_or_1d(predictions)
     y = column_or_1d(y)

-    F = df  # F follows Platt's notations
-
-    # Bayesian priors (see Platt end of section 2.2)
-    prior0 = float(np.sum(y <= 0))
-    prior1 = y.shape[0] - prior0
-    T = np.zeros(y.shape)
-    T[y > 0] = (prior1 + 1.) / (prior1 + 2.)
-    T[y <= 0] = 1. / (prior0 + 2.)
-    T1 = 1. - T
+    F = predictions  # F follows Platt's notations
+
+    # Bayesian priors (see Platt end of section 2.2):
+    # It corresponds to the number of samples, taking into account the
+    # `sample_weight`.
+    mask_negative_samples = y <= 0
+    if sample_weight is not None:
+        prior0 = (sample_weight[mask_negative_samples]).sum()
+        prior1 = (sample_weight[~mask_negative_samples]).sum()
+    else:
+        prior0 = float(np.sum(mask_negative_samples))
+        prior1 = y.shape[0] - prior0
+    T = np.zeros_like(y, dtype=np.float64)
+    T[y > 0] = (prior1 + 1.0) / (prior1 + 2.0)
+    T[y <= 0] = 1.0 / (prior0 + 2.0)
+    T1 = 1.0 - T

     def objective(AB):
         # From Platt (beginning of Section 2.2)
         P = expit(-(AB[0] * F + AB[1]))
-        loss = -(xlogy(T, P) + xlogy(T1, 1. - P))
+        loss = -(xlogy(T, P) + xlogy(T1, 1.0 - P))
         if sample_weight is not None:
             return (sample_weight * loss).sum()
         else:
@@ -450,21 +802,20 @@

     def grad(AB):
         # gradient of the objective function
-        E = np.exp(AB[0] * F + AB[1])
-        P = 1. / (1. + E)
-        TEP_minus_T1P = P * (T * E - T1)
+        P = expit(-(AB[0] * F + AB[1]))
+        TEP_minus_T1P = T - P
         if sample_weight is not None:
             TEP_minus_T1P *= sample_weight
         dA = np.dot(TEP_minus_T1P, F)
         dB = np.sum(TEP_minus_T1P)
         return np.array([dA, dB])

-    AB0 = np.array([0., log((prior0 + 1.) / (prior1 + 1.))])
+    AB0 = np.array([0.0, log((prior0 + 1.0) / (prior1 + 1.0))])
     AB_ = fmin_bfgs(objective, AB0, fprime=grad, disp=False)
     return AB_[0], AB_[1]


-class _SigmoidCalibration(BaseEstimator, RegressorMixin):
+class _SigmoidCalibration(RegressorMixin, BaseEstimator):
     """Sigmoid regression model.

     Attributes
@@ -475,18 +826,19 @@
     b_ : float
         The intercept.
     """
+
     def fit(self, X, y, sample_weight=None):
         """Fit the model using X, y as training data.

         Parameters
         ----------
-        X : array-like, shape (n_samples,)
+        X : array-like of shape (n_samples,)
             Training data.

-        y : array-like, shape (n_samples,)
+        y : array-like of shape (n_samples,)
             Training target.

-        sample_weight : array-like, shape = [n_samples] or None
+        sample_weight : array-like of shape (n_samples,), default=None
             Sample weights. If None, then samples are equally weighted.

         Returns
@@ -506,23 +858,31 @@

         Parameters
         ----------
-        T : array-like, shape (n_samples,)
+        T : array-like of shape (n_samples,)
             Data to predict from.

         Returns
         -------
-        T_ : array, shape (n_samples,)
+        T_ : ndarray of shape (n_samples,)
             The predicted data.
         """
         T = column_or_1d(T)
         return expit(-(self.a_ * T + self.b_))


-def calibration_curve(y_true, y_prob, normalize=False, n_bins=5,
-                      strategy='uniform'):
+def calibration_curve(
+    y_true,
+    y_prob,
+    *,
+    pos_label=None,
+    normalize="deprecated",
+    n_bins=5,
+    strategy="uniform",
+):
     """Compute true and predicted probabilities for a calibration curve.

-    The method assumes the inputs come from a binary classifier.
+    The method assumes the inputs come from a binary classifier, and
+    discretize the [0, 1] interval into bins.

     Calibration curves may also be referred to as reliability diagrams.

@@ -530,36 +890,49 @@

     Parameters
     ----------
-    y_true : array, shape (n_samples,)
+    y_true : array-like of shape (n_samples,)
         True targets.

-    y_prob : array, shape (n_samples,)
+    y_prob : array-like of shape (n_samples,)
         Probabilities of the positive class.

-    normalize : bool, optional, default=False
-        Whether y_prob needs to be normalized into the bin [0, 1], i.e. is not
-        a proper probability. If True, the smallest value in y_prob is mapped
-        onto 0 and the largest one onto 1.
-
-    n_bins : int
-        Number of bins. A bigger number requires more data. Bins with no data
-        points (i.e. without corresponding values in y_prob) will not be
-        returned, thus there may be fewer than n_bins in the return value.
-
-    strategy : {'uniform', 'quantile'}, (default='uniform')
+    pos_label : int or str, default=None
+        The label of the positive class.
+
+        .. versionadded:: 1.1
+
+    normalize : bool, default="deprecated"
+        Whether y_prob needs to be normalized into the [0, 1] interval, i.e.
+        is not a proper probability. If True, the smallest value in y_prob
+        is linearly mapped onto 0 and the largest one onto 1.
+
+        .. deprecated:: 1.1
+            The normalize argument is deprecated in v1.1 and will be removed in v1.3.
+            Explicitly normalizing `y_prob` will reproduce this behavior, but it is
+            recommended that a proper probability is used (i.e. a classifier's
+            `predict_proba` positive class).
+
+    n_bins : int, default=5
+        Number of bins to discretize the [0, 1] interval. A bigger number
+        requires more data. Bins with no samples (i.e. without
+        corresponding values in `y_prob`) will not be returned, thus the
+        returned arrays may have less than `n_bins` values.
+
+    strategy : {'uniform', 'quantile'}, default='uniform'
         Strategy used to define the widths of the bins.

         uniform
-            All bins have identical widths.
+            The bins have identical widths.
         quantile
-            All bins have the same number of points.
+            The bins have the same number of samples and depend on `y_prob`.

     Returns
     -------
-    prob_true : array, shape (n_bins,) or smaller
-        The true probability in each bin (fraction of positives).
-
-    prob_pred : array, shape (n_bins,) or smaller
+    prob_true : ndarray of shape (n_bins,) or smaller
+        The proportion of samples whose class is the positive class, in each
+        bin (fraction of positives).
+
+    prob_pred : ndarray of shape (n_bins,) or smaller
         The mean predicted probability in each bin.

     References
@@ -568,41 +941,452 @@
     Probabilities With Supervised Learning, in Proceedings of the 22nd
     International Conference on Machine Learning (ICML).
     See section 4 (Qualitative Analysis of Predictions).
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.calibration import calibration_curve
+    >>> y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])
+    >>> y_pred = np.array([0.1, 0.2, 0.3, 0.4, 0.65, 0.7, 0.8, 0.9,  1.])
+    >>> prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=3)
+    >>> prob_true
+    array([0. , 0.5, 1. ])
+    >>> prob_pred
+    array([0.2  , 0.525, 0.85 ])
     """
     y_true = column_or_1d(y_true)
     y_prob = column_or_1d(y_prob)
     check_consistent_length(y_true, y_prob)
-
-    if normalize:  # Normalize predicted values into interval [0, 1]
-        y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min())
-    elif y_prob.min() < 0 or y_prob.max() > 1:
-        raise ValueError("y_prob has values outside [0, 1] and normalize is "
-                         "set to False.")
+    pos_label = _check_pos_label_consistency(pos_label, y_true)
+
+    # TODO(1.3): Remove normalize conditional block.
+    if normalize != "deprecated":
+        warnings.warn(
+            "The normalize argument is deprecated in v1.1 and will be removed in v1.3."
+            " Explicitly normalizing y_prob will reproduce this behavior, but it is"
+            " recommended that a proper probability is used (i.e. a classifier's"
+            " `predict_proba` positive class or `decision_function` output calibrated"
+            " with `CalibratedClassifierCV`).",
+            FutureWarning,
+        )
+        if normalize:  # Normalize predicted values into interval [0, 1]
+            y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min())
+
+    if y_prob.min() < 0 or y_prob.max() > 1:
+        raise ValueError("y_prob has values outside [0, 1].")

     labels = np.unique(y_true)
     if len(labels) > 2:
-        raise ValueError("Only binary classification is supported. "
-                         "Provided labels %s." % labels)
-    y_true = label_binarize(y_true, labels)[:, 0]
-
-    if strategy == 'quantile':  # Determine bin edges by distribution of data
+        raise ValueError(
+            f"Only binary classification is supported. Provided labels {labels}."
+        )
+    y_true = y_true == pos_label
+
+    if strategy == "quantile":  # Determine bin edges by distribution of data
         quantiles = np.linspace(0, 1, n_bins + 1)
         bins = np.percentile(y_prob, quantiles * 100)
-        bins[-1] = bins[-1] + 1e-8
-    elif strategy == 'uniform':
-        bins = np.linspace(0., 1. + 1e-8, n_bins + 1)
+    elif strategy == "uniform":
+        bins = np.linspace(0.0, 1.0, n_bins + 1)
     else:
-        raise ValueError("Invalid entry to 'strategy' input. Strategy "
-                         "must be either 'quantile' or 'uniform'.")
-
-    binids = np.digitize(y_prob, bins) - 1
+        raise ValueError(
+            "Invalid entry to 'strategy' input. Strategy "
+            "must be either 'quantile' or 'uniform'."
+        )
+
+    binids = np.searchsorted(bins[1:-1], y_prob)

     bin_sums = np.bincount(binids, weights=y_prob, minlength=len(bins))
     bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))
     bin_total = np.bincount(binids, minlength=len(bins))

     nonzero = bin_total != 0
-    prob_true = (bin_true[nonzero] / bin_total[nonzero])
-    prob_pred = (bin_sums[nonzero] / bin_total[nonzero])
+    prob_true = bin_true[nonzero] / bin_total[nonzero]
+    prob_pred = bin_sums[nonzero] / bin_total[nonzero]

     return prob_true, prob_pred
+
+
+class CalibrationDisplay:
+    """Calibration curve (also known as reliability diagram) visualization.
+
+    It is recommended to use
+    :func:`~sklearn.calibration.CalibrationDisplay.from_estimator` or
+    :func:`~sklearn.calibration.CalibrationDisplay.from_predictions`
+    to create a `CalibrationDisplay`. All parameters are stored as attributes.
+
+    Read more about calibration in the :ref:`User Guide <calibration>` and
+    more about the scikit-learn visualization API in :ref:`visualizations`.
+
+    .. versionadded:: 1.0
+
+    Parameters
+    ----------
+    prob_true : ndarray of shape (n_bins,)
+        The proportion of samples whose class is the positive class (fraction
+        of positives), in each bin.
+
+    prob_pred : ndarray of shape (n_bins,)
+        The mean predicted probability in each bin.
+
+    y_prob : ndarray of shape (n_samples,)
+        Probability estimates for the positive class, for each sample.
+
+    estimator_name : str, default=None
+        Name of estimator. If None, the estimator name is not shown.
+
+    pos_label : str or int, default=None
+        The positive class when computing the calibration curve.
+        By default, `estimators.classes_[1]` is considered as the
+        positive class.
+
+        .. versionadded:: 1.1
+
+    Attributes
+    ----------
+    line_ : matplotlib Artist
+        Calibration curve.
+
+    ax_ : matplotlib Axes
+        Axes with calibration curve.
+
+    figure_ : matplotlib Figure
+        Figure containing the curve.
+
+    See Also
+    --------
+    calibration_curve : Compute true and predicted probabilities for a
+        calibration curve.
+    CalibrationDisplay.from_predictions : Plot calibration curve using true
+        and predicted labels.
+    CalibrationDisplay.from_estimator : Plot calibration curve using an
+        estimator and data.
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_classification
+    >>> from sklearn.model_selection import train_test_split
+    >>> from sklearn.linear_model import LogisticRegression
+    >>> from sklearn.calibration import calibration_curve, CalibrationDisplay
+    >>> X, y = make_classification(random_state=0)
+    >>> X_train, X_test, y_train, y_test = train_test_split(
+    ...     X, y, random_state=0)
+    >>> clf = LogisticRegression(random_state=0)
+    >>> clf.fit(X_train, y_train)
+    LogisticRegression(random_state=0)
+    >>> y_prob = clf.predict_proba(X_test)[:, 1]
+    >>> prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)
+    >>> disp = CalibrationDisplay(prob_true, prob_pred, y_prob)
+    >>> disp.plot()
+    <...>
+    """
+
+    def __init__(
+        self, prob_true, prob_pred, y_prob, *, estimator_name=None, pos_label=None
+    ):
+        self.prob_true = prob_true
+        self.prob_pred = prob_pred
+        self.y_prob = y_prob
+        self.estimator_name = estimator_name
+        self.pos_label = pos_label
+
+    def plot(self, *, ax=None, name=None, ref_line=True, **kwargs):
+        """Plot visualization.
+
+        Extra keyword arguments will be passed to
+        :func:`matplotlib.pyplot.plot`.
+
+        Parameters
+        ----------
+        ax : Matplotlib Axes, default=None
+            Axes object to plot on. If `None`, a new figure and axes is
+            created.
+
+        name : str, default=None
+            Name for labeling curve. If `None`, use `estimator_name` if
+            not `None`, otherwise no labeling is shown.
+
+        ref_line : bool, default=True
+            If `True`, plots a reference line representing a perfectly
+            calibrated classifier.
+
+        **kwargs : dict
+            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`.
+
+        Returns
+        -------
+        display : :class:`~sklearn.calibration.CalibrationDisplay`
+            Object that stores computed values.
+        """
+        check_matplotlib_support("CalibrationDisplay.plot")
+        import matplotlib.pyplot as plt
+
+        if ax is None:
+            fig, ax = plt.subplots()
+
+        name = self.estimator_name if name is None else name
+        info_pos_label = (
+            f"(Positive class: {self.pos_label})" if self.pos_label is not None else ""
+        )
+
+        line_kwargs = {}
+        if name is not None:
+            line_kwargs["label"] = name
+        line_kwargs.update(**kwargs)
+
+        ref_line_label = "Perfectly calibrated"
+        existing_ref_line = ref_line_label in ax.get_legend_handles_labels()[1]
+        if ref_line and not existing_ref_line:
+            ax.plot([0, 1], [0, 1], "k:", label=ref_line_label)
+        self.line_ = ax.plot(self.prob_pred, self.prob_true, "s-", **line_kwargs)[0]
+
+        # We always have to show the legend for at least the reference line
+        ax.legend(loc="lower right")
+
+        xlabel = f"Mean predicted probability {info_pos_label}"
+        ylabel = f"Fraction of positives {info_pos_label}"
+        ax.set(xlabel=xlabel, ylabel=ylabel)
+
+        self.ax_ = ax
+        self.figure_ = ax.figure
+        return self
+
+    @classmethod
+    def from_estimator(
+        cls,
+        estimator,
+        X,
+        y,
+        *,
+        n_bins=5,
+        strategy="uniform",
+        pos_label=None,
+        name=None,
+        ref_line=True,
+        ax=None,
+        **kwargs,
+    ):
+        """Plot calibration curve using a binary classifier and data.
+
+        A calibration curve, also known as a reliability diagram, uses inputs
+        from a binary classifier and plots the average predicted probability
+        for each bin against the fraction of positive classes, on the
+        y-axis.
+
+        Extra keyword arguments will be passed to
+        :func:`matplotlib.pyplot.plot`.
+
+        Read more about calibration in the :ref:`User Guide <calibration>` and
+        more about the scikit-learn visualization API in :ref:`visualizations`.
+
+        .. versionadded:: 1.0
+
+        Parameters
+        ----------
+        estimator : estimator instance
+            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`
+            in which the last estimator is a classifier. The classifier must
+            have a :term:`predict_proba` method.
+
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Input values.
+
+        y : array-like of shape (n_samples,)
+            Binary target values.
+
+        n_bins : int, default=5
+            Number of bins to discretize the [0, 1] interval into when
+            calculating the calibration curve. A bigger number requires more
+            data.
+
+        strategy : {'uniform', 'quantile'}, default='uniform'
+            Strategy used to define the widths of the bins.
+
+            - `'uniform'`: The bins have identical widths.
+            - `'quantile'`: The bins have the same number of samples and depend
+              on predicted probabilities.
+
+        pos_label : str or int, default=None
+            The positive class when computing the calibration curve.
+            By default, `estimators.classes_[1]` is considered as the
+            positive class.
+
+            .. versionadded:: 1.1
+
+        name : str, default=None
+            Name for labeling curve. If `None`, the name of the estimator is
+            used.
+
+        ref_line : bool, default=True
+            If `True`, plots a reference line representing a perfectly
+            calibrated classifier.
+
+        ax : matplotlib axes, default=None
+            Axes object to plot on. If `None`, a new figure and axes is
+            created.
+
+        **kwargs : dict
+            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`.
+
+        Returns
+        -------
+        display : :class:`~sklearn.calibration.CalibrationDisplay`.
+            Object that stores computed values.
+
+        See Also
+        --------
+        CalibrationDisplay.from_predictions : Plot calibration curve using true
+            and predicted labels.
+
+        Examples
+        --------
+        >>> import matplotlib.pyplot as plt
+        >>> from sklearn.datasets import make_classification
+        >>> from sklearn.model_selection import train_test_split
+        >>> from sklearn.linear_model import LogisticRegression
+        >>> from sklearn.calibration import CalibrationDisplay
+        >>> X, y = make_classification(random_state=0)
+        >>> X_train, X_test, y_train, y_test = train_test_split(
+        ...     X, y, random_state=0)
+        >>> clf = LogisticRegression(random_state=0)
+        >>> clf.fit(X_train, y_train)
+        LogisticRegression(random_state=0)
+        >>> disp = CalibrationDisplay.from_estimator(clf, X_test, y_test)
+        >>> plt.show()
+        """
+        method_name = f"{cls.__name__}.from_estimator"
+        check_matplotlib_support(method_name)
+
+        if not is_classifier(estimator):
+            raise ValueError("'estimator' should be a fitted classifier.")
+
+        y_prob, pos_label = _get_response(
+            X, estimator, response_method="predict_proba", pos_label=pos_label
+        )
+
+        name = name if name is not None else estimator.__class__.__name__
+        return cls.from_predictions(
+            y,
+            y_prob,
+            n_bins=n_bins,
+            strategy=strategy,
+            pos_label=pos_label,
+            name=name,
+            ref_line=ref_line,
+            ax=ax,
+            **kwargs,
+        )
+
+    @classmethod
+    def from_predictions(
+        cls,
+        y_true,
+        y_prob,
+        *,
+        n_bins=5,
+        strategy="uniform",
+        pos_label=None,
+        name=None,
+        ref_line=True,
+        ax=None,
+        **kwargs,
+    ):
+        """Plot calibration curve using true labels and predicted probabilities.
+
+        Calibration curve, also known as reliability diagram, uses inputs
+        from a binary classifier and plots the average predicted probability
+        for each bin against the fraction of positive classes, on the
+        y-axis.
+
+        Extra keyword arguments will be passed to
+        :func:`matplotlib.pyplot.plot`.
+
+        Read more about calibration in the :ref:`User Guide <calibration>` and
+        more about the scikit-learn visualization API in :ref:`visualizations`.
+
+        .. versionadded:: 1.0
+
+        Parameters
+        ----------
+        y_true : array-like of shape (n_samples,)
+            True labels.
+
+        y_prob : array-like of shape (n_samples,)
+            The predicted probabilities of the positive class.
+
+        n_bins : int, default=5
+            Number of bins to discretize the [0, 1] interval into when
+            calculating the calibration curve. A bigger number requires more
+            data.
+
+        strategy : {'uniform', 'quantile'}, default='uniform'
+            Strategy used to define the widths of the bins.
+
+            - `'uniform'`: The bins have identical widths.
+            - `'quantile'`: The bins have the same number of samples and depend
+              on predicted probabilities.
+
+        pos_label : str or int, default=None
+            The positive class when computing the calibration curve.
+            By default, `estimators.classes_[1]` is considered as the
+            positive class.
+
+            .. versionadded:: 1.1
+
+        name : str, default=None
+            Name for labeling curve.
+
+        ref_line : bool, default=True
+            If `True`, plots a reference line representing a perfectly
+            calibrated classifier.
+
+        ax : matplotlib axes, default=None
+            Axes object to plot on. If `None`, a new figure and axes is
+            created.
+
+        **kwargs : dict
+            Keyword arguments to be passed to :func:`matplotlib.pyplot.plot`.
+
+        Returns
+        -------
+        display : :class:`~sklearn.calibration.CalibrationDisplay`.
+            Object that stores computed values.
+
+        See Also
+        --------
+        CalibrationDisplay.from_estimator : Plot calibration curve using an
+            estimator and data.
+
+        Examples
+        --------
+        >>> import matplotlib.pyplot as plt
+        >>> from sklearn.datasets import make_classification
+        >>> from sklearn.model_selection import train_test_split
+        >>> from sklearn.linear_model import LogisticRegression
+        >>> from sklearn.calibration import CalibrationDisplay
+        >>> X, y = make_classification(random_state=0)
+        >>> X_train, X_test, y_train, y_test = train_test_split(
+        ...     X, y, random_state=0)
+        >>> clf = LogisticRegression(random_state=0)
+        >>> clf.fit(X_train, y_train)
+        LogisticRegression(random_state=0)
+        >>> y_prob = clf.predict_proba(X_test)[:, 1]
+        >>> disp = CalibrationDisplay.from_predictions(y_test, y_prob)
+        >>> plt.show()
+        """
+        method_name = f"{cls.__name__}.from_estimator"
+        check_matplotlib_support(method_name)
+
+        prob_true, prob_pred = calibration_curve(
+            y_true, y_prob, n_bins=n_bins, strategy=strategy, pos_label=pos_label
+        )
+        name = "Classifier" if name is None else name
+        pos_label = _check_pos_label_consistency(pos_label, y_true)
+
+        disp = cls(
+            prob_true=prob_true,
+            prob_pred=prob_pred,
+            y_prob=y_prob,
+            estimator_name=name,
+            pos_label=pos_label,
+        )
+        return disp.plot(ax=ax, ref_line=ref_line, **kwargs)
('sklearn/tree', '_utils.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,7 +1,3 @@
-# cython: cdivision=True
-# cython: boundscheck=False
-# cython: wraparound=False
-
 # Authors: Gilles Louppe <g.louppe@gmail.com>
 #          Peter Prettenhofer <peter.prettenhofer@gmail.com>
 #          Arnaud Joly <arnaud.v.joly@gmail.com>
@@ -77,213 +73,6 @@
 cdef inline double log(double x) nogil:
     return ln(x) / ln(2.0)

-
-# =============================================================================
-# Stack data structure
-# =============================================================================
-
-cdef class Stack:
-    """A LIFO data structure.
-
-    Attributes
-    ----------
-    capacity : SIZE_t
-        The elements the stack can hold; if more added then ``self.stack_``
-        needs to be resized.
-
-    top : SIZE_t
-        The number of elements currently on the stack.
-
-    stack : StackRecord pointer
-        The stack of records (upward in the stack corresponds to the right).
-    """
-
-    def __cinit__(self, SIZE_t capacity):
-        self.capacity = capacity
-        self.top = 0
-        self.stack_ = <StackRecord*> malloc(capacity * sizeof(StackRecord))
-
-    def __dealloc__(self):
-        free(self.stack_)
-
-    cdef bint is_empty(self) nogil:
-        return self.top <= 0
-
-    cdef int push(self, SIZE_t start, SIZE_t end, SIZE_t depth, SIZE_t parent,
-                  bint is_left, double impurity,
-                  SIZE_t n_constant_features) nogil except -1:
-        """Push a new element onto the stack.
-
-        Return -1 in case of failure to allocate memory (and raise MemoryError)
-        or 0 otherwise.
-        """
-        cdef SIZE_t top = self.top
-        cdef StackRecord* stack = NULL
-
-        # Resize if capacity not sufficient
-        if top >= self.capacity:
-            self.capacity *= 2
-            # Since safe_realloc can raise MemoryError, use `except -1`
-            safe_realloc(&self.stack_, self.capacity)
-
-        stack = self.stack_
-        stack[top].start = start
-        stack[top].end = end
-        stack[top].depth = depth
-        stack[top].parent = parent
-        stack[top].is_left = is_left
-        stack[top].impurity = impurity
-        stack[top].n_constant_features = n_constant_features
-
-        # Increment stack pointer
-        self.top = top + 1
-        return 0
-
-    cdef int pop(self, StackRecord* res) nogil:
-        """Remove the top element from the stack and copy to ``res``.
-
-        Returns 0 if pop was successful (and ``res`` is set); -1
-        otherwise.
-        """
-        cdef SIZE_t top = self.top
-        cdef StackRecord* stack = self.stack_
-
-        if top <= 0:
-            return -1
-
-        res[0] = stack[top - 1]
-        self.top = top - 1
-
-        return 0
-
-
-# =============================================================================
-# PriorityHeap data structure
-# =============================================================================
-
-cdef class PriorityHeap:
-    """A priority queue implemented as a binary heap.
-
-    The heap invariant is that the impurity improvement of the parent record
-    is larger then the impurity improvement of the children.
-
-    Attributes
-    ----------
-    capacity : SIZE_t
-        The capacity of the heap
-
-    heap_ptr : SIZE_t
-        The water mark of the heap; the heap grows from left to right in the
-        array ``heap_``. The following invariant holds ``heap_ptr < capacity``.
-
-    heap_ : PriorityHeapRecord*
-        The array of heap records. The maximum element is on the left;
-        the heap grows from left to right
-    """
-
-    def __cinit__(self, SIZE_t capacity):
-        self.capacity = capacity
-        self.heap_ptr = 0
-        safe_realloc(&self.heap_, capacity)
-
-    def __dealloc__(self):
-        free(self.heap_)
-
-    cdef bint is_empty(self) nogil:
-        return self.heap_ptr <= 0
-
-    cdef void heapify_up(self, PriorityHeapRecord* heap, SIZE_t pos) nogil:
-        """Restore heap invariant parent.improvement > child.improvement from
-           ``pos`` upwards. """
-        if pos == 0:
-            return
-
-        cdef SIZE_t parent_pos = (pos - 1) / 2
-
-        if heap[parent_pos].improvement < heap[pos].improvement:
-            heap[parent_pos], heap[pos] = heap[pos], heap[parent_pos]
-            self.heapify_up(heap, parent_pos)
-
-    cdef void heapify_down(self, PriorityHeapRecord* heap, SIZE_t pos,
-                           SIZE_t heap_length) nogil:
-        """Restore heap invariant parent.improvement > children.improvement from
-           ``pos`` downwards. """
-        cdef SIZE_t left_pos = 2 * (pos + 1) - 1
-        cdef SIZE_t right_pos = 2 * (pos + 1)
-        cdef SIZE_t largest = pos
-
-        if (left_pos < heap_length and
-                heap[left_pos].improvement > heap[largest].improvement):
-            largest = left_pos
-
-        if (right_pos < heap_length and
-                heap[right_pos].improvement > heap[largest].improvement):
-            largest = right_pos
-
-        if largest != pos:
-            heap[pos], heap[largest] = heap[largest], heap[pos]
-            self.heapify_down(heap, largest, heap_length)
-
-    cdef int push(self, SIZE_t node_id, SIZE_t start, SIZE_t end, SIZE_t pos,
-                  SIZE_t depth, bint is_leaf, double improvement,
-                  double impurity, double impurity_left,
-                  double impurity_right) nogil except -1:
-        """Push record on the priority heap.
-
-        Return -1 in case of failure to allocate memory (and raise MemoryError)
-        or 0 otherwise.
-        """
-        cdef SIZE_t heap_ptr = self.heap_ptr
-        cdef PriorityHeapRecord* heap = NULL
-
-        # Resize if capacity not sufficient
-        if heap_ptr >= self.capacity:
-            self.capacity *= 2
-            # Since safe_realloc can raise MemoryError, use `except -1`
-            safe_realloc(&self.heap_, self.capacity)
-
-        # Put element as last element of heap
-        heap = self.heap_
-        heap[heap_ptr].node_id = node_id
-        heap[heap_ptr].start = start
-        heap[heap_ptr].end = end
-        heap[heap_ptr].pos = pos
-        heap[heap_ptr].depth = depth
-        heap[heap_ptr].is_leaf = is_leaf
-        heap[heap_ptr].impurity = impurity
-        heap[heap_ptr].impurity_left = impurity_left
-        heap[heap_ptr].impurity_right = impurity_right
-        heap[heap_ptr].improvement = improvement
-
-        # Heapify up
-        self.heapify_up(heap, heap_ptr)
-
-        # Increase element count
-        self.heap_ptr = heap_ptr + 1
-        return 0
-
-    cdef int pop(self, PriorityHeapRecord* res) nogil:
-        """Remove max element from the heap. """
-        cdef SIZE_t heap_ptr = self.heap_ptr
-        cdef PriorityHeapRecord* heap = self.heap_
-
-        if heap_ptr <= 0:
-            return -1
-
-        # Take first element
-        res[0] = heap[0]
-
-        # Put last element to the front
-        heap[0], heap[heap_ptr - 1] = heap[heap_ptr - 1], heap[0]
-
-        # Restore heap invariant
-        if heap_ptr > 1:
-            self.heapify_down(heap, 0, heap_ptr - 1)
-
-        self.heap_ptr = heap_ptr - 1
-
-        return 0
-
 # =============================================================================
 # WeightedPQueue data structure
 # =============================================================================
@@ -511,7 +300,7 @@
         or 0 otherwise.
         """
         cdef int return_value
-        cdef DOUBLE_t original_median
+        cdef DOUBLE_t original_median = 0.0

         if self.size() != 0:
             original_median = self.get_median()
@@ -568,7 +357,7 @@
         from consideration in the median calculation
         """
         cdef int return_value
-        cdef DOUBLE_t original_median
+        cdef DOUBLE_t original_median = 0.0

         if self.size() != 0:
             original_median = self.get_median()
@@ -583,7 +372,7 @@
         left and moving to the right.
         """
         cdef int return_value
-        cdef double original_median
+        cdef double original_median = 0.0

         if self.size() != 0:
             original_median = self.get_median()
('sklearn/tree', '_tree.pxd')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -58,7 +58,7 @@
     cdef SIZE_t _add_node(self, SIZE_t parent, bint is_left, bint is_leaf,
                           SIZE_t feature, double threshold, double impurity,
                           SIZE_t n_node_samples,
-                          double weighted_n_samples) nogil except -1
+                          double weighted_n_node_samples) nogil except -1
     cdef int _resize(self, SIZE_t capacity) nogil except -1
     cdef int _resize_c(self, SIZE_t capacity=*) nogil except -1

@@ -96,10 +96,8 @@
     cdef SIZE_t min_samples_leaf        # Minimum number of samples in a leaf
     cdef double min_weight_leaf         # Minimum weight in a leaf
     cdef SIZE_t max_depth               # Maximal tree depth
-    cdef double min_impurity_split
     cdef double min_impurity_decrease   # Impurity threshold for early stopping

     cpdef build(self, Tree tree, object X, np.ndarray y,
-                np.ndarray sample_weight=*,
-                np.ndarray X_idx_sorted=*)
+                np.ndarray sample_weight=*)
     cdef _check_input(self, object X, np.ndarray y, np.ndarray sample_weight)
('sklearn/tree', '_criterion.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,7 +1,3 @@
-# cython: cdivision=True
-# cython: boundscheck=False
-# cython: wraparound=False
-
 # Authors: Gilles Louppe <g.louppe@gmail.com>
 #          Peter Prettenhofer <peter.prettenhofer@gmail.com>
 #          Brian Holt <bdholt1@gmail.com>
@@ -16,8 +12,6 @@
 #
 # License: BSD 3 clause

-from libc.stdlib cimport calloc
-from libc.stdlib cimport free
 from libc.string cimport memcpy
 from libc.string cimport memset
 from libc.math cimport fabs
@@ -26,10 +20,14 @@
 cimport numpy as np
 np.import_array()

+from numpy.math cimport INFINITY
+from scipy.special.cython_special cimport xlogy
+
 from ._utils cimport log
-from ._utils cimport safe_realloc
-from ._utils cimport sizet_ptr_to_ndarray
 from ._utils cimport WeightedMedianCalculator
+
+# EPSILON is used in the Poisson criterion
+cdef double EPSILON = 10 * np.finfo('double').eps

 cdef class Criterion:
     """Interface for impurity criteria.
@@ -37,14 +35,6 @@
     This object stores methods on how to calculate how good a split is using
     different metrics.
     """
-
-    def __dealloc__(self):
-        """Destructor."""
-
-        free(self.sum_total)
-        free(self.sum_left)
-        free(self.sum_right)
-
     def __getstate__(self):
         return {}

@@ -65,9 +55,9 @@
             y is a buffer that can store values for n_outputs target variables
         sample_weight : array-like, dtype=DOUBLE_t
             The weight of each sample
-        weighted_n_samples : DOUBLE_t
+        weighted_n_samples : double
             The total weight of the samples being considered
-        samples : array-like, dtype=DOUBLE_t
+        samples : array-like, dtype=SIZE_t
             Indices of the samples in X and y, where samples[start:end]
             correspond to the samples in this node
         start : SIZE_t
@@ -76,7 +66,6 @@
             The last sample used on this node

         """
-
         pass

     cdef int reset(self) nogil except -1:
@@ -84,7 +73,6 @@

         This method must be implemented by the subclass.
         """
-
         pass

     cdef int reverse_reset(self) nogil except -1:
@@ -106,7 +94,6 @@
         new_pos : SIZE_t
             New starting index position of the samples in the right child
         """
-
         pass

     cdef double node_impurity(self) nogil:
@@ -114,9 +101,9 @@

         Placeholder for a method which will evaluate the impurity of
         the current node, i.e. the impurity of samples[start:end]. This is the
-        primary function of the criterion class.
-        """
-
+        primary function of the criterion class. The smaller the impurity the
+        better.
+        """
         pass

     cdef void children_impurity(self, double* impurity_left,
@@ -136,7 +123,6 @@
             The memory address where the impurity of the right child should be
             stored
         """
-
         pass

     cdef void node_value(self, double* dest) nogil:
@@ -150,11 +136,10 @@
         dest : double pointer
             The memory address where the node value should be stored.
         """
-
         pass

     cdef double proxy_impurity_improvement(self) nogil:
-        """Compute a proxy of the impurity reduction
+        """Compute a proxy of the impurity reduction.

         This method is used to speed up the search for the best split.
         It is a proxy quantity such that the split that maximizes this value
@@ -171,8 +156,10 @@
         return (- self.weighted_n_right * impurity_right
                 - self.weighted_n_left * impurity_left)

-    cdef double impurity_improvement(self, double impurity) nogil:
-        """Compute the improvement in impurity
+    cdef double impurity_improvement(self, double impurity_parent,
+                                     double impurity_left,
+                                     double impurity_right) nogil:
+        """Compute the improvement in impurity.

         This method computes the improvement in impurity when a split occurs.
         The weighted impurity improvement equation is the following:
@@ -186,24 +173,24 @@

         Parameters
         ----------
-        impurity : double
-            The initial impurity of the node before the split
+        impurity_parent : double
+            The initial impurity of the parent node before the split
+
+        impurity_left : double
+            The impurity of the left child
+
+        impurity_right : double
+            The impurity of the right child

         Return
         ------
         double : improvement in impurity after the split occurs
         """
-
-        cdef double impurity_left
-        cdef double impurity_right
-
-        self.children_impurity(&impurity_left, &impurity_right)
-
         return ((self.weighted_n_node_samples / self.weighted_n_samples) *
-                (impurity - (self.weighted_n_right /
-                             self.weighted_n_node_samples * impurity_right)
-                          - (self.weighted_n_left /
-                             self.weighted_n_node_samples * impurity_left)))
+                (impurity_parent - (self.weighted_n_right /
+                                    self.weighted_n_node_samples * impurity_right)
+                                 - (self.weighted_n_left /
+                                    self.weighted_n_node_samples * impurity_left)))


 cdef class ClassificationCriterion(Criterion):
@@ -220,7 +207,6 @@
         n_classes : numpy.ndarray, dtype=SIZE_t
             The number of unique classes in each target
         """
-
         self.sample_weight = NULL

         self.samples = NULL
@@ -235,52 +221,37 @@
         self.weighted_n_left = 0.0
         self.weighted_n_right = 0.0

-        # Count labels for each output
-        self.sum_total = NULL
-        self.sum_left = NULL
-        self.sum_right = NULL
-        self.n_classes = NULL
-
-        safe_realloc(&self.n_classes, n_outputs)
+        self.n_classes = np.empty(n_outputs, dtype=np.intp)

         cdef SIZE_t k = 0
-        cdef SIZE_t sum_stride = 0
+        cdef SIZE_t max_n_classes = 0

         # For each target, set the number of unique classes in that target,
         # and also compute the maximal stride of all targets
         for k in range(n_outputs):
             self.n_classes[k] = n_classes[k]

-            if n_classes[k] > sum_stride:
-                sum_stride = n_classes[k]
-
-        self.sum_stride = sum_stride
-
-        cdef SIZE_t n_elements = n_outputs * sum_stride
-        self.sum_total = <double*> calloc(n_elements, sizeof(double))
-        self.sum_left = <double*> calloc(n_elements, sizeof(double))
-        self.sum_right = <double*> calloc(n_elements, sizeof(double))
-
-        if (self.sum_total == NULL or
-                self.sum_left == NULL or
-                self.sum_right == NULL):
-            raise MemoryError()
-
-    def __dealloc__(self):
-        """Destructor."""
-        free(self.n_classes)
+            if n_classes[k] > max_n_classes:
+                max_n_classes = n_classes[k]
+
+        self.max_n_classes = max_n_classes
+
+        # Count labels for each output
+        self.sum_total = np.zeros((n_outputs, max_n_classes), dtype=np.float64)
+        self.sum_left = np.zeros((n_outputs, max_n_classes), dtype=np.float64)
+        self.sum_right = np.zeros((n_outputs, max_n_classes), dtype=np.float64)

     def __reduce__(self):
         return (type(self),
-                (self.n_outputs,
-                 sizet_ptr_to_ndarray(self.n_classes, self.n_outputs)),
-                self.__getstate__())
+                (self.n_outputs, np.asarray(self.n_classes)), self.__getstate__())

     cdef int init(self, const DOUBLE_t[:, ::1] y,
                   DOUBLE_t* sample_weight, double weighted_n_samples,
                   SIZE_t* samples, SIZE_t start, SIZE_t end) nogil except -1:
-        """Initialize the criterion at node samples[start:end] and
-        children samples[start:start] and samples[start:end].
+        """Initialize the criterion.
+
+        This initializes the criterion at node samples[start:end] and children
+        samples[start:start] and samples[start:end].

         Returns -1 in case of failure to allocate memory (and raise MemoryError)
         or 0 otherwise.
@@ -289,9 +260,9 @@
         ----------
         y : array-like, dtype=DOUBLE_t
             The target stored as a buffer for memory efficiency
-        sample_weight : array-like, dtype=DTYPE_t
+        sample_weight : array-like, dtype=DOUBLE_t
             The weight of each sample
-        weighted_n_samples : SIZE_t
+        weighted_n_samples : double
             The total weight of all samples
         samples : array-like, dtype=SIZE_t
             A mask on the samples, showing which ones we want to use
@@ -300,7 +271,6 @@
         end : SIZE_t
             The last sample to use in the mask
         """
-
         self.y = y
         self.sample_weight = sample_weight
         self.samples = samples
@@ -310,19 +280,14 @@
         self.weighted_n_samples = weighted_n_samples
         self.weighted_n_node_samples = 0.0

-        cdef SIZE_t* n_classes = self.n_classes
-        cdef double* sum_total = self.sum_total
-
         cdef SIZE_t i
         cdef SIZE_t p
         cdef SIZE_t k
         cdef SIZE_t c
         cdef DOUBLE_t w = 1.0
-        cdef SIZE_t offset = 0
-
-        for k in range(self.n_outputs):
-            memset(sum_total + offset, 0, n_classes[k] * sizeof(double))
-            offset += self.sum_stride
+
+        for k in range(self.n_outputs):
+            memset(&self.sum_total[k, 0], 0, self.n_classes[k] * sizeof(double))

         for p in range(start, end):
             i = samples[p]
@@ -335,7 +300,7 @@
             # Count weighted class frequency for each target
             for k in range(self.n_outputs):
                 c = <SIZE_t> self.y[i, k]
-                sum_total[k * self.sum_stride + c] += w
+                self.sum_total[k, c] += w

             self.weighted_n_node_samples += w

@@ -344,7 +309,7 @@
         return 0

     cdef int reset(self) nogil except -1:
-        """Reset the criterion at pos=start
+        """Reset the criterion at pos=start.

         Returns -1 in case of failure to allocate memory (and raise MemoryError)
         or 0 otherwise.
@@ -353,25 +318,15 @@

         self.weighted_n_left = 0.0
         self.weighted_n_right = self.weighted_n_node_samples
-
-        cdef double* sum_total = self.sum_total
-        cdef double* sum_left = self.sum_left
-        cdef double* sum_right = self.sum_right
-
-        cdef SIZE_t* n_classes = self.n_classes
-        cdef SIZE_t k
-
-        for k in range(self.n_outputs):
-            memset(sum_left, 0, n_classes[k] * sizeof(double))
-            memcpy(sum_right, sum_total, n_classes[k] * sizeof(double))
-
-            sum_total += self.sum_stride
-            sum_left += self.sum_stride
-            sum_right += self.sum_stride
+        cdef SIZE_t k
+
+        for k in range(self.n_outputs):
+            memset(&self.sum_left[k, 0], 0, self.n_classes[k] * sizeof(double))
+            memcpy(&self.sum_right[k, 0], &self.sum_total[k, 0], self.n_classes[k] * sizeof(double))
         return 0

     cdef int reverse_reset(self) nogil except -1:
-        """Reset the criterion at pos=end
+        """Reset the criterion at pos=end.

         Returns -1 in case of failure to allocate memory (and raise MemoryError)
         or 0 otherwise.
@@ -380,21 +335,11 @@

         self.weighted_n_left = self.weighted_n_node_samples
         self.weighted_n_right = 0.0
-
-        cdef double* sum_total = self.sum_total
-        cdef double* sum_left = self.sum_left
-        cdef double* sum_right = self.sum_right
-
-        cdef SIZE_t* n_classes = self.n_classes
-        cdef SIZE_t k
-
-        for k in range(self.n_outputs):
-            memset(sum_right, 0, n_classes[k] * sizeof(double))
-            memcpy(sum_left, sum_total, n_classes[k] * sizeof(double))
-
-            sum_total += self.sum_stride
-            sum_left += self.sum_stride
-            sum_right += self.sum_stride
+        cdef SIZE_t k
+
+        for k in range(self.n_outputs):
+            memset(&self.sum_right[k, 0], 0, self.n_classes[k] * sizeof(double))
+            memcpy(&self.sum_left[k, 0],  &self.sum_total[k, 0], self.n_classes[k] * sizeof(double))
         return 0

     cdef int update(self, SIZE_t new_pos) nogil except -1:
@@ -412,11 +357,6 @@
         cdef SIZE_t pos = self.pos
         cdef SIZE_t end = self.end

-        cdef double* sum_left = self.sum_left
-        cdef double* sum_right = self.sum_right
-        cdef double* sum_total = self.sum_total
-
-        cdef SIZE_t* n_classes = self.n_classes
         cdef SIZE_t* samples = self.samples
         cdef DOUBLE_t* sample_weight = self.sample_weight

@@ -434,7 +374,6 @@
         # and that sum_total is known, we are going to update
         # sum_left from the direction that require the least amount
         # of computations, i.e. from pos to new_pos or from end to new_po.
-
         if (new_pos - pos) <= (end - new_pos):
             for p in range(pos, new_pos):
                 i = samples[p]
@@ -443,8 +382,7 @@
                     w = sample_weight[i]

                 for k in range(self.n_outputs):
-                    label_index = k * self.sum_stride + <SIZE_t> self.y[i, k]
-                    sum_left[label_index] += w
+                    self.sum_left[k, <SIZE_t> self.y[i, k]] += w

                 self.weighted_n_left += w

@@ -458,20 +396,15 @@
                     w = sample_weight[i]

                 for k in range(self.n_outputs):
-                    label_index = k * self.sum_stride + <SIZE_t> self.y[i, k]
-                    sum_left[label_index] -= w
+                    self.sum_left[k, <SIZE_t> self.y[i, k]] -= w

                 self.weighted_n_left -= w

         # Update right part statistics
         self.weighted_n_right = self.weighted_n_node_samples - self.weighted_n_left
         for k in range(self.n_outputs):
-            for c in range(n_classes[k]):
-                sum_right[c] = sum_total[c] - sum_left[c]
-
-            sum_right += self.sum_stride
-            sum_left += self.sum_stride
-            sum_total += self.sum_stride
+            for c in range(self.n_classes[k]):
+                self.sum_right[k, c] = self.sum_total[k, c] - self.sum_left[k, c]

         self.pos = new_pos
         return 0
@@ -491,15 +424,11 @@
         dest : double pointer
             The memory address which we will save the node value into.
         """
-
-        cdef double* sum_total = self.sum_total
-        cdef SIZE_t* n_classes = self.n_classes
-        cdef SIZE_t k
-
-        for k in range(self.n_outputs):
-            memcpy(dest, sum_total, n_classes[k] * sizeof(double))
-            dest += self.sum_stride
-            sum_total += self.sum_stride
+        cdef SIZE_t k
+
+        for k in range(self.n_outputs):
+            memcpy(dest, &self.sum_total[k, 0], self.n_classes[k] * sizeof(double))
+            dest += self.max_n_classes


 cdef class Entropy(ClassificationCriterion):
@@ -519,30 +448,29 @@
     """

     cdef double node_impurity(self) nogil:
-        """Evaluate the impurity of the current node, i.e. the impurity of
-        samples[start:end], using the cross-entropy criterion."""
-
-        cdef SIZE_t* n_classes = self.n_classes
-        cdef double* sum_total = self.sum_total
+        """Evaluate the impurity of the current node.
+
+        Evaluate the cross-entropy criterion as impurity of the current node,
+        i.e. the impurity of samples[start:end]. The smaller the impurity the
+        better.
+        """
         cdef double entropy = 0.0
         cdef double count_k
         cdef SIZE_t k
         cdef SIZE_t c

         for k in range(self.n_outputs):
-            for c in range(n_classes[k]):
-                count_k = sum_total[c]
+            for c in range(self.n_classes[k]):
+                count_k = self.sum_total[k, c]
                 if count_k > 0.0:
                     count_k /= self.weighted_n_node_samples
                     entropy -= count_k * log(count_k)

-            sum_total += self.sum_stride
-
         return entropy / self.n_outputs

     cdef void children_impurity(self, double* impurity_left,
                                 double* impurity_right) nogil:
-        """Evaluate the impurity in children nodes
+        """Evaluate the impurity in children nodes.

         i.e. the impurity of the left child (samples[start:pos]) and the
         impurity the right child (samples[pos:end]).
@@ -554,10 +482,6 @@
         impurity_right : double pointer
             The memory address to save the impurity of the right node
         """
-
-        cdef SIZE_t* n_classes = self.n_classes
-        cdef double* sum_left = self.sum_left
-        cdef double* sum_right = self.sum_right
         cdef double entropy_left = 0.0
         cdef double entropy_right = 0.0
         cdef double count_k
@@ -565,20 +489,17 @@
         cdef SIZE_t c

         for k in range(self.n_outputs):
-            for c in range(n_classes[k]):
-                count_k = sum_left[c]
+            for c in range(self.n_classes[k]):
+                count_k = self.sum_left[k, c]
                 if count_k > 0.0:
                     count_k /= self.weighted_n_left
                     entropy_left -= count_k * log(count_k)

-                count_k = sum_right[c]
+                count_k = self.sum_right[k, c]
                 if count_k > 0.0:
                     count_k /= self.weighted_n_right
                     entropy_right -= count_k * log(count_k)

-            sum_left += self.sum_stride
-            sum_right += self.sum_stride
-
         impurity_left[0] = entropy_left / self.n_outputs
         impurity_right[0] = entropy_right / self.n_outputs

@@ -601,12 +522,12 @@
     """

     cdef double node_impurity(self) nogil:
-        """Evaluate the impurity of the current node, i.e. the impurity of
-        samples[start:end] using the Gini criterion."""
-
-
-        cdef SIZE_t* n_classes = self.n_classes
-        cdef double* sum_total = self.sum_total
+        """Evaluate the impurity of the current node.
+
+        Evaluate the Gini criterion as impurity of the current node,
+        i.e. the impurity of samples[start:end]. The smaller the impurity the
+        better.
+        """
         cdef double gini = 0.0
         cdef double sq_count
         cdef double count_k
@@ -616,35 +537,29 @@
         for k in range(self.n_outputs):
             sq_count = 0.0

-            for c in range(n_classes[k]):
-                count_k = sum_total[c]
+            for c in range(self.n_classes[k]):
+                count_k = self.sum_total[k, c]
                 sq_count += count_k * count_k

             gini += 1.0 - sq_count / (self.weighted_n_node_samples *
                                       self.weighted_n_node_samples)

-            sum_total += self.sum_stride
-
         return gini / self.n_outputs

     cdef void children_impurity(self, double* impurity_left,
                                 double* impurity_right) nogil:
-        """Evaluate the impurity in children nodes
+        """Evaluate the impurity in children nodes.

         i.e. the impurity of the left child (samples[start:pos]) and the
         impurity the right child (samples[pos:end]) using the Gini index.

         Parameters
         ----------
-        impurity_left : DTYPE_t
+        impurity_left : double pointer
             The memory address to save the impurity of the left node to
-        impurity_right : DTYPE_t
+        impurity_right : double pointer
             The memory address to save the impurity of the right node to
         """
-
-        cdef SIZE_t* n_classes = self.n_classes
-        cdef double* sum_left = self.sum_left
-        cdef double* sum_right = self.sum_right
         cdef double gini_left = 0.0
         cdef double gini_right = 0.0
         cdef double sq_count_left
@@ -657,11 +572,11 @@
             sq_count_left = 0.0
             sq_count_right = 0.0

-            for c in range(n_classes[k]):
-                count_k = sum_left[c]
+            for c in range(self.n_classes[k]):
+                count_k = self.sum_left[k, c]
                 sq_count_left += count_k * count_k

-                count_k = sum_right[c]
+                count_k = self.sum_right[k, c]
                 sq_count_right += count_k * count_k

             gini_left += 1.0 - sq_count_left / (self.weighted_n_left *
@@ -669,9 +584,6 @@

             gini_right += 1.0 - sq_count_right / (self.weighted_n_right *
                                                   self.weighted_n_right)
-
-            sum_left += self.sum_stride
-            sum_right += self.sum_stride

         impurity_left[0] = gini_left / self.n_outputs
         impurity_right[0] = gini_right / self.n_outputs
@@ -700,7 +612,6 @@
         n_samples : SIZE_t
             The total number of samples to fit on
         """
-
         # Default values
         self.sample_weight = NULL

@@ -718,21 +629,9 @@

         self.sq_sum_total = 0.0

-        # Allocate accumulators. Make sure they are NULL, not uninitialized,
-        # before an exception can be raised (which triggers __dealloc__).
-        self.sum_total = NULL
-        self.sum_left = NULL
-        self.sum_right = NULL
-
-        # Allocate memory for the accumulators
-        self.sum_total = <double*> calloc(n_outputs, sizeof(double))
-        self.sum_left = <double*> calloc(n_outputs, sizeof(double))
-        self.sum_right = <double*> calloc(n_outputs, sizeof(double))
-
-        if (self.sum_total == NULL or
-                self.sum_left == NULL or
-                self.sum_right == NULL):
-            raise MemoryError()
+        self.sum_total = np.zeros(n_outputs, dtype=np.float64)
+        self.sum_left = np.zeros(n_outputs, dtype=np.float64)
+        self.sum_right = np.zeros(n_outputs, dtype=np.float64)

     def __reduce__(self):
         return (type(self), (self.n_outputs, self.n_samples), self.__getstate__())
@@ -740,8 +639,11 @@
     cdef int init(self, const DOUBLE_t[:, ::1] y, DOUBLE_t* sample_weight,
                   double weighted_n_samples, SIZE_t* samples, SIZE_t start,
                   SIZE_t end) nogil except -1:
-        """Initialize the criterion at node samples[start:end] and
-           children samples[start:start] and samples[start:end]."""
+        """Initialize the criterion.
+
+        This initializes the criterion at node samples[start:end] and children
+        samples[start:start] and samples[start:end].
+        """
         # Initialize fields
         self.y = y
         self.sample_weight = sample_weight
@@ -758,9 +660,8 @@
         cdef DOUBLE_t y_ik
         cdef DOUBLE_t w_y_ik
         cdef DOUBLE_t w = 1.0
-
         self.sq_sum_total = 0.0
-        memset(self.sum_total, 0, self.n_outputs * sizeof(double))
+        memset(&self.sum_total[0], 0, self.n_outputs * sizeof(double))

         for p in range(start, end):
             i = samples[p]
@@ -783,8 +684,8 @@
     cdef int reset(self) nogil except -1:
         """Reset the criterion at pos=start."""
         cdef SIZE_t n_bytes = self.n_outputs * sizeof(double)
-        memset(self.sum_left, 0, n_bytes)
-        memcpy(self.sum_right, self.sum_total, n_bytes)
+        memset(&self.sum_left[0], 0, n_bytes)
+        memcpy(&self.sum_right[0], &self.sum_total[0], n_bytes)

         self.weighted_n_left = 0.0
         self.weighted_n_right = self.weighted_n_node_samples
@@ -794,8 +695,8 @@
     cdef int reverse_reset(self) nogil except -1:
         """Reset the criterion at pos=end."""
         cdef SIZE_t n_bytes = self.n_outputs * sizeof(double)
-        memset(self.sum_right, 0, n_bytes)
-        memcpy(self.sum_left, self.sum_total, n_bytes)
+        memset(&self.sum_right[0], 0, n_bytes)
+        memcpy(&self.sum_left[0], &self.sum_total[0], n_bytes)

         self.weighted_n_right = 0.0
         self.weighted_n_left = self.weighted_n_node_samples
@@ -804,11 +705,6 @@

     cdef int update(self, SIZE_t new_pos) nogil except -1:
         """Updated statistics by moving samples[pos:new_pos] to the left."""
-
-        cdef double* sum_left = self.sum_left
-        cdef double* sum_right = self.sum_right
-        cdef double* sum_total = self.sum_total
-
         cdef double* sample_weight = self.sample_weight
         cdef SIZE_t* samples = self.samples

@@ -826,7 +722,6 @@
         # and that sum_total is known, we are going to update
         # sum_left from the direction that require the least amount
         # of computations, i.e. from pos to new_pos or from end to new_pos.
-
         if (new_pos - pos) <= (end - new_pos):
             for p in range(pos, new_pos):
                 i = samples[p]
@@ -835,7 +730,7 @@
                     w = sample_weight[i]

                 for k in range(self.n_outputs):
-                    sum_left[k] += w * self.y[i, k]
+                    self.sum_left[k] += w * self.y[i, k]

                 self.weighted_n_left += w
         else:
@@ -848,14 +743,14 @@
                     w = sample_weight[i]

                 for k in range(self.n_outputs):
-                    sum_left[k] -= w * self.y[i, k]
+                    self.sum_left[k] -= w * self.y[i, k]

                 self.weighted_n_left -= w

         self.weighted_n_right = (self.weighted_n_node_samples -
                                  self.weighted_n_left)
         for k in range(self.n_outputs):
-            sum_right[k] = sum_total[k] - sum_left[k]
+            self.sum_right[k] = self.sum_total[k] - self.sum_left[k]

         self.pos = new_pos
         return 0
@@ -869,7 +764,6 @@

     cdef void node_value(self, double* dest) nogil:
         """Compute the node value of samples[start:end] into dest."""
-
         cdef SIZE_t k

         for k in range(self.n_outputs):
@@ -883,21 +777,23 @@
     """

     cdef double node_impurity(self) nogil:
-        """Evaluate the impurity of the current node, i.e. the impurity of
-           samples[start:end]."""
-
-        cdef double* sum_total = self.sum_total
+        """Evaluate the impurity of the current node.
+
+        Evaluate the MSE criterion as impurity of the current node,
+        i.e. the impurity of samples[start:end]. The smaller the impurity the
+        better.
+        """
         cdef double impurity
         cdef SIZE_t k

         impurity = self.sq_sum_total / self.weighted_n_node_samples
         for k in range(self.n_outputs):
-            impurity -= (sum_total[k] / self.weighted_n_node_samples)**2.0
+            impurity -= (self.sum_total[k] / self.weighted_n_node_samples)**2.0

         return impurity / self.n_outputs

     cdef double proxy_impurity_improvement(self) nogil:
-        """Compute a proxy of the impurity reduction
+        """Compute a proxy of the impurity reduction.

         This method is used to speed up the search for the best split.
         It is a proxy quantity such that the split that maximizes this value
@@ -906,35 +802,39 @@

         The absolute impurity improvement is only computed by the
         impurity_improvement method once the best split has been found.
-        """
-
-        cdef double* sum_left = self.sum_left
-        cdef double* sum_right = self.sum_right
-
+
+        The MSE proxy is derived from
+
+            sum_{i left}(y_i - y_pred_L)^2 + sum_{i right}(y_i - y_pred_R)^2
+            = sum(y_i^2) - n_L * mean_{i left}(y_i)^2 - n_R * mean_{i right}(y_i)^2
+
+        Neglecting constant terms, this gives:
+
+            - 1/n_L * sum_{i left}(y_i)^2 - 1/n_R * sum_{i right}(y_i)^2
+        """
         cdef SIZE_t k
         cdef double proxy_impurity_left = 0.0
         cdef double proxy_impurity_right = 0.0

         for k in range(self.n_outputs):
-            proxy_impurity_left += sum_left[k] * sum_left[k]
-            proxy_impurity_right += sum_right[k] * sum_right[k]
+            proxy_impurity_left += self.sum_left[k] * self.sum_left[k]
+            proxy_impurity_right += self.sum_right[k] * self.sum_right[k]

         return (proxy_impurity_left / self.weighted_n_left +
                 proxy_impurity_right / self.weighted_n_right)

     cdef void children_impurity(self, double* impurity_left,
                                 double* impurity_right) nogil:
-        """Evaluate the impurity in children nodes, i.e. the impurity of the
-           left child (samples[start:pos]) and the impurity the right child
-           (samples[pos:end])."""
-
+        """Evaluate the impurity in children nodes.
+
+        i.e. the impurity of the left child (samples[start:pos]) and the
+        impurity the right child (samples[pos:end]).
+        """
         cdef DOUBLE_t* sample_weight = self.sample_weight
         cdef SIZE_t* samples = self.samples
         cdef SIZE_t pos = self.pos
         cdef SIZE_t start = self.start

-        cdef double* sum_left = self.sum_left
-        cdef double* sum_right = self.sum_right
         cdef DOUBLE_t y_ik

         cdef double sq_sum_left = 0.0
@@ -961,24 +861,22 @@
         impurity_right[0] = sq_sum_right / self.weighted_n_right

         for k in range(self.n_outputs):
-            impurity_left[0] -= (sum_left[k] / self.weighted_n_left) ** 2.0
-            impurity_right[0] -= (sum_right[k] / self.weighted_n_right) ** 2.0
+            impurity_left[0] -= (self.sum_left[k] / self.weighted_n_left) ** 2.0
+            impurity_right[0] -= (self.sum_right[k] / self.weighted_n_right) ** 2.0

         impurity_left[0] /= self.n_outputs
         impurity_right[0] /= self.n_outputs

+
 cdef class MAE(RegressionCriterion):
-    r"""Mean absolute error impurity criterion
+    r"""Mean absolute error impurity criterion.

        MAE = (1 / n)*(\sum_i |y_i - f_i|), where y_i is the true
        value and f_i is the predicted value."""
-    def __dealloc__(self):
-        """Destructor."""
-        free(self.node_medians)

     cdef np.ndarray left_child
     cdef np.ndarray right_child
-    cdef DOUBLE_t* node_medians
+    cdef DOUBLE_t[::1] node_medians

     def __cinit__(self, SIZE_t n_outputs, SIZE_t n_samples):
         """Initialize parameters for this criterion.
@@ -991,7 +889,6 @@
         n_samples : SIZE_t
             The total number of samples to fit on
         """
-
         # Default values
         self.sample_weight = NULL

@@ -1007,12 +904,7 @@
         self.weighted_n_left = 0.0
         self.weighted_n_right = 0.0

-        # Allocate accumulators. Make sure they are NULL, not uninitialized,
-        # before an exception can be raised (which triggers __dealloc__).
-        self.node_medians = NULL
-
-        # Allocate memory for the accumulators
-        safe_realloc(&self.node_medians, n_outputs)
+        self.node_medians = np.zeros(n_outputs, dtype=np.float64)

         self.left_child = np.empty(n_outputs, dtype='object')
         self.right_child = np.empty(n_outputs, dtype='object')
@@ -1024,9 +916,11 @@
     cdef int init(self, const DOUBLE_t[:, ::1] y, DOUBLE_t* sample_weight,
                   double weighted_n_samples, SIZE_t* samples, SIZE_t start,
                   SIZE_t end) nogil except -1:
-        """Initialize the criterion at node samples[start:end] and
-           children samples[start:start] and samples[start:end]."""
-
+        """Initialize the criterion.
+
+        This initializes the criterion at node samples[start:end] and children
+        samples[start:start] and samples[start:end].
+        """
         cdef SIZE_t i, p, k
         cdef DOUBLE_t w = 1.0

@@ -1072,12 +966,11 @@
         return 0

     cdef int reset(self) nogil except -1:
-        """Reset the criterion at pos=start
+        """Reset the criterion at pos=start.

         Returns -1 in case of failure to allocate memory (and raise MemoryError)
         or 0 otherwise.
         """
-
         cdef SIZE_t i, k
         cdef DOUBLE_t value
         cdef DOUBLE_t weight
@@ -1104,12 +997,11 @@
         return 0

     cdef int reverse_reset(self) nogil except -1:
-        """Reset the criterion at pos=end
+        """Reset the criterion at pos=end.

         Returns -1 in case of failure to allocate memory (and raise MemoryError)
         or 0 otherwise.
         """
-
         self.weighted_n_right = 0.0
         self.weighted_n_left = self.weighted_n_node_samples
         self.pos = self.end
@@ -1133,12 +1025,11 @@
         return 0

     cdef int update(self, SIZE_t new_pos) nogil except -1:
-        """Updated statistics by moving samples[pos:new_pos] to the left
+        """Updated statistics by moving samples[pos:new_pos] to the left.

         Returns -1 in case of failure to allocate memory (and raise MemoryError)
         or 0 otherwise.
         """
-
         cdef DOUBLE_t* sample_weight = self.sample_weight
         cdef SIZE_t* samples = self.samples

@@ -1155,7 +1046,6 @@
         # We are going to update right_child and left_child
         # from the direction that require the least amount of
         # computations, i.e. from pos to new_pos or from end to new_pos.
-
         if (new_pos - pos) <= (end - new_pos):
             for p in range(pos, new_pos):
                 i = samples[p]
@@ -1193,15 +1083,17 @@

     cdef void node_value(self, double* dest) nogil:
         """Computes the node value of samples[start:end] into dest."""
-
         cdef SIZE_t k
         for k in range(self.n_outputs):
             dest[k] = <double> self.node_medians[k]

     cdef double node_impurity(self) nogil:
-        """Evaluate the impurity of the current node, i.e. the impurity of
-           samples[start:end]"""
-
+        """Evaluate the impurity of the current node.
+
+        Evaluate the MAE criterion as impurity of the current node,
+        i.e. the impurity of samples[start:end]. The smaller the impurity the
+        better.
+        """
         cdef DOUBLE_t* sample_weight = self.sample_weight
         cdef SIZE_t* samples = self.samples
         cdef SIZE_t i, p, k
@@ -1221,11 +1113,11 @@

     cdef void children_impurity(self, double* p_impurity_left,
                                 double* p_impurity_right) nogil:
-        """Evaluate the impurity in children nodes, i.e. the impurity of the
-           left child (samples[start:pos]) and the impurity the right child
-           (samples[pos:end]).
-        """
-
+        """Evaluate the impurity in children nodes.
+
+        i.e. the impurity of the left child (samples[start:pos]) and the
+        impurity the right child (samples[pos:end]).
+        """
         cdef DOUBLE_t* sample_weight = self.sample_weight
         cdef SIZE_t* samples = self.samples

@@ -1251,7 +1143,7 @@
                     w = sample_weight[i]

                 impurity_left += fabs(self.y[i, k] - median) * w
-        p_impurity_left[0] = impurity_left / (self.weighted_n_left *
+        p_impurity_left[0] = impurity_left / (self.weighted_n_left *
                                               self.n_outputs)

         for k in range(self.n_outputs):
@@ -1263,12 +1155,12 @@
                     w = sample_weight[i]

                 impurity_right += fabs(self.y[i, k] - median) * w
-        p_impurity_right[0] = impurity_right / (self.weighted_n_right *
+        p_impurity_right[0] = impurity_right / (self.weighted_n_right *
                                                 self.n_outputs)


 cdef class FriedmanMSE(MSE):
-    """Mean squared error impurity criterion with improvement score by Friedman
+    """Mean squared error impurity criterion with improvement score by Friedman.

     Uses the formula (35) in Friedman's original Gradient Boosting paper:

@@ -1277,7 +1169,7 @@
     """

     cdef double proxy_impurity_improvement(self) nogil:
-        """Compute a proxy of the impurity reduction
+        """Compute a proxy of the impurity reduction.

         This method is used to speed up the search for the best split.
         It is a proxy quantity such that the split that maximizes this value
@@ -1287,10 +1179,6 @@
         The absolute impurity improvement is only computed by the
         impurity_improvement method once the best split has been found.
         """
-
-        cdef double* sum_left = self.sum_left
-        cdef double* sum_right = self.sum_right
-
         cdef double total_sum_left = 0.0
         cdef double total_sum_right = 0.0

@@ -1298,18 +1186,17 @@
         cdef double diff = 0.0

         for k in range(self.n_outputs):
-            total_sum_left += sum_left[k]
-            total_sum_right += sum_right[k]
+            total_sum_left += self.sum_left[k]
+            total_sum_right += self.sum_right[k]

         diff = (self.weighted_n_right * total_sum_left -
                 self.weighted_n_left * total_sum_right)

         return diff * diff / (self.weighted_n_left * self.weighted_n_right)

-    cdef double impurity_improvement(self, double impurity) nogil:
-        cdef double* sum_left = self.sum_left
-        cdef double* sum_right = self.sum_right
-
+    cdef double impurity_improvement(self, double impurity_parent, double
+                                     impurity_left, double impurity_right) nogil:
+        # Note: none of the arguments are used here
         cdef double total_sum_left = 0.0
         cdef double total_sum_right = 0.0

@@ -1317,11 +1204,147 @@
         cdef double diff = 0.0

         for k in range(self.n_outputs):
-            total_sum_left += sum_left[k]
-            total_sum_right += sum_right[k]
+            total_sum_left += self.sum_left[k]
+            total_sum_right += self.sum_right[k]

         diff = (self.weighted_n_right * total_sum_left -
                 self.weighted_n_left * total_sum_right) / self.n_outputs

         return (diff * diff / (self.weighted_n_left * self.weighted_n_right *
                                self.weighted_n_node_samples))
+
+
+cdef class Poisson(RegressionCriterion):
+    """Half Poisson deviance as impurity criterion.
+
+    Poisson deviance = 2/n * sum(y_true * log(y_true/y_pred) + y_pred - y_true)
+
+    Note that the deviance is >= 0, and since we have `y_pred = mean(y_true)`
+    at the leaves, one always has `sum(y_pred - y_true) = 0`. It remains the
+    implemented impurity (factor 2 is skipped):
+        1/n * sum(y_true * log(y_true/y_pred)
+    """
+    # FIXME in 1.0:
+    # min_impurity_split with default = 0 forces us to use a non-negative
+    # impurity like the Poisson deviance. Without this restriction, one could
+    # throw away the 'constant' term sum(y_true * log(y_true)) and just use
+    # Poisson loss = - 1/n * sum(y_true * log(y_pred))
+    #              = - 1/n * sum(y_true * log(mean(y_true))
+    #              = - mean(y_true) * log(mean(y_true))
+    # With this trick (used in proxy_impurity_improvement()), as for MSE,
+    # children_impurity would only need to go over left xor right split, not
+    # both. This could be faster.
+
+    cdef double node_impurity(self) nogil:
+        """Evaluate the impurity of the current node.
+
+        Evaluate the Poisson criterion as impurity of the current node,
+        i.e. the impurity of samples[start:end]. The smaller the impurity the
+        better.
+        """
+        return self.poisson_loss(self.start, self.end, self.sum_total,
+                                 self.weighted_n_node_samples)
+
+    cdef double proxy_impurity_improvement(self) nogil:
+        """Compute a proxy of the impurity reduction.
+
+        This method is used to speed up the search for the best split.
+        It is a proxy quantity such that the split that maximizes this value
+        also maximizes the impurity improvement. It neglects all constant terms
+        of the impurity decrease for a given split.
+
+        The absolute impurity improvement is only computed by the
+        impurity_improvement method once the best split has been found.
+
+        The Poisson proxy is derived from:
+
+              sum_{i left }(y_i * log(y_i / y_pred_L))
+            + sum_{i right}(y_i * log(y_i / y_pred_R))
+            = sum(y_i * log(y_i) - n_L * mean_{i left}(y_i) * log(mean_{i left}(y_i))
+                                 - n_R * mean_{i right}(y_i) * log(mean_{i right}(y_i))
+
+        Neglecting constant terms, this gives
+
+            - sum{i left }(y_i) * log(mean{i left}(y_i))
+            - sum{i right}(y_i) * log(mean{i right}(y_i))
+        """
+        cdef SIZE_t k
+        cdef double proxy_impurity_left = 0.0
+        cdef double proxy_impurity_right = 0.0
+        cdef double y_mean_left = 0.
+        cdef double y_mean_right = 0.
+
+        for k in range(self.n_outputs):
+            if (self.sum_left[k] <= EPSILON) or (self.sum_right[k] <= EPSILON):
+                # Poisson loss does not allow non-positive predictions. We
+                # therefore forbid splits that have child nodes with
+                # sum(y_i) <= 0.
+                # Since sum_right = sum_total - sum_left, it can lead to
+                # floating point rounding error and will not give zero. Thus,
+                # we relax the above comparison to sum(y_i) <= EPSILON.
+                return -INFINITY
+            else:
+                y_mean_left = self.sum_left[k] / self.weighted_n_left
+                y_mean_right = self.sum_right[k] / self.weighted_n_right
+                proxy_impurity_left -= self.sum_left[k] * log(y_mean_left)
+                proxy_impurity_right -= self.sum_right[k] * log(y_mean_right)
+
+        return - proxy_impurity_left - proxy_impurity_right
+
+    cdef void children_impurity(self, double* impurity_left,
+                                double* impurity_right) nogil:
+        """Evaluate the impurity in children nodes.
+
+        i.e. the impurity of the left child (samples[start:pos]) and the
+        impurity of the right child (samples[pos:end]) for Poisson.
+        """
+        cdef const DOUBLE_t[:, ::1] y = self.y
+
+        cdef SIZE_t start = self.start
+        cdef SIZE_t pos = self.pos
+        cdef SIZE_t end = self.end
+
+        cdef SIZE_t i, p, k
+        cdef DOUBLE_t y_mean = 0.
+        cdef DOUBLE_t w = 1.0
+
+        impurity_left[0] = self.poisson_loss(start, pos, self.sum_left,
+                                             self.weighted_n_left)
+
+        impurity_right[0] = self.poisson_loss(pos, end, self.sum_right,
+                                              self.weighted_n_right)
+
+    cdef inline DOUBLE_t poisson_loss(self,
+                                      SIZE_t start,
+                                      SIZE_t end,
+                                      const double[::1] y_sum,
+                                      DOUBLE_t weight_sum) nogil:
+        """Helper function to compute Poisson loss (~deviance) of a given node.
+        """
+        cdef const DOUBLE_t[:, ::1] y = self.y
+        cdef DOUBLE_t* weight = self.sample_weight
+
+        cdef DOUBLE_t y_mean = 0.
+        cdef DOUBLE_t poisson_loss = 0.
+        cdef DOUBLE_t w = 1.0
+        cdef SIZE_t n_outputs = self.n_outputs
+
+        for k in range(n_outputs):
+            if y_sum[k] <= EPSILON:
+                # y_sum could be computed from the subtraction
+                # sum_right = sum_total - sum_left leading to a potential
+                # floating point rounding error.
+                # Thus, we relax the comparison y_sum <= 0 to
+                # y_sum <= EPSILON.
+                return INFINITY
+
+            y_mean = y_sum[k] / weight_sum
+
+            for p in range(start, end):
+                i = self.samples[p]
+
+                if weight != NULL:
+                    w = weight[i]
+
+                poisson_loss += w * xlogy(y[i, k], y[i, k] / y_mean)
+        return poisson_loss / (weight_sum * n_outputs)
('sklearn/tree', '_splitter.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,7 +1,3 @@
-# cython: cdivision=True
-# cython: boundscheck=False
-# cython: wraparound=False
-
 # Authors: Gilles Louppe <g.louppe@gmail.com>
 #          Peter Prettenhofer <peter.prettenhofer@gmail.com>
 #          Brian Holt <bdholt1@gmail.com>
@@ -33,6 +29,7 @@
 from ._utils cimport rand_uniform
 from ._utils cimport RAND_R_MAX
 from ._utils cimport safe_realloc
+from ..utils._sorting cimport simultaneous_sort

 cdef double INFINITY = np.inf

@@ -60,7 +57,7 @@

     def __cinit__(self, Criterion criterion, SIZE_t max_features,
                   SIZE_t min_samples_leaf, double min_weight_leaf,
-                  object random_state, bint presort):
+                  object random_state):
         """
         Parameters
         ----------
@@ -98,7 +95,6 @@
         self.min_samples_leaf = min_samples_leaf
         self.min_weight_leaf = min_weight_leaf
         self.random_state = random_state
-        self.presort = presort

     def __dealloc__(self):
         """Destructor."""
@@ -117,8 +113,7 @@
     cdef int init(self,
                    object X,
                    const DOUBLE_t[:, ::1] y,
-                   DOUBLE_t* sample_weight,
-                   np.ndarray X_idx_sorted=None) except -1:
+                   DOUBLE_t* sample_weight) except -1:
         """Initialize the splitter.

         Take in the input data X, the target Y, and optional sample weights.
@@ -131,10 +126,10 @@
         X : object
             This contains the inputs. Usually it is a 2d numpy array.

-        y : numpy.ndarray, dtype=DOUBLE_t
+        y : ndarray, dtype=DOUBLE_t
             This is the vector of targets, or true labels, for the samples

-        sample_weight : numpy.ndarray, dtype=DOUBLE_t (optional)
+        sample_weight : DOUBLE_t*
             The weights of the samples, where higher weighted samples are fit
             closer than lower weight samples. If not provided, all samples
             are assumed to have uniform weight.
@@ -195,7 +190,7 @@
             The index of the first sample to consider
         end : SIZE_t
             The index of the last sample to consider
-        weighted_n_node_samples : numpy.ndarray, dtype=double pointer
+        weighted_n_node_samples : ndarray, dtype=double pointer
             The total weight of those samples
         """

@@ -238,31 +233,12 @@
 cdef class BaseDenseSplitter(Splitter):
     cdef const DTYPE_t[:, :] X

-    cdef np.ndarray X_idx_sorted
-    cdef INT32_t* X_idx_sorted_ptr
-    cdef SIZE_t X_idx_sorted_stride
     cdef SIZE_t n_total_samples
-    cdef SIZE_t* sample_mask
-
-    def __cinit__(self, Criterion criterion, SIZE_t max_features,
-                  SIZE_t min_samples_leaf, double min_weight_leaf,
-                  object random_state, bint presort):
-
-        self.X_idx_sorted_ptr = NULL
-        self.X_idx_sorted_stride = 0
-        self.sample_mask = NULL
-        self.presort = presort
-
-    def __dealloc__(self):
-        """Destructor."""
-        if self.presort == 1:
-            free(self.sample_mask)

     cdef int init(self,
                   object X,
                   const DOUBLE_t[:, ::1] y,
-                  DOUBLE_t* sample_weight,
-                  np.ndarray X_idx_sorted=None) except -1:
+                  DOUBLE_t* sample_weight) except -1:
         """Initialize the splitter

         Returns -1 in case of failure to allocate memory (and raise MemoryError)
@@ -273,17 +249,6 @@
         Splitter.init(self, X, y, sample_weight)

         self.X = X
-
-        if self.presort == 1:
-            self.X_idx_sorted = X_idx_sorted
-            self.X_idx_sorted_ptr = <INT32_t*> self.X_idx_sorted.data
-            self.X_idx_sorted_stride = (<SIZE_t> self.X_idx_sorted.strides[1] /
-                                        <SIZE_t> self.X_idx_sorted.itemsize)
-
-            self.n_total_samples = X.shape[0]
-            safe_realloc(&self.sample_mask, self.n_total_samples)
-            memset(self.sample_mask, 0, self.n_total_samples*sizeof(SIZE_t))
-
         return 0


@@ -294,8 +259,7 @@
                                self.max_features,
                                self.min_samples_leaf,
                                self.min_weight_leaf,
-                               self.random_state,
-                               self.presort), self.__getstate__())
+                               self.random_state), self.__getstate__())

     cdef int node_split(self, double impurity, SplitRecord* split,
                         SIZE_t* n_constant_features) nogil except -1:
@@ -319,16 +283,12 @@
         cdef double min_weight_leaf = self.min_weight_leaf
         cdef UINT32_t* random_state = &self.rand_r_state

-        cdef INT32_t* X_idx_sorted = self.X_idx_sorted_ptr
-        cdef SIZE_t* sample_mask = self.sample_mask
-
         cdef SplitRecord best, current
         cdef double current_proxy_improvement = -INFINITY
         cdef double best_proxy_improvement = -INFINITY

         cdef SIZE_t f_i = n_features
         cdef SIZE_t f_j
-        cdef SIZE_t tmp
         cdef SIZE_t p
         cdef SIZE_t feature_idx_offset
         cdef SIZE_t feature_offset
@@ -347,10 +307,6 @@
         cdef SIZE_t partition_end

         _init_split(&best, end)
-
-        if self.presort == 1:
-            for p in range(start, end):
-                sample_mask[samples[p]] = 1

         # Sample up to max_features without replacement using a
         # Fisher-Yates-based algorithm (using the local variables `f_i` and
@@ -386,9 +342,7 @@

             if f_j < n_known_constants:
                 # f_j in the interval [n_drawn_constants, n_known_constants[
-                tmp = features[f_j]
-                features[f_j] = features[n_drawn_constants]
-                features[n_drawn_constants] = tmp
+                features[n_drawn_constants], features[f_j] = features[f_j], features[n_drawn_constants]

                 n_drawn_constants += 1

@@ -398,29 +352,17 @@
                 # f_j in the interval [n_total_constants, f_i[
                 current.feature = features[f_j]

-                # Sort samples along that feature; either by utilizing
-                # presorting, or by copying the values into an array and
+                # Sort samples along that feature; by
+                # copying the values into an array and
                 # sorting the array in a manner which utilizes the cache more
                 # effectively.
-                if self.presort == 1:
-                    p = start
-                    feature_idx_offset = self.X_idx_sorted_stride * current.feature
-
-                    for i in range(self.n_total_samples):
-                        j = X_idx_sorted[i + feature_idx_offset]
-                        if sample_mask[j] == 1:
-                            samples[p] = j
-                            Xf[p] = self.X[j, current.feature]
-                            p += 1
-                else:
-                    for i in range(start, end):
-                        Xf[i] = self.X[samples[i], current.feature]
-
-                    sort(Xf + start, samples + start, end - start)
+                for i in range(start, end):
+                    Xf[i] = self.X[samples[i], current.feature]
+
+                simultaneous_sort(Xf + start, samples + start, end - start)

                 if Xf[end - 1] <= Xf[start] + FEATURE_THRESHOLD:
-                    features[f_j] = features[n_total_constants]
-                    features[n_total_constants] = current.feature
+                    features[f_j], features[n_total_constants] = features[n_total_constants], features[f_j]

                     n_found_constants += 1
                     n_total_constants += 1
@@ -485,20 +427,14 @@
                 else:
                     partition_end -= 1

-                    tmp = samples[partition_end]
-                    samples[partition_end] = samples[p]
-                    samples[p] = tmp
+                    samples[p], samples[partition_end] = samples[partition_end], samples[p]

             self.criterion.reset()
             self.criterion.update(best.pos)
-            best.improvement = self.criterion.impurity_improvement(impurity)
             self.criterion.children_impurity(&best.impurity_left,
                                              &best.impurity_right)
-
-        # Reset sample mask
-        if self.presort == 1:
-            for p in range(start, end):
-                sample_mask[samples[p]] = 0
+            best.improvement = self.criterion.impurity_improvement(
+                impurity, best.impurity_left, best.impurity_right)

         # Respect invariant for constant features: the original order of
         # element in features[:n_known_constants] must be preserved for sibling
@@ -514,120 +450,6 @@
         split[0] = best
         n_constant_features[0] = n_total_constants
         return 0
-
-
-# Sort n-element arrays pointed to by Xf and samples, simultaneously,
-# by the values in Xf. Algorithm: Introsort (Musser, SP&E, 1997).
-cdef inline void sort(DTYPE_t* Xf, SIZE_t* samples, SIZE_t n) nogil:
-    if n == 0:
-      return
-    cdef int maxd = 2 * <int>log(n)
-    introsort(Xf, samples, n, maxd)
-
-
-cdef inline void swap(DTYPE_t* Xf, SIZE_t* samples,
-        SIZE_t i, SIZE_t j) nogil:
-    # Helper for sort
-    Xf[i], Xf[j] = Xf[j], Xf[i]
-    samples[i], samples[j] = samples[j], samples[i]
-
-
-cdef inline DTYPE_t median3(DTYPE_t* Xf, SIZE_t n) nogil:
-    # Median of three pivot selection, after Bentley and McIlroy (1993).
-    # Engineering a sort function. SP&E. Requires 8/3 comparisons on average.
-    cdef DTYPE_t a = Xf[0], b = Xf[n / 2], c = Xf[n - 1]
-    if a < b:
-        if b < c:
-            return b
-        elif a < c:
-            return c
-        else:
-            return a
-    elif b < c:
-        if a < c:
-            return a
-        else:
-            return c
-    else:
-        return b
-
-
-# Introsort with median of 3 pivot selection and 3-way partition function
-# (robust to repeated elements, e.g. lots of zero features).
-cdef void introsort(DTYPE_t* Xf, SIZE_t *samples,
-                    SIZE_t n, int maxd) nogil:
-    cdef DTYPE_t pivot
-    cdef SIZE_t i, l, r
-
-    while n > 1:
-        if maxd <= 0:   # max depth limit exceeded ("gone quadratic")
-            heapsort(Xf, samples, n)
-            return
-        maxd -= 1
-
-        pivot = median3(Xf, n)
-
-        # Three-way partition.
-        i = l = 0
-        r = n
-        while i < r:
-            if Xf[i] < pivot:
-                swap(Xf, samples, i, l)
-                i += 1
-                l += 1
-            elif Xf[i] > pivot:
-                r -= 1
-                swap(Xf, samples, i, r)
-            else:
-                i += 1
-
-        introsort(Xf, samples, l, maxd)
-        Xf += r
-        samples += r
-        n -= r
-
-
-cdef inline void sift_down(DTYPE_t* Xf, SIZE_t* samples,
-                           SIZE_t start, SIZE_t end) nogil:
-    # Restore heap order in Xf[start:end] by moving the max element to start.
-    cdef SIZE_t child, maxind, root
-
-    root = start
-    while True:
-        child = root * 2 + 1
-
-        # find max of root, left child, right child
-        maxind = root
-        if child < end and Xf[maxind] < Xf[child]:
-            maxind = child
-        if child + 1 < end and Xf[maxind] < Xf[child + 1]:
-            maxind = child + 1
-
-        if maxind == root:
-            break
-        else:
-            swap(Xf, samples, root, maxind)
-            root = maxind
-
-
-cdef void heapsort(DTYPE_t* Xf, SIZE_t* samples, SIZE_t n) nogil:
-    cdef SIZE_t start, end
-
-    # heapify
-    start = (n - 2) / 2
-    end = n
-    while True:
-        sift_down(Xf, samples, start, end)
-        if start == 0:
-            break
-        start -= 1
-
-    # sort by shrinking the heap, putting the max element immediately after it
-    end = n - 1
-    while end > 0:
-        swap(Xf, samples, 0, end)
-        sift_down(Xf, samples, 0, end)
-        end = end - 1


 cdef class RandomSplitter(BaseDenseSplitter):
@@ -637,8 +459,7 @@
                                  self.max_features,
                                  self.min_samples_leaf,
                                  self.min_weight_leaf,
-                                 self.random_state,
-                                 self.presort), self.__getstate__())
+                                 self.random_state), self.__getstate__())

     cdef int node_split(self, double impurity, SplitRecord* split,
                         SIZE_t* n_constant_features) nogil except -1:
@@ -669,7 +490,7 @@
         cdef SIZE_t f_i = n_features
         cdef SIZE_t f_j
         cdef SIZE_t p
-        cdef SIZE_t tmp
+        cdef SIZE_t partition_end
         cdef SIZE_t feature_stride
         # Number of features discovered to be constant during the split search
         cdef SIZE_t n_found_constants = 0
@@ -682,7 +503,6 @@
         cdef DTYPE_t min_feature_value
         cdef DTYPE_t max_feature_value
         cdef DTYPE_t current_feature_value
-        cdef SIZE_t partition_end

         _init_split(&best, end)

@@ -719,10 +539,7 @@

             if f_j < n_known_constants:
                 # f_j in the interval [n_drawn_constants, n_known_constants[
-                tmp = features[f_j]
-                features[f_j] = features[n_drawn_constants]
-                features[n_drawn_constants] = tmp
-
+                features[n_drawn_constants], features[f_j] = features[f_j], features[n_drawn_constants]
                 n_drawn_constants += 1

             else:
@@ -747,8 +564,7 @@
                         max_feature_value = current_feature_value

                 if max_feature_value <= min_feature_value + FEATURE_THRESHOLD:
-                    features[f_j] = features[n_total_constants]
-                    features[n_total_constants] = current.feature
+                    features[f_j], features[n_total_constants] = features[n_total_constants], current.feature

                     n_found_constants += 1
                     n_total_constants += 1
@@ -766,21 +582,15 @@
                         current.threshold = min_feature_value

                     # Partition
-                    partition_end = end
-                    p = start
+                    p, partition_end = start, end
                     while p < partition_end:
-                        current_feature_value = Xf[p]
-                        if current_feature_value <= current.threshold:
+                        if Xf[p] <= current.threshold:
                             p += 1
                         else:
                             partition_end -= 1

-                            Xf[p] = Xf[partition_end]
-                            Xf[partition_end] = current_feature_value
-
-                            tmp = samples[partition_end]
-                            samples[partition_end] = samples[p]
-                            samples[p] = tmp
+                            Xf[p], Xf[partition_end] = Xf[partition_end], Xf[p]
+                            samples[p], samples[partition_end] = samples[partition_end], samples[p]

                     current.pos = partition_end

@@ -807,26 +617,22 @@
         # Reorganize into samples[start:best.pos] + samples[best.pos:end]
         if best.pos < end:
             if current.feature != best.feature:
-                partition_end = end
-                p = start
+                p, partition_end = start, end

                 while p < partition_end:
                     if self.X[samples[p], best.feature] <= best.threshold:
                         p += 1
-
                     else:
                         partition_end -= 1

-                        tmp = samples[partition_end]
-                        samples[partition_end] = samples[p]
-                        samples[p] = tmp
-
+                        samples[p], samples[partition_end] = samples[partition_end], samples[p]

             self.criterion.reset()
             self.criterion.update(best.pos)
-            best.improvement = self.criterion.impurity_improvement(impurity)
             self.criterion.children_impurity(&best.impurity_left,
                                              &best.impurity_right)
+            best.improvement = self.criterion.impurity_improvement(
+                impurity, best.impurity_left, best.impurity_right)

         # Respect invariant for constant features: the original order of
         # element in features[:n_known_constants] must be preserved for sibling
@@ -857,7 +663,7 @@

     def __cinit__(self, Criterion criterion, SIZE_t max_features,
                   SIZE_t min_samples_leaf, double min_weight_leaf,
-                  object random_state, bint presort):
+                  object random_state):
         # Parent __cinit__ is automatically called

         self.X_data = NULL
@@ -877,8 +683,7 @@
     cdef int init(self,
                   object X,
                   const DOUBLE_t[:, ::1] y,
-                  DOUBLE_t* sample_weight,
-                  np.ndarray X_idx_sorted=None) except -1:
+                  DOUBLE_t* sample_weight) except -1:
         """Initialize the splitter

         Returns -1 in case of failure to allocate memory (and raise MemoryError)
@@ -922,9 +727,8 @@
                                   SIZE_t zero_pos) nogil:
         """Partition samples[start:end] based on threshold."""

-        cdef double value
+        cdef SIZE_t p
         cdef SIZE_t partition_end
-        cdef SIZE_t p

         cdef DTYPE_t* Xf = self.feature_values
         cdef SIZE_t* samples = self.samples
@@ -941,16 +745,13 @@
             return zero_pos

         while p < partition_end:
-            value = Xf[p]
-
-            if value <= threshold:
+            if Xf[p] <= threshold:
                 p += 1

             else:
                 partition_end -= 1

-                Xf[p] = Xf[partition_end]
-                Xf[partition_end] = value
+                Xf[p], Xf[partition_end] = Xf[partition_end], Xf[p]
                 sparse_swap(index_to_samples, samples, p, partition_end)

         return partition_end
@@ -1176,8 +977,7 @@
                                      self.max_features,
                                      self.min_samples_leaf,
                                      self.min_weight_leaf,
-                                     self.random_state,
-                                     self.presort), self.__getstate__())
+                                     self.random_state), self.__getstate__())

     cdef int node_split(self, double impurity, SplitRecord* split,
                         SIZE_t* n_constant_features) nogil except -1:
@@ -1213,7 +1013,7 @@
         cdef double best_proxy_improvement = - INFINITY

         cdef SIZE_t f_i = n_features
-        cdef SIZE_t f_j, p, tmp
+        cdef SIZE_t f_j, p
         cdef SIZE_t n_visited_features = 0
         # Number of features discovered to be constant during the split search
         cdef SIZE_t n_found_constants = 0
@@ -1268,9 +1068,7 @@

             if f_j < n_known_constants:
                 # f_j in the interval [n_drawn_constants, n_known_constants[
-                tmp = features[f_j]
-                features[f_j] = features[n_drawn_constants]
-                features[n_drawn_constants] = tmp
+                features[f_j], features[n_drawn_constants] = features[n_drawn_constants], features[f_j]

                 n_drawn_constants += 1

@@ -1285,8 +1083,8 @@
                                  &is_samples_sorted)

                 # Sort the positive and negative parts of `Xf`
-                sort(Xf + start, samples + start, end_negative - start)
-                sort(Xf + start_positive, samples + start_positive,
+                simultaneous_sort(Xf + start, samples + start, end_negative - start)
+                simultaneous_sort(Xf + start_positive, samples + start_positive,
                      end - start_positive)

                 # Update index_to_samples to take into account the sort
@@ -1305,8 +1103,7 @@
                         end_negative += 1

                 if Xf[end - 1] <= Xf[start] + FEATURE_THRESHOLD:
-                    features[f_j] = features[n_total_constants]
-                    features[n_total_constants] = current.feature
+                    features[f_j], features[n_total_constants] = features[n_total_constants], features[f_j]

                     n_found_constants += 1
                     n_total_constants += 1
@@ -1381,9 +1178,10 @@

             self.criterion.reset()
             self.criterion.update(best.pos)
-            best.improvement = self.criterion.impurity_improvement(impurity)
             self.criterion.children_impurity(&best.impurity_left,
                                              &best.impurity_right)
+            best.improvement = self.criterion.impurity_improvement(
+                impurity, best.impurity_left, best.impurity_right)

         # Respect invariant for constant features: the original order of
         # element in features[:n_known_constants] must be preserved for sibling
@@ -1409,8 +1207,7 @@
                                        self.max_features,
                                        self.min_samples_leaf,
                                        self.min_weight_leaf,
-                                       self.random_state,
-                                       self.presort), self.__getstate__())
+                                       self.random_state), self.__getstate__())

     cdef int node_split(self, double impurity, SplitRecord* split,
                         SIZE_t* n_constant_features) nogil except -1:
@@ -1448,7 +1245,7 @@
         cdef DTYPE_t current_feature_value

         cdef SIZE_t f_i = n_features
-        cdef SIZE_t f_j, p, tmp
+        cdef SIZE_t f_j, p
         cdef SIZE_t n_visited_features = 0
         # Number of features discovered to be constant during the split search
         cdef SIZE_t n_found_constants = 0
@@ -1504,9 +1301,7 @@

             if f_j < n_known_constants:
                 # f_j in the interval [n_drawn_constants, n_known_constants[
-                tmp = features[f_j]
-                features[f_j] = features[n_drawn_constants]
-                features[n_drawn_constants] = tmp
+                features[f_j], features[n_drawn_constants] = features[n_drawn_constants], features[f_j]

                 n_drawn_constants += 1

@@ -1595,10 +1390,10 @@

                     if current_proxy_improvement > best_proxy_improvement:
                         best_proxy_improvement = current_proxy_improvement
-                        current.improvement = self.criterion.impurity_improvement(impurity)
-
                         self.criterion.children_impurity(&current.impurity_left,
                                                          &current.impurity_right)
+                        current.improvement = self.criterion.impurity_improvement(
+                            impurity, current.impurity_left, current.impurity_right)
                         best = current

         # Reorganize into samples[start:best.pos] + samples[best.pos:end]
@@ -1612,9 +1407,10 @@

             self.criterion.reset()
             self.criterion.update(best.pos)
-            best.improvement = self.criterion.impurity_improvement(impurity)
             self.criterion.children_impurity(&best.impurity_left,
                                              &best.impurity_right)
+            best.improvement = self.criterion.impurity_improvement(
+                impurity, best.impurity_left, best.impurity_right)

         # Respect invariant for constant features: the original order of
         # element in features[:n_known_constants] must be preserved for sibling
('sklearn/tree', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,12 +3,20 @@
 classification and regression.
 """

-from .tree import DecisionTreeClassifier
-from .tree import DecisionTreeRegressor
-from .tree import ExtraTreeClassifier
-from .tree import ExtraTreeRegressor
-from .export import export_graphviz, plot_tree, export_text
+from ._classes import BaseDecisionTree
+from ._classes import DecisionTreeClassifier
+from ._classes import DecisionTreeRegressor
+from ._classes import ExtraTreeClassifier
+from ._classes import ExtraTreeRegressor
+from ._export import export_graphviz, plot_tree, export_text

-__all__ = ["DecisionTreeClassifier", "DecisionTreeRegressor",
-           "ExtraTreeClassifier", "ExtraTreeRegressor", "export_graphviz",
-           "plot_tree", "export_text"]
+__all__ = [
+    "BaseDecisionTree",
+    "DecisionTreeClassifier",
+    "DecisionTreeRegressor",
+    "ExtraTreeClassifier",
+    "ExtraTreeRegressor",
+    "export_graphviz",
+    "plot_tree",
+    "export_text",
+]
('sklearn/tree', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,37 +7,44 @@
 def configuration(parent_package="", top_path=None):
     config = Configuration("tree", parent_package, top_path)
     libraries = []
-    if os.name == 'posix':
-        libraries.append('m')
-    config.add_extension("_tree",
-                         sources=["_tree.pyx"],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries,
-                         extra_compile_args=["-O3"])
-    config.add_extension("_splitter",
-                         sources=["_splitter.pyx"],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries,
-                         extra_compile_args=["-O3"])
-    config.add_extension("_criterion",
-                         sources=["_criterion.pyx"],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries,
-                         extra_compile_args=["-O3"])
-    config.add_extension("_utils",
-                         sources=["_utils.pyx"],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries,
-                         extra_compile_args=["-O3"])
+    if os.name == "posix":
+        libraries.append("m")
+    config.add_extension(
+        "_tree",
+        sources=["_tree.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+        language="c++",
+        extra_compile_args=["-O3"],
+    )
+    config.add_extension(
+        "_splitter",
+        sources=["_splitter.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+        extra_compile_args=["-O3"],
+    )
+    config.add_extension(
+        "_criterion",
+        sources=["_criterion.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+        extra_compile_args=["-O3"],
+    )
+    config.add_extension(
+        "_utils",
+        sources=["_utils.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+        extra_compile_args=["-O3"],
+    )

     config.add_subpackage("tests")
-    config.add_data_files("_criterion.pxd")
-    config.add_data_files("_splitter.pxd")
-    config.add_data_files("_tree.pxd")
-    config.add_data_files("_utils.pxd")

     return config

+
 if __name__ == "__main__":
     from numpy.distutils.core import setup
+
     setup(**configuration().todict())
('sklearn/tree', '_splitter.pxd')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -57,9 +57,6 @@
     cdef SIZE_t start                    # Start position for the current node
     cdef SIZE_t end                      # End position for the current node

-    cdef bint presort                    # Whether to use presorting, only
-                                         # allowed on dense data
-
     cdef const DOUBLE_t[:, ::1] y
     cdef DOUBLE_t* sample_weight

@@ -81,8 +78,7 @@

     # Methods
     cdef int init(self, object X, const DOUBLE_t[:, ::1] y,
-                  DOUBLE_t* sample_weight,
-                  np.ndarray X_idx_sorted=*) except -1
+                  DOUBLE_t* sample_weight) except -1

     cdef int node_reset(self, SIZE_t start, SIZE_t end,
                         double* weighted_n_node_samples) nogil except -1
('sklearn/tree', '_criterion.pxd')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -40,14 +40,6 @@
     cdef double weighted_n_left          # Weighted number of samples in the left node
     cdef double weighted_n_right         # Weighted number of samples in the right node

-    cdef double* sum_total          # For classification criteria, the sum of the
-                                    # weighted count of each label. For regression,
-                                    # the sum of w*y. sum_total[k] is equal to
-                                    # sum_{i=start}^{end-1} w[samples[i]]*y[samples[i], k],
-                                    # where k is output index.
-    cdef double* sum_left           # Same as above, but for the left side of the split
-    cdef double* sum_right          # same as above, but for the right side of the split
-
     # The criterion object is maintained such that left and right collected
     # statistics correspond to samples[start:pos] and samples[pos:end].

@@ -62,16 +54,26 @@
     cdef void children_impurity(self, double* impurity_left,
                                 double* impurity_right) nogil
     cdef void node_value(self, double* dest) nogil
-    cdef double impurity_improvement(self, double impurity) nogil
+    cdef double impurity_improvement(self, double impurity_parent,
+                                     double impurity_left,
+                                     double impurity_right) nogil
     cdef double proxy_impurity_improvement(self) nogil

 cdef class ClassificationCriterion(Criterion):
     """Abstract criterion for classification."""

-    cdef SIZE_t* n_classes
-    cdef SIZE_t sum_stride
+    cdef SIZE_t[::1] n_classes
+    cdef SIZE_t max_n_classes
+
+    cdef double[:, ::1] sum_total   # The sum of the weighted count of each label.
+    cdef double[:, ::1] sum_left    # Same as above, but for the left side of the split
+    cdef double[:, ::1] sum_right   # Same as above, but for the right side of the split

 cdef class RegressionCriterion(Criterion):
     """Abstract regression criterion."""

     cdef double sq_sum_total
+
+    cdef double[::1] sum_total   # The sum of w*y.
+    cdef double[::1] sum_left    # Same as above, but for the left side of the split
+    cdef double[::1] sum_right   # Same as above, but for the right side of the split
('sklearn/tree', '_reingold_tilford.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,32 +1,17 @@
-# taken from https://github.com/llimllib/pymag-trees/blob/master/buchheim.py
-# with slight modifications
-
-#            DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE
-#                    Version 2, December 2004
-#
-# Copyright (C) 2004 Sam Hocevar <sam@hocevar.net>
-#
-# Everyone is permitted to copy and distribute verbatim or modified
-# copies of this license document, and changing it is allowed as long
-# as the name is changed.
-#
-#            DO WHAT THE FUCK YOU WANT TO PUBLIC LICENSE
-#   TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION
-
-#  0. You just DO WHAT THE FUCK YOU WANT TO.
-
+# Authors: William Mill (bill@billmill.org)
+# License: BSD 3 clause

 import numpy as np


 class DrawTree:
     def __init__(self, tree, parent=None, depth=0, number=1):
-        self.x = -1.
+        self.x = -1.0
         self.y = depth
         self.tree = tree
-        self.children = [DrawTree(c, self, depth + 1, i + 1)
-                         for i, c
-                         in enumerate(tree.children)]
+        self.children = [
+            DrawTree(c, self, depth + 1, i + 1) for i, c in enumerate(tree.children)
+        ]
         self.parent = parent
         self.thread = None
         self.mod = 0
@@ -53,10 +38,10 @@
         return n

     def get_lmost_sibling(self):
-        if not self._lmost_sibling and self.parent and self != \
-                self.parent.children[0]:
+        if not self._lmost_sibling and self.parent and self != self.parent.children[0]:
             self._lmost_sibling = self.parent.children[0]
         return self._lmost_sibling
+
     lmost_sibling = property(get_lmost_sibling)

     def __str__(self):
@@ -66,7 +51,7 @@
         return self.__str__()

     def max_extents(self):
-        extents = [c.max_extents() for c in self. children]
+        extents = [c.max_extents() for c in self.children]
         extents.append((self.x, self.y))
         return np.max(extents, axis=0)

@@ -85,12 +70,12 @@
         third_walk(c, n)


-def first_walk(v, distance=1.):
+def first_walk(v, distance=1.0):
     if len(v.children) == 0:
         if v.lmost_sibling:
             v.x = v.lbrother().x + distance
         else:
-            v.x = 0.
+            v.x = 0.0
     else:
         default_ancestor = v.children[0]
         for w in v.children:
('sklearn/tree', '_utils.pxd')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -11,7 +11,7 @@
 import numpy as np
 cimport numpy as np
 from ._tree cimport Node
-from ..neighbors.quad_tree cimport Cell
+from ..neighbors._quad_tree cimport Cell

 ctypedef np.npy_float32 DTYPE_t          # Type of X
 ctypedef np.npy_float64 DOUBLE_t         # Type of y, sample_weight
@@ -43,8 +43,6 @@
     (Node*)
     (Cell*)
     (Node**)
-    (StackRecord*)
-    (PriorityHeapRecord*)

 cdef realloc_ptr safe_realloc(realloc_ptr* p, size_t nelems) nogil except *

@@ -61,63 +59,6 @@


 cdef double log(double x) nogil
-
-# =============================================================================
-# Stack data structure
-# =============================================================================
-
-# A record on the stack for depth-first tree growing
-cdef struct StackRecord:
-    SIZE_t start
-    SIZE_t end
-    SIZE_t depth
-    SIZE_t parent
-    bint is_left
-    double impurity
-    SIZE_t n_constant_features
-
-cdef class Stack:
-    cdef SIZE_t capacity
-    cdef SIZE_t top
-    cdef StackRecord* stack_
-
-    cdef bint is_empty(self) nogil
-    cdef int push(self, SIZE_t start, SIZE_t end, SIZE_t depth, SIZE_t parent,
-                  bint is_left, double impurity,
-                  SIZE_t n_constant_features) nogil except -1
-    cdef int pop(self, StackRecord* res) nogil
-
-
-# =============================================================================
-# PriorityHeap data structure
-# =============================================================================
-
-# A record on the frontier for best-first tree growing
-cdef struct PriorityHeapRecord:
-    SIZE_t node_id
-    SIZE_t start
-    SIZE_t end
-    SIZE_t pos
-    SIZE_t depth
-    bint is_leaf
-    double impurity
-    double impurity_left
-    double impurity_right
-    double improvement
-
-cdef class PriorityHeap:
-    cdef SIZE_t capacity
-    cdef SIZE_t heap_ptr
-    cdef PriorityHeapRecord* heap_
-
-    cdef bint is_empty(self) nogil
-    cdef void heapify_up(self, PriorityHeapRecord* heap, SIZE_t pos) nogil
-    cdef void heapify_down(self, PriorityHeapRecord* heap, SIZE_t pos, SIZE_t heap_length) nogil
-    cdef int push(self, SIZE_t node_id, SIZE_t start, SIZE_t end, SIZE_t pos,
-                  SIZE_t depth, bint is_leaf, double improvement,
-                  double impurity, double impurity_left,
-                  double impurity_right) nogil except -1
-    cdef int pop(self, PriorityHeapRecord* res) nogil

 # =============================================================================
 # WeightedPQueue data structure
('sklearn/tree', '_tree.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,7 +1,3 @@
-# cython: cdivision=True
-# cython: boundscheck=False
-# cython: wraparound=False
-
 # Authors: Gilles Louppe <g.louppe@gmail.com>
 #          Peter Prettenhofer <peter.prettenhofer@gmail.com>
 #          Brian Holt <bdholt1@gmail.com>
@@ -16,33 +12,45 @@
 #
 # License: BSD 3 clause

-from cpython cimport Py_INCREF, PyObject
+from cpython cimport Py_INCREF, PyObject, PyTypeObject

 from libc.stdlib cimport free
 from libc.math cimport fabs
 from libc.string cimport memcpy
 from libc.string cimport memset
+from libc.stdint cimport SIZE_MAX
+from libcpp.vector cimport vector
+from libcpp.algorithm cimport pop_heap
+from libcpp.algorithm cimport push_heap
+from libcpp cimport bool
+
+import struct

 import numpy as np
 cimport numpy as np
 np.import_array()

 from scipy.sparse import issparse
-from scipy.sparse import csc_matrix
 from scipy.sparse import csr_matrix

-from ._utils cimport Stack
-from ._utils cimport StackRecord
-from ._utils cimport PriorityHeap
-from ._utils cimport PriorityHeapRecord
 from ._utils cimport safe_realloc
 from ._utils cimport sizet_ptr_to_ndarray

 cdef extern from "numpy/arrayobject.h":
-    object PyArray_NewFromDescr(object subtype, np.dtype descr,
+    object PyArray_NewFromDescr(PyTypeObject* subtype, np.dtype descr,
                                 int nd, np.npy_intp* dims,
                                 np.npy_intp* strides,
                                 void* data, int flags, object obj)
+    int PyArray_SetBaseObject(np.ndarray arr, PyObject* obj)
+
+cdef extern from "<stack>" namespace "std" nogil:
+    cdef cppclass stack[T]:
+        ctypedef T value_type
+        stack() except +
+        bint empty()
+        void pop()
+        void push(T&) except +  # Raise c++ exception for bad_alloc -> MemoryError
+        T& top()

 # =============================================================================
 # Types and constants
@@ -64,24 +72,13 @@
 TREE_UNDEFINED = -2
 cdef SIZE_t _TREE_LEAF = TREE_LEAF
 cdef SIZE_t _TREE_UNDEFINED = TREE_UNDEFINED
-cdef SIZE_t INITIAL_STACK_SIZE = 10
-
-# Repeat struct definition for numpy
-NODE_DTYPE = np.dtype({
-    'names': ['left_child', 'right_child', 'feature', 'threshold', 'impurity',
-              'n_node_samples', 'weighted_n_node_samples'],
-    'formats': [np.intp, np.intp, np.intp, np.float64, np.float64, np.intp,
-                np.float64],
-    'offsets': [
-        <Py_ssize_t> &(<Node*> NULL).left_child,
-        <Py_ssize_t> &(<Node*> NULL).right_child,
-        <Py_ssize_t> &(<Node*> NULL).feature,
-        <Py_ssize_t> &(<Node*> NULL).threshold,
-        <Py_ssize_t> &(<Node*> NULL).impurity,
-        <Py_ssize_t> &(<Node*> NULL).n_node_samples,
-        <Py_ssize_t> &(<Node*> NULL).weighted_n_node_samples
-    ]
-})
+
+# Build the corresponding numpy dtype for Node.
+# This works by casting `dummy` to an array of Node of length 1, which numpy
+# can construct a `dtype`-object for. See https://stackoverflow.com/q/62448946
+# for a more detailed explanation.
+cdef Node dummy;
+NODE_DTYPE = np.asarray(<Node[:1]>(&dummy)).dtype

 # =============================================================================
 # TreeBuilder
@@ -91,8 +88,7 @@
     """Interface for different tree building strategies."""

     cpdef build(self, Tree tree, object X, np.ndarray y,
-                np.ndarray sample_weight=None,
-                np.ndarray X_idx_sorted=None):
+                np.ndarray sample_weight=None):
         """Build a decision tree from the training set (X, y)."""
         pass

@@ -126,25 +122,31 @@
         return X, y, sample_weight

 # Depth first builder ---------------------------------------------------------
+# A record on the stack for depth-first tree growing
+cdef struct StackRecord:
+    SIZE_t start
+    SIZE_t end
+    SIZE_t depth
+    SIZE_t parent
+    bint is_left
+    double impurity
+    SIZE_t n_constant_features

 cdef class DepthFirstTreeBuilder(TreeBuilder):
     """Build a decision tree in depth-first fashion."""

     def __cinit__(self, Splitter splitter, SIZE_t min_samples_split,
                   SIZE_t min_samples_leaf, double min_weight_leaf,
-                  SIZE_t max_depth, double min_impurity_decrease,
-                  double min_impurity_split):
+                  SIZE_t max_depth, double min_impurity_decrease):
         self.splitter = splitter
         self.min_samples_split = min_samples_split
         self.min_samples_leaf = min_samples_leaf
         self.min_weight_leaf = min_weight_leaf
         self.max_depth = max_depth
         self.min_impurity_decrease = min_impurity_decrease
-        self.min_impurity_split = min_impurity_split

     cpdef build(self, Tree tree, object X, np.ndarray y,
-                np.ndarray sample_weight=None,
-                np.ndarray X_idx_sorted=None):
+                np.ndarray sample_weight=None):
         """Build a decision tree from the training set (X, y)."""

         # check input
@@ -171,10 +173,9 @@
         cdef double min_weight_leaf = self.min_weight_leaf
         cdef SIZE_t min_samples_split = self.min_samples_split
         cdef double min_impurity_decrease = self.min_impurity_decrease
-        cdef double min_impurity_split = self.min_impurity_split

         # Recursive partition (without actual recursion)
-        splitter.init(X, y, sample_weight_ptr, X_idx_sorted)
+        splitter.init(X, y, sample_weight_ptr)

         cdef SIZE_t start
         cdef SIZE_t end
@@ -194,19 +195,23 @@
         cdef SIZE_t max_depth_seen = -1
         cdef int rc = 0

-        cdef Stack stack = Stack(INITIAL_STACK_SIZE)
+        cdef stack[StackRecord] builder_stack
         cdef StackRecord stack_record

         with nogil:
             # push root node onto stack
-            rc = stack.push(0, n_node_samples, 0, _TREE_UNDEFINED, 0, INFINITY, 0)
-            if rc == -1:
-                # got return code -1 - out-of-memory
-                with gil:
-                    raise MemoryError()
-
-            while not stack.is_empty():
-                stack.pop(&stack_record)
+            builder_stack.push({
+                "start": 0,
+                "end": n_node_samples,
+                "depth": 0,
+                "parent": _TREE_UNDEFINED,
+                "is_left": 0,
+                "impurity": INFINITY,
+                "n_constant_features": 0})
+
+            while not builder_stack.empty():
+                stack_record = builder_stack.top()
+                builder_stack.pop()

                 start = stack_record.start
                 end = stack_record.end
@@ -228,8 +233,8 @@
                     impurity = splitter.node_impurity()
                     first = 0

-                is_leaf = (is_leaf or
-                           (impurity <= min_impurity_split))
+                # impurity == 0 with tolerance due to rounding errors
+                is_leaf = is_leaf or impurity <= EPSILON

                 if not is_leaf:
                     splitter.node_split(impurity, &split, &n_constant_features)
@@ -244,7 +249,7 @@
                                          split.threshold, impurity, n_node_samples,
                                          weighted_n_node_samples)

-                if node_id == <SIZE_t>(-1):
+                if node_id == SIZE_MAX:
                     rc = -1
                     break

@@ -254,16 +259,24 @@

                 if not is_leaf:
                     # Push right child on stack
-                    rc = stack.push(split.pos, end, depth + 1, node_id, 0,
-                                    split.impurity_right, n_constant_features)
-                    if rc == -1:
-                        break
+                    builder_stack.push({
+                        "start": split.pos,
+                        "end": end,
+                        "depth": depth + 1,
+                        "parent": node_id,
+                        "is_left": 0,
+                        "impurity": split.impurity_right,
+                        "n_constant_features": n_constant_features})

                     # Push left child on stack
-                    rc = stack.push(start, split.pos, depth + 1, node_id, 1,
-                                    split.impurity_left, n_constant_features)
-                    if rc == -1:
-                        break
+                    builder_stack.push({
+                        "start": start,
+                        "end": split.pos,
+                        "depth": depth + 1,
+                        "parent": node_id,
+                        "is_left": 1,
+                        "impurity": split.impurity_left,
+                        "n_constant_features": n_constant_features})

                 if depth > max_depth_seen:
                     max_depth_seen = depth
@@ -278,17 +291,34 @@


 # Best first builder ----------------------------------------------------------
-
-cdef inline int _add_to_frontier(PriorityHeapRecord* rec,
-                                 PriorityHeap frontier) nogil except -1:
-    """Adds record ``rec`` to the priority queue ``frontier``
-
-    Returns -1 in case of failure to allocate memory (and raise MemoryError)
-    or 0 otherwise.
-    """
-    return frontier.push(rec.node_id, rec.start, rec.end, rec.pos, rec.depth,
-                         rec.is_leaf, rec.improvement, rec.impurity,
-                         rec.impurity_left, rec.impurity_right)
+cdef struct FrontierRecord:
+    # Record of information of a Node, the frontier for a split. Those records are
+    # maintained in a heap to access the Node with the best improvement in impurity,
+    # allowing growing trees greedily on this improvement.
+    SIZE_t node_id
+    SIZE_t start
+    SIZE_t end
+    SIZE_t pos
+    SIZE_t depth
+    bint is_leaf
+    double impurity
+    double impurity_left
+    double impurity_right
+    double improvement
+
+cdef inline bool _compare_records(
+    const FrontierRecord& left,
+    const FrontierRecord& right,
+):
+    return left.improvement < right.improvement
+
+cdef inline void _add_to_frontier(
+    FrontierRecord rec,
+    vector[FrontierRecord]& frontier,
+) nogil:
+    """Adds record `rec` to the priority queue `frontier`."""
+    frontier.push_back(rec)
+    push_heap(frontier.begin(), frontier.end(), &_compare_records)


 cdef class BestFirstTreeBuilder(TreeBuilder):
@@ -302,7 +332,7 @@
     def __cinit__(self, Splitter splitter, SIZE_t min_samples_split,
                   SIZE_t min_samples_leaf,  min_weight_leaf,
                   SIZE_t max_depth, SIZE_t max_leaf_nodes,
-                  double min_impurity_decrease, double min_impurity_split):
+                  double min_impurity_decrease):
         self.splitter = splitter
         self.min_samples_split = min_samples_split
         self.min_samples_leaf = min_samples_leaf
@@ -310,11 +340,9 @@
         self.max_depth = max_depth
         self.max_leaf_nodes = max_leaf_nodes
         self.min_impurity_decrease = min_impurity_decrease
-        self.min_impurity_split = min_impurity_split

     cpdef build(self, Tree tree, object X, np.ndarray y,
-                np.ndarray sample_weight=None,
-                np.ndarray X_idx_sorted=None):
+                np.ndarray sample_weight=None):
         """Build a decision tree from the training set (X, y)."""

         # check input
@@ -332,12 +360,12 @@
         cdef SIZE_t min_samples_split = self.min_samples_split

         # Recursive partition (without actual recursion)
-        splitter.init(X, y, sample_weight_ptr, X_idx_sorted)
-
-        cdef PriorityHeap frontier = PriorityHeap(INITIAL_STACK_SIZE)
-        cdef PriorityHeapRecord record
-        cdef PriorityHeapRecord split_node_left
-        cdef PriorityHeapRecord split_node_right
+        splitter.init(X, y, sample_weight_ptr)
+
+        cdef vector[FrontierRecord] frontier
+        cdef FrontierRecord record
+        cdef FrontierRecord split_node_left
+        cdef FrontierRecord split_node_right

         cdef SIZE_t n_node_samples = splitter.n_samples
         cdef SIZE_t max_split_nodes = max_leaf_nodes - 1
@@ -356,14 +384,12 @@
                                       INFINITY, IS_FIRST, IS_LEFT, NULL, 0,
                                       &split_node_left)
             if rc >= 0:
-                rc = _add_to_frontier(&split_node_left, frontier)
-
-            if rc == -1:
-                with gil:
-                    raise MemoryError()
-
-            while not frontier.is_empty():
-                frontier.pop(&record)
+                _add_to_frontier(split_node_left, frontier)
+
+            while not frontier.empty():
+                pop_heap(frontier.begin(), frontier.end(), &_compare_records)
+                record = frontier.back()
+                frontier.pop_back()

                 node = &tree.nodes[record.node_id]
                 is_leaf = (record.is_leaf or max_split_nodes <= 0)
@@ -405,13 +431,8 @@
                         break

                     # Add nodes to queue
-                    rc = _add_to_frontier(&split_node_left, frontier)
-                    if rc == -1:
-                        break
-
-                    rc = _add_to_frontier(&split_node_right, frontier)
-                    if rc == -1:
-                        break
+                    _add_to_frontier(split_node_left, frontier)
+                    _add_to_frontier(split_node_right, frontier)

                 if record.depth > max_depth_seen:
                     max_depth_seen = record.depth
@@ -429,7 +450,7 @@
                                     SIZE_t start, SIZE_t end, double impurity,
                                     bint is_first, bint is_left, Node* parent,
                                     SIZE_t depth,
-                                    PriorityHeapRecord* res) nogil except -1:
+                                    FrontierRecord* res) nogil except -1:
         """Adds node w/ partition ``[start, end)`` to the frontier. """
         cdef SplitRecord split
         cdef SIZE_t node_id
@@ -437,7 +458,6 @@
         cdef SIZE_t n_constant_features = 0
         cdef double weighted_n_samples = splitter.weighted_n_samples
         cdef double min_impurity_decrease = self.min_impurity_decrease
-        cdef double min_impurity_split = self.min_impurity_split
         cdef double weighted_n_node_samples
         cdef bint is_leaf
         cdef SIZE_t n_left, n_right
@@ -453,7 +473,8 @@
                    n_node_samples < self.min_samples_split or
                    n_node_samples < 2 * self.min_samples_leaf or
                    weighted_n_node_samples < 2 * self.min_weight_leaf or
-                   impurity <= min_impurity_split)
+                   impurity <= EPSILON  # impurity == 0 with tolerance
+                   )

         if not is_leaf:
             splitter.node_split(impurity, &split, &n_constant_features)
@@ -468,7 +489,7 @@
                                  is_left, is_leaf,
                                  split.feature, split.threshold, impurity, n_node_samples,
                                  weighted_n_node_samples)
-        if node_id == <SIZE_t>(-1):
+        if node_id == SIZE_MAX:
             return -1

         # compute values also for split nodes (might become leafs later).
@@ -603,9 +624,13 @@
         def __get__(self):
             return self._get_value_ndarray()[:self.node_count]

-    def __cinit__(self, int n_features, np.ndarray[SIZE_t, ndim=1] n_classes,
-                  int n_outputs):
+    def __cinit__(self, int n_features, np.ndarray n_classes, int n_outputs):
         """Constructor."""
+        cdef SIZE_t dummy = 0
+        size_t_dtype = np.array(dummy).dtype
+
+        n_classes = _check_n_classes(n_classes, size_t_dtype)
+
         # Input/Output layout
         self.n_features = n_features
         self.n_outputs = n_outputs
@@ -663,13 +688,13 @@

         value_shape = (node_ndarray.shape[0], self.n_outputs,
                        self.max_n_classes)
-        if (node_ndarray.ndim != 1 or
-                node_ndarray.dtype != NODE_DTYPE or
-                not node_ndarray.flags.c_contiguous or
-                value_ndarray.shape != value_shape or
-                not value_ndarray.flags.c_contiguous or
-                value_ndarray.dtype != np.float64):
-            raise ValueError('Did not recognise loaded array layout')
+
+        node_ndarray = _check_node_ndarray(node_ndarray, expected_dtype=NODE_DTYPE)
+        value_ndarray = _check_value_ndarray(
+            value_ndarray,
+            expected_dtype=np.dtype(np.float64),
+            expected_shape=value_shape
+        )

         self.capacity = node_ndarray.shape[0]
         if self._resize_c(self.capacity) != 0:
@@ -691,9 +716,7 @@
             with gil:
                 raise MemoryError()

-    # XXX using (size_t)(-1) is ugly, but SIZE_MAX is not available in C89
-    # (i.e., older MSVC).
-    cdef int _resize_c(self, SIZE_t capacity=<SIZE_t>(-1)) nogil except -1:
+    cdef int _resize_c(self, SIZE_t capacity=SIZE_MAX) nogil except -1:
         """Guts of _resize

         Returns -1 in case of failure to allocate memory (and raise MemoryError)
@@ -702,7 +725,7 @@
         if capacity == self.capacity and self.nodes != NULL:
             return 0

-        if capacity == <SIZE_t>(-1):
+        if capacity == SIZE_MAX:
             if self.capacity == 0:
                 capacity = 3  # default initial value
             else:
@@ -738,7 +761,7 @@

         if node_id >= self.capacity:
             if self._resize_c() != 0:
-                return <SIZE_t>(-1)
+                return SIZE_MAX

         cdef Node* node = &self.nodes[node_id]
         node.impurity = impurity
@@ -793,7 +816,7 @@
             raise ValueError("X.dtype should be np.float32, got %s" % X.dtype)

         # Extract input
-        cdef DTYPE_t[:, :] X_ndarray = X
+        cdef const DTYPE_t[:, :] X_ndarray = X
         cdef SIZE_t n_samples = X.shape[0]

         # Initialize output
@@ -913,7 +936,7 @@
             raise ValueError("X.dtype should be np.float32, got %s" % X.dtype)

         # Extract input
-        cdef DTYPE_t[:, :] X_ndarray = X
+        cdef const DTYPE_t[:, :] X_ndarray = X
         cdef SIZE_t n_samples = X.shape[0]

         # Initialize output
@@ -1101,7 +1124,8 @@
         cdef np.ndarray arr
         arr = np.PyArray_SimpleNewFromData(3, shape, np.NPY_DOUBLE, self.value)
         Py_INCREF(self)
-        arr.base = <PyObject*> self
+        if PyArray_SetBaseObject(arr, <PyObject*> self) < 0:
+            raise ValueError("Can't initialize array.")
         return arr

     cdef np.ndarray _get_node_ndarray(self):
@@ -1117,16 +1141,17 @@
         strides[0] = sizeof(Node)
         cdef np.ndarray arr
         Py_INCREF(NODE_DTYPE)
-        arr = PyArray_NewFromDescr(np.ndarray, <np.dtype> NODE_DTYPE, 1, shape,
+        arr = PyArray_NewFromDescr(<PyTypeObject *> np.ndarray,
+                                   <np.dtype> NODE_DTYPE, 1, shape,
                                    strides, <void*> self.nodes,
                                    np.NPY_DEFAULT, None)
         Py_INCREF(self)
-        arr.base = <PyObject*> self
+        if PyArray_SetBaseObject(arr, <PyObject*> self) < 0:
+            raise ValueError("Can't initialize array.")
         return arr

-
     def compute_partial_dependence(self, DTYPE_t[:, ::1] X,
-                                   int[::1] target_feature,
+                                   int[::1] target_features,
                                    double[::1] out):
         """Partial dependence of the response on the ``target_feature`` set.

@@ -1148,7 +1173,7 @@
         X : view on 2d ndarray, shape (n_samples, n_target_features)
             The grid points on which the partial dependence should be
             evaluated.
-        target_feature : view on 1d ndarray, shape (n_target_features)
+        target_features : view on 1d ndarray, shape (n_target_features)
             The set of target features for which the partial dependence
             should be evaluated.
         out : view on 1d ndarray, shape (n_samples)
@@ -1194,8 +1219,8 @@

                     # determine if the split feature is a target feature
                     is_target_feature = False
-                    for feature_idx in range(target_feature.shape[0]):
-                        if target_feature[feature_idx] == current_node.feature:
+                    for feature_idx in range(target_features.shape[0]):
+                        if target_features[feature_idx] == current_node.feature:
                             is_target_feature = True
                             break

@@ -1230,3 +1255,535 @@
             if not (0.999 < total_weight < 1.001):
                 raise ValueError("Total weight should be 1.0 but was %.9f" %
                                  total_weight)
+
+
+def _check_n_classes(n_classes, expected_dtype):
+    if n_classes.ndim != 1:
+        raise ValueError(
+            f"Wrong dimensions for n_classes from the pickle: "
+            f"expected 1, got {n_classes.ndim}"
+        )
+
+    if n_classes.dtype == expected_dtype:
+        return n_classes
+
+    # Handles both different endianness and different bitness
+    if n_classes.dtype.kind == "i" and n_classes.dtype.itemsize in [4, 8]:
+        return n_classes.astype(expected_dtype, casting="same_kind")
+
+    raise ValueError(
+        "n_classes from the pickle has an incompatible dtype:\n"
+        f"- expected: {expected_dtype}\n"
+        f"- got:      {n_classes.dtype}"
+    )
+
+
+def _check_value_ndarray(value_ndarray, expected_dtype, expected_shape):
+    if value_ndarray.shape != expected_shape:
+        raise ValueError(
+            "Wrong shape for value array from the pickle: "
+            f"expected {expected_shape}, got {value_ndarray.shape}"
+        )
+
+    if not value_ndarray.flags.c_contiguous:
+        raise ValueError(
+            "value array from the pickle should be a C-contiguous array"
+        )
+
+    if value_ndarray.dtype == expected_dtype:
+        return value_ndarray
+
+    # Handles different endianness
+    if value_ndarray.dtype.str.endswith('f8'):
+        return value_ndarray.astype(expected_dtype, casting='equiv')
+
+    raise ValueError(
+        "value array from the pickle has an incompatible dtype:\n"
+        f"- expected: {expected_dtype}\n"
+        f"- got:      {value_ndarray.dtype}"
+    )
+
+
+def _dtype_to_dict(dtype):
+    return {name: dt.str for name, (dt, *rest) in dtype.fields.items()}
+
+
+def _dtype_dict_with_modified_bitness(dtype_dict):
+    # field names in Node struct with SIZE_t types (see sklearn/tree/_tree.pxd)
+    indexing_field_names = ["left_child", "right_child", "feature", "n_node_samples"]
+
+    expected_dtype_size = str(struct.calcsize("P"))
+    allowed_dtype_size = "8" if expected_dtype_size == "4" else "4"
+
+    allowed_dtype_dict = dtype_dict.copy()
+    for name in indexing_field_names:
+        allowed_dtype_dict[name] = allowed_dtype_dict[name].replace(
+            expected_dtype_size, allowed_dtype_size
+        )
+
+    return allowed_dtype_dict
+
+
+def _all_compatible_dtype_dicts(dtype):
+    # The Cython code for decision trees uses platform-specific SIZE_t
+    # typed indexing fields that correspond to either i4 or i8 dtypes for
+    # the matching fields in the numpy array depending on the bitness of
+    # the platform (32 bit or 64 bit respectively).
+    #
+    # We need to cast the indexing fields of the NODE_DTYPE-dtyped array at
+    # pickle load time to enable cross-bitness deployment scenarios. We
+    # typically want to make it possible to run the expensive fit method of
+    # a tree estimator on a 64 bit server platform, pickle the estimator
+    # for deployment and run the predict method of a low power 32 bit edge
+    # platform.
+    #
+    # A similar thing happens for endianness, the machine where the pickle was
+    # saved can have a different endianness than the machine where the pickle
+    # is loaded
+
+    dtype_dict = _dtype_to_dict(dtype)
+    dtype_dict_with_modified_bitness = _dtype_dict_with_modified_bitness(dtype_dict)
+    dtype_dict_with_modified_endianness = _dtype_to_dict(dtype.newbyteorder())
+    dtype_dict_with_modified_bitness_and_endianness = _dtype_dict_with_modified_bitness(
+        dtype_dict_with_modified_endianness
+    )
+
+    return [
+        dtype_dict,
+        dtype_dict_with_modified_bitness,
+        dtype_dict_with_modified_endianness,
+        dtype_dict_with_modified_bitness_and_endianness,
+    ]
+
+
+def _check_node_ndarray(node_ndarray, expected_dtype):
+    if node_ndarray.ndim != 1:
+        raise ValueError(
+            "Wrong dimensions for node array from the pickle: "
+            f"expected 1, got {node_ndarray.ndim}"
+        )
+
+    if not node_ndarray.flags.c_contiguous:
+        raise ValueError(
+            "node array from the pickle should be a C-contiguous array"
+        )
+
+    node_ndarray_dtype = node_ndarray.dtype
+    if node_ndarray_dtype == expected_dtype:
+        return node_ndarray
+
+    node_ndarray_dtype_dict = _dtype_to_dict(node_ndarray_dtype)
+    all_compatible_dtype_dicts = _all_compatible_dtype_dicts(expected_dtype)
+
+    if node_ndarray_dtype_dict not in all_compatible_dtype_dicts:
+        raise ValueError(
+            "node array from the pickle has an incompatible dtype:\n"
+            f"- expected: {expected_dtype}\n"
+            f"- got     : {node_ndarray_dtype}"
+        )
+
+    return node_ndarray.astype(expected_dtype, casting="same_kind")
+
+
+# =============================================================================
+# Build Pruned Tree
+# =============================================================================
+
+
+cdef class _CCPPruneController:
+    """Base class used by build_pruned_tree_ccp and ccp_pruning_path
+    to control pruning.
+    """
+    cdef bint stop_pruning(self, DOUBLE_t effective_alpha) nogil:
+        """Return 1 to stop pruning and 0 to continue pruning"""
+        return 0
+
+    cdef void save_metrics(self, DOUBLE_t effective_alpha,
+                           DOUBLE_t subtree_impurities) nogil:
+        """Save metrics when pruning"""
+        pass
+
+    cdef void after_pruning(self, unsigned char[:] in_subtree) nogil:
+        """Called after pruning"""
+        pass
+
+
+cdef class _AlphaPruner(_CCPPruneController):
+    """Use alpha to control when to stop pruning."""
+    cdef DOUBLE_t ccp_alpha
+    cdef SIZE_t capacity
+
+    def __cinit__(self, DOUBLE_t ccp_alpha):
+        self.ccp_alpha = ccp_alpha
+        self.capacity = 0
+
+    cdef bint stop_pruning(self, DOUBLE_t effective_alpha) nogil:
+        # The subtree on the previous iteration has the greatest ccp_alpha
+        # less than or equal to self.ccp_alpha
+        return self.ccp_alpha < effective_alpha
+
+    cdef void after_pruning(self, unsigned char[:] in_subtree) nogil:
+        """Updates the number of leaves in subtree"""
+        for i in range(in_subtree.shape[0]):
+            if in_subtree[i]:
+                self.capacity += 1
+
+
+cdef class _PathFinder(_CCPPruneController):
+    """Record metrics used to return the cost complexity path."""
+    cdef DOUBLE_t[:] ccp_alphas
+    cdef DOUBLE_t[:] impurities
+    cdef UINT32_t count
+
+    def __cinit__(self,  int node_count):
+        self.ccp_alphas = np.zeros(shape=(node_count), dtype=np.float64)
+        self.impurities = np.zeros(shape=(node_count), dtype=np.float64)
+        self.count = 0
+
+    cdef void save_metrics(self,
+                           DOUBLE_t effective_alpha,
+                           DOUBLE_t subtree_impurities) nogil:
+        self.ccp_alphas[self.count] = effective_alpha
+        self.impurities[self.count] = subtree_impurities
+        self.count += 1
+
+
+cdef struct CostComplexityPruningRecord:
+    SIZE_t node_idx
+    SIZE_t parent
+
+cdef _cost_complexity_prune(unsigned char[:] leaves_in_subtree, # OUT
+                            Tree orig_tree,
+                            _CCPPruneController controller):
+    """Perform cost complexity pruning.
+
+    This function takes an already grown tree, `orig_tree` and outputs a
+    boolean mask `leaves_in_subtree` which are the leaves in the pruned tree.
+    During the pruning process, the controller is passed the effective alpha and
+    the subtree impurities. Furthermore, the controller signals when to stop
+    pruning.
+
+    Parameters
+    ----------
+    leaves_in_subtree : unsigned char[:]
+        Output for leaves of subtree
+    orig_tree : Tree
+        Original tree
+    ccp_controller : _CCPPruneController
+        Cost complexity controller
+    """
+
+    cdef:
+        SIZE_t i
+        SIZE_t n_nodes = orig_tree.node_count
+        # prior probability using weighted samples
+        DOUBLE_t[:] weighted_n_node_samples = orig_tree.weighted_n_node_samples
+        DOUBLE_t total_sum_weights = weighted_n_node_samples[0]
+        DOUBLE_t[:] impurity = orig_tree.impurity
+        # weighted impurity of each node
+        DOUBLE_t[:] r_node = np.empty(shape=n_nodes, dtype=np.float64)
+
+        SIZE_t[:] child_l = orig_tree.children_left
+        SIZE_t[:] child_r = orig_tree.children_right
+        SIZE_t[:] parent = np.zeros(shape=n_nodes, dtype=np.intp)
+
+        stack[CostComplexityPruningRecord] ccp_stack
+        CostComplexityPruningRecord stack_record
+        int rc = 0
+        SIZE_t node_idx
+        stack[SIZE_t] node_indices_stack
+
+        SIZE_t[:] n_leaves = np.zeros(shape=n_nodes, dtype=np.intp)
+        DOUBLE_t[:] r_branch = np.zeros(shape=n_nodes, dtype=np.float64)
+        DOUBLE_t current_r
+        SIZE_t leaf_idx
+        SIZE_t parent_idx
+
+        # candidate nodes that can be pruned
+        unsigned char[:] candidate_nodes = np.zeros(shape=n_nodes,
+                                                    dtype=np.uint8)
+        # nodes in subtree
+        unsigned char[:] in_subtree = np.ones(shape=n_nodes, dtype=np.uint8)
+        DOUBLE_t[:] g_node = np.zeros(shape=n_nodes, dtype=np.float64)
+        SIZE_t pruned_branch_node_idx
+        DOUBLE_t subtree_alpha
+        DOUBLE_t effective_alpha
+        SIZE_t child_l_idx
+        SIZE_t child_r_idx
+        SIZE_t n_pruned_leaves
+        DOUBLE_t r_diff
+        DOUBLE_t max_float64 = np.finfo(np.float64).max
+
+    # find parent node ids and leaves
+    with nogil:
+
+        for i in range(r_node.shape[0]):
+            r_node[i] = (
+                weighted_n_node_samples[i] * impurity[i] / total_sum_weights)
+
+        # Push the root node
+        ccp_stack.push({"node_idx": 0, "parent": _TREE_UNDEFINED})
+
+        while not ccp_stack.empty():
+            stack_record = ccp_stack.top()
+            ccp_stack.pop()
+
+            node_idx = stack_record.node_idx
+            parent[node_idx] = stack_record.parent
+
+            if child_l[node_idx] == _TREE_LEAF:
+                # ... and child_r[node_idx] == _TREE_LEAF:
+                leaves_in_subtree[node_idx] = 1
+            else:
+                ccp_stack.push({"node_idx": child_l[node_idx], "parent": node_idx})
+                ccp_stack.push({"node_idx": child_r[node_idx], "parent": node_idx})
+
+        # computes number of leaves in all branches and the overall impurity of
+        # the branch. The overall impurity is the sum of r_node in its leaves.
+        for leaf_idx in range(leaves_in_subtree.shape[0]):
+            if not leaves_in_subtree[leaf_idx]:
+                continue
+            r_branch[leaf_idx] = r_node[leaf_idx]
+
+            # bubble up values to ancestor nodes
+            current_r = r_node[leaf_idx]
+            while leaf_idx != 0:
+                parent_idx = parent[leaf_idx]
+                r_branch[parent_idx] += current_r
+                n_leaves[parent_idx] += 1
+                leaf_idx = parent_idx
+
+        for i in range(leaves_in_subtree.shape[0]):
+            candidate_nodes[i] = not leaves_in_subtree[i]
+
+        # save metrics before pruning
+        controller.save_metrics(0.0, r_branch[0])
+
+        # while root node is not a leaf
+        while candidate_nodes[0]:
+
+            # computes ccp_alpha for subtrees and finds the minimal alpha
+            effective_alpha = max_float64
+            for i in range(n_nodes):
+                if not candidate_nodes[i]:
+                    continue
+                subtree_alpha = (r_node[i] - r_branch[i]) / (n_leaves[i] - 1)
+                if subtree_alpha < effective_alpha:
+                    effective_alpha = subtree_alpha
+                    pruned_branch_node_idx = i
+
+            if controller.stop_pruning(effective_alpha):
+                break
+
+            node_indices_stack.push(pruned_branch_node_idx)
+
+            # descendants of branch are not in subtree
+            while not node_indices_stack.empty():
+                node_idx = node_indices_stack.top()
+                node_indices_stack.pop()
+
+                if not in_subtree[node_idx]:
+                    continue # branch has already been marked for pruning
+                candidate_nodes[node_idx] = 0
+                leaves_in_subtree[node_idx] = 0
+                in_subtree[node_idx] = 0
+
+                if child_l[node_idx] != _TREE_LEAF:
+                    # ... and child_r[node_idx] != _TREE_LEAF:
+                    node_indices_stack.push(child_l[node_idx])
+                    node_indices_stack.push(child_r[node_idx])
+            leaves_in_subtree[pruned_branch_node_idx] = 1
+            in_subtree[pruned_branch_node_idx] = 1
+
+            # updates number of leaves
+            n_pruned_leaves = n_leaves[pruned_branch_node_idx] - 1
+            n_leaves[pruned_branch_node_idx] = 0
+
+            # computes the increase in r_branch to bubble up
+            r_diff = r_node[pruned_branch_node_idx] - r_branch[pruned_branch_node_idx]
+            r_branch[pruned_branch_node_idx] = r_node[pruned_branch_node_idx]
+
+            # bubble up values to ancestors
+            node_idx = parent[pruned_branch_node_idx]
+            while node_idx != _TREE_UNDEFINED:
+                n_leaves[node_idx] -= n_pruned_leaves
+                r_branch[node_idx] += r_diff
+                node_idx = parent[node_idx]
+
+            controller.save_metrics(effective_alpha, r_branch[0])
+
+        controller.after_pruning(in_subtree)
+
+
+def _build_pruned_tree_ccp(
+    Tree tree, # OUT
+    Tree orig_tree,
+    DOUBLE_t ccp_alpha):
+    """Build a pruned tree from the original tree using cost complexity
+    pruning.
+
+    The values and nodes from the original tree are copied into the pruned
+    tree.
+
+    Parameters
+    ----------
+    tree : Tree
+        Location to place the pruned tree
+    orig_tree : Tree
+        Original tree
+    ccp_alpha : positive double
+        Complexity parameter. The subtree with the largest cost complexity
+        that is smaller than ``ccp_alpha`` will be chosen. By default,
+        no pruning is performed.
+    """
+
+    cdef:
+        SIZE_t n_nodes = orig_tree.node_count
+        unsigned char[:] leaves_in_subtree = np.zeros(
+            shape=n_nodes, dtype=np.uint8)
+
+    pruning_controller = _AlphaPruner(ccp_alpha=ccp_alpha)
+
+    _cost_complexity_prune(leaves_in_subtree, orig_tree, pruning_controller)
+
+    _build_pruned_tree(tree, orig_tree, leaves_in_subtree,
+                       pruning_controller.capacity)
+
+
+def ccp_pruning_path(Tree orig_tree):
+    """Computes the cost complexity pruning path.
+
+    Parameters
+    ----------
+    tree : Tree
+        Original tree.
+
+    Returns
+    -------
+    path_info : dict
+        Information about pruning path with attributes:
+
+        ccp_alphas : ndarray
+            Effective alphas of subtree during pruning.
+
+        impurities : ndarray
+            Sum of the impurities of the subtree leaves for the
+            corresponding alpha value in ``ccp_alphas``.
+    """
+    cdef:
+        unsigned char[:] leaves_in_subtree = np.zeros(
+            shape=orig_tree.node_count, dtype=np.uint8)
+
+    path_finder = _PathFinder(orig_tree.node_count)
+
+    _cost_complexity_prune(leaves_in_subtree, orig_tree, path_finder)
+
+    cdef:
+        UINT32_t total_items = path_finder.count
+        np.ndarray ccp_alphas = np.empty(shape=total_items,
+                                         dtype=np.float64)
+        np.ndarray impurities = np.empty(shape=total_items,
+                                         dtype=np.float64)
+        UINT32_t count = 0
+
+    while count < total_items:
+        ccp_alphas[count] = path_finder.ccp_alphas[count]
+        impurities[count] = path_finder.impurities[count]
+        count += 1
+
+    return {'ccp_alphas': ccp_alphas, 'impurities': impurities}
+
+
+cdef struct BuildPrunedRecord:
+    SIZE_t start
+    SIZE_t depth
+    SIZE_t parent
+    bint is_left
+
+cdef _build_pruned_tree(
+    Tree tree, # OUT
+    Tree orig_tree,
+    const unsigned char[:] leaves_in_subtree,
+    SIZE_t capacity):
+    """Build a pruned tree.
+
+    Build a pruned tree from the original tree by transforming the nodes in
+    ``leaves_in_subtree`` into leaves.
+
+    Parameters
+    ----------
+    tree : Tree
+        Location to place the pruned tree
+    orig_tree : Tree
+        Original tree
+    leaves_in_subtree : unsigned char memoryview, shape=(node_count, )
+        Boolean mask for leaves to include in subtree
+    capacity : SIZE_t
+        Number of nodes to initially allocate in pruned tree
+    """
+    tree._resize(capacity)
+
+    cdef:
+        SIZE_t orig_node_id
+        SIZE_t new_node_id
+        SIZE_t depth
+        SIZE_t parent
+        bint is_left
+        bint is_leaf
+
+        # value_stride for original tree and new tree are the same
+        SIZE_t value_stride = orig_tree.value_stride
+        SIZE_t max_depth_seen = -1
+        int rc = 0
+        Node* node
+        double* orig_value_ptr
+        double* new_value_ptr
+
+        stack[BuildPrunedRecord] prune_stack
+        BuildPrunedRecord stack_record
+
+    with nogil:
+        # push root node onto stack
+        prune_stack.push({"start": 0, "depth": 0, "parent": _TREE_UNDEFINED, "is_left": 0})
+
+        while not prune_stack.empty():
+            stack_record = prune_stack.top()
+            prune_stack.pop()
+
+            orig_node_id = stack_record.start
+            depth = stack_record.depth
+            parent = stack_record.parent
+            is_left = stack_record.is_left
+
+            is_leaf = leaves_in_subtree[orig_node_id]
+            node = &orig_tree.nodes[orig_node_id]
+
+            new_node_id = tree._add_node(
+                parent, is_left, is_leaf, node.feature, node.threshold,
+                node.impurity, node.n_node_samples,
+                node.weighted_n_node_samples)
+
+            if new_node_id == SIZE_MAX:
+                rc = -1
+                break
+
+            # copy value from original tree to new tree
+            orig_value_ptr = orig_tree.value + value_stride * orig_node_id
+            new_value_ptr = tree.value + value_stride * new_node_id
+            memcpy(new_value_ptr, orig_value_ptr, sizeof(double) * value_stride)
+
+            if not is_leaf:
+                # Push right child on stack
+                prune_stack.push({"start": node.right_child, "depth": depth + 1,
+                                  "parent": new_node_id, "is_left": 0})
+                # push left child on stack
+                prune_stack.push({"start": node.left_child, "depth": depth + 1,
+                                  "parent": new_node_id, "is_left": 1})
+
+            if depth > max_depth_seen:
+                max_depth_seen = depth
+
+        if rc >= 0:
+            tree.max_depth = max_depth_seen
+    if rc == -1:
+        raise MemoryError("pruning tree")
('sklearn/metrics', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,38 +4,45 @@
 """


-from .ranking import auc
-from .ranking import average_precision_score
-from .ranking import coverage_error
-from .ranking import label_ranking_average_precision_score
-from .ranking import label_ranking_loss
-from .ranking import precision_recall_curve
-from .ranking import roc_auc_score
-from .ranking import roc_curve
+from ._ranking import auc
+from ._ranking import average_precision_score
+from ._ranking import coverage_error
+from ._ranking import det_curve
+from ._ranking import dcg_score
+from ._ranking import label_ranking_average_precision_score
+from ._ranking import label_ranking_loss
+from ._ranking import ndcg_score
+from ._ranking import precision_recall_curve
+from ._ranking import roc_auc_score
+from ._ranking import roc_curve
+from ._ranking import top_k_accuracy_score

-from .classification import accuracy_score
-from .classification import balanced_accuracy_score
-from .classification import classification_report
-from .classification import cohen_kappa_score
-from .classification import confusion_matrix
-from .classification import f1_score
-from .classification import fbeta_score
-from .classification import hamming_loss
-from .classification import hinge_loss
-from .classification import jaccard_similarity_score
-from .classification import jaccard_score
-from .classification import log_loss
-from .classification import matthews_corrcoef
-from .classification import precision_recall_fscore_support
-from .classification import precision_score
-from .classification import recall_score
-from .classification import zero_one_loss
-from .classification import brier_score_loss
-from .classification import multilabel_confusion_matrix
+from ._classification import accuracy_score
+from ._classification import balanced_accuracy_score
+from ._classification import classification_report
+from ._classification import cohen_kappa_score
+from ._classification import confusion_matrix
+from ._classification import f1_score
+from ._classification import fbeta_score
+from ._classification import hamming_loss
+from ._classification import hinge_loss
+from ._classification import jaccard_score
+from ._classification import log_loss
+from ._classification import matthews_corrcoef
+from ._classification import precision_recall_fscore_support
+from ._classification import precision_score
+from ._classification import recall_score
+from ._classification import zero_one_loss
+from ._classification import brier_score_loss
+from ._classification import multilabel_confusion_matrix
+
+from ._dist_metrics import DistanceMetric

 from . import cluster
 from .cluster import adjusted_mutual_info_score
 from .cluster import adjusted_rand_score
+from .cluster import rand_score
+from .cluster import pair_confusion_matrix
 from .cluster import completeness_score
 from .cluster import consensus_score
 from .cluster import homogeneity_completeness_v_measure
@@ -46,90 +53,134 @@
 from .cluster import silhouette_samples
 from .cluster import silhouette_score
 from .cluster import calinski_harabasz_score
-from .cluster import calinski_harabaz_score
 from .cluster import v_measure_score
 from .cluster import davies_bouldin_score

 from .pairwise import euclidean_distances
+from .pairwise import nan_euclidean_distances
 from .pairwise import pairwise_distances
 from .pairwise import pairwise_distances_argmin
 from .pairwise import pairwise_distances_argmin_min
 from .pairwise import pairwise_kernels
 from .pairwise import pairwise_distances_chunked

-from .regression import explained_variance_score
-from .regression import max_error
-from .regression import mean_absolute_error
-from .regression import mean_squared_error
-from .regression import mean_squared_log_error
-from .regression import median_absolute_error
-from .regression import r2_score
+from ._regression import explained_variance_score
+from ._regression import max_error
+from ._regression import mean_absolute_error
+from ._regression import mean_squared_error
+from ._regression import mean_squared_log_error
+from ._regression import median_absolute_error
+from ._regression import mean_absolute_percentage_error
+from ._regression import mean_pinball_loss
+from ._regression import r2_score
+from ._regression import mean_tweedie_deviance
+from ._regression import mean_poisson_deviance
+from ._regression import mean_gamma_deviance
+from ._regression import d2_tweedie_score
+from ._regression import d2_pinball_score
+from ._regression import d2_absolute_error_score


-from .scorer import check_scoring
-from .scorer import make_scorer
-from .scorer import SCORERS
-from .scorer import get_scorer
+from ._scorer import check_scoring
+from ._scorer import make_scorer
+from ._scorer import SCORERS
+from ._scorer import get_scorer
+from ._scorer import get_scorer_names
+
+
+from ._plot.det_curve import plot_det_curve
+from ._plot.det_curve import DetCurveDisplay
+from ._plot.roc_curve import plot_roc_curve
+from ._plot.roc_curve import RocCurveDisplay
+from ._plot.precision_recall_curve import plot_precision_recall_curve
+from ._plot.precision_recall_curve import PrecisionRecallDisplay
+
+from ._plot.confusion_matrix import plot_confusion_matrix
+from ._plot.confusion_matrix import ConfusionMatrixDisplay
+

 __all__ = [
-    'accuracy_score',
-    'adjusted_mutual_info_score',
-    'adjusted_rand_score',
-    'auc',
-    'average_precision_score',
-    'balanced_accuracy_score',
-    'calinski_harabaz_score',
-    'calinski_harabasz_score',
-    'check_scoring',
-    'classification_report',
-    'cluster',
-    'cohen_kappa_score',
-    'completeness_score',
-    'confusion_matrix',
-    'consensus_score',
-    'coverage_error',
-    'davies_bouldin_score',
-    'euclidean_distances',
-    'explained_variance_score',
-    'f1_score',
-    'fbeta_score',
-    'fowlkes_mallows_score',
-    'get_scorer',
-    'hamming_loss',
-    'hinge_loss',
-    'homogeneity_completeness_v_measure',
-    'homogeneity_score',
-    'jaccard_score',
-    'jaccard_similarity_score',
-    'label_ranking_average_precision_score',
-    'label_ranking_loss',
-    'log_loss',
-    'make_scorer',
-    'matthews_corrcoef',
-    'max_error',
-    'mean_absolute_error',
-    'mean_squared_error',
-    'mean_squared_log_error',
-    'median_absolute_error',
-    'multilabel_confusion_matrix',
-    'mutual_info_score',
-    'normalized_mutual_info_score',
-    'pairwise_distances',
-    'pairwise_distances_argmin',
-    'pairwise_distances_argmin_min',
-    'pairwise_distances_chunked',
-    'pairwise_kernels',
-    'precision_recall_curve',
-    'precision_recall_fscore_support',
-    'precision_score',
-    'r2_score',
-    'recall_score',
-    'roc_auc_score',
-    'roc_curve',
-    'SCORERS',
-    'silhouette_samples',
-    'silhouette_score',
-    'v_measure_score',
-    'zero_one_loss',
-    'brier_score_loss',
+    "accuracy_score",
+    "adjusted_mutual_info_score",
+    "adjusted_rand_score",
+    "auc",
+    "average_precision_score",
+    "balanced_accuracy_score",
+    "calinski_harabasz_score",
+    "check_scoring",
+    "classification_report",
+    "cluster",
+    "cohen_kappa_score",
+    "completeness_score",
+    "ConfusionMatrixDisplay",
+    "confusion_matrix",
+    "consensus_score",
+    "coverage_error",
+    "d2_tweedie_score",
+    "d2_absolute_error_score",
+    "d2_pinball_score",
+    "dcg_score",
+    "davies_bouldin_score",
+    "DetCurveDisplay",
+    "det_curve",
+    "DistanceMetric",
+    "euclidean_distances",
+    "explained_variance_score",
+    "f1_score",
+    "fbeta_score",
+    "fowlkes_mallows_score",
+    "get_scorer",
+    "hamming_loss",
+    "hinge_loss",
+    "homogeneity_completeness_v_measure",
+    "homogeneity_score",
+    "jaccard_score",
+    "label_ranking_average_precision_score",
+    "label_ranking_loss",
+    "log_loss",
+    "make_scorer",
+    "nan_euclidean_distances",
+    "matthews_corrcoef",
+    "max_error",
+    "mean_absolute_error",
+    "mean_squared_error",
+    "mean_squared_log_error",
+    "mean_pinball_loss",
+    "mean_poisson_deviance",
+    "mean_gamma_deviance",
+    "mean_tweedie_deviance",
+    "median_absolute_error",
+    "mean_absolute_percentage_error",
+    "multilabel_confusion_matrix",
+    "mutual_info_score",
+    "ndcg_score",
+    "normalized_mutual_info_score",
+    "pair_confusion_matrix",
+    "pairwise_distances",
+    "pairwise_distances_argmin",
+    "pairwise_distances_argmin_min",
+    "pairwise_distances_chunked",
+    "pairwise_kernels",
+    "plot_confusion_matrix",
+    "plot_det_curve",
+    "plot_precision_recall_curve",
+    "plot_roc_curve",
+    "PrecisionRecallDisplay",
+    "precision_recall_curve",
+    "precision_recall_fscore_support",
+    "precision_score",
+    "r2_score",
+    "rand_score",
+    "recall_score",
+    "RocCurveDisplay",
+    "roc_auc_score",
+    "roc_curve",
+    "SCORERS",
+    "get_scorer_names",
+    "silhouette_samples",
+    "silhouette_score",
+    "top_k_accuracy_score",
+    "v_measure_score",
+    "zero_one_loss",
+    "brier_score_loss",
 ]
('sklearn/metrics', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,4 +1,5 @@
 import os
+import numpy as np

 from numpy.distutils.misc_util import Configuration

@@ -7,20 +8,39 @@
     config = Configuration("metrics", parent_package, top_path)

     libraries = []
-    if os.name == 'posix':
-        libraries.append('m')
+    if os.name == "posix":
+        libraries.append("m")

-    config.add_subpackage('cluster')
+    config.add_subpackage("_plot")
+    config.add_subpackage("_plot.tests")
+    config.add_subpackage("cluster")

-    config.add_extension("pairwise_fast",
-                         sources=["pairwise_fast.pyx"],
-                         libraries=libraries)
+    config.add_extension(
+        "_pairwise_fast", sources=["_pairwise_fast.pyx"], libraries=libraries
+    )

-    config.add_subpackage('tests')
+    config.add_extension(
+        "_dist_metrics",
+        sources=["_dist_metrics.pyx"],
+        include_dirs=[np.get_include(), os.path.join(np.get_include(), "numpy")],
+        libraries=libraries,
+    )
+
+    config.add_extension(
+        "_pairwise_distances_reduction",
+        sources=["_pairwise_distances_reduction.pyx"],
+        include_dirs=[np.get_include(), os.path.join(np.get_include(), "numpy")],
+        language="c++",
+        libraries=libraries,
+        extra_compile_args=["-std=c++11"],
+    )
+
+    config.add_subpackage("tests")

     return config


 if __name__ == "__main__":
     from numpy.distutils.core import setup
+
     setup(**configuration().todict())
('sklearn/metrics', 'pairwise.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,5 +1,3 @@
-# -*- coding: utf-8 -*-
-
 # Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
 #          Mathieu Blondel <mathieu@mblondel.org>
 #          Robert Layton <robertlayton@gmail.com>
@@ -17,19 +15,23 @@
 from scipy.spatial import distance
 from scipy.sparse import csr_matrix
 from scipy.sparse import issparse
-
+from joblib import Parallel, effective_n_jobs
+
+from .. import config_context
 from ..utils.validation import _num_samples
 from ..utils.validation import check_non_negative
 from ..utils import check_array
 from ..utils import gen_even_slices
 from ..utils import gen_batches, get_chunk_n_rows
+from ..utils import is_scalar_nan
 from ..utils.extmath import row_norms, safe_sparse_dot
 from ..preprocessing import normalize
-from ..utils._joblib import Parallel
-from ..utils._joblib import delayed
-from ..utils._joblib import effective_n_jobs
-
-from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan
+from ..utils._mask import _get_mask
+from ..utils.fixes import delayed
+from ..utils.fixes import sp_version, parse_version
+
+from ._pairwise_distances_reduction import PairwiseDistancesArgKmin
+from ._pairwise_fast import _chi2_kernel_fast, _sparse_manhattan
 from ..exceptions import DataConversionWarning


@@ -53,13 +55,22 @@
     if X.dtype == Y_dtype == np.float32:
         dtype = np.float32
     else:
-        dtype = np.float
+        dtype = float

     return X, Y, dtype


-def check_pairwise_arrays(X, Y, precomputed=False, dtype=None):
-    """ Set X and Y appropriately and checks inputs
+def check_pairwise_arrays(
+    X,
+    Y,
+    *,
+    precomputed=False,
+    dtype=None,
+    accept_sparse="csr",
+    force_all_finite=True,
+    copy=False,
+):
+    """Set X and Y appropriately and checks inputs.

     If Y is None, it is set as a pointer to X (i.e. not a copy).
     If Y is given, this does not happen.
@@ -74,61 +85,109 @@

     Parameters
     ----------
-    X : {array-like, sparse matrix}, shape (n_samples_a, n_features)
-
-    Y : {array-like, sparse matrix}, shape (n_samples_b, n_features)
-
-    precomputed : bool
+    X : {array-like, sparse matrix} of shape (n_samples_X, n_features)
+
+    Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)
+
+    precomputed : bool, default=False
         True if X is to be treated as precomputed distances to the samples in
         Y.

-    dtype : string, type, list of types or None (default=None)
+    dtype : str, type, list of type, default=None
         Data type required for X and Y. If None, the dtype will be an
         appropriate float type selected by _return_float_dtype.

         .. versionadded:: 0.18

-    Returns
-    -------
-    safe_X : {array-like, sparse matrix}, shape (n_samples_a, n_features)
+    accept_sparse : str, bool or list/tuple of str, default='csr'
+        String[s] representing allowed sparse matrix formats, such as 'csc',
+        'csr', etc. If the input is sparse but not in the allowed format,
+        it will be converted to the first listed format. True allows the input
+        to be any format. False means that a sparse matrix input will
+        raise an error.
+
+    force_all_finite : bool or 'allow-nan', default=True
+        Whether to raise an error on np.inf, np.nan, pd.NA in array. The
+        possibilities are:
+
+        - True: Force all values of array to be finite.
+        - False: accepts np.inf, np.nan, pd.NA in array.
+        - 'allow-nan': accepts only np.nan and pd.NA values in array. Values
+          cannot be infinite.
+
+        .. versionadded:: 0.22
+           ``force_all_finite`` accepts the string ``'allow-nan'``.
+
+        .. versionchanged:: 0.23
+           Accepts `pd.NA` and converts it into `np.nan`.
+
+    copy : bool, default=False
+        Whether a forced copy will be triggered. If copy=False, a copy might
+        be triggered by a conversion.
+
+        .. versionadded:: 0.22
+
+    Returns
+    -------
+    safe_X : {array-like, sparse matrix} of shape (n_samples_X, n_features)
         An array equal to X, guaranteed to be a numpy array.

-    safe_Y : {array-like, sparse matrix}, shape (n_samples_b, n_features)
+    safe_Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)
         An array equal to Y if Y was not None, guaranteed to be a numpy array.
         If Y was None, safe_Y will be a pointer to X.

     """
     X, Y, dtype_float = _return_float_dtype(X, Y)

-    estimator = 'check_pairwise_arrays'
+    estimator = "check_pairwise_arrays"
     if dtype is None:
         dtype = dtype_float

     if Y is X or Y is None:
-        X = Y = check_array(X, accept_sparse='csr', dtype=dtype,
-                            estimator=estimator)
+        X = Y = check_array(
+            X,
+            accept_sparse=accept_sparse,
+            dtype=dtype,
+            copy=copy,
+            force_all_finite=force_all_finite,
+            estimator=estimator,
+        )
     else:
-        X = check_array(X, accept_sparse='csr', dtype=dtype,
-                        estimator=estimator)
-        Y = check_array(Y, accept_sparse='csr', dtype=dtype,
-                        estimator=estimator)
+        X = check_array(
+            X,
+            accept_sparse=accept_sparse,
+            dtype=dtype,
+            copy=copy,
+            force_all_finite=force_all_finite,
+            estimator=estimator,
+        )
+        Y = check_array(
+            Y,
+            accept_sparse=accept_sparse,
+            dtype=dtype,
+            copy=copy,
+            force_all_finite=force_all_finite,
+            estimator=estimator,
+        )

     if precomputed:
         if X.shape[1] != Y.shape[0]:
-            raise ValueError("Precomputed metric requires shape "
-                             "(n_queries, n_indexed). Got (%d, %d) "
-                             "for %d indexed." %
-                             (X.shape[0], X.shape[1], Y.shape[0]))
+            raise ValueError(
+                "Precomputed metric requires shape "
+                "(n_queries, n_indexed). Got (%d, %d) "
+                "for %d indexed." % (X.shape[0], X.shape[1], Y.shape[0])
+            )
     elif X.shape[1] != Y.shape[1]:
-        raise ValueError("Incompatible dimension for X and Y matrices: "
-                         "X.shape[1] == %d while Y.shape[1] == %d" % (
-                             X.shape[1], Y.shape[1]))
+        raise ValueError(
+            "Incompatible dimension for X and Y matrices: "
+            "X.shape[1] == %d while Y.shape[1] == %d" % (X.shape[1], Y.shape[1])
+        )

     return X, Y


 def check_paired_arrays(X, Y):
-    """ Set X and Y appropriately and checks inputs for paired distances
+    """Set X and Y appropriately and checks inputs for paired distances.

     All paired distance metrics should use this function first to assert that
     the given parameters are correct and safe to use.
@@ -140,33 +199,35 @@

     Parameters
     ----------
-    X : {array-like, sparse matrix}, shape (n_samples_a, n_features)
-
-    Y : {array-like, sparse matrix}, shape (n_samples_b, n_features)
-
-    Returns
-    -------
-    safe_X : {array-like, sparse matrix}, shape (n_samples_a, n_features)
+    X : {array-like, sparse matrix} of shape (n_samples_X, n_features)
+
+    Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)
+
+    Returns
+    -------
+    safe_X : {array-like, sparse matrix} of shape (n_samples_X, n_features)
         An array equal to X, guaranteed to be a numpy array.

-    safe_Y : {array-like, sparse matrix}, shape (n_samples_b, n_features)
+    safe_Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)
         An array equal to Y if Y was not None, guaranteed to be a numpy array.
         If Y was None, safe_Y will be a pointer to X.

     """
     X, Y = check_pairwise_arrays(X, Y)
     if X.shape != Y.shape:
-        raise ValueError("X and Y should be of same shape. They were "
-                         "respectively %r and %r long." % (X.shape, Y.shape))
+        raise ValueError(
+            "X and Y should be of same shape. They were respectively %r and %r long."
+            % (X.shape, Y.shape)
+        )
     return X, Y


 # Pairwise distances
-def euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False,
-                        X_norm_squared=None):
-    """
-    Considering the rows of X (and Y=X) as vectors, compute the
-    distance matrix between each pair of vectors.
+def euclidean_distances(
+    X, Y=None, *, Y_norm_squared=None, squared=False, X_norm_squared=None
+):
+    """
+    Compute the distance matrix between each pair from a vector array X and Y.

     For efficiency reasons, the euclidean distance between a pair of row
     vector x and y is computed as::
@@ -178,39 +239,52 @@
     Second, if one argument varies but the other remains unchanged, then
     `dot(x, x)` and/or `dot(y, y)` can be pre-computed.

-    However, this is not the most precise way of doing this computation, and
-    the distance matrix returned by this function may not be exactly
+    However, this is not the most precise way of doing this computation,
+    because this equation potentially suffers from "catastrophic cancellation".
+    Also, the distance matrix returned by this function may not be exactly
     symmetric as required by, e.g., ``scipy.spatial.distance`` functions.

     Read more in the :ref:`User Guide <metrics>`.

     Parameters
     ----------
-    X : {array-like, sparse matrix}, shape (n_samples_1, n_features)
-
-    Y : {array-like, sparse matrix}, shape (n_samples_2, n_features)
-
-    Y_norm_squared : array-like, shape (n_samples_2, ), optional
+    X : {array-like, sparse matrix} of shape (n_samples_X, n_features)
+        An array where each row is a sample and each column is a feature.
+
+    Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features), \
+            default=None
+        An array where each row is a sample and each column is a feature.
+        If `None`, method uses `Y=X`.
+
+    Y_norm_squared : array-like of shape (n_samples_Y,) or (n_samples_Y, 1) \
+            or (1, n_samples_Y), default=None
         Pre-computed dot-products of vectors in Y (e.g.,
         ``(Y**2).sum(axis=1)``)
         May be ignored in some cases, see the note below.

-    squared : boolean, optional
+    squared : bool, default=False
         Return squared Euclidean distances.

-    X_norm_squared : array-like, shape = [n_samples_1], optional
+    X_norm_squared : array-like of shape (n_samples_X,) or (n_samples_X, 1) \
+            or (1, n_samples_X), default=None
         Pre-computed dot-products of vectors in X (e.g.,
         ``(X**2).sum(axis=1)``)
         May be ignored in some cases, see the note below.

+    Returns
+    -------
+    distances : ndarray of shape (n_samples_X, n_samples_Y)
+        Returns the distances between the row vectors of `X`
+        and the row vectors of `Y`.
+
+    See Also
+    --------
+    paired_distances : Distances betweens pairs of elements of X and Y.
+
     Notes
     -----
-    To achieve better accuracy, `X_norm_squared` and `Y_norm_squared` may be
-    unused if they are passed as ``float32``.
-
-    Returns
-    -------
-    distances : array, shape (n_samples_1, n_samples_2)
+    To achieve a better accuracy, `X_norm_squared` and `Y_norm_squared` may be
+    unused if they are passed as `np.float32`.

     Examples
     --------
@@ -224,45 +298,69 @@
     >>> euclidean_distances(X, [[0, 0]])
     array([[1.        ],
            [1.41421356]])
-
-    See also
-    --------
-    paired_distances : distances betweens pairs of elements of X and Y.
     """
     X, Y = check_pairwise_arrays(X, Y)

-    # If norms are passed as float32, they are unused. If arrays are passed as
-    # float32, norms needs to be recomputed on upcast chunks.
-    # TODO: use a float64 accumulator in row_norms to avoid the latter.
     if X_norm_squared is not None:
-        XX = check_array(X_norm_squared)
-        if XX.shape == (1, X.shape[0]):
-            XX = XX.T
-        elif XX.shape != (X.shape[0], 1):
+        X_norm_squared = check_array(X_norm_squared, ensure_2d=False)
+        original_shape = X_norm_squared.shape
+        if X_norm_squared.shape == (X.shape[0],):
+            X_norm_squared = X_norm_squared.reshape(-1, 1)
+        if X_norm_squared.shape == (1, X.shape[0]):
+            X_norm_squared = X_norm_squared.T
+        if X_norm_squared.shape != (X.shape[0], 1):
             raise ValueError(
-                "Incompatible dimensions for X and X_norm_squared")
-        if XX.dtype == np.float32:
+                f"Incompatible dimensions for X of shape {X.shape} and "
+                f"X_norm_squared of shape {original_shape}."
+            )
+
+    if Y_norm_squared is not None:
+        Y_norm_squared = check_array(Y_norm_squared, ensure_2d=False)
+        original_shape = Y_norm_squared.shape
+        if Y_norm_squared.shape == (Y.shape[0],):
+            Y_norm_squared = Y_norm_squared.reshape(1, -1)
+        if Y_norm_squared.shape == (Y.shape[0], 1):
+            Y_norm_squared = Y_norm_squared.T
+        if Y_norm_squared.shape != (1, Y.shape[0]):
+            raise ValueError(
+                f"Incompatible dimensions for Y of shape {Y.shape} and "
+                f"Y_norm_squared of shape {original_shape}."
+            )
+
+    return _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)
+
+
+def _euclidean_distances(X, Y, X_norm_squared=None, Y_norm_squared=None, squared=False):
+    """Computational part of euclidean_distances
+
+    Assumes inputs are already checked.
+
+    If norms are passed as float32, they are unused. If arrays are passed as
+    float32, norms needs to be recomputed on upcast chunks.
+    TODO: use a float64 accumulator in row_norms to avoid the latter.
+    """
+    if X_norm_squared is not None:
+        if X_norm_squared.dtype == np.float32:
             XX = None
+        else:
+            XX = X_norm_squared.reshape(-1, 1)
     elif X.dtype == np.float32:
         XX = None
     else:
         XX = row_norms(X, squared=True)[:, np.newaxis]

-    if X is Y and XX is not None:
-        # shortcut in the common case euclidean_distances(X, X)
-        YY = XX.T
-    elif Y_norm_squared is not None:
-        YY = np.atleast_2d(Y_norm_squared)
-
-        if YY.shape != (1, Y.shape[0]):
-            raise ValueError(
-                "Incompatible dimensions for Y and Y_norm_squared")
-        if YY.dtype == np.float32:
+    if Y is X:
+        YY = None if XX is None else XX.T
+    else:
+        if Y_norm_squared is not None:
+            if Y_norm_squared.dtype == np.float32:
+                YY = None
+            else:
+                YY = Y_norm_squared.reshape(1, -1)
+        elif Y.dtype == np.float32:
             YY = None
-    elif Y.dtype == np.float32:
-        YY = None
-    else:
-        YY = row_norms(Y, squared=True)[np.newaxis, :]
+        else:
+            YY = row_norms(Y, squared=True)[np.newaxis, :]

     if X.dtype == np.float32:
         # To minimize precision issues with float32, we compute the distance
@@ -270,7 +368,7 @@
         distances = _euclidean_distances_upcast(X, XX, Y, YY)
     else:
         # if dtype is already float64, no need to chunk and upcast
-        distances = - 2 * safe_sparse_dot(X, Y.T, dense_output=True)
+        distances = -2 * safe_sparse_dot(X, Y.T, dense_output=True)
         distances += XX
         distances += YY
     np.maximum(distances, 0, out=distances)
@@ -283,8 +381,130 @@
     return distances if squared else np.sqrt(distances, out=distances)


-def _euclidean_distances_upcast(X, XX=None, Y=None, YY=None):
-    """Euclidean distances between X and Y
+def nan_euclidean_distances(
+    X, Y=None, *, squared=False, missing_values=np.nan, copy=True
+):
+    """Calculate the euclidean distances in the presence of missing values.
+
+    Compute the euclidean distance between each pair of samples in X and Y,
+    where Y=X is assumed if Y=None. When calculating the distance between a
+    pair of samples, this formulation ignores feature coordinates with a
+    missing value in either sample and scales up the weight of the remaining
+    coordinates:
+
+        dist(x,y) = sqrt(weight * sq. distance from present coordinates)
+        where,
+        weight = Total # of coordinates / # of present coordinates
+
+    For example, the distance between ``[3, na, na, 6]`` and ``[1, na, 4, 5]``
+    is:
+
+        .. math::
+            \\sqrt{\\frac{4}{2}((3-1)^2 + (6-5)^2)}
+
+    If all the coordinates are missing or if there are no common present
+    coordinates then NaN is returned for that pair.
+
+    Read more in the :ref:`User Guide <metrics>`.
+
+    .. versionadded:: 0.22
+
+    Parameters
+    ----------
+    X : array-like of shape (n_samples_X, n_features)
+        An array where each row is a sample and each column is a feature.
+
+    Y : array-like of shape (n_samples_Y, n_features), default=None
+        An array where each row is a sample and each column is a feature.
+        If `None`, method uses `Y=X`.
+
+    squared : bool, default=False
+        Return squared Euclidean distances.
+
+    missing_values : np.nan or int, default=np.nan
+        Representation of missing value.
+
+    copy : bool, default=True
+        Make and use a deep copy of X and Y (if Y exists).
+
+    Returns
+    -------
+    distances : ndarray of shape (n_samples_X, n_samples_Y)
+        Returns the distances between the row vectors of `X`
+        and the row vectors of `Y`.
+
+    See Also
+    --------
+    paired_distances : Distances between pairs of elements of X and Y.
+
+    References
+    ----------
+    * John K. Dixon, "Pattern Recognition with Partly Missing Data",
+      IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue:
+      10, pp. 617 - 621, Oct. 1979.
+      http://ieeexplore.ieee.org/abstract/document/4310090/
+
+    Examples
+    --------
+    >>> from sklearn.metrics.pairwise import nan_euclidean_distances
+    >>> nan = float("NaN")
+    >>> X = [[0, 1], [1, nan]]
+    >>> nan_euclidean_distances(X, X) # distance between rows of X
+    array([[0.        , 1.41421356],
+           [1.41421356, 0.        ]])
+
+    >>> # get distance to origin
+    >>> nan_euclidean_distances(X, [[0, 0]])
+    array([[1.        ],
+           [1.41421356]])
+    """
+
+    force_all_finite = "allow-nan" if is_scalar_nan(missing_values) else True
+    X, Y = check_pairwise_arrays(
+        X, Y, accept_sparse=False, force_all_finite=force_all_finite, copy=copy
+    )
+    # Get missing mask for X
+    missing_X = _get_mask(X, missing_values)
+
+    # Get missing mask for Y
+    missing_Y = missing_X if Y is X else _get_mask(Y, missing_values)
+
+    # set missing values to zero
+    X[missing_X] = 0
+    Y[missing_Y] = 0
+
+    distances = euclidean_distances(X, Y, squared=True)
+
+    # Adjust distances for missing values
+    XX = X * X
+    YY = Y * Y
+    distances -= np.dot(XX, missing_Y.T)
+    distances -= np.dot(missing_X, YY.T)
+
+    np.clip(distances, 0, None, out=distances)
+
+    if X is Y:
+        # Ensure that distances between vectors and themselves are set to 0.0.
+        # This may not be the case due to floating point rounding errors.
+        np.fill_diagonal(distances, 0.0)
+
+    present_X = 1 - missing_X
+    present_Y = present_X if Y is X else ~missing_Y
+    present_count = np.dot(present_X, present_Y.T)
+    distances[present_count == 0] = np.nan
+    # avoid divide by zero
+    np.maximum(1, present_count, out=present_count)
+    distances /= present_count
+    distances *= X.shape[1]
+
+    if not squared:
+        np.sqrt(distances, out=distances)
+
+    return distances
+
+
+def _euclidean_distances_upcast(X, XX=None, Y=None, YY=None, batch_size=None):
+    """Euclidean distances between X and Y.

     Assumes X and Y have float32 dtype.
     Assumes XX and YY have float64 dtype or are None.
@@ -298,28 +518,32 @@

     distances = np.empty((n_samples_X, n_samples_Y), dtype=np.float32)

-    x_density = X.nnz / np.prod(X.shape) if issparse(X) else 1
-    y_density = Y.nnz / np.prod(Y.shape) if issparse(Y) else 1
-
-    # Allow 10% more memory than X, Y and the distance matrix take (at least
-    # 10MiB)
-    maxmem = max(
-        ((x_density * n_samples_X + y_density * n_samples_Y) * n_features
-         + (x_density * n_samples_X * y_density * n_samples_Y)) / 10,
-        10 * 2**17)
-
-    # The increase amount of memory in 8-byte blocks is:
-    # - x_density * batch_size * n_features (copy of chunk of X)
-    # - y_density * batch_size * n_features (copy of chunk of Y)
-    # - batch_size * batch_size (chunk of distance matrix)
-    # Hence x² + (xd+yd)kx = M, where x=batch_size, k=n_features, M=maxmem
-    #                                 xd=x_density and yd=y_density
-    tmp = (x_density + y_density) * n_features
-    batch_size = (-tmp + np.sqrt(tmp**2 + 4 * maxmem)) / 2
-    batch_size = max(int(batch_size), 1)
-
-    x_batches = gen_batches(X.shape[0], batch_size)
-    y_batches = gen_batches(Y.shape[0], batch_size)
+    if batch_size is None:
+        x_density = X.nnz / np.prod(X.shape) if issparse(X) else 1
+        y_density = Y.nnz / np.prod(Y.shape) if issparse(Y) else 1
+
+        # Allow 10% more memory than X, Y and the distance matrix take (at
+        # least 10MiB)
+        maxmem = max(
+            (
+                (x_density * n_samples_X + y_density * n_samples_Y) * n_features
+                + (x_density * n_samples_X * y_density * n_samples_Y)
+            )
+            / 10,
+            10 * 2**17,
+        )
+
+        # The increase amount of memory in 8-byte blocks is:
+        # - x_density * batch_size * n_features (copy of chunk of X)
+        # - y_density * batch_size * n_features (copy of chunk of Y)
+        # - batch_size * batch_size (chunk of distance matrix)
+        # Hence x² + (xd+yd)kx = M, where x=batch_size, k=n_features, M=maxmem
+        #                                 xd=x_density and yd=y_density
+        tmp = (x_density + y_density) * n_features
+        batch_size = (-tmp + np.sqrt(tmp**2 + 4 * maxmem)) / 2
+        batch_size = max(int(batch_size), 1)
+
+    x_batches = gen_batches(n_samples_X, batch_size)

     for i, x_slice in enumerate(x_batches):
         X_chunk = X[x_slice].astype(np.float64)
@@ -328,6 +552,8 @@
         else:
             XX_chunk = XX[x_slice]

+        y_batches = gen_batches(n_samples_Y, batch_size)
+
         for j, y_slice in enumerate(y_batches):
             if X is Y and j < i:
                 # when X is Y the distance matrix is symmetric so we only need
@@ -351,13 +577,24 @@


 def _argmin_min_reduce(dist, start):
+    # `start` is specified in the signature but not used. This is because the higher
+    # order `pairwise_distances_chunked` function needs reduction functions that are
+    # passed as argument to have a two arguments signature.
     indices = dist.argmin(axis=1)
     values = dist[np.arange(dist.shape[0]), indices]
     return indices, values


-def pairwise_distances_argmin_min(X, Y, axis=1, metric="euclidean",
-                                  batch_size=None, metric_kwargs=None):
+def _argmin_reduce(dist, start):
+    # `start` is specified in the signature but not used. This is because the higher
+    # order `pairwise_distances_chunked` function needs reduction functions that are
+    # passed as argument to have a two arguments signature.
+    return dist.argmin(axis=1)
+
+
+def pairwise_distances_argmin_min(
+    X, Y, *, axis=1, metric="euclidean", metric_kwargs=None
+):
     """Compute minimum distances between one point and a set of points.

     This function computes for each row in X, the index of the row of Y which
@@ -373,17 +610,17 @@

     Parameters
     ----------
-    X : {array-like, sparse matrix}, shape (n_samples1, n_features)
+    X : {array-like, sparse matrix} of shape (n_samples_X, n_features)
         Array containing points.

-    Y : {array-like, sparse matrix}, shape (n_samples2, n_features)
-        Arrays containing points.
-
-    axis : int, optional, default 1
+    Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)
+        Array containing points.
+
+    axis : int, default=1
         Axis along which the argmin and distances are to be computed.

-    metric : string or callable, default 'euclidean'
-        metric to use for distance computation. Any metric from scikit-learn
+    metric : str or callable, default='euclidean'
+        Metric to use for distance computation. Any metric from scikit-learn
         or scipy.spatial.distance can be used.

         If metric is a callable function, it is called on each
@@ -408,52 +645,68 @@
         See the documentation for scipy.spatial.distance for details on these
         metrics.

-    batch_size : integer
-        .. deprecated:: 0.20
-            Deprecated for removal in 0.22.
-            Use sklearn.set_config(working_memory=...) instead.
-
-    metric_kwargs : dict, optional
+    metric_kwargs : dict, default=None
         Keyword arguments to pass to specified metric function.

     Returns
     -------
-    argmin : numpy.ndarray
+    argmin : ndarray
         Y[argmin[i], :] is the row in Y that is closest to X[i, :].

-    distances : numpy.ndarray
+    distances : ndarray
         distances[i] is the distance between the i-th row in X and the
         argmin[i]-th row in Y.

-    See also
+    See Also
     --------
     sklearn.metrics.pairwise_distances
     sklearn.metrics.pairwise_distances_argmin
     """
-    if batch_size is not None:
-        warnings.warn("'batch_size' is ignored. It was deprecated in version "
-                      "0.20 and will be removed in version 0.22. "
-                      "Use sklearn.set_config(working_memory=...) instead.",
-                      DeprecationWarning)
     X, Y = check_pairwise_arrays(X, Y)
+
+    if axis == 0:
+        X, Y = Y, X

     if metric_kwargs is None:
         metric_kwargs = {}

-    if axis == 0:
-        X, Y = Y, X
-
-    indices, values = zip(*pairwise_distances_chunked(
-        X, Y, reduce_func=_argmin_min_reduce, metric=metric,
-        **metric_kwargs))
-    indices = np.concatenate(indices)
-    values = np.concatenate(values)
+    if PairwiseDistancesArgKmin.is_usable_for(X, Y, metric):
+        # This is an adaptor for one "sqeuclidean" specification.
+        # For this backend, we can directly use "sqeuclidean".
+        if metric_kwargs.get("squared", False) and metric == "euclidean":
+            metric = "sqeuclidean"
+            metric_kwargs = {}
+
+        values, indices = PairwiseDistancesArgKmin.compute(
+            X=X,
+            Y=Y,
+            k=1,
+            metric=metric,
+            metric_kwargs=metric_kwargs,
+            strategy="auto",
+            return_distance=True,
+        )
+        values = values.flatten()
+        indices = indices.flatten()
+    else:
+        # TODO: once PairwiseDistancesArgKmin supports sparse input matrices and 32 bit,
+        # we won't need to fallback to pairwise_distances_chunked anymore.
+
+        # Turn off check for finiteness because this is costly and because arrays
+        # have already been validated.
+        with config_context(assume_finite=True):
+            indices, values = zip(
+                *pairwise_distances_chunked(
+                    X, Y, reduce_func=_argmin_min_reduce, metric=metric, **metric_kwargs
+                )
+            )
+        indices = np.concatenate(indices)
+        values = np.concatenate(values)

     return indices, values


-def pairwise_distances_argmin(X, Y, axis=1, metric="euclidean",
-                              batch_size=None, metric_kwargs=None):
+def pairwise_distances_argmin(X, Y, *, axis=1, metric="euclidean", metric_kwargs=None):
     """Compute minimum distances between one point and a set of points.

     This function computes for each row in X, the index of the row of Y which
@@ -469,19 +722,17 @@

     Parameters
     ----------
-    X : array-like
-        Arrays containing points. Respective shapes (n_samples1, n_features)
-        and (n_samples2, n_features)
-
-    Y : array-like
-        Arrays containing points. Respective shapes (n_samples1, n_features)
-        and (n_samples2, n_features)
-
-    axis : int, optional, default 1
+    X : array-like of shape (n_samples_X, n_features)
+        Array containing points.
+
+    Y : array-like of shape (n_samples_Y, n_features)
+        Arrays containing points.
+
+    axis : int, default=1
         Axis along which the argmin and distances are to be computed.

-    metric : string or callable
-        metric to use for distance computation. Any metric from scikit-learn
+    metric : str or callable, default="euclidean"
+        Metric to use for distance computation. Any metric from scikit-learn
         or scipy.spatial.distance can be used.

         If metric is a callable function, it is called on each
@@ -506,20 +757,15 @@
         See the documentation for scipy.spatial.distance for details on these
         metrics.

-    batch_size : integer
-        .. deprecated:: 0.20
-            Deprecated for removal in 0.22.
-            Use sklearn.set_config(working_memory=...) instead.
-
-    metric_kwargs : dict
-        keyword arguments to pass to specified metric function.
+    metric_kwargs : dict, default=None
+        Keyword arguments to pass to specified metric function.

     Returns
     -------
     argmin : numpy.ndarray
         Y[argmin[i], :] is the row in Y that is closest to X[i, :].

-    See also
+    See Also
     --------
     sklearn.metrics.pairwise_distances
     sklearn.metrics.pairwise_distances_argmin_min
@@ -527,18 +773,58 @@
     if metric_kwargs is None:
         metric_kwargs = {}

-    return pairwise_distances_argmin_min(X, Y, axis, metric,
-                                         metric_kwargs=metric_kwargs,
-                                         batch_size=batch_size)[0]
+    X, Y = check_pairwise_arrays(X, Y)
+
+    if axis == 0:
+        X, Y = Y, X
+
+    if metric_kwargs is None:
+        metric_kwargs = {}
+
+    if PairwiseDistancesArgKmin.is_usable_for(X, Y, metric):
+        # This is an adaptor for one "sqeuclidean" specification.
+        # For this backend, we can directly use "sqeuclidean".
+        if metric_kwargs.get("squared", False) and metric == "euclidean":
+            metric = "sqeuclidean"
+            metric_kwargs = {}
+
+        indices = PairwiseDistancesArgKmin.compute(
+            X=X,
+            Y=Y,
+            k=1,
+            metric=metric,
+            metric_kwargs=metric_kwargs,
+            strategy="auto",
+            return_distance=False,
+        )
+        indices = indices.flatten()
+    else:
+        # TODO: once PairwiseDistancesArgKmin supports sparse input matrices and 32 bit,
+        # we won't need to fallback to pairwise_distances_chunked anymore.
+
+        # Turn off check for finiteness because this is costly and because arrays
+        # have already been validated.
+        with config_context(assume_finite=True):
+            indices = np.concatenate(
+                list(
+                    # This returns a np.ndarray generator whose arrays we need
+                    # to flatten into one.
+                    pairwise_distances_chunked(
+                        X, Y, reduce_func=_argmin_reduce, metric=metric, **metric_kwargs
+                    )
+                )
+            )
+
+    return indices


 def haversine_distances(X, Y=None):
-    """Compute the Haversine distance between samples in X and Y
+    """Compute the Haversine distance between samples in X and Y.

     The Haversine (or great circle) distance is the angular distance between
-    two points on the surface of a sphere. The first distance of each point is
-    assumed to be the latitude, the second is the longitude, given in radians.
-    The dimension of the data must be 2.
+    two points on the surface of a sphere. The first coordinate of each point
+    is assumed to be the latitude, the second is the longitude, given
+    in radians. The dimension of the data must be 2.

     .. math::
        D(x, y) = 2\\arcsin[\\sqrt{\\sin^2((x1 - y1) / 2)
@@ -546,13 +832,16 @@

     Parameters
     ----------
-    X : array_like, shape (n_samples_1, 2)
-
-    Y : array_like, shape (n_samples_2, 2), optional
-
-    Returns
-    -------
-    distance : {array}, shape (n_samples_1, n_samples_2)
+    X : array-like of shape (n_samples_X, 2)
+        A feature array.
+
+    Y : array-like of shape (n_samples_Y, 2), default=None
+        An optional second feature array. If `None`, uses `Y=X`.
+
+    Returns
+    -------
+    distance : ndarray of shape (n_samples_X, n_samples_Y)
+        The distance matrix.

     Notes
     -----
@@ -563,22 +852,27 @@
     Examples
     --------
     We want to calculate the distance between the Ezeiza Airport
-    (Buenos Aires, Argentina) and the Charles de Gaulle Airport (Paris, France)
+    (Buenos Aires, Argentina) and the Charles de Gaulle Airport (Paris,
+    France).

     >>> from sklearn.metrics.pairwise import haversine_distances
+    >>> from math import radians
     >>> bsas = [-34.83333, -58.5166646]
     >>> paris = [49.0083899664, 2.53844117956]
-    >>> result = haversine_distances([bsas, paris])
+    >>> bsas_in_radians = [radians(_) for _ in bsas]
+    >>> paris_in_radians = [radians(_) for _ in paris]
+    >>> result = haversine_distances([bsas_in_radians, paris_in_radians])
     >>> result * 6371000/1000  # multiply by Earth radius to get kilometers
-    array([[    0.        , 11279.45379464],
-           [11279.45379464,     0.        ]])
-    """
-    from sklearn.neighbors import DistanceMetric
-    return DistanceMetric.get_metric('haversine').pairwise(X, Y)
-
-
-def manhattan_distances(X, Y=None, sum_over_features=True):
-    """ Compute the L1 distances between the vectors in X and Y.
+    array([[    0.        , 11099.54035582],
+           [11099.54035582,     0.        ]])
+    """
+    from ..metrics import DistanceMetric
+
+    return DistanceMetric.get_metric("haversine").pairwise(X, Y)
+
+
+def manhattan_distances(X, Y=None, *, sum_over_features=True):
+    """Compute the L1 distances between the vectors in X and Y.

     With sum_over_features equal to False it returns the componentwise
     distances.
@@ -587,11 +881,12 @@

     Parameters
     ----------
-    X : array_like
-        An array with shape (n_samples_X, n_features).
-
-    Y : array_like, optional
-        An array with shape (n_samples_Y, n_features).
+    X : array-like of shape (n_samples_X, n_features)
+        An array where each row is a sample and each column is a feature.
+
+    Y : array-like of shape (n_samples_Y, n_features), default=None
+        An array where each row is a sample and each column is a feature.
+        If `None`, method uses `Y=X`.

     sum_over_features : bool, default=True
         If True the function returns the pairwise distance matrix
@@ -600,30 +895,37 @@

     Returns
     -------
-    D : array
+    D : ndarray of shape (n_samples_X * n_samples_Y, n_features) or \
+            (n_samples_X, n_samples_Y)
         If sum_over_features is False shape is
         (n_samples_X * n_samples_Y, n_features) and D contains the
         componentwise L1 pairwise-distances (ie. absolute difference),
         else shape is (n_samples_X, n_samples_Y) and D contains
         the pairwise L1 distances.

+    Notes
+    -----
+    When X and/or Y are CSR sparse matrices and they are not already
+    in canonical format, this function modifies them in-place to
+    make them canonical.
+
     Examples
     --------
     >>> from sklearn.metrics.pairwise import manhattan_distances
-    >>> manhattan_distances([[3]], [[3]])#doctest:+ELLIPSIS
+    >>> manhattan_distances([[3]], [[3]])
     array([[0.]])
-    >>> manhattan_distances([[3]], [[2]])#doctest:+ELLIPSIS
+    >>> manhattan_distances([[3]], [[2]])
     array([[1.]])
-    >>> manhattan_distances([[2]], [[3]])#doctest:+ELLIPSIS
+    >>> manhattan_distances([[2]], [[3]])
     array([[1.]])
     >>> manhattan_distances([[1, 2], [3, 4]],\
-         [[1, 2], [0, 3]])#doctest:+ELLIPSIS
+         [[1, 2], [0, 3]])
     array([[0., 2.],
            [4., 4.]])
     >>> import numpy as np
     >>> X = np.ones((1, 2))
     >>> y = np.full((2, 2), 2.)
-    >>> manhattan_distances(X, y, sum_over_features=False)#doctest:+ELLIPSIS
+    >>> manhattan_distances(X, y, sum_over_features=False)
     array([[1., 1.],
            [1., 1.]])
     """
@@ -631,19 +933,21 @@

     if issparse(X) or issparse(Y):
         if not sum_over_features:
-            raise TypeError("sum_over_features=%r not supported"
-                            " for sparse matrices" % sum_over_features)
+            raise TypeError(
+                "sum_over_features=%r not supported for sparse matrices"
+                % sum_over_features
+            )

         X = csr_matrix(X, copy=False)
         Y = csr_matrix(Y, copy=False)
+        X.sum_duplicates()  # this also sorts indices in-place
+        Y.sum_duplicates()
         D = np.zeros((X.shape[0], Y.shape[0]))
-        _sparse_manhattan(X.data, X.indices, X.indptr,
-                          Y.data, Y.indices, Y.indptr,
-                          X.shape[1], D)
+        _sparse_manhattan(X.data, X.indices, X.indptr, Y.data, Y.indices, Y.indptr, D)
         return D

     if sum_over_features:
-        return distance.cdist(X, Y, 'cityblock')
+        return distance.cdist(X, Y, "cityblock")

     D = X[:, np.newaxis, :] - Y[np.newaxis, :, :]
     D = np.abs(D, D)
@@ -659,21 +963,21 @@

     Parameters
     ----------
-    X : array_like, sparse matrix
-        with shape (n_samples_X, n_features).
-
-    Y : array_like, sparse matrix (optional)
-        with shape (n_samples_Y, n_features).
-
-    Returns
-    -------
-    distance matrix : array
-        An array with shape (n_samples_X, n_samples_Y).
-
-    See also
+    X : {array-like, sparse matrix} of shape (n_samples_X, n_features)
+        Matrix `X`.
+
+    Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features), \
+            default=None
+        Matrix `Y`.
+
+    Returns
+    -------
+    distance matrix : ndarray of shape (n_samples_X, n_samples_Y)
+
+    See Also
     --------
-    sklearn.metrics.pairwise.cosine_similarity
-    scipy.spatial.distance.cosine : dense matrices only
+    cosine_similarity
+    scipy.spatial.distance.cosine : Dense matrices only.
     """
     # 1.0 - cosine_similarity(X, Y) without copy
     S = cosine_similarity(X, Y)
@@ -689,20 +993,23 @@

 # Paired distances
 def paired_euclidean_distances(X, Y):
-    """
-    Computes the paired euclidean distances between X and Y
+    """Compute the paired euclidean distances between X and Y.

     Read more in the :ref:`User Guide <metrics>`.

     Parameters
     ----------
-    X : array-like, shape (n_samples, n_features)
-
-    Y : array-like, shape (n_samples, n_features)
-
-    Returns
-    -------
-    distances : ndarray (n_samples, )
+    X : array-like of shape (n_samples, n_features)
+        Input array/matrix X.
+
+    Y : array-like of shape (n_samples, n_features)
+        Input array/matrix Y.
+
+    Returns
+    -------
+    distances : ndarray of shape (n_samples,)
+        Output array/matrix containing the calculated paired euclidean
+        distances.
     """
     X, Y = check_paired_arrays(X, Y)
     return row_norms(X - Y)
@@ -715,13 +1022,13 @@

     Parameters
     ----------
-    X : array-like, shape (n_samples, n_features)
-
-    Y : array-like, shape (n_samples, n_features)
-
-    Returns
-    -------
-    distances : ndarray (n_samples, )
+    X : array-like of shape (n_samples, n_features)
+
+    Y : array-like of shape (n_samples, n_features)
+
+    Returns
+    -------
+    distances : ndarray of shape (n_samples,)
     """
     X, Y = check_paired_arrays(X, Y)
     diff = X - Y
@@ -734,67 +1041,82 @@

 def paired_cosine_distances(X, Y):
     """
-    Computes the paired cosine distances between X and Y
+    Compute the paired cosine distances between X and Y.

     Read more in the :ref:`User Guide <metrics>`.

     Parameters
     ----------
-    X : array-like, shape (n_samples, n_features)
-
-    Y : array-like, shape (n_samples, n_features)
-
-    Returns
-    -------
-    distances : ndarray, shape (n_samples, )
+    X : array-like of shape (n_samples, n_features)
+        An array where each row is a sample and each column is a feature.
+
+    Y : array-like of shape (n_samples, n_features)
+        An array where each row is a sample and each column is a feature.
+
+    Returns
+    -------
+    distances : ndarray of shape (n_samples,)
+        Returns the distances between the row vectors of `X`
+        and the row vectors of `Y`, where `distances[i]` is the
+        distance between `X[i]` and `Y[i]`.

     Notes
     -----
     The cosine distance is equivalent to the half the squared
-    euclidean distance if each sample is normalized to unit norm
+    euclidean distance if each sample is normalized to unit norm.
     """
     X, Y = check_paired_arrays(X, Y)
-    return .5 * row_norms(normalize(X) - normalize(Y), squared=True)
+    return 0.5 * row_norms(normalize(X) - normalize(Y), squared=True)


 PAIRED_DISTANCES = {
-    'cosine': paired_cosine_distances,
-    'euclidean': paired_euclidean_distances,
-    'l2': paired_euclidean_distances,
-    'l1': paired_manhattan_distances,
-    'manhattan': paired_manhattan_distances,
-    'cityblock': paired_manhattan_distances}
-
-
-def paired_distances(X, Y, metric="euclidean", **kwds):
-    """
-    Computes the paired distances between X and Y.
-
-    Computes the distances between (X[0], Y[0]), (X[1], Y[1]), etc...
+    "cosine": paired_cosine_distances,
+    "euclidean": paired_euclidean_distances,
+    "l2": paired_euclidean_distances,
+    "l1": paired_manhattan_distances,
+    "manhattan": paired_manhattan_distances,
+    "cityblock": paired_manhattan_distances,
+}
+
+
+def paired_distances(X, Y, *, metric="euclidean", **kwds):
+    """
+    Compute the paired distances between X and Y.
+
+    Compute the distances between (X[0], Y[0]), (X[1], Y[1]), etc...

     Read more in the :ref:`User Guide <metrics>`.

     Parameters
     ----------
-    X : ndarray (n_samples, n_features)
+    X : ndarray of shape (n_samples, n_features)
         Array 1 for distance computation.

-    Y : ndarray (n_samples, n_features)
+    Y : ndarray of shape (n_samples, n_features)
         Array 2 for distance computation.

-    metric : string or callable
+    metric : str or callable, default="euclidean"
         The metric to use when calculating distance between instances in a
         feature array. If metric is a string, it must be one of the options
         specified in PAIRED_DISTANCES, including "euclidean",
         "manhattan", or "cosine".
         Alternatively, if metric is a callable function, it is called on each
         pair of instances (rows) and the resulting value recorded. The callable
-        should take two arrays from X as input and return a value indicating
+        should take two arrays from `X` as input and return a value indicating
         the distance between them.

-    Returns
-    -------
-    distances : ndarray (n_samples, )
+    **kwds : dict
+        Unused parameters.
+
+    Returns
+    -------
+    distances : ndarray of shape (n_samples,)
+        Returns the distances between the row vectors of `X`
+        and the row vectors of `Y`.
+
+    See Also
+    --------
+    pairwise_distances : Computes the distance between every pair of samples.

     Examples
     --------
@@ -803,10 +1125,6 @@
     >>> Y = [[0, 1], [2, 1]]
     >>> paired_distances(X, Y)
     array([0., 1.])
-
-    See also
-    --------
-    pairwise_distances : Computes the distance between every pair of samples
     """

     if metric in PAIRED_DISTANCES:
@@ -820,7 +1138,7 @@
             distances[i] = metric(X[i], Y[i])
         return distances
     else:
-        raise ValueError('Unknown distance %s' % metric)
+        raise ValueError("Unknown distance %s" % metric)


 # Kernels
@@ -832,11 +1150,13 @@

     Parameters
     ----------
-    X : array of shape (n_samples_1, n_features)
-
-    Y : array of shape (n_samples_2, n_features)
-
-    dense_output : boolean (optional), default True
+    X : ndarray of shape (n_samples_X, n_features)
+        A feature array.
+
+    Y : ndarray of shape (n_samples_Y, n_features), default=None
+        An optional second feature array. If `None`, uses `Y=X`.
+
+    dense_output : bool, default=True
         Whether to return dense output even when the input is sparse. If
         ``False``, the output is sparse if both input arrays are sparse.

@@ -844,7 +1164,8 @@

     Returns
     -------
-    Gram matrix : array of shape (n_samples_1, n_samples_2)
+    Gram matrix : ndarray of shape (n_samples_X, n_samples_Y)
+        The Gram matrix of the linear kernel, i.e. `X @ Y.T`.
     """
     X, Y = check_pairwise_arrays(X, Y)
     return safe_sparse_dot(X, Y.T, dense_output=dense_output)
@@ -860,20 +1181,20 @@

     Parameters
     ----------
-    X : ndarray of shape (n_samples_1, n_features)
-
-    Y : ndarray of shape (n_samples_2, n_features)
-
-    degree : int, default 3
-
-    gamma : float, default None
-        if None, defaults to 1.0 / n_features
-
-    coef0 : float, default 1
-
-    Returns
-    -------
-    Gram matrix : array of shape (n_samples_1, n_samples_2)
+    X : ndarray of shape (n_samples_X, n_features)
+
+    Y : ndarray of shape (n_samples_Y, n_features), default=None
+
+    degree : int, default=3
+
+    gamma : float, default=None
+        If None, defaults to 1.0 / n_features.
+
+    coef0 : float, default=1
+
+    Returns
+    -------
+    Gram matrix : ndarray of shape (n_samples_X, n_samples_Y)
     """
     X, Y = check_pairwise_arrays(X, Y)
     if gamma is None:
@@ -896,18 +1217,19 @@

     Parameters
     ----------
-    X : ndarray of shape (n_samples_1, n_features)
-
-    Y : ndarray of shape (n_samples_2, n_features)
-
-    gamma : float, default None
-        If None, defaults to 1.0 / n_features
-
-    coef0 : float, default 1
-
-    Returns
-    -------
-    Gram matrix : array of shape (n_samples_1, n_samples_2)
+    X : ndarray of shape (n_samples_X, n_features)
+
+    Y : ndarray of shape (n_samples_Y, n_features), default=None
+        If `None`, uses `Y=X`.
+
+    gamma : float, default=None
+        If None, defaults to 1.0 / n_features.
+
+    coef0 : float, default=1
+
+    Returns
+    -------
+    Gram matrix : ndarray of shape (n_samples_X, n_samples_Y)
     """
     X, Y = check_pairwise_arrays(X, Y)
     if gamma is None:
@@ -916,7 +1238,7 @@
     K = safe_sparse_dot(X, Y.T, dense_output=True)
     K *= gamma
     K += coef0
-    np.tanh(K, K)   # compute tanh in-place
+    np.tanh(K, K)  # compute tanh in-place
     return K


@@ -932,16 +1254,17 @@

     Parameters
     ----------
-    X : array of shape (n_samples_X, n_features)
-
-    Y : array of shape (n_samples_Y, n_features)
-
-    gamma : float, default None
-        If None, defaults to 1.0 / n_features
-
-    Returns
-    -------
-    kernel_matrix : array of shape (n_samples_X, n_samples_Y)
+    X : ndarray of shape (n_samples_X, n_features)
+
+    Y : ndarray of shape (n_samples_Y, n_features), default=None
+        If `None`, uses `Y=X`.
+
+    gamma : float, default=None
+        If None, defaults to 1.0 / n_features.
+
+    Returns
+    -------
+    kernel_matrix : ndarray of shape (n_samples_X, n_samples_Y)
     """
     X, Y = check_pairwise_arrays(X, Y)
     if gamma is None:
@@ -949,7 +1272,7 @@

     K = euclidean_distances(X, Y, squared=True)
     K *= -gamma
-    np.exp(K, K)    # exponentiate K in-place
+    np.exp(K, K)  # exponentiate K in-place
     return K


@@ -967,23 +1290,26 @@

     Parameters
     ----------
-    X : array of shape (n_samples_X, n_features)
-
-    Y : array of shape (n_samples_Y, n_features)
-
-    gamma : float, default None
-        If None, defaults to 1.0 / n_features
-
-    Returns
-    -------
-    kernel_matrix : array of shape (n_samples_X, n_samples_Y)
+    X : ndarray of shape (n_samples_X, n_features)
+        A feature array.
+
+    Y : ndarray of shape (n_samples_Y, n_features), default=None
+        An optional second feature array. If `None`, uses `Y=X`.
+
+    gamma : float, default=None
+        If None, defaults to 1.0 / n_features.
+
+    Returns
+    -------
+    kernel_matrix : ndarray of shape (n_samples_X, n_samples_Y)
+        The kernel matrix.
     """
     X, Y = check_pairwise_arrays(X, Y)
     if gamma is None:
         gamma = 1.0 / X.shape[1]

     K = -gamma * manhattan_distances(X, Y)
-    np.exp(K, K)    # exponentiate K in-place
+    np.exp(K, K)  # exponentiate K in-place
     return K


@@ -1001,14 +1327,15 @@

     Parameters
     ----------
-    X : ndarray or sparse array, shape: (n_samples_X, n_features)
+    X : {ndarray, sparse matrix} of shape (n_samples_X, n_features)
         Input data.

-    Y : ndarray or sparse array, shape: (n_samples_Y, n_features)
+    Y : {ndarray, sparse matrix} of shape (n_samples_Y, n_features), \
+            default=None
         Input data. If ``None``, the output will be the pairwise
         similarities between all samples in ``X``.

-    dense_output : boolean (optional), default True
+    dense_output : bool, default=True
         Whether to return dense output even when the input is sparse. If
         ``False``, the output is sparse if both input arrays are sparse.

@@ -1017,8 +1344,7 @@

     Returns
     -------
-    kernel matrix : array
-        An array with shape (n_samples_X, n_samples_Y).
+    kernel matrix : ndarray of shape (n_samples_X, n_samples_Y)
     """
     # to avoid recursive import

@@ -1030,14 +1356,14 @@
     else:
         Y_normalized = normalize(Y, copy=True)

-    K = safe_sparse_dot(X_normalized, Y_normalized.T,
-                        dense_output=dense_output)
+    K = safe_sparse_dot(X_normalized, Y_normalized.T, dense_output=dense_output)

     return K


 def additive_chi2_kernel(X, Y=None):
-    """Computes the additive chi-squared kernel between observations in X and Y
+    """Computes the additive chi-squared kernel between observations in X and
+    Y.

     The chi-squared kernel is computed between each pair of rows in X and Y.  X
     and Y have to be non-negative. This kernel is most commonly applied to
@@ -1061,11 +1387,19 @@
     ----------
     X : array-like of shape (n_samples_X, n_features)

-    Y : array of shape (n_samples_Y, n_features)
-
-    Returns
-    -------
-    kernel_matrix : array of shape (n_samples_X, n_samples_Y)
+    Y : ndarray of shape (n_samples_Y, n_features), default=None
+        If `None`, uses `Y=X`.
+
+    Returns
+    -------
+    kernel_matrix : ndarray of shape (n_samples_X, n_samples_Y)
+
+    See Also
+    --------
+    chi2_kernel : The exponentiated version of the kernel, which is usually
+        preferable.
+    sklearn.kernel_approximation.AdditiveChi2Sampler : A Fourier approximation
+        to this kernel.

     References
     ----------
@@ -1074,15 +1408,6 @@
       categories: A comprehensive study
       International Journal of Computer Vision 2007
       https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf
-
-
-    See also
-    --------
-    chi2_kernel : The exponentiated version of the kernel, which is usually
-        preferable.
-
-    sklearn.kernel_approximation.AdditiveChi2Sampler : A Fourier approximation
-        to this kernel.
     """
     if issparse(X) or issparse(Y):
         raise ValueError("additive_chi2 does not support sparse matrices.")
@@ -1097,7 +1422,7 @@
     return result


-def chi2_kernel(X, Y=None, gamma=1.):
+def chi2_kernel(X, Y=None, gamma=1.0):
     """Computes the exponential chi-squared kernel X and Y.

     The chi-squared kernel is computed between each pair of rows in X and Y.  X
@@ -1116,14 +1441,20 @@
     ----------
     X : array-like of shape (n_samples_X, n_features)

-    Y : array of shape (n_samples_Y, n_features)
+    Y : ndarray of shape (n_samples_Y, n_features), default=None

     gamma : float, default=1.
         Scaling parameter of the chi2 kernel.

     Returns
     -------
-    kernel_matrix : array of shape (n_samples_X, n_samples_Y)
+    kernel_matrix : ndarray of shape (n_samples_X, n_samples_Y)
+
+    See Also
+    --------
+    additive_chi2_kernel : The additive version of this kernel.
+    sklearn.kernel_approximation.AdditiveChi2Sampler : A Fourier approximation
+        to the additive version of this kernel.

     References
     ----------
@@ -1132,13 +1463,6 @@
       categories: A comprehensive study
       International Journal of Computer Vision 2007
       https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf
-
-    See also
-    --------
-    additive_chi2_kernel : The additive version of this kernel
-
-    sklearn.kernel_approximation.AdditiveChi2Sampler : A Fourier approximation
-        to the additive version of this kernel.
     """
     K = additive_chi2_kernel(X, Y)
     K *= gamma
@@ -1149,14 +1473,15 @@
 PAIRWISE_DISTANCE_FUNCTIONS = {
     # If updating this dictionary, update the doc in both distance_metrics()
     # and also in pairwise_distances()!
-    'cityblock': manhattan_distances,
-    'cosine': cosine_distances,
-    'euclidean': euclidean_distances,
-    'haversine': haversine_distances,
-    'l2': euclidean_distances,
-    'l1': manhattan_distances,
-    'manhattan': manhattan_distances,
-    'precomputed': None,  # HACK: precomputed is always allowed, never called
+    "cityblock": manhattan_distances,
+    "cosine": cosine_distances,
+    "euclidean": euclidean_distances,
+    "haversine": haversine_distances,
+    "l2": euclidean_distances,
+    "l1": manhattan_distances,
+    "manhattan": manhattan_distances,
+    "precomputed": None,  # HACK: precomputed is always allowed, never called
+    "nan_euclidean": nan_euclidean_distances,
 }


@@ -1169,17 +1494,18 @@

     The valid distance metrics, and the function they map to, are:

-    ============   ====================================
-    metric         Function
-    ============   ====================================
-    'cityblock'    metrics.pairwise.manhattan_distances
-    'cosine'       metrics.pairwise.cosine_distances
-    'euclidean'    metrics.pairwise.euclidean_distances
-    'haversine'    metrics.pairwise.haversine_distances
-    'l1'           metrics.pairwise.manhattan_distances
-    'l2'           metrics.pairwise.euclidean_distances
-    'manhattan'    metrics.pairwise.manhattan_distances
-    ============   ====================================
+    =============== ========================================
+    metric          Function
+    =============== ========================================
+    'cityblock'     metrics.pairwise.manhattan_distances
+    'cosine'        metrics.pairwise.cosine_distances
+    'euclidean'     metrics.pairwise.euclidean_distances
+    'haversine'     metrics.pairwise.haversine_distances
+    'l1'            metrics.pairwise.manhattan_distances
+    'l2'            metrics.pairwise.euclidean_distances
+    'manhattan'     metrics.pairwise.manhattan_distances
+    'nan_euclidean' metrics.pairwise.nan_euclidean_distances
+    =============== ========================================

     Read more in the :ref:`User Guide <metrics>`.

@@ -1188,38 +1514,44 @@


 def _dist_wrapper(dist_func, dist_matrix, slice_, *args, **kwargs):
-    """Write in-place to a slice of a distance matrix"""
+    """Write in-place to a slice of a distance matrix."""
     dist_matrix[:, slice_] = dist_func(*args, **kwargs)


 def _parallel_pairwise(X, Y, func, n_jobs, **kwds):
     """Break the pairwise matrix in n_jobs even slices
-    and compute them in parallel"""
+    and compute them in parallel."""

     if Y is None:
         Y = X
+    X, Y, dtype = _return_float_dtype(X, Y)

     if effective_n_jobs(n_jobs) == 1:
         return func(X, Y, **kwds)

     # enforce a threading backend to prevent data communication overhead
     fd = delayed(_dist_wrapper)
-    ret = np.empty((X.shape[0], Y.shape[0]), dtype=X.dtype, order='F')
+    ret = np.empty((X.shape[0], Y.shape[0]), dtype=dtype, order="F")
     Parallel(backend="threading", n_jobs=n_jobs)(
         fd(func, ret, s, X, Y[s], **kwds)
-        for s in gen_even_slices(_num_samples(Y), effective_n_jobs(n_jobs)))
+        for s in gen_even_slices(_num_samples(Y), effective_n_jobs(n_jobs))
+    )
+
+    if (X is Y or Y is None) and func is euclidean_distances:
+        # zeroing diagonal for euclidean norm.
+        # TODO: do it also for other norms.
+        np.fill_diagonal(ret, 0)

     return ret


-def _pairwise_callable(X, Y, metric, **kwds):
-    """Handle the callable case for pairwise_{distances,kernels}
-    """
-    X, Y = check_pairwise_arrays(X, Y)
+def _pairwise_callable(X, Y, metric, force_all_finite=True, **kwds):
+    """Handle the callable case for pairwise_{distances,kernels}."""
+    X, Y = check_pairwise_arrays(X, Y, force_all_finite=force_all_finite)

     if X is Y:
         # Only calculate metric for upper triangle
-        out = np.zeros((X.shape[0], Y.shape[0]), dtype='float')
+        out = np.zeros((X.shape[0], Y.shape[0]), dtype="float")
         iterator = itertools.combinations(range(X.shape[0]), 2)
         for i, j in iterator:
             out[i, j] = metric(X[i], Y[j], **kwds)
@@ -1236,7 +1568,7 @@

     else:
         # Calculate all cells
-        out = np.empty((X.shape[0], Y.shape[0]), dtype='float')
+        out = np.empty((X.shape[0], Y.shape[0]), dtype="float")
         iterator = itertools.product(range(X.shape[0]), range(Y.shape[0]))
         for i, j in iterator:
             out[i, j] = metric(X[i], Y[j], **kwds)
@@ -1244,56 +1576,97 @@
     return out


-_VALID_METRICS = ['euclidean', 'l2', 'l1', 'manhattan', 'cityblock',
-                  'braycurtis', 'canberra', 'chebyshev', 'correlation',
-                  'cosine', 'dice', 'hamming', 'jaccard', 'kulsinski',
-                  'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',
-                  'russellrao', 'seuclidean', 'sokalmichener',
-                  'sokalsneath', 'sqeuclidean', 'yule', 'wminkowski',
-                  'haversine']
+_VALID_METRICS = [
+    "euclidean",
+    "l2",
+    "l1",
+    "manhattan",
+    "cityblock",
+    "braycurtis",
+    "canberra",
+    "chebyshev",
+    "correlation",
+    "cosine",
+    "dice",
+    "hamming",
+    "jaccard",
+    "kulsinski",
+    "mahalanobis",
+    "matching",
+    "minkowski",
+    "rogerstanimoto",
+    "russellrao",
+    "seuclidean",
+    "sokalmichener",
+    "sokalsneath",
+    "sqeuclidean",
+    "yule",
+    "wminkowski",
+    "nan_euclidean",
+    "haversine",
+]
+
+_NAN_METRICS = ["nan_euclidean"]


 def _check_chunk_size(reduced, chunk_size):
-    """Checks chunk is a sequence of expected size or a tuple of same
-    """
+    """Checks chunk is a sequence of expected size or a tuple of same."""
+    if reduced is None:
+        return
     is_tuple = isinstance(reduced, tuple)
     if not is_tuple:
         reduced = (reduced,)
-    if any(isinstance(r, tuple) or not hasattr(r, '__iter__')
-           for r in reduced):
-        raise TypeError('reduce_func returned %r. '
-                        'Expected sequence(s) of length %d.' %
-                        (reduced if is_tuple else reduced[0], chunk_size))
+    if any(isinstance(r, tuple) or not hasattr(r, "__iter__") for r in reduced):
+        raise TypeError(
+            "reduce_func returned %r. Expected sequence(s) of length %d."
+            % (reduced if is_tuple else reduced[0], chunk_size)
+        )
     if any(_num_samples(r) != chunk_size for r in reduced):
         actual_size = tuple(_num_samples(r) for r in reduced)
-        raise ValueError('reduce_func returned object of length %s. '
-                         'Expected same length as input: %d.' %
-                         (actual_size if is_tuple else actual_size[0],
-                          chunk_size))
+        raise ValueError(
+            "reduce_func returned object of length %s. "
+            "Expected same length as input: %d."
+            % (actual_size if is_tuple else actual_size[0], chunk_size)
+        )


 def _precompute_metric_params(X, Y, metric=None, **kwds):
-    """Precompute data-derived metric parameters if not provided
-    """
-    if metric == "seuclidean" and 'V' not in kwds:
+    """Precompute data-derived metric parameters if not provided."""
+    if metric == "seuclidean" and "V" not in kwds:
+        # There is a bug in scipy < 1.5 that will cause a crash if
+        # X.dtype != np.double (float64). See PR #15730
+        dtype = np.float64 if sp_version < parse_version("1.5") else None
         if X is Y:
-            V = np.var(X, axis=0, ddof=1)
+            V = np.var(X, axis=0, ddof=1, dtype=dtype)
         else:
-            V = np.var(np.vstack([X, Y]), axis=0, ddof=1)
-        return {'V': V}
-    if metric == "mahalanobis" and 'VI' not in kwds:
+            raise ValueError(
+                "The 'V' parameter is required for the seuclidean metric "
+                "when Y is passed."
+            )
+        return {"V": V}
+    if metric == "mahalanobis" and "VI" not in kwds:
         if X is Y:
             VI = np.linalg.inv(np.cov(X.T)).T
         else:
-            VI = np.linalg.inv(np.cov(np.vstack([X, Y]).T)).T
-        return {'VI': VI}
+            raise ValueError(
+                "The 'VI' parameter is required for the mahalanobis metric "
+                "when Y is passed."
+            )
+        return {"VI": VI}
     return {}


-def pairwise_distances_chunked(X, Y=None, reduce_func=None,
-                               metric='euclidean', n_jobs=None,
-                               working_memory=None, **kwds):
-    """Generate a distance matrix chunk by chunk with optional reduction
+def pairwise_distances_chunked(
+    X,
+    Y=None,
+    *,
+    reduce_func=None,
+    metric="euclidean",
+    n_jobs=None,
+    working_memory=None,
+    **kwds,
+):
+    """Generate a distance matrix chunk by chunk with optional reduction.

     In cases where not all of a pairwise distance matrix needs to be stored at
     once, this is used to calculate pairwise distances in
@@ -1303,26 +1676,29 @@

     Parameters
     ----------
-    X : array [n_samples_a, n_samples_a] if metric == "precomputed", or,
-        [n_samples_a, n_features] otherwise
+    X : ndarray of shape (n_samples_X, n_samples_X) or \
+            (n_samples_X, n_features)
         Array of pairwise distances between samples, or a feature array.
-
-    Y : array [n_samples_b, n_features], optional
+        The shape the array should be (n_samples_X, n_samples_X) if
+        metric='precomputed' and (n_samples_X, n_features) otherwise.
+
+    Y : ndarray of shape (n_samples_Y, n_features), default=None
         An optional second feature array. Only allowed if
         metric != "precomputed".

-    reduce_func : callable, optional
+    reduce_func : callable, default=None
         The function which is applied on each chunk of the distance matrix,
         reducing it to needed values.  ``reduce_func(D_chunk, start)``
         is called repeatedly, where ``D_chunk`` is a contiguous vertical
         slice of the pairwise distance matrix, starting at row ``start``.
-        It should return an array, a list, or a sparse matrix of length
-        ``D_chunk.shape[0]``, or a tuple of such objects.
+        It should return one of: None; an array, a list, or a sparse matrix
+        of length ``D_chunk.shape[0]``; or a tuple of such objects. Returning
+        None is useful for in-place operations, rather than reductions.

         If None, pairwise_distances_chunked returns a generator of vertical
         chunks of the distance matrix.

-    metric : string, or callable
+    metric : str or callable, default='euclidean'
         The metric to use when calculating distance between instances in a
         feature array. If metric is a string, it must be one of the options
         allowed by scipy.spatial.distance.pdist for its metric parameter, or
@@ -1333,7 +1709,7 @@
         should take two arrays from X as input and return a value indicating
         the distance between them.

-    n_jobs : int or None, optional (default=None)
+    n_jobs : int, default=None
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
@@ -1342,7 +1718,7 @@
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.

-    working_memory : int, optional
+    working_memory : int, default=None
         The sought maximum memory for temporary distance matrix chunks.
         When None (default), the value of
         ``sklearn.get_config()['working_memory']`` is used.
@@ -1354,7 +1730,7 @@

     Yields
     ------
-    D_chunk : array or sparse matrix
+    D_chunk : {ndarray, sparse matrix}
         A contiguous slice of distance matrix, optionally processed by
         ``reduce_func``.

@@ -1366,7 +1742,7 @@
     >>> from sklearn.metrics import pairwise_distances_chunked
     >>> X = np.random.RandomState(0).rand(5, 3)
     >>> D_chunk = next(pairwise_distances_chunked(X))
-    >>> D_chunk  # doctest: +ELLIPSIS
+    >>> D_chunk
     array([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],
            [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],
            [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],
@@ -1384,7 +1760,7 @@
     >>> neigh, avg_dist = next(gen)
     >>> neigh
     [array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]
-    >>> avg_dist  # doctest: +ELLIPSIS
+    >>> avg_dist
     array([0.039..., 0.        , 0.        , 0.039..., 0.        ])

     Where r is defined per sample, we need to make use of ``start``:
@@ -1408,7 +1784,7 @@
     [array([0, 1])]
     """
     n_samples_X = _num_samples(X)
-    if metric == 'precomputed':
+    if metric == "precomputed":
         slices = (slice(0, n_samples_X),)
     else:
         if Y is None:
@@ -1422,9 +1798,11 @@
         #  - this does not account for any temporary memory usage while
         #    calculating distances (e.g. difference of vectors in manhattan
         #    distance.
-        chunk_n_rows = get_chunk_n_rows(row_bytes=8 * _num_samples(Y),
-                                        max_n_rows=n_samples_X,
-                                        working_memory=working_memory)
+        chunk_n_rows = get_chunk_n_rows(
+            row_bytes=8 * _num_samples(Y),
+            max_n_rows=n_samples_X,
+            working_memory=working_memory,
+        )
         slices = gen_batches(n_samples_X, chunk_n_rows)

     # precompute data-derived metric params
@@ -1436,14 +1814,13 @@
             X_chunk = X  # enable optimised paths for X is Y
         else:
             X_chunk = X[sl]
-        D_chunk = pairwise_distances(X_chunk, Y, metric=metric,
-                                     n_jobs=n_jobs, **kwds)
-        if ((X is Y or Y is None)
-                and PAIRWISE_DISTANCE_FUNCTIONS.get(metric, None)
-                is euclidean_distances):
+        D_chunk = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwds)
+        if (X is Y or Y is None) and PAIRWISE_DISTANCE_FUNCTIONS.get(
+            metric, None
+        ) is euclidean_distances:
             # zeroing diagonal, taking care of aliases of "euclidean",
             # i.e. "l2"
-            D_chunk.flat[sl.start::_num_samples(X) + 1] = 0
+            D_chunk.flat[sl.start :: _num_samples(X) + 1] = 0
         if reduce_func is not None:
             chunk_size = D_chunk.shape[0]
             D_chunk = reduce_func(D_chunk, sl.start)
@@ -1451,8 +1828,10 @@
         yield D_chunk


-def pairwise_distances(X, Y=None, metric="euclidean", n_jobs=None, **kwds):
-    """ Compute the distance matrix from a vector array X and optional Y.
+def pairwise_distances(
+    X, Y=None, metric="euclidean", *, n_jobs=None, force_all_finite=True, **kwds
+):
+    """Compute the distance matrix from a vector array X and optional Y.

     This method takes either a vector array or a distance matrix, and returns
     a distance matrix. If the input is a vector array, the distances are
@@ -1468,7 +1847,9 @@
     Valid values for metric are:

     - From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',
-      'manhattan']. These metrics support sparse matrix inputs.
+      'manhattan']. These metrics support sparse matrix
+      inputs.
+      ['nan_euclidean'] but it does not yet support sparse matrices.

     - From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',
       'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',
@@ -1488,26 +1869,28 @@

     Parameters
     ----------
-    X : array [n_samples_a, n_samples_a] if metric == "precomputed", or, \
-             [n_samples_a, n_features] otherwise
+    X : ndarray of shape (n_samples_X, n_samples_X) or \
+            (n_samples_X, n_features)
         Array of pairwise distances between samples, or a feature array.
-
-    Y : array [n_samples_b, n_features], optional
+        The shape of the array should be (n_samples_X, n_samples_X) if
+        metric == "precomputed" and (n_samples_X, n_features) otherwise.
+
+    Y : ndarray of shape (n_samples_Y, n_features), default=None
         An optional second feature array. Only allowed if
         metric != "precomputed".

-    metric : string, or callable
+    metric : str or callable, default='euclidean'
         The metric to use when calculating distance between instances in a
         feature array. If metric is a string, it must be one of the options
         allowed by scipy.spatial.distance.pdist for its metric parameter, or
-        a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.
+        a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``.
         If metric is "precomputed", X is assumed to be a distance matrix.
         Alternatively, if metric is a callable function, it is called on each
         pair of instances (rows) and the resulting value recorded. The callable
         should take two arrays from X as input and return a value indicating
         the distance between them.

-    n_jobs : int or None, optional (default=None)
+    n_jobs : int, default=None
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
@@ -1515,6 +1898,22 @@
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.
+
+    force_all_finite : bool or 'allow-nan', default=True
+        Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored
+        for a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. The
+        possibilities are:
+
+        - True: Force all values of array to be finite.
+        - False: accepts np.inf, np.nan, pd.NA in array.
+        - 'allow-nan': accepts only np.nan and pd.NA values in array. Values
+          cannot be infinite.
+
+        .. versionadded:: 0.22
+           ``force_all_finite`` accepts the string ``'allow-nan'``.
+
+        .. versionchanged:: 0.23
+           Accepts `pd.NA` and converts it into `np.nan`.

     **kwds : optional keyword parameters
         Any further parameters are passed directly to the distance function.
@@ -1523,93 +1922,104 @@

     Returns
     -------
-    D : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b]
+    D : ndarray of shape (n_samples_X, n_samples_X) or \
+            (n_samples_X, n_samples_Y)
         A distance matrix D such that D_{i, j} is the distance between the
         ith and jth vectors of the given matrix X, if Y is None.
         If Y is not None, then D_{i, j} is the distance between the ith array
         from X and the jth array from Y.

-    See also
+    See Also
     --------
-    pairwise_distances_chunked : performs the same calculation as this
+    pairwise_distances_chunked : Performs the same calculation as this
         function, but returns a generator of chunks of the distance matrix, in
         order to limit memory usage.
-    paired_distances : Computes the distances between corresponding
-                       elements of two arrays
-    """
-    if (metric not in _VALID_METRICS and
-            not callable(metric) and metric != "precomputed"):
-        raise ValueError("Unknown metric %s. "
-                         "Valid metrics are %s, or 'precomputed', or a "
-                         "callable" % (metric, _VALID_METRICS))
+    paired_distances : Computes the distances between corresponding elements
+        of two arrays.
+    """
+    if (
+        metric not in _VALID_METRICS
+        and not callable(metric)
+        and metric != "precomputed"
+    ):
+        raise ValueError(
+            "Unknown metric %s. Valid metrics are %s, or 'precomputed', or a callable"
+            % (metric, _VALID_METRICS)
+        )

     if metric == "precomputed":
-        X, _ = check_pairwise_arrays(X, Y, precomputed=True)
-
-        whom = ("`pairwise_distances`. Precomputed distance "
-                " need to have non-negative values.")
+        X, _ = check_pairwise_arrays(
+            X, Y, precomputed=True, force_all_finite=force_all_finite
+        )
+
+        whom = (
+            "`pairwise_distances`. Precomputed distance "
+            " need to have non-negative values."
+        )
         check_non_negative(X, whom=whom)
         return X
     elif metric in PAIRWISE_DISTANCE_FUNCTIONS:
         func = PAIRWISE_DISTANCE_FUNCTIONS[metric]
     elif callable(metric):
-        func = partial(_pairwise_callable, metric=metric, **kwds)
+        func = partial(
+            _pairwise_callable, metric=metric, force_all_finite=force_all_finite, **kwds
+        )
     else:
         if issparse(X) or issparse(Y):
-            raise TypeError("scipy distance metrics do not"
-                            " support sparse matrices.")
+            raise TypeError("scipy distance metrics do not support sparse matrices.")

         dtype = bool if metric in PAIRWISE_BOOLEAN_FUNCTIONS else None

-        if dtype == bool and (X.dtype != bool or Y.dtype != bool):
+        if dtype == bool and (X.dtype != bool or (Y is not None and Y.dtype != bool)):
             msg = "Data was converted to boolean for metric %s" % metric
             warnings.warn(msg, DataConversionWarning)

-        X, Y = check_pairwise_arrays(X, Y, dtype=dtype)
+        X, Y = check_pairwise_arrays(
+            X, Y, dtype=dtype, force_all_finite=force_all_finite
+        )

         # precompute data-derived metric params
         params = _precompute_metric_params(X, Y, metric=metric, **kwds)
         kwds.update(**params)

         if effective_n_jobs(n_jobs) == 1 and X is Y:
-            return distance.squareform(distance.pdist(X, metric=metric,
-                                                      **kwds))
+            return distance.squareform(distance.pdist(X, metric=metric, **kwds))
         func = partial(distance.cdist, metric=metric, **kwds)

     return _parallel_pairwise(X, Y, func, n_jobs, **kwds)


-# These distances recquire boolean arrays, when using scipy.spatial.distance
+# These distances require boolean arrays, when using scipy.spatial.distance
 PAIRWISE_BOOLEAN_FUNCTIONS = [
-    'dice',
-    'jaccard',
-    'kulsinski',
-    'matching',
-    'rogerstanimoto',
-    'russellrao',
-    'sokalmichener',
-    'sokalsneath',
-    'yule',
+    "dice",
+    "jaccard",
+    "kulsinski",
+    "matching",
+    "rogerstanimoto",
+    "russellrao",
+    "sokalmichener",
+    "sokalsneath",
+    "yule",
 ]
-

 # Helper functions - distance
 PAIRWISE_KERNEL_FUNCTIONS = {
     # If updating this dictionary, update the doc in both distance_metrics()
     # and also in pairwise_distances()!
-    'additive_chi2': additive_chi2_kernel,
-    'chi2': chi2_kernel,
-    'linear': linear_kernel,
-    'polynomial': polynomial_kernel,
-    'poly': polynomial_kernel,
-    'rbf': rbf_kernel,
-    'laplacian': laplacian_kernel,
-    'sigmoid': sigmoid_kernel,
-    'cosine': cosine_similarity, }
+    "additive_chi2": additive_chi2_kernel,
+    "chi2": chi2_kernel,
+    "linear": linear_kernel,
+    "polynomial": polynomial_kernel,
+    "poly": polynomial_kernel,
+    "rbf": rbf_kernel,
+    "laplacian": laplacian_kernel,
+    "sigmoid": sigmoid_kernel,
+    "cosine": cosine_similarity,
+}


 def kernel_metrics():
-    """ Valid metrics for pairwise_kernels
+    """Valid metrics for pairwise_kernels.

     This function simply returns the valid pairwise distance metrics.
     It exists, however, to allow for a verbose description of the mapping for
@@ -1648,8 +2058,9 @@
 }


-def pairwise_kernels(X, Y=None, metric="linear", filter_params=False,
-                     n_jobs=None, **kwds):
+def pairwise_kernels(
+    X, Y=None, metric="linear", *, filter_params=False, n_jobs=None, **kwds
+):
     """Compute the kernel between arrays X and optional array Y.

     This method takes either a vector array or a kernel matrix, and returns
@@ -1663,35 +2074,40 @@
     If Y is given (default is None), then the returned matrix is the pairwise
     kernel between the arrays from both X and Y.

-    Valid values for metric are::
+    Valid values for metric are:
         ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',
-         'laplacian', 'sigmoid', 'cosine']
+        'laplacian', 'sigmoid', 'cosine']

     Read more in the :ref:`User Guide <metrics>`.

     Parameters
     ----------
-    X : array [n_samples_a, n_samples_a] if metric == "precomputed", or, \
-             [n_samples_a, n_features] otherwise
+    X : ndarray of shape (n_samples_X, n_samples_X) or \
+            (n_samples_X, n_features)
         Array of pairwise kernels between samples, or a feature array.
-
-    Y : array [n_samples_b, n_features]
-        A second feature array only if X has shape [n_samples_a, n_features].
-
-    metric : string, or callable
+        The shape of the array should be (n_samples_X, n_samples_X) if
+        metric == "precomputed" and (n_samples_X, n_features) otherwise.
+
+    Y : ndarray of shape (n_samples_Y, n_features), default=None
+        A second feature array only if X has shape (n_samples_X, n_features).
+
+    metric : str or callable, default="linear"
         The metric to use when calculating kernel between instances in a
         feature array. If metric is a string, it must be one of the metrics
         in pairwise.PAIRWISE_KERNEL_FUNCTIONS.
         If metric is "precomputed", X is assumed to be a kernel matrix.
         Alternatively, if metric is a callable function, it is called on each
         pair of instances (rows) and the resulting value recorded. The callable
-        should take two arrays from X as input and return a value indicating
-        the distance between them.
-
-    filter_params : boolean
+        should take two rows from X as input and return the corresponding
+        kernel value as a single number. This means that callables from
+        :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on
+        matrices, not single samples. Use the string identifying the kernel
+        instead.
+
+    filter_params : bool, default=False
         Whether to filter invalid parameters or not.

-    n_jobs : int or None, optional (default=None)
+    n_jobs : int, default=None
         The number of jobs to use for the computation. This works by breaking
         down the pairwise matrix into n_jobs even slices and computing them in
         parallel.
@@ -1705,7 +2121,8 @@

     Returns
     -------
-    K : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b]
+    K : ndarray of shape (n_samples_X, n_samples_X) or \
+            (n_samples_X, n_samples_Y)
         A kernel matrix K such that K_{i, j} is the kernel between the
         ith and jth vectors of the given matrix X, if Y is None.
         If Y is not None, then K_{i, j} is the kernel between the ith array
@@ -1726,8 +2143,7 @@
         func = metric.__call__
     elif metric in PAIRWISE_KERNEL_FUNCTIONS:
         if filter_params:
-            kwds = {k: kwds[k] for k in kwds
-                    if k in KERNEL_PARAMS[metric]}
+            kwds = {k: kwds[k] for k in kwds if k in KERNEL_PARAMS[metric]}
         func = PAIRWISE_KERNEL_FUNCTIONS[metric]
     elif callable(metric):
         func = partial(_pairwise_callable, metric=metric, **kwds)
('sklearn/metrics/cluster', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,30 +5,44 @@
 - supervised, which uses a ground truth class values for each sample.
 - unsupervised, which does not and measures the 'quality' of the model itself.
 """
-from .supervised import adjusted_mutual_info_score
-from .supervised import normalized_mutual_info_score
-from .supervised import adjusted_rand_score
-from .supervised import completeness_score
-from .supervised import contingency_matrix
-from .supervised import expected_mutual_information
-from .supervised import homogeneity_completeness_v_measure
-from .supervised import homogeneity_score
-from .supervised import mutual_info_score
-from .supervised import v_measure_score
-from .supervised import fowlkes_mallows_score
-from .supervised import entropy
-from .unsupervised import silhouette_samples
-from .unsupervised import silhouette_score
-from .unsupervised import calinski_harabasz_score
-from .unsupervised import calinski_harabaz_score
-from .unsupervised import davies_bouldin_score
-from .bicluster import consensus_score
+from ._supervised import adjusted_mutual_info_score
+from ._supervised import normalized_mutual_info_score
+from ._supervised import adjusted_rand_score
+from ._supervised import rand_score
+from ._supervised import completeness_score
+from ._supervised import contingency_matrix
+from ._supervised import pair_confusion_matrix
+from ._supervised import expected_mutual_information
+from ._supervised import homogeneity_completeness_v_measure
+from ._supervised import homogeneity_score
+from ._supervised import mutual_info_score
+from ._supervised import v_measure_score
+from ._supervised import fowlkes_mallows_score
+from ._supervised import entropy
+from ._unsupervised import silhouette_samples
+from ._unsupervised import silhouette_score
+from ._unsupervised import calinski_harabasz_score
+from ._unsupervised import davies_bouldin_score
+from ._bicluster import consensus_score

-__all__ = ["adjusted_mutual_info_score", "normalized_mutual_info_score",
-           "adjusted_rand_score", "completeness_score", "contingency_matrix",
-           "expected_mutual_information", "homogeneity_completeness_v_measure",
-           "homogeneity_score", "mutual_info_score", "v_measure_score",
-           "fowlkes_mallows_score", "entropy", "silhouette_samples",
-           "silhouette_score", "calinski_harabaz_score",
-           "calinski_harabasz_score", "davies_bouldin_score",
-           "consensus_score"]
+__all__ = [
+    "adjusted_mutual_info_score",
+    "normalized_mutual_info_score",
+    "adjusted_rand_score",
+    "rand_score",
+    "completeness_score",
+    "pair_confusion_matrix",
+    "contingency_matrix",
+    "expected_mutual_information",
+    "homogeneity_completeness_v_measure",
+    "homogeneity_score",
+    "mutual_info_score",
+    "v_measure_score",
+    "fowlkes_mallows_score",
+    "entropy",
+    "silhouette_samples",
+    "silhouette_score",
+    "calinski_harabasz_score",
+    "davies_bouldin_score",
+    "consensus_score",
+]
('sklearn/metrics/cluster', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,12 +7,14 @@
 def configuration(parent_package="", top_path=None):
     config = Configuration("cluster", parent_package, top_path)
     libraries = []
-    if os.name == 'posix':
-        libraries.append('m')
-    config.add_extension("expected_mutual_info_fast",
-                         sources=["expected_mutual_info_fast.pyx"],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
+    if os.name == "posix":
+        libraries.append("m")
+    config.add_extension(
+        "_expected_mutual_info_fast",
+        sources=["_expected_mutual_info_fast.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )

     config.add_subpackage("tests")

@@ -21,4 +23,5 @@

 if __name__ == "__main__":
     from numpy.distutils.core import setup
+
     setup(**configuration().todict())
('sklearn/ensemble', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -2,35 +2,46 @@
 The :mod:`sklearn.ensemble` module includes ensemble-based methods for
 classification, regression and anomaly detection.
 """
+from ._base import BaseEnsemble
+from ._forest import RandomForestClassifier
+from ._forest import RandomForestRegressor
+from ._forest import RandomTreesEmbedding
+from ._forest import ExtraTreesClassifier
+from ._forest import ExtraTreesRegressor
+from ._bagging import BaggingClassifier
+from ._bagging import BaggingRegressor
+from ._iforest import IsolationForest
+from ._weight_boosting import AdaBoostClassifier
+from ._weight_boosting import AdaBoostRegressor
+from ._gb import GradientBoostingClassifier
+from ._gb import GradientBoostingRegressor
+from ._voting import VotingClassifier
+from ._voting import VotingRegressor
+from ._stacking import StackingClassifier
+from ._stacking import StackingRegressor
+from ._hist_gradient_boosting.gradient_boosting import (
+    HistGradientBoostingRegressor,
+    HistGradientBoostingClassifier,
+)

-from .base import BaseEnsemble
-from .forest import RandomForestClassifier
-from .forest import RandomForestRegressor
-from .forest import RandomTreesEmbedding
-from .forest import ExtraTreesClassifier
-from .forest import ExtraTreesRegressor
-from .bagging import BaggingClassifier
-from .bagging import BaggingRegressor
-from .iforest import IsolationForest
-from .weight_boosting import AdaBoostClassifier
-from .weight_boosting import AdaBoostRegressor
-from .gradient_boosting import GradientBoostingClassifier
-from .gradient_boosting import GradientBoostingRegressor
-from .voting import VotingClassifier
-from .voting import VotingRegressor
-
-from . import bagging
-from . import forest
-from . import weight_boosting
-from . import gradient_boosting
-from . import partial_dependence
-
-__all__ = ["BaseEnsemble",
-           "RandomForestClassifier", "RandomForestRegressor",
-           "RandomTreesEmbedding", "ExtraTreesClassifier",
-           "ExtraTreesRegressor", "BaggingClassifier",
-           "BaggingRegressor", "IsolationForest", "GradientBoostingClassifier",
-           "GradientBoostingRegressor", "AdaBoostClassifier",
-           "AdaBoostRegressor", "VotingClassifier", "VotingRegressor",
-           "bagging", "forest", "gradient_boosting",
-           "partial_dependence", "weight_boosting"]
+__all__ = [
+    "BaseEnsemble",
+    "RandomForestClassifier",
+    "RandomForestRegressor",
+    "RandomTreesEmbedding",
+    "ExtraTreesClassifier",
+    "ExtraTreesRegressor",
+    "BaggingClassifier",
+    "BaggingRegressor",
+    "IsolationForest",
+    "GradientBoostingClassifier",
+    "GradientBoostingRegressor",
+    "AdaBoostClassifier",
+    "AdaBoostRegressor",
+    "VotingClassifier",
+    "VotingRegressor",
+    "StackingClassifier",
+    "StackingRegressor",
+    "HistGradientBoostingClassifier",
+    "HistGradientBoostingRegressor",
+]
('sklearn/ensemble', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,9 +5,11 @@
 def configuration(parent_package="", top_path=None):
     config = Configuration("ensemble", parent_package, top_path)

-    config.add_extension("_gradient_boosting",
-                         sources=["_gradient_boosting.pyx"],
-                         include_dirs=[numpy.get_include()])
+    config.add_extension(
+        "_gradient_boosting",
+        sources=["_gradient_boosting.pyx"],
+        include_dirs=[numpy.get_include()],
+    )

     config.add_subpackage("tests")

@@ -15,40 +17,57 @@
     config.add_extension(
         "_hist_gradient_boosting._gradient_boosting",
         sources=["_hist_gradient_boosting/_gradient_boosting.pyx"],
-        include_dirs=[numpy.get_include()])
+        include_dirs=[numpy.get_include()],
+    )

-    config.add_extension("_hist_gradient_boosting.histogram",
-                         sources=["_hist_gradient_boosting/histogram.pyx"],
-                         include_dirs=[numpy.get_include()])
+    config.add_extension(
+        "_hist_gradient_boosting.histogram",
+        sources=["_hist_gradient_boosting/histogram.pyx"],
+        include_dirs=[numpy.get_include()],
+    )

-    config.add_extension("_hist_gradient_boosting.splitting",
-                         sources=["_hist_gradient_boosting/splitting.pyx"],
-                         include_dirs=[numpy.get_include()])
+    config.add_extension(
+        "_hist_gradient_boosting.splitting",
+        sources=["_hist_gradient_boosting/splitting.pyx"],
+        include_dirs=[numpy.get_include()],
+    )

-    config.add_extension("_hist_gradient_boosting._binning",
-                         sources=["_hist_gradient_boosting/_binning.pyx"],
-                         include_dirs=[numpy.get_include()])
+    config.add_extension(
+        "_hist_gradient_boosting._binning",
+        sources=["_hist_gradient_boosting/_binning.pyx"],
+        include_dirs=[numpy.get_include()],
+    )

-    config.add_extension("_hist_gradient_boosting._predictor",
-                         sources=["_hist_gradient_boosting/_predictor.pyx"],
-                         include_dirs=[numpy.get_include()])
+    config.add_extension(
+        "_hist_gradient_boosting._predictor",
+        sources=["_hist_gradient_boosting/_predictor.pyx"],
+        include_dirs=[numpy.get_include()],
+    )

-    config.add_extension("_hist_gradient_boosting._loss",
-                         sources=["_hist_gradient_boosting/_loss.pyx"],
-                         include_dirs=[numpy.get_include()])
+    config.add_extension(
+        "_hist_gradient_boosting._bitset",
+        sources=["_hist_gradient_boosting/_bitset.pyx"],
+        include_dirs=[numpy.get_include()],
+    )

-    config.add_extension("_hist_gradient_boosting.types",
-                         sources=["_hist_gradient_boosting/types.pyx"],
-                         include_dirs=[numpy.get_include()])
+    config.add_extension(
+        "_hist_gradient_boosting.common",
+        sources=["_hist_gradient_boosting/common.pyx"],
+        include_dirs=[numpy.get_include()],
+    )

-    config.add_extension("_hist_gradient_boosting.utils",
-                         sources=["_hist_gradient_boosting/utils.pyx"],
-                         include_dirs=[numpy.get_include()])
+    config.add_extension(
+        "_hist_gradient_boosting.utils",
+        sources=["_hist_gradient_boosting/utils.pyx"],
+        include_dirs=[numpy.get_include()],
+    )

     config.add_subpackage("_hist_gradient_boosting.tests")

     return config

+
 if __name__ == "__main__":
     from numpy.distutils.core import setup
+
     setup(**configuration().todict())
('sklearn/ensemble', '_gradient_boosting.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,7 +1,3 @@
-# cython: cdivision=True
-# cython: boundscheck=False
-# cython: wraparound=False
-#
 # Author: Peter Prettenhofer
 #
 # License: BSD 3 clause
@@ -32,7 +28,6 @@
 # no namespace lookup for numpy dtype and array creation
 from numpy import zeros as np_zeros
 from numpy import ones as np_ones
-from numpy import bool as np_bool
 from numpy import float32 as np_float32
 from numpy import float64 as np_float64

@@ -224,7 +219,7 @@
                     tree.nodes, tree.value,
                     scale, k, K, X.shape[0], X.shape[1],
                     <float64 *> (<np.ndarray> out).data)
-                ## out += scale * tree.predict(X).reshape((X.shape[0], 1))
+                ## out[:, k] += scale * tree.predict(X).ravel()


 def predict_stage(np.ndarray[object, ndim=2] estimators,
@@ -261,9 +256,9 @@
          the others are ``False``.
      """
      cdef np.ndarray[float64, ndim=1, mode="c"] rand = \
-          random_state.rand(n_total_samples)
+          random_state.uniform(size=n_total_samples)
      cdef np.ndarray[uint8, ndim=1, mode="c", cast=True] sample_mask = \
-          np_zeros((n_total_samples,), dtype=np_bool)
+          np_zeros((n_total_samples,), dtype=bool)

      cdef np.npy_intp n_bagged = 0
      cdef np.npy_intp i = 0
('sklearn/ensemble', '_gb_losses.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,10 +6,9 @@
 from abc import abstractmethod

 import numpy as np
-from scipy.special import expit
+from scipy.special import expit, logsumexp

 from ..tree._tree import TREE_LEAF
-from ..utils.fixes import logsumexp
 from ..utils.stats import _weighted_percentile
 from ..dummy import DummyClassifier
 from ..dummy import DummyRegressor
@@ -37,7 +36,7 @@
         self.K = n_classes

     def init_estimator(self):
-        """Default ``init`` estimator for loss function. """
+        """Default ``init`` estimator for loss function."""
         raise NotImplementedError()

     @abstractmethod
@@ -46,13 +45,13 @@

         Parameters
         ----------
-        y : 1d array, shape (n_samples,)
+        y : ndarray of shape (n_samples,)
             True labels.

-        raw_predictions : 2d array, shape (n_samples, K)
+        raw_predictions : ndarray of shape (n_samples, K)
             The raw predictions (i.e. values from the tree leaves).

-        sample_weight : 1d array, shape (n_samples,), optional
+        sample_weight : ndarray of shape (n_samples,), default=None
             Sample weights.
         """

@@ -62,17 +61,26 @@

         Parameters
         ----------
-        y : 1d array, shape (n_samples,)
+        y : ndarray of shape (n_samples,)
             The target labels.

-        raw_predictions : 2d array, shape (n_samples, K)
+        raw_predictions : ndarray of shape (n_samples, K)
             The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble at iteration ``i - 1``.
         """

-    def update_terminal_regions(self, tree, X, y, residual, raw_predictions,
-                                sample_weight, sample_mask,
-                                learning_rate=0.1, k=0):
+    def update_terminal_regions(
+        self,
+        tree,
+        X,
+        y,
+        residual,
+        raw_predictions,
+        sample_weight,
+        sample_mask,
+        learning_rate=0.1,
+        k=0,
+    ):
         """Update the terminal regions (=leaves) of the given tree and
         updates the current predictions of the model. Traverses tree
         and invokes template method `_update_terminal_region`.
@@ -81,18 +89,18 @@
         ----------
         tree : tree.Tree
             The tree object.
-        X : 2d array, shape (n, m)
+        X : ndarray of shape (n_samples, n_features)
             The data array.
-        y : 1d array, shape (n,)
+        y : ndarray of shape (n_samples,)
             The target labels.
-        residual : 1d array, shape (n,)
+        residual : ndarray of shape (n_samples,)
             The residuals (usually the negative gradient).
-        raw_predictions : 2d array, shape (n_samples, K)
+        raw_predictions : ndarray of shape (n_samples, K)
             The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble at iteration ``i - 1``.
-        sample_weight : 1d array, shape (n,)
+        sample_weight : ndarray of shape (n_samples,)
             The weight of each sample.
-        sample_mask : 1d array, shape (n,)
+        sample_mask : ndarray of shape (n_samples,)
             The sample mask to be used.
         learning_rate : float, default=0.1
             Learning rate shrinks the contribution of each tree by
@@ -110,17 +118,34 @@

         # update each leaf (= perform line search)
         for leaf in np.where(tree.children_left == TREE_LEAF)[0]:
-            self._update_terminal_region(tree, masked_terminal_regions,
-                                         leaf, X, y, residual,
-                                         raw_predictions[:, k], sample_weight)
+            self._update_terminal_region(
+                tree,
+                masked_terminal_regions,
+                leaf,
+                X,
+                y,
+                residual,
+                raw_predictions[:, k],
+                sample_weight,
+            )

         # update predictions (both in-bag and out-of-bag)
-        raw_predictions[:, k] += \
-            learning_rate * tree.value[:, 0, 0].take(terminal_regions, axis=0)
+        raw_predictions[:, k] += learning_rate * tree.value[:, 0, 0].take(
+            terminal_regions, axis=0
+        )

     @abstractmethod
-    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
-                                residual, raw_predictions, sample_weight):
+    def _update_terminal_region(
+        self,
+        tree,
+        terminal_regions,
+        leaf,
+        X,
+        y,
+        residual,
+        raw_predictions,
+        sample_weight,
+    ):
         """Template method for updating terminal regions (i.e., leaves)."""

     @abstractmethod
@@ -129,14 +154,14 @@

         Parameters
         ----------
-        X : 2d array, shape (n_samples, n_features)
+        X : ndarray of shape (n_samples, n_features)
             The data array.
-        estimator : estimator instance
+        estimator : object
             The estimator to use to compute the predictions.

         Returns
         -------
-        raw_predictions : 2d array, shape (n_samples, K)
+        raw_predictions : ndarray of shape (n_samples, K)
             The initial raw predictions. K is equal to 1 for binary
             classification and regression, and equal to the number of classes
             for multiclass classification. ``raw_predictions`` is casted
@@ -146,88 +171,92 @@


 class RegressionLossFunction(LossFunction, metaclass=ABCMeta):
-    """Base class for regression loss functions.
+    """Base class for regression loss functions."""
+
+    def __init__(self):
+        super().__init__(n_classes=1)
+
+    def check_init_estimator(self, estimator):
+        """Make sure estimator has the required fit and predict methods.
+
+        Parameters
+        ----------
+        estimator : object
+            The init estimator to check.
+        """
+        if not (hasattr(estimator, "fit") and hasattr(estimator, "predict")):
+            raise ValueError(
+                "The init parameter must be a valid estimator and "
+                "support both fit and predict."
+            )
+
+    def get_init_raw_predictions(self, X, estimator):
+        predictions = estimator.predict(X)
+        return predictions.reshape(-1, 1).astype(np.float64)
+
+
+class LeastSquaresError(RegressionLossFunction):
+    """Loss function for least squares (LS) estimation.
+    Terminal regions do not need to be updated for least squares.

     Parameters
     ----------
     n_classes : int
         Number of classes.
     """
-    def __init__(self, n_classes):
-        if n_classes != 1:
-            raise ValueError("``n_classes`` must be 1 for regression but "
-                             "was %r" % n_classes)
-        super().__init__(n_classes)
-
-    def check_init_estimator(self, estimator):
-        """Make sure estimator has the required fit and predict methods.
-
-        Parameters
-        ----------
-        estimator : estimator instance
-            The init estimator to check.
-        """
-        if not (hasattr(estimator, 'fit') and hasattr(estimator, 'predict')):
-            raise ValueError(
-                "The init parameter must be a valid estimator and "
-                "support both fit and predict."
-            )
-
-    def get_init_raw_predictions(self, X, estimator):
-        predictions = estimator.predict(X)
-        return predictions.reshape(-1, 1).astype(np.float64)
-
-
-class LeastSquaresError(RegressionLossFunction):
-    """Loss function for least squares (LS) estimation.
-    Terminal regions do not need to be updated for least squares.
-
-    Parameters
-    ----------
-    n_classes : int
-        Number of classes.
-    """

     def init_estimator(self):
-        return DummyRegressor(strategy='mean')
+        return DummyRegressor(strategy="mean")

     def __call__(self, y, raw_predictions, sample_weight=None):
         """Compute the least squares loss.

         Parameters
         ----------
-        y : 1d array, shape (n_samples,)
+        y : ndarray of shape (n_samples,)
             True labels.

-        raw_predictions : 2d array, shape (n_samples, K)
-            The raw_predictions (i.e. values from the tree leaves).
-
-        sample_weight : 1d array, shape (n_samples,), optional
+        raw_predictions : ndarray of shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves).
+
+        sample_weight : ndarray of shape (n_samples,), default=None
             Sample weights.
         """
         if sample_weight is None:
             return np.mean((y - raw_predictions.ravel()) ** 2)
         else:
-            return (1 / sample_weight.sum() * np.sum(
-                sample_weight * ((y - raw_predictions.ravel()) ** 2)))
+            return (
+                1
+                / sample_weight.sum()
+                * np.sum(sample_weight * ((y - raw_predictions.ravel()) ** 2))
+            )

     def negative_gradient(self, y, raw_predictions, **kargs):
-        """Compute the negative gradient.
-
-        Parameters
-        ----------
-        y : 1d array, shape (n_samples,)
+        """Compute half of the negative gradient.
+
+        Parameters
+        ----------
+        y : ndarray of shape (n_samples,)
             The target labels.

-        raw_predictions : 1d array, shape (n_samples,)
+        raw_predictions : ndarray of shape (n_samples,)
             The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble at iteration ``i - 1``.
         """
         return y - raw_predictions.ravel()

-    def update_terminal_regions(self, tree, X, y, residual, raw_predictions,
-                                sample_weight, sample_mask,
-                                learning_rate=0.1, k=0):
+    def update_terminal_regions(
+        self,
+        tree,
+        X,
+        y,
+        residual,
+        raw_predictions,
+        sample_weight,
+        sample_mask,
+        learning_rate=0.1,
+        k=0,
+    ):
         """Least squares does not need to update terminal regions.

         But it has to update the predictions.
@@ -236,18 +265,18 @@
         ----------
         tree : tree.Tree
             The tree object.
-        X : 2d array, shape (n, m)
+        X : ndarray of shape (n_samples, n_features)
             The data array.
-        y : 1d array, shape (n,)
+        y : ndarray of shape (n_samples,)
             The target labels.
-        residual : 1d array, shape (n,)
+        residual : ndarray of shape (n_samples,)
             The residuals (usually the negative gradient).
-        raw_predictions : 2d array, shape (n_samples, K)
+        raw_predictions : ndarray of shape (n_samples, K)
             The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble at iteration ``i - 1``.
-        sample_weight : 1d array, shape (n,)
+        sample_weight : ndarray of shape (n,)
             The weight of each sample.
-        sample_mask : 1d array, shape (n,)
+        sample_mask : ndarray of shape (n,)
             The sample mask to be used.
         learning_rate : float, default=0.1
             Learning rate shrinks the contribution of each tree by
@@ -258,8 +287,17 @@
         # update predictions
         raw_predictions[:, k] += learning_rate * tree.predict(X).ravel()

-    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
-                                residual, raw_predictions, sample_weight):
+    def _update_terminal_region(
+        self,
+        tree,
+        terminal_regions,
+        leaf,
+        X,
+        y,
+        residual,
+        raw_predictions,
+        sample_weight,
+    ):
         pass


@@ -271,28 +309,32 @@
     n_classes : int
         Number of classes
     """
+
     def init_estimator(self):
-        return DummyRegressor(strategy='quantile', quantile=.5)
+        return DummyRegressor(strategy="quantile", quantile=0.5)

     def __call__(self, y, raw_predictions, sample_weight=None):
         """Compute the least absolute error.

         Parameters
         ----------
-        y : array, shape (n_samples,)
+        y : ndarray of shape (n_samples,)
             True labels.

-        raw_predictions : array, shape (n_samples, K)
-            The raw_predictions (i.e. values from the tree leaves).
-
-        sample_weight : 1d array, shape (n_samples,), optional
+        raw_predictions : ndarray of shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves).
+
+        sample_weight : ndarray of shape (n_samples,), default=None
             Sample weights.
         """
         if sample_weight is None:
             return np.abs(y - raw_predictions.ravel()).mean()
         else:
-            return (1 / sample_weight.sum() * np.sum(
-                sample_weight * np.abs(y - raw_predictions.ravel())))
+            return (
+                1
+                / sample_weight.sum()
+                * np.sum(sample_weight * np.abs(y - raw_predictions.ravel()))
+            )

     def negative_gradient(self, y, raw_predictions, **kargs):
         """Compute the negative gradient.
@@ -301,67 +343,75 @@

         Parameters
         ----------
-        y : 1d array, shape (n_samples,)
+        y : ndarray of shape (n_samples,)
             The target labels.

-        raw_predictions : array, shape (n_samples, K)
+        raw_predictions : ndarray of shape (n_samples, K)
             The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble at iteration ``i - 1``.
         """
         raw_predictions = raw_predictions.ravel()
         return 2 * (y - raw_predictions > 0) - 1

-    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
-                                residual, raw_predictions, sample_weight):
+    def _update_terminal_region(
+        self,
+        tree,
+        terminal_regions,
+        leaf,
+        X,
+        y,
+        residual,
+        raw_predictions,
+        sample_weight,
+    ):
         """LAD updates terminal regions to median estimates."""
         terminal_region = np.where(terminal_regions == leaf)[0]
         sample_weight = sample_weight.take(terminal_region, axis=0)
-        diff = (y.take(terminal_region, axis=0) -
-                raw_predictions.take(terminal_region, axis=0))
-        tree.value[leaf, 0, 0] = _weighted_percentile(diff, sample_weight,
-                                                      percentile=50)
+        diff = y.take(terminal_region, axis=0) - raw_predictions.take(
+            terminal_region, axis=0
+        )
+        tree.value[leaf, 0, 0] = _weighted_percentile(
+            diff, sample_weight, percentile=50
+        )


 class HuberLossFunction(RegressionLossFunction):
     """Huber loss function for robust regression.

     M-Regression proposed in Friedman 2001.
+
+    Parameters
+    ----------
+    alpha : float, default=0.9
+        Percentile at which to extract score.

     References
     ----------
     J. Friedman, Greedy Function Approximation: A Gradient Boosting
     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
-
-    Parameters
-    ----------
-    n_classes : int
-        Number of classes.
-
-    alpha : float, default=0.9
-        Percentile at which to extract score.
     """

-    def __init__(self, n_classes, alpha=0.9):
-        super().__init__(n_classes)
+    def __init__(self, alpha=0.9):
+        super().__init__()
         self.alpha = alpha
         self.gamma = None

     def init_estimator(self):
-        return DummyRegressor(strategy='quantile', quantile=.5)
+        return DummyRegressor(strategy="quantile", quantile=0.5)

     def __call__(self, y, raw_predictions, sample_weight=None):
         """Compute the Huber loss.

         Parameters
         ----------
-        y : 1d array, shape (n_samples,)
+        y : ndarray of shape (n_samples,)
             True labels.

-        raw_predictions : 2d array, shape (n_samples, K)
+        raw_predictions : ndarray of shape (n_samples, K)
             The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble.

-        sample_weight : 1d array, shape (n_samples,), optional
+        sample_weight : ndarray of shape (n_samples,), default=None
             Sample weights.
         """
         raw_predictions = raw_predictions.ravel()
@@ -371,37 +421,38 @@
             if sample_weight is None:
                 gamma = np.percentile(np.abs(diff), self.alpha * 100)
             else:
-                gamma = _weighted_percentile(np.abs(diff), sample_weight,
-                                             self.alpha * 100)
+                gamma = _weighted_percentile(
+                    np.abs(diff), sample_weight, self.alpha * 100
+                )

         gamma_mask = np.abs(diff) <= gamma
         if sample_weight is None:
             sq_loss = np.sum(0.5 * diff[gamma_mask] ** 2)
-            lin_loss = np.sum(gamma * (np.abs(diff[~gamma_mask]) -
-                                       gamma / 2))
+            lin_loss = np.sum(gamma * (np.abs(diff[~gamma_mask]) - gamma / 2))
             loss = (sq_loss + lin_loss) / y.shape[0]
         else:
-            sq_loss = np.sum(0.5 * sample_weight[gamma_mask] *
-                             diff[gamma_mask] ** 2)
-            lin_loss = np.sum(gamma * sample_weight[~gamma_mask] *
-                              (np.abs(diff[~gamma_mask]) - gamma / 2))
+            sq_loss = np.sum(0.5 * sample_weight[gamma_mask] * diff[gamma_mask] ** 2)
+            lin_loss = np.sum(
+                gamma
+                * sample_weight[~gamma_mask]
+                * (np.abs(diff[~gamma_mask]) - gamma / 2)
+            )
             loss = (sq_loss + lin_loss) / sample_weight.sum()
         return loss

-    def negative_gradient(self, y, raw_predictions, sample_weight=None,
-                          **kargs):
+    def negative_gradient(self, y, raw_predictions, sample_weight=None, **kargs):
         """Compute the negative gradient.

         Parameters
         ----------
-        y : 1d array, shape (n_samples,)
+        y : ndarray of shape (n_samples,)
             The target labels.

-        raw_predictions : 2d array, shape (n_samples, K)
+        raw_predictions : ndarray of shape (n_samples, K)
             The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble at iteration ``i - 1``.

-        sample_weight : 1d array, shape (n_samples,), optional
+        sample_weight : ndarray of shape (n_samples,), default=None
             Sample weights.
         """
         raw_predictions = raw_predictions.ravel()
@@ -409,8 +460,7 @@
         if sample_weight is None:
             gamma = np.percentile(np.abs(diff), self.alpha * 100)
         else:
-            gamma = _weighted_percentile(np.abs(diff), sample_weight,
-                                         self.alpha * 100)
+            gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)
         gamma_mask = np.abs(diff) <= gamma
         residual = np.zeros((y.shape[0],), dtype=np.float64)
         residual[gamma_mask] = diff[gamma_mask]
@@ -418,18 +468,28 @@
         self.gamma = gamma
         return residual

-    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
-                                residual, raw_predictions, sample_weight):
+    def _update_terminal_region(
+        self,
+        tree,
+        terminal_regions,
+        leaf,
+        X,
+        y,
+        residual,
+        raw_predictions,
+        sample_weight,
+    ):
         terminal_region = np.where(terminal_regions == leaf)[0]
         sample_weight = sample_weight.take(terminal_region, axis=0)
         gamma = self.gamma
-        diff = (y.take(terminal_region, axis=0)
-                - raw_predictions.take(terminal_region, axis=0))
+        diff = y.take(terminal_region, axis=0) - raw_predictions.take(
+            terminal_region, axis=0
+        )
         median = _weighted_percentile(diff, sample_weight, percentile=50)
         diff_minus_median = diff - median
         tree.value[leaf, 0] = median + np.mean(
-            np.sign(diff_minus_median) *
-            np.minimum(np.abs(diff_minus_median), gamma))
+            np.sign(diff_minus_median) * np.minimum(np.abs(diff_minus_median), gamma)
+        )


 class QuantileLossFunction(RegressionLossFunction):
@@ -440,33 +500,31 @@

     Parameters
     ----------
-    n_classes : int
-        Number of classes.
-
-    alpha : float, optional (default = 0.9)
+    alpha : float, default=0.9
         The percentile.
     """
-    def __init__(self, n_classes, alpha=0.9):
-        super().__init__(n_classes)
+
+    def __init__(self, alpha=0.9):
+        super().__init__()
         self.alpha = alpha
         self.percentile = alpha * 100

     def init_estimator(self):
-        return DummyRegressor(strategy='quantile', quantile=self.alpha)
+        return DummyRegressor(strategy="quantile", quantile=self.alpha)

     def __call__(self, y, raw_predictions, sample_weight=None):
         """Compute the Quantile loss.

         Parameters
         ----------
-        y : 1d array, shape (n_samples,)
+        y : ndarray of shape (n_samples,)
             True labels.

-        raw_predictions : 2d array, shape (n_samples, K)
+        raw_predictions : ndarray of shape (n_samples, K)
             The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble.

-        sample_weight : 1d array, shape (n_samples,), optional
+        sample_weight : ndarray of shape (n_samples,), default=None
             Sample weights.
         """
         raw_predictions = raw_predictions.ravel()
@@ -475,12 +533,14 @@

         mask = y > raw_predictions
         if sample_weight is None:
-            loss = (alpha * diff[mask].sum() -
-                    (1 - alpha) * diff[~mask].sum()) / y.shape[0]
+            loss = (
+                alpha * diff[mask].sum() - (1 - alpha) * diff[~mask].sum()
+            ) / y.shape[0]
         else:
-            loss = ((alpha * np.sum(sample_weight[mask] * diff[mask]) -
-                    (1 - alpha) * np.sum(sample_weight[~mask] *
-                                         diff[~mask])) / sample_weight.sum())
+            loss = (
+                alpha * np.sum(sample_weight[mask] * diff[mask])
+                - (1 - alpha) * np.sum(sample_weight[~mask] * diff[~mask])
+            ) / sample_weight.sum()
         return loss

     def negative_gradient(self, y, raw_predictions, **kargs):
@@ -488,11 +548,11 @@

         Parameters
         ----------
-        y : 1d array, shape (n_samples,)
+        y : ndarray of shape (n_samples,)
             The target labels.

-        raw_predictions : 2d array, shape (n_samples, K)
-            The raw_predictions (i.e. values from the tree leaves) of the
+        raw_predictions : ndarray of shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble at iteration ``i - 1``.
         """
         alpha = self.alpha
@@ -500,11 +560,21 @@
         mask = y > raw_predictions
         return (alpha * mask) - ((1 - alpha) * ~mask)

-    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
-                                residual, raw_predictions, sample_weight):
+    def _update_terminal_region(
+        self,
+        tree,
+        terminal_regions,
+        leaf,
+        X,
+        y,
+        residual,
+        raw_predictions,
+        sample_weight,
+    ):
         terminal_region = np.where(terminal_regions == leaf)[0]
-        diff = (y.take(terminal_region, axis=0)
-                - raw_predictions.take(terminal_region, axis=0))
+        diff = y.take(terminal_region, axis=0) - raw_predictions.take(
+            terminal_region, axis=0
+        )
         sample_weight = sample_weight.take(terminal_region, axis=0)

         val = _weighted_percentile(diff, sample_weight, self.percentile)
@@ -512,20 +582,20 @@


 class ClassificationLossFunction(LossFunction, metaclass=ABCMeta):
-    """Base class for classification loss functions. """
+    """Base class for classification loss functions."""

     def _raw_prediction_to_proba(self, raw_predictions):
         """Template method to convert raw predictions into probabilities.

         Parameters
         ----------
-        raw_predictions : 2d array, shape (n_samples, K)
+        raw_predictions : ndarray of shape (n_samples, K)
             The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble.

         Returns
         -------
-        probas : 2d array, shape (n_samples, K)
+        probas : ndarray of shape (n_samples, K)
             The predicted probabilities.
         """

@@ -535,13 +605,13 @@

         Parameters
         ----------
-        raw_predictions : 2d array, shape (n_samples, K)
+        raw_predictions : ndarray of shape (n_samples, K)
             The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble.

         Returns
         -------
-        encoded_predictions : 2d array, shape (n_samples, K)
+        encoded_predictions : ndarray of shape (n_samples, K)
             The predicted encoded labels.
         """

@@ -550,11 +620,10 @@

         Parameters
         ----------
-        estimator : estimator instance
+        estimator : object
             The init estimator to check.
         """
-        if not (hasattr(estimator, 'fit') and
-                hasattr(estimator, 'predict_proba')):
+        if not (hasattr(estimator, "fit") and hasattr(estimator, "predict_proba")):
             raise ValueError(
                 "The init parameter must be a valid estimator "
                 "and support both fit and predict_proba."
@@ -572,59 +641,78 @@
     n_classes : int
         Number of classes.
     """
+
     def __init__(self, n_classes):
         if n_classes != 2:
-            raise ValueError("{0:s} requires 2 classes; got {1:d} class(es)"
-                             .format(self.__class__.__name__, n_classes))
+            raise ValueError(
+                "{0:s} requires 2 classes; got {1:d} class(es)".format(
+                    self.__class__.__name__, n_classes
+                )
+            )
         # we only need to fit one tree for binary clf.
         super().__init__(n_classes=1)

     def init_estimator(self):
         # return the most common class, taking into account the samples
         # weights
-        return DummyClassifier(strategy='prior')
+        return DummyClassifier(strategy="prior")

     def __call__(self, y, raw_predictions, sample_weight=None):
         """Compute the deviance (= 2 * negative log-likelihood).

         Parameters
         ----------
-        y : 1d array, shape (n_samples,)
+        y : ndarray of shape (n_samples,)
             True labels.

-        raw_predictions : 2d array, shape (n_samples, K)
+        raw_predictions : ndarray of shape (n_samples, K)
             The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble.

-        sample_weight : 1d array , shape (n_samples,), optional
+        sample_weight : ndarray of shape (n_samples,), default=None
             Sample weights.
         """
         # logaddexp(0, v) == log(1.0 + exp(v))
         raw_predictions = raw_predictions.ravel()
         if sample_weight is None:
-            return -2 * np.mean((y * raw_predictions) -
-                                np.logaddexp(0, raw_predictions))
+            return -2 * np.mean(
+                (y * raw_predictions) - np.logaddexp(0, raw_predictions)
+            )
         else:
-            return (-2 / sample_weight.sum() * np.sum(
-                sample_weight * ((y * raw_predictions) -
-                                 np.logaddexp(0, raw_predictions))))
+            return (
+                -2
+                / sample_weight.sum()
+                * np.sum(
+                    sample_weight
+                    * ((y * raw_predictions) - np.logaddexp(0, raw_predictions))
+                )
+            )

     def negative_gradient(self, y, raw_predictions, **kargs):
-        """Compute the residual (= negative gradient).
-
-        Parameters
-        ----------
-        y : 1d array, shape (n_samples,)
+        """Compute half of the negative gradient.
+
+        Parameters
+        ----------
+        y : ndarray of shape (n_samples,)
             True labels.

-        raw_predictions : 2d array, shape (n_samples, K)
-            The raw_predictions (i.e. values from the tree leaves) of the
+        raw_predictions : ndarray of shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble at iteration ``i - 1``.
         """
         return y - expit(raw_predictions.ravel())

-    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
-                                residual, raw_predictions, sample_weight):
+    def _update_terminal_region(
+        self,
+        tree,
+        terminal_regions,
+        leaf,
+        X,
+        y,
+        residual,
+        raw_predictions,
+        sample_weight,
+    ):
         """Make a single Newton-Raphson step.

         our node estimate is given by:
@@ -639,8 +727,7 @@
         sample_weight = sample_weight.take(terminal_region, axis=0)

         numerator = np.sum(sample_weight * residual)
-        denominator = np.sum(sample_weight *
-                             (y - residual) * (1 - y + residual))
+        denominator = np.sum(sample_weight * (y - residual) * (1 - y + residual))

         # prevents overflow and division by zero
         if abs(denominator) < 1e-150:
@@ -684,26 +771,27 @@

     def __init__(self, n_classes):
         if n_classes < 3:
-            raise ValueError("{0:s} requires more than 2 classes.".format(
-                self.__class__.__name__))
+            raise ValueError(
+                "{0:s} requires more than 2 classes.".format(self.__class__.__name__)
+            )
         super().__init__(n_classes)

     def init_estimator(self):
-        return DummyClassifier(strategy='prior')
+        return DummyClassifier(strategy="prior")

     def __call__(self, y, raw_predictions, sample_weight=None):
         """Compute the Multinomial deviance.

         Parameters
         ----------
-        y : 1d array, shape (n_samples,)
+        y : ndarray of shape (n_samples,)
             True labels.

-        raw_predictions : 2d array, shape (n_samples, K)
+        raw_predictions : ndarray of shape (n_samples, K)
             The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble.

-        sample_weight : 1d array, shape (n_samples,), optional
+        sample_weight : ndarray of shape (n_samples,), default=None
             Sample weights.
         """
         # create one-hot label encoding
@@ -711,35 +799,42 @@
         for k in range(self.K):
             Y[:, k] = y == k

-        if sample_weight is None:
-            return np.sum(-1 * (Y * raw_predictions).sum(axis=1) +
-                          logsumexp(raw_predictions, axis=1))
-        else:
-            return np.sum(
-                -1 * sample_weight * (Y * raw_predictions).sum(axis=1) +
-                logsumexp(raw_predictions, axis=1))
+        return np.average(
+            -1 * (Y * raw_predictions).sum(axis=1) + logsumexp(raw_predictions, axis=1),
+            weights=sample_weight,
+        )

     def negative_gradient(self, y, raw_predictions, k=0, **kwargs):
         """Compute negative gradient for the ``k``-th class.

         Parameters
         ----------
-        y : 1d array, shape (n_samples,)
+        y : ndarray of shape (n_samples,)
             The target labels.

-        raw_predictions : 2d array, shape (n_samples, K)
-            The raw_predictions (i.e. values from the tree leaves) of the
+        raw_predictions : ndarray of shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble at iteration ``i - 1``.

-        k : int, optional default=0
+        k : int, default=0
             The index of the class.
         """
-        return y - np.nan_to_num(np.exp(raw_predictions[:, k] -
-                                        logsumexp(raw_predictions, axis=1)))
-
-    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
-                                residual, raw_predictions, sample_weight):
-        """Make a single Newton-Raphson step. """
+        return y - np.nan_to_num(
+            np.exp(raw_predictions[:, k] - logsumexp(raw_predictions, axis=1))
+        )
+
+    def _update_terminal_region(
+        self,
+        tree,
+        terminal_regions,
+        leaf,
+        X,
+        y,
+        residual,
+        raw_predictions,
+        sample_weight,
+    ):
+        """Make a single Newton-Raphson step."""
         terminal_region = np.where(terminal_regions == leaf)[0]
         residual = residual.take(terminal_region, axis=0)
         y = y.take(terminal_region, axis=0)
@@ -748,8 +843,7 @@
         numerator = np.sum(sample_weight * residual)
         numerator *= (self.K - 1) / self.K

-        denominator = np.sum(sample_weight * (y - residual) *
-                             (1 - y + residual))
+        denominator = np.sum(sample_weight * (y - residual) * (1 - y + residual))

         # prevents overflow and division by zero
         if abs(denominator) < 1e-150:
@@ -759,8 +853,10 @@

     def _raw_prediction_to_proba(self, raw_predictions):
         return np.nan_to_num(
-            np.exp(raw_predictions -
-                   (logsumexp(raw_predictions, axis=1)[:, np.newaxis])))
+            np.exp(
+                raw_predictions - (logsumexp(raw_predictions, axis=1)[:, np.newaxis])
+            )
+        )

     def _raw_prediction_to_decision(self, raw_predictions):
         proba = self._raw_prediction_to_proba(raw_predictions)
@@ -779,70 +875,86 @@

     Same loss as AdaBoost.

-    References
-    ----------
-    Greg Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007
-
     Parameters
     ----------
     n_classes : int
         Number of classes.
+
+    References
+    ----------
+    Greg Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007
     """
+
     def __init__(self, n_classes):
         if n_classes != 2:
-            raise ValueError("{0:s} requires 2 classes; got {1:d} class(es)"
-                             .format(self.__class__.__name__, n_classes))
+            raise ValueError(
+                "{0:s} requires 2 classes; got {1:d} class(es)".format(
+                    self.__class__.__name__, n_classes
+                )
+            )
         # we only need to fit one tree for binary clf.
         super().__init__(n_classes=1)

     def init_estimator(self):
-        return DummyClassifier(strategy='prior')
+        return DummyClassifier(strategy="prior")

     def __call__(self, y, raw_predictions, sample_weight=None):
         """Compute the exponential loss

         Parameters
         ----------
-        y : 1d array, shape (n_samples,)
+        y : ndarray of shape (n_samples,)
             True labels.

-        raw_predictions : 2d array, shape (n_samples, K)
+        raw_predictions : ndarray of shape (n_samples, K)
             The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble.

-        sample_weight : 1d array, shape (n_samples,), optional
+        sample_weight : ndarray of shape (n_samples,), default=None
             Sample weights.
         """
         raw_predictions = raw_predictions.ravel()
         if sample_weight is None:
-            return np.mean(np.exp(-(2. * y - 1.) * raw_predictions))
+            return np.mean(np.exp(-(2.0 * y - 1.0) * raw_predictions))
         else:
-            return (1.0 / sample_weight.sum() * np.sum(
-                sample_weight * np.exp(-(2 * y - 1) * raw_predictions)))
+            return (
+                1.0
+                / sample_weight.sum()
+                * np.sum(sample_weight * np.exp(-(2 * y - 1) * raw_predictions))
+            )

     def negative_gradient(self, y, raw_predictions, **kargs):
         """Compute the residual (= negative gradient).

         Parameters
         ----------
-        y : 1d array, shape (n_samples,)
+        y : ndarray of shape (n_samples,)
             True labels.

-        raw_predictions : 2d array, shape (n_samples, K)
+        raw_predictions : ndarray of shape (n_samples, K)
             The raw predictions (i.e. values from the tree leaves) of the
             tree ensemble at iteration ``i - 1``.
         """
-        y_ = -(2. * y - 1.)
-        return y_ * np.exp(y_ * raw_predictions.ravel())
-
-    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
-                                residual, raw_predictions, sample_weight):
+        y_ = 2.0 * y - 1.0
+        return y_ * np.exp(-y_ * raw_predictions.ravel())
+
+    def _update_terminal_region(
+        self,
+        tree,
+        terminal_regions,
+        leaf,
+        X,
+        y,
+        residual,
+        raw_predictions,
+        sample_weight,
+    ):
         terminal_region = np.where(terminal_regions == leaf)[0]
         raw_predictions = raw_predictions.take(terminal_region, axis=0)
         y = y.take(terminal_region, axis=0)
         sample_weight = sample_weight.take(terminal_region, axis=0)

-        y_ = 2. * y - 1.
+        y_ = 2.0 * y - 1.0

         numerator = np.sum(y_ * sample_weight * np.exp(-y_ * raw_predictions))
         denominator = np.sum(sample_weight * np.exp(-y_ * raw_predictions))
@@ -860,7 +972,7 @@
         return proba

     def _raw_prediction_to_decision(self, raw_predictions):
-        return (raw_predictions.ravel() >= 0).astype(np.int)
+        return (raw_predictions.ravel() >= 0).astype(int)

     def get_init_raw_predictions(self, X, estimator):
         probas = estimator.predict_proba(X)
@@ -870,15 +982,19 @@
         # according to The Elements of Statistical Learning sec. 10.5, the
         # minimizer of the exponential loss is .5 * log odds ratio. So this is
         # the equivalent to .5 * binomial_deviance.get_init_raw_predictions()
-        raw_predictions = .5 * np.log(proba_pos_class / (1 - proba_pos_class))
+        raw_predictions = 0.5 * np.log(proba_pos_class / (1 - proba_pos_class))
         return raw_predictions.reshape(-1, 1).astype(np.float64)


+# TODO: Remove entry 'ls' and 'lad' in version 1.2.
 LOSS_FUNCTIONS = {
-    'ls': LeastSquaresError,
-    'lad': LeastAbsoluteError,
-    'huber': HuberLossFunction,
-    'quantile': QuantileLossFunction,
-    'deviance': None,  # for both, multinomial and binomial
-    'exponential': ExponentialLoss,
+    "squared_error": LeastSquaresError,
+    "ls": LeastSquaresError,
+    "absolute_error": LeastAbsoluteError,
+    "lad": LeastAbsoluteError,
+    "huber": HuberLossFunction,
+    "quantile": QuantileLossFunction,
+    "deviance": None,  # for both, multinomial and binomial
+    "log_loss": None,  # for both, multinomial and binomial
+    "exponential": ExponentialLoss,
 }
('sklearn/ensemble/_hist_gradient_boosting', 'predictor.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,25 +5,10 @@

 import numpy as np

-from .types import X_DTYPE
-from .types import Y_DTYPE
-from .types import X_BINNED_DTYPE
-from ._predictor import _predict_from_numeric_data
+from .common import Y_DTYPE
+from ._predictor import _predict_from_raw_data
 from ._predictor import _predict_from_binned_data
-
-
-PREDICTOR_RECORD_DTYPE = np.dtype([
-    ('value', Y_DTYPE),
-    ('count', np.uint32),
-    ('feature_idx', np.uint32),
-    ('threshold', X_DTYPE),
-    ('left', np.uint32),
-    ('right', np.uint32),
-    ('gain', Y_DTYPE),
-    ('depth', np.uint32),
-    ('is_leaf', np.uint8),
-    ('bin_threshold', X_BINNED_DTYPE),
-])
+from ._predictor import _compute_partial_dependence


 class TreePredictor:
@@ -31,21 +16,33 @@

     Parameters
     ----------
-    nodes : list of PREDICTOR_RECORD_DTYPE
+    nodes : ndarray of PREDICTOR_RECORD_DTYPE
         The nodes of the tree.
+    binned_left_cat_bitsets : ndarray of shape (n_categorical_splits, 8), \
+            dtype=uint32
+        Array of bitsets for binned categories used in predict_binned when a
+        split is categorical.
+    raw_left_cat_bitsets : ndarray of shape (n_categorical_splits, 8), \
+            dtype=uint32
+        Array of bitsets for raw categories used in predict when a split is
+        categorical.
+
     """
-    def __init__(self, nodes):
+
+    def __init__(self, nodes, binned_left_cat_bitsets, raw_left_cat_bitsets):
         self.nodes = nodes
+        self.binned_left_cat_bitsets = binned_left_cat_bitsets
+        self.raw_left_cat_bitsets = raw_left_cat_bitsets

     def get_n_leaf_nodes(self):
         """Return number of leaves."""
-        return int(self.nodes['is_leaf'].sum())
+        return int(self.nodes["is_leaf"].sum())

     def get_max_depth(self):
         """Return maximum depth among all leaves."""
-        return int(self.nodes['depth'].max())
+        return int(self.nodes["depth"].max())

-    def predict(self, X):
+    def predict(self, X, known_cat_bitsets, f_idx_map, n_threads):
         """Predict raw values for non-binned data.

         Parameters
@@ -53,22 +50,15 @@
         X : ndarray, shape (n_samples, n_features)
             The input samples.

-        Returns
-        -------
-        y : ndarray, shape (n_samples,)
-            The raw predicted values.
-        """
-        out = np.empty(X.shape[0], dtype=Y_DTYPE)
-        _predict_from_numeric_data(self.nodes, X, out)
-        return out
+        known_cat_bitsets : ndarray of shape (n_categorical_features, 8)
+            Array of bitsets of known categories, for each categorical feature.

-    def predict_binned(self, X):
-        """Predict raw values for binned data.
+        f_idx_map : ndarray of shape (n_features,)
+            Map from original feature index to the corresponding index in the
+            known_cat_bitsets array.

-        Parameters
-        ----------
-        X : ndarray, shape (n_samples, n_features)
-            The input samples.
+        n_threads : int
+            Number of OpenMP threads to use.

         Returns
         -------
@@ -76,5 +66,60 @@
             The raw predicted values.
         """
         out = np.empty(X.shape[0], dtype=Y_DTYPE)
-        _predict_from_binned_data(self.nodes, X, out)
+        _predict_from_raw_data(
+            self.nodes,
+            X,
+            self.raw_left_cat_bitsets,
+            known_cat_bitsets,
+            f_idx_map,
+            n_threads,
+            out,
+        )
         return out
+
+    def predict_binned(self, X, missing_values_bin_idx, n_threads):
+        """Predict raw values for binned data.
+
+        Parameters
+        ----------
+        X : ndarray, shape (n_samples, n_features)
+            The input samples.
+        missing_values_bin_idx : uint8
+            Index of the bin that is used for missing values. This is the
+            index of the last bin and is always equal to max_bins (as passed
+            to the GBDT classes), or equivalently to n_bins - 1.
+        n_threads : int
+            Number of OpenMP threads to use.
+
+        Returns
+        -------
+        y : ndarray, shape (n_samples,)
+            The raw predicted values.
+        """
+        out = np.empty(X.shape[0], dtype=Y_DTYPE)
+        _predict_from_binned_data(
+            self.nodes,
+            X,
+            self.binned_left_cat_bitsets,
+            missing_values_bin_idx,
+            n_threads,
+            out,
+        )
+        return out
+
+    def compute_partial_dependence(self, grid, target_features, out):
+        """Fast partial dependence computation.
+
+        Parameters
+        ----------
+        grid : ndarray, shape (n_samples, n_target_features)
+            The grid points on which the partial dependence should be
+            evaluated.
+        target_features : ndarray, shape (n_target_features)
+            The set of target features for which the partial dependence
+            should be evaluated.
+        out : ndarray, shape (n_samples)
+            The value of the partial dependence function on each grid
+            point.
+        """
+        _compute_partial_dependence(self.nodes, grid, target_features, out)
('sklearn/ensemble/_hist_gradient_boosting', 'binning.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -12,104 +12,168 @@
 from ...utils import check_random_state, check_array
 from ...base import BaseEstimator, TransformerMixin
 from ...utils.validation import check_is_fitted
+from ...utils.fixes import percentile
+from ...utils._openmp_helpers import _openmp_effective_n_threads
 from ._binning import _map_to_bins
-from .types import X_DTYPE, X_BINNED_DTYPE
-
-
-def _find_binning_thresholds(data, max_bins, subsample, random_state):
-    """Extract feature-wise quantiles from numerical data.
+from .common import X_DTYPE, X_BINNED_DTYPE, ALMOST_INF, X_BITSET_INNER_DTYPE
+from ._bitset import set_bitset_memoryview
+
+
+def _find_binning_thresholds(col_data, max_bins):
+    """Extract quantiles from a continuous feature.
+
+    Missing values are ignored for finding the thresholds.

     Parameters
     ----------
-    data : array-like, shape (n_samples, n_features)
-        The data to bin.
-    max_bins : int
-        The maximum number of bins to use. If for a given feature the number of
-        unique values is less than ``max_bins``, then those unique values
-        will be used to compute the bin thresholds, instead of the quantiles.
-    subsample : int or None
-        If ``n_samples > subsample``, then ``sub_samples`` samples will be
-        randomly choosen to compute the quantiles. If ``None``, the whole data
-        is used.
-    random_state: int or numpy.random.RandomState or None
-        Pseudo-random number generator to control the random sub-sampling.
-        See :term:`random_state`.
+    col_data : array-like, shape (n_samples,)
+        The continuous feature to bin.
+    max_bins: int
+        The maximum number of bins to use for non-missing values. If for a
+        given feature the number of unique values is less than ``max_bins``,
+        then those unique values will be used to compute the bin thresholds,
+        instead of the quantiles

     Return
     ------
-    binning_thresholds: list of arrays
-        For each feature, stores the increasing numeric values that can
-        be used to separate the bins. Thus ``len(binning_thresholds) ==
-        n_features``.
+    binning_thresholds : ndarray of shape(min(max_bins, n_unique_values) - 1,)
+        The increasing numeric values that can be used to separate the bins.
+        A given value x will be mapped into bin value i iff
+        bining_thresholds[i - 1] < x <= binning_thresholds[i]
     """
-    if not (2 <= max_bins <= 256):
-        raise ValueError('max_bins={} should be no smaller than 2 '
-                         'and no larger than 256.'.format(max_bins))
-    rng = check_random_state(random_state)
-    if subsample is not None and data.shape[0] > subsample:
-        subset = rng.choice(np.arange(data.shape[0]), subsample, replace=False)
-        data = data.take(subset, axis=0)
-
-    percentiles = np.linspace(0, 100, num=max_bins + 1)
-    percentiles = percentiles[1:-1]
-    binning_thresholds = []
-    for f_idx in range(data.shape[1]):
-        col_data = np.ascontiguousarray(data[:, f_idx], dtype=X_DTYPE)
-        distinct_values = np.unique(col_data)
-        if len(distinct_values) <= max_bins:
-            midpoints = distinct_values[:-1] + distinct_values[1:]
-            midpoints *= .5
-        else:
-            # We sort again the data in this case. We could compute
-            # approximate midpoint percentiles using the output of
-            # np.unique(col_data, return_counts) instead but this is more
-            # work and the performance benefit will be limited because we
-            # work on a fixed-size subsample of the full data.
-            midpoints = np.percentile(col_data, percentiles,
-                                      interpolation='midpoint').astype(X_DTYPE)
-        binning_thresholds.append(midpoints)
-    return binning_thresholds
-
-
-class _BinMapper(BaseEstimator, TransformerMixin):
+    # ignore missing values when computing bin thresholds
+    missing_mask = np.isnan(col_data)
+    if missing_mask.any():
+        col_data = col_data[~missing_mask]
+    col_data = np.ascontiguousarray(col_data, dtype=X_DTYPE)
+    distinct_values = np.unique(col_data)
+    if len(distinct_values) <= max_bins:
+        midpoints = distinct_values[:-1] + distinct_values[1:]
+        midpoints *= 0.5
+    else:
+        # We sort again the data in this case. We could compute
+        # approximate midpoint percentiles using the output of
+        # np.unique(col_data, return_counts) instead but this is more
+        # work and the performance benefit will be limited because we
+        # work on a fixed-size subsample of the full data.
+        percentiles = np.linspace(0, 100, num=max_bins + 1)
+        percentiles = percentiles[1:-1]
+        midpoints = percentile(col_data, percentiles, method="midpoint").astype(X_DTYPE)
+        assert midpoints.shape[0] == max_bins - 1
+
+    # We avoid having +inf thresholds: +inf thresholds are only allowed in
+    # a "split on nan" situation.
+    np.clip(midpoints, a_min=None, a_max=ALMOST_INF, out=midpoints)
+    return midpoints
+
+
+class _BinMapper(TransformerMixin, BaseEstimator):
     """Transformer that maps a dataset into integer-valued bins.

-    The bins are created in a feature-wise fashion, using quantiles so that
-    each bins contains approximately the same number of samples.
-
-    For large datasets, quantiles are computed on a subset of the data to
-    speed-up the binning, but the quantiles should remain stable.
-
-    If the number of unique values for a given feature is less than
-    ``max_bins``, then the unique values of this feature are used instead of
-    the quantiles.
+    For continuous features, the bins are created in a feature-wise fashion,
+    using quantiles so that each bins contains approximately the same number
+    of samples. For large datasets, quantiles are computed on a subset of the
+    data to speed-up the binning, but the quantiles should remain stable.
+
+    For categorical features, the raw categorical values are expected to be
+    in [0, 254] (this is not validated here though) and each category
+    corresponds to a bin. All categorical values must be known at
+    initialization: transform() doesn't know how to bin unknown categorical
+    values. Note that transform() is only used on non-training data in the
+    case of early stopping.
+
+    Features with a small number of values may be binned into less than
+    ``n_bins`` bins. The last bin (at index ``n_bins - 1``) is always reserved
+    for missing values.

     Parameters
     ----------
-    max_bins : int, optional (default=256)
-        The maximum number of bins to use. If for a given feature the number of
-        unique values is less than ``max_bins``, then those unique values
-        will be used to compute the bin thresholds, instead of the quantiles.
-    subsample : int or None, optional (default=2e5)
+    n_bins : int, default=256
+        The maximum number of bins to use (including the bin for missing
+        values). Should be in [3, 256]. Non-missing values are binned on
+        ``max_bins = n_bins - 1`` bins. The last bin is always reserved for
+        missing values. If for a given feature the number of unique values is
+        less than ``max_bins``, then those unique values will be used to
+        compute the bin thresholds, instead of the quantiles. For categorical
+        features indicated by ``is_categorical``, the docstring for
+        ``is_categorical`` details on this procedure.
+    subsample : int or None, default=2e5
         If ``n_samples > subsample``, then ``sub_samples`` samples will be
-        randomly choosen to compute the quantiles. If ``None``, the whole data
+        randomly chosen to compute the quantiles. If ``None``, the whole data
         is used.
-    random_state: int or numpy.random.RandomState or None, \
-        optional (default=None)
+    is_categorical : ndarray of bool of shape (n_features,), default=None
+        Indicates categorical features. By default, all features are
+        considered continuous.
+    known_categories : list of {ndarray, None} of shape (n_features,), \
+            default=none
+        For each categorical feature, the array indicates the set of unique
+        categorical values. These should be the possible values over all the
+        data, not just the training data. For continuous features, the
+        corresponding entry should be None.
+    random_state: int, RandomState instance or None, default=None
         Pseudo-random number generator to control the random sub-sampling.
-        See :term:`random_state`.
+        Pass an int for reproducible output across multiple
+        function calls.
+        See :term:`Glossary <random_state>`.
+    n_threads : int, default=None
+        Number of OpenMP threads to use. `_openmp_effective_n_threads` is called
+        to determine the effective number of threads use, which takes cgroups CPU
+        quotes into account. See the docstring of `_openmp_effective_n_threads`
+        for details.
+
+    Attributes
+    ----------
+    bin_thresholds_ : list of ndarray
+        For each feature, each array indicates how to map a feature into a
+        binned feature. The semantic and size depends on the nature of the
+        feature:
+        - for real-valued features, the array corresponds to the real-valued
+          bin thresholds (the upper bound of each bin). There are ``max_bins
+          - 1`` thresholds, where ``max_bins = n_bins - 1`` is the number of
+          bins used for non-missing values.
+        - for categorical features, the array is a map from a binned category
+          value to the raw category value. The size of the array is equal to
+          ``min(max_bins, category_cardinality)`` where we ignore missing
+          values in the cardinality.
+    n_bins_non_missing_ : ndarray, dtype=np.uint32
+        For each feature, gives the number of bins actually used for
+        non-missing values. For features with a lot of unique values, this is
+        equal to ``n_bins - 1``.
+    is_categorical_ : ndarray of shape (n_features,), dtype=np.uint8
+        Indicator for categorical features.
+    missing_values_bin_idx_ : np.uint8
+        The index of the bin where missing values are mapped. This is a
+        constant across all features. This corresponds to the last bin, and
+        it is always equal to ``n_bins - 1``. Note that if ``n_bins_missing_``
+        is less than ``n_bins - 1`` for a given feature, then there are
+        empty (and unused) bins.
     """
-    def __init__(self, max_bins=256, subsample=int(2e5), random_state=None):
-        self.max_bins = max_bins
+
+    def __init__(
+        self,
+        n_bins=256,
+        subsample=int(2e5),
+        is_categorical=None,
+        known_categories=None,
+        random_state=None,
+        n_threads=None,
+    ):
+        self.n_bins = n_bins
         self.subsample = subsample
+        self.is_categorical = is_categorical
+        self.known_categories = known_categories
         self.random_state = random_state
+        self.n_threads = n_threads

     def fit(self, X, y=None):
         """Fit data X by computing the binning thresholds.

+        The last bin is reserved for missing values, whether missing values
+        are present in the data or not.
+
         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
+        X : array-like of shape (n_samples, n_features)
             The data to bin.
         y: None
             Ignored.
@@ -118,38 +182,135 @@
         -------
         self : object
         """
-        X = check_array(X, dtype=[X_DTYPE])
-        self.bin_thresholds_ = _find_binning_thresholds(
-            X, self.max_bins, subsample=self.subsample,
-            random_state=self.random_state)
-
-        self.actual_n_bins_ = np.array(
-            [thresholds.shape[0] + 1 for thresholds in self.bin_thresholds_],
-            dtype=np.uint32)
-
+        if not (3 <= self.n_bins <= 256):
+            # min is 3: at least 2 distinct bins and a missing values bin
+            raise ValueError(
+                "n_bins={} should be no smaller than 3 and no larger than 256.".format(
+                    self.n_bins
+                )
+            )
+
+        X = check_array(X, dtype=[X_DTYPE], force_all_finite=False)
+        max_bins = self.n_bins - 1
+
+        rng = check_random_state(self.random_state)
+        if self.subsample is not None and X.shape[0] > self.subsample:
+            subset = rng.choice(X.shape[0], self.subsample, replace=False)
+            X = X.take(subset, axis=0)
+
+        if self.is_categorical is None:
+            self.is_categorical_ = np.zeros(X.shape[1], dtype=np.uint8)
+        else:
+            self.is_categorical_ = np.asarray(self.is_categorical, dtype=np.uint8)
+
+        n_features = X.shape[1]
+        known_categories = self.known_categories
+        if known_categories is None:
+            known_categories = [None] * n_features
+
+        # validate is_categorical and known_categories parameters
+        for f_idx in range(n_features):
+            is_categorical = self.is_categorical_[f_idx]
+            known_cats = known_categories[f_idx]
+            if is_categorical and known_cats is None:
+                raise ValueError(
+                    f"Known categories for feature {f_idx} must be provided."
+                )
+            if not is_categorical and known_cats is not None:
+                raise ValueError(
+                    f"Feature {f_idx} isn't marked as a categorical feature, "
+                    "but categories were passed."
+                )
+
+        self.missing_values_bin_idx_ = self.n_bins - 1
+
+        self.bin_thresholds_ = []
+        n_bins_non_missing = []
+
+        for f_idx in range(n_features):
+            if not self.is_categorical_[f_idx]:
+                thresholds = _find_binning_thresholds(X[:, f_idx], max_bins)
+                n_bins_non_missing.append(thresholds.shape[0] + 1)
+            else:
+                # Since categories are assumed to be encoded in
+                # [0, n_cats] and since n_cats <= max_bins,
+                # the thresholds *are* the unique categorical values. This will
+                # lead to the correct mapping in transform()
+                thresholds = known_categories[f_idx]
+                n_bins_non_missing.append(thresholds.shape[0])
+
+            self.bin_thresholds_.append(thresholds)
+
+        self.n_bins_non_missing_ = np.array(n_bins_non_missing, dtype=np.uint32)
         return self

     def transform(self, X):
         """Bin data X.

+        Missing values will be mapped to the last bin.
+
+        For categorical features, the mapping will be incorrect for unknown
+        categories. Since the BinMapper is given known_categories of the
+        entire training data (i.e. before the call to train_test_split() in
+        case of early-stopping), this never happens.
+
         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
+        X : array-like of shape (n_samples, n_features)
             The data to bin.

         Returns
         -------
-        X_binned : array-like, shape (n_samples, n_features)
-            The binned data.
+        X_binned : array-like of shape (n_samples, n_features)
+            The binned data (fortran-aligned).
         """
-        X = check_array(X, dtype=[X_DTYPE])
-        check_is_fitted(self, ['bin_thresholds_', 'actual_n_bins_'])
-        if X.shape[1] != self.actual_n_bins_.shape[0]:
+        X = check_array(X, dtype=[X_DTYPE], force_all_finite=False)
+        check_is_fitted(self)
+        if X.shape[1] != self.n_bins_non_missing_.shape[0]:
             raise ValueError(
-                'This estimator was fitted with {} features but {} got passed '
-                'to transform()'.format(self.actual_n_bins_.shape[0],
-                                        X.shape[1])
+                "This estimator was fitted with {} features but {} got passed "
+                "to transform()".format(self.n_bins_non_missing_.shape[0], X.shape[1])
             )
-        binned = np.zeros_like(X, dtype=X_BINNED_DTYPE, order='F')
-        _map_to_bins(X, self.bin_thresholds_, binned)
+
+        n_threads = _openmp_effective_n_threads(self.n_threads)
+        binned = np.zeros_like(X, dtype=X_BINNED_DTYPE, order="F")
+        _map_to_bins(
+            X, self.bin_thresholds_, self.missing_values_bin_idx_, n_threads, binned
+        )
         return binned
+
+    def make_known_categories_bitsets(self):
+        """Create bitsets of known categories.
+
+        Returns
+        -------
+        - known_cat_bitsets : ndarray of shape (n_categorical_features, 8)
+            Array of bitsets of known categories, for each categorical feature.
+        - f_idx_map : ndarray of shape (n_features,)
+            Map from original feature index to the corresponding index in the
+            known_cat_bitsets array.
+        """
+
+        categorical_features_indices = np.flatnonzero(self.is_categorical_)
+
+        n_features = self.is_categorical_.size
+        n_categorical_features = categorical_features_indices.size
+
+        f_idx_map = np.zeros(n_features, dtype=np.uint32)
+        f_idx_map[categorical_features_indices] = np.arange(
+            n_categorical_features, dtype=np.uint32
+        )
+
+        known_categories = self.bin_thresholds_
+
+        known_cat_bitsets = np.zeros(
+            (n_categorical_features, 8), dtype=X_BITSET_INNER_DTYPE
+        )
+
+        # TODO: complexity is O(n_categorical_features * 255). Maybe this is
+        # worth cythonizing
+        for mapped_f_idx, f_idx in enumerate(categorical_features_indices):
+            for raw_cat_val in known_categories[f_idx]:
+                set_bitset_memoryview(known_cat_bitsets[mapped_f_idx], raw_cat_val)
+
+        return known_cat_bitsets, f_idx_map
('sklearn/ensemble/_hist_gradient_boosting', '_predictor.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,100 +1,257 @@
-# cython: cdivision=True
-# cython: boundscheck=False
-# cython: wraparound=False
-# cython: language_level=3
-
 # Author: Nicolas Hug

 cimport cython
 from cython.parallel import prange
+from libc.math cimport isnan
 import numpy as np
 cimport numpy as np
-
-from .types cimport X_DTYPE_C
-from .types cimport Y_DTYPE_C
-from .types cimport X_BINNED_DTYPE_C
-
-
-cdef packed struct node_struct:
-    # Equivalent struct to PREDICTOR_RECORD_DTYPE to use in memory views. It
-    # needs to be packed since by default numpy dtypes aren't aligned
-    Y_DTYPE_C value
-    unsigned int count
-    unsigned int feature_idx
-    X_DTYPE_C threshold
-    unsigned int left
-    unsigned int right
-    Y_DTYPE_C gain
-    unsigned int depth
-    unsigned char is_leaf
-    X_BINNED_DTYPE_C bin_threshold
-
-
-def _predict_from_numeric_data(nodes, numeric_data, out):
-    _predict_from_numeric_data_parallel(nodes, numeric_data, out)
-
-
-def _predict_from_binned_data(nodes, binned_data, out):
-    _predict_from_binned_data_parallel(nodes, binned_data, out)
-
-
-cdef void _predict_from_numeric_data_parallel(
+from numpy.math cimport INFINITY
+
+from .common cimport X_DTYPE_C
+from .common cimport Y_DTYPE_C
+from .common import Y_DTYPE
+from .common cimport X_BINNED_DTYPE_C
+from .common cimport BITSET_INNER_DTYPE_C
+from .common cimport BITSET_DTYPE_C
+from .common cimport node_struct
+from ._bitset cimport in_bitset_2d_memoryview
+
+np.import_array()
+
+
+def _predict_from_raw_data(  # raw data = non-binned data
         node_struct [:] nodes,
         const X_DTYPE_C [:, :] numeric_data,
+        const BITSET_INNER_DTYPE_C [:, ::1] raw_left_cat_bitsets,
+        const BITSET_INNER_DTYPE_C [:, ::1] known_cat_bitsets,
+        const unsigned int [::1] f_idx_map,
+        int n_threads,
         Y_DTYPE_C [:] out):

     cdef:
         int i

-    for i in prange(numeric_data.shape[0], schedule='static', nogil=True):
-        out[i] = _predict_one_from_numeric_data(nodes, numeric_data, i)
-
-
-cdef inline Y_DTYPE_C _predict_one_from_numeric_data(
+    for i in prange(numeric_data.shape[0], schedule='static', nogil=True,
+                    num_threads=n_threads):
+        out[i] = _predict_one_from_raw_data(
+            nodes, numeric_data, raw_left_cat_bitsets,
+            known_cat_bitsets,
+            f_idx_map, i)
+
+
+cdef inline Y_DTYPE_C _predict_one_from_raw_data(
         node_struct [:] nodes,
         const X_DTYPE_C [:, :] numeric_data,
+        const BITSET_INNER_DTYPE_C [:, ::1] raw_left_cat_bitsets,
+        const BITSET_INNER_DTYPE_C [:, ::1] known_cat_bitsets,
+        const unsigned int [::1] f_idx_map,
         const int row) nogil:
     # Need to pass the whole array and the row index, else prange won't work.
     # See issue Cython #2798

     cdef:
         node_struct node = nodes[0]
+        unsigned int node_idx = 0
+        X_DTYPE_C data_val

     while True:
         if node.is_leaf:
             return node.value
-        if numeric_data[row, node.feature_idx] <= node.threshold:
-            node = nodes[node.left]
+
+        data_val = numeric_data[row, node.feature_idx]
+
+        if isnan(data_val):
+            if node.missing_go_to_left:
+                node_idx = node.left
+            else:
+                node_idx = node.right
+        elif node.is_categorical:
+            if in_bitset_2d_memoryview(
+                    raw_left_cat_bitsets,
+                    <X_BINNED_DTYPE_C>data_val,
+                    node.bitset_idx):
+                node_idx = node.left
+            elif in_bitset_2d_memoryview(
+                    known_cat_bitsets,
+                    <X_BINNED_DTYPE_C>data_val,
+                    f_idx_map[node.feature_idx]):
+                node_idx = node.right
+            else:
+                # Treat unknown categories as missing.
+                node_idx = node.left if node.missing_go_to_left else node.right
         else:
-            node = nodes[node.right]
-
-
-cdef void _predict_from_binned_data_parallel(
+            if data_val <= node.num_threshold:
+                node_idx = node.left
+            else:
+                node_idx = node.right
+        node = nodes[node_idx]
+
+
+def _predict_from_binned_data(
         node_struct [:] nodes,
         const X_BINNED_DTYPE_C [:, :] binned_data,
+        BITSET_INNER_DTYPE_C [:, :] binned_left_cat_bitsets,
+        const unsigned char missing_values_bin_idx,
+        int n_threads,
         Y_DTYPE_C [:] out):

     cdef:
         int i

-    for i in prange(binned_data.shape[0], schedule='static', nogil=True):
-        out[i] = _predict_one_from_binned_data(nodes, binned_data, i)
+    for i in prange(binned_data.shape[0], schedule='static', nogil=True,
+                    num_threads=n_threads):
+        out[i] = _predict_one_from_binned_data(nodes,
+                                               binned_data,
+                                               binned_left_cat_bitsets, i,
+                                               missing_values_bin_idx)


 cdef inline Y_DTYPE_C _predict_one_from_binned_data(
         node_struct [:] nodes,
         const X_BINNED_DTYPE_C [:, :] binned_data,
-        const int row) nogil:
+        const BITSET_INNER_DTYPE_C [:, :] binned_left_cat_bitsets,
+        const int row,
+        const unsigned char missing_values_bin_idx) nogil:
     # Need to pass the whole array and the row index, else prange won't work.
     # See issue Cython #2798

     cdef:
         node_struct node = nodes[0]
+        unsigned int node_idx = 0
+        X_BINNED_DTYPE_C data_val

     while True:
         if node.is_leaf:
             return node.value
-        if binned_data[row, node.feature_idx] <= node.bin_threshold:
-            node = nodes[node.left]
+
+        data_val = binned_data[row, node.feature_idx]
+
+        if data_val == missing_values_bin_idx:
+            if node.missing_go_to_left:
+                node_idx = node.left
+            else:
+                node_idx = node.right
+        elif node.is_categorical:
+            if in_bitset_2d_memoryview(
+                    binned_left_cat_bitsets,
+                    data_val,
+                    node.bitset_idx):
+                node_idx = node.left
+            else:
+                node_idx = node.right
         else:
-            node = nodes[node.right]
+            if data_val <= node.bin_threshold:
+                node_idx = node.left
+            else:
+                node_idx = node.right
+        node = nodes[node_idx]
+
+
+def _compute_partial_dependence(
+    node_struct [:] nodes,
+    const X_DTYPE_C [:, ::1] X,
+    int [:] target_features,
+    Y_DTYPE_C [:] out):
+    """Partial dependence of the response on the ``target_features`` set.
+
+    For each sample in ``X`` a tree traversal is performed.
+    Each traversal starts from the root with weight 1.0.
+
+    At each non-leaf node that splits on a target feature, either
+    the left child or the right child is visited based on the feature
+    value of the current sample, and the weight is not modified.
+    At each non-leaf node that splits on a complementary feature,
+    both children are visited and the weight is multiplied by the fraction
+    of training samples which went to each child.
+
+    At each leaf, the value of the node is multiplied by the current
+    weight (weights sum to 1 for all visited terminal nodes).
+
+    Parameters
+    ----------
+    nodes : view on array of PREDICTOR_RECORD_DTYPE, shape (n_nodes)
+        The array representing the predictor tree.
+    X : view on 2d ndarray, shape (n_samples, n_target_features)
+        The grid points on which the partial dependence should be
+        evaluated.
+    target_features : view on 1d ndarray, shape (n_target_features)
+        The set of target features for which the partial dependence
+        should be evaluated.
+    out : view on 1d ndarray, shape (n_samples)
+        The value of the partial dependence function on each grid
+        point.
+    """
+
+    cdef:
+        unsigned int current_node_idx
+        unsigned int [:] node_idx_stack = np.zeros(shape=nodes.shape[0],
+                                                   dtype=np.uint32)
+        Y_DTYPE_C [::1] weight_stack = np.zeros(shape=nodes.shape[0],
+                                                dtype=Y_DTYPE)
+        node_struct * current_node  # pointer to avoid copying attributes
+
+        unsigned int sample_idx
+        unsigned feature_idx
+        unsigned stack_size
+        Y_DTYPE_C left_sample_frac
+        Y_DTYPE_C current_weight
+        Y_DTYPE_C total_weight  # used for sanity check only
+        bint is_target_feature
+
+    for sample_idx in range(X.shape[0]):
+        # init stacks for current sample
+        stack_size = 1
+        node_idx_stack[0] = 0  # root node
+        weight_stack[0] = 1  # all the samples are in the root node
+        total_weight = 0
+
+        while stack_size > 0:
+
+            # pop the stack
+            stack_size -= 1
+            current_node_idx = node_idx_stack[stack_size]
+            current_node = &nodes[current_node_idx]
+
+            if current_node.is_leaf:
+                out[sample_idx] += (weight_stack[stack_size] *
+                                    current_node.value)
+                total_weight += weight_stack[stack_size]
+            else:
+                # determine if the split feature is a target feature
+                is_target_feature = False
+                for feature_idx in range(target_features.shape[0]):
+                    if target_features[feature_idx] == current_node.feature_idx:
+                        is_target_feature = True
+                        break
+
+                if is_target_feature:
+                    # In this case, we push left or right child on stack
+                    if X[sample_idx, feature_idx] <= current_node.num_threshold:
+                        node_idx_stack[stack_size] = current_node.left
+                    else:
+                        node_idx_stack[stack_size] = current_node.right
+                    stack_size += 1
+                else:
+                    # In this case, we push both children onto the stack,
+                    # and give a weight proportional to the number of
+                    # samples going through each branch.
+
+                    # push left child
+                    node_idx_stack[stack_size] = current_node.left
+                    left_sample_frac = (
+                        <Y_DTYPE_C> nodes[current_node.left].count /
+                        current_node.count)
+                    current_weight = weight_stack[stack_size]
+                    weight_stack[stack_size] = current_weight * left_sample_frac
+                    stack_size += 1
+
+                    # push right child
+                    node_idx_stack[stack_size] = current_node.right
+                    weight_stack[stack_size] = (
+                        current_weight * (1 - left_sample_frac))
+                    stack_size += 1
+
+        # Sanity check. Should never happen.
+        if not (0.999 < total_weight < 1.001):
+            raise ValueError("Total weight should be 1.0 but was %.9f" %
+                                total_weight)
('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,13 +1,9 @@
-# cython: cdivision=True
-# cython: boundscheck=False
-# cython: wraparound=False
-# cython: language_level=3
 """This module contains routines and data structures to:

 - Find the best possible split of a node. For a given node, a split is
   characterized by a feature and a bin.
 - Apply a split to a node, i.e. split the indices of the samples at the node
-  into the newly created left and right childs.
+  into the newly created left and right children.
 """
 # Author: Nicolas Hug

@@ -15,15 +11,22 @@
 from cython.parallel import prange
 import numpy as np
 cimport numpy as np
-IF SKLEARN_OPENMP_SUPPORTED:
-    from openmp cimport omp_get_max_threads
-from libc.stdlib cimport malloc, free
+from libc.stdlib cimport malloc, free, qsort
 from libc.string cimport memcpy
-
-from .types cimport X_BINNED_DTYPE_C
-from .types cimport Y_DTYPE_C
-from .types cimport hist_struct
-from .types import HISTOGRAM_DTYPE
+from numpy.math cimport INFINITY
+
+from .common cimport X_BINNED_DTYPE_C
+from .common cimport Y_DTYPE_C
+from .common cimport hist_struct
+from .common import HISTOGRAM_DTYPE
+from .common cimport BITSET_INNER_DTYPE_C
+from .common cimport BITSET_DTYPE_C
+from .common cimport MonotonicConstraint
+from ._bitset cimport init_bitset
+from ._bitset cimport set_bitset
+from ._bitset cimport in_bitset
+
+np.import_array()


 cdef struct split_info_struct:
@@ -32,12 +35,24 @@
     Y_DTYPE_C gain
     int feature_idx
     unsigned int bin_idx
+    unsigned char missing_go_to_left
     Y_DTYPE_C sum_gradient_left
     Y_DTYPE_C sum_gradient_right
     Y_DTYPE_C sum_hessian_left
     Y_DTYPE_C sum_hessian_right
     unsigned int n_samples_left
     unsigned int n_samples_right
+    Y_DTYPE_C value_left
+    Y_DTYPE_C value_right
+    unsigned char is_categorical
+    BITSET_DTYPE_C left_cat_bitset
+
+
+# used in categorical splits for sorting categories by increasing values of
+# sum_gradients / sum_hessians
+cdef struct categorical_info:
+    X_BINNED_DTYPE_C bin_idx
+    Y_DTYPE_C value


 class SplitInfo:
@@ -50,7 +65,12 @@
     feature_idx : int
         The index of the feature to be split.
     bin_idx : int
-        The index of the bin on which the split is made.
+        The index of the bin on which the split is made. Should be ignored if
+        `is_categorical` is True: `left_cat_bitset` will be used to determine
+        the split.
+    missing_go_to_left : bool
+        Whether missing values should go to the left child. This is used
+        whether the split is categorical or not.
     sum_gradient_left : float
         The sum of the gradients of all the samples in the left child.
     sum_hessian_left : float
@@ -63,19 +83,35 @@
         The number of samples in the left child.
     n_samples_right : int
         The number of samples in the right child.
+    is_categorical : bool
+        Whether the split is done on a categorical feature.
+    left_cat_bitset : ndarray of shape=(8,), dtype=uint32 or None
+        Bitset representing the categories that go to the left. This is used
+        only when `is_categorical` is True.
+        Note that missing values are part of that bitset if there are missing
+        values in the training data. For missing values, we rely on that
+        bitset for splitting, but at prediction time, we rely on
+        missing_go_to_left.
     """
-    def __init__(self, gain, feature_idx, bin_idx, sum_gradient_left,
-                 sum_hessian_left, sum_gradient_right, sum_hessian_right,
-                 n_samples_left, n_samples_right):
+    def __init__(self, gain, feature_idx, bin_idx,
+                 missing_go_to_left, sum_gradient_left, sum_hessian_left,
+                 sum_gradient_right, sum_hessian_right, n_samples_left,
+                 n_samples_right, value_left, value_right,
+                 is_categorical, left_cat_bitset):
         self.gain = gain
         self.feature_idx = feature_idx
         self.bin_idx = bin_idx
+        self.missing_go_to_left = missing_go_to_left
         self.sum_gradient_left = sum_gradient_left
         self.sum_hessian_left = sum_hessian_left
         self.sum_gradient_right = sum_gradient_right
         self.sum_hessian_right = sum_hessian_right
         self.n_samples_left = n_samples_left
         self.n_samples_right = n_samples_right
+        self.value_left = value_left
+        self.value_right = value_right
+        self.is_categorical = is_categorical
+        self.left_cat_bitset = left_cat_bitset


 @cython.final
@@ -91,12 +127,18 @@
     ----------
     X_binned : ndarray of int, shape (n_samples, n_features)
         The binned input samples. Must be Fortran-aligned.
-    max_bins : int
-        The maximum number of bins. Used to define the shape of the
-        histograms.
-    actual_n_bins : ndarray, shape (n_features,)
-        The actual number of bins needed for each feature, which is lower or
-        equal to max_bins.
+    n_bins_non_missing : ndarray, shape (n_features,)
+        For each feature, gives the number of bins actually used for
+        non-missing values.
+    missing_values_bin_idx : uint8
+        Index of the bin that is used for missing values. This is the index of
+        the last bin and is always equal to max_bins (as passed to the GBDT
+        classes), or equivalently to n_bins - 1.
+    has_missing_values : ndarray, shape (n_features,)
+        Whether missing values were observed in the training data, for each
+        feature.
+    is_categorical : ndarray of bool of shape (n_features,)
+        Indicates categorical features.
     l2_regularization : float
         The L2 regularization parameter.
     min_hessian_to_split : float, default=1e-3
@@ -114,8 +156,11 @@
     cdef public:
         const X_BINNED_DTYPE_C [::1, :] X_binned
         unsigned int n_features
-        unsigned int max_bins
-        unsigned int [::1] actual_n_bins
+        const unsigned int [::1] n_bins_non_missing
+        unsigned char missing_values_bin_idx
+        const unsigned char [::1] has_missing_values
+        const unsigned char [::1] is_categorical
+        const signed char [::1] monotonic_cst
         unsigned char hessians_are_constant
         Y_DTYPE_C l2_regularization
         Y_DTYPE_C min_hessian_to_split
@@ -125,28 +170,38 @@
         unsigned int [::1] partition
         unsigned int [::1] left_indices_buffer
         unsigned int [::1] right_indices_buffer
-
-    def __init__(self, const X_BINNED_DTYPE_C [::1, :] X_binned, unsigned int
-                 max_bins, np.ndarray[np.uint32_t] actual_n_bins,
-                 Y_DTYPE_C l2_regularization, Y_DTYPE_C
-                 min_hessian_to_split=1e-3, unsigned int
-                 min_samples_leaf=20, Y_DTYPE_C min_gain_to_split=0.,
-                 unsigned char hessians_are_constant=False):
+        int n_threads
+
+    def __init__(self,
+                 const X_BINNED_DTYPE_C [::1, :] X_binned,
+                 const unsigned int [::1] n_bins_non_missing,
+                 const unsigned char missing_values_bin_idx,
+                 const unsigned char [::1] has_missing_values,
+                 const unsigned char [::1] is_categorical,
+                 const signed char [::1] monotonic_cst,
+                 Y_DTYPE_C l2_regularization,
+                 Y_DTYPE_C min_hessian_to_split=1e-3,
+                 unsigned int min_samples_leaf=20,
+                 Y_DTYPE_C min_gain_to_split=0.,
+                 unsigned char hessians_are_constant=False,
+                 unsigned int n_threads=1):

         self.X_binned = X_binned
         self.n_features = X_binned.shape[1]
-        # Note: all histograms will have <max_bins> bins, but some of the
-        # last bins may be unused if actual_n_bins[f] < max_bins
-        self.max_bins = max_bins
-        self.actual_n_bins = actual_n_bins
+        self.n_bins_non_missing = n_bins_non_missing
+        self.missing_values_bin_idx = missing_values_bin_idx
+        self.has_missing_values = has_missing_values
+        self.monotonic_cst = monotonic_cst
+        self.is_categorical = is_categorical
         self.l2_regularization = l2_regularization
         self.min_hessian_to_split = min_hessian_to_split
         self.min_samples_leaf = min_samples_leaf
         self.min_gain_to_split = min_gain_to_split
         self.hessians_are_constant = hessians_are_constant
+        self.n_threads = n_threads

         # The partition array maps each sample index into the leaves of the
-        # tree (a leaf in this context is a node that isn't splitted yet, not
+        # tree (a leaf in this context is a node that isn't split yet, not
         # necessarily a 'finalized' leaf). Initially, the root contains all
         # the indices, e.g.:
         # partition = [abcdefghijkl]
@@ -235,16 +290,19 @@
         cdef:
             int n_samples = sample_indices.shape[0]
             X_BINNED_DTYPE_C bin_idx = split_info.bin_idx
+            unsigned char missing_go_to_left = split_info.missing_go_to_left
+            unsigned char missing_values_bin_idx = self.missing_values_bin_idx
             int feature_idx = split_info.feature_idx
             const X_BINNED_DTYPE_C [::1] X_binned = \
                 self.X_binned[:, feature_idx]
             unsigned int [::1] left_indices_buffer = self.left_indices_buffer
             unsigned int [::1] right_indices_buffer = self.right_indices_buffer
-
-            IF SKLEARN_OPENMP_SUPPORTED:
-                int n_threads = omp_get_max_threads()
-            ELSE:
-                int n_threads = 1
+            unsigned char is_categorical = split_info.is_categorical
+            # Cython is unhappy if we set left_cat_bitset to
+            # split_info.left_cat_bitset directly, so we need a tmp var
+            BITSET_INNER_DTYPE_C [:] cat_bitset_tmp = split_info.left_cat_bitset
+            BITSET_DTYPE_C left_cat_bitset
+            int n_threads = self.n_threads

             int [:] sizes = np.full(n_threads, n_samples // n_threads,
                                     dtype=np.int32)
@@ -259,8 +317,13 @@
             int thread_idx
             int sample_idx
             int right_child_position
+            unsigned char turn_left
             int [:] left_offset = np.zeros(n_threads, dtype=np.int32)
             int [:] right_offset = np.zeros(n_threads, dtype=np.int32)
+
+        # only set left_cat_bitset when is_categorical is True
+        if is_categorical:
+            left_cat_bitset = &cat_bitset_tmp[0]

         with nogil:
             for thread_idx in range(n_samples % n_threads):
@@ -272,7 +335,7 @@

             # map indices from sample_indices to left/right_indices_buffer
             for thread_idx in prange(n_threads, schedule='static',
-                                     chunksize=1):
+                                     chunksize=1, num_threads=n_threads):
                 left_count = 0
                 right_count = 0

@@ -280,7 +343,13 @@
                 stop = start + sizes[thread_idx]
                 for i in range(start, stop):
                     sample_idx = sample_indices[i]
-                    if X_binned[sample_idx] <= bin_idx:
+                    turn_left = sample_goes_left(
+                        missing_go_to_left,
+                        missing_values_bin_idx, bin_idx,
+                        X_binned[sample_idx], is_categorical,
+                        left_cat_bitset)
+
+                    if turn_left:
                         left_indices_buffer[start + left_count] = sample_idx
                         left_count = left_count + 1
                     else:
@@ -308,17 +377,31 @@
             # sample_indices. This also updates self.partition since
             # sample_indices is a view.
             for thread_idx in prange(n_threads, schedule='static',
-                                     chunksize=1):
+                                     chunksize=1, num_threads=n_threads):
                 memcpy(
                     &sample_indices[left_offset[thread_idx]],
                     &left_indices_buffer[offset_in_buffers[thread_idx]],
                     sizeof(unsigned int) * left_counts[thread_idx]
                 )
-                memcpy(
-                    &sample_indices[right_offset[thread_idx]],
-                    &right_indices_buffer[offset_in_buffers[thread_idx]],
-                    sizeof(unsigned int) * right_counts[thread_idx]
-                )
+                if right_counts[thread_idx] > 0:
+                    # If we're splitting the rightmost node of the tree, i.e. the
+                    # rightmost node in the partition array, and if n_threads >= 2, one
+                    # might have right_counts[-1] = 0 and right_offset[-1] = len(sample_indices)
+                    # leading to evaluating
+                    #
+                    #    &sample_indices[right_offset[-1]] = &samples_indices[n_samples_at_node]
+                    #                                      = &partition[n_samples_in_tree]
+                    #
+                    # which is an out-of-bounds read access that can cause a segmentation fault.
+                    # When boundscheck=True, removing this check produces this exception:
+                    #
+                    #    IndexError: Out of bounds on buffer access
+                    #
+                    memcpy(
+                        &sample_indices[right_offset[thread_idx]],
+                        &right_indices_buffer[offset_in_buffers[thread_idx]],
+                        sizeof(unsigned int) * right_counts[thread_idx]
+                    )

         return (sample_indices[:right_child_position],
                 sample_indices[right_child_position:],
@@ -326,18 +409,22 @@

     def find_node_split(
             Splitter self,
-            const unsigned int [::1] sample_indices,  # IN
+            unsigned int n_samples,
             hist_struct [:, ::1] histograms,  # IN
             const Y_DTYPE_C sum_gradients,
-            const Y_DTYPE_C sum_hessians):
+            const Y_DTYPE_C sum_hessians,
+            const Y_DTYPE_C value,
+            const Y_DTYPE_C lower_bound=-INFINITY,
+            const Y_DTYPE_C upper_bound=INFINITY,
+            ):
         """For each feature, find the best bin to split on at a given node.

         Return the best split info among all features.

         Parameters
         ----------
-        sample_indices : ndarray of unsigned int, shape (n_samples_at_node,)
-            The indices of the samples at the node to split.
+        n_samples : int
+            The number of samples at the node.
         histograms : ndarray of HISTOGRAM_DTYPE of \
                 shape (n_features, max_bins)
             The histograms of the current node.
@@ -345,6 +432,22 @@
             The sum of the gradients for each sample at the node.
         sum_hessians : float
             The sum of the hessians for each sample at the node.
+        value : float
+            The bounded value of the current node. We directly pass the value
+            instead of re-computing it from sum_gradients and sum_hessians,
+            because we need to compute the loss and the gain based on the
+            *bounded* value: computing the value from
+            sum_gradients / sum_hessians would give the unbounded value, and
+            the interaction with min_gain_to_split would not be correct
+            anymore. Side note: we can't use the lower_bound / upper_bound
+            parameters either because these refer to the bounds of the
+            children, not the bounds of the current node.
+        lower_bound : float
+            Lower bound for the children values for respecting the monotonic
+            constraints.
+        upper_bound : float
+            Upper bound for the children values for respecting the monotonic
+            constraints.

         Returns
         -------
@@ -352,25 +455,65 @@
             The info about the best possible split among all features.
         """
         cdef:
-            int n_samples
             int feature_idx
             int best_feature_idx
             int n_features = self.n_features
             split_info_struct split_info
             split_info_struct * split_infos
+            const unsigned char [::1] has_missing_values = self.has_missing_values
+            const unsigned char [::1] is_categorical = self.is_categorical
+            const signed char [::1] monotonic_cst = self.monotonic_cst
+            int n_threads = self.n_threads

         with nogil:
-            n_samples = sample_indices.shape[0]

             split_infos = <split_info_struct *> malloc(
                 self.n_features * sizeof(split_info_struct))

-            for feature_idx in prange(n_features, schedule='static'):
+            for feature_idx in prange(n_features, schedule='static',
+                                      num_threads=n_threads):
+                split_infos[feature_idx].feature_idx = feature_idx
+
                 # For each feature, find best bin to split on
-                split_info = self._find_best_bin_to_split_helper(
-                    feature_idx, histograms, n_samples,
-                    sum_gradients, sum_hessians)
-                split_infos[feature_idx] = split_info
+                # Start with a gain of -1 (if no better split is found, that
+                # means one of the constraints isn't respected
+                # (min_samples_leaf, etc) and the grower will later turn the
+                # node into a leaf.
+                split_infos[feature_idx].gain = -1
+                split_infos[feature_idx].is_categorical = is_categorical[feature_idx]
+
+                if is_categorical[feature_idx]:
+                    self._find_best_bin_to_split_category(
+                        feature_idx, has_missing_values[feature_idx],
+                        histograms, n_samples, sum_gradients, sum_hessians,
+                        value, monotonic_cst[feature_idx], lower_bound,
+                        upper_bound, &split_infos[feature_idx])
+                else:
+                    # We will scan bins from left to right (in all cases), and
+                    # if there are any missing values, we will also scan bins
+                    # from right to left. This way, we can consider whichever
+                    # case yields the best gain: either missing values go to
+                    # the right (left to right scan) or to the left (right to
+                    # left case). See algo 3 from the XGBoost paper
+                    # https://arxiv.org/abs/1603.02754
+                    # Note: for the categorical features above, this isn't
+                    # needed since missing values are considered a native
+                    # category.
+                    self._find_best_bin_to_split_left_to_right(
+                        feature_idx, has_missing_values[feature_idx],
+                        histograms, n_samples, sum_gradients, sum_hessians,
+                        value, monotonic_cst[feature_idx],
+                        lower_bound, upper_bound, &split_infos[feature_idx])
+
+                    if has_missing_values[feature_idx]:
+                        # We need to explore both directions to check whether
+                        # sending the nans to the left child would lead to a higher
+                        # gain
+                        self._find_best_bin_to_split_right_to_left(
+                            feature_idx, histograms, n_samples,
+                            sum_gradients, sum_hessians,
+                            value, monotonic_cst[feature_idx],
+                            lower_bound, upper_bound, &split_infos[feature_idx])

             # then compute best possible split among all features
             best_feature_idx = self._find_best_feature_to_split_helper(
@@ -381,23 +524,32 @@
             split_info.gain,
             split_info.feature_idx,
             split_info.bin_idx,
+            split_info.missing_go_to_left,
             split_info.sum_gradient_left,
             split_info.sum_hessian_left,
             split_info.sum_gradient_right,
             split_info.sum_hessian_right,
             split_info.n_samples_left,
             split_info.n_samples_right,
+            split_info.value_left,
+            split_info.value_right,
+            split_info.is_categorical,
+            None,  # left_cat_bitset will only be set if the split is categorical
         )
+        # Only set bitset if the split is categorical
+        if split_info.is_categorical:
+            out.left_cat_bitset = np.asarray(split_info.left_cat_bitset, dtype=np.uint32)
+
         free(split_infos)
         return out

-    cdef int _find_best_feature_to_split_helper(
+    cdef unsigned int _find_best_feature_to_split_helper(
             self,
             split_info_struct * split_infos) nogil:  # IN
         """Returns the best feature among those in splits_infos."""
         cdef:
-            int feature_idx
-            int best_feature_idx = 0
+            unsigned int feature_idx
+            unsigned int best_feature_idx = 0

         for feature_idx in range(1, self.n_features):
             if (split_infos[feature_idx].gain >
@@ -405,21 +557,146 @@
                 best_feature_idx = feature_idx
         return best_feature_idx

-    cdef split_info_struct _find_best_bin_to_split_helper(
+    cdef void _find_best_bin_to_split_left_to_right(
+            Splitter self,
+            unsigned int feature_idx,
+            unsigned char has_missing_values,
+            const hist_struct [:, ::1] histograms,  # IN
+            unsigned int n_samples,
+            Y_DTYPE_C sum_gradients,
+            Y_DTYPE_C sum_hessians,
+            Y_DTYPE_C value,
+            signed char monotonic_cst,
+            Y_DTYPE_C lower_bound,
+            Y_DTYPE_C upper_bound,
+            split_info_struct * split_info) nogil:  # OUT
+        """Find best bin to split on for a given feature.
+
+        Splits that do not satisfy the splitting constraints
+        (min_gain_to_split, etc.) are discarded here.
+
+        We scan node from left to right. This version is called whether there
+        are missing values or not. If any, missing values are assigned to the
+        right node.
+        """
+        cdef:
+            unsigned int bin_idx
+            unsigned int n_samples_left
+            unsigned int n_samples_right
+            unsigned int n_samples_ = n_samples
+            # We set the 'end' variable such that the last non-missing-values
+            # bin never goes to the left child (which would result in and
+            # empty right child), unless there are missing values, since these
+            # would go to the right child.
+            unsigned int end = \
+                self.n_bins_non_missing[feature_idx] - 1 + has_missing_values
+            Y_DTYPE_C sum_hessian_left
+            Y_DTYPE_C sum_hessian_right
+            Y_DTYPE_C sum_gradient_left
+            Y_DTYPE_C sum_gradient_right
+            Y_DTYPE_C loss_current_node
+            Y_DTYPE_C gain
+            unsigned char found_better_split = False
+
+            Y_DTYPE_C best_sum_hessian_left
+            Y_DTYPE_C best_sum_gradient_left
+            unsigned int best_bin_idx
+            unsigned int best_n_samples_left
+            Y_DTYPE_C best_gain = -1
+
+        sum_gradient_left, sum_hessian_left = 0., 0.
+        n_samples_left = 0
+
+        loss_current_node = _loss_from_value(value, sum_gradients)
+
+        for bin_idx in range(end):
+            n_samples_left += histograms[feature_idx, bin_idx].count
+            n_samples_right = n_samples_ - n_samples_left
+
+            if self.hessians_are_constant:
+                sum_hessian_left += histograms[feature_idx, bin_idx].count
+            else:
+                sum_hessian_left += \
+                    histograms[feature_idx, bin_idx].sum_hessians
+            sum_hessian_right = sum_hessians - sum_hessian_left
+
+            sum_gradient_left += histograms[feature_idx, bin_idx].sum_gradients
+            sum_gradient_right = sum_gradients - sum_gradient_left
+
+            if n_samples_left < self.min_samples_leaf:
+                continue
+            if n_samples_right < self.min_samples_leaf:
+                # won't get any better
+                break
+
+            if sum_hessian_left < self.min_hessian_to_split:
+                continue
+            if sum_hessian_right < self.min_hessian_to_split:
+                # won't get any better (hessians are > 0 since loss is convex)
+                break
+
+            gain = _split_gain(sum_gradient_left, sum_hessian_left,
+                               sum_gradient_right, sum_hessian_right,
+                               loss_current_node,
+                               monotonic_cst,
+                               lower_bound,
+                               upper_bound,
+                               self.l2_regularization)
+
+            if gain > best_gain and gain > self.min_gain_to_split:
+                found_better_split = True
+                best_gain = gain
+                best_bin_idx = bin_idx
+                best_sum_gradient_left = sum_gradient_left
+                best_sum_hessian_left = sum_hessian_left
+                best_n_samples_left = n_samples_left
+
+        if found_better_split:
+            split_info.gain = best_gain
+            split_info.bin_idx = best_bin_idx
+            # we scan from left to right so missing values go to the right
+            split_info.missing_go_to_left = False
+            split_info.sum_gradient_left = best_sum_gradient_left
+            split_info.sum_gradient_right = sum_gradients - best_sum_gradient_left
+            split_info.sum_hessian_left = best_sum_hessian_left
+            split_info.sum_hessian_right = sum_hessians - best_sum_hessian_left
+            split_info.n_samples_left = best_n_samples_left
+            split_info.n_samples_right = n_samples - best_n_samples_left
+
+            # We recompute best values here but it's cheap
+            split_info.value_left = compute_node_value(
+                split_info.sum_gradient_left, split_info.sum_hessian_left,
+                lower_bound, upper_bound, self.l2_regularization)
+
+            split_info.value_right = compute_node_value(
+                split_info.sum_gradient_right, split_info.sum_hessian_right,
+                lower_bound, upper_bound, self.l2_regularization)
+
+    cdef void _find_best_bin_to_split_right_to_left(
             self,
             unsigned int feature_idx,
             const hist_struct [:, ::1] histograms,  # IN
             unsigned int n_samples,
             Y_DTYPE_C sum_gradients,
-            Y_DTYPE_C sum_hessians) nogil:
+            Y_DTYPE_C sum_hessians,
+            Y_DTYPE_C value,
+            signed char monotonic_cst,
+            Y_DTYPE_C lower_bound,
+            Y_DTYPE_C upper_bound,
+            split_info_struct * split_info) nogil:  # OUT
         """Find best bin to split on for a given feature.

         Splits that do not satisfy the splitting constraints
-        (min_gain_to_split, etc.) are discarded here. If no split can
-        satisfy the constraints, a SplitInfo with a gain of -1 is returned.
-        If for a given node the best SplitInfo has a gain of -1, it is
-        finalized into a leaf in the grower.
+        (min_gain_to_split, etc.) are discarded here.
+
+        We scan node from right to left. This version is only called when
+        there are missing values. Missing values are assigned to the left
+        child.
+
+        If no missing value are present in the data this method isn't called
+        since only calling _find_best_bin_to_split_left_to_right is enough.
         """
+
         cdef:
             unsigned int bin_idx
             unsigned int n_samples_left
@@ -429,65 +706,323 @@
             Y_DTYPE_C sum_hessian_right
             Y_DTYPE_C sum_gradient_left
             Y_DTYPE_C sum_gradient_right
+            Y_DTYPE_C loss_current_node
             Y_DTYPE_C gain
-            split_info_struct best_split
-
-        best_split.gain = -1.
-        sum_gradient_left, sum_hessian_left = 0., 0.
-        n_samples_left = 0
-
-        for bin_idx in range(self.actual_n_bins[feature_idx]):
-            n_samples_left += histograms[feature_idx, bin_idx].count
-            n_samples_right = n_samples_ - n_samples_left
+            unsigned int start = self.n_bins_non_missing[feature_idx] - 2
+            unsigned char found_better_split = False
+
+            Y_DTYPE_C best_sum_hessian_left
+            Y_DTYPE_C best_sum_gradient_left
+            unsigned int best_bin_idx
+            unsigned int best_n_samples_left
+            Y_DTYPE_C best_gain = split_info.gain  # computed during previous scan
+
+        sum_gradient_right, sum_hessian_right = 0., 0.
+        n_samples_right = 0
+
+        loss_current_node = _loss_from_value(value, sum_gradients)
+
+        for bin_idx in range(start, -1, -1):
+            n_samples_right += histograms[feature_idx, bin_idx + 1].count
+            n_samples_left = n_samples_ - n_samples_right

             if self.hessians_are_constant:
-                sum_hessian_left += histograms[feature_idx, bin_idx].count
+                sum_hessian_right += histograms[feature_idx, bin_idx + 1].count
             else:
-                sum_hessian_left += \
-                    histograms[feature_idx, bin_idx].sum_hessians
-            sum_hessian_right = sum_hessians - sum_hessian_left
-
-            sum_gradient_left += histograms[feature_idx, bin_idx].sum_gradients
-            sum_gradient_right = sum_gradients - sum_gradient_left
-
+                sum_hessian_right += \
+                    histograms[feature_idx, bin_idx + 1].sum_hessians
+            sum_hessian_left = sum_hessians - sum_hessian_right
+
+            sum_gradient_right += \
+                histograms[feature_idx, bin_idx + 1].sum_gradients
+            sum_gradient_left = sum_gradients - sum_gradient_right
+
+            if n_samples_right < self.min_samples_leaf:
+                continue
             if n_samples_left < self.min_samples_leaf:
-                continue
-            if n_samples_right < self.min_samples_leaf:
                 # won't get any better
                 break

+            if sum_hessian_right < self.min_hessian_to_split:
+                continue
             if sum_hessian_left < self.min_hessian_to_split:
-                continue
-            if sum_hessian_right < self.min_hessian_to_split:
                 # won't get any better (hessians are > 0 since loss is convex)
                 break

             gain = _split_gain(sum_gradient_left, sum_hessian_left,
                                sum_gradient_right, sum_hessian_right,
-                               sum_gradients, sum_hessians,
+                               loss_current_node,
+                               monotonic_cst,
+                               lower_bound,
+                               upper_bound,
                                self.l2_regularization)

-            if gain > best_split.gain and gain > self.min_gain_to_split:
-                best_split.gain = gain
-                best_split.feature_idx = feature_idx
-                best_split.bin_idx = bin_idx
-                best_split.sum_gradient_left = sum_gradient_left
-                best_split.sum_gradient_right = sum_gradient_right
-                best_split.sum_hessian_left = sum_hessian_left
-                best_split.sum_hessian_right = sum_hessian_right
-                best_split.n_samples_left = n_samples_left
-                best_split.n_samples_right = n_samples_right
-
-        return best_split
-
+            if gain > best_gain and gain > self.min_gain_to_split:
+                found_better_split = True
+                best_gain = gain
+                best_bin_idx = bin_idx
+                best_sum_gradient_left = sum_gradient_left
+                best_sum_hessian_left = sum_hessian_left
+                best_n_samples_left = n_samples_left
+
+        if found_better_split:
+            split_info.gain = best_gain
+            split_info.bin_idx = best_bin_idx
+            # we scan from right to left so missing values go to the left
+            split_info.missing_go_to_left = True
+            split_info.sum_gradient_left = best_sum_gradient_left
+            split_info.sum_gradient_right = sum_gradients - best_sum_gradient_left
+            split_info.sum_hessian_left = best_sum_hessian_left
+            split_info.sum_hessian_right = sum_hessians - best_sum_hessian_left
+            split_info.n_samples_left = best_n_samples_left
+            split_info.n_samples_right = n_samples - best_n_samples_left
+
+            # We recompute best values here but it's cheap
+            split_info.value_left = compute_node_value(
+                split_info.sum_gradient_left, split_info.sum_hessian_left,
+                lower_bound, upper_bound, self.l2_regularization)
+
+            split_info.value_right = compute_node_value(
+                split_info.sum_gradient_right, split_info.sum_hessian_right,
+                lower_bound, upper_bound, self.l2_regularization)
+
+    cdef void _find_best_bin_to_split_category(
+            self,
+            unsigned int feature_idx,
+            unsigned char has_missing_values,
+            const hist_struct [:, ::1] histograms,  # IN
+            unsigned int n_samples,
+            Y_DTYPE_C sum_gradients,
+            Y_DTYPE_C sum_hessians,
+            Y_DTYPE_C value,
+            char monotonic_cst,
+            Y_DTYPE_C lower_bound,
+            Y_DTYPE_C upper_bound,
+            split_info_struct * split_info) nogil:  # OUT
+        """Find best split for categorical features.
+
+        Categories are first sorted according to their variance, and then
+        a scan is performed as if categories were ordered quantities.
+
+        Ref: "On Grouping for Maximum Homogeneity", Walter D. Fisher
+        """
+
+        cdef:
+            unsigned int bin_idx
+            unsigned int n_bins_non_missing = self.n_bins_non_missing[feature_idx]
+            unsigned int missing_values_bin_idx = self.missing_values_bin_idx
+            categorical_info * cat_infos
+            unsigned int sorted_cat_idx
+            unsigned int n_used_bins = 0
+            int [2] scan_direction
+            int direction = 0
+            int best_direction = 0
+            unsigned int middle
+            unsigned int i
+            const hist_struct[::1] feature_hist = histograms[feature_idx, :]
+            Y_DTYPE_C sum_gradients_bin
+            Y_DTYPE_C sum_hessians_bin
+            Y_DTYPE_C loss_current_node
+            Y_DTYPE_C sum_gradient_left, sum_hessian_left
+            Y_DTYPE_C sum_gradient_right, sum_hessian_right
+            unsigned int n_samples_left, n_samples_right
+            Y_DTYPE_C gain
+            Y_DTYPE_C best_gain = -1.0
+            unsigned char found_better_split = False
+            Y_DTYPE_C best_sum_hessian_left
+            Y_DTYPE_C best_sum_gradient_left
+            unsigned int best_n_samples_left
+            unsigned int best_cat_infos_thresh
+            # Reduces the effect of noises in categorical features,
+            # especially for categories with few data. Called cat_smooth in
+            # LightGBM. TODO: Make this user adjustable?
+            Y_DTYPE_C MIN_CAT_SUPPORT = 10.
+            # this is equal to 1 for losses where hessians are constant
+            Y_DTYPE_C support_factor = n_samples / sum_hessians
+
+        # Details on the split finding:
+        # We first order categories by their sum_gradients / sum_hessians
+        # values, and we exclude categories that don't respect MIN_CAT_SUPPORT
+        # from this sorted array. Missing values are treated just like any
+        # other category. The low-support categories will always be mapped to
+        # the right child. We scan the sorted categories array from left to
+        # right and from right to left, and we stop at the middle.
+
+        # Considering ordered categories A B C D, with E being a low-support
+        # category: A B C D
+        #              ^
+        #           midpoint
+        # The scans will consider the following split-points:
+        # * left to right:
+        #   A - B C D E
+        #   A B - C D E
+        # * right to left:
+        #   D - A B C E
+        #   C D - A B E
+
+        # Note that since we stop at the middle and since low-support
+        # categories (E) are always mapped to the right, the following splits
+        # aren't considered:
+        # A E - B C D
+        # D E - A B C
+        # Basically, we're forcing E to always be mapped to the child that has
+        # *at least half of the categories* (and this child is always the right
+        # child, by convention).
+
+        # Also note that if we scanned in only one direction (e.g. left to
+        # right), we would only consider the following splits:
+        # A - B C D E
+        # A B - C D E
+        # A B C - D E
+        # and thus we would be missing on D - A B C E and on C D - A B E
+
+        cat_infos = <categorical_info *> malloc(
+            (n_bins_non_missing + has_missing_values) * sizeof(categorical_info))
+
+        # fill cat_infos while filtering out categories based on MIN_CAT_SUPPORT
+        for bin_idx in range(n_bins_non_missing):
+            if self.hessians_are_constant:
+                sum_hessians_bin = feature_hist[bin_idx].count
+            else:
+                sum_hessians_bin = feature_hist[bin_idx].sum_hessians
+            if sum_hessians_bin * support_factor >= MIN_CAT_SUPPORT:
+                cat_infos[n_used_bins].bin_idx = bin_idx
+                sum_gradients_bin = feature_hist[bin_idx].sum_gradients
+
+                cat_infos[n_used_bins].value = (
+                    sum_gradients_bin / (sum_hessians_bin + MIN_CAT_SUPPORT)
+                )
+                n_used_bins += 1
+
+        # Also add missing values bin so that nans are considered as a category
+        if has_missing_values:
+            if self.hessians_are_constant:
+                sum_hessians_bin = feature_hist[missing_values_bin_idx].count
+            else:
+                sum_hessians_bin = feature_hist[missing_values_bin_idx].sum_hessians
+            if sum_hessians_bin * support_factor >= MIN_CAT_SUPPORT:
+                cat_infos[n_used_bins].bin_idx = missing_values_bin_idx
+                sum_gradients_bin = (
+                    feature_hist[missing_values_bin_idx].sum_gradients
+                )
+
+                cat_infos[n_used_bins].value = (
+                    sum_gradients_bin / (sum_hessians_bin + MIN_CAT_SUPPORT)
+                )
+                n_used_bins += 1
+
+        # not enough categories to form a split
+        if n_used_bins <= 1:
+            free(cat_infos)
+            return
+
+        qsort(cat_infos, n_used_bins, sizeof(categorical_info),
+              compare_cat_infos)
+
+        loss_current_node = _loss_from_value(value, sum_gradients)
+
+        scan_direction[0], scan_direction[1] = 1, -1
+        for direction in scan_direction:
+            if direction == 1:
+                middle = (n_used_bins + 1) // 2
+            else:
+                middle = (n_used_bins + 1) // 2 - 1
+
+            # The categories we'll consider will go to the left child
+            sum_gradient_left, sum_hessian_left = 0., 0.
+            n_samples_left = 0
+
+            for i in range(middle):
+                sorted_cat_idx = i if direction == 1 else n_used_bins - 1 - i
+                bin_idx = cat_infos[sorted_cat_idx].bin_idx;
+
+                n_samples_left += feature_hist[bin_idx].count
+                n_samples_right = n_samples - n_samples_left
+
+                if self.hessians_are_constant:
+                    sum_hessian_left += feature_hist[bin_idx].count
+                else:
+                    sum_hessian_left += feature_hist[bin_idx].sum_hessians
+                sum_hessian_right = sum_hessians - sum_hessian_left
+
+                sum_gradient_left += feature_hist[bin_idx].sum_gradients
+                sum_gradient_right = sum_gradients - sum_gradient_left
+
+                if (n_samples_left < self.min_samples_leaf or
+                    sum_hessian_left < self.min_hessian_to_split):
+                    continue
+                if (n_samples_right < self.min_samples_leaf or
+                    sum_hessian_right < self.min_hessian_to_split):
+                    break
+
+                gain = _split_gain(sum_gradient_left, sum_hessian_left,
+                                    sum_gradient_right, sum_hessian_right,
+                                    loss_current_node, monotonic_cst,
+                                    lower_bound, upper_bound,
+                                    self.l2_regularization)
+                if gain > best_gain and gain > self.min_gain_to_split:
+                    found_better_split = True
+                    best_gain = gain
+                    best_cat_infos_thresh = sorted_cat_idx
+                    best_sum_gradient_left = sum_gradient_left
+                    best_sum_hessian_left = sum_hessian_left
+                    best_n_samples_left = n_samples_left
+                    best_direction = direction
+
+
+        if found_better_split:
+            split_info.gain = best_gain
+
+            # split_info.bin_idx is unused for categorical splits: left_cat_bitset
+            # is used instead and set below
+            split_info.bin_idx = 0
+
+            split_info.sum_gradient_left = best_sum_gradient_left
+            split_info.sum_gradient_right = sum_gradients - best_sum_gradient_left
+            split_info.sum_hessian_left = best_sum_hessian_left
+            split_info.sum_hessian_right = sum_hessians - best_sum_hessian_left
+            split_info.n_samples_left = best_n_samples_left
+            split_info.n_samples_right = n_samples - best_n_samples_left
+
+            # We recompute best values here but it's cheap
+            split_info.value_left = compute_node_value(
+                split_info.sum_gradient_left, split_info.sum_hessian_left,
+                lower_bound, upper_bound, self.l2_regularization)
+
+            split_info.value_right = compute_node_value(
+                split_info.sum_gradient_right, split_info.sum_hessian_right,
+                lower_bound, upper_bound, self.l2_regularization)
+
+            # create bitset with values from best_cat_infos_thresh
+            init_bitset(split_info.left_cat_bitset)
+            if best_direction == 1:
+                for sorted_cat_idx in range(best_cat_infos_thresh + 1):
+                    bin_idx = cat_infos[sorted_cat_idx].bin_idx
+                    set_bitset(split_info.left_cat_bitset, bin_idx)
+            else:
+                for sorted_cat_idx in range(n_used_bins - 1, best_cat_infos_thresh - 1, -1):
+                    bin_idx = cat_infos[sorted_cat_idx].bin_idx
+                    set_bitset(split_info.left_cat_bitset, bin_idx)
+
+            if has_missing_values:
+                split_info.missing_go_to_left = in_bitset(
+                    split_info.left_cat_bitset, missing_values_bin_idx)
+
+        free(cat_infos)
+
+
+cdef int compare_cat_infos(const void * a, const void * b) nogil:
+    return -1 if (<categorical_info *>a).value < (<categorical_info *>b).value else 1

 cdef inline Y_DTYPE_C _split_gain(
         Y_DTYPE_C sum_gradient_left,
         Y_DTYPE_C sum_hessian_left,
         Y_DTYPE_C sum_gradient_right,
         Y_DTYPE_C sum_hessian_right,
-        Y_DTYPE_C sum_gradients,
-        Y_DTYPE_C sum_hessians,
+        Y_DTYPE_C loss_current_node,
+        signed char monotonic_cst,
+        Y_DTYPE_C lower_bound,
+        Y_DTYPE_C upper_bound,
         Y_DTYPE_C l2_regularization) nogil:
     """Loss reduction

@@ -495,20 +1030,97 @@
     the node a leaf of the tree.

     See Equation 7 of:
-    XGBoost: A Scalable Tree Boosting System, T. Chen, C. Guestrin, 2016
-    https://arxiv.org/abs/1603.02754
+    :arxiv:`T. Chen, C. Guestrin, (2016) XGBoost: A Scalable Tree Boosting System,
+    <1603.02754>.`
     """
     cdef:
         Y_DTYPE_C gain
-    gain = negative_loss(sum_gradient_left, sum_hessian_left,
-                         l2_regularization)
-    gain += negative_loss(sum_gradient_right, sum_hessian_right,
-                          l2_regularization)
-    gain -= negative_loss(sum_gradients, sum_hessians, l2_regularization)
+        Y_DTYPE_C value_left
+        Y_DTYPE_C value_right
+
+    # Compute values of potential left and right children
+    value_left = compute_node_value(sum_gradient_left, sum_hessian_left,
+                                    lower_bound, upper_bound,
+                                    l2_regularization)
+    value_right = compute_node_value(sum_gradient_right, sum_hessian_right,
+                                    lower_bound, upper_bound,
+                                    l2_regularization)
+
+    if ((monotonic_cst == MonotonicConstraint.POS and value_left > value_right) or
+            (monotonic_cst == MonotonicConstraint.NEG and value_left < value_right)):
+        # don't consider this split since it does not respect the monotonic
+        # constraints. Note that these comparisons need to be done on values
+        # that have already been clipped to take the monotonic constraints into
+        # account (if any).
+        return -1
+
+    gain = loss_current_node
+    gain -= _loss_from_value(value_left, sum_gradient_left)
+    gain -= _loss_from_value(value_right, sum_gradient_right)
+    # Note that for the gain to be correct (and for min_gain_to_split to work
+    # as expected), we need all values to be bounded (current node, left child
+    # and right child).
+
     return gain

-cdef inline Y_DTYPE_C negative_loss(
-        Y_DTYPE_C gradient,
-        Y_DTYPE_C hessian,
+cdef inline Y_DTYPE_C _loss_from_value(
+        Y_DTYPE_C value,
+        Y_DTYPE_C sum_gradient) nogil:
+    """Return loss of a node from its (bounded) value
+
+    See Equation 6 of:
+    :arxiv:`T. Chen, C. Guestrin, (2016) XGBoost: A Scalable Tree Boosting System,
+    <1603.02754>.`
+    """
+    return sum_gradient * value
+
+cdef inline unsigned char sample_goes_left(
+        unsigned char missing_go_to_left,
+        unsigned char missing_values_bin_idx,
+        X_BINNED_DTYPE_C split_bin_idx,
+        X_BINNED_DTYPE_C bin_value,
+        unsigned char is_categorical,
+        BITSET_DTYPE_C left_cat_bitset) nogil:
+    """Helper to decide whether sample should go to left or right child."""
+
+    if is_categorical:
+        # note: if any, missing values are encoded in left_cat_bitset
+        return in_bitset(left_cat_bitset, bin_value)
+    else:
+        return (
+            (
+                missing_go_to_left and
+                bin_value == missing_values_bin_idx
+            )
+            or (
+                bin_value <= split_bin_idx
+            ))
+
+
+cpdef inline Y_DTYPE_C compute_node_value(
+        Y_DTYPE_C sum_gradient,
+        Y_DTYPE_C sum_hessian,
+        Y_DTYPE_C lower_bound,
+        Y_DTYPE_C upper_bound,
         Y_DTYPE_C l2_regularization) nogil:
-    return (gradient * gradient) / (hessian + l2_regularization)
+    """Compute a node's value.
+
+    The value is capped in the [lower_bound, upper_bound] interval to respect
+    monotonic constraints. Shrinkage is ignored.
+
+    See Equation 5 of:
+    :arxiv:`T. Chen, C. Guestrin, (2016) XGBoost: A Scalable Tree Boosting System,
+    <1603.02754>.`
+    """
+
+    cdef:
+        Y_DTYPE_C value
+
+    value = -sum_gradient / (sum_hessian + l2_regularization + 1e-15)
+
+    if value < lower_bound:
+        value = lower_bound
+    elif value > upper_bound:
+        value = upper_bound
+
+    return value
('sklearn/ensemble/_hist_gradient_boosting', 'histogram.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,7 +1,3 @@
-# cython: cdivision=True
-# cython: boundscheck=False
-# cython: wraparound=False
-# cython: language_level=3
 """This module contains routines for building histograms."""

 # Author: Nicolas Hug
@@ -12,11 +8,12 @@
 import numpy as np
 cimport numpy as np

-from .types import HISTOGRAM_DTYPE
-from .types cimport hist_struct
-from .types cimport X_BINNED_DTYPE_C
-from .types cimport G_H_DTYPE_C
-from .types cimport hist_struct
+from .common import HISTOGRAM_DTYPE
+from .common cimport hist_struct
+from .common cimport X_BINNED_DTYPE_C
+from .common cimport G_H_DTYPE_C
+
+np.import_array()

 # Notes:
 # - IN views are read-only, OUT views are write-only
@@ -52,7 +49,7 @@

     There are different ways to build a histogram:
     - by subtraction: hist(child) = hist(parent) - hist(sibling)
-    - from scratch. In this case we have rountines that update the hessians
+    - from scratch. In this case we have routines that update the hessians
       or not (not useful when hessians are constant for some losses e.g.
       least squares). Also, there's a special case for the root which
       contains all the samples, leading to some possible optimizations.
@@ -63,9 +60,9 @@
     ----------
     X_binned : ndarray of int, shape (n_samples, n_features)
         The binned input samples. Must be Fortran-aligned.
-    max_bins : int
-        The maximum number of bins. Used to define the shape of the
-        histograms.
+    n_bins : int
+        The total number of bins, including the bin for missing values. Used
+        to define the shape of the histograms.
     gradients : ndarray, shape (n_samples,)
         The gradients of each training sample. Those are the gradients of the
         loss w.r.t the predictions, evaluated at iteration i - 1.
@@ -78,29 +75,32 @@
     cdef public:
         const X_BINNED_DTYPE_C [::1, :] X_binned
         unsigned int n_features
-        unsigned int max_bins
+        unsigned int n_bins
         G_H_DTYPE_C [::1] gradients
         G_H_DTYPE_C [::1] hessians
         G_H_DTYPE_C [::1] ordered_gradients
         G_H_DTYPE_C [::1] ordered_hessians
         unsigned char hessians_are_constant
+        int n_threads

     def __init__(self, const X_BINNED_DTYPE_C [::1, :] X_binned,
-                 unsigned int max_bins, G_H_DTYPE_C [::1] gradients,
+                 unsigned int n_bins, G_H_DTYPE_C [::1] gradients,
                  G_H_DTYPE_C [::1] hessians,
-                 unsigned char hessians_are_constant):
+                 unsigned char hessians_are_constant,
+                 int n_threads):

         self.X_binned = X_binned
         self.n_features = X_binned.shape[1]
-        # Note: all histograms will have <max_bins> bins, but some of the
-        # last bins may be unused if actual_n_bins[f] < max_bins
-        self.max_bins = max_bins
+        # Note: all histograms will have <n_bins> bins, but some of the
+        # bins may be unused if a feature has a small number of unique values.
+        self.n_bins = n_bins
         self.gradients = gradients
         self.hessians = hessians
         # for root node, gradients and hessians are already ordered
         self.ordered_gradients = gradients.copy()
         self.ordered_hessians = hessians.copy()
         self.hessians_are_constant = hessians_are_constant
+        self.n_threads = n_threads

     def compute_histograms_brute(
             HistogramBuilder self,
@@ -116,7 +116,7 @@

         Returns
         -------
-        histograms : ndarray of HISTOGRAM_DTYPE, shape (n_features, max_bins)
+        histograms : ndarray of HISTOGRAM_DTYPE, shape (n_features, n_bins)
             The computed histograms of the current node.
         """
         cdef:
@@ -131,10 +131,12 @@
             G_H_DTYPE_C [::1] gradients = self.gradients
             G_H_DTYPE_C [::1] ordered_hessians = self.ordered_hessians
             G_H_DTYPE_C [::1] hessians = self.hessians
-            hist_struct [:, ::1] histograms = np.zeros(
-                shape=(self.n_features, self.max_bins),
+            # Histograms will be initialized to zero later within a prange
+            hist_struct [:, ::1] histograms = np.empty(
+                shape=(self.n_features, self.n_bins),
                 dtype=HISTOGRAM_DTYPE
             )
+            int n_threads = self.n_threads

         with nogil:
             n_samples = sample_indices.shape[0]
@@ -144,14 +146,17 @@
             # cache hit.
             if sample_indices.shape[0] != gradients.shape[0]:
                 if hessians_are_constant:
-                    for i in prange(n_samples, schedule='static'):
+                    for i in prange(n_samples, schedule='static',
+                                    num_threads=n_threads):
                         ordered_gradients[i] = gradients[sample_indices[i]]
                 else:
-                    for i in prange(n_samples, schedule='static'):
+                    for i in prange(n_samples, schedule='static',
+                                    num_threads=n_threads):
                         ordered_gradients[i] = gradients[sample_indices[i]]
                         ordered_hessians[i] = hessians[sample_indices[i]]

-            for feature_idx in prange(n_features, schedule='static'):
+            for feature_idx in prange(n_features, schedule='static',
+                                      num_threads=n_threads):
                 # Compute histogram of each feature
                 self._compute_histogram_brute_single_feature(
                     feature_idx, sample_indices, histograms)
@@ -176,6 +181,12 @@
                 self.ordered_hessians[:n_samples]
             unsigned char hessians_are_constant = \
                 self.hessians_are_constant
+            unsigned int bin_idx = 0
+
+        for bin_idx in range(self.n_bins):
+            histograms[feature_idx, bin_idx].sum_gradients = 0.
+            histograms[feature_idx, bin_idx].sum_hessians = 0.
+            histograms[feature_idx, bin_idx].count = 0

         if root_node:
             if hessians_are_constant:
@@ -211,30 +222,32 @@
         Parameters
         ----------
         parent_histograms : ndarray of HISTOGRAM_DTYPE, \
-                shape (n_features, max_bins)
+                shape (n_features, n_bins)
             The histograms of the parent.
         sibling_histograms : ndarray of HISTOGRAM_DTYPE, \
-                shape (n_features, max_bins)
+                shape (n_features, n_bins)
             The histograms of the sibling.

         Returns
         -------
-        histograms : ndarray of HISTOGRAM_DTYPE, shape(n_features, max_bins)
+        histograms : ndarray of HISTOGRAM_DTYPE, shape(n_features, n_bins)
             The computed histograms of the current node.
         """

         cdef:
             int feature_idx
             int n_features = self.n_features
-            hist_struct [:, ::1] histograms = np.zeros(
-                shape=(self.n_features, self.max_bins),
+            hist_struct [:, ::1] histograms = np.empty(
+                shape=(self.n_features, self.n_bins),
                 dtype=HISTOGRAM_DTYPE
             )
-
-        for feature_idx in prange(n_features, schedule='static', nogil=True):
+            int n_threads = self.n_threads
+
+        for feature_idx in prange(n_features, schedule='static', nogil=True,
+                                  num_threads=n_threads):
             # Compute histogram of each feature
             _subtract_histograms(feature_idx,
-                                 self.max_bins,
+                                 self.n_bins,
                                  parent_histograms,
                                  sibling_histograms,
                                  histograms)
('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -2,32 +2,112 @@
 # Author: Nicolas Hug

 from abc import ABC, abstractmethod
+from functools import partial
+import warnings

 import numpy as np
 from timeit import default_timer as time
-from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin
-from sklearn.utils import check_X_y, check_random_state, check_array
-from sklearn.utils.validation import check_is_fitted
-from sklearn.utils.multiclass import check_classification_targets
-from sklearn.metrics import check_scoring
-from sklearn.model_selection import train_test_split
-from sklearn.preprocessing import LabelEncoder
+from ..._loss.loss import (
+    _LOSSES,
+    BaseLoss,
+    AbsoluteError,
+    HalfBinomialLoss,
+    HalfMultinomialLoss,
+    HalfPoissonLoss,
+    HalfSquaredError,
+    PinballLoss,
+)
+from ...base import BaseEstimator, RegressorMixin, ClassifierMixin, is_classifier
+from ...utils import check_random_state, resample
+from ...utils.validation import (
+    check_is_fitted,
+    check_consistent_length,
+    _check_sample_weight,
+)
+from ...utils._openmp_helpers import _openmp_effective_n_threads
+from ...utils.multiclass import check_classification_targets
+from ...metrics import check_scoring
+from ...model_selection import train_test_split
+from ...preprocessing import LabelEncoder
 from ._gradient_boosting import _update_raw_predictions
-from .types import Y_DTYPE, X_DTYPE, X_BINNED_DTYPE
+from .common import Y_DTYPE, X_DTYPE, G_H_DTYPE

 from .binning import _BinMapper
 from .grower import TreeGrower
-from .loss import _LOSSES
+
+
+_LOSSES = _LOSSES.copy()
+# TODO(1.2): Remove "least_squares" and "least_absolute_deviation"
+# TODO(1.3): Remove "binary_crossentropy" and "categorical_crossentropy"
+_LOSSES.update(
+    {
+        "least_squares": HalfSquaredError,
+        "least_absolute_deviation": AbsoluteError,
+        "poisson": HalfPoissonLoss,
+        "quantile": PinballLoss,
+        "binary_crossentropy": HalfBinomialLoss,
+        "categorical_crossentropy": HalfMultinomialLoss,
+    }
+)
+
+
+def _update_leaves_values(loss, grower, y_true, raw_prediction, sample_weight):
+    """Update the leaf values to be predicted by the tree.
+
+    Update equals:
+        loss.fit_intercept_only(y_true - raw_prediction)
+
+    This is only applied if loss.need_update_leaves_values is True.
+    Note: It only works, if the loss is a function of the residual, as is the
+    case for AbsoluteError and PinballLoss. Otherwise, one would need to get
+    the minimum of loss(y_true, raw_prediction + x) in x. A few examples:
+      - AbsoluteError: median(y_true - raw_prediction).
+      - PinballLoss: quantile(y_true - raw_prediction).
+    See also notes about need_update_leaves_values in BaseLoss.
+    """
+    # TODO: Ideally this should be computed in parallel over the leaves using something
+    # similar to _update_raw_predictions(), but this requires a cython version of
+    # median().
+    for leaf in grower.finalized_leaves:
+        indices = leaf.sample_indices
+        if sample_weight is None:
+            sw = None
+        else:
+            sw = sample_weight[indices]
+        update = loss.fit_intercept_only(
+            y_true=y_true[indices] - raw_prediction[indices],
+            sample_weight=sw,
+        )
+        leaf.value = grower.shrinkage * update
+        # Note that the regularization is ignored here


 class BaseHistGradientBoosting(BaseEstimator, ABC):
     """Base class for histogram-based gradient boosting estimators."""

     @abstractmethod
-    def __init__(self, loss, learning_rate, max_iter, max_leaf_nodes,
-                 max_depth, min_samples_leaf, l2_regularization, max_bins,
-                 scoring, validation_fraction, n_iter_no_change, tol, verbose,
-                 random_state):
+    def __init__(
+        self,
+        loss,
+        *,
+        learning_rate,
+        max_iter,
+        max_leaf_nodes,
+        max_depth,
+        min_samples_leaf,
+        l2_regularization,
+        max_bins,
+        categorical_features,
+        monotonic_cst,
+        warm_start,
+        early_stopping,
+        scoring,
+        validation_fraction,
+        n_iter_no_change,
+        tol,
+        verbose,
+        random_state,
+    ):
         self.loss = loss
         self.learning_rate = learning_rate
         self.max_iter = max_iter
@@ -36,9 +116,13 @@
         self.min_samples_leaf = min_samples_leaf
         self.l2_regularization = l2_regularization
         self.max_bins = max_bins
+        self.monotonic_cst = monotonic_cst
+        self.categorical_features = categorical_features
+        self.warm_start = warm_start
+        self.early_stopping = early_stopping
+        self.scoring = scoring
+        self.validation_fraction = validation_fraction
         self.n_iter_no_change = n_iter_no_change
-        self.validation_fraction = validation_fraction
-        self.scoring = scoring
         self.tol = tol
         self.verbose = verbose
         self.random_state = random_state
@@ -49,248 +133,533 @@
         The parameters that are directly passed to the grower are checked in
         TreeGrower."""

-        if self.loss not in self._VALID_LOSSES:
+        if self.loss not in self._VALID_LOSSES and not isinstance(self.loss, BaseLoss):
             raise ValueError(
-                "Loss {} is not supported for {}. Accepted losses: "
-                "{}.".format(self.loss, self.__class__.__name__,
-                             ', '.join(self._VALID_LOSSES)))
+                "Loss {} is not supported for {}. Accepted losses: {}.".format(
+                    self.loss, self.__class__.__name__, ", ".join(self._VALID_LOSSES)
+                )
+            )

         if self.learning_rate <= 0:
-            raise ValueError('learning_rate={} must '
-                             'be strictly positive'.format(self.learning_rate))
+            raise ValueError(
+                "learning_rate={} must be strictly positive".format(self.learning_rate)
+            )
         if self.max_iter < 1:
-            raise ValueError('max_iter={} must not be smaller '
-                             'than 1.'.format(self.max_iter))
-        if self.n_iter_no_change is not None and self.n_iter_no_change < 0:
-            raise ValueError('n_iter_no_change={} must be '
-                             'positive.'.format(self.n_iter_no_change))
-        if (self.validation_fraction is not None and
-                self.validation_fraction <= 0):
             raise ValueError(
-                'validation_fraction={} must be strictly '
-                'positive, or None.'.format(self.validation_fraction))
-        if self.tol is not None and self.tol < 0:
-            raise ValueError('tol={} '
-                             'must not be smaller than 0.'.format(self.tol))
-
-    def fit(self, X, y):
+                "max_iter={} must not be smaller than 1.".format(self.max_iter)
+            )
+        if self.n_iter_no_change < 0:
+            raise ValueError(
+                "n_iter_no_change={} must be positive.".format(self.n_iter_no_change)
+            )
+        if self.validation_fraction is not None and self.validation_fraction <= 0:
+            raise ValueError(
+                "validation_fraction={} must be strictly positive, or None.".format(
+                    self.validation_fraction
+                )
+            )
+        if self.tol < 0:
+            raise ValueError("tol={} must not be smaller than 0.".format(self.tol))
+
+        if not (2 <= self.max_bins <= 255):
+            raise ValueError(
+                "max_bins={} should be no smaller than 2 "
+                "and no larger than 255.".format(self.max_bins)
+            )
+
+        if self.monotonic_cst is not None and self.n_trees_per_iteration_ != 1:
+            raise ValueError(
+                "monotonic constraints are not supported for multiclass classification."
+            )
+
+    def _check_categories(self, X):
+        """Check and validate categorical features in X
+
+        Return
+        ------
+        is_categorical : ndarray of shape (n_features,) or None, dtype=bool
+            Indicates whether a feature is categorical. If no feature is
+            categorical, this is None.
+        known_categories : list of size n_features or None
+            The list contains, for each feature:
+                - an array of shape (n_categories,) with the unique cat values
+                - None if the feature is not categorical
+            None if no feature is categorical.
+        """
+        if self.categorical_features is None:
+            return None, None
+
+        categorical_features = np.asarray(self.categorical_features)
+
+        if categorical_features.size == 0:
+            return None, None
+
+        if categorical_features.dtype.kind not in ("i", "b"):
+            raise ValueError(
+                "categorical_features must be an array-like of "
+                "bools or array-like of ints."
+            )
+
+        n_features = X.shape[1]
+
+        # check for categorical features as indices
+        if categorical_features.dtype.kind == "i":
+            if (
+                np.max(categorical_features) >= n_features
+                or np.min(categorical_features) < 0
+            ):
+                raise ValueError(
+                    "categorical_features set as integer "
+                    "indices must be in [0, n_features - 1]"
+                )
+            is_categorical = np.zeros(n_features, dtype=bool)
+            is_categorical[categorical_features] = True
+        else:
+            if categorical_features.shape[0] != n_features:
+                raise ValueError(
+                    "categorical_features set as a boolean mask "
+                    "must have shape (n_features,), got: "
+                    f"{categorical_features.shape}"
+                )
+            is_categorical = categorical_features
+
+        if not np.any(is_categorical):
+            return None, None
+
+        # compute the known categories in the training data. We need to do
+        # that here instead of in the BinMapper because in case of early
+        # stopping, the mapper only gets a fraction of the training data.
+        known_categories = []
+
+        for f_idx in range(n_features):
+            if is_categorical[f_idx]:
+                categories = np.unique(X[:, f_idx])
+                missing = np.isnan(categories)
+                if missing.any():
+                    categories = categories[~missing]
+
+                if categories.size > self.max_bins:
+                    raise ValueError(
+                        f"Categorical feature at index {f_idx} is "
+                        "expected to have a "
+                        f"cardinality <= {self.max_bins}"
+                    )
+
+                if (categories >= self.max_bins).any():
+                    raise ValueError(
+                        f"Categorical feature at index {f_idx} is "
+                        "expected to be encoded with "
+                        f"values < {self.max_bins}"
+                    )
+            else:
+                categories = None
+            known_categories.append(categories)
+
+        return is_categorical, known_categories
+
+    def fit(self, X, y, sample_weight=None):
         """Fit the gradient boosting model.

         Parameters
         ----------
-        X : array-like, shape=(n_samples, n_features)
+        X : array-like of shape (n_samples, n_features)
             The input samples.

-        y : array-like, shape=(n_samples,)
+        y : array-like of shape (n_samples,)
             Target values.
+
+        sample_weight : array-like of shape (n_samples,) default=None
+            Weights of training data.
+
+            .. versionadded:: 0.23

         Returns
         -------
         self : object
+            Fitted estimator.
         """
-
         fit_start_time = time()
-        acc_find_split_time = 0.  # time spent finding the best splits
-        acc_apply_split_time = 0.  # time spent splitting nodes
-        acc_compute_hist_time = 0.  # time spent computing histograms
+        acc_find_split_time = 0.0  # time spent finding the best splits
+        acc_apply_split_time = 0.0  # time spent splitting nodes
+        acc_compute_hist_time = 0.0  # time spent computing histograms
         # time spent predicting X for gradient and hessians update
-        acc_prediction_time = 0.
-        X, y = check_X_y(X, y, dtype=[X_DTYPE])
+        acc_prediction_time = 0.0
+        X, y = self._validate_data(X, y, dtype=[X_DTYPE], force_all_finite=False)
         y = self._encode_y(y)
+        check_consistent_length(X, y)
+        # Do not create unit sample weights by default to later skip some
+        # computation
+        if sample_weight is not None:
+            sample_weight = _check_sample_weight(sample_weight, X, dtype=np.float64)
+            # TODO: remove when PDP supports sample weights
+            self._fitted_with_sw = True
+
         rng = check_random_state(self.random_state)

+        # When warm starting, we want to re-use the same seed that was used
+        # the first time fit was called (e.g. for subsampling or for the
+        # train/val split).
+        if not (self.warm_start and self._is_fitted()):
+            self._random_seed = rng.randint(np.iinfo(np.uint32).max, dtype="u8")
+
         self._validate_parameters()
-        self.n_features_ = X.shape[1]  # used for validation in predict()
+
+        # used for validation in predict
+        n_samples, self._n_features = X.shape
+
+        self.is_categorical_, known_categories = self._check_categories(X)

         # we need this stateful variable to tell raw_predict() that it was
         # called from fit() (this current method), and that the data it has
         # received is pre-binned.
         # predicting is faster on pre-binned data, so we want early stopping
-        # predictions to be made on pre-binned data. Unfortunately the scorer_
+        # predictions to be made on pre-binned data. Unfortunately the _scorer
         # can only call predict() or predict_proba(), not raw_predict(), and
         # there's no way to tell the scorer that it needs to predict binned
         # data.
         self._in_fit = True

-        # bin the data
-        if self.verbose:
-            print("Binning {:.3f} GB of data: ".format(X.nbytes / 1e9), end="",
-                  flush=True)
-        tic = time()
-        self.bin_mapper_ = _BinMapper(max_bins=self.max_bins, random_state=rng)
-        X_binned = self.bin_mapper_.fit_transform(X)
-        toc = time()
-        if self.verbose:
-            duration = toc - tic
-            print("{:.3f} s".format(duration))
-
-        self.loss_ = self._get_loss()
-
-        self.do_early_stopping_ = (self.n_iter_no_change is not None and
-                                   self.n_iter_no_change > 0)
+        # `_openmp_effective_n_threads` is used to take cgroups CPU quotes
+        # into account when determine the maximum number of threads to use.
+        n_threads = _openmp_effective_n_threads()
+
+        if isinstance(self.loss, str):
+            self._loss = self._get_loss(sample_weight=sample_weight)
+        elif isinstance(self.loss, BaseLoss):
+            self._loss = self.loss
+
+        if self.early_stopping == "auto":
+            self.do_early_stopping_ = n_samples > 10000
+        else:
+            self.do_early_stopping_ = self.early_stopping

         # create validation data if needed
         self._use_validation_data = self.validation_fraction is not None
         if self.do_early_stopping_ and self._use_validation_data:
             # stratify for classification
-            stratify = y if hasattr(self.loss_, 'predict_proba') else None
-
-            X_binned_train, X_binned_val, y_train, y_val = train_test_split(
-                X_binned, y, test_size=self.validation_fraction,
-                stratify=stratify, random_state=rng)
-
-            # Predicting is faster of C-contiguous arrays, training is faster
-            # on Fortran arrays.
-            X_binned_val = np.ascontiguousarray(X_binned_val)
-            X_binned_train = np.asfortranarray(X_binned_train)
+            # instead of checking predict_proba, loss.n_classes >= 2 would also work
+            stratify = y if hasattr(self._loss, "predict_proba") else None
+
+            # Save the state of the RNG for the training and validation split.
+            # This is needed in order to have the same split when using
+            # warm starting.
+
+            if sample_weight is None:
+                X_train, X_val, y_train, y_val = train_test_split(
+                    X,
+                    y,
+                    test_size=self.validation_fraction,
+                    stratify=stratify,
+                    random_state=self._random_seed,
+                )
+                sample_weight_train = sample_weight_val = None
+            else:
+                # TODO: incorporate sample_weight in sampling here, as well as
+                # stratify
+                (
+                    X_train,
+                    X_val,
+                    y_train,
+                    y_val,
+                    sample_weight_train,
+                    sample_weight_val,
+                ) = train_test_split(
+                    X,
+                    y,
+                    sample_weight,
+                    test_size=self.validation_fraction,
+                    stratify=stratify,
+                    random_state=self._random_seed,
+                )
         else:
-            X_binned_train, y_train = X_binned, y
-            X_binned_val, y_val = None, None
+            X_train, y_train, sample_weight_train = X, y, sample_weight
+            X_val = y_val = sample_weight_val = None
+
+        # Bin the data
+        # For ease of use of the API, the user-facing GBDT classes accept the
+        # parameter max_bins, which doesn't take into account the bin for
+        # missing values (which is always allocated). However, since max_bins
+        # isn't the true maximal number of bins, all other private classes
+        # (binmapper, histbuilder...) accept n_bins instead, which is the
+        # actual total number of bins. Everywhere in the code, the
+        # convention is that n_bins == max_bins + 1
+        n_bins = self.max_bins + 1  # + 1 for missing values
+        self._bin_mapper = _BinMapper(
+            n_bins=n_bins,
+            is_categorical=self.is_categorical_,
+            known_categories=known_categories,
+            random_state=self._random_seed,
+            n_threads=n_threads,
+        )
+        X_binned_train = self._bin_data(X_train, is_training_data=True)
+        if X_val is not None:
+            X_binned_val = self._bin_data(X_val, is_training_data=False)
+        else:
+            X_binned_val = None
+
+        # Uses binned data to check for missing values
+        has_missing_values = (
+            (X_binned_train == self._bin_mapper.missing_values_bin_idx_)
+            .any(axis=0)
+            .astype(np.uint8)
+        )

         if self.verbose:
             print("Fitting gradient boosted rounds:")

-        # initialize raw_predictions: those are the accumulated values
-        # predicted by the trees for the training data. raw_predictions has
-        # shape (n_trees_per_iteration, n_samples) where
-        # n_trees_per_iterations is n_classes in multiclass classification,
-        # else 1.
         n_samples = X_binned_train.shape[0]
-        self._baseline_prediction = self.loss_.get_baseline_prediction(
-            y_train, self.n_trees_per_iteration_
+
+        # First time calling fit, or no warm start
+        if not (self._is_fitted() and self.warm_start):
+            # Clear random state and score attributes
+            self._clear_state()
+
+            # initialize raw_predictions: those are the accumulated values
+            # predicted by the trees for the training data. raw_predictions has
+            # shape (n_samples, n_trees_per_iteration) where
+            # n_trees_per_iterations is n_classes in multiclass classification,
+            # else 1.
+            # self._baseline_prediction has shape (1, n_trees_per_iteration)
+            self._baseline_prediction = self._loss.fit_intercept_only(
+                y_true=y_train, sample_weight=sample_weight_train
+            ).reshape((1, -1))
+            raw_predictions = np.zeros(
+                shape=(n_samples, self.n_trees_per_iteration_),
+                dtype=self._baseline_prediction.dtype,
+                order="F",
+            )
+            raw_predictions += self._baseline_prediction
+
+            # predictors is a matrix (list of lists) of TreePredictor objects
+            # with shape (n_iter_, n_trees_per_iteration)
+            self._predictors = predictors = []
+
+            # Initialize structures and attributes related to early stopping
+            self._scorer = None  # set if scoring != loss
+            raw_predictions_val = None  # set if scoring == loss and use val
+            self.train_score_ = []
+            self.validation_score_ = []
+
+            if self.do_early_stopping_:
+                # populate train_score and validation_score with the
+                # predictions of the initial model (before the first tree)
+
+                if self.scoring == "loss":
+                    # we're going to compute scoring w.r.t the loss. As losses
+                    # take raw predictions as input (unlike the scorers), we
+                    # can optimize a bit and avoid repeating computing the
+                    # predictions of the previous trees. We'll re-use
+                    # raw_predictions (as it's needed for training anyway) for
+                    # evaluating the training loss, and create
+                    # raw_predictions_val for storing the raw predictions of
+                    # the validation data.
+
+                    if self._use_validation_data:
+                        raw_predictions_val = np.zeros(
+                            shape=(X_binned_val.shape[0], self.n_trees_per_iteration_),
+                            dtype=self._baseline_prediction.dtype,
+                            order="F",
+                        )
+
+                        raw_predictions_val += self._baseline_prediction
+
+                    self._check_early_stopping_loss(
+                        raw_predictions=raw_predictions,
+                        y_train=y_train,
+                        sample_weight_train=sample_weight_train,
+                        raw_predictions_val=raw_predictions_val,
+                        y_val=y_val,
+                        sample_weight_val=sample_weight_val,
+                        n_threads=n_threads,
+                    )
+                else:
+                    self._scorer = check_scoring(self, self.scoring)
+                    # _scorer is a callable with signature (est, X, y) and
+                    # calls est.predict() or est.predict_proba() depending on
+                    # its nature.
+                    # Unfortunately, each call to _scorer() will compute
+                    # the predictions of all the trees. So we use a subset of
+                    # the training set to compute train scores.
+
+                    # Compute the subsample set
+                    (
+                        X_binned_small_train,
+                        y_small_train,
+                        sample_weight_small_train,
+                    ) = self._get_small_trainset(
+                        X_binned_train, y_train, sample_weight_train, self._random_seed
+                    )
+
+                    self._check_early_stopping_scorer(
+                        X_binned_small_train,
+                        y_small_train,
+                        sample_weight_small_train,
+                        X_binned_val,
+                        y_val,
+                        sample_weight_val,
+                    )
+            begin_at_stage = 0
+
+        # warm start: this is not the first time fit was called
+        else:
+            # Check that the maximum number of iterations is not smaller
+            # than the number of iterations from the previous fit
+            if self.max_iter < self.n_iter_:
+                raise ValueError(
+                    "max_iter=%d must be larger than or equal to "
+                    "n_iter_=%d when warm_start==True" % (self.max_iter, self.n_iter_)
+                )
+
+            # Convert array attributes to lists
+            self.train_score_ = self.train_score_.tolist()
+            self.validation_score_ = self.validation_score_.tolist()
+
+            # Compute raw predictions
+            raw_predictions = self._raw_predict(X_binned_train, n_threads=n_threads)
+            if self.do_early_stopping_ and self._use_validation_data:
+                raw_predictions_val = self._raw_predict(
+                    X_binned_val, n_threads=n_threads
+                )
+            else:
+                raw_predictions_val = None
+
+            if self.do_early_stopping_ and self.scoring != "loss":
+                # Compute the subsample set
+                (
+                    X_binned_small_train,
+                    y_small_train,
+                    sample_weight_small_train,
+                ) = self._get_small_trainset(
+                    X_binned_train, y_train, sample_weight_train, self._random_seed
+                )
+
+            # Get the predictors from the previous fit
+            predictors = self._predictors
+
+            begin_at_stage = self.n_iter_
+
+        # initialize gradients and hessians (empty arrays).
+        # shape = (n_samples, n_trees_per_iteration).
+        gradient, hessian = self._loss.init_gradient_and_hessian(
+            n_samples=n_samples, dtype=G_H_DTYPE, order="F"
         )
-        raw_predictions = np.zeros(
-            shape=(self.n_trees_per_iteration_, n_samples),
-            dtype=self._baseline_prediction.dtype
-        )
-        raw_predictions += self._baseline_prediction
-
-        # initialize gradients and hessians (empty arrays).
-        # shape = (n_trees_per_iteration, n_samples).
-        gradients, hessians = self.loss_.init_gradients_and_hessians(
-            n_samples=n_samples,
-            prediction_dim=self.n_trees_per_iteration_
-        )
-
-        # predictors is a matrix (list of lists) of TreePredictor objects
-        # with shape (n_iter_, n_trees_per_iteration)
-        self._predictors = predictors = []
-
-        # Initialize structures and attributes related to early stopping
-        self.scorer_ = None  # set if scoring != loss
-        raw_predictions_val = None  # set if scoring == loss and use val
-        self.train_score_ = []
-        self.validation_score_ = []
-        if self.do_early_stopping_:
-            # populate train_score and validation_score with the predictions
-            # of the initial model (before the first tree)
-
-            if self.scoring == 'loss':
-                # we're going to compute scoring w.r.t the loss. As losses
-                # take raw predictions as input (unlike the scorers), we can
-                # optimize a bit and avoid repeating computing the predictions
-                # of the previous trees. We'll re-use raw_predictions (as it's
-                # needed for training anyway) for evaluating the training
-                # loss, and create raw_predictions_val for storing the
-                # raw predictions of the validation data.
-
-                if self._use_validation_data:
-                    raw_predictions_val = np.zeros(
-                        shape=(self.n_trees_per_iteration_,
-                               X_binned_val.shape[0]),
-                        dtype=self._baseline_prediction.dtype
-                    )
-
-                    raw_predictions_val += self._baseline_prediction
-
-                self._check_early_stopping_loss(raw_predictions, y_train,
-                                                raw_predictions_val, y_val)
-            else:
-                self.scorer_ = check_scoring(self, self.scoring)
-                # scorer_ is a callable with signature (est, X, y) and calls
-                # est.predict() or est.predict_proba() depending on its nature.
-                # Unfortunately, each call to scorer_() will compute
-                # the predictions of all the trees. So we use a subset of the
-                # training set to compute train scores.
-                subsample_size = 10000  # should we expose this parameter?
-                indices = np.arange(X_binned_train.shape[0])
-                if X_binned_train.shape[0] > subsample_size:
-                    # TODO: not critical but stratify using resample()
-                    indices = rng.choice(indices, subsample_size,
-                                         replace=False)
-                X_binned_small_train = X_binned_train[indices]
-                y_small_train = y_train[indices]
-                # Predicting is faster on C-contiguous arrays.
-                X_binned_small_train = np.ascontiguousarray(
-                    X_binned_small_train)
-
-                self._check_early_stopping_scorer(
-                    X_binned_small_train, y_small_train,
-                    X_binned_val, y_val,
-                )
-
-        for iteration in range(self.max_iter):
+
+        for iteration in range(begin_at_stage, self.max_iter):

             if self.verbose:
                 iteration_start_time = time()
-                print("[{}/{}] ".format(iteration + 1, self.max_iter),
-                      end='', flush=True)
+                print(
+                    "[{}/{}] ".format(iteration + 1, self.max_iter), end="", flush=True
+                )

             # Update gradients and hessians, inplace
-            self.loss_.update_gradients_and_hessians(gradients, hessians,
-                                                     y_train, raw_predictions)
+            # Note that self._loss expects shape (n_samples,) for
+            # n_trees_per_iteration = 1 else shape (n_samples, n_trees_per_iteration).
+            if self._loss.constant_hessian:
+                self._loss.gradient(
+                    y_true=y_train,
+                    raw_prediction=raw_predictions,
+                    sample_weight=sample_weight_train,
+                    gradient_out=gradient,
+                    n_threads=n_threads,
+                )
+            else:
+                self._loss.gradient_hessian(
+                    y_true=y_train,
+                    raw_prediction=raw_predictions,
+                    sample_weight=sample_weight_train,
+                    gradient_out=gradient,
+                    hessian_out=hessian,
+                    n_threads=n_threads,
+                )

             # Append a list since there may be more than 1 predictor per iter
             predictors.append([])

+            # 2-d views of shape (n_samples, n_trees_per_iteration_) or (n_samples, 1)
+            # on gradient and hessian to simplify the loop over n_trees_per_iteration_.
+            if gradient.ndim == 1:
+                g_view = gradient.reshape((-1, 1))
+                h_view = hessian.reshape((-1, 1))
+            else:
+                g_view = gradient
+                h_view = hessian
+
             # Build `n_trees_per_iteration` trees.
             for k in range(self.n_trees_per_iteration_):
-
                 grower = TreeGrower(
-                    X_binned_train, gradients[k, :], hessians[k, :],
-                    max_bins=self.max_bins,
-                    actual_n_bins=self.bin_mapper_.actual_n_bins_,
+                    X_binned=X_binned_train,
+                    gradients=g_view[:, k],
+                    hessians=h_view[:, k],
+                    n_bins=n_bins,
+                    n_bins_non_missing=self._bin_mapper.n_bins_non_missing_,
+                    has_missing_values=has_missing_values,
+                    is_categorical=self.is_categorical_,
+                    monotonic_cst=self.monotonic_cst,
                     max_leaf_nodes=self.max_leaf_nodes,
                     max_depth=self.max_depth,
                     min_samples_leaf=self.min_samples_leaf,
                     l2_regularization=self.l2_regularization,
-                    shrinkage=self.learning_rate)
+                    shrinkage=self.learning_rate,
+                    n_threads=n_threads,
+                )
                 grower.grow()

                 acc_apply_split_time += grower.total_apply_split_time
                 acc_find_split_time += grower.total_find_split_time
                 acc_compute_hist_time += grower.total_compute_hist_time

+                if self._loss.need_update_leaves_values:
+                    _update_leaves_values(
+                        loss=self._loss,
+                        grower=grower,
+                        y_true=y_train,
+                        raw_prediction=raw_predictions[:, k],
+                        sample_weight=sample_weight_train,
+                    )
+
                 predictor = grower.make_predictor(
-                    bin_thresholds=self.bin_mapper_.bin_thresholds_
+                    binning_thresholds=self._bin_mapper.bin_thresholds_
                 )
                 predictors[-1].append(predictor)

                 # Update raw_predictions with the predictions of the newly
                 # created tree.
                 tic_pred = time()
-                _update_raw_predictions(raw_predictions[k, :], grower)
+                _update_raw_predictions(raw_predictions[:, k], grower, n_threads)
                 toc_pred = time()
                 acc_prediction_time += toc_pred - tic_pred

             should_early_stop = False
             if self.do_early_stopping_:
-                if self.scoring == 'loss':
+                if self.scoring == "loss":
                     # Update raw_predictions_val with the newest tree(s)
                     if self._use_validation_data:
                         for k, pred in enumerate(self._predictors[-1]):
-                            raw_predictions_val[k, :] += (
-                                pred.predict_binned(X_binned_val))
+                            raw_predictions_val[:, k] += pred.predict_binned(
+                                X_binned_val,
+                                self._bin_mapper.missing_values_bin_idx_,
+                                n_threads,
+                            )

                     should_early_stop = self._check_early_stopping_loss(
-                        raw_predictions, y_train,
-                        raw_predictions_val, y_val
+                        raw_predictions=raw_predictions,
+                        y_train=y_train,
+                        sample_weight_train=sample_weight_train,
+                        raw_predictions_val=raw_predictions_val,
+                        y_val=y_val,
+                        sample_weight_val=sample_weight_val,
+                        n_threads=n_threads,
                     )

                 else:
                     should_early_stop = self._check_early_stopping_scorer(
-                        X_binned_small_train, y_small_train,
-                        X_binned_val, y_val,
+                        X_binned_small_train,
+                        y_small_train,
+                        sample_weight_small_train,
+                        X_binned_val,
+                        y_val,
+                        sample_weight_val,
                     )

             if self.verbose:
@@ -309,59 +678,151 @@
             )
             n_predictors = sum(
                 len(predictors_at_ith_iteration)
-                for predictors_at_ith_iteration in self._predictors)
-            print("Fit {} trees in {:.3f} s, ({} total leaves)".format(
-                n_predictors, duration, n_total_leaves))
-            print("{:<32} {:.3f}s".format('Time spent computing histograms:',
-                                          acc_compute_hist_time))
-            print("{:<32} {:.3f}s".format('Time spent finding best splits:',
-                                          acc_find_split_time))
-            print("{:<32} {:.3f}s".format('Time spent applying splits:',
-                                          acc_apply_split_time))
-            print("{:<32} {:.3f}s".format('Time spent predicting:',
-                                          acc_prediction_time))
+                for predictors_at_ith_iteration in self._predictors
+            )
+            print(
+                "Fit {} trees in {:.3f} s, ({} total leaves)".format(
+                    n_predictors, duration, n_total_leaves
+                )
+            )
+            print(
+                "{:<32} {:.3f}s".format(
+                    "Time spent computing histograms:", acc_compute_hist_time
+                )
+            )
+            print(
+                "{:<32} {:.3f}s".format(
+                    "Time spent finding best splits:", acc_find_split_time
+                )
+            )
+            print(
+                "{:<32} {:.3f}s".format(
+                    "Time spent applying splits:", acc_apply_split_time
+                )
+            )
+            print(
+                "{:<32} {:.3f}s".format("Time spent predicting:", acc_prediction_time)
+            )

         self.train_score_ = np.asarray(self.train_score_)
         self.validation_score_ = np.asarray(self.validation_score_)
         del self._in_fit  # hard delete so we're sure it can't be used anymore
         return self

-    def _check_early_stopping_scorer(self, X_binned_small_train, y_small_train,
-                                     X_binned_val, y_val):
+    def _is_fitted(self):
+        return len(getattr(self, "_predictors", [])) > 0
+
+    def _clear_state(self):
+        """Clear the state of the gradient boosting model."""
+        for var in ("train_score_", "validation_score_"):
+            if hasattr(self, var):
+                delattr(self, var)
+
+    def _get_small_trainset(self, X_binned_train, y_train, sample_weight_train, seed):
+        """Compute the indices of the subsample set and return this set.
+
+        For efficiency, we need to subsample the training set to compute scores
+        with scorers.
+        """
+        # TODO: incorporate sample_weights here in `resample`
+        subsample_size = 10000
+        if X_binned_train.shape[0] > subsample_size:
+            indices = np.arange(X_binned_train.shape[0])
+            stratify = y_train if is_classifier(self) else None
+            indices = resample(
+                indices,
+                n_samples=subsample_size,
+                replace=False,
+                random_state=seed,
+                stratify=stratify,
+            )
+            X_binned_small_train = X_binned_train[indices]
+            y_small_train = y_train[indices]
+            if sample_weight_train is not None:
+                sample_weight_small_train = sample_weight_train[indices]
+            else:
+                sample_weight_small_train = None
+            X_binned_small_train = np.ascontiguousarray(X_binned_small_train)
+            return (X_binned_small_train, y_small_train, sample_weight_small_train)
+        else:
+            return X_binned_train, y_train, sample_weight_train
+
+    def _check_early_stopping_scorer(
+        self,
+        X_binned_small_train,
+        y_small_train,
+        sample_weight_small_train,
+        X_binned_val,
+        y_val,
+        sample_weight_val,
+    ):
         """Check if fitting should be early-stopped based on scorer.

         Scores are computed on validation data or on training data.
         """
-
-        self.train_score_.append(
-            self.scorer_(self, X_binned_small_train, y_small_train)
-        )
+        if is_classifier(self):
+            y_small_train = self.classes_[y_small_train.astype(int)]
+
+        if sample_weight_small_train is None:
+            self.train_score_.append(
+                self._scorer(self, X_binned_small_train, y_small_train)
+            )
+        else:
+            self.train_score_.append(
+                self._scorer(
+                    self,
+                    X_binned_small_train,
+                    y_small_train,
+                    sample_weight=sample_weight_small_train,
+                )
+            )

         if self._use_validation_data:
-            self.validation_score_.append(
-                self.scorer_(self, X_binned_val, y_val)
-            )
+            if is_classifier(self):
+                y_val = self.classes_[y_val.astype(int)]
+            if sample_weight_val is None:
+                self.validation_score_.append(self._scorer(self, X_binned_val, y_val))
+            else:
+                self.validation_score_.append(
+                    self._scorer(
+                        self, X_binned_val, y_val, sample_weight=sample_weight_val
+                    )
+                )
             return self._should_stop(self.validation_score_)
         else:
             return self._should_stop(self.train_score_)

-    def _check_early_stopping_loss(self,
-                                   raw_predictions,
-                                   y_train,
-                                   raw_predictions_val,
-                                   y_val):
+    def _check_early_stopping_loss(
+        self,
+        raw_predictions,
+        y_train,
+        sample_weight_train,
+        raw_predictions_val,
+        y_val,
+        sample_weight_val,
+        n_threads=1,
+    ):
         """Check if fitting should be early-stopped based on loss.

         Scores are computed on validation data or on training data.
         """
-
         self.train_score_.append(
-            -self.loss_(y_train, raw_predictions)
+            -self._loss(
+                y_true=y_train,
+                raw_prediction=raw_predictions,
+                sample_weight=sample_weight_train,
+                n_threads=n_threads,
+            )
         )

         if self._use_validation_data:
             self.validation_score_.append(
-                -self.loss_(y_val, raw_predictions_val)
+                -self._loss(
+                    y_true=y_val,
+                    raw_prediction=raw_predictions_val,
+                    sample_weight=sample_weight_val,
+                    n_threads=n_threads,
+                )
             )
             return self._should_stop(self.validation_score_)
         else:
@@ -380,90 +841,248 @@
         # harder for subsequent iteration to be considered an improvement upon
         # the reference score, and therefore it is more likely to early stop
         # because of the lack of significant improvement.
-        tol = 0 if self.tol is None else self.tol
-        reference_score = scores[-reference_position] + tol
-        recent_scores = scores[-reference_position + 1:]
-        recent_improvements = [score > reference_score
-                               for score in recent_scores]
+        reference_score = scores[-reference_position] + self.tol
+        recent_scores = scores[-reference_position + 1 :]
+        recent_improvements = [score > reference_score for score in recent_scores]
         return not any(recent_improvements)
+
+    def _bin_data(self, X, is_training_data):
+        """Bin data X.
+
+        If is_training_data, then fit the _bin_mapper attribute.
+        Else, the binned data is converted to a C-contiguous array.
+        """
+
+        description = "training" if is_training_data else "validation"
+        if self.verbose:
+            print(
+                "Binning {:.3f} GB of {} data: ".format(X.nbytes / 1e9, description),
+                end="",
+                flush=True,
+            )
+        tic = time()
+        if is_training_data:
+            X_binned = self._bin_mapper.fit_transform(X)  # F-aligned array
+        else:
+            X_binned = self._bin_mapper.transform(X)  # F-aligned array
+            # We convert the array to C-contiguous since predicting is faster
+            # with this layout (training is faster on F-arrays though)
+            X_binned = np.ascontiguousarray(X_binned)
+        toc = time()
+        if self.verbose:
+            duration = toc - tic
+            print("{:.3f} s".format(duration))
+
+        return X_binned

     def _print_iteration_stats(self, iteration_start_time):
         """Print info about the current fitting iteration."""
-        log_msg = ''
+        log_msg = ""

         predictors_of_ith_iteration = [
-            predictors_list for predictors_list in self._predictors[-1]
+            predictors_list
+            for predictors_list in self._predictors[-1]
             if predictors_list
         ]
         n_trees = len(predictors_of_ith_iteration)
-        max_depth = max(predictor.get_max_depth()
-                        for predictor in predictors_of_ith_iteration)
-        n_leaves = sum(predictor.get_n_leaf_nodes()
-                       for predictor in predictors_of_ith_iteration)
+        max_depth = max(
+            predictor.get_max_depth() for predictor in predictors_of_ith_iteration
+        )
+        n_leaves = sum(
+            predictor.get_n_leaf_nodes() for predictor in predictors_of_ith_iteration
+        )

         if n_trees == 1:
-            log_msg += ("{} tree, {} leaves, ".format(n_trees, n_leaves))
+            log_msg += "{} tree, {} leaves, ".format(n_trees, n_leaves)
         else:
-            log_msg += ("{} trees, {} leaves ".format(n_trees, n_leaves))
-            log_msg += ("({} on avg), ".format(int(n_leaves / n_trees)))
+            log_msg += "{} trees, {} leaves ".format(n_trees, n_leaves)
+            log_msg += "({} on avg), ".format(int(n_leaves / n_trees))

         log_msg += "max depth = {}, ".format(max_depth)

         if self.do_early_stopping_:
-            if self.scoring == 'loss':
+            if self.scoring == "loss":
                 factor = -1  # score_ arrays contain the negative loss
-                name = 'loss'
+                name = "loss"
             else:
                 factor = 1
-                name = 'score'
-            log_msg += "train {}: {:.5f}, ".format(name, factor *
-                                                   self.train_score_[-1])
+                name = "score"
+            log_msg += "train {}: {:.5f}, ".format(name, factor * self.train_score_[-1])
             if self._use_validation_data:
                 log_msg += "val {}: {:.5f}, ".format(
-                    name, factor * self.validation_score_[-1])
+                    name, factor * self.validation_score_[-1]
+                )

         iteration_time = time() - iteration_start_time
         log_msg += "in {:0.3f}s".format(iteration_time)

         print(log_msg)

-    def _raw_predict(self, X):
+    def _raw_predict(self, X, n_threads=None):
         """Return the sum of the leaves values over all predictors.

         Parameters
         ----------
-        X : array-like, shape=(n_samples, n_features)
+        X : array-like of shape (n_samples, n_features)
             The input samples.
+        n_threads : int, default=None
+            Number of OpenMP threads to use. `_openmp_effective_n_threads` is called
+            to determine the effective number of threads use, which takes cgroups CPU
+            quotes into account. See the docstring of `_openmp_effective_n_threads`
+            for details.

         Returns
         -------
-        raw_predictions : array, shape (n_samples * n_trees_per_iteration,)
+        raw_predictions : array, shape (n_samples, n_trees_per_iteration)
             The raw predicted values.
         """
-        X = check_array(X, dtype=[X_DTYPE, X_BINNED_DTYPE])
-        check_is_fitted(self, '_predictors')
-        if X.shape[1] != self.n_features_:
+        is_binned = getattr(self, "_in_fit", False)
+        if not is_binned:
+            X = self._validate_data(
+                X, dtype=X_DTYPE, force_all_finite=False, reset=False
+            )
+        check_is_fitted(self)
+        if X.shape[1] != self._n_features:
             raise ValueError(
-                'X has {} features but this estimator was trained with '
-                '{} features.'.format(X.shape[1], self.n_features_)
-            )
-        is_binned = getattr(self, '_in_fit', False)
+                "X has {} features but this estimator was trained with "
+                "{} features.".format(X.shape[1], self._n_features)
+            )
         n_samples = X.shape[0]
         raw_predictions = np.zeros(
-            shape=(self.n_trees_per_iteration_, n_samples),
-            dtype=self._baseline_prediction.dtype
+            shape=(n_samples, self.n_trees_per_iteration_),
+            dtype=self._baseline_prediction.dtype,
+            order="F",
         )
         raw_predictions += self._baseline_prediction
+
+        # We intentionally decouple the number of threads used at prediction
+        # time from the number of threads used at fit time because the model
+        # can be deployed on a different machine for prediction purposes.
+        n_threads = _openmp_effective_n_threads(n_threads)
+        self._predict_iterations(
+            X, self._predictors, raw_predictions, is_binned, n_threads
+        )
+        return raw_predictions
+
+    def _predict_iterations(self, X, predictors, raw_predictions, is_binned, n_threads):
+        """Add the predictions of the predictors to raw_predictions."""
+        if not is_binned:
+            (
+                known_cat_bitsets,
+                f_idx_map,
+            ) = self._bin_mapper.make_known_categories_bitsets()
+
+        for predictors_of_ith_iteration in predictors:
+            for k, predictor in enumerate(predictors_of_ith_iteration):
+                if is_binned:
+                    predict = partial(
+                        predictor.predict_binned,
+                        missing_values_bin_idx=self._bin_mapper.missing_values_bin_idx_,
+                        n_threads=n_threads,
+                    )
+                else:
+                    predict = partial(
+                        predictor.predict,
+                        known_cat_bitsets=known_cat_bitsets,
+                        f_idx_map=f_idx_map,
+                        n_threads=n_threads,
+                    )
+                raw_predictions[:, k] += predict(X)
+
+    def _staged_raw_predict(self, X):
+        """Compute raw predictions of ``X`` for each iteration.
+
+        This method allows monitoring (i.e. determine error on testing set)
+        after each stage.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            The input samples.
+
+        Yields
+        ------
+        raw_predictions : generator of ndarray of shape \
+            (n_samples, n_trees_per_iteration)
+            The raw predictions of the input samples. The order of the
+            classes corresponds to that in the attribute :term:`classes_`.
+        """
+        X = self._validate_data(X, dtype=X_DTYPE, force_all_finite=False, reset=False)
+        check_is_fitted(self)
+        if X.shape[1] != self._n_features:
+            raise ValueError(
+                "X has {} features but this estimator was trained with "
+                "{} features.".format(X.shape[1], self._n_features)
+            )
+        n_samples = X.shape[0]
+        raw_predictions = np.zeros(
+            shape=(n_samples, self.n_trees_per_iteration_),
+            dtype=self._baseline_prediction.dtype,
+            order="F",
+        )
+        raw_predictions += self._baseline_prediction
+
+        # We intentionally decouple the number of threads used at prediction
+        # time from the number of threads used at fit time because the model
+        # can be deployed on a different machine for prediction purposes.
+        n_threads = _openmp_effective_n_threads()
+        for iteration in range(len(self._predictors)):
+            self._predict_iterations(
+                X,
+                self._predictors[iteration : iteration + 1],
+                raw_predictions,
+                is_binned=False,
+                n_threads=n_threads,
+            )
+            yield raw_predictions.copy()
+
+    def _compute_partial_dependence_recursion(self, grid, target_features):
+        """Fast partial dependence computation.
+
+        Parameters
+        ----------
+        grid : ndarray, shape (n_samples, n_target_features)
+            The grid points on which the partial dependence should be
+            evaluated.
+        target_features : ndarray, shape (n_target_features)
+            The set of target features for which the partial dependence
+            should be evaluated.
+
+        Returns
+        -------
+        averaged_predictions : ndarray, shape \
+                (n_trees_per_iteration, n_samples)
+            The value of the partial dependence function on each grid point.
+        """
+
+        if getattr(self, "_fitted_with_sw", False):
+            raise NotImplementedError(
+                "{} does not support partial dependence "
+                "plots with the 'recursion' method when "
+                "sample weights were given during fit "
+                "time.".format(self.__class__.__name__)
+            )
+
+        grid = np.asarray(grid, dtype=X_DTYPE, order="C")
+        averaged_predictions = np.zeros(
+            (self.n_trees_per_iteration_, grid.shape[0]), dtype=Y_DTYPE
+        )
+
         for predictors_of_ith_iteration in self._predictors:
             for k, predictor in enumerate(predictors_of_ith_iteration):
-                predict = (predictor.predict_binned if is_binned
-                           else predictor.predict)
-                raw_predictions[k, :] += predict(X)
-
-        return raw_predictions
+                predictor.compute_partial_dependence(
+                    grid, target_features, averaged_predictions[k]
+                )
+        # Note that the learning rate is already accounted for in the leaves
+        # values.
+
+        return averaged_predictions
+
+    def _more_tags(self):
+        return {"allow_nan": True}

     @abstractmethod
-    def _get_loss(self):
+    def _get_loss(self, sample_weight):
         pass

     @abstractmethod
@@ -472,147 +1091,269 @@

     @property
     def n_iter_(self):
-        check_is_fitted(self, '_predictors')
+        """Number of iterations of the boosting process."""
+        check_is_fitted(self)
         return len(self._predictors)


-class HistGradientBoostingRegressor(BaseHistGradientBoosting, RegressorMixin):
+class HistGradientBoostingRegressor(RegressorMixin, BaseHistGradientBoosting):
     """Histogram-based Gradient Boosting Regression Tree.

     This estimator is much faster than
     :class:`GradientBoostingRegressor<sklearn.ensemble.GradientBoostingRegressor>`
-    for big datasets (n_samples >= 10 000). The input data ``X`` is pre-binned
-    into integer-valued bins, which considerably reduces the number of
-    splitting points to consider, and allows the algorithm to leverage
-    integer-based data structures. For small sample sizes,
-    :class:`GradientBoostingRegressor<sklearn.ensemble.GradientBoostingRegressor>`
-    might be preferred since binning may lead to split points that are too
-    approximate in this setting.
+    for big datasets (n_samples >= 10 000).
+
+    This estimator has native support for missing values (NaNs). During
+    training, the tree grower learns at each split point whether samples
+    with missing values should go to the left or right child, based on the
+    potential gain. When predicting, samples with missing values are
+    assigned to the left or right child consequently. If no missing values
+    were encountered for a given feature during training, then samples with
+    missing values are mapped to whichever child has the most samples.

     This implementation is inspired by
     `LightGBM <https://github.com/Microsoft/LightGBM>`_.

-    .. note::
-
-      This estimator is still **experimental** for now: the predictions
-      and the API might change without any deprecation cycle. To use it,
-      you need to explicitly import ``enable_hist_gradient_boosting``::
-
-        >>> # explicitly require this experimental feature
-        >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa
-        >>> # now you can import normally from ensemble
-        >>> from sklearn.ensemble import HistGradientBoostingClassifier
-
+    Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.
+
+    .. versionadded:: 0.21

     Parameters
     ----------
-    loss : {'least_squares'}, optional (default='least_squares')
+    loss : {'squared_error', 'absolute_error', 'poisson', 'quantile'}, \
+            default='squared_error'
         The loss function to use in the boosting process. Note that the
-        "least squares" loss actually implements an "half least squares loss"
-        to simplify the computation of the gradient.
-    learning_rate : float, optional (default=0.1)
+        "squared error" and "poisson" losses actually implement
+        "half least squares loss" and "half poisson deviance" to simplify the
+        computation of the gradient. Furthermore, "poisson" loss internally
+        uses a log-link and requires ``y >= 0``.
+        "quantile" uses the pinball loss.
+
+        .. versionchanged:: 0.23
+           Added option 'poisson'.
+
+        .. versionchanged:: 1.1
+           Added option 'quantile'.
+
+        .. deprecated:: 1.0
+            The loss 'least_squares' was deprecated in v1.0 and will be removed
+            in version 1.2. Use `loss='squared_error'` which is equivalent.
+
+        .. deprecated:: 1.0
+            The loss 'least_absolute_deviation' was deprecated in v1.0 and will
+            be removed in version 1.2. Use `loss='absolute_error'` which is
+            equivalent.
+
+    quantile : float, default=None
+        If loss is "quantile", this parameter specifies which quantile to be estimated
+        and must be between 0 and 1.
+    learning_rate : float, default=0.1
         The learning rate, also known as *shrinkage*. This is used as a
         multiplicative factor for the leaves values. Use ``1`` for no
         shrinkage.
-    max_iter : int, optional (default=100)
+    max_iter : int, default=100
         The maximum number of iterations of the boosting process, i.e. the
         maximum number of trees.
-    max_leaf_nodes : int or None, optional (default=31)
+    max_leaf_nodes : int or None, default=31
         The maximum number of leaves for each tree. Must be strictly greater
         than 1. If None, there is no maximum limit.
-    max_depth : int or None, optional (default=None)
+    max_depth : int or None, default=None
         The maximum depth of each tree. The depth of a tree is the number of
-        nodes to go from the root to the deepest leaf. Must be strictly greater
-        than 1. Depth isn't constrained by default.
-    min_samples_leaf : int, optional (default=20)
+        edges to go from the root to the deepest leaf.
+        Depth isn't constrained by default.
+    min_samples_leaf : int, default=20
         The minimum number of samples per leaf. For small datasets with less
         than a few hundred samples, it is recommended to lower this value
         since only very shallow trees would be built.
-    l2_regularization : float, optional (default=0)
+    l2_regularization : float, default=0
         The L2 regularization parameter. Use ``0`` for no regularization
         (default).
-    max_bins : int, optional (default=256)
-        The maximum number of bins to use. Before training, each feature of
-        the input array ``X`` is binned into at most ``max_bins`` bins, which
-        allows for a much faster training stage. Features with a small
-        number of unique values may use less than ``max_bins`` bins. Must be no
-        larger than 256.
-    scoring : str or callable or None, optional (default=None)
+    max_bins : int, default=255
+        The maximum number of bins to use for non-missing values. Before
+        training, each feature of the input array `X` is binned into
+        integer-valued bins, which allows for a much faster training stage.
+        Features with a small number of unique values may use less than
+        ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin
+        is always reserved for missing values. Must be no larger than 255.
+    categorical_features : array-like of {bool, int} of shape (n_features) \
+            or shape (n_categorical_features,), default=None
+        Indicates the categorical features.
+
+        - None : no feature will be considered categorical.
+        - boolean array-like : boolean mask indicating categorical features.
+        - integer array-like : integer indices indicating categorical
+          features.
+
+        For each categorical feature, there must be at most `max_bins` unique
+        categories, and each categorical value must be in [0, max_bins -1].
+
+        Read more in the :ref:`User Guide <categorical_support_gbdt>`.
+
+        .. versionadded:: 0.24
+
+    monotonic_cst : array-like of int of shape (n_features), default=None
+        Indicates the monotonic constraint to enforce on each feature. -1, 1
+        and 0 respectively correspond to a negative constraint, positive
+        constraint and no constraint. Read more in the :ref:`User Guide
+        <monotonic_cst_gbdt>`.
+
+        .. versionadded:: 0.23
+
+    warm_start : bool, default=False
+        When set to ``True``, reuse the solution of the previous call to fit
+        and add more estimators to the ensemble. For results to be valid, the
+        estimator should be re-trained on the same data only.
+        See :term:`the Glossary <warm_start>`.
+    early_stopping : 'auto' or bool, default='auto'
+        If 'auto', early stopping is enabled if the sample size is larger than
+        10000. If True, early stopping is enabled, otherwise early stopping is
+        disabled.
+
+        .. versionadded:: 0.23
+
+    scoring : str or callable or None, default='loss'
         Scoring parameter to use for early stopping. It can be a single
         string (see :ref:`scoring_parameter`) or a callable (see
         :ref:`scoring`). If None, the estimator's default scorer is used. If
         ``scoring='loss'``, early stopping is checked w.r.t the loss value.
-        Only used if ``n_iter_no_change`` is not None.
-    validation_fraction : int or float or None, optional (default=0.1)
+        Only used if early stopping is performed.
+    validation_fraction : int or float or None, default=0.1
         Proportion (or absolute size) of training data to set aside as
         validation data for early stopping. If None, early stopping is done on
-        the training data. Only used if ``n_iter_no_change`` is not None.
-    n_iter_no_change : int or None, optional (default=None)
+        the training data. Only used if early stopping is performed.
+    n_iter_no_change : int, default=10
         Used to determine when to "early stop". The fitting process is
         stopped when none of the last ``n_iter_no_change`` scores are better
-        than the ``n_iter_no_change - 1``th-to-last one, up to some
-        tolerance. If None or 0, no early-stopping is done.
-    tol : float or None, optional (default=1e-7)
+        than the ``n_iter_no_change - 1`` -th-to-last one, up to some
+        tolerance. Only used if early stopping is performed.
+    tol : float, default=1e-7
         The absolute tolerance to use when comparing scores during early
         stopping. The higher the tolerance, the more likely we are to early
         stop: higher tolerance means that it will be harder for subsequent
         iterations to be considered an improvement upon the reference score.
-    verbose: int, optional (default=0)
+    verbose : int, default=0
         The verbosity level. If not zero, print some information about the
         fitting process.
-    random_state : int, np.random.RandomStateInstance or None, \
-        optional (default=None)
+    random_state : int, RandomState instance or None, default=None
         Pseudo-random number generator to control the subsampling in the
         binning process, and the train/validation data split if early stopping
-        is enabled. See :term:`random_state`.
+        is enabled.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.

     Attributes
     ----------
+    do_early_stopping_ : bool
+        Indicates whether early stopping is used during training.
     n_iter_ : int
-        The number of iterations as selected by early stopping (if
-        n_iter_no_change is not None). Otherwise it corresponds to max_iter.
+        The number of iterations as selected by early stopping, depending on
+        the `early_stopping` parameter. Otherwise it corresponds to max_iter.
     n_trees_per_iteration_ : int
         The number of tree that are built at each iteration. For regressors,
         this is always 1.
-    train_score_ : ndarray, shape (max_iter + 1,)
+    train_score_ : ndarray, shape (n_iter_+1,)
         The scores at each iteration on the training data. The first entry
         is the score of the ensemble before the first iteration. Scores are
         computed according to the ``scoring`` parameter. If ``scoring`` is
         not 'loss', scores are computed on a subset of at most 10 000
         samples. Empty if no early stopping.
-    validation_score_ : ndarray, shape (max_iter + 1,)
+    validation_score_ : ndarray, shape (n_iter_+1,)
         The scores at each iteration on the held-out validation data. The
         first entry is the score of the ensemble before the first iteration.
         Scores are computed according to the ``scoring`` parameter. Empty if
         no early stopping or if ``validation_fraction`` is None.
+    is_categorical_ : ndarray, shape (n_features, ) or None
+        Boolean mask for the categorical features. ``None`` if there are no
+        categorical features.
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    GradientBoostingRegressor : Exact gradient boosting method that does not
+        scale as good on datasets with a large number of samples.
+    sklearn.tree.DecisionTreeRegressor : A decision tree regressor.
+    RandomForestRegressor : A meta-estimator that fits a number of decision
+        tree regressors on various sub-samples of the dataset and uses
+        averaging to improve the statistical performance and control
+        over-fitting.
+    AdaBoostRegressor : A meta-estimator that begins by fitting a regressor
+        on the original dataset and then fits additional copies of the
+        regressor on the same dataset but where the weights of instances are
+        adjusted according to the error of the current prediction. As such,
+        subsequent regressors focus more on difficult cases.

     Examples
     --------
-    >>> # To use this experimental feature, we need to explicitly ask for it:
-    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa
     >>> from sklearn.ensemble import HistGradientBoostingRegressor
-    >>> from sklearn.datasets import load_boston
-    >>> X, y = load_boston(return_X_y=True)
+    >>> from sklearn.datasets import load_diabetes
+    >>> X, y = load_diabetes(return_X_y=True)
     >>> est = HistGradientBoostingRegressor().fit(X, y)
     >>> est.score(X, y)
-    0.98...
+    0.92...
     """

-    _VALID_LOSSES = ('least_squares',)
-
-    def __init__(self, loss='least_squares', learning_rate=0.1,
-                 max_iter=100, max_leaf_nodes=31, max_depth=None,
-                 min_samples_leaf=20, l2_regularization=0., max_bins=256,
-                 scoring=None, validation_fraction=0.1, n_iter_no_change=None,
-                 tol=1e-7, verbose=0, random_state=None):
+    # TODO(1.2): remove "least_absolute_deviation"
+    _VALID_LOSSES = (
+        "squared_error",
+        "least_squares",
+        "absolute_error",
+        "least_absolute_deviation",
+        "poisson",
+        "quantile",
+    )
+
+    def __init__(
+        self,
+        loss="squared_error",
+        *,
+        quantile=None,
+        learning_rate=0.1,
+        max_iter=100,
+        max_leaf_nodes=31,
+        max_depth=None,
+        min_samples_leaf=20,
+        l2_regularization=0.0,
+        max_bins=255,
+        categorical_features=None,
+        monotonic_cst=None,
+        warm_start=False,
+        early_stopping="auto",
+        scoring="loss",
+        validation_fraction=0.1,
+        n_iter_no_change=10,
+        tol=1e-7,
+        verbose=0,
+        random_state=None,
+    ):
         super(HistGradientBoostingRegressor, self).__init__(
-            loss=loss, learning_rate=learning_rate, max_iter=max_iter,
-            max_leaf_nodes=max_leaf_nodes, max_depth=max_depth,
+            loss=loss,
+            learning_rate=learning_rate,
+            max_iter=max_iter,
+            max_leaf_nodes=max_leaf_nodes,
+            max_depth=max_depth,
             min_samples_leaf=min_samples_leaf,
-            l2_regularization=l2_regularization, max_bins=max_bins,
-            scoring=scoring, validation_fraction=validation_fraction,
-            n_iter_no_change=n_iter_no_change, tol=tol, verbose=verbose,
-            random_state=random_state)
+            l2_regularization=l2_regularization,
+            max_bins=max_bins,
+            monotonic_cst=monotonic_cst,
+            categorical_features=categorical_features,
+            early_stopping=early_stopping,
+            warm_start=warm_start,
+            scoring=scoring,
+            validation_fraction=validation_fraction,
+            n_iter_no_change=n_iter_no_change,
+            tol=tol,
+            verbose=verbose,
+            random_state=random_state,
+        )
+        self.quantile = quantile

     def predict(self, X):
         """Predict values for X.
@@ -627,139 +1368,262 @@
         y : ndarray, shape (n_samples,)
             The predicted values.
         """
-        # Return raw predictions after converting shape
-        # (n_samples, 1) to (n_samples,)
-        return self._raw_predict(X).ravel()
+        check_is_fitted(self)
+        # Return inverse link of raw predictions after converting
+        # shape (n_samples, 1) to (n_samples,)
+        return self._loss.link.inverse(self._raw_predict(X).ravel())
+
+    def staged_predict(self, X):
+        """Predict regression target for each iteration.
+
+        This method allows monitoring (i.e. determine error on testing set)
+        after each stage.
+
+        .. versionadded:: 0.24
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            The input samples.
+
+        Yields
+        ------
+        y : generator of ndarray of shape (n_samples,)
+            The predicted values of the input samples, for each iteration.
+        """
+        for raw_predictions in self._staged_raw_predict(X):
+            yield self._loss.link.inverse(raw_predictions.ravel())

     def _encode_y(self, y):
         # Just convert y to the expected dtype
         self.n_trees_per_iteration_ = 1
         y = y.astype(Y_DTYPE, copy=False)
+        if self.loss == "poisson":
+            # Ensure y >= 0 and sum(y) > 0
+            if not (np.all(y >= 0) and np.sum(y) > 0):
+                raise ValueError(
+                    "loss='poisson' requires non-negative y and sum(y) > 0."
+                )
         return y

-    def _get_loss(self):
-        return _LOSSES[self.loss]()
-
-
-class HistGradientBoostingClassifier(BaseHistGradientBoosting,
-                                     ClassifierMixin):
+    def _get_loss(self, sample_weight):
+        # TODO: Remove in v1.2
+        if self.loss == "least_squares":
+            warnings.warn(
+                "The loss 'least_squares' was deprecated in v1.0 and will be "
+                "removed in version 1.2. Use 'squared_error' which is "
+                "equivalent.",
+                FutureWarning,
+            )
+            return _LOSSES["squared_error"](sample_weight=sample_weight)
+        elif self.loss == "least_absolute_deviation":
+            warnings.warn(
+                "The loss 'least_absolute_deviation' was deprecated in v1.0 "
+                " and will be removed in version 1.2. Use 'absolute_error' "
+                "which is equivalent.",
+                FutureWarning,
+            )
+            return _LOSSES["absolute_error"](sample_weight=sample_weight)
+
+        if self.loss == "quantile":
+            return _LOSSES[self.loss](
+                sample_weight=sample_weight, quantile=self.quantile
+            )
+        else:
+            return _LOSSES[self.loss](sample_weight=sample_weight)
+
+
+class HistGradientBoostingClassifier(ClassifierMixin, BaseHistGradientBoosting):
     """Histogram-based Gradient Boosting Classification Tree.

     This estimator is much faster than
     :class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`
-    for big datasets (n_samples >= 10 000). The input data ``X`` is pre-binned
-    into integer-valued bins, which considerably reduces the number of
-    splitting points to consider, and allows the algorithm to leverage
-    integer-based data structures. For small sample sizes,
-    :class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>`
-    might be preferred since binning may lead to split points that are too
-    approximate in this setting.
+    for big datasets (n_samples >= 10 000).
+
+    This estimator has native support for missing values (NaNs). During
+    training, the tree grower learns at each split point whether samples
+    with missing values should go to the left or right child, based on the
+    potential gain. When predicting, samples with missing values are
+    assigned to the left or right child consequently. If no missing values
+    were encountered for a given feature during training, then samples with
+    missing values are mapped to whichever child has the most samples.

     This implementation is inspired by
     `LightGBM <https://github.com/Microsoft/LightGBM>`_.

-    .. note::
-
-      This estimator is still **experimental** for now: the predictions
-      and the API might change without any deprecation cycle. To use it,
-      you need to explicitly import ``enable_hist_gradient_boosting``::
-
-        >>> # explicitly require this experimental feature
-        >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa
-        >>> # now you can import normally from ensemble
-        >>> from sklearn.ensemble import HistGradientBoostingClassifier
+    Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`.
+
+    .. versionadded:: 0.21

     Parameters
     ----------
-    loss : {'auto', 'binary_crossentropy', 'categorical_crossentropy'}, \
-            optional (default='auto')
-        The loss function to use in the boosting process. 'binary_crossentropy'
-        (also known as logistic loss) is used for binary classification and
-        generalizes to 'categorical_crossentropy' for multiclass
-        classification. 'auto' will automatically choose either loss depending
-        on the nature of the problem.
-    learning_rate : float, optional (default=1)
+    loss : {'log_loss', 'auto', 'binary_crossentropy', 'categorical_crossentropy'}, \
+            default='log_loss'
+        The loss function to use in the boosting process.
+
+        For binary classification problems, 'log_loss' is also known as logistic loss,
+        binomial deviance or binary crossentropy. Internally, the model fits one tree
+        per boosting iteration and uses the logistic sigmoid function (expit) as
+        inverse link function to compute the predicted positive class probability.
+
+        For multiclass classification problems, 'log_loss' is also known as multinomial
+        deviance or categorical crossentropy. Internally, the model fits one tree per
+        boosting iteration and per class and uses the softmax function as inverse link
+        function to compute the predicted probabilities of the classes.
+
+        .. deprecated:: 1.1
+            The loss arguments 'auto', 'binary_crossentropy' and
+            'categorical_crossentropy' were deprecated in v1.1 and will be removed in
+            version 1.3. Use `loss='log_loss'` which is equivalent.
+
+    learning_rate : float, default=0.1
         The learning rate, also known as *shrinkage*. This is used as a
         multiplicative factor for the leaves values. Use ``1`` for no
         shrinkage.
-    max_iter : int, optional (default=100)
+    max_iter : int, default=100
         The maximum number of iterations of the boosting process, i.e. the
         maximum number of trees for binary classification. For multiclass
         classification, `n_classes` trees per iteration are built.
-    max_leaf_nodes : int or None, optional (default=31)
+    max_leaf_nodes : int or None, default=31
         The maximum number of leaves for each tree. Must be strictly greater
         than 1. If None, there is no maximum limit.
-    max_depth : int or None, optional (default=None)
+    max_depth : int or None, default=None
         The maximum depth of each tree. The depth of a tree is the number of
-        nodes to go from the root to the deepest leaf. Must be strictly greater
-        than 1. Depth isn't constrained by default.
-    min_samples_leaf : int, optional (default=20)
+        edges to go from the root to the deepest leaf.
+        Depth isn't constrained by default.
+    min_samples_leaf : int, default=20
         The minimum number of samples per leaf. For small datasets with less
         than a few hundred samples, it is recommended to lower this value
         since only very shallow trees would be built.
-    l2_regularization : float, optional (default=0)
+    l2_regularization : float, default=0
         The L2 regularization parameter. Use 0 for no regularization.
-    max_bins : int, optional (default=256)
-        The maximum number of bins to use. Before training, each feature of
-        the input array ``X`` is binned into at most ``max_bins`` bins, which
-        allows for a much faster training stage. Features with a small
-        number of unique values may use less than ``max_bins`` bins. Must be no
-        larger than 256.
-    scoring : str or callable or None, optional (default=None)
+    max_bins : int, default=255
+        The maximum number of bins to use for non-missing values. Before
+        training, each feature of the input array `X` is binned into
+        integer-valued bins, which allows for a much faster training stage.
+        Features with a small number of unique values may use less than
+        ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin
+        is always reserved for missing values. Must be no larger than 255.
+    categorical_features : array-like of {bool, int} of shape (n_features) \
+            or shape (n_categorical_features,), default=None
+        Indicates the categorical features.
+
+        - None : no feature will be considered categorical.
+        - boolean array-like : boolean mask indicating categorical features.
+        - integer array-like : integer indices indicating categorical
+          features.
+
+        For each categorical feature, there must be at most `max_bins` unique
+        categories, and each categorical value must be in [0, max_bins -1].
+
+        Read more in the :ref:`User Guide <categorical_support_gbdt>`.
+
+        .. versionadded:: 0.24
+
+    monotonic_cst : array-like of int of shape (n_features), default=None
+        Indicates the monotonic constraint to enforce on each feature. -1, 1
+        and 0 respectively correspond to a negative constraint, positive
+        constraint and no constraint. Read more in the :ref:`User Guide
+        <monotonic_cst_gbdt>`.
+
+        .. versionadded:: 0.23
+
+    warm_start : bool, default=False
+        When set to ``True``, reuse the solution of the previous call to fit
+        and add more estimators to the ensemble. For results to be valid, the
+        estimator should be re-trained on the same data only.
+        See :term:`the Glossary <warm_start>`.
+    early_stopping : 'auto' or bool, default='auto'
+        If 'auto', early stopping is enabled if the sample size is larger than
+        10000. If True, early stopping is enabled, otherwise early stopping is
+        disabled.
+
+        .. versionadded:: 0.23
+
+    scoring : str or callable or None, default='loss'
         Scoring parameter to use for early stopping. It can be a single
         string (see :ref:`scoring_parameter`) or a callable (see
         :ref:`scoring`). If None, the estimator's default scorer
         is used. If ``scoring='loss'``, early stopping is checked
-        w.r.t the loss value. Only used if ``n_iter_no_change`` is not None.
-    validation_fraction : int or float or None, optional (default=0.1)
+        w.r.t the loss value. Only used if early stopping is performed.
+    validation_fraction : int or float or None, default=0.1
         Proportion (or absolute size) of training data to set aside as
         validation data for early stopping. If None, early stopping is done on
-        the training data.
-    n_iter_no_change : int or None, optional (default=None)
+        the training data. Only used if early stopping is performed.
+    n_iter_no_change : int, default=10
         Used to determine when to "early stop". The fitting process is
         stopped when none of the last ``n_iter_no_change`` scores are better
-        than the ``n_iter_no_change - 1``th-to-last one, up to some
-        tolerance. If None or 0, no early-stopping is done.
-    tol : float or None, optional (default=1e-7)
+        than the ``n_iter_no_change - 1`` -th-to-last one, up to some
+        tolerance. Only used if early stopping is performed.
+    tol : float, default=1e-7
         The absolute tolerance to use when comparing scores. The higher the
         tolerance, the more likely we are to early stop: higher tolerance
         means that it will be harder for subsequent iterations to be
         considered an improvement upon the reference score.
-    verbose: int, optional (default=0)
+    verbose : int, default=0
         The verbosity level. If not zero, print some information about the
         fitting process.
-    random_state : int, np.random.RandomStateInstance or None, \
-        optional (default=None)
+    random_state : int, RandomState instance or None, default=None
         Pseudo-random number generator to control the subsampling in the
         binning process, and the train/validation data split if early stopping
-        is enabled. See :term:`random_state`.
+        is enabled.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.

     Attributes
     ----------
+    classes_ : array, shape = (n_classes,)
+        Class labels.
+    do_early_stopping_ : bool
+        Indicates whether early stopping is used during training.
     n_iter_ : int
-        The number of estimators as selected by early stopping (if
-        n_iter_no_change is not None). Otherwise it corresponds to max_iter.
+        The number of iterations as selected by early stopping, depending on
+        the `early_stopping` parameter. Otherwise it corresponds to max_iter.
     n_trees_per_iteration_ : int
         The number of tree that are built at each iteration. This is equal to 1
         for binary classification, and to ``n_classes`` for multiclass
         classification.
-    train_score_ : ndarray, shape (max_iter + 1,)
+    train_score_ : ndarray, shape (n_iter_+1,)
         The scores at each iteration on the training data. The first entry
         is the score of the ensemble before the first iteration. Scores are
         computed according to the ``scoring`` parameter. If ``scoring`` is
         not 'loss', scores are computed on a subset of at most 10 000
         samples. Empty if no early stopping.
-    validation_score_ : ndarray, shape (max_iter + 1,)
+    validation_score_ : ndarray, shape (n_iter_+1,)
         The scores at each iteration on the held-out validation data. The
         first entry is the score of the ensemble before the first iteration.
         Scores are computed according to the ``scoring`` parameter. Empty if
         no early stopping or if ``validation_fraction`` is None.
+    is_categorical_ : ndarray, shape (n_features, ) or None
+        Boolean mask for the categorical features. ``None`` if there are no
+        categorical features.
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    GradientBoostingClassifier : Exact gradient boosting method that does not
+        scale as good on datasets with a large number of samples.
+    sklearn.tree.DecisionTreeClassifier : A decision tree classifier.
+    RandomForestClassifier : A meta-estimator that fits a number of decision
+        tree classifiers on various sub-samples of the dataset and uses
+        averaging to improve the predictive accuracy and control over-fitting.
+    AdaBoostClassifier : A meta-estimator that begins by fitting a classifier
+        on the original dataset and then fits additional copies of the
+        classifier on the same dataset where the weights of incorrectly
+        classified instances are adjusted such that subsequent classifiers
+        focus more on difficult cases.

     Examples
     --------
-    >>> # To use this experimental feature, we need to explicitly ask for it:
-    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa
-    >>> from sklearn.ensemble import HistGradientBoostingRegressor
+    >>> from sklearn.ensemble import HistGradientBoostingClassifier
     >>> from sklearn.datasets import load_iris
     >>> X, y = load_iris(return_X_y=True)
     >>> clf = HistGradientBoostingClassifier().fit(X, y)
@@ -767,22 +1631,56 @@
     1.0
     """

-    _VALID_LOSSES = ('binary_crossentropy', 'categorical_crossentropy',
-                     'auto')
-
-    def __init__(self, loss='auto', learning_rate=0.1, max_iter=100,
-                 max_leaf_nodes=31, max_depth=None, min_samples_leaf=20,
-                 l2_regularization=0., max_bins=256, scoring=None,
-                 validation_fraction=0.1, n_iter_no_change=None, tol=1e-7,
-                 verbose=0, random_state=None):
+    # TODO(1.3): Remove "binary_crossentropy", "categorical_crossentropy", "auto"
+    _VALID_LOSSES = (
+        "log_loss",
+        "binary_crossentropy",
+        "categorical_crossentropy",
+        "auto",
+    )
+
+    def __init__(
+        self,
+        loss="log_loss",
+        *,
+        learning_rate=0.1,
+        max_iter=100,
+        max_leaf_nodes=31,
+        max_depth=None,
+        min_samples_leaf=20,
+        l2_regularization=0.0,
+        max_bins=255,
+        categorical_features=None,
+        monotonic_cst=None,
+        warm_start=False,
+        early_stopping="auto",
+        scoring="loss",
+        validation_fraction=0.1,
+        n_iter_no_change=10,
+        tol=1e-7,
+        verbose=0,
+        random_state=None,
+    ):
         super(HistGradientBoostingClassifier, self).__init__(
-            loss=loss, learning_rate=learning_rate, max_iter=max_iter,
-            max_leaf_nodes=max_leaf_nodes, max_depth=max_depth,
+            loss=loss,
+            learning_rate=learning_rate,
+            max_iter=max_iter,
+            max_leaf_nodes=max_leaf_nodes,
+            max_depth=max_depth,
             min_samples_leaf=min_samples_leaf,
-            l2_regularization=l2_regularization, max_bins=max_bins,
-            scoring=scoring, validation_fraction=validation_fraction,
-            n_iter_no_change=n_iter_no_change, tol=tol, verbose=verbose,
-            random_state=random_state)
+            l2_regularization=l2_regularization,
+            max_bins=max_bins,
+            categorical_features=categorical_features,
+            monotonic_cst=monotonic_cst,
+            warm_start=warm_start,
+            early_stopping=early_stopping,
+            scoring=scoring,
+            validation_fraction=validation_fraction,
+            n_iter_no_change=n_iter_no_change,
+            tol=tol,
+            verbose=verbose,
+            random_state=random_state,
+        )

     def predict(self, X):
         """Predict classes for X.
@@ -801,6 +1699,28 @@
         encoded_classes = np.argmax(self.predict_proba(X), axis=1)
         return self.classes_[encoded_classes]

+    def staged_predict(self, X):
+        """Predict classes at each iteration.
+
+        This method allows monitoring (i.e. determine error on testing set)
+        after each stage.
+
+        .. versionadded:: 0.24
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            The input samples.
+
+        Yields
+        ------
+        y : generator of ndarray of shape (n_samples,)
+            The predicted classes of the input samples, for each iteration.
+        """
+        for proba in self.staged_predict_proba(X):
+            encoded_classes = np.argmax(proba, axis=1)
+            yield self.classes_.take(encoded_classes, axis=0)
+
     def predict_proba(self, X):
         """Predict class probabilities for X.

@@ -815,10 +1735,30 @@
             The class probabilities of the input samples.
         """
         raw_predictions = self._raw_predict(X)
-        return self.loss_.predict_proba(raw_predictions)
+        return self._loss.predict_proba(raw_predictions)
+
+    def staged_predict_proba(self, X):
+        """Predict class probabilities at each iteration.
+
+        This method allows monitoring (i.e. determine error on testing set)
+        after each stage.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            The input samples.
+
+        Yields
+        ------
+        y : generator of ndarray of shape (n_samples,)
+            The predicted class probabilities of the input samples,
+            for each iteration.
+        """
+        for raw_predictions in self._staged_raw_predict(X):
+            yield self._loss.predict_proba(raw_predictions)

     def decision_function(self, X):
-        """Compute the decision function of X.
+        """Compute the decision function of ``X``.

         Parameters
         ----------
@@ -834,9 +1774,33 @@
             classes in multiclass classification.
         """
         decision = self._raw_predict(X)
-        if decision.shape[0] == 1:
+        if decision.shape[1] == 1:
             decision = decision.ravel()
-        return decision.T
+        return decision
+
+    def staged_decision_function(self, X):
+        """Compute decision function of ``X`` for each iteration.
+
+        This method allows monitoring (i.e. determine error on testing set)
+        after each stage.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            The input samples.
+
+        Yields
+        ------
+        decision : generator of ndarray of shape (n_samples,) or \
+                (n_samples, n_trees_per_iteration)
+            The decision function of the input samples, which corresponds to
+            the raw values predicted from the trees of the ensemble . The
+            classes corresponds to that in the attribute :term:`classes_`.
+        """
+        for staged_decision in self._staged_raw_predict(X):
+            if staged_decision.shape[1] == 1:
+                staged_decision = staged_decision.ravel()
+            yield staged_decision

     def _encode_y(self, y):
         # encode classes into 0 ... n_classes - 1 and sets attributes classes_
@@ -853,11 +1817,38 @@
         encoded_y = encoded_y.astype(Y_DTYPE, copy=False)
         return encoded_y

-    def _get_loss(self):
-        if self.loss == 'auto':
+    def _get_loss(self, sample_weight):
+        # TODO(1.3): Remove "auto", "binary_crossentropy", "categorical_crossentropy"
+        if self.loss in ("auto", "binary_crossentropy", "categorical_crossentropy"):
+            warnings.warn(
+                f"The loss '{self.loss}' was deprecated in v1.1 and will be removed in "
+                "version 1.3. Use 'log_loss' which is equivalent.",
+                FutureWarning,
+            )
+
+        if self.loss in ("log_loss", "auto"):
             if self.n_trees_per_iteration_ == 1:
-                return _LOSSES['binary_crossentropy']()
+                return HalfBinomialLoss(sample_weight=sample_weight)
             else:
-                return _LOSSES['categorical_crossentropy']()
-
-        return _LOSSES[self.loss]()
+                return HalfMultinomialLoss(
+                    sample_weight=sample_weight, n_classes=self.n_trees_per_iteration_
+                )
+        if self.loss == "categorical_crossentropy":
+            if self.n_trees_per_iteration_ == 1:
+                raise ValueError(
+                    f"loss='{self.loss}' is not suitable for a binary classification "
+                    "problem. Please use loss='log_loss' instead."
+                )
+            else:
+                return HalfMultinomialLoss(
+                    sample_weight=sample_weight, n_classes=self.n_trees_per_iteration_
+                )
+        if self.loss == "binary_crossentropy":
+            if self.n_trees_per_iteration_ > 1:
+                raise ValueError(
+                    f"loss='{self.loss}' is not defined for multiclass "
+                    f"classification with n_classes={self.n_trees_per_iteration_}, "
+                    "use loss='log_loss' instead."
+                )
+            else:
+                return HalfBinomialLoss(sample_weight=sample_weight)
('sklearn/ensemble/_hist_gradient_boosting', '_gradient_boosting.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,8 +1,3 @@
-# cython: cdivision=True
-# cython: boundscheck=False
-# cython: wraparound=False
-# cython: language_level=3
-
 # Author: Nicolas Hug

 cimport cython
@@ -10,13 +5,17 @@
 import numpy as np
 cimport numpy as np

-from .types import Y_DTYPE
-from .types cimport Y_DTYPE_C
+from .common import Y_DTYPE
+from .common cimport Y_DTYPE_C
+
+np.import_array()


 def _update_raw_predictions(
         Y_DTYPE_C [::1] raw_predictions,  # OUT
-        grower):
+        grower,
+        n_threads,
+):
     """Update raw_predictions with the predictions of the newest tree.

     This is equivalent to (and much faster than):
@@ -40,7 +39,7 @@
     values = np.array([leaf.value for leaf in leaves], dtype=Y_DTYPE)

     _update_raw_predictions_helper(raw_predictions, starts, stops, partition,
-                                   values)
+                                   values, n_threads)


 cdef inline void _update_raw_predictions_helper(
@@ -48,13 +47,16 @@
         const unsigned int [::1] starts,
         const unsigned int [::1] stops,
         const unsigned int [::1] partition,
-        const Y_DTYPE_C [::1] values):
+        const Y_DTYPE_C [::1] values,
+        int n_threads,
+):

     cdef:
         unsigned int position
         int leaf_idx
         int n_leaves = starts.shape[0]

-    for leaf_idx in prange(n_leaves, nogil=True):
+    for leaf_idx in prange(n_leaves, schedule='static', nogil=True,
+                           num_threads=n_threads):
         for position in range(starts[leaf_idx], stops[leaf_idx]):
             raw_predictions[partition[position]] += values[leaf_idx]
('sklearn/ensemble/_hist_gradient_boosting', 'grower.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,7 +1,7 @@
 """
 This module contains the TreeGrower class.

-TreeGrowee builds a regression tree fitting a Newton-Raphson step, based on
+TreeGrower builds a regression tree fitting a Newton-Raphson step, based on
 the gradients and hessians of the training data.
 """
 # Author: Nicolas Hug
@@ -13,8 +13,17 @@

 from .splitting import Splitter
 from .histogram import HistogramBuilder
-from .predictor import TreePredictor, PREDICTOR_RECORD_DTYPE
+from .predictor import TreePredictor
 from .utils import sum_parallel
+from .common import PREDICTOR_RECORD_DTYPE
+from .common import X_BITSET_INNER_DTYPE
+from .common import Y_DTYPE
+from .common import MonotonicConstraint
+from ._bitset import set_raw_bitset_from_binned_bitset
+from sklearn.utils._openmp_helpers import _openmp_effective_n_threads
+
+
+EPS = np.finfo(Y_DTYPE).eps  # to avoid zero division errors


 class TreeNode:
@@ -27,27 +36,23 @@
     ----------
     depth : int
         The depth of the node, i.e. its distance from the root.
-    sample_indices : ndarray of unsigned int, shape (n_samples_at_node,)
+    sample_indices : ndarray of shape (n_samples_at_node,), dtype=np.uint
         The indices of the samples at the node.
     sum_gradients : float
         The sum of the gradients of the samples at the node.
     sum_hessians : float
         The sum of the hessians of the samples at the node.
-    parent : TreeNode or None, optional (default=None)
-        The parent of the node. None for root.

     Attributes
     ----------
     depth : int
         The depth of the node, i.e. its distance from the root.
-    sample_indices : ndarray of unsigned int, shape (n_samples_at_node,)
+    sample_indices : ndarray of shape (n_samples_at_node,), dtype=np.uint
         The indices of the samples at the node.
     sum_gradients : float
         The sum of the gradients of the samples at the node.
     sum_hessians : float
         The sum of the hessians of the samples at the node.
-    parent : TreeNode or None
-        The parent of the node. None for root.
     split_info : SplitInfo or None
         The result of the split evaluation.
     left_child : TreeNode or None
@@ -66,10 +71,7 @@
     split_info = None
     left_child = None
     right_child = None
-    value = None
     histograms = None
-    sibling = None
-    parent = None

     # start and stop indices of the node in the splitter.partition
     # array. Concretely,
@@ -82,14 +84,24 @@
     partition_start = 0
     partition_stop = 0

-    def __init__(self, depth, sample_indices, sum_gradients,
-                 sum_hessians, parent=None):
+    def __init__(self, depth, sample_indices, sum_gradients, sum_hessians, value=None):
         self.depth = depth
         self.sample_indices = sample_indices
         self.n_samples = sample_indices.shape[0]
         self.sum_gradients = sum_gradients
         self.sum_hessians = sum_hessians
-        self.parent = parent
+        self.value = value
+        self.is_leaf = False
+        self.set_children_bounds(float("-inf"), float("+inf"))
+
+    def set_children_bounds(self, lower, upper):
+        """Set children values bounds to respect monotonic constraints."""
+
+        # These are bounds for the node's *children* values, not the node's
+        # value. The bounds are used in the splitter when considering potential
+        # left and right child.
+        self.children_lower_bound = lower
+        self.children_upper_bound = upper

     def __lt__(self, other_node):
         """Comparison for priority queue.
@@ -117,137 +129,265 @@

     Parameters
     ----------
-    X_binned : ndarray of int, shape (n_samples, n_features)
+    X_binned : ndarray of shape (n_samples, n_features), dtype=np.uint8
         The binned input samples. Must be Fortran-aligned.
-    gradients : ndarray, shape (n_samples,)
+    gradients : ndarray of shape (n_samples,)
         The gradients of each training sample. Those are the gradients of the
         loss w.r.t the predictions, evaluated at iteration ``i - 1``.
-    hessians : ndarray, shape (n_samples,)
+    hessians : ndarray of shape (n_samples,)
         The hessians of each training sample. Those are the hessians of the
         loss w.r.t the predictions, evaluated at iteration ``i - 1``.
-    max_leaf_nodes : int or None, optional (default=None)
+    max_leaf_nodes : int, default=None
         The maximum number of leaves for each tree. If None, there is no
         maximum limit.
-    max_depth : int or None, optional (default=None)
+    max_depth : int, default=None
         The maximum depth of each tree. The depth of a tree is the number of
-        nodes to go from the root to the deepest leaf.
-    min_samples_leaf : int, optional (default=20)
+        edges to go from the root to the deepest leaf.
+        Depth isn't constrained by default.
+    min_samples_leaf : int, default=20
         The minimum number of samples per leaf.
-    min_gain_to_split : float, optional (default=0.)
+    min_gain_to_split : float, default=0.
         The minimum gain needed to split a node. Splits with lower gain will
         be ignored.
-    max_bins : int, optional (default=256)
-        The maximum number of bins. Used to define the shape of the
-        histograms.
-    actual_n_bins : ndarray of int or int, optional (default=None)
-        The actual number of bins needed for each feature, which is lower or
-        equal to ``max_bins``. If it's an int, all features are considered to
-        have the same number of bins. If None, all features are considered to
-        have ``max_bins`` bins.
-    l2_regularization : float, optional (default=0)
+    n_bins : int, default=256
+        The total number of bins, including the bin for missing values. Used
+        to define the shape of the histograms.
+    n_bins_non_missing : ndarray, dtype=np.uint32, default=None
+        For each feature, gives the number of bins actually used for
+        non-missing values. For features with a lot of unique values, this
+        is equal to ``n_bins - 1``. If it's an int, all features are
+        considered to have the same number of bins. If None, all features
+        are considered to have ``n_bins - 1`` bins.
+    has_missing_values : bool or ndarray, dtype=bool, default=False
+        Whether each feature contains missing values (in the training data).
+        If it's a bool, the same value is used for all features.
+    is_categorical : ndarray of bool of shape (n_features,), default=None
+        Indicates categorical features.
+    monotonic_cst : array-like of shape (n_features,), dtype=int, default=None
+        Indicates the monotonic constraint to enforce on each feature. -1, 1
+        and 0 respectively correspond to a positive constraint, negative
+        constraint and no constraint. Read more in the :ref:`User Guide
+        <monotonic_cst_gbdt>`.
+    l2_regularization : float, default=0.
         The L2 regularization parameter.
-    min_hessian_to_split : float, optional (default=1e-3)
+    min_hessian_to_split : float, default=1e-3
         The minimum sum of hessians needed in each node. Splits that result in
         at least one child having a sum of hessians less than
         ``min_hessian_to_split`` are discarded.
-    shrinkage : float, optional (default=1)
+    shrinkage : float, default=1.
         The shrinkage parameter to apply to the leaves values, also known as
         learning rate.
+    n_threads : int, default=None
+        Number of OpenMP threads to use. `_openmp_effective_n_threads` is called
+        to determine the effective number of threads use, which takes cgroups CPU
+        quotes into account. See the docstring of `_openmp_effective_n_threads`
+        for details.
     """
-    def __init__(self, X_binned, gradients, hessians, max_leaf_nodes=None,
-                 max_depth=None, min_samples_leaf=20, min_gain_to_split=0.,
-                 max_bins=256, actual_n_bins=None, l2_regularization=0.,
-                 min_hessian_to_split=1e-3, shrinkage=1.):
-
-        self._validate_parameters(X_binned, max_leaf_nodes, max_depth,
-                                  min_samples_leaf, min_gain_to_split,
-                                  l2_regularization, min_hessian_to_split)
-
-        if actual_n_bins is None:
-            actual_n_bins = max_bins
-
-        if isinstance(actual_n_bins, numbers.Integral):
-            actual_n_bins = np.array(
-                [actual_n_bins] * X_binned.shape[1],
-                dtype=np.uint32)
+
+    def __init__(
+        self,
+        X_binned,
+        gradients,
+        hessians,
+        max_leaf_nodes=None,
+        max_depth=None,
+        min_samples_leaf=20,
+        min_gain_to_split=0.0,
+        n_bins=256,
+        n_bins_non_missing=None,
+        has_missing_values=False,
+        is_categorical=None,
+        monotonic_cst=None,
+        l2_regularization=0.0,
+        min_hessian_to_split=1e-3,
+        shrinkage=1.0,
+        n_threads=None,
+    ):
+
+        self._validate_parameters(
+            X_binned,
+            max_leaf_nodes,
+            max_depth,
+            min_samples_leaf,
+            min_gain_to_split,
+            l2_regularization,
+            min_hessian_to_split,
+        )
+        n_threads = _openmp_effective_n_threads(n_threads)
+
+        if n_bins_non_missing is None:
+            n_bins_non_missing = n_bins - 1
+
+        if isinstance(n_bins_non_missing, numbers.Integral):
+            n_bins_non_missing = np.array(
+                [n_bins_non_missing] * X_binned.shape[1], dtype=np.uint32
+            )
         else:
-            actual_n_bins = np.asarray(actual_n_bins, dtype=np.uint32)
+            n_bins_non_missing = np.asarray(n_bins_non_missing, dtype=np.uint32)
+
+        if isinstance(has_missing_values, bool):
+            has_missing_values = [has_missing_values] * X_binned.shape[1]
+        has_missing_values = np.asarray(has_missing_values, dtype=np.uint8)
+
+        if monotonic_cst is None:
+            self.with_monotonic_cst = False
+            monotonic_cst = np.full(
+                shape=X_binned.shape[1],
+                fill_value=MonotonicConstraint.NO_CST,
+                dtype=np.int8,
+            )
+        else:
+            self.with_monotonic_cst = True
+            monotonic_cst = np.asarray(monotonic_cst, dtype=np.int8)
+
+            if monotonic_cst.shape[0] != X_binned.shape[1]:
+                raise ValueError(
+                    "monotonic_cst has shape {} but the input data "
+                    "X has {} features.".format(
+                        monotonic_cst.shape[0], X_binned.shape[1]
+                    )
+                )
+            if np.any(monotonic_cst < -1) or np.any(monotonic_cst > 1):
+                raise ValueError(
+                    "monotonic_cst must be None or an array-like of -1, 0 or 1."
+                )
+
+        if is_categorical is None:
+            is_categorical = np.zeros(shape=X_binned.shape[1], dtype=np.uint8)
+        else:
+            is_categorical = np.asarray(is_categorical, dtype=np.uint8)
+
+        if np.any(
+            np.logical_and(
+                is_categorical == 1, monotonic_cst != MonotonicConstraint.NO_CST
+            )
+        ):
+            raise ValueError("Categorical features cannot have monotonic constraints.")

         hessians_are_constant = hessians.shape[0] == 1
         self.histogram_builder = HistogramBuilder(
-            X_binned, max_bins, gradients, hessians, hessians_are_constant)
+            X_binned, n_bins, gradients, hessians, hessians_are_constant, n_threads
+        )
+        missing_values_bin_idx = n_bins - 1
         self.splitter = Splitter(
-            X_binned, max_bins, actual_n_bins, l2_regularization,
-            min_hessian_to_split, min_samples_leaf, min_gain_to_split,
-            hessians_are_constant)
+            X_binned,
+            n_bins_non_missing,
+            missing_values_bin_idx,
+            has_missing_values,
+            is_categorical,
+            monotonic_cst,
+            l2_regularization,
+            min_hessian_to_split,
+            min_samples_leaf,
+            min_gain_to_split,
+            hessians_are_constant,
+            n_threads,
+        )
+        self.n_bins_non_missing = n_bins_non_missing
+        self.missing_values_bin_idx = missing_values_bin_idx
         self.max_leaf_nodes = max_leaf_nodes
-        self.max_bins = max_bins
+        self.has_missing_values = has_missing_values
+        self.monotonic_cst = monotonic_cst
+        self.is_categorical = is_categorical
+        self.l2_regularization = l2_regularization
         self.n_features = X_binned.shape[1]
         self.max_depth = max_depth
         self.min_samples_leaf = min_samples_leaf
         self.X_binned = X_binned
         self.min_gain_to_split = min_gain_to_split
         self.shrinkage = shrinkage
+        self.n_threads = n_threads
         self.splittable_nodes = []
         self.finalized_leaves = []
-        self.total_find_split_time = 0.  # time spent finding the best splits
-        self.total_compute_hist_time = 0.  # time spent computing histograms
-        self.total_apply_split_time = 0.  # time spent splitting nodes
+        self.total_find_split_time = 0.0  # time spent finding the best splits
+        self.total_compute_hist_time = 0.0  # time spent computing histograms
+        self.total_apply_split_time = 0.0  # time spent splitting nodes
+        self.n_categorical_splits = 0
         self._intilialize_root(gradients, hessians, hessians_are_constant)
         self.n_nodes = 1

-    def _validate_parameters(self, X_binned, max_leaf_nodes, max_depth,
-                             min_samples_leaf, min_gain_to_split,
-                             l2_regularization, min_hessian_to_split):
+    def _validate_parameters(
+        self,
+        X_binned,
+        max_leaf_nodes,
+        max_depth,
+        min_samples_leaf,
+        min_gain_to_split,
+        l2_regularization,
+        min_hessian_to_split,
+    ):
         """Validate parameters passed to __init__.

         Also validate parameters passed to splitter.
         """
         if X_binned.dtype != np.uint8:
-            raise NotImplementedError(
-                "X_binned must be of type uint8.")
+            raise NotImplementedError("X_binned must be of type uint8.")
         if not X_binned.flags.f_contiguous:
             raise ValueError(
                 "X_binned should be passed as Fortran contiguous "
-                "array for maximum efficiency.")
+                "array for maximum efficiency."
+            )
         if max_leaf_nodes is not None and max_leaf_nodes <= 1:
-            raise ValueError('max_leaf_nodes={} should not be'
-                             ' smaller than 2'.format(max_leaf_nodes))
-        if max_depth is not None and max_depth <= 1:
-            raise ValueError('max_depth={} should not be'
-                             ' smaller than 2'.format(max_depth))
+            raise ValueError(
+                "max_leaf_nodes={} should not be smaller than 2".format(max_leaf_nodes)
+            )
+        if max_depth is not None and max_depth < 1:
+            raise ValueError(
+                "max_depth={} should not be smaller than 1".format(max_depth)
+            )
         if min_samples_leaf < 1:
-            raise ValueError('min_samples_leaf={} should '
-                             'not be smaller than 1'.format(min_samples_leaf))
+            raise ValueError(
+                "min_samples_leaf={} should not be smaller than 1".format(
+                    min_samples_leaf
+                )
+            )
         if min_gain_to_split < 0:
-            raise ValueError('min_gain_to_split={} '
-                             'must be positive.'.format(min_gain_to_split))
+            raise ValueError(
+                "min_gain_to_split={} must be positive.".format(min_gain_to_split)
+            )
         if l2_regularization < 0:
-            raise ValueError('l2_regularization={} must be '
-                             'positive.'.format(l2_regularization))
+            raise ValueError(
+                "l2_regularization={} must be positive.".format(l2_regularization)
+            )
         if min_hessian_to_split < 0:
-            raise ValueError('min_hessian_to_split={} '
-                             'must be positive.'.format(min_hessian_to_split))
+            raise ValueError(
+                "min_hessian_to_split={} must be positive.".format(min_hessian_to_split)
+            )

     def grow(self):
         """Grow the tree, from root to leaves."""
         while self.splittable_nodes:
             self.split_next()

+        self._apply_shrinkage()
+
+    def _apply_shrinkage(self):
+        """Multiply leaves values by shrinkage parameter.
+
+        This must be done at the very end of the growing process. If this were
+        done during the growing process e.g. in finalize_leaf(), then a leaf
+        would be shrunk but its sibling would potentially not be (if it's a
+        non-leaf), which would lead to a wrong computation of the 'middle'
+        value needed to enforce the monotonic constraints.
+        """
+        for leaf in self.finalized_leaves:
+            leaf.value *= self.shrinkage
+
     def _intilialize_root(self, gradients, hessians, hessians_are_constant):
         """Initialize root node and finalize it if needed."""
         n_samples = self.X_binned.shape[0]
         depth = 0
-        sum_gradients = sum_parallel(gradients)
+        sum_gradients = sum_parallel(gradients, self.n_threads)
         if self.histogram_builder.hessians_are_constant:
             sum_hessians = hessians[0] * n_samples
         else:
-            sum_hessians = sum_parallel(hessians)
+            sum_hessians = sum_parallel(hessians, self.n_threads)
         self.root = TreeNode(
             depth=depth,
             sample_indices=self.splitter.partition,
             sum_gradients=sum_gradients,
-            sum_hessians=sum_hessians
+            sum_hessians=sum_hessians,
+            value=0,
         )

         self.root.partition_start = 0
@@ -262,7 +402,8 @@
             return

         self.root.histograms = self.histogram_builder.compute_histograms_brute(
-            self.root.sample_indices)
+            self.root.sample_indices
+        )
         self._compute_best_split_and_push(self.root)

     def _compute_best_split_and_push(self, node):
@@ -275,8 +416,14 @@
         """

         node.split_info = self.splitter.find_node_split(
-            node.sample_indices, node.histograms, node.sum_gradients,
-            node.sum_hessians)
+            node.n_samples,
+            node.histograms,
+            node.sum_gradients,
+            node.sum_hessians,
+            node.value,
+            node.children_lower_bound,
+            node.children_upper_bound,
+        )

         if node.split_info.gain <= 0:  # no valid split
             self._finalize_leaf(node)
@@ -297,28 +444,32 @@
         node = heappop(self.splittable_nodes)

         tic = time()
-        (sample_indices_left,
-         sample_indices_right,
-         right_child_pos) = self.splitter.split_indices(node.split_info,
-                                                        node.sample_indices)
+        (
+            sample_indices_left,
+            sample_indices_right,
+            right_child_pos,
+        ) = self.splitter.split_indices(node.split_info, node.sample_indices)
         self.total_apply_split_time += time() - tic

         depth = node.depth + 1
         n_leaf_nodes = len(self.finalized_leaves) + len(self.splittable_nodes)
         n_leaf_nodes += 2

-        left_child_node = TreeNode(depth,
-                                   sample_indices_left,
-                                   node.split_info.sum_gradient_left,
-                                   node.split_info.sum_hessian_left,
-                                   parent=node)
-        right_child_node = TreeNode(depth,
-                                    sample_indices_right,
-                                    node.split_info.sum_gradient_right,
-                                    node.split_info.sum_hessian_right,
-                                    parent=node)
-        left_child_node.sibling = right_child_node
-        right_child_node.sibling = left_child_node
+        left_child_node = TreeNode(
+            depth,
+            sample_indices_left,
+            node.split_info.sum_gradient_left,
+            node.split_info.sum_hessian_left,
+            value=node.split_info.value_left,
+        )
+        right_child_node = TreeNode(
+            depth,
+            sample_indices_right,
+            node.split_info.sum_gradient_right,
+            node.split_info.sum_hessian_right,
+            value=node.split_info.value_right,
+        )
+
         node.right_child = right_child_node
         node.left_child = left_child_node

@@ -328,29 +479,60 @@
         right_child_node.partition_start = left_child_node.partition_stop
         right_child_node.partition_stop = node.partition_stop

+        if not self.has_missing_values[node.split_info.feature_idx]:
+            # If no missing values are encountered at fit time, then samples
+            # with missing values during predict() will go to whichever child
+            # has the most samples.
+            node.split_info.missing_go_to_left = (
+                left_child_node.n_samples > right_child_node.n_samples
+            )
+
         self.n_nodes += 2
+        self.n_categorical_splits += node.split_info.is_categorical
+
+        if self.max_leaf_nodes is not None and n_leaf_nodes == self.max_leaf_nodes:
+            self._finalize_leaf(left_child_node)
+            self._finalize_leaf(right_child_node)
+            self._finalize_splittable_nodes()
+            return left_child_node, right_child_node

         if self.max_depth is not None and depth == self.max_depth:
             self._finalize_leaf(left_child_node)
             self._finalize_leaf(right_child_node)
             return left_child_node, right_child_node

-        if (self.max_leaf_nodes is not None
-                and n_leaf_nodes == self.max_leaf_nodes):
-            self._finalize_leaf(left_child_node)
-            self._finalize_leaf(right_child_node)
-            self._finalize_splittable_nodes()
-            return left_child_node, right_child_node
-
         if left_child_node.n_samples < self.min_samples_leaf * 2:
             self._finalize_leaf(left_child_node)
         if right_child_node.n_samples < self.min_samples_leaf * 2:
             self._finalize_leaf(right_child_node)

-        # Compute histograms of childs, and compute their best possible split
+        if self.with_monotonic_cst:
+            # Set value bounds for respecting monotonic constraints
+            # See test_nodes_values() for details
+            if (
+                self.monotonic_cst[node.split_info.feature_idx]
+                == MonotonicConstraint.NO_CST
+            ):
+                lower_left = lower_right = node.children_lower_bound
+                upper_left = upper_right = node.children_upper_bound
+            else:
+                mid = (left_child_node.value + right_child_node.value) / 2
+                if (
+                    self.monotonic_cst[node.split_info.feature_idx]
+                    == MonotonicConstraint.POS
+                ):
+                    lower_left, upper_left = node.children_lower_bound, mid
+                    lower_right, upper_right = mid, node.children_upper_bound
+                else:  # NEG
+                    lower_left, upper_left = mid, node.children_upper_bound
+                    lower_right, upper_right = node.children_lower_bound, mid
+            left_child_node.set_children_bounds(lower_left, upper_left)
+            right_child_node.set_children_bounds(lower_right, upper_right)
+
+        # Compute histograms of children, and compute their best possible split
         # (if needed)
-        should_split_left = left_child_node.value is None  # node isn't a leaf
-        should_split_right = right_child_node.value is None
+        should_split_left = not left_child_node.is_leaf
+        should_split_right = not right_child_node.is_leaf
         if should_split_left or should_split_right:

             # We will compute the histograms of both nodes even if one of them
@@ -369,12 +551,14 @@
             # smallest number of samples, and the subtraction trick O(n_bins)
             # on the other one.
             tic = time()
-            smallest_child.histograms = \
-                self.histogram_builder.compute_histograms_brute(
-                    smallest_child.sample_indices)
-            largest_child.histograms = \
+            smallest_child.histograms = self.histogram_builder.compute_histograms_brute(
+                smallest_child.sample_indices
+            )
+            largest_child.histograms = (
                 self.histogram_builder.compute_histograms_subtraction(
-                    node.histograms, smallest_child.histograms)
+                    node.histograms, smallest_child.histograms
+                )
+            )
             self.total_compute_hist_time += time() - tic

             tic = time()
@@ -384,20 +568,22 @@
                 self._compute_best_split_and_push(right_child_node)
             self.total_find_split_time += time() - tic

+            # Release memory used by histograms as they are no longer needed
+            # for leaf nodes since they won't be split.
+            for child in (left_child_node, right_child_node):
+                if child.is_leaf:
+                    del child.histograms
+
+        # Release memory used by histograms as they are no longer needed for
+        # internal nodes once children histograms have been computed.
+        del node.histograms
+
         return left_child_node, right_child_node

     def _finalize_leaf(self, node):
-        """Compute the prediction value that minimizes the objective function.
-
-        This sets the node.value attribute (node is a leaf iff node.value is
-        not None).
-
-        See Equation 5 of:
-        XGBoost: A Scalable Tree Boosting System, T. Chen, C. Guestrin, 2016
-        https://arxiv.org/abs/1603.02754
-        """
-        node.value = -self.shrinkage * node.sum_gradients / (
-            node.sum_hessians + self.splitter.l2_regularization)
+        """Make node a leaf of the tree being grown."""
+
+        node.is_leaf = True
         self.finalized_leaves.append(node)

     def _finalize_splittable_nodes(self):
@@ -409,57 +595,115 @@
             node = self.splittable_nodes.pop()
             self._finalize_leaf(node)

-    def make_predictor(self, bin_thresholds=None):
+    def make_predictor(self, binning_thresholds):
         """Make a TreePredictor object out of the current tree.

         Parameters
         ----------
-        bin_thresholds : array-like of floats, optional (default=None)
-            The actual thresholds values of each bin.
+        binning_thresholds : array-like of floats
+            Corresponds to the bin_thresholds_ attribute of the BinMapper.
+            For each feature, this stores:
+
+            - the bin frontiers for continuous features
+            - the unique raw category values for categorical features

         Returns
         -------
         A TreePredictor object.
         """
         predictor_nodes = np.zeros(self.n_nodes, dtype=PREDICTOR_RECORD_DTYPE)
-        _fill_predictor_node_array(predictor_nodes, self.root,
-                                   bin_thresholds=bin_thresholds)
-        return TreePredictor(predictor_nodes)
-
-
-def _fill_predictor_node_array(predictor_nodes, grower_node,
-                               bin_thresholds, next_free_idx=0):
+        binned_left_cat_bitsets = np.zeros(
+            (self.n_categorical_splits, 8), dtype=X_BITSET_INNER_DTYPE
+        )
+        raw_left_cat_bitsets = np.zeros(
+            (self.n_categorical_splits, 8), dtype=X_BITSET_INNER_DTYPE
+        )
+        _fill_predictor_arrays(
+            predictor_nodes,
+            binned_left_cat_bitsets,
+            raw_left_cat_bitsets,
+            self.root,
+            binning_thresholds,
+            self.n_bins_non_missing,
+        )
+        return TreePredictor(
+            predictor_nodes, binned_left_cat_bitsets, raw_left_cat_bitsets
+        )
+
+
+def _fill_predictor_arrays(
+    predictor_nodes,
+    binned_left_cat_bitsets,
+    raw_left_cat_bitsets,
+    grower_node,
+    binning_thresholds,
+    n_bins_non_missing,
+    next_free_node_idx=0,
+    next_free_bitset_idx=0,
+):
     """Helper used in make_predictor to set the TreePredictor fields."""
-    node = predictor_nodes[next_free_idx]
-    node['count'] = grower_node.n_samples
-    node['depth'] = grower_node.depth
+    node = predictor_nodes[next_free_node_idx]
+    node["count"] = grower_node.n_samples
+    node["depth"] = grower_node.depth
     if grower_node.split_info is not None:
-        node['gain'] = grower_node.split_info.gain
+        node["gain"] = grower_node.split_info.gain
     else:
-        node['gain'] = -1
-
-    if grower_node.value is not None:
+        node["gain"] = -1
+
+    node["value"] = grower_node.value
+
+    if grower_node.is_leaf:
         # Leaf node
-        node['is_leaf'] = True
-        node['value'] = grower_node.value
-        return next_free_idx + 1
+        node["is_leaf"] = True
+        return next_free_node_idx + 1, next_free_bitset_idx
+
+    split_info = grower_node.split_info
+    feature_idx, bin_idx = split_info.feature_idx, split_info.bin_idx
+    node["feature_idx"] = feature_idx
+    node["bin_threshold"] = bin_idx
+    node["missing_go_to_left"] = split_info.missing_go_to_left
+    node["is_categorical"] = split_info.is_categorical
+
+    if split_info.bin_idx == n_bins_non_missing[feature_idx] - 1:
+        # Split is on the last non-missing bin: it's a "split on nans".
+        # All nans go to the right, the rest go to the left.
+        # Note: for categorical splits, bin_idx is 0 and we rely on the bitset
+        node["num_threshold"] = np.inf
+    elif split_info.is_categorical:
+        categories = binning_thresholds[feature_idx]
+        node["bitset_idx"] = next_free_bitset_idx
+        binned_left_cat_bitsets[next_free_bitset_idx] = split_info.left_cat_bitset
+        set_raw_bitset_from_binned_bitset(
+            raw_left_cat_bitsets[next_free_bitset_idx],
+            split_info.left_cat_bitset,
+            categories,
+        )
+        next_free_bitset_idx += 1
     else:
-        # Decision node
-        split_info = grower_node.split_info
-        feature_idx, bin_idx = split_info.feature_idx, split_info.bin_idx
-        node['feature_idx'] = feature_idx
-        node['bin_threshold'] = bin_idx
-        if bin_thresholds is not None:
-            threshold = bin_thresholds[feature_idx][bin_idx]
-            node['threshold'] = threshold
-        next_free_idx += 1
-
-        node['left'] = next_free_idx
-        next_free_idx = _fill_predictor_node_array(
-            predictor_nodes, grower_node.left_child,
-            bin_thresholds=bin_thresholds, next_free_idx=next_free_idx)
-
-        node['right'] = next_free_idx
-        return _fill_predictor_node_array(
-            predictor_nodes, grower_node.right_child,
-            bin_thresholds=bin_thresholds, next_free_idx=next_free_idx)
+        node["num_threshold"] = binning_thresholds[feature_idx][bin_idx]
+
+    next_free_node_idx += 1
+
+    node["left"] = next_free_node_idx
+    next_free_node_idx, next_free_bitset_idx = _fill_predictor_arrays(
+        predictor_nodes,
+        binned_left_cat_bitsets,
+        raw_left_cat_bitsets,
+        grower_node.left_child,
+        binning_thresholds=binning_thresholds,
+        n_bins_non_missing=n_bins_non_missing,
+        next_free_node_idx=next_free_node_idx,
+        next_free_bitset_idx=next_free_bitset_idx,
+    )
+
+    node["right"] = next_free_node_idx
+    return _fill_predictor_arrays(
+        predictor_nodes,
+        binned_left_cat_bitsets,
+        raw_left_cat_bitsets,
+        grower_node.right_child,
+        binning_thresholds=binning_thresholds,
+        n_bins_non_missing=n_bins_non_missing,
+        next_free_node_idx=next_free_node_idx,
+        next_free_bitset_idx=next_free_bitset_idx,
+    )
('sklearn/ensemble/_hist_gradient_boosting', '_binning.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,30 +1,37 @@
-# cython: cdivision=True
-# cython: boundscheck=False
-# cython: wraparound=False
-# cython: nonecheck=False
-# cython: language_level=3
-
 # Author: Nicolas Hug

 cimport cython

 import numpy as np
 cimport numpy as np
+from numpy.math cimport INFINITY
 from cython.parallel import prange
+from libc.math cimport isnan

-from .types cimport X_DTYPE_C, X_BINNED_DTYPE_C
+from .common cimport X_DTYPE_C, X_BINNED_DTYPE_C

-cpdef _map_to_bins(const X_DTYPE_C [:, :] data, list binning_thresholds,
-                   X_BINNED_DTYPE_C [::1, :] binned):
-    """Bin numerical values to discrete integer-coded levels.
+np.import_array()
+
+
+def _map_to_bins(const X_DTYPE_C [:, :] data,
+                 list binning_thresholds,
+                 const unsigned char missing_values_bin_idx,
+                 int n_threads,
+                 X_BINNED_DTYPE_C [::1, :] binned):
+    """Bin continuous and categorical values to discrete integer-coded levels.
+
+    A given value x is mapped into bin value i iff
+    thresholds[i - 1] < x <= thresholds[i]

     Parameters
     ----------
     data : ndarray, shape (n_samples, n_features)
-        The numerical data to bin.
+        The data to bin.
     binning_thresholds : list of arrays
         For each feature, stores the increasing numeric values that are
         used to separate the bins.
+    n_threads : int
+        Number of OpenMP threads to use.
     binned : ndarray, shape (n_samples, n_features)
         Output array, must be fortran aligned.
     """
@@ -32,14 +39,18 @@
         int feature_idx

     for feature_idx in range(data.shape[1]):
-        _map_num_col_to_bins(data[:, feature_idx],
+        _map_col_to_bins(data[:, feature_idx],
                              binning_thresholds[feature_idx],
+                             missing_values_bin_idx,
+                             n_threads,
                              binned[:, feature_idx])


-cpdef void _map_num_col_to_bins(const X_DTYPE_C [:] data,
-                                const X_DTYPE_C [:] binning_thresholds,
-                                X_BINNED_DTYPE_C [:] binned):
+cdef void _map_col_to_bins(const X_DTYPE_C [:] data,
+                               const X_DTYPE_C [:] binning_thresholds,
+                               const unsigned char missing_values_bin_idx,
+                               int n_threads,
+                               X_BINNED_DTYPE_C [:] binned):
     """Binary search to find the bin index for each value in the data."""
     cdef:
         int i
@@ -47,12 +58,19 @@
         int right
         int middle

-    for i in prange(data.shape[0], schedule='static', nogil=True):
-        left, right = 0, binning_thresholds.shape[0]
-        while left < right:
-            middle = (right + left - 1) // 2
-            if data[i] <= binning_thresholds[middle]:
-                right = middle
-            else:
-                left = middle + 1
-        binned[i] = left
+    for i in prange(data.shape[0], schedule='static', nogil=True,
+                    num_threads=n_threads):
+        if isnan(data[i]):
+            binned[i] = missing_values_bin_idx
+        else:
+            # for known values, use binary search
+            left, right = 0, binning_thresholds.shape[0]
+            while left < right:
+                # equal to (right + left - 1) // 2 but avoids overflow
+                middle = left + (right - left - 1) // 2
+                if data[i] <= binning_thresholds[middle]:
+                    right = middle
+                else:
+                    left = middle + 1
+
+            binned[i] = left
('sklearn/ensemble/_hist_gradient_boosting', 'utils.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,7 +1,3 @@
-# cython: cdivision=True
-# cython: boundscheck=False
-# cython: wraparound=False
-# cython: language_level=3
 """This module contains utility routines."""
 # Author: Nicolas Hug

@@ -9,11 +5,11 @@

 from ...base import is_classifier
 from .binning import _BinMapper
-from .types cimport G_H_DTYPE_C
-from .types cimport Y_DTYPE_C
+from .common cimport G_H_DTYPE_C
+from .common cimport Y_DTYPE_C


-def get_equivalent_estimator(estimator, lib='lightgbm'):
+def get_equivalent_estimator(estimator, lib='lightgbm', n_classes=None):
     """Return an unfitted estimator from another lib with matching hyperparams.

     This utility function takes care of renaming the sklearn parameters into
@@ -38,13 +34,13 @@
     if sklearn_params['loss'] == 'auto':
         raise ValueError('auto loss is not accepted. We need to know if '
                          'the problem is binary or multiclass classification.')
-    if sklearn_params['n_iter_no_change'] is not None:
+    if sklearn_params['early_stopping']:
         raise NotImplementedError('Early stopping should be deactivated.')

     lightgbm_loss_mapping = {
-        'least_squares': 'regression_l2',
-        'binary_crossentropy': 'binary',
-        'categorical_crossentropy': 'multiclass'
+        'squared_error': 'regression_l2',
+        'absolute_error': 'regression_l1',
+        'log_loss': 'binary' if n_classes == 2 else 'multiclass',
     }

     lightgbm_params = {
@@ -63,20 +59,23 @@
         'verbosity': 10 if sklearn_params['verbose'] else -10,
         'boost_from_average': True,
         'enable_bundle': False,  # also makes feature order consistent
-        'min_data_in_bin': 1,
         'subsample_for_bin': _BinMapper().subsample,
     }

-    if sklearn_params['loss'] == 'categorical_crossentropy':
+    if sklearn_params['loss'] == 'log_loss' and n_classes > 2:
         # LightGBM multiplies hessians by 2 in multiclass loss.
         lightgbm_params['min_sum_hessian_in_leaf'] *= 2
-        lightgbm_params['learning_rate'] *= 2
+        # LightGBM 3.0 introduced a different scaling of the hessian for the multiclass case.
+        # It is equivalent of scaling the learning rate.
+        # See https://github.com/microsoft/LightGBM/pull/3256.
+        if n_classes is not None:
+            lightgbm_params['learning_rate'] *= n_classes / (n_classes - 1)

     # XGB
     xgboost_loss_mapping = {
-        'least_squares': 'reg:linear',
-        'binary_crossentropy': 'reg:logistic',
-        'categorical_crossentropy': 'multi:softmax'
+        'squared_error': 'reg:linear',
+        'absolute_error': 'LEAST_ABSOLUTE_DEV_NOT_SUPPORTED',
+        'log_loss': 'reg:logistic' if n_classes == 2 else 'multi:softmax',
     }

     xgboost_params = {
@@ -97,9 +96,10 @@

     # Catboost
     catboost_loss_mapping = {
-        'least_squares': 'RMSE',
-        'binary_crossentropy': 'Logloss',
-        'categorical_crossentropy': 'MultiClass'
+        'squared_error': 'RMSE',
+        # catboost does not support MAE when leaf_estimation_method is Newton
+        'absolute_error': 'LEAST_ASBOLUTE_DEV_NOT_SUPPORTED',
+        'log_loss': 'Logloss' if n_classes == 2 else 'MultiClass',
     }

     catboost_params = {
@@ -139,13 +139,14 @@
             return CatBoostRegressor(**catboost_params)


-def sum_parallel(G_H_DTYPE_C [:] array):
+def sum_parallel(G_H_DTYPE_C [:] array, int n_threads):

     cdef:
         Y_DTYPE_C out = 0.
         int i = 0

-    for i in prange(array.shape[0], schedule='static', nogil=True):
+    for i in prange(array.shape[0], schedule='static', nogil=True,
+                    num_threads=n_threads):
         out += array[i]

     return out
('sklearn/experimental', 'enable_hist_gradient_boosting.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,32 +1,21 @@
-"""Enables histogram-based gradient boosting estimators.
+"""This is now a no-op and can be safely removed from your code.

-The API and results of these estimators might change without any deprecation
-cycle.
-
-Importing this file dynamically sets the
-:class:`sklearn.ensemble.HistGradientBoostingClassifier` and
-:class:`sklearn.ensemble.HistGradientBoostingRegressor` as attributes of the
-ensemble module::
-
-    >>> # explicitly require this experimental feature
-    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa
-    >>> # now you can import normally from ensemble
-    >>> from sklearn.ensemble import HistGradientBoostingClassifier
-    >>> from sklearn.ensemble import HistGradientBoostingRegressor
+It used to enable the use of
+:class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
+:class:`~sklearn.ensemble.HistGradientBoostingRegressor` when they were still
+:term:`experimental`, but these estimators are now stable and can be imported
+normally from `sklearn.ensemble`.
+"""
+# Don't remove this file, we don't want to break users code just because the
+# feature isn't experimental anymore.


-The ``# noqa`` comment comment can be removed: it just tells linters like
-flake8 to ignore the import, which appears as unused.
-"""
+import warnings

-from ..ensemble._hist_gradient_boosting.gradient_boosting import (
-    HistGradientBoostingClassifier,
-    HistGradientBoostingRegressor
+
+warnings.warn(
+    "Since version 1.0, "
+    "it is not needed to import enable_hist_gradient_boosting anymore. "
+    "HistGradientBoostingClassifier and HistGradientBoostingRegressor are now "
+    "stable and can be normally imported from sklearn.ensemble."
 )
-
-from .. import ensemble
-
-ensemble.HistGradientBoostingClassifier = HistGradientBoostingClassifier
-ensemble.HistGradientBoostingRegressor = HistGradientBoostingRegressor
-ensemble.__all__ += ['HistGradientBoostingClassifier',
-                     'HistGradientBoostingRegressor']
('sklearn/experimental', 'enable_iterative_imputer.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,9 +1,9 @@
 """Enables IterativeImputer

-The API and results of this estimators might change without any deprecation
+The API and results of this estimator might change without any deprecation
 cycle.

-Importing this file dynamically sets :class:`sklearn.impute.IterativeImputer`
+Importing this file dynamically sets :class:`~sklearn.impute.IterativeImputer`
 as an attribute of the impute module::

     >>> # explicitly require this experimental feature
@@ -15,5 +15,6 @@
 from ..impute._iterative import IterativeImputer
 from .. import impute

-impute.IterativeImputer = IterativeImputer
-impute.__all__ += ['IterativeImputer']
+# use settattr to avoid mypy errors when monkeypatching
+setattr(impute, "IterativeImputer", IterativeImputer)
+impute.__all__ += ["IterativeImputer"]
('sklearn/cluster', '_feature_agglomeration.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,7 +8,6 @@
 import numpy as np

 from ..base import TransformerMixin
-from ..utils import check_array
 from ..utils.validation import check_is_fitted
 from scipy.sparse import issparse

@@ -18,63 +17,59 @@

 class AgglomerationTransform(TransformerMixin):
     """
-    A class for feature agglomeration via the transform interface
+    A class for feature agglomeration via the transform interface.
     """
-
-    pooling_func = np.mean

     def transform(self, X):
         """
-        Transform a new matrix using the built clustering
+        Transform a new matrix using the built clustering.

         Parameters
         ----------
-        X : array-like, shape = [n_samples, n_features] or [n_features]
+        X : array-like of shape (n_samples, n_features) or \
+                (n_samples, n_samples)
             A M by N array of M observations in N dimensions or a length
             M array of M one-dimensional observations.

         Returns
         -------
-        Y : array, shape = [n_samples, n_clusters] or [n_clusters]
+        Y : ndarray of shape (n_samples, n_clusters) or (n_clusters,)
             The pooled values for each feature cluster.
         """
-        check_is_fitted(self, "labels_")
+        check_is_fitted(self)

-        pooling_func = self.pooling_func
-        X = check_array(X)
-        if len(self.labels_) != X.shape[1]:
-            raise ValueError("X has a different number of features than "
-                             "during fitting.")
-        if pooling_func == np.mean and not issparse(X):
+        X = self._validate_data(X, reset=False)
+        if self.pooling_func == np.mean and not issparse(X):
             size = np.bincount(self.labels_)
             n_samples = X.shape[0]
             # a fast way to compute the mean of grouped features
-            nX = np.array([np.bincount(self.labels_, X[i, :]) / size
-                          for i in range(n_samples)])
+            nX = np.array(
+                [np.bincount(self.labels_, X[i, :]) / size for i in range(n_samples)]
+            )
         else:
-            nX = [pooling_func(X[:, self.labels_ == l], axis=1)
-                  for l in np.unique(self.labels_)]
+            nX = [
+                self.pooling_func(X[:, self.labels_ == l], axis=1)
+                for l in np.unique(self.labels_)
+            ]
             nX = np.array(nX).T
         return nX

     def inverse_transform(self, Xred):
         """
-        Inverse the transformation.
-        Return a vector of size nb_features with the values of Xred assigned
-        to each group of features
+        Inverse the transformation and return a vector of size `n_features`.

         Parameters
         ----------
-        Xred : array-like, shape=[n_samples, n_clusters] or [n_clusters,]
-            The values to be assigned to each cluster of samples
+        Xred : array-like of shape (n_samples, n_clusters) or (n_clusters,)
+            The values to be assigned to each cluster of samples.

         Returns
         -------
-        X : array, shape=[n_samples, n_features] or [n_features]
-            A vector of size n_samples with the values of Xred assigned to
+        X : ndarray of shape (n_samples, n_features) or (n_features,)
+            A vector of size `n_samples` with the values of `Xred` assigned to
             each of the cluster of samples.
         """
-        check_is_fitted(self, "labels_")
+        check_is_fitted(self)

         unil, inverse = np.unique(self.labels_, return_inverse=True)
         return Xred[..., inverse]
('sklearn/cluster', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,40 +3,52 @@
 algorithms.
 """

-from .spectral import spectral_clustering, SpectralClustering
-from .mean_shift_ import (mean_shift, MeanShift,
-                          estimate_bandwidth, get_bin_seeds)
-from .affinity_propagation_ import affinity_propagation, AffinityPropagation
-from .hierarchical import (ward_tree, AgglomerativeClustering, linkage_tree,
-                           FeatureAgglomeration)
-from .k_means_ import k_means, KMeans, MiniBatchKMeans
-from .dbscan_ import dbscan, DBSCAN
-from .optics_ import (OPTICS, cluster_optics_dbscan, compute_optics_graph,
-                      cluster_optics_xi)
-from .bicluster import SpectralBiclustering, SpectralCoclustering
-from .birch import Birch
+from ._spectral import spectral_clustering, SpectralClustering
+from ._mean_shift import mean_shift, MeanShift, estimate_bandwidth, get_bin_seeds
+from ._affinity_propagation import affinity_propagation, AffinityPropagation
+from ._agglomerative import (
+    ward_tree,
+    AgglomerativeClustering,
+    linkage_tree,
+    FeatureAgglomeration,
+)
+from ._kmeans import k_means, KMeans, MiniBatchKMeans, kmeans_plusplus
+from ._bisect_k_means import BisectingKMeans
+from ._dbscan import dbscan, DBSCAN
+from ._optics import (
+    OPTICS,
+    cluster_optics_dbscan,
+    compute_optics_graph,
+    cluster_optics_xi,
+)
+from ._bicluster import SpectralBiclustering, SpectralCoclustering
+from ._birch import Birch

-__all__ = ['AffinityPropagation',
-           'AgglomerativeClustering',
-           'Birch',
-           'DBSCAN',
-           'OPTICS',
-           'cluster_optics_dbscan',
-           'cluster_optics_xi',
-           'compute_optics_graph',
-           'KMeans',
-           'FeatureAgglomeration',
-           'MeanShift',
-           'MiniBatchKMeans',
-           'SpectralClustering',
-           'affinity_propagation',
-           'dbscan',
-           'estimate_bandwidth',
-           'get_bin_seeds',
-           'k_means',
-           'linkage_tree',
-           'mean_shift',
-           'spectral_clustering',
-           'ward_tree',
-           'SpectralBiclustering',
-           'SpectralCoclustering']
+__all__ = [
+    "AffinityPropagation",
+    "AgglomerativeClustering",
+    "Birch",
+    "DBSCAN",
+    "OPTICS",
+    "cluster_optics_dbscan",
+    "cluster_optics_xi",
+    "compute_optics_graph",
+    "KMeans",
+    "BisectingKMeans",
+    "FeatureAgglomeration",
+    "MeanShift",
+    "MiniBatchKMeans",
+    "SpectralClustering",
+    "affinity_propagation",
+    "dbscan",
+    "estimate_bandwidth",
+    "get_bin_seeds",
+    "k_means",
+    "kmeans_plusplus",
+    "linkage_tree",
+    "mean_shift",
+    "spectral_clustering",
+    "ward_tree",
+    "SpectralBiclustering",
+    "SpectralCoclustering",
+]
('sklearn/cluster', '_dbscan_inner.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,19 +1,13 @@
 # Fast inner loop for DBSCAN.
 # Author: Lars Buitinck
 # License: 3-clause BSD
-#
-# cython: boundscheck=False, wraparound=False

 cimport cython
 from libcpp.vector cimport vector
 cimport numpy as np
 import numpy as np

-
-# Work around Cython bug: C++ exceptions are not caught unless thrown within
-# a cdef function with an "except +" declaration.
-cdef inline void push(vector[np.npy_intp] &stack, np.npy_intp i) except +:
-    stack.push_back(i)
+np.import_array()


 def dbscan_inner(np.ndarray[np.uint8_t, ndim=1, mode='c'] is_core,
@@ -39,7 +33,7 @@
                     for i in range(neighb.shape[0]):
                         v = neighb[i]
                         if labels[v] == -1:
-                            push(stack, v)
+                            stack.push_back(v)

             if stack.size() == 0:
                 break
('sklearn/cluster', '_k_means_elkan.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,38 +1,44 @@
-# cython: cdivision=True
-# cython: boundscheck=False
-# cython: wraparound=False
-#
 # Author: Andreas Mueller
 #
 # Licence: BSD 3 clause

+# TODO: We still need to use ndarrays instead of typed memoryviews when using
+# fused types and when the array may be read-only (for instance when it's
+# provided by the user). This is fixed in cython > 0.3.
+
 import numpy as np
 cimport numpy as np
+IF SKLEARN_OPENMP_PARALLELISM_ENABLED:
+    cimport openmp
 cimport cython
 from cython cimport floating
-
+from cython.parallel import prange, parallel
 from libc.math cimport sqrt
-
-from ..metrics import euclidean_distances
-from ._k_means import _centers_dense
-
-
-cdef floating euclidean_dist(floating* a, floating* b, int n_features) nogil:
-    cdef floating result, tmp
-    result = 0
-    cdef int i
-    for i in range(n_features):
-        tmp = (a[i] - b[i])
-        result += tmp * tmp
-    return sqrt(result)
-
-
-cdef update_labels_distances_inplace(
-        floating* X, floating* centers, floating[:, :] center_half_distances,
-        int[:] labels, floating[:, :] lower_bounds, floating[:] upper_bounds,
-        int n_samples, int n_features, int n_clusters):
-    """
-    Calculate upper and lower bounds for each sample.
+from libc.stdlib cimport calloc, free
+from libc.string cimport memset, memcpy
+
+from ..utils.extmath import row_norms
+from ._k_means_common import CHUNK_SIZE
+from ._k_means_common cimport _relocate_empty_clusters_dense
+from ._k_means_common cimport _relocate_empty_clusters_sparse
+from ._k_means_common cimport _euclidean_dense_dense
+from ._k_means_common cimport _euclidean_sparse_dense
+from ._k_means_common cimport _average_centers
+from ._k_means_common cimport _center_shift
+
+
+np.import_array()
+
+
+def init_bounds_dense(
+        floating[:, ::1] X,                      # IN READ-ONLY
+        floating[:, ::1] centers,                # IN
+        floating[:, ::1] center_half_distances,  # IN
+        int[::1] labels,                         # OUT
+        floating[::1] upper_bounds,              # OUT
+        floating[:, ::1] lower_bounds,           # OUT
+        int n_threads):
+    """Initialize upper and lower bounds for each sample for dense input data.

     Given X, centers and the pairwise distances divided by 2.0 between the
     centers this calculates the upper bounds and lower bounds for each sample.
@@ -49,212 +55,628 @@

     Parameters
     ----------
-    X : nd-array, shape (n_samples, n_features)
+    X : ndarray of shape (n_samples, n_features), dtype=floating
         The input data.

-    centers : nd-array, shape (n_clusters, n_features)
+    centers : ndarray of shape (n_clusters, n_features), dtype=floating
         The cluster centers.

-    center_half_distances : nd-array, shape (n_clusters, n_clusters)
+    center_half_distances : ndarray of shape (n_clusters, n_clusters), \
+            dtype=floating
         The half of the distance between any 2 clusters centers.

-    labels : nd-array, shape(n_samples)
+    labels : ndarray of shape(n_samples), dtype=int
         The label for each sample. This array is modified in place.

-    lower_bounds : nd-array, shape(n_samples, n_clusters)
-        The lower bound on the distance between a sample and each cluster
-        center. It is modified in place.
-
-    upper_bounds : nd-array, shape(n_samples,)
-        The distance of each sample from its closest cluster center.  This is
-        modified in place by the function.
-
-    n_samples : int
-        The number of samples.
-
-    n_features : int
-        The number of features.
-
-    n_clusters : int
-        The number of clusters.
+    upper_bounds : ndarray of shape(n_samples,), dtype=floating
+        The upper bound on the distance between each sample and its closest
+        cluster center. This array is modified in place.
+
+    lower_bounds : ndarray, of shape(n_samples, n_clusters), dtype=floating
+        The lower bound on the distance between each sample and each cluster
+        center. This array is modified in place.
+
+    n_threads : int
+        The number of threads to be used by openmp.
     """
-    # assigns closest center to X
-    # uses triangle inequality
-    cdef floating* x
-    cdef floating* c
-    cdef floating d_c, dist
-    cdef int c_x, j, sample
-    for sample in range(n_samples):
-        # assign first cluster center
-        c_x = 0
-        x = X + sample * n_features
-        d_c = euclidean_dist(x, centers, n_features)
-        lower_bounds[sample, 0] = d_c
+    cdef:
+        int n_samples = X.shape[0]
+        int n_clusters = centers.shape[0]
+        int n_features = X.shape[1]
+
+        floating min_dist, dist
+        int best_cluster, i, j
+
+    for i in prange(
+        n_samples, num_threads=n_threads, schedule='static', nogil=True
+    ):
+        best_cluster = 0
+        min_dist = _euclidean_dense_dense(&X[i, 0], &centers[0, 0],
+                                          n_features, False)
+        lower_bounds[i, 0] = min_dist
         for j in range(1, n_clusters):
-            if d_c > center_half_distances[c_x, j]:
-                c = centers + j * n_features
-                dist = euclidean_dist(x, c, n_features)
-                lower_bounds[sample, j] = dist
-                if dist < d_c:
-                    d_c = dist
-                    c_x = j
-        labels[sample] = c_x
-        upper_bounds[sample] = d_c
-
-
-def k_means_elkan(np.ndarray[floating, ndim=2, mode='c'] X_,
-                  np.ndarray[floating, ndim=1, mode='c'] sample_weight,
-                  int n_clusters,
-                  np.ndarray[floating, ndim=2, mode='c'] init,
-                  float tol=1e-4, int max_iter=30, verbose=False):
-    """Run Elkan's k-means.
+            if min_dist > center_half_distances[best_cluster, j]:
+                dist = _euclidean_dense_dense(&X[i, 0], &centers[j, 0],
+                                              n_features, False)
+                lower_bounds[i, j] = dist
+                if dist < min_dist:
+                    min_dist = dist
+                    best_cluster = j
+        labels[i] = best_cluster
+        upper_bounds[i] = min_dist
+
+
+def init_bounds_sparse(
+        X,                                       # IN
+        floating[:, ::1] centers,                # IN
+        floating[:, ::1] center_half_distances,  # IN
+        int[::1] labels,                         # OUT
+        floating[::1] upper_bounds,              # OUT
+        floating[:, ::1] lower_bounds,           # OUT
+        int n_threads):
+    """Initialize upper and lower bounds for each sample for sparse input data.
+
+    Given X, centers and the pairwise distances divided by 2.0 between the
+    centers this calculates the upper bounds and lower bounds for each sample.
+    The upper bound for each sample is set to the distance between the sample
+    and the closest center.
+
+    The lower bound for each sample is a one-dimensional array of n_clusters.
+    For each sample i assume that the previously assigned cluster is c1 and the
+    previous closest distance is dist, for a new cluster c2, the
+    lower_bound[i][c2] is set to distance between the sample and this new
+    cluster, if and only if dist > center_half_distances[c1][c2]. This prevents
+    computation of unnecessary distances for each sample to the clusters that
+    it is unlikely to be assigned to.

     Parameters
     ----------
-    X_ : nd-array, shape (n_samples, n_features)
-
-    sample_weight : nd-array, shape (n_samples,)
+    X : sparse matrix of shape (n_samples, n_features), dtype=floating
+        The input data. Must be in CSR format.
+
+    centers : ndarray of shape (n_clusters, n_features), dtype=floating
+        The cluster centers.
+
+    center_half_distances : ndarray of shape (n_clusters, n_clusters), \
+            dtype=floating
+        The half of the distance between any 2 clusters centers.
+
+    labels : ndarray of shape(n_samples), dtype=int
+        The label for each sample. This array is modified in place.
+
+    upper_bounds : ndarray of shape(n_samples,), dtype=floating
+        The upper bound on the distance between each sample and its closest
+        cluster center. This array is modified in place.
+
+    lower_bounds : ndarray of shape(n_samples, n_clusters), dtype=floating
+        The lower bound on the distance between each sample and each cluster
+        center. This array is modified in place.
+
+    n_threads : int
+        The number of threads to be used by openmp.
+    """
+    cdef:
+        int n_samples = X.shape[0]
+        int n_clusters = centers.shape[0]
+        int n_features = X.shape[1]
+
+        floating[::1] X_data = X.data
+        int[::1] X_indices = X.indices
+        int[::1] X_indptr = X.indptr
+
+        floating min_dist, dist
+        int best_cluster, i, j
+
+        floating[::1] centers_squared_norms = row_norms(centers, squared=True)
+
+    for i in prange(
+        n_samples, num_threads=n_threads, schedule='static', nogil=True
+    ):
+        best_cluster = 0
+        min_dist = _euclidean_sparse_dense(
+            X_data[X_indptr[i]: X_indptr[i + 1]],
+            X_indices[X_indptr[i]: X_indptr[i + 1]],
+            centers[0], centers_squared_norms[0], False)
+
+        lower_bounds[i, 0] = min_dist
+        for j in range(1, n_clusters):
+            if min_dist > center_half_distances[best_cluster, j]:
+                dist = _euclidean_sparse_dense(
+                    X_data[X_indptr[i]: X_indptr[i + 1]],
+                    X_indices[X_indptr[i]: X_indptr[i + 1]],
+                    centers[j], centers_squared_norms[j], False)
+                lower_bounds[i, j] = dist
+                if dist < min_dist:
+                    min_dist = dist
+                    best_cluster = j
+        labels[i] = best_cluster
+        upper_bounds[i] = min_dist
+
+
+def elkan_iter_chunked_dense(
+        floating[:, ::1] X,                      # IN READ-ONLY
+        floating[::1] sample_weight,             # IN READ-ONLY
+        floating[:, ::1] centers_old,            # IN
+        floating[:, ::1] centers_new,            # OUT
+        floating[::1] weight_in_clusters,        # OUT
+        floating[:, ::1] center_half_distances,  # IN
+        floating[::1] distance_next_center,      # IN
+        floating[::1] upper_bounds,              # INOUT
+        floating[:, ::1] lower_bounds,           # INOUT
+        int[::1] labels,                         # INOUT
+        floating[::1] center_shift,              # OUT
+        int n_threads,
+        bint update_centers=True):
+    """Single iteration of K-means Elkan algorithm with dense input.
+
+    Update labels and centers (inplace), for one iteration, distributed
+    over data chunks.
+
+    Parameters
+    ----------
+    X : ndarray of shape (n_samples, n_features), dtype=floating
+        The observations to cluster.
+
+    sample_weight : ndarray of shape (n_samples,), dtype=floating
         The weights for each observation in X.

-    n_clusters : int
-        Number of clusters to find.
-
-    init : nd-array, shape (n_clusters, n_features)
-        Initial position of centers.
-
-    tol : float, default=1e-4
-        The relative increment in cluster means before declaring convergence.
-
-    max_iter : int, default=30
-    Maximum number of iterations of the k-means algorithm.
-
-    verbose : bool, default=False
-        Whether to be verbose.
-
+    centers_old : ndarray of shape (n_clusters, n_features), dtype=floating
+        Centers before previous iteration, placeholder for the centers after
+        previous iteration.
+
+    centers_new : ndarray of shape (n_clusters, n_features), dtype=floating
+        Centers after previous iteration, placeholder for the new centers
+        computed during this iteration.
+
+    weight_in_clusters : ndarray of shape (n_clusters,), dtype=floating
+        Placeholder for the sums of the weights of every observation assigned
+        to each center.
+
+    center_half_distances : ndarray of shape (n_clusters, n_clusters), \
+            dtype=floating
+        Half pairwise distances between centers.
+
+    distance_next_center : ndarray of shape (n_clusters,), dtype=floating
+        Distance between each center its closest center.
+
+    upper_bounds : ndarray of shape (n_samples,), dtype=floating
+        Upper bound for the distance between each sample and its center,
+        updated inplace.
+
+    lower_bounds : ndarray of shape (n_samples, n_clusters), dtype=floating
+        Lower bound for the distance between each sample and each center,
+        updated inplace.
+
+    labels : ndarray of shape (n_samples,), dtype=int
+        labels assignment.
+
+    center_shift : ndarray of shape (n_clusters,), dtype=floating
+        Distance between old and new centers.
+
+    n_threads : int
+        The number of threads to be used by openmp.
+
+    update_centers : bool
+        - If True, the labels and the new centers will be computed, i.e. runs
+          the E-step and the M-step of the algorithm.
+        - If False, only the labels will be computed, i.e runs the E-step of
+          the algorithm. This is useful especially when calling predict on a
+          fitted model.
     """
-    if floating is float:
-        dtype = np.float32
-    else:
-        dtype = np.float64
-
-    # initialize
-    cdef np.ndarray[floating, ndim=2, mode='c'] centers_ = init
-    cdef floating* centers_p = <floating*>centers_.data
-    cdef floating* X_p = <floating*>X_.data
-    cdef floating* x_p
-    cdef Py_ssize_t n_samples = X_.shape[0]
-    cdef Py_ssize_t n_features = X_.shape[1]
-    cdef int point_index, center_index, label
-    cdef floating upper_bound, distance
-    cdef floating[:, :] center_half_distances = euclidean_distances(centers_) / 2.
-    cdef floating[:, :] lower_bounds = np.zeros((n_samples, n_clusters), dtype=dtype)
-    cdef floating[:] distance_next_center
-    labels_ = np.empty(n_samples, dtype=np.int32)
-    cdef int[:] labels = labels_
-    upper_bounds_ = np.empty(n_samples, dtype=dtype)
-    cdef floating[:] upper_bounds = upper_bounds_
-
-    # Get the initial set of upper bounds and lower bounds for each sample.
-    update_labels_distances_inplace(X_p, centers_p, center_half_distances,
-                                    labels, lower_bounds, upper_bounds,
-                                    n_samples, n_features, n_clusters)
-    cdef np.uint8_t[:] bounds_tight = np.ones(n_samples, dtype=np.uint8)
-    cdef np.uint8_t[:] points_to_update = np.zeros(n_samples, dtype=np.uint8)
-    cdef np.ndarray[floating, ndim=2, mode='c'] new_centers
-
-    if max_iter <= 0:
-        raise ValueError('Number of iterations should be a positive number'
-        ', got %d instead' % max_iter)
-
-    col_indices = np.arange(center_half_distances.shape[0], dtype=np.int)
-    for iteration in range(max_iter):
-        if verbose:
-            print("start iteration")
-
-        cd =  np.asarray(center_half_distances)
-        distance_next_center = np.partition(cd, kth=1, axis=0)[1]
-
-        if verbose:
-            print("done sorting")
-
-        for point_index in range(n_samples):
-            upper_bound = upper_bounds[point_index]
-            label = labels[point_index]
-
-            # This means that the next likely center is far away from the
-            # currently assigned center and the sample is unlikely to be
-            # reassigned.
-            if distance_next_center[label] >= upper_bound:
-                continue
-            x_p = X_p + point_index * n_features
-
-            # TODO: get pointer to lower_bounds[point_index, center_index]
-            for center_index in range(n_clusters):
+    cdef:
+        int n_samples = X.shape[0]
+        int n_features = X.shape[1]
+        int n_clusters = centers_new.shape[0]
+
+        # hard-coded number of samples per chunk. Splitting in chunks is
+        # necessary to get parallelism. Chunk size chosen to be same as lloyd's
+        int n_samples_chunk = CHUNK_SIZE if n_samples > CHUNK_SIZE else n_samples
+        int n_chunks = n_samples // n_samples_chunk
+        int n_samples_rem = n_samples % n_samples_chunk
+        int chunk_idx, n_samples_chunk_eff
+        int start, end
+
+        int i, j, k
+
+        floating *centers_new_chunk
+        floating *weight_in_clusters_chunk
+
+        IF SKLEARN_OPENMP_PARALLELISM_ENABLED:
+            openmp.omp_lock_t lock
+
+    # count remainder chunk in total number of chunks
+    n_chunks += n_samples != n_chunks * n_samples_chunk
+
+    # number of threads should not be bigger than number of chunks
+    n_threads = min(n_threads, n_chunks)
+
+    if update_centers:
+        memset(&centers_new[0, 0], 0, n_clusters * n_features * sizeof(floating))
+        memset(&weight_in_clusters[0], 0, n_clusters * sizeof(floating))
+        IF SKLEARN_OPENMP_PARALLELISM_ENABLED:
+            openmp.omp_init_lock(&lock)
+
+    with nogil, parallel(num_threads=n_threads):
+        # thread local buffers
+        centers_new_chunk = <floating*> calloc(n_clusters * n_features, sizeof(floating))
+        weight_in_clusters_chunk = <floating*> calloc(n_clusters, sizeof(floating))
+
+        for chunk_idx in prange(n_chunks, schedule='static'):
+            start = chunk_idx * n_samples_chunk
+            if chunk_idx == n_chunks - 1 and n_samples_rem > 0:
+                end = start + n_samples_rem
+            else:
+                end = start + n_samples_chunk
+
+            _update_chunk_dense(
+                X[start: end],
+                sample_weight[start: end],
+                centers_old,
+                center_half_distances,
+                distance_next_center,
+                labels[start: end],
+                upper_bounds[start: end],
+                lower_bounds[start: end],
+                centers_new_chunk,
+                weight_in_clusters_chunk,
+                update_centers)
+
+        # reduction from local buffers.
+        if update_centers:
+            IF SKLEARN_OPENMP_PARALLELISM_ENABLED:
+                # The lock is necessary to avoid race conditions when aggregating
+                # info from different thread-local buffers.
+                openmp.omp_set_lock(&lock)
+            for j in range(n_clusters):
+                weight_in_clusters[j] += weight_in_clusters_chunk[j]
+                for k in range(n_features):
+                    centers_new[j, k] += centers_new_chunk[j * n_features + k]
+            IF SKLEARN_OPENMP_PARALLELISM_ENABLED:
+                openmp.omp_unset_lock(&lock)
+
+        free(centers_new_chunk)
+        free(weight_in_clusters_chunk)
+
+    if update_centers:
+        IF SKLEARN_OPENMP_PARALLELISM_ENABLED:
+            openmp.omp_destroy_lock(&lock)
+        _relocate_empty_clusters_dense(X, sample_weight, centers_old,
+                                       centers_new, weight_in_clusters, labels)
+
+        _average_centers(centers_new, weight_in_clusters)
+        _center_shift(centers_old, centers_new, center_shift)
+
+        # update lower and upper bounds
+        for i in range(n_samples):
+            upper_bounds[i] += center_shift[labels[i]]
+
+            for j in range(n_clusters):
+                lower_bounds[i, j] -= center_shift[j]
+                if lower_bounds[i, j] < 0:
+                    lower_bounds[i, j] = 0
+
+
+cdef void _update_chunk_dense(
+        floating[:, ::1] X,                      # IN READ-ONLY
+        floating[::1] sample_weight,             # IN READ-ONLY
+        floating[:, ::1] centers_old,            # IN
+        floating[:, ::1] center_half_distances,  # IN
+        floating[::1] distance_next_center,      # IN
+        int[::1] labels,                         # INOUT
+        floating[::1] upper_bounds,              # INOUT
+        floating[:, ::1] lower_bounds,           # INOUT
+        floating *centers_new,                   # OUT
+        floating *weight_in_clusters,            # OUT
+        bint update_centers) nogil:
+    """K-means combined EM step for one dense data chunk.
+
+    Compute the partial contribution of a single data chunk to the labels and
+    centers.
+    """
+    cdef:
+        int n_samples = labels.shape[0]
+        int n_clusters = centers_old.shape[0]
+        int n_features = centers_old.shape[1]
+
+        floating upper_bound, distance
+        int i, j, k, label
+
+    for i in range(n_samples):
+        upper_bound = upper_bounds[i]
+        bounds_tight = 0
+        label = labels[i]
+
+        # Next center is not far away from the currently assigned center.
+        # Sample might need to be assigned to another center.
+        if not distance_next_center[label] >= upper_bound:
+
+            for j in range(n_clusters):

                 # If this holds, then center_index is a good candidate for the
                 # sample to be relabelled, and we need to confirm this by
                 # recomputing the upper and lower bounds.
-                if (center_index != label
-                        and (upper_bound > lower_bounds[point_index, center_index])
-                        and (upper_bound > center_half_distances[center_index, label])):
-
-                    # Recompute the upper bound by calculating the actual distance
-                    # between the sample and label.
-                    if not bounds_tight[point_index]:
-                        upper_bound = euclidean_dist(x_p, centers_p + label * n_features, n_features)
-                        lower_bounds[point_index, label] = upper_bound
-                        bounds_tight[point_index] = 1
-
-                    # If the condition still holds, then compute the actual distance between
-                    # the sample and center_index. If this is still lesser than the previous
-                    # distance, reassign labels.
-                    if (upper_bound > lower_bounds[point_index, center_index]
-                            or (upper_bound > center_half_distances[label, center_index])):
-                        distance = euclidean_dist(x_p, centers_p + center_index * n_features, n_features)
-                        lower_bounds[point_index, center_index] = distance
+                if (j != label
+                    and (upper_bound > lower_bounds[i, j])
+                    and (upper_bound > center_half_distances[label, j])):
+
+                    # Recompute upper bound by calculating the actual distance
+                    # between the sample and its current assigned center.
+                    if not bounds_tight:
+                        upper_bound = _euclidean_dense_dense(
+                            &X[i, 0], &centers_old[label, 0], n_features, False)
+                        lower_bounds[i, label] = upper_bound
+                        bounds_tight = 1
+
+                    # If the condition still holds, then compute the actual
+                    # distance between the sample and center. If this is less
+                    # than the previous distance, reassign label.
+                    if (upper_bound > lower_bounds[i, j]
+                        or (upper_bound > center_half_distances[label, j])):
+
+                        distance = _euclidean_dense_dense(
+                            &X[i, 0], &centers_old[j, 0], n_features, False)
+                        lower_bounds[i, j] = distance
                         if distance < upper_bound:
-                            label = center_index
+                            label = j
                             upper_bound = distance

-            labels[point_index] = label
-            upper_bounds[point_index] = upper_bound
-
-        if verbose:
-            print("end inner loop")
-
-        # compute new centers
-        new_centers = _centers_dense(X_, sample_weight, labels_,
-                                     n_clusters, upper_bounds_)
-        bounds_tight[:] = 0
-
-        # compute distance each center moved
-        center_shift = np.sqrt(np.sum((centers_ - new_centers) ** 2, axis=1))
-
-        # update bounds accordingly
-        lower_bounds = np.maximum(lower_bounds - center_shift, 0)
-        upper_bounds = upper_bounds + center_shift[labels_]
-
-        # reassign centers
-        centers_ = new_centers
-        centers_p = <floating*>new_centers.data
-
-        # update between-center distances
-        center_half_distances = euclidean_distances(centers_) / 2.
-        if verbose:
-            print('Iteration %i, inertia %s'
-                    % (iteration, np.sum((X_ - centers_[labels]) ** 2 *
-                                         sample_weight[:,np.newaxis])))
-        center_shift_total = np.sum(center_shift)
-        if center_shift_total ** 2 < tol:
-            if verbose:
-                print("center shift %e within tolerance %e"
-                      % (center_shift_total, tol))
-            break
-
-    # We need this to make sure that the labels give the same output as
-    # predict(X)
-    if center_shift_total > 0:
-        update_labels_distances_inplace(X_p, centers_p, center_half_distances,
-                                        labels, lower_bounds, upper_bounds,
-                                        n_samples, n_features, n_clusters)
-    return centers_, labels_, iteration + 1
+            labels[i] = label
+            upper_bounds[i] = upper_bound
+
+        if update_centers:
+            weight_in_clusters[label] += sample_weight[i]
+            for k in range(n_features):
+                centers_new[label * n_features + k] += X[i, k] * sample_weight[i]
+
+
+def elkan_iter_chunked_sparse(
+        X,                                       # IN
+        floating[::1] sample_weight,             # IN
+        floating[:, ::1] centers_old,            # IN
+        floating[:, ::1] centers_new,            # OUT
+        floating[::1] weight_in_clusters,        # OUT
+        floating[:, ::1] center_half_distances,  # IN
+        floating[::1] distance_next_center,      # IN
+        floating[::1] upper_bounds,              # INOUT
+        floating[:, ::1] lower_bounds,           # INOUT
+        int[::1] labels,                         # INOUT
+        floating[::1] center_shift,              # OUT
+        int n_threads,
+        bint update_centers=True):
+    """Single iteration of K-means Elkan algorithm with sparse input.
+
+    Update labels and centers (inplace), for one iteration, distributed
+    over data chunks.
+
+    Parameters
+    ----------
+    X : sparse matrix of shape (n_samples, n_features)
+        The observations to cluster. Must be in CSR format.
+
+    sample_weight : ndarray of shape (n_samples,), dtype=floating
+        The weights for each observation in X.
+
+    centers_old : ndarray of shape (n_clusters, n_features), dtype=floating
+        Centers before previous iteration, placeholder for the centers after
+        previous iteration.
+
+    centers_new : ndarray of shape (n_clusters, n_features), dtype=floating
+        Centers after previous iteration, placeholder for the new centers
+        computed during this iteration.
+
+    weight_in_clusters : ndarray of shape (n_clusters,), dtype=floating
+        Placeholder for the sums of the weights of every observation assigned
+        to each center.
+
+    center_half_distances : ndarray of shape (n_clusters, n_clusters), \
+            dtype=floating
+        Half pairwise distances between centers.
+
+    distance_next_center : ndarray of shape (n_clusters,), dtype=floating
+        Distance between each center its closest center.
+
+    upper_bounds : ndarray of shape (n_samples,), dtype=floating
+        Upper bound for the distance between each sample and its center,
+        updated inplace.
+
+    lower_bounds : ndarray of shape (n_samples, n_clusters), dtype=floating
+        Lower bound for the distance between each sample and each center,
+        updated inplace.
+
+    labels : ndarray of shape (n_samples,), dtype=int
+        labels assignment.
+
+    center_shift : ndarray of shape (n_clusters,), dtype=floating
+        Distance between old and new centers.
+
+    n_threads : int
+        The number of threads to be used by openmp.
+
+    update_centers : bool
+        - If True, the labels and the new centers will be computed, i.e. runs
+          the E-step and the M-step of the algorithm.
+        - If False, only the labels will be computed, i.e runs the E-step of
+          the algorithm. This is useful especially when calling predict on a
+          fitted model.
+    """
+    cdef:
+        int n_samples = X.shape[0]
+        int n_features = X.shape[1]
+        int n_clusters = centers_new.shape[0]
+
+        floating[::1] X_data = X.data
+        int[::1] X_indices = X.indices
+        int[::1] X_indptr = X.indptr
+
+        # hard-coded number of samples per chunk. Splitting in chunks is
+        # necessary to get parallelism. Chunk size chosen to be same as lloyd's
+        int n_samples_chunk = CHUNK_SIZE if n_samples > CHUNK_SIZE else n_samples
+        int n_chunks = n_samples // n_samples_chunk
+        int n_samples_rem = n_samples % n_samples_chunk
+        int chunk_idx, n_samples_chunk_eff
+        int start, end
+
+        int i, j, k
+
+        floating[::1] centers_squared_norms = row_norms(centers_old, squared=True)
+
+        floating *centers_new_chunk
+        floating *weight_in_clusters_chunk
+
+        IF SKLEARN_OPENMP_PARALLELISM_ENABLED:
+            openmp.omp_lock_t lock
+
+    # count remainder chunk in total number of chunks
+    n_chunks += n_samples != n_chunks * n_samples_chunk
+
+    # number of threads should not be bigger than number of chunks
+    n_threads = min(n_threads, n_chunks)
+
+    if update_centers:
+        memset(&centers_new[0, 0], 0, n_clusters * n_features * sizeof(floating))
+        memset(&weight_in_clusters[0], 0, n_clusters * sizeof(floating))
+        IF SKLEARN_OPENMP_PARALLELISM_ENABLED:
+            openmp.omp_init_lock(&lock)
+
+    with nogil, parallel(num_threads=n_threads):
+        # thread local buffers
+        centers_new_chunk = <floating*> calloc(n_clusters * n_features, sizeof(floating))
+        weight_in_clusters_chunk = <floating*> calloc(n_clusters, sizeof(floating))
+
+        for chunk_idx in prange(n_chunks, schedule='static'):
+            start = chunk_idx * n_samples_chunk
+            if chunk_idx == n_chunks - 1 and n_samples_rem > 0:
+                end = start + n_samples_rem
+            else:
+                end = start + n_samples_chunk
+
+            _update_chunk_sparse(
+                X_data[X_indptr[start]: X_indptr[end]],
+                X_indices[X_indptr[start]: X_indptr[end]],
+                X_indptr[start: end+1],
+                sample_weight[start: end],
+                centers_old,
+                centers_squared_norms,
+                center_half_distances,
+                distance_next_center,
+                labels[start: end],
+                upper_bounds[start: end],
+                lower_bounds[start: end],
+                centers_new_chunk,
+                weight_in_clusters_chunk,
+                update_centers)
+
+        # reduction from local buffers.
+        if update_centers:
+            IF SKLEARN_OPENMP_PARALLELISM_ENABLED:
+                # The lock is necessary to avoid race conditions when aggregating
+                # info from different thread-local buffers.
+                openmp.omp_set_lock(&lock)
+            for j in range(n_clusters):
+                weight_in_clusters[j] += weight_in_clusters_chunk[j]
+                for k in range(n_features):
+                    centers_new[j, k] += centers_new_chunk[j * n_features + k]
+            IF SKLEARN_OPENMP_PARALLELISM_ENABLED:
+                openmp.omp_unset_lock(&lock)
+
+        free(centers_new_chunk)
+        free(weight_in_clusters_chunk)
+
+    if update_centers:
+        IF SKLEARN_OPENMP_PARALLELISM_ENABLED:
+            openmp.omp_destroy_lock(&lock)
+        _relocate_empty_clusters_sparse(
+            X_data, X_indices, X_indptr, sample_weight,
+            centers_old, centers_new, weight_in_clusters, labels)
+
+        _average_centers(centers_new, weight_in_clusters)
+        _center_shift(centers_old, centers_new, center_shift)
+
+        # update lower and upper bounds
+        for i in range(n_samples):
+            upper_bounds[i] += center_shift[labels[i]]
+
+            for j in range(n_clusters):
+                lower_bounds[i, j] -= center_shift[j]
+                if lower_bounds[i, j] < 0:
+                    lower_bounds[i, j] = 0
+
+
+cdef void _update_chunk_sparse(
+        floating[::1] X_data,                    # IN
+        int[::1] X_indices,                      # IN
+        int[::1] X_indptr,                       # IN
+        floating[::1] sample_weight,             # IN
+        floating[:, ::1] centers_old,            # IN
+        floating[::1] centers_squared_norms,     # IN
+        floating[:, ::1] center_half_distances,  # IN
+        floating[::1] distance_next_center,      # IN
+        int[::1] labels,                         # INOUT
+        floating[::1] upper_bounds,              # INOUT
+        floating[:, ::1] lower_bounds,           # INOUT
+        floating *centers_new,                   # OUT
+        floating *weight_in_clusters,            # OUT
+        bint update_centers) nogil:
+    """K-means combined EM step for one sparse data chunk.
+
+    Compute the partial contribution of a single data chunk to the labels and
+    centers.
+    """
+    cdef:
+        int n_samples = labels.shape[0]
+        int n_clusters = centers_old.shape[0]
+        int n_features = centers_old.shape[1]
+
+        floating upper_bound, distance
+        int i, j, k, label
+        int s = X_indptr[0]
+
+    for i in range(n_samples):
+        upper_bound = upper_bounds[i]
+        bounds_tight = 0
+        label = labels[i]
+
+        # Next center is not far away from the currently assigned center.
+        # Sample might need to be assigned to another center.
+        if not distance_next_center[label] >= upper_bound:
+
+            for j in range(n_clusters):
+
+                # If this holds, then center_index is a good candidate for the
+                # sample to be relabelled, and we need to confirm this by
+                # recomputing the upper and lower bounds.
+                if (j != label
+                    and (upper_bound > lower_bounds[i, j])
+                    and (upper_bound > center_half_distances[label, j])):
+
+                    # Recompute upper bound by calculating the actual distance
+                    # between the sample and its current assigned center.
+                    if not bounds_tight:
+                        upper_bound = _euclidean_sparse_dense(
+                            X_data[X_indptr[i] - s: X_indptr[i + 1] - s],
+                            X_indices[X_indptr[i] - s: X_indptr[i + 1] - s],
+                            centers_old[label], centers_squared_norms[label], False)
+                        lower_bounds[i, label] = upper_bound
+                        bounds_tight = 1
+
+                    # If the condition still holds, then compute the actual
+                    # distance between the sample and center. If this is less
+                    # than the previous distance, reassign label.
+                    if (upper_bound > lower_bounds[i, j]
+                        or (upper_bound > center_half_distances[label, j])):
+                        distance = _euclidean_sparse_dense(
+                            X_data[X_indptr[i] - s: X_indptr[i + 1] - s],
+                            X_indices[X_indptr[i] - s: X_indptr[i + 1] - s],
+                            centers_old[j], centers_squared_norms[j], False)
+                        lower_bounds[i, j] = distance
+                        if distance < upper_bound:
+                            label = j
+                            upper_bound = distance
+
+            labels[i] = label
+            upper_bounds[i] = upper_bound
+
+        if update_centers:
+            weight_in_clusters[label] += sample_weight[i]
+            for k in range(X_indptr[i] - s, X_indptr[i + 1] - s):
+                centers_new[label * n_features + X_indices[k]] += X_data[k] * sample_weight[i]
('sklearn/cluster', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,40 +5,64 @@
 import numpy


-def configuration(parent_package='', top_path=None):
+def configuration(parent_package="", top_path=None):
     from numpy.distutils.misc_util import Configuration

     libraries = []
-    if os.name == 'posix':
-        libraries.append('m')
+    if os.name == "posix":
+        libraries.append("m")

-    config = Configuration('cluster', parent_package, top_path)
-    config.add_extension('_dbscan_inner',
-                         sources=['_dbscan_inner.pyx'],
-                         include_dirs=[numpy.get_include()],
-                         language="c++")
+    config = Configuration("cluster", parent_package, top_path)

-    config.add_extension('_hierarchical',
-                         sources=['_hierarchical.pyx'],
-                         language="c++",
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
+    config.add_extension(
+        "_dbscan_inner",
+        sources=["_dbscan_inner.pyx"],
+        include_dirs=[numpy.get_include()],
+        language="c++",
+    )

-    config.add_extension('_k_means_elkan',
-                         sources=['_k_means_elkan.pyx'],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
+    config.add_extension(
+        "_hierarchical_fast",
+        sources=["_hierarchical_fast.pyx"],
+        language="c++",
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )

-    config.add_extension('_k_means',
-                         sources=['_k_means.pyx'],
-                         include_dirs=numpy.get_include(),
-                         libraries=libraries)
+    config.add_extension(
+        "_k_means_common",
+        sources=["_k_means_common.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )

-    config.add_subpackage('tests')
+    config.add_extension(
+        "_k_means_lloyd",
+        sources=["_k_means_lloyd.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )
+
+    config.add_extension(
+        "_k_means_elkan",
+        sources=["_k_means_elkan.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )
+
+    config.add_extension(
+        "_k_means_minibatch",
+        sources=["_k_means_minibatch.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )
+
+    config.add_subpackage("tests")

     return config


-if __name__ == '__main__':
+if __name__ == "__main__":
     from numpy.distutils.core import setup
-    setup(**configuration(top_path='').todict())
+
+    setup(**configuration(top_path="").todict())
('sklearn/feature_extraction', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,10 +4,16 @@
 images.
 """

-from .dict_vectorizer import DictVectorizer
-from .hashing import FeatureHasher
+from ._dict_vectorizer import DictVectorizer
+from ._hash import FeatureHasher
 from .image import img_to_graph, grid_to_graph
 from . import text

-__all__ = ['DictVectorizer', 'image', 'img_to_graph', 'grid_to_graph', 'text',
-           'FeatureHasher']
+__all__ = [
+    "DictVectorizer",
+    "image",
+    "img_to_graph",
+    "grid_to_graph",
+    "text",
+    "FeatureHasher",
+]
('sklearn/feature_extraction', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,21 +1,22 @@
 import os
-import platform


-def configuration(parent_package='', top_path=None):
+def configuration(parent_package="", top_path=None):
     import numpy
     from numpy.distutils.misc_util import Configuration

-    config = Configuration('feature_extraction', parent_package, top_path)
+    config = Configuration("feature_extraction", parent_package, top_path)
     libraries = []
-    if os.name == 'posix':
-        libraries.append('m')
+    if os.name == "posix":
+        libraries.append("m")

-    if platform.python_implementation() != 'PyPy':
-        config.add_extension('_hashing',
-                             sources=['_hashing.pyx'],
-                             include_dirs=[numpy.get_include()],
-                             libraries=libraries)
+    config.add_extension(
+        "_hashing_fast",
+        sources=["_hashing_fast.pyx"],
+        include_dirs=[numpy.get_include()],
+        language="c++",
+        libraries=libraries,
+    )
     config.add_subpackage("tests")

     return config
('sklearn/feature_extraction', 'text.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,4 +1,3 @@
-# -*- coding: utf-8 -*-
 # Authors: Olivier Grisel <olivier.grisel@ensta.org>
 #          Mathieu Blondel <mathieu@mblondel.org>
 #          Lars Buitinck
@@ -15,6 +14,7 @@
 import array
 from collections import defaultdict
 from collections.abc import Mapping
+from functools import partial
 import numbers
 from operator import itemgetter
 import re
@@ -24,24 +24,99 @@
 import numpy as np
 import scipy.sparse as sp

-from ..base import BaseEstimator, TransformerMixin
+from ..base import BaseEstimator, TransformerMixin, _OneToOneFeatureMixin
 from ..preprocessing import normalize
-from .hashing import FeatureHasher
-from .stop_words import ENGLISH_STOP_WORDS
-from ..utils.validation import check_is_fitted, check_array, FLOAT_DTYPES
+from ._hash import FeatureHasher
+from ._stop_words import ENGLISH_STOP_WORDS
+from ..utils.validation import check_is_fitted, check_array, FLOAT_DTYPES, check_scalar
+from ..utils.deprecation import deprecated
 from ..utils import _IS_32BIT
-from ..utils.fixes import _astype_copy_false
-from ..exceptions import ChangedBehaviorWarning
-
-
-__all__ = ['HashingVectorizer',
-           'CountVectorizer',
-           'ENGLISH_STOP_WORDS',
-           'TfidfTransformer',
-           'TfidfVectorizer',
-           'strip_accents_ascii',
-           'strip_accents_unicode',
-           'strip_tags']
+from ..exceptions import NotFittedError
+
+
+__all__ = [
+    "HashingVectorizer",
+    "CountVectorizer",
+    "ENGLISH_STOP_WORDS",
+    "TfidfTransformer",
+    "TfidfVectorizer",
+    "strip_accents_ascii",
+    "strip_accents_unicode",
+    "strip_tags",
+]
+
+
+def _preprocess(doc, accent_function=None, lower=False):
+    """Chain together an optional series of text preprocessing steps to
+    apply to a document.
+
+    Parameters
+    ----------
+    doc: str
+        The string to preprocess
+    accent_function: callable, default=None
+        Function for handling accented characters. Common strategies include
+        normalizing and removing.
+    lower: bool, default=False
+        Whether to use str.lower to lowercase all of the text
+
+    Returns
+    -------
+    doc: str
+        preprocessed string
+    """
+    if lower:
+        doc = doc.lower()
+    if accent_function is not None:
+        doc = accent_function(doc)
+    return doc
+
+
+def _analyze(
+    doc,
+    analyzer=None,
+    tokenizer=None,
+    ngrams=None,
+    preprocessor=None,
+    decoder=None,
+    stop_words=None,
+):
+    """Chain together an optional series of text processing steps to go from
+    a single document to ngrams, with or without tokenizing or preprocessing.
+
+    If analyzer is used, only the decoder argument is used, as the analyzer is
+    intended to replace the preprocessor, tokenizer, and ngrams steps.
+
+    Parameters
+    ----------
+    analyzer: callable, default=None
+    tokenizer: callable, default=None
+    ngrams: callable, default=None
+    preprocessor: callable, default=None
+    decoder: callable, default=None
+    stop_words: list, default=None
+
+    Returns
+    -------
+    ngrams: list
+        A sequence of tokens, possibly with pairs, triples, etc.
+    """
+
+    if decoder is not None:
+        doc = decoder(doc)
+    if analyzer is not None:
+        doc = analyzer(doc)
+    else:
+        if preprocessor is not None:
+            doc = preprocessor(doc)
+        if tokenizer is not None:
+            doc = tokenizer(doc)
+        if ngrams is not None:
+            if stop_words is not None:
+                doc = ngrams(doc, stop_words)
+            else:
+                doc = ngrams(doc)
+    return doc


 def strip_accents_unicode(s):
@@ -56,17 +131,19 @@
     s : string
         The string to strip

-    See also
+    See Also
     --------
-    strip_accents_ascii
-        Remove accentuated char for any unicode symbol that has a direct
-        ASCII equivalent.
+    strip_accents_ascii : Remove accentuated char for any unicode symbol that
+        has a direct ASCII equivalent.
     """
-    normalized = unicodedata.normalize('NFKD', s)
-    if normalized == s:
+    try:
+        # If `s` is ASCII-compatible, then it does not contain any accented
+        # characters and we can avoid an expensive list comprehension
+        s.encode("ASCII", errors="strict")
         return s
-    else:
-        return ''.join([c for c in normalized if not unicodedata.combining(c)])
+    except UnicodeEncodeError:
+        normalized = unicodedata.normalize("NFKD", s)
+        return "".join([c for c in normalized if not unicodedata.combining(c)])


 def strip_accents_ascii(s):
@@ -77,16 +154,15 @@

     Parameters
     ----------
-    s : string
+    s : str
         The string to strip

-    See also
+    See Also
     --------
-    strip_accents_unicode
-        Remove accentuated char for any unicode symbol.
+    strip_accents_unicode : Remove accentuated char for any unicode symbol.
     """
-    nkfd_form = unicodedata.normalize('NFKD', s)
-    return nkfd_form.encode('ASCII', 'ignore').decode('ASCII')
+    nkfd_form = unicodedata.normalize("NFKD", s)
+    return nkfd_form.encode("ASCII", "ignore").decode("ASCII")


 def strip_tags(s):
@@ -97,7 +173,7 @@

     Parameters
     ----------
-    s : string
+    s : str
         The string to strip
     """
     return re.compile(r"<([^>]+)>", flags=re.UNICODE).sub(" ", s)
@@ -114,34 +190,40 @@
         return frozenset(stop)


-class VectorizerMixin:
+class _VectorizerMixin:
     """Provides common code for text vectorizers (tokenization logic)."""

     _white_spaces = re.compile(r"\s\s+")

     def decode(self, doc):
-        """Decode the input into a string of unicode symbols
+        """Decode the input into a string of unicode symbols.

         The decoding strategy depends on the vectorizer parameters.

         Parameters
         ----------
-        doc : string
-            The string to decode
-        """
-        if self.input == 'filename':
-            with open(doc, 'rb') as fh:
+        doc : bytes or str
+            The string to decode.
+
+        Returns
+        -------
+        doc: str
+            A string of unicode symbols.
+        """
+        if self.input == "filename":
+            with open(doc, "rb") as fh:
                 doc = fh.read()

-        elif self.input == 'file':
+        elif self.input == "file":
             doc = doc.read()

         if isinstance(doc, bytes):
             doc = doc.decode(self.encoding, self.decode_error)

         if doc is np.nan:
-            raise ValueError("np.nan is an invalid document, expected byte or "
-                             "unicode string.")
+            raise ValueError(
+                "np.nan is an invalid document, expected byte or unicode string."
+            )

         return doc

@@ -169,10 +251,9 @@
             tokens_append = tokens.append
             space_join = " ".join

-            for n in range(min_n,
-                           min(max_n + 1, n_original_tokens + 1)):
+            for n in range(min_n, min(max_n + 1, n_original_tokens + 1)):
                 for i in range(n_original_tokens - n + 1):
-                    tokens_append(space_join(original_tokens[i: i + n]))
+                    tokens_append(space_join(original_tokens[i : i + n]))

         return tokens

@@ -196,7 +277,7 @@

         for n in range(min_n, min(max_n + 1, text_len + 1)):
             for i in range(text_len - n + 1):
-                ngrams_append(text_document[i: i + n])
+                ngrams_append(text_document[i : i + n])
         return ngrams

     def _char_wb_ngrams(self, text_document):
@@ -215,57 +296,73 @@
         ngrams_append = ngrams.append

         for w in text_document.split():
-            w = ' ' + w + ' '
+            w = " " + w + " "
             w_len = len(w)
             for n in range(min_n, max_n + 1):
                 offset = 0
-                ngrams_append(w[offset:offset + n])
+                ngrams_append(w[offset : offset + n])
                 while offset + n < w_len:
                     offset += 1
-                    ngrams_append(w[offset:offset + n])
-                if offset == 0:   # count a short word (w_len < n) only once
+                    ngrams_append(w[offset : offset + n])
+                if offset == 0:  # count a short word (w_len < n) only once
                     break
         return ngrams

     def build_preprocessor(self):
-        """Return a function to preprocess the text before tokenization"""
+        """Return a function to preprocess the text before tokenization.
+
+        Returns
+        -------
+        preprocessor: callable
+              A function to preprocess the text before tokenization.
+        """
         if self.preprocessor is not None:
             return self.preprocessor

-        # unfortunately python functools package does not have an efficient
-        # `compose` function that would have allowed us to chain a dynamic
-        # number of functions. However the cost of a lambda call is a few
-        # hundreds of nanoseconds which is negligible when compared to the
-        # cost of tokenizing a string of 1000 chars for instance.
-        noop = lambda x: x
-
         # accent stripping
         if not self.strip_accents:
-            strip_accents = noop
+            strip_accents = None
         elif callable(self.strip_accents):
             strip_accents = self.strip_accents
-        elif self.strip_accents == 'ascii':
+        elif self.strip_accents == "ascii":
             strip_accents = strip_accents_ascii
-        elif self.strip_accents == 'unicode':
+        elif self.strip_accents == "unicode":
             strip_accents = strip_accents_unicode
         else:
-            raise ValueError('Invalid value for "strip_accents": %s' %
-                             self.strip_accents)
-
-        if self.lowercase:
-            return lambda x: strip_accents(x.lower())
-        else:
-            return strip_accents
+            raise ValueError(
+                'Invalid value for "strip_accents": %s' % self.strip_accents
+            )
+
+        return partial(_preprocess, accent_function=strip_accents, lower=self.lowercase)

     def build_tokenizer(self):
-        """Return a function that splits a string into a sequence of tokens"""
+        """Return a function that splits a string into a sequence of tokens.
+
+        Returns
+        -------
+        tokenizer: callable
+              A function to split a string into a sequence of tokens.
+        """
         if self.tokenizer is not None:
             return self.tokenizer
         token_pattern = re.compile(self.token_pattern)
-        return lambda doc: token_pattern.findall(doc)
+
+        if token_pattern.groups > 1:
+            raise ValueError(
+                "More than 1 capturing group in token pattern. Only a single "
+                "group should be captured."
+            )
+
+        return token_pattern.findall

     def get_stop_words(self):
-        """Build or fetch the effective stop words list"""
+        """Build or fetch the effective stop words list.
+
+        Returns
+        -------
+        stop_words: list or None
+                A list of stop words.
+        """
         return _check_stop_list(self.stop_words)

     def _check_stop_words_consistency(self, stop_words, preprocess, tokenize):
@@ -279,7 +376,7 @@
                         performed (e.g. because of the use of a custom
                         preprocessor / tokenizer)
         """
-        if id(self.stop_words) == getattr(self, '_stop_words_id', None):
+        if id(self.stop_words) == getattr(self, "_stop_words_id", None):
             # Stop words are were previously validated
             return None

@@ -294,66 +391,72 @@
             self._stop_words_id = id(self.stop_words)

             if inconsistent:
-                warnings.warn('Your stop_words may be inconsistent with '
-                              'your preprocessing. Tokenizing the stop '
-                              'words generated tokens %r not in '
-                              'stop_words.' % sorted(inconsistent))
+                warnings.warn(
+                    "Your stop_words may be inconsistent with "
+                    "your preprocessing. Tokenizing the stop "
+                    "words generated tokens %r not in "
+                    "stop_words."
+                    % sorted(inconsistent)
+                )
             return not inconsistent
         except Exception:
             # Failed to check stop words consistency (e.g. because a custom
             # preprocessor or tokenizer was used)
             self._stop_words_id = id(self.stop_words)
-            return 'error'
-
-    def _validate_custom_analyzer(self):
-        # This is to check if the given custom analyzer expects file or a
-        # filename instead of data.
-        # Behavior changed in v0.21, function could be removed in v0.23
-        import tempfile
-        with tempfile.NamedTemporaryFile() as f:
-            fname = f.name
-        # now we're sure fname doesn't exist
-
-        msg = ("Since v0.21, vectorizers pass the data to the custom analyzer "
-               "and not the file names or the file objects. This warning "
-               "will be removed in v0.23.")
-        try:
-            self.analyzer(fname)
-        except FileNotFoundError:
-            warnings.warn(msg, ChangedBehaviorWarning)
-        except AttributeError as e:
-            if str(e) == "'str' object has no attribute 'read'":
-                warnings.warn(msg, ChangedBehaviorWarning)
-        except Exception:
-            pass
+            return "error"

     def build_analyzer(self):
-        """Return a callable that handles preprocessing and tokenization"""
+        """Return a callable to process input data.
+
+        The callable handles that handles preprocessing, tokenization, and
+        n-grams generation.
+
+        Returns
+        -------
+        analyzer: callable
+            A function to handle preprocessing, tokenization
+            and n-grams generation.
+        """
+
         if callable(self.analyzer):
-            if self.input in ['file', 'filename']:
-                self._validate_custom_analyzer()
-            return lambda doc: self.analyzer(self.decode(doc))
+            return partial(_analyze, analyzer=self.analyzer, decoder=self.decode)

         preprocess = self.build_preprocessor()

-        if self.analyzer == 'char':
-            return lambda doc: self._char_ngrams(preprocess(self.decode(doc)))
-
-        elif self.analyzer == 'char_wb':
-            return lambda doc: self._char_wb_ngrams(
-                preprocess(self.decode(doc)))
-
-        elif self.analyzer == 'word':
+        if self.analyzer == "char":
+            return partial(
+                _analyze,
+                ngrams=self._char_ngrams,
+                preprocessor=preprocess,
+                decoder=self.decode,
+            )
+
+        elif self.analyzer == "char_wb":
+
+            return partial(
+                _analyze,
+                ngrams=self._char_wb_ngrams,
+                preprocessor=preprocess,
+                decoder=self.decode,
+            )
+
+        elif self.analyzer == "word":
             stop_words = self.get_stop_words()
             tokenize = self.build_tokenizer()
-            self._check_stop_words_consistency(stop_words, preprocess,
-                                               tokenize)
-            return lambda doc: self._word_ngrams(
-                tokenize(preprocess(self.decode(doc))), stop_words)
+            self._check_stop_words_consistency(stop_words, preprocess, tokenize)
+            return partial(
+                _analyze,
+                ngrams=self._word_ngrams,
+                tokenizer=tokenize,
+                preprocessor=preprocess,
+                decoder=self.decode,
+                stop_words=stop_words,
+            )

         else:
-            raise ValueError('%s is not a valid tokenization scheme/analyzer' %
-                             self.analyzer)
+            raise ValueError(
+                "%s is not a valid tokenization scheme/analyzer" % self.analyzer
+            )

     def _validate_vocabulary(self):
         vocabulary = self.vocabulary
@@ -373,8 +476,10 @@
                     raise ValueError("Vocabulary contains repeated indices.")
                 for i in range(len(vocabulary)):
                     if i not in indices:
-                        msg = ("Vocabulary of size %d doesn't contain index "
-                               "%d." % (len(vocabulary), i))
+                        msg = "Vocabulary of size %d doesn't contain index %d." % (
+                            len(vocabulary),
+                            i,
+                        )
                         raise ValueError(msg)
             if not vocabulary:
                 raise ValueError("empty vocabulary passed to fit")
@@ -384,9 +489,11 @@
             self.fixed_vocabulary_ = False

     def _check_vocabulary(self):
-        """Check if vocabulary is empty or missing (not fit-ed)"""
-        msg = "%(name)s - Vocabulary wasn't fitted."
-        check_is_fitted(self, 'vocabulary_', msg=msg),
+        """Check if vocabulary is empty or missing (not fitted)"""
+        if not hasattr(self, "vocabulary_"):
+            self._validate_vocabulary()
+            if not self.fixed_vocabulary_:
+                raise NotFittedError("Vocabulary not fitted or provided")

         if len(self.vocabulary_) == 0:
             raise ValueError("Vocabulary is empty")
@@ -398,11 +505,55 @@
             raise ValueError(
                 "Invalid value for ngram_range=%s "
                 "lower boundary larger than the upper boundary."
-                % str(self.ngram_range))
-
-
-class HashingVectorizer(BaseEstimator, VectorizerMixin, TransformerMixin):
-    """Convert a collection of text documents to a matrix of token occurrences
+                % str(self.ngram_range)
+            )
+
+    def _warn_for_unused_params(self):
+
+        if self.tokenizer is not None and self.token_pattern is not None:
+            warnings.warn(
+                "The parameter 'token_pattern' will not be used"
+                " since 'tokenizer' is not None'"
+            )
+
+        if self.preprocessor is not None and callable(self.analyzer):
+            warnings.warn(
+                "The parameter 'preprocessor' will not be used"
+                " since 'analyzer' is callable'"
+            )
+
+        if (
+            self.ngram_range != (1, 1)
+            and self.ngram_range is not None
+            and callable(self.analyzer)
+        ):
+            warnings.warn(
+                "The parameter 'ngram_range' will not be used"
+                " since 'analyzer' is callable'"
+            )
+        if self.analyzer != "word" or callable(self.analyzer):
+            if self.stop_words is not None:
+                warnings.warn(
+                    "The parameter 'stop_words' will not be used"
+                    " since 'analyzer' != 'word'"
+                )
+            if (
+                self.token_pattern is not None
+                and self.token_pattern != r"(?u)\b\w\w+\b"
+            ):
+                warnings.warn(
+                    "The parameter 'token_pattern' will not be used"
+                    " since 'analyzer' != 'word'"
+                )
+            if self.tokenizer is not None:
+                warnings.warn(
+                    "The parameter 'tokenizer' will not be used"
+                    " since 'analyzer' != 'word'"
+                )
+
+
+class HashingVectorizer(TransformerMixin, _VectorizerMixin, BaseEstimator):
+    r"""Convert a collection of text documents to a matrix of token occurrences.

     It turns a collection of text documents into a scipy.sparse matrix holding
     token occurrence counts (or binary occurrence information), possibly
@@ -415,10 +566,10 @@
     This strategy has several advantages:

     - it is very low memory scalable to large datasets as there is no need to
-      store a vocabulary dictionary in memory
+      store a vocabulary dictionary in memory.

     - it is fast to pickle and un-pickle as it holds no state besides the
-      constructor parameters
+      constructor parameters.

     - it can be used in a streaming (partial fit) or parallel pipeline as there
       is no state computed during fit.
@@ -442,52 +593,52 @@

     Parameters
     ----------
-
-    input : string {'filename', 'file', 'content'}
-        If 'filename', the sequence passed as an argument to fit is
-        expected to be a list of filenames that need reading to fetch
-        the raw content to analyze.
-
-        If 'file', the sequence items must have a 'read' method (file-like
-        object) that is called to fetch the bytes in memory.
-
-        Otherwise the input is expected to be the sequence strings or
-        bytes items are expected to be analyzed directly.
-
-    encoding : string, default='utf-8'
+    input : {'filename', 'file', 'content'}, default='content'
+        - If `'filename'`, the sequence passed as an argument to fit is
+          expected to be a list of filenames that need reading to fetch
+          the raw content to analyze.
+
+        - If `'file'`, the sequence items must have a 'read' method (file-like
+          object) that is called to fetch the bytes in memory.
+
+        - If `'content'`, the input is expected to be a sequence of items that
+          can be of type string or byte.
+
+    encoding : str, default='utf-8'
         If bytes or files are given to analyze, this encoding is used to
         decode.

-    decode_error : {'strict', 'ignore', 'replace'}
+    decode_error : {'strict', 'ignore', 'replace'}, default='strict'
         Instruction on what to do if a byte sequence is given to analyze that
         contains characters not of the given `encoding`. By default, it is
         'strict', meaning that a UnicodeDecodeError will be raised. Other
         values are 'ignore' and 'replace'.

-    strip_accents : {'ascii', 'unicode', None}
+    strip_accents : {'ascii', 'unicode'}, default=None
         Remove accents and perform other character normalization
         during the preprocessing step.
         'ascii' is a fast method that only works on characters that have
-        an direct ASCII mapping.
+        a direct ASCII mapping.
         'unicode' is a slightly slower method that works on any characters.
         None (default) does nothing.

         Both 'ascii' and 'unicode' use NFKD normalization from
         :func:`unicodedata.normalize`.

-    lowercase : boolean, default=True
+    lowercase : bool, default=True
         Convert all characters to lowercase before tokenizing.

-    preprocessor : callable or None (default)
+    preprocessor : callable, default=None
         Override the preprocessing (string transformation) stage while
         preserving the tokenizing and n-grams generation steps.
-
-    tokenizer : callable or None (default)
+        Only applies if ``analyzer`` is not callable.
+
+    tokenizer : callable, default=None
         Override the string tokenization step while preserving the
         preprocessing and n-grams generation steps.
         Only applies if ``analyzer == 'word'``.

-    stop_words : string {'english'}, list, or None (default)
+    stop_words : {'english'}, list, default=None
         If 'english', a built-in stop word list for English is used.
         There are several known issues with 'english' and you should
         consider an alternative (see :ref:`stop_words`).
@@ -496,18 +647,25 @@
         will be removed from the resulting tokens.
         Only applies if ``analyzer == 'word'``.

-    token_pattern : string
+    token_pattern : str, default=r"(?u)\\b\\w\\w+\\b"
         Regular expression denoting what constitutes a "token", only used
         if ``analyzer == 'word'``. The default regexp selects tokens of 2
         or more alphanumeric characters (punctuation is completely ignored
         and always treated as a token separator).

+        If there is a capturing group in token_pattern then the
+        captured group content, not the entire match, becomes the token.
+        At most one capturing group is permitted.
+
     ngram_range : tuple (min_n, max_n), default=(1, 1)
         The lower and upper boundary of the range of n-values for different
         n-grams to be extracted. All values of n such that min_n <= n <= max_n
-        will be used.
-
-    analyzer : string, {'word', 'char', 'char_wb'} or callable
+        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only
+        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means
+        only bigrams.
+        Only applies if ``analyzer`` is not callable.
+
+    analyzer : {'word', 'char', 'char_wb'} or callable, default='word'
         Whether the feature should be made of word or character n-grams.
         Option 'char_wb' creates character n-grams only from text inside
         word boundaries; n-grams at the edges of words are padded with space.
@@ -516,32 +674,39 @@
         out of the raw, unprocessed input.

         .. versionchanged:: 0.21
-        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is
-        first read from the file and then passed to the given callable
-        analyzer.
-
-    n_features : integer, default=(2 ** 20)
+            Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data
+            is first read from the file and then passed to the given callable
+            analyzer.
+
+    n_features : int, default=(2 ** 20)
         The number of features (columns) in the output matrices. Small numbers
         of features are likely to cause hash collisions, but large numbers
         will cause larger coefficient dimensions in linear learners.

-    binary : boolean, default=False.
+    binary : bool, default=False
         If True, all non zero counts are set to 1. This is useful for discrete
         probabilistic models that model binary events rather than integer
         counts.

-    norm : 'l1', 'l2' or None, optional
+    norm : {'l1', 'l2'}, default='l2'
         Norm used to normalize term vectors. None for no normalization.

-    alternate_sign : boolean, optional, default True
+    alternate_sign : bool, default=True
         When True, an alternating sign is added to the features as to
         approximately conserve the inner product in the hashed space even for
         small n_features. This approach is similar to sparse random projection.

         .. versionadded:: 0.19

-    dtype : type, optional
+    dtype : type, default=np.float64
         Type of the matrix returned by fit_transform() or transform().
+
+    See Also
+    --------
+    CountVectorizer : Convert a collection of text documents to a matrix of
+        token counts.
+    TfidfVectorizer : Convert a collection of raw documents to a matrix of
+        TF-IDF features.

     Examples
     --------
@@ -556,19 +721,28 @@
     >>> X = vectorizer.fit_transform(corpus)
     >>> print(X.shape)
     (4, 16)
-
-    See also
-    --------
-    CountVectorizer, TfidfVectorizer
-
     """
-    def __init__(self, input='content', encoding='utf-8',
-                 decode_error='strict', strip_accents=None,
-                 lowercase=True, preprocessor=None, tokenizer=None,
-                 stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
-                 ngram_range=(1, 1), analyzer='word', n_features=(2 ** 20),
-                 binary=False, norm='l2', alternate_sign=True,
-                 dtype=np.float64):
+
+    def __init__(
+        self,
+        *,
+        input="content",
+        encoding="utf-8",
+        decode_error="strict",
+        strip_accents=None,
+        lowercase=True,
+        preprocessor=None,
+        tokenizer=None,
+        stop_words=None,
+        token_pattern=r"(?u)\b\w\w+\b",
+        ngram_range=(1, 1),
+        analyzer="word",
+        n_features=(2**20),
+        binary=False,
+        norm="l2",
+        alternate_sign=True,
+        dtype=np.float64,
+    ):
         self.input = input
         self.encoding = encoding
         self.decode_error = decode_error
@@ -587,32 +761,49 @@
         self.dtype = dtype

     def partial_fit(self, X, y=None):
-        """Does nothing: this transformer is stateless.
+        """No-op: this transformer is stateless.

         This method is just there to mark the fact that this transformer
         can work in a streaming setup.

         Parameters
         ----------
-        X : array-like, shape [n_samples, n_features]
+        X : ndarray of shape [n_samples, n_features]
             Training data.
+
+        y : Ignored
+            Not used, present for API consistency by convention.
+
+        Returns
+        -------
+        self : object
+            HashingVectorizer instance.
         """
         return self

     def fit(self, X, y=None):
-        """Does nothing: this transformer is stateless.
+        """No-op: this transformer is stateless.

         Parameters
         ----------
-        X : array-like, shape [n_samples, n_features]
+        X : ndarray of shape [n_samples, n_features]
             Training data.
+
+        y : Ignored
+            Not used, present for API consistency by convention.
+
+        Returns
+        -------
+        self : object
+            HashingVectorizer instance.
         """
         # triggers a parameter validation
         if isinstance(X, str):
             raise ValueError(
-                "Iterable over raw text documents expected, "
-                "string object received.")
-
+                "Iterable over raw text documents expected, string object received."
+            )
+
+        self._warn_for_unused_params()
         self._validate_params()

         self._get_hasher().fit(X, y=y)
@@ -630,13 +821,13 @@

         Returns
         -------
-        X : scipy.sparse matrix, shape = (n_samples, self.n_features)
+        X : sparse matrix of shape (n_samples, n_features)
             Document-term matrix.
         """
         if isinstance(X, str):
             raise ValueError(
-                "Iterable over raw text documents expected, "
-                "string object received.")
+                "Iterable over raw text documents expected, string object received."
+            )

         self._validate_params()

@@ -663,18 +854,21 @@

         Returns
         -------
-        X : scipy.sparse matrix, shape = (n_samples, self.n_features)
+        X : sparse matrix of shape (n_samples, n_features)
             Document-term matrix.
         """
         return self.fit(X, y).transform(X)

     def _get_hasher(self):
-        return FeatureHasher(n_features=self.n_features,
-                             input_type='string', dtype=self.dtype,
-                             alternate_sign=self.alternate_sign)
+        return FeatureHasher(
+            n_features=self.n_features,
+            input_type="string",
+            dtype=self.dtype,
+            alternate_sign=self.alternate_sign,
+        )

     def _more_tags(self):
-        return {'X_types': ['string']}
+        return {"X_types": ["string"]}


 def _document_frequency(X):
@@ -685,8 +879,8 @@
         return np.diff(X.indptr)


-class CountVectorizer(BaseEstimator, VectorizerMixin):
-    """Convert a collection of text documents to a matrix of token counts
+class CountVectorizer(_VectorizerMixin, BaseEstimator):
+    r"""Convert a collection of text documents to a matrix of token counts.

     This implementation produces a sparse representation of the counts using
     scipy.sparse.csr_matrix.
@@ -699,28 +893,28 @@

     Parameters
     ----------
-    input : string {'filename', 'file', 'content'}
-        If 'filename', the sequence passed as an argument to fit is
-        expected to be a list of filenames that need reading to fetch
-        the raw content to analyze.
-
-        If 'file', the sequence items must have a 'read' method (file-like
-        object) that is called to fetch the bytes in memory.
-
-        Otherwise the input is expected to be the sequence strings or
-        bytes items are expected to be analyzed directly.
-
-    encoding : string, 'utf-8' by default.
+    input : {'filename', 'file', 'content'}, default='content'
+        - If `'filename'`, the sequence passed as an argument to fit is
+          expected to be a list of filenames that need reading to fetch
+          the raw content to analyze.
+
+        - If `'file'`, the sequence items must have a 'read' method (file-like
+          object) that is called to fetch the bytes in memory.
+
+        - If `'content'`, the input is expected to be a sequence of items that
+          can be of type string or byte.
+
+    encoding : str, default='utf-8'
         If bytes or files are given to analyze, this encoding is used to
         decode.

-    decode_error : {'strict', 'ignore', 'replace'}
+    decode_error : {'strict', 'ignore', 'replace'}, default='strict'
         Instruction on what to do if a byte sequence is given to analyze that
         contains characters not of the given `encoding`. By default, it is
         'strict', meaning that a UnicodeDecodeError will be raised. Other
         values are 'ignore' and 'replace'.

-    strip_accents : {'ascii', 'unicode', None}
+    strip_accents : {'ascii', 'unicode'}, default=None
         Remove accents and perform other character normalization
         during the preprocessing step.
         'ascii' is a fast method that only works on characters that have
@@ -731,19 +925,20 @@
         Both 'ascii' and 'unicode' use NFKD normalization from
         :func:`unicodedata.normalize`.

-    lowercase : boolean, True by default
+    lowercase : bool, default=True
         Convert all characters to lowercase before tokenizing.

-    preprocessor : callable or None (default)
-        Override the preprocessing (string transformation) stage while
+    preprocessor : callable, default=None
+        Override the preprocessing (strip_accents and lowercase) stage while
         preserving the tokenizing and n-grams generation steps.
-
-    tokenizer : callable or None (default)
+        Only applies if ``analyzer`` is not callable.
+
+    tokenizer : callable, default=None
         Override the string tokenization step while preserving the
         preprocessing and n-grams generation steps.
         Only applies if ``analyzer == 'word'``.

-    stop_words : string {'english'}, list, or None (default)
+    stop_words : {'english'}, list, default=None
         If 'english', a built-in stop word list for English is used.
         There are several known issues with 'english' and you should
         consider an alternative (see :ref:`stop_words`).
@@ -756,19 +951,27 @@
         in the range [0.7, 1.0) to automatically detect and filter stop
         words based on intra corpus document frequency of terms.

-    token_pattern : string
+    token_pattern : str, default=r"(?u)\\b\\w\\w+\\b"
         Regular expression denoting what constitutes a "token", only used
         if ``analyzer == 'word'``. The default regexp select tokens of 2
         or more alphanumeric characters (punctuation is completely ignored
         and always treated as a token separator).

-    ngram_range : tuple (min_n, max_n)
+        If there is a capturing group in token_pattern then the
+        captured group content, not the entire match, becomes the token.
+        At most one capturing group is permitted.
+
+    ngram_range : tuple (min_n, max_n), default=(1, 1)
         The lower and upper boundary of the range of n-values for different
-        n-grams to be extracted. All values of n such that min_n <= n <= max_n
-        will be used.
-
-    analyzer : string, {'word', 'char', 'char_wb'} or callable
-        Whether the feature should be made of word or character n-grams.
+        word n-grams or char n-grams to be extracted. All values of n such
+        such that min_n <= n <= max_n will be used. For example an
+        ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means
+        unigrams and bigrams, and ``(2, 2)`` means only bigrams.
+        Only applies if ``analyzer`` is not callable.
+
+    analyzer : {'word', 'char', 'char_wb'} or callable, default='word'
+        Whether the feature should be made of word n-gram or character
+        n-grams.
         Option 'char_wb' creates character n-grams only from text inside
         word boundaries; n-grams at the edges of words are padded with space.

@@ -776,6 +979,7 @@
         out of the raw, unprocessed input.

         .. versionchanged:: 0.21
+
         Since v0.21, if ``input`` is ``filename`` or ``file``, the data is
         first read from the file and then passed to the given callable
         analyzer.
@@ -796,25 +1000,25 @@
         absolute counts.
         This parameter is ignored if vocabulary is not None.

-    max_features : int or None, default=None
+    max_features : int, default=None
         If not None, build a vocabulary that only consider the top
         max_features ordered by term frequency across the corpus.

         This parameter is ignored if vocabulary is not None.

-    vocabulary : Mapping or iterable, optional
+    vocabulary : Mapping or iterable, default=None
         Either a Mapping (e.g., a dict) where keys are terms and values are
         indices in the feature matrix, or an iterable over terms. If not
         given, a vocabulary is determined from the input documents. Indices
         in the mapping should not be repeated and should not have any gap
         between 0 and the largest index.

-    binary : boolean, default=False
+    binary : bool, default=False
         If True, all non zero counts are set to 1. This is useful for discrete
         probabilistic models that model binary events rather than integer
         counts.

-    dtype : type, optional
+    dtype : type, default=np.int64
         Type of the matrix returned by fit_transform() or transform().

     Attributes
@@ -822,6 +1026,10 @@
     vocabulary_ : dict
         A mapping of terms to feature indices.

+    fixed_vocabulary_ : bool
+        True if a fixed vocabulary of term to indices mapping
+        is provided by the user.
+
     stop_words_ : set
         Terms that were ignored because they either:

@@ -830,6 +1038,20 @@
           - were cut off by feature selection (`max_features`).

         This is only available if no vocabulary was given.
+
+    See Also
+    --------
+    HashingVectorizer : Convert a collection of text documents to a
+        matrix of token counts.
+
+    TfidfVectorizer : Convert a collection of raw documents to a matrix
+        of TF-IDF features.
+
+    Notes
+    -----
+    The ``stop_words_`` attribute can get large and increase the model size
+    when pickling. This attribute is provided only for introspection and can
+    be safely removed using delattr or set to None before pickling.

     Examples
     --------
@@ -842,32 +1064,48 @@
     ... ]
     >>> vectorizer = CountVectorizer()
     >>> X = vectorizer.fit_transform(corpus)
-    >>> print(vectorizer.get_feature_names())
-    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
-    >>> print(X.toarray())  # doctest: +NORMALIZE_WHITESPACE
+    >>> vectorizer.get_feature_names_out()
+    array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',
+           'this'], ...)
+    >>> print(X.toarray())
     [[0 1 1 1 0 0 1 0 1]
      [0 2 0 1 0 1 1 0 1]
      [1 0 0 1 1 0 1 1 1]
      [0 1 1 1 0 0 1 0 1]]
-
-    See also
-    --------
-    HashingVectorizer, TfidfVectorizer
-
-    Notes
-    -----
-    The ``stop_words_`` attribute can get large and increase the model size
-    when pickling. This attribute is provided only for introspection and can
-    be safely removed using delattr or set to None before pickling.
+    >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))
+    >>> X2 = vectorizer2.fit_transform(corpus)
+    >>> vectorizer2.get_feature_names_out()
+    array(['and this', 'document is', 'first document', 'is the', 'is this',
+           'second document', 'the first', 'the second', 'the third', 'third one',
+           'this document', 'this is', 'this the'], ...)
+     >>> print(X2.toarray())
+     [[0 0 1 1 0 0 1 0 0 0 0 1 0]
+     [0 1 0 1 0 1 0 1 0 0 1 0 0]
+     [1 0 0 1 0 0 0 0 1 1 0 1 0]
+     [0 0 1 0 1 0 1 0 0 0 0 0 1]]
     """

-    def __init__(self, input='content', encoding='utf-8',
-                 decode_error='strict', strip_accents=None,
-                 lowercase=True, preprocessor=None, tokenizer=None,
-                 stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
-                 ngram_range=(1, 1), analyzer='word',
-                 max_df=1.0, min_df=1, max_features=None,
-                 vocabulary=None, binary=False, dtype=np.int64):
+    def __init__(
+        self,
+        *,
+        input="content",
+        encoding="utf-8",
+        decode_error="strict",
+        strip_accents=None,
+        lowercase=True,
+        preprocessor=None,
+        tokenizer=None,
+        stop_words=None,
+        token_pattern=r"(?u)\b\w\w+\b",
+        ngram_range=(1, 1),
+        analyzer="word",
+        max_df=1.0,
+        min_df=1,
+        max_features=None,
+        vocabulary=None,
+        binary=False,
+        dtype=np.int64,
+    ):
         self.input = input
         self.encoding = encoding
         self.decode_error = decode_error
@@ -880,15 +1118,7 @@
         self.stop_words = stop_words
         self.max_df = max_df
         self.min_df = min_df
-        if max_df < 0 or min_df < 0:
-            raise ValueError("negative value for max_df or min_df")
         self.max_features = max_features
-        if max_features is not None:
-            if (not isinstance(max_features, numbers.Integral) or
-                    max_features <= 0):
-                raise ValueError(
-                    "max_features=%r, neither a positive integer nor None"
-                    % max_features)
         self.ngram_range = ngram_range
         self.vocabulary = vocabulary
         self.binary = binary
@@ -905,11 +1135,10 @@
             vocabulary[term] = new_val
             map_index[old_val] = new_val

-        X.indices = map_index.take(X.indices, mode='clip')
+        X.indices = map_index.take(X.indices, mode="clip")
         return X

-    def _limit_features(self, X, vocabulary, high=None, low=None,
-                        limit=None):
+    def _limit_features(self, X, vocabulary, high=None, low=None, limit=None):
         """Remove too rare or too common features.

         Prune features that are non zero in more samples than high or less
@@ -923,13 +1152,13 @@

         # Calculate a mask based on document frequencies
         dfs = _document_frequency(X)
-        tfs = np.asarray(X.sum(axis=0)).ravel()
         mask = np.ones(len(dfs), dtype=bool)
         if high is not None:
             mask &= dfs <= high
         if low is not None:
             mask &= dfs >= low
         if limit is not None and mask.sum() > limit:
+            tfs = np.asarray(X.sum(axis=0)).ravel()
             mask_inds = (-tfs[mask]).argsort()[:limit]
             new_mask = np.zeros(len(dfs), dtype=bool)
             new_mask[np.where(mask)[0][mask_inds]] = True
@@ -945,13 +1174,13 @@
                 removed_terms.add(term)
         kept_indices = np.where(mask)[0]
         if len(kept_indices) == 0:
-            raise ValueError("After pruning, no terms remain. Try a lower"
-                             " min_df or a higher max_df.")
+            raise ValueError(
+                "After pruning, no terms remain. Try a lower min_df or a higher max_df."
+            )
         return X[:, kept_indices], removed_terms

     def _count_vocab(self, raw_documents, fixed_vocab):
-        """Create sparse feature matrix, and vocabulary where fixed_vocab=False
-        """
+        """Create sparse feature matrix, and vocabulary where fixed_vocab=False"""
         if fixed_vocab:
             vocabulary = self.vocabulary_
         else:
@@ -986,15 +1215,19 @@
             # disable defaultdict behaviour
             vocabulary = dict(vocabulary)
             if not vocabulary:
-                raise ValueError("empty vocabulary; perhaps the documents only"
-                                 " contain stop words")
-
-        if indptr[-1] > 2147483648:  # = 2**31 - 1
+                raise ValueError(
+                    "empty vocabulary; perhaps the documents only contain stop words"
+                )
+
+        if indptr[-1] > np.iinfo(np.int32).max:  # = 2**31 - 1
             if _IS_32BIT:
-                raise ValueError(('sparse CSR array has {} non-zero '
-                                  'elements and requires 64 bit indexing, '
-                                  'which is unsupported with 32 bit Python.')
-                                 .format(indptr[-1]))
+                raise ValueError(
+                    (
+                        "sparse CSR array has {} non-zero "
+                        "elements and requires 64 bit indexing, "
+                        "which is unsupported with 32 bit Python."
+                    ).format(indptr[-1])
+                )
             indices_dtype = np.int64

         else:
@@ -1003,11 +1236,30 @@
         indptr = np.asarray(indptr, dtype=indices_dtype)
         values = np.frombuffer(values, dtype=np.intc)

-        X = sp.csr_matrix((values, j_indices, indptr),
-                          shape=(len(indptr) - 1, len(vocabulary)),
-                          dtype=self.dtype)
+        X = sp.csr_matrix(
+            (values, j_indices, indptr),
+            shape=(len(indptr) - 1, len(vocabulary)),
+            dtype=self.dtype,
+        )
         X.sort_indices()
         return vocabulary, X
+
+    def _validate_params(self):
+        """Validation of min_df, max_df and max_features"""
+        super()._validate_params()
+
+        if self.max_features is not None:
+            check_scalar(self.max_features, "max_features", numbers.Integral, min_val=0)
+
+        if isinstance(self.min_df, numbers.Integral):
+            check_scalar(self.min_df, "min_df", numbers.Integral, min_val=0)
+        else:
+            check_scalar(self.min_df, "min_df", numbers.Real, min_val=0.0, max_val=1.0)
+
+        if isinstance(self.max_df, numbers.Integral):
+            check_scalar(self.max_df, "max_df", numbers.Integral, min_val=0)
+        else:
+            check_scalar(self.max_df, "max_df", numbers.Real, min_val=0.0, max_val=1.0)

     def fit(self, raw_documents, y=None):
         """Learn a vocabulary dictionary of all tokens in the raw documents.
@@ -1015,17 +1267,22 @@
         Parameters
         ----------
         raw_documents : iterable
-            An iterable which yields either str, unicode or file objects.
-
-        Returns
-        -------
-        self
-        """
+            An iterable which generates either str, unicode or file objects.
+
+        y : None
+            This parameter is ignored.
+
+        Returns
+        -------
+        self : object
+            Fitted vectorizer.
+        """
+        self._warn_for_unused_params()
         self.fit_transform(raw_documents)
         return self

     def fit_transform(self, raw_documents, y=None):
-        """Learn the vocabulary dictionary and return term-document matrix.
+        """Learn the vocabulary dictionary and return document-term matrix.

         This is equivalent to fit followed by transform, but more efficiently
         implemented.
@@ -1033,11 +1290,14 @@
         Parameters
         ----------
         raw_documents : iterable
-            An iterable which yields either str, unicode or file objects.
-
-        Returns
-        -------
-        X : array, [n_samples, n_features]
+            An iterable which generates either str, unicode or file objects.
+
+        y : None
+            This parameter is ignored.
+
+        Returns
+        -------
+        X : array of shape (n_samples, n_features)
             Document-term matrix.
         """
         # We intentionally don't call the transform method to make
@@ -1045,8 +1305,8 @@
         # TfidfVectorizer.
         if isinstance(raw_documents, str):
             raise ValueError(
-                "Iterable over raw text documents expected, "
-                "string object received.")
+                "Iterable over raw text documents expected, string object received."
+            )

         self._validate_params()
         self._validate_vocabulary()
@@ -1054,30 +1314,39 @@
         min_df = self.min_df
         max_features = self.max_features

-        vocabulary, X = self._count_vocab(raw_documents,
-                                          self.fixed_vocabulary_)
+        if self.fixed_vocabulary_ and self.lowercase:
+            for term in self.vocabulary:
+                if any(map(str.isupper, term)):
+                    warnings.warn(
+                        "Upper case characters found in"
+                        " vocabulary while 'lowercase'"
+                        " is True. These entries will not"
+                        " be matched with any documents"
+                    )
+                    break
+
+        vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)

         if self.binary:
             X.data.fill(1)

         if not self.fixed_vocabulary_:
-            X = self._sort_features(X, vocabulary)
-
             n_doc = X.shape[0]
-            max_doc_count = (max_df
-                             if isinstance(max_df, numbers.Integral)
-                             else max_df * n_doc)
-            min_doc_count = (min_df
-                             if isinstance(min_df, numbers.Integral)
-                             else min_df * n_doc)
+            max_doc_count = (
+                max_df if isinstance(max_df, numbers.Integral) else max_df * n_doc
+            )
+            min_doc_count = (
+                min_df if isinstance(min_df, numbers.Integral) else min_df * n_doc
+            )
             if max_doc_count < min_doc_count:
-                raise ValueError(
-                    "max_df corresponds to < documents than min_df")
-            X, self.stop_words_ = self._limit_features(X, vocabulary,
-                                                       max_doc_count,
-                                                       min_doc_count,
-                                                       max_features)
-
+                raise ValueError("max_df corresponds to < documents than min_df")
+            if max_features is not None:
+                X = self._sort_features(X, vocabulary)
+            X, self.stop_words_ = self._limit_features(
+                X, vocabulary, max_doc_count, min_doc_count, max_features
+            )
+            if max_features is None:
+                X = self._sort_features(X, vocabulary)
             self.vocabulary_ = vocabulary

         return X
@@ -1091,21 +1360,17 @@
         Parameters
         ----------
         raw_documents : iterable
-            An iterable which yields either str, unicode or file objects.
-
-        Returns
-        -------
-        X : sparse matrix, [n_samples, n_features]
+            An iterable which generates either str, unicode or file objects.
+
+        Returns
+        -------
+        X : sparse matrix of shape (n_samples, n_features)
             Document-term matrix.
         """
         if isinstance(raw_documents, str):
             raise ValueError(
-                "Iterable over raw text documents expected, "
-                "string object received.")
-
-        if not hasattr(self, 'vocabulary_'):
-            self._validate_vocabulary()
-
+                "Iterable over raw text documents expected, string object received."
+            )
         self._check_vocabulary()

         # use the same matrix-building strategy as fit_transform
@@ -1119,43 +1384,71 @@

         Parameters
         ----------
-        X : {array, sparse matrix}, shape = [n_samples, n_features]
-
-        Returns
-        -------
-        X_inv : list of arrays, len = n_samples
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Document-term matrix.
+
+        Returns
+        -------
+        X_inv : list of arrays of shape (n_samples,)
             List of arrays of terms.
         """
         self._check_vocabulary()
-
-        if sp.issparse(X):
-            # We need CSR format for fast row manipulations.
-            X = X.tocsr()
-        else:
-            # We need to convert X to a matrix, so that the indexing
-            # returns 2D objects
-            X = np.asmatrix(X)
+        # We need CSR format for fast row manipulations.
+        X = check_array(X, accept_sparse="csr")
         n_samples = X.shape[0]

         terms = np.array(list(self.vocabulary_.keys()))
         indices = np.array(list(self.vocabulary_.values()))
         inverse_vocabulary = terms[np.argsort(indices)]

-        return [inverse_vocabulary[X[i, :].nonzero()[1]].ravel()
-                for i in range(n_samples)]
-
+        if sp.issparse(X):
+            return [
+                inverse_vocabulary[X[i, :].nonzero()[1]].ravel()
+                for i in range(n_samples)
+            ]
+        else:
+            return [
+                inverse_vocabulary[np.flatnonzero(X[i, :])].ravel()
+                for i in range(n_samples)
+            ]
+
+    @deprecated(
+        "get_feature_names is deprecated in 1.0 and will be removed "
+        "in 1.2. Please use get_feature_names_out instead."
+    )
     def get_feature_names(self):
-        """Array mapping from feature integer indices to feature name"""
-        if not hasattr(self, 'vocabulary_'):
-            self._validate_vocabulary()
-
+        """Array mapping from feature integer indices to feature name.
+
+        Returns
+        -------
+        feature_names : list
+            A list of feature names.
+        """
         self._check_vocabulary()

-        return [t for t, i in sorted(self.vocabulary_.items(),
-                                     key=itemgetter(1))]
+        return [t for t, i in sorted(self.vocabulary_.items(), key=itemgetter(1))]
+
+    def get_feature_names_out(self, input_features=None):
+        """Get output feature names for transformation.
+
+        Parameters
+        ----------
+        input_features : array-like of str or None, default=None
+            Not used, present here for API consistency by convention.
+
+        Returns
+        -------
+        feature_names_out : ndarray of str objects
+            Transformed feature names.
+        """
+        self._check_vocabulary()
+        return np.asarray(
+            [t for t, i in sorted(self.vocabulary_.items(), key=itemgetter(1))],
+            dtype=object,
+        )

     def _more_tags(self):
-        return {'X_types': ['string']}
+        return {"X_types": ["string"]}


 def _make_int_array():
@@ -1163,8 +1456,8 @@
     return array.array(str("i"))


-class TfidfTransformer(BaseEstimator, TransformerMixin):
-    """Transform a count matrix to a normalized tf or tf-idf representation
+class TfidfTransformer(_OneToOneFeatureMixin, TransformerMixin, BaseEstimator):
+    """Transform a count matrix to a normalized tf or tf-idf representation.

     Tf means term-frequency while tf-idf means term-frequency times inverse
     document-frequency. This is a common term weighting scheme in information
@@ -1192,7 +1485,7 @@
     If ``smooth_idf=True`` (the default), the constant "1" is added to the
     numerator and denominator of the idf as if an extra document was seen
     containing every term in the collection exactly once, which prevents
-    zero divisions: idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.
+    zero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1.

     Furthermore, the formulas used to compute tf and idf depend
     on parameter settings that correspond to the SMART notation used in IR
@@ -1208,58 +1501,117 @@

     Parameters
     ----------
-    norm : 'l1', 'l2' or None, optional (default='l2')
+    norm : {'l1', 'l2'}, default='l2'
         Each output row will have unit norm, either:
-        * 'l2': Sum of squares of vector elements is 1. The cosine
-        similarity between two vectors is their dot product when l2 norm has
-        been applied.
-        * 'l1': Sum of absolute values of vector elements is 1.
-        See :func:`preprocessing.normalize`
-
-    use_idf : boolean (default=True)
-        Enable inverse-document-frequency reweighting.
-
-    smooth_idf : boolean (default=True)
+
+        - 'l2': Sum of squares of vector elements is 1. The cosine
+          similarity between two vectors is their dot product when l2 norm has
+          been applied.
+        - 'l1': Sum of absolute values of vector elements is 1.
+          See :func:`preprocessing.normalize`.
+
+    use_idf : bool, default=True
+        Enable inverse-document-frequency reweighting. If False, idf(t) = 1.
+
+    smooth_idf : bool, default=True
         Smooth idf weights by adding one to document frequencies, as if an
         extra document was seen containing every term in the collection
         exactly once. Prevents zero divisions.

-    sublinear_tf : boolean (default=False)
+    sublinear_tf : bool, default=False
         Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).

     Attributes
     ----------
-    idf_ : array, shape (n_features)
+    idf_ : array of shape (n_features)
         The inverse document frequency (IDF) vector; only defined
         if  ``use_idf`` is True.

+        .. versionadded:: 0.20
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 1.0
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    CountVectorizer : Transforms text into a sparse matrix of n-gram counts.
+
+    TfidfVectorizer : Convert a collection of raw documents to a matrix of
+        TF-IDF features.
+
+    HashingVectorizer : Convert a collection of text documents to a matrix
+        of token occurrences.
+
     References
     ----------
-
     .. [Yates2011] R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern
                    Information Retrieval. Addison Wesley, pp. 68-74.

     .. [MRS2008] C.D. Manning, P. Raghavan and H. Schütze  (2008).
                    Introduction to Information Retrieval. Cambridge University
                    Press, pp. 118-120.
+
+    Examples
+    --------
+    >>> from sklearn.feature_extraction.text import TfidfTransformer
+    >>> from sklearn.feature_extraction.text import CountVectorizer
+    >>> from sklearn.pipeline import Pipeline
+    >>> corpus = ['this is the first document',
+    ...           'this document is the second document',
+    ...           'and this is the third one',
+    ...           'is this the first document']
+    >>> vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',
+    ...               'and', 'one']
+    >>> pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),
+    ...                  ('tfid', TfidfTransformer())]).fit(corpus)
+    >>> pipe['count'].transform(corpus).toarray()
+    array([[1, 1, 1, 1, 0, 1, 0, 0],
+           [1, 2, 0, 1, 1, 1, 0, 0],
+           [1, 0, 0, 1, 0, 1, 1, 1],
+           [1, 1, 1, 1, 0, 1, 0, 0]])
+    >>> pipe['tfid'].idf_
+    array([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,
+           1.        , 1.91629073, 1.91629073])
+    >>> pipe.transform(corpus).shape
+    (4, 8)
     """

-    def __init__(self, norm='l2', use_idf=True, smooth_idf=True,
-                 sublinear_tf=False):
+    def __init__(self, *, norm="l2", use_idf=True, smooth_idf=True, sublinear_tf=False):
         self.norm = norm
         self.use_idf = use_idf
         self.smooth_idf = smooth_idf
         self.sublinear_tf = sublinear_tf

     def fit(self, X, y=None):
-        """Learn the idf vector (global term weights)
+        """Learn the idf vector (global term weights).

         Parameters
         ----------
-        X : sparse matrix, [n_samples, n_features]
-            a matrix of term/token counts
-        """
-        X = check_array(X, accept_sparse=('csr', 'csc'))
+        X : sparse matrix of shape n_samples, n_features)
+            A matrix of term/token counts.
+
+        y : None
+            This parameter is not needed to compute tf-idf.
+
+        Returns
+        -------
+        self : object
+            Fitted transformer.
+        """
+        # large sparse data is not supported for 32bit platforms because
+        # _document_frequency uses np.bincount which works on arrays of
+        # dtype NPY_INTP which is int32 for 32bit platforms. See #20923
+        X = self._validate_data(
+            X, accept_sparse=("csr", "csc"), accept_large_sparse=not _IS_32BIT
+        )
         if not sp.issparse(X):
             X = sp.csr_matrix(X)
         dtype = X.dtype if X.dtype in FLOAT_DTYPES else np.float64
@@ -1267,7 +1619,7 @@
         if self.use_idf:
             n_samples, n_features = X.shape
             df = _document_frequency(X)
-            df = df.astype(dtype, **_astype_copy_false(df))
+            df = df.astype(dtype, copy=False)

             # perform idf smoothing if required
             df += int(self.smooth_idf)
@@ -1276,47 +1628,49 @@
             # log+1 instead of log makes sure terms with zero idf don't get
             # suppressed entirely.
             idf = np.log(n_samples / df) + 1
-            self._idf_diag = sp.diags(idf, offsets=0,
-                                      shape=(n_features, n_features),
-                                      format='csr',
-                                      dtype=dtype)
+            self._idf_diag = sp.diags(
+                idf,
+                offsets=0,
+                shape=(n_features, n_features),
+                format="csr",
+                dtype=dtype,
+            )

         return self

     def transform(self, X, copy=True):
-        """Transform a count matrix to a tf or tf-idf representation
+        """Transform a count matrix to a tf or tf-idf representation.

         Parameters
         ----------
-        X : sparse matrix, [n_samples, n_features]
-            a matrix of term/token counts
-
-        copy : boolean, default True
+        X : sparse matrix of (n_samples, n_features)
+            A matrix of term/token counts.
+
+        copy : bool, default=True
             Whether to copy X and operate on the copy or perform in-place
             operations.

         Returns
         -------
-        vectors : sparse matrix, [n_samples, n_features]
-        """
-        X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, copy=copy)
+        vectors : sparse matrix of shape (n_samples, n_features)
+            Tf-idf-weighted document-term matrix.
+        """
+        X = self._validate_data(
+            X, accept_sparse="csr", dtype=FLOAT_DTYPES, copy=copy, reset=False
+        )
         if not sp.issparse(X):
             X = sp.csr_matrix(X, dtype=np.float64)
-
-        n_samples, n_features = X.shape

         if self.sublinear_tf:
             np.log(X.data, X.data)
             X.data += 1

         if self.use_idf:
-            check_is_fitted(self, '_idf_diag', 'idf vector is not fitted')
-
-            expected_n_features = self._idf_diag.shape[0]
-            if n_features != expected_n_features:
-                raise ValueError("Input has n_features=%d while the model"
-                                 " has been trained with n_features=%d" % (
-                                     n_features, expected_n_features))
+            # idf_ being a property, the automatic attributes detection
+            # does not work as usual and we need to specify the attribute
+            # name:
+            check_is_fitted(self, attributes=["idf_"], msg="idf vector is not fitted")
+
             # *= doesn't work
             X = X * self._idf_diag

@@ -1327,6 +1681,12 @@

     @property
     def idf_(self):
+        """Inverse document frequency vector, only defined if `use_idf=True`.
+
+        Returns
+        -------
+        ndarray of shape (n_features,)
+        """
         # if _idf_diag is not set, this will raise an attribute error,
         # which means hasattr(self, "idf_") is False
         return np.ravel(self._idf_diag.sum(axis=0))
@@ -1335,15 +1695,16 @@
     def idf_(self, value):
         value = np.asarray(value, dtype=np.float64)
         n_features = value.shape[0]
-        self._idf_diag = sp.spdiags(value, diags=0, m=n_features,
-                                    n=n_features, format='csr')
+        self._idf_diag = sp.spdiags(
+            value, diags=0, m=n_features, n=n_features, format="csr"
+        )

     def _more_tags(self):
-        return {'X_types': 'sparse'}
+        return {"X_types": ["2darray", "sparse"]}


 class TfidfVectorizer(CountVectorizer):
-    """Convert a collection of raw documents to a matrix of TF-IDF features.
+    r"""Convert a collection of raw documents to a matrix of TF-IDF features.

     Equivalent to :class:`CountVectorizer` followed by
     :class:`TfidfTransformer`.
@@ -1352,28 +1713,28 @@

     Parameters
     ----------
-    input : string {'filename', 'file', 'content'}
-        If 'filename', the sequence passed as an argument to fit is
-        expected to be a list of filenames that need reading to fetch
-        the raw content to analyze.
-
-        If 'file', the sequence items must have a 'read' method (file-like
-        object) that is called to fetch the bytes in memory.
-
-        Otherwise the input is expected to be the sequence strings or
-        bytes items are expected to be analyzed directly.
-
-    encoding : string, 'utf-8' by default.
+    input : {'filename', 'file', 'content'}, default='content'
+        - If `'filename'`, the sequence passed as an argument to fit is
+          expected to be a list of filenames that need reading to fetch
+          the raw content to analyze.
+
+        - If `'file'`, the sequence items must have a 'read' method (file-like
+          object) that is called to fetch the bytes in memory.
+
+        - If `'content'`, the input is expected to be a sequence of items that
+          can be of type string or byte.
+
+    encoding : str, default='utf-8'
         If bytes or files are given to analyze, this encoding is used to
         decode.

-    decode_error : {'strict', 'ignore', 'replace'} (default='strict')
+    decode_error : {'strict', 'ignore', 'replace'}, default='strict'
         Instruction on what to do if a byte sequence is given to analyze that
         contains characters not of the given `encoding`. By default, it is
         'strict', meaning that a UnicodeDecodeError will be raised. Other
         values are 'ignore' and 'replace'.

-    strip_accents : {'ascii', 'unicode', None} (default=None)
+    strip_accents : {'ascii', 'unicode'}, default=None
         Remove accents and perform other character normalization
         during the preprocessing step.
         'ascii' is a fast method that only works on characters that have
@@ -1384,19 +1745,20 @@
         Both 'ascii' and 'unicode' use NFKD normalization from
         :func:`unicodedata.normalize`.

-    lowercase : boolean (default=True)
+    lowercase : bool, default=True
         Convert all characters to lowercase before tokenizing.

-    preprocessor : callable or None (default=None)
+    preprocessor : callable, default=None
         Override the preprocessing (string transformation) stage while
         preserving the tokenizing and n-grams generation steps.
-
-    tokenizer : callable or None (default=None)
+        Only applies if ``analyzer`` is not callable.
+
+    tokenizer : callable, default=None
         Override the string tokenization step while preserving the
         preprocessing and n-grams generation steps.
         Only applies if ``analyzer == 'word'``.

-    analyzer : string, {'word', 'char', 'char_wb'} or callable
+    analyzer : {'word', 'char', 'char_wb'} or callable, default='word'
         Whether the feature should be made of word or character n-grams.
         Option 'char_wb' creates character n-grams only from text inside
         word boundaries; n-grams at the edges of words are padded with space.
@@ -1405,11 +1767,11 @@
         out of the raw, unprocessed input.

         .. versionchanged:: 0.21
-        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is
-        first read from the file and then passed to the given callable
-        analyzer.
-
-    stop_words : string {'english'}, list, or None (default=None)
+            Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data
+            is first read from the file and then passed to the given callable
+            analyzer.
+
+    stop_words : {'english'}, list, default=None
         If a string, it is passed to _check_stop_list and the appropriate stop
         list is returned. 'english' is currently the only supported string
         value.
@@ -1424,69 +1786,77 @@
         in the range [0.7, 1.0) to automatically detect and filter stop
         words based on intra corpus document frequency of terms.

-    token_pattern : string
+    token_pattern : str, default=r"(?u)\\b\\w\\w+\\b"
         Regular expression denoting what constitutes a "token", only used
         if ``analyzer == 'word'``. The default regexp selects tokens of 2
         or more alphanumeric characters (punctuation is completely ignored
         and always treated as a token separator).

-    ngram_range : tuple (min_n, max_n) (default=(1, 1))
+        If there is a capturing group in token_pattern then the
+        captured group content, not the entire match, becomes the token.
+        At most one capturing group is permitted.
+
+    ngram_range : tuple (min_n, max_n), default=(1, 1)
         The lower and upper boundary of the range of n-values for different
         n-grams to be extracted. All values of n such that min_n <= n <= max_n
-        will be used.
-
-    max_df : float in range [0.0, 1.0] or int (default=1.0)
+        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only
+        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means
+        only bigrams.
+        Only applies if ``analyzer`` is not callable.
+
+    max_df : float or int, default=1.0
         When building the vocabulary ignore terms that have a document
         frequency strictly higher than the given threshold (corpus-specific
         stop words).
-        If float, the parameter represents a proportion of documents, integer
-        absolute counts.
+        If float in range [0.0, 1.0], the parameter represents a proportion of
+        documents, integer absolute counts.
         This parameter is ignored if vocabulary is not None.

-    min_df : float in range [0.0, 1.0] or int (default=1)
+    min_df : float or int, default=1
         When building the vocabulary ignore terms that have a document
         frequency strictly lower than the given threshold. This value is also
         called cut-off in the literature.
-        If float, the parameter represents a proportion of documents, integer
-        absolute counts.
+        If float in range of [0.0, 1.0], the parameter represents a proportion
+        of documents, integer absolute counts.
         This parameter is ignored if vocabulary is not None.

-    max_features : int or None (default=None)
+    max_features : int, default=None
         If not None, build a vocabulary that only consider the top
         max_features ordered by term frequency across the corpus.

         This parameter is ignored if vocabulary is not None.

-    vocabulary : Mapping or iterable, optional (default=None)
+    vocabulary : Mapping or iterable, default=None
         Either a Mapping (e.g., a dict) where keys are terms and values are
         indices in the feature matrix, or an iterable over terms. If not
         given, a vocabulary is determined from the input documents.

-    binary : boolean (default=False)
+    binary : bool, default=False
         If True, all non-zero term counts are set to 1. This does not mean
         outputs will have only 0/1 values, only that the tf term in tf-idf
-        is binary. (Set idf and normalization to False to get 0/1 outputs.)
-
-    dtype : type, optional (default=float64)
+        is binary. (Set idf and normalization to False to get 0/1 outputs).
+
+    dtype : dtype, default=float64
         Type of the matrix returned by fit_transform() or transform().

-    norm : 'l1', 'l2' or None, optional (default='l2')
+    norm : {'l1', 'l2'}, default='l2'
         Each output row will have unit norm, either:
-        * 'l2': Sum of squares of vector elements is 1. The cosine
-        similarity between two vectors is their dot product when l2 norm has
-        been applied.
-        * 'l1': Sum of absolute values of vector elements is 1.
-        See :func:`preprocessing.normalize`
-
-    use_idf : boolean (default=True)
-        Enable inverse-document-frequency reweighting.
-
-    smooth_idf : boolean (default=True)
+
+        - 'l2': Sum of squares of vector elements is 1. The cosine
+          similarity between two vectors is their dot product when l2 norm has
+          been applied.
+        - 'l1': Sum of absolute values of vector elements is 1.
+          See :func:`preprocessing.normalize`.
+
+    use_idf : bool, default=True
+        Enable inverse-document-frequency reweighting. If False, idf(t) = 1.
+
+    smooth_idf : bool, default=True
         Smooth idf weights by adding one to document frequencies, as if an
         extra document was seen containing every term in the collection
         exactly once. Prevents zero divisions.

-    sublinear_tf : boolean (default=False)
+    sublinear_tf : bool, default=False
         Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).

     Attributes
@@ -1494,7 +1864,11 @@
     vocabulary_ : dict
         A mapping of terms to feature indices.

-    idf_ : array, shape (n_features)
+    fixed_vocabulary_ : bool
+        True if a fixed vocabulary of term to indices mapping
+        is provided by the user.
+
+    idf_ : array of shape (n_features,)
         The inverse document frequency (IDF) vector; only defined
         if ``use_idf`` is True.

@@ -1506,6 +1880,19 @@
           - were cut off by feature selection (`max_features`).

         This is only available if no vocabulary was given.
+
+    See Also
+    --------
+    CountVectorizer : Transforms text into a sparse matrix of n-gram counts.
+
+    TfidfTransformer : Performs the TF-IDF transformation from a provided
+        matrix of counts.
+
+    Notes
+    -----
+    The ``stop_words_`` attribute can get large and increase the model size
+    when pickling. This attribute is provided only for introspection and can
+    be safely removed using delattr or set to None before pickling.

     Examples
     --------
@@ -1518,102 +1905,111 @@
     ... ]
     >>> vectorizer = TfidfVectorizer()
     >>> X = vectorizer.fit_transform(corpus)
-    >>> print(vectorizer.get_feature_names())
-    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
+    >>> vectorizer.get_feature_names_out()
+    array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',
+           'this'], ...)
     >>> print(X.shape)
     (4, 9)
-
-    See also
-    --------
-    CountVectorizer : Transforms text into a sparse matrix of n-gram counts.
-
-    TfidfTransformer : Performs the TF-IDF transformation from a provided
-        matrix of counts.
-
-    Notes
-    -----
-    The ``stop_words_`` attribute can get large and increase the model size
-    when pickling. This attribute is provided only for introspection and can
-    be safely removed using delattr or set to None before pickling.
     """

-    def __init__(self, input='content', encoding='utf-8',
-                 decode_error='strict', strip_accents=None, lowercase=True,
-                 preprocessor=None, tokenizer=None, analyzer='word',
-                 stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
-                 ngram_range=(1, 1), max_df=1.0, min_df=1,
-                 max_features=None, vocabulary=None, binary=False,
-                 dtype=np.float64, norm='l2', use_idf=True, smooth_idf=True,
-                 sublinear_tf=False):
+    def __init__(
+        self,
+        *,
+        input="content",
+        encoding="utf-8",
+        decode_error="strict",
+        strip_accents=None,
+        lowercase=True,
+        preprocessor=None,
+        tokenizer=None,
+        analyzer="word",
+        stop_words=None,
+        token_pattern=r"(?u)\b\w\w+\b",
+        ngram_range=(1, 1),
+        max_df=1.0,
+        min_df=1,
+        max_features=None,
+        vocabulary=None,
+        binary=False,
+        dtype=np.float64,
+        norm="l2",
+        use_idf=True,
+        smooth_idf=True,
+        sublinear_tf=False,
+    ):

         super().__init__(
-            input=input, encoding=encoding, decode_error=decode_error,
-            strip_accents=strip_accents, lowercase=lowercase,
-            preprocessor=preprocessor, tokenizer=tokenizer, analyzer=analyzer,
-            stop_words=stop_words, token_pattern=token_pattern,
-            ngram_range=ngram_range, max_df=max_df, min_df=min_df,
-            max_features=max_features, vocabulary=vocabulary, binary=binary,
-            dtype=dtype)
-
-        self._tfidf = TfidfTransformer(norm=norm, use_idf=use_idf,
-                                       smooth_idf=smooth_idf,
-                                       sublinear_tf=sublinear_tf)
+            input=input,
+            encoding=encoding,
+            decode_error=decode_error,
+            strip_accents=strip_accents,
+            lowercase=lowercase,
+            preprocessor=preprocessor,
+            tokenizer=tokenizer,
+            analyzer=analyzer,
+            stop_words=stop_words,
+            token_pattern=token_pattern,
+            ngram_range=ngram_range,
+            max_df=max_df,
+            min_df=min_df,
+            max_features=max_features,
+            vocabulary=vocabulary,
+            binary=binary,
+            dtype=dtype,
+        )
+        self.norm = norm
+        self.use_idf = use_idf
+        self.smooth_idf = smooth_idf
+        self.sublinear_tf = sublinear_tf

     # Broadcast the TF-IDF parameters to the underlying transformer instance
     # for easy grid search and repr

     @property
-    def norm(self):
-        return self._tfidf.norm
-
-    @norm.setter
-    def norm(self, value):
-        self._tfidf.norm = value
-
-    @property
-    def use_idf(self):
-        return self._tfidf.use_idf
-
-    @use_idf.setter
-    def use_idf(self, value):
-        self._tfidf.use_idf = value
-
-    @property
-    def smooth_idf(self):
-        return self._tfidf.smooth_idf
-
-    @smooth_idf.setter
-    def smooth_idf(self, value):
-        self._tfidf.smooth_idf = value
-
-    @property
-    def sublinear_tf(self):
-        return self._tfidf.sublinear_tf
-
-    @sublinear_tf.setter
-    def sublinear_tf(self, value):
-        self._tfidf.sublinear_tf = value
-
-    @property
     def idf_(self):
+        """Inverse document frequency vector, only defined if `use_idf=True`.
+
+        Returns
+        -------
+        ndarray of shape (n_features,)
+        """
+        if not hasattr(self, "_tfidf"):
+            raise NotFittedError(
+                f"{self.__class__.__name__} is not fitted yet. Call 'fit' with "
+                "appropriate arguments before using this attribute."
+            )
         return self._tfidf.idf_

     @idf_.setter
     def idf_(self, value):
+        if not self.use_idf:
+            raise ValueError("`idf_` cannot be set when `user_idf=False`.")
+        if not hasattr(self, "_tfidf"):
+            # We should support transferring `idf_` from another `TfidfTransformer`
+            # and therefore, we need to create the transformer instance it does not
+            # exist yet.
+            self._tfidf = TfidfTransformer(
+                norm=self.norm,
+                use_idf=self.use_idf,
+                smooth_idf=self.smooth_idf,
+                sublinear_tf=self.sublinear_tf,
+            )
         self._validate_vocabulary()
-        if hasattr(self, 'vocabulary_'):
+        if hasattr(self, "vocabulary_"):
             if len(self.vocabulary_) != len(value):
-                raise ValueError("idf length = %d must be equal "
-                                 "to vocabulary size = %d" %
-                                 (len(value), len(self.vocabulary)))
+                raise ValueError(
+                    "idf length = %d must be equal to vocabulary size = %d"
+                    % (len(value), len(self.vocabulary))
+                )
         self._tfidf.idf_ = value

     def _check_params(self):
         if self.dtype not in FLOAT_DTYPES:
-            warnings.warn("Only {} 'dtype' should be used. {} 'dtype' will "
-                          "be converted to np.float64."
-                          .format(FLOAT_DTYPES, self.dtype),
-                          UserWarning)
+            warnings.warn(
+                "Only {} 'dtype' should be used. {} 'dtype' will "
+                "be converted to np.float64.".format(FLOAT_DTYPES, self.dtype),
+                UserWarning,
+            )

     def fit(self, raw_documents, y=None):
         """Learn vocabulary and idf from training set.
@@ -1621,19 +2017,30 @@
         Parameters
         ----------
         raw_documents : iterable
-            an iterable which yields either str, unicode or file objects
-
-        Returns
-        -------
-        self : TfidfVectorizer
+            An iterable which generates either str, unicode or file objects.
+
+        y : None
+            This parameter is not needed to compute tfidf.
+
+        Returns
+        -------
+        self : object
+            Fitted vectorizer.
         """
         self._check_params()
+        self._warn_for_unused_params()
+        self._tfidf = TfidfTransformer(
+            norm=self.norm,
+            use_idf=self.use_idf,
+            smooth_idf=self.smooth_idf,
+            sublinear_tf=self.sublinear_tf,
+        )
         X = super().fit_transform(raw_documents)
         self._tfidf.fit(X)
         return self

     def fit_transform(self, raw_documents, y=None):
-        """Learn vocabulary and idf, return term-document matrix.
+        """Learn vocabulary and idf, return document-term matrix.

         This is equivalent to fit followed by transform, but more efficiently
         implemented.
@@ -1641,21 +2048,30 @@
         Parameters
         ----------
         raw_documents : iterable
-            an iterable which yields either str, unicode or file objects
-
-        Returns
-        -------
-        X : sparse matrix, [n_samples, n_features]
+            An iterable which generates either str, unicode or file objects.
+
+        y : None
+            This parameter is ignored.
+
+        Returns
+        -------
+        X : sparse matrix of (n_samples, n_features)
             Tf-idf-weighted document-term matrix.
         """
         self._check_params()
+        self._tfidf = TfidfTransformer(
+            norm=self.norm,
+            use_idf=self.use_idf,
+            smooth_idf=self.smooth_idf,
+            sublinear_tf=self.sublinear_tf,
+        )
         X = super().fit_transform(raw_documents)
         self._tfidf.fit(X)
         # X is already a transformed view of raw_documents so
         # we set copy to False
         return self._tfidf.transform(X, copy=False)

-    def transform(self, raw_documents, copy=True):
+    def transform(self, raw_documents):
         """Transform documents to document-term matrix.

         Uses the vocabulary and document frequencies (df) learned by fit (or
@@ -1664,21 +2080,17 @@
         Parameters
         ----------
         raw_documents : iterable
-            an iterable which yields either str, unicode or file objects
-
-        copy : boolean, default True
-            Whether to copy X and operate on the copy or perform in-place
-            operations.
-
-        Returns
-        -------
-        X : sparse matrix, [n_samples, n_features]
+            An iterable which generates either str, unicode or file objects.
+
+        Returns
+        -------
+        X : sparse matrix of (n_samples, n_features)
             Tf-idf-weighted document-term matrix.
         """
-        check_is_fitted(self, '_tfidf', 'The tfidf vector is not fitted')
+        check_is_fitted(self, msg="The TF-IDF vectorizer is not fitted")

         X = super().transform(raw_documents)
         return self._tfidf.transform(X, copy=False)

     def _more_tags(self):
-        return {'X_types': ['string'], '_skip_test': True}
+        return {"X_types": ["string"], "_skip_test": True}
('sklearn/feature_extraction', 'image.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -18,11 +18,13 @@
 from ..utils import check_array, check_random_state
 from ..base import BaseEstimator

-__all__ = ['PatchExtractor',
-           'extract_patches_2d',
-           'grid_to_graph',
-           'img_to_graph',
-           'reconstruct_from_patches_2d']
+__all__ = [
+    "PatchExtractor",
+    "extract_patches_2d",
+    "grid_to_graph",
+    "img_to_graph",
+    "reconstruct_from_patches_2d",
+]

 ###############################################################################
 # From an image to a graph
@@ -33,18 +35,16 @@

     Parameters
     ----------
-    n_x : integer
+    n_x : int
         The size of the grid in the x direction.
-    n_y : integer
+    n_y : int
         The size of the grid in the y direction.
-    n_z : integer, optional
+    n_z : integer, default=1
         The size of the grid in the z direction, defaults to 1
     """
     vertices = np.arange(n_x * n_y * n_z).reshape((n_x, n_y, n_z))
-    edges_deep = np.vstack((vertices[:, :, :-1].ravel(),
-                            vertices[:, :, 1:].ravel()))
-    edges_right = np.vstack((vertices[:, :-1].ravel(),
-                             vertices[:, 1:].ravel()))
+    edges_deep = np.vstack((vertices[:, :, :-1].ravel(), vertices[:, :, 1:].ravel()))
+    edges_right = np.vstack((vertices[:, :-1].ravel(), vertices[:, 1:].ravel()))
     edges_down = np.vstack((vertices[:-1].ravel(), vertices[1:].ravel()))
     edges = np.hstack((edges_deep, edges_right, edges_down))
     return edges
@@ -52,23 +52,29 @@

 def _compute_gradient_3d(edges, img):
     _, n_y, n_z = img.shape
-    gradient = np.abs(img[edges[0] // (n_y * n_z),
-                      (edges[0] % (n_y * n_z)) // n_z,
-                      (edges[0] % (n_y * n_z)) % n_z] -
-                      img[edges[1] // (n_y * n_z),
-                      (edges[1] % (n_y * n_z)) // n_z,
-                      (edges[1] % (n_y * n_z)) % n_z])
+    gradient = np.abs(
+        img[
+            edges[0] // (n_y * n_z),
+            (edges[0] % (n_y * n_z)) // n_z,
+            (edges[0] % (n_y * n_z)) % n_z,
+        ]
+        - img[
+            edges[1] // (n_y * n_z),
+            (edges[1] % (n_y * n_z)) // n_z,
+            (edges[1] % (n_y * n_z)) % n_z,
+        ]
+    )
     return gradient


 # XXX: Why mask the image after computing the weights?
+

 def _mask_edges_weights(mask, edges, weights=None):
     """Apply a mask to edges (weighted or not)"""
     inds = np.arange(mask.size)
     inds = inds[mask.ravel()]
-    ind_mask = np.logical_and(np.in1d(edges[0], inds),
-                              np.in1d(edges[1], inds))
+    ind_mask = np.logical_and(np.in1d(edges[0], inds), np.in1d(edges[1], inds))
     edges = edges[:, ind_mask]
     if weights is not None:
         weights = weights[ind_mask]
@@ -76,7 +82,7 @@
         maxval = edges.max()
     else:
         maxval = 0
-    order = np.searchsorted(np.unique(edges.ravel()), np.arange(maxval + 1))
+    order = np.searchsorted(np.flatnonzero(mask), np.arange(maxval + 1))
     edges = order[edges]
     if weights is None:
         return edges
@@ -84,15 +90,15 @@
         return edges, weights


-def _to_graph(n_x, n_y, n_z, mask=None, img=None,
-              return_as=sparse.coo_matrix, dtype=None):
-    """Auxiliary function for img_to_graph and grid_to_graph
-    """
+def _to_graph(
+    n_x, n_y, n_z, mask=None, img=None, return_as=sparse.coo_matrix, dtype=None
+):
+    """Auxiliary function for img_to_graph and grid_to_graph"""
     edges = _make_edges_3d(n_x, n_y, n_z)

     if dtype is None:
         if img is None:
-            dtype = np.int
+            dtype = int
         else:
             dtype = img.dtype

@@ -107,8 +113,8 @@
         n_voxels = diag.size
     else:
         if mask is not None:
-            mask = mask.astype(dtype=np.bool, copy=False)
-            mask = np.asarray(mask, dtype=np.bool)
+            mask = mask.astype(dtype=bool, copy=False)
+            mask = np.asarray(mask, dtype=bool)
             edges = _mask_edges_weights(mask, edges)
             n_voxels = np.sum(mask)
         else:
@@ -119,17 +125,20 @@
     diag_idx = np.arange(n_voxels)
     i_idx = np.hstack((edges[0], edges[1]))
     j_idx = np.hstack((edges[1], edges[0]))
-    graph = sparse.coo_matrix((np.hstack((weights, weights, diag)),
-                              (np.hstack((i_idx, diag_idx)),
-                               np.hstack((j_idx, diag_idx)))),
-                              (n_voxels, n_voxels),
-                              dtype=dtype)
+    graph = sparse.coo_matrix(
+        (
+            np.hstack((weights, weights, diag)),
+            (np.hstack((i_idx, diag_idx)), np.hstack((j_idx, diag_idx))),
+        ),
+        (n_voxels, n_voxels),
+        dtype=dtype,
+    )
     if return_as is np.ndarray:
         return graph.toarray()
     return return_as(graph)


-def img_to_graph(img, mask=None, return_as=sparse.coo_matrix, dtype=None):
+def img_to_graph(img, *, mask=None, return_as=sparse.coo_matrix, dtype=None):
     """Graph of the pixel-to-pixel gradient connections

     Edges are weighted with the gradient values.
@@ -138,14 +147,16 @@

     Parameters
     ----------
-    img : ndarray, 2D or 3D
-        2D or 3D image
-    mask : ndarray of booleans, optional
+    img : ndarray of shape (height, width) or (height, width, channel)
+        2D or 3D image.
+    mask : ndarray of shape (height, width) or \
+            (height, width, channel), dtype=bool, default=None
         An optional mask of the image, to consider only part of the
         pixels.
-    return_as : np.ndarray or a sparse matrix class, optional
+    return_as : np.ndarray or a sparse matrix class, \
+            default=sparse.coo_matrix
         The class to use to build the returned adjacency matrix.
-    dtype : None or dtype, optional
+    dtype : dtype, default=None
         The data of the returned sparse matrix. By default it is the
         dtype of img

@@ -163,27 +174,34 @@
     return _to_graph(n_x, n_y, n_z, mask, img, return_as, dtype)


-def grid_to_graph(n_x, n_y, n_z=1, mask=None, return_as=sparse.coo_matrix,
-                  dtype=np.int):
-    """Graph of the pixel-to-pixel connections
+def grid_to_graph(
+    n_x, n_y, n_z=1, *, mask=None, return_as=sparse.coo_matrix, dtype=int
+):
+    """Graph of the pixel-to-pixel connections.

     Edges exist if 2 voxels are connected.

     Parameters
     ----------
     n_x : int
-        Dimension in x axis
+        Dimension in x axis.
     n_y : int
-        Dimension in y axis
-    n_z : int, optional, default 1
-        Dimension in z axis
-    mask : ndarray of booleans, optional
+        Dimension in y axis.
+    n_z : int, default=1
+        Dimension in z axis.
+    mask : ndarray of shape (n_x, n_y, n_z), dtype=bool, default=None
         An optional mask of the image, to consider only part of the
         pixels.
-    return_as : np.ndarray or a sparse matrix class, optional
+    return_as : np.ndarray or a sparse matrix class, \
+            default=sparse.coo_matrix
         The class to use to build the returned adjacency matrix.
-    dtype : dtype, optional, default int
-        The data of the returned sparse matrix. By default it is int
+    dtype : dtype, default=int
+        The data of the returned sparse matrix. By default it is int.
+
+    Returns
+    -------
+    graph : np.ndarray or a sparse matrix class
+        The computed adjacency matrix.

     Notes
     -----
@@ -194,12 +212,12 @@
     For compatibility, user code relying on this method should wrap its
     calls in ``np.asarray`` to avoid type issues.
     """
-    return _to_graph(n_x, n_y, n_z, mask=mask, return_as=return_as,
-                     dtype=dtype)
+    return _to_graph(n_x, n_y, n_z, mask=mask, return_as=return_as, dtype=dtype)


 ###############################################################################
 # From an image to a set of small image patches
+

 def _compute_n_patches(i_h, i_w, p_h, p_w, max_patches=None):
     """Compute the number of patches that will be extracted in an image.
@@ -216,7 +234,7 @@
         The height of a patch
     p_w : int
         The width of a patch
-    max_patches : integer or float, optional default is None
+    max_patches : int or float, default=None
         The maximum number of patches to extract. If max_patches is a float
         between 0 and 1, it is taken to be a proportion of the total number
         of patches.
@@ -226,14 +244,11 @@
     all_patches = n_h * n_w

     if max_patches:
-        if (isinstance(max_patches, (numbers.Integral))
-                and max_patches < all_patches):
+        if isinstance(max_patches, (numbers.Integral)) and max_patches < all_patches:
             return max_patches
-        elif (isinstance(max_patches, (numbers.Integral))
-              and max_patches >= all_patches):
+        elif isinstance(max_patches, (numbers.Integral)) and max_patches >= all_patches:
             return all_patches
-        elif (isinstance(max_patches, (numbers.Real))
-                and 0 < max_patches < 1):
+        elif isinstance(max_patches, (numbers.Real)) and 0 < max_patches < 1:
             return int(max_patches * all_patches)
         else:
             raise ValueError("Invalid value for max_patches: %r" % max_patches)
@@ -241,7 +256,7 @@
         return all_patches


-def extract_patches(arr, patch_shape=8, extraction_step=1):
+def _extract_patches(arr, patch_shape=8, extraction_step=1):
     """Extracts patches of any n-dimensional array in place using strides.

     Given an n-dimensional array it will return a 2n-dimensional array with
@@ -257,12 +272,12 @@
     arr : ndarray
         n-dimensional array of which patches are to be extracted

-    patch_shape : integer or tuple of length arr.ndim
+    patch_shape : int or tuple of length arr.ndim.default=8
         Indicates the shape of the patches to be extracted. If an
         integer is given, the shape will be a hypercube of
         sidelength given by its value.

-    extraction_step : integer or tuple of length arr.ndim
+    extraction_step : int or tuple of length arr.ndim, default=1
         Indicates step size at which extraction shall be performed.
         If integer is given, then the step is uniform in all dimensions.

@@ -289,8 +304,9 @@
     slices = tuple(slice(None, None, st) for st in extraction_step)
     indexing_strides = arr[slices].strides

-    patch_indices_shape = ((np.array(arr.shape) - np.array(patch_shape)) //
-                           np.array(extraction_step)) + 1
+    patch_indices_shape = (
+        (np.array(arr.shape) - np.array(patch_shape)) // np.array(extraction_step)
+    ) + 1

     shape = tuple(list(patch_indices_shape) + list(patch_shape))
     strides = tuple(list(indexing_strides) + list(patch_strides))
@@ -299,7 +315,7 @@
     return patches


-def extract_patches_2d(image, patch_size, max_patches=None, random_state=None):
+def extract_patches_2d(image, patch_size, *, max_patches=None, random_state=None):
     """Reshape a 2D image into a collection of patches

     The resulting patches are allocated in a dedicated array.
@@ -308,29 +324,28 @@

     Parameters
     ----------
-    image : array, shape = (image_height, image_width) or
+    image : ndarray of shape (image_height, image_width) or \
         (image_height, image_width, n_channels)
         The original image data. For color images, the last dimension specifies
         the channel: a RGB image would have `n_channels=3`.

-    patch_size : tuple of ints (patch_height, patch_width)
-        the dimensions of one patch
-
-    max_patches : integer or float, optional default is None
-        The maximum number of patches to extract. If max_patches is a float
+    patch_size : tuple of int (patch_height, patch_width)
+        The dimensions of one patch.
+
+    max_patches : int or float, default=None
+        The maximum number of patches to extract. If `max_patches` is a float
         between 0 and 1, it is taken to be a proportion of the total number
         of patches.

-    random_state : int, RandomState instance or None, optional (default=None)
-        Pseudo number generator state used for random sampling to use if
-        `max_patches` is not None.  If int, random_state is the seed used by
-        the random number generator; If RandomState instance, random_state is
-        the random number generator; If None, the random number generator is
-        the RandomState instance used by `np.random`.
+    random_state : int, RandomState instance, default=None
+        Determines the random number generator used for random sampling when
+        `max_patches` is not None. Use an int to make the randomness
+        deterministic.
+        See :term:`Glossary <random_state>`.

     Returns
     -------
-    patches : array, shape = (n_patches, patch_height, patch_width) or
+    patches : array of shape (n_patches, patch_height, patch_width) or \
         (n_patches, patch_height, patch_width, n_channels)
         The collection of patches extracted from the image, where `n_patches`
         is either `max_patches` or the total number of patches that can be
@@ -348,12 +363,12 @@
     >>> print('Patches shape: {}'.format(patches.shape))
     Patches shape: (272214, 2, 2, 3)
     >>> # Here are just two of these patches:
-    >>> print(patches[1]) # doctest: +NORMALIZE_WHITESPACE
+    >>> print(patches[1])
     [[[174 201 231]
       [174 201 231]]
      [[173 200 230]
       [173 200 230]]]
-    >>> print(patches[800])# doctest: +NORMALIZE_WHITESPACE
+    >>> print(patches[800])
     [[[187 214 243]
       [188 215 244]]
      [[187 214 243]
@@ -363,20 +378,22 @@
     p_h, p_w = patch_size

     if p_h > i_h:
-        raise ValueError("Height of the patch should be less than the height"
-                         " of the image.")
+        raise ValueError(
+            "Height of the patch should be less than the height of the image."
+        )

     if p_w > i_w:
-        raise ValueError("Width of the patch should be less than the width"
-                         " of the image.")
+        raise ValueError(
+            "Width of the patch should be less than the width of the image."
+        )

     image = check_array(image, allow_nd=True)
     image = image.reshape((i_h, i_w, -1))
     n_colors = image.shape[-1]

-    extracted_patches = extract_patches(image,
-                                        patch_shape=(p_h, p_w, n_colors),
-                                        extraction_step=1)
+    extracted_patches = _extract_patches(
+        image, patch_shape=(p_h, p_w, n_colors), extraction_step=1
+    )

     n_patches = _compute_n_patches(i_h, i_w, p_h, p_w, max_patches)
     if max_patches:
@@ -406,20 +423,20 @@

     Parameters
     ----------
-    patches : array, shape = (n_patches, patch_height, patch_width) or
+    patches : ndarray of shape (n_patches, patch_height, patch_width) or \
         (n_patches, patch_height, patch_width, n_channels)
         The complete set of patches. If the patches contain colour information,
         channels are indexed along the last dimension: RGB patches would
         have `n_channels=3`.

-    image_size : tuple of ints (image_height, image_width) or
+    image_size : tuple of int (image_height, image_width) or \
         (image_height, image_width, n_channels)
-        the size of the image that will be reconstructed
+        The size of the image that will be reconstructed.

     Returns
     -------
-    image : array, shape = image_size
-        the reconstructed image
+    image : ndarray of shape image_size
+        The reconstructed image.
     """
     i_h, i_w = image_size[:2]
     p_h, p_w = patches.shape[1:3]
@@ -428,37 +445,42 @@
     n_h = i_h - p_h + 1
     n_w = i_w - p_w + 1
     for p, (i, j) in zip(patches, product(range(n_h), range(n_w))):
-        img[i:i + p_h, j:j + p_w] += p
+        img[i : i + p_h, j : j + p_w] += p

     for i in range(i_h):
         for j in range(i_w):
             # divide by the amount of overlap
             # XXX: is this the most efficient way? memory-wise yes, cpu wise?
-            img[i, j] /= float(min(i + 1, p_h, i_h - i) *
-                               min(j + 1, p_w, i_w - j))
+            img[i, j] /= float(min(i + 1, p_h, i_h - i) * min(j + 1, p_w, i_w - j))
     return img


 class PatchExtractor(BaseEstimator):
-    """Extracts patches from a collection of images
+    """Extracts patches from a collection of images.

     Read more in the :ref:`User Guide <image_feature_extraction>`.

-    Parameters
-    ----------
-    patch_size : tuple of ints (patch_height, patch_width)
-        the dimensions of one patch
-
-    max_patches : integer or float, optional default is None
-        The maximum number of patches per image to extract. If max_patches is a
-        float in (0, 1), it is taken to mean a proportion of the total number
+    .. versionadded:: 0.9
+
+    Parameters
+    ----------
+    patch_size : tuple of int (patch_height, patch_width), default=None
+        The dimensions of one patch.
+
+    max_patches : int or float, default=None
+        The maximum number of patches per image to extract. If `max_patches` is
+        a float in (0, 1), it is taken to mean a proportion of the total number
         of patches.

-    random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance, default=None
+        Determines the random number generator used for random sampling when
+        `max_patches is not None`. Use an int to make the randomness
+        deterministic.
+        See :term:`Glossary <random_state>`.
+
+    See Also
+    --------
+    reconstruct_from_patches_2d : Reconstruct image from all of its patches.

     Examples
     --------
@@ -475,30 +497,38 @@
     Patches shape: (545706, 2, 2)
     """

-    def __init__(self, patch_size=None, max_patches=None, random_state=None):
+    def __init__(self, *, patch_size=None, max_patches=None, random_state=None):
         self.patch_size = patch_size
         self.max_patches = max_patches
         self.random_state = random_state

     def fit(self, X, y=None):
-        """Do nothing and return the estimator unchanged
+        """Do nothing and return the estimator unchanged.

         This method is just there to implement the usual API and hence
         work in pipelines.

         Parameters
         ----------
-        X : array-like, shape [n_samples, n_features]
+        X : array-like of shape (n_samples, n_features)
             Training data.
+
+        y : Ignored
+            Not used, present for API consistency by convention.
+
+        Returns
+        -------
+        self : object
+            Returns the instance itself.
         """
         return self

     def transform(self, X):
-        """Transforms the image samples in X into a matrix of patch data.
+        """Transform the image samples in `X` into a matrix of patch data.

         Parameters
         ----------
-        X : array, shape = (n_samples, image_height, image_width) or
+        X : ndarray of shape (n_samples, image_height, image_width) or \
             (n_samples, image_height, image_width, n_channels)
             Array of images from which to extract patches. For color images,
             the last dimension specifies the channel: a RGB image would have
@@ -506,7 +536,7 @@

         Returns
         -------
-        patches : array, shape = (n_patches, patch_height, patch_width) or
+        patches : array of shape (n_patches, patch_height, patch_width) or \
              (n_patches, patch_height, patch_width, n_channels)
              The collection of patches extracted from the images, where
              `n_patches` is either `n_samples * max_patches` or the total
@@ -531,9 +561,13 @@
         # extract the patches
         patches = np.empty(patches_shape)
         for ii, image in enumerate(X):
-            patches[ii * n_patches:(ii + 1) * n_patches] = extract_patches_2d(
-                image, patch_size, self.max_patches, self.random_state)
+            patches[ii * n_patches : (ii + 1) * n_patches] = extract_patches_2d(
+                image,
+                patch_size,
+                max_patches=self.max_patches,
+                random_state=self.random_state,
+            )
         return patches

     def _more_tags(self):
-        return {'X_types': ['3darray']}
+        return {"X_types": ["3darray"]}
('sklearn/__check_build', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -24,11 +24,12 @@
         msg = INPLACE_MSG
     dir_content = list()
     for i, filename in enumerate(os.listdir(local_dir)):
-        if ((i + 1) % 3):
+        if (i + 1) % 3:
             dir_content.append(filename.ljust(26))
         else:
-            dir_content.append(filename + '\n')
-    raise ImportError("""%s
+            dir_content.append(filename + "\n")
+    raise ImportError(
+        """%s
 ___________________________________________________________________________
 Contents of %s:
 %s
@@ -38,7 +39,10 @@
 If you have installed scikit-learn from source, please do not forget
 to build the package before using it: run `python setup.py install` or
 `make` in the source directory.
-%s""" % (e, local_dir, ''.join(dir_content).strip(), msg))
+%s"""
+        % (e, local_dir, "".join(dir_content).strip(), msg)
+    )
+

 try:
     from ._check_build import check_build  # noqa
('sklearn/__check_build', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,15 +4,18 @@
 import numpy


-def configuration(parent_package='', top_path=None):
+def configuration(parent_package="", top_path=None):
     from numpy.distutils.misc_util import Configuration
-    config = Configuration('__check_build', parent_package, top_path)
-    config.add_extension('_check_build',
-                         sources=['_check_build.pyx'],
-                         include_dirs=[numpy.get_include()])
+
+    config = Configuration("__check_build", parent_package, top_path)
+    config.add_extension(
+        "_check_build", sources=["_check_build.pyx"], include_dirs=[numpy.get_include()]
+    )

     return config

-if __name__ == '__main__':
+
+if __name__ == "__main__":
     from numpy.distutils.core import setup
-    setup(**configuration(top_path='').todict())
+
+    setup(**configuration(top_path="").todict())
('sklearn/semi_supervised', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,10 +1,11 @@
 """
 The :mod:`sklearn.semi_supervised` module implements semi-supervised learning
-algorithms. These algorithms utilized small amounts of labeled data and large
+algorithms. These algorithms utilize small amounts of labeled data and large
 amounts of unlabeled data for classification tasks. This module includes Label
 Propagation.
 """

-from .label_propagation import LabelPropagation, LabelSpreading
+from ._label_propagation import LabelPropagation, LabelSpreading
+from ._self_training import SelfTrainingClassifier

-__all__ = ['LabelPropagation', 'LabelSpreading']
+__all__ = ["SelfTrainingClassifier", "LabelPropagation", "LabelSpreading"]
('sklearn/gaussian_process', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,5 +1,3 @@
-# -*- coding: utf-8 -*-
-
 # Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 #         Vincent Dubourg <vincent.dubourg@gmail.com>
 #         (mostly translation, see implementation details)
@@ -10,13 +8,9 @@
 based regression and classification.
 """

-from .gpr import GaussianProcessRegressor
-from .gpc import GaussianProcessClassifier
+from ._gpr import GaussianProcessRegressor
+from ._gpc import GaussianProcessClassifier
 from . import kernels

-from . import correlation_models
-from . import regression_models

-__all__ = ['correlation_models', 'regression_models',
-           'GaussianProcessRegressor', 'GaussianProcessClassifier',
-           'kernels']
+__all__ = ["GaussianProcessRegressor", "GaussianProcessClassifier", "kernels"]
('sklearn/gaussian_process', 'kernels.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -30,6 +30,10 @@

 from ..metrics.pairwise import pairwise_kernels
 from ..base import clone
+from ..utils.validation import _num_samples
+from ..exceptions import ConvergenceWarning
+
+import warnings


 def _check_length_scale(X, length_scale):
@@ -37,27 +41,30 @@
     if np.ndim(length_scale) > 1:
         raise ValueError("length_scale cannot be of dimension greater than 1")
     if np.ndim(length_scale) == 1 and X.shape[1] != length_scale.shape[0]:
-        raise ValueError("Anisotropic kernel must have the same number of "
-                         "dimensions as data (%d!=%d)"
-                         % (length_scale.shape[0], X.shape[1]))
+        raise ValueError(
+            "Anisotropic kernel must have the same number of "
+            "dimensions as data (%d!=%d)" % (length_scale.shape[0], X.shape[1])
+        )
     return length_scale


-class Hyperparameter(namedtuple('Hyperparameter',
-                                ('name', 'value_type', 'bounds',
-                                 'n_elements', 'fixed'))):
+class Hyperparameter(
+    namedtuple(
+        "Hyperparameter", ("name", "value_type", "bounds", "n_elements", "fixed")
+    )
+):
     """A kernel hyperparameter's specification in form of a namedtuple.

     .. versionadded:: 0.18

     Attributes
     ----------
-    name : string
+    name : str
         The name of the hyperparameter. Note that a kernel using a
         hyperparameter with name "x" must have the attributes self.x and
         self.x_bounds

-    value_type : string
+    value_type : str
         The type of the hyperparameter. Currently, only "numeric"
         hyperparameters are supported.

@@ -73,12 +80,34 @@
         corresponds to a hyperparameter which is vector-valued,
         such as, e.g., anisotropic length-scales.

-    fixed : bool, default: None
+    fixed : bool, default=None
         Whether the value of this hyperparameter is fixed, i.e., cannot be
         changed during hyperparameter tuning. If None is passed, the "fixed" is
         derived based on the given bounds.

+    Examples
+    --------
+    >>> from sklearn.gaussian_process.kernels import ConstantKernel
+    >>> from sklearn.datasets import make_friedman2
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor
+    >>> from sklearn.gaussian_process.kernels import Hyperparameter
+    >>> X, y = make_friedman2(n_samples=50, noise=0, random_state=0)
+    >>> kernel = ConstantKernel(constant_value=1.0,
+    ...    constant_value_bounds=(0.0, 10.0))
+
+    We can access each hyperparameter:
+
+    >>> for hyperparameter in kernel.hyperparameters:
+    ...    print(hyperparameter)
+    Hyperparameter(name='constant_value', value_type='numeric',
+    bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)
+
+    >>> params = kernel.get_params()
+    >>> for key in sorted(params): print(f"{key} : {params[key]}")
+    constant_value : 1.0
+    constant_value_bounds : (0.0, 10.0)
     """
+
     # A raw namedtuple is very memory efficient as it packs the attributes
     # in a struct to get rid of the __dict__ of attributes in particular it
     # does not copy the string for the keys on each instance.
@@ -96,23 +125,28 @@
                 if bounds.shape[0] == 1:
                     bounds = np.repeat(bounds, n_elements, 0)
                 elif bounds.shape[0] != n_elements:
-                    raise ValueError("Bounds on %s should have either 1 or "
-                                     "%d dimensions. Given are %d"
-                                     % (name, n_elements, bounds.shape[0]))
+                    raise ValueError(
+                        "Bounds on %s should have either 1 or "
+                        "%d dimensions. Given are %d"
+                        % (name, n_elements, bounds.shape[0])
+                    )

         if fixed is None:
             fixed = isinstance(bounds, str) and bounds == "fixed"
         return super(Hyperparameter, cls).__new__(
-            cls, name, value_type, bounds, n_elements, fixed)
+            cls, name, value_type, bounds, n_elements, fixed
+        )

     # This is mainly a testing utility to check that two hyperparameters
     # are equal.
     def __eq__(self, other):
-        return (self.name == other.name and
-                self.value_type == other.value_type and
-                np.all(self.bounds == other.bounds) and
-                self.n_elements == other.n_elements and
-                self.fixed == other.fixed)
+        return (
+            self.name == other.name
+            and self.value_type == other.value_type
+            and np.all(self.bounds == other.bounds)
+            and self.n_elements == other.n_elements
+            and self.fixed == other.fixed
+        )


 class Kernel(metaclass=ABCMeta):
@@ -126,13 +160,13 @@

         Parameters
         ----------
-        deep : boolean, optional
+        deep : bool, default=True
             If True, will return the parameters for this estimator and
             contained subobjects that are estimators.

         Returns
         -------
-        params : mapping of string to any
+        params : dict
             Parameter names mapped to their values.
         """
         params = dict()
@@ -140,24 +174,25 @@
         # introspect the constructor arguments to find the model parameters
         # to represent
         cls = self.__class__
-        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)
+        init = getattr(cls.__init__, "deprecated_original", cls.__init__)
         init_sign = signature(init)
         args, varargs = [], []
         for parameter in init_sign.parameters.values():
-            if (parameter.kind != parameter.VAR_KEYWORD and
-                    parameter.name != 'self'):
+            if parameter.kind != parameter.VAR_KEYWORD and parameter.name != "self":
                 args.append(parameter.name)
             if parameter.kind == parameter.VAR_POSITIONAL:
                 varargs.append(parameter.name)

         if len(varargs) != 0:
-            raise RuntimeError("scikit-learn kernels should always "
-                               "specify their parameters in the signature"
-                               " of their __init__ (no varargs)."
-                               " %s doesn't follow this convention."
-                               % (cls, ))
+            raise RuntimeError(
+                "scikit-learn kernels should always "
+                "specify their parameters in the signature"
+                " of their __init__ (no varargs)."
+                " %s doesn't follow this convention." % (cls,)
+            )
         for arg in args:
-            params[arg] = getattr(self, arg, None)
+            params[arg] = getattr(self, arg)
+
         return params

     def set_params(self, **params):
@@ -176,24 +211,27 @@
             return self
         valid_params = self.get_params(deep=True)
         for key, value in params.items():
-            split = key.split('__', 1)
+            split = key.split("__", 1)
             if len(split) > 1:
                 # nested objects case
                 name, sub_name = split
                 if name not in valid_params:
-                    raise ValueError('Invalid parameter %s for kernel %s. '
-                                     'Check the list of available parameters '
-                                     'with `kernel.get_params().keys()`.' %
-                                     (name, self))
+                    raise ValueError(
+                        "Invalid parameter %s for kernel %s. "
+                        "Check the list of available parameters "
+                        "with `kernel.get_params().keys()`." % (name, self)
+                    )
                 sub_object = valid_params[name]
                 sub_object.set_params(**{sub_name: value})
             else:
                 # simple objects case
                 if key not in valid_params:
-                    raise ValueError('Invalid parameter %s for kernel %s. '
-                                     'Check the list of available parameters '
-                                     'with `kernel.get_params().keys()`.' %
-                                     (key, self.__class__.__name__))
+                    raise ValueError(
+                        "Invalid parameter %s for kernel %s. "
+                        "Check the list of available parameters "
+                        "with `kernel.get_params().keys()`."
+                        % (key, self.__class__.__name__)
+                    )
                 setattr(self, key, value)
         return self

@@ -202,7 +240,7 @@

         Parameters
         ----------
-        theta : array, shape (n_dims,)
+        theta : ndarray of shape (n_dims,)
             The hyperparameters
         """
         cloned = clone(self)
@@ -217,8 +255,11 @@
     @property
     def hyperparameters(self):
         """Returns a list of all hyperparameter specifications."""
-        r = [getattr(self, attr) for attr in dir(self)
-             if attr.startswith("hyperparameter_")]
+        r = [
+            getattr(self, attr)
+            for attr in dir(self)
+            if attr.startswith("hyperparameter_")
+        ]
         return r

     @property
@@ -232,7 +273,7 @@

         Returns
         -------
-        theta : array, shape (n_dims,)
+        theta : ndarray of shape (n_dims,)
             The non-fixed, log-transformed hyperparameters of the kernel
         """
         theta = []
@@ -251,7 +292,7 @@

         Parameters
         ----------
-        theta : array, shape (n_dims,)
+        theta : ndarray of shape (n_dims,)
             The non-fixed, log-transformed hyperparameters of the kernel
         """
         params = self.get_params()
@@ -262,16 +303,18 @@
             if hyperparameter.n_elements > 1:
                 # vector-valued parameter
                 params[hyperparameter.name] = np.exp(
-                    theta[i:i + hyperparameter.n_elements])
+                    theta[i : i + hyperparameter.n_elements]
+                )
                 i += hyperparameter.n_elements
             else:
                 params[hyperparameter.name] = np.exp(theta[i])
                 i += 1

         if i != len(theta):
-            raise ValueError("theta has not the correct number of entries."
-                             " Should be %d; given are %d"
-                             % (i, len(theta)))
+            raise ValueError(
+                "theta has not the correct number of entries."
+                " Should be %d; given are %d" % (i, len(theta))
+            )
         self.set_params(**params)

     @property
@@ -280,12 +323,14 @@

         Returns
         -------
-        bounds : array, shape (n_dims, 2)
+        bounds : ndarray of shape (n_dims, 2)
             The log-transformed bounds on the kernel's hyperparameters theta
         """
-        bounds = [hyperparameter.bounds
-                  for hyperparameter in self.hyperparameters
-                  if not hyperparameter.fixed]
+        bounds = [
+            hyperparameter.bounds
+            for hyperparameter in self.hyperparameters
+            if not hyperparameter.fixed
+        ]
         if len(bounds) > 0:
             return np.log(np.vstack(bounds))
         else:
@@ -325,8 +370,9 @@
         return True

     def __repr__(self):
-        return "{0}({1})".format(self.__class__.__name__,
-                                 ", ".join(map("{0:.3g}".format, self.theta)))
+        return "{0}({1})".format(
+            self.__class__.__name__, ", ".join(map("{0:.3g}".format, self.theta))
+        )

     @abstractmethod
     def __call__(self, X, Y=None, eval_gradient=False):
@@ -342,18 +388,55 @@

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
+        X : array-like of shape (n_samples,)
             Left argument of the returned kernel k(X, Y)

         Returns
         -------
-        K_diag : array, shape (n_samples_X,)
+        K_diag : ndarray of shape (n_samples_X,)
             Diagonal of kernel k(X, X)
         """

     @abstractmethod
     def is_stationary(self):
-        """Returns whether the kernel is stationary. """
+        """Returns whether the kernel is stationary."""
+
+    @property
+    def requires_vector_input(self):
+        """Returns whether the kernel is defined on fixed-length feature
+        vectors or generic objects. Defaults to True for backward
+        compatibility."""
+        return True
+
+    def _check_bounds_params(self):
+        """Called after fitting to warn if bounds may have been too tight."""
+        list_close = np.isclose(self.bounds, np.atleast_2d(self.theta).T)
+        idx = 0
+        for hyp in self.hyperparameters:
+            if hyp.fixed:
+                continue
+            for dim in range(hyp.n_elements):
+                if list_close[idx, 0]:
+                    warnings.warn(
+                        "The optimal value found for "
+                        "dimension %s of parameter %s is "
+                        "close to the specified lower "
+                        "bound %s. Decreasing the bound and"
+                        " calling fit again may find a "
+                        "better value." % (dim, hyp.name, hyp.bounds[dim][0]),
+                        ConvergenceWarning,
+                    )
+                elif list_close[idx, 1]:
+                    warnings.warn(
+                        "The optimal value found for "
+                        "dimension %s of parameter %s is "
+                        "close to the specified upper "
+                        "bound %s. Increasing the bound and"
+                        " calling fit again may find a "
+                        "better value." % (dim, hyp.name, hyp.bounds[dim][1]),
+                        ConvergenceWarning,
+                    )
+                idx += 1


 class NormalizedKernelMixin:
@@ -371,12 +454,12 @@

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
+        X : ndarray of shape (n_samples_X, n_features)
             Left argument of the returned kernel k(X, Y)

         Returns
         -------
-        K_diag : array, shape (n_samples_X,)
+        K_diag : ndarray of shape (n_samples_X,)
             Diagonal of kernel k(X, X)
         """
         return np.ones(X.shape[0])
@@ -389,8 +472,21 @@
     """

     def is_stationary(self):
-        """Returns whether the kernel is stationary. """
+        """Returns whether the kernel is stationary."""
         return True
+
+
+class GenericKernelMixin:
+    """Mixin for kernels which operate on generic objects such as variable-
+    length sequences, trees, and graphs.
+
+    .. versionadded:: 0.22
+    """
+
+    @property
+    def requires_vector_input(self):
+        """Whether the kernel works only on fixed-length feature vectors."""
+        return False


 class CompoundKernel(Kernel):
@@ -400,8 +496,23 @@

     Parameters
     ----------
-    kernels : list of Kernel objects
+    kernels : list of Kernels
         The other kernels
+
+    Examples
+    --------
+    >>> from sklearn.gaussian_process.kernels import WhiteKernel
+    >>> from sklearn.gaussian_process.kernels import RBF
+    >>> from sklearn.gaussian_process.kernels import CompoundKernel
+    >>> kernel = CompoundKernel(
+    ...     [WhiteKernel(noise_level=3.0), RBF(length_scale=2.0)])
+    >>> print(kernel.bounds)
+    [[-11.51292546  11.51292546]
+     [-11.51292546  11.51292546]]
+    >>> print(kernel.n_dims)
+    2
+    >>> print(kernel.theta)
+    [1.09861229 0.69314718]
     """

     def __init__(self, kernels):
@@ -412,13 +523,13 @@

         Parameters
         ----------
-        deep : boolean, optional
+        deep : bool, default=True
             If True, will return the parameters for this estimator and
             contained subobjects that are estimators.

         Returns
         -------
-        params : mapping of string to any
+        params : dict
             Parameter names mapped to their values.
         """
         return dict(kernels=self.kernels)
@@ -434,7 +545,7 @@

         Returns
         -------
-        theta : array, shape (n_dims,)
+        theta : ndarray of shape (n_dims,)
             The non-fixed, log-transformed hyperparameters of the kernel
         """
         return np.hstack([kernel.theta for kernel in self.kernels])
@@ -445,12 +556,12 @@

         Parameters
         ----------
-        theta : array, shape (n_dims,)
+        theta : array of shape (n_dims,)
             The non-fixed, log-transformed hyperparameters of the kernel
         """
         k_dims = self.k1.n_dims
         for i, kernel in enumerate(self.kernels):
-            kernel.theta = theta[i * k_dims:(i + 1) * k_dims]
+            kernel.theta = theta[i * k_dims : (i + 1) * k_dims]

     @property
     def bounds(self):
@@ -458,7 +569,7 @@

         Returns
         -------
-        bounds : array, shape (n_dims, 2)
+        bounds : array of shape (n_dims, 2)
             The log-transformed bounds on the kernel's hyperparameters theta
         """
         return np.vstack([kernel.bounds for kernel in self.kernels])
@@ -471,25 +582,28 @@

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
+        X : array-like of shape (n_samples_X, n_features) or list of object, \
+            default=None
             Left argument of the returned kernel k(X, Y)

-        Y : array, shape (n_samples_Y, n_features), (optional, default=None)
+        Y : array-like of shape (n_samples_X, n_features) or list of object, \
+            default=None
             Right argument of the returned kernel k(X, Y). If None, k(X, X)
-            if evaluated instead.
-
-        eval_gradient : bool (optional, default=False)
-            Determines whether the gradient with respect to the kernel
-            hyperparameter is determined.
-
-        Returns
-        -------
-        K : array, shape (n_samples_X, n_samples_Y, n_kernels)
+            is evaluated instead.
+
+        eval_gradient : bool, default=False
+            Determines whether the gradient with respect to the log of the
+            kernel hyperparameter is computed.
+
+        Returns
+        -------
+        K : ndarray of shape (n_samples_X, n_samples_Y, n_kernels)
             Kernel k(X, Y)

-        K_gradient : array, shape (n_samples_X, n_samples_X, n_dims, n_kernels)
-            The gradient of the kernel k(X, X) with respect to the
-            hyperparameter of the kernel. Only returned when eval_gradient
+        K_gradient : ndarray of shape \
+                (n_samples_X, n_samples_X, n_dims, n_kernels), optional
+            The gradient of the kernel k(X, X) with respect to the log of the
+            hyperparameter of the kernel. Only returned when `eval_gradient`
             is True.
         """
         if eval_gradient:
@@ -501,34 +615,39 @@
                 K_grad.append(K_grad_single[..., np.newaxis])
             return np.dstack(K), np.concatenate(K_grad, 3)
         else:
-            return np.dstack([kernel(X, Y, eval_gradient)
-                              for kernel in self.kernels])
+            return np.dstack([kernel(X, Y, eval_gradient) for kernel in self.kernels])

     def __eq__(self, b):
         if type(self) != type(b) or len(self.kernels) != len(b.kernels):
             return False
-        return np.all([self.kernels[i] == b.kernels[i]
-                       for i in range(len(self.kernels))])
+        return np.all(
+            [self.kernels[i] == b.kernels[i] for i in range(len(self.kernels))]
+        )

     def is_stationary(self):
-        """Returns whether the kernel is stationary. """
+        """Returns whether the kernel is stationary."""
         return np.all([kernel.is_stationary() for kernel in self.kernels])
+
+    @property
+    def requires_vector_input(self):
+        """Returns whether the kernel is defined on discrete structures."""
+        return np.any([kernel.requires_vector_input for kernel in self.kernels])

     def diag(self, X):
         """Returns the diagonal of the kernel k(X, X).

-        The result of this method is identical to np.diag(self(X)); however,
+        The result of this method is identical to `np.diag(self(X))`; however,
         it can be evaluated more efficiently since only the diagonal is
         evaluated.

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
-            Left argument of the returned kernel k(X, Y)
-
-        Returns
-        -------
-        K_diag : array, shape (n_samples_X, n_kernels)
+        X : array-like of shape (n_samples_X, n_features) or list of object
+            Argument to the kernel.
+
+        Returns
+        -------
+        K_diag : ndarray of shape (n_samples_X, n_kernels)
             Diagonal of kernel k(X, X)
         """
         return np.vstack([kernel.diag(X) for kernel in self.kernels]).T
@@ -549,37 +668,46 @@

         Parameters
         ----------
-        deep : boolean, optional
+        deep : bool, default=True
             If True, will return the parameters for this estimator and
             contained subobjects that are estimators.

         Returns
         -------
-        params : mapping of string to any
+        params : dict
             Parameter names mapped to their values.
         """
         params = dict(k1=self.k1, k2=self.k2)
         if deep:
             deep_items = self.k1.get_params().items()
-            params.update(('k1__' + k, val) for k, val in deep_items)
+            params.update(("k1__" + k, val) for k, val in deep_items)
             deep_items = self.k2.get_params().items()
-            params.update(('k2__' + k, val) for k, val in deep_items)
+            params.update(("k2__" + k, val) for k, val in deep_items)

         return params

     @property
     def hyperparameters(self):
         """Returns a list of all hyperparameter."""
-        r = [Hyperparameter("k1__" + hyperparameter.name,
-                            hyperparameter.value_type,
-                            hyperparameter.bounds, hyperparameter.n_elements)
-             for hyperparameter in self.k1.hyperparameters]
+        r = [
+            Hyperparameter(
+                "k1__" + hyperparameter.name,
+                hyperparameter.value_type,
+                hyperparameter.bounds,
+                hyperparameter.n_elements,
+            )
+            for hyperparameter in self.k1.hyperparameters
+        ]

         for hyperparameter in self.k2.hyperparameters:
-            r.append(Hyperparameter("k2__" + hyperparameter.name,
-                                    hyperparameter.value_type,
-                                    hyperparameter.bounds,
-                                    hyperparameter.n_elements))
+            r.append(
+                Hyperparameter(
+                    "k2__" + hyperparameter.name,
+                    hyperparameter.value_type,
+                    hyperparameter.bounds,
+                    hyperparameter.n_elements,
+                )
+            )
         return r

     @property
@@ -593,7 +721,7 @@

         Returns
         -------
-        theta : array, shape (n_dims,)
+        theta : ndarray of shape (n_dims,)
             The non-fixed, log-transformed hyperparameters of the kernel
         """
         return np.append(self.k1.theta, self.k2.theta)
@@ -604,7 +732,7 @@

         Parameters
         ----------
-        theta : array, shape (n_dims,)
+        theta : ndarray of shape (n_dims,)
             The non-fixed, log-transformed hyperparameters of the kernel
         """
         k1_dims = self.k1.n_dims
@@ -617,7 +745,7 @@

         Returns
         -------
-        bounds : array, shape (n_dims, 2)
+        bounds : ndarray of shape (n_dims, 2)
             The log-transformed bounds on the kernel's hyperparameters theta
         """
         if self.k1.bounds.size == 0:
@@ -629,30 +757,57 @@
     def __eq__(self, b):
         if type(self) != type(b):
             return False
-        return (self.k1 == b.k1 and self.k2 == b.k2) \
-            or (self.k1 == b.k2 and self.k2 == b.k1)
+        return (self.k1 == b.k1 and self.k2 == b.k2) or (
+            self.k1 == b.k2 and self.k2 == b.k1
+        )

     def is_stationary(self):
-        """Returns whether the kernel is stationary. """
+        """Returns whether the kernel is stationary."""
         return self.k1.is_stationary() and self.k2.is_stationary()

+    @property
+    def requires_vector_input(self):
+        """Returns whether the kernel is stationary."""
+        return self.k1.requires_vector_input or self.k2.requires_vector_input
+

 class Sum(KernelOperator):
-    """Sum-kernel k1 + k2 of two kernels k1 and k2.
-
-    The resulting kernel is defined as
-    k_sum(X, Y) = k1(X, Y) + k2(X, Y)
+    """The `Sum` kernel takes two kernels :math:`k_1` and :math:`k_2`
+    and combines them via
+
+    .. math::
+        k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)
+
+    Note that the `__add__` magic method is overridden, so
+    `Sum(RBF(), RBF())` is equivalent to using the + operator
+    with `RBF() + RBF()`.
+
+
+    Read more in the :ref:`User Guide <gp_kernels>`.

     .. versionadded:: 0.18

     Parameters
     ----------
-    k1 : Kernel object
+    k1 : Kernel
         The first base-kernel of the sum-kernel

-    k2 : Kernel object
+    k2 : Kernel
         The second base-kernel of the sum-kernel

+    Examples
+    --------
+    >>> from sklearn.datasets import make_friedman2
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor
+    >>> from sklearn.gaussian_process.kernels import RBF, Sum, ConstantKernel
+    >>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)
+    >>> kernel = Sum(ConstantKernel(2), RBF())
+    >>> gpr = GaussianProcessRegressor(kernel=kernel,
+    ...         random_state=0).fit(X, y)
+    >>> gpr.score(X, y)
+    1.0
+    >>> kernel
+    1.41**2 + RBF(length_scale=1)
     """

     def __call__(self, X, Y=None, eval_gradient=False):
@@ -660,25 +815,27 @@

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
+        X : array-like of shape (n_samples_X, n_features) or list of object
             Left argument of the returned kernel k(X, Y)

-        Y : array, shape (n_samples_Y, n_features), (optional, default=None)
+        Y : array-like of shape (n_samples_X, n_features) or list of object,\
+                default=None
             Right argument of the returned kernel k(X, Y). If None, k(X, X)
-            if evaluated instead.
-
-        eval_gradient : bool (optional, default=False)
-            Determines whether the gradient with respect to the kernel
-            hyperparameter is determined.
-
-        Returns
-        -------
-        K : array, shape (n_samples_X, n_samples_Y)
+            is evaluated instead.
+
+        eval_gradient : bool, default=False
+            Determines whether the gradient with respect to the log of
+            the kernel hyperparameter is computed.
+
+        Returns
+        -------
+        K : ndarray of shape (n_samples_X, n_samples_Y)
             Kernel k(X, Y)

-        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)
-            The gradient of the kernel k(X, X) with respect to the
-            hyperparameter of the kernel. Only returned when eval_gradient
+        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims),\
+                optional
+            The gradient of the kernel k(X, X) with respect to the log of the
+            hyperparameter of the kernel. Only returned when `eval_gradient`
             is True.
         """
         if eval_gradient:
@@ -691,118 +848,167 @@
     def diag(self, X):
         """Returns the diagonal of the kernel k(X, X).

+        The result of this method is identical to `np.diag(self(X))`; however,
+        it can be evaluated more efficiently since only the diagonal is
+        evaluated.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples_X, n_features) or list of object
+            Argument to the kernel.
+
+        Returns
+        -------
+        K_diag : ndarray of shape (n_samples_X,)
+            Diagonal of kernel k(X, X)
+        """
+        return self.k1.diag(X) + self.k2.diag(X)
+
+    def __repr__(self):
+        return "{0} + {1}".format(self.k1, self.k2)
+
+
+class Product(KernelOperator):
+    """The `Product` kernel takes two kernels :math:`k_1` and :math:`k_2`
+    and combines them via
+
+    .. math::
+        k_{prod}(X, Y) = k_1(X, Y) * k_2(X, Y)
+
+    Note that the `__mul__` magic method is overridden, so
+    `Product(RBF(), RBF())` is equivalent to using the * operator
+    with `RBF() * RBF()`.
+
+    Read more in the :ref:`User Guide <gp_kernels>`.
+
+    .. versionadded:: 0.18
+
+    Parameters
+    ----------
+    k1 : Kernel
+        The first base-kernel of the product-kernel
+
+    k2 : Kernel
+        The second base-kernel of the product-kernel
+
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_friedman2
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor
+    >>> from sklearn.gaussian_process.kernels import (RBF, Product,
+    ...            ConstantKernel)
+    >>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)
+    >>> kernel = Product(ConstantKernel(2), RBF())
+    >>> gpr = GaussianProcessRegressor(kernel=kernel,
+    ...         random_state=0).fit(X, y)
+    >>> gpr.score(X, y)
+    1.0
+    >>> kernel
+    1.41**2 * RBF(length_scale=1)
+    """
+
+    def __call__(self, X, Y=None, eval_gradient=False):
+        """Return the kernel k(X, Y) and optionally its gradient.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples_X, n_features) or list of object
+            Left argument of the returned kernel k(X, Y)
+
+        Y : array-like of shape (n_samples_Y, n_features) or list of object,\
+            default=None
+            Right argument of the returned kernel k(X, Y). If None, k(X, X)
+            is evaluated instead.
+
+        eval_gradient : bool, default=False
+            Determines whether the gradient with respect to the log of
+            the kernel hyperparameter is computed.
+
+        Returns
+        -------
+        K : ndarray of shape (n_samples_X, n_samples_Y)
+            Kernel k(X, Y)
+
+        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims), \
+                optional
+            The gradient of the kernel k(X, X) with respect to the log of the
+            hyperparameter of the kernel. Only returned when `eval_gradient`
+            is True.
+        """
+        if eval_gradient:
+            K1, K1_gradient = self.k1(X, Y, eval_gradient=True)
+            K2, K2_gradient = self.k2(X, Y, eval_gradient=True)
+            return K1 * K2, np.dstack(
+                (K1_gradient * K2[:, :, np.newaxis], K2_gradient * K1[:, :, np.newaxis])
+            )
+        else:
+            return self.k1(X, Y) * self.k2(X, Y)
+
+    def diag(self, X):
+        """Returns the diagonal of the kernel k(X, X).
+
         The result of this method is identical to np.diag(self(X)); however,
         it can be evaluated more efficiently since only the diagonal is
         evaluated.

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
-            Left argument of the returned kernel k(X, Y)
-
-        Returns
-        -------
-        K_diag : array, shape (n_samples_X,)
+        X : array-like of shape (n_samples_X, n_features) or list of object
+            Argument to the kernel.
+
+        Returns
+        -------
+        K_diag : ndarray of shape (n_samples_X,)
             Diagonal of kernel k(X, X)
         """
-        return self.k1.diag(X) + self.k2.diag(X)
+        return self.k1.diag(X) * self.k2.diag(X)

     def __repr__(self):
-        return "{0} + {1}".format(self.k1, self.k2)
-
-
-class Product(KernelOperator):
-    """Product-kernel k1 * k2 of two kernels k1 and k2.
-
-    The resulting kernel is defined as
-    k_prod(X, Y) = k1(X, Y) * k2(X, Y)
+        return "{0} * {1}".format(self.k1, self.k2)
+
+
+class Exponentiation(Kernel):
+    """The Exponentiation kernel takes one base kernel and a scalar parameter
+    :math:`p` and combines them via
+
+    .. math::
+        k_{exp}(X, Y) = k(X, Y) ^p
+
+    Note that the `__pow__` magic method is overridden, so
+    `Exponentiation(RBF(), 2)` is equivalent to using the ** operator
+    with `RBF() ** 2`.
+
+
+    Read more in the :ref:`User Guide <gp_kernels>`.

     .. versionadded:: 0.18

     Parameters
     ----------
-    k1 : Kernel object
-        The first base-kernel of the product-kernel
-
-    k2 : Kernel object
-        The second base-kernel of the product-kernel
-
-    """
-
-    def __call__(self, X, Y=None, eval_gradient=False):
-        """Return the kernel k(X, Y) and optionally its gradient.
-
-        Parameters
-        ----------
-        X : array, shape (n_samples_X, n_features)
-            Left argument of the returned kernel k(X, Y)
-
-        Y : array, shape (n_samples_Y, n_features), (optional, default=None)
-            Right argument of the returned kernel k(X, Y). If None, k(X, X)
-            if evaluated instead.
-
-        eval_gradient : bool (optional, default=False)
-            Determines whether the gradient with respect to the kernel
-            hyperparameter is determined.
-
-        Returns
-        -------
-        K : array, shape (n_samples_X, n_samples_Y)
-            Kernel k(X, Y)
-
-        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)
-            The gradient of the kernel k(X, X) with respect to the
-            hyperparameter of the kernel. Only returned when eval_gradient
-            is True.
-        """
-        if eval_gradient:
-            K1, K1_gradient = self.k1(X, Y, eval_gradient=True)
-            K2, K2_gradient = self.k2(X, Y, eval_gradient=True)
-            return K1 * K2, np.dstack((K1_gradient * K2[:, :, np.newaxis],
-                                       K2_gradient * K1[:, :, np.newaxis]))
-        else:
-            return self.k1(X, Y) * self.k2(X, Y)
-
-    def diag(self, X):
-        """Returns the diagonal of the kernel k(X, X).
-
-        The result of this method is identical to np.diag(self(X)); however,
-        it can be evaluated more efficiently since only the diagonal is
-        evaluated.
-
-        Parameters
-        ----------
-        X : array, shape (n_samples_X, n_features)
-            Left argument of the returned kernel k(X, Y)
-
-        Returns
-        -------
-        K_diag : array, shape (n_samples_X,)
-            Diagonal of kernel k(X, X)
-        """
-        return self.k1.diag(X) * self.k2.diag(X)
-
-    def __repr__(self):
-        return "{0} * {1}".format(self.k1, self.k2)
-
-
-class Exponentiation(Kernel):
-    """Exponentiate kernel by given exponent.
-
-    The resulting kernel is defined as
-    k_exp(X, Y) = k(X, Y) ** exponent
-
-    .. versionadded:: 0.18
-
-    Parameters
-    ----------
-    kernel : Kernel object
+    kernel : Kernel
         The base kernel

     exponent : float
         The exponent for the base kernel

+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_friedman2
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor
+    >>> from sklearn.gaussian_process.kernels import (RationalQuadratic,
+    ...            Exponentiation)
+    >>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)
+    >>> kernel = Exponentiation(RationalQuadratic(), exponent=2)
+    >>> gpr = GaussianProcessRegressor(kernel=kernel, alpha=5,
+    ...         random_state=0).fit(X, y)
+    >>> gpr.score(X, y)
+    0.419...
+    >>> gpr.predict(X[:1,:], return_std=True)
+    (array([635.5...]), array([0.559...]))
     """
+
     def __init__(self, kernel, exponent):
         self.kernel = kernel
         self.exponent = exponent
@@ -812,19 +1018,19 @@

         Parameters
         ----------
-        deep : boolean, optional
+        deep : bool, default=True
             If True, will return the parameters for this estimator and
             contained subobjects that are estimators.

         Returns
         -------
-        params : mapping of string to any
+        params : dict
             Parameter names mapped to their values.
         """
         params = dict(kernel=self.kernel, exponent=self.exponent)
         if deep:
             deep_items = self.kernel.get_params().items()
-            params.update(('kernel__' + k, val) for k, val in deep_items)
+            params.update(("kernel__" + k, val) for k, val in deep_items)
         return params

     @property
@@ -832,10 +1038,14 @@
         """Returns a list of all hyperparameter."""
         r = []
         for hyperparameter in self.kernel.hyperparameters:
-            r.append(Hyperparameter("kernel__" + hyperparameter.name,
-                                    hyperparameter.value_type,
-                                    hyperparameter.bounds,
-                                    hyperparameter.n_elements))
+            r.append(
+                Hyperparameter(
+                    "kernel__" + hyperparameter.name,
+                    hyperparameter.value_type,
+                    hyperparameter.bounds,
+                    hyperparameter.n_elements,
+                )
+            )
         return r

     @property
@@ -849,7 +1059,7 @@

         Returns
         -------
-        theta : array, shape (n_dims,)
+        theta : ndarray of shape (n_dims,)
             The non-fixed, log-transformed hyperparameters of the kernel
         """
         return self.kernel.theta
@@ -860,7 +1070,7 @@

         Parameters
         ----------
-        theta : array, shape (n_dims,)
+        theta : ndarray of shape (n_dims,)
             The non-fixed, log-transformed hyperparameters of the kernel
         """
         self.kernel.theta = theta
@@ -871,7 +1081,7 @@

         Returns
         -------
-        bounds : array, shape (n_dims, 2)
+        bounds : ndarray of shape (n_dims, 2)
             The log-transformed bounds on the kernel's hyperparameters theta
         """
         return self.kernel.bounds
@@ -879,42 +1089,43 @@
     def __eq__(self, b):
         if type(self) != type(b):
             return False
-        return (self.kernel == b.kernel and self.exponent == b.exponent)
+        return self.kernel == b.kernel and self.exponent == b.exponent

     def __call__(self, X, Y=None, eval_gradient=False):
         """Return the kernel k(X, Y) and optionally its gradient.

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
+        X : array-like of shape (n_samples_X, n_features) or list of object
             Left argument of the returned kernel k(X, Y)

-        Y : array, shape (n_samples_Y, n_features), (optional, default=None)
+        Y : array-like of shape (n_samples_Y, n_features) or list of object,\
+            default=None
             Right argument of the returned kernel k(X, Y). If None, k(X, X)
-            if evaluated instead.
-
-        eval_gradient : bool (optional, default=False)
-            Determines whether the gradient with respect to the kernel
-            hyperparameter is determined.
-
-        Returns
-        -------
-        K : array, shape (n_samples_X, n_samples_Y)
+            is evaluated instead.
+
+        eval_gradient : bool, default=False
+            Determines whether the gradient with respect to the log of
+            the kernel hyperparameter is computed.
+
+        Returns
+        -------
+        K : ndarray of shape (n_samples_X, n_samples_Y)
             Kernel k(X, Y)

-        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)
-            The gradient of the kernel k(X, X) with respect to the
-            hyperparameter of the kernel. Only returned when eval_gradient
+        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims),\
+                optional
+            The gradient of the kernel k(X, X) with respect to the log of the
+            hyperparameter of the kernel. Only returned when `eval_gradient`
             is True.
         """
         if eval_gradient:
             K, K_gradient = self.kernel(X, Y, eval_gradient=True)
-            K_gradient *= \
-                self.exponent * K[:, :, np.newaxis] ** (self.exponent - 1)
-            return K ** self.exponent, K_gradient
+            K_gradient *= self.exponent * K[:, :, np.newaxis] ** (self.exponent - 1)
+            return K**self.exponent, K_gradient
         else:
             K = self.kernel(X, Y, eval_gradient=False)
-            return K ** self.exponent
+            return K**self.exponent

     def diag(self, X):
         """Returns the diagonal of the kernel k(X, X).
@@ -925,12 +1136,12 @@

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
-            Left argument of the returned kernel k(X, Y)
-
-        Returns
-        -------
-        K_diag : array, shape (n_samples_X,)
+        X : array-like of shape (n_samples_X, n_features) or list of object
+            Argument to the kernel.
+
+        Returns
+        -------
+        K_diag : ndarray of shape (n_samples_X,)
             Diagonal of kernel k(X, X)
         """
         return self.kernel.diag(X) ** self.exponent
@@ -939,81 +1150,123 @@
         return "{0} ** {1}".format(self.kernel, self.exponent)

     def is_stationary(self):
-        """Returns whether the kernel is stationary. """
+        """Returns whether the kernel is stationary."""
         return self.kernel.is_stationary()

-
-class ConstantKernel(StationaryKernelMixin, Kernel):
+    @property
+    def requires_vector_input(self):
+        """Returns whether the kernel is defined on discrete structures."""
+        return self.kernel.requires_vector_input
+
+
+class ConstantKernel(StationaryKernelMixin, GenericKernelMixin, Kernel):
     """Constant kernel.

     Can be used as part of a product-kernel where it scales the magnitude of
     the other factor (kernel) or as part of a sum-kernel, where it modifies
     the mean of the Gaussian process.

-    k(x_1, x_2) = constant_value for all x_1, x_2
+    .. math::
+        k(x_1, x_2) = constant\\_value \\;\\forall\\; x_1, x_2
+
+    Adding a constant kernel is equivalent to adding a constant::
+
+            kernel = RBF() + ConstantKernel(constant_value=2)
+
+    is the same as::
+
+            kernel = RBF() + 2
+
+
+    Read more in the :ref:`User Guide <gp_kernels>`.

     .. versionadded:: 0.18

     Parameters
     ----------
-    constant_value : float, default: 1.0
+    constant_value : float, default=1.0
         The constant value which defines the covariance:
         k(x_1, x_2) = constant_value

-    constant_value_bounds : pair of floats >= 0, default: (1e-5, 1e5)
-        The lower and upper bound on constant_value
-
+    constant_value_bounds : pair of floats >= 0 or "fixed", default=(1e-5, 1e5)
+        The lower and upper bound on `constant_value`.
+        If set to "fixed", `constant_value` cannot be changed during
+        hyperparameter tuning.
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_friedman2
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor
+    >>> from sklearn.gaussian_process.kernels import RBF, ConstantKernel
+    >>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)
+    >>> kernel = RBF() + ConstantKernel(constant_value=2)
+    >>> gpr = GaussianProcessRegressor(kernel=kernel, alpha=5,
+    ...         random_state=0).fit(X, y)
+    >>> gpr.score(X, y)
+    0.3696...
+    >>> gpr.predict(X[:1,:], return_std=True)
+    (array([606.1...]), array([0.24...]))
     """
+
     def __init__(self, constant_value=1.0, constant_value_bounds=(1e-5, 1e5)):
         self.constant_value = constant_value
         self.constant_value_bounds = constant_value_bounds

     @property
     def hyperparameter_constant_value(self):
-        return Hyperparameter(
-            "constant_value", "numeric", self.constant_value_bounds)
+        return Hyperparameter("constant_value", "numeric", self.constant_value_bounds)

     def __call__(self, X, Y=None, eval_gradient=False):
         """Return the kernel k(X, Y) and optionally its gradient.

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
+        X : array-like of shape (n_samples_X, n_features) or list of object
             Left argument of the returned kernel k(X, Y)

-        Y : array, shape (n_samples_Y, n_features), (optional, default=None)
+        Y : array-like of shape (n_samples_X, n_features) or list of object, \
+            default=None
             Right argument of the returned kernel k(X, Y). If None, k(X, X)
-            if evaluated instead.
-
-        eval_gradient : bool (optional, default=False)
-            Determines whether the gradient with respect to the kernel
-            hyperparameter is determined. Only supported when Y is None.
-
-        Returns
-        -------
-        K : array, shape (n_samples_X, n_samples_Y)
+            is evaluated instead.
+
+        eval_gradient : bool, default=False
+            Determines whether the gradient with respect to the log of
+            the kernel hyperparameter is computed.
+            Only supported when Y is None.
+
+        Returns
+        -------
+        K : ndarray of shape (n_samples_X, n_samples_Y)
             Kernel k(X, Y)

-        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)
-            The gradient of the kernel k(X, X) with respect to the
+        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims), \
+            optional
+            The gradient of the kernel k(X, X) with respect to the log of the
             hyperparameter of the kernel. Only returned when eval_gradient
             is True.
         """
-        X = np.atleast_2d(X)
         if Y is None:
             Y = X
         elif eval_gradient:
             raise ValueError("Gradient can only be evaluated when Y is None.")

-        K = np.full((X.shape[0], Y.shape[0]), self.constant_value,
-                    dtype=np.array(self.constant_value).dtype)
+        K = np.full(
+            (_num_samples(X), _num_samples(Y)),
+            self.constant_value,
+            dtype=np.array(self.constant_value).dtype,
+        )
         if eval_gradient:
             if not self.hyperparameter_constant_value.fixed:
-                return (K, np.full((X.shape[0], X.shape[0], 1),
-                                   self.constant_value,
-                                   dtype=np.array(self.constant_value).dtype))
+                return (
+                    K,
+                    np.full(
+                        (_num_samples(X), _num_samples(X), 1),
+                        self.constant_value,
+                        dtype=np.array(self.constant_value).dtype,
+                    ),
+                )
             else:
-                return K, np.empty((X.shape[0], X.shape[0], 0))
+                return K, np.empty((_num_samples(X), _num_samples(X), 0))
         else:
             return K

@@ -1026,92 +1279,119 @@

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
-            Left argument of the returned kernel k(X, Y)
-
-        Returns
-        -------
-        K_diag : array, shape (n_samples_X,)
+        X : array-like of shape (n_samples_X, n_features) or list of object
+            Argument to the kernel.
+
+        Returns
+        -------
+        K_diag : ndarray of shape (n_samples_X,)
             Diagonal of kernel k(X, X)
         """
-        return np.full(X.shape[0], self.constant_value,
-                       dtype=np.array(self.constant_value).dtype)
+        return np.full(
+            _num_samples(X),
+            self.constant_value,
+            dtype=np.array(self.constant_value).dtype,
+        )

     def __repr__(self):
         return "{0:.3g}**2".format(np.sqrt(self.constant_value))


-class WhiteKernel(StationaryKernelMixin, Kernel):
+class WhiteKernel(StationaryKernelMixin, GenericKernelMixin, Kernel):
     """White kernel.

     The main use-case of this kernel is as part of a sum-kernel where it
-    explains the noise-component of the signal. Tuning its parameter
-    corresponds to estimating the noise-level.
-
-    k(x_1, x_2) = noise_level if x_1 == x_2 else 0
+    explains the noise of the signal as independently and identically
+    normally-distributed. The parameter noise_level equals the variance of this
+    noise.
+
+    .. math::
+        k(x_1, x_2) = noise\\_level \\text{ if } x_i == x_j \\text{ else } 0
+
+
+    Read more in the :ref:`User Guide <gp_kernels>`.

     .. versionadded:: 0.18

     Parameters
     ----------
-    noise_level : float, default: 1.0
-        Parameter controlling the noise level
-
-    noise_level_bounds : pair of floats >= 0, default: (1e-5, 1e5)
-        The lower and upper bound on noise_level
-
+    noise_level : float, default=1.0
+        Parameter controlling the noise level (variance)
+
+    noise_level_bounds : pair of floats >= 0 or "fixed", default=(1e-5, 1e5)
+        The lower and upper bound on 'noise_level'.
+        If set to "fixed", 'noise_level' cannot be changed during
+        hyperparameter tuning.
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_friedman2
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor
+    >>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel
+    >>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)
+    >>> kernel = DotProduct() + WhiteKernel(noise_level=0.5)
+    >>> gpr = GaussianProcessRegressor(kernel=kernel,
+    ...         random_state=0).fit(X, y)
+    >>> gpr.score(X, y)
+    0.3680...
+    >>> gpr.predict(X[:2,:], return_std=True)
+    (array([653.0..., 592.1... ]), array([316.6..., 316.6...]))
     """
+
     def __init__(self, noise_level=1.0, noise_level_bounds=(1e-5, 1e5)):
         self.noise_level = noise_level
         self.noise_level_bounds = noise_level_bounds

     @property
     def hyperparameter_noise_level(self):
-        return Hyperparameter(
-            "noise_level", "numeric", self.noise_level_bounds)
+        return Hyperparameter("noise_level", "numeric", self.noise_level_bounds)

     def __call__(self, X, Y=None, eval_gradient=False):
         """Return the kernel k(X, Y) and optionally its gradient.

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
+        X : array-like of shape (n_samples_X, n_features) or list of object
             Left argument of the returned kernel k(X, Y)

-        Y : array, shape (n_samples_Y, n_features), (optional, default=None)
+        Y : array-like of shape (n_samples_X, n_features) or list of object,\
+            default=None
             Right argument of the returned kernel k(X, Y). If None, k(X, X)
-            if evaluated instead.
-
-        eval_gradient : bool (optional, default=False)
-            Determines whether the gradient with respect to the kernel
-            hyperparameter is determined. Only supported when Y is None.
-
-        Returns
-        -------
-        K : array, shape (n_samples_X, n_samples_Y)
+            is evaluated instead.
+
+        eval_gradient : bool, default=False
+            Determines whether the gradient with respect to the log of
+            the kernel hyperparameter is computed.
+            Only supported when Y is None.
+
+        Returns
+        -------
+        K : ndarray of shape (n_samples_X, n_samples_Y)
             Kernel k(X, Y)

-        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)
-            The gradient of the kernel k(X, X) with respect to the
+        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims),\
+            optional
+            The gradient of the kernel k(X, X) with respect to the log of the
             hyperparameter of the kernel. Only returned when eval_gradient
             is True.
         """
-        X = np.atleast_2d(X)
         if Y is not None and eval_gradient:
             raise ValueError("Gradient can only be evaluated when Y is None.")

         if Y is None:
-            K = self.noise_level * np.eye(X.shape[0])
+            K = self.noise_level * np.eye(_num_samples(X))
             if eval_gradient:
                 if not self.hyperparameter_noise_level.fixed:
-                    return (K, self.noise_level
-                            * np.eye(X.shape[0])[:, :, np.newaxis])
+                    return (
+                        K,
+                        self.noise_level * np.eye(_num_samples(X))[:, :, np.newaxis],
+                    )
                 else:
-                    return K, np.empty((X.shape[0], X.shape[0], 0))
+                    return K, np.empty((_num_samples(X), _num_samples(X), 0))
             else:
                 return K
         else:
-            return np.zeros((X.shape[0], Y.shape[0]))
+            return np.zeros((_num_samples(X), _num_samples(Y)))

     def diag(self, X):
         """Returns the diagonal of the kernel k(X, X).
@@ -1122,50 +1402,87 @@

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
-            Left argument of the returned kernel k(X, Y)
-
-        Returns
-        -------
-        K_diag : array, shape (n_samples_X,)
+        X : array-like of shape (n_samples_X, n_features) or list of object
+            Argument to the kernel.
+
+        Returns
+        -------
+        K_diag : ndarray of shape (n_samples_X,)
             Diagonal of kernel k(X, X)
         """
-        return np.full(X.shape[0], self.noise_level,
-                       dtype=np.array(self.noise_level).dtype)
+        return np.full(
+            _num_samples(X), self.noise_level, dtype=np.array(self.noise_level).dtype
+        )

     def __repr__(self):
-        return "{0}(noise_level={1:.3g})".format(self.__class__.__name__,
-                                                 self.noise_level)
+        return "{0}(noise_level={1:.3g})".format(
+            self.__class__.__name__, self.noise_level
+        )


 class RBF(StationaryKernelMixin, NormalizedKernelMixin, Kernel):
     """Radial-basis function kernel (aka squared-exponential kernel).

     The RBF kernel is a stationary kernel. It is also known as the
-    "squared exponential" kernel. It is parameterized by a length-scale
-    parameter length_scale>0, which can either be a scalar (isotropic variant
+    "squared exponential" kernel. It is parameterized by a length scale
+    parameter :math:`l>0`, which can either be a scalar (isotropic variant
     of the kernel) or a vector with the same number of dimensions as the inputs
     X (anisotropic variant of the kernel). The kernel is given by:

-    k(x_i, x_j) = exp(-1 / 2 d(x_i / length_scale, x_j / length_scale)^2)
+    .. math::
+        k(x_i, x_j) = \\exp\\left(- \\frac{d(x_i, x_j)^2}{2l^2} \\right)
+
+    where :math:`l` is the length scale of the kernel and
+    :math:`d(\\cdot,\\cdot)` is the Euclidean distance.
+    For advice on how to set the length scale parameter, see e.g. [1]_.

     This kernel is infinitely differentiable, which implies that GPs with this
     kernel as covariance function have mean square derivatives of all orders,
     and are thus very smooth.
+    See [2]_, Chapter 4, Section 4.2, for further details of the RBF kernel.
+
+    Read more in the :ref:`User Guide <gp_kernels>`.

     .. versionadded:: 0.18

     Parameters
     ----------
-    length_scale : float or array with shape (n_features,), default: 1.0
+    length_scale : float or ndarray of shape (n_features,), default=1.0
         The length scale of the kernel. If a float, an isotropic kernel is
         used. If an array, an anisotropic kernel is used where each dimension
         of l defines the length-scale of the respective feature dimension.

-    length_scale_bounds : pair of floats >= 0, default: (1e-5, 1e5)
-        The lower and upper bound on length_scale
-
+    length_scale_bounds : pair of floats >= 0 or "fixed", default=(1e-5, 1e5)
+        The lower and upper bound on 'length_scale'.
+        If set to "fixed", 'length_scale' cannot be changed during
+        hyperparameter tuning.
+
+    References
+    ----------
+    .. [1] `David Duvenaud (2014). "The Kernel Cookbook:
+        Advice on Covariance functions".
+        <https://www.cs.toronto.edu/~duvenaud/cookbook/>`_
+
+    .. [2] `Carl Edward Rasmussen, Christopher K. I. Williams (2006).
+        "Gaussian Processes for Machine Learning". The MIT Press.
+        <http://www.gaussianprocess.org/gpml/>`_
+
+    Examples
+    --------
+    >>> from sklearn.datasets import load_iris
+    >>> from sklearn.gaussian_process import GaussianProcessClassifier
+    >>> from sklearn.gaussian_process.kernels import RBF
+    >>> X, y = load_iris(return_X_y=True)
+    >>> kernel = 1.0 * RBF(1.0)
+    >>> gpc = GaussianProcessClassifier(kernel=kernel,
+    ...         random_state=0).fit(X, y)
+    >>> gpc.score(X, y)
+    0.9866...
+    >>> gpc.predict_proba(X[:2,:])
+    array([[0.8354..., 0.03228..., 0.1322...],
+           [0.7906..., 0.0652..., 0.1441...]])
     """
+
     def __init__(self, length_scale=1.0, length_scale_bounds=(1e-5, 1e5)):
         self.length_scale = length_scale
         self.length_scale_bounds = length_scale_bounds
@@ -1177,66 +1494,68 @@
     @property
     def hyperparameter_length_scale(self):
         if self.anisotropic:
-            return Hyperparameter("length_scale", "numeric",
-                                  self.length_scale_bounds,
-                                  len(self.length_scale))
-        return Hyperparameter(
-            "length_scale", "numeric", self.length_scale_bounds)
+            return Hyperparameter(
+                "length_scale",
+                "numeric",
+                self.length_scale_bounds,
+                len(self.length_scale),
+            )
+        return Hyperparameter("length_scale", "numeric", self.length_scale_bounds)

     def __call__(self, X, Y=None, eval_gradient=False):
         """Return the kernel k(X, Y) and optionally its gradient.

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
+        X : ndarray of shape (n_samples_X, n_features)
             Left argument of the returned kernel k(X, Y)

-        Y : array, shape (n_samples_Y, n_features), (optional, default=None)
+        Y : ndarray of shape (n_samples_Y, n_features), default=None
             Right argument of the returned kernel k(X, Y). If None, k(X, X)
             if evaluated instead.

-        eval_gradient : bool (optional, default=False)
-            Determines whether the gradient with respect to the kernel
-            hyperparameter is determined. Only supported when Y is None.
-
-        Returns
-        -------
-        K : array, shape (n_samples_X, n_samples_Y)
+        eval_gradient : bool, default=False
+            Determines whether the gradient with respect to the log of
+            the kernel hyperparameter is computed.
+            Only supported when Y is None.
+
+        Returns
+        -------
+        K : ndarray of shape (n_samples_X, n_samples_Y)
             Kernel k(X, Y)

-        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)
-            The gradient of the kernel k(X, X) with respect to the
-            hyperparameter of the kernel. Only returned when eval_gradient
+        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims), \
+                optional
+            The gradient of the kernel k(X, X) with respect to the log of the
+            hyperparameter of the kernel. Only returned when `eval_gradient`
             is True.
         """
         X = np.atleast_2d(X)
         length_scale = _check_length_scale(X, self.length_scale)
         if Y is None:
-            dists = pdist(X / length_scale, metric='sqeuclidean')
-            K = np.exp(-.5 * dists)
+            dists = pdist(X / length_scale, metric="sqeuclidean")
+            K = np.exp(-0.5 * dists)
             # convert from upper-triangular matrix to square matrix
             K = squareform(K)
             np.fill_diagonal(K, 1)
         else:
             if eval_gradient:
-                raise ValueError(
-                    "Gradient can only be evaluated when Y is None.")
-            dists = cdist(X / length_scale, Y / length_scale,
-                          metric='sqeuclidean')
-            K = np.exp(-.5 * dists)
+                raise ValueError("Gradient can only be evaluated when Y is None.")
+            dists = cdist(X / length_scale, Y / length_scale, metric="sqeuclidean")
+            K = np.exp(-0.5 * dists)

         if eval_gradient:
             if self.hyperparameter_length_scale.fixed:
                 # Hyperparameter l kept fixed
                 return K, np.empty((X.shape[0], X.shape[0], 0))
             elif not self.anisotropic or length_scale.shape[0] == 1:
-                K_gradient = \
-                    (K * squareform(dists))[:, :, np.newaxis]
+                K_gradient = (K * squareform(dists))[:, :, np.newaxis]
                 return K, K_gradient
             elif self.anisotropic:
                 # We need to recompute the pairwise dimension-wise distances
-                K_gradient = (X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2 \
-                    / (length_scale ** 2)
+                K_gradient = (X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2 / (
+                    length_scale**2
+                )
                 K_gradient *= K[..., np.newaxis]
                 return K, K_gradient
         else:
@@ -1245,40 +1564,62 @@
     def __repr__(self):
         if self.anisotropic:
             return "{0}(length_scale=[{1}])".format(
-                self.__class__.__name__, ", ".join(map("{0:.3g}".format,
-                                                   self.length_scale)))
+                self.__class__.__name__,
+                ", ".join(map("{0:.3g}".format, self.length_scale)),
+            )
         else:  # isotropic
             return "{0}(length_scale={1:.3g})".format(
-                self.__class__.__name__, np.ravel(self.length_scale)[0])
+                self.__class__.__name__, np.ravel(self.length_scale)[0]
+            )


 class Matern(RBF):
-    """ Matern kernel.
-
-    The class of Matern kernels is a generalization of the RBF and the
-    absolute exponential kernel parameterized by an additional parameter
-    nu. The smaller nu, the less smooth the approximated function is.
-    For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5
-    to the absolute exponential kernel. Important intermediate values are
-    nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable
-    functions).
-
-    See Rasmussen and Williams 2006, pp84 for details regarding the
-    different variants of the Matern kernel.
+    """Matern kernel.
+
+    The class of Matern kernels is a generalization of the :class:`RBF`.
+    It has an additional parameter :math:`\\nu` which controls the
+    smoothness of the resulting function. The smaller :math:`\\nu`,
+    the less smooth the approximated function is.
+    As :math:`\\nu\\rightarrow\\infty`, the kernel becomes equivalent to
+    the :class:`RBF` kernel. When :math:`\\nu = 1/2`, the Matérn kernel
+    becomes identical to the absolute exponential kernel.
+    Important intermediate values are
+    :math:`\\nu=1.5` (once differentiable functions)
+    and :math:`\\nu=2.5` (twice differentiable functions).
+
+    The kernel is given by:
+
+    .. math::
+         k(x_i, x_j) =  \\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(
+         \\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )
+         \\Bigg)^\\nu K_\\nu\\Bigg(
+         \\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg)
+
+
+
+    where :math:`d(\\cdot,\\cdot)` is the Euclidean distance,
+    :math:`K_{\\nu}(\\cdot)` is a modified Bessel function and
+    :math:`\\Gamma(\\cdot)` is the gamma function.
+    See [1]_, Chapter 4, Section 4.2, for details regarding the different
+    variants of the Matern kernel.
+
+    Read more in the :ref:`User Guide <gp_kernels>`.

     .. versionadded:: 0.18

     Parameters
     ----------
-    length_scale : float or array with shape (n_features,), default: 1.0
+    length_scale : float or ndarray of shape (n_features,), default=1.0
         The length scale of the kernel. If a float, an isotropic kernel is
         used. If an array, an anisotropic kernel is used where each dimension
         of l defines the length-scale of the respective feature dimension.

-    length_scale_bounds : pair of floats >= 0, default: (1e-5, 1e5)
-        The lower and upper bound on length_scale
-
-    nu : float, default: 1.5
+    length_scale_bounds : pair of floats >= 0 or "fixed", default=(1e-5, 1e5)
+        The lower and upper bound on 'length_scale'.
+        If set to "fixed", 'length_scale' cannot be changed during
+        hyperparameter tuning.
+
+    nu : float, default=1.5
         The parameter nu controlling the smoothness of the learned function.
         The smaller nu, the less smooth the approximated function is.
         For nu=inf, the kernel becomes equivalent to the RBF kernel and for
@@ -1290,9 +1631,29 @@
         Bessel function. Furthermore, in contrast to l, nu is kept fixed to
         its initial value and not optimized.

+    References
+    ----------
+    .. [1] `Carl Edward Rasmussen, Christopher K. I. Williams (2006).
+        "Gaussian Processes for Machine Learning". The MIT Press.
+        <http://www.gaussianprocess.org/gpml/>`_
+
+    Examples
+    --------
+    >>> from sklearn.datasets import load_iris
+    >>> from sklearn.gaussian_process import GaussianProcessClassifier
+    >>> from sklearn.gaussian_process.kernels import Matern
+    >>> X, y = load_iris(return_X_y=True)
+    >>> kernel = 1.0 * Matern(length_scale=1.0, nu=1.5)
+    >>> gpc = GaussianProcessClassifier(kernel=kernel,
+    ...         random_state=0).fit(X, y)
+    >>> gpc.score(X, y)
+    0.9866...
+    >>> gpc.predict_proba(X[:2,:])
+    array([[0.8513..., 0.0368..., 0.1117...],
+            [0.8086..., 0.0693..., 0.1220...]])
     """
-    def __init__(self, length_scale=1.0, length_scale_bounds=(1e-5, 1e5),
-                 nu=1.5):
+
+    def __init__(self, length_scale=1.0, length_scale_bounds=(1e-5, 1e5), nu=1.5):
         super().__init__(length_scale, length_scale_bounds)
         self.nu = nu

@@ -1301,52 +1662,54 @@

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
+        X : ndarray of shape (n_samples_X, n_features)
             Left argument of the returned kernel k(X, Y)

-        Y : array, shape (n_samples_Y, n_features), (optional, default=None)
+        Y : ndarray of shape (n_samples_Y, n_features), default=None
             Right argument of the returned kernel k(X, Y). If None, k(X, X)
             if evaluated instead.

-        eval_gradient : bool (optional, default=False)
-            Determines whether the gradient with respect to the kernel
-            hyperparameter is determined. Only supported when Y is None.
-
-        Returns
-        -------
-        K : array, shape (n_samples_X, n_samples_Y)
+        eval_gradient : bool, default=False
+            Determines whether the gradient with respect to the log of
+            the kernel hyperparameter is computed.
+            Only supported when Y is None.
+
+        Returns
+        -------
+        K : ndarray of shape (n_samples_X, n_samples_Y)
             Kernel k(X, Y)

-        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)
-            The gradient of the kernel k(X, X) with respect to the
-            hyperparameter of the kernel. Only returned when eval_gradient
+        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims), \
+                optional
+            The gradient of the kernel k(X, X) with respect to the log of the
+            hyperparameter of the kernel. Only returned when `eval_gradient`
             is True.
         """
         X = np.atleast_2d(X)
         length_scale = _check_length_scale(X, self.length_scale)
         if Y is None:
-            dists = pdist(X / length_scale, metric='euclidean')
+            dists = pdist(X / length_scale, metric="euclidean")
         else:
             if eval_gradient:
-                raise ValueError(
-                    "Gradient can only be evaluated when Y is None.")
-            dists = cdist(X / length_scale, Y / length_scale,
-                          metric='euclidean')
+                raise ValueError("Gradient can only be evaluated when Y is None.")
+            dists = cdist(X / length_scale, Y / length_scale, metric="euclidean")

         if self.nu == 0.5:
             K = np.exp(-dists)
         elif self.nu == 1.5:
             K = dists * math.sqrt(3)
-            K = (1. + K) * np.exp(-K)
+            K = (1.0 + K) * np.exp(-K)
         elif self.nu == 2.5:
             K = dists * math.sqrt(5)
-            K = (1. + K + K ** 2 / 3.0) * np.exp(-K)
+            K = (1.0 + K + K**2 / 3.0) * np.exp(-K)
+        elif self.nu == np.inf:
+            K = np.exp(-(dists**2) / 2.0)
         else:  # general case; expensive to evaluate
             K = dists
             K[K == 0.0] += np.finfo(float).eps  # strict zeros result in nan
-            tmp = (math.sqrt(2 * self.nu) * K)
-            K.fill((2 ** (1. - self.nu)) / gamma(self.nu))
-            K *= tmp ** self.nu
+            tmp = math.sqrt(2 * self.nu) * K
+            K.fill((2 ** (1.0 - self.nu)) / gamma(self.nu))
+            K *= tmp**self.nu
             K *= kv(self.nu, tmp)

         if Y is None:
@@ -1362,25 +1725,29 @@

             # We need to recompute the pairwise dimension-wise distances
             if self.anisotropic:
-                D = (X[:, np.newaxis, :] - X[np.newaxis, :, :])**2 \
-                    / (length_scale ** 2)
+                D = (X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2 / (
+                    length_scale**2
+                )
             else:
                 D = squareform(dists**2)[:, :, np.newaxis]

             if self.nu == 0.5:
-                K_gradient = K[..., np.newaxis] * D \
-                    / np.sqrt(D.sum(2))[:, :, np.newaxis]
-                K_gradient[~np.isfinite(K_gradient)] = 0
+                denominator = np.sqrt(D.sum(axis=2))[:, :, np.newaxis]
+                K_gradient = K[..., np.newaxis] * np.divide(
+                    D, denominator, where=denominator != 0
+                )
             elif self.nu == 1.5:
-                K_gradient = \
-                    3 * D * np.exp(-np.sqrt(3 * D.sum(-1)))[..., np.newaxis]
+                K_gradient = 3 * D * np.exp(-np.sqrt(3 * D.sum(-1)))[..., np.newaxis]
             elif self.nu == 2.5:
                 tmp = np.sqrt(5 * D.sum(-1))[..., np.newaxis]
                 K_gradient = 5.0 / 3.0 * D * (tmp + 1) * np.exp(-tmp)
+            elif self.nu == np.inf:
+                K_gradient = D * K[..., np.newaxis]
             else:
                 # approximate gradient numerically
                 def f(theta):  # helper function
                     return self.clone_with_theta(theta)(X, Y)
+
                 return K, _approx_fprime(self.theta, f, 1e-10)

             if not self.anisotropic:
@@ -1395,43 +1762,84 @@
             return "{0}(length_scale=[{1}], nu={2:.3g})".format(
                 self.__class__.__name__,
                 ", ".join(map("{0:.3g}".format, self.length_scale)),
-                self.nu)
+                self.nu,
+            )
         else:
             return "{0}(length_scale={1:.3g}, nu={2:.3g})".format(
-                self.__class__.__name__, np.ravel(self.length_scale)[0],
-                self.nu)
+                self.__class__.__name__, np.ravel(self.length_scale)[0], self.nu
+            )


 class RationalQuadratic(StationaryKernelMixin, NormalizedKernelMixin, Kernel):
     """Rational Quadratic kernel.

     The RationalQuadratic kernel can be seen as a scale mixture (an infinite
-    sum) of RBF kernels with different characteristic length-scales. It is
-    parameterized by a length-scale parameter length_scale>0 and a scale
-    mixture parameter alpha>0. Only the isotropic variant where length_scale is
-    a scalar is supported at the moment. The kernel given by:
-
-    k(x_i, x_j) = (1 + d(x_i, x_j)^2 / (2*alpha * length_scale^2))^-alpha
+    sum) of RBF kernels with different characteristic length scales. It is
+    parameterized by a length scale parameter :math:`l>0` and a scale
+    mixture parameter :math:`\\alpha>0`. Only the isotropic variant
+    where length_scale :math:`l` is a scalar is supported at the moment.
+    The kernel is given by:
+
+    .. math::
+        k(x_i, x_j) = \\left(
+        1 + \\frac{d(x_i, x_j)^2 }{ 2\\alpha  l^2}\\right)^{-\\alpha}
+
+    where :math:`\\alpha` is the scale mixture parameter, :math:`l` is
+    the length scale of the kernel and :math:`d(\\cdot,\\cdot)` is the
+    Euclidean distance.
+    For advice on how to set the parameters, see e.g. [1]_.
+
+    Read more in the :ref:`User Guide <gp_kernels>`.

     .. versionadded:: 0.18

     Parameters
     ----------
-    length_scale : float > 0, default: 1.0
+    length_scale : float > 0, default=1.0
         The length scale of the kernel.

-    alpha : float > 0, default: 1.0
+    alpha : float > 0, default=1.0
         Scale mixture parameter

-    length_scale_bounds : pair of floats >= 0, default: (1e-5, 1e5)
-        The lower and upper bound on length_scale
-
-    alpha_bounds : pair of floats >= 0, default: (1e-5, 1e5)
-        The lower and upper bound on alpha
-
+    length_scale_bounds : pair of floats >= 0 or "fixed", default=(1e-5, 1e5)
+        The lower and upper bound on 'length_scale'.
+        If set to "fixed", 'length_scale' cannot be changed during
+        hyperparameter tuning.
+
+    alpha_bounds : pair of floats >= 0 or "fixed", default=(1e-5, 1e5)
+        The lower and upper bound on 'alpha'.
+        If set to "fixed", 'alpha' cannot be changed during
+        hyperparameter tuning.
+
+    References
+    ----------
+    .. [1] `David Duvenaud (2014). "The Kernel Cookbook:
+        Advice on Covariance functions".
+        <https://www.cs.toronto.edu/~duvenaud/cookbook/>`_
+
+    Examples
+    --------
+    >>> from sklearn.datasets import load_iris
+    >>> from sklearn.gaussian_process import GaussianProcessClassifier
+    >>> from sklearn.gaussian_process.kernels import RationalQuadratic
+    >>> X, y = load_iris(return_X_y=True)
+    >>> kernel = RationalQuadratic(length_scale=1.0, alpha=1.5)
+    >>> gpc = GaussianProcessClassifier(kernel=kernel,
+    ...         random_state=0).fit(X, y)
+    >>> gpc.score(X, y)
+    0.9733...
+    >>> gpc.predict_proba(X[:2,:])
+    array([[0.8881..., 0.0566..., 0.05518...],
+            [0.8678..., 0.0707... , 0.0614...]])
     """
-    def __init__(self, length_scale=1.0, alpha=1.0,
-                 length_scale_bounds=(1e-5, 1e5), alpha_bounds=(1e-5, 1e5)):
+
+    def __init__(
+        self,
+        length_scale=1.0,
+        alpha=1.0,
+        length_scale_bounds=(1e-5, 1e5),
+        alpha_bounds=(1e-5, 1e5),
+    ):
         self.length_scale = length_scale
         self.alpha = alpha
         self.length_scale_bounds = length_scale_bounds
@@ -1439,8 +1847,7 @@

     @property
     def hyperparameter_length_scale(self):
-        return Hyperparameter(
-            "length_scale", "numeric", self.length_scale_bounds)
+        return Hyperparameter("length_scale", "numeric", self.length_scale_bounds)

     @property
     def hyperparameter_alpha(self):
@@ -1451,56 +1858,60 @@

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
+        X : ndarray of shape (n_samples_X, n_features)
             Left argument of the returned kernel k(X, Y)

-        Y : array, shape (n_samples_Y, n_features), (optional, default=None)
+        Y : ndarray of shape (n_samples_Y, n_features), default=None
             Right argument of the returned kernel k(X, Y). If None, k(X, X)
             if evaluated instead.

-        eval_gradient : bool (optional, default=False)
-            Determines whether the gradient with respect to the kernel
-            hyperparameter is determined. Only supported when Y is None.
-
-        Returns
-        -------
-        K : array, shape (n_samples_X, n_samples_Y)
+        eval_gradient : bool, default=False
+            Determines whether the gradient with respect to the log of
+            the kernel hyperparameter is computed.
+            Only supported when Y is None.
+
+        Returns
+        -------
+        K : ndarray of shape (n_samples_X, n_samples_Y)
             Kernel k(X, Y)

-        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)
-            The gradient of the kernel k(X, X) with respect to the
+        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims)
+            The gradient of the kernel k(X, X) with respect to the log of the
             hyperparameter of the kernel. Only returned when eval_gradient
             is True.
         """
+        if len(np.atleast_1d(self.length_scale)) > 1:
+            raise AttributeError(
+                "RationalQuadratic kernel only supports isotropic version, "
+                "please use a single scalar for length_scale"
+            )
         X = np.atleast_2d(X)
         if Y is None:
-            dists = squareform(pdist(X, metric='sqeuclidean'))
-            tmp = dists / (2 * self.alpha * self.length_scale ** 2)
-            base = (1 + tmp)
-            K = base ** -self.alpha
+            dists = squareform(pdist(X, metric="sqeuclidean"))
+            tmp = dists / (2 * self.alpha * self.length_scale**2)
+            base = 1 + tmp
+            K = base**-self.alpha
             np.fill_diagonal(K, 1)
         else:
             if eval_gradient:
-                raise ValueError(
-                    "Gradient can only be evaluated when Y is None.")
-            dists = cdist(X, Y, metric='sqeuclidean')
-            K = (1 + dists / (2 * self.alpha * self.length_scale ** 2)) \
-                ** -self.alpha
+                raise ValueError("Gradient can only be evaluated when Y is None.")
+            dists = cdist(X, Y, metric="sqeuclidean")
+            K = (1 + dists / (2 * self.alpha * self.length_scale**2)) ** -self.alpha

         if eval_gradient:
             # gradient with respect to length_scale
             if not self.hyperparameter_length_scale.fixed:
-                length_scale_gradient = \
-                    dists * K / (self.length_scale ** 2 * base)
+                length_scale_gradient = dists * K / (self.length_scale**2 * base)
                 length_scale_gradient = length_scale_gradient[:, :, np.newaxis]
             else:  # l is kept fixed
                 length_scale_gradient = np.empty((K.shape[0], K.shape[1], 0))

             # gradient with respect to alpha
             if not self.hyperparameter_alpha.fixed:
-                alpha_gradient = \
-                    K * (-self.alpha * np.log(base)
-                         + dists / (2 * self.length_scale ** 2 * base))
+                alpha_gradient = K * (
+                    -self.alpha * np.log(base)
+                    + dists / (2 * self.length_scale**2 * base)
+                )
                 alpha_gradient = alpha_gradient[:, :, np.newaxis]
             else:  # alpha is kept fixed
                 alpha_gradient = np.empty((K.shape[0], K.shape[1], 0))
@@ -1511,40 +1922,72 @@

     def __repr__(self):
         return "{0}(alpha={1:.3g}, length_scale={2:.3g})".format(
-            self.__class__.__name__, self.alpha, self.length_scale)
+            self.__class__.__name__, self.alpha, self.length_scale
+        )


 class ExpSineSquared(StationaryKernelMixin, NormalizedKernelMixin, Kernel):
-    r"""Exp-Sine-Squared kernel.
-
-    The ExpSineSquared kernel allows modeling periodic functions. It is
-    parameterized by a length-scale parameter length_scale>0 and a periodicity
-    parameter periodicity>0. Only the isotropic variant where l is a scalar is
-    supported at the moment. The kernel given by:
-
-    k(x_i, x_j) =
-    exp(-2 (sin(\pi / periodicity * d(x_i, x_j)) / length_scale) ^ 2)
+    r"""Exp-Sine-Squared kernel (aka periodic kernel).
+
+    The ExpSineSquared kernel allows one to model functions which repeat
+    themselves exactly. It is parameterized by a length scale
+    parameter :math:`l>0` and a periodicity parameter :math:`p>0`.
+    Only the isotropic variant where :math:`l` is a scalar is
+    supported at the moment. The kernel is given by:
+
+    .. math::
+        k(x_i, x_j) = \text{exp}\left(-
+        \frac{ 2\sin^2(\pi d(x_i, x_j)/p) }{ l^ 2} \right)
+
+    where :math:`l` is the length scale of the kernel, :math:`p` the
+    periodicity of the kernel and :math:`d(\\cdot,\\cdot)` is the
+    Euclidean distance.
+
+    Read more in the :ref:`User Guide <gp_kernels>`.

     .. versionadded:: 0.18

     Parameters
     ----------
-    length_scale : float > 0, default: 1.0
+
+    length_scale : float > 0, default=1.0
         The length scale of the kernel.

-    periodicity : float > 0, default: 1.0
+    periodicity : float > 0, default=1.0
         The periodicity of the kernel.

-    length_scale_bounds : pair of floats >= 0, default: (1e-5, 1e5)
-        The lower and upper bound on length_scale
-
-    periodicity_bounds : pair of floats >= 0, default: (1e-5, 1e5)
-        The lower and upper bound on periodicity
-
+    length_scale_bounds : pair of floats >= 0 or "fixed", default=(1e-5, 1e5)
+        The lower and upper bound on 'length_scale'.
+        If set to "fixed", 'length_scale' cannot be changed during
+        hyperparameter tuning.
+
+    periodicity_bounds : pair of floats >= 0 or "fixed", default=(1e-5, 1e5)
+        The lower and upper bound on 'periodicity'.
+        If set to "fixed", 'periodicity' cannot be changed during
+        hyperparameter tuning.
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_friedman2
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor
+    >>> from sklearn.gaussian_process.kernels import ExpSineSquared
+    >>> X, y = make_friedman2(n_samples=50, noise=0, random_state=0)
+    >>> kernel = ExpSineSquared(length_scale=1, periodicity=1)
+    >>> gpr = GaussianProcessRegressor(kernel=kernel, alpha=5,
+    ...         random_state=0).fit(X, y)
+    >>> gpr.score(X, y)
+    0.0144...
+    >>> gpr.predict(X[:2,:], return_std=True)
+    (array([425.6..., 457.5...]), array([0.3894..., 0.3467...]))
     """
-    def __init__(self, length_scale=1.0, periodicity=1.0,
-                 length_scale_bounds=(1e-5, 1e5),
-                 periodicity_bounds=(1e-5, 1e5)):
+
+    def __init__(
+        self,
+        length_scale=1.0,
+        periodicity=1.0,
+        length_scale_bounds=(1e-5, 1e5),
+        periodicity_bounds=(1e-5, 1e5),
+    ):
         self.length_scale = length_scale
         self.periodicity = periodicity
         self.length_scale_bounds = length_scale_bounds
@@ -1552,68 +1995,68 @@

     @property
     def hyperparameter_length_scale(self):
-        return Hyperparameter(
-            "length_scale", "numeric", self.length_scale_bounds)
+        """Returns the length scale"""
+        return Hyperparameter("length_scale", "numeric", self.length_scale_bounds)

     @property
     def hyperparameter_periodicity(self):
-        return Hyperparameter(
-            "periodicity", "numeric", self.periodicity_bounds)
+        return Hyperparameter("periodicity", "numeric", self.periodicity_bounds)

     def __call__(self, X, Y=None, eval_gradient=False):
         """Return the kernel k(X, Y) and optionally its gradient.

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
+        X : ndarray of shape (n_samples_X, n_features)
             Left argument of the returned kernel k(X, Y)

-        Y : array, shape (n_samples_Y, n_features), (optional, default=None)
+        Y : ndarray of shape (n_samples_Y, n_features), default=None
             Right argument of the returned kernel k(X, Y). If None, k(X, X)
             if evaluated instead.

-        eval_gradient : bool (optional, default=False)
-            Determines whether the gradient with respect to the kernel
-            hyperparameter is determined. Only supported when Y is None.
-
-        Returns
-        -------
-        K : array, shape (n_samples_X, n_samples_Y)
+        eval_gradient : bool, default=False
+            Determines whether the gradient with respect to the log of
+            the kernel hyperparameter is computed.
+            Only supported when Y is None.
+
+        Returns
+        -------
+        K : ndarray of shape (n_samples_X, n_samples_Y)
             Kernel k(X, Y)

-        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)
-            The gradient of the kernel k(X, X) with respect to the
-            hyperparameter of the kernel. Only returned when eval_gradient
+        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims), \
+                optional
+            The gradient of the kernel k(X, X) with respect to the log of the
+            hyperparameter of the kernel. Only returned when `eval_gradient`
             is True.
         """
         X = np.atleast_2d(X)
         if Y is None:
-            dists = squareform(pdist(X, metric='euclidean'))
+            dists = squareform(pdist(X, metric="euclidean"))
             arg = np.pi * dists / self.periodicity
             sin_of_arg = np.sin(arg)
-            K = np.exp(- 2 * (sin_of_arg / self.length_scale) ** 2)
+            K = np.exp(-2 * (sin_of_arg / self.length_scale) ** 2)
         else:
             if eval_gradient:
-                raise ValueError(
-                    "Gradient can only be evaluated when Y is None.")
-            dists = cdist(X, Y, metric='euclidean')
-            K = np.exp(- 2 * (np.sin(np.pi / self.periodicity * dists)
-                              / self.length_scale) ** 2)
+                raise ValueError("Gradient can only be evaluated when Y is None.")
+            dists = cdist(X, Y, metric="euclidean")
+            K = np.exp(
+                -2 * (np.sin(np.pi / self.periodicity * dists) / self.length_scale) ** 2
+            )

         if eval_gradient:
             cos_of_arg = np.cos(arg)
             # gradient with respect to length_scale
             if not self.hyperparameter_length_scale.fixed:
-                length_scale_gradient = \
-                    4 / self.length_scale**2 * sin_of_arg**2 * K
+                length_scale_gradient = 4 / self.length_scale**2 * sin_of_arg**2 * K
                 length_scale_gradient = length_scale_gradient[:, :, np.newaxis]
             else:  # length_scale is kept fixed
                 length_scale_gradient = np.empty((K.shape[0], K.shape[1], 0))
             # gradient with respect to p
             if not self.hyperparameter_periodicity.fixed:
-                periodicity_gradient = \
-                    4 * arg / self.length_scale**2 * cos_of_arg \
-                    * sin_of_arg * K
+                periodicity_gradient = (
+                    4 * arg / self.length_scale**2 * cos_of_arg * sin_of_arg * K
+                )
                 periodicity_gradient = periodicity_gradient[:, :, np.newaxis]
             else:  # p is kept fixed
                 periodicity_gradient = np.empty((K.shape[0], K.shape[1], 0))
@@ -1624,35 +2067,65 @@

     def __repr__(self):
         return "{0}(length_scale={1:.3g}, periodicity={2:.3g})".format(
-            self.__class__.__name__, self.length_scale, self.periodicity)
+            self.__class__.__name__, self.length_scale, self.periodicity
+        )


 class DotProduct(Kernel):
     r"""Dot-Product kernel.

     The DotProduct kernel is non-stationary and can be obtained from linear
-    regression by putting N(0, 1) priors on the coefficients of x_d (d = 1, . .
-    . , D) and a prior of N(0, \sigma_0^2) on the bias. The DotProduct kernel
-    is invariant to a rotation of the coordinates about the origin, but not
-    translations. It is parameterized by a parameter sigma_0^2. For
-    sigma_0^2 =0, the kernel is called the homogeneous linear kernel, otherwise
+    regression by putting :math:`N(0, 1)` priors on the coefficients
+    of :math:`x_d (d = 1, . . . , D)` and a prior of :math:`N(0, \sigma_0^2)`
+    on the bias. The DotProduct kernel is invariant to a rotation of
+    the coordinates about the origin, but not translations.
+    It is parameterized by a parameter sigma_0 :math:`\sigma`
+    which controls the inhomogenity of the kernel. For :math:`\sigma_0^2 =0`,
+    the kernel is called the homogeneous linear kernel, otherwise
     it is inhomogeneous. The kernel is given by

-    k(x_i, x_j) = sigma_0 ^ 2 + x_i \cdot x_j
+    .. math::
+        k(x_i, x_j) = \sigma_0 ^ 2 + x_i \cdot x_j

     The DotProduct kernel is commonly combined with exponentiation.
+
+    See [1]_, Chapter 4, Section 4.2, for further details regarding the
+    DotProduct kernel.
+
+    Read more in the :ref:`User Guide <gp_kernels>`.

     .. versionadded:: 0.18

     Parameters
     ----------
-    sigma_0 : float >= 0, default: 1.0
+    sigma_0 : float >= 0, default=1.0
         Parameter controlling the inhomogenity of the kernel. If sigma_0=0,
-        the kernel is homogenous.
-
-    sigma_0_bounds : pair of floats >= 0, default: (1e-5, 1e5)
-        The lower and upper bound on l
-
+        the kernel is homogeneous.
+
+    sigma_0_bounds : pair of floats >= 0 or "fixed", default=(1e-5, 1e5)
+        The lower and upper bound on 'sigma_0'.
+        If set to "fixed", 'sigma_0' cannot be changed during
+        hyperparameter tuning.
+
+    References
+    ----------
+    .. [1] `Carl Edward Rasmussen, Christopher K. I. Williams (2006).
+        "Gaussian Processes for Machine Learning". The MIT Press.
+        <http://www.gaussianprocess.org/gpml/>`_
+
+    Examples
+    --------
+    >>> from sklearn.datasets import make_friedman2
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor
+    >>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel
+    >>> X, y = make_friedman2(n_samples=500, noise=0, random_state=0)
+    >>> kernel = DotProduct() + WhiteKernel()
+    >>> gpr = GaussianProcessRegressor(kernel=kernel,
+    ...         random_state=0).fit(X, y)
+    >>> gpr.score(X, y)
+    0.3680...
+    >>> gpr.predict(X[:2,:], return_std=True)
+    (array([653.0..., 592.1...]), array([316.6..., 316.6...]))
     """

     def __init__(self, sigma_0=1.0, sigma_0_bounds=(1e-5, 1e5)):
@@ -1668,40 +2141,41 @@

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
+        X : ndarray of shape (n_samples_X, n_features)
             Left argument of the returned kernel k(X, Y)

-        Y : array, shape (n_samples_Y, n_features), (optional, default=None)
+        Y : ndarray of shape (n_samples_Y, n_features), default=None
             Right argument of the returned kernel k(X, Y). If None, k(X, X)
             if evaluated instead.

-        eval_gradient : bool (optional, default=False)
-            Determines whether the gradient with respect to the kernel
-            hyperparameter is determined. Only supported when Y is None.
-
-        Returns
-        -------
-        K : array, shape (n_samples_X, n_samples_Y)
+        eval_gradient : bool, default=False
+            Determines whether the gradient with respect to the log of
+            the kernel hyperparameter is computed.
+            Only supported when Y is None.
+
+        Returns
+        -------
+        K : ndarray of shape (n_samples_X, n_samples_Y)
             Kernel k(X, Y)

-        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)
-            The gradient of the kernel k(X, X) with respect to the
-            hyperparameter of the kernel. Only returned when eval_gradient
+        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims),\
+                optional
+            The gradient of the kernel k(X, X) with respect to the log of the
+            hyperparameter of the kernel. Only returned when `eval_gradient`
             is True.
         """
         X = np.atleast_2d(X)
         if Y is None:
-            K = np.inner(X, X) + self.sigma_0 ** 2
+            K = np.inner(X, X) + self.sigma_0**2
         else:
             if eval_gradient:
-                raise ValueError(
-                    "Gradient can only be evaluated when Y is None.")
-            K = np.inner(X, Y) + self.sigma_0 ** 2
+                raise ValueError("Gradient can only be evaluated when Y is None.")
+            K = np.inner(X, Y) + self.sigma_0**2

         if eval_gradient:
             if not self.hyperparameter_sigma_0.fixed:
                 K_gradient = np.empty((K.shape[0], K.shape[1], 1))
-                K_gradient[..., 0] = 2 * self.sigma_0 ** 2
+                K_gradient[..., 0] = 2 * self.sigma_0**2
                 return K, K_gradient
             else:
                 return K, np.empty((X.shape[0], X.shape[0], 0))
@@ -1717,30 +2191,29 @@

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
-            Left argument of the returned kernel k(X, Y)
-
-        Returns
-        -------
-        K_diag : array, shape (n_samples_X,)
-            Diagonal of kernel k(X, X)
-        """
-        return np.einsum('ij,ij->i', X, X) + self.sigma_0 ** 2
+        X : ndarray of shape (n_samples_X, n_features)
+            Left argument of the returned kernel k(X, Y).
+
+        Returns
+        -------
+        K_diag : ndarray of shape (n_samples_X,)
+            Diagonal of kernel k(X, X).
+        """
+        return np.einsum("ij,ij->i", X, X) + self.sigma_0**2

     def is_stationary(self):
-        """Returns whether the kernel is stationary. """
+        """Returns whether the kernel is stationary."""
         return False

     def __repr__(self):
-        return "{0}(sigma_0={1:.3g})".format(
-            self.__class__.__name__, self.sigma_0)
+        return "{0}(sigma_0={1:.3g})".format(self.__class__.__name__, self.sigma_0)


 # adapted from scipy/optimize/optimize.py for functions with 2d output
 def _approx_fprime(xk, f, epsilon, args=()):
     f0 = f(*((xk,) + args))
     grad = np.zeros((f0.shape[0], f0.shape[1], len(xk)), float)
-    ei = np.zeros((len(xk), ), float)
+    ei = np.zeros((len(xk),), float)
     for k in range(len(xk)):
         ei[k] = 1.0
         d = epsilon * ei
@@ -1765,13 +2238,18 @@

     Parameters
     ----------
-    gamma : float >= 0, default: 1.0
-        Parameter gamma of the pairwise kernel specified by metric
-
-    gamma_bounds : pair of floats >= 0, default: (1e-5, 1e5)
-        The lower and upper bound on gamma
-
-    metric : string, or callable, default: "linear"
+    gamma : float, default=1.0
+        Parameter gamma of the pairwise kernel specified by metric. It should
+        be positive.
+
+    gamma_bounds : pair of floats >= 0 or "fixed", default=(1e-5, 1e5)
+        The lower and upper bound on 'gamma'.
+        If set to "fixed", 'gamma' cannot be changed during
+        hyperparameter tuning.
+
+    metric : {"linear", "additive_chi2", "chi2", "poly", "polynomial", \
+              "rbf", "laplacian", "sigmoid", "cosine"} or callable, \
+              default="linear"
         The metric to use when calculating kernel between instances in a
         feature array. If metric is a string, it must be one of the metrics
         in pairwise.PAIRWISE_KERNEL_FUNCTIONS.
@@ -1781,14 +2259,33 @@
         should take two arrays from X as input and return a value indicating
         the distance between them.

-    pairwise_kernels_kwargs : dict, default: None
+    pairwise_kernels_kwargs : dict, default=None
         All entries of this dict (if any) are passed as keyword arguments to
         the pairwise kernel function.

+    Examples
+    --------
+    >>> from sklearn.datasets import load_iris
+    >>> from sklearn.gaussian_process import GaussianProcessClassifier
+    >>> from sklearn.gaussian_process.kernels import PairwiseKernel
+    >>> X, y = load_iris(return_X_y=True)
+    >>> kernel = PairwiseKernel(metric='rbf')
+    >>> gpc = GaussianProcessClassifier(kernel=kernel,
+    ...         random_state=0).fit(X, y)
+    >>> gpc.score(X, y)
+    0.9733...
+    >>> gpc.predict_proba(X[:2,:])
+    array([[0.8880..., 0.05663..., 0.05532...],
+           [0.8676..., 0.07073..., 0.06165...]])
     """

-    def __init__(self, gamma=1.0, gamma_bounds=(1e-5, 1e5), metric="linear",
-                 pairwise_kernels_kwargs=None):
+    def __init__(
+        self,
+        gamma=1.0,
+        gamma_bounds=(1e-5, 1e5),
+        metric="linear",
+        pairwise_kernels_kwargs=None,
+    ):
         self.gamma = gamma
         self.gamma_bounds = gamma_bounds
         self.metric = metric
@@ -1803,25 +2300,27 @@

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
+        X : ndarray of shape (n_samples_X, n_features)
             Left argument of the returned kernel k(X, Y)

-        Y : array, shape (n_samples_Y, n_features), (optional, default=None)
+        Y : ndarray of shape (n_samples_Y, n_features), default=None
             Right argument of the returned kernel k(X, Y). If None, k(X, X)
             if evaluated instead.

-        eval_gradient : bool (optional, default=False)
-            Determines whether the gradient with respect to the kernel
-            hyperparameter is determined. Only supported when Y is None.
-
-        Returns
-        -------
-        K : array, shape (n_samples_X, n_samples_Y)
+        eval_gradient : bool, default=False
+            Determines whether the gradient with respect to the log of
+            the kernel hyperparameter is computed.
+            Only supported when Y is None.
+
+        Returns
+        -------
+        K : ndarray of shape (n_samples_X, n_samples_Y)
             Kernel k(X, Y)

-        K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)
-            The gradient of the kernel k(X, X) with respect to the
-            hyperparameter of the kernel. Only returned when eval_gradient
+        K_gradient : ndarray of shape (n_samples_X, n_samples_X, n_dims),\
+                optional
+            The gradient of the kernel k(X, X) with respect to the log of the
+            hyperparameter of the kernel. Only returned when `eval_gradient`
             is True.
         """
         pairwise_kernels_kwargs = self.pairwise_kernels_kwargs
@@ -1829,9 +2328,14 @@
             pairwise_kernels_kwargs = {}

         X = np.atleast_2d(X)
-        K = pairwise_kernels(X, Y, metric=self.metric, gamma=self.gamma,
-                             filter_params=True,
-                             **pairwise_kernels_kwargs)
+        K = pairwise_kernels(
+            X,
+            Y,
+            metric=self.metric,
+            gamma=self.gamma,
+            filter_params=True,
+            **pairwise_kernels_kwargs,
+        )
         if eval_gradient:
             if self.hyperparameter_gamma.fixed:
                 return K, np.empty((X.shape[0], X.shape[0], 0))
@@ -1839,8 +2343,14 @@
                 # approximate gradient numerically
                 def f(gamma):  # helper function
                     return pairwise_kernels(
-                        X, Y, metric=self.metric, gamma=np.exp(gamma),
-                        filter_params=True, **pairwise_kernels_kwargs)
+                        X,
+                        Y,
+                        metric=self.metric,
+                        gamma=np.exp(gamma),
+                        filter_params=True,
+                        **pairwise_kernels_kwargs,
+                    )
+
                 return K, _approx_fprime(self.theta, f, 1e-10)
         else:
             return K
@@ -1854,21 +2364,22 @@

         Parameters
         ----------
-        X : array, shape (n_samples_X, n_features)
+        X : ndarray of shape (n_samples_X, n_features)
             Left argument of the returned kernel k(X, Y)

         Returns
         -------
-        K_diag : array, shape (n_samples_X,)
+        K_diag : ndarray of shape (n_samples_X,)
             Diagonal of kernel k(X, X)
         """
         # We have to fall back to slow way of computing diagonal
         return np.apply_along_axis(self, 1, X).ravel()

     def is_stationary(self):
-        """Returns whether the kernel is stationary. """
+        """Returns whether the kernel is stationary."""
         return self.metric in ["rbf"]

     def __repr__(self):
         return "{0}(gamma={1}, metric={2})".format(
-            self.__class__.__name__, self.gamma, self.metric)
+            self.__class__.__name__, self.gamma, self.metric
+        )
('sklearn/compose', '_target.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,21 +8,23 @@

 from ..base import BaseEstimator, RegressorMixin, clone
 from ..utils.validation import check_is_fitted
-from ..utils import check_array, safe_indexing
+from ..utils._tags import _safe_tags
+from ..utils import check_array, _safe_indexing
 from ..preprocessing import FunctionTransformer
-
-__all__ = ['TransformedTargetRegressor']
-
-
-class TransformedTargetRegressor(BaseEstimator, RegressorMixin):
+from ..exceptions import NotFittedError
+
+__all__ = ["TransformedTargetRegressor"]
+
+
+class TransformedTargetRegressor(RegressorMixin, BaseEstimator):
     """Meta-estimator to regress on a transformed target.

-    Useful for applying a non-linear transformation in regression
-    problems. This transformation can be given as a Transformer such as the
-    QuantileTransformer or as a function and its inverse such as ``log`` and
-    ``exp``.
-
-    The computation during ``fit`` is::
+    Useful for applying a non-linear transformation to the target `y` in
+    regression problems. This transformation can be given as a Transformer
+    such as the :class:`~sklearn.preprocessing.QuantileTransformer` or as a
+    function and its inverse such as `np.log` and `np.exp`.
+
+    The computation during :meth:`fit` is::

         regressor.fit(X, func(y))

@@ -30,7 +32,7 @@

         regressor.fit(X, transformer.transform(y))

-    The computation during ``predict`` is::
+    The computation during :meth:`predict` is::

         inverse_func(regressor.predict(X))

@@ -38,37 +40,41 @@

         transformer.inverse_transform(regressor.predict(X))

-    Read more in the :ref:`User Guide <preprocessing_targets>`.
+    Read more in the :ref:`User Guide <transformed_target_regressor>`.
+
+    .. versionadded:: 0.20

     Parameters
     ----------
-    regressor : object, default=LinearRegression()
-        Regressor object such as derived from ``RegressorMixin``. This
-        regressor will automatically be cloned each time prior to fitting.
+    regressor : object, default=None
+        Regressor object such as derived from
+        :class:`~sklearn.base.RegressorMixin`. This regressor will
+        automatically be cloned each time prior to fitting. If `regressor is
+        None`, :class:`~sklearn.linear_model.LinearRegression` is created and used.

     transformer : object, default=None
-        Estimator object such as derived from ``TransformerMixin``. Cannot be
-        set at the same time as ``func`` and ``inverse_func``. If
-        ``transformer`` is ``None`` as well as ``func`` and ``inverse_func``,
-        the transformer will be an identity transformer. Note that the
-        transformer will be cloned during fitting. Also, the transformer is
-        restricting ``y`` to be a numpy array.
-
-    func : function, optional
-        Function to apply to ``y`` before passing to ``fit``. Cannot be set at
-        the same time as ``transformer``. The function needs to return a
-        2-dimensional array. If ``func`` is ``None``, the function used will be
-        the identity function.
-
-    inverse_func : function, optional
+        Estimator object such as derived from
+        :class:`~sklearn.base.TransformerMixin`. Cannot be set at the same time
+        as `func` and `inverse_func`. If `transformer is None` as well as
+        `func` and `inverse_func`, the transformer will be an identity
+        transformer. Note that the transformer will be cloned during fitting.
+        Also, the transformer is restricting `y` to be a numpy array.
+
+    func : function, default=None
+        Function to apply to `y` before passing to :meth:`fit`. Cannot be set
+        at the same time as `transformer`. The function needs to return a
+        2-dimensional array. If `func is None`, the function used will be the
+        identity function.
+
+    inverse_func : function, default=None
         Function to apply to the prediction of the regressor. Cannot be set at
-        the same time as ``transformer`` as well. The function needs to return
-        a 2-dimensional array. The inverse function is used to return
+        the same time as `transformer`. The function needs to return a
+        2-dimensional array. The inverse function is used to return
         predictions to the same space of the original training labels.

     check_inverse : bool, default=True
-        Whether to check that ``transform`` followed by ``inverse_transform``
-        or ``func`` followed by ``inverse_func`` leads to the original targets.
+        Whether to check that `transform` followed by `inverse_transform`
+        or `func` followed by `inverse_func` leads to the original targets.

     Attributes
     ----------
@@ -76,7 +82,33 @@
         Fitted regressor.

     transformer_ : object
-        Transformer used in ``fit`` and ``predict``.
+        Transformer used in :meth:`fit` and :meth:`predict`.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`. Only defined if the
+        underlying regressor exposes such an attribute when fit.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    sklearn.preprocessing.FunctionTransformer : Construct a transformer from an
+        arbitrary callable.
+
+    Notes
+    -----
+    Internally, the target `y` is always converted into a 2-dimensional array
+    to be used by scikit-learn transformers. At the time of prediction, the
+    output will be reshaped to a have the same number of dimensions as `y`.
+
+    See :ref:`examples/compose/plot_transformed_target.py
+    <sphx_glr_auto_examples_compose_plot_transformed_target.py>`.

     Examples
     --------
@@ -87,25 +119,23 @@
     ...                                 func=np.log, inverse_func=np.exp)
     >>> X = np.arange(4).reshape(-1, 1)
     >>> y = np.exp(2 * X).ravel()
-    >>> tt.fit(X, y) # doctest: +ELLIPSIS
+    >>> tt.fit(X, y)
     TransformedTargetRegressor(...)
     >>> tt.score(X, y)
     1.0
     >>> tt.regressor_.coef_
     array([2.])
-
-    Notes
-    -----
-    Internally, the target ``y`` is always converted into a 2-dimensional array
-    to be used by scikit-learn transformers. At the time of prediction, the
-    output will be reshaped to a have the same number of dimensions as ``y``.
-
-    See :ref:`examples/compose/plot_transformed_target.py
-    <sphx_glr_auto_examples_compose_plot_transformed_target.py>`.
-
     """
-    def __init__(self, regressor=None, transformer=None,
-                 func=None, inverse_func=None, check_inverse=True):
+
+    def __init__(
+        self,
+        regressor=None,
+        *,
+        transformer=None,
+        func=None,
+        inverse_func=None,
+        check_inverse=True,
+    ):
         self.regressor = regressor
         self.transformer = transformer
         self.func = func
@@ -119,19 +149,25 @@
         check on a subset (optional).

         """
-        if (self.transformer is not None and
-                (self.func is not None or self.inverse_func is not None)):
-            raise ValueError("'transformer' and functions 'func'/"
-                             "'inverse_func' cannot both be set.")
+        if self.transformer is not None and (
+            self.func is not None or self.inverse_func is not None
+        ):
+            raise ValueError(
+                "'transformer' and functions 'func'/'inverse_func' cannot both be set."
+            )
         elif self.transformer is not None:
             self.transformer_ = clone(self.transformer)
         else:
             if self.func is not None and self.inverse_func is None:
-                raise ValueError("When 'func' is provided, 'inverse_func' must"
-                                 " also be provided")
+                raise ValueError(
+                    "When 'func' is provided, 'inverse_func' must also be provided"
+                )
             self.transformer_ = FunctionTransformer(
-                func=self.func, inverse_func=self.inverse_func, validate=True,
-                check_inverse=self.check_inverse)
+                func=self.func,
+                inverse_func=self.inverse_func,
+                validate=True,
+                check_inverse=self.check_inverse,
+            )
         # XXX: sample_weight is not currently passed to the
         # transformer. However, if transformer starts using sample_weight, the
         # code should be modified accordingly. At the time to consider the
@@ -139,37 +175,52 @@
         self.transformer_.fit(y)
         if self.check_inverse:
             idx_selected = slice(None, None, max(1, y.shape[0] // 10))
-            y_sel = safe_indexing(y, idx_selected)
+            y_sel = _safe_indexing(y, idx_selected)
             y_sel_t = self.transformer_.transform(y_sel)
-            if not np.allclose(y_sel,
-                               self.transformer_.inverse_transform(y_sel_t)):
-                warnings.warn("The provided functions or transformer are"
-                              " not strictly inverse of each other. If"
-                              " you are sure you want to proceed regardless"
-                              ", set 'check_inverse=False'", UserWarning)
-
-    def fit(self, X, y, sample_weight=None):
+            if not np.allclose(y_sel, self.transformer_.inverse_transform(y_sel_t)):
+                warnings.warn(
+                    "The provided functions or transformer are"
+                    " not strictly inverse of each other. If"
+                    " you are sure you want to proceed regardless"
+                    ", set 'check_inverse=False'",
+                    UserWarning,
+                )
+
+    def fit(self, X, y, **fit_params):
         """Fit the model according to the given training data.

         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            Training vector, where n_samples is the number of samples and
-            n_features is the number of features.
-
-        y : array-like, shape (n_samples,)
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Training vector, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,)
             Target values.

-        sample_weight : array-like, shape (n_samples,) optional
-            Array of weights that are assigned to individual samples.
-            If not provided, then each sample is given unit weight.
+        **fit_params : dict
+            Parameters passed to the `fit` method of the underlying
+            regressor.

         Returns
         -------
         self : object
+            Fitted estimator.
         """
-        y = check_array(y, accept_sparse=False, force_all_finite=True,
-                        ensure_2d=False, dtype='numeric')
+        if y is None:
+            raise ValueError(
+                f"This {self.__class__.__name__} estimator "
+                "requires y to be passed, but the target y is None."
+            )
+        y = check_array(
+            y,
+            input_name="y",
+            accept_sparse=False,
+            force_all_finite=True,
+            ensure_2d=False,
+            dtype="numeric",
+            allow_nd=True,
+        )

         # store the number of dimension of the target to predict an array of
         # similar shape at predict
@@ -193,46 +244,77 @@

         if self.regressor is None:
             from ..linear_model import LinearRegression
+
             self.regressor_ = LinearRegression()
         else:
             self.regressor_ = clone(self.regressor)

-        if sample_weight is None:
-            self.regressor_.fit(X, y_trans)
-        else:
-            self.regressor_.fit(X, y_trans, sample_weight=sample_weight)
+        self.regressor_.fit(X, y_trans, **fit_params)
+
+        if hasattr(self.regressor_, "feature_names_in_"):
+            self.feature_names_in_ = self.regressor_.feature_names_in_

         return self

-    def predict(self, X):
+    def predict(self, X, **predict_params):
         """Predict using the base regressor, applying inverse.

-        The regressor is used to predict and the ``inverse_func`` or
-        ``inverse_transform`` is applied before returning the prediction.
+        The regressor is used to predict and the `inverse_func` or
+        `inverse_transform` is applied before returning the prediction.

         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape = (n_samples, n_features)
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
             Samples.
+
+        **predict_params : dict of str -> object
+            Parameters passed to the `predict` method of the underlying
+            regressor.

         Returns
         -------
-        y_hat : array, shape = (n_samples,)
+        y_hat : ndarray of shape (n_samples,)
             Predicted values.
-
         """
-        check_is_fitted(self, "regressor_")
-        pred = self.regressor_.predict(X)
+        check_is_fitted(self)
+        pred = self.regressor_.predict(X, **predict_params)
         if pred.ndim == 1:
-            pred_trans = self.transformer_.inverse_transform(
-                pred.reshape(-1, 1))
+            pred_trans = self.transformer_.inverse_transform(pred.reshape(-1, 1))
         else:
             pred_trans = self.transformer_.inverse_transform(pred)
-        if (self._training_dim == 1 and
-                pred_trans.ndim == 2 and pred_trans.shape[1] == 1):
+        if (
+            self._training_dim == 1
+            and pred_trans.ndim == 2
+            and pred_trans.shape[1] == 1
+        ):
             pred_trans = pred_trans.squeeze(axis=1)

         return pred_trans

     def _more_tags(self):
-        return {'poor_score': True, 'no_validation': True}
+        regressor = self.regressor
+        if regressor is None:
+            from ..linear_model import LinearRegression
+
+            regressor = LinearRegression()
+
+        return {
+            "poor_score": True,
+            "multioutput": _safe_tags(regressor, key="multioutput"),
+        }
+
+    @property
+    def n_features_in_(self):
+        """Number of features seen during :term:`fit`."""
+        # For consistency with other estimators we raise a AttributeError so
+        # that hasattr() returns False the estimator isn't fitted.
+        try:
+            check_is_fitted(self)
+        except NotFittedError as nfe:
+            raise AttributeError(
+                "{} object has no n_features_in_ attribute.".format(
+                    self.__class__.__name__
+                )
+            ) from nfe
+
+        return self.regressor_.n_features_in_
('sklearn/compose', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,12 +5,17 @@

 """

-from ._column_transformer import ColumnTransformer, make_column_transformer
+from ._column_transformer import (
+    ColumnTransformer,
+    make_column_transformer,
+    make_column_selector,
+)
 from ._target import TransformedTargetRegressor


 __all__ = [
-    'ColumnTransformer',
-    'make_column_transformer',
-    'TransformedTargetRegressor',
+    "ColumnTransformer",
+    "make_column_transformer",
+    "TransformedTargetRegressor",
+    "make_column_selector",
 ]
('sklearn/compose', '_column_transformer.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,31 +6,37 @@
 # Author: Andreas Mueller
 #         Joris Van den Bossche
 # License: BSD
-
 from itertools import chain
+from collections import Counter

 import numpy as np
-import warnings
 from scipy import sparse
+from joblib import Parallel

 from ..base import clone, TransformerMixin
-from ..utils._joblib import Parallel, delayed
+from ..utils._estimator_html_repr import _VisualBlock
 from ..pipeline import _fit_transform_one, _transform_one, _name_estimators
 from ..preprocessing import FunctionTransformer
 from ..utils import Bunch
+from ..utils import _safe_indexing
+from ..utils import _get_column_indices
+from ..utils.deprecation import deprecated
 from ..utils.metaestimators import _BaseComposition
-from ..utils.validation import check_array, check_is_fitted
-
-
-__all__ = ['ColumnTransformer', 'make_column_transformer']
-
-
-_ERR_MSG_1DCOLUMN = ("1D data passed to a transformer that expects 2D data. "
-                     "Try to specify the column selection as a list of one "
-                     "item instead of a scalar.")
-
-
-class ColumnTransformer(_BaseComposition, TransformerMixin):
+from ..utils.validation import check_array, check_is_fitted, _check_feature_names_in
+from ..utils.fixes import delayed
+
+
+__all__ = ["ColumnTransformer", "make_column_transformer", "make_column_selector"]
+
+
+_ERR_MSG_1DCOLUMN = (
+    "1D data passed to a transformer that expects 2D data. "
+    "Try to specify the column selection as a list of one "
+    "item instead of a scalar."
+)
+
+
+class ColumnTransformer(TransformerMixin, _BaseComposition):
     """Applies transformers to columns of an array or pandas DataFrame.

     This estimator allows different columns or column subsets of the input
@@ -46,29 +52,30 @@
     Parameters
     ----------
     transformers : list of tuples
-        List of (name, transformer, column(s)) tuples specifying the
+        List of (name, transformer, columns) tuples specifying the
         transformer objects to be applied to subsets of the data.

-        name : string
+        name : str
             Like in Pipeline and FeatureUnion, this allows the transformer and
             its parameters to be set using ``set_params`` and searched in grid
             search.
-        transformer : estimator or {'passthrough', 'drop'}
-            Estimator must support `fit` and `transform`. Special-cased
-            strings 'drop' and 'passthrough' are accepted as well, to
-            indicate to drop the columns or to pass them through untransformed,
-            respectively.
-        column(s) : string or int, array-like of string or int, slice, \
-boolean mask array or callable
+        transformer : {'drop', 'passthrough'} or estimator
+            Estimator must support :term:`fit` and :term:`transform`.
+            Special-cased strings 'drop' and 'passthrough' are accepted as
+            well, to indicate to drop the columns or to pass them through
+            untransformed, respectively.
+        columns :  str, array-like of str, int, array-like of int, \
+                array-like of bool, slice or callable
             Indexes the data on its second axis. Integers are interpreted as
             positional columns, while strings can reference DataFrame columns
             by name.  A scalar string or int should be used where
             ``transformer`` expects X to be a 1d array-like (vector),
             otherwise a 2d array will be passed to the transformer.
             A callable is passed the input data `X` and can return any of the
-            above.
-
-    remainder : {'drop', 'passthrough'} or estimator, default 'drop'
+            above. To select multiple columns by name or dtype, you can use
+            :obj:`make_column_selector`.
+
+    remainder : {'drop', 'passthrough'} or estimator, default='drop'
         By default, only the specified columns in `transformers` are
         transformed and combined in the output, and the non-specified
         columns are dropped. (default of ``'drop'``).
@@ -79,28 +86,38 @@
         By setting ``remainder`` to be an estimator, the remaining
         non-specified columns will use the ``remainder`` estimator. The
         estimator must support :term:`fit` and :term:`transform`.
-
-    sparse_threshold : float, default = 0.3
+        Note that using this feature requires that the DataFrame columns
+        input at :term:`fit` and :term:`transform` have identical order.
+
+    sparse_threshold : float, default=0.3
         If the output of the different transformers contains sparse matrices,
         these will be stacked as a sparse matrix if the overall density is
         lower than this value. Use ``sparse_threshold=0`` to always return
         dense.  When the transformed output consists of all dense data, the
         stacked result will be dense, and this keyword will be ignored.

-    n_jobs : int or None, optional (default=None)
+    n_jobs : int, default=None
         Number of jobs to run in parallel.
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.

-    transformer_weights : dict, optional
+    transformer_weights : dict, default=None
         Multiplicative weights for features per transformer. The output of the
         transformer is multiplied by these weights. Keys are transformer names,
         values the weights.

-    verbose : boolean, optional(default=False)
+    verbose : bool, default=False
         If True, the time elapsed while fitting each transformer will be
         printed as it is completed.
+
+    verbose_feature_names_out : bool, default=True
+        If True, :meth:`get_feature_names_out` will prefix all feature names
+        with the name of the transformer that generated that feature.
+        If False, :meth:`get_feature_names_out` will not prefix any feature
+        names and will error if feature names are not unique.
+
+        .. versionadded:: 1.0

     Attributes
     ----------
@@ -116,15 +133,37 @@
         ``len(transformers_)==len(transformers)+1``, otherwise
         ``len(transformers_)==len(transformers)``.

-    named_transformers_ : Bunch object, a dictionary with attribute access
+    named_transformers_ : :class:`~sklearn.utils.Bunch`
         Read-only attribute to access any transformer by given name.
         Keys are transformer names and values are the fitted transformer
         objects.

-    sparse_output_ : boolean
-        Boolean flag indicating wether the output of ``transform`` is a
+    sparse_output_ : bool
+        Boolean flag indicating whether the output of ``transform`` is a
         sparse matrix or a dense numpy array, which depends on the output
         of the individual transformers and the `sparse_threshold` keyword.
+
+    output_indices_ : dict
+        A dictionary from each transformer name to a slice, where the slice
+        corresponds to indices in the transformed output. This is useful to
+        inspect which transformer is responsible for which transformed
+        feature(s).
+
+        .. versionadded:: 1.0
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`. Only defined if the
+        underlying transformers expose such an attribute when fit.
+
+        .. versionadded:: 0.24
+
+    See Also
+    --------
+    make_column_transformer : Convenience function for
+        combining the outputs of multiple transformer objects applied to
+        column subsets of the original feature space.
+    make_column_selector : Convenience function for selecting
+        columns based on datatype or the columns name with a regex pattern.

     Notes
     -----
@@ -134,12 +173,6 @@
     dropped from the resulting transformed feature matrix, unless specified
     in the `passthrough` keyword. Those columns specified with `passthrough`
     are added at the right to the output of the transformers.
-
-    See also
-    --------
-    sklearn.compose.make_column_transformer : convenience function for
-        combining the outputs of multiple transformer objects applied to
-        column subsets of the original feature space.

     Examples
     --------
@@ -154,26 +187,31 @@
     >>> # Normalizer scales each row of X to unit norm. A separate scaling
     >>> # is applied for the two first and two last elements of each
     >>> # row independently.
-    >>> ct.fit_transform(X)    # doctest: +NORMALIZE_WHITESPACE
+    >>> ct.fit_transform(X)
     array([[0. , 1. , 0.5, 0.5],
            [0.5, 0.5, 0. , 1. ]])
-
     """
-    _required_parameters = ['transformers']
-
-    def __init__(self,
-                 transformers,
-                 remainder='drop',
-                 sparse_threshold=0.3,
-                 n_jobs=None,
-                 transformer_weights=None,
-                 verbose=False):
+
+    _required_parameters = ["transformers"]
+
+    def __init__(
+        self,
+        transformers,
+        *,
+        remainder="drop",
+        sparse_threshold=0.3,
+        n_jobs=None,
+        transformer_weights=None,
+        verbose=False,
+        verbose_feature_names_out=True,
+    ):
         self.transformers = transformers
         self.remainder = remainder
         self.sparse_threshold = sparse_threshold
         self.n_jobs = n_jobs
         self.transformer_weights = transformer_weights
         self.verbose = verbose
+        self.verbose_feature_names_out = verbose_feature_names_out

     @property
     def _transformers(self):
@@ -183,43 +221,62 @@
         of get_params via BaseComposition._get_params which expects lists
         of tuples of len 2.
         """
-        return [(name, trans) for name, trans, _ in self.transformers]
+        try:
+            return [(name, trans) for name, trans, _ in self.transformers]
+        except (TypeError, ValueError):
+            return self.transformers

     @_transformers.setter
     def _transformers(self, value):
-        self.transformers = [
-            (name, trans, col) for ((name, trans), (_, _, col))
-            in zip(value, self.transformers)]
+        try:
+            self.transformers = [
+                (name, trans, col)
+                for ((name, trans), (_, _, col)) in zip(value, self.transformers)
+            ]
+        except (TypeError, ValueError):
+            self.transformers = value

     def get_params(self, deep=True):
         """Get parameters for this estimator.

+        Returns the parameters given in the constructor as well as the
+        estimators contained within the `transformers` of the
+        `ColumnTransformer`.
+
         Parameters
         ----------
-        deep : boolean, optional
+        deep : bool, default=True
             If True, will return the parameters for this estimator and
             contained subobjects that are estimators.

         Returns
         -------
-        params : mapping of string to any
+        params : dict
             Parameter names mapped to their values.
         """
-        return self._get_params('_transformers', deep=deep)
+        return self._get_params("_transformers", deep=deep)

     def set_params(self, **kwargs):
         """Set the parameters of this estimator.

-        Valid parameter keys can be listed with ``get_params()``.
+        Valid parameter keys can be listed with ``get_params()``. Note that you
+        can directly set the parameters of the estimators contained in
+        `transformers` of `ColumnTransformer`.
+
+        Parameters
+        ----------
+        **kwargs : dict
+            Estimator parameters.

         Returns
         -------
-        self
-        """
-        self._set_params('_transformers', **kwargs)
+        self : ColumnTransformer
+            This estimator.
+        """
+        self._set_params("_transformers", **kwargs)
         return self

-    def _iter(self, fitted=False, replace_strings=False):
+    def _iter(self, fitted=False, replace_strings=False, column_as_strings=False):
         """
         Generate (name, trans, column, weight) tuples.

@@ -233,28 +290,37 @@
         else:
             # interleave the validated column specifiers
             transformers = [
-                (name, trans, column) for (name, trans, _), column
-                in zip(self.transformers, self._columns)
+                (name, trans, column)
+                for (name, trans, _), column in zip(self.transformers, self._columns)
             ]
             # add transformer tuple for remainder
-            if self._remainder[2] is not None:
+            if self._remainder[2]:
                 transformers = chain(transformers, [self._remainder])
         get_weight = (self.transformer_weights or {}).get

-        for name, trans, column in transformers:
+        for name, trans, columns in transformers:
             if replace_strings:
                 # replace 'passthrough' with identity transformer and
                 # skip in case of 'drop'
-                if trans == 'passthrough':
-                    trans = FunctionTransformer(
-                        validate=False, accept_sparse=True,
-                        check_inverse=False)
-                elif trans == 'drop':
+                if trans == "passthrough":
+                    trans = FunctionTransformer(accept_sparse=True, check_inverse=False)
+                elif trans == "drop":
                     continue
-                elif _is_empty_column_selection(column):
+                elif _is_empty_column_selection(columns):
                     continue

-            yield (name, trans, column, get_weight(name))
+            if column_as_strings:
+                # Convert all columns to using their string labels
+                columns_is_scalar = np.isscalar(columns)
+
+                indices = self._transformer_to_input_indices[name]
+                columns = self.feature_names_in_[indices]
+
+                if columns_is_scalar:
+                    # selection is done with one dimension
+                    columns = columns[0]
+
+            yield (name, trans, columns, get_weight(name))

     def _validate_transformers(self):
         if not self.transformers:
@@ -267,48 +333,52 @@

         # validate estimators
         for t in transformers:
-            if t in ('drop', 'passthrough'):
+            if t in ("drop", "passthrough"):
                 continue
-            if (not (hasattr(t, "fit") or hasattr(t, "fit_transform")) or not
-                    hasattr(t, "transform")):
-                raise TypeError("All estimators should implement fit and "
-                                "transform, or can be 'drop' or 'passthrough' "
-                                "specifiers. '%s' (type %s) doesn't." %
-                                (t, type(t)))
+            if not (hasattr(t, "fit") or hasattr(t, "fit_transform")) or not hasattr(
+                t, "transform"
+            ):
+                raise TypeError(
+                    "All estimators should implement fit and "
+                    "transform, or can be 'drop' or 'passthrough' "
+                    "specifiers. '%s' (type %s) doesn't." % (t, type(t))
+                )

     def _validate_column_callables(self, X):
         """
         Converts callable column specifications.
         """
-        columns = []
-        for _, _, column in self.transformers:
-            if callable(column):
-                column = column(X)
-            columns.append(column)
-        self._columns = columns
+        all_columns = []
+        transformer_to_input_indices = {}
+        for name, _, columns in self.transformers:
+            if callable(columns):
+                columns = columns(X)
+            all_columns.append(columns)
+            transformer_to_input_indices[name] = _get_column_indices(X, columns)
+
+        self._columns = all_columns
+        self._transformer_to_input_indices = transformer_to_input_indices

     def _validate_remainder(self, X):
         """
         Validates ``remainder`` and defines ``_remainder`` targeting
         the remaining columns.
         """
-        is_transformer = ((hasattr(self.remainder, "fit")
-                           or hasattr(self.remainder, "fit_transform"))
-                          and hasattr(self.remainder, "transform"))
-        if (self.remainder not in ('drop', 'passthrough')
-                and not is_transformer):
+        is_transformer = (
+            hasattr(self.remainder, "fit") or hasattr(self.remainder, "fit_transform")
+        ) and hasattr(self.remainder, "transform")
+        if self.remainder not in ("drop", "passthrough") and not is_transformer:
             raise ValueError(
                 "The remainder keyword needs to be one of 'drop', "
-                "'passthrough', or estimator. '%s' was passed instead" %
-                self.remainder)
-
-        n_columns = X.shape[1]
-        cols = []
-        for columns in self._columns:
-            cols.extend(_get_column_indices(X, columns))
-        remaining_idx = sorted(list(set(range(n_columns)) - set(cols))) or None
-
-        self._remainder = ('remainder', self.remainder, remaining_idx)
+                "'passthrough', or estimator. '%s' was passed instead"
+                % self.remainder
+            )
+
+        self._n_features = X.shape[1]
+        cols = set(chain(*self._transformer_to_input_indices.values()))
+        remaining = sorted(set(range(self._n_features)) - cols)
+        self._remainder = ("remainder", self.remainder, remaining)
+        self._transformer_to_input_indices["remainder"] = remaining

     @property
     def named_transformers_(self):
@@ -317,12 +387,14 @@
         Read-only attribute to access any transformer by given name.
         Keys are transformer names and values are the fitted transformer
         objects.
-
         """
         # Use Bunch object to improve autocomplete
-        return Bunch(**{name: trans for name, trans, _
-                        in self.transformers_})
-
+        return Bunch(**{name: trans for name, trans, _ in self.transformers_})
+
+    @deprecated(
+        "get_feature_names is deprecated in 1.0 and will be removed "
+        "in 1.2. Please use get_feature_names_out instead."
+    )
     def get_feature_names(self):
         """Get feature names from all transformers.

@@ -331,22 +403,124 @@
         feature_names : list of strings
             Names of the features produced by transform.
         """
-        check_is_fitted(self, 'transformers_')
+        check_is_fitted(self)
         feature_names = []
-        for name, trans, _, _ in self._iter(fitted=True):
-            if trans == 'drop':
+        for name, trans, column, _ in self._iter(fitted=True):
+            if trans == "drop" or _is_empty_column_selection(column):
                 continue
-            elif trans == 'passthrough':
-                raise NotImplementedError(
-                    "get_feature_names is not yet supported when using "
-                    "a 'passthrough' transformer.")
-            elif not hasattr(trans, 'get_feature_names'):
-                raise AttributeError("Transformer %s (type %s) does not "
-                                     "provide get_feature_names."
-                                     % (str(name), type(trans).__name__))
-            feature_names.extend([name + "__" + f for f in
-                                  trans.get_feature_names()])
+            if trans == "passthrough":
+                if hasattr(self, "feature_names_in_"):
+                    if (not isinstance(column, slice)) and all(
+                        isinstance(col, str) for col in column
+                    ):
+                        feature_names.extend(column)
+                    else:
+                        feature_names.extend(self.feature_names_in_[column])
+                else:
+                    indices = np.arange(self._n_features)
+                    feature_names.extend(["x%d" % i for i in indices[column]])
+                continue
+            if not hasattr(trans, "get_feature_names"):
+                raise AttributeError(
+                    "Transformer %s (type %s) does not provide get_feature_names."
+                    % (str(name), type(trans).__name__)
+                )
+            feature_names.extend([f"{name}__{f}" for f in trans.get_feature_names()])
         return feature_names
+
+    def _get_feature_name_out_for_transformer(
+        self, name, trans, column, feature_names_in
+    ):
+        """Gets feature names of transformer.
+
+        Used in conjunction with self._iter(fitted=True) in get_feature_names_out.
+        """
+        column_indices = self._transformer_to_input_indices[name]
+        names = feature_names_in[column_indices]
+        if trans == "drop" or _is_empty_column_selection(column):
+            return
+        elif trans == "passthrough":
+            return names
+
+        # An actual transformer
+        if not hasattr(trans, "get_feature_names_out"):
+            raise AttributeError(
+                f"Transformer {name} (type {type(trans).__name__}) does "
+                "not provide get_feature_names_out."
+            )
+        return trans.get_feature_names_out(names)
+
+    def get_feature_names_out(self, input_features=None):
+        """Get output feature names for transformation.
+
+        Parameters
+        ----------
+        input_features : array-like of str or None, default=None
+            Input features.
+
+            - If `input_features` is `None`, then `feature_names_in_` is
+              used as feature names in. If `feature_names_in_` is not defined,
+              then the following input feature names are generated:
+              `["x0", "x1", ..., "x(n_features_in_ - 1)"]`.
+            - If `input_features` is an array-like, then `input_features` must
+              match `feature_names_in_` if `feature_names_in_` is defined.
+
+        Returns
+        -------
+        feature_names_out : ndarray of str objects
+            Transformed feature names.
+        """
+        check_is_fitted(self)
+        input_features = _check_feature_names_in(self, input_features)
+
+        # List of tuples (name, feature_names_out)
+        transformer_with_feature_names_out = []
+        for name, trans, column, _ in self._iter(fitted=True):
+            feature_names_out = self._get_feature_name_out_for_transformer(
+                name, trans, column, input_features
+            )
+            if feature_names_out is None:
+                continue
+            transformer_with_feature_names_out.append((name, feature_names_out))
+
+        if not transformer_with_feature_names_out:
+            # No feature names
+            return np.array([], dtype=object)
+
+        if self.verbose_feature_names_out:
+            # Prefix the feature names out with the transformers name
+            names = list(
+                chain.from_iterable(
+                    (f"{name}__{i}" for i in feature_names_out)
+                    for name, feature_names_out in transformer_with_feature_names_out
+                )
+            )
+            return np.asarray(names, dtype=object)
+
+        # verbose_feature_names_out is False
+        # Check that names are all unique without a prefix
+        feature_names_count = Counter(
+            chain.from_iterable(s for _, s in transformer_with_feature_names_out)
+        )
+        top_6_overlap = [
+            name for name, count in feature_names_count.most_common(6) if count > 1
+        ]
+        top_6_overlap.sort()
+        if top_6_overlap:
+            if len(top_6_overlap) == 6:
+                # There are more than 5 overlapping names, we only show the 5
+                # of the feature names
+                names_repr = str(top_6_overlap[:5])[:-1] + ", ...]"
+            else:
+                names_repr = str(top_6_overlap)
+            raise ValueError(
+                f"Output feature names: {names_repr} are not unique. Please set "
+                "verbose_feature_names_out=True to add prefixes to feature names"
+            )
+
+        return np.concatenate(
+            [name for _, name in transformer_with_feature_names_out],
+        )

     def _update_fitted_transformers(self, transformers):
         # transformers are fitted; excludes 'drop' cases
@@ -354,13 +528,13 @@
         transformers_ = []

         for name, old, column, _ in self._iter():
-            if old == 'drop':
-                trans = 'drop'
-            elif old == 'passthrough':
+            if old == "drop":
+                trans = "drop"
+            elif old == "passthrough":
                 # FunctionTransformer is present in list of transformers,
                 # so get next transformer, but save original string
                 next(fitted_transformers)
-                trans = 'passthrough'
+                trans = "passthrough"
             elif _is_empty_column_selection(column):
                 trans = old
             else:
@@ -376,20 +550,44 @@
         Ensure that the output of each transformer is 2D. Otherwise
         hstack can raise an error or produce incorrect results.
         """
-        names = [name for name, _, _, _ in self._iter(fitted=True,
-                                                      replace_strings=True)]
+        names = [
+            name for name, _, _, _ in self._iter(fitted=True, replace_strings=True)
+        ]
         for Xs, name in zip(result, names):
-            if not getattr(Xs, 'ndim', 0) == 2:
+            if not getattr(Xs, "ndim", 0) == 2:
                 raise ValueError(
                     "The output of the '{0}' transformer should be 2D (scipy "
-                    "matrix, array, or pandas DataFrame).".format(name))
+                    "matrix, array, or pandas DataFrame).".format(name)
+                )
+
+    def _record_output_indices(self, Xs):
+        """
+        Record which transformer produced which column.
+        """
+        idx = 0
+        self.output_indices_ = {}
+
+        for transformer_idx, (name, _, _, _) in enumerate(
+            self._iter(fitted=True, replace_strings=True)
+        ):
+            n_columns = Xs[transformer_idx].shape[1]
+            self.output_indices_[name] = slice(idx, idx + n_columns)
+            idx += n_columns
+
+        # `_iter` only generates transformers that have a non empty
+        # selection. Here we set empty slices for transformers that
+        # generate no output, which are safe for indexing
+        all_names = [t[0] for t in self.transformers] + ["remainder"]
+        for name in all_names:
+            if name not in self.output_indices_:
+                self.output_indices_[name] = slice(0, 0)

     def _log_message(self, name, idx, total):
         if not self.verbose:
             return None
-        return '(%d of %d) Processing %s' % (idx, total, name)
-
-    def _fit_transform(self, X, y, func, fitted=False):
+        return "(%d of %d) Processing %s" % (idx, total, name)
+
+    def _fit_transform(self, X, y, func, fitted=False, column_as_strings=False):
         """
         Private function to fit and/or transform on demand.

@@ -398,21 +596,25 @@
         ``fitted=True`` ensures the fitted transformers are used.
         """
         transformers = list(
-            self._iter(fitted=fitted, replace_strings=True))
+            self._iter(
+                fitted=fitted, replace_strings=True, column_as_strings=column_as_strings
+            )
+        )
         try:
             return Parallel(n_jobs=self.n_jobs)(
                 delayed(func)(
                     transformer=clone(trans) if not fitted else trans,
-                    X=_get_column(X, column),
+                    X=_safe_indexing(X, column, axis=1),
                     y=y,
                     weight=weight,
-                    message_clsname='ColumnTransformer',
-                    message=self._log_message(name, idx, len(transformers)))
-                for idx, (name, trans, column, weight) in enumerate(
-                        self._iter(fitted=fitted, replace_strings=True), 1))
+                    message_clsname="ColumnTransformer",
+                    message=self._log_message(name, idx, len(transformers)),
+                )
+                for idx, (name, trans, column, weight) in enumerate(transformers, 1)
+            )
         except ValueError as e:
             if "Expected 2D array, got 1D array instead" in str(e):
-                raise ValueError(_ERR_MSG_1DCOLUMN)
+                raise ValueError(_ERR_MSG_1DCOLUMN) from e
             else:
                 raise

@@ -421,18 +623,17 @@

         Parameters
         ----------
-        X : array-like or DataFrame of shape [n_samples, n_features]
+        X : {array-like, dataframe} of shape (n_samples, n_features)
             Input data, of which specified subsets are used to fit the
             transformers.

-        y : array-like, shape (n_samples, ...), optional
+        y : array-like of shape (n_samples,...), default=None
             Targets for supervised learning.

         Returns
         -------
         self : ColumnTransformer
-            This estimator
-
+            This estimator.
         """
         # we use fit_transform to make sure to set sparse_output_ (for which we
         # need the transformed data) to have consistent output type in predict
@@ -444,23 +645,27 @@

         Parameters
         ----------
-        X : array-like or DataFrame of shape [n_samples, n_features]
+        X : {array-like, dataframe} of shape (n_samples, n_features)
             Input data, of which specified subsets are used to fit the
             transformers.

-        y : array-like, shape (n_samples, ...), optional
+        y : array-like of shape (n_samples,), default=None
             Targets for supervised learning.

         Returns
         -------
-        X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)
-            hstack of results of transformers. sum_n_components is the
+        X_t : {array-like, sparse matrix} of \
+                shape (n_samples, sum_n_components)
+            Horizontally stacked results of transformers. sum_n_components is the
             sum of n_components (output dimension) over transformers. If
             any result is a sparse matrix, everything will be converted to
             sparse matrices.
-
-        """
+        """
+        self._check_feature_names(X, reset=True)
+
         X = _check_X(X)
+        # set n_features_in_ attribute
+        self._check_n_features(X, reset=True)
         self._validate_transformers()
         self._validate_column_callables(X)
         self._validate_remainder(X)
@@ -477,8 +682,9 @@
         # determine if concatenated output will be sparse or not
         if any(sparse.issparse(X) for X in Xs):
             nnz = sum(X.nnz if sparse.issparse(X) else X.size for X in Xs)
-            total = sum(X.shape[0] * X.shape[1] if sparse.issparse(X)
-                        else X.size for X in Xs)
+            total = sum(
+                X.shape[0] * X.shape[1] if sparse.issparse(X) else X.size for X in Xs
+            )
             density = nnz / total
             self.sparse_output_ = density < self.sparse_threshold
         else:
@@ -486,6 +692,7 @@

         self._update_fitted_transformers(transformers)
         self._validate_output(Xs)
+        self._record_output_indices(Xs)

         return self._hstack(list(Xs))

@@ -494,22 +701,55 @@

         Parameters
         ----------
-        X : array-like or DataFrame of shape [n_samples, n_features]
+        X : {array-like, dataframe} of shape (n_samples, n_features)
             The data to be transformed by subset.

         Returns
         -------
-        X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)
-            hstack of results of transformers. sum_n_components is the
+        X_t : {array-like, sparse matrix} of \
+                shape (n_samples, sum_n_components)
+            Horizontally stacked results of transformers. sum_n_components is the
             sum of n_components (output dimension) over transformers. If
             any result is a sparse matrix, everything will be converted to
             sparse matrices.
-
-        """
-        check_is_fitted(self, 'transformers_')
-
+        """
+        check_is_fitted(self)
         X = _check_X(X)
-        Xs = self._fit_transform(X, None, _transform_one, fitted=True)
+
+        fit_dataframe_and_transform_dataframe = hasattr(
+            self, "feature_names_in_"
+        ) and hasattr(X, "columns")
+
+        if fit_dataframe_and_transform_dataframe:
+            named_transformers = self.named_transformers_
+            # check that all names seen in fit are in transform, unless
+            # they were dropped
+            non_dropped_indices = [
+                ind
+                for name, ind in self._transformer_to_input_indices.items()
+                if name in named_transformers
+                and isinstance(named_transformers[name], str)
+                and named_transformers[name] != "drop"
+            ]
+
+            all_indices = set(chain(*non_dropped_indices))
+            all_names = set(self.feature_names_in_[ind] for ind in all_indices)
+
+            diff = all_names - set(X.columns)
+            if diff:
+                raise ValueError(f"columns are missing: {diff}")
+        else:
+            # ndarray was used for fitting or transforming, thus we only
+            # check that n_features_in_ is consistent
+            self._check_n_features(X, reset=False)
+
+        Xs = self._fit_transform(
+            X,
+            None,
+            _transform_one,
+            fitted=True,
+            column_as_strings=fit_dataframe_and_transform_dataframe,
+        )
         self._validate_output(Xs)

         if not Xs:
@@ -526,158 +766,56 @@

         Parameters
         ----------
-        Xs : List of numpy arrays, sparse arrays, or DataFrames
+        Xs : list of {array-like, sparse matrix, dataframe}
         """
         if self.sparse_output_:
             try:
                 # since all columns should be numeric before stacking them
                 # in a sparse matrix, `check_array` is used for the
                 # dtype conversion if necessary.
-                converted_Xs = [check_array(X,
-                                            accept_sparse=True,
-                                            force_all_finite=False)
-                                for X in Xs]
-            except ValueError:
-                raise ValueError("For a sparse output, all columns should"
-                                 " be a numeric or convertible to a numeric.")
+                converted_Xs = [
+                    check_array(X, accept_sparse=True, force_all_finite=False)
+                    for X in Xs
+                ]
+            except ValueError as e:
+                raise ValueError(
+                    "For a sparse output, all columns should "
+                    "be a numeric or convertible to a numeric."
+                ) from e

             return sparse.hstack(converted_Xs).tocsr()
         else:
             Xs = [f.toarray() if sparse.issparse(f) else f for f in Xs]
             return np.hstack(Xs)

+    def _sk_visual_block_(self):
+        if isinstance(self.remainder, str) and self.remainder == "drop":
+            transformers = self.transformers
+        elif hasattr(self, "_remainder"):
+            remainder_columns = self._remainder[2]
+            if (
+                hasattr(self, "feature_names_in_")
+                and remainder_columns
+                and not all(isinstance(col, str) for col in remainder_columns)
+            ):
+                remainder_columns = self.feature_names_in_[remainder_columns].tolist()
+            transformers = chain(
+                self.transformers, [("remainder", self.remainder, remainder_columns)]
+            )
+        else:
+            transformers = chain(self.transformers, [("remainder", self.remainder, "")])
+
+        names, transformers, name_details = zip(*transformers)
+        return _VisualBlock(
+            "parallel", transformers, names=names, name_details=name_details
+        )
+

 def _check_X(X):
     """Use check_array only on lists and other non-array-likes / sparse"""
-    if hasattr(X, '__array__') or sparse.issparse(X):
+    if hasattr(X, "__array__") or sparse.issparse(X):
         return X
-    return check_array(X, force_all_finite='allow-nan', dtype=np.object)
-
-
-def _check_key_type(key, superclass):
-    """
-    Check that scalar, list or slice is of a certain type.
-
-    This is only used in _get_column and _get_column_indices to check
-    if the `key` (column specification) is fully integer or fully string-like.
-
-    Parameters
-    ----------
-    key : scalar, list, slice, array-like
-        The column specification to check
-    superclass : int or str
-        The type for which to check the `key`
-
-    """
-    if isinstance(key, superclass):
-        return True
-    if isinstance(key, slice):
-        return (isinstance(key.start, (superclass, type(None))) and
-                isinstance(key.stop, (superclass, type(None))))
-    if isinstance(key, list):
-        return all(isinstance(x, superclass) for x in key)
-    if hasattr(key, 'dtype'):
-        if superclass is int:
-            return key.dtype.kind == 'i'
-        else:
-            # superclass = str
-            return key.dtype.kind in ('O', 'U', 'S')
-    return False
-
-
-def _get_column(X, key):
-    """
-    Get feature column(s) from input data X.
-
-    Supported input types (X): numpy arrays, sparse arrays and DataFrames
-
-    Supported key types (key):
-    - scalar: output is 1D
-    - lists, slices, boolean masks: output is 2D
-    - callable that returns any of the above
-
-    Supported key data types:
-
-    - integer or boolean mask (positional):
-        - supported for arrays, sparse matrices and dataframes
-    - string (key-based):
-        - only supported for dataframes
-        - So no keys other than strings are allowed (while in principle you
-          can use any hashable object as key).
-
-    """
-    # check whether we have string column names or integers
-    if _check_key_type(key, int):
-        column_names = False
-    elif _check_key_type(key, str):
-        column_names = True
-    elif hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool_):
-        # boolean mask
-        column_names = False
-        if hasattr(X, 'loc'):
-            # pandas boolean masks don't work with iloc, so take loc path
-            column_names = True
-    else:
-        raise ValueError("No valid specification of the columns. Only a "
-                         "scalar, list or slice of all integers or all "
-                         "strings, or boolean mask is allowed")
-
-    if column_names:
-        if hasattr(X, 'loc'):
-            # pandas dataframes
-            return X.loc[:, key]
-        else:
-            raise ValueError("Specifying the columns using strings is only "
-                             "supported for pandas DataFrames")
-    else:
-        if hasattr(X, 'iloc'):
-            # pandas dataframes
-            return X.iloc[:, key]
-        else:
-            # numpy arrays, sparse arrays
-            return X[:, key]
-
-
-def _get_column_indices(X, key):
-    """
-    Get feature column indices for input data X and key.
-
-    For accepted values of `key`, see the docstring of _get_column
-
-    """
-    n_columns = X.shape[1]
-
-    if (_check_key_type(key, int)
-            or hasattr(key, 'dtype') and np.issubdtype(key.dtype, np.bool_)):
-        # Convert key into positive indexes
-        idx = np.arange(n_columns)[key]
-        return np.atleast_1d(idx).tolist()
-    elif _check_key_type(key, str):
-        try:
-            all_columns = list(X.columns)
-        except AttributeError:
-            raise ValueError("Specifying the columns using strings is only "
-                             "supported for pandas DataFrames")
-        if isinstance(key, str):
-            columns = [key]
-        elif isinstance(key, slice):
-            start, stop = key.start, key.stop
-            if start is not None:
-                start = all_columns.index(start)
-            if stop is not None:
-                # pandas indexing with strings is endpoint included
-                stop = all_columns.index(stop) + 1
-            else:
-                stop = n_columns + 1
-            return list(range(n_columns)[slice(start, stop)])
-        else:
-            columns = list(key)
-
-        return [all_columns.index(col) for col in columns]
-    else:
-        raise ValueError("No valid specification of the columns. Only a "
-                         "scalar, list or slice of all integers or all "
-                         "strings, or boolean mask is allowed")
+    return check_array(X, force_all_finite="allow-nan", dtype=object)


 def _is_empty_column_selection(column):
@@ -686,50 +824,16 @@
     boolean array).

     """
-    if hasattr(column, 'dtype') and np.issubdtype(column.dtype, np.bool_):
+    if hasattr(column, "dtype") and np.issubdtype(column.dtype, np.bool_):
         return not column.any()
-    elif hasattr(column, '__len__'):
-        return len(column) == 0
+    elif hasattr(column, "__len__"):
+        return (
+            len(column) == 0
+            or all(isinstance(col, bool) for col in column)
+            and not any(column)
+        )
     else:
         return False
-
-
-def _validate_transformers(transformers):
-    """Checks if given transformers are valid.
-
-    This is a helper function to support the deprecated tuple order.
-    XXX Remove in v0.22
-    """
-    if not transformers:
-        return True
-
-    for t in transformers:
-        if isinstance(t, str) and t in ('drop', 'passthrough'):
-            continue
-        if (not (hasattr(t, "fit") or hasattr(t, "fit_transform")) or not
-                hasattr(t, "transform")):
-            return False
-
-    return True
-
-
-def _is_deprecated_tuple_order(tuples):
-    """Checks if the input follows the deprecated tuple order.
-
-    Returns
-    -------
-    Returns true if (transformer, columns) is not a valid assumption for the
-    input, but (columns, transformer) is valid. The latter is deprecated and
-    its support will stop in v0.22.
-
-    XXX Remove in v0.22
-    """
-    transformers, columns = zip(*tuples)
-    if (not _validate_transformers(transformers)
-            and _validate_transformers(columns)):
-        return True
-
-    return False


 def _get_transformer_list(estimators):
@@ -737,26 +841,21 @@
     Construct (name, trans, column) tuples from list

     """
-    message = ('`make_column_transformer` now expects (transformer, columns) '
-               'as input tuples instead of (columns, transformer). This '
-               'has been introduced in v0.20.1. `make_column_transformer` '
-               'will stop accepting the deprecated (columns, transformer) '
-               'order in v0.22.')
-
     transformers, columns = zip(*estimators)
-
-    # XXX Remove in v0.22
-    if _is_deprecated_tuple_order(estimators):
-        transformers, columns = columns, transformers
-        warnings.warn(message, DeprecationWarning)
-
     names, _ = zip(*_name_estimators(transformers))

     transformer_list = list(zip(names, transformers, columns))
     return transformer_list


-def make_column_transformer(*transformers, **kwargs):
+def make_column_transformer(
+    *transformers,
+    remainder="drop",
+    sparse_threshold=0.3,
+    n_jobs=None,
+    verbose=False,
+    verbose_feature_names_out=True,
+):
     """Construct a ColumnTransformer from the given transformers.

     This is a shorthand for the ColumnTransformer constructor; it does not
@@ -764,11 +863,31 @@
     be given names automatically based on their types. It also does not allow
     weighting with ``transformer_weights``.

+    Read more in the :ref:`User Guide <make_column_transformer>`.
+
     Parameters
     ----------
-    *transformers : tuples of transformers and column selections
-
-    remainder : {'drop', 'passthrough'} or estimator, default 'drop'
+    *transformers : tuples
+        Tuples of the form (transformer, columns) specifying the
+        transformer objects to be applied to subsets of the data.
+
+        transformer : {'drop', 'passthrough'} or estimator
+            Estimator must support :term:`fit` and :term:`transform`.
+            Special-cased strings 'drop' and 'passthrough' are accepted as
+            well, to indicate to drop the columns or to pass them through
+            untransformed, respectively.
+        columns : str,  array-like of str, int, array-like of int, slice, \
+                array-like of bool or callable
+            Indexes the data on its second axis. Integers are interpreted as
+            positional columns, while strings can reference DataFrame columns
+            by name. A scalar string or int should be used where
+            ``transformer`` expects X to be a 1d array-like (vector),
+            otherwise a 2d array will be passed to the transformer.
+            A callable is passed the input data `X` and can return any of the
+            above. To select multiple columns by name or dtype, you can use
+            :obj:`make_column_selector`.
+
+    remainder : {'drop', 'passthrough'} or estimator, default='drop'
         By default, only the specified columns in `transformers` are
         transformed and combined in the output, and the non-specified
         columns are dropped. (default of ``'drop'``).
@@ -780,7 +899,7 @@
         non-specified columns will use the ``remainder`` estimator. The
         estimator must support :term:`fit` and :term:`transform`.

-    sparse_threshold : float, default = 0.3
+    sparse_threshold : float, default=0.3
         If the transformed output consists of a mix of sparse and dense data,
         it will be stacked as a sparse matrix if the density is lower than this
         value. Use ``sparse_threshold=0`` to always return dense.
@@ -788,23 +907,32 @@
         the stacked result will be sparse or dense, respectively, and this
         keyword will be ignored.

-    n_jobs : int or None, optional (default=None)
+    n_jobs : int, default=None
         Number of jobs to run in parallel.
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.

-    verbose : boolean, optional(default=False)
+    verbose : bool, default=False
         If True, the time elapsed while fitting each transformer will be
         printed as it is completed.
+
+    verbose_feature_names_out : bool, default=True
+        If True, :meth:`get_feature_names_out` will prefix all feature names
+        with the name of the transformer that generated that feature.
+        If False, :meth:`get_feature_names_out` will not prefix any feature
+        names and will error if feature names are not unique.
+
+        .. versionadded:: 1.0

     Returns
     -------
     ct : ColumnTransformer
-
-    See also
+        Returns a :class:`ColumnTransformer` object.
+
+    See Also
     --------
-    sklearn.compose.ColumnTransformer : Class that allows combining the
+    ColumnTransformer : Class that allows combining the
         outputs of multiple transformer objects used on column subsets
         of the data into a single feature space.

@@ -815,28 +943,103 @@
     >>> make_column_transformer(
     ...     (StandardScaler(), ['numerical_column']),
     ...     (OneHotEncoder(), ['categorical_column']))
-    ...     # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-    ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,
-             transformer_weights=None,
-             transformers=[('standardscaler',
-                            StandardScaler(...),
-                            ['numerical_column']),
-                           ('onehotencoder',
-                            OneHotEncoder(...),
-                            ['categorical_column'])], verbose=False)
-
+    ColumnTransformer(transformers=[('standardscaler', StandardScaler(...),
+                                     ['numerical_column']),
+                                    ('onehotencoder', OneHotEncoder(...),
+                                     ['categorical_column'])])
     """
     # transformer_weights keyword is not passed through because the user
     # would need to know the automatically generated names of the transformers
-    n_jobs = kwargs.pop('n_jobs', None)
-    remainder = kwargs.pop('remainder', 'drop')
-    sparse_threshold = kwargs.pop('sparse_threshold', 0.3)
-    verbose = kwargs.pop('verbose', False)
-    if kwargs:
-        raise TypeError('Unknown keyword arguments: "{}"'
-                        .format(list(kwargs.keys())[0]))
     transformer_list = _get_transformer_list(transformers)
-    return ColumnTransformer(transformer_list, n_jobs=n_jobs,
-                             remainder=remainder,
-                             sparse_threshold=sparse_threshold,
-                             verbose=verbose)
+    return ColumnTransformer(
+        transformer_list,
+        n_jobs=n_jobs,
+        remainder=remainder,
+        sparse_threshold=sparse_threshold,
+        verbose=verbose,
+        verbose_feature_names_out=verbose_feature_names_out,
+    )
+
+
+class make_column_selector:
+    """Create a callable to select columns to be used with
+    :class:`ColumnTransformer`.
+
+    :func:`make_column_selector` can select columns based on datatype or the
+    columns name with a regex. When using multiple selection criteria, **all**
+    criteria must match for a column to be selected.
+
+    Parameters
+    ----------
+    pattern : str, default=None
+        Name of columns containing this regex pattern will be included. If
+        None, column selection will not be selected based on pattern.
+
+    dtype_include : column dtype or list of column dtypes, default=None
+        A selection of dtypes to include. For more details, see
+        :meth:`pandas.DataFrame.select_dtypes`.
+
+    dtype_exclude : column dtype or list of column dtypes, default=None
+        A selection of dtypes to exclude. For more details, see
+        :meth:`pandas.DataFrame.select_dtypes`.
+
+    Returns
+    -------
+    selector : callable
+        Callable for column selection to be used by a
+        :class:`ColumnTransformer`.
+
+    See Also
+    --------
+    ColumnTransformer : Class that allows combining the
+        outputs of multiple transformer objects used on column subsets
+        of the data into a single feature space.
+
+    Examples
+    --------
+    >>> from sklearn.preprocessing import StandardScaler, OneHotEncoder
+    >>> from sklearn.compose import make_column_transformer
+    >>> from sklearn.compose import make_column_selector
+    >>> import numpy as np
+    >>> import pandas as pd  # doctest: +SKIP
+    >>> X = pd.DataFrame({'city': ['London', 'London', 'Paris', 'Sallisaw'],
+    ...                   'rating': [5, 3, 4, 5]})  # doctest: +SKIP
+    >>> ct = make_column_transformer(
+    ...       (StandardScaler(),
+    ...        make_column_selector(dtype_include=np.number)),  # rating
+    ...       (OneHotEncoder(),
+    ...        make_column_selector(dtype_include=object)))  # city
+    >>> ct.fit_transform(X)  # doctest: +SKIP
+    array([[ 0.90453403,  1.        ,  0.        ,  0.        ],
+           [-1.50755672,  1.        ,  0.        ,  0.        ],
+           [-0.30151134,  0.        ,  1.        ,  0.        ],
+           [ 0.90453403,  0.        ,  0.        ,  1.        ]])
+    """
+
+    def __init__(self, pattern=None, *, dtype_include=None, dtype_exclude=None):
+        self.pattern = pattern
+        self.dtype_include = dtype_include
+        self.dtype_exclude = dtype_exclude
+
+    def __call__(self, df):
+        """Callable for column selection to be used by a
+        :class:`ColumnTransformer`.
+
+        Parameters
+        ----------
+        df : dataframe of shape (n_features, n_samples)
+            DataFrame to select columns from.
+        """
+        if not hasattr(df, "iloc"):
+            raise ValueError(
+                "make_column_selector can only be applied to pandas dataframes"
+            )
+        df_row = df.iloc[:1]
+        if self.dtype_include is not None or self.dtype_exclude is not None:
+            df_row = df_row.select_dtypes(
+                include=self.dtype_include, exclude=self.dtype_exclude
+            )
+        cols = df_row.columns
+        if self.pattern is not None:
+            cols = cols[cols.str.contains(self.pattern, regex=True)]
+        return cols.tolist()
('sklearn/datasets', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,100 +3,99 @@
 including methods to load and fetch popular reference datasets. It also
 features some artificial data generators.
 """
-from .base import load_breast_cancer
-from .base import load_boston
-from .base import load_diabetes
-from .base import load_digits
-from .base import load_files
-from .base import load_iris
-from .base import load_linnerud
-from .base import load_sample_images
-from .base import load_sample_image
-from .base import load_wine
-from .base import get_data_home
-from .base import clear_data_home
-from .covtype import fetch_covtype
-from .kddcup99 import fetch_kddcup99
-from .lfw import fetch_lfw_pairs
-from .lfw import fetch_lfw_people
-from .twenty_newsgroups import fetch_20newsgroups
-from .twenty_newsgroups import fetch_20newsgroups_vectorized
-from .mldata import fetch_mldata, mldata_filename
-from .openml import fetch_openml
-from .samples_generator import make_classification
-from .samples_generator import make_multilabel_classification
-from .samples_generator import make_hastie_10_2
-from .samples_generator import make_regression
-from .samples_generator import make_blobs
-from .samples_generator import make_moons
-from .samples_generator import make_circles
-from .samples_generator import make_friedman1
-from .samples_generator import make_friedman2
-from .samples_generator import make_friedman3
-from .samples_generator import make_low_rank_matrix
-from .samples_generator import make_sparse_coded_signal
-from .samples_generator import make_sparse_uncorrelated
-from .samples_generator import make_spd_matrix
-from .samples_generator import make_swiss_roll
-from .samples_generator import make_s_curve
-from .samples_generator import make_sparse_spd_matrix
-from .samples_generator import make_gaussian_quantiles
-from .samples_generator import make_biclusters
-from .samples_generator import make_checkerboard
-from .svmlight_format import load_svmlight_file
-from .svmlight_format import load_svmlight_files
-from .svmlight_format import dump_svmlight_file
-from .olivetti_faces import fetch_olivetti_faces
-from .species_distributions import fetch_species_distributions
-from .california_housing import fetch_california_housing
-from .rcv1 import fetch_rcv1
+from ._base import load_breast_cancer
+from ._base import load_boston
+from ._base import load_diabetes
+from ._base import load_digits
+from ._base import load_files
+from ._base import load_iris
+from ._base import load_linnerud
+from ._base import load_sample_images
+from ._base import load_sample_image
+from ._base import load_wine
+from ._base import get_data_home
+from ._base import clear_data_home
+from ._covtype import fetch_covtype
+from ._kddcup99 import fetch_kddcup99
+from ._lfw import fetch_lfw_pairs
+from ._lfw import fetch_lfw_people
+from ._twenty_newsgroups import fetch_20newsgroups
+from ._twenty_newsgroups import fetch_20newsgroups_vectorized
+from ._openml import fetch_openml
+from ._samples_generator import make_classification
+from ._samples_generator import make_multilabel_classification
+from ._samples_generator import make_hastie_10_2
+from ._samples_generator import make_regression
+from ._samples_generator import make_blobs
+from ._samples_generator import make_moons
+from ._samples_generator import make_circles
+from ._samples_generator import make_friedman1
+from ._samples_generator import make_friedman2
+from ._samples_generator import make_friedman3
+from ._samples_generator import make_low_rank_matrix
+from ._samples_generator import make_sparse_coded_signal
+from ._samples_generator import make_sparse_uncorrelated
+from ._samples_generator import make_spd_matrix
+from ._samples_generator import make_swiss_roll
+from ._samples_generator import make_s_curve
+from ._samples_generator import make_sparse_spd_matrix
+from ._samples_generator import make_gaussian_quantiles
+from ._samples_generator import make_biclusters
+from ._samples_generator import make_checkerboard
+from ._svmlight_format_io import load_svmlight_file
+from ._svmlight_format_io import load_svmlight_files
+from ._svmlight_format_io import dump_svmlight_file
+from ._olivetti_faces import fetch_olivetti_faces
+from ._species_distributions import fetch_species_distributions
+from ._california_housing import fetch_california_housing
+from ._rcv1 import fetch_rcv1


-__all__ = ['clear_data_home',
-           'dump_svmlight_file',
-           'fetch_20newsgroups',
-           'fetch_20newsgroups_vectorized',
-           'fetch_lfw_pairs',
-           'fetch_lfw_people',
-           'fetch_mldata',
-           'fetch_olivetti_faces',
-           'fetch_species_distributions',
-           'fetch_california_housing',
-           'fetch_covtype',
-           'fetch_rcv1',
-           'fetch_kddcup99',
-           'fetch_openml',
-           'get_data_home',
-           'load_boston',
-           'load_diabetes',
-           'load_digits',
-           'load_files',
-           'load_iris',
-           'load_breast_cancer',
-           'load_linnerud',
-           'load_sample_image',
-           'load_sample_images',
-           'load_svmlight_file',
-           'load_svmlight_files',
-           'load_wine',
-           'make_biclusters',
-           'make_blobs',
-           'make_circles',
-           'make_classification',
-           'make_checkerboard',
-           'make_friedman1',
-           'make_friedman2',
-           'make_friedman3',
-           'make_gaussian_quantiles',
-           'make_hastie_10_2',
-           'make_low_rank_matrix',
-           'make_moons',
-           'make_multilabel_classification',
-           'make_regression',
-           'make_s_curve',
-           'make_sparse_coded_signal',
-           'make_sparse_spd_matrix',
-           'make_sparse_uncorrelated',
-           'make_spd_matrix',
-           'make_swiss_roll',
-           'mldata_filename']
+__all__ = [
+    "clear_data_home",
+    "dump_svmlight_file",
+    "fetch_20newsgroups",
+    "fetch_20newsgroups_vectorized",
+    "fetch_lfw_pairs",
+    "fetch_lfw_people",
+    "fetch_olivetti_faces",
+    "fetch_species_distributions",
+    "fetch_california_housing",
+    "fetch_covtype",
+    "fetch_rcv1",
+    "fetch_kddcup99",
+    "fetch_openml",
+    "get_data_home",
+    "load_boston",
+    "load_diabetes",
+    "load_digits",
+    "load_files",
+    "load_iris",
+    "load_breast_cancer",
+    "load_linnerud",
+    "load_sample_image",
+    "load_sample_images",
+    "load_svmlight_file",
+    "load_svmlight_files",
+    "load_wine",
+    "make_biclusters",
+    "make_blobs",
+    "make_circles",
+    "make_classification",
+    "make_checkerboard",
+    "make_friedman1",
+    "make_friedman2",
+    "make_friedman3",
+    "make_gaussian_quantiles",
+    "make_hastie_10_2",
+    "make_low_rank_matrix",
+    "make_moons",
+    "make_multilabel_classification",
+    "make_regression",
+    "make_s_curve",
+    "make_sparse_coded_signal",
+    "make_sparse_spd_matrix",
+    "make_sparse_uncorrelated",
+    "make_spd_matrix",
+    "make_swiss_roll",
+]
('sklearn/datasets', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,24 +1,27 @@
-
 import numpy
 import os
 import platform


-def configuration(parent_package='', top_path=None):
+def configuration(parent_package="", top_path=None):
     from numpy.distutils.misc_util import Configuration
-    config = Configuration('datasets', parent_package, top_path)
-    config.add_data_dir('data')
-    config.add_data_dir('descr')
-    config.add_data_dir('images')
-    config.add_data_dir(os.path.join('tests', 'data'))
-    if platform.python_implementation() != 'PyPy':
-        config.add_extension('_svmlight_format',
-                             sources=['_svmlight_format.pyx'],
-                             include_dirs=[numpy.get_include()])
-    config.add_subpackage('tests')
+
+    config = Configuration("datasets", parent_package, top_path)
+    config.add_data_dir("data")
+    config.add_data_dir("descr")
+    config.add_data_dir("images")
+    config.add_data_dir(os.path.join("tests", "data"))
+    if platform.python_implementation() != "PyPy":
+        config.add_extension(
+            "_svmlight_format_fast",
+            sources=["_svmlight_format_fast.pyx"],
+            include_dirs=[numpy.get_include()],
+        )
+    config.add_subpackage("tests")
     return config


-if __name__ == '__main__':
+if __name__ == "__main__":
     from numpy.distutils.core import setup
-    setup(**configuration(top_path='').todict())
+
+    setup(**configuration(top_path="").todict())
('sklearn/datasets/descr', 'boston_house_prices.rst')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -21,7 +21,7 @@
         - RAD      index of accessibility to radial highways
         - TAX      full-value property-tax rate per $10,000
         - PTRATIO  pupil-teacher ratio by town
-        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
+        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town
         - LSTAT    % lower status of the population
         - MEDV     Median value of owner-occupied homes in $1000's

('sklearn/datasets/descr', 'diabetes.rst')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -17,22 +17,22 @@
   :Target: Column 11 is a quantitative measure of disease progression one year after baseline

   :Attribute Information:
-      - Age
-      - Sex
-      - Body mass index
-      - Average blood pressure
-      - S1
-      - S2
-      - S3
-      - S4
-      - S5
-      - S6
+      - age     age in years
+      - sex
+      - bmi     body mass index
+      - bp      average blood pressure
+      - s1      tc, total serum cholesterol
+      - s2      ldl, low-density lipoproteins
+      - s3      hdl, high-density lipoproteins
+      - s4      tch, total cholesterol / HDL
+      - s5      ltg, possibly log of serum triglycerides level
+      - s6      glu, blood sugar level

-Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).
+Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).

 Source URL:
 https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html

 For more information see:
 Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) "Least Angle Regression," Annals of Statistics (with discussion), 407-499.
-(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)
+(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)
('sklearn/datasets/descr', 'digits.rst')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,7 +5,7 @@

 **Data Set Characteristics:**

-    :Number of Instances: 5620
+    :Number of Instances: 1797
     :Number of Attributes: 64
     :Attribute Information: 8x8 image of integer pixels in the range 0..16.
     :Missing Attribute Values: None
@@ -43,4 +43,4 @@
     Electrical and Electronic Engineering Nanyang Technological University.
     2005.
   - Claudio Gentile. A New Approximate Maximal Margin Classification
-    Algorithm. NIPS. 2000.
+    Algorithm. NIPS. 2000.
('sklearn/datasets/descr', 'twenty_newsgroups.rst')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -12,7 +12,7 @@
 This module contains two loaders. The first one,
 :func:`sklearn.datasets.fetch_20newsgroups`,
 returns a list of the raw texts that can be fed to text feature
-extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`
+extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`
 with custom parameters so as to extract feature vectors.
 The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,
 returns ready-to-use features, i.e., it is not necessary to use a feature
@@ -113,10 +113,10 @@
 components by sample in a more than 30000-dimensional space
 (less than .5% non-zero features)::

-  >>> vectors.nnz / float(vectors.shape[0])       # doctest: +ELLIPSIS
+  >>> vectors.nnz / float(vectors.shape[0])
   159.01327...

-:func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which
+:func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which
 returns ready-to-use token counts features instead of file names.

 .. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/
@@ -144,7 +144,7 @@
   MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)

   >>> pred = clf.predict(vectors_test)
-  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS
+  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')
   0.88213...

 (The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles
@@ -156,7 +156,7 @@

   >>> import numpy as np
   >>> def show_top10(classifier, vectorizer, categories):
-  ...     feature_names = np.asarray(vectorizer.get_feature_names())
+  ...     feature_names = vectorizer.get_feature_names_out()
   ...     for i, category in enumerate(categories):
   ...         top10 = np.argsort(classifier.coef_[i])[-10:]
   ...         print("%s: %s" % (category, " ".join(feature_names[top10])))
@@ -195,7 +195,7 @@
   ...                                      categories=categories)
   >>> vectors_test = vectorizer.transform(newsgroups_test.data)
   >>> pred = clf.predict(vectors_test)
-  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  # doctest: +ELLIPSIS
+  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')
   0.77310...

 This classifier lost over a lot of its F-score, just because we removed
@@ -212,19 +212,39 @@

   >>> vectors_test = vectorizer.transform(newsgroups_test.data)
   >>> pred = clf.predict(vectors_test)
-  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS
+  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')
   0.76995...

 Some other classifiers cope better with this harder version of the task. Try
 running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without
 the ``--filter`` option to compare the results.

+.. topic:: Data Considerations
+
+  The Cleveland Indians is a major league baseball team based in Cleveland,
+  Ohio, USA. In December 2020, it was reported that "After several months of
+  discussion sparked by the death of George Floyd and a national reckoning over
+  race and colonialism, the Cleveland Indians have decided to change their
+  name." Team owner Paul Dolan "did make it clear that the team will not make
+  its informal nickname -- the Tribe -- its new team name." "It’s not going to
+  be a half-step away from the Indians," Dolan said."We will not have a Native
+  American-themed name."
+
+  https://www.mlb.com/news/cleveland-indians-team-name-change
+
 .. topic:: Recommendation

-  When evaluating text classifiers on the 20 Newsgroups data, you
-  should strip newsgroup-related metadata. In scikit-learn, you can do this by
-  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be
-  lower because it is more realistic.
+  - When evaluating text classifiers on the 20 Newsgroups data, you
+    should strip newsgroup-related metadata. In scikit-learn, you can do this
+    by setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be
+    lower because it is more realistic.
+  - This text dataset contains data which may be inappropriate for certain NLP
+    applications. An example is listed in the "Data Considerations" section
+    above. The challenge with using current text datasets in NLP for tasks such
+    as sentence completion, clustering, and other applications is that text
+    that is culturally biased and inflammatory will propagate biases. This
+    should be taken into consideration when using the dataset, reviewing the
+    output, and the bias should be documented.

 .. topic:: Examples

('sklearn/datasets/descr', 'breast_cancer.rst')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -18,13 +18,13 @@
         - compactness (perimeter^2 / area - 1.0)
         - concavity (severity of concave portions of the contour)
         - concave points (number of concave portions of the contour)
-        - symmetry
+        - symmetry
         - fractal dimension ("coastline approximation" - 1)

         The mean, standard error, and "worst" or largest (mean of the three
-        largest values) of these features were computed for each image,
-        resulting in 30 features.  For instance, field 3 is Mean Radius, field
-        13 is Radius SE, field 23 is Worst Radius.
+        worst/largest values) of these features were computed for each image,
+        resulting in 30 features.  For instance, field 0 is Mean Radius, field
+        10 is Radius SE, field 20 is Worst Radius.

         - class:
                 - WDBC-Malignant
('sklearn/datasets/descr', 'kddcup99.rst')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,31 +5,30 @@

 The KDD Cup '99 dataset was created by processing the tcpdump portions
 of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,
-created by MIT Lincoln Lab [1]. The artificial data (described on the `dataset's
+created by MIT Lincoln Lab [2]_. The artificial data (described on the `dataset's
 homepage <https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html>`_) was
 generated using a closed network and hand-injected attacks to produce a
 large number of different types of attack with normal activity in the
 background. As the initial goal was to produce a large training set for
 supervised learning algorithms, there is a large proportion (80.1%) of
 abnormal data which is unrealistic in real world, and inappropriate for
-unsupervised anomaly detection which aims at detecting 'abnormal' data, ie
+unsupervised anomaly detection which aims at detecting 'abnormal' data, i.e.:

-1) qualitatively different from normal data
-
-2) in large minority among the observations.
+* qualitatively different from normal data
+* in large minority among the observations.

 We thus transform the KDD Data set into two different data sets: SA and SF.

--SA is obtained by simply selecting all the normal data, and a small
-proportion of abnormal data to gives an anomaly proportion of 1%.
+* SA is obtained by simply selecting all the normal data, and a small
+  proportion of abnormal data to gives an anomaly proportion of 1%.

--SF is obtained as in [2]
-by simply picking up the data whose attribute logged_in is positive, thus
-focusing on the intrusion attack, which gives a proportion of 0.3% of
-attack.
+* SF is obtained as in [3]_
+  by simply picking up the data whose attribute logged_in is positive, thus
+  focusing on the intrusion attack, which gives a proportion of 0.3% of
+  attack.

--http and smtp are two subsets of SF corresponding with third feature
-equal to 'http' (resp. to 'smtp')
+* http and smtp are two subsets of SF corresponding with third feature
+  equal to 'http' (resp. to 'smtp').

 General KDD structure :

@@ -78,18 +77,18 @@

 :func:`sklearn.datasets.fetch_kddcup99` will load the kddcup99 dataset; it
 returns a dictionary-like object with the feature matrix in the ``data`` member
-and the target values in ``target``. The dataset will be downloaded from the
-web if necessary.
+and the target values in ``target``. The "as_frame" optional argument converts
+``data`` into a pandas DataFrame and ``target`` into a pandas Series. The
+dataset will be downloaded from the web if necessary.

-.. topic: References
+.. topic:: References

-    .. [1] Analysis and Results of the 1999 DARPA Off-Line Intrusion
-           Detection Evaluation Richard Lippmann, Joshua W. Haines,
-           David J. Fried, Jonathan Korba, Kumar Das
+    .. [2] Analysis and Results of the 1999 DARPA Off-Line Intrusion
+           Detection Evaluation, Richard Lippmann, Joshua W. Haines,
+           David J. Fried, Jonathan Korba, Kumar Das.

-    .. [2] K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online
+    .. [3] K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online
            unsupervised outlier detection using finite mixtures with
            discounting learning algorithms. In Proceedings of the sixth
            ACM SIGKDD international conference on Knowledge discovery
            and data mining, pages 320-324. ACM Press, 2000.
-
('sklearn/datasets/descr', 'california_housing.rst')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -10,26 +10,32 @@
     :Number of Attributes: 8 numeric, predictive attributes and the target

     :Attribute Information:
-        - MedInc        median income in block
-        - HouseAge      median house age in block
-        - AveRooms      average number of rooms
-        - AveBedrms     average number of bedrooms
-        - Population    block population
-        - AveOccup      average house occupancy
-        - Latitude      house block latitude
-        - Longitude     house block longitude
+        - MedInc        median income in block group
+        - HouseAge      median house age in block group
+        - AveRooms      average number of rooms per household
+        - AveBedrms     average number of bedrooms per household
+        - Population    block group population
+        - AveOccup      average number of household members
+        - Latitude      block group latitude
+        - Longitude     block group longitude

     :Missing Attribute Values: None

 This dataset was obtained from the StatLib repository.
-http://lib.stat.cmu.edu/datasets/
+https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html

-The target variable is the median house value for California districts.
+The target variable is the median house value for California districts,
+expressed in hundreds of thousands of dollars ($100,000).

 This dataset was derived from the 1990 U.S. census, using one row per census
 block group. A block group is the smallest geographical unit for which the U.S.
 Census Bureau publishes sample data (a block group typically has a population
 of 600 to 3,000 people).
+
+An household is a group of people residing within a home. Since the average
+number of rooms and bedrooms in this dataset are provided per household, these
+columns may take surpinsingly large values for block groups with few households
+and many empty houses, such as vacation resorts.

 It can be downloaded/loaded using the
 :func:`sklearn.datasets.fetch_california_housing` function.
('sklearn/datasets/descr', 'linnerud.rst')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -9,14 +9,16 @@
     :Number of Attributes: 3
     :Missing Attribute Values: None

-The Linnerud dataset constains two small dataset:
+The Linnerud dataset is a multi-output regression dataset. It consists of three
+exercise (data) and three physiological (target) variables collected from
+twenty middle-aged men in a fitness club:

-- *physiological* - CSV containing 20 observations on 3 exercise variables:
+- *physiological* - CSV containing 20 observations on 3 physiological variables:
    Weight, Waist and Pulse.
-
-- *exercise* - CSV containing 20 observations on 3 physiological variables:
+- *exercise* - CSV containing 20 observations on 3 exercise variables:
    Chins, Situps and Jumps.

 .. topic:: References

-  * Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic.
+  * Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:
+    Editions Technic.
('sklearn/datasets/descr', 'covtype.rst')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -22,7 +22,9 @@
     =================   ============

 :func:`sklearn.datasets.fetch_covtype` will load the covertype dataset;
-it returns a dictionary-like object
+it returns a dictionary-like 'Bunch' object
 with the feature matrix in the ``data`` member
-and the target values in ``target``.
+and the target values in ``target``. If optional argument 'as_frame' is
+set to 'True', it will return ``data`` and ``target`` as pandas
+data frame, and there will be an additional member ``frame`` as well.
 The dataset will be downloaded from the web if necessary.
('sklearn/externals', '_arff.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,4 +1,3 @@
-# -*- coding: utf-8 -*-
 # =============================================================================
 # Federal University of Rio Grande do Sul (UFRGS)
 # Connectionist Artificial Intelligence Laboratory (LIAC)
@@ -33,7 +32,7 @@

 ARFF (Attribute-Relation File Format) is an file format specially created for
 describe datasets which are commonly used for machine learning experiments and
-softwares. This file format was created to be used in Weka, the best
+software. This file format was created to be used in Weka, the best
 representative software for machine learning automated experiments.

 An ARFF file can be divided into two sections: header and data. The Header
@@ -138,7 +137,7 @@
 - Supports read and write the descriptions of files;
 - Supports missing values and names with spaces;
 - Supports unicode values and names;
-- Fully compatible with Python 2.7+, Python 3.3+, pypy and pypy3;
+- Fully compatible with Python 2.7+, Python 3.5+, pypy and pypy3;
 - Under `MIT License <http://opensource.org/licenses/MIT>`_

 '''
@@ -149,8 +148,9 @@
 __version__ = '2.4.0'

 import re
-import sys
 import csv
+from typing import TYPE_CHECKING
+from typing import Optional, List, Dict, Any, Iterator, Union, Tuple

 # CONSTANTS ===================================================================
 _SIMPLE_TYPES = ['NUMERIC', 'REAL', 'INTEGER', 'STRING']
@@ -163,11 +163,27 @@

 _RE_RELATION     = re.compile(r'^([^\{\}%,\s]*|\".*\"|\'.*\')$', re.UNICODE)
 _RE_ATTRIBUTE    = re.compile(r'^(\".*\"|\'.*\'|[^\{\}%,\s]*)\s+(.+)$', re.UNICODE)
-_RE_TYPE_NOMINAL = re.compile(r'^\{\s*((\".*\"|\'.*\'|\S*)\s*,\s*)*(\".*\"|\'.*\'|\S*)\s*\}$', re.UNICODE)
 _RE_QUOTE_CHARS = re.compile(r'["\'\\\s%,\000-\031]', re.UNICODE)
 _RE_ESCAPE_CHARS = re.compile(r'(?=["\'\\%])|[\n\r\t\000-\031]')
 _RE_SPARSE_LINE = re.compile(r'^\s*\{.*\}\s*$', re.UNICODE)
 _RE_NONTRIVIAL_DATA = re.compile('["\'{}\\s]', re.UNICODE)
+
+ArffDenseDataType = Iterator[List]
+ArffSparseDataType = Tuple[List, ...]
+
+
+if TYPE_CHECKING:
+    # typing_extensions is available when mypy is installed
+    from typing_extensions import TypedDict
+
+    class ArffContainerType(TypedDict):
+        description: str
+        relation: str
+        attributes: List
+        data: Union[ArffDenseDataType, ArffSparseDataType]
+
+else:
+    ArffContainerType = Dict[str, Any]


 def _build_re_values():
@@ -198,10 +214,10 @@
     dense = re.compile(r'''(?x)
         ,                # may follow ','
         \s*
-        ((?=,)|$|%(value_re)s)  # empty or value
+        ((?=,)|$|{value_re})  # empty or value
         |
         (\S.*)           # error
-        ''' % {'value_re': value_re})
+        '''.format(value_re=value_re))

     # This captures (key, value) groups and will have an empty key/value
     # in case of syntax errors.
@@ -220,6 +236,7 @@
     return dense, sparse


+
 _RE_DENSE_VALUES, _RE_SPARSE_KEY_VALUES = _build_re_values()


@@ -248,7 +265,7 @@
         except KeyError:
             raise ValueError('Unsupported escape sequence: %s' % s)
     if s[1] == 'u':
-        return unichr(int(s[2:], 16))
+        return chr(int(s[2:], 16))
     else:
         return chr(int(s[1:], 8))

@@ -279,7 +296,7 @@
         try:
             return {int(k): _unquote(v)
                     for k, v in _RE_SPARSE_KEY_VALUES.findall(s)}
-        except ValueError as exc:
+        except ValueError:
             # an ARFF syntax error in sparse data
             for match in _RE_SPARSE_KEY_VALUES.finditer(s):
                 if not match.group(1):
@@ -301,24 +318,10 @@
 LOD_GEN = 4   # Generator of dictionaries
 _SUPPORTED_DATA_STRUCTURES = [DENSE, COO, LOD, DENSE_GEN, LOD_GEN]

-# =============================================================================
-
-# COMPATIBILITY WITH PYTHON 3 =================================================
-PY3 = sys.version_info[0] == 3
-if PY3:
-    unicode = str
-    basestring = str
-    xrange = range
-    unichr = chr
-# COMPABILITY WITH PYTHON 2 ===================================================
-# =============================================================================
-PY2 = sys.version_info[0] == 2
-if PY2:
-    from itertools import izip as zip

 # EXCEPTIONS ==================================================================
 class ArffException(Exception):
-    message = None
+    message: Optional[str] = None

     def __init__(self):
         self.line = -1
@@ -337,7 +340,7 @@
 class BadDataFormat(ArffException):
     '''Error raised when some data instance is in an invalid format.'''
     def __init__(self, value):
-        super(BadDataFormat, self).__init__()
+        super().__init__()
         self.message = (
             'Bad @DATA instance format in line %d: ' +
             ('%s' % value)
@@ -353,7 +356,7 @@
     declaration.'''

     def __init__(self, value, value2):
-        super(BadAttributeName, self).__init__()
+        super().__init__()
         self.message = (
             ('Bad @ATTRIBUTE name %s at line' % value) +
             ' %d, this name is already in use in line' +
@@ -365,7 +368,7 @@
     declared into it respective attribute declaration.'''

     def __init__(self, value):
-        super(BadNominalValue, self).__init__()
+        super().__init__()
         self.message = (
             ('Data value %s not found in nominal declaration, ' % value)
             + 'at line %d.'
@@ -374,7 +377,7 @@
 class BadNominalFormatting(ArffException):
     '''Error raised when a nominal value with space is not properly quoted.'''
     def __init__(self, value):
-        super(BadNominalFormatting, self).__init__()
+        super().__init__()
         self.message = (
             ('Nominal data value "%s" not properly quoted in line ' % value) +
             '%d.'
@@ -394,7 +397,7 @@
     message = 'Invalid layout of the ARFF file, at line %d.'

     def __init__(self, msg=''):
-        super(BadLayout, self).__init__()
+        super().__init__()
         if msg:
             self.message = BadLayout.message + ' ' + msg.replace('%', '%%')

@@ -417,11 +420,11 @@

 def encode_string(s):
     if _RE_QUOTE_CHARS.search(s):
-        return u"'%s'" % _RE_ESCAPE_CHARS.sub(_unescape_sub_callback, s)
+        return "'%s'" % _RE_ESCAPE_CHARS.sub(_unescape_sub_callback, s)
     return s


-class EncodedNominalConversor(object):
+class EncodedNominalConversor:
     def __init__(self, values):
         self.values = {v: i for i, v in enumerate(values)}
         self.values[0] = 0
@@ -433,7 +436,7 @@
             raise BadNominalValue(value)


-class NominalConversor(object):
+class NominalConversor:
     def __init__(self, values):
         self.values = set(values)
         self.zero_value = values[0]
@@ -447,10 +450,10 @@
                 # with EncodedNominalConversor.
                 return self.zero_value
             raise BadNominalValue(value)
-        return unicode(value)
-
-
-class DenseGeneratorData(object):
+        return str(value)
+
+
+class DenseGeneratorData:
     '''Internal helper class to allow for different matrix types without
     making the code a huge collection of if statements.'''

@@ -463,7 +466,7 @@
                     raise BadDataFormat(row)
                 # XXX: int 0 is used for implicit values, not '0'
                 values = [values[i] if i in values else 0 for i in
-                          xrange(len(conversors))]
+                          range(len(conversors))]
             else:
                 if len(values) != len(conversors):
                     raise BadDataFormat(row)
@@ -502,27 +505,27 @@

             new_data = []
             for value in inst:
-                if value is None or value == u'' or value != value:
+                if value is None or value == '' or value != value:
                     s = '?'
                 else:
-                    s = encode_string(unicode(value))
+                    s = encode_string(str(value))
                 new_data.append(s)

             current_row += 1
-            yield u','.join(new_data)
-
-
-class _DataListMixin(object):
+            yield ','.join(new_data)
+
+
+class _DataListMixin:
     """Mixin to return a list from decode_rows instead of a generator"""
     def decode_rows(self, stream, conversors):
-        return list(super(_DataListMixin, self).decode_rows(stream, conversors))
+        return list(super().decode_rows(stream, conversors))


 class Data(_DataListMixin, DenseGeneratorData):
     pass


-class COOData(object):
+class COOData:
     def decode_rows(self, stream, conversors):
         data, rows, cols = [], [], []
         for i, row in enumerate(stream):
@@ -559,7 +562,7 @@
         data = data.data

         # Check if the rows are sorted
-        if not all(row[i] <= row[i + 1] for i in xrange(len(row) - 1)):
+        if not all(row[i] <= row[i + 1] for i in range(len(row) - 1)):
             raise ValueError("liac-arff can only output COO matrices with "
                              "sorted rows.")

@@ -567,7 +570,7 @@
             if row > current_row:
                 # Add empty rows if necessary
                 while current_row < row:
-                    yield " ".join([u"{", u','.join(new_data), u"}"])
+                    yield " ".join(["{", ','.join(new_data), "}"])
                     new_data = []
                     current_row += 1

@@ -577,15 +580,15 @@
                     (current_row, col + 1, num_attributes)
                 )

-            if v is None or v == u'' or v != v:
+            if v is None or v == '' or v != v:
                 s = '?'
             else:
-                s = encode_string(unicode(v))
+                s = encode_string(str(v))
             new_data.append("%d %s" % (col, s))

-        yield " ".join([u"{", u','.join(new_data), u"}"])
-
-class LODGeneratorData(object):
+        yield " ".join(["{", ','.join(new_data), "}"])
+
+class LODGeneratorData:
     def decode_rows(self, stream, conversors):
         for row in stream:
             values = _parse_values(row)
@@ -618,14 +621,14 @@

             for col in sorted(row):
                 v = row[col]
-                if v is None or v == u'' or v != v:
+                if v is None or v == '' or v != v:
                     s = '?'
                 else:
-                    s = encode_string(unicode(v))
+                    s = encode_string(str(v))
                 new_data.append("%d %s" % (col, s))

             current_row += 1
-            yield " ".join([u"{", u','.join(new_data), u"}"])
+            yield " ".join(["{", ','.join(new_data), "}"])

 class LODData(_DataListMixin, LODGeneratorData):
     pass
@@ -660,7 +663,7 @@
 # =============================================================================

 # ADVANCED INTERFACE ==========================================================
-class ArffDecoder(object):
+class ArffDecoder:
     '''An ARFF decoder.'''

     def __init__(self):
@@ -704,7 +707,7 @@
         if not _RE_RELATION.match(v):
             raise BadRelationFormat()

-        res = unicode(v.strip('"\''))
+        res = str(v.strip('"\''))
         return res

     def _decode_attribute(self, s):
@@ -746,10 +749,10 @@
         name, type_ = m.groups()

         # Extracts the final name
-        name = unicode(name.strip('"\''))
+        name = str(name.strip('"\''))

         # Extracts the final type
-        if _RE_TYPE_NOMINAL.match(type_):
+        if type_[:1] == "{" and type_[-1:] == "}":
             try:
                 type_ = _parse_values(type_.strip('{} '))
             except Exception:
@@ -759,7 +762,7 @@

         else:
             # If not nominal, verify the type name
-            type_ = unicode(type_).upper()
+            type_ = str(type_).upper()
             if type_ not in ['NUMERIC', 'REAL', 'INTEGER', 'STRING']:
                 raise BadAttributeType()

@@ -772,15 +775,15 @@
         self._current_line = 0

         # If string, convert to a list of lines
-        if isinstance(s, basestring):
+        if isinstance(s, str):
             s = s.strip('\r\n ').replace('\r\n', '\n').split('\n')

         # Create the return object
-        obj = {
-            u'description': u'',
-            u'relation': u'',
-            u'attributes': [],
-            u'data': []
+        obj: ArffContainerType = {
+            'description': '',
+            'relation': '',
+            'attributes': [],
+            'data': []
         }
         attribute_names = {}

@@ -832,7 +835,7 @@
                     else:
                         conversor = NominalConversor(attr[1])
                 else:
-                    CONVERSOR_MAP = {'STRING': unicode,
+                    CONVERSOR_MAP = {'STRING': str,
                                      'INTEGER': lambda x: int(float(x)),
                                      'NUMERIC': float,
                                      'REAL': float}
@@ -895,7 +898,7 @@
             raise e


-class ArffEncoder(object):
+class ArffEncoder:
     '''An ARFF encoder.'''

     def _encode_comment(self, s=''):
@@ -911,9 +914,9 @@
         :return: a string with the encoded comment line.
         '''
         if s:
-            return u'%s %s'%(_TK_COMMENT, s)
+            return '%s %s'%(_TK_COMMENT, s)
         else:
-            return u'%s' % _TK_COMMENT
+            return '%s' % _TK_COMMENT

     def _encode_relation(self, name):
         '''(INTERNAL) Decodes a relation line.
@@ -929,7 +932,7 @@
                 name = '"%s"'%name
                 break

-        return u'%s %s'%(_TK_RELATION, name)
+        return '%s %s'%(_TK_RELATION, name)

     def _encode_attribute(self, name, type_):
         '''(INTERNAL) Encodes an attribute line.
@@ -960,20 +963,20 @@
                 break

         if isinstance(type_, (tuple, list)):
-            type_tmp = [u'%s' % encode_string(type_k) for type_k in type_]
-            type_ = u'{%s}'%(u', '.join(type_tmp))
-
-        return u'%s %s %s'%(_TK_ATTRIBUTE, name, type_)
+            type_tmp = ['%s' % encode_string(type_k) for type_k in type_]
+            type_ = '{%s}'%(', '.join(type_tmp))
+
+        return '%s %s %s'%(_TK_ATTRIBUTE, name, type_)

     def encode(self, obj):
         '''Encodes a given object to an ARFF file.

         :param obj: the object containing the ARFF information.
-        :return: the ARFF file as an unicode string.
+        :return: the ARFF file as an string.
         '''
         data = [row for row in self.iter_encode(obj)]

-        return u'\n'.join(data)
+        return '\n'.join(data)

     def iter_encode(self, obj):
         '''The iterative version of `arff.ArffEncoder.encode`.
@@ -982,7 +985,7 @@
         lines of the ARFF file.

         :param obj: the object containing the ARFF information.
-        :return: (yields) the ARFF file as unicode strings.
+        :return: (yields) the ARFF file as strings.
         '''
         # DESCRIPTION
         if obj.get('description', None):
@@ -994,7 +997,7 @@
             raise BadObject('Relation name not found or with invalid value.')

         yield self._encode_relation(obj['relation'])
-        yield u''
+        yield ''

         # ATTRIBUTES
         if not obj.get('attributes'):
@@ -1005,10 +1008,10 @@
             # Verify for bad object format
             if not isinstance(attr, (tuple, list)) or \
                len(attr) != 2 or \
-               not isinstance(attr[0], basestring):
+               not isinstance(attr[0], str):
                 raise BadObject('Invalid attribute declaration "%s"'%str(attr))

-            if isinstance(attr[1], basestring):
+            if isinstance(attr[1], str):
                 # Verify for invalid types
                 if attr[1] not in _SIMPLE_TYPES:
                     raise BadObject('Invalid attribute type "%s"'%str(attr))
@@ -1025,17 +1028,16 @@
                 attribute_names.add(attr[0])

             yield self._encode_attribute(attr[0], attr[1])
-        yield u''
+        yield ''
         attributes = obj['attributes']

         # DATA
         yield _TK_DATA
         if 'data' in obj:
             data = _get_data_object_for_encoding(obj.get('data'))
-            for line in data.encode_data(obj.get('data'), attributes):
-                yield line
-
-        yield u''
+            yield from data.encode_data(obj.get('data'), attributes)
+
+        yield ''

 # =============================================================================

@@ -1088,7 +1090,7 @@

     last_row = next(generator)
     for row in generator:
-        fp.write(last_row + u'\n')
+        fp.write(last_row + '\n')
         last_row = row
     fp.write(last_row)

('sklearn/linear_model', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,81 +1,102 @@
 """
-The :mod:`sklearn.linear_model` module implements generalized linear models. It
-includes Ridge regression, Bayesian Regression, Lasso and Elastic Net
-estimators computed with Least Angle Regression and coordinate descent. It also
-implements Stochastic Gradient Descent related algorithms.
+The :mod:`sklearn.linear_model` module implements a variety of linear models.
 """

 # See http://scikit-learn.sourceforge.net/modules/sgd.html and
 # http://scikit-learn.sourceforge.net/modules/linear_model.html for
 # complete documentation.

-from .base import LinearRegression
+from ._base import LinearRegression
+from ._bayes import BayesianRidge, ARDRegression
+from ._least_angle import (
+    Lars,
+    LassoLars,
+    lars_path,
+    lars_path_gram,
+    LarsCV,
+    LassoLarsCV,
+    LassoLarsIC,
+)
+from ._coordinate_descent import (
+    Lasso,
+    ElasticNet,
+    LassoCV,
+    ElasticNetCV,
+    lasso_path,
+    enet_path,
+    MultiTaskLasso,
+    MultiTaskElasticNet,
+    MultiTaskElasticNetCV,
+    MultiTaskLassoCV,
+)
+from ._glm import PoissonRegressor, GammaRegressor, TweedieRegressor
+from ._huber import HuberRegressor
+from ._sgd_fast import Hinge, Log, ModifiedHuber, SquaredLoss, Huber
+from ._stochastic_gradient import SGDClassifier, SGDRegressor, SGDOneClassSVM
+from ._ridge import Ridge, RidgeCV, RidgeClassifier, RidgeClassifierCV, ridge_regression
+from ._logistic import LogisticRegression, LogisticRegressionCV
+from ._omp import (
+    orthogonal_mp,
+    orthogonal_mp_gram,
+    OrthogonalMatchingPursuit,
+    OrthogonalMatchingPursuitCV,
+)
+from ._passive_aggressive import PassiveAggressiveClassifier
+from ._passive_aggressive import PassiveAggressiveRegressor
+from ._perceptron import Perceptron

-from .bayes import BayesianRidge, ARDRegression
-from .least_angle import (Lars, LassoLars, lars_path, lars_path_gram, LarsCV,
-                          LassoLarsCV, LassoLarsIC)
-from .coordinate_descent import (Lasso, ElasticNet, LassoCV, ElasticNetCV,
-                                 lasso_path, enet_path, MultiTaskLasso,
-                                 MultiTaskElasticNet, MultiTaskElasticNetCV,
-                                 MultiTaskLassoCV)
-from .huber import HuberRegressor
-from .sgd_fast import Hinge, Log, ModifiedHuber, SquaredLoss, Huber
-from .stochastic_gradient import SGDClassifier, SGDRegressor
-from .ridge import (Ridge, RidgeCV, RidgeClassifier, RidgeClassifierCV,
-                    ridge_regression)
-from .logistic import (LogisticRegression, LogisticRegressionCV,
-                       logistic_regression_path)
-from .omp import (orthogonal_mp, orthogonal_mp_gram, OrthogonalMatchingPursuit,
-                  OrthogonalMatchingPursuitCV)
-from .passive_aggressive import PassiveAggressiveClassifier
-from .passive_aggressive import PassiveAggressiveRegressor
-from .perceptron import Perceptron
+from ._quantile import QuantileRegressor
+from ._ransac import RANSACRegressor
+from ._theil_sen import TheilSenRegressor

-from .ransac import RANSACRegressor
-from .theil_sen import TheilSenRegressor
-
-__all__ = ['ARDRegression',
-           'BayesianRidge',
-           'ElasticNet',
-           'ElasticNetCV',
-           'Hinge',
-           'Huber',
-           'HuberRegressor',
-           'Lars',
-           'LarsCV',
-           'Lasso',
-           'LassoCV',
-           'LassoLars',
-           'LassoLarsCV',
-           'LassoLarsIC',
-           'LinearRegression',
-           'Log',
-           'LogisticRegression',
-           'LogisticRegressionCV',
-           'ModifiedHuber',
-           'MultiTaskElasticNet',
-           'MultiTaskElasticNetCV',
-           'MultiTaskLasso',
-           'MultiTaskLassoCV',
-           'OrthogonalMatchingPursuit',
-           'OrthogonalMatchingPursuitCV',
-           'PassiveAggressiveClassifier',
-           'PassiveAggressiveRegressor',
-           'Perceptron',
-           'Ridge',
-           'RidgeCV',
-           'RidgeClassifier',
-           'RidgeClassifierCV',
-           'SGDClassifier',
-           'SGDRegressor',
-           'SquaredLoss',
-           'TheilSenRegressor',
-           'enet_path',
-           'lars_path',
-           'lars_path_gram',
-           'lasso_path',
-           'logistic_regression_path',
-           'orthogonal_mp',
-           'orthogonal_mp_gram',
-           'ridge_regression',
-           'RANSACRegressor']
+__all__ = [
+    "ARDRegression",
+    "BayesianRidge",
+    "ElasticNet",
+    "ElasticNetCV",
+    "Hinge",
+    "Huber",
+    "HuberRegressor",
+    "Lars",
+    "LarsCV",
+    "Lasso",
+    "LassoCV",
+    "LassoLars",
+    "LassoLarsCV",
+    "LassoLarsIC",
+    "LinearRegression",
+    "Log",
+    "LogisticRegression",
+    "LogisticRegressionCV",
+    "ModifiedHuber",
+    "MultiTaskElasticNet",
+    "MultiTaskElasticNetCV",
+    "MultiTaskLasso",
+    "MultiTaskLassoCV",
+    "OrthogonalMatchingPursuit",
+    "OrthogonalMatchingPursuitCV",
+    "PassiveAggressiveClassifier",
+    "PassiveAggressiveRegressor",
+    "Perceptron",
+    "QuantileRegressor",
+    "Ridge",
+    "RidgeCV",
+    "RidgeClassifier",
+    "RidgeClassifierCV",
+    "SGDClassifier",
+    "SGDRegressor",
+    "SGDOneClassSVM",
+    "SquaredLoss",
+    "TheilSenRegressor",
+    "enet_path",
+    "lars_path",
+    "lars_path_gram",
+    "lasso_path",
+    "orthogonal_mp",
+    "orthogonal_mp_gram",
+    "ridge_regression",
+    "RANSACRegressor",
+    "PoissonRegressor",
+    "GammaRegressor",
+    "TweedieRegressor",
+]
('sklearn/linear_model', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,52 +1,49 @@
 import os
-
 import numpy

-from Cython import Tempita
+from sklearn._build_utils import gen_from_templates

-def configuration(parent_package='', top_path=None):
+
+def configuration(parent_package="", top_path=None):
     from numpy.distutils.misc_util import Configuration

-    config = Configuration('linear_model', parent_package, top_path)
+    config = Configuration("linear_model", parent_package, top_path)

     libraries = []
-    if os.name == 'posix':
-        libraries.append('m')
+    if os.name == "posix":
+        libraries.append("m")

-    config.add_extension('cd_fast',
-                         sources=['cd_fast.pyx'],
-                         include_dirs=numpy.get_include(),
-                         libraries=libraries)
+    config.add_extension(
+        "_cd_fast",
+        sources=["_cd_fast.pyx"],
+        include_dirs=numpy.get_include(),
+        libraries=libraries,
+    )

-    config.add_extension('sgd_fast',
-                         sources=['sgd_fast.pyx'],
-                         include_dirs=numpy.get_include(),
-                         libraries=libraries)
+    config.add_extension(
+        "_sgd_fast",
+        sources=["_sgd_fast.pyx"],
+        include_dirs=numpy.get_include(),
+        libraries=libraries,
+    )

     # generate sag_fast from template
-    sag_cython_file = 'sklearn/linear_model/sag_fast.pyx.tp'
-    sag_file = sag_cython_file.replace('.tp', '')
+    templates = ["sklearn/linear_model/_sag_fast.pyx.tp"]
+    gen_from_templates(templates)

-    if not (os.path.exists(sag_file) and
-            os.stat(sag_cython_file).st_mtime < os.stat(sag_file).st_mtime):
-
-        with open(sag_cython_file, "r") as f:
-            tmpl = f.read()
-        tmpl_ = Tempita.sub(tmpl)
-
-        with open(sag_file, "w") as f:
-            f.write(tmpl_)
-
-    config.add_extension('sag_fast',
-                         sources=['sag_fast.pyx'],
-                         include_dirs=numpy.get_include())
+    config.add_extension(
+        "_sag_fast", sources=["_sag_fast.pyx"], include_dirs=numpy.get_include()
+    )

     # add other directories
-    config.add_subpackage('tests')
+    config.add_subpackage("tests")
+    config.add_subpackage("_glm")
+    config.add_subpackage("_glm/tests")

     return config


-if __name__ == '__main__':
+if __name__ == "__main__":
     from numpy.distutils.core import setup
-    setup(**configuration(top_path='').todict())
+
+    setup(**configuration(top_path="").todict())
('sklearn/impute', '_base.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -2,66 +2,58 @@
 #          Sergey Feldman <sergeyfeldman@gmail.com>
 # License: BSD 3 clause

-from __future__ import division
-
+import numbers
 import warnings
-import numbers
+from collections import Counter

 import numpy as np
 import numpy.ma as ma
-from scipy import sparse
+from scipy import sparse as sp
 from scipy import stats

 from ..base import BaseEstimator, TransformerMixin
 from ..utils.sparsefuncs import _get_median
 from ..utils.validation import check_is_fitted
 from ..utils.validation import FLOAT_DTYPES
-from ..utils.fixes import _object_dtype_isnan
+from ..utils.validation import _check_feature_names_in
+from ..utils._mask import _get_mask
+from ..utils import _is_pandas_na
 from ..utils import is_scalar_nan
-from ..utils import check_array


 def _check_inputs_dtype(X, missing_values):
-    if (X.dtype.kind in ("f", "i", "u") and
-            not isinstance(missing_values, numbers.Real)):
-        raise ValueError("'X' and 'missing_values' types are expected to be"
-                         " both numerical. Got X.dtype={} and "
-                         " type(missing_values)={}."
-                         .format(X.dtype, type(missing_values)))
-
-
-def _get_mask(X, value_to_mask):
-    """Compute the boolean mask X == missing_values."""
-    if is_scalar_nan(value_to_mask):
-        if X.dtype.kind == "f":
-            return np.isnan(X)
-        elif X.dtype.kind in ("i", "u"):
-            # can't have NaNs in integer array.
-            return np.zeros(X.shape, dtype=bool)
-        else:
-            # np.isnan does not work on object dtypes.
-            return _object_dtype_isnan(X)
-    else:
-        # X == value_to_mask with object dytpes does not always perform
-        # element-wise for old versions of numpy
-        return np.equal(X, value_to_mask)
+    if _is_pandas_na(missing_values):
+        # Allow using `pd.NA` as missing values to impute numerical arrays.
+        return
+    if X.dtype.kind in ("f", "i", "u") and not isinstance(missing_values, numbers.Real):
+        raise ValueError(
+            "'X' and 'missing_values' types are expected to be"
+            " both numerical. Got X.dtype={} and "
+            " type(missing_values)={}.".format(X.dtype, type(missing_values))
+        )


 def _most_frequent(array, extra_value, n_repeat):
     """Compute the most frequent value in a 1d array extended with
-       [extra_value] * n_repeat, where extra_value is assumed to be not part
-       of the array."""
+    [extra_value] * n_repeat, where extra_value is assumed to be not part
+    of the array."""
     # Compute the most frequent value in array only
     if array.size > 0:
-        with warnings.catch_warnings():
-            # stats.mode raises a warning when input array contains objects due
-            # to incapacity to detect NaNs. Irrelevant here since input array
-            # has already been NaN-masked.
-            warnings.simplefilter("ignore", RuntimeWarning)
+        if array.dtype == object:
+            # scipy.stats.mode is slow with object dtype array.
+            # Python Counter is more efficient
+            counter = Counter(array)
+            most_frequent_count = counter.most_common(1)[0][1]
+            # tie breaking similarly to scipy.stats.mode
+            most_frequent_value = min(
+                value
+                for value, count in counter.items()
+                if count == most_frequent_count
+            )
+        else:
             mode = stats.mode(array)
-
-        most_frequent_value = mode[0][0]
-        most_frequent_count = mode[1][0]
+            most_frequent_value = mode[0][0]
+            most_frequent_count = mode[1][0]
     else:
         most_frequent_value = 0
         most_frequent_count = 0
@@ -74,25 +66,87 @@
     elif most_frequent_count > n_repeat:
         return most_frequent_value
     elif most_frequent_count == n_repeat:
-        # Ties the breaks. Copy the behaviour of scipy.stats.mode
-        if most_frequent_value < extra_value:
-            return most_frequent_value
-        else:
-            return extra_value
-
-
-class SimpleImputer(BaseEstimator, TransformerMixin):
+        # tie breaking similarly to scipy.stats.mode
+        return min(most_frequent_value, extra_value)
+
+
+class _BaseImputer(TransformerMixin, BaseEstimator):
+    """Base class for all imputers.
+
+    It adds automatically support for `add_indicator`.
+    """
+
+    def __init__(self, *, missing_values=np.nan, add_indicator=False):
+        self.missing_values = missing_values
+        self.add_indicator = add_indicator
+
+    def _fit_indicator(self, X):
+        """Fit a MissingIndicator."""
+        if self.add_indicator:
+            self.indicator_ = MissingIndicator(
+                missing_values=self.missing_values, error_on_new=False
+            )
+            self.indicator_._fit(X, precomputed=True)
+        else:
+            self.indicator_ = None
+
+    def _transform_indicator(self, X):
+        """Compute the indicator mask.'
+
+        Note that X must be the original data as passed to the imputer before
+        any imputation, since imputation may be done inplace in some cases.
+        """
+        if self.add_indicator:
+            if not hasattr(self, "indicator_"):
+                raise ValueError(
+                    "Make sure to call _fit_indicator before _transform_indicator"
+                )
+            return self.indicator_.transform(X)
+
+    def _concatenate_indicator(self, X_imputed, X_indicator):
+        """Concatenate indicator mask with the imputed data."""
+        if not self.add_indicator:
+            return X_imputed
+
+        hstack = sp.hstack if sp.issparse(X_imputed) else np.hstack
+        if X_indicator is None:
+            raise ValueError(
+                "Data from the missing indicator are not provided. Call "
+                "_fit_indicator and _transform_indicator in the imputer "
+                "implementation."
+            )
+
+        return hstack((X_imputed, X_indicator))
+
+    def _concatenate_indicator_feature_names_out(self, names, input_features):
+        if not self.add_indicator:
+            return names
+
+        indicator_names = self.indicator_.get_feature_names_out(input_features)
+        return np.concatenate([names, indicator_names])
+
+    def _more_tags(self):
+        return {"allow_nan": is_scalar_nan(self.missing_values)}
+
+
+class SimpleImputer(_BaseImputer):
     """Imputation transformer for completing missing values.

     Read more in the :ref:`User Guide <impute>`.
+
+    .. versionadded:: 0.20
+       `SimpleImputer` replaces the previous `sklearn.preprocessing.Imputer`
+       estimator which is now removed.

     Parameters
     ----------
-    missing_values : number, string, np.nan (default) or None
+    missing_values : int, float, str, np.nan, None or pandas.NA, default=np.nan
         The placeholder for the missing values. All occurrences of
-        `missing_values` will be imputed.
-
-    strategy : string, optional (default="mean")
+        `missing_values` will be imputed. For pandas' dataframes with
+        nullable integer dtypes with missing values, `missing_values`
+        can be set to either `np.nan` or `pd.NA`.
+
+    strategy : str, default='mean'
         The imputation strategy.

         - If "mean", then replace missing values using the mean along
@@ -101,32 +155,38 @@
           each column. Can only be used with numeric data.
         - If "most_frequent", then replace missing using the most frequent
           value along each column. Can be used with strings or numeric data.
+          If there is more than one such value, only the smallest is returned.
         - If "constant", then replace missing values with fill_value. Can be
           used with strings or numeric data.

         .. versionadded:: 0.20
            strategy="constant" for fixed value imputation.

-    fill_value : string or numerical value, optional (default=None)
+    fill_value : str or numerical value, default=None
         When strategy == "constant", fill_value is used to replace all
         occurrences of missing_values.
         If left to the default, fill_value will be 0 when imputing numerical
         data and "missing_value" for strings or object data types.

-    verbose : integer, optional (default=0)
+    verbose : int, default=0
         Controls the verbosity of the imputer.

-    copy : boolean, optional (default=True)
+        .. deprecated:: 1.1
+           The 'verbose' parameter was deprecated in version 1.1 and will be
+           removed in 1.3. A warning will always be raised upon the removal of
+           empty columns in the future version.
+
+    copy : bool, default=True
         If True, a copy of X will be created. If False, imputation will
         be done in-place whenever possible. Note that, in the following cases,
         a new copy will always be made, even if `copy=False`:

-        - If X is not an array of floating values;
-        - If X is encoded as a CSR matrix;
-        - If add_indicator=True.
-
-    add_indicator : boolean, optional (default=False)
-        If True, a `MissingIndicator` transform will stack onto output
+        - If `X` is not an array of floating values;
+        - If `X` is encoded as a CSR matrix;
+        - If `add_indicator=True`.
+
+    add_indicator : bool, default=False
+        If True, a :class:`MissingIndicator` transform will stack onto output
         of the imputer's transform. This allows a predictive estimator
         to account for missingness despite imputation. If a feature has no
         missing values at fit/train time, the feature won't appear on
@@ -137,14 +197,33 @@
     ----------
     statistics_ : array of shape (n_features,)
         The imputation fill value for each feature.
-
-    indicator_ : :class:`sklearn.impute.MissingIndicator`
+        Computing statistics can result in `np.nan` values.
+        During :meth:`transform`, features corresponding to `np.nan`
+        statistics will be discarded.
+
+    indicator_ : :class:`~sklearn.impute.MissingIndicator`
         Indicator used to add binary indicators for missing values.
-        ``None`` if add_indicator is False.
-
-    See also
+        `None` if `add_indicator=False`.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
     --------
     IterativeImputer : Multivariate imputation of missing values.
+
+    Notes
+    -----
+    Columns which only contained missing values at :meth:`fit` are discarded
+    upon :meth:`transform` if strategy is not `"constant"`.

     Examples
     --------
@@ -152,84 +231,118 @@
     >>> from sklearn.impute import SimpleImputer
     >>> imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
     >>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])
-    ... # doctest: +NORMALIZE_WHITESPACE
-    SimpleImputer(add_indicator=False, copy=True, fill_value=None,
-            missing_values=nan, strategy='mean', verbose=0)
+    SimpleImputer()
     >>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]
     >>> print(imp_mean.transform(X))
-    ... # doctest: +NORMALIZE_WHITESPACE
     [[ 7.   2.   3. ]
      [ 4.   3.5  6. ]
      [10.   3.5  9. ]]
-
-    Notes
-    -----
-    Columns which only contained missing values at `fit` are discarded upon
-    `transform` if strategy is not "constant".
-
     """
-    def __init__(self, missing_values=np.nan, strategy="mean",
-                 fill_value=None, verbose=0, copy=True, add_indicator=False):
-        self.missing_values = missing_values
+
+    def __init__(
+        self,
+        *,
+        missing_values=np.nan,
+        strategy="mean",
+        fill_value=None,
+        verbose="deprecated",
+        copy=True,
+        add_indicator=False,
+    ):
+        super().__init__(missing_values=missing_values, add_indicator=add_indicator)
         self.strategy = strategy
         self.fill_value = fill_value
         self.verbose = verbose
         self.copy = copy
-        self.add_indicator = add_indicator
-
-    def _validate_input(self, X):
+
+    def _validate_input(self, X, in_fit):
         allowed_strategies = ["mean", "median", "most_frequent", "constant"]
         if self.strategy not in allowed_strategies:
-            raise ValueError("Can only use these strategies: {0} "
-                             " got strategy={1}".format(allowed_strategies,
-                                                        self.strategy))
+            raise ValueError(
+                "Can only use these strategies: {0}  got strategy={1}".format(
+                    allowed_strategies, self.strategy
+                )
+            )

         if self.strategy in ("most_frequent", "constant"):
-            dtype = None
+            # If input is a list of strings, dtype = object.
+            # Otherwise ValueError is raised in SimpleImputer
+            # with strategy='most_frequent' or 'constant'
+            # because the list is converted to Unicode numpy array
+            if isinstance(X, list) and any(
+                isinstance(elem, str) for row in X for elem in row
+            ):
+                dtype = object
+            else:
+                dtype = None
         else:
             dtype = FLOAT_DTYPES

-        if not is_scalar_nan(self.missing_values):
+        if _is_pandas_na(self.missing_values) or is_scalar_nan(self.missing_values):
+            force_all_finite = "allow-nan"
+        else:
             force_all_finite = True
-        else:
-            force_all_finite = "allow-nan"

         try:
-            X = check_array(X, accept_sparse='csc', dtype=dtype,
-                            force_all_finite=force_all_finite, copy=self.copy)
+            X = self._validate_data(
+                X,
+                reset=in_fit,
+                accept_sparse="csc",
+                dtype=dtype,
+                force_all_finite=force_all_finite,
+                copy=self.copy,
+            )
         except ValueError as ve:
             if "could not convert" in str(ve):
-                raise ValueError("Cannot use {0} strategy with non-numeric "
-                                 "data. Received datatype :{1}."
-                                 "".format(self.strategy, X.dtype.kind))
+                new_ve = ValueError(
+                    "Cannot use {} strategy with non-numeric data:\n{}".format(
+                        self.strategy, ve
+                    )
+                )
+                raise new_ve from None
             else:
                 raise ve

         _check_inputs_dtype(X, self.missing_values)
         if X.dtype.kind not in ("i", "u", "f", "O"):
-            raise ValueError("SimpleImputer does not support data with dtype "
-                             "{0}. Please provide either a numeric array (with"
-                             " a floating point or integer dtype) or "
-                             "categorical data represented either as an array "
-                             "with integer dtype or an array of string values "
-                             "with an object dtype.".format(X.dtype))
+            raise ValueError(
+                "SimpleImputer does not support data with dtype "
+                "{0}. Please provide either a numeric array (with"
+                " a floating point or integer dtype) or "
+                "categorical data represented either as an array "
+                "with integer dtype or an array of string values "
+                "with an object dtype.".format(X.dtype)
+            )

         return X

     def fit(self, X, y=None):
-        """Fit the imputer on X.
+        """Fit the imputer on `X`.

         Parameters
         ----------
         X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            Input data, where ``n_samples`` is the number of samples and
-            ``n_features`` is the number of features.
+            Input data, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+
+        y : Ignored
+            Not used, present here for API consistency by convention.

         Returns
         -------
-        self : SimpleImputer
+        self : object
+            Fitted estimator.
         """
-        X = self._validate_input(X)
+        if self.verbose != "deprecated":
+            warnings.warn(
+                "The 'verbose' parameter was deprecated in version "
+                "1.1 and will be removed in 1.3. A warning will "
+                "always be raised upon the removal of empty columns "
+                "in the future version.",
+                FutureWarning,
+            )
+
+        X = self._validate_input(X, in_fit=True)

         # default fill_value is 0 for numerical input and "missing_value"
         # otherwise
@@ -242,43 +355,42 @@
             fill_value = self.fill_value

         # fill_value should be numerical in case of numerical input
-        if (self.strategy == "constant" and
-                X.dtype.kind in ("i", "u", "f") and
-                not isinstance(fill_value, numbers.Real)):
-            raise ValueError("'fill_value'={0} is invalid. Expected a "
-                             "numerical value when imputing numerical "
-                             "data".format(fill_value))
-
-        if sparse.issparse(X):
+        if (
+            self.strategy == "constant"
+            and X.dtype.kind in ("i", "u", "f")
+            and not isinstance(fill_value, numbers.Real)
+        ):
+            raise ValueError(
+                "'fill_value'={0} is invalid. Expected a "
+                "numerical value when imputing numerical "
+                "data".format(fill_value)
+            )
+
+        if sp.issparse(X):
             # missing_values = 0 not allowed with sparse data as it would
             # force densification
             if self.missing_values == 0:
-                raise ValueError("Imputation not possible when missing_values "
-                                 "== 0 and input is sparse. Provide a dense "
-                                 "array instead.")
+                raise ValueError(
+                    "Imputation not possible when missing_values "
+                    "== 0 and input is sparse. Provide a dense "
+                    "array instead."
+                )
             else:
-                self.statistics_ = self._sparse_fit(X,
-                                                    self.strategy,
-                                                    self.missing_values,
-                                                    fill_value)
-        else:
-            self.statistics_ = self._dense_fit(X,
-                                               self.strategy,
-                                               self.missing_values,
-                                               fill_value)
-
-        if self.add_indicator:
-            self.indicator_ = MissingIndicator(
-                missing_values=self.missing_values)
-            self.indicator_.fit(X)
-        else:
-            self.indicator_ = None
+                self.statistics_ = self._sparse_fit(
+                    X, self.strategy, self.missing_values, fill_value
+                )
+
+        else:
+            self.statistics_ = self._dense_fit(
+                X, self.strategy, self.missing_values, fill_value
+            )

         return self

     def _sparse_fit(self, X, strategy, missing_values, fill_value):
         """Fit the transformer on sparse data."""
-        mask_data = _get_mask(X.data, missing_values)
+        missing_mask = _get_mask(X, missing_values)
+        mask_data = missing_mask.data
         n_implicit_zeros = X.shape[0] - np.diff(X.indptr)

         statistics = np.empty(X.shape[1])
@@ -289,8 +401,8 @@
             statistics.fill(fill_value)
         else:
             for i in range(X.shape[1]):
-                column = X.data[X.indptr[i]:X.indptr[i + 1]]
-                mask_column = mask_data[X.indptr[i]:X.indptr[i + 1]]
+                column = X.data[X.indptr[i] : X.indptr[i + 1]]
+                mask_column = mask_data[X.indptr[i] : X.indptr[i + 1]]
                 column = column[~mask_column]

                 # combine explicit and implicit zeros
@@ -304,19 +416,20 @@
                     statistics[i] = np.nan if s == 0 else column.sum() / s

                 elif strategy == "median":
-                    statistics[i] = _get_median(column,
-                                                n_zeros)
+                    statistics[i] = _get_median(column, n_zeros)

                 elif strategy == "most_frequent":
-                    statistics[i] = _most_frequent(column,
-                                                   0,
-                                                   n_zeros)
+                    statistics[i] = _most_frequent(column, 0, n_zeros)
+        super()._fit_indicator(missing_mask)
+
         return statistics

     def _dense_fit(self, X, strategy, missing_values, fill_value):
         """Fit the transformer on dense data."""
-        mask = _get_mask(X, missing_values)
-        masked_X = ma.masked_array(X, mask=mask)
+        missing_mask = _get_mask(X, missing_values)
+        masked_X = ma.masked_array(X, mask=missing_mask)
+
+        super()._fit_indicator(missing_mask)

         # Mean
         if strategy == "mean":
@@ -338,14 +451,13 @@

         # Most frequent
         elif strategy == "most_frequent":
-            # scipy.stats.mstats.mode cannot be used because it will no work
-            # properly if the first element is masked and if its frequency
-            # is equal to the frequency of the most frequent valid element
-            # See https://github.com/scipy/scipy/issues/2636
+            # Avoid use of scipy.stats.mstats.mode due to the required
+            # additional overhead and slow benchmarking performance.
+            # See Issue 14325 and PR 14399 for full discussion.

             # To be able access the elements by columns
             X = X.transpose()
-            mask = mask.transpose()
+            mask = missing_mask.transpose()

             if X.dtype.kind == "O":
                 most_frequent = np.empty(X.shape[0], dtype=object)
@@ -353,7 +465,7 @@
                 most_frequent = np.empty(X.shape[0])

             for i, (row, row_mask) in enumerate(zip(X[:], mask[:])):
-                row_mask = np.logical_not(row_mask).astype(np.bool)
+                row_mask = np.logical_not(row_mask).astype(bool)
                 row = row[row_mask]
                 most_frequent[i] = _most_frequent(row, np.nan, 0)

@@ -366,29 +478,37 @@
             return np.full(X.shape[1], fill_value, dtype=X.dtype)

     def transform(self, X):
-        """Impute all missing values in X.
+        """Impute all missing values in `X`.

         Parameters
         ----------
         X : {array-like, sparse matrix}, shape (n_samples, n_features)
             The input data to complete.
+
+        Returns
+        -------
+        X_imputed : {ndarray, sparse matrix} of shape \
+                (n_samples, n_features_out)
+            `X` with imputed values.
         """
-        check_is_fitted(self, 'statistics_')
-
-        X = self._validate_input(X)
-
+        check_is_fitted(self)
+
+        X = self._validate_input(X, in_fit=False)
         statistics = self.statistics_

         if X.shape[1] != statistics.shape[0]:
-            raise ValueError("X has %d features per sample, expected %d"
-                             % (X.shape[1], self.statistics_.shape[0]))
-
-        if self.add_indicator:
-            X_trans_indicator = self.indicator_.transform(X)
+            raise ValueError(
+                "X has %d features per sample, expected %d"
+                % (X.shape[1], self.statistics_.shape[0])
+            )
+
+        # compute mask before eliminating invalid features
+        missing_mask = _get_mask(X, self.missing_values)

         # Delete the invalid columns if strategy is not constant
         if self.strategy == "constant":
             valid_statistics = statistics
+            valid_statistics_indexes = None
         else:
             # same as np.isnan but also works for object dtypes
             invalid_mask = _get_mask(statistics, np.nan)
@@ -397,44 +517,150 @@
             valid_statistics_indexes = np.flatnonzero(valid_mask)

             if invalid_mask.any():
-                missing = np.arange(X.shape[1])[invalid_mask]
-                if self.verbose:
-                    warnings.warn("Deleting features without "
-                                  "observed values: %s" % missing)
+                invalid_features = np.arange(X.shape[1])[invalid_mask]
+                if self.verbose != "deprecated" and self.verbose:
+                    # use feature names warning if features are provided
+                    if hasattr(self, "feature_names_in_"):
+                        invalid_features = self.feature_names_in_[invalid_features]
+                    warnings.warn(
+                        "Skipping features without any observed values:"
+                        f" {invalid_features}. At least one non-missing value is needed"
+                        f" for imputation with strategy='{self.strategy}'."
+                    )
                 X = X[:, valid_statistics_indexes]

         # Do actual imputation
-        if sparse.issparse(X):
+        if sp.issparse(X):
             if self.missing_values == 0:
-                raise ValueError("Imputation not possible when missing_values "
-                                 "== 0 and input is sparse. Provide a dense "
-                                 "array instead.")
+                raise ValueError(
+                    "Imputation not possible when missing_values "
+                    "== 0 and input is sparse. Provide a dense "
+                    "array instead."
+                )
             else:
-                mask = _get_mask(X.data, self.missing_values)
-                indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),
-                                    np.diff(X.indptr))[mask]
-
-                X.data[mask] = valid_statistics[indexes].astype(X.dtype,
-                                                                copy=False)
-        else:
-            mask = _get_mask(X, self.missing_values)
-            n_missing = np.sum(mask, axis=0)
+                # if no invalid statistics are found, use the mask computed
+                # before, else recompute mask
+                if valid_statistics_indexes is None:
+                    mask = missing_mask.data
+                else:
+                    mask = _get_mask(X.data, self.missing_values)
+                indexes = np.repeat(
+                    np.arange(len(X.indptr) - 1, dtype=int), np.diff(X.indptr)
+                )[mask]
+
+                X.data[mask] = valid_statistics[indexes].astype(X.dtype, copy=False)
+        else:
+            # use mask computed before eliminating invalid mask
+            if valid_statistics_indexes is None:
+                mask_valid_features = missing_mask
+            else:
+                mask_valid_features = missing_mask[:, valid_statistics_indexes]
+            n_missing = np.sum(mask_valid_features, axis=0)
             values = np.repeat(valid_statistics, n_missing)
-            coordinates = np.where(mask.transpose())[::-1]
+            coordinates = np.where(mask_valid_features.transpose())[::-1]

             X[coordinates] = values

-        if self.add_indicator:
-            hstack = sparse.hstack if sparse.issparse(X) else np.hstack
-            X = hstack((X, X_trans_indicator))
-
-        return X
+        X_indicator = super()._transform_indicator(missing_mask)
+
+        return super()._concatenate_indicator(X, X_indicator)
+
+    def inverse_transform(self, X):
+        """Convert the data back to the original representation.
+
+        Inverts the `transform` operation performed on an array.
+        This operation can only be performed after :class:`SimpleImputer` is
+        instantiated with `add_indicator=True`.
+
+        Note that `inverse_transform` can only invert the transform in
+        features that have binary indicators for missing values. If a feature
+        has no missing values at `fit` time, the feature won't have a binary
+        indicator, and the imputation done at `transform` time won't be
+        inverted.
+
+        .. versionadded:: 0.24
+
+        Parameters
+        ----------
+        X : array-like of shape \
+                (n_samples, n_features + n_features_missing_indicator)
+            The imputed data to be reverted to original data. It has to be
+            an augmented array of imputed data and the missing indicator mask.
+
+        Returns
+        -------
+        X_original : ndarray of shape (n_samples, n_features)
+            The original `X` with missing values as it was prior
+            to imputation.
+        """
+        check_is_fitted(self)
+
+        if not self.add_indicator:
+            raise ValueError(
+                "'inverse_transform' works only when "
+                "'SimpleImputer' is instantiated with "
+                "'add_indicator=True'. "
+                f"Got 'add_indicator={self.add_indicator}' "
+                "instead."
+            )
+
+        n_features_missing = len(self.indicator_.features_)
+        non_empty_feature_count = X.shape[1] - n_features_missing
+        array_imputed = X[:, :non_empty_feature_count].copy()
+        missing_mask = X[:, non_empty_feature_count:].astype(bool)
+
+        n_features_original = len(self.statistics_)
+        shape_original = (X.shape[0], n_features_original)
+        X_original = np.zeros(shape_original)
+        X_original[:, self.indicator_.features_] = missing_mask
+        full_mask = X_original.astype(bool)
+
+        imputed_idx, original_idx = 0, 0
+        while imputed_idx < len(array_imputed.T):
+            if not np.all(X_original[:, original_idx]):
+                X_original[:, original_idx] = array_imputed.T[imputed_idx]
+                imputed_idx += 1
+                original_idx += 1
+            else:
+                original_idx += 1
+
+        X_original[full_mask] = self.missing_values
+        return X_original

     def _more_tags(self):
-        return {'allow_nan': True}
-
-
-class MissingIndicator(BaseEstimator, TransformerMixin):
+        return {
+            "allow_nan": (
+                _is_pandas_na(self.missing_values) or is_scalar_nan(self.missing_values)
+            )
+        }
+
+    def get_feature_names_out(self, input_features=None):
+        """Get output feature names for transformation.
+
+        Parameters
+        ----------
+        input_features : array-like of str or None, default=None
+            Input features.
+
+            - If `input_features` is `None`, then `feature_names_in_` is
+              used as feature names in. If `feature_names_in_` is not defined,
+              then the following input feature names are generated:
+              `["x0", "x1", ..., "x(n_features_in_ - 1)"]`.
+            - If `input_features` is an array-like, then `input_features` must
+              match `feature_names_in_` if `feature_names_in_` is defined.
+
+        Returns
+        -------
+        feature_names_out : ndarray of str objects
+            Transformed feature names.
+        """
+        input_features = _check_feature_names_in(self, input_features)
+        non_missing_mask = np.logical_not(_get_mask(self.statistics_, np.nan))
+        names = input_features[non_missing_mask]
+        return self._concatenate_indicator_feature_names_out(names, input_features)
+
+
+class MissingIndicator(TransformerMixin, BaseEstimator):
     """Binary indicators for missing values.

     Note that this component typically should not be used in a vanilla
@@ -443,40 +669,59 @@

     Read more in the :ref:`User Guide <impute>`.

+    .. versionadded:: 0.20
+
     Parameters
     ----------
-    missing_values : number, string, np.nan (default) or None
+    missing_values : int, float, str, np.nan or None, default=np.nan
         The placeholder for the missing values. All occurrences of
-        `missing_values` will be indicated (True in the output array), the
-        other values will be marked as False.
-
-    features : str, optional
+        `missing_values` will be imputed. For pandas' dataframes with
+        nullable integer dtypes with missing values, `missing_values`
+        should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.
+
+    features : {'missing-only', 'all'}, default='missing-only'
         Whether the imputer mask should represent all or a subset of
         features.

-        - If "missing-only" (default), the imputer mask will only represent
+        - If `'missing-only'` (default), the imputer mask will only represent
           features containing missing values during fit time.
-        - If "all", the imputer mask will represent all features.
-
-    sparse : boolean or "auto", optional
+        - If `'all'`, the imputer mask will represent all features.
+
+    sparse : bool or 'auto', default='auto'
         Whether the imputer mask format should be sparse or dense.

-        - If "auto" (default), the imputer mask will be of same type as
+        - If `'auto'` (default), the imputer mask will be of same type as
           input.
-        - If True, the imputer mask will be a sparse matrix.
-        - If False, the imputer mask will be a numpy array.
-
-    error_on_new : boolean, optional
-        If True (default), transform will raise an error when there are
-        features with missing values in transform that have no missing values
-        in fit. This is applicable only when ``features="missing-only"``.
+        - If `True`, the imputer mask will be a sparse matrix.
+        - If `False`, the imputer mask will be a numpy array.
+
+    error_on_new : bool, default=True
+        If `True`, :meth:`transform` will raise an error when there are
+        features with missing values that have no missing values in
+        :meth:`fit`. This is applicable only when `features='missing-only'`.

     Attributes
     ----------
-    features_ : ndarray, shape (n_missing_features,) or (n_features,)
-        The features indices which will be returned when calling ``transform``.
-        They are computed during ``fit``. For ``features='all'``, it is
-        to ``range(n_features)``.
+    features_ : ndarray of shape (n_missing_features,) or (n_features,)
+        The features indices which will be returned when calling
+        :meth:`transform`. They are computed during :meth:`fit`. If
+        `features='all'`, `features_` is equal to `range(n_features)`.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    SimpleImputer : Univariate imputation of missing values.
+    IterativeImputer : Multivariate imputation of missing values.

     Examples
     --------
@@ -489,19 +734,23 @@
     ...                [np.nan, 2, 3],
     ...                [2, 4, 0]])
     >>> indicator = MissingIndicator()
-    >>> indicator.fit(X1)  # doctest: +NORMALIZE_WHITESPACE
-    MissingIndicator(error_on_new=True, features='missing-only',
-             missing_values=nan, sparse='auto')
+    >>> indicator.fit(X1)
+    MissingIndicator()
     >>> X2_tr = indicator.transform(X2)
     >>> X2_tr
     array([[False,  True],
            [ True, False],
            [False, False]])
-
     """

-    def __init__(self, missing_values=np.nan, features="missing-only",
-                 sparse="auto", error_on_new=True):
+    def __init__(
+        self,
+        *,
+        missing_values=np.nan,
+        features="missing-only",
+        sparse="auto",
+        error_on_new=True,
+    ):
         self.missing_values = missing_values
         self.features = features
         self.sparse = sparse
@@ -513,140 +762,198 @@

         Parameters
         ----------
-        X : {ndarray or sparse matrix}, shape (n_samples, n_features)
-            The input data with missing values. Note that ``X`` has been
-            checked in ``fit`` and ``transform`` before to call this function.
+        X : {ndarray, sparse matrix} of shape (n_samples, n_features)
+            The input data with missing values. Note that `X` has been
+            checked in :meth:`fit` and :meth:`transform` before to call this
+            function.

         Returns
         -------
-        imputer_mask : {ndarray or sparse matrix}, shape \
-(n_samples, n_features) or (n_samples, n_features_with_missing)
+        imputer_mask : {ndarray, sparse matrix} of shape \
+        (n_samples, n_features)
             The imputer mask of the original data.

-        features_with_missing : ndarray, shape (n_features_with_missing)
+        features_with_missing : ndarray of shape (n_features_with_missing)
             The features containing missing values.
-
         """
-        if sparse.issparse(X):
-            mask = _get_mask(X.data, self.missing_values)
-
-            # The imputer mask will be constructed with the same sparse format
-            # as X.
-            sparse_constructor = (sparse.csr_matrix if X.format == 'csr'
-                                  else sparse.csc_matrix)
-            imputer_mask = sparse_constructor(
-                (mask, X.indices.copy(), X.indptr.copy()),
-                shape=X.shape, dtype=bool)
+        if not self._precomputed:
+            imputer_mask = _get_mask(X, self.missing_values)
+        else:
+            imputer_mask = X
+
+        if sp.issparse(X):
             imputer_mask.eliminate_zeros()

-            if self.features == 'missing-only':
+            if self.features == "missing-only":
                 n_missing = imputer_mask.getnnz(axis=0)

             if self.sparse is False:
                 imputer_mask = imputer_mask.toarray()
-            elif imputer_mask.format == 'csr':
+            elif imputer_mask.format == "csr":
                 imputer_mask = imputer_mask.tocsc()
         else:
-            imputer_mask = _get_mask(X, self.missing_values)
-
-            if self.features == 'missing-only':
+            if not self._precomputed:
+                imputer_mask = _get_mask(X, self.missing_values)
+            else:
+                imputer_mask = X
+
+            if self.features == "missing-only":
                 n_missing = imputer_mask.sum(axis=0)

             if self.sparse is True:
-                imputer_mask = sparse.csc_matrix(imputer_mask)
-
-        if self.features == 'all':
+                imputer_mask = sp.csc_matrix(imputer_mask)
+
+        if self.features == "all":
             features_indices = np.arange(X.shape[1])
         else:
             features_indices = np.flatnonzero(n_missing)

         return imputer_mask, features_indices

-    def _validate_input(self, X):
+    def _validate_input(self, X, in_fit):
         if not is_scalar_nan(self.missing_values):
             force_all_finite = True
         else:
             force_all_finite = "allow-nan"
-        X = check_array(X, accept_sparse=('csc', 'csr'), dtype=None,
-                        force_all_finite=force_all_finite)
+        X = self._validate_data(
+            X,
+            reset=in_fit,
+            accept_sparse=("csc", "csr"),
+            dtype=None,
+            force_all_finite=force_all_finite,
+        )
         _check_inputs_dtype(X, self.missing_values)
         if X.dtype.kind not in ("i", "u", "f", "O"):
-            raise ValueError("MissingIndicator does not support data with "
-                             "dtype {0}. Please provide either a numeric array"
-                             " (with a floating point or integer dtype) or "
-                             "categorical data represented either as an array "
-                             "with integer dtype or an array of string values "
-                             "with an object dtype.".format(X.dtype))
-
-        if sparse.issparse(X) and self.missing_values == 0:
+            raise ValueError(
+                "MissingIndicator does not support data with "
+                "dtype {0}. Please provide either a numeric array"
+                " (with a floating point or integer dtype) or "
+                "categorical data represented either as an array "
+                "with integer dtype or an array of string values "
+                "with an object dtype.".format(X.dtype)
+            )
+
+        if sp.issparse(X) and self.missing_values == 0:
             # missing_values = 0 not allowed with sparse data as it would
             # force densification
-            raise ValueError("Sparse input with missing_values=0 is "
-                             "not supported. Provide a dense "
-                             "array instead.")
+            raise ValueError(
+                "Sparse input with missing_values=0 is "
+                "not supported. Provide a dense "
+                "array instead."
+            )

         return X

-    def fit(self, X, y=None):
-        """Fit the transformer on X.
+    def _fit(self, X, y=None, precomputed=False):
+        """Fit the transformer on `X`.

         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
-            Input data, where ``n_samples`` is the number of samples and
-            ``n_features`` is the number of features.
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Input data, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+            If `precomputed=True`, then `X` is a mask of the input data.
+
+        precomputed : bool
+            Whether the input data is a mask.
+
+        Returns
+        -------
+        imputer_mask : {ndarray, sparse matrix} of shape (n_samples, \
+        n_features)
+            The imputer mask of the original data.
+        """
+        if precomputed:
+            if not (hasattr(X, "dtype") and X.dtype.kind == "b"):
+                raise ValueError("precomputed is True but the input data is not a mask")
+            self._precomputed = True
+        else:
+            self._precomputed = False
+
+        # Need not validate X again as it would have already been validated
+        # in the Imputer calling MissingIndicator
+        if not self._precomputed:
+            X = self._validate_input(X, in_fit=True)
+
+        self._n_features = X.shape[1]
+
+        if self.features not in ("missing-only", "all"):
+            raise ValueError(
+                "'features' has to be either 'missing-only' or "
+                "'all'. Got {} instead.".format(self.features)
+            )
+
+        if not (
+            (isinstance(self.sparse, str) and self.sparse == "auto")
+            or isinstance(self.sparse, bool)
+        ):
+            raise ValueError(
+                "'sparse' has to be a boolean or 'auto'. Got {!r} instead.".format(
+                    self.sparse
+                )
+            )
+
+        missing_features_info = self._get_missing_features_info(X)
+        self.features_ = missing_features_info[1]
+
+        return missing_features_info[0]
+
+    def fit(self, X, y=None):
+        """Fit the transformer on `X`.
+
+        Parameters
+        ----------
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
+            Input data, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+
+        y : Ignored
+            Not used, present for API consistency by convention.

         Returns
         -------
         self : object
-            Returns self.
+            Fitted estimator.
         """
-        X = self._validate_input(X)
-        self._n_features = X.shape[1]
-
-        if self.features not in ('missing-only', 'all'):
-            raise ValueError("'features' has to be either 'missing-only' or "
-                             "'all'. Got {} instead.".format(self.features))
-
-        if not ((isinstance(self.sparse, str) and
-                self.sparse == "auto") or isinstance(self.sparse, bool)):
-            raise ValueError("'sparse' has to be a boolean or 'auto'. "
-                             "Got {!r} instead.".format(self.sparse))
-
-        self.features_ = self._get_missing_features_info(X)[1]
+        self._fit(X, y)

         return self

     def transform(self, X):
-        """Generate missing values indicator for X.
+        """Generate missing values indicator for `X`.

         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
             The input data to complete.

         Returns
         -------
-        Xt : {ndarray or sparse matrix}, shape (n_samples, n_features)
-            The missing indicator for input data. The data type of ``Xt``
+        Xt : {ndarray, sparse matrix} of shape (n_samples, n_features) \
+        or (n_samples, n_features_with_missing)
+            The missing indicator for input data. The data type of `Xt`
             will be boolean.
-
         """
-        check_is_fitted(self, "features_")
-        X = self._validate_input(X)
-
-        if X.shape[1] != self._n_features:
-            raise ValueError("X has a different number of features "
-                             "than during fitting.")
+        check_is_fitted(self)
+
+        # Need not validate X again as it would have already been validated
+        # in the Imputer calling MissingIndicator
+        if not self._precomputed:
+            X = self._validate_input(X, in_fit=False)
+        else:
+            if not (hasattr(X, "dtype") and X.dtype.kind == "b"):
+                raise ValueError("precomputed is True but the input data is not a mask")

         imputer_mask, features = self._get_missing_features_info(X)

         if self.features == "missing-only":
             features_diff_fit_trans = np.setdiff1d(features, self.features_)
-            if (self.error_on_new and features_diff_fit_trans.size > 0):
-                raise ValueError("The features {} have missing values "
-                                 "in transform but have no missing values "
-                                 "in fit.".format(features_diff_fit_trans))
+            if self.error_on_new and features_diff_fit_trans.size > 0:
+                raise ValueError(
+                    "The features {} have missing values "
+                    "in transform but have no missing values "
+                    "in fit.".format(features_diff_fit_trans)
+                )

             if self.features_.size < self._n_features:
                 imputer_mask = imputer_mask[:, self.features_]
@@ -654,22 +961,63 @@
         return imputer_mask

     def fit_transform(self, X, y=None):
-        """Generate missing values indicator for X.
+        """Generate missing values indicator for `X`.

         Parameters
         ----------
-        X : {array-like, sparse matrix}, shape (n_samples, n_features)
+        X : {array-like, sparse matrix} of shape (n_samples, n_features)
             The input data to complete.
+
+        y : Ignored
+            Not used, present for API consistency by convention.

         Returns
         -------
-        Xt : {ndarray or sparse matrix}, shape (n_samples, n_features)
-            The missing indicator for input data. The data type of ``Xt``
+        Xt : {ndarray, sparse matrix} of shape (n_samples, n_features) \
+        or (n_samples, n_features_with_missing)
+            The missing indicator for input data. The data type of `Xt`
             will be boolean.
-
         """
-        return self.fit(X, y).transform(X)
+        imputer_mask = self._fit(X, y)
+
+        if self.features_.size < self._n_features:
+            imputer_mask = imputer_mask[:, self.features_]
+
+        return imputer_mask
+
+    def get_feature_names_out(self, input_features=None):
+        """Get output feature names for transformation.
+
+        Parameters
+        ----------
+        input_features : array-like of str or None, default=None
+            Input features.
+
+            - If `input_features` is `None`, then `feature_names_in_` is
+              used as feature names in. If `feature_names_in_` is not defined,
+              then the following input feature names are generated:
+              `["x0", "x1", ..., "x(n_features_in_ - 1)"]`.
+            - If `input_features` is an array-like, then `input_features` must
+              match `feature_names_in_` if `feature_names_in_` is defined.
+
+        Returns
+        -------
+        feature_names_out : ndarray of str objects
+            Transformed feature names.
+        """
+        input_features = _check_feature_names_in(self, input_features)
+        prefix = self.__class__.__name__.lower()
+        return np.asarray(
+            [
+                f"{prefix}_{feature_name}"
+                for feature_name in input_features[self.features_]
+            ],
+            dtype=object,
+        )

     def _more_tags(self):
-        return {'allow_nan': True,
-                'X_types': ['2darray', 'str']}
+        return {
+            "allow_nan": True,
+            "X_types": ["2darray", "string"],
+            "preserves_dtype": [],
+        }
('sklearn/impute', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,8 +1,12 @@
 """Transformers for missing value imputation"""
+import typing

 from ._base import MissingIndicator, SimpleImputer
+from ._knn import KNNImputer

-__all__ = [
-    'MissingIndicator',
-    'SimpleImputer',
-]
+if typing.TYPE_CHECKING:
+    # Avoid errors in type checkers (e.g. mypy) for experimental estimators.
+    # TODO: remove this check once the estimator is no longer experimental.
+    from ._iterative import IterativeImputer  # noqa
+
+__all__ = ["MissingIndicator", "SimpleImputer", "KNNImputer"]
('sklearn/impute', '_iterative.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,30 +1,29 @@
-
 from time import time
-from distutils.version import LooseVersion
 from collections import namedtuple
 import warnings

-import scipy
 from scipy import stats
 import numpy as np

-from ..base import clone, BaseEstimator, TransformerMixin
+from ..base import clone
 from ..exceptions import ConvergenceWarning
 from ..preprocessing import normalize
-from ..utils import check_array, check_random_state, safe_indexing
+from ..utils import check_array, check_random_state, _safe_indexing, is_scalar_nan
 from ..utils.validation import FLOAT_DTYPES, check_is_fitted
-from ..utils import is_scalar_nan
-
-from ._base import (_get_mask, MissingIndicator, SimpleImputer,
-                    _check_inputs_dtype)
-
-
-_ImputerTriplet = namedtuple('_ImputerTriplet', ['feat_idx',
-                                                 'neighbor_feat_idx',
-                                                 'estimator'])
-
-
-class IterativeImputer(BaseEstimator, TransformerMixin):
+from ..utils.validation import _check_feature_names_in
+from ..utils._mask import _get_mask
+
+from ._base import _BaseImputer
+from ._base import SimpleImputer
+from ._base import _check_inputs_dtype
+
+
+_ImputerTriplet = namedtuple(
+    "_ImputerTriplet", ["feat_idx", "neighbor_feat_idx", "estimator"]
+)
+
+
+class IterativeImputer(_BaseImputer):
     """Multivariate imputer that estimates each feature from all the others.

     A strategy for imputing missing values by modeling each feature with
@@ -32,11 +31,13 @@

     Read more in the :ref:`User Guide <iterative_imputer>`.

+    .. versionadded:: 0.21
+
     .. note::

       This estimator is still **experimental** for now: the predictions
       and the API might change without any deprecation cycle. To use it,
-      you need to explicitly import ``enable_iterative_imputer``::
+      you need to explicitly import `enable_iterative_imputer`::

         >>> # explicitly require this experimental feature
         >>> from sklearn.experimental import enable_iterative_imputer  # noqa
@@ -47,31 +48,33 @@
     ----------
     estimator : estimator object, default=BayesianRidge()
         The estimator to use at each step of the round-robin imputation.
-        If ``sample_posterior`` is True, the estimator must support
-        ``return_std`` in its ``predict`` method.
-
-    missing_values : int, np.nan, optional (default=np.nan)
+        If `sample_posterior=True`, the estimator must support
+        `return_std` in its `predict` method.
+
+    missing_values : int or np.nan, default=np.nan
         The placeholder for the missing values. All occurrences of
-        ``missing_values`` will be imputed.
-
-    sample_posterior : boolean, default=False
+        `missing_values` will be imputed. For pandas' dataframes with
+        nullable integer dtypes with missing values, `missing_values`
+        should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.
+
+    sample_posterior : bool, default=False
         Whether to sample from the (Gaussian) predictive posterior of the
         fitted estimator for each imputation. Estimator must support
-        ``return_std`` in its ``predict`` method if set to ``True``. Set to
-        ``True`` if using ``IterativeImputer`` for multiple imputations.
-
-    max_iter : int, optional (default=10)
+        `return_std` in its `predict` method if set to `True`. Set to
+        `True` if using `IterativeImputer` for multiple imputations.
+
+    max_iter : int, default=10
         Maximum number of imputation rounds to perform before returning the
         imputations computed during the final round. A round is a single
         imputation of each feature with missing values. The stopping criterion
-        is met once `abs(max(X_t - X_{t-1}))/abs(max(X[known_vals]))` < tol,
-        where `X_t` is `X` at iteration `t. Note that early stopping is only
-        applied if ``sample_posterior=False``.
-
-    tol : float, optional (default=1e-3)
+        is met once `max(abs(X_t - X_{t-1}))/max(abs(X[known_vals])) < tol`,
+        where `X_t` is `X` at iteration `t`. Note that early stopping is only
+        applied if `sample_posterior=False`.
+
+    tol : float, default=1e-3
         Tolerance of the stopping condition.

-    n_nearest_features : int, optional (default=None)
+    n_nearest_features : int, default=None
         Number of other features to use to estimate the missing values of
         each feature column. Nearness between features is measured using
         the absolute correlation coefficient between each feature pair (after
@@ -79,49 +82,60 @@
         imputation process, the neighbor features are not necessarily nearest,
         but are drawn with probability proportional to correlation for each
         imputed target feature. Can provide significant speed-up when the
-        number of features is huge. If ``None``, all features will be used.
-
-    initial_strategy : str, optional (default="mean")
+        number of features is huge. If `None`, all features will be used.
+
+    initial_strategy : {'mean', 'median', 'most_frequent', 'constant'}, \
+            default='mean'
         Which strategy to use to initialize the missing values. Same as the
-        ``strategy`` parameter in :class:`sklearn.impute.SimpleImputer`
-        Valid values: {"mean", "median", "most_frequent", or "constant"}.
-
-    imputation_order : str, optional (default="ascending")
+        `strategy` parameter in :class:`~sklearn.impute.SimpleImputer`.
+
+    imputation_order : {'ascending', 'descending', 'roman', 'arabic', \
+            'random'}, default='ascending'
         The order in which the features will be imputed. Possible values:

-        "ascending"
-            From features with fewest missing values to most.
-        "descending"
-            From features with most missing values to fewest.
-        "roman"
-            Left to right.
-        "arabic"
-            Right to left.
-        "random"
-            A random order for each round.
-
-    min_value : float, optional (default=None)
-        Minimum possible imputed value. Default of ``None`` will set minimum
-        to negative infinity.
-
-    max_value : float, optional (default=None)
-        Maximum possible imputed value. Default of ``None`` will set maximum
-        to positive infinity.
-
-    verbose : int, optional (default=0)
+        - `'ascending'`: From features with fewest missing values to most.
+        - `'descending'`: From features with most missing values to fewest.
+        - `'roman'`: Left to right.
+        - `'arabic'`: Right to left.
+        - `'random'`: A random order for each round.
+
+    skip_complete : bool, default=False
+        If `True` then features with missing values during :meth:`transform`
+        which did not have any missing values during :meth:`fit` will be
+        imputed with the initial imputation method only. Set to `True` if you
+        have many features with no missing values at both :meth:`fit` and
+        :meth:`transform` time to save compute.
+
+    min_value : float or array-like of shape (n_features,), default=-np.inf
+        Minimum possible imputed value. Broadcast to shape `(n_features,)` if
+        scalar. If array-like, expects shape `(n_features,)`, one min value for
+        each feature. The default is `-np.inf`.
+
+        .. versionchanged:: 0.23
+           Added support for array-like.
+
+    max_value : float or array-like of shape (n_features,), default=np.inf
+        Maximum possible imputed value. Broadcast to shape `(n_features,)` if
+        scalar. If array-like, expects shape `(n_features,)`, one max value for
+        each feature. The default is `np.inf`.
+
+        .. versionchanged:: 0.23
+           Added support for array-like.
+
+    verbose : int, default=0
         Verbosity flag, controls the debug messages that are issued
         as functions are evaluated. The higher, the more verbose. Can be 0, 1,
         or 2.

-    random_state : int, RandomState instance or None, optional (default=None)
+    random_state : int, RandomState instance or None, default=None
         The seed of the pseudo random number generator to use. Randomizes
-        selection of estimator features if n_nearest_features is not None, the
-        ``imputation_order`` if ``random``, and the sampling from posterior if
-        ``sample_posterior`` is True. Use an integer for determinism.
+        selection of estimator features if `n_nearest_features` is not `None`,
+        the `imputation_order` if `random`, and the sampling from posterior if
+        `sample_posterior=True`. Use an integer for determinism.
         See :term:`the Glossary <random_state>`.

-    add_indicator : boolean, optional (default=False)
-        If True, a `MissingIndicator` transform will stack onto output
+    add_indicator : bool, default=False
+        If `True`, a :class:`MissingIndicator` transform will stack onto output
         of the imputer's transform. This allows a predictive estimator
         to account for missingness despite imputation. If a feature has no
         missing values at fit/train time, the feature won't appear on
@@ -130,44 +144,55 @@

     Attributes
     ----------
-    initial_imputer_ : object of type :class:`sklearn.impute.SimpleImputer`
+    initial_imputer_ : object of type :class:`~sklearn.impute.SimpleImputer`
         Imputer used to initialize the missing values.

     imputation_sequence_ : list of tuples
-        Each tuple has ``(feat_idx, neighbor_feat_idx, estimator)``, where
-        ``feat_idx`` is the current feature to be imputed,
-        ``neighbor_feat_idx`` is the array of other features used to impute the
-        current feature, and ``estimator`` is the trained estimator used for
-        the imputation. Length is ``self.n_features_with_missing_ *
-        self.n_iter_``.
+        Each tuple has `(feat_idx, neighbor_feat_idx, estimator)`, where
+        `feat_idx` is the current feature to be imputed,
+        `neighbor_feat_idx` is the array of other features used to impute the
+        current feature, and `estimator` is the trained estimator used for
+        the imputation. Length is `self.n_features_with_missing_ *
+        self.n_iter_`.

     n_iter_ : int
         Number of iteration rounds that occurred. Will be less than
-        ``self.max_iter`` if early stopping criterion was reached.
+        `self.max_iter` if early stopping criterion was reached.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0

     n_features_with_missing_ : int
         Number of features with missing values.

-    indicator_ : :class:`sklearn.impute.MissingIndicator`
+    indicator_ : :class:`~sklearn.impute.MissingIndicator`
         Indicator used to add binary indicators for missing values.
-        ``None`` if add_indicator is False.
-
-    See also
+        `None` if `add_indicator=False`.
+
+    random_state_ : RandomState instance
+        RandomState instance that is generated either from a seed, the random
+        number generator or by `np.random`.
+
+    See Also
     --------
     SimpleImputer : Univariate imputation of missing values.

     Notes
     -----
     To support imputation in inductive mode we store each feature's estimator
-    during the ``fit`` phase, and predict without refitting (in order) during
-    the ``transform`` phase.
-
-    Features which contain all missing values at ``fit`` are discarded upon
-    ``transform``.
-
-    Features with missing values during ``transform`` which did not have any
-    missing values during ``fit`` will be imputed with the initial imputation
-    method only.
+    during the :meth:`fit` phase, and predict without refitting (in order)
+    during the :meth:`transform` phase.
+
+    Features which contain all missing values at :meth:`fit` are discarded upon
+    :meth:`transform`.

     References
     ----------
@@ -180,49 +205,69 @@
         Multivariate Data Suitable for use with an Electronic Computer".
         Journal of the Royal Statistical Society 22(2): 302-306.
         <https://www.jstor.org/stable/2984099>`_
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.experimental import enable_iterative_imputer
+    >>> from sklearn.impute import IterativeImputer
+    >>> imp_mean = IterativeImputer(random_state=0)
+    >>> imp_mean.fit([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])
+    IterativeImputer(random_state=0)
+    >>> X = [[np.nan, 2, 3], [4, np.nan, 6], [10, np.nan, 9]]
+    >>> imp_mean.transform(X)
+    array([[ 6.9584...,  2.       ,  3.        ],
+           [ 4.       ,  2.6000...,  6.        ],
+           [10.       ,  4.9999...,  9.        ]])
     """

-    def __init__(self,
-                 estimator=None,
-                 missing_values=np.nan,
-                 sample_posterior=False,
-                 max_iter=10,
-                 tol=1e-3,
-                 n_nearest_features=None,
-                 initial_strategy="mean",
-                 imputation_order='ascending',
-                 min_value=None,
-                 max_value=None,
-                 verbose=0,
-                 random_state=None,
-                 add_indicator=False):
+    def __init__(
+        self,
+        estimator=None,
+        *,
+        missing_values=np.nan,
+        sample_posterior=False,
+        max_iter=10,
+        tol=1e-3,
+        n_nearest_features=None,
+        initial_strategy="mean",
+        imputation_order="ascending",
+        skip_complete=False,
+        min_value=-np.inf,
+        max_value=np.inf,
+        verbose=0,
+        random_state=None,
+        add_indicator=False,
+    ):
+        super().__init__(missing_values=missing_values, add_indicator=add_indicator)

         self.estimator = estimator
-        self.missing_values = missing_values
         self.sample_posterior = sample_posterior
         self.max_iter = max_iter
         self.tol = tol
         self.n_nearest_features = n_nearest_features
         self.initial_strategy = initial_strategy
         self.imputation_order = imputation_order
+        self.skip_complete = skip_complete
         self.min_value = min_value
         self.max_value = max_value
         self.verbose = verbose
         self.random_state = random_state
-        self.add_indicator = add_indicator
-
-    def _impute_one_feature(self,
-                            X_filled,
-                            mask_missing_values,
-                            feat_idx,
-                            neighbor_feat_idx,
-                            estimator=None,
-                            fit_mode=True):
+
+    def _impute_one_feature(
+        self,
+        X_filled,
+        mask_missing_values,
+        feat_idx,
+        neighbor_feat_idx,
+        estimator=None,
+        fit_mode=True,
+    ):
         """Impute a single feature from the others provided.

         This function predicts the missing values of one of the features using
-        the current estimates of all the other features. The ``estimator`` must
-        support ``return_std=True`` in its ``predict`` method for this function
+        the current estimates of all the other features. The `estimator` must
+        support `return_std=True` in its `predict` method for this function
         to work.

         Parameters
@@ -237,12 +282,12 @@
             Index of the feature currently being imputed.

         neighbor_feat_idx : ndarray
-            Indices of the features to be used in imputing ``feat_idx``.
+            Indices of the features to be used in imputing `feat_idx`.

         estimator : object
             The estimator to use at this step of the round-robin imputation.
-            If ``sample_posterior`` is True, the estimator must support
-            ``return_std`` in its ``predict`` method.
+            If `sample_posterior=True`, the estimator must support
+            `return_std` in its `predict` method.
             If None, it will be cloned from self._estimator.

         fit_mode : boolean, default=True
@@ -251,111 +296,96 @@
         Returns
         -------
         X_filled : ndarray
-            Input data with ``X_filled[missing_row_mask, feat_idx]`` updated.
+            Input data with `X_filled[missing_row_mask, feat_idx]` updated.

         estimator : estimator with sklearn API
             The fitted estimator used to impute
-            ``X_filled[missing_row_mask, feat_idx]``.
-        """
-
-        # if nothing is missing, just return the default
-        # (should not happen at fit time because feat_ids would be excluded)
-        missing_row_mask = mask_missing_values[:, feat_idx]
-        if not np.any(missing_row_mask):
-            return X_filled, estimator
-
+            `X_filled[missing_row_mask, feat_idx]`.
+        """
         if estimator is None and fit_mode is False:
-            raise ValueError("If fit_mode is False, then an already-fitted "
-                             "estimator should be passed in.")
+            raise ValueError(
+                "If fit_mode is False, then an already-fitted "
+                "estimator should be passed in."
+            )

         if estimator is None:
             estimator = clone(self._estimator)

+        missing_row_mask = mask_missing_values[:, feat_idx]
         if fit_mode:
-            X_train = safe_indexing(X_filled[:, neighbor_feat_idx],
-                                    ~missing_row_mask)
-            y_train = safe_indexing(X_filled[:, feat_idx],
-                                    ~missing_row_mask)
+            X_train = _safe_indexing(X_filled[:, neighbor_feat_idx], ~missing_row_mask)
+            y_train = _safe_indexing(X_filled[:, feat_idx], ~missing_row_mask)
             estimator.fit(X_train, y_train)

-        # get posterior samples
-        X_test = safe_indexing(X_filled[:, neighbor_feat_idx],
-                               missing_row_mask)
+        # if no missing values, don't predict
+        if np.sum(missing_row_mask) == 0:
+            return X_filled, estimator
+
+        # get posterior samples if there is at least one missing value
+        X_test = _safe_indexing(X_filled[:, neighbor_feat_idx], missing_row_mask)
         if self.sample_posterior:
             mus, sigmas = estimator.predict(X_test, return_std=True)
             imputed_values = np.zeros(mus.shape, dtype=X_filled.dtype)
-            # two types of problems: (1) non-positive sigmas, (2) mus outside
-            # legal range of min_value and max_value (results in inf sample)
+            # two types of problems: (1) non-positive sigmas
+            # (2) mus outside legal range of min_value and max_value
+            # (results in inf sample)
             positive_sigmas = sigmas > 0
             imputed_values[~positive_sigmas] = mus[~positive_sigmas]
-            mus_too_low = mus < self._min_value
-            imputed_values[mus_too_low] = self._min_value
-            mus_too_high = mus > self._max_value
-            imputed_values[mus_too_high] = self._max_value
+            mus_too_low = mus < self._min_value[feat_idx]
+            imputed_values[mus_too_low] = self._min_value[feat_idx]
+            mus_too_high = mus > self._max_value[feat_idx]
+            imputed_values[mus_too_high] = self._max_value[feat_idx]
             # the rest can be sampled without statistical issues
             inrange_mask = positive_sigmas & ~mus_too_low & ~mus_too_high
             mus = mus[inrange_mask]
             sigmas = sigmas[inrange_mask]
-            a = (self._min_value - mus) / sigmas
-            b = (self._max_value - mus) / sigmas
-
-            if scipy.__version__ < LooseVersion('0.18'):
-                # bug with vector-valued `a` in old scipy
-                imputed_values[inrange_mask] = [
-                    stats.truncnorm(a=a_, b=b_,
-                                    loc=loc_, scale=scale_).rvs(
-                                        random_state=self.random_state_)
-                    for a_, b_, loc_, scale_
-                    in zip(a, b, mus, sigmas)]
-            else:
-                truncated_normal = stats.truncnorm(a=a, b=b,
-                                                   loc=mus, scale=sigmas)
-                imputed_values[inrange_mask] = truncated_normal.rvs(
-                    random_state=self.random_state_)
+            a = (self._min_value[feat_idx] - mus) / sigmas
+            b = (self._max_value[feat_idx] - mus) / sigmas
+
+            truncated_normal = stats.truncnorm(a=a, b=b, loc=mus, scale=sigmas)
+            imputed_values[inrange_mask] = truncated_normal.rvs(
+                random_state=self.random_state_
+            )
         else:
             imputed_values = estimator.predict(X_test)
-            imputed_values = np.clip(imputed_values,
-                                     self._min_value,
-                                     self._max_value)
+            imputed_values = np.clip(
+                imputed_values, self._min_value[feat_idx], self._max_value[feat_idx]
+            )

         # update the feature
         X_filled[missing_row_mask, feat_idx] = imputed_values
         return X_filled, estimator

-    def _get_neighbor_feat_idx(self,
-                               n_features,
-                               feat_idx,
-                               abs_corr_mat):
-        """Get a list of other features to predict ``feat_idx``.
-
-        If self.n_nearest_features is less than or equal to the total
+    def _get_neighbor_feat_idx(self, n_features, feat_idx, abs_corr_mat):
+        """Get a list of other features to predict `feat_idx`.
+
+        If `self.n_nearest_features` is less than or equal to the total
         number of features, then use a probability proportional to the absolute
-        correlation between ``feat_idx`` and each other feature to randomly
+        correlation between `feat_idx` and each other feature to randomly
         choose a subsample of the other features (without replacement).

         Parameters
         ----------
         n_features : int
-            Number of features in ``X``.
+            Number of features in `X`.

         feat_idx : int
             Index of the feature currently being imputed.

         abs_corr_mat : ndarray, shape (n_features, n_features)
-            Absolute correlation matrix of ``X``. The diagonal has been zeroed
+            Absolute correlation matrix of `X`. The diagonal has been zeroed
             out and each feature has been normalized to sum to 1. Can be None.

         Returns
         -------
         neighbor_feat_idx : array-like
-            The features to use to impute ``feat_idx``.
-        """
-        if (self.n_nearest_features is not None and
-                self.n_nearest_features < n_features):
+            The features to use to impute `feat_idx`.
+        """
+        if self.n_nearest_features is not None and self.n_nearest_features < n_features:
             p = abs_corr_mat[:, feat_idx]
             neighbor_feat_idx = self.random_state_.choice(
-                np.arange(n_features), self.n_nearest_features, replace=False,
-                p=p)
+                np.arange(n_features), self.n_nearest_features, replace=False, p=p
+            )
         else:
             inds_left = np.arange(feat_idx)
             inds_right = np.arange(feat_idx + 1, n_features)
@@ -374,8 +404,8 @@
         Parameters
         ----------
         mask_missing_values : array-like, shape (n_samples, n_features)
-            Input data's missing indicator matrix, where "n_samples" is the
-            number of samples and "n_features" is the number of features.
+            Input data's missing indicator matrix, where `n_samples` is the
+            number of samples and `n_features` is the number of features.

         Returns
         -------
@@ -383,27 +413,30 @@
             The order in which to impute the features.
         """
         frac_of_missing_values = mask_missing_values.mean(axis=0)
-        missing_values_idx = np.nonzero(frac_of_missing_values)[0]
-        if self.imputation_order == 'roman':
+        if self.skip_complete:
+            missing_values_idx = np.flatnonzero(frac_of_missing_values)
+        else:
+            missing_values_idx = np.arange(np.shape(frac_of_missing_values)[0])
+        if self.imputation_order == "roman":
             ordered_idx = missing_values_idx
-        elif self.imputation_order == 'arabic':
+        elif self.imputation_order == "arabic":
             ordered_idx = missing_values_idx[::-1]
-        elif self.imputation_order == 'ascending':
+        elif self.imputation_order == "ascending":
             n = len(frac_of_missing_values) - len(missing_values_idx)
-            ordered_idx = np.argsort(frac_of_missing_values,
-                                     kind='mergesort')[n:][::-1]
-        elif self.imputation_order == 'descending':
+            ordered_idx = np.argsort(frac_of_missing_values, kind="mergesort")[n:]
+        elif self.imputation_order == "descending":
             n = len(frac_of_missing_values) - len(missing_values_idx)
-            ordered_idx = np.argsort(frac_of_missing_values,
-                                     kind='mergesort')[n:]
-        elif self.imputation_order == 'random':
+            ordered_idx = np.argsort(frac_of_missing_values, kind="mergesort")[n:][::-1]
+        elif self.imputation_order == "random":
             ordered_idx = missing_values_idx
             self.random_state_.shuffle(ordered_idx)
         else:
-            raise ValueError("Got an invalid imputation order: '{0}'. It must "
-                             "be one of the following: 'roman', 'arabic', "
-                             "'ascending', 'descending', or "
-                             "'random'.".format(self.imputation_order))
+            raise ValueError(
+                "Got an invalid imputation order: '{0}'. It must "
+                "be one of the following: 'roman', 'arabic', "
+                "'ascending', 'descending', or "
+                "'random'.".format(self.imputation_order)
+            )
         return ordered_idx

     def _get_abs_corr_mat(self, X_filled, tolerance=1e-6):
@@ -414,23 +447,26 @@
         X_filled : ndarray, shape (n_samples, n_features)
             Input data with the most recent imputations.

-        tolerance : float, optional (default=1e-6)
-            ``abs_corr_mat`` can have nans, which will be replaced
-            with ``tolerance``.
+        tolerance : float, default=1e-6
+            `abs_corr_mat` can have nans, which will be replaced
+            with `tolerance`.

         Returns
         -------
         abs_corr_mat : ndarray, shape (n_features, n_features)
-            Absolute correlation matrix of ``X`` at the beginning of the
+            Absolute correlation matrix of `X` at the beginning of the
             current round. The diagonal has been zeroed out and each feature's
             absolute correlations with all others have been normalized to sum
             to 1.
         """
         n_features = X_filled.shape[1]
-        if (self.n_nearest_features is None or
-                self.n_nearest_features >= n_features):
+        if self.n_nearest_features is None or self.n_nearest_features >= n_features:
             return None
-        abs_corr_mat = np.abs(np.corrcoef(X_filled.T))
+        with np.errstate(invalid="ignore"):
+            # if a feature in the neighborhood has only a single value
+            # (e.g., categorical feature), the std. dev. will be null and
+            # np.corrcoef will raise a warning due to a division by zero
+            abs_corr_mat = np.abs(np.corrcoef(X_filled.T))
         # np.corrcoef is not defined for features with zero std
         abs_corr_mat[np.isnan(abs_corr_mat)] = tolerance
         # ensures exploration, i.e. at least some probability of sampling
@@ -438,113 +474,169 @@
         # features are not their own neighbors
         np.fill_diagonal(abs_corr_mat, 0)
         # needs to sum to 1 for np.random.choice sampling
-        abs_corr_mat = normalize(abs_corr_mat, norm='l1', axis=0, copy=False)
+        abs_corr_mat = normalize(abs_corr_mat, norm="l1", axis=0, copy=False)
         return abs_corr_mat

-    def _initial_imputation(self, X):
-        """Perform initial imputation for input X.
+    def _initial_imputation(self, X, in_fit=False):
+        """Perform initial imputation for input `X`.

         Parameters
         ----------
         X : ndarray, shape (n_samples, n_features)
-            Input data, where "n_samples" is the number of samples and
-            "n_features" is the number of features.
+            Input data, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+
+        in_fit : bool, default=False
+            Whether function is called in :meth:`fit`.

         Returns
         -------
         Xt : ndarray, shape (n_samples, n_features)
-            Input data, where "n_samples" is the number of samples and
-            "n_features" is the number of features.
+            Input data, where `n_samples` is the number of samples and
+            `n_features` is the number of features.

         X_filled : ndarray, shape (n_samples, n_features)
             Input data with the most recent imputations.

         mask_missing_values : ndarray, shape (n_samples, n_features)
-            Input data's missing indicator matrix, where "n_samples" is the
-            number of samples and "n_features" is the number of features.
+            Input data's missing indicator matrix, where `n_samples` is the
+            number of samples and `n_features` is the number of features.
+
+        X_missing_mask : ndarray, shape (n_samples, n_features)
+            Input data's mask matrix indicating missing datapoints, where
+            `n_samples` is the number of samples and `n_features` is the
+            number of features.
         """
         if is_scalar_nan(self.missing_values):
             force_all_finite = "allow-nan"
         else:
             force_all_finite = True

-        X = check_array(X, dtype=FLOAT_DTYPES, order="F",
-                        force_all_finite=force_all_finite)
+        X = self._validate_data(
+            X,
+            dtype=FLOAT_DTYPES,
+            order="F",
+            reset=in_fit,
+            force_all_finite=force_all_finite,
+        )
         _check_inputs_dtype(X, self.missing_values)

-        mask_missing_values = _get_mask(X, self.missing_values)
+        X_missing_mask = _get_mask(X, self.missing_values)
+        mask_missing_values = X_missing_mask.copy()
         if self.initial_imputer_ is None:
             self.initial_imputer_ = SimpleImputer(
-                                            missing_values=self.missing_values,
-                                            strategy=self.initial_strategy)
+                missing_values=self.missing_values, strategy=self.initial_strategy
+            )
             X_filled = self.initial_imputer_.fit_transform(X)
         else:
             X_filled = self.initial_imputer_.transform(X)

-        valid_mask = np.flatnonzero(np.logical_not(
-            np.isnan(self.initial_imputer_.statistics_)))
+        valid_mask = np.flatnonzero(
+            np.logical_not(np.isnan(self.initial_imputer_.statistics_))
+        )
         Xt = X[:, valid_mask]
         mask_missing_values = mask_missing_values[:, valid_mask]

-        return Xt, X_filled, mask_missing_values
+        return Xt, X_filled, mask_missing_values, X_missing_mask
+
+    @staticmethod
+    def _validate_limit(limit, limit_type, n_features):
+        """Validate the limits (min/max) of the feature values.
+
+        Converts scalar min/max limits to vectors of shape `(n_features,)`.
+
+        Parameters
+        ----------
+        limit: scalar or array-like
+            The user-specified limit (i.e, min_value or max_value).
+        limit_type: {'max', 'min'}
+            Type of limit to validate.
+        n_features: int
+            Number of features in the dataset.
+
+        Returns
+        -------
+        limit: ndarray, shape(n_features,)
+            Array of limits, one for each feature.
+        """
+        limit_bound = np.inf if limit_type == "max" else -np.inf
+        limit = limit_bound if limit is None else limit
+        if np.isscalar(limit):
+            limit = np.full(n_features, limit)
+        limit = check_array(limit, force_all_finite=False, copy=False, ensure_2d=False)
+        if not limit.shape[0] == n_features:
+            raise ValueError(
+                f"'{limit_type}_value' should be of "
+                f"shape ({n_features},) when an array-like "
+                f"is provided. Got {limit.shape}, instead."
+            )
+        return limit

     def fit_transform(self, X, y=None):
-        """Fits the imputer on X and return the transformed X.
+        """Fit the imputer on `X` and return the transformed `X`.

         Parameters
         ----------
         X : array-like, shape (n_samples, n_features)
-            Input data, where "n_samples" is the number of samples and
-            "n_features" is the number of features.
-
-        y : ignored.
+            Input data, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+
+        y : Ignored
+            Not used, present for API consistency by convention.

         Returns
         -------
         Xt : array-like, shape (n_samples, n_features)
-             The imputed input data.
-        """
-        self.random_state_ = getattr(self, "random_state_",
-                                     check_random_state(self.random_state))
+            The imputed input data.
+        """
+        self.random_state_ = getattr(
+            self, "random_state_", check_random_state(self.random_state)
+        )

         if self.max_iter < 0:
             raise ValueError(
-                "'max_iter' should be a positive integer. Got {} instead."
-                .format(self.max_iter))
+                "'max_iter' should be a positive integer. Got {} instead.".format(
+                    self.max_iter
+                )
+            )

         if self.tol < 0:
             raise ValueError(
-                "'tol' should be a non-negative float. Got {} instead."
-                .format(self.tol)
-            )
-
-        if self.add_indicator:
-            self.indicator_ = MissingIndicator(
-                missing_values=self.missing_values)
-            X_trans_indicator = self.indicator_.fit_transform(X)
-        else:
-            self.indicator_ = None
+                "'tol' should be a non-negative float. Got {} instead.".format(self.tol)
+            )

         if self.estimator is None:
             from ..linear_model import BayesianRidge
+
             self._estimator = BayesianRidge()
         else:
             self._estimator = clone(self.estimator)

         self.imputation_sequence_ = []

-        if hasattr(self._estimator, 'random_state'):
-            self._estimator.random_state = self.random_state_
-
-        self._min_value = -np.inf if self.min_value is None else self.min_value
-        self._max_value = np.inf if self.max_value is None else self.max_value
-
         self.initial_imputer_ = None
-        X, Xt, mask_missing_values = self._initial_imputation(X)
+
+        X, Xt, mask_missing_values, complete_mask = self._initial_imputation(
+            X, in_fit=True
+        )
+
+        super()._fit_indicator(complete_mask)
+        X_indicator = super()._transform_indicator(complete_mask)

         if self.max_iter == 0 or np.all(mask_missing_values):
             self.n_iter_ = 0
-            return Xt
+            return super()._concatenate_indicator(Xt, X_indicator)
+
+        # Edge case: a single feature. We return the initial ...
+        if Xt.shape[1] == 1:
+            self.n_iter_ = 0
+            return super()._concatenate_indicator(Xt, X_indicator)
+
+        self._min_value = self._validate_limit(self.min_value, "min", X.shape[1])
+        self._max_value = self._validate_limit(self.max_value, "max", X.shape[1])
+
+        if not np.all(np.greater(self._max_value, self._min_value)):
+            raise ValueError("One (or more) features have min_value >= max_value.")

         # order in which to impute
         # note this is probably too slow for large feature data (d > 100000)
@@ -557,61 +649,70 @@

         n_samples, n_features = Xt.shape
         if self.verbose > 0:
-            print("[IterativeImputer] Completing matrix with shape %s"
-                  % (X.shape,))
+            print("[IterativeImputer] Completing matrix with shape %s" % (X.shape,))
         start_t = time()
         if not self.sample_posterior:
             Xt_previous = Xt.copy()
             normalized_tol = self.tol * np.max(np.abs(X[~mask_missing_values]))
         for self.n_iter_ in range(1, self.max_iter + 1):
-            if self.imputation_order == 'random':
+            if self.imputation_order == "random":
                 ordered_idx = self._get_ordered_idx(mask_missing_values)

             for feat_idx in ordered_idx:
-                neighbor_feat_idx = self._get_neighbor_feat_idx(n_features,
-                                                                feat_idx,
-                                                                abs_corr_mat)
+                neighbor_feat_idx = self._get_neighbor_feat_idx(
+                    n_features, feat_idx, abs_corr_mat
+                )
                 Xt, estimator = self._impute_one_feature(
-                    Xt, mask_missing_values, feat_idx, neighbor_feat_idx,
-                    estimator=None, fit_mode=True)
-                estimator_triplet = _ImputerTriplet(feat_idx,
-                                                    neighbor_feat_idx,
-                                                    estimator)
+                    Xt,
+                    mask_missing_values,
+                    feat_idx,
+                    neighbor_feat_idx,
+                    estimator=None,
+                    fit_mode=True,
+                )
+                estimator_triplet = _ImputerTriplet(
+                    feat_idx, neighbor_feat_idx, estimator
+                )
                 self.imputation_sequence_.append(estimator_triplet)

             if self.verbose > 1:
-                print('[IterativeImputer] Ending imputation round '
-                      '%d/%d, elapsed time %0.2f'
-                      % (self.n_iter_, self.max_iter, time() - start_t))
+                print(
+                    "[IterativeImputer] Ending imputation round "
+                    "%d/%d, elapsed time %0.2f"
+                    % (self.n_iter_, self.max_iter, time() - start_t)
+                )

             if not self.sample_posterior:
-                inf_norm = np.linalg.norm(Xt - Xt_previous, ord=np.inf,
-                                          axis=None)
+                inf_norm = np.linalg.norm(Xt - Xt_previous, ord=np.inf, axis=None)
+                if self.verbose > 0:
+                    print(
+                        "[IterativeImputer] Change: {}, scaled tolerance: {} ".format(
+                            inf_norm, normalized_tol
+                        )
+                    )
                 if inf_norm < normalized_tol:
                     if self.verbose > 0:
-                        print('[IterativeImputer] Early stopping criterion '
-                              'reached.')
+                        print("[IterativeImputer] Early stopping criterion reached.")
                     break
                 Xt_previous = Xt.copy()
         else:
             if not self.sample_posterior:
-                warnings.warn("[IterativeImputer] Early stopping criterion not"
-                              " reached.", ConvergenceWarning)
+                warnings.warn(
+                    "[IterativeImputer] Early stopping criterion not reached.",
+                    ConvergenceWarning,
+                )
         Xt[~mask_missing_values] = X[~mask_missing_values]
-
-        if self.add_indicator:
-            Xt = np.hstack((Xt, X_trans_indicator))
-        return Xt
+        return super()._concatenate_indicator(Xt, X_indicator)

     def transform(self, X):
-        """Imputes all missing values in X.
-
-        Note that this is stochastic, and that if random_state is not fixed,
-        repeated calls, or permuted input, will yield different results.
-
-        Parameters
-        ----------
-        X : array-like, shape = [n_samples, n_features]
+        """Impute all missing values in `X`.
+
+        Note that this is stochastic, and that if `random_state` is not fixed,
+        repeated calls, or permuted input, results will differ.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
             The input data to complete.

         Returns
@@ -619,21 +720,19 @@
         Xt : array-like, shape (n_samples, n_features)
              The imputed input data.
         """
-        check_is_fitted(self, 'initial_imputer_')
-
-        if self.add_indicator:
-            X_trans_indicator = self.indicator_.transform(X)
-
-        X, Xt, mask_missing_values = self._initial_imputation(X)
+        check_is_fitted(self)
+
+        X, Xt, mask_missing_values, complete_mask = self._initial_imputation(X)
+
+        X_indicator = super()._transform_indicator(complete_mask)

         if self.n_iter_ == 0 or np.all(mask_missing_values):
-            return Xt
+            return super()._concatenate_indicator(Xt, X_indicator)

         imputations_per_round = len(self.imputation_sequence_) // self.n_iter_
         i_rnd = 0
         if self.verbose > 0:
-            print("[IterativeImputer] Completing matrix with shape %s"
-                  % (X.shape,))
+            print("[IterativeImputer] Completing matrix with shape %s" % (X.shape,))
         start_t = time()
         for it, estimator_triplet in enumerate(self.imputation_sequence_):
             Xt, _ = self._impute_one_feature(
@@ -642,39 +741,61 @@
                 estimator_triplet.feat_idx,
                 estimator_triplet.neighbor_feat_idx,
                 estimator=estimator_triplet.estimator,
-                fit_mode=False
+                fit_mode=False,
             )
             if not (it + 1) % imputations_per_round:
                 if self.verbose > 1:
-                    print('[IterativeImputer] Ending imputation round '
-                          '%d/%d, elapsed time %0.2f'
-                          % (i_rnd + 1, self.n_iter_, time() - start_t))
+                    print(
+                        "[IterativeImputer] Ending imputation round "
+                        "%d/%d, elapsed time %0.2f"
+                        % (i_rnd + 1, self.n_iter_, time() - start_t)
+                    )
                 i_rnd += 1

         Xt[~mask_missing_values] = X[~mask_missing_values]

-        if self.add_indicator:
-            Xt = np.hstack((Xt, X_trans_indicator))
-        return Xt
+        return super()._concatenate_indicator(Xt, X_indicator)

     def fit(self, X, y=None):
-        """Fits the imputer on X and return self.
+        """Fit the imputer on `X` and return self.

         Parameters
         ----------
         X : array-like, shape (n_samples, n_features)
-            Input data, where "n_samples" is the number of samples and
-            "n_features" is the number of features.
-
-        y : ignored
+            Input data, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+
+        y : Ignored
+            Not used, present for API consistency by convention.

         Returns
         -------
         self : object
-            Returns self.
+            Fitted estimator.
         """
         self.fit_transform(X)
         return self

-    def _more_tags(self):
-        return {'allow_nan': True}
+    def get_feature_names_out(self, input_features=None):
+        """Get output feature names for transformation.
+
+        Parameters
+        ----------
+        input_features : array-like of str or None, default=None
+            Input features.
+
+            - If `input_features` is `None`, then `feature_names_in_` is
+              used as feature names in. If `feature_names_in_` is not defined,
+              then the following input feature names are generated:
+              `["x0", "x1", ..., "x(n_features_in_ - 1)"]`.
+            - If `input_features` is an array-like, then `input_features` must
+              match `feature_names_in_` if `feature_names_in_` is defined.
+
+        Returns
+        -------
+        feature_names_out : ndarray of str objects
+            Transformed feature names.
+        """
+        input_features = _check_feature_names_in(self, input_features)
+        names = self.initial_imputer_.get_feature_names_out(input_features)
+        return self._concatenate_indicator_feature_names_out(names, input_features)
('sklearn/utils', 'optimize.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -15,8 +15,8 @@

 import numpy as np
 import warnings
-from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1
-
+
+from .fixes import line_search_wolfe1, line_search_wolfe2
 from ..exceptions import ConvergenceWarning


@@ -24,8 +24,7 @@
     pass


-def _line_search_wolfe12(f, fprime, xk, pk, gfk, old_fval, old_old_fval,
-                         **kwargs):
+def _line_search_wolfe12(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs):
     """
     Same as line_search_wolfe1, but fall back to line_search_wolfe2 if
     suitable step length is not found, and raise an exception if a
@@ -34,17 +33,16 @@
     Raises
     ------
     _LineSearchError
-        If no suitable step size is found
-
-    """
-    ret = line_search_wolfe1(f, fprime, xk, pk, gfk,
-                             old_fval, old_old_fval,
-                             **kwargs)
+        If no suitable step size is found.
+
+    """
+    ret = line_search_wolfe1(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)

     if ret[0] is None:
         # line search failed: try different one.
-        ret = line_search_wolfe2(f, fprime, xk, pk, gfk,
-                                 old_fval, old_old_fval, **kwargs)
+        ret = line_search_wolfe2(
+            f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs
+        )

     if ret[0] is None:
         raise _LineSearchError()
@@ -61,10 +59,10 @@
     ----------
     fhess_p : callable
         Function that takes the gradient as a parameter and returns the
-        matrix product of the Hessian and gradient
-
-    fgrad : ndarray, shape (n_features,) or (n_features + 1,)
-        Gradient vector
+        matrix product of the Hessian and gradient.
+
+    fgrad : ndarray of shape (n_features,) or (n_features + 1,)
+        Gradient vector.

     maxiter : int
         Number of CG iterations.
@@ -74,8 +72,8 @@

     Returns
     -------
-    xsupi : ndarray, shape (n_features,) or (n_features + 1,)
-        Estimated solution
+    xsupi : ndarray of shape (n_features,) or (n_features + 1,)
+        Estimated solution.
     """
     xsupi = np.zeros(len(fgrad), dtype=fgrad.dtype)
     ri = fgrad
@@ -106,13 +104,23 @@
         betai = dri1 / dri0
         psupi = -ri + betai * psupi
         i = i + 1
-        dri0 = dri1          # update np.dot(ri,ri) for next time.
+        dri0 = dri1  # update np.dot(ri,ri) for next time.

     return xsupi


-def newton_cg(grad_hess, func, grad, x0, args=(), tol=1e-4,
-              maxiter=100, maxinner=200, line_search=True, warn=True):
+def _newton_cg(
+    grad_hess,
+    func,
+    grad,
+    x0,
+    args=(),
+    tol=1e-4,
+    maxiter=100,
+    maxinner=200,
+    line_search=True,
+    warn=True,
+):
     """
     Minimization of scalar function of one or more variables using the
     Newton-CG algorithm.
@@ -133,24 +141,24 @@
     x0 : array of float
         Initial guess.

-    args : tuple, optional
+    args : tuple, default=()
         Arguments passed to func_grad_hess, func and grad.

-    tol : float
+    tol : float, default=1e-4
         Stopping criterion. The iteration will stop when
         ``max{|g_i | i = 1, ..., n} <= tol``
         where ``g_i`` is the i-th component of the gradient.

-    maxiter : int
+    maxiter : int, default=100
         Number of Newton iterations.

-    maxinner : int
+    maxinner : int, default=200
         Number of CG iterations.

-    line_search : boolean
+    line_search : bool, default=True
         Whether to use a line search or not.

-    warn : boolean
+    warn : bool, default=True
         Whether to warn when didn't converge.

     Returns
@@ -173,7 +181,7 @@
         fgrad, fhess_p = grad_hess(xk, *args)

         absgrad = np.abs(fgrad)
-        if np.max(absgrad) < tol:
+        if np.max(absgrad) <= tol:
             break

         maggrad = np.sum(absgrad)
@@ -188,17 +196,71 @@

         if line_search:
             try:
-                alphak, fc, gc, old_fval, old_old_fval, gfkp1 = \
-                    _line_search_wolfe12(func, grad, xk, xsupi, fgrad,
-                                         old_fval, old_old_fval, args=args)
+                alphak, fc, gc, old_fval, old_old_fval, gfkp1 = _line_search_wolfe12(
+                    func, grad, xk, xsupi, fgrad, old_fval, old_old_fval, args=args
+                )
             except _LineSearchError:
-                warnings.warn('Line Search failed')
+                warnings.warn("Line Search failed")
                 break

-        xk = xk + alphak * xsupi        # upcast if necessary
+        xk = xk + alphak * xsupi  # upcast if necessary
         k += 1

     if warn and k >= maxiter:
-        warnings.warn("newton-cg failed to converge. Increase the "
-                      "number of iterations.", ConvergenceWarning)
+        warnings.warn(
+            "newton-cg failed to converge. Increase the number of iterations.",
+            ConvergenceWarning,
+        )
     return xk, k
+
+
+def _check_optimize_result(solver, result, max_iter=None, extra_warning_msg=None):
+    """Check the OptimizeResult for successful convergence
+
+    Parameters
+    ----------
+    solver : str
+       Solver name. Currently only `lbfgs` is supported.
+
+    result : OptimizeResult
+       Result of the scipy.optimize.minimize function.
+
+    max_iter : int, default=None
+       Expected maximum number of iterations.
+
+    extra_warning_msg : str, default=None
+        Extra warning message.
+
+    Returns
+    -------
+    n_iter : int
+       Number of iterations.
+    """
+    # handle both scipy and scikit-learn solver names
+    if solver == "lbfgs":
+        if result.status != 0:
+            try:
+                # The message is already decoded in scipy>=1.6.0
+                result_message = result.message.decode("latin1")
+            except AttributeError:
+                result_message = result.message
+            warning_msg = (
+                "{} failed to converge (status={}):\n{}.\n\n"
+                "Increase the number of iterations (max_iter) "
+                "or scale the data as shown in:\n"
+                "    https://scikit-learn.org/stable/modules/"
+                "preprocessing.html"
+            ).format(solver, result.status, result_message)
+            if extra_warning_msg is not None:
+                warning_msg += "\n" + extra_warning_msg
+            warnings.warn(warning_msg, ConvergenceWarning, stacklevel=2)
+        if max_iter is not None:
+            # In scipy <= 1.0.0, nit may exceed maxiter for lbfgs.
+            # See https://github.com/scipy/scipy/issues/7854
+            n_iter_i = min(result.nit, max_iter)
+        else:
+            n_iter_i = result.nit
+    else:
+        raise NotImplementedError
+
+    return n_iter_i
('sklearn/utils', 'fixes.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,7 +1,7 @@
 """Compatibility fixes for older version of python, numpy and scipy

 If you add content to this file, please give the version of the package
-at which the fixe is no longer needed.
+at which the fix is no longer needed.
 """
 # Authors: Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>
 #          Gael Varoquaux <gael.varoquaux@normalesup.org>
@@ -10,236 +10,156 @@
 #
 # License: BSD 3 clause

-from distutils.version import LooseVersion
+from functools import update_wrapper
+import functools

+import sklearn
 import numpy as np
-import scipy.sparse as sp
 import scipy
-from scipy.sparse.linalg import lsqr as sparse_lsqr  # noqa
+import scipy.stats
+import threadpoolctl
+from .._config import config_context, get_config
+from ..externals._packaging.version import parse as parse_version


-def _parse_version(version_string):
-    version = []
-    for x in version_string.split('.'):
-        try:
-            version.append(int(x))
-        except ValueError:
-            # x may be of the form dev-1ea1592
-            version.append(x)
-    return tuple(version)
+np_version = parse_version(np.__version__)
+sp_version = parse_version(scipy.__version__)


-np_version = _parse_version(np.__version__)
-sp_version = _parse_version(scipy.__version__)
+if sp_version >= parse_version("1.4"):
+    from scipy.sparse.linalg import lobpcg
+else:
+    # Backport of lobpcg functionality from scipy 1.4.0, can be removed
+    # once support for sp_version < parse_version('1.4') is dropped
+    # mypy error: Name 'lobpcg' already defined (possibly by an import)
+    from ..externals._lobpcg import lobpcg  # type: ignore  # noqa
+
+try:
+    from scipy.optimize._linesearch import line_search_wolfe2, line_search_wolfe1
+except ImportError:  # SciPy < 1.8
+    from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1  # type: ignore  # noqa


-try:  # SciPy >= 0.19
-    from scipy.special import comb, logsumexp
-except ImportError:
-    from scipy.misc import comb, logsumexp  # noqa
+def _object_dtype_isnan(X):
+    return X != X


-if sp_version >= (0, 19):
-    def _argmax(arr_or_spmatrix, axis=None):
-        return arr_or_spmatrix.argmax(axis=axis)
-else:
-    # Backport of argmax functionality from scipy 0.19.1, can be removed
-    # once support for scipy 0.18 and below is dropped
-
-    def _find_missing_index(ind, n):
-        for k, a in enumerate(ind):
-            if k != a:
-                return k
-
-        k += 1
-        if k < n:
-            return k
-        else:
-            return -1
-
-    def _arg_min_or_max_axis(self, axis, op, compare):
-        if self.shape[axis] == 0:
-            raise ValueError("Can't apply the operation along a zero-sized "
-                             "dimension.")
-
-        if axis < 0:
-            axis += 2
-
-        zero = self.dtype.type(0)
-
-        mat = self.tocsc() if axis == 0 else self.tocsr()
-        mat.sum_duplicates()
-
-        ret_size, line_size = mat._swap(mat.shape)
-        ret = np.zeros(ret_size, dtype=int)
-
-        nz_lines, = np.nonzero(np.diff(mat.indptr))
-        for i in nz_lines:
-            p, q = mat.indptr[i:i + 2]
-            data = mat.data[p:q]
-            indices = mat.indices[p:q]
-            am = op(data)
-            m = data[am]
-            if compare(m, zero) or q - p == line_size:
-                ret[i] = indices[am]
-            else:
-                zero_ind = _find_missing_index(indices, line_size)
-                if m == zero:
-                    ret[i] = min(am, zero_ind)
-                else:
-                    ret[i] = zero_ind
-
-        if axis == 1:
-            ret = ret.reshape(-1, 1)
-
-        return np.asmatrix(ret)
-
-    def _arg_min_or_max(self, axis, out, op, compare):
-        if out is not None:
-            raise ValueError("Sparse matrices do not support "
-                             "an 'out' parameter.")
-
-        # validateaxis(axis)
-
-        if axis is None:
-            if 0 in self.shape:
-                raise ValueError("Can't apply the operation to "
-                                 "an empty matrix.")
-
-            if self.nnz == 0:
-                return 0
-            else:
-                zero = self.dtype.type(0)
-                mat = self.tocoo()
-                mat.sum_duplicates()
-                am = op(mat.data)
-                m = mat.data[am]
-
-                if compare(m, zero):
-                    return mat.row[am] * mat.shape[1] + mat.col[am]
-                else:
-                    size = np.product(mat.shape)
-                    if size == mat.nnz:
-                        return am
-                    else:
-                        ind = mat.row * mat.shape[1] + mat.col
-                        zero_ind = _find_missing_index(ind, size)
-                        if m == zero:
-                            return min(zero_ind, am)
-                        else:
-                            return zero_ind
-
-        return _arg_min_or_max_axis(self, axis, op, compare)
-
-    def _sparse_argmax(self, axis=None, out=None):
-        return _arg_min_or_max(self, axis, out, np.argmax, np.greater)
-
-    def _argmax(arr_or_matrix, axis=None):
-        if sp.issparse(arr_or_matrix):
-            return _sparse_argmax(arr_or_matrix, axis=axis)
-        else:
-            return arr_or_matrix.argmax(axis=axis)
-
-
-def parallel_helper(obj, methodname, *args, **kwargs):
-    """Workaround for Python 2 limitations of pickling instance methods
+class loguniform(scipy.stats.reciprocal):
+    """A class supporting log-uniform random variables.

     Parameters
     ----------
-    obj
-    methodname
-    *args
-    **kwargs
+    low : float
+        The minimum value
+    high : float
+        The maximum value

+    Methods
+    -------
+    rvs(self, size=None, random_state=None)
+        Generate log-uniform random variables
+
+    The most useful method for Scikit-learn usage is highlighted here.
+    For a full list, see
+    `scipy.stats.reciprocal
+    <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.reciprocal.html>`_.
+    This list includes all functions of ``scipy.stats`` continuous
+    distributions such as ``pdf``.
+
+    Notes
+    -----
+    This class generates values between ``low`` and ``high`` or
+
+        low <= loguniform(low, high).rvs() <= high
+
+    The logarithmic probability density function (PDF) is uniform. When
+    ``x`` is a uniformly distributed random variable between 0 and 1, ``10**x``
+    are random variables that are equally likely to be returned.
+
+    This class is an alias to ``scipy.stats.reciprocal``, which uses the
+    reciprocal distribution:
+    https://en.wikipedia.org/wiki/Reciprocal_distribution
+
+    Examples
+    --------
+
+    >>> from sklearn.utils.fixes import loguniform
+    >>> rv = loguniform(1e-3, 1e1)
+    >>> rvs = rv.rvs(random_state=42, size=1000)
+    >>> rvs.min()  # doctest: +SKIP
+    0.0010435856341129003
+    >>> rvs.max()  # doctest: +SKIP
+    9.97403052786026
     """
-    return getattr(obj, methodname)(*args, **kwargs)


-if np_version < (1, 12):
-    class MaskedArray(np.ma.MaskedArray):
-        # Before numpy 1.12, np.ma.MaskedArray object is not picklable
-        # This fix is needed to make our model_selection.GridSearchCV
-        # picklable as the ``cv_results_`` param uses MaskedArray
-        def __getstate__(self):
-            """Return the internal state of the masked array, for pickling
-            purposes.
+# remove when https://github.com/joblib/joblib/issues/1071 is fixed
+def delayed(function):
+    """Decorator used to capture the arguments of a function."""

-            """
-            cf = 'CF'[self.flags.fnc]
-            data_state = super(np.ma.MaskedArray, self).__reduce__()[2]
-            return data_state + (np.ma.getmaskarray(self).tostring(cf),
-                                 self._fill_value)
-else:
-    from numpy.ma import MaskedArray    # noqa
+    @functools.wraps(function)
+    def delayed_function(*args, **kwargs):
+        return _FuncWrapper(function), args, kwargs
+
+    return delayed_function


-# Fix for behavior inconsistency on numpy.equal for object dtypes.
-# For numpy versions < 1.13, numpy.equal tests element-wise identity of objects
-# instead of equality. This fix returns the mask of NaNs in an array of
-# numerical or object values for all numpy versions.
-if np_version < (1, 13):
-    def _object_dtype_isnan(X):
-        return np.frompyfunc(lambda x: x != x, 1, 1)(X).astype(bool)
-else:
-    def _object_dtype_isnan(X):
-        return X != X
+class _FuncWrapper:
+    """ "Load the global configuration before calling the function."""
+
+    def __init__(self, function):
+        self.function = function
+        self.config = get_config()
+        update_wrapper(self, self.function)
+
+    def __call__(self, *args, **kwargs):
+        with config_context(**self.config):
+            return self.function(*args, **kwargs)


-# TODO: replace by copy=False, when only scipy > 1.1 is supported.
-def _astype_copy_false(X):
-    """Returns the copy=False parameter for
-    {ndarray, csr_matrix, csc_matrix}.astype when possible,
-    otherwise don't specify
-    """
-    if sp_version >= (1, 1) or not sp.issparse(X):
-        return {'copy': False}
-    else:
-        return {}
+# Rename the `method` kwarg to `interpolation` for NumPy < 1.22, because
+# `interpolation` kwarg was deprecated in favor of `method` in NumPy >= 1.22.
+def _percentile(a, q, *, method="linear", **kwargs):
+    return np.percentile(a, q, interpolation=method, **kwargs)


-def _joblib_parallel_args(**kwargs):
-    """Set joblib.Parallel arguments in a compatible way for 0.11 and 0.12+
+if np_version < parse_version("1.22"):
+    percentile = _percentile
+else:  # >= 1.22
+    from numpy import percentile  # type: ignore  # noqa

-    For joblib 0.11 this maps both ``prefer`` and ``require`` parameters to
-    a specific ``backend``.

-    Parameters
-    ----------
+# compatibility fix for threadpoolctl >= 3.0.0
+# since version 3 it's possible to setup a global threadpool controller to avoid
+# looping through all loaded shared libraries each time.
+# the global controller is created during the first call to threadpoolctl.
+def _get_threadpool_controller():
+    if not hasattr(threadpoolctl, "ThreadpoolController"):
+        return None

-    prefer : str in {'processes', 'threads'} or None
-        Soft hint to choose the default backend if no specific backend
-        was selected with the parallel_backend context manager.
+    if not hasattr(sklearn, "_sklearn_threadpool_controller"):
+        sklearn._sklearn_threadpool_controller = threadpoolctl.ThreadpoolController()

-    require : 'sharedmem' or None
-        Hard condstraint to select the backend. If set to 'sharedmem',
-        the selected backend will be single-host and thread-based even
-        if the user asked for a non-thread based backend with
-        parallel_backend.
+    return sklearn._sklearn_threadpool_controller

-    See joblib.Parallel documentation for more details
-    """
-    from . import _joblib

-    if _joblib.__version__ >= LooseVersion('0.12'):
-        return kwargs
+def threadpool_limits(limits=None, user_api=None):
+    controller = _get_threadpool_controller()
+    if controller is not None:
+        return controller.limit(limits=limits, user_api=user_api)
+    else:
+        return threadpoolctl.threadpool_limits(limits=limits, user_api=user_api)

-    extra_args = set(kwargs.keys()).difference({'prefer', 'require'})
-    if extra_args:
-        raise NotImplementedError('unhandled arguments %s with joblib %s'
-                                  % (list(extra_args), _joblib.__version__))
-    args = {}
-    if 'prefer' in kwargs:
-        prefer = kwargs['prefer']
-        if prefer not in ['threads', 'processes', None]:
-            raise ValueError('prefer=%s is not supported' % prefer)
-        args['backend'] = {'threads': 'threading',
-                           'processes': 'multiprocessing',
-                           None: None}[prefer]

-    if 'require' in kwargs:
-        require = kwargs['require']
-        if require not in [None, 'sharedmem']:
-            raise ValueError('require=%s is not supported' % require)
-        if require == 'sharedmem':
-            args['backend'] = 'threading'
-    return args
+threadpool_limits.__doc__ = threadpoolctl.threadpool_limits.__doc__
+
+
+def threadpool_info():
+    controller = _get_threadpool_controller()
+    if controller is not None:
+        return controller.info()
+    else:
+        return threadpoolctl.threadpool_info()
+
+
+threadpool_info.__doc__ = threadpoolctl.threadpool_info.__doc__
('sklearn/utils', 'murmurhash.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,4 +1,4 @@
-"""Cython wrapper for MurmurHash3 non-cryptographic hash function
+"""Cython wrapper for MurmurHash3 non-cryptographic hash function.

 MurmurHash is an extensively tested and very fast hash function that has
 good distribution properties suitable for machine learning use cases
@@ -55,7 +55,6 @@
     return out


-@cython.boundscheck(False)
 cpdef np.ndarray[np.uint32_t, ndim=1] murmurhash3_bytes_array_u32(
     np.ndarray[np.int32_t] key, unsigned int seed):
     """Compute 32bit murmurhash3 hashes of a key int array at seed."""
@@ -67,7 +66,6 @@
     return out


-@cython.boundscheck(False)
 cpdef np.ndarray[np.int32_t, ndim=1] murmurhash3_bytes_array_s32(
     np.ndarray[np.int32_t] key, unsigned int seed):
     """Compute 32bit murmurhash3 hashes of a key int array at seed."""
@@ -88,13 +86,13 @@

     Parameters
     ----------
-    key : int32, bytes, unicode or ndarray with dtype int32
-        the physical object to hash
+    key : np.int32, bytes, unicode or ndarray of dtype=np.int32
+        The physical object to hash.

-    seed : int, optional default is 0
-        integer seed for the hashing algorithm.
+    seed : int, default=0
+        Integer seed for the hashing algorithm.

-    positive : boolean, optional default is False
+    positive : bool, default=False
         True: the results is casted to an unsigned int
           from 0 to 2 ** 32 - 1
         False: the results is casted to a signed int
('sklearn/utils', 'sparsefuncs_fast.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,12 +7,10 @@
 # License: BSD 3 clause

 #!python
-# cython: boundscheck=False, wraparound=False, cdivision=True

 from libc.math cimport fabs, sqrt, pow
 cimport numpy as np
 import numpy as np
-import scipy.sparse as sp
 cimport cython
 from cython cimport floating
 from numpy.math cimport isnan
@@ -24,6 +22,7 @@
     long long

 ctypedef np.float64_t DOUBLE
+

 def csr_row_norms(X):
     """L2 norm of each row in CSR matrix X."""
@@ -38,31 +37,42 @@
                    np.ndarray[integral, ndim=1, mode="c"] X_indptr):
     cdef:
         unsigned long long n_samples = shape[0]
-        unsigned long long n_features = shape[1]
-        np.ndarray[DOUBLE, ndim=1, mode="c"] norms
-
-        np.npy_intp i, j
+        unsigned long long i
+        integral j
         double sum_

-    norms = np.zeros(n_samples, dtype=np.float64)
+    norms = np.empty(n_samples, dtype=X_data.dtype)
+    cdef floating[::1] norms_view = norms

     for i in range(n_samples):
         sum_ = 0.0
         for j in range(X_indptr[i], X_indptr[i + 1]):
             sum_ += X_data[j] * X_data[j]
-        norms[i] = sum_
+        norms_view[i] = sum_

     return norms


-def csr_mean_variance_axis0(X):
+def csr_mean_variance_axis0(X, weights=None, return_sum_weights=False):
     """Compute mean and variance along axis 0 on a CSR matrix
+
+    Uses a np.float64 accumulator.

     Parameters
     ----------
     X : CSR sparse matrix, shape (n_samples, n_features)
         Input data.

+    weights : ndarray of shape (n_samples,), dtype=floating, default=None
+        If it is set to None samples will be equally weighted.
+
+        .. versionadded:: 0.24
+
+    return_sum_weights : bool, default=False
+        If True, returns the sum of weights seen for each feature.
+
+        .. versionadded:: 0.24
+
     Returns
     -------
     means : float array with shape (n_features,)
@@ -71,78 +81,120 @@
     variances : float array with shape (n_features,)
         Feature-wise variances

+    sum_weights : ndarray of shape (n_features,), dtype=floating
+        Returned if return_sum_weights is True.
     """
     if X.dtype not in [np.float32, np.float64]:
         X = X.astype(np.float64)
-    means, variances, _ =  _csr_mean_variance_axis0(X.data, X.shape[0],
-                                                    X.shape[1], X.indices)
+
+    if weights is None:
+        weights = np.ones(X.shape[0], dtype=X.dtype)
+
+    means, variances, sum_weights = _csr_mean_variance_axis0(
+        X.data, X.shape[0], X.shape[1], X.indices, X.indptr, weights)
+
+    if return_sum_weights:
+        return means, variances, sum_weights
     return means, variances


 def _csr_mean_variance_axis0(np.ndarray[floating, ndim=1, mode="c"] X_data,
                              unsigned long long n_samples,
                              unsigned long long n_features,
-                             np.ndarray[integral, ndim=1] X_indices):
+                             np.ndarray[integral, ndim=1] X_indices,
+                             np.ndarray[integral, ndim=1] X_indptr,
+                             np.ndarray[floating, ndim=1] weights):
     # Implement the function here since variables using fused types
     # cannot be declared directly and can only be passed as function arguments
     cdef:
         np.npy_intp i
-        unsigned long long non_zero = X_indices.shape[0]
-        np.npy_intp col_ind
-        floating diff
+        unsigned long long row_ind
+        integral col_ind
+        np.float64_t diff
         # means[j] contains the mean of feature j
-        np.ndarray[floating, ndim=1] means
+        np.ndarray[np.float64_t, ndim=1] means = np.zeros(n_features)
         # variances[j] contains the variance of feature j
-        np.ndarray[floating, ndim=1] variances
+        np.ndarray[np.float64_t, ndim=1] variances = np.zeros(n_features)
+
+        np.ndarray[np.float64_t, ndim=1] sum_weights = np.full(
+            fill_value=np.sum(weights, dtype=np.float64), shape=n_features)
+        np.ndarray[np.float64_t, ndim=1] sum_weights_nz = np.zeros(
+            shape=n_features)
+        np.ndarray[np.float64_t, ndim=1] correction = np.zeros(
+            shape=n_features)
+
+        np.ndarray[np.uint64_t, ndim=1] counts = np.full(
+            fill_value=weights.shape[0], shape=n_features, dtype=np.uint64)
+        np.ndarray[np.uint64_t, ndim=1] counts_nz = np.zeros(
+            shape=n_features, dtype=np.uint64)
+
+    for row_ind in range(len(X_indptr) - 1):
+        for i in range(X_indptr[row_ind], X_indptr[row_ind + 1]):
+            col_ind = X_indices[i]
+            if not isnan(X_data[i]):
+                means[col_ind] += <np.float64_t>(X_data[i]) * weights[row_ind]
+                # sum of weights where X[:, col_ind] is non-zero
+                sum_weights_nz[col_ind] += weights[row_ind]
+                # number of non-zero elements of X[:, col_ind]
+                counts_nz[col_ind] += 1
+            else:
+                # sum of weights where X[:, col_ind] is not nan
+                sum_weights[col_ind] -= weights[row_ind]
+                # number of non nan elements of X[:, col_ind]
+                counts[col_ind] -= 1
+
+    for i in range(n_features):
+        means[i] /= sum_weights[i]
+
+    for row_ind in range(len(X_indptr) - 1):
+        for i in range(X_indptr[row_ind], X_indptr[row_ind + 1]):
+            col_ind = X_indices[i]
+            if not isnan(X_data[i]):
+                diff = X_data[i] - means[col_ind]
+                # correction term of the corrected 2 pass algorithm.
+                # See "Algorithms for computing the sample variance: analysis
+                # and recommendations", by Chan, Golub, and LeVeque.
+                correction[col_ind] += diff * weights[row_ind]
+                variances[col_ind] += diff * diff * weights[row_ind]
+
+    for i in range(n_features):
+        if counts[i] != counts_nz[i]:
+            correction[i] -= (sum_weights[i] - sum_weights_nz[i]) * means[i]
+        correction[i] = correction[i]**2 / sum_weights[i]
+        if counts[i] != counts_nz[i]:
+            # only compute it when it's guaranteed to be non-zero to avoid
+            # catastrophic cancellation.
+            variances[i] += (sum_weights[i] - sum_weights_nz[i]) * means[i]**2
+        variances[i] = (variances[i] - correction[i]) / sum_weights[i]

     if floating is float:
-        dtype = np.float32
+        return (np.array(means, dtype=np.float32),
+                np.array(variances, dtype=np.float32),
+                np.array(sum_weights, dtype=np.float32))
     else:
-        dtype = np.float64
-
-    means = np.zeros(n_features, dtype=dtype)
-    variances = np.zeros_like(means, dtype=dtype)
-
-    cdef:
-        # counts[j] contains the number of samples where feature j is non-zero
-        np.ndarray[np.int64_t, ndim=1] counts = np.zeros(n_features,
-                                                         dtype=np.int64)
-        # counts_nan[j] contains the number of NaNs for feature j
-        np.ndarray[np.int64_t, ndim=1] counts_nan = np.zeros(n_features,
-                                                             dtype=np.int64)
-
-    for i in range(non_zero):
-        col_ind = X_indices[i]
-        if not isnan(X_data[i]):
-            means[col_ind] += X_data[i]
-        else:
-            counts_nan[col_ind] += 1
-
-    for i in range(n_features):
-        means[i] /= (n_samples - counts_nan[i])
-
-    for i in range(non_zero):
-        col_ind = X_indices[i]
-        if not isnan(X_data[i]):
-            diff = X_data[i] - means[col_ind]
-            variances[col_ind] += diff * diff
-            counts[col_ind] += 1
-
-    for i in range(n_features):
-        variances[i] += (n_samples - counts_nan[i] - counts[i]) * means[i]**2
-        variances[i] /= (n_samples - counts_nan[i])
-
-    return means, variances, counts_nan
-
-
-def csc_mean_variance_axis0(X):
+        return means, variances, sum_weights
+
+
+def csc_mean_variance_axis0(X, weights=None, return_sum_weights=False):
     """Compute mean and variance along axis 0 on a CSC matrix
+
+    Uses a np.float64 accumulator.

     Parameters
     ----------
     X : CSC sparse matrix, shape (n_samples, n_features)
         Input data.

+    weights : ndarray of shape (n_samples,), dtype=floating, default=None
+        If it is set to None samples will be equally weighted.
+
+        .. versionadded:: 0.24
+
+    return_sum_weights : bool, default=False
+        If True, returns the sum of weights seen for each feature.
+
+        .. versionadded:: 0.24
+
     Returns
     -------
     means : float array with shape (n_features,)
@@ -151,70 +203,101 @@
     variances : float array with shape (n_features,)
         Feature-wise variances

+    sum_weights : ndarray of shape (n_features,), dtype=floating
+        Returned if return_sum_weights is True.
     """
     if X.dtype not in [np.float32, np.float64]:
         X = X.astype(np.float64)
-    means, variances, _ = _csc_mean_variance_axis0(X.data, X.shape[0],
-                                                   X.shape[1], X.indices,
-                                                  X.indptr)
+
+    if weights is None:
+        weights = np.ones(X.shape[0], dtype=X.dtype)
+
+    means, variances, sum_weights = _csc_mean_variance_axis0(
+        X.data, X.shape[0], X.shape[1], X.indices, X.indptr, weights)
+
+    if return_sum_weights:
+        return means, variances, sum_weights
     return means, variances


-def _csc_mean_variance_axis0(np.ndarray[floating, ndim=1] X_data,
+def _csc_mean_variance_axis0(np.ndarray[floating, ndim=1, mode="c"] X_data,
                              unsigned long long n_samples,
                              unsigned long long n_features,
                              np.ndarray[integral, ndim=1] X_indices,
-                             np.ndarray[integral, ndim=1] X_indptr):
+                             np.ndarray[integral, ndim=1] X_indptr,
+                             np.ndarray[floating, ndim=1] weights):
     # Implement the function here since variables using fused types
     # cannot be declared directly and can only be passed as function arguments
     cdef:
-        np.npy_intp i, j
-        unsigned long long counts
-        unsigned long long startptr
-        unsigned long long endptr
-        floating diff
+        np.npy_intp i
+        unsigned long long col_ind
+        integral row_ind
+        np.float64_t diff
         # means[j] contains the mean of feature j
-        np.ndarray[floating, ndim=1] means
+        np.ndarray[np.float64_t, ndim=1] means = np.zeros(n_features)
         # variances[j] contains the variance of feature j
-        np.ndarray[floating, ndim=1] variances
+        np.ndarray[np.float64_t, ndim=1] variances = np.zeros(n_features)
+
+        np.ndarray[np.float64_t, ndim=1] sum_weights = np.full(
+            fill_value=np.sum(weights, dtype=np.float64), shape=n_features)
+        np.ndarray[np.float64_t, ndim=1] sum_weights_nz = np.zeros(
+            shape=n_features)
+        np.ndarray[np.float64_t, ndim=1] correction = np.zeros(
+            shape=n_features)
+
+        np.ndarray[np.uint64_t, ndim=1] counts = np.full(
+            fill_value=weights.shape[0], shape=n_features, dtype=np.uint64)
+        np.ndarray[np.uint64_t, ndim=1] counts_nz = np.zeros(
+            shape=n_features, dtype=np.uint64)
+
+    for col_ind in range(n_features):
+        for i in range(X_indptr[col_ind], X_indptr[col_ind + 1]):
+            row_ind = X_indices[i]
+            if not isnan(X_data[i]):
+                means[col_ind] += <np.float64_t>(X_data[i]) * weights[row_ind]
+                # sum of weights where X[:, col_ind] is non-zero
+                sum_weights_nz[col_ind] += weights[row_ind]
+                # number of non-zero elements of X[:, col_ind]
+                counts_nz[col_ind] += 1
+            else:
+                # sum of weights where X[:, col_ind] is not nan
+                sum_weights[col_ind] -= weights[row_ind]
+                # number of non nan elements of X[:, col_ind]
+                counts[col_ind] -= 1
+
+    for i in range(n_features):
+        means[i] /= sum_weights[i]
+
+    for col_ind in range(n_features):
+        for i in range(X_indptr[col_ind], X_indptr[col_ind + 1]):
+            row_ind = X_indices[i]
+            if not isnan(X_data[i]):
+                diff = X_data[i] - means[col_ind]
+                # correction term of the corrected 2 pass algorithm.
+                # See "Algorithms for computing the sample variance: analysis
+                # and recommendations", by Chan, Golub, and LeVeque.
+                correction[col_ind] += diff * weights[row_ind]
+                variances[col_ind] += diff * diff * weights[row_ind]
+
+    for i in range(n_features):
+        if counts[i] != counts_nz[i]:
+            correction[i] -= (sum_weights[i] - sum_weights_nz[i]) * means[i]
+        correction[i] = correction[i]**2 / sum_weights[i]
+        if counts[i] != counts_nz[i]:
+            # only compute it when it's guaranteed to be non-zero to avoid
+            # catastrophic cancellation.
+            variances[i] += (sum_weights[i] - sum_weights_nz[i]) * means[i]**2
+        variances[i] = (variances[i] - correction[i]) / sum_weights[i]

     if floating is float:
-        dtype = np.float32
+        return (np.array(means, dtype=np.float32),
+                np.array(variances, dtype=np.float32),
+                np.array(sum_weights, dtype=np.float32))
     else:
-        dtype = np.float64
-
-    means = np.zeros(n_features, dtype=dtype)
-    variances = np.zeros_like(means, dtype=dtype)
-
-    cdef np.ndarray[np.int64_t, ndim=1] counts_nan = np.zeros(n_features,
-                                                              dtype=np.int64)
-
-    for i in range(n_features):
-
-        startptr = X_indptr[i]
-        endptr = X_indptr[i + 1]
-        counts = endptr - startptr
-
-        for j in range(startptr, endptr):
-            if not isnan(X_data[j]):
-                means[i] += X_data[j]
-            else:
-                counts_nan[i] += 1
-        counts -= counts_nan[i]
-        means[i] /= (n_samples - counts_nan[i])
-
-        for j in range(startptr, endptr):
-            if not isnan(X_data[j]):
-                diff = X_data[j] - means[i]
-                variances[i] += diff * diff
-
-        variances[i] += (n_samples - counts_nan[i] - counts) * means[i]**2
-        variances[i] /= (n_samples - counts_nan[i])
-
-    return means, variances, counts_nan
-
-
-def incr_mean_variance_axis0(X, last_mean, last_var, last_n):
+        return means, variances, sum_weights
+
+
+def incr_mean_variance_axis0(X, last_mean, last_var, last_n, weights=None):
     """Compute mean and variance along axis 0 on a CSR or CSC matrix.

     last_mean, last_var are the statistics computed at the last step by this
@@ -232,8 +315,12 @@
     last_var : float array with shape (n_features,)
       Array of feature-wise var to update with the new data X.

-    last_n : int array with shape (n_features,)
-      Number of samples seen so far, before X.
+    last_n : float array with shape (n_features,)
+      Sum of the weights seen so far (if weights are all set to 1
+      this will be the same as number of samples seen so far, before X).
+
+    weights : float array with shape (n_samples,) or None. If it is set
+      to None samples will be equally weighted.

     Returns
     -------
@@ -262,20 +349,38 @@
     """
     if X.dtype not in [np.float32, np.float64]:
         X = X.astype(np.float64)
-    return _incr_mean_variance_axis0(X.data, X.shape[0], X.shape[1], X.indices,
-                                     X.indptr, X.format, last_mean, last_var,
-                                     last_n)
+    X_dtype = X.dtype
+    if weights is None:
+        weights = np.ones(X.shape[0], dtype=X_dtype)
+    elif weights.dtype not in [np.float32, np.float64]:
+        weights = weights.astype(np.float64, copy=False)
+    if last_n.dtype not in [np.float32, np.float64]:
+        last_n = last_n.astype(np.float64, copy=False)
+
+    return _incr_mean_variance_axis0(X.data,
+                                     np.sum(weights),
+                                     X.shape[1],
+                                     X.indices,
+                                     X.indptr,
+                                     X.format,
+                                     last_mean.astype(X_dtype, copy=False),
+                                     last_var.astype(X_dtype, copy=False),
+                                     last_n.astype(X_dtype, copy=False),
+                                     weights.astype(X_dtype, copy=False))


 def _incr_mean_variance_axis0(np.ndarray[floating, ndim=1] X_data,
-                              unsigned long long n_samples,
+                              floating n_samples,
                               unsigned long long n_features,
-                              np.ndarray[integral, ndim=1] X_indices,
+                              np.ndarray[int, ndim=1] X_indices,
+                              # X_indptr might be either in32 or int64
                               np.ndarray[integral, ndim=1] X_indptr,
                               str X_format,
                               np.ndarray[floating, ndim=1] last_mean,
                               np.ndarray[floating, ndim=1] last_var,
-                              np.ndarray[np.int64_t, ndim=1] last_n):
+                              np.ndarray[floating, ndim=1] last_n,
+                              # previous sum of the weights (ie float)
+                              np.ndarray[floating, ndim=1] weights):
     # Implement the function here since variables using fused types
     # cannot be declared directly and can only be passed as function arguments
     cdef:
@@ -302,27 +407,21 @@
     updated_var = np.zeros_like(new_mean, dtype=dtype)

     cdef:
-        np.ndarray[np.int64_t, ndim=1] new_n
-        np.ndarray[np.int64_t, ndim=1] updated_n
+        np.ndarray[floating, ndim=1] new_n
+        np.ndarray[floating, ndim=1] updated_n
         np.ndarray[floating, ndim=1] last_over_new_n
-        np.ndarray[np.int64_t, ndim=1] counts_nan

     # Obtain new stats first
-    new_n = np.full(n_features, n_samples, dtype=np.int64)
-    updated_n = np.zeros_like(new_n, dtype=np.int64)
-    last_over_new_n = np.zeros_like(new_n, dtype=dtype)
-
+    updated_n = np.zeros(shape=n_features, dtype=dtype)
+    last_over_new_n = np.zeros_like(updated_n, dtype=dtype)
+
+    # X can be a CSR or CSC matrix
     if X_format == 'csr':
-        # X is a CSR matrix
-        new_mean, new_var, counts_nan = _csr_mean_variance_axis0(
-            X_data, n_samples, n_features, X_indices)
-    else:
-        # X is a CSC matrix
-        new_mean, new_var, counts_nan = _csc_mean_variance_axis0(
-            X_data, n_samples, n_features, X_indices, X_indptr)
-
-    for i in range(n_features):
-        new_n[i] -= counts_nan[i]
+        new_mean, new_var, new_n = _csr_mean_variance_axis0(
+            X_data, n_samples, n_features, X_indices, X_indptr, weights)
+    else:  # X_format == 'csc'
+        new_mean, new_var, new_n = _csc_mean_variance_axis0(
+            X_data, n_samples, n_features, X_indices, X_indptr, weights)

     # First pass
     cdef bint is_first_pass = True
@@ -330,28 +429,34 @@
         if last_n[i] > 0:
             is_first_pass = False
             break
+
     if is_first_pass:
         return new_mean, new_var, new_n

+    for i in range(n_features):
+        updated_n[i] = last_n[i] + new_n[i]
+
     # Next passes
     for i in range(n_features):
-        updated_n[i] = last_n[i] + new_n[i]
-        last_over_new_n[i] = last_n[i] / new_n[i]
-
-    # Unnormalized stats
-    for i in range(n_features):
-        last_mean[i] *= last_n[i]
-        last_var[i] *= last_n[i]
-        new_mean[i] *= new_n[i]
-        new_var[i] *= new_n[i]
-
-    # Update stats
-    for i in range(n_features):
-        updated_var[i] = (last_var[i] + new_var[i] +
-                          last_over_new_n[i] / updated_n[i] *
-                          (last_mean[i] / last_over_new_n[i] - new_mean[i])**2)
-        updated_mean[i] = (last_mean[i] + new_mean[i]) / updated_n[i]
-        updated_var[i] /= updated_n[i]
+        if new_n[i] > 0:
+            last_over_new_n[i] = dtype(last_n[i]) / dtype(new_n[i])
+            # Unnormalized stats
+            last_mean[i] *= last_n[i]
+            last_var[i] *= last_n[i]
+            new_mean[i] *= new_n[i]
+            new_var[i] *= new_n[i]
+            # Update stats
+            updated_var[i] = (
+                last_var[i] + new_var[i] +
+                last_over_new_n[i] / updated_n[i] *
+                (last_mean[i] / last_over_new_n[i] - new_mean[i])**2
+            )
+            updated_mean[i] = (last_mean[i] + new_mean[i]) / updated_n[i]
+            updated_var[i] /= updated_n[i]
+        else:
+            updated_var[i] = last_var[i]
+            updated_mean[i] = last_mean[i]
+            updated_n[i] = last_n[i]

     return updated_mean, updated_var, updated_n

('sklearn/utils', 'deprecation.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,5 +1,6 @@
 import warnings
 import functools
+

 __all__ = ["deprecated"]

@@ -15,7 +16,7 @@
     in an empty of parentheses:

     >>> from sklearn.utils import deprecated
-    >>> deprecated() # doctest: +ELLIPSIS
+    >>> deprecated()
     <sklearn.utils.deprecation.deprecated object at ...>

     >>> @deprecated()
@@ -23,14 +24,14 @@

     Parameters
     ----------
-    extra : string
-          to be added to the deprecation messages
+    extra : str, default=''
+          To be added to the deprecation messages.
     """

     # Adapted from https://wiki.python.org/moin/PythonDecoratorLibrary,
     # but with many changes.

-    def __init__(self, extra=''):
+    def __init__(self, extra=""):
         self.extra = extra

     def __call__(self, obj):
@@ -63,11 +64,12 @@
         init = cls.__init__

         def wrapped(*args, **kwargs):
-            warnings.warn(msg, category=DeprecationWarning)
+            warnings.warn(msg, category=FutureWarning)
             return init(*args, **kwargs)
+
         cls.__init__ = wrapped

-        wrapped.__name__ = '__init__'
+        wrapped.__name__ = "__init__"
         wrapped.__doc__ = self._update_doc(init.__doc__)
         wrapped.deprecated_original = init

@@ -82,7 +84,7 @@

         @functools.wraps(fun)
         def wrapped(*args, **kwargs):
-            warnings.warn(msg, category=DeprecationWarning)
+            warnings.warn(msg, category=FutureWarning)
             return fun(*args, **kwargs)

         wrapped.__doc__ = self._update_doc(wrapped.__doc__)
@@ -96,9 +98,12 @@
         msg = self.extra

         @property
+        @functools.wraps(prop)
         def wrapped(*args, **kwargs):
-            warnings.warn(msg, category=DeprecationWarning)
+            warnings.warn(msg, category=FutureWarning)
             return prop.fget(*args, **kwargs)
+
+        wrapped.__doc__ = self._update_doc(wrapped.__doc__)

         return wrapped

@@ -107,16 +112,16 @@
         if self.extra:
             newdoc = "%s: %s" % (newdoc, self.extra)
         if olddoc:
-            newdoc = "%s\n\n%s" % (newdoc, olddoc)
+            newdoc = "%s\n\n    %s" % (newdoc, olddoc)
         return newdoc


 def _is_deprecated(func):
-    """Helper to check if func is wraped by our deprecated decorator"""
-    closures = getattr(func, '__closure__', [])
+    """Helper to check if func is wrapped by our deprecated decorator"""
+    closures = getattr(func, "__closure__", [])
     if closures is None:
         closures = []
-    is_deprecated = ('deprecated' in ''.join([c.cell_contents
-                                              for c in closures
-                     if isinstance(c.cell_contents, str)]))
+    is_deprecated = "deprecated" in "".join(
+        [c.cell_contents for c in closures if isinstance(c.cell_contents, str)]
+    )
     return is_deprecated
('sklearn/utils', 'estimator_checks.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,88 +1,96 @@
 import types
 import warnings
-import sys
-import traceback
 import pickle
+import re
 from copy import deepcopy
-from functools import partial
+from functools import partial, wraps
 from inspect import signature

 import numpy as np
 from scipy import sparse
 from scipy.stats import rankdata
+import joblib

 from . import IS_PYPY
-from . import _joblib
-from .testing import assert_raises, _get_args
-from .testing import assert_raises_regex
-from .testing import assert_raise_message
-from .testing import assert_equal
-from .testing import assert_not_equal
-from .testing import assert_in
-from .testing import assert_array_equal
-from .testing import assert_array_almost_equal
-from .testing import assert_allclose
-from .testing import assert_allclose_dense_sparse
-from .testing import assert_warns_message
-from .testing import set_random_state
-from .testing import assert_greater
-from .testing import assert_greater_equal
-from .testing import SkipTest
-from .testing import ignore_warnings
-from .testing import assert_dict_equal
-from .testing import create_memmap_backed_data
+from .. import config_context
+from ._testing import _get_args
+from ._testing import assert_raise_message
+from ._testing import assert_array_equal
+from ._testing import assert_array_almost_equal
+from ._testing import assert_allclose
+from ._testing import assert_allclose_dense_sparse
+from ._testing import assert_array_less
+from ._testing import set_random_state
+from ._testing import SkipTest
+from ._testing import ignore_warnings
+from ._testing import create_memmap_backed_data
+from ._testing import raises
 from . import is_scalar_nan
-from ..discriminant_analysis import LinearDiscriminantAnalysis
+
+from ..linear_model import LinearRegression
+from ..linear_model import LogisticRegression
+from ..linear_model import RANSACRegressor
 from ..linear_model import Ridge

-
-from ..base import (clone, ClusterMixin, is_classifier, is_regressor,
-                          _DEFAULT_TAGS, RegressorMixin, is_outlier_detector)
+from ..base import (
+    clone,
+    ClusterMixin,
+    is_classifier,
+    is_regressor,
+    is_outlier_detector,
+    RegressorMixin,
+)

 from ..metrics import accuracy_score, adjusted_rand_score, f1_score
-
 from ..random_projection import BaseRandomProjection
 from ..feature_selection import SelectKBest
 from ..pipeline import make_pipeline
 from ..exceptions import DataConversionWarning
+from ..exceptions import NotFittedError
 from ..exceptions import SkipTestWarning
 from ..model_selection import train_test_split
 from ..model_selection import ShuffleSplit
 from ..model_selection._validation import _safe_split
-from ..metrics.pairwise import (rbf_kernel, linear_kernel,
-                                      pairwise_distances)
-
-from .import shuffle
+from ..metrics.pairwise import rbf_kernel, linear_kernel, pairwise_distances
+from ..utils.fixes import threadpool_info
+from ..utils.validation import check_is_fitted
+
+from . import shuffle
+from ._tags import (
+    _DEFAULT_TAGS,
+    _safe_tags,
+)
 from .validation import has_fit_parameter, _num_samples
 from ..preprocessing import StandardScaler
-from ..datasets import load_iris, load_boston, make_blobs
-
-
-BOSTON = None
-CROSS_DECOMPOSITION = ['PLSCanonical', 'PLSRegression', 'CCA', 'PLSSVD']
-
-
-def _safe_tags(estimator, key=None):
-    # if estimator doesn't have _get_tags, use _DEFAULT_TAGS
-    # if estimator has tags but not key, use _DEFAULT_TAGS[key]
-    if hasattr(estimator, "_get_tags"):
-        if key is not None:
-            return estimator._get_tags().get(key, _DEFAULT_TAGS[key])
-        tags = estimator._get_tags()
-        return {key: tags.get(key, _DEFAULT_TAGS[key])
-                for key in _DEFAULT_TAGS.keys()}
-    if key is not None:
-        return _DEFAULT_TAGS[key]
-    return _DEFAULT_TAGS
-
-
-def _yield_checks(name, estimator):
+from ..preprocessing import scale
+from ..datasets import (
+    load_iris,
+    make_blobs,
+    make_multilabel_classification,
+    make_regression,
+)
+
+REGRESSION_DATASET = None
+CROSS_DECOMPOSITION = ["PLSCanonical", "PLSRegression", "CCA", "PLSSVD"]
+
+
+def _yield_checks(estimator):
+    name = estimator.__class__.__name__
     tags = _safe_tags(estimator)
+
+    yield check_no_attributes_set_in_init
     yield check_estimators_dtypes
     yield check_fit_score_takes_y
-    yield check_sample_weights_pandas_series
-    yield check_sample_weights_list
-    yield check_sample_weights_invariance
+    if has_fit_parameter(estimator, "sample_weight"):
+        yield check_sample_weights_pandas_series
+        yield check_sample_weights_not_an_array
+        yield check_sample_weights_list
+        if not tags["pairwise"]:
+            # We skip pairwise because the data is not pairwise
+            yield check_sample_weights_shape
+            yield check_sample_weights_not_overwritten
+            yield partial(check_sample_weights_invariance, kind="ones")
+            yield partial(check_sample_weights_invariance, kind="zeros")
     yield check_estimators_fit_returns_self
     yield partial(check_estimators_fit_returns_self, readonly_memmap=True)

@@ -101,8 +109,12 @@
         # Test that all estimators check their input for NaN's and infs
         yield check_estimators_nan_inf

+    if tags["pairwise"]:
+        # Check that pairwise estimator throws error on non-square input
+        yield check_nonsquare_error
+
     yield check_estimators_overwrite_params
-    if hasattr(estimator, 'sparsify'):
+    if hasattr(estimator, "sparsify"):
         yield check_sparsify_coefficients

     yield check_estimator_sparse_data
@@ -111,25 +123,37 @@
     # give the same answer as before.
     yield check_estimators_pickle

-
-def _yield_classifier_checks(name, classifier):
+    yield check_estimator_get_tags_default_keys
+
+
+def _yield_classifier_checks(classifier):
     tags = _safe_tags(classifier)

-    # test classifiers can handle non-array data
+    # test classifiers can handle non-array data and pandas objects
     yield check_classifier_data_not_an_array
     # test classifiers trained on a single label always return this label
     yield check_classifiers_one_label
     yield check_classifiers_classes
     yield check_estimators_partial_fit_n_features
+    if tags["multioutput"]:
+        yield check_classifier_multioutput
     # basic consistency testing
     yield check_classifiers_train
     yield partial(check_classifiers_train, readonly_memmap=True)
+    yield partial(check_classifiers_train, readonly_memmap=True, X_dtype="float32")
     yield check_classifiers_regression_target
+    if tags["multilabel"]:
+        yield check_classifiers_multilabel_representation_invariance
+        yield check_classifiers_multilabel_output_format_predict
+        yield check_classifiers_multilabel_output_format_predict_proba
+        yield check_classifiers_multilabel_output_format_decision_function
     if not tags["no_validation"]:
         yield check_supervised_y_no_nan
-        yield check_supervised_y_2d
-    yield check_estimators_unfitted
-    if 'class_weight' in classifier.get_params().keys():
+        if not tags["multioutput_only"]:
+            yield check_supervised_y_2d
+    if tags["requires_fit"]:
+        yield check_estimators_unfitted
+    if "class_weight" in classifier.get_params().keys():
         yield check_class_weight_classifiers

     yield check_non_transformer_estimators_n_iter
@@ -137,233 +161,539 @@
     yield check_decision_proba_consistency


-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+@ignore_warnings(category=FutureWarning)
 def check_supervised_y_no_nan(name, estimator_orig):
     # Checks that the Estimator targets are not NaN.
     estimator = clone(estimator_orig)
     rng = np.random.RandomState(888)
-    X = rng.randn(10, 5)
-    y = np.full(10, np.inf)
-    y = multioutput_estimator_convert_y_2d(estimator, y)
-
-    errmsg = "Input contains NaN, infinity or a value too large for " \
-             "dtype('float64')."
-    try:
-        estimator.fit(X, y)
-    except ValueError as e:
-        if str(e) != errmsg:
-            raise ValueError("Estimator {0} raised error as expected, but "
-                             "does not match expected error message"
-                             .format(name))
-    else:
-        raise ValueError("Estimator {0} should have raised error on fitting "
-                         "array y with NaN value.".format(name))
-
-
-def _yield_regressor_checks(name, regressor):
+    X = rng.standard_normal(size=(10, 5))
+
+    for value in [np.nan, np.inf]:
+        y = np.full(10, value)
+        y = _enforce_estimator_tags_y(estimator, y)
+
+        module_name = estimator.__module__
+        if module_name.startswith("sklearn.") and not (
+            "test_" in module_name or module_name.endswith("_testing")
+        ):
+            # In scikit-learn we want the error message to mention the input
+            # name and be specific about the kind of unexpected value.
+            if np.isinf(value):
+                match = (
+                    r"Input (y|Y) contains infinity or a value too large for"
+                    r" dtype\('float64'\)."
+                )
+            else:
+                match = r"Input (y|Y) contains NaN."
+        else:
+            # Do not impose a particular error message to third-party libraries.
+            match = None
+        err_msg = (
+            f"Estimator {name} should have raised error on fitting array y with inf"
+            " value."
+        )
+        with raises(ValueError, match=match, err_msg=err_msg):
+            estimator.fit(X, y)
+
+
+def _yield_regressor_checks(regressor):
     tags = _safe_tags(regressor)
     # TODO: test with intercept
     # TODO: test with multiple responses
     # basic testing
     yield check_regressors_train
     yield partial(check_regressors_train, readonly_memmap=True)
+    yield partial(check_regressors_train, readonly_memmap=True, X_dtype="float32")
     yield check_regressor_data_not_an_array
     yield check_estimators_partial_fit_n_features
+    if tags["multioutput"]:
+        yield check_regressor_multioutput
     yield check_regressors_no_decision_function
-    if not tags["no_validation"]:
+    if not tags["no_validation"] and not tags["multioutput_only"]:
         yield check_supervised_y_2d
     yield check_supervised_y_no_nan
-    if name != 'CCA':
+    name = regressor.__class__.__name__
+    if name != "CCA":
         # check that the regressor handles int input
         yield check_regressors_int
-    yield check_estimators_unfitted
+    if tags["requires_fit"]:
+        yield check_estimators_unfitted
     yield check_non_transformer_estimators_n_iter


-def _yield_transformer_checks(name, transformer):
+def _yield_transformer_checks(transformer):
+    tags = _safe_tags(transformer)
     # All transformers should either deal with sparse data or raise an
     # exception with type TypeError and an intelligible error message
-    yield check_transformer_data_not_an_array
+    if not tags["no_validation"]:
+        yield check_transformer_data_not_an_array
     # these don't actually fit the data, so don't raise errors
     yield check_transformer_general
+    if tags["preserves_dtype"]:
+        yield check_transformer_preserve_dtypes
     yield partial(check_transformer_general, readonly_memmap=True)
-
-    if not _safe_tags(transformer, "stateless"):
+    if not _safe_tags(transformer, key="stateless"):
         yield check_transformers_unfitted
     # Dependent on external solvers and hence accessing the iter
     # param is non-trivial.
-    external_solver = ['Isomap', 'KernelPCA', 'LocallyLinearEmbedding',
-                       'RandomizedLasso', 'LogisticRegressionCV']
+    external_solver = [
+        "Isomap",
+        "KernelPCA",
+        "LocallyLinearEmbedding",
+        "RandomizedLasso",
+        "LogisticRegressionCV",
+        "BisectingKMeans",
+    ]
+
+    name = transformer.__class__.__name__
     if name not in external_solver:
         yield check_transformer_n_iter


-def _yield_clustering_checks(name, clusterer):
+def _yield_clustering_checks(clusterer):
     yield check_clusterer_compute_labels_predict
-    if name not in ('WardAgglomeration', "FeatureAgglomeration"):
+    name = clusterer.__class__.__name__
+    if name not in ("WardAgglomeration", "FeatureAgglomeration"):
         # this is clustering on the features
         # let's not test that here.
         yield check_clustering
         yield partial(check_clustering, readonly_memmap=True)
         yield check_estimators_partial_fit_n_features
-    yield check_non_transformer_estimators_n_iter
-
-
-def _yield_outliers_checks(name, estimator):
+    if not hasattr(clusterer, "transform"):
+        yield check_non_transformer_estimators_n_iter
+
+
+def _yield_outliers_checks(estimator):

     # checks for outlier detectors that have a fit_predict method
-    if hasattr(estimator, 'fit_predict'):
+    if hasattr(estimator, "fit_predict"):
         yield check_outliers_fit_predict

     # checks for estimators that can be used on a test set
-    if hasattr(estimator, 'predict'):
+    if hasattr(estimator, "predict"):
         yield check_outliers_train
         yield partial(check_outliers_train, readonly_memmap=True)
         # test outlier detectors can handle non-array data
         yield check_classifier_data_not_an_array
         # test if NotFittedError is raised
-        yield check_estimators_unfitted
-
-
-def _yield_all_checks(name, estimator):
+        if _safe_tags(estimator, key="requires_fit"):
+            yield check_estimators_unfitted
+    yield check_non_transformer_estimators_n_iter
+
+
+def _yield_all_checks(estimator):
+    name = estimator.__class__.__name__
     tags = _safe_tags(estimator)
     if "2darray" not in tags["X_types"]:
-        warnings.warn("Can't test estimator {} which requires input "
-                      " of type {}".format(name, tags["X_types"]),
-                      SkipTestWarning)
+        warnings.warn(
+            "Can't test estimator {} which requires input  of type {}".format(
+                name, tags["X_types"]
+            ),
+            SkipTestWarning,
+        )
         return
     if tags["_skip_test"]:
-        warnings.warn("Explicit SKIP via _skip_test tag for estimator "
-                      "{}.".format(name),
-                      SkipTestWarning)
+        warnings.warn(
+            "Explicit SKIP via _skip_test tag for estimator {}.".format(name),
+            SkipTestWarning,
+        )
         return

-    for check in _yield_checks(name, estimator):
+    for check in _yield_checks(estimator):
         yield check
     if is_classifier(estimator):
-        for check in _yield_classifier_checks(name, estimator):
+        for check in _yield_classifier_checks(estimator):
             yield check
     if is_regressor(estimator):
-        for check in _yield_regressor_checks(name, estimator):
+        for check in _yield_regressor_checks(estimator):
             yield check
-    if hasattr(estimator, 'transform'):
-        for check in _yield_transformer_checks(name, estimator):
+    if hasattr(estimator, "transform"):
+        for check in _yield_transformer_checks(estimator):
             yield check
     if isinstance(estimator, ClusterMixin):
-        for check in _yield_clustering_checks(name, estimator):
+        for check in _yield_clustering_checks(estimator):
             yield check
     if is_outlier_detector(estimator):
-        for check in _yield_outliers_checks(name, estimator):
+        for check in _yield_outliers_checks(estimator):
             yield check
-    yield check_fit2d_predict1d
-    yield check_methods_subset_invariance
+    yield check_parameters_default_constructible
+    if not tags["non_deterministic"]:
+        yield check_methods_sample_order_invariance
+        yield check_methods_subset_invariance
     yield check_fit2d_1sample
     yield check_fit2d_1feature
-    yield check_fit1d
     yield check_get_params_invariance
     yield check_set_params
     yield check_dict_unchanged
     yield check_dont_overwrite_parameters
     yield check_fit_idempotent
-
-
-def check_estimator(Estimator):
+    yield check_fit_check_is_fitted
+    if not tags["no_validation"]:
+        yield check_n_features_in
+        yield check_fit1d
+        yield check_fit2d_predict1d
+        if tags["requires_y"]:
+            yield check_requires_y_none
+    if tags["requires_positive_X"]:
+        yield check_fit_non_negative
+
+
+def _get_check_estimator_ids(obj):
+    """Create pytest ids for checks.
+
+    When `obj` is an estimator, this returns the pprint version of the
+    estimator (with `print_changed_only=True`). When `obj` is a function, the
+    name of the function is returned with its keyword arguments.
+
+    `_get_check_estimator_ids` is designed to be used as the `id` in
+    `pytest.mark.parametrize` where `check_estimator(..., generate_only=True)`
+    is yielding estimators and checks.
+
+    Parameters
+    ----------
+    obj : estimator or function
+        Items generated by `check_estimator`.
+
+    Returns
+    -------
+    id : str or None
+
+    See Also
+    --------
+    check_estimator
+    """
+    if callable(obj):
+        if not isinstance(obj, partial):
+            return obj.__name__
+
+        if not obj.keywords:
+            return obj.func.__name__
+
+        kwstring = ",".join(["{}={}".format(k, v) for k, v in obj.keywords.items()])
+        return "{}({})".format(obj.func.__name__, kwstring)
+    if hasattr(obj, "get_params"):
+        with config_context(print_changed_only=True):
+            return re.sub(r"\s", "", str(obj))
+
+
+def _construct_instance(Estimator):
+    """Construct Estimator instance if possible."""
+    required_parameters = getattr(Estimator, "_required_parameters", [])
+    if len(required_parameters):
+        if required_parameters in (["estimator"], ["base_estimator"]):
+            # `RANSACRegressor` will raise an error with any model other
+            # than `LinearRegression` if we don't fix `min_samples` parameter.
+            # For common test, we can enforce using `LinearRegression` that
+            # is the default estimator in `RANSACRegressor` instead of `Ridge`.
+            if issubclass(Estimator, RANSACRegressor):
+                estimator = Estimator(LinearRegression())
+            elif issubclass(Estimator, RegressorMixin):
+                estimator = Estimator(Ridge())
+            else:
+                estimator = Estimator(LogisticRegression(C=1))
+        elif required_parameters in (["estimators"],):
+            # Heterogeneous ensemble classes (i.e. stacking, voting)
+            if issubclass(Estimator, RegressorMixin):
+                estimator = Estimator(
+                    estimators=[("est1", Ridge(alpha=0.1)), ("est2", Ridge(alpha=1))]
+                )
+            else:
+                estimator = Estimator(
+                    estimators=[
+                        ("est1", LogisticRegression(C=0.1)),
+                        ("est2", LogisticRegression(C=1)),
+                    ]
+                )
+        else:
+            msg = (
+                f"Can't instantiate estimator {Estimator.__name__} "
+                f"parameters {required_parameters}"
+            )
+            # raise additional warning to be shown by pytest
+            warnings.warn(msg, SkipTestWarning)
+            raise SkipTest(msg)
+    else:
+        estimator = Estimator()
+    return estimator
+
+
+def _maybe_mark_xfail(estimator, check, pytest):
+    # Mark (estimator, check) pairs as XFAIL if needed (see conditions in
+    # _should_be_skipped_or_marked())
+    # This is similar to _maybe_skip(), but this one is used by
+    # @parametrize_with_checks() instead of check_estimator()
+
+    should_be_marked, reason = _should_be_skipped_or_marked(estimator, check)
+    if not should_be_marked:
+        return estimator, check
+    else:
+        return pytest.param(estimator, check, marks=pytest.mark.xfail(reason=reason))
+
+
+def _maybe_skip(estimator, check):
+    # Wrap a check so that it's skipped if needed (see conditions in
+    # _should_be_skipped_or_marked())
+    # This is similar to _maybe_mark_xfail(), but this one is used by
+    # check_estimator() instead of @parametrize_with_checks which requires
+    # pytest
+    should_be_skipped, reason = _should_be_skipped_or_marked(estimator, check)
+    if not should_be_skipped:
+        return check
+
+    check_name = check.func.__name__ if isinstance(check, partial) else check.__name__
+
+    @wraps(check)
+    def wrapped(*args, **kwargs):
+        raise SkipTest(
+            f"Skipping {check_name} for {estimator.__class__.__name__}: {reason}"
+        )
+
+    return wrapped
+
+
+def _should_be_skipped_or_marked(estimator, check):
+    # Return whether a check should be skipped (when using check_estimator())
+    # or marked as XFAIL (when using @parametrize_with_checks()), along with a
+    # reason.
+    # Currently, a check should be skipped or marked if
+    # the check is in the _xfail_checks tag of the estimator
+
+    check_name = check.func.__name__ if isinstance(check, partial) else check.__name__
+
+    xfail_checks = _safe_tags(estimator, key="_xfail_checks") or {}
+    if check_name in xfail_checks:
+        return True, xfail_checks[check_name]
+
+    return False, "placeholder reason that will never be used"
+
+
+def parametrize_with_checks(estimators):
+    """Pytest specific decorator for parametrizing estimator checks.
+
+    The `id` of each check is set to be a pprint version of the estimator
+    and the name of the check with its keyword arguments.
+    This allows to use `pytest -k` to specify which tests to run::
+
+        pytest test_check_estimators.py -k check_estimators_fit_returns_self
+
+    Parameters
+    ----------
+    estimators : list of estimators instances
+        Estimators to generated checks for.
+
+        .. versionchanged:: 0.24
+           Passing a class was deprecated in version 0.23, and support for
+           classes was removed in 0.24. Pass an instance instead.
+
+        .. versionadded:: 0.24
+
+    Returns
+    -------
+    decorator : `pytest.mark.parametrize`
+
+    See Also
+    --------
+    check_estimator : Check if estimator adheres to scikit-learn conventions.
+
+    Examples
+    --------
+    >>> from sklearn.utils.estimator_checks import parametrize_with_checks
+    >>> from sklearn.linear_model import LogisticRegression
+    >>> from sklearn.tree import DecisionTreeRegressor
+
+    >>> @parametrize_with_checks([LogisticRegression(),
+    ...                           DecisionTreeRegressor()])
+    ... def test_sklearn_compatible_estimator(estimator, check):
+    ...     check(estimator)
+
+    """
+    import pytest
+
+    if any(isinstance(est, type) for est in estimators):
+        msg = (
+            "Passing a class was deprecated in version 0.23 "
+            "and isn't supported anymore from 0.24."
+            "Please pass an instance instead."
+        )
+        raise TypeError(msg)
+
+    def checks_generator():
+        for estimator in estimators:
+            name = type(estimator).__name__
+            for check in _yield_all_checks(estimator):
+                check = partial(check, name)
+                yield _maybe_mark_xfail(estimator, check, pytest)
+
+    return pytest.mark.parametrize(
+        "estimator, check", checks_generator(), ids=_get_check_estimator_ids
+    )
+
+
+def check_estimator(estimator=None, generate_only=False, Estimator="deprecated"):
     """Check if estimator adheres to scikit-learn conventions.

-    This estimator will run an extensive test-suite for input validation,
-    shapes, etc.
+    This function will run an extensive test-suite for input validation,
+    shapes, etc, making sure that the estimator complies with `scikit-learn`
+    conventions as detailed in :ref:`rolling_your_own_estimator`.
     Additional tests for classifiers, regressors, clustering or transformers
     will be run if the Estimator class inherits from the corresponding mixin
     from sklearn.base.

-    This test can be applied to classes or instances.
-    Classes currently have some additional tests that related to construction,
-    while passing instances allows the testing of multiple options.
+    Setting `generate_only=True` returns a generator that yields (estimator,
+    check) tuples where the check can be called independently from each
+    other, i.e. `check(estimator)`. This allows all checks to be run
+    independently and report the checks that are failing.
+
+    scikit-learn provides a pytest specific decorator,
+    :func:`~sklearn.utils.parametrize_with_checks`, making it easier to test
+    multiple estimators.

     Parameters
     ----------
-    estimator : estimator object or class
-        Estimator to check. Estimator is a class object or instance.
-
+    estimator : estimator object
+        Estimator instance to check.
+
+        .. versionadded:: 1.1
+           Passing a class was deprecated in version 0.23, and support for
+           classes was removed in 0.24.
+
+    generate_only : bool, default=False
+        When `False`, checks are evaluated when `check_estimator` is called.
+        When `True`, `check_estimator` returns a generator that yields
+        (estimator, check) tuples. The check is run by calling
+        `check(estimator)`.
+
+        .. versionadded:: 0.22
+
+    Estimator : estimator object
+        Estimator instance to check.
+
+        .. deprecated:: 1.1
+            ``Estimator`` was deprecated in favor of ``estimator`` in version 1.1
+            and will be removed in version 1.3.
+
+    Returns
+    -------
+    checks_generator : generator
+        Generator that yields (estimator, check) tuples. Returned when
+        `generate_only=True`.
+
+    See Also
+    --------
+    parametrize_with_checks : Pytest specific decorator for parametrizing estimator
+        checks.
     """
-    if isinstance(Estimator, type):
-        # got a class
-        name = Estimator.__name__
-        estimator = Estimator()
-        check_parameters_default_constructible(name, Estimator)
-        check_no_attributes_set_in_init(name, estimator)
-    else:
-        # got an instance
+
+    if estimator is None and Estimator == "deprecated":
+        msg = "Either estimator or Estimator should be passed to check_estimator."
+        raise ValueError(msg)
+
+    if Estimator != "deprecated":
+        msg = (
+            "'Estimator' was deprecated in favor of 'estimator' in version 1.1 "
+            "and will be removed in version 1.3."
+        )
+        warnings.warn(msg, FutureWarning)
         estimator = Estimator
-        name = type(estimator).__name__
-
-    for check in _yield_all_checks(name, estimator):
+    if isinstance(estimator, type):
+        msg = (
+            "Passing a class was deprecated in version 0.23 "
+            "and isn't supported anymore from 0.24."
+            "Please pass an instance instead."
+        )
+        raise TypeError(msg)
+
+    name = type(estimator).__name__
+
+    def checks_generator():
+        for check in _yield_all_checks(estimator):
+            check = _maybe_skip(estimator, check)
+            yield estimator, partial(check, name)
+
+    if generate_only:
+        return checks_generator()
+
+    for estimator, check in checks_generator():
         try:
-            check(name, estimator)
+            check(estimator)
         except SkipTest as exception:
-            # the only SkipTest thrown currently results from not
-            # being able to import pandas.
+            # SkipTest is thrown when pandas can't be imported, or by checks
+            # that are in the xfail_checks tag
             warnings.warn(str(exception), SkipTestWarning)


-def _boston_subset(n_samples=200):
-    global BOSTON
-    if BOSTON is None:
-        boston = load_boston()
-        X, y = boston.data, boston.target
-        X, y = shuffle(X, y, random_state=0)
-        X, y = X[:n_samples], y[:n_samples]
+def _regression_dataset():
+    global REGRESSION_DATASET
+    if REGRESSION_DATASET is None:
+        X, y = make_regression(
+            n_samples=200,
+            n_features=10,
+            n_informative=1,
+            bias=5.0,
+            noise=20,
+            random_state=42,
+        )
         X = StandardScaler().fit_transform(X)
-        BOSTON = X, y
-    return BOSTON
-
-
-def set_checking_parameters(estimator):
+        REGRESSION_DATASET = X, y
+    return REGRESSION_DATASET
+
+
+def _set_checking_parameters(estimator):
     # set parameters to speed up some estimators and
     # avoid deprecated behaviour
     params = estimator.get_params()
     name = estimator.__class__.__name__
-    if ("n_iter" in params and name != "TSNE"):
+    if "n_iter" in params and name != "TSNE":
         estimator.set_params(n_iter=5)
     if "max_iter" in params:
         if estimator.max_iter is not None:
             estimator.set_params(max_iter=min(5, estimator.max_iter))
         # LinearSVR, LinearSVC
-        if estimator.__class__.__name__ in ['LinearSVR', 'LinearSVC']:
+        if name in ["LinearSVR", "LinearSVC"]:
             estimator.set_params(max_iter=20)
         # NMF
-        if estimator.__class__.__name__ == 'NMF':
+        if name == "NMF":
+            estimator.set_params(max_iter=500)
+        # MiniBatchNMF
+        if estimator.__class__.__name__ == "MiniBatchNMF":
+            estimator.set_params(max_iter=20, fresh_restarts=True)
+        # MLP
+        if name in ["MLPClassifier", "MLPRegressor"]:
             estimator.set_params(max_iter=100)
-        # MLP
-        if estimator.__class__.__name__ in ['MLPClassifier', 'MLPRegressor']:
-            estimator.set_params(max_iter=100)
+        # MiniBatchDictionaryLearning
+        if name == "MiniBatchDictionaryLearning":
+            estimator.set_params(max_iter=5)
+
     if "n_resampling" in params:
         # randomized lasso
         estimator.set_params(n_resampling=5)
     if "n_estimators" in params:
-        # especially gradient boosting with default 100
-        # FIXME: The default number of trees was changed and is set to 'warn'
-        # for some of the ensemble methods. We need to catch this case to avoid
-        # an error during the comparison. To be reverted in 0.22.
-        if estimator.n_estimators == 'warn':
-            estimator.set_params(n_estimators=5)
-        else:
-            estimator.set_params(n_estimators=min(5, estimator.n_estimators))
+        estimator.set_params(n_estimators=min(5, estimator.n_estimators))
     if "max_trials" in params:
         # RANSAC
         estimator.set_params(max_trials=10)
     if "n_init" in params:
         # K-Means
         estimator.set_params(n_init=2)
-
-    if hasattr(estimator, "n_components"):
-        estimator.n_components = 2
-
-    if name == 'TruncatedSVD':
+    if "batch_size" in params:
+        estimator.set_params(batch_size=10)
+
+    if name == "MeanShift":
+        # In the case of check_fit2d_1sample, bandwidth is set to None and
+        # is thus estimated. De facto it is 0.0 as a single sample is provided
+        # and this makes the test fails. Hence we give it a placeholder value.
+        estimator.set_params(bandwidth=1.0)
+
+    if name == "TruncatedSVD":
         # TruncatedSVD doesn't run with n_components = n_features
         # This is ugly :-/
         estimator.n_components = 1

+    if name == "LassoLarsIC":
+        # Noise variance estimation does not work when `n_samples < n_features`.
+        # We need to provide the noise variance explicitly.
+        estimator.set_params(noise_variance=1.0)
+
     if hasattr(estimator, "n_clusters"):
         estimator.n_clusters = min(estimator.n_clusters, 2)

@@ -372,17 +702,10 @@

     if name == "SelectFdr":
         # be tolerant of noisy datasets (not actually speed)
-        estimator.set_params(alpha=.5)
+        estimator.set_params(alpha=0.5)

     if name == "TheilSenRegressor":
         estimator.max_subpopulation = 100
-
-    if estimator.__class__.__name__ == "IsolationForest":
-        # XXX to be removed in 0.22.
-        # this is used because the old IsolationForest does not
-        # respect the outlier detection API and thus and does not
-        # pass the outlier detection common tests.
-        estimator.set_params(behaviour='new')

     if isinstance(estimator, BaseRandomProjection):
         # Due to the jl lemma and often very few samples, the number
@@ -396,43 +719,49 @@
         # which is more feature than we have in most case.
         estimator.set_params(k=1)

-    if name in ('HistGradientBoostingClassifier',
-                'HistGradientBoostingRegressor'):
+    if name in ("HistGradientBoostingClassifier", "HistGradientBoostingRegressor"):
         # The default min_samples_leaf (20) isn't appropriate for small
         # datasets (only very shallow trees are built) that the checks use.
         estimator.set_params(min_samples_leaf=5)

-
-class NotAnArray:
-    """An object that is convertible to an array
+    if name == "DummyClassifier":
+        # the default strategy prior would output constant predictions and fail
+        # for check_classifiers_predictions
+        estimator.set_params(strategy="stratified")
+
+    # Speed-up by reducing the number of CV or splits for CV estimators
+    loo_cv = ["RidgeCV", "RidgeClassifierCV"]
+    if name not in loo_cv and hasattr(estimator, "cv"):
+        estimator.set_params(cv=3)
+    if hasattr(estimator, "n_splits"):
+        estimator.set_params(n_splits=3)
+
+    if name == "OneHotEncoder":
+        estimator.set_params(handle_unknown="ignore")
+
+    if name in CROSS_DECOMPOSITION:
+        estimator.set_params(n_components=1)
+
+
+class _NotAnArray:
+    """An object that is convertible to an array.

     Parameters
     ----------
-    data : array_like
+    data : array-like
         The data.
     """

     def __init__(self, data):
-        self.data = data
+        self.data = np.asarray(data)

     def __array__(self, dtype=None):
         return self.data

-
-def _is_pairwise(estimator):
-    """Returns True if estimator has a _pairwise attribute set to True.
-
-    Parameters
-    ----------
-    estimator : object
-        Estimator object to test.
-
-    Returns
-    -------
-    out : bool
-        True if _pairwise is set to True and False otherwise.
-    """
-    return bool(getattr(estimator, "_pairwise", False))
+    def __array_function__(self, func, types, args, kwargs):
+        if func.__name__ == "may_share_memory":
+            return True
+        raise TypeError("Don't want to call array_function {}!".format(func.__name__))


 def _is_pairwise_metric(estimator):
@@ -450,192 +779,351 @@
     """
     metric = getattr(estimator, "metric", None)

-    return bool(metric == 'precomputed')
-
-
-def pairwise_estimator_convert_X(X, estimator, kernel=linear_kernel):
+    return bool(metric == "precomputed")
+
+
+def _pairwise_estimator_convert_X(X, estimator, kernel=linear_kernel):

     if _is_pairwise_metric(estimator):
-        return pairwise_distances(X, metric='euclidean')
-    if _is_pairwise(estimator):
+        return pairwise_distances(X, metric="euclidean")
+    tags = _safe_tags(estimator)
+    if tags["pairwise"]:
         return kernel(X, X)

     return X


 def _generate_sparse_matrix(X_csr):
-    """Generate sparse matrices with {32,64}bit indices of diverse format
-
-        Parameters
-        ----------
-        X_csr: CSR Matrix
-            Input matrix in CSR format
-
-        Returns
-        -------
-        out: iter(Matrices)
-            In format['dok', 'lil', 'dia', 'bsr', 'csr', 'csc', 'coo',
-             'coo_64', 'csc_64', 'csr_64']
+    """Generate sparse matrices with {32,64}bit indices of diverse format.
+
+    Parameters
+    ----------
+    X_csr: CSR Matrix
+        Input matrix in CSR format.
+
+    Returns
+    -------
+    out: iter(Matrices)
+        In format['dok', 'lil', 'dia', 'bsr', 'csr', 'csc', 'coo',
+        'coo_64', 'csc_64', 'csr_64']
     """

-    assert X_csr.format == 'csr'
-    yield 'csr', X_csr.copy()
-    for sparse_format in ['dok', 'lil', 'dia', 'bsr', 'csc', 'coo']:
+    assert X_csr.format == "csr"
+    yield "csr", X_csr.copy()
+    for sparse_format in ["dok", "lil", "dia", "bsr", "csc", "coo"]:
         yield sparse_format, X_csr.asformat(sparse_format)

     # Generate large indices matrix only if its supported by scipy
-    X_coo = X_csr.asformat('coo')
-    X_coo.row = X_coo.row.astype('int64')
-    X_coo.col = X_coo.col.astype('int64')
+    X_coo = X_csr.asformat("coo")
+    X_coo.row = X_coo.row.astype("int64")
+    X_coo.col = X_coo.col.astype("int64")
     yield "coo_64", X_coo

-    for sparse_format in ['csc', 'csr']:
+    for sparse_format in ["csc", "csr"]:
         X = X_csr.asformat(sparse_format)
-        X.indices = X.indices.astype('int64')
-        X.indptr = X.indptr.astype('int64')
+        X.indices = X.indices.astype("int64")
+        X.indptr = X.indptr.astype("int64")
         yield sparse_format + "_64", X


 def check_estimator_sparse_data(name, estimator_orig):
-
     rng = np.random.RandomState(0)
-    X = rng.rand(40, 10)
-    X[X < .8] = 0
-    X = pairwise_estimator_convert_X(X, estimator_orig)
+    X = rng.uniform(size=(40, 3))
+    X[X < 0.8] = 0
+    X = _pairwise_estimator_convert_X(X, estimator_orig)
     X_csr = sparse.csr_matrix(X)
-    y = (4 * rng.rand(40)).astype(np.int)
+    y = (4 * rng.uniform(size=40)).astype(int)
     # catch deprecation warnings
-    with ignore_warnings(category=DeprecationWarning):
+    with ignore_warnings(category=FutureWarning):
         estimator = clone(estimator_orig)
-    y = multioutput_estimator_convert_y_2d(estimator, y)
+    y = _enforce_estimator_tags_y(estimator, y)
+    tags = _safe_tags(estimator_orig)
     for matrix_format, X in _generate_sparse_matrix(X_csr):
         # catch deprecation warnings
-        with ignore_warnings(category=(DeprecationWarning, FutureWarning)):
+        with ignore_warnings(category=FutureWarning):
             estimator = clone(estimator_orig)
-            if name in ['Scaler', 'StandardScaler']:
+            if name in ["Scaler", "StandardScaler"]:
                 estimator.set_params(with_mean=False)
         # fit and predict
-        try:
-            with ignore_warnings(category=(DeprecationWarning, FutureWarning)):
+        if "64" in matrix_format:
+            err_msg = (
+                f"Estimator {name} doesn't seem to support {matrix_format} "
+                "matrix, and is not failing gracefully, e.g. by using "
+                "check_array(X, accept_large_sparse=False)"
+            )
+        else:
+            err_msg = (
+                f"Estimator {name} doesn't seem to fail gracefully on sparse "
+                "data: error message should state explicitly that sparse "
+                "input is not supported if this is not the case."
+            )
+        with raises(
+            (TypeError, ValueError),
+            match=["sparse", "Sparse"],
+            may_pass=True,
+            err_msg=err_msg,
+        ):
+            with ignore_warnings(category=FutureWarning):
                 estimator.fit(X, y)
             if hasattr(estimator, "predict"):
                 pred = estimator.predict(X)
-                if _safe_tags(estimator, "multioutput_only"):
-                    assert_equal(pred.shape, (X.shape[0], 1))
+                if tags["multioutput_only"]:
+                    assert pred.shape == (X.shape[0], 1)
                 else:
-                    assert_equal(pred.shape, (X.shape[0],))
-            if hasattr(estimator, 'predict_proba'):
+                    assert pred.shape == (X.shape[0],)
+            if hasattr(estimator, "predict_proba"):
                 probs = estimator.predict_proba(X)
-                assert_equal(probs.shape, (X.shape[0], 4))
-        except (TypeError, ValueError) as e:
-            if 'sparse' not in repr(e).lower():
-                if "64" in matrix_format:
-                    msg = ("Estimator %s doesn't seem to support %s matrix, "
-                           "and is not failing gracefully, e.g. by using "
-                           "check_array(X, accept_large_sparse=False)")
-                    raise AssertionError(msg % (name, matrix_format))
+                if tags["binary_only"]:
+                    expected_probs_shape = (X.shape[0], 2)
                 else:
-                    print("Estimator %s doesn't seem to fail gracefully on "
-                          "sparse data: error message state explicitly that "
-                          "sparse input is not supported if this is not"
-                          " the case." % name)
-                    raise
-        except Exception:
-            print("Estimator %s doesn't seem to fail gracefully on "
-                  "sparse data: it should raise a TypeError if sparse input "
-                  "is explicitly not supported." % name)
-            raise
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+                    expected_probs_shape = (X.shape[0], 4)
+                assert probs.shape == expected_probs_shape
+
+
+@ignore_warnings(category=FutureWarning)
 def check_sample_weights_pandas_series(name, estimator_orig):
     # check that estimators will accept a 'sample_weight' parameter of
     # type pandas.Series in the 'fit' function.
     estimator = clone(estimator_orig)
-    if has_fit_parameter(estimator, "sample_weight"):
+    try:
+        import pandas as pd
+
+        X = np.array(
+            [
+                [1, 1],
+                [1, 2],
+                [1, 3],
+                [1, 4],
+                [2, 1],
+                [2, 2],
+                [2, 3],
+                [2, 4],
+                [3, 1],
+                [3, 2],
+                [3, 3],
+                [3, 4],
+            ]
+        )
+        X = pd.DataFrame(_pairwise_estimator_convert_X(X, estimator_orig))
+        y = pd.Series([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2])
+        weights = pd.Series([1] * 12)
+        if _safe_tags(estimator, key="multioutput_only"):
+            y = pd.DataFrame(y)
         try:
-            import pandas as pd
-            X = np.array([[1, 1], [1, 2], [1, 3], [1, 4],
-                          [2, 1], [2, 2], [2, 3], [2, 4]])
-            X = pd.DataFrame(pairwise_estimator_convert_X(X, estimator_orig))
-            y = pd.Series([1, 1, 1, 1, 2, 2, 2, 2])
-            weights = pd.Series([1] * 8)
-            if _safe_tags(estimator, "multioutput_only"):
-                y = pd.DataFrame(y)
-            try:
-                estimator.fit(X, y, sample_weight=weights)
-            except ValueError:
-                raise ValueError("Estimator {0} raises error if "
-                                 "'sample_weight' parameter is of "
-                                 "type pandas.Series".format(name))
-        except ImportError:
-            raise SkipTest("pandas is not installed: not testing for "
-                           "input of type pandas.Series to class weight.")
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+            estimator.fit(X, y, sample_weight=weights)
+        except ValueError:
+            raise ValueError(
+                "Estimator {0} raises error if "
+                "'sample_weight' parameter is of "
+                "type pandas.Series".format(name)
+            )
+    except ImportError:
+        raise SkipTest(
+            "pandas is not installed: not testing for "
+            "input of type pandas.Series to class weight."
+        )
+
+
+@ignore_warnings(category=(FutureWarning))
+def check_sample_weights_not_an_array(name, estimator_orig):
+    # check that estimators will accept a 'sample_weight' parameter of
+    # type _NotAnArray in the 'fit' function.
+    estimator = clone(estimator_orig)
+    X = np.array(
+        [
+            [1, 1],
+            [1, 2],
+            [1, 3],
+            [1, 4],
+            [2, 1],
+            [2, 2],
+            [2, 3],
+            [2, 4],
+            [3, 1],
+            [3, 2],
+            [3, 3],
+            [3, 4],
+        ]
+    )
+    X = _NotAnArray(_pairwise_estimator_convert_X(X, estimator_orig))
+    y = _NotAnArray([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2])
+    weights = _NotAnArray([1] * 12)
+    if _safe_tags(estimator, key="multioutput_only"):
+        y = _NotAnArray(y.data.reshape(-1, 1))
+    estimator.fit(X, y, sample_weight=weights)
+
+
+@ignore_warnings(category=(FutureWarning))
 def check_sample_weights_list(name, estimator_orig):
     # check that estimators will accept a 'sample_weight' parameter of
     # type list in the 'fit' function.
-    if has_fit_parameter(estimator_orig, "sample_weight"):
-        estimator = clone(estimator_orig)
-        rnd = np.random.RandomState(0)
-        X = pairwise_estimator_convert_X(rnd.uniform(size=(10, 3)),
-                                         estimator_orig)
-        y = np.arange(10) % 3
-        y = multioutput_estimator_convert_y_2d(estimator, y)
-        sample_weight = [3] * 10
-        # Test that estimators don't raise any exception
-        estimator.fit(X, y, sample_weight=sample_weight)
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
-def check_sample_weights_invariance(name, estimator_orig):
-    # check that the estimators yield same results for
+    estimator = clone(estimator_orig)
+    rnd = np.random.RandomState(0)
+    n_samples = 30
+    X = _pairwise_estimator_convert_X(rnd.uniform(size=(n_samples, 3)), estimator_orig)
+    y = np.arange(n_samples) % 3
+    y = _enforce_estimator_tags_y(estimator, y)
+    sample_weight = [3] * n_samples
+    # Test that estimators don't raise any exception
+    estimator.fit(X, y, sample_weight=sample_weight)
+
+
+@ignore_warnings(category=FutureWarning)
+def check_sample_weights_shape(name, estimator_orig):
+    # check that estimators raise an error if sample_weight
+    # shape mismatches the input
+    estimator = clone(estimator_orig)
+    X = np.array(
+        [
+            [1, 3],
+            [1, 3],
+            [1, 3],
+            [1, 3],
+            [2, 1],
+            [2, 1],
+            [2, 1],
+            [2, 1],
+            [3, 3],
+            [3, 3],
+            [3, 3],
+            [3, 3],
+            [4, 1],
+            [4, 1],
+            [4, 1],
+            [4, 1],
+        ]
+    )
+    y = np.array([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2])
+    y = _enforce_estimator_tags_y(estimator, y)
+
+    estimator.fit(X, y, sample_weight=np.ones(len(y)))
+
+    with raises(ValueError):
+        estimator.fit(X, y, sample_weight=np.ones(2 * len(y)))
+
+    with raises(ValueError):
+        estimator.fit(X, y, sample_weight=np.ones((len(y), 2)))
+
+
+@ignore_warnings(category=FutureWarning)
+def check_sample_weights_invariance(name, estimator_orig, kind="ones"):
+    # For kind="ones" check that the estimators yield same results for
     # unit weights and no weights
-    if (has_fit_parameter(estimator_orig, "sample_weight") and
-            not (hasattr(estimator_orig, "_pairwise")
-                 and estimator_orig._pairwise)):
-        # We skip pairwise because the data is not pairwise
-
-        estimator1 = clone(estimator_orig)
-        estimator2 = clone(estimator_orig)
-        set_random_state(estimator1, random_state=0)
-        set_random_state(estimator2, random_state=0)
-
-        X = np.array([[1, 3], [1, 3], [1, 3], [1, 3],
-                      [2, 1], [2, 1], [2, 1], [2, 1],
-                      [3, 3], [3, 3], [3, 3], [3, 3],
-                      [4, 1], [4, 1], [4, 1], [4, 1]], dtype=np.dtype('float'))
-        y = np.array([1, 1, 1, 1, 2, 2, 2, 2,
-                      1, 1, 1, 1, 2, 2, 2, 2], dtype=np.dtype('int'))
-        y = multioutput_estimator_convert_y_2d(estimator1, y)
-
-        estimator1.fit(X, y=y, sample_weight=np.ones(shape=len(y)))
-        estimator2.fit(X, y=y, sample_weight=None)
-
-        for method in ["predict", "transform"]:
-            if hasattr(estimator_orig, method):
-                X_pred1 = getattr(estimator1, method)(X)
-                X_pred2 = getattr(estimator2, method)(X)
-                if sparse.issparse(X_pred1):
-                    X_pred1 = X_pred1.toarray()
-                    X_pred2 = X_pred2.toarray()
-                assert_allclose(X_pred1, X_pred2,
-                                err_msg="For %s sample_weight=None is not"
-                                        " equivalent to sample_weight=ones"
-                                        % name)
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning, UserWarning))
+    # For kind="zeros" check that setting sample_weight to 0 is equivalent
+    # to removing corresponding samples.
+    estimator1 = clone(estimator_orig)
+    estimator2 = clone(estimator_orig)
+    set_random_state(estimator1, random_state=0)
+    set_random_state(estimator2, random_state=0)
+
+    X1 = np.array(
+        [
+            [1, 3],
+            [1, 3],
+            [1, 3],
+            [1, 3],
+            [2, 1],
+            [2, 1],
+            [2, 1],
+            [2, 1],
+            [3, 3],
+            [3, 3],
+            [3, 3],
+            [3, 3],
+            [4, 1],
+            [4, 1],
+            [4, 1],
+            [4, 1],
+        ],
+        dtype=np.float64,
+    )
+    y1 = np.array([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2], dtype=int)
+
+    if kind == "ones":
+        X2 = X1
+        y2 = y1
+        sw2 = np.ones(shape=len(y1))
+        err_msg = (
+            f"For {name} sample_weight=None is not equivalent to sample_weight=ones"
+        )
+    elif kind == "zeros":
+        # Construct a dataset that is very different to (X, y) if weights
+        # are disregarded, but identical to (X, y) given weights.
+        X2 = np.vstack([X1, X1 + 1])
+        y2 = np.hstack([y1, 3 - y1])
+        sw2 = np.ones(shape=len(y1) * 2)
+        sw2[len(y1) :] = 0
+        X2, y2, sw2 = shuffle(X2, y2, sw2, random_state=0)
+
+        err_msg = (
+            f"For {name}, a zero sample_weight is not equivalent to removing the sample"
+        )
+    else:  # pragma: no cover
+        raise ValueError
+
+    y1 = _enforce_estimator_tags_y(estimator1, y1)
+    y2 = _enforce_estimator_tags_y(estimator2, y2)
+
+    estimator1.fit(X1, y=y1, sample_weight=None)
+    estimator2.fit(X2, y=y2, sample_weight=sw2)
+
+    for method in ["predict", "predict_proba", "decision_function", "transform"]:
+        if hasattr(estimator_orig, method):
+            X_pred1 = getattr(estimator1, method)(X1)
+            X_pred2 = getattr(estimator2, method)(X1)
+            assert_allclose_dense_sparse(X_pred1, X_pred2, err_msg=err_msg)
+
+
+def check_sample_weights_not_overwritten(name, estimator_orig):
+    # check that estimators don't override the passed sample_weight parameter
+    estimator = clone(estimator_orig)
+    set_random_state(estimator, random_state=0)
+
+    X = np.array(
+        [
+            [1, 3],
+            [1, 3],
+            [1, 3],
+            [1, 3],
+            [2, 1],
+            [2, 1],
+            [2, 1],
+            [2, 1],
+            [3, 3],
+            [3, 3],
+            [3, 3],
+            [3, 3],
+            [4, 1],
+            [4, 1],
+            [4, 1],
+            [4, 1],
+        ],
+        dtype=np.float64,
+    )
+    y = np.array([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2], dtype=int)
+    y = _enforce_estimator_tags_y(estimator, y)
+
+    sample_weight_original = np.ones(y.shape[0])
+    sample_weight_original[0] = 10.0
+
+    sample_weight_fit = sample_weight_original.copy()
+
+    estimator.fit(X, y, sample_weight=sample_weight_fit)
+
+    err_msg = f"{name} overwrote the original `sample_weight` given during fit"
+    assert_allclose(sample_weight_fit, sample_weight_original, err_msg=err_msg)
+
+
+@ignore_warnings(category=(FutureWarning, UserWarning))
 def check_dtype_object(name, estimator_orig):
     # check that estimators treat dtype object as numeric if possible
     rng = np.random.RandomState(0)
-    X = pairwise_estimator_convert_X(rng.rand(40, 10), estimator_orig)
+    X = _pairwise_estimator_convert_X(rng.uniform(size=(40, 10)), estimator_orig)
     X = X.astype(object)
-    y = (X[:, 0] * 4).astype(np.int)
+    tags = _safe_tags(estimator_orig)
+    y = (X[:, 0] * 4).astype(int)
     estimator = clone(estimator_orig)
-    y = multioutput_estimator_convert_y_2d(estimator, y)
+    y = _enforce_estimator_tags_y(estimator, y)

     estimator.fit(X, y)
     if hasattr(estimator, "predict"):
@@ -644,17 +1132,14 @@
     if hasattr(estimator, "transform"):
         estimator.transform(X)

-    try:
+    with raises(Exception, match="Unknown label type", may_pass=True):
         estimator.fit(X, y.astype(object))
-    except Exception as e:
-        if "Unknown label type" not in str(e):
-            raise
-
-    tags = _safe_tags(estimator)
-    if 'str' not in tags['X_types']:
-        X[0, 0] = {'foo': 'bar'}
+
+    if "string" not in tags["X_types"]:
+        X[0, 0] = {"foo": "bar"}
         msg = "argument must be a string.* number"
-        assert_raises_regex(TypeError, msg, estimator.fit, X, y)
+        with raises(TypeError, match=msg):
+            estimator.fit(X, y)
     else:
         # Estimators supporting string will not call np.asarray to convert the
         # data to numeric and therefore, the error will not be raised.
@@ -664,13 +1149,17 @@


 def check_complex_data(name, estimator_orig):
+    rng = np.random.RandomState(42)
     # check that estimators raise an exception on providing complex data
-    X = np.random.sample(10) + 1j * np.random.sample(10)
+    X = rng.uniform(size=10) + 1j * rng.uniform(size=10)
     X = X.reshape(-1, 1)
-    y = np.random.sample(10) + 1j * np.random.sample(10)
+
+    # Something both valid for classification and regression
+    y = rng.randint(low=0, high=2, size=10) + 1j
     estimator = clone(estimator_orig)
-    assert_raises_regex(ValueError, "Complex data not supported",
-                        estimator.fit, X, y)
+    set_random_state(estimator, random_state=0)
+    with raises(ValueError, match="Complex data not supported"):
+        estimator.fit(X, y)


 @ignore_warnings
@@ -679,19 +1168,19 @@
     # ValueError: Found array with 0 feature(s) (shape=(23, 0))
     # while a minimum of 1 is required.
     # error
-    if name in ['SpectralCoclustering']:
+    if name in ["SpectralCoclustering"]:
         return
     rnd = np.random.RandomState(0)
-    if name in ['RANSACRegressor']:
+    if name in ["RANSACRegressor"]:
         X = 3 * rnd.uniform(size=(20, 3))
     else:
         X = 2 * rnd.uniform(size=(20, 3))

-    X = pairwise_estimator_convert_X(X, estimator_orig)
-
-    y = X[:, 0].astype(np.int)
+    X = _pairwise_estimator_convert_X(X, estimator_orig)
+
+    y = X[:, 0].astype(int)
     estimator = clone(estimator_orig)
-    y = multioutput_estimator_convert_y_2d(estimator, y)
+    y = _enforce_estimator_tags_y(estimator, y)
     if hasattr(estimator, "n_components"):
         estimator.n_components = 1

@@ -704,20 +1193,20 @@
     set_random_state(estimator, 1)

     estimator.fit(X, y)
-    for method in ["predict", "transform", "decision_function",
-                   "predict_proba"]:
+    for method in ["predict", "transform", "decision_function", "predict_proba"]:
         if hasattr(estimator, method):
             dict_before = estimator.__dict__.copy()
             getattr(estimator, method)(X)
-            assert_dict_equal(estimator.__dict__, dict_before,
-                              'Estimator changes __dict__ during %s' % method)
-
-
-def is_public_parameter(attr):
-    return not (attr.startswith('_') or attr.endswith('_'))
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+            assert estimator.__dict__ == dict_before, (
+                "Estimator changes __dict__ during %s" % method
+            )
+
+
+def _is_public_parameter(attr):
+    return not (attr.startswith("_") or attr.endswith("_"))
+
+
+@ignore_warnings(category=FutureWarning)
 def check_dont_overwrite_parameters(name, estimator_orig):
     # check that fit method only changes or sets private attributes
     if hasattr(estimator_orig.__init__, "deprecated_original"):
@@ -726,9 +1215,9 @@
     estimator = clone(estimator_orig)
     rnd = np.random.RandomState(0)
     X = 3 * rnd.uniform(size=(20, 3))
-    X = pairwise_estimator_convert_X(X, estimator_orig)
-    y = X[:, 0].astype(np.int)
-    y = multioutput_estimator_convert_y_2d(estimator, y)
+    X = _pairwise_estimator_convert_X(X, estimator_orig)
+    y = X[:, 0].astype(int)
+    y = _enforce_estimator_tags_y(estimator, y)

     if hasattr(estimator, "n_components"):
         estimator.n_components = 1
@@ -741,44 +1230,50 @@

     dict_after_fit = estimator.__dict__

-    public_keys_after_fit = [key for key in dict_after_fit.keys()
-                             if is_public_parameter(key)]
-
-    attrs_added_by_fit = [key for key in public_keys_after_fit
-                          if key not in dict_before_fit.keys()]
+    public_keys_after_fit = [
+        key for key in dict_after_fit.keys() if _is_public_parameter(key)
+    ]
+
+    attrs_added_by_fit = [
+        key for key in public_keys_after_fit if key not in dict_before_fit.keys()
+    ]

     # check that fit doesn't add any public attribute
     assert not attrs_added_by_fit, (
-            'Estimator adds public attribute(s) during'
-            ' the fit method.'
-            ' Estimators are only allowed to add private attributes'
-            ' either started with _ or ended'
-            ' with _ but %s added'
-            % ', '.join(attrs_added_by_fit))
+        "Estimator adds public attribute(s) during"
+        " the fit method."
+        " Estimators are only allowed to add private attributes"
+        " either started with _ or ended"
+        " with _ but %s added"
+        % ", ".join(attrs_added_by_fit)
+    )

     # check that fit doesn't change any public attribute
-    attrs_changed_by_fit = [key for key in public_keys_after_fit
-                            if (dict_before_fit[key]
-                                is not dict_after_fit[key])]
+    attrs_changed_by_fit = [
+        key
+        for key in public_keys_after_fit
+        if (dict_before_fit[key] is not dict_after_fit[key])
+    ]

     assert not attrs_changed_by_fit, (
-            'Estimator changes public attribute(s) during'
-            ' the fit method. Estimators are only allowed'
-            ' to change attributes started'
-            ' or ended with _, but'
-            ' %s changed'
-            % ', '.join(attrs_changed_by_fit))
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+        "Estimator changes public attribute(s) during"
+        " the fit method. Estimators are only allowed"
+        " to change attributes started"
+        " or ended with _, but"
+        " %s changed"
+        % ", ".join(attrs_changed_by_fit)
+    )
+
+
+@ignore_warnings(category=FutureWarning)
 def check_fit2d_predict1d(name, estimator_orig):
     # check by fitting a 2d array and predicting with a 1d array
     rnd = np.random.RandomState(0)
     X = 3 * rnd.uniform(size=(20, 3))
-    X = pairwise_estimator_convert_X(X, estimator_orig)
-    y = X[:, 0].astype(np.int)
+    X = _pairwise_estimator_convert_X(X, estimator_orig)
+    y = X[:, 0].astype(int)
     estimator = clone(estimator_orig)
-    y = multioutput_estimator_convert_y_2d(estimator, y)
+    y = _enforce_estimator_tags_y(estimator, y)

     if hasattr(estimator, "n_components"):
         estimator.n_components = 1
@@ -787,24 +1282,20 @@

     set_random_state(estimator, 1)
     estimator.fit(X, y)
-    tags = _safe_tags(estimator)
-    if tags["no_validation"]:
-        # FIXME this is a bit loose
-        return
-
-    for method in ["predict", "transform", "decision_function",
-                   "predict_proba"]:
+
+    for method in ["predict", "transform", "decision_function", "predict_proba"]:
         if hasattr(estimator, method):
-            assert_raise_message(ValueError, "Reshape your data",
-                                 getattr(estimator, method), X[0])
+            assert_raise_message(
+                ValueError, "Reshape your data", getattr(estimator, method), X[0]
+            )


 def _apply_on_subsets(func, X):
     # apply function on the whole set and on mini batches
     result_full = func(X)
     n_features = X.shape[1]
-    result_by_batch = [func(batch.reshape(1, n_features))
-                       for batch in X]
+    result_by_batch = [func(batch.reshape(1, n_features)) for batch in X]
+
     # func can output tuple (e.g. score_samples)
     if type(result_full) == tuple:
         result_full = result_full[0]
@@ -813,19 +1304,20 @@
     if sparse.issparse(result_full):
         result_full = result_full.A
         result_by_batch = [x.A for x in result_by_batch]
+
     return np.ravel(result_full), np.ravel(result_by_batch)


-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+@ignore_warnings(category=FutureWarning)
 def check_methods_subset_invariance(name, estimator_orig):
     # check that method gives invariant results if applied
-    # on mini bathes or the whole set
+    # on mini batches or the whole set
     rnd = np.random.RandomState(0)
     X = 3 * rnd.uniform(size=(20, 3))
-    X = pairwise_estimator_convert_X(X, estimator_orig)
-    y = X[:, 0].astype(np.int)
+    X = _pairwise_estimator_convert_X(X, estimator_orig)
+    y = X[:, 0].astype(int)
     estimator = clone(estimator_orig)
-    y = multioutput_estimator_convert_y_2d(estimator, y)
+    y = _enforce_estimator_tags_y(estimator, y)

     if hasattr(estimator, "n_components"):
         estimator.n_components = 1
@@ -835,24 +1327,67 @@
     set_random_state(estimator, 1)
     estimator.fit(X, y)

-    for method in ["predict", "transform", "decision_function",
-                   "score_samples", "predict_proba"]:
-
-        msg = ("{method} of {name} is not invariant when applied "
-               "to a subset.").format(method=method, name=name)
-        # TODO remove cases when corrected
-        if (name, method) in [('NuSVC', 'decision_function'),
-                              ('SparsePCA', 'transform'),
-                              ('MiniBatchSparsePCA', 'transform'),
-                              ('DummyClassifier', 'predict'),
-                              ('BernoulliRBM', 'score_samples')]:
-            raise SkipTest(msg)
+    for method in [
+        "predict",
+        "transform",
+        "decision_function",
+        "score_samples",
+        "predict_proba",
+    ]:
+
+        msg = ("{method} of {name} is not invariant when applied to a subset.").format(
+            method=method, name=name
+        )

         if hasattr(estimator, method):
             result_full, result_by_batch = _apply_on_subsets(
-                getattr(estimator, method), X)
-            assert_allclose(result_full, result_by_batch,
-                            atol=1e-7, err_msg=msg)
+                getattr(estimator, method), X
+            )
+            assert_allclose(result_full, result_by_batch, atol=1e-7, err_msg=msg)
+
+
+@ignore_warnings(category=FutureWarning)
+def check_methods_sample_order_invariance(name, estimator_orig):
+    # check that method gives invariant results if applied
+    # on a subset with different sample order
+    rnd = np.random.RandomState(0)
+    X = 3 * rnd.uniform(size=(20, 3))
+    X = _pairwise_estimator_convert_X(X, estimator_orig)
+    y = X[:, 0].astype(np.int64)
+    if _safe_tags(estimator_orig, key="binary_only"):
+        y[y == 2] = 1
+    estimator = clone(estimator_orig)
+    y = _enforce_estimator_tags_y(estimator, y)
+
+    if hasattr(estimator, "n_components"):
+        estimator.n_components = 1
+    if hasattr(estimator, "n_clusters"):
+        estimator.n_clusters = 2
+
+    set_random_state(estimator, 1)
+    estimator.fit(X, y)
+
+    idx = np.random.permutation(X.shape[0])
+
+    for method in [
+        "predict",
+        "transform",
+        "decision_function",
+        "score_samples",
+        "predict_proba",
+    ]:
+        msg = (
+            "{method} of {name} is not invariant when applied to a dataset"
+            "with different sample order."
+        ).format(method=method, name=name)
+
+        if hasattr(estimator, method):
+            assert_allclose_dense_sparse(
+                getattr(estimator, method)(X)[idx],
+                getattr(estimator, method)(X[idx]),
+                atol=1e-9,
+                err_msg=msg,
+            )


 @ignore_warnings
@@ -862,9 +1397,11 @@
     # the number of samples or the number of classes.
     rnd = np.random.RandomState(0)
     X = 3 * rnd.uniform(size=(1, 10))
-    y = X[:, 0].astype(np.int)
+    X = _pairwise_estimator_convert_X(X, estimator_orig)
+
+    y = X[:, 0].astype(int)
     estimator = clone(estimator_orig)
-    y = multioutput_estimator_convert_y_2d(estimator, y)
+    y = _enforce_estimator_tags_y(estimator, y)

     if hasattr(estimator, "n_components"):
         estimator.n_components = 1
@@ -874,17 +1411,20 @@
     set_random_state(estimator, 1)

     # min_cluster_size cannot be less than the data size for OPTICS.
-    if name == 'OPTICS':
+    if name == "OPTICS":
         estimator.set_params(min_samples=1)

-    msgs = ["1 sample", "n_samples = 1", "n_samples=1", "one sample",
-            "1 class", "one class"]
-
-    try:
+    msgs = [
+        "1 sample",
+        "n_samples = 1",
+        "n_samples=1",
+        "one sample",
+        "1 class",
+        "one class",
+    ]
+
+    with raises(ValueError, match=msgs, may_pass=True):
         estimator.fit(X, y)
-    except ValueError as e:
-        if all(msg not in repr(e) for msg in msgs):
-            raise e


 @ignore_warnings
@@ -893,32 +1433,29 @@
     # informative message
     rnd = np.random.RandomState(0)
     X = 3 * rnd.uniform(size=(10, 1))
-    X = pairwise_estimator_convert_X(X, estimator_orig)
-    y = X[:, 0].astype(np.int)
+    X = _pairwise_estimator_convert_X(X, estimator_orig)
+    y = X[:, 0].astype(int)
     estimator = clone(estimator_orig)
-    y = multioutput_estimator_convert_y_2d(estimator, y)
+    y = _enforce_estimator_tags_y(estimator, y)

     if hasattr(estimator, "n_components"):
         estimator.n_components = 1
     if hasattr(estimator, "n_clusters"):
         estimator.n_clusters = 1
     # ensure two labels in subsample for RandomizedLogisticRegression
-    if name == 'RandomizedLogisticRegression':
+    if name == "RandomizedLogisticRegression":
         estimator.sample_fraction = 1
     # ensure non skipped trials for RANSACRegressor
-    if name == 'RANSACRegressor':
+    if name == "RANSACRegressor":
         estimator.residual_threshold = 0.5

-    y = multioutput_estimator_convert_y_2d(estimator, y)
+    y = _enforce_estimator_tags_y(estimator, y)
     set_random_state(estimator, 1)

-    msgs = ["1 feature(s)", "n_features = 1", "n_features=1"]
-
-    try:
+    msgs = [r"1 feature\(s\)", "n_features = 1", "n_features=1"]
+
+    with raises(ValueError, match=msgs, may_pass=True):
         estimator.fit(X, y)
-    except ValueError as e:
-        if all(msg not in repr(e) for msg in msgs):
-            raise e


 @ignore_warnings
@@ -926,13 +1463,9 @@
     # check fitting 1d X array raises a ValueError
     rnd = np.random.RandomState(0)
     X = 3 * rnd.uniform(size=(20))
-    y = X.astype(np.int)
+    y = X.astype(int)
     estimator = clone(estimator_orig)
-    tags = _safe_tags(estimator)
-    if tags["no_validation"]:
-        # FIXME this is a bit loose
-        return
-    y = multioutput_estimator_convert_y_2d(estimator, y)
+    y = _enforce_estimator_tags_y(estimator, y)

     if hasattr(estimator, "n_components"):
         estimator.n_components = 1
@@ -940,45 +1473,64 @@
         estimator.n_clusters = 1

     set_random_state(estimator, 1)
-    assert_raises(ValueError, estimator.fit, X, y)
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+    with raises(ValueError):
+        estimator.fit(X, y)
+
+
+@ignore_warnings(category=FutureWarning)
 def check_transformer_general(name, transformer, readonly_memmap=False):
-    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],
-                      random_state=0, n_features=2, cluster_std=0.1)
+    X, y = make_blobs(
+        n_samples=30,
+        centers=[[0, 0, 0], [1, 1, 1]],
+        random_state=0,
+        n_features=2,
+        cluster_std=0.1,
+    )
     X = StandardScaler().fit_transform(X)
     X -= X.min()
+    X = _pairwise_estimator_convert_X(X, transformer)

     if readonly_memmap:
         X, y = create_memmap_backed_data([X, y])

     _check_transformer(name, transformer, X, y)
-    _check_transformer(name, transformer, X.tolist(), y.tolist())
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+
+
+@ignore_warnings(category=FutureWarning)
 def check_transformer_data_not_an_array(name, transformer):
-    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],
-                      random_state=0, n_features=2, cluster_std=0.1)
+    X, y = make_blobs(
+        n_samples=30,
+        centers=[[0, 0, 0], [1, 1, 1]],
+        random_state=0,
+        n_features=2,
+        cluster_std=0.1,
+    )
     X = StandardScaler().fit_transform(X)
     # We need to make sure that we have non negative data, for things
     # like NMF
-    X -= X.min() - .1
-    this_X = NotAnArray(X)
-    this_y = NotAnArray(np.asarray(y))
+    X -= X.min() - 0.1
+    X = _pairwise_estimator_convert_X(X, transformer)
+    this_X = _NotAnArray(X)
+    this_y = _NotAnArray(np.asarray(y))
     _check_transformer(name, transformer, this_X, this_y)
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+    # try the same with some list
+    _check_transformer(name, transformer, X.tolist(), y.tolist())
+
+
+@ignore_warnings(category=FutureWarning)
 def check_transformers_unfitted(name, transformer):
-    X, y = _boston_subset()
+    X, y = _regression_dataset()

     transformer = clone(transformer)
-    with assert_raises((AttributeError, ValueError), msg="The unfitted "
-                       "transformer {} does not raise an error when "
-                       "transform is called. Perhaps use "
-                       "check_is_fitted in transform.".format(name)):
+    with raises(
+        (AttributeError, ValueError),
+        err_msg=(
+            "The unfitted "
+            f"transformer {name} does not raise an error when "
+            "transform is called. Perhaps use "
+            "check_is_fitted in transform."
+        ),
+    ):
         transformer.transform(X)


@@ -990,8 +1542,10 @@
     # fit

     if name in CROSS_DECOMPOSITION:
-        y_ = np.c_[y, y]
+        y_ = np.c_[np.asarray(y), np.asarray(y)]
         y_[::2, 1] *= 2
+        if isinstance(X, _NotAnArray):
+            y_ = _NotAnArray(y_)
     else:
         y_ = y

@@ -1002,12 +1556,12 @@

     if isinstance(X_pred, tuple):
         for x_pred in X_pred:
-            assert_equal(x_pred.shape[0], n_samples)
+            assert x_pred.shape[0] == n_samples
     else:
         # check for consistent n_samples
-        assert_equal(X_pred.shape[0], n_samples)
-
-    if hasattr(transformer, 'transform'):
+        assert X_pred.shape[0] == n_samples
+
+    if hasattr(transformer, "transform"):
         if name in CROSS_DECOMPOSITION:
             X_pred2 = transformer.transform(X, y_)
             X_pred3 = transformer.fit_transform(X, y=y_)
@@ -1015,59 +1569,81 @@
             X_pred2 = transformer.transform(X)
             X_pred3 = transformer.fit_transform(X, y=y_)

-        if _safe_tags(transformer_orig, 'non_deterministic'):
-            msg = name + ' is non deterministic'
+        if _safe_tags(transformer_orig, key="non_deterministic"):
+            msg = name + " is non deterministic"
             raise SkipTest(msg)
         if isinstance(X_pred, tuple) and isinstance(X_pred2, tuple):
             for x_pred, x_pred2, x_pred3 in zip(X_pred, X_pred2, X_pred3):
                 assert_allclose_dense_sparse(
-                    x_pred, x_pred2, atol=1e-2,
-                    err_msg="fit_transform and transform outcomes "
-                            "not consistent in %s"
-                    % transformer)
+                    x_pred,
+                    x_pred2,
+                    atol=1e-2,
+                    err_msg="fit_transform and transform outcomes not consistent in %s"
+                    % transformer,
+                )
                 assert_allclose_dense_sparse(
-                    x_pred, x_pred3, atol=1e-2,
-                    err_msg="consecutive fit_transform outcomes "
-                            "not consistent in %s"
-                    % transformer)
+                    x_pred,
+                    x_pred3,
+                    atol=1e-2,
+                    err_msg="consecutive fit_transform outcomes not consistent in %s"
+                    % transformer,
+                )
         else:
             assert_allclose_dense_sparse(
-                X_pred, X_pred2,
-                err_msg="fit_transform and transform outcomes "
-                        "not consistent in %s"
-                % transformer, atol=1e-2)
+                X_pred,
+                X_pred2,
+                err_msg="fit_transform and transform outcomes not consistent in %s"
+                % transformer,
+                atol=1e-2,
+            )
             assert_allclose_dense_sparse(
-                X_pred, X_pred3, atol=1e-2,
-                err_msg="consecutive fit_transform outcomes "
-                        "not consistent in %s"
-                % transformer)
-            assert_equal(_num_samples(X_pred2), n_samples)
-            assert_equal(_num_samples(X_pred3), n_samples)
+                X_pred,
+                X_pred3,
+                atol=1e-2,
+                err_msg="consecutive fit_transform outcomes not consistent in %s"
+                % transformer,
+            )
+            assert _num_samples(X_pred2) == n_samples
+            assert _num_samples(X_pred3) == n_samples

         # raises error on malformed input for transform
-        if hasattr(X, 'T') and not _safe_tags(transformer, "stateless"):
+        if (
+            hasattr(X, "shape")
+            and not _safe_tags(transformer, key="stateless")
+            and X.ndim == 2
+            and X.shape[1] > 1
+        ):
+
             # If it's not an array, it does not have a 'T' property
-            with assert_raises(ValueError, msg="The transformer {} does "
-                               "not raise an error when the number of "
-                               "features in transform is different from"
-                               " the number of features in "
-                               "fit.".format(name)):
-                transformer.transform(X.T)
+            with raises(
+                ValueError,
+                err_msg=(
+                    f"The transformer {name} does not raise an error "
+                    "when the number of features in transform is different from "
+                    "the number of features in fit."
+                ),
+            ):
+                transformer.transform(X[:, :-1])


 @ignore_warnings
 def check_pipeline_consistency(name, estimator_orig):
-    if _safe_tags(estimator_orig, 'non_deterministic'):
-        msg = name + ' is non deterministic'
+    if _safe_tags(estimator_orig, key="non_deterministic"):
+        msg = name + " is non deterministic"
         raise SkipTest(msg)

     # check that make_pipeline(est) gives same score as est
-    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],
-                      random_state=0, n_features=2, cluster_std=0.1)
+    X, y = make_blobs(
+        n_samples=30,
+        centers=[[0, 0, 0], [1, 1, 1]],
+        random_state=0,
+        n_features=2,
+        cluster_std=0.1,
+    )
     X -= X.min()
-    X = pairwise_estimator_convert_X(X, estimator_orig, kernel=rbf_kernel)
+    X = _pairwise_estimator_convert_X(X, estimator_orig, kernel=rbf_kernel)
     estimator = clone(estimator_orig)
-    y = multioutput_estimator_convert_y_2d(estimator, y)
+    y = _enforce_estimator_tags_y(estimator, y)
     set_random_state(estimator)
     pipeline = make_pipeline(estimator)
     estimator.fit(X, y)
@@ -1089,11 +1665,12 @@
     # check that all estimators accept an optional y
     # in fit and score so they can be used in pipelines
     rnd = np.random.RandomState(0)
-    X = rnd.uniform(size=(10, 3))
-    X = pairwise_estimator_convert_X(X, estimator_orig)
-    y = np.arange(10) % 3
+    n_samples = 30
+    X = rnd.uniform(size=(n_samples, 3))
+    X = _pairwise_estimator_convert_X(X, estimator_orig)
+    y = np.arange(n_samples) % 3
     estimator = clone(estimator_orig)
-    y = multioutput_estimator_convert_y_2d(estimator, y)
+    y = _enforce_estimator_tags_y(estimator, y)
     set_random_state(estimator)

     funcs = ["fit", "score", "partial_fit", "fit_predict", "fit_transform"]
@@ -1107,21 +1684,22 @@
                 # with an explicit "self", so need to shift arguments
                 args = args[1:]
             assert args[1] in ["y", "Y"], (
-                    "Expected y or Y as second argument for method "
-                    "%s of %s. Got arguments: %r."
-                    % (func_name, type(estimator).__name__, args))
+                "Expected y or Y as second argument for method "
+                "%s of %s. Got arguments: %r."
+                % (func_name, type(estimator).__name__, args)
+            )


 @ignore_warnings
 def check_estimators_dtypes(name, estimator_orig):
     rnd = np.random.RandomState(0)
     X_train_32 = 3 * rnd.uniform(size=(20, 5)).astype(np.float32)
-    X_train_32 = pairwise_estimator_convert_X(X_train_32, estimator_orig)
+    X_train_32 = _pairwise_estimator_convert_X(X_train_32, estimator_orig)
     X_train_64 = X_train_32.astype(np.float64)
     X_train_int_64 = X_train_32.astype(np.int64)
     X_train_int_32 = X_train_32.astype(np.int32)
     y = X_train_int_64[:, 0]
-    y = multioutput_estimator_convert_y_2d(estimator_orig, y)
+    y = _enforce_estimator_tags_y(estimator_orig, y)

     methods = ["predict", "transform", "decision_function", "predict_proba"]

@@ -1135,7 +1713,38 @@
                 getattr(estimator, method)(X_train)


-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+def check_transformer_preserve_dtypes(name, transformer_orig):
+    # check that dtype are preserved meaning if input X is of some dtype
+    # X_transformed should be from the same dtype.
+    X, y = make_blobs(
+        n_samples=30,
+        centers=[[0, 0, 0], [1, 1, 1]],
+        random_state=0,
+        cluster_std=0.1,
+    )
+    X = StandardScaler().fit_transform(X)
+    X -= X.min()
+    X = _pairwise_estimator_convert_X(X, transformer_orig)
+
+    for dtype in _safe_tags(transformer_orig, key="preserves_dtype"):
+        X_cast = X.astype(dtype)
+        transformer = clone(transformer_orig)
+        set_random_state(transformer)
+        X_trans = transformer.fit_transform(X_cast, y)
+
+        if isinstance(X_trans, tuple):
+            # cross-decompostion returns a tuple of (x_scores, y_scores)
+            # when given y with fit_transform; only check the first element
+            X_trans = X_trans[0]
+
+        # check that the output dtype is preserved
+        assert X_trans.dtype == dtype, (
+            f"Estimator transform dtype: {X_trans.dtype} - "
+            f"original/expected dtype: {dtype.__name__}"
+        )
+
+
+@ignore_warnings(category=FutureWarning)
 def check_estimators_empty_data_messages(name, estimator_orig):
     e = clone(estimator_orig)
     set_random_state(e, 1)
@@ -1143,108 +1752,107 @@
     X_zero_samples = np.empty(0).reshape(0, 3)
     # The precise message can change depending on whether X or y is
     # validated first. Let us test the type of exception only:
-    with assert_raises(ValueError, msg="The estimator {} does not"
-                       " raise an error when an empty data is used "
-                       "to train. Perhaps use "
-                       "check_array in train.".format(name)):
+    err_msg = (
+        f"The estimator {name} does not raise a ValueError when an "
+        "empty data is used to train. Perhaps use check_array in train."
+    )
+    with raises(ValueError, err_msg=err_msg):
         e.fit(X_zero_samples, [])

-    X_zero_features = np.empty(0).reshape(3, 0)
+    X_zero_features = np.empty(0).reshape(12, 0)
     # the following y should be accepted by both classifiers and regressors
     # and ignored by unsupervised models
-    y = multioutput_estimator_convert_y_2d(e, np.array([1, 0, 1]))
-    msg = (r"0 feature\(s\) \(shape=\(3, 0\)\) while a minimum of \d* "
-           "is required.")
-    assert_raises_regex(ValueError, msg, e.fit, X_zero_features, y)
-
-
-@ignore_warnings(category=DeprecationWarning)
+    y = _enforce_estimator_tags_y(e, np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]))
+    msg = r"0 feature\(s\) \(shape=\(\d*, 0\)\) while a minimum of \d* " "is required."
+    with raises(ValueError, match=msg):
+        e.fit(X_zero_features, y)
+
+
+@ignore_warnings(category=FutureWarning)
 def check_estimators_nan_inf(name, estimator_orig):
     # Checks that Estimator X's do not contain NaN or inf.
     rnd = np.random.RandomState(0)
-    X_train_finite = pairwise_estimator_convert_X(rnd.uniform(size=(10, 3)),
-                                                  estimator_orig)
+    X_train_finite = _pairwise_estimator_convert_X(
+        rnd.uniform(size=(10, 3)), estimator_orig
+    )
     X_train_nan = rnd.uniform(size=(10, 3))
     X_train_nan[0, 0] = np.nan
     X_train_inf = rnd.uniform(size=(10, 3))
     X_train_inf[0, 0] = np.inf
     y = np.ones(10)
     y[:5] = 0
-    y = multioutput_estimator_convert_y_2d(estimator_orig, y)
-    error_string_fit = "Estimator doesn't check for NaN and inf in fit."
-    error_string_predict = ("Estimator doesn't check for NaN and inf in"
-                            " predict.")
-    error_string_transform = ("Estimator doesn't check for NaN and inf in"
-                              " transform.")
+    y = _enforce_estimator_tags_y(estimator_orig, y)
+    error_string_fit = f"Estimator {name} doesn't check for NaN and inf in fit."
+    error_string_predict = f"Estimator {name} doesn't check for NaN and inf in predict."
+    error_string_transform = (
+        f"Estimator {name} doesn't check for NaN and inf in transform."
+    )
     for X_train in [X_train_nan, X_train_inf]:
         # catch deprecation warnings
-        with ignore_warnings(category=(DeprecationWarning, FutureWarning)):
+        with ignore_warnings(category=FutureWarning):
             estimator = clone(estimator_orig)
             set_random_state(estimator, 1)
             # try to fit
-            try:
+            with raises(ValueError, match=["inf", "NaN"], err_msg=error_string_fit):
                 estimator.fit(X_train, y)
-            except ValueError as e:
-                if 'inf' not in repr(e) and 'NaN' not in repr(e):
-                    print(error_string_fit, estimator, e)
-                    traceback.print_exc(file=sys.stdout)
-                    raise e
-            except Exception as exc:
-                print(error_string_fit, estimator, exc)
-                traceback.print_exc(file=sys.stdout)
-                raise exc
-            else:
-                raise AssertionError(error_string_fit, estimator)
             # actually fit
             estimator.fit(X_train_finite, y)

             # predict
             if hasattr(estimator, "predict"):
-                try:
+                with raises(
+                    ValueError,
+                    match=["inf", "NaN"],
+                    err_msg=error_string_predict,
+                ):
                     estimator.predict(X_train)
-                except ValueError as e:
-                    if 'inf' not in repr(e) and 'NaN' not in repr(e):
-                        print(error_string_predict, estimator, e)
-                        traceback.print_exc(file=sys.stdout)
-                        raise e
-                except Exception as exc:
-                    print(error_string_predict, estimator, exc)
-                    traceback.print_exc(file=sys.stdout)
-                else:
-                    raise AssertionError(error_string_predict, estimator)

             # transform
             if hasattr(estimator, "transform"):
-                try:
+                with raises(
+                    ValueError,
+                    match=["inf", "NaN"],
+                    err_msg=error_string_transform,
+                ):
                     estimator.transform(X_train)
-                except ValueError as e:
-                    if 'inf' not in repr(e) and 'NaN' not in repr(e):
-                        print(error_string_transform, estimator, e)
-                        traceback.print_exc(file=sys.stdout)
-                        raise e
-                except Exception as exc:
-                    print(error_string_transform, estimator, exc)
-                    traceback.print_exc(file=sys.stdout)
-                else:
-                    raise AssertionError(error_string_transform, estimator)
+
+
+@ignore_warnings
+def check_nonsquare_error(name, estimator_orig):
+    """Test that error is thrown when non-square data provided."""
+
+    X, y = make_blobs(n_samples=20, n_features=10)
+    estimator = clone(estimator_orig)
+
+    with raises(
+        ValueError,
+        err_msg=(
+            f"The pairwise estimator {name} does not raise an error on non-square data"
+        ),
+    ):
+        estimator.fit(X, y)


 @ignore_warnings
 def check_estimators_pickle(name, estimator_orig):
-    """Test that we can pickle all estimators"""
-    check_methods = ["predict", "transform", "decision_function",
-                     "predict_proba"]
-
-    X, y = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],
-                      random_state=0, n_features=2, cluster_std=0.1)
+    """Test that we can pickle all estimators."""
+    check_methods = ["predict", "transform", "decision_function", "predict_proba"]
+
+    X, y = make_blobs(
+        n_samples=30,
+        centers=[[0, 0, 0], [1, 1, 1]],
+        random_state=0,
+        n_features=2,
+        cluster_std=0.1,
+    )

     # some estimators can't do features less than 0
     X -= X.min()
-    X = pairwise_estimator_convert_X(X, estimator_orig, kernel=rbf_kernel)
+    X = _pairwise_estimator_convert_X(X, estimator_orig, kernel=rbf_kernel)

     tags = _safe_tags(estimator_orig)
     # include NaN values when the estimator should deal with them
-    if tags['allow_nan']:
+    if tags["allow_nan"]:
         # set randomly 10 elements to np.nan
         rng = np.random.RandomState(42)
         mask = rng.choice(X.size, 10, replace=False)
@@ -1252,41 +1860,41 @@

     estimator = clone(estimator_orig)

-    # some estimators only take multioutputs
-    y = multioutput_estimator_convert_y_2d(estimator, y)
+    y = _enforce_estimator_tags_y(estimator, y)

     set_random_state(estimator)
     estimator.fit(X, y)
+
+    # pickle and unpickle!
+    pickled_estimator = pickle.dumps(estimator)
+    module_name = estimator.__module__
+    if module_name.startswith("sklearn.") and not (
+        "test_" in module_name or module_name.endswith("_testing")
+    ):
+        # strict check for sklearn estimators that are not implemented in test
+        # modules.
+        assert b"version" in pickled_estimator
+    unpickled_estimator = pickle.loads(pickled_estimator)

     result = dict()
     for method in check_methods:
         if hasattr(estimator, method):
             result[method] = getattr(estimator, method)(X)

-    # pickle and unpickle!
-    pickled_estimator = pickle.dumps(estimator)
-    if estimator.__module__.startswith('sklearn.'):
-        assert b"version" in pickled_estimator
-    unpickled_estimator = pickle.loads(pickled_estimator)
-
-    result = dict()
-    for method in check_methods:
-        if hasattr(estimator, method):
-            result[method] = getattr(estimator, method)(X)
-
     for method in result:
         unpickled_result = getattr(unpickled_estimator, method)(X)
         assert_allclose_dense_sparse(result[method], unpickled_result)


-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+@ignore_warnings(category=FutureWarning)
 def check_estimators_partial_fit_n_features(name, estimator_orig):
     # check if number of features changes between calls to partial_fit.
-    if not hasattr(estimator_orig, 'partial_fit'):
+    if not hasattr(estimator_orig, "partial_fit"):
         return
     estimator = clone(estimator_orig)
     X, y = make_blobs(n_samples=50, random_state=1)
     X -= X.min()
+    y = _enforce_estimator_tags_y(estimator_orig, y)

     try:
         if is_classifier(estimator):
@@ -1297,15 +1905,104 @@
     except NotImplementedError:
         return

-    with assert_raises(ValueError,
-                       msg="The estimator {} does not raise an"
-                           " error when the number of features"
-                           " changes between calls to "
-                           "partial_fit.".format(name)):
+    with raises(
+        ValueError,
+        err_msg=(
+            f"The estimator {name} does not raise an error when the "
+            "number of features changes between calls to partial_fit."
+        ),
+    ):
         estimator.partial_fit(X[:, :-1], y)


-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+@ignore_warnings(category=FutureWarning)
+def check_classifier_multioutput(name, estimator):
+    n_samples, n_labels, n_classes = 42, 5, 3
+    tags = _safe_tags(estimator)
+    estimator = clone(estimator)
+    X, y = make_multilabel_classification(
+        random_state=42, n_samples=n_samples, n_labels=n_labels, n_classes=n_classes
+    )
+    estimator.fit(X, y)
+    y_pred = estimator.predict(X)
+
+    assert y_pred.shape == (n_samples, n_classes), (
+        "The shape of the prediction for multioutput data is "
+        "incorrect. Expected {}, got {}.".format((n_samples, n_labels), y_pred.shape)
+    )
+    assert y_pred.dtype.kind == "i"
+
+    if hasattr(estimator, "decision_function"):
+        decision = estimator.decision_function(X)
+        assert isinstance(decision, np.ndarray)
+        assert decision.shape == (n_samples, n_classes), (
+            "The shape of the decision function output for "
+            "multioutput data is incorrect. Expected {}, got {}.".format(
+                (n_samples, n_classes), decision.shape
+            )
+        )
+
+        dec_pred = (decision > 0).astype(int)
+        dec_exp = estimator.classes_[dec_pred]
+        assert_array_equal(dec_exp, y_pred)
+
+    if hasattr(estimator, "predict_proba"):
+        y_prob = estimator.predict_proba(X)
+
+        if isinstance(y_prob, list) and not tags["poor_score"]:
+            for i in range(n_classes):
+                assert y_prob[i].shape == (n_samples, 2), (
+                    "The shape of the probability for multioutput data is"
+                    " incorrect. Expected {}, got {}.".format(
+                        (n_samples, 2), y_prob[i].shape
+                    )
+                )
+                assert_array_equal(
+                    np.argmax(y_prob[i], axis=1).astype(int), y_pred[:, i]
+                )
+        elif not tags["poor_score"]:
+            assert y_prob.shape == (n_samples, n_classes), (
+                "The shape of the probability for multioutput data is"
+                " incorrect. Expected {}, got {}.".format(
+                    (n_samples, n_classes), y_prob.shape
+                )
+            )
+            assert_array_equal(y_prob.round().astype(int), y_pred)
+
+    if hasattr(estimator, "decision_function") and hasattr(estimator, "predict_proba"):
+        for i in range(n_classes):
+            y_proba = estimator.predict_proba(X)[:, i]
+            y_decision = estimator.decision_function(X)
+            assert_array_equal(rankdata(y_proba), rankdata(y_decision[:, i]))
+
+
+@ignore_warnings(category=FutureWarning)
+def check_regressor_multioutput(name, estimator):
+    estimator = clone(estimator)
+    n_samples = n_features = 10
+
+    if not _is_pairwise_metric(estimator):
+        n_samples = n_samples + 1
+
+    X, y = make_regression(
+        random_state=42, n_targets=5, n_samples=n_samples, n_features=n_features
+    )
+    X = _pairwise_estimator_convert_X(X, estimator)
+
+    estimator.fit(X, y)
+    y_pred = estimator.predict(X)
+
+    assert y_pred.dtype == np.dtype("float64"), (
+        "Multioutput predictions by a regressor are expected to be"
+        " floating-point precision. Got {} instead".format(y_pred.dtype)
+    )
+    assert y_pred.shape == y.shape, (
+        "The shape of the prediction for multioutput data is incorrect."
+        " Expected {}, got {}."
+    )
+
+
+@ignore_warnings(category=FutureWarning)
 def check_clustering(name, clusterer_orig, readonly_memmap=False):
     clusterer = clone(clusterer_orig)
     X, y = make_blobs(n_samples=50, random_state=1)
@@ -1322,7 +2019,7 @@
     if hasattr(clusterer, "n_clusters"):
         clusterer.set_params(n_clusters=3)
     set_random_state(clusterer)
-    if name == 'AffinityPropagation':
+    if name == "AffinityPropagation":
         clusterer.set_params(preference=-100)
         clusterer.set_params(max_iter=100)

@@ -1332,9 +2029,9 @@
     clusterer.fit(X.tolist())

     pred = clusterer.labels_
-    assert_equal(pred.shape, (n_samples,))
-    assert_greater(adjusted_rand_score(pred, y), 0.4)
-    if _safe_tags(clusterer, 'non_deterministic'):
+    assert pred.shape == (n_samples,)
+    assert adjusted_rand_score(pred, y) > 0.4
+    if _safe_tags(clusterer, key="non_deterministic"):
         return
     set_random_state(clusterer)
     with warnings.catch_warnings(record=True):
@@ -1342,8 +2039,8 @@
     assert_array_equal(pred, pred2)

     # fit_predict(X) and labels_ should be of type int
-    assert_in(pred.dtype, [np.dtype('int32'), np.dtype('int64')])
-    assert_in(pred2.dtype, [np.dtype('int32'), np.dtype('int64')])
+    assert pred.dtype in [np.dtype("int32"), np.dtype("int64")]
+    assert pred2.dtype in [np.dtype("int32"), np.dtype("int64")]

     # Add noise to X to test the possible values of the labels
     labels = clusterer.fit_predict(X_noise)
@@ -1352,21 +2049,22 @@
     # labels_ should contain all the consecutive values between its
     # min and its max.
     labels_sorted = np.unique(labels)
-    assert_array_equal(labels_sorted, np.arange(labels_sorted[0],
-                                                labels_sorted[-1] + 1))
+    assert_array_equal(
+        labels_sorted, np.arange(labels_sorted[0], labels_sorted[-1] + 1)
+    )

     # Labels are expected to start at 0 (no noise) or -1 (if noise)
     assert labels_sorted[0] in [0, -1]
     # Labels should be less than n_clusters - 1
-    if hasattr(clusterer, 'n_clusters'):
-        n_clusters = getattr(clusterer, 'n_clusters')
-        assert_greater_equal(n_clusters - 1, labels_sorted[-1])
+    if hasattr(clusterer, "n_clusters"):
+        n_clusters = getattr(clusterer, "n_clusters")
+        assert n_clusters - 1 >= labels_sorted[-1]
     # else labels should be less than max(labels_) which is necessarily true


-@ignore_warnings(category=DeprecationWarning)
+@ignore_warnings(category=FutureWarning)
 def check_clusterer_compute_labels_predict(name, clusterer_orig):
-    """Check that predict is invariant of compute_labels"""
+    """Check that predict is invariant of compute_labels."""
     X, y = make_blobs(n_samples=20, random_state=0)
     clusterer = clone(clusterer_orig)
     set_random_state(clusterer)
@@ -1379,75 +2077,89 @@
         assert_array_equal(X_pred1, X_pred2)


-@ignore_warnings(category=DeprecationWarning)
+@ignore_warnings(category=FutureWarning)
 def check_classifiers_one_label(name, classifier_orig):
     error_string_fit = "Classifier can't train when only one class is present."
-    error_string_predict = ("Classifier can't predict when only one class is "
-                            "present.")
+    error_string_predict = "Classifier can't predict when only one class is present."
     rnd = np.random.RandomState(0)
     X_train = rnd.uniform(size=(10, 3))
     X_test = rnd.uniform(size=(10, 3))
     y = np.ones(10)
     # catch deprecation warnings
-    with ignore_warnings(category=(DeprecationWarning, FutureWarning)):
+    with ignore_warnings(category=FutureWarning):
         classifier = clone(classifier_orig)
-        # try to fit
-        try:
+        with raises(
+            ValueError, match="class", may_pass=True, err_msg=error_string_fit
+        ) as cm:
             classifier.fit(X_train, y)
-        except ValueError as e:
-            if 'class' not in repr(e):
-                print(error_string_fit, classifier, e)
-                traceback.print_exc(file=sys.stdout)
-                raise e
-            else:
-                return
-        except Exception as exc:
-            print(error_string_fit, classifier, exc)
-            traceback.print_exc(file=sys.stdout)
-            raise exc
-        # predict
-        try:
-            assert_array_equal(classifier.predict(X_test), y)
-        except Exception as exc:
-            print(error_string_predict, classifier, exc)
-            raise exc
+
+        if cm.raised_and_matched:
+            # ValueError was raised with proper error message
+            return
+
+        assert_array_equal(classifier.predict(X_test), y, err_msg=error_string_predict)
+
+
+def _create_memmap_backed_data(numpy_arrays):
+    # OpenBLAS is known to segfault with unaligned data on the Prescott architecture
+    # See: https://github.com/scipy/scipy/issues/14886
+    has_prescott_openblas = any(
+        True
+        for info in threadpool_info()
+        if info["internal_api"] == "openblas"
+        # Prudently assume Prescott might be the architecture if it is unknown.
+        and info.get("architecture", "prescott").lower() == "prescott"
+    )
+    return [
+        create_memmap_backed_data(array, aligned=has_prescott_openblas)
+        for array in numpy_arrays
+    ]


 @ignore_warnings  # Warnings are raised by decision function
-def check_classifiers_train(name, classifier_orig, readonly_memmap=False):
+def check_classifiers_train(
+    name, classifier_orig, readonly_memmap=False, X_dtype="float64"
+):
     X_m, y_m = make_blobs(n_samples=300, random_state=0)
+    X_m = X_m.astype(X_dtype)
     X_m, y_m = shuffle(X_m, y_m, random_state=7)
     X_m = StandardScaler().fit_transform(X_m)
     # generate binary problem from multi-class one
     y_b = y_m[y_m != 2]
     X_b = X_m[y_m != 2]
-    tags = _safe_tags(classifier_orig)
-
-    if name in ['BernoulliNB', 'MultinomialNB', 'ComplementNB']:
+
+    if name in ["BernoulliNB", "MultinomialNB", "ComplementNB", "CategoricalNB"]:
         X_m -= X_m.min()
         X_b -= X_b.min()

     if readonly_memmap:
-        X_m, y_m, X_b, y_b = create_memmap_backed_data([X_m, y_m, X_b, y_b])
-
-    for (X, y) in [(X_m, y_m), (X_b, y_b)]:
+        X_m, y_m, X_b, y_b = _create_memmap_backed_data([X_m, y_m, X_b, y_b])
+
+    problems = [(X_b, y_b)]
+    tags = _safe_tags(classifier_orig)
+    if not tags["binary_only"]:
+        problems.append((X_m, y_m))
+
+    for X, y in problems:
         classes = np.unique(y)
         n_classes = len(classes)
         n_samples, n_features = X.shape
         classifier = clone(classifier_orig)
-        X = pairwise_estimator_convert_X(X, classifier)
-        y = multioutput_estimator_convert_y_2d(classifier, y)
+        X = _pairwise_estimator_convert_X(X, classifier)
+        y = _enforce_estimator_tags_y(classifier, y)

         set_random_state(classifier)
         # raises error on malformed input for fit
         if not tags["no_validation"]:
-            with assert_raises(
+            with raises(
                 ValueError,
-                msg="The classifier {} does not "
-                    "raise an error when incorrect/malformed input "
-                    "data for fit is passed. The number of training "
-                    "examples is not the same as the number of labels. "
-                    "Perhaps use check_X_y in fit.".format(name)):
+                err_msg=(
+                    f"The classifier {name} does not raise an error when "
+                    "incorrect/malformed input data for fit is passed. The number "
+                    "of training examples is not the same as the number of "
+                    "labels. Perhaps use check_X_y in fit."
+                ),
+            ):
                 classifier.fit(X, y[:-1])

         # fit
@@ -1457,27 +2169,31 @@
         assert hasattr(classifier, "classes_")
         y_pred = classifier.predict(X)

-        assert_equal(y_pred.shape, (n_samples,))
+        assert y_pred.shape == (n_samples,)
         # training set performance
-        if not tags['poor_score']:
-            assert_greater(accuracy_score(y, y_pred), 0.83)
+        if not tags["poor_score"]:
+            assert accuracy_score(y, y_pred) > 0.83

         # raises error on malformed input for predict
         msg_pairwise = (
             "The classifier {} does not raise an error when shape of X in "
-            " {} is not equal to (n_test_samples, n_training_samples)")
-        msg = ("The classifier {} does not raise an error when the number of "
-               "features in {} is different from the number of features in "
-               "fit.")
+            " {} is not equal to (n_test_samples, n_training_samples)"
+        )
+        msg = (
+            "The classifier {} does not raise an error when the number of "
+            "features in {} is different from the number of features in "
+            "fit."
+        )

         if not tags["no_validation"]:
-            if _is_pairwise(classifier):
-                with assert_raises(ValueError,
-                                   msg=msg_pairwise.format(name, "predict")):
+            if tags["pairwise"]:
+                with raises(
+                    ValueError,
+                    err_msg=msg_pairwise.format(name, "predict"),
+                ):
                     classifier.predict(X.reshape(-1, 1))
             else:
-                with assert_raises(ValueError,
-                                   msg=msg.format(name, "predict")):
+                with raises(ValueError, err_msg=msg.format(name, "predict")):
                     classifier.predict(X.T)
         if hasattr(classifier, "decision_function"):
             try:
@@ -1485,24 +2201,28 @@
                 decision = classifier.decision_function(X)
                 if n_classes == 2:
                     if not tags["multioutput_only"]:
-                        assert_equal(decision.shape, (n_samples,))
+                        assert decision.shape == (n_samples,)
                     else:
-                        assert_equal(decision.shape, (n_samples, 1))
-                    dec_pred = (decision.ravel() > 0).astype(np.int)
+                        assert decision.shape == (n_samples, 1)
+                    dec_pred = (decision.ravel() > 0).astype(int)
                     assert_array_equal(dec_pred, y_pred)
                 else:
-                    assert_equal(decision.shape, (n_samples, n_classes))
+                    assert decision.shape == (n_samples, n_classes)
                     assert_array_equal(np.argmax(decision, axis=1), y_pred)

                 # raises error on malformed input for decision_function
                 if not tags["no_validation"]:
-                    if _is_pairwise(classifier):
-                        with assert_raises(ValueError, msg=msg_pairwise.format(
-                                name, "decision_function")):
+                    if tags["pairwise"]:
+                        with raises(
+                            ValueError,
+                            err_msg=msg_pairwise.format(name, "decision_function"),
+                        ):
                             classifier.decision_function(X.reshape(-1, 1))
                     else:
-                        with assert_raises(ValueError, msg=msg.format(
-                                name, "decision_function")):
+                        with raises(
+                            ValueError,
+                            err_msg=msg.format(name, "decision_function"),
+                        ):
                             classifier.decision_function(X.T)
             except NotImplementedError:
                 pass
@@ -1510,20 +2230,23 @@
         if hasattr(classifier, "predict_proba"):
             # predict_proba agrees with predict
             y_prob = classifier.predict_proba(X)
-            assert_equal(y_prob.shape, (n_samples, n_classes))
+            assert y_prob.shape == (n_samples, n_classes)
             assert_array_equal(np.argmax(y_prob, axis=1), y_pred)
             # check that probas for all classes sum to one
-            assert_array_almost_equal(np.sum(y_prob, axis=1),
-                                      np.ones(n_samples))
+            assert_array_almost_equal(np.sum(y_prob, axis=1), np.ones(n_samples))
             if not tags["no_validation"]:
                 # raises error on malformed input for predict_proba
-                if _is_pairwise(classifier_orig):
-                    with assert_raises(ValueError, msg=msg_pairwise.format(
-                            name, "predict_proba")):
+                if tags["pairwise"]:
+                    with raises(
+                        ValueError,
+                        err_msg=msg_pairwise.format(name, "predict_proba"),
+                    ):
                         classifier.predict_proba(X.reshape(-1, 1))
                 else:
-                    with assert_raises(ValueError, msg=msg.format(
-                            name, "predict_proba")):
+                    with raises(
+                        ValueError,
+                        err_msg=msg.format(name, "predict_proba"),
+                    ):
                         classifier.predict_proba(X.T)
             if hasattr(classifier, "predict_log_proba"):
                 # predict_log_proba is a transformation of predict_proba
@@ -1546,9 +2269,11 @@
     # leading to the observed discrepancy between provided
     # and actual contamination levels.
     sorted_decision = np.sort(decision)
-    msg = ('The number of predicted outliers is not equal to the expected '
-           'number of outliers and this difference is not explained by the '
-           'number of ties in the decision_function values')
+    msg = (
+        "The number of predicted outliers is not equal to the expected "
+        "number of outliers and this difference is not explained by the "
+        "number of ties in the decision_function values"
+    )
     assert len(np.unique(sorted_decision[start:end])) == 1, msg


@@ -1571,36 +2296,38 @@

     y_pred = estimator.predict(X)
     assert y_pred.shape == (n_samples,)
-    assert y_pred.dtype.kind == 'i'
+    assert y_pred.dtype.kind == "i"
     assert_array_equal(np.unique(y_pred), np.array([-1, 1]))

     decision = estimator.decision_function(X)
     scores = estimator.score_samples(X)
     for output in [decision, scores]:
-        assert output.dtype == np.dtype('float')
+        assert output.dtype == np.dtype("float")
         assert output.shape == (n_samples,)

     # raises error on malformed input for predict
-    assert_raises(ValueError, estimator.predict, X.T)
+    with raises(ValueError):
+        estimator.predict(X.T)

     # decision_function agrees with predict
-    dec_pred = (decision >= 0).astype(np.int)
+    dec_pred = (decision >= 0).astype(int)
     dec_pred[dec_pred == 0] = -1
     assert_array_equal(dec_pred, y_pred)

     # raises error on malformed input for decision_function
-    assert_raises(ValueError, estimator.decision_function, X.T)
+    with raises(ValueError):
+        estimator.decision_function(X.T)

     # decision_function is a translation of score_samples
     y_dec = scores - estimator.offset_
     assert_allclose(y_dec, decision)

     # raises error on malformed input for score_samples
-    assert_raises(ValueError, estimator.score_samples, X.T)
+    with raises(ValueError):
+        estimator.score_samples(X.T)

     # contamination parameter (not for OneClassSVM which has the nu parameter)
-    if (hasattr(estimator, 'contamination')
-            and not hasattr(estimator, 'novelty')):
+    if hasattr(estimator, "contamination") and not hasattr(estimator, "novelty"):
         # proportion of outliers equal to contamination parameter when not
         # set to 'auto'. This is true for the training set and cannot thus be
         # checked as follows for estimators with a novelty parameter such as
@@ -1622,22 +2349,239 @@
             check_outlier_corruption(num_outliers, expected_outliers, decision)

         # raises error when contamination is a scalar and not in [0,1]
+        msg = r"contamination must be in \(0, 0.5]"
         for contamination in [-0.5, 2.3]:
             estimator.set_params(contamination=contamination)
-            assert_raises(ValueError, estimator.fit, X)
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
-def check_estimators_fit_returns_self(name, estimator_orig,
-                                      readonly_memmap=False):
-    """Check if self is returned when calling fit"""
-    X, y = make_blobs(random_state=0, n_samples=9, n_features=4)
+            with raises(ValueError, match=msg):
+                estimator.fit(X)
+
+
+@ignore_warnings(category=FutureWarning)
+def check_classifiers_multilabel_representation_invariance(name, classifier_orig):
+    X, y = make_multilabel_classification(
+        n_samples=100,
+        n_features=2,
+        n_classes=5,
+        n_labels=3,
+        length=50,
+        allow_unlabeled=True,
+        random_state=0,
+    )
+    X = scale(X)
+
+    X_train, y_train = X[:80], y[:80]
+    X_test = X[80:]
+
+    y_train_list_of_lists = y_train.tolist()
+    y_train_list_of_arrays = list(y_train)
+
+    classifier = clone(classifier_orig)
+    set_random_state(classifier)
+
+    y_pred = classifier.fit(X_train, y_train).predict(X_test)
+
+    y_pred_list_of_lists = classifier.fit(X_train, y_train_list_of_lists).predict(
+        X_test
+    )
+
+    y_pred_list_of_arrays = classifier.fit(X_train, y_train_list_of_arrays).predict(
+        X_test
+    )
+
+    assert_array_equal(y_pred, y_pred_list_of_arrays)
+    assert_array_equal(y_pred, y_pred_list_of_lists)
+
+    assert y_pred.dtype == y_pred_list_of_arrays.dtype
+    assert y_pred.dtype == y_pred_list_of_lists.dtype
+    assert type(y_pred) == type(y_pred_list_of_arrays)
+    assert type(y_pred) == type(y_pred_list_of_lists)
+
+
+@ignore_warnings(category=FutureWarning)
+def check_classifiers_multilabel_output_format_predict(name, classifier_orig):
+    """Check the output of the `predict` method for classifiers supporting
+    multilabel-indicator targets."""
+    classifier = clone(classifier_orig)
+    set_random_state(classifier)
+
+    n_samples, test_size, n_outputs = 100, 25, 5
+    X, y = make_multilabel_classification(
+        n_samples=n_samples,
+        n_features=2,
+        n_classes=n_outputs,
+        n_labels=3,
+        length=50,
+        allow_unlabeled=True,
+        random_state=0,
+    )
+    X = scale(X)
+
+    X_train, X_test = X[:-test_size], X[-test_size:]
+    y_train, y_test = y[:-test_size], y[-test_size:]
+    classifier.fit(X_train, y_train)
+
+    response_method_name = "predict"
+    predict_method = getattr(classifier, response_method_name, None)
+    if predict_method is None:
+        raise SkipTest(f"{name} does not have a {response_method_name} method.")
+
+    y_pred = predict_method(X_test)
+
+    # y_pred.shape -> y_test.shape with the same dtype
+    assert isinstance(y_pred, np.ndarray), (
+        f"{name}.predict is expected to output a NumPy array. Got "
+        f"{type(y_pred)} instead."
+    )
+    assert y_pred.shape == y_test.shape, (
+        f"{name}.predict outputs a NumPy array of shape {y_pred.shape} "
+        f"instead of {y_test.shape}."
+    )
+    assert y_pred.dtype == y_test.dtype, (
+        f"{name}.predict does not output the same dtype than the targets. "
+        f"Got {y_pred.dtype} instead of {y_test.dtype}."
+    )
+
+
+@ignore_warnings(category=FutureWarning)
+def check_classifiers_multilabel_output_format_predict_proba(name, classifier_orig):
+    """Check the output of the `predict_proba` method for classifiers supporting
+    multilabel-indicator targets."""
+    classifier = clone(classifier_orig)
+    set_random_state(classifier)
+
+    n_samples, test_size, n_outputs = 100, 25, 5
+    X, y = make_multilabel_classification(
+        n_samples=n_samples,
+        n_features=2,
+        n_classes=n_outputs,
+        n_labels=3,
+        length=50,
+        allow_unlabeled=True,
+        random_state=0,
+    )
+    X = scale(X)
+
+    X_train, X_test = X[:-test_size], X[-test_size:]
+    y_train = y[:-test_size]
+    classifier.fit(X_train, y_train)
+
+    response_method_name = "predict_proba"
+    predict_proba_method = getattr(classifier, response_method_name, None)
+    if predict_proba_method is None:
+        raise SkipTest(f"{name} does not have a {response_method_name} method.")
+
+    y_pred = predict_proba_method(X_test)
+
+    # y_pred.shape -> 2 possibilities:
+    # - list of length n_outputs of shape (n_samples, 2);
+    # - ndarray of shape (n_samples, n_outputs).
+    # dtype should be floating
+    if isinstance(y_pred, list):
+        assert len(y_pred) == n_outputs, (
+            f"When {name}.predict_proba returns a list, the list should "
+            "be of length n_outputs and contain NumPy arrays. Got length "
+            f"of {len(y_pred)} instead of {n_outputs}."
+        )
+        for pred in y_pred:
+            assert pred.shape == (test_size, 2), (
+                f"When {name}.predict_proba returns a list, this list "
+                "should contain NumPy arrays of shape (n_samples, 2). Got "
+                f"NumPy arrays of shape {pred.shape} instead of "
+                f"{(test_size, 2)}."
+            )
+            assert pred.dtype.kind == "f", (
+                f"When {name}.predict_proba returns a list, it should "
+                "contain NumPy arrays with floating dtype. Got "
+                f"{pred.dtype} instead."
+            )
+            # check that we have the correct probabilities
+            err_msg = (
+                f"When {name}.predict_proba returns a list, each NumPy "
+                "array should contain probabilities for each class and "
+                "thus each row should sum to 1 (or close to 1 due to "
+                "numerical errors)."
+            )
+            assert_allclose(pred.sum(axis=1), 1, err_msg=err_msg)
+    elif isinstance(y_pred, np.ndarray):
+        assert y_pred.shape == (test_size, n_outputs), (
+            f"When {name}.predict_proba returns a NumPy array, the "
+            f"expected shape is (n_samples, n_outputs). Got {y_pred.shape}"
+            f" instead of {(test_size, n_outputs)}."
+        )
+        assert y_pred.dtype.kind == "f", (
+            f"When {name}.predict_proba returns a NumPy array, the "
+            f"expected data type is floating. Got {y_pred.dtype} instead."
+        )
+        err_msg = (
+            f"When {name}.predict_proba returns a NumPy array, this array "
+            "is expected to provide probabilities of the positive class "
+            "and should therefore contain values between 0 and 1."
+        )
+        assert_array_less(0, y_pred, err_msg=err_msg)
+        assert_array_less(y_pred, 1, err_msg=err_msg)
+    else:
+        raise ValueError(
+            f"Unknown returned type {type(y_pred)} by {name}."
+            "predict_proba. A list or a Numpy array is expected."
+        )
+
+
+@ignore_warnings(category=FutureWarning)
+def check_classifiers_multilabel_output_format_decision_function(name, classifier_orig):
+    """Check the output of the `decision_function` method for classifiers supporting
+    multilabel-indicator targets."""
+    classifier = clone(classifier_orig)
+    set_random_state(classifier)
+
+    n_samples, test_size, n_outputs = 100, 25, 5
+    X, y = make_multilabel_classification(
+        n_samples=n_samples,
+        n_features=2,
+        n_classes=n_outputs,
+        n_labels=3,
+        length=50,
+        allow_unlabeled=True,
+        random_state=0,
+    )
+    X = scale(X)
+
+    X_train, X_test = X[:-test_size], X[-test_size:]
+    y_train = y[:-test_size]
+    classifier.fit(X_train, y_train)
+
+    response_method_name = "decision_function"
+    decision_function_method = getattr(classifier, response_method_name, None)
+    if decision_function_method is None:
+        raise SkipTest(f"{name} does not have a {response_method_name} method.")
+
+    y_pred = decision_function_method(X_test)
+
+    # y_pred.shape -> y_test.shape with floating dtype
+    assert isinstance(y_pred, np.ndarray), (
+        f"{name}.decision_function is expected to output a NumPy array."
+        f" Got {type(y_pred)} instead."
+    )
+    assert y_pred.shape == (test_size, n_outputs), (
+        f"{name}.decision_function is expected to provide a NumPy array "
+        f"of shape (n_samples, n_outputs). Got {y_pred.shape} instead of "
+        f"{(test_size, n_outputs)}."
+    )
+    assert y_pred.dtype.kind == "f", (
+        f"{name}.decision_function is expected to output a floating dtype."
+        f" Got {y_pred.dtype} instead."
+    )
+
+
+@ignore_warnings(category=FutureWarning)
+def check_estimators_fit_returns_self(name, estimator_orig, readonly_memmap=False):
+    """Check if self is returned when calling fit."""
+    X, y = make_blobs(random_state=0, n_samples=21)
     # some want non-negative input
     X -= X.min()
-    X = pairwise_estimator_convert_X(X, estimator_orig)
+    X = _pairwise_estimator_convert_X(X, estimator_orig)

     estimator = clone(estimator_orig)
-    y = multioutput_estimator_convert_y_2d(estimator, y)
+    y = _enforce_estimator_tags_y(estimator, y)

     if readonly_memmap:
         X, y = create_memmap_backed_data([X, y])
@@ -1650,57 +2594,31 @@
 def check_estimators_unfitted(name, estimator_orig):
     """Check that predict raises an exception in an unfitted estimator.

-    Unfitted estimators should raise either AttributeError or ValueError.
-    The specific exception type NotFittedError inherits from both and can
-    therefore be adequately raised for that purpose.
+    Unfitted estimators should raise a NotFittedError.
     """
-
     # Common test for Regressors, Classifiers and Outlier detection estimators
-    X, y = _boston_subset()
+    X, y = _regression_dataset()

     estimator = clone(estimator_orig)
-
-    msg = "fit"
-    if hasattr(estimator, 'predict'):
-        can_predict = False
-        try:
-            # some models can predict without fitting
-            # like GaussianProcess regressors
-            # in this case, we skip this test
-            pred = estimator.predict(X)
-            assert pred.shape[0] == X.shape[0]
-            can_predict = True
-        except ValueError:
-            pass
-        if can_predict:
-            raise SkipTest(
-                "{} can predict without fitting, skipping "
-                "check_estimator_unfitted.".format(name))
-
-        assert_raise_message((AttributeError, ValueError), msg,
-                             estimator.predict, X)
-
-    if hasattr(estimator, 'decision_function'):
-        assert_raise_message((AttributeError, ValueError), msg,
-                             estimator.decision_function, X)
-
-    if hasattr(estimator, 'predict_proba'):
-        assert_raise_message((AttributeError, ValueError), msg,
-                             estimator.predict_proba, X)
-
-    if hasattr(estimator, 'predict_log_proba'):
-        assert_raise_message((AttributeError, ValueError), msg,
-                             estimator.predict_log_proba, X)
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+    for method in (
+        "decision_function",
+        "predict",
+        "predict_proba",
+        "predict_log_proba",
+    ):
+        if hasattr(estimator, method):
+            with raises(NotFittedError):
+                getattr(estimator, method)(X)
+
+
+@ignore_warnings(category=FutureWarning)
 def check_supervised_y_2d(name, estimator_orig):
-    if _safe_tags(estimator_orig, "multioutput_only"):
-        # These only work on 2d, so this test makes no sense
-        return
+    tags = _safe_tags(estimator_orig)
     rnd = np.random.RandomState(0)
-    X = pairwise_estimator_convert_X(rnd.uniform(size=(10, 3)), estimator_orig)
-    y = np.arange(10) % 3
+    n_samples = 30
+    X = _pairwise_estimator_convert_X(rnd.uniform(size=(n_samples, 3)), estimator_orig)
+    y = np.arange(n_samples) % 3
+    y = _enforce_estimator_tags_y(estimator_orig, y)
     estimator = clone(estimator_orig)
     set_random_state(estimator)
     # fit
@@ -1715,13 +2633,17 @@
         warnings.simplefilter("ignore", RuntimeWarning)
         estimator.fit(X, y[:, np.newaxis])
     y_pred_2d = estimator.predict(X)
-    msg = "expected 1 DataConversionWarning, got: %s" % (
-        ", ".join([str(w_x) for w_x in w]))
-    if not _safe_tags(estimator, "multioutput"):
+    msg = "expected 1 DataConversionWarning, got: %s" % ", ".join(
+        [str(w_x) for w_x in w]
+    )
+    if not tags["multioutput"]:
         # check that we warned if we don't support multi-output
-        assert_greater(len(w), 0, msg)
-        assert "DataConversionWarning('A column-vector y" \
-               " was passed when a 1d array was expected" in msg
+        assert len(w) > 0, msg
+        assert (
+            "DataConversionWarning('A column-vector y"
+            " was passed when a 1d array was expected"
+            in msg
+        )
     assert_allclose(y_pred.ravel(), y_pred_2d.ravel())


@@ -1729,7 +2651,7 @@
 def check_classifiers_predictions(X, y, name, classifier_orig):
     classes = np.unique(y)
     classifier = clone(classifier_orig)
-    if name == 'BernoulliNB':
+    if name == "BernoulliNB":
         X = X > X.mean()
     set_random_state(classifier)

@@ -1740,21 +2662,33 @@
         decision = classifier.decision_function(X)
         assert isinstance(decision, np.ndarray)
         if len(classes) == 2:
-            dec_pred = (decision.ravel() > 0).astype(np.int)
+            dec_pred = (decision.ravel() > 0).astype(int)
             dec_exp = classifier.classes_[dec_pred]
-            assert_array_equal(dec_exp, y_pred,
-                               err_msg="decision_function does not match "
-                               "classifier for %r: expected '%s', got '%s'" %
-                               (classifier, ", ".join(map(str, dec_exp)),
-                                ", ".join(map(str, y_pred))))
-        elif getattr(classifier, 'decision_function_shape', 'ovr') == 'ovr':
+            assert_array_equal(
+                dec_exp,
+                y_pred,
+                err_msg=(
+                    "decision_function does not match "
+                    "classifier for %r: expected '%s', got '%s'"
+                )
+                % (
+                    classifier,
+                    ", ".join(map(str, dec_exp)),
+                    ", ".join(map(str, y_pred)),
+                ),
+            )
+        elif getattr(classifier, "decision_function_shape", "ovr") == "ovr":
             decision_y = np.argmax(decision, axis=1).astype(int)
             y_exp = classifier.classes_[decision_y]
-            assert_array_equal(y_exp, y_pred,
-                               err_msg="decision_function does not match "
-                               "classifier for %r: expected '%s', got '%s'" %
-                               (classifier, ", ".join(map(str, y_exp)),
-                                ", ".join(map(str, y_pred))))
+            assert_array_equal(
+                y_exp,
+                y_pred,
+                err_msg=(
+                    "decision_function does not match "
+                    "classifier for %r: expected '%s', got '%s'"
+                )
+                % (classifier, ", ".join(map(str, y_exp)), ", ".join(map(str, y_pred))),
+            )

     # training set performance
     if name != "ComplementNB":
@@ -1762,32 +2696,43 @@
         # For some specific cases 'ComplementNB' predicts less classes
         # than expected
         assert_array_equal(np.unique(y), np.unique(y_pred))
-    assert_array_equal(classes, classifier.classes_,
-                       err_msg="Unexpected classes_ attribute for %r: "
-                       "expected '%s', got '%s'" %
-                       (classifier, ", ".join(map(str, classes)),
-                        ", ".join(map(str, classifier.classes_))))
-
-
-def choose_check_classifiers_labels(name, y, y_names):
-    return y if name in ["LabelPropagation", "LabelSpreading"] else y_names
+    assert_array_equal(
+        classes,
+        classifier.classes_,
+        err_msg="Unexpected classes_ attribute for %r: expected '%s', got '%s'"
+        % (
+            classifier,
+            ", ".join(map(str, classes)),
+            ", ".join(map(str, classifier.classes_)),
+        ),
+    )
+
+
+def _choose_check_classifiers_labels(name, y, y_names):
+    # Semisupervised classifiers use -1 as the indicator for an unlabeled
+    # sample.
+    return (
+        y
+        if name in ["LabelPropagation", "LabelSpreading", "SelfTrainingClassifier"]
+        else y_names
+    )


 def check_classifiers_classes(name, classifier_orig):
-    X_multiclass, y_multiclass = make_blobs(n_samples=30, random_state=0,
-                                            cluster_std=0.1)
-    X_multiclass, y_multiclass = shuffle(X_multiclass, y_multiclass,
-                                         random_state=7)
+    X_multiclass, y_multiclass = make_blobs(
+        n_samples=30, random_state=0, cluster_std=0.1
+    )
+    X_multiclass, y_multiclass = shuffle(X_multiclass, y_multiclass, random_state=7)
     X_multiclass = StandardScaler().fit_transform(X_multiclass)
     # We need to make sure that we have non negative data, for things
     # like NMF
-    X_multiclass -= X_multiclass.min() - .1
+    X_multiclass -= X_multiclass.min() - 0.1

     X_binary = X_multiclass[y_multiclass != 2]
     y_binary = y_multiclass[y_multiclass != 2]

-    X_multiclass = pairwise_estimator_convert_X(X_multiclass, classifier_orig)
-    X_binary = pairwise_estimator_convert_X(X_binary, classifier_orig)
+    X_multiclass = _pairwise_estimator_convert_X(X_multiclass, classifier_orig)
+    X_binary = _pairwise_estimator_convert_X(X_binary, classifier_orig)

     labels_multiclass = ["one", "two", "three"]
     labels_binary = ["one", "two"]
@@ -1795,25 +2740,28 @@
     y_names_multiclass = np.take(labels_multiclass, y_multiclass)
     y_names_binary = np.take(labels_binary, y_binary)

-    for X, y, y_names in [(X_multiclass, y_multiclass, y_names_multiclass),
-                          (X_binary, y_binary, y_names_binary)]:
-        for y_names_i in [y_names, y_names.astype('O')]:
-            y_ = choose_check_classifiers_labels(name, y, y_names_i)
+    problems = [(X_binary, y_binary, y_names_binary)]
+    if not _safe_tags(classifier_orig, key="binary_only"):
+        problems.append((X_multiclass, y_multiclass, y_names_multiclass))
+
+    for X, y, y_names in problems:
+        for y_names_i in [y_names, y_names.astype("O")]:
+            y_ = _choose_check_classifiers_labels(name, y, y_names_i)
             check_classifiers_predictions(X, y_, name, classifier_orig)

     labels_binary = [-1, 1]
     y_names_binary = np.take(labels_binary, y_binary)
-    y_binary = choose_check_classifiers_labels(name, y_binary, y_names_binary)
+    y_binary = _choose_check_classifiers_labels(name, y_binary, y_names_binary)
     check_classifiers_predictions(X_binary, y_binary, name, classifier_orig)


-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+@ignore_warnings(category=FutureWarning)
 def check_regressors_int(name, regressor_orig):
-    X, _ = _boston_subset()
-    X = pairwise_estimator_convert_X(X[:50], regressor_orig)
+    X, _ = _regression_dataset()
+    X = _pairwise_estimator_convert_X(X[:50], regressor_orig)
     rnd = np.random.RandomState(0)
     y = rnd.randint(3, size=X.shape[0])
-    y = multioutput_estimator_convert_y_2d(regressor_orig, y)
+    y = _enforce_estimator_tags_y(regressor_orig, y)
     rnd = np.random.RandomState(0)
     # separate estimators to control random seeds
     regressor_1 = clone(regressor_orig)
@@ -1830,19 +2778,21 @@
     # fit
     regressor_1.fit(X, y_)
     pred1 = regressor_1.predict(X)
-    regressor_2.fit(X, y_.astype(np.float))
+    regressor_2.fit(X, y_.astype(float))
     pred2 = regressor_2.predict(X)
     assert_allclose(pred1, pred2, atol=1e-2, err_msg=name)


-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
-def check_regressors_train(name, regressor_orig, readonly_memmap=False):
-    X, y = _boston_subset()
-    X = pairwise_estimator_convert_X(X, regressor_orig)
-    y = StandardScaler().fit_transform(y.reshape(-1, 1))  # X is already scaled
-    y = y.ravel()
+@ignore_warnings(category=FutureWarning)
+def check_regressors_train(
+    name, regressor_orig, readonly_memmap=False, X_dtype=np.float64
+):
+    X, y = _regression_dataset()
+    X = X.astype(X_dtype)
+    X = _pairwise_estimator_convert_X(X, regressor_orig)
+    y = scale(y)  # X is already scaled
     regressor = clone(regressor_orig)
-    y = multioutput_estimator_convert_y_2d(regressor, y)
+    y = _enforce_estimator_tags_y(regressor, y)
     if name in CROSS_DECOMPOSITION:
         rnd = np.random.RandomState(0)
         y_ = np.vstack([y, 2 * y + rnd.randint(2, size=len(y))])
@@ -1851,77 +2801,73 @@
         y_ = y

     if readonly_memmap:
-        X, y, y_ = create_memmap_backed_data([X, y, y_])
-
-    if not hasattr(regressor, 'alphas') and hasattr(regressor, 'alpha'):
+        X, y, y_ = _create_memmap_backed_data([X, y, y_])
+
+    if not hasattr(regressor, "alphas") and hasattr(regressor, "alpha"):
         # linear regressors need to set alpha, but not generalized CV ones
         regressor.alpha = 0.01
-    if name == 'PassiveAggressiveRegressor':
+    if name == "PassiveAggressiveRegressor":
         regressor.C = 0.01

     # raises error on malformed input for fit
-    with assert_raises(ValueError, msg="The classifier {} does not"
-                       " raise an error when incorrect/malformed input "
-                       "data for fit is passed. The number of training "
-                       "examples is not the same as the number of "
-                       "labels. Perhaps use check_X_y in fit.".format(name)):
+    with raises(
+        ValueError,
+        err_msg=(
+            f"The classifier {name} does not raise an error when "
+            "incorrect/malformed input data for fit is passed. The number of "
+            "training examples is not the same as the number of labels. Perhaps "
+            "use check_X_y in fit."
+        ),
+    ):
         regressor.fit(X, y[:-1])
     # fit
     set_random_state(regressor)
     regressor.fit(X, y_)
     regressor.fit(X.tolist(), y_.tolist())
     y_pred = regressor.predict(X)
-    assert_equal(y_pred.shape, y_.shape)
+    assert y_pred.shape == y_.shape

     # TODO: find out why PLS and CCA fail. RANSAC is random
     # and furthermore assumes the presence of outliers, hence
     # skipped
-    if not _safe_tags(regressor, "poor_score"):
-        assert_greater(regressor.score(X, y_), 0.5)
+    if not _safe_tags(regressor, key="poor_score"):
+        assert regressor.score(X, y_) > 0.5


 @ignore_warnings
 def check_regressors_no_decision_function(name, regressor_orig):
-    # checks whether regressors have decision_function or predict_proba
+    # check that regressors don't have a decision_function, predict_proba, or
+    # predict_log_proba method.
     rng = np.random.RandomState(0)
+    regressor = clone(regressor_orig)
+
     X = rng.normal(size=(10, 4))
-    regressor = clone(regressor_orig)
-    y = multioutput_estimator_convert_y_2d(regressor, X[:, 0])
-
-    if hasattr(regressor, "n_components"):
-        # FIXME CCA, PLS is not robust to rank 1 effects
-        regressor.n_components = 1
+    X = _pairwise_estimator_convert_X(X, regressor_orig)
+    y = _enforce_estimator_tags_y(regressor, X[:, 0])

     regressor.fit(X, y)
     funcs = ["decision_function", "predict_proba", "predict_log_proba"]
     for func_name in funcs:
-        func = getattr(regressor, func_name, None)
-        if func is None:
-            # doesn't have function
-            continue
-        # has function. Should raise deprecation warning
-        msg = func_name
-        assert_warns_message(DeprecationWarning, msg, func, X)
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+        assert not hasattr(regressor, func_name)
+
+
+@ignore_warnings(category=FutureWarning)
 def check_class_weight_classifiers(name, classifier_orig):
-    if name == "NuSVC":
-        # the sparse version has a parameter that doesn't do anything
-        raise SkipTest("Not testing NuSVC class weight as it is ignored.")
-    if name.endswith("NB"):
-        # NaiveBayes classifiers have a somewhat different interface.
-        # FIXME SOON!
-        raise SkipTest
-
-    for n_centers in [2, 3]:
+
+    if _safe_tags(classifier_orig, key="binary_only"):
+        problems = [2]
+    else:
+        problems = [2, 3]
+
+    for n_centers in problems:
         # create a very noisy dataset
         X, y = make_blobs(centers=n_centers, random_state=0, cluster_std=20)
-        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,
-                                                            random_state=0)
+        X_train, X_test, y_train, y_test = train_test_split(
+            X, y, test_size=0.5, random_state=0
+        )

         # can't use gram_if_pairwise() here, setting up gram matrix manually
-        if _is_pairwise(classifier_orig):
+        if _safe_tags(classifier_orig, key="pairwise"):
             X_test = rbf_kernel(X_test, X_train)
             X_train = rbf_kernel(X_train, X_train)

@@ -1932,8 +2878,7 @@
         else:
             class_weight = {0: 1000, 1: 0.0001, 2: 0.0001}

-        classifier = clone(classifier_orig).set_params(
-            class_weight=class_weight)
+        classifier = clone(classifier_orig).set_params(class_weight=class_weight)
         if hasattr(classifier, "n_iter"):
             classifier.set_params(n_iter=100)
         if hasattr(classifier, "max_iter"):
@@ -1948,12 +2893,14 @@
         y_pred = classifier.predict(X_test)
         # XXX: Generally can use 0.89 here. On Windows, LinearSVC gets
         #      0.88 (Issue #9111)
-        assert_greater(np.mean(y_pred == 0), 0.87)
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
-def check_class_weight_balanced_classifiers(name, classifier_orig, X_train,
-                                            y_train, X_test, y_test, weights):
+        if not _safe_tags(classifier_orig, key="poor_score"):
+            assert np.mean(y_pred == 0) > 0.87
+
+
+@ignore_warnings(category=FutureWarning)
+def check_class_weight_balanced_classifiers(
+    name, classifier_orig, X_train, y_train, X_test, y_test, weights
+):
     classifier = clone(classifier_orig)
     if hasattr(classifier, "n_iter"):
         classifier.set_params(n_iter=100)
@@ -1964,19 +2911,19 @@
     classifier.fit(X_train, y_train)
     y_pred = classifier.predict(X_test)

-    classifier.set_params(class_weight='balanced')
+    classifier.set_params(class_weight="balanced")
     classifier.fit(X_train, y_train)
     y_pred_balanced = classifier.predict(X_test)
-    assert_greater(f1_score(y_test, y_pred_balanced, average='weighted'),
-                   f1_score(y_test, y_pred, average='weighted'))
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+    assert f1_score(y_test, y_pred_balanced, average="weighted") > f1_score(
+        y_test, y_pred, average="weighted"
+    )
+
+
+@ignore_warnings(category=FutureWarning)
 def check_class_weight_balanced_linear_classifier(name, Classifier):
     """Test class weights with non-contiguous class labels."""
     # this is run on classes, not instances, though this should be changed
-    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],
-                  [1.0, 1.0], [1.0, 0.0]])
+    X = np.array([[-1.0, -1.0], [-1.0, 0], [-0.8, -1.0], [1.0, 1.0], [1.0, 0.0]])
     y = np.array([1, 1, 1, -1, -1])

     classifier = Classifier()
@@ -1987,35 +2934,40 @@
         classifier.set_params(n_iter=1000)
     if hasattr(classifier, "max_iter"):
         classifier.set_params(max_iter=1000)
+    if hasattr(classifier, "cv"):
+        classifier.set_params(cv=3)
     set_random_state(classifier)

     # Let the model compute the class frequencies
-    classifier.set_params(class_weight='balanced')
+    classifier.set_params(class_weight="balanced")
     coef_balanced = classifier.fit(X, y).coef_.copy()

     # Count each label occurrence to reweight manually
     n_samples = len(y)
     n_classes = float(len(np.unique(y)))

-    class_weight = {1: n_samples / (np.sum(y == 1) * n_classes),
-                    -1: n_samples / (np.sum(y == -1) * n_classes)}
+    class_weight = {
+        1: n_samples / (np.sum(y == 1) * n_classes),
+        -1: n_samples / (np.sum(y == -1) * n_classes),
+    }
     classifier.set_params(class_weight=class_weight)
     coef_manual = classifier.fit(X, y).coef_.copy()

-    assert_allclose(coef_balanced, coef_manual,
-                    err_msg="Classifier %s is not computing"
-                    " class_weight=balanced properly."
-                    % name)
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+    assert_allclose(
+        coef_balanced,
+        coef_manual,
+        err_msg="Classifier %s is not computing class_weight=balanced properly." % name,
+    )
+
+
+@ignore_warnings(category=FutureWarning)
 def check_estimators_overwrite_params(name, estimator_orig):
-    X, y = make_blobs(random_state=0, n_samples=9)
+    X, y = make_blobs(random_state=0, n_samples=21)
     # some want non-negative input
     X -= X.min()
-    X = pairwise_estimator_convert_X(X, estimator_orig, kernel=rbf_kernel)
+    X = _pairwise_estimator_convert_X(X, estimator_orig, kernel=rbf_kernel)
     estimator = clone(estimator_orig)
-    y = multioutput_estimator_convert_y_2d(estimator, y)
+    y = _enforce_estimator_tags_y(estimator, y)

     set_random_state(estimator)

@@ -2037,14 +2989,24 @@
         # The only exception to this rule of immutable constructor parameters
         # is possible RandomState instance but in this check we explicitly
         # fixed the random_state params recursively to be integer seeds.
-        assert_equal(_joblib.hash(new_value), _joblib.hash(original_value),
-                     "Estimator %s should not change or mutate "
-                     " the parameter %s from %s to %s during fit."
-                     % (name, param_name, original_value, new_value))
-
-
-def check_no_attributes_set_in_init(name, estimator):
-    """Check setting during init. """
+        assert joblib.hash(new_value) == joblib.hash(original_value), (
+            "Estimator %s should not change or mutate "
+            " the parameter %s from %s to %s during fit."
+            % (name, param_name, original_value, new_value)
+        )
+
+
+@ignore_warnings(category=FutureWarning)
+def check_no_attributes_set_in_init(name, estimator_orig):
+    """Check setting during init."""
+    try:
+        # Clone fails if the estimator does not store
+        # all parameters as an attribute during init
+        estimator = clone(estimator_orig)
+    except AttributeError:
+        raise AttributeError(
+            f"Estimator {name} should store all parameters as an attribute during init."
+        )

     if hasattr(type(estimator).__init__, "deprecated_original"):
         return
@@ -2052,35 +3014,41 @@
     init_params = _get_args(type(estimator).__init__)
     if IS_PYPY:
         # __init__ signature has additional objects in PyPy
-        for key in ['obj']:
+        for key in ["obj"]:
             if key in init_params:
                 init_params.remove(key)
-    parents_init_params = [param for params_parent in
-                           (_get_args(parent) for parent in
-                            type(estimator).__mro__)
-                           for param in params_parent]
+    parents_init_params = [
+        param
+        for params_parent in (_get_args(parent) for parent in type(estimator).__mro__)
+        for param in params_parent
+    ]

     # Test for no setting apart from parameters during init
-    invalid_attr = (set(vars(estimator)) - set(init_params)
-                    - set(parents_init_params))
+    invalid_attr = set(vars(estimator)) - set(init_params) - set(parents_init_params)
     assert not invalid_attr, (
-            "Estimator %s should not set any attribute apart"
-            " from parameters during init. Found attributes %s."
-            % (name, sorted(invalid_attr)))
-    # Ensure that each parameter is set in init
-    invalid_attr = set(init_params) - set(vars(estimator)) - {"self"}
-    assert not invalid_attr, (
-            "Estimator %s should store all parameters"
-            " as an attribute during init. Did not find "
-            "attributes %s."
-            % (name, sorted(invalid_attr)))
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+        "Estimator %s should not set any attribute apart"
+        " from parameters during init. Found attributes %s."
+        % (name, sorted(invalid_attr))
+    )
+
+
+@ignore_warnings(category=FutureWarning)
 def check_sparsify_coefficients(name, estimator_orig):
-    X = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1],
-                  [-1, -2], [2, 2], [-2, -2]])
-    y = [1, 1, 1, 2, 2, 2, 3, 3, 3]
+    X = np.array(
+        [
+            [-2, -1],
+            [-1, -1],
+            [-1, -2],
+            [1, 1],
+            [1, 2],
+            [2, 1],
+            [-1, -2],
+            [2, 2],
+            [-2, -2],
+        ]
+    )
+    y = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])
+    y = _enforce_estimator_tags_y(estimator_orig, y)
     est = clone(estimator_orig)

     est.fit(X, y)
@@ -2099,37 +3067,78 @@
     assert_array_equal(pred, pred_orig)


-@ignore_warnings(category=DeprecationWarning)
+@ignore_warnings(category=FutureWarning)
 def check_classifier_data_not_an_array(name, estimator_orig):
-    X = np.array([[3, 0], [0, 1], [0, 2], [1, 1], [1, 2], [2, 1]])
-    X = pairwise_estimator_convert_X(X, estimator_orig)
-    y = [1, 1, 1, 2, 2, 2]
-    y = multioutput_estimator_convert_y_2d(estimator_orig, y)
-    check_estimators_data_not_an_array(name, estimator_orig, X, y)
-
-
-@ignore_warnings(category=DeprecationWarning)
+    X = np.array(
+        [
+            [3, 0],
+            [0, 1],
+            [0, 2],
+            [1, 1],
+            [1, 2],
+            [2, 1],
+            [0, 3],
+            [1, 0],
+            [2, 0],
+            [4, 4],
+            [2, 3],
+            [3, 2],
+        ]
+    )
+    X = _pairwise_estimator_convert_X(X, estimator_orig)
+    y = np.array([1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2])
+    y = _enforce_estimator_tags_y(estimator_orig, y)
+    for obj_type in ["NotAnArray", "PandasDataframe"]:
+        check_estimators_data_not_an_array(name, estimator_orig, X, y, obj_type)
+
+
+@ignore_warnings(category=FutureWarning)
 def check_regressor_data_not_an_array(name, estimator_orig):
-    X, y = _boston_subset(n_samples=50)
-    X = pairwise_estimator_convert_X(X, estimator_orig)
-    y = multioutput_estimator_convert_y_2d(estimator_orig, y)
-    check_estimators_data_not_an_array(name, estimator_orig, X, y)
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
-def check_estimators_data_not_an_array(name, estimator_orig, X, y):
+    X, y = _regression_dataset()
+    X = _pairwise_estimator_convert_X(X, estimator_orig)
+    y = _enforce_estimator_tags_y(estimator_orig, y)
+    for obj_type in ["NotAnArray", "PandasDataframe"]:
+        check_estimators_data_not_an_array(name, estimator_orig, X, y, obj_type)
+
+
+@ignore_warnings(category=FutureWarning)
+def check_estimators_data_not_an_array(name, estimator_orig, X, y, obj_type):
     if name in CROSS_DECOMPOSITION:
-        raise SkipTest("Skipping check_estimators_data_not_an_array "
-                       "for cross decomposition module as estimators "
-                       "are not deterministic.")
+        raise SkipTest(
+            "Skipping check_estimators_data_not_an_array "
+            "for cross decomposition module as estimators "
+            "are not deterministic."
+        )
     # separate estimators to control random seeds
     estimator_1 = clone(estimator_orig)
     estimator_2 = clone(estimator_orig)
     set_random_state(estimator_1)
     set_random_state(estimator_2)

-    y_ = NotAnArray(np.asarray(y))
-    X_ = NotAnArray(np.asarray(X))
+    if obj_type not in ["NotAnArray", "PandasDataframe"]:
+        raise ValueError("Data type {0} not supported".format(obj_type))
+
+    if obj_type == "NotAnArray":
+        y_ = _NotAnArray(np.asarray(y))
+        X_ = _NotAnArray(np.asarray(X))
+    else:
+        # Here pandas objects (Series and DataFrame) are tested explicitly
+        # because some estimators may handle them (especially their indexing)
+        # specially.
+        try:
+            import pandas as pd
+
+            y_ = np.asarray(y)
+            if y_.ndim == 1:
+                y_ = pd.Series(y_)
+            else:
+                y_ = pd.DataFrame(y_)
+            X_ = pd.DataFrame(np.asarray(X))
+
+        except ImportError:
+            raise SkipTest(
+                "pandas is not installed: not checking estimators for pandas objects."
+            )

     # fit
     estimator_1.fit(X_, y_)
@@ -2140,23 +3149,13 @@


 def check_parameters_default_constructible(name, Estimator):
-    # this check works on classes, not instances
     # test default-constructibility
     # get rid of deprecation warnings
-    with ignore_warnings(category=(DeprecationWarning, FutureWarning)):
-        required_parameters = getattr(Estimator, "_required_parameters", [])
-        if required_parameters:
-            if required_parameters in (["base_estimator"], ["estimator"]):
-                if issubclass(Estimator, RegressorMixin):
-                    estimator = Estimator(Ridge())
-                else:
-                    estimator = Estimator(LinearDiscriminantAnalysis())
-            else:
-                raise SkipTest("Can't instantiate estimator {} which"
-                               " requires parameters {}".format(
-                                   name, required_parameters))
-        else:
-            estimator = Estimator()
+
+    Estimator = Estimator.__class__
+
+    with ignore_warnings(category=FutureWarning):
+        estimator = _construct_instance(Estimator)
         # test cloning
         clone(estimator)
         # test __repr__
@@ -2170,73 +3169,136 @@
         # compare these against the actual values of the attributes.

         # this comes from getattr. Gets rid of deprecation decorator.
-        init = getattr(estimator.__init__, 'deprecated_original',
-                       estimator.__init__)
+        init = getattr(estimator.__init__, "deprecated_original", estimator.__init__)

         try:
+
             def param_filter(p):
-                """Identify hyper parameters of an estimator"""
-                return (p.name != 'self' and
-                        p.kind != p.VAR_KEYWORD and
-                        p.kind != p.VAR_POSITIONAL)
-
-            init_params = [p for p in signature(init).parameters.values()
-                           if param_filter(p)]
+                """Identify hyper parameters of an estimator."""
+                return (
+                    p.name != "self"
+                    and p.kind != p.VAR_KEYWORD
+                    and p.kind != p.VAR_POSITIONAL
+                )
+
+            init_params = [
+                p for p in signature(init).parameters.values() if param_filter(p)
+            ]

         except (TypeError, ValueError):
             # init is not a python function.
             # true for mixins
             return
         params = estimator.get_params()
-        if required_parameters == ["estimator"]:
-            # they can need a non-default argument
-            init_params = init_params[1:]
+        # they can need a non-default argument
+        init_params = init_params[len(getattr(estimator, "_required_parameters", [])) :]

         for init_param in init_params:
-            assert_not_equal(init_param.default, init_param.empty,
-                             "parameter %s for %s has no default value"
-                             % (init_param.name, type(estimator).__name__))
-            if type(init_param.default) is type:
-                assert_in(init_param.default, [np.float64, np.int64])
-            else:
-                assert_in(type(init_param.default),
-                          [str, int, float, bool, tuple, type(None),
-                           np.float64, types.FunctionType, _joblib.Memory])
+            assert (
+                init_param.default != init_param.empty
+            ), "parameter %s for %s has no default value" % (
+                init_param.name,
+                type(estimator).__name__,
+            )
+            allowed_types = {
+                str,
+                int,
+                float,
+                bool,
+                tuple,
+                type(None),
+                type,
+                types.FunctionType,
+                joblib.Memory,
+            }
+            # Any numpy numeric such as np.int32.
+            allowed_types.update(np.core.numerictypes.allTypes.values())
+            assert type(init_param.default) in allowed_types, (
+                f"Parameter '{init_param.name}' of estimator "
+                f"'{Estimator.__name__}' is of type "
+                f"{type(init_param.default).__name__} which is not "
+                "allowed. All init parameters have to be immutable to "
+                "make cloning possible. Therefore we restrict the set of "
+                "legal types to "
+                f"{set(type.__name__ for type in allowed_types)}."
+            )
             if init_param.name not in params.keys():
                 # deprecated parameter, not in get_params
-                assert init_param.default is None
+                assert init_param.default is None, (
+                    f"Estimator parameter '{init_param.name}' of estimator "
+                    f"'{Estimator.__name__}' is not returned by get_params. "
+                    "If it is deprecated, set its default value to None."
+                )
                 continue

             param_value = params[init_param.name]
             if isinstance(param_value, np.ndarray):
                 assert_array_equal(param_value, init_param.default)
             else:
+                failure_text = (
+                    f"Parameter {init_param.name} was mutated on init. All "
+                    "parameters must be stored unchanged."
+                )
                 if is_scalar_nan(param_value):
                     # Allows to set default parameters to np.nan
-                    assert param_value is init_param.default, init_param.name
+                    assert param_value is init_param.default, failure_text
                 else:
-                    assert param_value == init_param.default, init_param.name
-
-
-def multioutput_estimator_convert_y_2d(estimator, y):
+                    assert param_value == init_param.default, failure_text
+
+
+def _enforce_estimator_tags_y(estimator, y):
+    # Estimators with a `requires_positive_y` tag only accept strictly positive
+    # data
+    if _safe_tags(estimator, key="requires_positive_y"):
+        # Create strictly positive y. The minimal increment above 0 is 1, as
+        # y could be of integer dtype.
+        y += 1 + abs(y.min())
+    # Estimators with a `binary_only` tag only accept up to two unique y values
+    if _safe_tags(estimator, key="binary_only") and y.size > 0:
+        y = np.where(y == y.flat[0], y, y.flat[0] + 1)
     # Estimators in mono_output_task_error raise ValueError if y is of 1-D
     # Convert into a 2-D y for those estimators.
-    if _safe_tags(estimator, "multioutput_only"):
+    if _safe_tags(estimator, key="multioutput_only"):
         return np.reshape(y, (-1, 1))
     return y


-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+def _enforce_estimator_tags_x(estimator, X):
+    # Pairwise estimators only accept
+    # X of shape (`n_samples`, `n_samples`)
+    if _safe_tags(estimator, key="pairwise"):
+        X = X.dot(X.T)
+    # Estimators with `1darray` in `X_types` tag only accept
+    # X of shape (`n_samples`,)
+    if "1darray" in _safe_tags(estimator, key="X_types"):
+        X = X[:, 0]
+    # Estimators with a `requires_positive_X` tag only accept
+    # strictly positive data
+    if _safe_tags(estimator, key="requires_positive_X"):
+        X -= X.min()
+    if "categorical" in _safe_tags(estimator, key="X_types"):
+        X = (X - X.min()).astype(np.int32)
+    return X
+
+
+@ignore_warnings(category=FutureWarning)
 def check_non_transformer_estimators_n_iter(name, estimator_orig):
     # Test that estimators that are not transformers with a parameter
     # max_iter, return the attribute of n_iter_ at least 1.

     # These models are dependent on external solvers like
     # libsvm and accessing the iter parameter is non-trivial.
-    not_run_check_n_iter = ['Ridge', 'SVR', 'NuSVR', 'NuSVC',
-                            'RidgeClassifier', 'SVC', 'RandomizedLasso',
-                            'LogisticRegressionCV', 'LinearSVC',
-                            'LogisticRegression']
+    # SelfTrainingClassifier does not perform an iteration if all samples are
+    # labeled, hence n_iter_ = 0 is valid.
+    not_run_check_n_iter = [
+        "Ridge",
+        "RidgeClassifier",
+        "RandomizedLasso",
+        "LogisticRegressionCV",
+        "LinearSVC",
+        "LogisticRegression",
+        "SelfTrainingClassifier",
+    ]

     # Tested in test_transformer_n_iter
     not_run_check_n_iter += CROSS_DECOMPOSITION
@@ -2244,25 +3306,25 @@
         return

     # LassoLars stops early for the default alpha=1.0 the iris dataset.
-    if name == 'LassoLars':
-        estimator = clone(estimator_orig).set_params(alpha=0.)
+    if name == "LassoLars":
+        estimator = clone(estimator_orig).set_params(alpha=0.0)
     else:
         estimator = clone(estimator_orig)
-    if hasattr(estimator, 'max_iter'):
+    if hasattr(estimator, "max_iter"):
         iris = load_iris()
         X, y_ = iris.data, iris.target
-        y_ = multioutput_estimator_convert_y_2d(estimator, y_)
+        y_ = _enforce_estimator_tags_y(estimator, y_)

         set_random_state(estimator, 0)
-        if name == 'AffinityPropagation':
-            estimator.fit(X)
-        else:
-            estimator.fit(X, y_)
-
-        assert estimator.n_iter_ >= 1
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+
+        X = _pairwise_estimator_convert_X(X, estimator_orig)
+
+        estimator.fit(X, y_)
+
+        assert np.all(estimator.n_iter_ >= 1)
+
+
+@ignore_warnings(category=FutureWarning)
 def check_transformer_n_iter(name, estimator_orig):
     # Test that transformers with a parameter max_iter, return the
     # attribute of n_iter_ at least 1.
@@ -2270,12 +3332,17 @@
     if hasattr(estimator, "max_iter"):
         if name in CROSS_DECOMPOSITION:
             # Check using default data
-            X = [[0., 0., 1.], [1., 0., 0.], [2., 2., 2.], [2., 5., 4.]]
+            X = [[0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [2.0, 2.0, 2.0], [2.0, 5.0, 4.0]]
             y_ = [[0.1, -0.2], [0.9, 1.1], [0.1, -0.5], [0.3, -0.2]]

         else:
-            X, y_ = make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]],
-                               random_state=0, n_features=2, cluster_std=0.1)
+            X, y_ = make_blobs(
+                n_samples=30,
+                centers=[[0, 0, 0], [1, 1, 1]],
+                random_state=0,
+                n_features=2,
+                cluster_std=0.1,
+            )
             X -= X.min() - 0.1
         set_random_state(estimator, 0)
         estimator.fit(X, y_)
@@ -2283,12 +3350,12 @@
         # These return a n_iter per component.
         if name in CROSS_DECOMPOSITION:
             for iter_ in estimator.n_iter_:
-                assert_greater_equal(iter_, 1)
+                assert iter_ >= 1
         else:
-            assert_greater_equal(estimator.n_iter_, 1)
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+            assert estimator.n_iter_ >= 1
+
+
+@ignore_warnings(category=FutureWarning)
 def check_get_params_invariance(name, estimator_orig):
     # Checks if get_params(deep=False) is a subset of get_params(deep=True)
     e = clone(estimator_orig)
@@ -2296,22 +3363,21 @@
     shallow_params = e.get_params(deep=False)
     deep_params = e.get_params(deep=True)

-    assert all(item in deep_params.items() for item in
-               shallow_params.items())
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+    assert all(item in deep_params.items() for item in shallow_params.items())
+
+
+@ignore_warnings(category=FutureWarning)
 def check_set_params(name, estimator_orig):
     # Check that get_params() returns the same thing
     # before and after set_params() with some fuzz
     estimator = clone(estimator_orig)

     orig_params = estimator.get_params(deep=False)
-    msg = ("get_params result does not match what was passed to set_params")
+    msg = "get_params result does not match what was passed to set_params"

     estimator.set_params(**orig_params)
     curr_params = estimator.get_params(deep=False)
-    assert_equal(set(orig_params.keys()), set(curr_params.keys()), msg)
+    assert set(orig_params.keys()) == set(curr_params.keys()), msg
     for k, v in curr_params.items():
         assert orig_params[k] is v, msg

@@ -2328,62 +3394,78 @@
             except (TypeError, ValueError) as e:
                 e_type = e.__class__.__name__
                 # Exception occurred, possibly parameter validation
-                warnings.warn("{0} occurred during set_params of param {1} on "
-                              "{2}. It is recommended to delay parameter "
-                              "validation until fit.".format(e_type,
-                                                             param_name,
-                                                             name))
-
-                change_warning_msg = "Estimator's parameters changed after " \
-                                     "set_params raised {}".format(e_type)
+                warnings.warn(
+                    "{0} occurred during set_params of param {1} on "
+                    "{2}. It is recommended to delay parameter "
+                    "validation until fit.".format(e_type, param_name, name)
+                )
+
+                change_warning_msg = (
+                    "Estimator's parameters changed after set_params raised {}".format(
+                        e_type
+                    )
+                )
                 params_before_exception = curr_params
                 curr_params = estimator.get_params(deep=False)
                 try:
-                    assert_equal(set(params_before_exception.keys()),
-                                 set(curr_params.keys()))
+                    assert set(params_before_exception.keys()) == set(
+                        curr_params.keys()
+                    )
                     for k, v in curr_params.items():
                         assert params_before_exception[k] is v
                 except AssertionError:
                     warnings.warn(change_warning_msg)
             else:
                 curr_params = estimator.get_params(deep=False)
-                assert_equal(set(test_params.keys()),
-                             set(curr_params.keys()),
-                             msg)
+                assert set(test_params.keys()) == set(curr_params.keys()), msg
                 for k, v in curr_params.items():
                     assert test_params[k] is v, msg
         test_params[param_name] = default_value


-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+@ignore_warnings(category=FutureWarning)
 def check_classifiers_regression_target(name, estimator_orig):
     # Check if classifier throws an exception when fed regression targets

-    boston = load_boston()
-    X, y = boston.data, boston.target
+    X, y = _regression_dataset()
+
+    X = X + 1 + abs(X.min(axis=0))  # be sure that X is non-negative
     e = clone(estimator_orig)
-    msg = 'Unknown label type: '
-    if not _safe_tags(e, "no_validation"):
-        assert_raises_regex(ValueError, msg, e.fit, X, y)
-
-
-@ignore_warnings(category=(DeprecationWarning, FutureWarning))
+    msg = "Unknown label type: "
+    if not _safe_tags(e, key="no_validation"):
+        with raises(ValueError, match=msg):
+            e.fit(X, y)
+
+
+@ignore_warnings(category=FutureWarning)
 def check_decision_proba_consistency(name, estimator_orig):
     # Check whether an estimator having both decision_function and
     # predict_proba methods has outputs with perfect rank correlation.

     centers = [(2, 2), (4, 4)]
-    X, y = make_blobs(n_samples=100, random_state=0, n_features=4,
-                      centers=centers, cluster_std=1.0, shuffle=True)
-    X_test = np.random.randn(20, 2) + 4
+    X, y = make_blobs(
+        n_samples=100,
+        random_state=0,
+        n_features=4,
+        centers=centers,
+        cluster_std=1.0,
+        shuffle=True,
+    )
+    X_train, X_test, y_train, y_test = train_test_split(
+        X, y, test_size=0.2, random_state=0
+    )
     estimator = clone(estimator_orig)

-    if (hasattr(estimator, "decision_function") and
-            hasattr(estimator, "predict_proba")):
-
-        estimator.fit(X, y)
-        a = estimator.predict_proba(X_test)[:, 1]
-        b = estimator.decision_function(X_test)
+    if hasattr(estimator, "decision_function") and hasattr(estimator, "predict_proba"):
+
+        estimator.fit(X_train, y_train)
+        # Since the link function from decision_function() to predict_proba()
+        # is sometimes not precise enough (typically expit), we round to the
+        # 10th decimal to avoid numerical issues: we compare the rank
+        # with deterministic ties rather than get platform specific rank
+        # inversions in case of machine level differences.
+        a = estimator.predict_proba(X_test)[:, 1].round(decimals=10)
+        b = estimator.decision_function(X_test).round(decimals=10)
         assert_array_equal(rankdata(a), rankdata(b))


@@ -2400,13 +3482,13 @@

     y_pred = estimator.fit_predict(X)
     assert y_pred.shape == (n_samples,)
-    assert y_pred.dtype.kind == 'i'
+    assert y_pred.dtype.kind == "i"
     assert_array_equal(np.unique(y_pred), np.array([-1, 1]))

     # check fit_predict = fit.predict when the estimator has both a predict and
     # a fit_predict method. recall that it is already assumed here that the
     # estimator has a fit_predict method
-    if hasattr(estimator, 'predict'):
+    if hasattr(estimator, "predict"):
         y_pred_2 = estimator.fit(X).predict(X)
         assert_array_equal(y_pred, y_pred_2)

@@ -2414,7 +3496,7 @@
         # proportion of outliers equal to contamination parameter when not
         # set to 'auto'
         expected_outliers = 30
-        contamination = float(expected_outliers)/n_samples
+        contamination = float(expected_outliers) / n_samples
         estimator.set_params(contamination=contamination)
         y_pred = estimator.fit_predict(X)

@@ -2423,15 +3505,28 @@
         # there are ties in the decision_function values. this can
         # only be tested for estimators with a decision_function
         # method
-        if (num_outliers != expected_outliers and
-                hasattr(estimator, 'decision_function')):
+        if num_outliers != expected_outliers and hasattr(
+            estimator, "decision_function"
+        ):
             decision = estimator.decision_function(X)
             check_outlier_corruption(num_outliers, expected_outliers, decision)

         # raises error when contamination is a scalar and not in [0,1]
-        for contamination in [-0.5, 2.3]:
+        msg = r"contamination must be in \(0, 0.5]"
+        for contamination in [-0.5, -0.001, 0.5001, 2.3]:
             estimator.set_params(contamination=contamination)
-            assert_raises(ValueError, estimator.fit_predict, X)
+            with raises(ValueError, match=msg):
+                estimator.fit_predict(X)
+
+
+def check_fit_non_negative(name, estimator_orig):
+    # Check that proper warning is raised for non-negative X
+    # when tag requires_positive_X is present
+    X = np.array([[-1.0, 1], [-1.0, 1]])
+    y = np.array([1, 2])
+    estimator = clone(estimator_orig)
+    with raises(ValueError):
+        estimator.fit(X, y)


 def check_fit_idempotent(name, estimator_orig):
@@ -2442,34 +3537,35 @@
     # predict(), predict_proba(), decision_function() and transform() return
     # the same results.

-    check_methods = ["predict", "transform", "decision_function",
-                     "predict_proba"]
+    check_methods = ["predict", "transform", "decision_function", "predict_proba"]
     rng = np.random.RandomState(0)

     estimator = clone(estimator_orig)
     set_random_state(estimator)
-    if 'warm_start' in estimator.get_params().keys():
+    if "warm_start" in estimator.get_params().keys():
         estimator.set_params(warm_start=False)

     n_samples = 100
     X = rng.normal(loc=100, size=(n_samples, 2))
-    X = pairwise_estimator_convert_X(X, estimator)
+    X = _pairwise_estimator_convert_X(X, estimator)
     if is_regressor(estimator_orig):
         y = rng.normal(size=n_samples)
     else:
         y = rng.randint(low=0, high=2, size=n_samples)
-    y = multioutput_estimator_convert_y_2d(estimator, y)
-
-    train, test = next(ShuffleSplit(test_size=.2, random_state=rng).split(X))
+    y = _enforce_estimator_tags_y(estimator, y)
+
+    train, test = next(ShuffleSplit(test_size=0.2, random_state=rng).split(X))
     X_train, y_train = _safe_split(estimator, X, y, train)
     X_test, y_test = _safe_split(estimator, X, y, test, train)

     # Fit for the first time
     estimator.fit(X_train, y_train)

-    result = {method: getattr(estimator, method)(X_test)
-              for method in check_methods
-              if hasattr(estimator, method)}
+    result = {
+        method: getattr(estimator, method)(X_test)
+        for method in check_methods
+        if hasattr(estimator, method)
+    }

     # Fit again
     set_random_state(estimator)
@@ -2478,4 +3574,453 @@
     for method in check_methods:
         if hasattr(estimator, method):
             new_result = getattr(estimator, method)(X_test)
-            assert_allclose_dense_sparse(result[method], new_result)
+            if np.issubdtype(new_result.dtype, np.floating):
+                tol = 2 * np.finfo(new_result.dtype).eps
+            else:
+                tol = 2 * np.finfo(np.float64).eps
+            assert_allclose_dense_sparse(
+                result[method],
+                new_result,
+                atol=max(tol, 1e-9),
+                rtol=max(tol, 1e-7),
+                err_msg="Idempotency check failed for method {}".format(method),
+            )
+
+
+def check_fit_check_is_fitted(name, estimator_orig):
+    # Make sure that estimator doesn't pass check_is_fitted before calling fit
+    # and that passes check_is_fitted once it's fit.
+
+    rng = np.random.RandomState(42)
+
+    estimator = clone(estimator_orig)
+    set_random_state(estimator)
+    if "warm_start" in estimator.get_params():
+        estimator.set_params(warm_start=False)
+
+    n_samples = 100
+    X = rng.normal(loc=100, size=(n_samples, 2))
+    X = _pairwise_estimator_convert_X(X, estimator)
+    if is_regressor(estimator_orig):
+        y = rng.normal(size=n_samples)
+    else:
+        y = rng.randint(low=0, high=2, size=n_samples)
+    y = _enforce_estimator_tags_y(estimator, y)
+
+    if not _safe_tags(estimator).get("stateless", False):
+        # stateless estimators (such as FunctionTransformer) are always "fit"!
+        try:
+            check_is_fitted(estimator)
+            raise AssertionError(
+                f"{estimator.__class__.__name__} passes check_is_fitted before being"
+                " fit!"
+            )
+        except NotFittedError:
+            pass
+    estimator.fit(X, y)
+    try:
+        check_is_fitted(estimator)
+    except NotFittedError as e:
+        raise NotFittedError(
+            "Estimator fails to pass `check_is_fitted` even though it has been fit."
+        ) from e
+
+
+def check_n_features_in(name, estimator_orig):
+    # Make sure that n_features_in_ attribute doesn't exist until fit is
+    # called, and that its value is correct.
+
+    rng = np.random.RandomState(0)
+
+    estimator = clone(estimator_orig)
+    set_random_state(estimator)
+    if "warm_start" in estimator.get_params():
+        estimator.set_params(warm_start=False)
+
+    n_samples = 100
+    X = rng.normal(loc=100, size=(n_samples, 2))
+    X = _pairwise_estimator_convert_X(X, estimator)
+    if is_regressor(estimator_orig):
+        y = rng.normal(size=n_samples)
+    else:
+        y = rng.randint(low=0, high=2, size=n_samples)
+    y = _enforce_estimator_tags_y(estimator, y)
+
+    assert not hasattr(estimator, "n_features_in_")
+    estimator.fit(X, y)
+    assert hasattr(estimator, "n_features_in_")
+    assert estimator.n_features_in_ == X.shape[1]
+
+
+def check_requires_y_none(name, estimator_orig):
+    # Make sure that an estimator with requires_y=True fails gracefully when
+    # given y=None
+
+    rng = np.random.RandomState(0)
+
+    estimator = clone(estimator_orig)
+    set_random_state(estimator)
+
+    n_samples = 100
+    X = rng.normal(loc=100, size=(n_samples, 2))
+    X = _pairwise_estimator_convert_X(X, estimator)
+
+    expected_err_msgs = (
+        "requires y to be passed, but the target y is None",
+        "Expected array-like (array or non-string sequence), got None",
+        "y should be a 1d array",
+    )
+
+    try:
+        estimator.fit(X, None)
+    except ValueError as ve:
+        if not any(msg in str(ve) for msg in expected_err_msgs):
+            raise ve
+
+
+@ignore_warnings(category=FutureWarning)
+def check_n_features_in_after_fitting(name, estimator_orig):
+    # Make sure that n_features_in are checked after fitting
+    tags = _safe_tags(estimator_orig)
+
+    is_supported_X_types = (
+        "2darray" in tags["X_types"] or "categorical" in tags["X_types"]
+    )
+
+    if not is_supported_X_types or tags["no_validation"]:
+        return
+
+    rng = np.random.RandomState(0)
+
+    estimator = clone(estimator_orig)
+    set_random_state(estimator)
+    if "warm_start" in estimator.get_params():
+        estimator.set_params(warm_start=False)
+
+    n_samples = 150
+    X = rng.normal(size=(n_samples, 8))
+    X = _enforce_estimator_tags_x(estimator, X)
+    X = _pairwise_estimator_convert_X(X, estimator)
+
+    if is_regressor(estimator):
+        y = rng.normal(size=n_samples)
+    else:
+        y = rng.randint(low=0, high=2, size=n_samples)
+    y = _enforce_estimator_tags_y(estimator, y)
+
+    estimator.fit(X, y)
+    assert estimator.n_features_in_ == X.shape[1]
+
+    # check methods will check n_features_in_
+    check_methods = [
+        "predict",
+        "transform",
+        "decision_function",
+        "predict_proba",
+        "score",
+    ]
+    X_bad = X[:, [1]]
+
+    msg = f"X has 1 features, but \\w+ is expecting {X.shape[1]} features as input"
+    for method in check_methods:
+        if not hasattr(estimator, method):
+            continue
+
+        callable_method = getattr(estimator, method)
+        if method == "score":
+            callable_method = partial(callable_method, y=y)
+
+        with raises(ValueError, match=msg):
+            callable_method(X_bad)
+
+    # partial_fit will check in the second call
+    if not hasattr(estimator, "partial_fit"):
+        return
+
+    estimator = clone(estimator_orig)
+    if is_classifier(estimator):
+        estimator.partial_fit(X, y, classes=np.unique(y))
+    else:
+        estimator.partial_fit(X, y)
+    assert estimator.n_features_in_ == X.shape[1]
+
+    with raises(ValueError, match=msg):
+        estimator.partial_fit(X_bad, y)
+
+
+def check_estimator_get_tags_default_keys(name, estimator_orig):
+    # check that if _get_tags is implemented, it contains all keys from
+    # _DEFAULT_KEYS
+    estimator = clone(estimator_orig)
+    if not hasattr(estimator, "_get_tags"):
+        return
+
+    tags_keys = set(estimator._get_tags().keys())
+    default_tags_keys = set(_DEFAULT_TAGS.keys())
+    assert tags_keys.intersection(default_tags_keys) == default_tags_keys, (
+        f"{name}._get_tags() is missing entries for the following default tags"
+        f": {default_tags_keys - tags_keys.intersection(default_tags_keys)}"
+    )
+
+
+def check_dataframe_column_names_consistency(name, estimator_orig):
+    try:
+        import pandas as pd
+    except ImportError:
+        raise SkipTest(
+            "pandas is not installed: not checking column name consistency for pandas"
+        )
+
+    tags = _safe_tags(estimator_orig)
+    is_supported_X_types = (
+        "2darray" in tags["X_types"] or "categorical" in tags["X_types"]
+    )
+
+    if not is_supported_X_types or tags["no_validation"]:
+        return
+
+    rng = np.random.RandomState(0)
+
+    estimator = clone(estimator_orig)
+    set_random_state(estimator)
+
+    X_orig = rng.normal(size=(150, 8))
+
+    # Some picky estimators (e.g. SkewedChi2Sampler) only accept skewed positive data.
+    X_orig -= X_orig.min() + 0.5
+    X_orig = _enforce_estimator_tags_x(estimator, X_orig)
+    X_orig = _pairwise_estimator_convert_X(X_orig, estimator)
+    n_samples, n_features = X_orig.shape
+
+    names = np.array([f"col_{i}" for i in range(n_features)])
+    X = pd.DataFrame(X_orig, columns=names)
+
+    if is_regressor(estimator):
+        y = rng.normal(size=n_samples)
+    else:
+        y = rng.randint(low=0, high=2, size=n_samples)
+    y = _enforce_estimator_tags_y(estimator, y)
+
+    # Check that calling `fit` does not raise any warnings about feature names.
+    with warnings.catch_warnings():
+        warnings.filterwarnings(
+            "error",
+            message="X does not have valid feature names",
+            category=UserWarning,
+            module="sklearn",
+        )
+        estimator.fit(X, y)
+
+    if not hasattr(estimator, "feature_names_in_"):
+        raise ValueError(
+            "Estimator does not have a feature_names_in_ "
+            "attribute after fitting with a dataframe"
+        )
+    assert isinstance(estimator.feature_names_in_, np.ndarray)
+    assert estimator.feature_names_in_.dtype == object
+    assert_array_equal(estimator.feature_names_in_, names)
+
+    # Only check sklearn estimators for feature_names_in_ in docstring
+    module_name = estimator_orig.__module__
+    if (
+        module_name.startswith("sklearn.")
+        and not ("test_" in module_name or module_name.endswith("_testing"))
+        and ("feature_names_in_" not in (estimator_orig.__doc__))
+    ):
+        raise ValueError(
+            f"Estimator {name} does not document its feature_names_in_ attribute"
+        )
+
+    check_methods = []
+    for method in (
+        "predict",
+        "transform",
+        "decision_function",
+        "predict_proba",
+        "score",
+        "score_samples",
+        "predict_log_proba",
+    ):
+        if not hasattr(estimator, method):
+            continue
+
+        callable_method = getattr(estimator, method)
+        if method == "score":
+            callable_method = partial(callable_method, y=y)
+        check_methods.append((method, callable_method))
+
+    for _, method in check_methods:
+        with warnings.catch_warnings():
+            warnings.filterwarnings(
+                "error",
+                message="X does not have valid feature names",
+                category=UserWarning,
+                module="sklearn",
+            )
+            method(X)  # works without UserWarning for valid features
+
+    invalid_names = [
+        (names[::-1], "Feature names must be in the same order as they were in fit."),
+        (
+            [f"another_prefix_{i}" for i in range(n_features)],
+            "Feature names unseen at fit time:\n- another_prefix_0\n-"
+            " another_prefix_1\n",
+        ),
+        (
+            names[:3],
+            f"Feature names seen at fit time, yet now missing:\n- {min(names[3:])}\n",
+        ),
+    ]
+    params = {
+        key: value
+        for key, value in estimator.get_params().items()
+        if "early_stopping" in key
+    }
+    early_stopping_enabled = any(value is True for value in params.values())
+
+    for invalid_name, additional_message in invalid_names:
+        X_bad = pd.DataFrame(X, columns=invalid_name)
+
+        expected_msg = re.escape(
+            "The feature names should match those that were passed "
+            "during fit. Starting version 1.2, an error will be raised.\n"
+            f"{additional_message}"
+        )
+        for name, method in check_methods:
+            # TODO In 1.2, this will be an error.
+            with warnings.catch_warnings():
+                warnings.filterwarnings(
+                    "error",
+                    category=FutureWarning,
+                    module="sklearn",
+                )
+                with raises(
+                    FutureWarning, match=expected_msg, err_msg=f"{name} did not raise"
+                ):
+                    method(X_bad)
+
+        # partial_fit checks on second call
+        # Do not call partial fit if early_stopping is on
+        if not hasattr(estimator, "partial_fit") or early_stopping_enabled:
+            continue
+
+        estimator = clone(estimator_orig)
+        if is_classifier(estimator):
+            classes = np.unique(y)
+            estimator.partial_fit(X, y, classes=classes)
+        else:
+            estimator.partial_fit(X, y)
+
+        with warnings.catch_warnings():
+            warnings.filterwarnings("error", category=FutureWarning, module="sklearn")
+            with raises(FutureWarning, match=expected_msg):
+                estimator.partial_fit(X_bad, y)
+
+
+def check_transformer_get_feature_names_out(name, transformer_orig):
+    tags = transformer_orig._get_tags()
+    if "2darray" not in tags["X_types"] or tags["no_validation"]:
+        return
+
+    X, y = make_blobs(
+        n_samples=30,
+        centers=[[0, 0, 0], [1, 1, 1]],
+        random_state=0,
+        n_features=2,
+        cluster_std=0.1,
+    )
+    X = StandardScaler().fit_transform(X)
+    X -= X.min()
+
+    transformer = clone(transformer_orig)
+    X = _enforce_estimator_tags_x(transformer, X)
+    X = _pairwise_estimator_convert_X(X, transformer)
+
+    n_features = X.shape[1]
+    set_random_state(transformer)
+
+    y_ = y
+    if name in CROSS_DECOMPOSITION:
+        y_ = np.c_[np.asarray(y), np.asarray(y)]
+        y_[::2, 1] *= 2
+
+    X_transform = transformer.fit_transform(X, y=y_)
+    input_features = [f"feature{i}" for i in range(n_features)]
+
+    # input_features names is not the same length as n_features_in_
+    with raises(ValueError, match="input_features should have length equal"):
+        transformer.get_feature_names_out(input_features[::2])
+
+    feature_names_out = transformer.get_feature_names_out(input_features)
+    assert feature_names_out is not None
+    assert isinstance(feature_names_out, np.ndarray)
+    assert feature_names_out.dtype == object
+    assert all(isinstance(name, str) for name in feature_names_out)
+
+    if isinstance(X_transform, tuple):
+        n_features_out = X_transform[0].shape[1]
+    else:
+        n_features_out = X_transform.shape[1]
+
+    assert (
+        len(feature_names_out) == n_features_out
+    ), f"Expected {n_features_out} feature names, got {len(feature_names_out)}"
+
+
+def check_transformer_get_feature_names_out_pandas(name, transformer_orig):
+    try:
+        import pandas as pd
+    except ImportError:
+        raise SkipTest(
+            "pandas is not installed: not checking column name consistency for pandas"
+        )
+
+    tags = transformer_orig._get_tags()
+    if "2darray" not in tags["X_types"] or tags["no_validation"]:
+        return
+
+    X, y = make_blobs(
+        n_samples=30,
+        centers=[[0, 0, 0], [1, 1, 1]],
+        random_state=0,
+        n_features=2,
+        cluster_std=0.1,
+    )
+    X = StandardScaler().fit_transform(X)
+    X -= X.min()
+
+    transformer = clone(transformer_orig)
+    X = _enforce_estimator_tags_x(transformer, X)
+    X = _pairwise_estimator_convert_X(X, transformer)
+
+    n_features = X.shape[1]
+    set_random_state(transformer)
+
+    y_ = y
+    if name in CROSS_DECOMPOSITION:
+        y_ = np.c_[np.asarray(y), np.asarray(y)]
+        y_[::2, 1] *= 2
+
+    feature_names_in = [f"col{i}" for i in range(n_features)]
+    df = pd.DataFrame(X, columns=feature_names_in)
+    X_transform = transformer.fit_transform(df, y=y_)
+
+    # error is raised when `input_features` do not match feature_names_in
+    invalid_feature_names = [f"bad{i}" for i in range(n_features)]
+    with raises(ValueError, match="input_features is not equal to feature_names_in_"):
+        transformer.get_feature_names_out(invalid_feature_names)
+
+    feature_names_out_default = transformer.get_feature_names_out()
+    feature_names_in_explicit_names = transformer.get_feature_names_out(
+        feature_names_in
+    )
+    assert_array_equal(feature_names_out_default, feature_names_in_explicit_names)
+
+    if isinstance(X_transform, tuple):
+        n_features_out = X_transform[0].shape[1]
+    else:
+        n_features_out = X_transform.shape[1]
+
+    assert (
+        len(feature_names_out_default) == n_features_out
+    ), f"Expected {n_features_out} feature names, got {len(feature_names_out_default)}"
('sklearn/utils', 'multiclass.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,9 +8,9 @@
 """
 from collections.abc import Sequence
 from itertools import chain
+import warnings

 from scipy.sparse import issparse
-from scipy.sparse.base import spmatrix
 from scipy.sparse import dok_matrix
 from scipy.sparse import lil_matrix

@@ -20,25 +20,27 @@


 def _unique_multiclass(y):
-    if hasattr(y, '__array__'):
+    if hasattr(y, "__array__"):
         return np.unique(np.asarray(y))
     else:
         return set(y)


 def _unique_indicator(y):
-    return np.arange(check_array(y, ['csr', 'csc', 'coo']).shape[1])
+    return np.arange(
+        check_array(y, input_name="y", accept_sparse=["csr", "csc", "coo"]).shape[1]
+    )


 _FN_UNIQUE_LABELS = {
-    'binary': _unique_multiclass,
-    'multiclass': _unique_multiclass,
-    'multilabel-indicator': _unique_indicator,
+    "binary": _unique_multiclass,
+    "multiclass": _unique_multiclass,
+    "multilabel-indicator": _unique_indicator,
 }


 def unique_labels(*ys):
-    """Extract an ordered array of unique labels
+    """Extract an ordered array of unique labels.

     We don't allow:
         - mix of multilabel and multiclass (single label) targets
@@ -55,7 +57,7 @@

     Returns
     -------
-    out : numpy array of shape [n_unique_labels]
+    out : ndarray of shape (n_unique_labels,)
         An ordered array of unique labels.

     Examples
@@ -69,7 +71,7 @@
     array([ 1,  2,  5, 10, 11])
     """
     if not ys:
-        raise ValueError('No argument has been passed.')
+        raise ValueError("No argument has been passed.")
     # Check that we don't mix label format

     ys_types = set(type_of_target(x) for x in ys)
@@ -82,11 +84,18 @@
     label_type = ys_types.pop()

     # Check consistency for the indicator format
-    if (label_type == "multilabel-indicator" and
-            len(set(check_array(y, ['csr', 'csc', 'coo']).shape[1]
-                    for y in ys)) > 1):
-        raise ValueError("Multi-label binary indicator input with "
-                         "different numbers of labels")
+    if (
+        label_type == "multilabel-indicator"
+        and len(
+            set(
+                check_array(y, accept_sparse=["csr", "csc", "coo"]).shape[1] for y in ys
+            )
+        )
+        > 1
+    ):
+        raise ValueError(
+            "Multi-label binary indicator input with different numbers of labels"
+        )

     # Get the unique set of labels
     _unique_labels = _FN_UNIQUE_LABELS.get(label_type, None)
@@ -96,27 +105,27 @@
     ys_labels = set(chain.from_iterable(_unique_labels(y) for y in ys))

     # Check that we don't mix string type with number type
-    if (len(set(isinstance(label, str) for label in ys_labels)) > 1):
+    if len(set(isinstance(label, str) for label in ys_labels)) > 1:
         raise ValueError("Mix of label input types (string and number)")

     return np.array(sorted(ys_labels))


 def _is_integral_float(y):
-    return y.dtype.kind == 'f' and np.all(y.astype(int) == y)
+    return y.dtype.kind == "f" and np.all(y.astype(int) == y)


 def is_multilabel(y):
-    """ Check if ``y`` is in a multilabel format.
-
-    Parameters
-    ----------
-    y : numpy array of shape [n_samples]
+    """Check if ``y`` is in a multilabel format.
+
+    Parameters
+    ----------
+    y : ndarray of shape (n_samples,)
         Target values.

     Returns
     -------
-    out : bool,
+    out : bool
         Return ``True``, if ``y`` is in a multilabel format, else ```False``.

     Examples
@@ -134,22 +143,38 @@
     >>> is_multilabel(np.array([[1, 0, 0]]))
     True
     """
-    if hasattr(y, '__array__'):
-        y = np.asarray(y)
+    if hasattr(y, "__array__") or isinstance(y, Sequence):
+        # DeprecationWarning will be replaced by ValueError, see NEP 34
+        # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html
+        with warnings.catch_warnings():
+            warnings.simplefilter("error", np.VisibleDeprecationWarning)
+            try:
+                y = np.asarray(y)
+            except np.VisibleDeprecationWarning:
+                # dtype=object should be provided explicitly for ragged arrays,
+                # see NEP 34
+                y = np.array(y, dtype=object)
+
     if not (hasattr(y, "shape") and y.ndim == 2 and y.shape[1] > 1):
         return False

     if issparse(y):
         if isinstance(y, (dok_matrix, lil_matrix)):
             y = y.tocsr()
-        return (len(y.data) == 0 or np.unique(y.data).size == 1 and
-                (y.dtype.kind in 'biu' or  # bool, int, uint
-                 _is_integral_float(np.unique(y.data))))
+        return (
+            len(y.data) == 0
+            or np.unique(y.data).size == 1
+            and (
+                y.dtype.kind in "biu"
+                or _is_integral_float(np.unique(y.data))  # bool, int, uint
+            )
+        )
     else:
         labels = np.unique(y)

-        return len(labels) < 3 and (y.dtype.kind in 'biu' or  # bool, int, uint
-                                    _is_integral_float(labels))
+        return len(labels) < 3 and (
+            y.dtype.kind in "biu" or _is_integral_float(labels)  # bool, int, uint
+        )


 def check_classification_targets(y):
@@ -162,14 +187,20 @@
     Parameters
     ----------
     y : array-like
-    """
-    y_type = type_of_target(y)
-    if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',
-                      'multilabel-indicator', 'multilabel-sequences']:
+        Target values.
+    """
+    y_type = type_of_target(y, input_name="y")
+    if y_type not in [
+        "binary",
+        "multiclass",
+        "multiclass-multioutput",
+        "multilabel-indicator",
+        "multilabel-sequences",
+    ]:
         raise ValueError("Unknown label type: %r" % y_type)


-def type_of_target(y):
+def type_of_target(y, input_name=""):
     """Determine the type of data indicated by the target.

     Note that this type is the most specific type that can be inferred.
@@ -185,9 +216,14 @@
     ----------
     y : array-like

+    input_name : str, default=""
+        The data name used to construct the error message.
+
+        .. versionadded:: 1.1.0
+
     Returns
     -------
-    target_type : string
+    target_type : str
         One of:

         * 'continuous': `y` is an array-like of floats that are not all
@@ -209,6 +245,7 @@

     Examples
     --------
+    >>> from sklearn.utils.multiclass import type_of_target
     >>> import numpy as np
     >>> type_of_target([0.1, 0.6])
     'continuous'
@@ -227,51 +264,62 @@
     >>> type_of_target(np.array([[1, 2], [3, 1]]))
     'multiclass-multioutput'
     >>> type_of_target([[1, 2]])
-    'multiclass-multioutput'
+    'multilabel-indicator'
     >>> type_of_target(np.array([[1.5, 2.0], [3.0, 1.6]]))
     'continuous-multioutput'
     >>> type_of_target(np.array([[0, 1], [1, 1]]))
     'multilabel-indicator'
     """
-    valid = ((isinstance(y, (Sequence, spmatrix)) or hasattr(y, '__array__'))
-             and not isinstance(y, str))
+    valid = (
+        isinstance(y, Sequence) or issparse(y) or hasattr(y, "__array__")
+    ) and not isinstance(y, str)

     if not valid:
-        raise ValueError('Expected array-like (array or non-string sequence), '
-                         'got %r' % y)
-
-    sparseseries = (y.__class__.__name__ == 'SparseSeries')
-    if sparseseries:
-        raise ValueError("y cannot be class 'SparseSeries'.")
+        raise ValueError(
+            "Expected array-like (array or non-string sequence), got %r" % y
+        )
+
+    sparse_pandas = y.__class__.__name__ in ["SparseSeries", "SparseArray"]
+    if sparse_pandas:
+        raise ValueError("y cannot be class 'SparseSeries' or 'SparseArray'")

     if is_multilabel(y):
-        return 'multilabel-indicator'
-
-    try:
-        y = np.asarray(y)
-    except ValueError:
-        # Known to fail in numpy 1.3 for array of arrays
-        return 'unknown'
+        return "multilabel-indicator"
+
+    # DeprecationWarning will be replaced by ValueError, see NEP 34
+    # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html
+    with warnings.catch_warnings():
+        warnings.simplefilter("error", np.VisibleDeprecationWarning)
+        try:
+            y = np.asarray(y)
+        except np.VisibleDeprecationWarning:
+            # dtype=object should be provided explicitly for ragged arrays,
+            # see NEP 34
+            y = np.asarray(y, dtype=object)

     # The old sequence of sequences format
     try:
-        if (not hasattr(y[0], '__array__') and isinstance(y[0], Sequence)
-                and not isinstance(y[0], str)):
-            raise ValueError('You appear to be using a legacy multi-label data'
-                             ' representation. Sequence of sequences are no'
-                             ' longer supported; use a binary array or sparse'
-                             ' matrix instead - the MultiLabelBinarizer'
-                             ' transformer can convert to this format.')
+        if (
+            not hasattr(y[0], "__array__")
+            and isinstance(y[0], Sequence)
+            and not isinstance(y[0], str)
+        ):
+            raise ValueError(
+                "You appear to be using a legacy multi-label data"
+                " representation. Sequence of sequences are no"
+                " longer supported; use a binary array or sparse"
+                " matrix instead - the MultiLabelBinarizer"
+                " transformer can convert to this format."
+            )
     except IndexError:
         pass

     # Invalid inputs
-    if y.ndim > 2 or (y.dtype == object and len(y) and
-                      not isinstance(y.flat[0], str)):
-        return 'unknown'  # [[[1, 2]]] or [obj_1] and not ["label_1"]
+    if y.ndim > 2 or (y.dtype == object and len(y) and not isinstance(y.flat[0], str)):
+        return "unknown"  # [[[1, 2]]] or [obj_1] and not ["label_1"]

     if y.ndim == 2 and y.shape[1] == 0:
-        return 'unknown'  # [[]]
+        return "unknown"  # [[]]

     if y.ndim == 2 and y.shape[1] > 1:
         suffix = "-multioutput"  # [[1, 2], [1, 2]]
@@ -279,19 +327,19 @@
         suffix = ""  # [1, 2, 3] or [[1], [2], [3]]

     # check float and contains non-integer float values
-    if y.dtype.kind == 'f' and np.any(y != y.astype(int)):
+    if y.dtype.kind == "f" and np.any(y != y.astype(int)):
         # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]
-        _assert_all_finite(y)
-        return 'continuous' + suffix
+        _assert_all_finite(y, input_name=input_name)
+        return "continuous" + suffix

     if (len(np.unique(y)) > 2) or (y.ndim >= 2 and len(y[0]) > 1):
-        return 'multiclass' + suffix  # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]
+        return "multiclass" + suffix  # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]
     else:
-        return 'binary'  # [1, 2] or [["a"], ["b"]]
+        return "binary"  # [1, 2] or [["a"], ["b"]]


 def _check_partial_fit_first_call(clf, classes=None):
-    """Private helper function for factorizing common classes param logic
+    """Private helper function for factorizing common classes param logic.

     Estimators that implement the ``partial_fit`` API need to be provided with
     the list of possible classes at the first call to partial_fit.
@@ -304,16 +352,16 @@
     set on ``clf``.

     """
-    if getattr(clf, 'classes_', None) is None and classes is None:
-        raise ValueError("classes must be passed on the first call "
-                         "to partial_fit.")
+    if getattr(clf, "classes_", None) is None and classes is None:
+        raise ValueError("classes must be passed on the first call to partial_fit.")

     elif classes is not None:
-        if getattr(clf, 'classes_', None) is not None:
+        if getattr(clf, "classes_", None) is not None:
             if not np.array_equal(clf.classes_, unique_labels(classes)):
                 raise ValueError(
                     "`classes=%r` is not the same as on last call "
-                    "to partial_fit, was: %r" % (classes, clf.classes_))
+                    "to partial_fit, was: %r" % (classes, clf.classes_)
+                )

         else:
             # This is the first call to partial_fit
@@ -326,25 +374,25 @@


 def class_distribution(y, sample_weight=None):
-    """Compute class priors from multioutput-multiclass target data
-
-    Parameters
-    ----------
-    y : array like or sparse matrix of size (n_samples, n_outputs)
+    """Compute class priors from multioutput-multiclass target data.
+
+    Parameters
+    ----------
+    y : {array-like, sparse matrix} of size (n_samples, n_outputs)
         The labels for each example.

-    sample_weight : array-like of shape = (n_samples,), optional
+    sample_weight : array-like of shape (n_samples,), default=None
         Sample weights.

     Returns
     -------
-    classes : list of size n_outputs of arrays of size (n_classes,)
+    classes : list of size n_outputs of ndarray of size (n_classes,)
         List of classes for each column.

-    n_classes : list of integers of size n_outputs
-        Number of classes in each column
-
-    class_prior : list of size n_outputs of arrays of size (n_classes,)
+    n_classes : list of int of size n_outputs
+        Number of classes in each column.
+
+    class_prior : list of size n_outputs of ndarray of size (n_classes,)
         Class distribution of each column.

     """
@@ -353,24 +401,26 @@
     class_prior = []

     n_samples, n_outputs = y.shape
+    if sample_weight is not None:
+        sample_weight = np.asarray(sample_weight)

     if issparse(y):
         y = y.tocsc()
         y_nnz = np.diff(y.indptr)

         for k in range(n_outputs):
-            col_nonzero = y.indices[y.indptr[k]:y.indptr[k + 1]]
+            col_nonzero = y.indices[y.indptr[k] : y.indptr[k + 1]]
             # separate sample weights for zero and non-zero elements
             if sample_weight is not None:
-                nz_samp_weight = np.asarray(sample_weight)[col_nonzero]
-                zeros_samp_weight_sum = (np.sum(sample_weight) -
-                                         np.sum(nz_samp_weight))
+                nz_samp_weight = sample_weight[col_nonzero]
+                zeros_samp_weight_sum = np.sum(sample_weight) - np.sum(nz_samp_weight)
             else:
                 nz_samp_weight = None
                 zeros_samp_weight_sum = y.shape[0] - y_nnz[k]

-            classes_k, y_k = np.unique(y.data[y.indptr[k]:y.indptr[k + 1]],
-                                       return_inverse=True)
+            classes_k, y_k = np.unique(
+                y.data[y.indptr[k] : y.indptr[k + 1]], return_inverse=True
+            )
             class_prior_k = np.bincount(y_k, weights=nz_samp_weight)

             # An explicit zero was found, combine its weight with the weight
@@ -382,8 +432,7 @@
             # class_prior, make an entry for it
             if 0 not in classes_k and y_nnz[k] < y.shape[0]:
                 classes_k = np.insert(classes_k, 0, 0)
-                class_prior_k = np.insert(class_prior_k, 0,
-                                          zeros_samp_weight_sum)
+                class_prior_k = np.insert(class_prior_k, 0, zeros_samp_weight_sum)

             classes.append(classes_k)
             n_classes.append(classes_k.shape[0])
@@ -407,16 +456,16 @@

     Parameters
     ----------
-    predictions : array-like, shape (n_samples, n_classifiers)
+    predictions : array-like of shape (n_samples, n_classifiers)
         Predicted classes for each binary classifier.

-    confidences : array-like, shape (n_samples, n_classifiers)
+    confidences : array-like of shape (n_samples, n_classifiers)
         Decision functions or predicted probabilities for positive class
         for each binary classifier.

     n_classes : int
         Number of classes. n_classifiers must be
-        ``n_classes * (n_classes - 1 ) / 2``
+        ``n_classes * (n_classes - 1 ) / 2``.
     """
     n_samples = predictions.shape[0]
     votes = np.zeros((n_samples, n_classes))
@@ -438,6 +487,7 @@
     # The motivation is to use confidence levels as a way to break ties in
     # the votes without switching any decision made based on a difference
     # of 1 vote.
-    transformed_confidences = (sum_of_confidences /
-                               (3 * (np.abs(sum_of_confidences) + 1)))
+    transformed_confidences = sum_of_confidences / (
+        3 * (np.abs(sum_of_confidences) + 1)
+    )
     return votes + transformed_confidences
('sklearn/utils', 'graph.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -10,29 +10,32 @@
 #          Jake Vanderplas <vanderplas@astro.washington.edu>
 # License: BSD 3 clause

+import numpy as np
 from scipy import sparse

-from .graph_shortest_path import graph_shortest_path  # noqa
+from .deprecation import deprecated
+from ..metrics.pairwise import pairwise_distances


 ###############################################################################
 # Path and connected component analysis.
 # Code adapted from networkx
-
-def single_source_shortest_path_length(graph, source, cutoff=None):
+def single_source_shortest_path_length(graph, source, *, cutoff=None):
     """Return the shortest path length from source to all reachable nodes.

     Returns a dictionary of shortest path lengths keyed by target.

     Parameters
     ----------
-    graph : sparse matrix or 2D array (preferably LIL matrix)
-        Adjacency matrix of the graph
-    source : integer
-       Starting node for path
-    cutoff : integer, optional
-        Depth to stop the search - only
-        paths of length <= cutoff are returned.
+    graph : {sparse matrix, ndarray} of shape (n, n)
+        Adjacency matrix of the graph. Sparse matrix of format LIL is
+        preferred.
+
+    source : int
+       Starting node for path.
+
+    cutoff : int, default=None
+        Depth to stop the search - only paths of length <= cutoff are returned.

     Examples
     --------
@@ -52,17 +55,150 @@
         graph = graph.tolil()
     else:
         graph = sparse.lil_matrix(graph)
-    seen = {}                   # level (number of hops) when seen in BFS
-    level = 0                   # the current level
-    next_level = [source]       # dict of nodes to check at next level
+    seen = {}  # level (number of hops) when seen in BFS
+    level = 0  # the current level
+    next_level = [source]  # dict of nodes to check at next level
     while next_level:
-        this_level = next_level     # advance to next level
-        next_level = set()          # and start a new list (fringe)
+        this_level = next_level  # advance to next level
+        next_level = set()  # and start a new list (fringe)
         for v in this_level:
             if v not in seen:
-                seen[v] = level     # set the level of vertex v
+                seen[v] = level  # set the level of vertex v
                 next_level.update(graph.rows[v])
         if cutoff is not None and cutoff <= level:
             break
         level += 1
     return seen  # return all path lengths as dictionary
+
+
+@deprecated(
+    "`graph_shortest_path` is deprecated in 1.0 (renaming of 0.25) and will "
+    "be removed in 1.2. Use `scipy.sparse.csgraph.shortest_path` instead."
+)
+def graph_shortest_path(dist_matrix, directed=True, method="auto"):
+    """Shortest-path graph search on a positive directed or undirected graph.
+
+    Parameters
+    ----------
+    dist_matrix : arraylike or sparse matrix, shape = (N,N)
+        Array of positive distances.
+        If vertex i is connected to vertex j, then dist_matrix[i,j] gives
+        the distance between the vertices.
+        If vertex i is not connected to vertex j, then dist_matrix[i,j] = 0
+
+    directed : boolean
+        if True, then find the shortest path on a directed graph: only
+        progress from a point to its neighbors, not the other way around.
+        if False, then find the shortest path on an undirected graph: the
+        algorithm can progress from a point to its neighbors and vice versa.
+
+    method : {'auto', 'FW', 'D'}, default='auto'
+        method to use.  Options are
+        'auto' : attempt to choose the best method for the current problem
+        'FW' : Floyd-Warshall algorithm.  O[N^3]
+        'D' : Dijkstra's algorithm with Fibonacci stacks.  O[(k+log(N))N^2]
+
+    Returns
+    -------
+    G : np.ndarray, float, shape = [N,N]
+        G[i,j] gives the shortest distance from point i to point j
+        along the graph.
+
+    Notes
+    -----
+    As currently implemented, Dijkstra's algorithm does not work for
+    graphs with direction-dependent distances when directed == False.
+    i.e., if dist_matrix[i,j] and dist_matrix[j,i] are not equal and
+    both are nonzero, method='D' will not necessarily yield the correct
+    result.
+    Also, these routines have not been tested for graphs with negative
+    distances.  Negative distances can lead to infinite cycles that must
+    be handled by specialized algorithms.
+    """
+    return sparse.csgraph.shortest_path(dist_matrix, method=method, directed=directed)
+
+
+def _fix_connected_components(
+    X,
+    graph,
+    n_connected_components,
+    component_labels,
+    mode="distance",
+    metric="euclidean",
+    **kwargs,
+):
+    """Add connections to sparse graph to connect unconnected components.
+
+    For each pair of unconnected components, compute all pairwise distances
+    from one component to the other, and add a connection on the closest pair
+    of samples. This is a hacky way to get a graph with a single connected
+    component, which is necessary for example to compute a shortest path
+    between all pairs of samples in the graph.
+
+    Parameters
+    ----------
+    X : array of shape (n_samples, n_features) or (n_samples, n_samples)
+        Features to compute the pairwise distances. If `metric =
+        "precomputed"`, X is the matrix of pairwise distances.
+
+    graph : sparse matrix of shape (n_samples, n_samples)
+        Graph of connection between samples.
+
+    n_connected_components : int
+        Number of connected components, as computed by
+        `scipy.sparse.csgraph.connected_components`.
+
+    component_labels : array of shape (n_samples)
+        Labels of connected components, as computed by
+        `scipy.sparse.csgraph.connected_components`.
+
+    mode : {'connectivity', 'distance'}, default='distance'
+        Type of graph matrix: 'connectivity' corresponds to the connectivity
+        matrix with ones and zeros, and 'distance' corresponds to the distances
+        between neighbors according to the given metric.
+
+    metric : str
+        Metric used in `sklearn.metrics.pairwise.pairwise_distances`.
+
+    kwargs : kwargs
+        Keyword arguments passed to
+        `sklearn.metrics.pairwise.pairwise_distances`.
+
+    Returns
+    -------
+    graph : sparse matrix of shape (n_samples, n_samples)
+        Graph of connection between samples, with a single connected component.
+    """
+    if metric == "precomputed" and sparse.issparse(X):
+        raise RuntimeError(
+            "_fix_connected_components with metric='precomputed' requires the "
+            "full distance matrix in X, and does not work with a sparse "
+            "neighbors graph."
+        )
+
+    for i in range(n_connected_components):
+        idx_i = np.flatnonzero(component_labels == i)
+        Xi = X[idx_i]
+        for j in range(i):
+            idx_j = np.flatnonzero(component_labels == j)
+            Xj = X[idx_j]
+
+            if metric == "precomputed":
+                D = X[np.ix_(idx_i, idx_j)]
+            else:
+                D = pairwise_distances(Xi, Xj, metric=metric, **kwargs)
+
+            ii, jj = np.unravel_index(D.argmin(axis=None), D.shape)
+            if mode == "connectivity":
+                graph[idx_i[ii], idx_j[jj]] = 1
+                graph[idx_j[jj], idx_i[ii]] = 1
+            elif mode == "distance":
+                graph[idx_i[ii], idx_j[jj]] = D[ii, jj]
+                graph[idx_j[jj], idx_i[ii]] = D[ii, jj]
+            else:
+                raise ValueError(
+                    "Unknown mode=%r, should be one of ['connectivity', 'distance']."
+                    % mode
+                )
+
+    return graph
('sklearn/utils', '_joblib.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,4 +1,3 @@
-import os as _os
 import warnings as _warnings

 with _warnings.catch_warnings():
@@ -15,6 +14,18 @@
     from joblib import parallel_backend, register_parallel_backend


-__all__ = ["parallel_backend", "register_parallel_backend", "cpu_count",
-           "Parallel", "Memory", "delayed", "effective_n_jobs", "hash",
-           "logger", "dump", "load", "joblib", "__version__"]
+__all__ = [
+    "parallel_backend",
+    "register_parallel_backend",
+    "cpu_count",
+    "Parallel",
+    "Memory",
+    "delayed",
+    "effective_n_jobs",
+    "hash",
+    "logger",
+    "dump",
+    "load",
+    "joblib",
+    "__version__",
+]
('sklearn/utils', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,12 +1,21 @@
 """
 The :mod:`sklearn.utils` module includes various utilities.
 """
+import pkgutil
+import inspect
+from importlib import import_module
+from operator import itemgetter
 from collections.abc import Sequence
 from contextlib import contextmanager
+from itertools import compress
+from itertools import islice
+import math
 import numbers
 import platform
 import struct
 import timeit
+from pathlib import Path
+from contextlib import suppress

 import warnings
 import numpy as np
@@ -17,12 +26,22 @@
 from . import _joblib
 from ..exceptions import DataConversionWarning
 from .deprecation import deprecated
-from .validation import (as_float_array,
-                         assert_all_finite,
-                         check_random_state, column_or_1d, check_array,
-                         check_consistent_length, check_X_y, indexable,
-                         check_symmetric, check_scalar)
+from .fixes import parse_version, threadpool_info
+from ._estimator_html_repr import estimator_html_repr
+from .validation import (
+    as_float_array,
+    assert_all_finite,
+    check_random_state,
+    column_or_1d,
+    check_array,
+    check_consistent_length,
+    check_X_y,
+    indexable,
+    check_symmetric,
+    check_scalar,
+)
 from .. import get_config
+from ._bunch import Bunch


 # Do not deprecate parallel_backend and register_parallel_backend as they are
@@ -33,89 +52,68 @@
 parallel_backend = _joblib.parallel_backend
 register_parallel_backend = _joblib.register_parallel_backend

-# deprecate the joblib API in sklearn in favor of using directly joblib
-msg = ("deprecated in version 0.20.1 to be removed in version 0.23. "
-       "Please import this functionality directly from joblib, which can "
-       "be installed with: pip install joblib.")
-deprecate = deprecated(msg)
-
-delayed = deprecate(_joblib.delayed)
-cpu_count = deprecate(_joblib.cpu_count)
-hash = deprecate(_joblib.hash)
-effective_n_jobs = deprecate(_joblib.effective_n_jobs)
-
-
-# for classes, deprecated will change the object in _joblib module so we need
-# to subclass them.
-@deprecate
-class Memory(_joblib.Memory):
-    pass
-
-
-@deprecate
-class Parallel(_joblib.Parallel):
-    pass
-
-
-__all__ = ["murmurhash3_32", "as_float_array",
-           "assert_all_finite", "check_array",
-           "check_random_state",
-           "compute_class_weight", "compute_sample_weight",
-           "column_or_1d", "safe_indexing",
-           "check_consistent_length", "check_X_y", "check_scalar", 'indexable',
-           "check_symmetric", "indices_to_mask", "deprecated",
-           "cpu_count", "Parallel", "Memory", "delayed", "parallel_backend",
-           "register_parallel_backend", "hash", "effective_n_jobs",
-           "resample", "shuffle", "check_matplotlib_support"]
-
-IS_PYPY = platform.python_implementation() == 'PyPy'
+__all__ = [
+    "murmurhash3_32",
+    "as_float_array",
+    "assert_all_finite",
+    "check_array",
+    "check_random_state",
+    "compute_class_weight",
+    "compute_sample_weight",
+    "column_or_1d",
+    "check_consistent_length",
+    "check_X_y",
+    "check_scalar",
+    "indexable",
+    "check_symmetric",
+    "indices_to_mask",
+    "deprecated",
+    "parallel_backend",
+    "register_parallel_backend",
+    "resample",
+    "shuffle",
+    "check_matplotlib_support",
+    "all_estimators",
+    "DataConversionWarning",
+    "estimator_html_repr",
+    "Bunch",
+]
+
+IS_PYPY = platform.python_implementation() == "PyPy"
 _IS_32BIT = 8 * struct.calcsize("P") == 32


-class Bunch(dict):
-    """Container object for datasets
-
-    Dictionary-like object that exposes its keys as attributes.
-
-    >>> b = Bunch(a=1, b=2)
-    >>> b['b']
-    2
-    >>> b.b
-    2
-    >>> b.a = 3
-    >>> b['a']
-    3
-    >>> b.c = 6
-    >>> b['c']
-    6
-
-    """
-
-    def __init__(self, **kwargs):
-        super().__init__(kwargs)
-
-    def __setattr__(self, key, value):
-        self[key] = value
-
-    def __dir__(self):
-        return self.keys()
-
-    def __getattr__(self, key):
-        try:
-            return self[key]
-        except KeyError:
-            raise AttributeError(key)
-
-    def __setstate__(self, state):
-        # Bunch pickles generated with scikit-learn 0.16.* have an non
-        # empty __dict__. This causes a surprising behaviour when
-        # loading these pickles scikit-learn 0.17: reading bunch.key
-        # uses __dict__ but assigning to bunch.key use __setattr__ and
-        # only changes bunch['key']. More details can be found at:
-        # https://github.com/scikit-learn/scikit-learn/issues/6196.
-        # Overriding __setstate__ to be a noop has the effect of
-        # ignoring the pickled __dict__
-        pass
+def _in_unstable_openblas_configuration():
+    """Return True if in an unstable configuration for OpenBLAS"""
+
+    # Import libraries which might load OpenBLAS.
+    import numpy  # noqa
+    import scipy  # noqa
+
+    modules_info = threadpool_info()
+
+    open_blas_used = any(info["internal_api"] == "openblas" for info in modules_info)
+    if not open_blas_used:
+        return False
+
+    # OpenBLAS 0.3.16 fixed unstability for arm64, see:
+    # https://github.com/xianyi/OpenBLAS/blob/1b6db3dbba672b4f8af935bd43a1ff6cff4d20b7/Changelog.txt#L56-L58 # noqa
+    openblas_arm64_stable_version = parse_version("0.3.16")
+    for info in modules_info:
+        if info["internal_api"] != "openblas":
+            continue
+        openblas_version = info.get("version")
+        openblas_architecture = info.get("architecture")
+        if openblas_version is None or openblas_architecture is None:
+            # Cannot be sure that OpenBLAS is good enough. Assume unstable:
+            return True
+        if (
+            openblas_architecture == "neoversen1"
+            and parse_version(openblas_version) < openblas_arm64_stable_version
+        ):
+            # See discussions in https://github.com/numpy/numpy/issues/19411
+            return True
+    return False


 def safe_mask(X, mask):
@@ -126,7 +124,7 @@
     X : {array-like, sparse matrix}
         Data on which to apply mask.

-    mask : array
+    mask : ndarray
         Mask to be used on X.

     Returns
@@ -163,7 +161,7 @@
     X : {array-like, sparse matrix}
         Data on which to apply mask.

-    mask : array
+    mask : ndarray
         Mask to be used on X.

     len_mask : int
@@ -178,89 +176,300 @@
     return np.zeros(shape=(0, X.shape[1]))


-def safe_indexing(X, indices):
-    """Return items or rows from X using indices.
-
-    Allows simple indexing of lists or arrays.
-
-    Parameters
-    ----------
-    X : array-like, sparse-matrix, list, pandas.DataFrame, pandas.Series.
-        Data from which to sample rows or items.
-    indices : array-like of int
-        Indices according to which X will be subsampled.
+def _array_indexing(array, key, key_dtype, axis):
+    """Index an array or scipy.sparse consistently across NumPy version."""
+    if issparse(array) and key_dtype == "bool":
+        key = np.asarray(key)
+    if isinstance(key, tuple):
+        key = list(key)
+    return array[key] if axis == 0 else array[:, key]
+
+
+def _pandas_indexing(X, key, key_dtype, axis):
+    """Index a pandas dataframe or a series."""
+    if hasattr(key, "shape"):
+        # Work-around for indexing with read-only key in pandas
+        # FIXME: solved in pandas 0.25
+        key = np.asarray(key)
+        key = key if key.flags.writeable else key.copy()
+    elif isinstance(key, tuple):
+        key = list(key)
+
+    if key_dtype == "int" and not (isinstance(key, slice) or np.isscalar(key)):
+        # using take() instead of iloc[] ensures the return value is a "proper"
+        # copy that will not raise SettingWithCopyWarning
+        return X.take(key, axis=axis)
+    else:
+        # check whether we should index with loc or iloc
+        indexer = X.iloc if key_dtype == "int" else X.loc
+        return indexer[:, key] if axis else indexer[key]
+
+
+def _list_indexing(X, key, key_dtype):
+    """Index a Python list."""
+    if np.isscalar(key) or isinstance(key, slice):
+        # key is a slice or a scalar
+        return X[key]
+    if key_dtype == "bool":
+        # key is a boolean array-like
+        return list(compress(X, key))
+    # key is a integer array-like of key
+    return [X[idx] for idx in key]
+
+
+def _determine_key_type(key, accept_slice=True):
+    """Determine the data type of key.
+
+    Parameters
+    ----------
+    key : scalar, slice or array-like
+        The key from which we want to infer the data type.
+
+    accept_slice : bool, default=True
+        Whether or not to raise an error if the key is a slice.
+
+    Returns
+    -------
+    dtype : {'int', 'str', 'bool', None}
+        Returns the data type of key.
+    """
+    err_msg = (
+        "No valid specification of the columns. Only a scalar, list or "
+        "slice of all integers or all strings, or boolean mask is "
+        "allowed"
+    )
+
+    dtype_to_str = {int: "int", str: "str", bool: "bool", np.bool_: "bool"}
+    array_dtype_to_str = {
+        "i": "int",
+        "u": "int",
+        "b": "bool",
+        "O": "str",
+        "U": "str",
+        "S": "str",
+    }
+
+    if key is None:
+        return None
+    if isinstance(key, tuple(dtype_to_str.keys())):
+        try:
+            return dtype_to_str[type(key)]
+        except KeyError:
+            raise ValueError(err_msg)
+    if isinstance(key, slice):
+        if not accept_slice:
+            raise TypeError(
+                "Only array-like or scalar are supported. A Python slice was given."
+            )
+        if key.start is None and key.stop is None:
+            return None
+        key_start_type = _determine_key_type(key.start)
+        key_stop_type = _determine_key_type(key.stop)
+        if key_start_type is not None and key_stop_type is not None:
+            if key_start_type != key_stop_type:
+                raise ValueError(err_msg)
+        if key_start_type is not None:
+            return key_start_type
+        return key_stop_type
+    if isinstance(key, (list, tuple)):
+        unique_key = set(key)
+        key_type = {_determine_key_type(elt) for elt in unique_key}
+        if not key_type:
+            return None
+        if len(key_type) != 1:
+            raise ValueError(err_msg)
+        return key_type.pop()
+    if hasattr(key, "dtype"):
+        try:
+            return array_dtype_to_str[key.dtype.kind]
+        except KeyError:
+            raise ValueError(err_msg)
+    raise ValueError(err_msg)
+
+
+def _safe_indexing(X, indices, *, axis=0):
+    """Return rows, items or columns of X using indices.
+
+    .. warning::
+
+        This utility is documented, but **private**. This means that
+        backward compatibility might be broken without any deprecation
+        cycle.
+
+    Parameters
+    ----------
+    X : array-like, sparse-matrix, list, pandas.DataFrame, pandas.Series
+        Data from which to sample rows, items or columns. `list` are only
+        supported when `axis=0`.
+    indices : bool, int, str, slice, array-like
+        - If `axis=0`, boolean and integer array-like, integer slice,
+          and scalar integer are supported.
+        - If `axis=1`:
+            - to select a single column, `indices` can be of `int` type for
+              all `X` types and `str` only for dataframe. The selected subset
+              will be 1D, unless `X` is a sparse matrix in which case it will
+              be 2D.
+            - to select multiples columns, `indices` can be one of the
+              following: `list`, `array`, `slice`. The type used in
+              these containers can be one of the following: `int`, 'bool' and
+              `str`. However, `str` is only supported when `X` is a dataframe.
+              The selected subset will be 2D.
+    axis : int, default=0
+        The axis along which `X` will be subsampled. `axis=0` will select
+        rows while `axis=1` will select columns.

     Returns
     -------
     subset
-        Subset of X on first axis
+        Subset of X on axis 0 or 1.

     Notes
     -----
     CSR, CSC, and LIL sparse matrices are supported. COO sparse matrices are
     not supported.
     """
+    if indices is None:
+        return X
+
+    if axis not in (0, 1):
+        raise ValueError(
+            "'axis' should be either 0 (to index rows) or 1 (to index "
+            " column). Got {} instead.".format(axis)
+        )
+
+    indices_dtype = _determine_key_type(indices)
+
+    if axis == 0 and indices_dtype == "str":
+        raise ValueError("String indexing is not supported with 'axis=0'")
+
+    if axis == 1 and X.ndim != 2:
+        raise ValueError(
+            "'X' should be a 2D NumPy array, 2D sparse matrix or pandas "
+            "dataframe when indexing the columns (i.e. 'axis=1'). "
+            "Got {} instead with {} dimension(s).".format(type(X), X.ndim)
+        )
+
+    if axis == 1 and indices_dtype == "str" and not hasattr(X, "loc"):
+        raise ValueError(
+            "Specifying the columns using strings is only supported for "
+            "pandas DataFrames"
+        )
+
     if hasattr(X, "iloc"):
-        # Work-around for indexing with read-only indices in pandas
-        indices = indices if indices.flags.writeable else indices.copy()
-        # Pandas Dataframes and Series
+        return _pandas_indexing(X, indices, indices_dtype, axis=axis)
+    elif hasattr(X, "shape"):
+        return _array_indexing(X, indices, indices_dtype, axis=axis)
+    else:
+        return _list_indexing(X, indices, indices_dtype)
+
+
+def _get_column_indices(X, key):
+    """Get feature column indices for input data X and key.
+
+    For accepted values of `key`, see the docstring of
+    :func:`_safe_indexing_column`.
+    """
+    n_columns = X.shape[1]
+
+    key_dtype = _determine_key_type(key)
+
+    if isinstance(key, (list, tuple)) and not key:
+        # we get an empty list
+        return []
+    elif key_dtype in ("bool", "int"):
+        # Convert key into positive indexes
         try:
-            return X.iloc[indices]
-        except ValueError:
-            # Cython typed memoryviews internally used in pandas do not support
-            # readonly buffers.
-            warnings.warn("Copying input dataframe for slicing.",
-                          DataConversionWarning)
-            return X.copy().iloc[indices]
-    elif hasattr(X, "shape"):
-        if hasattr(X, 'take') and (hasattr(indices, 'dtype') and
-                                   indices.dtype.kind == 'i'):
-            # This is often substantially faster than X[indices]
-            return X.take(indices, axis=0)
+            idx = _safe_indexing(np.arange(n_columns), key)
+        except IndexError as e:
+            raise ValueError(
+                "all features must be in [0, {}] or [-{}, 0]".format(
+                    n_columns - 1, n_columns
+                )
+            ) from e
+        return np.atleast_1d(idx).tolist()
+    elif key_dtype == "str":
+        try:
+            all_columns = X.columns
+        except AttributeError:
+            raise ValueError(
+                "Specifying the columns using strings is only "
+                "supported for pandas DataFrames"
+            )
+        if isinstance(key, str):
+            columns = [key]
+        elif isinstance(key, slice):
+            start, stop = key.start, key.stop
+            if start is not None:
+                start = all_columns.get_loc(start)
+            if stop is not None:
+                # pandas indexing with strings is endpoint included
+                stop = all_columns.get_loc(stop) + 1
+            else:
+                stop = n_columns + 1
+            return list(range(n_columns)[slice(start, stop)])
         else:
-            return X[indices]
+            columns = list(key)
+
+        try:
+            column_indices = []
+            for col in columns:
+                col_idx = all_columns.get_loc(col)
+                if not isinstance(col_idx, numbers.Integral):
+                    raise ValueError(
+                        f"Selected columns, {columns}, are not unique in dataframe"
+                    )
+                column_indices.append(col_idx)
+
+        except KeyError as e:
+            raise ValueError("A given column is not a column of the dataframe") from e
+
+        return column_indices
     else:
-        return [X[idx] for idx in indices]
-
-
-def resample(*arrays, **options):
-    """Resample arrays or sparse matrices in a consistent way
+        raise ValueError(
+            "No valid specification of the columns. Only a "
+            "scalar, list or slice of all integers or all "
+            "strings, or boolean mask is allowed"
+        )
+
+
+def resample(*arrays, replace=True, n_samples=None, random_state=None, stratify=None):
+    """Resample arrays or sparse matrices in a consistent way.

     The default strategy implements one step of the bootstrapping
     procedure.

     Parameters
     ----------
-    *arrays : sequence of indexable data-structures
+    *arrays : sequence of array-like of shape (n_samples,) or \
+            (n_samples, n_outputs)
         Indexable data-structures can be arrays, lists, dataframes or scipy
         sparse matrices with consistent first dimension.

-    Other Parameters
-    ----------------
-    replace : boolean, True by default
+    replace : bool, default=True
         Implements resampling with replacement. If False, this will implement
         (sliced) random permutations.

-    n_samples : int, None by default
+    n_samples : int, default=None
         Number of samples to generate. If left to None this is
         automatically set to the first dimension of the arrays.
         If replace is False it should not be larger than the length of
         arrays.

-    random_state : int, RandomState instance or None, optional (default=None)
-        The seed of the pseudo random number generator to use when shuffling
-        the data.  If int, random_state is the seed used by the random number
-        generator; If RandomState instance, random_state is the random number
-        generator; If None, the random number generator is the RandomState
-        instance used by `np.random`.
-
-    stratify : array-like or None (default=None)
+    random_state : int, RandomState instance or None, default=None
+        Determines random number generation for shuffling
+        the data.
+        Pass an int for reproducible results across multiple function calls.
+        See :term:`Glossary <random_state>`.
+
+    stratify : array-like of shape (n_samples,) or (n_samples, n_outputs), \
+            default=None
         If not None, data is split in a stratified fashion, using this as
         the class labels.

     Returns
     -------
-    resampled_arrays : sequence of indexable data-structures
+    resampled_arrays : sequence of array-like of shape (n_samples,) or \
+            (n_samples, n_outputs)
         Sequence of resampled copies of the collections. The original arrays
         are not impacted.

@@ -268,6 +477,7 @@
     --------
     It is possible to mix sparse and dense arrays in the same run::

+      >>> import numpy as np
       >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])
       >>> y = np.array([0, 1, 2])

@@ -281,7 +491,7 @@
              [2., 1.],
              [1., 0.]])

-      >>> X_sparse                   # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
+      >>> X_sparse
       <3x2 sparse matrix of type '<... 'numpy.float64'>'
           with 4 stored elements in Compressed Sparse Row format>

@@ -303,31 +513,26 @@
       ...          random_state=0)
       [1, 1, 1, 0, 1]

-
-    See also
+    See Also
     --------
-    :func:`sklearn.utils.shuffle`
-    """
-
-    random_state = check_random_state(options.pop('random_state', None))
-    replace = options.pop('replace', True)
-    max_n_samples = options.pop('n_samples', None)
-    stratify = options.pop('stratify', None)
-    if options:
-        raise ValueError("Unexpected kw arguments: %r" % options.keys())
+    shuffle
+    """
+    max_n_samples = n_samples
+    random_state = check_random_state(random_state)

     if len(arrays) == 0:
         return None

     first = arrays[0]
-    n_samples = first.shape[0] if hasattr(first, 'shape') else len(first)
+    n_samples = first.shape[0] if hasattr(first, "shape") else len(first)

     if max_n_samples is None:
         max_n_samples = n_samples
     elif (max_n_samples > n_samples) and (not replace):
-        raise ValueError("Cannot sample %d out of arrays with dim %d "
-                         "when replace is False" % (max_n_samples,
-                                                    n_samples))
+        raise ValueError(
+            "Cannot sample %d out of arrays with dim %d when replace is False"
+            % (max_n_samples, n_samples)
+        )

     check_consistent_length(*arrays)

@@ -344,7 +549,7 @@
         if y.ndim == 2:
             # for multi-label y, map each distinct row to a string repr
             # using join because str(row) uses an ellipsis if len(row) > 1000
-            y = np.array([' '.join(row.astype('str')) for row in y])
+            y = np.array([" ".join(row.astype("str")) for row in y])

         classes, y_indices = np.unique(y, return_inverse=True)
         n_classes = classes.shape[0]
@@ -353,24 +558,23 @@

         # Find the sorted list of instances for each class:
         # (np.unique above performs a sort, so code is O(n logn) already)
-        class_indices = np.split(np.argsort(y_indices, kind='mergesort'),
-                                 np.cumsum(class_counts)[:-1])
+        class_indices = np.split(
+            np.argsort(y_indices, kind="mergesort"), np.cumsum(class_counts)[:-1]
+        )

         n_i = _approximate_mode(class_counts, max_n_samples, random_state)

         indices = []

         for i in range(n_classes):
-            indices_i = random_state.choice(class_indices[i], n_i[i],
-                                            replace=replace)
+            indices_i = random_state.choice(class_indices[i], n_i[i], replace=replace)
             indices.extend(indices_i)

         indices = random_state.permutation(indices)
-

     # convert sparse matrices to CSR for row-based indexing
     arrays = [a.tocsr() if issparse(a) else a for a in arrays]
-    resampled_arrays = [safe_indexing(a, indices) for a in arrays]
+    resampled_arrays = [_safe_indexing(a, indices) for a in arrays]
     if len(resampled_arrays) == 1:
         # syntactic sugar for the unit argument case
         return resampled_arrays[0]
@@ -378,8 +582,8 @@
         return resampled_arrays


-def shuffle(*arrays, **options):
-    """Shuffle arrays or sparse matrices in a consistent way
+def shuffle(*arrays, random_state=None, n_samples=None):
+    """Shuffle arrays or sparse matrices in a consistent way.

     This is a convenience alias to ``resample(*arrays, replace=False)`` to do
     random permutations of the collections.
@@ -390,18 +594,16 @@
         Indexable data-structures can be arrays, lists, dataframes or scipy
         sparse matrices with consistent first dimension.

-    Other Parameters
-    ----------------
-    random_state : int, RandomState instance or None, optional (default=None)
-        The seed of the pseudo random number generator to use when shuffling
-        the data.  If int, random_state is the seed used by the random number
-        generator; If RandomState instance, random_state is the random number
-        generator; If None, the random number generator is the RandomState
-        instance used by `np.random`.
-
-    n_samples : int, None by default
+    random_state : int, RandomState instance or None, default=None
+        Determines random number generation for shuffling
+        the data.
+        Pass an int for reproducible results across multiple function calls.
+        See :term:`Glossary <random_state>`.
+
+    n_samples : int, default=None
         Number of samples to generate. If left to None this is
-        automatically set to the first dimension of the arrays.
+        automatically set to the first dimension of the arrays.  It should
+        not be larger than the length of arrays.

     Returns
     -------
@@ -413,6 +615,7 @@
     --------
     It is possible to mix sparse and dense arrays in the same run::

+      >>> import numpy as np
       >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]])
       >>> y = np.array([0, 1, 2])

@@ -426,7 +629,7 @@
              [2., 1.],
              [1., 0.]])

-      >>> X_sparse                   # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
+      >>> X_sparse
       <3x2 sparse matrix of type '<... 'numpy.float64'>'
           with 3 stored elements in Compressed Sparse Row format>

@@ -441,22 +644,23 @@
       >>> shuffle(y, n_samples=2, random_state=0)
       array([0, 1])

-    See also
+    See Also
     --------
-    :func:`sklearn.utils.resample`
-    """
-    options['replace'] = False
-    return resample(*arrays, **options)
-
-
-def safe_sqr(X, copy=True):
+    resample
+    """
+    return resample(
+        *arrays, replace=False, n_samples=n_samples, random_state=random_state
+    )
+
+
+def safe_sqr(X, *, copy=True):
     """Element wise squaring of array-likes and sparse matrices.

     Parameters
     ----------
-    X : array like, matrix, sparse matrix
-
-    copy : boolean, optional, default True
+    X : {array-like, ndarray, sparse matrix}
+
+    copy : bool, default=True
         Whether to create a copy of X and operate on it or to perform
         inplace computation (default behaviour).

@@ -464,20 +668,31 @@
     -------
     X ** 2 : element wise square
     """
-    X = check_array(X, accept_sparse=['csr', 'csc', 'coo'], ensure_2d=False)
+    X = check_array(X, accept_sparse=["csr", "csc", "coo"], ensure_2d=False)
     if issparse(X):
         if copy:
             X = X.copy()
         X.data **= 2
     else:
         if copy:
-            X = X ** 2
+            X = X**2
         else:
             X **= 2
     return X


-def gen_batches(n, batch_size, min_batch_size=0):
+def _chunk_generator(gen, chunksize):
+    """Chunk generator, ``gen`` into lists of length ``chunksize``. The last
+    chunk may have a length less than ``chunksize``."""
+    while True:
+        chunk = list(islice(gen, chunksize))
+        if chunk:
+            yield chunk
+        else:
+            return
+
+
+def gen_batches(n, batch_size, *, min_batch_size=0):
     """Generator to create slices containing batch_size elements, from 0 to n.

     The last slice may contain less than batch_size elements, when batch_size
@@ -487,13 +702,17 @@
     ----------
     n : int
     batch_size : int
-        Number of element in each batch
+        Number of element in each batch.
     min_batch_size : int, default=0
         Minimum batch size to produce.

     Yields
     ------
     slice of batch_size elements
+
+    See Also
+    --------
+    gen_even_slices: Generator to create n_packs slices going up to n.

     Examples
     --------
@@ -509,6 +728,12 @@
     >>> list(gen_batches(7, 3, min_batch_size=2))
     [slice(0, 3, None), slice(3, 7, None)]
     """
+    if not isinstance(batch_size, numbers.Integral):
+        raise TypeError(
+            "gen_batches got batch_size=%s, must be an integer" % batch_size
+        )
+    if batch_size <= 0:
+        raise ValueError("gen_batches got batch_size=%s, must be positive" % batch_size)
     start = 0
     for _ in range(int(n // batch_size)):
         end = start + batch_size
@@ -520,7 +745,7 @@
         yield slice(start, n)


-def gen_even_slices(n, n_packs, n_samples=None):
+def gen_even_slices(n, n_packs, *, n_samples=None):
     """Generator to create n_packs slices going up to n.

     Parameters
@@ -528,7 +753,7 @@
     n : int
     n_packs : int
         Number of slices to generate.
-    n_samples : int or None (default = None)
+    n_samples : int, default=None
         Number of samples. Pass n_samples when the slices are to be used for
         sparse matrix indexing; slicing off-the-end raises an exception, while
         it works for NumPy arrays.
@@ -536,23 +761,27 @@
     Yields
     ------
     slice
+
+    See Also
+    --------
+    gen_batches: Generator to create slices containing batch_size elements
+        from 0 to n.

     Examples
     --------
     >>> from sklearn.utils import gen_even_slices
     >>> list(gen_even_slices(10, 1))
     [slice(0, 10, None)]
-    >>> list(gen_even_slices(10, 10))                     #doctest: +ELLIPSIS
+    >>> list(gen_even_slices(10, 10))
     [slice(0, 1, None), slice(1, 2, None), ..., slice(9, 10, None)]
-    >>> list(gen_even_slices(10, 5))                      #doctest: +ELLIPSIS
+    >>> list(gen_even_slices(10, 5))
     [slice(0, 2, None), slice(2, 4, None), ..., slice(8, 10, None)]
     >>> list(gen_even_slices(10, 3))
     [slice(0, 4, None), slice(4, 7, None), slice(7, 10, None)]
     """
     start = 0
     if n_packs < 1:
-        raise ValueError("gen_even_slices got n_packs=%s, must be >=1"
-                         % n_packs)
+        raise ValueError("gen_even_slices got n_packs=%s, must be >=1" % n_packs)
     for pack_num in range(n_packs):
         this_n = n // n_packs
         if pack_num < n % n_packs:
@@ -571,6 +800,14 @@
     Parameters
     ----------
     x : iterable
+        The iterable to be converted.
+
+    Returns
+    -------
+    x : Sequence
+        If `x` is a NumPy array, it returns it as a `ndarray`. If `x`
+        is a `Sequence`, `x` is returned as-is. If `x` is from any other
+        type, `x` is returned casted as a list.
     """
     if isinstance(x, np.ndarray):
         return np.asarray(x)
@@ -580,6 +817,42 @@
         return list(x)


+def _to_object_array(sequence):
+    """Convert sequence to a 1-D NumPy array of object dtype.
+
+    numpy.array constructor has a similar use but it's output
+    is ambiguous. It can be 1-D NumPy array of object dtype if
+    the input is a ragged array, but if the input is a list of
+    equal length arrays, then the output is a 2D numpy.array.
+    _to_object_array solves this ambiguity by guarantying that
+    the output is a 1-D NumPy array of objects for any input.
+
+    Parameters
+    ----------
+    sequence : array-like of shape (n_elements,)
+        The sequence to be converted.
+
+    Returns
+    -------
+    out : ndarray of shape (n_elements,), dtype=object
+        The converted sequence into a 1-D NumPy array of object dtype.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.utils import _to_object_array
+    >>> _to_object_array([np.array([0]), np.array([1])])
+    array([array([0]), array([1])], dtype=object)
+    >>> _to_object_array([np.array([0]), np.array([1, 2])])
+    array([array([0]), array([1, 2])], dtype=object)
+    >>> _to_object_array([np.array([0]), np.array([1, 2])])
+    array([array([0]), array([1, 2])], dtype=object)
+    """
+    out = np.empty(len(sequence), dtype=object)
+    out[:] = sequence
+    return out
+
+
 def indices_to_mask(indices, mask_length):
     """Convert list of indices to boolean mask.

@@ -589,7 +862,7 @@
         List of integers treated as indices.
     mask_length : int
         Length of boolean mask to be generated.
-        This parameter must be greater than max(indices)
+        This parameter must be greater than max(indices).

     Returns
     -------
@@ -606,25 +879,25 @@
     if mask_length <= np.max(indices):
         raise ValueError("mask_length must be greater than max(indices)")

-    mask = np.zeros(mask_length, dtype=np.bool)
+    mask = np.zeros(mask_length, dtype=bool)
     mask[indices] = True

     return mask


 def _message_with_time(source, message, time):
-    """Create one line message for logging purposes
+    """Create one line message for logging purposes.

     Parameters
     ----------
     source : str
-        String indicating the source or the reference of the message
+        String indicating the source or the reference of the message.

     message : str
-        Short message
+        Short message.

     time : int
-        Time in seconds
+        Time in seconds.
     """
     start_message = "[%s] " % source

@@ -635,81 +908,105 @@
     else:
         time_str = " %5.1fs" % time
     end_message = " %s, total=%s" % (message, time_str)
-    dots_len = (70 - len(start_message) - len(end_message))
-    return "%s%s%s" % (start_message, dots_len * '.', end_message)
+    dots_len = 70 - len(start_message) - len(end_message)
+    return "%s%s%s" % (start_message, dots_len * ".", end_message)


 @contextmanager
 def _print_elapsed_time(source, message=None):
-    """Log elapsed time to stdout when the context is exited
+    """Log elapsed time to stdout when the context is exited.

     Parameters
     ----------
     source : str
-        String indicating the source or the reference of the message
-
-    message : str or None
-        Short message. If None, nothing will be printed
+        String indicating the source or the reference of the message.
+
+    message : str, default=None
+        Short message. If None, nothing will be printed.

     Returns
     -------
     context_manager
-        Prints elapsed time upon exit if verbose
+        Prints elapsed time upon exit if verbose.
     """
     if message is None:
         yield
     else:
         start = timeit.default_timer()
         yield
-        print(
-            _message_with_time(source, message,
-                               timeit.default_timer() - start))
-
-
-def get_chunk_n_rows(row_bytes, max_n_rows=None,
-                     working_memory=None):
-    """Calculates how many rows can be processed within working_memory
+        print(_message_with_time(source, message, timeit.default_timer() - start))
+
+
+def get_chunk_n_rows(row_bytes, *, max_n_rows=None, working_memory=None):
+    """Calculate how many rows can be processed within `working_memory`.

     Parameters
     ----------
     row_bytes : int
         The expected number of bytes of memory that will be consumed
         during the processing of each row.
-    max_n_rows : int, optional
+    max_n_rows : int, default=None
         The maximum return value.
-    working_memory : int or float, optional
-        The number of rows to fit inside this number of MiB will be returned.
-        When None (default), the value of
+    working_memory : int or float, default=None
+        The number of rows to fit inside this number of MiB will be
+        returned. When None (default), the value of
         ``sklearn.get_config()['working_memory']`` is used.

     Returns
     -------
-    int or the value of n_samples
+    int
+        The number of rows which can be processed within `working_memory`.

     Warns
     -----
-    Issues a UserWarning if ``row_bytes`` exceeds ``working_memory`` MiB.
+    Issues a UserWarning if `row_bytes exceeds `working_memory` MiB.
     """

     if working_memory is None:
-        working_memory = get_config()['working_memory']
-
-    chunk_n_rows = int(working_memory * (2 ** 20) // row_bytes)
+        working_memory = get_config()["working_memory"]
+
+    chunk_n_rows = int(working_memory * (2**20) // row_bytes)
     if max_n_rows is not None:
         chunk_n_rows = min(chunk_n_rows, max_n_rows)
     if chunk_n_rows < 1:
-        warnings.warn('Could not adhere to working_memory config. '
-                      'Currently %.0fMiB, %.0fMiB required.' %
-                      (working_memory, np.ceil(row_bytes * 2 ** -20)))
+        warnings.warn(
+            "Could not adhere to working_memory config. "
+            "Currently %.0fMiB, %.0fMiB required."
+            % (working_memory, np.ceil(row_bytes * 2**-20))
+        )
         chunk_n_rows = 1
     return chunk_n_rows


+def _is_pandas_na(x):
+    """Test if x is pandas.NA.
+
+    We intentionally do not use this function to return `True` for `pd.NA` in
+    `is_scalar_nan`, because estimators that support `pd.NA` are the exception
+    rather than the rule at the moment. When `pd.NA` is more universally
+    supported, we may reconsider this decision.
+
+    Parameters
+    ----------
+    x : any type
+
+    Returns
+    -------
+    boolean
+    """
+    with suppress(ImportError):
+        from pandas import NA
+
+        return x is NA
+
+    return False
+
+
 def is_scalar_nan(x):
-    """Tests if x is NaN
+    """Tests if x is NaN.

     This function is meant to overcome the issue that np.isnan does not allow
-    non-numerical types as input, and that np.nan is not np.float('nan').
+    non-numerical types as input, and that np.nan is not float('nan').

     Parameters
     ----------
@@ -721,6 +1018,8 @@

     Examples
     --------
+    >>> import numpy as np
+    >>> from sklearn.utils import is_scalar_nan
     >>> is_scalar_nan(np.nan)
     True
     >>> is_scalar_nan(float("nan"))
@@ -732,9 +1031,7 @@
     >>> is_scalar_nan([np.nan])
     False
     """
-    # convert from numpy.bool_ to python bool to ensure that testing
-    # is_scalar_nan(x) is True does not fail.
-    return bool(isinstance(x, numbers.Real) and np.isnan(x))
+    return isinstance(x, numbers.Real) and math.isnan(x)


 def _approximate_mode(class_counts, n_draws, rng):
@@ -780,7 +1077,7 @@
     rng = check_random_state(rng)
     # this computes a bad approximation to the mode of the
     # multivariate hypergeometric given by class_counts and n_draws
-    continuous = n_draws * class_counts / class_counts.sum()
+    continuous = class_counts / class_counts.sum() * n_draws
     # floored means we don't overshoot n_samples, but probably undershoot
     floored = np.floor(continuous)
     # we add samples according to how much "left over" probability
@@ -792,7 +1089,7 @@
         # add according to remainder, but break ties
         # randomly to avoid biases
         for value in values:
-            inds, = np.where(remainder == value)
+            (inds,) = np.where(remainder == value)
             # if we need_to_add less than what's in inds
             # we draw randomly from them.
             # if we need to add more, we add them all and
@@ -803,13 +1100,13 @@
             need_to_add -= add_now
             if need_to_add == 0:
                 break
-    return floored.astype(np.int)
+    return floored.astype(int)


 def check_matplotlib_support(caller_name):
     """Raise ImportError with detailed error message if mpl is not installed.

-    Plot utilities like :func:`plot_partial_dependence` should lazily import
+    Plot utilities like any of the Display's plotting functions should lazily import
     matplotlib and call this helper before any computation.

     Parameters
@@ -824,3 +1121,136 @@
             "{} requires matplotlib. You can install matplotlib with "
             "`pip install matplotlib`".format(caller_name)
         ) from e
+
+
+def check_pandas_support(caller_name):
+    """Raise ImportError with detailed error message if pandas is not installed.
+
+    Plot utilities like :func:`fetch_openml` should lazily import
+    pandas and call this helper before any computation.
+
+    Parameters
+    ----------
+    caller_name : str
+        The name of the caller that requires pandas.
+
+    Returns
+    -------
+    pandas
+        The pandas package.
+    """
+    try:
+        import pandas  # noqa
+
+        return pandas
+    except ImportError as e:
+        raise ImportError("{} requires pandas.".format(caller_name)) from e
+
+
+def all_estimators(type_filter=None):
+    """Get a list of all estimators from sklearn.
+
+    This function crawls the module and gets all classes that inherit
+    from BaseEstimator. Classes that are defined in test-modules are not
+    included.
+
+    Parameters
+    ----------
+    type_filter : {"classifier", "regressor", "cluster", "transformer"} \
+            or list of such str, default=None
+        Which kind of estimators should be returned. If None, no filter is
+        applied and all estimators are returned.  Possible values are
+        'classifier', 'regressor', 'cluster' and 'transformer' to get
+        estimators only of these specific types, or a list of these to
+        get the estimators that fit at least one of the types.
+
+    Returns
+    -------
+    estimators : list of tuples
+        List of (name, class), where ``name`` is the class name as string
+        and ``class`` is the actual type of the class.
+    """
+    # lazy import to avoid circular imports from sklearn.base
+    from ._testing import ignore_warnings
+    from ..base import (
+        BaseEstimator,
+        ClassifierMixin,
+        RegressorMixin,
+        TransformerMixin,
+        ClusterMixin,
+    )
+
+    def is_abstract(c):
+        if not (hasattr(c, "__abstractmethods__")):
+            return False
+        if not len(c.__abstractmethods__):
+            return False
+        return True
+
+    all_classes = []
+    modules_to_ignore = {
+        "tests",
+        "externals",
+        "setup",
+        "conftest",
+        "enable_hist_gradient_boosting",
+    }
+    root = str(Path(__file__).parent.parent)  # sklearn package
+    # Ignore deprecation warnings triggered at import time and from walking
+    # packages
+    with ignore_warnings(category=FutureWarning):
+        for importer, modname, ispkg in pkgutil.walk_packages(
+            path=[root], prefix="sklearn."
+        ):
+            mod_parts = modname.split(".")
+            if any(part in modules_to_ignore for part in mod_parts) or "._" in modname:
+                continue
+            module = import_module(modname)
+            classes = inspect.getmembers(module, inspect.isclass)
+            classes = [
+                (name, est_cls) for name, est_cls in classes if not name.startswith("_")
+            ]
+            all_classes.extend(classes)
+
+    all_classes = set(all_classes)
+
+    estimators = [
+        c
+        for c in all_classes
+        if (issubclass(c[1], BaseEstimator) and c[0] != "BaseEstimator")
+    ]
+    # get rid of abstract base classes
+    estimators = [c for c in estimators if not is_abstract(c[1])]
+
+    if type_filter is not None:
+        if not isinstance(type_filter, list):
+            type_filter = [type_filter]
+        else:
+            type_filter = list(type_filter)  # copy
+        filtered_estimators = []
+        filters = {
+            "classifier": ClassifierMixin,
+            "regressor": RegressorMixin,
+            "transformer": TransformerMixin,
+            "cluster": ClusterMixin,
+        }
+        for name, mixin in filters.items():
+            if name in type_filter:
+                type_filter.remove(name)
+                filtered_estimators.extend(
+                    [est for est in estimators if issubclass(est[1], mixin)]
+                )
+        estimators = filtered_estimators
+        if type_filter:
+            raise ValueError(
+                "Parameter type_filter must be 'classifier', "
+                "'regressor', 'transformer', 'cluster' or "
+                "None, got"
+                " %s."
+                % repr(type_filter)
+            )
+
+    # drop duplicates, sort for reproducibility
+    # itemgetter is used to ensure the sort does not extend to the 2nd item of
+    # the tuple
+    return sorted(set(estimators), key=itemgetter(0))
('sklearn/utils', '_pprint.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -63,7 +63,7 @@
 # - long sequences (lists, tuples, dict items) with more than N elements are
 #   shortened using ellipsis (', ...') at the end.

-from inspect import signature
+import inspect
 import pprint
 from collections import OrderedDict

@@ -74,6 +74,7 @@

 class KeyValTuple(tuple):
     """Dummy class for correctly rendering key-value tuples from dicts."""
+
     def __repr__(self):
         # needed for _dispatch[tuple.__repr__] not to be overridden
         return super().__repr__()
@@ -81,6 +82,7 @@

 class KeyValTupleParam(KeyValTuple):
     """Dummy class for correctly rendering key-value tuples from parameters."""
+
     pass


@@ -89,16 +91,26 @@
     estimator with non-default values."""

     params = estimator.get_params(deep=False)
-    filtered_params = {}
-    init_func = getattr(estimator.__init__, 'deprecated_original',
-                        estimator.__init__)
-    init_params = signature(init_func).parameters
+    init_func = getattr(estimator.__init__, "deprecated_original", estimator.__init__)
+    init_params = inspect.signature(init_func).parameters
     init_params = {name: param.default for name, param in init_params.items()}
-    for k, v in params.items():
-        if (repr(v) != repr(init_params[k]) and
-                not (is_scalar_nan(init_params[k]) and is_scalar_nan(v))):
-            filtered_params[k] = v
-    return filtered_params
+
+    def has_changed(k, v):
+        if k not in init_params:  # happens if k is part of a **kwargs
+            return True
+        if init_params[k] == inspect._empty:  # k has no default value
+            return True
+        # try to avoid calling repr on nested estimators
+        if isinstance(v, BaseEstimator) and v.__class__ != init_params[k].__class__:
+            return True
+        # Use repr as a last resort. It may be expensive.
+        if repr(v) != repr(init_params[k]) and not (
+            is_scalar_nan(init_params[k]) and is_scalar_nan(v)
+        ):
+            return True
+        return False
+
+    return {k: v for k, v in params.items() if has_changed(k, v)}


 class _EstimatorPrettyPrinter(pprint.PrettyPrinter):
@@ -117,7 +129,7 @@
       here)
     - format() directly calls _safe_repr() for a first try at rendering the
       object
-    - _safe_repr formats the whole object reccursively, only calling itself,
+    - _safe_repr formats the whole object recursively, only calling itself,
       not caring about line length or anything
     - back to _format(), if the output string is too long, _format() then calls
       the appropriate _pprint_TYPE() method (e.g. _pprint_list()) depending on
@@ -152,26 +164,34 @@
     KeyValTupleParam for this.
     """

-    def __init__(self, indent=1, width=80, depth=None, stream=None, *,
-                 compact=False, indent_at_name=True,
-                 n_max_elements_to_show=None):
+    def __init__(
+        self,
+        indent=1,
+        width=80,
+        depth=None,
+        stream=None,
+        *,
+        compact=False,
+        indent_at_name=True,
+        n_max_elements_to_show=None,
+    ):
         super().__init__(indent, width, depth, stream, compact=compact)
         self._indent_at_name = indent_at_name
         if self._indent_at_name:
             self._indent_per_level = 1  # ignore indent param
-        self._changed_only = get_config()['print_changed_only']
+        self._changed_only = get_config()["print_changed_only"]
         # Max number of elements in a list, dict, tuple until we start using
         # ellipsis. This also affects the number of arguments of an estimators
         # (they are treated as dicts)
         self.n_max_elements_to_show = n_max_elements_to_show

     def format(self, object, context, maxlevels, level):
-        return _safe_repr(object, context, maxlevels, level,
-                          changed_only=self._changed_only)
-
-    def _pprint_estimator(self, object, stream, indent, allowance, context,
-                          level):
-        stream.write(object.__class__.__name__ + '(')
+        return _safe_repr(
+            object, context, maxlevels, level, changed_only=self._changed_only
+        )
+
+    def _pprint_estimator(self, object, stream, indent, allowance, context, level):
+        stream.write(object.__class__.__name__ + "(")
         if self._indent_at_name:
             indent += len(object.__class__.__name__)

@@ -180,24 +200,26 @@
         else:
             params = object.get_params(deep=False)

-        params = OrderedDict((name, val)
-                             for (name, val) in sorted(params.items()))
-
-        self._format_params(params.items(), stream, indent, allowance + 1,
-                            context, level)
-        stream.write(')')
-
-    def _format_dict_items(self, items, stream, indent, allowance, context,
-                           level):
+        params = OrderedDict((name, val) for (name, val) in sorted(params.items()))
+
+        self._format_params(
+            params.items(), stream, indent, allowance + 1, context, level
+        )
+        stream.write(")")
+
+    def _format_dict_items(self, items, stream, indent, allowance, context, level):
         return self._format_params_or_dict_items(
-            items, stream, indent, allowance, context, level, is_dict=True)
+            items, stream, indent, allowance, context, level, is_dict=True
+        )

     def _format_params(self, items, stream, indent, allowance, context, level):
         return self._format_params_or_dict_items(
-            items, stream, indent, allowance, context, level, is_dict=False)
-
-    def _format_params_or_dict_items(self, object, stream, indent, allowance,
-                                     context, level, is_dict):
+            items, stream, indent, allowance, context, level, is_dict=False
+        )
+
+    def _format_params_or_dict_items(
+        self, object, stream, indent, allowance, context, level, is_dict
+    ):
         """Format dict items or parameters respecting the compact=True
         parameter. For some reason, the builtin rendering of dict items doesn't
         respect compact=True and will use one line per key-value if all cannot
@@ -210,8 +232,8 @@
         """
         write = stream.write
         indent += self._indent_per_level
-        delimnl = ',\n' + ' ' * indent
-        delim = ''
+        delimnl = ",\n" + " " * indent
+        delim = ""
         width = max_width = self._width - indent + 1
         it = iter(object)
         try:
@@ -222,7 +244,7 @@
         n_items = 0
         while not last:
             if n_items == self.n_max_elements_to_show:
-                write(', ...')
+                write(", ...")
                 break
             n_items += 1
             ent = next_ent
@@ -238,7 +260,7 @@
                 vrepr = self._repr(v, context, level)
                 if not is_dict:
                     krepr = krepr.strip("'")
-                middle = ': ' if is_dict else '='
+                middle = ": " if is_dict else "="
                 rep = krepr + middle + vrepr
                 w = len(rep) + 2
                 if width < w:
@@ -248,14 +270,15 @@
                 if width >= w:
                     width -= w
                     write(delim)
-                    delim = ', '
+                    delim = ", "
                     write(rep)
                     continue
             write(delim)
             delim = delimnl
             class_ = KeyValTuple if is_dict else KeyValTupleParam
-            self._format(class_(ent), stream, indent,
-                         allowance if last else 1, context, level)
+            self._format(
+                class_(ent), stream, indent, allowance if last else 1, context, level
+            )

     def _format_items(self, items, stream, indent, allowance, context, level):
         """Format the items of an iterable (list, tuple...). Same as the
@@ -265,9 +288,9 @@
         write = stream.write
         indent += self._indent_per_level
         if self._indent_per_level > 1:
-            write((self._indent_per_level - 1) * ' ')
-        delimnl = ',\n' + ' ' * indent
-        delim = ''
+            write((self._indent_per_level - 1) * " ")
+        delimnl = ",\n" + " " * indent
+        delim = ""
         width = max_width = self._width - indent + 1
         it = iter(items)
         try:
@@ -278,7 +301,7 @@
         n_items = 0
         while not last:
             if n_items == self.n_max_elements_to_show:
-                write(', ...')
+                write(", ...")
                 break
             n_items += 1
             ent = next_ent
@@ -298,33 +321,33 @@
                 if width >= w:
                     width -= w
                     write(delim)
-                    delim = ', '
+                    delim = ", "
                     write(rep)
                     continue
             write(delim)
             delim = delimnl
-            self._format(ent, stream, indent,
-                         allowance if last else 1, context, level)
-
-    def _pprint_key_val_tuple(self, object, stream, indent, allowance, context,
-                              level):
+            self._format(ent, stream, indent, allowance if last else 1, context, level)
+
+    def _pprint_key_val_tuple(self, object, stream, indent, allowance, context, level):
         """Pretty printing for key-value tuples from dict or parameters."""
         k, v = object
         rep = self._repr(k, context, level)
         if isinstance(object, KeyValTupleParam):
             rep = rep.strip("'")
-            middle = '='
+            middle = "="
         else:
-            middle = ': '
+            middle = ": "
         stream.write(rep)
         stream.write(middle)
-        self._format(v, stream, indent + len(rep) + len(middle), allowance,
-                     context, level)
+        self._format(
+            v, stream, indent + len(rep) + len(middle), allowance, context, level
+        )

     # Note: need to copy _dispatch to prevent instances of the builtin
     # PrettyPrinter class to call methods of _EstimatorPrettyPrinter (see issue
     # 12906)
-    _dispatch = pprint.PrettyPrinter._dispatch.copy()
+    # mypy error: "Type[PrettyPrinter]" has no attribute "_dispatch"
+    _dispatch = pprint.PrettyPrinter._dispatch.copy()  # type: ignore
     _dispatch[BaseEstimator.__repr__] = _pprint_estimator
     _dispatch[KeyValTuple.__repr__] = _pprint_key_val_tuple

@@ -356,9 +379,11 @@
         items = sorted(object.items(), key=pprint._safe_tuple)
         for k, v in items:
             krepr, kreadable, krecur = saferepr(
-                k, context, maxlevels, level, changed_only=changed_only)
+                k, context, maxlevels, level, changed_only=changed_only
+            )
             vrepr, vreadable, vrecur = saferepr(
-                v, context, maxlevels, level, changed_only=changed_only)
+                v, context, maxlevels, level, changed_only=changed_only
+            )
             append("%s: %s" % (krepr, vrepr))
             readable = readable and kreadable and vreadable
             if krecur or vrecur:
@@ -366,8 +391,9 @@
         del context[objid]
         return "{%s}" % ", ".join(components), readable, recursive

-    if (issubclass(typ, list) and r is list.__repr__) or \
-       (issubclass(typ, tuple) and r is tuple.__repr__):
+    if (issubclass(typ, list) and r is list.__repr__) or (
+        issubclass(typ, tuple) and r is tuple.__repr__
+    ):
         if issubclass(typ, list):
             if not object:
                 return "[]", True, False
@@ -391,7 +417,8 @@
         level += 1
         for o in object:
             orepr, oreadable, orecur = _safe_repr(
-                o, context, maxlevels, level, changed_only=changed_only)
+                o, context, maxlevels, level, changed_only=changed_only
+            )
             append(orepr)
             if not oreadable:
                 readable = False
@@ -420,16 +447,17 @@
         items = sorted(params.items(), key=pprint._safe_tuple)
         for k, v in items:
             krepr, kreadable, krecur = saferepr(
-                k, context, maxlevels, level, changed_only=changed_only)
+                k, context, maxlevels, level, changed_only=changed_only
+            )
             vrepr, vreadable, vrecur = saferepr(
-                v, context, maxlevels, level, changed_only=changed_only)
+                v, context, maxlevels, level, changed_only=changed_only
+            )
             append("%s=%s" % (krepr.strip("'"), vrepr))
             readable = readable and kreadable and vreadable
             if krecur or vrecur:
                 recursive = True
         del context[objid]
-        return ("%s(%s)" % (typ.__name__, ", ".join(components)), readable,
-                recursive)
+        return ("%s(%s)" % (typ.__name__, ", ".join(components)), readable, recursive)

     rep = repr(object)
-    return rep, (rep and not rep.startswith('<')), False
+    return rep, (rep and not rep.startswith("<")), False
('sklearn/utils', 'random.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,11 +8,10 @@
 from . import check_random_state
 from ._random import sample_without_replacement

-__all__ = ['sample_without_replacement']
+__all__ = ["sample_without_replacement"]


-def random_choice_csc(n_samples, classes, class_probability=None,
-                      random_state=None):
+def _random_choice_csc(n_samples, classes, class_probability=None, random_state=None):
     """Generate a sparse random matrix given column class distributions

     Parameters
@@ -23,30 +22,28 @@
     classes : list of size n_outputs of arrays of size (n_classes,)
         List of classes for each column.

-    class_probability : list of size n_outputs of arrays of size (n_classes,)
-        Optional (default=None). Class distribution of each column. If None the
-        uniform distribution is assumed.
+    class_probability : list of size n_outputs of arrays of \
+        shape (n_classes,), default=None
+        Class distribution of each column. If None, uniform distribution is
+        assumed.

-    random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None, default=None
+        Controls the randomness of the sampled classes.
+        See :term:`Glossary <random_state>`.

     Returns
     -------
     random_matrix : sparse csc matrix of size (n_samples, n_outputs)

     """
-    data = array.array('i')
-    indices = array.array('i')
-    indptr = array.array('i', [0])
+    data = array.array("i")
+    indices = array.array("i")
+    indptr = array.array("i", [0])

     for j in range(len(classes)):
         classes[j] = np.asarray(classes[j])
-        if classes[j].dtype.kind != 'i':
-            raise ValueError("class dtype %s is not supported" %
-                             classes[j].dtype)
+        if classes[j].dtype.kind != "i":
+            raise ValueError("class dtype %s is not supported" % classes[j].dtype)
         classes[j] = classes[j].astype(np.int64, copy=False)

         # use uniform distribution if no class_probability is given
@@ -57,15 +54,18 @@
             class_prob_j = np.asarray(class_probability[j])

         if not np.isclose(np.sum(class_prob_j), 1.0):
-            raise ValueError("Probability array at index {0} does not sum to "
-                             "one".format(j))
+            raise ValueError(
+                "Probability array at index {0} does not sum to one".format(j)
+            )

         if class_prob_j.shape[0] != classes[j].shape[0]:
-            raise ValueError("classes[{0}] (length {1}) and "
-                             "class_probability[{0}] (length {2}) have "
-                             "different length.".format(j,
-                                                        classes[j].shape[0],
-                                                        class_prob_j.shape[0]))
+            raise ValueError(
+                "classes[{0}] (length {1}) and "
+                "class_probability[{0}] (length {2}) have "
+                "different length.".format(
+                    j, classes[j].shape[0], class_prob_j.shape[0]
+                )
+            )

         # If 0 is not present in the classes insert it with a probability 0.0
         if 0 not in classes[j]:
@@ -77,21 +77,21 @@
         if classes[j].shape[0] > 1:
             p_nonzero = 1 - class_prob_j[classes[j] == 0]
             nnz = int(n_samples * p_nonzero)
-            ind_sample = sample_without_replacement(n_population=n_samples,
-                                                    n_samples=nnz,
-                                                    random_state=random_state)
+            ind_sample = sample_without_replacement(
+                n_population=n_samples, n_samples=nnz, random_state=random_state
+            )
             indices.extend(ind_sample)

             # Normalize probabilities for the nonzero elements
             classes_j_nonzero = classes[j] != 0
             class_probability_nz = class_prob_j[classes_j_nonzero]
-            class_probability_nz_norm = (class_probability_nz /
-                                         np.sum(class_probability_nz))
-            classes_ind = np.searchsorted(class_probability_nz_norm.cumsum(),
-                                          rng.rand(nnz))
+            class_probability_nz_norm = class_probability_nz / np.sum(
+                class_probability_nz
+            )
+            classes_ind = np.searchsorted(
+                class_probability_nz_norm.cumsum(), rng.uniform(size=nnz)
+            )
             data.extend(classes[j][classes_j_nonzero][classes_ind])
         indptr.append(len(indices))

-    return sp.csc_matrix((data, indices, indptr),
-                         (n_samples, len(classes)),
-                         dtype=int)
+    return sp.csc_matrix((data, indices, indptr), (n_samples, len(classes)), dtype=int)
('sklearn/utils', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,93 +1,128 @@
 import os
 from os.path import join

+from sklearn._build_utils import gen_from_templates

-def configuration(parent_package='', top_path=None):
+
+def configuration(parent_package="", top_path=None):
     import numpy
     from numpy.distutils.misc_util import Configuration
-    from Cython import Tempita

-    config = Configuration('utils', parent_package, top_path)
+    config = Configuration("utils", parent_package, top_path)

     libraries = []
-    if os.name == 'posix':
-        libraries.append('m')
+    if os.name == "posix":
+        libraries.append("m")

-    config.add_extension('sparsefuncs_fast',
-                         sources=['sparsefuncs_fast.pyx'],
-                         libraries=libraries)
+    config.add_extension(
+        "sparsefuncs_fast", sources=["sparsefuncs_fast.pyx"], libraries=libraries
+    )

-    config.add_extension('_cython_blas',
-                         sources=['_cython_blas.pyx'],
-                         libraries=libraries)
+    config.add_extension(
+        "_cython_blas", sources=["_cython_blas.pyx"], libraries=libraries
+    )

-    config.add_extension('arrayfuncs',
-                         sources=['arrayfuncs.pyx'],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
+    config.add_extension(
+        "arrayfuncs",
+        sources=["arrayfuncs.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )

-    config.add_extension('murmurhash',
-                         sources=['murmurhash.pyx', join(
-                             'src', 'MurmurHash3.cpp')],
-                         include_dirs=['src'])
+    config.add_extension(
+        "murmurhash",
+        sources=["murmurhash.pyx", join("src", "MurmurHash3.cpp")],
+        include_dirs=["src"],
+    )

-    config.add_extension('lgamma',
-                         sources=['lgamma.pyx', join('src', 'gamma.c')],
-                         include_dirs=['src'],
-                         libraries=libraries)
+    config.add_extension(
+        "_fast_dict",
+        sources=["_fast_dict.pyx"],
+        language="c++",
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )

-    config.add_extension('graph_shortest_path',
-                         sources=['graph_shortest_path.pyx'],
-                         include_dirs=[numpy.get_include()])
-
-    config.add_extension('fast_dict',
-                         sources=['fast_dict.pyx'],
-                         language="c++",
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
+    config.add_extension(
+        "_openmp_helpers", sources=["_openmp_helpers.pyx"], libraries=libraries
+    )

     # generate files from a template
-    pyx_templates = ['sklearn/utils/seq_dataset.pyx.tp',
-                     'sklearn/utils/seq_dataset.pxd.tp']
+    templates = [
+        "sklearn/utils/_seq_dataset.pyx.tp",
+        "sklearn/utils/_seq_dataset.pxd.tp",
+        "sklearn/utils/_weight_vector.pyx.tp",
+        "sklearn/utils/_weight_vector.pxd.tp",
+    ]

-    for pyxfiles in pyx_templates:
-        outfile = pyxfiles.replace('.tp', '')
-        # if .pyx.tp is not updated, no need to output .pyx
-        if (os.path.exists(outfile) and
-                os.stat(pyxfiles).st_mtime < os.stat(outfile).st_mtime):
-            continue
+    gen_from_templates(templates)

-        with open(pyxfiles, "r") as f:
-            tmpl = f.read()
-        pyxcontent = Tempita.sub(tmpl)
+    config.add_extension(
+        "_seq_dataset", sources=["_seq_dataset.pyx"], include_dirs=[numpy.get_include()]
+    )

-        with open(outfile, "w") as f:
-            f.write(pyxcontent)
+    config.add_extension(
+        "_weight_vector",
+        sources=["_weight_vector.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )

-    config.add_extension('seq_dataset',
-                         sources=['seq_dataset.pyx'],
-                         include_dirs=[numpy.get_include()])
+    config.add_extension(
+        "_random",
+        sources=["_random.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )

-    config.add_extension('weight_vector',
-                         sources=['weight_vector.pyx'],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
+    config.add_extension(
+        "_logistic_sigmoid",
+        sources=["_logistic_sigmoid.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )

-    config.add_extension("_random",
-                         sources=["_random.pyx"],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
+    config.add_extension(
+        "_readonly_array_wrapper",
+        sources=["_readonly_array_wrapper.pyx"],
+        libraries=libraries,
+    )

-    config.add_extension("_logistic_sigmoid",
-                         sources=["_logistic_sigmoid.pyx"],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
+    config.add_extension(
+        "_typedefs",
+        sources=["_typedefs.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )

-    config.add_subpackage('tests')
+    config.add_extension(
+        "_heap",
+        sources=["_heap.pyx"],
+        libraries=libraries,
+    )
+
+    config.add_extension(
+        "_sorting",
+        sources=["_sorting.pyx"],
+        include_dirs=[numpy.get_include()],
+        language="c++",
+        libraries=libraries,
+    )
+
+    config.add_extension(
+        "_vector_sentinel",
+        sources=["_vector_sentinel.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+        language="c++",
+    )
+
+    config.add_subpackage("tests")

     return config


-if __name__ == '__main__':
+if __name__ == "__main__":
     from numpy.distutils.core import setup
-    setup(**configuration(top_path='').todict())
+
+    setup(**configuration(top_path="").todict())
('sklearn/utils', 'arrayfuncs.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -19,35 +19,26 @@


 def min_pos(np.ndarray X):
-   """
-   Find the minimum value of an array over positive values
+    """Find the minimum value of an array over positive values

-   Returns a huge value if none of the values are positive
-   """
-   if X.dtype.name == 'float32':
-      return _float_min_pos(<float *> X.data, X.size)
-   elif X.dtype.name == 'float64':
-      return _double_min_pos(<double *> X.data, X.size)
-   else:
-      raise ValueError('Unsupported dtype for array X')
+    Returns the maximum representable value of the input dtype if none of the
+    values are positive.
+    """
+    if X.dtype == np.float32:
+        return _min_pos[float](<float *> X.data, X.size)
+    elif X.dtype == np.float64:
+        return _min_pos[double](<double *> X.data, X.size)
+    else:
+        raise ValueError('Unsupported dtype for array X')


-cdef float _float_min_pos(float *X, Py_ssize_t size):
-   cdef Py_ssize_t i
-   cdef float min_val = DBL_MAX
-   for i in range(size):
-      if 0. < X[i] < min_val:
-         min_val = X[i]
-   return min_val
-
-
-cdef double _double_min_pos(double *X, Py_ssize_t size):
-   cdef Py_ssize_t i
-   cdef np.float64_t min_val = FLT_MAX
-   for i in range(size):
-      if 0. < X[i] < min_val:
-         min_val = X[i]
-   return min_val
+cdef floating _min_pos(floating* X, Py_ssize_t size):
+    cdef Py_ssize_t i
+    cdef floating min_val = FLT_MAX if floating is float else DBL_MAX
+    for i in range(size):
+        if 0. < X[i] < min_val:
+            min_val = X[i]
+    return min_val


 # General Cholesky Delete.
('sklearn/utils', 'stats.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,15 +4,66 @@


 def _weighted_percentile(array, sample_weight, percentile=50):
+    """Compute weighted percentile
+
+    Computes lower weighted percentile. If `array` is a 2D array, the
+    `percentile` is computed along the axis 0.
+
+        .. versionchanged:: 0.24
+            Accepts 2D `array`.
+
+    Parameters
+    ----------
+    array : 1D or 2D array
+        Values to take the weighted percentile of.
+
+    sample_weight: 1D or 2D array
+        Weights for each value in `array`. Must be same shape as `array` or
+        of shape `(array.shape[0],)`.
+
+    percentile: int or float, default=50
+        Percentile to compute. Must be value between 0 and 100.
+
+    Returns
+    -------
+    percentile : int if `array` 1D, ndarray if `array` 2D
+        Weighted percentile.
     """
-    Compute the weighted ``percentile`` of ``array`` with ``sample_weight``.
-    """
-    sorted_idx = np.argsort(array)
+    n_dim = array.ndim
+    if n_dim == 0:
+        return array[()]
+    if array.ndim == 1:
+        array = array.reshape((-1, 1))
+    # When sample_weight 1D, repeat for each array.shape[1]
+    if array.shape != sample_weight.shape and array.shape[0] == sample_weight.shape[0]:
+        sample_weight = np.tile(sample_weight, (array.shape[1], 1)).T
+    sorted_idx = np.argsort(array, axis=0)
+    sorted_weights = np.take_along_axis(sample_weight, sorted_idx, axis=0)

     # Find index of median prediction for each sample
-    weight_cdf = stable_cumsum(sample_weight[sorted_idx])
-    percentile_idx = np.searchsorted(
-        weight_cdf, (percentile / 100.) * weight_cdf[-1])
-    # in rare cases, percentile_idx equals to len(sorted_idx)
-    percentile_idx = np.clip(percentile_idx, 0, len(sorted_idx)-1)
-    return array[sorted_idx[percentile_idx]]
+    weight_cdf = stable_cumsum(sorted_weights, axis=0)
+    adjusted_percentile = percentile / 100 * weight_cdf[-1]
+
+    # For percentile=0, ignore leading observations with sample_weight=0. GH20528
+    mask = adjusted_percentile == 0
+    adjusted_percentile[mask] = np.nextafter(
+        adjusted_percentile[mask], adjusted_percentile[mask] + 1
+    )
+
+    percentile_idx = np.array(
+        [
+            np.searchsorted(weight_cdf[:, i], adjusted_percentile[i])
+            for i in range(weight_cdf.shape[1])
+        ]
+    )
+    percentile_idx = np.array(percentile_idx)
+    # In rare cases, percentile_idx equals to sorted_idx.shape[0]
+    max_idx = sorted_idx.shape[0] - 1
+    percentile_idx = np.apply_along_axis(
+        lambda x: np.clip(x, 0, max_idx), axis=0, arr=percentile_idx
+    )
+
+    col_index = np.arange(array.shape[1])
+    percentile_in_sorted = sorted_idx[percentile_idx, col_index]
+    percentile = array[percentile_in_sorted, col_index]
+    return percentile[0] if n_dim == 1 else percentile
('sklearn/utils', 'class_weight.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,7 +5,7 @@
 import numpy as np


-def compute_class_weight(class_weight, classes, y):
+def compute_class_weight(class_weight, *, classes, y):
     """Estimate class weights for unbalanced datasets.

     Parameters
@@ -21,13 +21,13 @@
         Array of the classes occurring in the data, as given by
         ``np.unique(y_org)`` with ``y_org`` the original class labels.

-    y : array-like, shape (n_samples,)
-        Array of original class labels per sample;
+    y : array-like of shape (n_samples,)
+        Array of original class labels per sample.

     Returns
     -------
-    class_weight_vect : ndarray, shape (n_classes,)
-        Array with class_weight_vect[i] the weight for i-th class
+    class_weight_vect : ndarray of shape (n_classes,)
+        Array with class_weight_vect[i] the weight for i-th class.

     References
     ----------
@@ -38,43 +38,48 @@
     from ..preprocessing import LabelEncoder

     if set(y) - set(classes):
-        raise ValueError("classes should include all valid labels that can "
-                         "be in y")
+        raise ValueError("classes should include all valid labels that can be in y")
     if class_weight is None or len(class_weight) == 0:
         # uniform class weights
-        weight = np.ones(classes.shape[0], dtype=np.float64, order='C')
-    elif class_weight == 'balanced':
+        weight = np.ones(classes.shape[0], dtype=np.float64, order="C")
+    elif class_weight == "balanced":
         # Find the weight of each class as present in y.
         le = LabelEncoder()
         y_ind = le.fit_transform(y)
         if not all(np.in1d(classes, le.classes_)):
             raise ValueError("classes should have valid labels that are in y")

-        recip_freq = len(y) / (len(le.classes_) *
-                               np.bincount(y_ind).astype(np.float64))
+        recip_freq = len(y) / (len(le.classes_) * np.bincount(y_ind).astype(np.float64))
         weight = recip_freq[le.transform(classes)]
     else:
         # user-defined dictionary
-        weight = np.ones(classes.shape[0], dtype=np.float64, order='C')
+        weight = np.ones(classes.shape[0], dtype=np.float64, order="C")
         if not isinstance(class_weight, dict):
-            raise ValueError("class_weight must be dict, 'balanced', or None,"
-                             " got: %r" % class_weight)
-        for c in class_weight:
-            i = np.searchsorted(classes, c)
-            if i >= len(classes) or classes[i] != c:
-                raise ValueError("Class label {} not present.".format(c))
+            raise ValueError(
+                "class_weight must be dict, 'balanced', or None, got: %r" % class_weight
+            )
+        unweighted_classes = []
+        for i, c in enumerate(classes):
+            if c in class_weight:
+                weight[i] = class_weight[c]
             else:
-                weight[i] = class_weight[c]
+                unweighted_classes.append(c)
+
+        n_weighted_classes = len(classes) - len(unweighted_classes)
+        if unweighted_classes and n_weighted_classes != len(class_weight):
+            raise ValueError(
+                f"The classes, {unweighted_classes}, are not in class_weight"
+            )

     return weight


-def compute_sample_weight(class_weight, y, indices=None):
+def compute_sample_weight(class_weight, y, *, indices=None):
     """Estimate sample weights by class for unbalanced datasets.

     Parameters
     ----------
-    class_weight : dict, list of dicts, "balanced", or None, optional
+    class_weight : dict, list of dicts, "balanced", or None
         Weights associated with classes in the form ``{class_label: weight}``.
         If not given, all classes are supposed to have weight one. For
         multi-output problems, a list of dicts can be provided in the same
@@ -92,10 +97,10 @@

         For multi-output, the weights of each column of y will be multiplied.

-    y : array-like, shape = [n_samples] or [n_samples, n_outputs]
+    y : array-like of shape (n_samples,) or (n_samples, n_outputs)
         Array of original class labels per sample.

-    indices : array-like, shape (n_subsample,), or None
+    indices : array-like of shape (n_subsample,), default=None
         Array of indices to be used in a subsample. Can be of length less than
         n_samples in the case of a subsample, or equal to n_samples in the
         case of a bootstrap subsample with repeated indices. If None, the
@@ -104,8 +109,8 @@

     Returns
     -------
-    sample_weight_vect : ndarray, shape (n_samples,)
-        Array with sample weights as applied to the original y
+    sample_weight_vect : ndarray of shape (n_samples,)
+        Array with sample weights as applied to the original y.
     """

     y = np.atleast_1d(y)
@@ -114,21 +119,27 @@
     n_outputs = y.shape[1]

     if isinstance(class_weight, str):
-        if class_weight not in ['balanced']:
-            raise ValueError('The only valid preset for class_weight is '
-                             '"balanced". Given "%s".' % class_weight)
-    elif (indices is not None and
-          not isinstance(class_weight, str)):
-        raise ValueError('The only valid class_weight for subsampling is '
-                         '"balanced". Given "%s".' % class_weight)
+        if class_weight not in ["balanced"]:
+            raise ValueError(
+                'The only valid preset for class_weight is "balanced". Given "%s".'
+                % class_weight
+            )
+    elif indices is not None and not isinstance(class_weight, str):
+        raise ValueError(
+            'The only valid class_weight for subsampling is "balanced". Given "%s".'
+            % class_weight
+        )
     elif n_outputs > 1:
-        if (not hasattr(class_weight, "__iter__") or
-                isinstance(class_weight, dict)):
-            raise ValueError("For multi-output, class_weight should be a "
-                             "list of dicts, or a valid string.")
+        if not hasattr(class_weight, "__iter__") or isinstance(class_weight, dict):
+            raise ValueError(
+                "For multi-output, class_weight should be a "
+                "list of dicts, or a valid string."
+            )
         if len(class_weight) != n_outputs:
-            raise ValueError("For multi-output, number of elements in "
-                             "class_weight should match number of outputs.")
+            raise ValueError(
+                "For multi-output, number of elements in "
+                "class_weight should match number of outputs."
+            )

     expanded_class_weight = []
     for k in range(n_outputs):
@@ -137,7 +148,7 @@
         classes_full = np.unique(y_full)
         classes_missing = None

-        if class_weight == 'balanced' or n_outputs == 1:
+        if class_weight == "balanced" or n_outputs == 1:
             class_weight_k = class_weight
         else:
             class_weight_k = class_weight[k]
@@ -149,29 +160,28 @@
             y_subsample = y[indices, k]
             classes_subsample = np.unique(y_subsample)

-            weight_k = np.take(compute_class_weight(class_weight_k,
-                                                    classes_subsample,
-                                                    y_subsample),
-                               np.searchsorted(classes_subsample,
-                                               classes_full),
-                               mode='clip')
+            weight_k = np.take(
+                compute_class_weight(
+                    class_weight_k, classes=classes_subsample, y=y_subsample
+                ),
+                np.searchsorted(classes_subsample, classes_full),
+                mode="clip",
+            )

             classes_missing = set(classes_full) - set(classes_subsample)
         else:
-            weight_k = compute_class_weight(class_weight_k,
-                                            classes_full,
-                                            y_full)
+            weight_k = compute_class_weight(
+                class_weight_k, classes=classes_full, y=y_full
+            )

         weight_k = weight_k[np.searchsorted(classes_full, y_full)]

         if classes_missing:
             # Make missing classes' weight zero
-            weight_k[np.in1d(y_full, list(classes_missing))] = 0.
+            weight_k[np.in1d(y_full, list(classes_missing))] = 0.0

         expanded_class_weight.append(weight_k)

-    expanded_class_weight = np.prod(expanded_class_weight,
-                                    axis=0,
-                                    dtype=np.float64)
+    expanded_class_weight = np.prod(expanded_class_weight, axis=0, dtype=np.float64)

     return expanded_class_weight
('sklearn/utils', '_random.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,3 @@
-# cython: boundscheck=False
-# cython: wraparound=False
-#
 # Author: Arnaud Joly
 #
 # License: BSD 3 clause
@@ -62,13 +59,13 @@

     Parameters
     ----------
-    n_population : int,
+    n_population : int
         The size of the set to sample from.

-    n_samples : int,
+    n_samples : int
         The number of integer to sample.

-    random_state : int, RandomState instance or None, optional (default=None)
+    random_state : int, RandomState instance or None, default=None
         If int, random_state is the seed used by the random number generator;
         If RandomState instance, random_state is the random number generator;
         If None, the random number generator is the RandomState instance used
@@ -76,15 +73,14 @@

     Returns
     -------
-    out : array of size (n_samples, )
+    out : ndarray of shape (n_samples,)
         The sampled subsets of integer.
     """
     _sample_without_replacement_check_input(n_population, n_samples)

     cdef np.int_t i
     cdef np.int_t j
-    cdef np.ndarray[np.int_t, ndim=1] out = np.empty((n_samples, ),
-                                                     dtype=np.int)
+    cdef np.ndarray[np.int_t, ndim=1] out = np.empty((n_samples, ), dtype=int)

     rng = check_random_state(random_state)
     rng_randint = rng.randint
@@ -118,13 +114,13 @@

     Parameters
     ----------
-    n_population : int,
+    n_population : int
         The size of the set to sample from.

-    n_samples : int,
+    n_samples : int
         The number of integer to sample.

-    random_state : int, RandomState instance or None, optional (default=None)
+    random_state : int, RandomState instance or None, default=None
         If int, random_state is the seed used by the random number generator;
         If RandomState instance, random_state is the random number generator;
         If None, the random number generator is the RandomState instance used
@@ -132,18 +128,17 @@

     Returns
     -------
-    out : array of size (n_samples, )
+    out : ndarray of shape (n_samples,)
         The sampled subsets of integer.
     """
     _sample_without_replacement_check_input(n_population, n_samples)

     cdef np.int_t i
     cdef np.int_t j
-    cdef np.ndarray[np.int_t, ndim=1] out = np.empty((n_samples, ),
-                                                     dtype=np.int)
+    cdef np.ndarray[np.int_t, ndim=1] out = np.empty((n_samples, ), dtype=int)

     cdef np.ndarray[np.int_t, ndim=1] pool = np.empty((n_population, ),
-                                                      dtype=np.int)
+                                                      dtype=int)

     rng = check_random_state(random_state)
     rng_randint = rng.randint
@@ -179,13 +174,13 @@

     Parameters
     ----------
-    n_population : int,
+    n_population : int
         The size of the set to sample from.

-    n_samples : int,
+    n_samples : int
          The number of integer to sample.

-    random_state : int, RandomState instance or None, optional (default=None)
+    random_state : int, RandomState instance or None, default=None
         If int, random_state is the seed used by the random number generator;
         If RandomState instance, random_state is the random number generator;
         If None, the random number generator is the RandomState instance used
@@ -193,7 +188,7 @@

     Returns
     -------
-    out : array of size (n_samples, )
+    out : ndarray of shape (n_samples,)
         The sampled subsets of integer. The order of the items is not
         necessarily random. Use a random permutation of the array if the order
         of the items has to be randomized.
@@ -202,8 +197,7 @@

     cdef np.int_t i
     cdef np.int_t j
-    cdef np.ndarray[np.int_t, ndim=1] out = np.empty((n_samples, ),
-                                                     dtype=np.int)
+    cdef np.ndarray[np.int_t, ndim=1] out = np.empty((n_samples, ), dtype=int)

     rng = check_random_state(random_state)
     rng_randint = rng.randint
@@ -235,19 +229,20 @@

     Parameters
     ----------
-    n_population : int,
+    n_population : int
         The size of the set to sample from.

-    n_samples : int,
+    n_samples : int
         The number of integer to sample.

-    random_state : int, RandomState instance or None, optional (default=None)
+    random_state : int, RandomState instance or None, default=None
         If int, random_state is the seed used by the random number generator;
         If RandomState instance, random_state is the random number generator;
         If None, the random number generator is the RandomState instance used
         by `np.random`.

-    method : "auto", "tracking_selection", "reservoir_sampling" or "pool"
+    method : {"auto", "tracking_selection", "reservoir_sampling", "pool"}, \
+            default='auto'
         If method == "auto", the ratio of n_samples / n_population is used
         to determine which algorithm to use:
         If ratio is between 0 and 0.01, tracking selection is used.
@@ -266,13 +261,13 @@
         desired, the selected subset should be shuffled.

         If method == "pool", a pool based algorithm is particularly fast, even
-        faster than the tracking selection method. Hovewer, a vector containing
+        faster than the tracking selection method. However, a vector containing
         the entire population has to be initialized.
         If n_samples ~ n_population, the reservoir sampling method is faster.

     Returns
     -------
-    out : array of size (n_samples, )
+    out : ndarray of shape (n_samples,)
         The sampled subsets of integer. The subset of selected integer might
         not be randomized, see the method argument.
     """
@@ -280,7 +275,7 @@

     all_methods = ("auto", "tracking_selection", "reservoir_sampling", "pool")

-    ratio = n_samples / n_population if n_population != 0.0 else 1.0
+    ratio = <double> n_samples / n_population if n_population != 0.0 else 1.0

     # Check ratio and use permutation unless ratio < 0.01 or ratio > 0.99
     if method == "auto" and ratio > 0.01 and ratio < 0.99:
('sklearn/utils', '_show_versions.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,23 +7,27 @@

 import platform
 import sys
-import importlib
+from ..utils.fixes import threadpool_info
+from .. import __version__
+
+
+from ._openmp_helpers import _openmp_parallelism_enabled


 def _get_sys_info():
     """System information

-    Return
-    ------
+    Returns
+    -------
     sys_info : dict
         system and Python version information

     """
-    python = sys.version.replace('\n', ' ')
+    python = sys.version.replace("\n", " ")

     blob = [
         ("python", python),
-        ('executable', sys.executable),
+        ("executable", sys.executable),
         ("machine", platform.platform()),
     ]

@@ -32,6 +36,9 @@

 def _get_deps_info():
     """Overview of the installed version of main dependencies
+
+    This function does not import the modules to collect the version numbers
+    but instead relies on standard Python package metadata.

     Returns
     -------
@@ -42,75 +49,81 @@
     deps = [
         "pip",
         "setuptools",
-        "sklearn",
         "numpy",
         "scipy",
         "Cython",
         "pandas",
+        "matplotlib",
+        "joblib",
+        "threadpoolctl",
     ]

-    def get_version(module):
-        return module.__version__
+    deps_info = {
+        "sklearn": __version__,
+    }

-    deps_info = {}
+    if sys.version_info < (3, 8):
+        # Backwards compatibility with Python < 3.8, primarily for PyPy.
+        # TODO: remove once PyPy 3.8 is available on conda-forge and
+        # therefore on our CI.
+        # https://github.com/conda-forge/conda-forge-pinning-feedstock/issues/2089
+        try:
+            from pkg_resources import get_distribution, DistributionNotFound

-    for modname in deps:
-        try:
-            if modname in sys.modules:
-                mod = sys.modules[modname]
-            else:
-                mod = importlib.import_module(modname)
-            ver = get_version(mod)
-            deps_info[modname] = ver
+            for modname in deps:
+                try:
+                    deps_info[modname] = get_distribution(modname).version
+                except DistributionNotFound:
+                    deps_info[modname] = None
+
         except ImportError:
-            deps_info[modname] = None
+            # Setuptools not installed
+            for modname in deps:
+                deps_info[modname] = None
+
+    else:
+        from importlib.metadata import version, PackageNotFoundError
+
+        for modname in deps:
+            try:
+                deps_info[modname] = version(modname)
+            except PackageNotFoundError:
+                deps_info[modname] = None

     return deps_info


-def _get_blas_info():
-    """Information on system BLAS
+def show_versions():
+    """Print useful debugging information"

-    Uses the `scikit-learn` builtin method
-    :func:`sklearn._build_utils.get_blas_info` which may fail from time to time
-
-    Returns
-    -------
-    blas_info: dict
-        system BLAS information
-
+    .. versionadded:: 0.20
     """
-    from .._build_utils import get_blas_info
-
-    cblas_libs, blas_dict = get_blas_info()
-
-    macros = ['{key}={val}'.format(key=a, val=b)
-              for (a, b) in blas_dict.get('define_macros', [])]
-
-    blas_blob = [
-        ('macros', ', '.join(macros)),
-        ('lib_dirs', ':'.join(blas_dict.get('library_dirs', ''))),
-        ('cblas_libs', ', '.join(cblas_libs)),
-    ]
-
-    return dict(blas_blob)
-
-
-def show_versions():
-    "Print useful debugging information"

     sys_info = _get_sys_info()
     deps_info = _get_deps_info()
-    blas_info = _get_blas_info()

-    print('\nSystem:')
+    print("\nSystem:")
     for k, stat in sys_info.items():
         print("{k:>10}: {stat}".format(k=k, stat=stat))

-    print('\nBLAS:')
-    for k, stat in blas_info.items():
-        print("{k:>10}: {stat}".format(k=k, stat=stat))
+    print("\nPython dependencies:")
+    for k, stat in deps_info.items():
+        print("{k:>13}: {stat}".format(k=k, stat=stat))

-    print('\nPython deps:')
-    for k, stat in deps_info.items():
-        print("{k:>10}: {stat}".format(k=k, stat=stat))
+    print(
+        "\n{k}: {stat}".format(
+            k="Built with OpenMP", stat=_openmp_parallelism_enabled()
+        )
+    )
+
+    # show threadpoolctl results
+    threadpool_results = threadpool_info()
+    if threadpool_results:
+        print()
+        print("threadpoolctl info:")
+
+        for i, result in enumerate(threadpool_results):
+            for key, val in result.items():
+                print(f"{key:>15}: {val}")
+            if i != len(threadpool_results) - 1:
+                print()
('sklearn/utils', 'metaestimators.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -2,21 +2,29 @@
 # Author: Joel Nothman
 #         Andreas Mueller
 # License: BSD
+from typing import List, Any
+from types import MethodType
+import warnings
+from functools import wraps

 from abc import ABCMeta, abstractmethod
 from operator import attrgetter
 from functools import update_wrapper
 import numpy as np
-
-from ..utils import safe_indexing
+from contextlib import suppress
+
+from ..utils import _safe_indexing
+from ..utils._tags import _safe_tags
 from ..base import BaseEstimator

-__all__ = ['if_delegate_has_method']
+__all__ = ["available_if", "if_delegate_has_method"]


 class _BaseComposition(BaseEstimator, metaclass=ABCMeta):
-    """Handles parameter management for classifiers composed of named estimators.
-    """
+    """Handles parameter management for classifiers composed of named estimators."""
+
+    steps: List[Any]
+
     @abstractmethod
     def __init__(self):
         pass
@@ -25,12 +33,22 @@
         out = super().get_params(deep=deep)
         if not deep:
             return out
+
         estimators = getattr(self, attr)
-        out.update(estimators)
+        try:
+            out.update(estimators)
+        except (TypeError, ValueError):
+            # Ignore TypeError for cases where estimators is not a list of
+            # (name, estimator) and ignore ValueError when the list is not
+            # formatted correctly. This is to prevent errors when calling
+            # `set_params`. `BaseEstimator.set_params` calls `get_params` which
+            # can error for invalid values for `estimators`.
+            return out
+
         for name, estimator in estimators:
-            if hasattr(estimator, 'get_params'):
+            if hasattr(estimator, "get_params"):
                 for key, value in estimator.get_params(deep=True).items():
-                    out['%s__%s' % (name, key)] = value
+                    out["%s__%s" % (name, key)] = value
         return out

     def _set_params(self, attr, **params):
@@ -38,14 +56,18 @@
         # 1. All steps
         if attr in params:
             setattr(self, attr, params.pop(attr))
-        # 2. Step replacement
+        # 2. Replace items with estimators in params
         items = getattr(self, attr)
-        names = []
-        if items:
-            names, _ = zip(*items)
-        for name in list(params.keys()):
-            if '__' not in name and name in names:
-                self._replace_estimator(attr, name, params.pop(name))
+        if isinstance(items, list) and items:
+            # Get item names used to identify valid names in params
+            # `zip` raises a TypeError when `items` does not contains
+            # elements of length 2
+            with suppress(TypeError):
+                item_names, _ = zip(*items)
+                for name in list(params.keys()):
+                    if "__" not in name and name in item_names:
+                        self._replace_estimator(attr, name, params.pop(name))
+
         # 3. Step parameters and other initialisation arguments
         super().set_params(**params)
         return self
@@ -61,19 +83,101 @@

     def _validate_names(self, names):
         if len(set(names)) != len(names):
-            raise ValueError('Names provided are not unique: '
-                             '{0!r}'.format(list(names)))
+            raise ValueError("Names provided are not unique: {0!r}".format(list(names)))
         invalid_names = set(names).intersection(self.get_params(deep=False))
         if invalid_names:
-            raise ValueError('Estimator names conflict with constructor '
-                             'arguments: {0!r}'.format(sorted(invalid_names)))
-        invalid_names = [name for name in names if '__' in name]
+            raise ValueError(
+                "Estimator names conflict with constructor arguments: {0!r}".format(
+                    sorted(invalid_names)
+                )
+            )
+        invalid_names = [name for name in names if "__" in name]
         if invalid_names:
-            raise ValueError('Estimator names must not contain __: got '
-                             '{0!r}'.format(invalid_names))
-
-
-class _IffHasAttrDescriptor:
+            raise ValueError(
+                "Estimator names must not contain __: got {0!r}".format(invalid_names)
+            )
+
+
+class _AvailableIfDescriptor:
+    """Implements a conditional property using the descriptor protocol.
+
+    Using this class to create a decorator will raise an ``AttributeError``
+    if check(self) returns a falsey value. Note that if check raises an error
+    this will also result in hasattr returning false.
+
+    See https://docs.python.org/3/howto/descriptor.html for an explanation of
+    descriptors.
+    """
+
+    def __init__(self, fn, check, attribute_name):
+        self.fn = fn
+        self.check = check
+        self.attribute_name = attribute_name
+
+        # update the docstring of the descriptor
+        update_wrapper(self, fn)
+
+    def __get__(self, obj, owner=None):
+        attr_err = AttributeError(
+            f"This {repr(owner.__name__)} has no attribute {repr(self.attribute_name)}"
+        )
+        if obj is not None:
+            # delegate only on instances, not the classes.
+            # this is to allow access to the docstrings.
+            if not self.check(obj):
+                raise attr_err
+            out = MethodType(self.fn, obj)
+
+        else:
+            # This makes it possible to use the decorated method as an unbound method,
+            # for instance when monkeypatching.
+            @wraps(self.fn)
+            def out(*args, **kwargs):
+                if not self.check(args[0]):
+                    raise attr_err
+                return self.fn(*args, **kwargs)
+
+        return out
+
+
+def available_if(check):
+    """An attribute that is available only if check returns a truthy value
+
+    Parameters
+    ----------
+    check : callable
+        When passed the object with the decorated method, this should return
+        a truthy value if the attribute is available, and either return False
+        or raise an AttributeError if not available.
+
+    Examples
+    --------
+    >>> from sklearn.utils.metaestimators import available_if
+    >>> class HelloIfEven:
+    ...    def __init__(self, x):
+    ...        self.x = x
+    ...
+    ...    def _x_is_even(self):
+    ...        return self.x % 2 == 0
+    ...
+    ...    @available_if(_x_is_even)
+    ...    def say_hello(self):
+    ...        print("Hello")
+    ...
+    >>> obj = HelloIfEven(1)
+    >>> hasattr(obj, "say_hello")
+    False
+    >>> obj.x = 2
+    >>> hasattr(obj, "say_hello")
+    True
+    >>> obj.say_hello()
+    Hello
+    """
+    return lambda fn: _AvailableIfDescriptor(fn, check, attribute_name=fn.__name__)
+
+
+# TODO(1.3) remove
+class _IffHasAttrDescriptor(_AvailableIfDescriptor):
     """Implements a conditional property using the descriptor protocol.

     Using this class to create a decorator will raise an ``AttributeError``
@@ -88,46 +192,48 @@
     See https://docs.python.org/3/howto/descriptor.html for an explanation of
     descriptors.
     """
+
     def __init__(self, fn, delegate_names, attribute_name):
-        self.fn = fn
+        super().__init__(fn, self._check, attribute_name)
         self.delegate_names = delegate_names
-        self.attribute_name = attribute_name
-
-        # update the docstring of the descriptor
-        update_wrapper(self, fn)
-
-    def __get__(self, obj, type=None):
-        # raise an AttributeError if the attribute is not present on the object
-        if obj is not None:
-            # delegate only on instances, not the classes.
-            # this is to allow access to the docstrings.
-            for delegate_name in self.delegate_names:
-                try:
-                    delegate = attrgetter(delegate_name)(obj)
-                except AttributeError:
-                    continue
-                else:
-                    getattr(delegate, self.attribute_name)
-                    break
-            else:
-                attrgetter(self.delegate_names[-1])(obj)
-
-        # lambda, but not partial, allows help() to work with update_wrapper
-        out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
-        # update the docstring of the returned function
-        update_wrapper(out, self.fn)
-        return out
-
-
+
+    def _check(self, obj):
+        warnings.warn(
+            "if_delegate_has_method was deprecated in version 1.1 and will be "
+            "removed in version 1.3. Use if_available instead.",
+            FutureWarning,
+        )
+
+        delegate = None
+        for delegate_name in self.delegate_names:
+            try:
+                delegate = attrgetter(delegate_name)(obj)
+                break
+            except AttributeError:
+                continue
+
+        if delegate is None:
+            return False
+        # raise original AttributeError
+        getattr(delegate, self.attribute_name)
+
+        return True
+
+
+# TODO(1.3) remove
 def if_delegate_has_method(delegate):
     """Create a decorator for methods that are delegated to a sub-estimator

     This enables ducktyping by hasattr returning True according to the
     sub-estimator.

+    .. deprecated:: 1.3
+        `if_delegate_has_method` is deprecated in version 1.1 and will be removed in
+        version 1.3. Use `available_if` instead.
+
     Parameters
     ----------
-    delegate : string, list of strings or tuple of strings
+    delegate : str, list of str or tuple of str
         Name of the sub-estimator that can be accessed as an attribute of the
         base object. If a list or a tuple of names are provided, the first
         sub-estimator that is an attribute of the base object will be used.
@@ -138,8 +244,7 @@
     if not isinstance(delegate, tuple):
         delegate = (delegate,)

-    return lambda fn: _IffHasAttrDescriptor(fn, delegate,
-                                            attribute_name=fn.__name__)
+    return lambda fn: _IffHasAttrDescriptor(fn, delegate, attribute_name=fn.__name__)


 def _safe_split(estimator, X, y, indices, train_indices=None):
@@ -186,10 +291,12 @@
         Indexed targets.

     """
-    if getattr(estimator, "_pairwise", False):
+    if _safe_tags(estimator, key="pairwise"):
         if not hasattr(X, "shape"):
-            raise ValueError("Precomputed kernels or affinity matrices have "
-                             "to be passed as arrays or sparse matrices.")
+            raise ValueError(
+                "Precomputed kernels or affinity matrices have "
+                "to be passed as arrays or sparse matrices."
+            )
         # X is a precomputed square kernel matrix
         if X.shape[0] != X.shape[1]:
             raise ValueError("X should be a square kernel matrix")
@@ -198,10 +305,10 @@
         else:
             X_subset = X[np.ix_(indices, train_indices)]
     else:
-        X_subset = safe_indexing(X, indices)
+        X_subset = _safe_indexing(X, indices)

     if y is not None:
-        y_subset = safe_indexing(y, indices)
+        y_subset = _safe_indexing(y, indices)
     else:
         y_subset = None

('sklearn/utils', 'extmath.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -29,7 +29,7 @@

     Parameters
     ----------
-    x : array_like
+    x : array-like

     Returns
     -------
@@ -37,11 +37,13 @@
         The Euclidean norm when x is a vector, the Frobenius norm when x
         is a matrix (2-d array).
     """
-    x = np.ravel(x, order='K')
+    x = np.ravel(x, order="K")
     if np.issubdtype(x.dtype, np.integer):
-        warnings.warn('Array type is integer, np.dot may overflow. '
-                      'Data should be float type to avoid this issue',
-                      UserWarning)
+        warnings.warn(
+            "Array type is integer, np.dot may overflow. "
+            "Data should be float type to avoid this issue",
+            UserWarning,
+        )
     return np.dot(x, x)


@@ -55,14 +57,14 @@

     Parameters
     ----------
-    X : array_like
-        The input array
-    squared : bool, optional (default = False)
+    X : array-like
+        The input array.
+    squared : bool, default=False
         If True, return squared norms.

     Returns
     -------
-    array_like
+    array-like
         The row-wise (squared) Euclidean norm of X.
     """
     if sparse.issparse(X):
@@ -70,7 +72,7 @@
             X = sparse.csr_matrix(X)
         norms = csr_row_norms(X)
     else:
-        norms = np.einsum('ij,ij->i', X, X)
+        norms = np.einsum("ij,ij->i", X, X)

     if not squared:
         np.sqrt(norms, norms)
@@ -78,15 +80,15 @@


 def fast_logdet(A):
-    """Compute log(det(A)) for A symmetric
+    """Compute log(det(A)) for A symmetric.

     Equivalent to : np.log(nl.det(A)) but more robust.
     It returns -Inf if det(A) is non positive or is not defined.

     Parameters
     ----------
-    A : array_like
-        The matrix
+    A : array-like
+        The matrix.
     """
     sign, ld = np.linalg.slogdet(A)
     if not sign > 0:
@@ -95,17 +97,17 @@


 def density(w, **kwargs):
-    """Compute density of a sparse vector
-
-    Parameters
-    ----------
-    w : array_like
-        The sparse vector
+    """Compute density of a sparse vector.
+
+    Parameters
+    ----------
+    w : array-like
+        The sparse vector.

     Returns
     -------
     float
-        The density of w, between 0 and 1
+        The density of w, between 0 and 1.
     """
     if hasattr(w, "toarray"):
         d = float(w.nnz) / (w.shape[0] * w.shape[1])
@@ -114,51 +116,68 @@
     return d


-def safe_sparse_dot(a, b, dense_output=False):
-    """Dot product that handle the sparse matrix case correctly
-
-    Uses BLAS GEMM as replacement for numpy.dot where possible
-    to avoid unnecessary copies.
-
-    Parameters
-    ----------
-    a : array or sparse matrix
-    b : array or sparse matrix
-    dense_output : boolean, default False
-        When False, either ``a`` or ``b`` being sparse will yield sparse
-        output. When True, output will always be an array.
-
-    Returns
-    -------
-    dot_product : array or sparse matrix
-        sparse if ``a`` or ``b`` is sparse and ``dense_output=False``.
-    """
-    if sparse.issparse(a) or sparse.issparse(b):
-        ret = a * b
-        if dense_output and hasattr(ret, "toarray"):
-            ret = ret.toarray()
-        return ret
+def safe_sparse_dot(a, b, *, dense_output=False):
+    """Dot product that handle the sparse matrix case correctly.
+
+    Parameters
+    ----------
+    a : {ndarray, sparse matrix}
+    b : {ndarray, sparse matrix}
+    dense_output : bool, default=False
+        When False, ``a`` and ``b`` both being sparse will yield sparse output.
+        When True, output will always be a dense array.
+
+    Returns
+    -------
+    dot_product : {ndarray, sparse matrix}
+        Sparse if ``a`` and ``b`` are sparse and ``dense_output=False``.
+    """
+    if a.ndim > 2 or b.ndim > 2:
+        if sparse.issparse(a):
+            # sparse is always 2D. Implies b is 3D+
+            # [i, j] @ [k, ..., l, m, n] -> [i, k, ..., l, n]
+            b_ = np.rollaxis(b, -2)
+            b_2d = b_.reshape((b.shape[-2], -1))
+            ret = a @ b_2d
+            ret = ret.reshape(a.shape[0], *b_.shape[1:])
+        elif sparse.issparse(b):
+            # sparse is always 2D. Implies a is 3D+
+            # [k, ..., l, m] @ [i, j] -> [k, ..., l, j]
+            a_2d = a.reshape(-1, a.shape[-1])
+            ret = a_2d @ b
+            ret = ret.reshape(*a.shape[:-1], b.shape[1])
+        else:
+            ret = np.dot(a, b)
     else:
-        return np.dot(a, b)
-
-
-def randomized_range_finder(A, size, n_iter,
-                            power_iteration_normalizer='auto',
-                            random_state=None):
-    """Computes an orthonormal matrix whose range approximates the range of A.
+        ret = a @ b
+
+    if (
+        sparse.issparse(a)
+        and sparse.issparse(b)
+        and dense_output
+        and hasattr(ret, "toarray")
+    ):
+        return ret.toarray()
+    return ret
+
+
+def randomized_range_finder(
+    A, *, size, n_iter, power_iteration_normalizer="auto", random_state=None
+):
+    """Compute an orthonormal matrix whose range approximates the range of A.

     Parameters
     ----------
     A : 2D array
-        The input data matrix
-
-    size : integer
-        Size of the return array
-
-    n_iter : integer
-        Number of power iterations used to stabilize the result
-
-    power_iteration_normalizer : 'auto' (default), 'QR', 'LU', 'none'
+        The input data matrix.
+
+    size : int
+        Size of the return array.
+
+    n_iter : int
+        Number of power iterations used to stabilize the result.
+
+    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'
         Whether the power iterations are normalized with step-by-step
         QR factorization (the slowest but most accurate), 'none'
         (the fastest but numerically unstable when `n_iter` is large, e.g.
@@ -168,16 +187,15 @@

         .. versionadded:: 0.18

-    random_state : int, RandomState instance or None, optional (default=None)
+    random_state : int, RandomState instance or None, default=None
         The seed of the pseudo random number generator to use when shuffling
-        the data.  If int, random_state is the seed used by the random number
-        generator; If RandomState instance, random_state is the random number
-        generator; If None, the random number generator is the RandomState
-        instance used by `np.random`.
-
-    Returns
-    -------
-    Q : 2D array
+        the data, i.e. getting the random vectors to initialize the algorithm.
+        Pass an int for reproducible results across multiple function calls.
+        See :term:`Glossary <random_state>`.
+
+    Returns
+    -------
+    Q : ndarray
         A (size x size) projection matrix, the range of which
         approximates well the range of the input matrix A.

@@ -185,9 +203,10 @@
     -----

     Follows Algorithm 4.3 of
-    Finding structure with randomness: Stochastic algorithms for constructing
-    approximate matrix decompositions
-    Halko, et al., 2009 (arXiv:909) https://arxiv.org/pdf/0909.4061.pdf
+    :arxiv:`"Finding structure with randomness:
+    Stochastic algorithms for constructing approximate matrix decompositions"
+    <0909.4061>`
+    Halko, et al. (2009)

     An implementation of a randomized algorithm for principal component
     analysis
@@ -197,65 +216,86 @@

     # Generating normal random vectors with shape: (A.shape[1], size)
     Q = random_state.normal(size=(A.shape[1], size))
-    if A.dtype.kind == 'f':
+    if A.dtype.kind == "f":
         # Ensure f32 is preserved as f32
         Q = Q.astype(A.dtype, copy=False)

     # Deal with "auto" mode
-    if power_iteration_normalizer == 'auto':
+    if power_iteration_normalizer == "auto":
         if n_iter <= 2:
-            power_iteration_normalizer = 'none'
+            power_iteration_normalizer = "none"
         else:
-            power_iteration_normalizer = 'LU'
+            power_iteration_normalizer = "LU"

     # Perform power iterations with Q to further 'imprint' the top
     # singular vectors of A in Q
     for i in range(n_iter):
-        if power_iteration_normalizer == 'none':
+        if power_iteration_normalizer == "none":
             Q = safe_sparse_dot(A, Q)
             Q = safe_sparse_dot(A.T, Q)
-        elif power_iteration_normalizer == 'LU':
+        elif power_iteration_normalizer == "LU":
             Q, _ = linalg.lu(safe_sparse_dot(A, Q), permute_l=True)
             Q, _ = linalg.lu(safe_sparse_dot(A.T, Q), permute_l=True)
-        elif power_iteration_normalizer == 'QR':
-            Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode='economic')
-            Q, _ = linalg.qr(safe_sparse_dot(A.T, Q), mode='economic')
+        elif power_iteration_normalizer == "QR":
+            Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode="economic")
+            Q, _ = linalg.qr(safe_sparse_dot(A.T, Q), mode="economic")

     # Sample the range of A using by linear projection of Q
     # Extract an orthonormal basis
-    Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode='economic')
+    Q, _ = linalg.qr(safe_sparse_dot(A, Q), mode="economic")
     return Q


-def randomized_svd(M, n_components, n_oversamples=10, n_iter='auto',
-                   power_iteration_normalizer='auto', transpose='auto',
-                   flip_sign=True, random_state=0):
-    """Computes a truncated randomized SVD
-
-    Parameters
-    ----------
-    M : ndarray or sparse matrix
-        Matrix to decompose
+def randomized_svd(
+    M,
+    n_components,
+    *,
+    n_oversamples=10,
+    n_iter="auto",
+    power_iteration_normalizer="auto",
+    transpose="auto",
+    flip_sign=True,
+    random_state="warn",
+):
+    """Computes a truncated randomized SVD.
+
+    This method solves the fixed-rank approximation problem described in the
+    Halko et al paper (problem (1.5), p5).
+
+    Parameters
+    ----------
+    M : {ndarray, sparse matrix}
+        Matrix to decompose.

     n_components : int
         Number of singular values and vectors to extract.

-    n_oversamples : int (default is 10)
+    n_oversamples : int, default=10
         Additional number of random vectors to sample the range of M so as
         to ensure proper conditioning. The total number of random vectors
         used to find the range of M is n_components + n_oversamples. Smaller
         number can improve speed but can negatively impact the quality of
-        approximation of singular vectors and singular values.
-
-    n_iter : int or 'auto' (default is 'auto')
+        approximation of singular vectors and singular values. Users might wish
+        to increase this parameter up to `2*k - n_components` where k is the
+        effective rank, for large matrices, noisy problems, matrices with
+        slowly decaying spectrums, or to increase precision accuracy. See Halko
+        et al (pages 5, 23 and 26).
+
+    n_iter : int or 'auto', default='auto'
         Number of power iterations. It can be used to deal with very noisy
         problems. When 'auto', it is set to 4, unless `n_components` is small
-        (< .1 * min(X.shape)) `n_iter` in which case is set to 7.
-        This improves precision with few components.
+        (< .1 * min(X.shape)) in which case `n_iter` is set to 7.
+        This improves precision with few components. Note that in general
+        users should rather increase `n_oversamples` before increasing `n_iter`
+        as the principle of the randomized method is to avoid usage of these
+        more costly power iterations steps. When `n_components` is equal
+        or greater to the effective matrix rank and the spectrum does not
+        present a slow decay, `n_iter=0` or `1` should even work fine in theory
+        (see Halko et al paper, page 9).

         .. versionchanged:: 0.18

-    power_iteration_normalizer : 'auto' (default), 'QR', 'LU', 'none'
+    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'
         Whether the power iterations are normalized with step-by-step
         QR factorization (the slowest but most accurate), 'none'
         (the fastest but numerically unstable when `n_iter` is large, e.g.
@@ -265,7 +305,7 @@

         .. versionadded:: 0.18

-    transpose : True, False or 'auto' (default)
+    transpose : bool or 'auto', default='auto'
         Whether the algorithm should be applied to M.T instead of M. The
         result should approximately be the same. The 'auto' mode will
         trigger the transposition if M.shape[1] > M.shape[0] since this
@@ -274,18 +314,23 @@

         .. versionchanged:: 0.18

-    flip_sign : boolean, (True by default)
+    flip_sign : bool, default=True
         The output of a singular value decomposition is only unique up to a
         permutation of the signs of the singular vectors. If `flip_sign` is
         set to `True`, the sign ambiguity is resolved by making the largest
         loadings for each component in the left singular vectors positive.

-    random_state : int, RandomState instance or None, optional (default=None)
-        The seed of the pseudo random number generator to use when shuffling
-        the data.  If int, random_state is the seed used by the random number
-        generator; If RandomState instance, random_state is the random number
-        generator; If None, the random number generator is the RandomState
-        instance used by `np.random`.
+    random_state : int, RandomState instance or None, default='warn'
+        The seed of the pseudo random number generator to use when
+        shuffling the data, i.e. getting the random vectors to initialize
+        the algorithm. Pass an int for reproducible results across multiple
+        function calls. See :term:`Glossary <random_state>`.
+
+        .. versionchanged:: 1.2
+            The previous behavior (`random_state=0`) is deprecated, and
+            from v1.2 the default value will be `random_state=None`. Set
+            the value of `random_state` explicitly to suppress the deprecation
+            warning.

     Notes
     -----
@@ -294,13 +339,17 @@
     computations. It is particularly fast on large matrices on which
     you wish to extract only a small number of components. In order to
     obtain further speed up, `n_iter` can be set <=2 (at the cost of
-    loss of precision).
+    loss of precision). To increase the precision it is recommended to
+    increase `n_oversamples`, up to `2*k-n_components` where k is the
+    effective rank. Usually, `n_components` is chosen to be greater than k
+    so increasing `n_oversamples` up to `n_components` should be enough.

     References
     ----------
-    * Finding structure with randomness: Stochastic algorithms for constructing
-      approximate matrix decompositions
-      Halko, et al., 2009 https://arxiv.org/abs/0909.4061
+    * :arxiv:`"Finding structure with randomness:
+      Stochastic algorithms for constructing approximate matrix decompositions"
+      <0909.4061>`
+      Halko, et al. (2009)

     * A randomized algorithm for the decomposition of matrices
       Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert
@@ -310,55 +359,229 @@
       A. Szlam et al. 2014
     """
     if isinstance(M, (sparse.lil_matrix, sparse.dok_matrix)):
-        warnings.warn("Calculating SVD of a {} is expensive. "
-                      "csr_matrix is more efficient.".format(
-                          type(M).__name__),
-                      sparse.SparseEfficiencyWarning)
+        warnings.warn(
+            "Calculating SVD of a {} is expensive. "
+            "csr_matrix is more efficient.".format(type(M).__name__),
+            sparse.SparseEfficiencyWarning,
+        )
+
+    if random_state == "warn":
+        warnings.warn(
+            "If 'random_state' is not supplied, the current default "
+            "is to use 0 as a fixed seed. This will change to  "
+            "None in version 1.2 leading to non-deterministic results "
+            "that better reflect nature of the randomized_svd solver. "
+            "If you want to silence this warning, set 'random_state' "
+            "to an integer seed or to None explicitly depending "
+            "if you want your code to be deterministic or not.",
+            FutureWarning,
+        )
+        random_state = 0

     random_state = check_random_state(random_state)
     n_random = n_components + n_oversamples
     n_samples, n_features = M.shape

-    if n_iter == 'auto':
+    if n_iter == "auto":
         # Checks if the number of iterations is explicitly specified
         # Adjust n_iter. 7 was found a good compromise for PCA. See #5299
-        n_iter = 7 if n_components < .1 * min(M.shape) else 4
-
-    if transpose == 'auto':
+        n_iter = 7 if n_components < 0.1 * min(M.shape) else 4
+
+    if transpose == "auto":
         transpose = n_samples < n_features
     if transpose:
         # this implementation is a bit faster with smaller shape[1]
         M = M.T

-    Q = randomized_range_finder(M, n_random, n_iter,
-                                power_iteration_normalizer, random_state)
+    Q = randomized_range_finder(
+        M,
+        size=n_random,
+        n_iter=n_iter,
+        power_iteration_normalizer=power_iteration_normalizer,
+        random_state=random_state,
+    )

     # project M to the (k + p) dimensional space using the basis vectors
     B = safe_sparse_dot(Q.T, M)

     # compute the SVD on the thin matrix: (k + p) wide
-    Uhat, s, V = linalg.svd(B, full_matrices=False)
+    Uhat, s, Vt = linalg.svd(B, full_matrices=False)

     del B
     U = np.dot(Q, Uhat)

     if flip_sign:
         if not transpose:
-            U, V = svd_flip(U, V)
+            U, Vt = svd_flip(U, Vt)
         else:
             # In case of transpose u_based_decision=false
             # to actually flip based on u and not v.
-            U, V = svd_flip(U, V, u_based_decision=False)
+            U, Vt = svd_flip(U, Vt, u_based_decision=False)

     if transpose:
         # transpose back the results according to the input convention
-        return V[:n_components, :].T, s[:n_components], U[:, :n_components].T
+        return Vt[:n_components, :].T, s[:n_components], U[:, :n_components].T
     else:
-        return U[:, :n_components], s[:n_components], V[:n_components, :]
-
-
-def weighted_mode(a, w, axis=0):
-    """Returns an array of the weighted modal (most common) value in a
+        return U[:, :n_components], s[:n_components], Vt[:n_components, :]
+
+
+def _randomized_eigsh(
+    M,
+    n_components,
+    *,
+    n_oversamples=10,
+    n_iter="auto",
+    power_iteration_normalizer="auto",
+    selection="module",
+    random_state=None,
+):
+    """Computes a truncated eigendecomposition using randomized methods
+
+    This method solves the fixed-rank approximation problem described in the
+    Halko et al paper.
+
+    The choice of which components to select can be tuned with the `selection`
+    parameter.
+
+    .. versionadded:: 0.24
+
+    Parameters
+    ----------
+    M : ndarray or sparse matrix
+        Matrix to decompose, it should be real symmetric square or complex
+        hermitian
+
+    n_components : int
+        Number of eigenvalues and vectors to extract.
+
+    n_oversamples : int, default=10
+        Additional number of random vectors to sample the range of M so as
+        to ensure proper conditioning. The total number of random vectors
+        used to find the range of M is n_components + n_oversamples. Smaller
+        number can improve speed but can negatively impact the quality of
+        approximation of eigenvectors and eigenvalues. Users might wish
+        to increase this parameter up to `2*k - n_components` where k is the
+        effective rank, for large matrices, noisy problems, matrices with
+        slowly decaying spectrums, or to increase precision accuracy. See Halko
+        et al (pages 5, 23 and 26).
+
+    n_iter : int or 'auto', default='auto'
+        Number of power iterations. It can be used to deal with very noisy
+        problems. When 'auto', it is set to 4, unless `n_components` is small
+        (< .1 * min(X.shape)) in which case `n_iter` is set to 7.
+        This improves precision with few components. Note that in general
+        users should rather increase `n_oversamples` before increasing `n_iter`
+        as the principle of the randomized method is to avoid usage of these
+        more costly power iterations steps. When `n_components` is equal
+        or greater to the effective matrix rank and the spectrum does not
+        present a slow decay, `n_iter=0` or `1` should even work fine in theory
+        (see Halko et al paper, page 9).
+
+    power_iteration_normalizer : {'auto', 'QR', 'LU', 'none'}, default='auto'
+        Whether the power iterations are normalized with step-by-step
+        QR factorization (the slowest but most accurate), 'none'
+        (the fastest but numerically unstable when `n_iter` is large, e.g.
+        typically 5 or larger), or 'LU' factorization (numerically stable
+        but can lose slightly in accuracy). The 'auto' mode applies no
+        normalization if `n_iter` <= 2 and switches to LU otherwise.
+
+    selection : {'value', 'module'}, default='module'
+        Strategy used to select the n components. When `selection` is `'value'`
+        (not yet implemented, will become the default when implemented), the
+        components corresponding to the n largest eigenvalues are returned.
+        When `selection` is `'module'`, the components corresponding to the n
+        eigenvalues with largest modules are returned.
+
+    random_state : int, RandomState instance, default=None
+        The seed of the pseudo random number generator to use when shuffling
+        the data, i.e. getting the random vectors to initialize the algorithm.
+        Pass an int for reproducible results across multiple function calls.
+        See :term:`Glossary <random_state>`.
+
+    Notes
+    -----
+    This algorithm finds a (usually very good) approximate truncated
+    eigendecomposition using randomized methods to speed up the computations.
+
+    This method is particularly fast on large matrices on which
+    you wish to extract only a small number of components. In order to
+    obtain further speed up, `n_iter` can be set <=2 (at the cost of
+    loss of precision). To increase the precision it is recommended to
+    increase `n_oversamples`, up to `2*k-n_components` where k is the
+    effective rank. Usually, `n_components` is chosen to be greater than k
+    so increasing `n_oversamples` up to `n_components` should be enough.
+
+    Strategy 'value': not implemented yet.
+    Algorithms 5.3, 5.4 and 5.5 in the Halko et al paper should provide good
+    condidates for a future implementation.
+
+    Strategy 'module':
+    The principle is that for diagonalizable matrices, the singular values and
+    eigenvalues are related: if t is an eigenvalue of A, then :math:`|t|` is a
+    singular value of A. This method relies on a randomized SVD to find the n
+    singular components corresponding to the n singular values with largest
+    modules, and then uses the signs of the singular vectors to find the true
+    sign of t: if the sign of left and right singular vectors are different
+    then the corresponding eigenvalue is negative.
+
+    Returns
+    -------
+    eigvals : 1D array of shape (n_components,) containing the `n_components`
+        eigenvalues selected (see ``selection`` parameter).
+    eigvecs : 2D array of shape (M.shape[0], n_components) containing the
+        `n_components` eigenvectors corresponding to the `eigvals`, in the
+        corresponding order. Note that this follows the `scipy.linalg.eigh`
+        convention.
+
+    See Also
+    --------
+    :func:`randomized_svd`
+
+    References
+    ----------
+    * :arxiv:`"Finding structure with randomness:
+      Stochastic algorithms for constructing approximate matrix decompositions"
+      (Algorithm 4.3 for strategy 'module') <0909.4061>`
+      Halko, et al. (2009)
+    """
+    if selection == "value":  # pragma: no cover
+        # to do : an algorithm can be found in the Halko et al reference
+        raise NotImplementedError()
+
+    elif selection == "module":
+        # Note: no need for deterministic U and Vt (flip_sign=True),
+        # as we only use the dot product UVt afterwards
+        U, S, Vt = randomized_svd(
+            M,
+            n_components=n_components,
+            n_oversamples=n_oversamples,
+            n_iter=n_iter,
+            power_iteration_normalizer=power_iteration_normalizer,
+            flip_sign=False,
+            random_state=random_state,
+        )
+
+        eigvecs = U[:, :n_components]
+        eigvals = S[:n_components]
+
+        # Conversion of Singular values into Eigenvalues:
+        # For any eigenvalue t, the corresponding singular value is |t|.
+        # So if there is a negative eigenvalue t, the corresponding singular
+        # value will be -t, and the left (U) and right (V) singular vectors
+        # will have opposite signs.
+        # Fastest way: see <https://stackoverflow.com/a/61974002/7262247>
+        diag_VtU = np.einsum("ji,ij->j", Vt[:n_components, :], U[:, :n_components])
+        signs = np.sign(diag_VtU)
+        eigvals = eigvals * signs
+
+    else:  # pragma: no cover
+        raise ValueError("Invalid `selection`: %r" % selection)
+
+    return eigvals, eigvecs
+
+
+def weighted_mode(a, w, *, axis=0):
+    """Returns an array of the weighted modal (most common) value in a.

     If there is more than one such value, only the first is returned.
     The bin-count for the modal bins is also returned.
@@ -367,11 +590,11 @@

     Parameters
     ----------
-    a : array_like
+    a : array-like
         n-dimensional array of which to find mode(s).
-    w : array_like
-        n-dimensional array of weights for each value
-    axis : int, optional
+    w : array-like
+        n-dimensional array of weights for each value.
+    axis : int, default=0
         Axis along which to operate. Default is 0, i.e. the first axis.

     Returns
@@ -414,14 +637,14 @@
     if a.shape != w.shape:
         w = np.full(a.shape, w, dtype=w.dtype)

-    scores = np.unique(np.ravel(a))       # get ALL unique values
+    scores = np.unique(np.ravel(a))  # get ALL unique values
     testshape = list(a.shape)
     testshape[axis] = 1
     oldmostfreq = np.zeros(testshape)
     oldcounts = np.zeros(testshape)
     for score in scores:
         template = np.zeros(a.shape)
-        ind = (a == score)
+        ind = a == score
         template[ind] = w[ind]
         counts = np.expand_dims(np.sum(template, axis), axis)
         mostfrequent = np.where(counts > oldcounts, score, oldmostfreq)
@@ -437,17 +660,22 @@
     ----------
     arrays : list of array-like
         1-D arrays to form the cartesian product of.
-    out : ndarray
+    out : ndarray of shape (M, len(arrays)), default=None
         Array to place the cartesian product in.

     Returns
     -------
-    out : ndarray
-        2-D array of shape (M, len(arrays)) containing cartesian products
-        formed of input arrays.
+    out : ndarray of shape (M, len(arrays))
+        Array containing the cartesian products formed of input arrays.
+
+    Notes
+    -----
+    This function may not be used on more than 32 arrays
+    because the underlying numpy functions do not support it.

     Examples
     --------
+    >>> from sklearn.utils.extmath import cartesian
     >>> cartesian(([1, 2, 3], [4, 5], [6, 7]))
     array([[1, 4, 6],
            [1, 4, 7],
@@ -461,7 +689,6 @@
            [3, 4, 7],
            [3, 5, 6],
            [3, 5, 7]])
-
     """
     arrays = [np.asarray(x) for x in arrays]
     shape = (len(x) for x in arrays)
@@ -489,15 +716,17 @@
     ----------
     u : ndarray
         u and v are the output of `linalg.svd` or
-        `sklearn.utils.extmath.randomized_svd`, with matching inner dimensions
-        so one can compute `np.dot(u * s, v)`.
+        :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner
+        dimensions so one can compute `np.dot(u * s, v)`.

     v : ndarray
         u and v are the output of `linalg.svd` or
-        `sklearn.utils.extmath.randomized_svd`, with matching inner dimensions
-        so one can compute `np.dot(u * s, v)`.
-
-    u_based_decision : boolean, (default=True)
+        :func:`~sklearn.utils.extmath.randomized_svd`, with matching inner
+        dimensions so one can compute `np.dot(u * s, v)`.
+        The input v should really be called vt to be consistent with scipy's
+        output.
+
+    u_based_decision : bool, default=True
         If True, use the columns of u as the basis for sign flipping.
         Otherwise, use the rows of v. The choice of which variable to base the
         decision on is generally algorithm dependent.
@@ -536,16 +765,16 @@

     Parameters
     ----------
-    X : array-like, shape (M, N) or (M, )
-        Argument to the logistic function
-
-    out : array-like, shape: (M, N) or (M, ), optional:
+    X : array-like of shape (M, N) or (M,)
+        Argument to the logistic function.
+
+    out : array-like of shape (M, N) or (M,), default=None
         Preallocated output array.

     Returns
     -------
-    out : array, shape (M, N) or (M, )
-        Log of the logistic function evaluated at every point in x
+    out : ndarray of shape (M, N) or (M,)
+        Log of the logistic function evaluated at every point in x.

     Notes
     -----
@@ -581,16 +810,16 @@

     Parameters
     ----------
-    X : array-like of floats, shape (M, N)
-        Argument to the logistic function
-
-    copy : bool, optional
+    X : array-like of float of shape (M, N)
+        Argument to the logistic function.
+
+    copy : bool, default=True
         Copy X or not.

     Returns
     -------
-    out : array, shape (M, N)
-        Softmax function evaluated at every point in x
+    out : ndarray of shape (M, N)
+        Softmax function evaluated at every point in x.
     """
     if copy:
         X = np.copy(X)
@@ -602,57 +831,35 @@
     return X


-def safe_min(X):
-    """Returns the minimum value of a dense or a CSR/CSC matrix.
-
-    Adapated from https://stackoverflow.com/q/13426580
-
-    Parameters
-    ----------
-    X : array_like
-        The input array or sparse matrix
-
-    Returns
-    -------
-    Float
-        The min value of X
-    """
-    if sparse.issparse(X):
-        if len(X.data) == 0:
-            return 0
-        m = X.data.min()
-        return m if X.getnnz() == X.size else min(m, 0)
-    else:
-        return X.min()
-
-
 def make_nonnegative(X, min_value=0):
     """Ensure `X.min()` >= `min_value`.

     Parameters
     ----------
-    X : array_like
-        The matrix to make non-negative
-    min_value : float
-        The threshold value
-
-    Returns
-    -------
-    array_like
-        The thresholded array
+    X : array-like
+        The matrix to make non-negative.
+    min_value : float, default=0
+        The threshold value.
+
+    Returns
+    -------
+    array-like
+        The thresholded array.

     Raises
     ------
     ValueError
-        When X is sparse
-    """
-    min_ = safe_min(X)
+        When X is sparse.
+    """
+    min_ = X.min()
     if min_ < min_value:
         if sparse.issparse(X):
-            raise ValueError("Cannot make the data matrix"
-                             " nonnegative because it is sparse."
-                             " Adding a value to every entry would"
-                             " make it no longer sparse.")
+            raise ValueError(
+                "Cannot make the data matrix"
+                " nonnegative because it is sparse."
+                " Adding a value to every entry would"
+                " make it no longer sparse."
+            )
         X = X + (min_value - min_)
     return X

@@ -669,18 +876,19 @@
     Parameters
     ----------
     op : function
-        A numpy accumulator function such as np.mean or np.sum
-    x : numpy array
-        A numpy array to apply the accumulator function
+        A numpy accumulator function such as np.mean or np.sum.
+    x : ndarray
+        A numpy array to apply the accumulator function.
     *args : positional arguments
         Positional arguments passed to the accumulator function after the
-        input x
+        input x.
     **kwargs : keyword arguments
-        Keyword arguments passed to the accumulator function
-
-    Returns
-    -------
-    result : The output of the accumulator function passed to this function
+        Keyword arguments passed to the accumulator function.
+
+    Returns
+    -------
+    result
+        The output of the accumulator function passed to this function.
     """
     if np.issubdtype(x.dtype, np.floating) and x.dtype.itemsize < 8:
         result = op(x, *args, **kwargs, dtype=np.float64)
@@ -689,37 +897,46 @@
     return result


-def _incremental_mean_and_var(X, last_mean, last_variance, last_sample_count):
+def _incremental_mean_and_var(
+    X, last_mean, last_variance, last_sample_count, sample_weight=None
+):
     """Calculate mean update and a Youngs and Cramer variance update.

-    last_mean and last_variance are statistics computed at the last step by the
-    function. Both must be initialized to 0.0. In case no scaling is required
-    last_variance can be None. The mean is always required and returned because
-    necessary for the calculation of the variance. last_n_samples_seen is the
-    number of samples encountered until now.
+    If sample_weight is given, the weighted mean and variance is computed.
+
+    Update a given mean and (possibly) variance according to new data given
+    in X. last_mean is always required to compute the new mean.
+    If last_variance is None, no variance is computed and None return for
+    updated_variance.

     From the paper "Algorithms for computing the sample variance: analysis and
     recommendations", by Chan, Golub, and LeVeque.

     Parameters
     ----------
-    X : array-like, shape (n_samples, n_features)
-        Data to use for variance update
-
-    last_mean : array-like, shape: (n_features,)
-
-    last_variance : array-like, shape: (n_features,)
-
-    last_sample_count : array-like, shape (n_features,)
-
-    Returns
-    -------
-    updated_mean : array, shape (n_features,)
-
-    updated_variance : array, shape (n_features,)
-        If None, only mean is computed
-
-    updated_sample_count : array, shape (n_features,)
+    X : array-like of shape (n_samples, n_features)
+        Data to use for variance update.
+
+    last_mean : array-like of shape (n_features,)
+
+    last_variance : array-like of shape (n_features,)
+
+    last_sample_count : array-like of shape (n_features,)
+        The number of samples encountered until now if sample_weight is None.
+        If sample_weight is not None, this is the sum of sample_weight
+        encountered.
+
+    sample_weight : array-like of shape (n_samples,) or None
+        Sample weights. If None, compute the unweighted mean/variance.
+
+    Returns
+    -------
+    updated_mean : ndarray of shape (n_features,)
+
+    updated_variance : ndarray of shape (n_features,)
+        None if last_variance was None.
+
+    updated_sample_count : ndarray of shape (n_features,)

     Notes
     -----
@@ -739,9 +956,25 @@
     # new = the current increment
     # updated = the aggregated stats
     last_sum = last_mean * last_sample_count
-    new_sum = _safe_accumulator_op(np.nansum, X, axis=0)
-
-    new_sample_count = np.sum(~np.isnan(X), axis=0)
+    X_nan_mask = np.isnan(X)
+    if np.any(X_nan_mask):
+        sum_op = np.nansum
+    else:
+        sum_op = np.sum
+    if sample_weight is not None:
+        # equivalent to np.nansum(X * sample_weight, axis=0)
+        # safer because np.float64(X*W) != np.float64(X)*np.float64(W)
+        new_sum = _safe_accumulator_op(
+            np.matmul, sample_weight, np.where(X_nan_mask, 0, X)
+        )
+        new_sample_count = _safe_accumulator_op(
+            np.sum, sample_weight[:, None] * (~X_nan_mask), axis=0
+        )
+    else:
+        new_sum = _safe_accumulator_op(sum_op, X, axis=0)
+        n_samples = X.shape[0]
+        new_sample_count = n_samples - np.sum(X_nan_mask, axis=0)
+
     updated_sample_count = last_sample_count + new_sample_count

     updated_mean = (last_sum + new_sum) / updated_sample_count
@@ -749,16 +982,39 @@
     if last_variance is None:
         updated_variance = None
     else:
-        new_unnormalized_variance = (
-            _safe_accumulator_op(np.nanvar, X, axis=0) * new_sample_count)
+        T = new_sum / new_sample_count
+        temp = X - T
+        if sample_weight is not None:
+            # equivalent to np.nansum((X-T)**2 * sample_weight, axis=0)
+            # safer because np.float64(X*W) != np.float64(X)*np.float64(W)
+            correction = _safe_accumulator_op(
+                np.matmul, sample_weight, np.where(X_nan_mask, 0, temp)
+            )
+            temp **= 2
+            new_unnormalized_variance = _safe_accumulator_op(
+                np.matmul, sample_weight, np.where(X_nan_mask, 0, temp)
+            )
+        else:
+            correction = _safe_accumulator_op(sum_op, temp, axis=0)
+            temp **= 2
+            new_unnormalized_variance = _safe_accumulator_op(sum_op, temp, axis=0)
+
+        # correction term of the corrected 2 pass algorithm.
+        # See "Algorithms for computing the sample variance: analysis
+        # and recommendations", by Chan, Golub, and LeVeque.
+        new_unnormalized_variance -= correction**2 / new_sample_count
+
         last_unnormalized_variance = last_variance * last_sample_count

-        with np.errstate(divide='ignore', invalid='ignore'):
+        with np.errstate(divide="ignore", invalid="ignore"):
             last_over_new_count = last_sample_count / new_sample_count
             updated_unnormalized_variance = (
-                last_unnormalized_variance + new_unnormalized_variance +
-                last_over_new_count / updated_sample_count *
-                (last_sum / last_over_new_count - new_sum) ** 2)
+                last_unnormalized_variance
+                + new_unnormalized_variance
+                + last_over_new_count
+                / updated_sample_count
+                * (last_sum / last_over_new_count - new_sum) ** 2
+            )

         zeros = last_sample_count == 0
         updated_unnormalized_variance[zeros] = new_unnormalized_variance[zeros]
@@ -768,7 +1024,7 @@


 def _deterministic_vector_sign_flip(u):
-    """Modify the sign of vectors for reproducibility
+    """Modify the sign of vectors for reproducibility.

     Flips the sign of elements of all the vectors (rows of u) such that
     the absolute maximum element of each vector is positive.
@@ -790,25 +1046,30 @@


 def stable_cumsum(arr, axis=None, rtol=1e-05, atol=1e-08):
-    """Use high precision for cumsum and check that final value matches sum
+    """Use high precision for cumsum and check that final value matches sum.

     Parameters
     ----------
     arr : array-like
-        To be cumulatively summed as flat
-    axis : int, optional
+        To be cumulatively summed as flat.
+    axis : int, default=None
         Axis along which the cumulative sum is computed.
         The default (None) is to compute the cumsum over the flattened array.
-    rtol : float
-        Relative tolerance, see ``np.allclose``
-    atol : float
-        Absolute tolerance, see ``np.allclose``
+    rtol : float, default=1e-05
+        Relative tolerance, see ``np.allclose``.
+    atol : float, default=1e-08
+        Absolute tolerance, see ``np.allclose``.
     """
     out = np.cumsum(arr, axis=axis, dtype=np.float64)
     expected = np.sum(arr, axis=axis, dtype=np.float64)
-    if not np.all(np.isclose(out.take(-1, axis=axis), expected, rtol=rtol,
-                             atol=atol, equal_nan=True)):
-        warnings.warn('cumsum was found to be unstable: '
-                      'its last element does not correspond to sum',
-                      RuntimeWarning)
+    if not np.all(
+        np.isclose(
+            out.take(-1, axis=axis), expected, rtol=rtol, atol=atol, equal_nan=True
+        )
+    ):
+        warnings.warn(
+            "cumsum was found to be unstable: "
+            "its last element does not correspond to sum",
+            RuntimeWarning,
+        )
     return out
('sklearn/utils', 'sparsefuncs.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -9,7 +9,9 @@
 from .sparsefuncs_fast import (
     csr_mean_variance_axis0 as _csr_mean_var_axis0,
     csc_mean_variance_axis0 as _csc_mean_var_axis0,
-    incr_mean_variance_axis0 as _incr_mean_var_axis0)
+    incr_mean_variance_axis0 as _incr_mean_var_axis0,
+)
+from ..utils.validation import _check_sample_weight


 def _raise_typeerror(X):
@@ -22,7 +24,8 @@
 def _raise_error_wrong_axis(axis):
     if axis not in (0, 1):
         raise ValueError(
-            "Unknown axis value: %d. Use 0 for rows, or 1 for columns" % axis)
+            "Unknown axis value: %d. Use 0 for rows, or 1 for columns" % axis
+        )


 def inplace_csr_column_scale(X, scale):
@@ -33,73 +36,97 @@

     Parameters
     ----------
-    X : CSR matrix with shape (n_samples, n_features)
+    X : sparse matrix of shape (n_samples, n_features)
         Matrix to normalize using the variance of the features.
-
-    scale : float array with shape (n_features,)
+        It should be of CSR format.
+
+    scale : ndarray of shape (n_features,), dtype={np.float32, np.float64}
         Array of precomputed feature-wise values to use for scaling.
     """
     assert scale.shape[0] == X.shape[1]
-    X.data *= scale.take(X.indices, mode='clip')
+    X.data *= scale.take(X.indices, mode="clip")


 def inplace_csr_row_scale(X, scale):
-    """ Inplace row scaling of a CSR matrix.
+    """Inplace row scaling of a CSR matrix.

     Scale each sample of the data matrix by multiplying with specific scale
     provided by the caller assuming a (n_samples, n_features) shape.

     Parameters
     ----------
-    X : CSR sparse matrix, shape (n_samples, n_features)
-        Matrix to be scaled.
-
-    scale : float array with shape (n_samples,)
+    X : sparse matrix of shape (n_samples, n_features)
+        Matrix to be scaled. It should be of CSR format.
+
+    scale : ndarray of float of shape (n_samples,)
         Array of precomputed sample-wise values to use for scaling.
     """
     assert scale.shape[0] == X.shape[0]
     X.data *= np.repeat(scale, np.diff(X.indptr))


-def mean_variance_axis(X, axis):
-    """Compute mean and variance along an axix on a CSR or CSC matrix
-
-    Parameters
-    ----------
-    X : CSR or CSC sparse matrix, shape (n_samples, n_features)
-        Input data.
-
-    axis : int (either 0 or 1)
+def mean_variance_axis(X, axis, weights=None, return_sum_weights=False):
+    """Compute mean and variance along an axis on a CSR or CSC matrix.
+
+    Parameters
+    ----------
+    X : sparse matrix of shape (n_samples, n_features)
+        Input data. It can be of CSR or CSC format.
+
+    axis : {0, 1}
         Axis along which the axis should be computed.
+
+    weights : ndarray of shape (n_samples,) or (n_features,), default=None
+        if axis is set to 0 shape is (n_samples,) or
+        if axis is set to 1 shape is (n_features,).
+        If it is set to None, then samples are equally weighted.
+
+        .. versionadded:: 0.24
+
+    return_sum_weights : bool, default=False
+        If True, returns the sum of weights seen for each feature
+        if `axis=0` or each sample if `axis=1`.
+
+        .. versionadded:: 0.24

     Returns
     -------

-    means : float array with shape (n_features,)
-        Feature-wise means
-
-    variances : float array with shape (n_features,)
-        Feature-wise variances
-
+    means : ndarray of shape (n_features,), dtype=floating
+        Feature-wise means.
+
+    variances : ndarray of shape (n_features,), dtype=floating
+        Feature-wise variances.
+
+    sum_weights : ndarray of shape (n_features,), dtype=floating
+        Returned if `return_sum_weights` is `True`.
     """
     _raise_error_wrong_axis(axis)

     if isinstance(X, sp.csr_matrix):
         if axis == 0:
-            return _csr_mean_var_axis0(X)
+            return _csr_mean_var_axis0(
+                X, weights=weights, return_sum_weights=return_sum_weights
+            )
         else:
-            return _csc_mean_var_axis0(X.T)
+            return _csc_mean_var_axis0(
+                X.T, weights=weights, return_sum_weights=return_sum_weights
+            )
     elif isinstance(X, sp.csc_matrix):
         if axis == 0:
-            return _csc_mean_var_axis0(X)
+            return _csc_mean_var_axis0(
+                X, weights=weights, return_sum_weights=return_sum_weights
+            )
         else:
-            return _csr_mean_var_axis0(X.T)
+            return _csr_mean_var_axis0(
+                X.T, weights=weights, return_sum_weights=return_sum_weights
+            )
     else:
         _raise_typeerror(X)


-def incr_mean_variance_axis(X, axis, last_mean, last_var, last_n):
-    """Compute incremental mean and variance along an axix on a CSR or
+def incr_mean_variance_axis(X, *, axis, last_mean, last_var, last_n, weights=None):
+    """Compute incremental mean and variance along an axis on a CSR or
     CSC matrix.

     last_mean, last_var are the statistics computed at the last step by this
@@ -109,56 +136,88 @@

     Parameters
     ----------
-    X : CSR or CSC sparse matrix, shape (n_samples, n_features)
+    X : CSR or CSC sparse matrix of shape (n_samples, n_features)
         Input data.

-    axis : int (either 0 or 1)
+    axis : {0, 1}
         Axis along which the axis should be computed.

-    last_mean : float array with shape (n_features,)
-        Array of feature-wise means to update with the new data X.
-
-    last_var : float array with shape (n_features,)
-        Array of feature-wise var to update with the new data X.
-
-    last_n : int with shape (n_features,)
-        Number of samples seen so far, excluded X.
+    last_mean : ndarray of shape (n_features,) or (n_samples,), dtype=floating
+        Array of means to update with the new data X.
+        Should be of shape (n_features,) if axis=0 or (n_samples,) if axis=1.
+
+    last_var : ndarray of shape (n_features,) or (n_samples,), dtype=floating
+        Array of variances to update with the new data X.
+        Should be of shape (n_features,) if axis=0 or (n_samples,) if axis=1.
+
+    last_n : float or ndarray of shape (n_features,) or (n_samples,), \
+            dtype=floating
+        Sum of the weights seen so far, excluding the current weights
+        If not float, it should be of shape (n_samples,) if
+        axis=0 or (n_features,) if axis=1. If float it corresponds to
+        having same weights for all samples (or features).
+
+    weights : ndarray of shape (n_samples,) or (n_features,), default=None
+        If axis is set to 0 shape is (n_samples,) or
+        if axis is set to 1 shape is (n_features,).
+        If it is set to None, then samples are equally weighted.
+
+        .. versionadded:: 0.24

     Returns
     -------
-
-    means : float array with shape (n_features,)
-        Updated feature-wise means.
-
-    variances : float array with shape (n_features,)
-        Updated feature-wise variances.
-
-    n : int with shape (n_features,)
-        Updated number of seen samples.
+    means : ndarray of shape (n_features,) or (n_samples,), dtype=floating
+        Updated feature-wise means if axis = 0 or
+        sample-wise means if axis = 1.
+
+    variances : ndarray of shape (n_features,) or (n_samples,), dtype=floating
+        Updated feature-wise variances if axis = 0 or
+        sample-wise variances if axis = 1.
+
+    n : ndarray of shape (n_features,) or (n_samples,), dtype=integral
+        Updated number of seen samples per feature if axis=0
+        or number of seen features per sample if axis=1.
+
+        If weights is not None, n is a sum of the weights of the seen
+        samples or features instead of the actual number of seen
+        samples or features.

     Notes
     -----
     NaNs are ignored in the algorithm.
-
     """
     _raise_error_wrong_axis(axis)

-    if isinstance(X, sp.csr_matrix):
-        if axis == 0:
-            return _incr_mean_var_axis0(X, last_mean=last_mean,
-                                        last_var=last_var, last_n=last_n)
-        else:
-            return _incr_mean_var_axis0(X.T, last_mean=last_mean,
-                                        last_var=last_var, last_n=last_n)
-    elif isinstance(X, sp.csc_matrix):
-        if axis == 0:
-            return _incr_mean_var_axis0(X, last_mean=last_mean,
-                                        last_var=last_var, last_n=last_n)
-        else:
-            return _incr_mean_var_axis0(X.T, last_mean=last_mean,
-                                        last_var=last_var, last_n=last_n)
-    else:
+    if not isinstance(X, (sp.csr_matrix, sp.csc_matrix)):
         _raise_typeerror(X)
+
+    if np.size(last_n) == 1:
+        last_n = np.full(last_mean.shape, last_n, dtype=last_mean.dtype)
+
+    if not (np.size(last_mean) == np.size(last_var) == np.size(last_n)):
+        raise ValueError("last_mean, last_var, last_n do not have the same shapes.")
+
+    if axis == 1:
+        if np.size(last_mean) != X.shape[0]:
+            raise ValueError(
+                "If axis=1, then last_mean, last_n, last_var should be of "
+                f"size n_samples {X.shape[0]} (Got {np.size(last_mean)})."
+            )
+    else:  # axis == 0
+        if np.size(last_mean) != X.shape[1]:
+            raise ValueError(
+                "If axis=0, then last_mean, last_n, last_var should be of "
+                f"size n_features {X.shape[1]} (Got {np.size(last_mean)})."
+            )
+
+    X = X.T if axis == 1 else X
+
+    if weights is not None:
+        weights = _check_sample_weight(weights, X, dtype=X.dtype)
+
+    return _incr_mean_var_axis0(
+        X, last_mean=last_mean, last_var=last_var, last_n=last_n, weights=weights
+    )


 def inplace_column_scale(X, scale):
@@ -169,10 +228,11 @@

     Parameters
     ----------
-    X : CSC or CSR matrix with shape (n_samples, n_features)
-        Matrix to normalize using the variance of the features.
-
-    scale : float array with shape (n_features,)
+    X : sparse matrix of shape (n_samples, n_features)
+        Matrix to normalize using the variance of the features. It should be
+        of CSC or CSR format.
+
+    scale : ndarray of shape (n_features,), dtype={np.float32, np.float64}
         Array of precomputed feature-wise values to use for scaling.
     """
     if isinstance(X, sp.csc_matrix):
@@ -184,17 +244,17 @@


 def inplace_row_scale(X, scale):
-    """ Inplace row scaling of a CSR or CSC matrix.
+    """Inplace row scaling of a CSR or CSC matrix.

     Scale each row of the data matrix by multiplying with specific scale
     provided by the caller assuming a (n_samples, n_features) shape.

     Parameters
     ----------
-    X : CSR or CSC sparse matrix, shape (n_samples, n_features)
-        Matrix to be scaled.
-
-    scale : float array with shape (n_features,)
+    X : sparse matrix of shape (n_samples, n_features)
+        Matrix to be scaled. It should be of CSR or CSC format.
+
+    scale : ndarray of shape (n_features,), dtype={np.float32, np.float64}
         Array of precomputed sample-wise values to use for scaling.
     """
     if isinstance(X, sp.csc_matrix):
@@ -211,8 +271,9 @@

     Parameters
     ----------
-    X : scipy.sparse.csc_matrix, shape=(n_samples, n_features)
-        Matrix whose two rows are to be swapped.
+    X : sparse matrix of shape (n_samples, n_features)
+        Matrix whose two rows are to be swapped. It should be of
+        CSC format.

     m : int
         Index of the row of X to be swapped.
@@ -240,8 +301,9 @@

     Parameters
     ----------
-    X : scipy.sparse.csr_matrix, shape=(n_samples, n_features)
-        Matrix whose two rows are to be swapped.
+    X : sparse matrix of shape (n_samples, n_features)
+        Matrix whose two rows are to be swapped. It should be of
+        CSR format.

     m : int
         Index of the row of X to be swapped.
@@ -273,20 +335,28 @@

     if nz_m != nz_n:
         # Modify indptr first
-        X.indptr[m + 2:n] += nz_n - nz_m
+        X.indptr[m + 2 : n] += nz_n - nz_m
         X.indptr[m + 1] = m_start + nz_n
         X.indptr[n] = n_stop - nz_m

-    X.indices = np.concatenate([X.indices[:m_start],
-                                X.indices[n_start:n_stop],
-                                X.indices[m_stop:n_start],
-                                X.indices[m_start:m_stop],
-                                X.indices[n_stop:]])
-    X.data = np.concatenate([X.data[:m_start],
-                             X.data[n_start:n_stop],
-                             X.data[m_stop:n_start],
-                             X.data[m_start:m_stop],
-                             X.data[n_stop:]])
+    X.indices = np.concatenate(
+        [
+            X.indices[:m_start],
+            X.indices[n_start:n_stop],
+            X.indices[m_stop:n_start],
+            X.indices[m_start:m_stop],
+            X.indices[n_stop:],
+        ]
+    )
+    X.data = np.concatenate(
+        [
+            X.data[:m_start],
+            X.data[n_start:n_stop],
+            X.data[m_stop:n_start],
+            X.data[m_start:m_stop],
+            X.data[n_stop:],
+        ]
+    )


 def inplace_swap_row(X, m, n):
@@ -295,8 +365,9 @@

     Parameters
     ----------
-    X : CSR or CSC sparse matrix, shape=(n_samples, n_features)
-        Matrix whose two rows are to be swapped.
+    X : sparse matrix of shape (n_samples, n_features)
+        Matrix whose two rows are to be swapped. It should be of CSR or
+        CSC format.

     m : int
         Index of the row of X to be swapped.
@@ -318,8 +389,9 @@

     Parameters
     ----------
-    X : CSR or CSC sparse matrix, shape=(n_samples, n_features)
-        Matrix whose two columns are to be swapped.
+    X : sparse matrix of shape (n_samples, n_features)
+        Matrix whose two columns are to be swapped. It should be of
+        CSR or CSC format.

     m : int
         Index of the column of X to be swapped.
@@ -341,6 +413,11 @@

 def _minor_reduce(X, ufunc):
     major_index = np.flatnonzero(np.diff(X.indptr))
+
+    # reduceat tries casts X.indptr to intp, which errors
+    # if it is int64 on a 32 bit system.
+    # Reinitializing prevents this where possible, see #13737
+    X = type(X)((X.data, X.indices, X.indptr), shape=X.shape)
     value = ufunc.reduceat(X.data, X.indptr[major_index])
     return major_index, value

@@ -360,11 +437,13 @@
     value = np.compress(mask, value)

     if axis == 0:
-        res = sp.coo_matrix((value, (np.zeros(len(value)), major_index)),
-                            dtype=X.dtype, shape=(1, M))
-    else:
-        res = sp.coo_matrix((value, (major_index, np.zeros(len(value)))),
-                            dtype=X.dtype, shape=(M, 1))
+        res = sp.coo_matrix(
+            (value, (np.zeros(len(value)), major_index)), dtype=X.dtype, shape=(1, M)
+        )
+    else:
+        res = sp.coo_matrix(
+            (value, (major_index, np.zeros(len(value)))), dtype=X.dtype, shape=(M, 1)
+        )
     return res.A.ravel()


@@ -388,28 +467,30 @@


 def _sparse_min_max(X, axis):
-        return (_sparse_min_or_max(X, axis, np.minimum),
-                _sparse_min_or_max(X, axis, np.maximum))
+    return (
+        _sparse_min_or_max(X, axis, np.minimum),
+        _sparse_min_or_max(X, axis, np.maximum),
+    )


 def _sparse_nan_min_max(X, axis):
-    return(_sparse_min_or_max(X, axis, np.fmin),
-           _sparse_min_or_max(X, axis, np.fmax))
+    return (_sparse_min_or_max(X, axis, np.fmin), _sparse_min_or_max(X, axis, np.fmax))


 def min_max_axis(X, axis, ignore_nan=False):
-    """Compute minimum and maximum along an axis on a CSR or CSC matrix and
-    optionally ignore NaN values.
-
-    Parameters
-    ----------
-    X : CSR or CSC sparse matrix, shape (n_samples, n_features)
-        Input data.
-
-    axis : int (either 0 or 1)
+    """Compute minimium and maximum along an axis on a CSR or CSC matrix.
+
+     Optionally ignore NaN values.
+
+    Parameters
+    ----------
+    X : sparse matrix of shape (n_samples, n_features)
+        Input data. It should be of CSR or CSC format.
+
+    axis : {0, 1}
         Axis along which the axis should be computed.

-    ignore_nan : bool, default is False
+    ignore_nan : bool, default=False
         Ignore or passing through NaN values.

         .. versionadded:: 0.20
@@ -417,13 +498,13 @@
     Returns
     -------

-    mins : float array with shape (n_features,)
-        Feature-wise minima
-
-    maxs : float array with shape (n_features,)
-        Feature-wise maxima
-    """
-    if isinstance(X, sp.csr_matrix) or isinstance(X, sp.csc_matrix):
+    mins : ndarray of shape (n_features,), dtype={np.float32, np.float64}
+        Feature-wise minima.
+
+    maxs : ndarray of shape (n_features,), dtype={np.float32, np.float64}
+        Feature-wise maxima.
+    """
+    if isinstance(X, (sp.csr_matrix, sp.csc_matrix)):
         if ignore_nan:
             return _sparse_nan_min_max(X, axis=axis)
         else:
@@ -439,21 +520,21 @@

     Parameters
     ----------
-    X : CSR sparse matrix, shape = (n_samples, n_labels)
-        Input data.
-
-    axis : None, 0 or 1
+    X : sparse matrix of shape (n_samples, n_labels)
+        Input data. It should be of CSR format.
+
+    axis : {0, 1}, default=None
         The axis on which the data is aggregated.

-    sample_weight : array, shape = (n_samples,), optional
+    sample_weight : array-like of shape (n_samples,), default=None
         Weight for each row of X.
     """
     if axis == -1:
         axis = 1
     elif axis == -2:
         axis = 0
-    elif X.format != 'csr':
-        raise TypeError('Expected CSR sparse format, got {0}'.format(X.format))
+    elif X.format != "csr":
+        raise TypeError("Expected CSR sparse format, got {0}".format(X.format))

     # We rely here on the fact that np.diff(Y.indptr) for a CSR
     # will return the number of nonzero entries in each row.
@@ -468,23 +549,23 @@
         out = np.diff(X.indptr)
         if sample_weight is None:
             # astype here is for consistency with axis=0 dtype
-            return out.astype('intp')
+            return out.astype("intp")
         return out * sample_weight
     elif axis == 0:
         if sample_weight is None:
             return np.bincount(X.indices, minlength=X.shape[1])
         else:
             weights = np.repeat(sample_weight, np.diff(X.indptr))
-            return np.bincount(X.indices, minlength=X.shape[1],
-                            weights=weights)
-    else:
-        raise ValueError('Unsupported axis: {0}'.format(axis))
+            return np.bincount(X.indices, minlength=X.shape[1], weights=weights)
+    else:
+        raise ValueError("Unsupported axis: {0}".format(axis))


 def _get_median(data, n_zeros):
     """Compute the median of data with n_zeros additional zeros.

-    This function is used to support sparse matrices; it modifies data in-place
+    This function is used to support sparse matrices; it modifies data
+    in-place.
     """
     n_elems = len(data) + n_zeros
     if not n_elems:
@@ -496,8 +577,10 @@
     if is_odd:
         return _get_elem_at_rank(middle, data, n_negative, n_zeros)

-    return (_get_elem_at_rank(middle - 1, data, n_negative, n_zeros) +
-            _get_elem_at_rank(middle, data, n_negative, n_zeros)) / 2.
+    return (
+        _get_elem_at_rank(middle - 1, data, n_negative, n_zeros)
+        + _get_elem_at_rank(middle, data, n_negative, n_zeros)
+    ) / 2.0


 def _get_elem_at_rank(rank, data, n_negative, n_zeros):
@@ -515,12 +598,12 @@

     Parameters
     ----------
-    X : CSC sparse matrix, shape (n_samples, n_features)
-        Input data.
+    X : sparse matrix of shape (n_samples, n_features)
+        Input data. It should be of CSC format.

     Returns
     -------
-    median : ndarray, shape (n_features,)
+    median : ndarray of shape (n_features,)
         Median.

     """
@@ -534,7 +617,7 @@
     for f_ind, (start, end) in enumerate(zip(indptr[:-1], indptr[1:])):

         # Prevent modifying X in place
-        data = np.copy(X.data[start: end])
+        data = np.copy(X.data[start:end])
         nz = n_samples - data.size
         median[f_ind] = _get_median(data, nz)

('sklearn/utils', '_logistic_sigmoid.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,12 +1,9 @@
-#cython: boundscheck=False
-#cython: cdivision=True
-#cython: wraparound=False
-
 from libc.math cimport log, exp

 import numpy as np
 cimport numpy as np

+np.import_array()
 ctypedef np.float64_t DTYPE_t


('sklearn/utils', 'validation.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,74 +6,183 @@
 #          Lars Buitinck
 #          Alexandre Gramfort
 #          Nicolas Tresegnie
+#          Sylvain Marie
 # License: BSD 3 clause

+from functools import wraps
 import warnings
 import numbers
+import operator

 import numpy as np
 import scipy.sparse as sp
-from distutils.version import LooseVersion
-from inspect import signature
-
-from numpy.core.numeric import ComplexWarning
+from inspect import signature, isclass, Parameter
+
+# mypy error: Module 'numpy.core.numeric' has no attribute 'ComplexWarning'
+from numpy.core.numeric import ComplexWarning  # type: ignore
+import joblib
+
+from contextlib import suppress

 from .fixes import _object_dtype_isnan
 from .. import get_config as _get_config
-from ..exceptions import NonBLASDotWarning
+from ..exceptions import PositiveSpectrumWarning
 from ..exceptions import NotFittedError
 from ..exceptions import DataConversionWarning
-from ._joblib import Memory
-from ._joblib import __version__ as joblib_version

 FLOAT_DTYPES = (np.float64, np.float32, np.float16)

-# Silenced by default to reduce verbosity. Turn on at runtime for
-# performance profiling.
-warnings.simplefilter('ignore', NonBLASDotWarning)
-
-
-def _assert_all_finite(X, allow_nan=False):
+
+# This function is not used anymore at this moment in the code base but we keep it in
+# case that we merge a new public function without kwarg only by mistake, which would
+# require a deprecation cycle to fix.
+def _deprecate_positional_args(func=None, *, version="1.3"):
+    """Decorator for methods that issues warnings for positional arguments.
+
+    Using the keyword-only argument syntax in pep 3102, arguments after the
+    * will issue a warning when passed as a positional argument.
+
+    Parameters
+    ----------
+    func : callable, default=None
+        Function to check arguments on.
+    version : callable, default="1.3"
+        The version when positional arguments will result in error.
+    """
+
+    def _inner_deprecate_positional_args(f):
+        sig = signature(f)
+        kwonly_args = []
+        all_args = []
+
+        for name, param in sig.parameters.items():
+            if param.kind == Parameter.POSITIONAL_OR_KEYWORD:
+                all_args.append(name)
+            elif param.kind == Parameter.KEYWORD_ONLY:
+                kwonly_args.append(name)
+
+        @wraps(f)
+        def inner_f(*args, **kwargs):
+            extra_args = len(args) - len(all_args)
+            if extra_args <= 0:
+                return f(*args, **kwargs)
+
+            # extra_args > 0
+            args_msg = [
+                "{}={}".format(name, arg)
+                for name, arg in zip(kwonly_args[:extra_args], args[-extra_args:])
+            ]
+            args_msg = ", ".join(args_msg)
+            warnings.warn(
+                f"Pass {args_msg} as keyword args. From version "
+                f"{version} passing these as positional arguments "
+                "will result in an error",
+                FutureWarning,
+            )
+            kwargs.update(zip(sig.parameters, args))
+            return f(**kwargs)
+
+        return inner_f
+
+    if func is not None:
+        return _inner_deprecate_positional_args(func)
+
+    return _inner_deprecate_positional_args
+
+
+def _assert_all_finite(
+    X, allow_nan=False, msg_dtype=None, estimator_name=None, input_name=""
+):
     """Like assert_all_finite, but only for ndarray."""
     # validation is also imported in extmath
     from .extmath import _safe_accumulator_op

-    if _get_config()['assume_finite']:
+    if _get_config()["assume_finite"]:
         return
     X = np.asanyarray(X)
     # First try an O(n) time, O(1) space solution for the common case that
     # everything is finite; fall back to O(n) space np.isfinite to prevent
     # false positives from overflow in sum method. The sum is also calculated
     # safely to reduce dtype induced overflows.
-    is_float = X.dtype.kind in 'fc'
+    is_float = X.dtype.kind in "fc"
     if is_float and (np.isfinite(_safe_accumulator_op(np.sum, X))):
         pass
     elif is_float:
-        msg_err = "Input contains {} or a value too large for {!r}."
-        if (allow_nan and np.isinf(X).any() or
-                not allow_nan and not np.isfinite(X).all()):
-            type_err = 'infinity' if allow_nan else 'NaN, infinity'
-            raise ValueError(msg_err.format(type_err, X.dtype))
+        if (
+            allow_nan
+            and np.isinf(X).any()
+            or not allow_nan
+            and not np.isfinite(X).all()
+        ):
+            if not allow_nan and np.isnan(X).any():
+                type_err = "NaN"
+            else:
+                msg_dtype = msg_dtype if msg_dtype is not None else X.dtype
+                type_err = f"infinity or a value too large for {msg_dtype!r}"
+            padded_input_name = input_name + " " if input_name else ""
+            msg_err = f"Input {padded_input_name}contains {type_err}."
+            if (
+                not allow_nan
+                and estimator_name
+                and input_name == "X"
+                and np.isnan(X).any()
+            ):
+                # Improve the error message on how to handle missing values in
+                # scikit-learn.
+                msg_err += (
+                    f"\n{estimator_name} does not accept missing values"
+                    " encoded as NaN natively. For supervised learning, you might want"
+                    " to consider sklearn.ensemble.HistGradientBoostingClassifier and"
+                    " Regressor which accept missing values encoded as NaNs natively."
+                    " Alternatively, it is possible to preprocess the data, for"
+                    " instance by using an imputer transformer in a pipeline or drop"
+                    " samples with missing values. See"
+                    " https://scikit-learn.org/stable/modules/impute.html"
+                )
+            raise ValueError(msg_err)
+
     # for object dtype data, we only check for NaNs (GH-13254)
-    elif X.dtype == np.dtype('object') and not allow_nan:
+    elif X.dtype == np.dtype("object") and not allow_nan:
         if _object_dtype_isnan(X).any():
             raise ValueError("Input contains NaN")


-def assert_all_finite(X, allow_nan=False):
+def assert_all_finite(
+    X,
+    *,
+    allow_nan=False,
+    estimator_name=None,
+    input_name="",
+):
     """Throw a ValueError if X contains NaN or infinity.

     Parameters
     ----------
-    X : array or sparse matrix
-
-    allow_nan : bool
-    """
-    _assert_all_finite(X.data if sp.issparse(X) else X, allow_nan)
-
-
-def as_float_array(X, copy=True, force_all_finite=True):
-    """Converts an array-like to an array of floats.
+    X : {ndarray, sparse matrix}
+        The input data.
+
+    allow_nan : bool, default=False
+        If True, do not throw error when `X` contains NaN.
+
+    estimator_name : str, default=None
+        The estimator name, used to construct the error message.
+
+    input_name : str, default=""
+        The data name used to construct the error message. In particular
+        if `input_name` is "X" and the data has NaN values and
+        allow_nan is False, the error message will link to the imputer
+        documentation.
+    """
+    _assert_all_finite(
+        X.data if sp.issparse(X) else X,
+        allow_nan=allow_nan,
+        estimator_name=estimator_name,
+        input_name=input_name,
+    )
+
+
+def as_float_array(X, *, copy=True, force_all_finite=True):
+    """Convert an array-like to an array of floats.

     The new dtype will be np.float32 or np.float64, depending on the original
     type. The function can create a copy or modify the argument depending
@@ -82,39 +191,49 @@
     Parameters
     ----------
     X : {array-like, sparse matrix}
-
-    copy : bool, optional
+        The input data.
+
+    copy : bool, default=True
         If True, a copy of X will be created. If False, a copy may still be
         returned if X's dtype is not a floating point type.

-    force_all_finite : boolean or 'allow-nan', (default=True)
-        Whether to raise an error on np.inf and np.nan in X. The possibilities
-        are:
+    force_all_finite : bool or 'allow-nan', default=True
+        Whether to raise an error on np.inf, np.nan, pd.NA in X. The
+        possibilities are:

         - True: Force all values of X to be finite.
-        - False: accept both np.inf and np.nan in X.
-        - 'allow-nan': accept only np.nan values in X. Values cannot be
-          infinite.
+        - False: accepts np.inf, np.nan, pd.NA in X.
+        - 'allow-nan': accepts only np.nan and pd.NA values in X. Values cannot
+          be infinite.

         .. versionadded:: 0.20
            ``force_all_finite`` accepts the string ``'allow-nan'``.

+        .. versionchanged:: 0.23
+           Accepts `pd.NA` and converts it into `np.nan`
+
     Returns
     -------
-    XT : {array, sparse matrix}
-        An array of type np.float
-    """
-    if isinstance(X, np.matrix) or (not isinstance(X, np.ndarray)
-                                    and not sp.issparse(X)):
-        return check_array(X, ['csr', 'csc', 'coo'], dtype=np.float64,
-                           copy=copy, force_all_finite=force_all_finite,
-                           ensure_2d=False)
+    XT : {ndarray, sparse matrix}
+        An array of type float.
+    """
+    if isinstance(X, np.matrix) or (
+        not isinstance(X, np.ndarray) and not sp.issparse(X)
+    ):
+        return check_array(
+            X,
+            accept_sparse=["csr", "csc", "coo"],
+            dtype=np.float64,
+            copy=copy,
+            force_all_finite=force_all_finite,
+            ensure_2d=False,
+        )
     elif sp.issparse(X) and X.dtype in [np.float32, np.float64]:
         return X.copy() if copy else X
     elif X.dtype in [np.float32, np.float64]:  # is numpy array
-        return X.copy('F' if X.flags['F_CONTIGUOUS'] else 'C') if copy else X
+        return X.copy("F" if X.flags["F_CONTIGUOUS"] else "C") if copy else X
     else:
-        if X.dtype.kind in 'uib' and X.dtype.itemsize <= 4:
+        if X.dtype.kind in "uib" and X.dtype.itemsize <= 4:
             return_dtype = np.float32
         else:
             return_dtype = np.float64
@@ -122,36 +241,95 @@


 def _is_arraylike(x):
-    """Returns whether the input is array-like"""
-    return (hasattr(x, '__len__') or
-            hasattr(x, 'shape') or
-            hasattr(x, '__array__'))
+    """Returns whether the input is array-like."""
+    return hasattr(x, "__len__") or hasattr(x, "shape") or hasattr(x, "__array__")
+
+
+def _is_arraylike_not_scalar(array):
+    """Return True if array is array-like and not a scalar"""
+    return _is_arraylike(array) and not np.isscalar(array)
+
+
+def _num_features(X):
+    """Return the number of features in an array-like X.
+
+    This helper function tries hard to avoid to materialize an array version
+    of X unless necessary. For instance, if X is a list of lists,
+    this function will return the length of the first element, assuming
+    that subsequent elements are all lists of the same length without
+    checking.
+    Parameters
+    ----------
+    X : array-like
+        array-like to get the number of features.
+
+    Returns
+    -------
+    features : int
+        Number of features
+    """
+    type_ = type(X)
+    if type_.__module__ == "builtins":
+        type_name = type_.__qualname__
+    else:
+        type_name = f"{type_.__module__}.{type_.__qualname__}"
+    message = f"Unable to find the number of features from X of type {type_name}"
+    if not hasattr(X, "__len__") and not hasattr(X, "shape"):
+        if not hasattr(X, "__array__"):
+            raise TypeError(message)
+        # Only convert X to a numpy array if there is no cheaper, heuristic
+        # option.
+        X = np.asarray(X)
+
+    if hasattr(X, "shape"):
+        if not hasattr(X.shape, "__len__") or len(X.shape) <= 1:
+            message += f" with shape {X.shape}"
+            raise TypeError(message)
+        return X.shape[1]
+
+    first_sample = X[0]
+
+    # Do not consider an array-like of strings or dicts to be a 2D array
+    if isinstance(first_sample, (str, bytes, dict)):
+        message += f" where the samples are of type {type(first_sample).__qualname__}"
+        raise TypeError(message)
+
+    try:
+        # If X is a list of lists, for instance, we assume that all nested
+        # lists have the same length without checking or converting to
+        # a numpy array to keep this function call as cheap as possible.
+        return len(first_sample)
+    except Exception as err:
+        raise TypeError(message) from err


 def _num_samples(x):
     """Return number of samples in array-like x."""
-    if hasattr(x, 'fit') and callable(x.fit):
+    message = "Expected sequence or array-like, got %s" % type(x)
+    if hasattr(x, "fit") and callable(x.fit):
         # Don't get num_samples from an ensembles length!
-        raise TypeError('Expected sequence or array-like, got '
-                        'estimator %s' % x)
-    if not hasattr(x, '__len__') and not hasattr(x, 'shape'):
-        if hasattr(x, '__array__'):
+        raise TypeError(message)
+
+    if not hasattr(x, "__len__") and not hasattr(x, "shape"):
+        if hasattr(x, "__array__"):
             x = np.asarray(x)
         else:
-            raise TypeError("Expected sequence or array-like, got %s" %
-                            type(x))
-    if hasattr(x, 'shape'):
+            raise TypeError(message)
+
+    if hasattr(x, "shape") and x.shape is not None:
         if len(x.shape) == 0:
-            raise TypeError("Singleton array %r cannot be considered"
-                            " a valid collection." % x)
+            raise TypeError(
+                "Singleton array %r cannot be considered a valid collection." % x
+            )
         # Check that shape is returning an integer or default to len
         # Dask dataframes may not return numeric shape[0] value
         if isinstance(x.shape[0], numbers.Integral):
             return x.shape[0]
-        else:
-            return len(x)
-    else:
+
+    try:
         return len(x)
+    except TypeError as type_error:
+        raise TypeError(message) from type_error


 def check_memory(memory):
@@ -164,26 +342,27 @@
     Parameters
     ----------
     memory : None, str or object with the joblib.Memory interface
+        - If string, the location where to create the `joblib.Memory` interface.
+        - If None, no caching is done and the Memory object is completely transparent.

     Returns
     -------
     memory : object with the joblib.Memory interface
+        A correct joblib.Memory object.

     Raises
     ------
     ValueError
         If ``memory`` is not joblib.Memory-like.
     """
-
     if memory is None or isinstance(memory, str):
-        if LooseVersion(joblib_version) < '0.12':
-            memory = Memory(cachedir=memory, verbose=0)
-        else:
-            memory = Memory(location=memory, verbose=0)
-    elif not hasattr(memory, 'cache'):
-        raise ValueError("'memory' should be None, a string or have the same"
-                         " interface as joblib.Memory."
-                         " Got memory='{}' instead.".format(memory))
+        memory = joblib.Memory(location=memory, verbose=0)
+    elif not hasattr(memory, "cache"):
+        raise ValueError(
+            "'memory' should be None, a string or have the same"
+            " interface as joblib.Memory."
+            " Got memory='{}' instead.".format(memory)
+        )
     return memory


@@ -201,8 +380,30 @@
     lengths = [_num_samples(X) for X in arrays if X is not None]
     uniques = np.unique(lengths)
     if len(uniques) > 1:
-        raise ValueError("Found input variables with inconsistent numbers of"
-                         " samples: %r" % [int(l) for l in lengths])
+        raise ValueError(
+            "Found input variables with inconsistent numbers of samples: %r"
+            % [int(l) for l in lengths]
+        )
+
+
+def _make_indexable(iterable):
+    """Ensure iterable supports indexing or convert to an indexable variant.
+
+    Convert sparse matrices to csr and other non-indexable iterable to arrays.
+    Let `None` and indexable objects (e.g. pandas dataframes) pass unchanged.
+
+    Parameters
+    ----------
+    iterable : {list, dataframe, ndarray, sparse matrix} or None
+        Object to be converted to an indexable iterable.
+    """
+    if sp.issparse(iterable):
+        return iterable.tocsr()
+    elif hasattr(iterable, "__getitem__") or hasattr(iterable, "iloc"):
+        return iterable
+    elif iterable is None:
+        return iterable
+    return np.array(iterable)


 def indexable(*iterables):
@@ -214,63 +415,82 @@

     Parameters
     ----------
-    *iterables : lists, dataframes, arrays, sparse matrices
+    *iterables : {lists, dataframes, ndarrays, sparse matrices}
         List of objects to ensure sliceability.
-    """
-    result = []
-    for X in iterables:
-        if sp.issparse(X):
-            result.append(X.tocsr())
-        elif hasattr(X, "__getitem__") or hasattr(X, "iloc"):
-            result.append(X)
-        elif X is None:
-            result.append(X)
-        else:
-            result.append(np.array(X))
+
+    Returns
+    -------
+    result : list of {ndarray, sparse matrix, dataframe} or None
+        Returns a list containing indexable arrays (i.e. NumPy array,
+        sparse matrix, or dataframe) or `None`.
+    """
+
+    result = [_make_indexable(X) for X in iterables]
     check_consistent_length(*result)
     return result


-def _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy,
-                          force_all_finite, accept_large_sparse):
+def _ensure_sparse_format(
+    spmatrix,
+    accept_sparse,
+    dtype,
+    copy,
+    force_all_finite,
+    accept_large_sparse,
+    estimator_name=None,
+    input_name="",
+):
     """Convert a sparse matrix to a given format.

     Checks the sparse format of spmatrix and converts if necessary.

     Parameters
     ----------
-    spmatrix : scipy sparse matrix
+    spmatrix : sparse matrix
         Input to validate and convert.

-    accept_sparse : string, boolean or list/tuple of strings
+    accept_sparse : str, bool or list/tuple of str
         String[s] representing allowed sparse matrix formats ('csc',
         'csr', 'coo', 'dok', 'bsr', 'lil', 'dia'). If the input is sparse but
         not in the allowed format, it will be converted to the first listed
         format. True allows the input to be any format. False means
         that a sparse matrix input will raise an error.

-    dtype : string, type or None
+    dtype : str, type or None
         Data type of result. If None, the dtype of the input is preserved.

-    copy : boolean
+    copy : bool
         Whether a forced copy will be triggered. If copy=False, a copy might
         be triggered by a conversion.

-    force_all_finite : boolean or 'allow-nan', (default=True)
-        Whether to raise an error on np.inf and np.nan in X. The possibilities
-        are:
+    force_all_finite : bool or 'allow-nan'
+        Whether to raise an error on np.inf, np.nan, pd.NA in X. The
+        possibilities are:

         - True: Force all values of X to be finite.
-        - False: accept both np.inf and np.nan in X.
-        - 'allow-nan': accept only np.nan values in X. Values cannot be
-          infinite.
+        - False: accepts np.inf, np.nan, pd.NA in X.
+        - 'allow-nan': accepts only np.nan and pd.NA values in X. Values cannot
+          be infinite.

         .. versionadded:: 0.20
            ``force_all_finite`` accepts the string ``'allow-nan'``.

+        .. versionchanged:: 0.23
+           Accepts `pd.NA` and converts it into `np.nan`
+
+
+    estimator_name : str, default=None
+        The estimator name, used to construct the error message.
+
+    input_name : str, default=""
+        The data name used to construct the error message. In particular
+        if `input_name` is "X" and the data has NaN values and
+        allow_nan is False, the error message will link to the imputer
+        documentation.
+
     Returns
     -------
-    spmatrix_converted : scipy sparse matrix.
+    spmatrix_converted : sparse matrix.
         Matrix that is ensured to have an allowed type.
     """
     if dtype is None:
@@ -285,14 +505,18 @@
     _check_large_sparse(spmatrix, accept_large_sparse)

     if accept_sparse is False:
-        raise TypeError('A sparse matrix was passed, but dense '
-                        'data is required. Use X.toarray() to '
-                        'convert to a dense numpy array.')
+        raise TypeError(
+            "A sparse matrix was passed, but dense "
+            "data is required. Use X.toarray() to "
+            "convert to a dense numpy array."
+        )
     elif isinstance(accept_sparse, (list, tuple)):
         if len(accept_sparse) == 0:
-            raise ValueError("When providing 'accept_sparse' "
-                             "as a tuple or list, it must contain at "
-                             "least one string value.")
+            raise ValueError(
+                "When providing 'accept_sparse' "
+                "as a tuple or list, it must contain at "
+                "least one string value."
+            )
         # ensure correct sparse format
         if spmatrix.format not in accept_sparse:
             # create new with correct sparse
@@ -300,9 +524,11 @@
             changed_format = True
     elif accept_sparse is not True:
         # any other type
-        raise ValueError("Parameter 'accept_sparse' should be a string, "
-                         "boolean or list of strings. You provided "
-                         "'accept_sparse={}'.".format(accept_sparse))
+        raise ValueError(
+            "Parameter 'accept_sparse' should be a string, "
+            "boolean or list of strings. You provided "
+            "'accept_sparse={}'.".format(accept_sparse)
+        )

     if dtype != spmatrix.dtype:
         # convert dtype
@@ -313,26 +539,95 @@

     if force_all_finite:
         if not hasattr(spmatrix, "data"):
-            warnings.warn("Can't check %s sparse matrix for nan or inf."
-                          % spmatrix.format)
+            warnings.warn(
+                "Can't check %s sparse matrix for nan or inf." % spmatrix.format,
+                stacklevel=2,
+            )
         else:
-            _assert_all_finite(spmatrix.data,
-                               allow_nan=force_all_finite == 'allow-nan')
+            _assert_all_finite(
+                spmatrix.data,
+                allow_nan=force_all_finite == "allow-nan",
+                estimator_name=estimator_name,
+                input_name=input_name,
+            )

     return spmatrix


 def _ensure_no_complex_data(array):
-    if hasattr(array, 'dtype') and array.dtype is not None \
-            and hasattr(array.dtype, 'kind') and array.dtype.kind == "c":
-        raise ValueError("Complex data not supported\n"
-                         "{}\n".format(array))
-
-
-def check_array(array, accept_sparse=False, accept_large_sparse=True,
-                dtype="numeric", order=None, copy=False, force_all_finite=True,
-                ensure_2d=True, allow_nd=False, ensure_min_samples=1,
-                ensure_min_features=1, warn_on_dtype=None, estimator=None):
+    if (
+        hasattr(array, "dtype")
+        and array.dtype is not None
+        and hasattr(array.dtype, "kind")
+        and array.dtype.kind == "c"
+    ):
+        raise ValueError("Complex data not supported\n{}\n".format(array))
+
+
+def _check_estimator_name(estimator):
+    if estimator is not None:
+        if isinstance(estimator, str):
+            return estimator
+        else:
+            return estimator.__class__.__name__
+    return None
+
+
+def _pandas_dtype_needs_early_conversion(pd_dtype):
+    """Return True if pandas extension pd_dtype need to be converted early."""
+    # Check these early for pandas versions without extension dtypes
+    from pandas.api.types import (
+        is_bool_dtype,
+        is_sparse,
+        is_float_dtype,
+        is_integer_dtype,
+    )
+
+    if is_bool_dtype(pd_dtype):
+        # bool and extension booleans need early converstion because __array__
+        # converts mixed dtype dataframes into object dtypes
+        return True
+
+    if is_sparse(pd_dtype):
+        # Sparse arrays will be converted later in `check_array`
+        return False
+
+    try:
+        from pandas.api.types import is_extension_array_dtype
+    except ImportError:
+        return False
+
+    if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):
+        # Sparse arrays will be converted later in `check_array`
+        # Only handle extension arrays for integer and floats
+        return False
+    elif is_float_dtype(pd_dtype):
+        # Float ndarrays can normally support nans. They need to be converted
+        # first to map pd.NA to np.nan
+        return True
+    elif is_integer_dtype(pd_dtype):
+        # XXX: Warn when converting from a high integer to a float
+        return True
+
+    return False
+
+
+def check_array(
+    array,
+    accept_sparse=False,
+    *,
+    accept_large_sparse=True,
+    dtype="numeric",
+    order=None,
+    copy=False,
+    force_all_finite=True,
+    ensure_2d=True,
+    allow_nd=False,
+    ensure_min_samples=1,
+    ensure_min_features=1,
+    estimator=None,
+    input_name="",
+):

     """Input validation on an array, list, sparse matrix or similar.

@@ -345,91 +640,93 @@
     array : object
         Input object to check / convert.

-    accept_sparse : string, boolean or list/tuple of strings (default=False)
+    accept_sparse : str, bool or list/tuple of str, default=False
         String[s] representing allowed sparse matrix formats, such as 'csc',
         'csr', etc. If the input is sparse but not in the allowed format,
         it will be converted to the first listed format. True allows the input
         to be any format. False means that a sparse matrix input will
         raise an error.

-    accept_large_sparse : bool (default=True)
+    accept_large_sparse : bool, default=True
         If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by
         accept_sparse, accept_large_sparse=False will cause it to be accepted
         only if its indices are stored with a 32-bit dtype.

         .. versionadded:: 0.20

-    dtype : string, type, list of types or None (default="numeric")
+    dtype : 'numeric', type, list of type or None, default='numeric'
         Data type of result. If None, the dtype of the input is preserved.
         If "numeric", dtype is preserved unless array.dtype is object.
         If dtype is a list of types, conversion on the first type is only
         performed if the dtype of the input is not in the list.

-    order : 'F', 'C' or None (default=None)
+    order : {'F', 'C'} or None, default=None
         Whether an array will be forced to be fortran or c-style.
         When order is None (default), then if copy=False, nothing is ensured
         about the memory layout of the output array; otherwise (copy=True)
         the memory layout of the returned array is kept as close as possible
         to the original array.

-    copy : boolean (default=False)
+    copy : bool, default=False
         Whether a forced copy will be triggered. If copy=False, a copy might
         be triggered by a conversion.

-    force_all_finite : boolean or 'allow-nan', (default=True)
-        Whether to raise an error on np.inf and np.nan in array. The
+    force_all_finite : bool or 'allow-nan', default=True
+        Whether to raise an error on np.inf, np.nan, pd.NA in array. The
         possibilities are:

         - True: Force all values of array to be finite.
-        - False: accept both np.inf and np.nan in array.
-        - 'allow-nan': accept only np.nan values in array. Values cannot
-          be infinite.
-
-        For object dtyped data, only np.nan is checked and not np.inf.
+        - False: accepts np.inf, np.nan, pd.NA in array.
+        - 'allow-nan': accepts only np.nan and pd.NA values in array. Values
+          cannot be infinite.

         .. versionadded:: 0.20
            ``force_all_finite`` accepts the string ``'allow-nan'``.

-    ensure_2d : boolean (default=True)
+        .. versionchanged:: 0.23
+           Accepts `pd.NA` and converts it into `np.nan`
+
+    ensure_2d : bool, default=True
         Whether to raise a value error if array is not 2D.

-    allow_nd : boolean (default=False)
+    allow_nd : bool, default=False
         Whether to allow array.ndim > 2.

-    ensure_min_samples : int (default=1)
+    ensure_min_samples : int, default=1
         Make sure that the array has a minimum number of samples in its first
         axis (rows for a 2D array). Setting to 0 disables this check.

-    ensure_min_features : int (default=1)
+    ensure_min_features : int, default=1
         Make sure that the 2D array has some minimum number of features
         (columns). The default value of 1 rejects empty datasets.
         This check is only enforced when the input data has effectively 2
         dimensions or is originally 1D and ``ensure_2d`` is True. Setting to 0
         disables this check.

-    warn_on_dtype : boolean or None, optional (default=None)
-        Raise DataConversionWarning if the dtype of the input data structure
-        does not match the requested dtype, causing a memory copy.
-
-        .. deprecated:: 0.21
-            ``warn_on_dtype`` is deprecated in version 0.21 and will be
-            removed in 0.23.
-
-    estimator : str or estimator instance (default=None)
+    estimator : str or estimator instance, default=None
         If passed, include the name of the estimator in warning messages.
+
+    input_name : str, default=""
+        The data name used to construct the error message. In particular
+        if `input_name` is "X" and the data has NaN values and
+        allow_nan is False, the error message will link to the imputer
+        documentation.
+
+        .. versionadded:: 1.1.0

     Returns
     -------
     array_converted : object
         The converted and validated array.
     """
-    # warn_on_dtype deprecation
-    if warn_on_dtype is not None:
+    if isinstance(array, np.matrix):
         warnings.warn(
-            "'warn_on_dtype' is deprecated in version 0.21 and will be "
-            "removed in 0.23. Don't set `warn_on_dtype` to remove this "
-            "warning.",
-            DeprecationWarning)
+            "np.matrix usage is deprecated in 1.0 and will raise a TypeError "
+            "in 1.2. Please convert to a numpy array with np.asarray. For "
+            "more information see: "
+            "https://numpy.org/doc/stable/reference/generated/numpy.matrix.html",  # noqa
+            FutureWarning,
+        )

     # store reference to original array to check if copy is needed when
     # function returns
@@ -439,15 +736,32 @@
     dtype_numeric = isinstance(dtype, str) and dtype == "numeric"

     dtype_orig = getattr(array, "dtype", None)
-    if not hasattr(dtype_orig, 'kind'):
+    if not hasattr(dtype_orig, "kind"):
         # not a data type (e.g. a column named dtype in a pandas DataFrame)
         dtype_orig = None

     # check if the object contains several dtypes (typically a pandas
     # DataFrame), and store them. If not, store None.
     dtypes_orig = None
-    if hasattr(array, "dtypes") and hasattr(array.dtypes, '__array__'):
-        dtypes_orig = np.array(array.dtypes)
+    pandas_requires_conversion = False
+    if hasattr(array, "dtypes") and hasattr(array.dtypes, "__array__"):
+        # throw warning if columns are sparse. If all columns are sparse, then
+        # array.sparse exists and sparsity will be preserved (later).
+        with suppress(ImportError):
+            from pandas.api.types import is_sparse
+
+            if not hasattr(array, "sparse") and array.dtypes.apply(is_sparse).any():
+                warnings.warn(
+                    "pandas.DataFrame with sparse columns found."
+                    "It will be converted to a dense numpy array."
+                )
+
+        dtypes_orig = list(array.dtypes)
+        pandas_requires_conversion = any(
+            _pandas_dtype_needs_early_conversion(i) for i in dtypes_orig
+        )
+        if all(isinstance(dtype_iter, np.dtype) for dtype_iter in dtypes_orig):
+            dtype_orig = np.result_type(*dtypes_orig)

     if dtype_numeric:
         if dtype_orig is not None and dtype_orig.kind == "O":
@@ -465,25 +779,52 @@
             # list of accepted types.
             dtype = dtype[0]

-    if force_all_finite not in (True, False, 'allow-nan'):
-        raise ValueError('force_all_finite should be a bool or "allow-nan"'
-                         '. Got {!r} instead'.format(force_all_finite))
-
-    if estimator is not None:
-        if isinstance(estimator, str):
-            estimator_name = estimator
-        else:
-            estimator_name = estimator.__class__.__name__
-    else:
-        estimator_name = "Estimator"
+    if pandas_requires_conversion:
+        # pandas dataframe requires conversion earlier to handle extension dtypes with
+        # nans
+        # Use the original dtype for conversion if dtype is None
+        new_dtype = dtype_orig if dtype is None else dtype
+        array = array.astype(new_dtype)
+        # Since we converted here, we do not need to convert again later
+        dtype = None
+
+    if force_all_finite not in (True, False, "allow-nan"):
+        raise ValueError(
+            'force_all_finite should be a bool or "allow-nan". Got {!r} instead'.format(
+                force_all_finite
+            )
+        )
+
+    estimator_name = _check_estimator_name(estimator)
     context = " by %s" % estimator_name if estimator is not None else ""
+
+    # When all dataframe columns are sparse, convert to a sparse array
+    if hasattr(array, "sparse") and array.ndim > 1:
+        # DataFrame.sparse only supports `to_coo`
+        array = array.sparse.to_coo()
+        if array.dtype == np.dtype("object"):
+            unique_dtypes = set([dt.subtype.name for dt in array_orig.dtypes])
+            if len(unique_dtypes) > 1:
+                raise ValueError(
+                    "Pandas DataFrame with mixed sparse extension arrays "
+                    "generated a sparse matrix with object dtype which "
+                    "can not be converted to a scipy sparse matrix."
+                    "Sparse extension arrays should all have the same "
+                    "numeric type."
+                )

     if sp.issparse(array):
         _ensure_no_complex_data(array)
-        array = _ensure_sparse_format(array, accept_sparse=accept_sparse,
-                                      dtype=dtype, copy=copy,
-                                      force_all_finite=force_all_finite,
-                                      accept_large_sparse=accept_large_sparse)
+        array = _ensure_sparse_format(
+            array,
+            accept_sparse=accept_sparse,
+            dtype=dtype,
+            copy=copy,
+            force_all_finite=force_all_finite,
+            accept_large_sparse=accept_large_sparse,
+            estimator_name=estimator_name,
+            input_name=input_name,
+        )
     else:
         # If np.array(..) gives ComplexWarning, then we convert the warning
         # to an error. This is needed because specifying a non complex
@@ -492,11 +833,27 @@
         # of warnings context manager.
         with warnings.catch_warnings():
             try:
-                warnings.simplefilter('error', ComplexWarning)
-                array = np.asarray(array, dtype=dtype, order=order)
-            except ComplexWarning:
-                raise ValueError("Complex data not supported\n"
-                                 "{}\n".format(array))
+                warnings.simplefilter("error", ComplexWarning)
+                if dtype is not None and np.dtype(dtype).kind in "iu":
+                    # Conversion float -> int should not contain NaN or
+                    # inf (numpy#14412). We cannot use casting='safe' because
+                    # then conversion float -> int would be disallowed.
+                    array = np.asarray(array, order=order)
+                    if array.dtype.kind == "f":
+                        _assert_all_finite(
+                            array,
+                            allow_nan=False,
+                            msg_dtype=dtype,
+                            estimator_name=estimator_name,
+                            input_name=input_name,
+                        )
+                    array = array.astype(dtype, casting="unsafe", copy=False)
+                else:
+                    array = np.asarray(array, order=order, dtype=dtype)
+            except ComplexWarning as complex_warning:
+                raise ValueError(
+                    "Complex data not supported\n{}\n".format(array)
+                ) from complex_warning

         # It is possible that the np.array(..) gave no warning. This happens
         # when no dtype conversion happened, for example dtype = None. The
@@ -511,97 +868,98 @@
                     "Expected 2D array, got scalar array instead:\narray={}.\n"
                     "Reshape your data either using array.reshape(-1, 1) if "
                     "your data has a single feature or array.reshape(1, -1) "
-                    "if it contains a single sample.".format(array))
+                    "if it contains a single sample.".format(array)
+                )
             # If input is 1D raise error
             if array.ndim == 1:
                 raise ValueError(
                     "Expected 2D array, got 1D array instead:\narray={}.\n"
                     "Reshape your data either using array.reshape(-1, 1) if "
                     "your data has a single feature or array.reshape(1, -1) "
-                    "if it contains a single sample.".format(array))
-
-        # in the future np.flexible dtypes will be handled like object dtypes
-        if dtype_numeric and np.issubdtype(array.dtype, np.flexible):
-            warnings.warn(
-                "Beginning in version 0.22, arrays of bytes/strings will be "
-                "converted to decimal numbers if dtype='numeric'. "
-                "It is recommended that you convert the array to "
-                "a float dtype before using it in scikit-learn, "
-                "for example by using "
-                "your_array = your_array.astype(np.float64).",
-                FutureWarning)
-
-        # make sure we actually converted to numeric:
-        if dtype_numeric and array.dtype.kind == "O":
-            array = array.astype(np.float64)
+                    "if it contains a single sample.".format(array)
+                )
+
+        if dtype_numeric and array.dtype.kind in "USV":
+            raise ValueError(
+                "dtype='numeric' is not compatible with arrays of bytes/strings."
+                "Convert your data to numeric values explicitly instead."
+            )
+
         if not allow_nd and array.ndim >= 3:
-            raise ValueError("Found array with dim %d. %s expected <= 2."
-                             % (array.ndim, estimator_name))
+            raise ValueError(
+                "Found array with dim %d. %s expected <= 2."
+                % (array.ndim, estimator_name)
+            )
+
         if force_all_finite:
-            _assert_all_finite(array,
-                               allow_nan=force_all_finite == 'allow-nan')
+            _assert_all_finite(
+                array,
+                input_name=input_name,
+                estimator_name=estimator_name,
+                allow_nan=force_all_finite == "allow-nan",
+            )

     if ensure_min_samples > 0:
         n_samples = _num_samples(array)
         if n_samples < ensure_min_samples:
-            raise ValueError("Found array with %d sample(s) (shape=%s) while a"
-                             " minimum of %d is required%s."
-                             % (n_samples, array.shape, ensure_min_samples,
-                                context))
+            raise ValueError(
+                "Found array with %d sample(s) (shape=%s) while a"
+                " minimum of %d is required%s."
+                % (n_samples, array.shape, ensure_min_samples, context)
+            )

     if ensure_min_features > 0 and array.ndim == 2:
         n_features = array.shape[1]
         if n_features < ensure_min_features:
-            raise ValueError("Found array with %d feature(s) (shape=%s) while"
-                             " a minimum of %d is required%s."
-                             % (n_features, array.shape, ensure_min_features,
-                                context))
-
-    if warn_on_dtype and dtype_orig is not None and array.dtype != dtype_orig:
-        msg = ("Data with input dtype %s was converted to %s%s."
-               % (dtype_orig, array.dtype, context))
-        warnings.warn(msg, DataConversionWarning)
+            raise ValueError(
+                "Found array with %d feature(s) (shape=%s) while"
+                " a minimum of %d is required%s."
+                % (n_features, array.shape, ensure_min_features, context)
+            )

     if copy and np.may_share_memory(array, array_orig):
         array = np.array(array, dtype=dtype, order=order)

-    if (warn_on_dtype and dtypes_orig is not None and
-            {array.dtype} != set(dtypes_orig)):
-        # if there was at the beginning some other types than the final one
-        # (for instance in a DataFrame that can contain several dtypes) then
-        # some data must have been converted
-        msg = ("Data with input dtype %s were all converted to %s%s."
-               % (', '.join(map(str, sorted(set(dtypes_orig)))), array.dtype,
-                  context))
-        warnings.warn(msg, DataConversionWarning, stacklevel=3)
-
     return array


 def _check_large_sparse(X, accept_large_sparse=False):
-    """Raise a ValueError if X has 64bit indices and accept_large_sparse=False
-    """
+    """Raise a ValueError if X has 64bit indices and accept_large_sparse=False"""
     if not accept_large_sparse:
         supported_indices = ["int32"]
         if X.getformat() == "coo":
-            index_keys = ['col', 'row']
+            index_keys = ["col", "row"]
         elif X.getformat() in ["csr", "csc", "bsr"]:
-            index_keys = ['indices', 'indptr']
+            index_keys = ["indices", "indptr"]
         else:
             return
         for key in index_keys:
             indices_datatype = getattr(X, key).dtype
-            if (indices_datatype not in supported_indices):
-                raise ValueError("Only sparse matrices with 32-bit integer"
-                                 " indices are accepted. Got %s indices."
-                                 % indices_datatype)
-
-
-def check_X_y(X, y, accept_sparse=False, accept_large_sparse=True,
-              dtype="numeric", order=None, copy=False, force_all_finite=True,
-              ensure_2d=True, allow_nd=False, multi_output=False,
-              ensure_min_samples=1, ensure_min_features=1, y_numeric=False,
-              warn_on_dtype=None, estimator=None):
+            if indices_datatype not in supported_indices:
+                raise ValueError(
+                    "Only sparse matrices with 32-bit integer"
+                    " indices are accepted. Got %s indices." % indices_datatype
+                )
+
+
+def check_X_y(
+    X,
+    y,
+    accept_sparse=False,
+    *,
+    accept_large_sparse=True,
+    dtype="numeric",
+    order=None,
+    copy=False,
+    force_all_finite=True,
+    ensure_2d=True,
+    allow_nd=False,
+    multi_output=False,
+    ensure_min_samples=1,
+    ensure_min_features=1,
+    y_numeric=False,
+    estimator=None,
+):
     """Input validation for standard estimators.

     Checks X and y for consistent length, enforces X to be 2D and y 1D. By
@@ -613,88 +971,83 @@

     Parameters
     ----------
-    X : nd-array, list or sparse matrix
+    X : {ndarray, list, sparse matrix}
         Input data.

-    y : nd-array, list or sparse matrix
+    y : {ndarray, list, sparse matrix}
         Labels.

-    accept_sparse : string, boolean or list of string (default=False)
+    accept_sparse : str, bool or list of str, default=False
         String[s] representing allowed sparse matrix formats, such as 'csc',
         'csr', etc. If the input is sparse but not in the allowed format,
         it will be converted to the first listed format. True allows the input
         to be any format. False means that a sparse matrix input will
         raise an error.

-    accept_large_sparse : bool (default=True)
+    accept_large_sparse : bool, default=True
         If a CSR, CSC, COO or BSR sparse matrix is supplied and accepted by
         accept_sparse, accept_large_sparse will cause it to be accepted only
         if its indices are stored with a 32-bit dtype.

         .. versionadded:: 0.20

-    dtype : string, type, list of types or None (default="numeric")
+    dtype : 'numeric', type, list of type or None, default='numeric'
         Data type of result. If None, the dtype of the input is preserved.
         If "numeric", dtype is preserved unless array.dtype is object.
         If dtype is a list of types, conversion on the first type is only
         performed if the dtype of the input is not in the list.

-    order : 'F', 'C' or None (default=None)
+    order : {'F', 'C'}, default=None
         Whether an array will be forced to be fortran or c-style.

-    copy : boolean (default=False)
+    copy : bool, default=False
         Whether a forced copy will be triggered. If copy=False, a copy might
         be triggered by a conversion.

-    force_all_finite : boolean or 'allow-nan', (default=True)
-        Whether to raise an error on np.inf and np.nan in X. This parameter
-        does not influence whether y can have np.inf or np.nan values.
+    force_all_finite : bool or 'allow-nan', default=True
+        Whether to raise an error on np.inf, np.nan, pd.NA in X. This parameter
+        does not influence whether y can have np.inf, np.nan, pd.NA values.
         The possibilities are:

         - True: Force all values of X to be finite.
-        - False: accept both np.inf and np.nan in X.
-        - 'allow-nan': accept only np.nan values in X. Values cannot be
-          infinite.
+        - False: accepts np.inf, np.nan, pd.NA in X.
+        - 'allow-nan': accepts only np.nan or pd.NA values in X. Values cannot
+          be infinite.

         .. versionadded:: 0.20
            ``force_all_finite`` accepts the string ``'allow-nan'``.

-    ensure_2d : boolean (default=True)
+        .. versionchanged:: 0.23
+           Accepts `pd.NA` and converts it into `np.nan`
+
+    ensure_2d : bool, default=True
         Whether to raise a value error if X is not 2D.

-    allow_nd : boolean (default=False)
+    allow_nd : bool, default=False
         Whether to allow X.ndim > 2.

-    multi_output : boolean (default=False)
+    multi_output : bool, default=False
         Whether to allow 2D y (array or sparse matrix). If false, y will be
         validated as a vector. y cannot have np.nan or np.inf values if
         multi_output=True.

-    ensure_min_samples : int (default=1)
+    ensure_min_samples : int, default=1
         Make sure that X has a minimum number of samples in its first
         axis (rows for a 2D array).

-    ensure_min_features : int (default=1)
+    ensure_min_features : int, default=1
         Make sure that the 2D array has some minimum number of features
         (columns). The default value of 1 rejects empty datasets.
         This check is only enforced when X has effectively 2 dimensions or
         is originally 1D and ``ensure_2d`` is True. Setting to 0 disables
         this check.

-    y_numeric : boolean (default=False)
+    y_numeric : bool, default=False
         Whether to ensure that y has a numeric type. If dtype of y is object,
         it is converted to float64. Should only be used for regression
         algorithms.

-    warn_on_dtype : boolean or None, optional (default=None)
-        Raise DataConversionWarning if the dtype of the input data structure
-        does not match the requested dtype, causing a memory copy.
-
-        .. deprecated:: 0.21
-            ``warn_on_dtype`` is deprecated in version 0.21 and will be
-             removed in 0.23.
-
-    estimator : str or estimator instance (default=None)
+    estimator : str or estimator instance, default=None
         If passed, include the name of the estimator in warning messages.

     Returns
@@ -706,83 +1059,130 @@
         The converted and validated y.
     """
     if y is None:
-        raise ValueError("y cannot be None")
-
-    X = check_array(X, accept_sparse=accept_sparse,
-                    accept_large_sparse=accept_large_sparse,
-                    dtype=dtype, order=order, copy=copy,
-                    force_all_finite=force_all_finite,
-                    ensure_2d=ensure_2d, allow_nd=allow_nd,
-                    ensure_min_samples=ensure_min_samples,
-                    ensure_min_features=ensure_min_features,
-                    warn_on_dtype=warn_on_dtype,
-                    estimator=estimator)
+        if estimator is None:
+            estimator_name = "estimator"
+        else:
+            estimator_name = _check_estimator_name(estimator)
+        raise ValueError(
+            f"{estimator_name} requires y to be passed, but the target y is None"
+        )
+
+    X = check_array(
+        X,
+        accept_sparse=accept_sparse,
+        accept_large_sparse=accept_large_sparse,
+        dtype=dtype,
+        order=order,
+        copy=copy,
+        force_all_finite=force_all_finite,
+        ensure_2d=ensure_2d,
+        allow_nd=allow_nd,
+        ensure_min_samples=ensure_min_samples,
+        ensure_min_features=ensure_min_features,
+        estimator=estimator,
+        input_name="X",
+    )
+
+    y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
+
+    check_consistent_length(X, y)
+
+    return X, y
+
+
+def _check_y(y, multi_output=False, y_numeric=False, estimator=None):
+    """Isolated part of check_X_y dedicated to y validation"""
     if multi_output:
-        y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,
-                        dtype=None)
+        y = check_array(
+            y,
+            accept_sparse="csr",
+            force_all_finite=True,
+            ensure_2d=False,
+            dtype=None,
+            input_name="y",
+            estimator=estimator,
+        )
     else:
+        estimator_name = _check_estimator_name(estimator)
         y = column_or_1d(y, warn=True)
-        _assert_all_finite(y)
-    if y_numeric and y.dtype.kind == 'O':
+        _assert_all_finite(y, input_name="y", estimator_name=estimator_name)
+        _ensure_no_complex_data(y)
+    if y_numeric and y.dtype.kind == "O":
         y = y.astype(np.float64)

-    check_consistent_length(X, y)
-
-    return X, y
-
-
-def column_or_1d(y, warn=False):
-    """ Ravel column or 1d numpy array, else raises an error
+    return y
+
+
+def column_or_1d(y, *, warn=False):
+    """Ravel column or 1d numpy array, else raises an error.

     Parameters
     ----------
     y : array-like
-
-    warn : boolean, default False
+       Input data.
+
+    warn : bool, default=False
        To control display of warnings.

     Returns
     -------
-    y : array
-
-    """
+    y : ndarray
+       Output data.
+
+    Raises
+    ------
+    ValueError
+        If `y` is not a 1D array or a 2D array with a single row or column.
+    """
+    y = np.asarray(y)
     shape = np.shape(y)
     if len(shape) == 1:
         return np.ravel(y)
     if len(shape) == 2 and shape[1] == 1:
         if warn:
-            warnings.warn("A column-vector y was passed when a 1d array was"
-                          " expected. Please change the shape of y to "
-                          "(n_samples, ), for example using ravel().",
-                          DataConversionWarning, stacklevel=2)
+            warnings.warn(
+                "A column-vector y was passed when a 1d array was"
+                " expected. Please change the shape of y to "
+                "(n_samples, ), for example using ravel().",
+                DataConversionWarning,
+                stacklevel=2,
+            )
         return np.ravel(y)

-    raise ValueError("bad input shape {0}".format(shape))
+    raise ValueError(
+        "y should be a 1d array, got an array of shape {} instead.".format(shape)
+    )


 def check_random_state(seed):
-    """Turn seed into a np.random.RandomState instance
-
-    Parameters
-    ----------
-    seed : None | int | instance of RandomState
+    """Turn seed into a np.random.RandomState instance.
+
+    Parameters
+    ----------
+    seed : None, int or instance of RandomState
         If seed is None, return the RandomState singleton used by np.random.
         If seed is an int, return a new RandomState instance seeded with seed.
         If seed is already a RandomState instance, return it.
         Otherwise raise ValueError.
+
+    Returns
+    -------
+    None
+        No returns.
     """
     if seed is None or seed is np.random:
         return np.random.mtrand._rand
-    if isinstance(seed, (numbers.Integral, np.integer)):
+    if isinstance(seed, numbers.Integral):
         return np.random.RandomState(seed)
     if isinstance(seed, np.random.RandomState):
         return seed
-    raise ValueError('%r cannot be used to seed a numpy.random.RandomState'
-                     ' instance' % seed)
+    raise ValueError(
+        "%r cannot be used to seed a numpy.random.RandomState instance" % seed
+    )


 def has_fit_parameter(estimator, parameter):
-    """Checks whether the estimator's fit method supports the given parameter.
+    """Check whether the estimator's fit method supports the given parameter.

     Parameters
     ----------
@@ -794,22 +1194,21 @@

     Returns
     -------
-    is_parameter: bool
+    is_parameter : bool
         Whether the parameter was found to be a named parameter of the
         estimator's fit method.

     Examples
     --------
     >>> from sklearn.svm import SVC
+    >>> from sklearn.utils.validation import has_fit_parameter
     >>> has_fit_parameter(SVC(), "sample_weight")
     True
-
     """
     return parameter in signature(estimator.fit).parameters


-def check_symmetric(array, tol=1E-10, raise_warning=True,
-                    raise_exception=False):
+def check_symmetric(array, *, tol=1e-10, raise_warning=True, raise_exception=False):
     """Make sure that array is 2D, square and symmetric.

     If the array is not symmetric, then a symmetrized version is returned.
@@ -818,31 +1217,35 @@

     Parameters
     ----------
-    array : nd-array or sparse matrix
+    array : {ndarray, sparse matrix}
         Input object to check / convert. Must be two-dimensional and square,
         otherwise a ValueError will be raised.
-    tol : float
+
+    tol : float, default=1e-10
         Absolute tolerance for equivalence of arrays. Default = 1E-10.
-    raise_warning : boolean (default=True)
+
+    raise_warning : bool, default=True
         If True then raise a warning if conversion is required.
-    raise_exception : boolean (default=False)
+
+    raise_exception : bool, default=False
         If True then raise an exception if array is not symmetric.

     Returns
     -------
-    array_sym : ndarray or sparse matrix
+    array_sym : {ndarray, sparse matrix}
         Symmetrized version of the input array, i.e. the average of array
         and array.transpose(). If sparse, then duplicate entries are first
         summed and zeros are eliminated.
     """
     if (array.ndim != 2) or (array.shape[0] != array.shape[1]):
-        raise ValueError("array must be 2-dimensional and square. "
-                         "shape = {0}".format(array.shape))
+        raise ValueError(
+            "array must be 2-dimensional and square. shape = {0}".format(array.shape)
+        )

     if sp.issparse(array):
         diff = array - array.T
         # only csr, csc, and coo have `data` attribute
-        if diff.format not in ['csr', 'csc', 'coo']:
+        if diff.format not in ["csr", "csc", "coo"]:
             diff = diff.tocsr()
         symmetric = np.all(abs(diff.data) < tol)
     else:
@@ -852,10 +1255,13 @@
         if raise_exception:
             raise ValueError("Array must be symmetric")
         if raise_warning:
-            warnings.warn("Array is not symmetric, and will be converted "
-                          "to symmetric by average with its transpose.")
+            warnings.warn(
+                "Array is not symmetric, and will be converted "
+                "to symmetric by average with its transpose.",
+                stacklevel=2,
+            )
         if sp.issparse(array):
-            conversion = 'to' + array.format
+            conversion = "to" + array.format
             array = getattr(0.5 * (array + array.T), conversion)()
         else:
             array = 0.5 * (array + array.T)
@@ -863,32 +1269,41 @@
     return array


-def check_is_fitted(estimator, attributes, msg=None, all_or_any=all):
+def check_is_fitted(estimator, attributes=None, *, msg=None, all_or_any=all):
     """Perform is_fitted validation for estimator.

     Checks if the estimator is fitted by verifying the presence of
-    "all_or_any" of the passed attributes and raises a NotFittedError with the
-    given message.
-
-    Parameters
-    ----------
-    estimator : estimator instance.
+    fitted attributes (ending with a trailing underscore) and otherwise
+    raises a NotFittedError with the given message.
+
+    If an estimator does not set any attributes with a trailing underscore, it
+    can define a ``__sklearn_is_fitted__`` method returning a boolean to specify if the
+    estimator is fitted or not.
+
+    Parameters
+    ----------
+    estimator : estimator instance
         estimator instance for which the check is performed.

-    attributes : attribute name(s) given as string or a list/tuple of strings
-        Eg.:
-            ``["coef_", "estimator_", ...], "coef_"``
-
-    msg : string
+    attributes : str, list or tuple of str, default=None
+        Attribute name(s) given as string or a list/tuple of strings
+        Eg.: ``["coef_", "estimator_", ...], "coef_"``
+
+        If `None`, `estimator` is considered fitted if there exist an
+        attribute that ends with a underscore and does not start with double
+        underscore.
+
+    msg : str, default=None
         The default error message is, "This %(name)s instance is not fitted
-        yet. Call 'fit' with appropriate arguments before using this method."
+        yet. Call 'fit' with appropriate arguments before using this
+        estimator."

         For custom messages if "%(name)s" is present in the message string,
         it is substituted for the estimator name.

         Eg. : "Estimator, %(name)s, must be fitted before sparsifying".

-    all_or_any : callable, {all, any}, default all
+    all_or_any : callable, {all, any}, default=all
         Specify whether all or any of the given attributes must exist.

     Returns
@@ -900,18 +1315,30 @@
     NotFittedError
         If the attributes are not found.
     """
+    if isclass(estimator):
+        raise TypeError("{} is a class, not an instance.".format(estimator))
     if msg is None:
-        msg = ("This %(name)s instance is not fitted yet. Call 'fit' with "
-               "appropriate arguments before using this method.")
-
-    if not hasattr(estimator, 'fit'):
+        msg = (
+            "This %(name)s instance is not fitted yet. Call 'fit' with "
+            "appropriate arguments before using this estimator."
+        )
+
+    if not hasattr(estimator, "fit"):
         raise TypeError("%s is not an estimator instance." % (estimator))

-    if not isinstance(attributes, (list, tuple)):
-        attributes = [attributes]
-
-    if not all_or_any([hasattr(estimator, attr) for attr in attributes]):
-        raise NotFittedError(msg % {'name': type(estimator).__name__})
+    if attributes is not None:
+        if not isinstance(attributes, (list, tuple)):
+            attributes = [attributes]
+        fitted = all_or_any([hasattr(estimator, attr) for attr in attributes])
+    elif hasattr(estimator, "__sklearn_is_fitted__"):
+        fitted = estimator.__sklearn_is_fitted__()
+    else:
+        fitted = [
+            v for v in vars(estimator) if v.endswith("_") and not v.startswith("__")
+        ]
+
+    if not fitted:
+        raise NotFittedError(msg % {"name": type(estimator).__name__})


 def check_non_negative(X, whom):
@@ -920,15 +1347,15 @@

     Parameters
     ----------
-    X : array-like or sparse matrix
+    X : {array-like, sparse matrix}
         Input data.

-    whom : string
+    whom : str
         Who passed X to this function.
     """
     # avoid X.min() on sparse matrix since it also sorts the indices
     if sp.issparse(X):
-        if X.format in ['lil', 'dok']:
+        if X.format in ["lil", "dok"]:
             X = X.tocsr()
         if X.data.size == 0:
             X_min = 0
@@ -941,7 +1368,15 @@
         raise ValueError("Negative values in data passed to %s" % whom)


-def check_scalar(x, name, target_type, min_val=None, max_val=None):
+def check_scalar(
+    x,
+    name,
+    target_type,
+    *,
+    min_val=None,
+    max_val=None,
+    include_boundaries="both",
+):
     """Validate scalar parameters type and value.

     Parameters
@@ -955,29 +1390,563 @@
     target_type : type or tuple
         Acceptable data types for the parameter.

-    min_val : float or int, optional (default=None)
+    min_val : float or int, default=None
         The minimum valid value the parameter can take. If None (default) it
         is implied that the parameter does not have a lower bound.

-    max_val : float or int, optional (default=None)
+    max_val : float or int, default=None
         The maximum valid value the parameter can take. If None (default) it
         is implied that the parameter does not have an upper bound.

+    include_boundaries : {"left", "right", "both", "neither"}, default="both"
+        Whether the interval defined by `min_val` and `max_val` should include
+        the boundaries. Possible choices are:
+
+        - `"left"`: only `min_val` is included in the valid interval.
+          It is equivalent to the interval `[ min_val, max_val )`.
+        - `"right"`: only `max_val` is included in the valid interval.
+          It is equivalent to the interval `( min_val, max_val ]`.
+        - `"both"`: `min_val` and `max_val` are included in the valid interval.
+          It is equivalent to the interval `[ min_val, max_val ]`.
+        - `"neither"`: neither `min_val` nor `max_val` are included in the
+          valid interval. It is equivalent to the interval `( min_val, max_val )`.
+
+    Returns
+    -------
+    x : numbers.Number
+        The validated number.
+
     Raises
-    -------
+    ------
     TypeError
         If the parameter's type does not match the desired type.

     ValueError
         If the parameter's value violates the given bounds.
-    """
+        If `min_val`, `max_val` and `include_boundaries` are inconsistent.
+    """
+
+    def type_name(t):
+        """Convert type into humman readable string."""
+        module = t.__module__
+        qualname = t.__qualname__
+        if module == "builtins":
+            return qualname
+        elif t == numbers.Real:
+            return "float"
+        elif t == numbers.Integral:
+            return "int"
+        return f"{module}.{qualname}"

     if not isinstance(x, target_type):
-        raise TypeError('`{}` must be an instance of {}, not {}.'
-                        .format(name, target_type, type(x)))
-
-    if min_val is not None and x < min_val:
-        raise ValueError('`{}`= {}, must be >= {}.'.format(name, x, min_val))
-
-    if max_val is not None and x > max_val:
-        raise ValueError('`{}`= {}, must be <= {}.'.format(name, x, max_val))
+        if isinstance(target_type, tuple):
+            types_str = ", ".join(type_name(t) for t in target_type)
+            target_type_str = f"{{{types_str}}}"
+        else:
+            target_type_str = type_name(target_type)
+
+        raise TypeError(
+            f"{name} must be an instance of {target_type_str}, not"
+            f" {type(x).__qualname__}."
+        )
+
+    expected_include_boundaries = ("left", "right", "both", "neither")
+    if include_boundaries not in expected_include_boundaries:
+        raise ValueError(
+            f"Unknown value for `include_boundaries`: {repr(include_boundaries)}. "
+            f"Possible values are: {expected_include_boundaries}."
+        )
+
+    if max_val is None and include_boundaries == "right":
+        raise ValueError(
+            "`include_boundaries`='right' without specifying explicitly `max_val` "
+            "is inconsistent."
+        )
+
+    if min_val is None and include_boundaries == "left":
+        raise ValueError(
+            "`include_boundaries`='left' without specifying explicitly `min_val` "
+            "is inconsistent."
+        )
+
+    comparison_operator = (
+        operator.lt if include_boundaries in ("left", "both") else operator.le
+    )
+    if min_val is not None and comparison_operator(x, min_val):
+        raise ValueError(
+            f"{name} == {x}, must be"
+            f" {'>=' if include_boundaries in ('left', 'both') else '>'} {min_val}."
+        )
+
+    comparison_operator = (
+        operator.gt if include_boundaries in ("right", "both") else operator.ge
+    )
+    if max_val is not None and comparison_operator(x, max_val):
+        raise ValueError(
+            f"{name} == {x}, must be"
+            f" {'<=' if include_boundaries in ('right', 'both') else '<'} {max_val}."
+        )
+
+    return x
+
+
+def _check_psd_eigenvalues(lambdas, enable_warnings=False):
+    """Check the eigenvalues of a positive semidefinite (PSD) matrix.
+
+    Checks the provided array of PSD matrix eigenvalues for numerical or
+    conditioning issues and returns a fixed validated version. This method
+    should typically be used if the PSD matrix is user-provided (e.g. a
+    Gram matrix) or computed using a user-provided dissimilarity metric
+    (e.g. kernel function), or if the decomposition process uses approximation
+    methods (randomized SVD, etc.).
+
+    It checks for three things:
+
+    - that there are no significant imaginary parts in eigenvalues (more than
+      1e-5 times the maximum real part). If this check fails, it raises a
+      ``ValueError``. Otherwise all non-significant imaginary parts that may
+      remain are set to zero. This operation is traced with a
+      ``PositiveSpectrumWarning`` when ``enable_warnings=True``.
+
+    - that eigenvalues are not all negative. If this check fails, it raises a
+      ``ValueError``
+
+    - that there are no significant negative eigenvalues with absolute value
+      more than 1e-10 (1e-6) and more than 1e-5 (5e-3) times the largest
+      positive eigenvalue in double (simple) precision. If this check fails,
+      it raises a ``ValueError``. Otherwise all negative eigenvalues that may
+      remain are set to zero. This operation is traced with a
+      ``PositiveSpectrumWarning`` when ``enable_warnings=True``.
+
+    Finally, all the positive eigenvalues that are too small (with a value
+    smaller than the maximum eigenvalue multiplied by 1e-12 (2e-7)) are set to
+    zero. This operation is traced with a ``PositiveSpectrumWarning`` when
+    ``enable_warnings=True``.
+
+    Parameters
+    ----------
+    lambdas : array-like of shape (n_eigenvalues,)
+        Array of eigenvalues to check / fix.
+
+    enable_warnings : bool, default=False
+        When this is set to ``True``, a ``PositiveSpectrumWarning`` will be
+        raised when there are imaginary parts, negative eigenvalues, or
+        extremely small non-zero eigenvalues. Otherwise no warning will be
+        raised. In both cases, imaginary parts, negative eigenvalues, and
+        extremely small non-zero eigenvalues will be set to zero.
+
+    Returns
+    -------
+    lambdas_fixed : ndarray of shape (n_eigenvalues,)
+        A fixed validated copy of the array of eigenvalues.
+
+    Examples
+    --------
+    >>> from sklearn.utils.validation import _check_psd_eigenvalues
+    >>> _check_psd_eigenvalues([1, 2])      # nominal case
+    array([1, 2])
+    >>> _check_psd_eigenvalues([5, 5j])     # significant imag part
+    Traceback (most recent call last):
+        ...
+    ValueError: There are significant imaginary parts in eigenvalues (1
+        of the maximum real part). Either the matrix is not PSD, or there was
+        an issue while computing the eigendecomposition of the matrix.
+    >>> _check_psd_eigenvalues([5, 5e-5j])  # insignificant imag part
+    array([5., 0.])
+    >>> _check_psd_eigenvalues([-5, -1])    # all negative
+    Traceback (most recent call last):
+        ...
+    ValueError: All eigenvalues are negative (maximum is -1). Either the
+        matrix is not PSD, or there was an issue while computing the
+        eigendecomposition of the matrix.
+    >>> _check_psd_eigenvalues([5, -1])     # significant negative
+    Traceback (most recent call last):
+        ...
+    ValueError: There are significant negative eigenvalues (0.2 of the
+        maximum positive). Either the matrix is not PSD, or there was an issue
+        while computing the eigendecomposition of the matrix.
+    >>> _check_psd_eigenvalues([5, -5e-5])  # insignificant negative
+    array([5., 0.])
+    >>> _check_psd_eigenvalues([5, 4e-12])  # bad conditioning (too small)
+    array([5., 0.])
+
+    """
+
+    lambdas = np.array(lambdas)
+    is_double_precision = lambdas.dtype == np.float64
+
+    # note: the minimum value available is
+    #  - single-precision: np.finfo('float32').eps = 1.2e-07
+    #  - double-precision: np.finfo('float64').eps = 2.2e-16
+
+    # the various thresholds used for validation
+    # we may wish to change the value according to precision.
+    significant_imag_ratio = 1e-5
+    significant_neg_ratio = 1e-5 if is_double_precision else 5e-3
+    significant_neg_value = 1e-10 if is_double_precision else 1e-6
+    small_pos_ratio = 1e-12 if is_double_precision else 2e-7
+
+    # Check that there are no significant imaginary parts
+    if not np.isreal(lambdas).all():
+        max_imag_abs = np.abs(np.imag(lambdas)).max()
+        max_real_abs = np.abs(np.real(lambdas)).max()
+        if max_imag_abs > significant_imag_ratio * max_real_abs:
+            raise ValueError(
+                "There are significant imaginary parts in eigenvalues (%g "
+                "of the maximum real part). Either the matrix is not PSD, or "
+                "there was an issue while computing the eigendecomposition "
+                "of the matrix." % (max_imag_abs / max_real_abs)
+            )
+
+        # warn about imaginary parts being removed
+        if enable_warnings:
+            warnings.warn(
+                "There are imaginary parts in eigenvalues (%g "
+                "of the maximum real part). Either the matrix is not"
+                " PSD, or there was an issue while computing the "
+                "eigendecomposition of the matrix. Only the real "
+                "parts will be kept." % (max_imag_abs / max_real_abs),
+                PositiveSpectrumWarning,
+            )
+
+    # Remove all imaginary parts (even if zero)
+    lambdas = np.real(lambdas)
+
+    # Check that there are no significant negative eigenvalues
+    max_eig = lambdas.max()
+    if max_eig < 0:
+        raise ValueError(
+            "All eigenvalues are negative (maximum is %g). "
+            "Either the matrix is not PSD, or there was an "
+            "issue while computing the eigendecomposition of "
+            "the matrix." % max_eig
+        )
+
+    else:
+        min_eig = lambdas.min()
+        if (
+            min_eig < -significant_neg_ratio * max_eig
+            and min_eig < -significant_neg_value
+        ):
+            raise ValueError(
+                "There are significant negative eigenvalues (%g"
+                " of the maximum positive). Either the matrix is "
+                "not PSD, or there was an issue while computing "
+                "the eigendecomposition of the matrix." % (-min_eig / max_eig)
+            )
+        elif min_eig < 0:
+            # Remove all negative values and warn about it
+            if enable_warnings:
+                warnings.warn(
+                    "There are negative eigenvalues (%g of the "
+                    "maximum positive). Either the matrix is not "
+                    "PSD, or there was an issue while computing the"
+                    " eigendecomposition of the matrix. Negative "
+                    "eigenvalues will be replaced with 0." % (-min_eig / max_eig),
+                    PositiveSpectrumWarning,
+                )
+            lambdas[lambdas < 0] = 0
+
+    # Check for conditioning (small positive non-zeros)
+    too_small_lambdas = (0 < lambdas) & (lambdas < small_pos_ratio * max_eig)
+    if too_small_lambdas.any():
+        if enable_warnings:
+            warnings.warn(
+                "Badly conditioned PSD matrix spectrum: the largest "
+                "eigenvalue is more than %g times the smallest. "
+                "Small eigenvalues will be replaced with 0."
+                "" % (1 / small_pos_ratio),
+                PositiveSpectrumWarning,
+            )
+        lambdas[too_small_lambdas] = 0
+
+    return lambdas
+
+
+def _check_sample_weight(
+    sample_weight, X, dtype=None, copy=False, only_non_negative=False
+):
+    """Validate sample weights.
+
+    Note that passing sample_weight=None will output an array of ones.
+    Therefore, in some cases, you may want to protect the call with:
+    if sample_weight is not None:
+        sample_weight = _check_sample_weight(...)
+
+    Parameters
+    ----------
+    sample_weight : {ndarray, Number or None}, shape (n_samples,)
+        Input sample weights.
+
+    X : {ndarray, list, sparse matrix}
+        Input data.
+
+    only_non_negative : bool, default=False,
+        Whether or not the weights are expected to be non-negative.
+
+        .. versionadded:: 1.0
+
+    dtype : dtype, default=None
+        dtype of the validated `sample_weight`.
+        If None, and the input `sample_weight` is an array, the dtype of the
+        input is preserved; otherwise an array with the default numpy dtype
+        is be allocated.  If `dtype` is not one of `float32`, `float64`,
+        `None`, the output will be of dtype `float64`.
+
+    copy : bool, default=False
+        If True, a copy of sample_weight will be created.
+
+    Returns
+    -------
+    sample_weight : ndarray of shape (n_samples,)
+        Validated sample weight. It is guaranteed to be "C" contiguous.
+    """
+    n_samples = _num_samples(X)
+
+    if dtype is not None and dtype not in [np.float32, np.float64]:
+        dtype = np.float64
+
+    if sample_weight is None:
+        sample_weight = np.ones(n_samples, dtype=dtype)
+    elif isinstance(sample_weight, numbers.Number):
+        sample_weight = np.full(n_samples, sample_weight, dtype=dtype)
+    else:
+        if dtype is None:
+            dtype = [np.float64, np.float32]
+        sample_weight = check_array(
+            sample_weight,
+            accept_sparse=False,
+            ensure_2d=False,
+            dtype=dtype,
+            order="C",
+            copy=copy,
+            input_name="sample_weight",
+        )
+        if sample_weight.ndim != 1:
+            raise ValueError("Sample weights must be 1D array or scalar")
+
+        if sample_weight.shape != (n_samples,):
+            raise ValueError(
+                "sample_weight.shape == {}, expected {}!".format(
+                    sample_weight.shape, (n_samples,)
+                )
+            )
+
+    if only_non_negative:
+        check_non_negative(sample_weight, "`sample_weight`")
+
+    return sample_weight
+
+
+def _allclose_dense_sparse(x, y, rtol=1e-7, atol=1e-9):
+    """Check allclose for sparse and dense data.
+
+    Both x and y need to be either sparse or dense, they
+    can't be mixed.
+
+    Parameters
+    ----------
+    x : {array-like, sparse matrix}
+        First array to compare.
+
+    y : {array-like, sparse matrix}
+        Second array to compare.
+
+    rtol : float, default=1e-7
+        Relative tolerance; see numpy.allclose.
+
+    atol : float, default=1e-9
+        absolute tolerance; see numpy.allclose. Note that the default here is
+        more tolerant than the default for numpy.testing.assert_allclose, where
+        atol=0.
+    """
+    if sp.issparse(x) and sp.issparse(y):
+        x = x.tocsr()
+        y = y.tocsr()
+        x.sum_duplicates()
+        y.sum_duplicates()
+        return (
+            np.array_equal(x.indices, y.indices)
+            and np.array_equal(x.indptr, y.indptr)
+            and np.allclose(x.data, y.data, rtol=rtol, atol=atol)
+        )
+    elif not sp.issparse(x) and not sp.issparse(y):
+        return np.allclose(x, y, rtol=rtol, atol=atol)
+    raise ValueError(
+        "Can only compare two sparse matrices, not a sparse matrix and an array"
+    )
+
+
+def _check_fit_params(X, fit_params, indices=None):
+    """Check and validate the parameters passed during `fit`.
+
+    Parameters
+    ----------
+    X : array-like of shape (n_samples, n_features)
+        Data array.
+
+    fit_params : dict
+        Dictionary containing the parameters passed at fit.
+
+    indices : array-like of shape (n_samples,), default=None
+        Indices to be selected if the parameter has the same size as `X`.
+
+    Returns
+    -------
+    fit_params_validated : dict
+        Validated parameters. We ensure that the values support indexing.
+    """
+    from . import _safe_indexing
+
+    fit_params_validated = {}
+    for param_key, param_value in fit_params.items():
+        if not _is_arraylike(param_value) or _num_samples(param_value) != _num_samples(
+            X
+        ):
+            # Non-indexable pass-through (for now for backward-compatibility).
+            # https://github.com/scikit-learn/scikit-learn/issues/15805
+            fit_params_validated[param_key] = param_value
+        else:
+            # Any other fit_params should support indexing
+            # (e.g. for cross-validation).
+            fit_params_validated[param_key] = _make_indexable(param_value)
+            fit_params_validated[param_key] = _safe_indexing(
+                fit_params_validated[param_key], indices
+            )
+
+    return fit_params_validated
+
+
+def _get_feature_names(X):
+    """Get feature names from X.
+
+    Support for other array containers should place its implementation here.
+
+    Parameters
+    ----------
+    X : {ndarray, dataframe} of shape (n_samples, n_features)
+        Array container to extract feature names.
+
+        - pandas dataframe : The columns will be considered to be feature
+          names. If the dataframe contains non-string feature names, `None` is
+          returned.
+        - All other array containers will return `None`.
+
+    Returns
+    -------
+    names: ndarray or None
+        Feature names of `X`. Unrecognized array containers will return `None`.
+    """
+    feature_names = None
+
+    # extract feature names for support array containers
+    if hasattr(X, "columns"):
+        feature_names = np.asarray(X.columns, dtype=object)
+
+    if feature_names is None or len(feature_names) == 0:
+        return
+
+    types = sorted(t.__qualname__ for t in set(type(v) for v in feature_names))
+
+    # Warn when types are mixed and string is one of the types
+    if len(types) > 1 and "str" in types:
+        # TODO: Convert to an error in 1.2
+        warnings.warn(
+            "Feature names only support names that are all strings. "
+            f"Got feature names with dtypes: {types}. An error will be raised "
+            "in 1.2.",
+            FutureWarning,
+        )
+        return
+
+    # Only feature names of all strings are supported
+    if len(types) == 1 and types[0] == "str":
+        return feature_names
+
+
+def _check_feature_names_in(estimator, input_features=None, *, generate_names=True):
+    """Check `input_features` and generate names if needed.
+
+    Commonly used in :term:`get_feature_names_out`.
+
+    Parameters
+    ----------
+    input_features : array-like of str or None, default=None
+        Input features.
+
+        - If `input_features` is `None`, then `feature_names_in_` is
+          used as feature names in. If `feature_names_in_` is not defined,
+          then the following input feature names are generated:
+          `["x0", "x1", ..., "x(n_features_in_ - 1)"]`.
+        - If `input_features` is an array-like, then `input_features` must
+          match `feature_names_in_` if `feature_names_in_` is defined.
+
+    generate_names : bool, default=True
+        Whether to generate names when `input_features` is `None` and
+        `estimator.feature_names_in_` is not defined. This is useful for transformers
+        that validates `input_features` but do not require them in
+        :term:`get_feature_names_out` e.g. `PCA`.
+
+    Returns
+    -------
+    feature_names_in : ndarray of str or `None`
+        Feature names in.
+    """
+
+    feature_names_in_ = getattr(estimator, "feature_names_in_", None)
+    n_features_in_ = getattr(estimator, "n_features_in_", None)
+
+    if input_features is not None:
+        input_features = np.asarray(input_features, dtype=object)
+        if feature_names_in_ is not None and not np.array_equal(
+            feature_names_in_, input_features
+        ):
+            raise ValueError("input_features is not equal to feature_names_in_")
+
+        if n_features_in_ is not None and len(input_features) != n_features_in_:
+            raise ValueError(
+                "input_features should have length equal to number of "
+                f"features ({n_features_in_}), got {len(input_features)}"
+            )
+        return input_features
+
+    if feature_names_in_ is not None:
+        return feature_names_in_
+
+    if not generate_names:
+        return
+
+    # Generates feature names if `n_features_in_` is defined
+    if n_features_in_ is None:
+        raise ValueError("Unable to generate feature names without n_features_in_")
+
+    return np.asarray([f"x{i}" for i in range(n_features_in_)], dtype=object)
+
+
+def _generate_get_feature_names_out(estimator, n_features_out, input_features=None):
+    """Generate feature names out for estimator using the estimator name as the prefix.
+
+    The input_feature names are validated but not used. This function is useful
+    for estimators that generate their own names based on `n_features_out`, i.e. PCA.
+
+    Parameters
+    ----------
+    estimator : estimator instance
+        Estimator producing output feature names.
+
+    n_feature_out : int
+        Number of feature names out.
+
+    input_features : array-like of str or None, default=None
+        Only used to validate feature names with `estimator.feature_names_in_`.
+
+    Returns
+    -------
+    feature_names_in : ndarray of str or `None`
+        Feature names in.
+    """
+    _check_feature_names_in(estimator, input_features, generate_names=False)
+    estimator_name = estimator.__class__.__name__.lower()
+    return np.asarray(
+        [f"{estimator_name}{i}" for i in range(n_features_out)], dtype=object
+    )
('sklearn/covariance', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,33 +6,40 @@
 Models.
 """

-from .empirical_covariance_ import empirical_covariance, EmpiricalCovariance, \
-    log_likelihood
-from .shrunk_covariance_ import shrunk_covariance, ShrunkCovariance, \
-    ledoit_wolf, ledoit_wolf_shrinkage, \
-    LedoitWolf, oas, OAS
-from .robust_covariance import fast_mcd, MinCovDet
-from .graph_lasso_ import graph_lasso, GraphLasso, GraphLassoCV,\
-    graphical_lasso, GraphicalLasso, GraphicalLassoCV
-from .elliptic_envelope import EllipticEnvelope
+from ._empirical_covariance import (
+    empirical_covariance,
+    EmpiricalCovariance,
+    log_likelihood,
+)
+from ._shrunk_covariance import (
+    shrunk_covariance,
+    ShrunkCovariance,
+    ledoit_wolf,
+    ledoit_wolf_shrinkage,
+    LedoitWolf,
+    oas,
+    OAS,
+)
+from ._robust_covariance import fast_mcd, MinCovDet
+from ._graph_lasso import graphical_lasso, GraphicalLasso, GraphicalLassoCV
+from ._elliptic_envelope import EllipticEnvelope


-__all__ = ['EllipticEnvelope',
-           'EmpiricalCovariance',
-           'GraphLasso',
-           'GraphLassoCV',
-           'GraphicalLasso',
-           'GraphicalLassoCV',
-           'LedoitWolf',
-           'MinCovDet',
-           'OAS',
-           'ShrunkCovariance',
-           'empirical_covariance',
-           'fast_mcd',
-           'graph_lasso',
-           'graphical_lasso',
-           'ledoit_wolf',
-           'ledoit_wolf_shrinkage',
-           'log_likelihood',
-           'oas',
-           'shrunk_covariance']
+__all__ = [
+    "EllipticEnvelope",
+    "EmpiricalCovariance",
+    "GraphicalLasso",
+    "GraphicalLassoCV",
+    "LedoitWolf",
+    "MinCovDet",
+    "OAS",
+    "ShrunkCovariance",
+    "empirical_covariance",
+    "fast_mcd",
+    "graphical_lasso",
+    "ledoit_wolf",
+    "ledoit_wolf_shrinkage",
+    "log_likelihood",
+    "oas",
+    "shrunk_covariance",
+]
('sklearn/neural_network', '_base.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -10,94 +10,71 @@
 from scipy.special import xlogy


-def identity(X):
-    """Simply return the input array.
-
-    Parameters
-    ----------
-    X : {array-like, sparse matrix}, shape (n_samples, n_features)
-        Data, where n_samples is the number of samples
-        and n_features is the number of features.
-
-    Returns
-    -------
-    X : {array-like, sparse matrix}, shape (n_samples, n_features)
-        Same as the input data.
-    """
-    return X
-
-
-def logistic(X):
+def inplace_identity(X):
+    """Simply leave the input array unchanged.
+
+    Parameters
+    ----------
+    X : {array-like, sparse matrix}, shape (n_samples, n_features)
+        Data, where `n_samples` is the number of samples
+        and `n_features` is the number of features.
+    """
+    # Nothing to do
+
+
+def inplace_logistic(X):
     """Compute the logistic function inplace.

     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         The input data.
-
-    Returns
-    -------
-    X_new : {array-like, sparse matrix}, shape (n_samples, n_features)
-        The transformed data.
-    """
-    return logistic_sigmoid(X, out=X)
-
-
-def tanh(X):
+    """
+    logistic_sigmoid(X, out=X)
+
+
+def inplace_tanh(X):
     """Compute the hyperbolic tan function inplace.

     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         The input data.
-
-    Returns
-    -------
-    X_new : {array-like, sparse matrix}, shape (n_samples, n_features)
-        The transformed data.
-    """
-    return np.tanh(X, out=X)
-
-
-def relu(X):
+    """
+    np.tanh(X, out=X)
+
+
+def inplace_relu(X):
     """Compute the rectified linear unit function inplace.

     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         The input data.
-
-    Returns
-    -------
-    X_new : {array-like, sparse matrix}, shape (n_samples, n_features)
-        The transformed data.
-    """
-    np.clip(X, 0, np.finfo(X.dtype).max, out=X)
-    return X
-
-
-def softmax(X):
+    """
+    np.maximum(X, 0, out=X)
+
+
+def inplace_softmax(X):
     """Compute the K-way softmax function inplace.

     Parameters
     ----------
     X : {array-like, sparse matrix}, shape (n_samples, n_features)
         The input data.
-
-    Returns
-    -------
-    X_new : {array-like, sparse matrix}, shape (n_samples, n_features)
-        The transformed data.
     """
     tmp = X - X.max(axis=1)[:, np.newaxis]
     np.exp(tmp, out=X)
     X /= X.sum(axis=1)[:, np.newaxis]

-    return X
-
-
-ACTIVATIONS = {'identity': identity, 'tanh': tanh, 'logistic': logistic,
-               'relu': relu, 'softmax': softmax}
+
+ACTIVATIONS = {
+    "identity": inplace_identity,
+    "tanh": inplace_tanh,
+    "logistic": inplace_logistic,
+    "relu": inplace_relu,
+    "softmax": inplace_softmax,
+}


 def inplace_identity_derivative(Z, delta):
@@ -131,7 +108,7 @@
          The backpropagated error signal to be modified inplace.
     """
     delta *= Z
-    delta *= (1 - Z)
+    delta *= 1 - Z


 def inplace_tanh_derivative(Z, delta):
@@ -149,7 +126,7 @@
     delta : {array-like}, shape (n_samples, n_features)
          The backpropagated error signal to be modified inplace.
     """
-    delta *= (1 - Z ** 2)
+    delta *= 1 - Z**2


 def inplace_relu_derivative(Z, delta):
@@ -170,10 +147,12 @@
     delta[Z == 0] = 0


-DERIVATIVES = {'identity': inplace_identity_derivative,
-               'tanh': inplace_tanh_derivative,
-               'logistic': inplace_logistic_derivative,
-               'relu': inplace_relu_derivative}
+DERIVATIVES = {
+    "identity": inplace_identity_derivative,
+    "tanh": inplace_tanh_derivative,
+    "logistic": inplace_logistic_derivative,
+    "relu": inplace_relu_derivative,
+}


 def squared_loss(y_true, y_pred):
@@ -212,13 +191,15 @@
     loss : float
         The degree to which the samples are correctly predicted.
     """
+    eps = np.finfo(y_prob.dtype).eps
+    y_prob = np.clip(y_prob, eps, 1 - eps)
     if y_prob.shape[1] == 1:
         y_prob = np.append(1 - y_prob, y_prob, axis=1)

     if y_true.shape[1] == 1:
         y_true = np.append(1 - y_true, y_true, axis=1)

-    return - xlogy(y_true, y_prob).sum() / y_prob.shape[0]
+    return -xlogy(y_true, y_prob).sum() / y_prob.shape[0]


 def binary_log_loss(y_true, y_prob):
@@ -232,7 +213,7 @@
     y_true : array-like or label indicator matrix
         Ground truth (correct) labels.

-    y_prob : array-like of float, shape = (n_samples, n_classes)
+    y_prob : array-like of float, shape = (n_samples, 1)
         Predicted probabilities, as returned by a classifier's
         predict_proba method.

@@ -241,9 +222,16 @@
     loss : float
         The degree to which the samples are correctly predicted.
     """
-    return -(xlogy(y_true, y_prob) +
-             xlogy(1 - y_true, 1 - y_prob)).sum() / y_prob.shape[0]
-
-
-LOSS_FUNCTIONS = {'squared_loss': squared_loss, 'log_loss': log_loss,
-                  'binary_log_loss': binary_log_loss}
+    eps = np.finfo(y_prob.dtype).eps
+    y_prob = np.clip(y_prob, eps, 1 - eps)
+    return (
+        -(xlogy(y_true, y_prob).sum() + xlogy(1 - y_true, 1 - y_prob).sum())
+        / y_prob.shape[0]
+    )
+
+
+LOSS_FUNCTIONS = {
+    "squared_error": squared_loss,
+    "log_loss": log_loss,
+    "binary_log_loss": binary_log_loss,
+}
('sklearn/neural_network', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,11 +5,9 @@

 # License: BSD 3 clause

-from .rbm import BernoulliRBM
+from ._rbm import BernoulliRBM

-from .multilayer_perceptron import MLPClassifier
-from .multilayer_perceptron import MLPRegressor
+from ._multilayer_perceptron import MLPClassifier
+from ._multilayer_perceptron import MLPRegressor

-__all__ = ["BernoulliRBM",
-           "MLPClassifier",
-           "MLPRegressor"]
+__all__ = ["BernoulliRBM", "MLPClassifier", "MLPRegressor"]
('sklearn/neural_network', '_stochastic_optimizers.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -12,11 +12,7 @@

     Parameters
     ----------
-    params : list, length = len(coefs_) + len(intercepts_)
-        The concatenated list containing coefs_ and intercepts_ in MLP model.
-        Used for initializing velocities and updating params
-
-    learning_rate_init : float, optional, default 0.1
+    learning_rate_init : float, default=0.1
         The initial learning rate used. It controls the step-size in updating
         the weights

@@ -26,22 +22,25 @@
         the current learning rate
     """

-    def __init__(self, params, learning_rate_init=0.1):
-        self.params = [param for param in params]
+    def __init__(self, learning_rate_init=0.1):
         self.learning_rate_init = learning_rate_init
         self.learning_rate = float(learning_rate_init)

-    def update_params(self, grads):
+    def update_params(self, params, grads):
         """Update parameters with given gradients

         Parameters
         ----------
-        grads : list, length = len(params)
+        params : list of length = len(coefs_) + len(intercepts_)
+            The concatenated list containing coefs_ and intercepts_ in MLP
+            model. Used for initializing velocities and updating params
+
+        grads : list of length = len(params)
             Containing gradients with respect to coefs_ and intercepts_ in MLP
             model. So length should be aligned with params
         """
         updates = self._get_updates(grads)
-        for param, update in zip(self.params, updates):
+        for param, update in zip((p for p in params), updates):
             param += update

     def iteration_ends(self, time_step):
@@ -80,11 +79,11 @@
         The concatenated list containing coefs_ and intercepts_ in MLP model.
         Used for initializing velocities and updating params

-    learning_rate_init : float, optional, default 0.1
+    learning_rate_init : float, default=0.1
         The initial learning rate used. It controls the step-size in updating
         the weights

-    lr_schedule : {'constant', 'adaptive', 'invscaling'}, default 'constant'
+    lr_schedule : {'constant', 'adaptive', 'invscaling'}, default='constant'
         Learning rate schedule for weight updates.

         -'constant', is a constant learning rate given by
@@ -100,11 +99,15 @@
          tol, or fail to increase validation score by tol if 'early_stopping'
          is on, the current learning rate is divided by 5.

-    momentum : float, optional, default 0.9
+    momentum : float, default=0.9
         Value of momentum used, must be larger than or equal to 0

-    nesterov : bool, optional, default True
+    nesterov : bool, default=True
         Whether to use nesterov's momentum or not. Use nesterov's if True
+
+    power_t : float, default=0.5
+        Power of time step 't' in inverse scaling. See `lr_schedule` for
+        more details.

     Attributes
     ----------
@@ -115,9 +118,16 @@
         velocities that are used to update params
     """

-    def __init__(self, params, learning_rate_init=0.1, lr_schedule='constant',
-                 momentum=0.9, nesterov=True, power_t=0.5):
-        super().__init__(params, learning_rate_init)
+    def __init__(
+        self,
+        params,
+        learning_rate_init=0.1,
+        lr_schedule="constant",
+        momentum=0.9,
+        nesterov=True,
+        power_t=0.5,
+    ):
+        super().__init__(learning_rate_init)

         self.lr_schedule = lr_schedule
         self.momentum = momentum
@@ -135,12 +145,13 @@
             number of training samples trained on so far, used to update
             learning rate for 'invscaling'
         """
-        if self.lr_schedule == 'invscaling':
-            self.learning_rate = (float(self.learning_rate_init) /
-                                  (time_step + 1) ** self.power_t)
+        if self.lr_schedule == "invscaling":
+            self.learning_rate = (
+                float(self.learning_rate_init) / (time_step + 1) ** self.power_t
+            )

     def trigger_stopping(self, msg, verbose):
-        if self.lr_schedule != 'adaptive':
+        if self.lr_schedule != "adaptive":
             if verbose:
                 print(msg + " Stopping.")
             return True
@@ -150,10 +161,9 @@
                 print(msg + " Learning rate too small. Stopping.")
             return True

-        self.learning_rate /= 5.
+        self.learning_rate /= 5.0
         if verbose:
-            print(msg + " Setting learning rate to %f" %
-                  self.learning_rate)
+            print(msg + " Setting learning rate to %f" % self.learning_rate)
         return False

     def _get_updates(self, grads):
@@ -170,13 +180,17 @@
         updates : list, length = len(grads)
             The values to add to params
         """
-        updates = [self.momentum * velocity - self.learning_rate * grad
-                   for velocity, grad in zip(self.velocities, grads)]
+        updates = [
+            self.momentum * velocity - self.learning_rate * grad
+            for velocity, grad in zip(self.velocities, grads)
+        ]
         self.velocities = updates

         if self.nesterov:
-            updates = [self.momentum * velocity - self.learning_rate * grad
-                       for velocity, grad in zip(self.velocities, grads)]
+            updates = [
+                self.momentum * velocity - self.learning_rate * grad
+                for velocity, grad in zip(self.velocities, grads)
+            ]

         return updates

@@ -192,19 +206,19 @@
         The concatenated list containing coefs_ and intercepts_ in MLP model.
         Used for initializing velocities and updating params

-    learning_rate_init : float, optional, default 0.1
+    learning_rate_init : float, default=0.001
         The initial learning rate used. It controls the step-size in updating
         the weights

-    beta_1 : float, optional, default 0.9
+    beta_1 : float, default=0.9
         Exponential decay rate for estimates of first moment vector, should be
         in [0, 1)

-    beta_2 : float, optional, default 0.999
+    beta_2 : float, default=0.999
         Exponential decay rate for estimates of second moment vector, should be
         in [0, 1)

-    epsilon : float, optional, default 1e-8
+    epsilon : float, default=1e-8
         Value for numerical stability

     Attributes
@@ -223,14 +237,14 @@

     References
     ----------
-    Kingma, Diederik, and Jimmy Ba.
-    "Adam: A method for stochastic optimization."
-    arXiv preprint arXiv:1412.6980 (2014).
+    :arxiv:`Kingma, Diederik, and Jimmy Ba (2014) "Adam: A method for
+        stochastic optimization." <1412.6980>
     """

-    def __init__(self, params, learning_rate_init=0.001, beta_1=0.9,
-                 beta_2=0.999, epsilon=1e-8):
-        super().__init__(params, learning_rate_init)
+    def __init__(
+        self, params, learning_rate_init=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8
+    ):
+        super().__init__(learning_rate_init)

         self.beta_1 = beta_1
         self.beta_2 = beta_2
@@ -254,13 +268,21 @@
             The values to add to params
         """
         self.t += 1
-        self.ms = [self.beta_1 * m + (1 - self.beta_1) * grad
-                   for m, grad in zip(self.ms, grads)]
-        self.vs = [self.beta_2 * v + (1 - self.beta_2) * (grad ** 2)
-                   for v, grad in zip(self.vs, grads)]
-        self.learning_rate = (self.learning_rate_init *
-                              np.sqrt(1 - self.beta_2 ** self.t) /
-                              (1 - self.beta_1 ** self.t))
-        updates = [-self.learning_rate * m / (np.sqrt(v) + self.epsilon)
-                   for m, v in zip(self.ms, self.vs)]
+        self.ms = [
+            self.beta_1 * m + (1 - self.beta_1) * grad
+            for m, grad in zip(self.ms, grads)
+        ]
+        self.vs = [
+            self.beta_2 * v + (1 - self.beta_2) * (grad**2)
+            for v, grad in zip(self.vs, grads)
+        ]
+        self.learning_rate = (
+            self.learning_rate_init
+            * np.sqrt(1 - self.beta_2**self.t)
+            / (1 - self.beta_1**self.t)
+        )
+        updates = [
+            -self.learning_rate * m / (np.sqrt(v) + self.epsilon)
+            for m, v in zip(self.ms, self.vs)
+        ]
         return updates
('sklearn/feature_selection', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,40 +4,50 @@
 recursive feature elimination algorithm.
 """

-from .univariate_selection import chi2
-from .univariate_selection import f_classif
-from .univariate_selection import f_oneway
-from .univariate_selection import f_regression
-from .univariate_selection import SelectPercentile
-from .univariate_selection import SelectKBest
-from .univariate_selection import SelectFpr
-from .univariate_selection import SelectFdr
-from .univariate_selection import SelectFwe
-from .univariate_selection import GenericUnivariateSelect
+from ._univariate_selection import chi2
+from ._univariate_selection import f_classif
+from ._univariate_selection import f_oneway
+from ._univariate_selection import f_regression
+from ._univariate_selection import r_regression
+from ._univariate_selection import SelectPercentile
+from ._univariate_selection import SelectKBest
+from ._univariate_selection import SelectFpr
+from ._univariate_selection import SelectFdr
+from ._univariate_selection import SelectFwe
+from ._univariate_selection import GenericUnivariateSelect

-from .variance_threshold import VarianceThreshold
+from ._variance_threshold import VarianceThreshold

-from .rfe import RFE
-from .rfe import RFECV
+from ._rfe import RFE
+from ._rfe import RFECV

-from .from_model import SelectFromModel
+from ._from_model import SelectFromModel

-from .mutual_info_ import mutual_info_regression, mutual_info_classif
+from ._sequential import SequentialFeatureSelector
+
+from ._mutual_info import mutual_info_regression, mutual_info_classif
+
+from ._base import SelectorMixin


-__all__ = ['GenericUnivariateSelect',
-           'RFE',
-           'RFECV',
-           'SelectFdr',
-           'SelectFpr',
-           'SelectFwe',
-           'SelectKBest',
-           'SelectFromModel',
-           'SelectPercentile',
-           'VarianceThreshold',
-           'chi2',
-           'f_classif',
-           'f_oneway',
-           'f_regression',
-           'mutual_info_classif',
-           'mutual_info_regression']
+__all__ = [
+    "GenericUnivariateSelect",
+    "SequentialFeatureSelector",
+    "RFE",
+    "RFECV",
+    "SelectFdr",
+    "SelectFpr",
+    "SelectFwe",
+    "SelectKBest",
+    "SelectFromModel",
+    "SelectPercentile",
+    "VarianceThreshold",
+    "chi2",
+    "f_classif",
+    "f_oneway",
+    "f_regression",
+    "r_regression",
+    "mutual_info_classif",
+    "mutual_info_regression",
+    "SelectorMixin",
+]
('sklearn/inspection', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,9 +1,18 @@
 """The :mod:`sklearn.inspection` module includes tools for model inspection."""
-from .partial_dependence import partial_dependence
-from .partial_dependence import plot_partial_dependence
+
+
+from ._permutation_importance import permutation_importance
+from ._plot.decision_boundary import DecisionBoundaryDisplay
+
+from ._partial_dependence import partial_dependence
+from ._plot.partial_dependence import plot_partial_dependence
+from ._plot.partial_dependence import PartialDependenceDisplay


 __all__ = [
-    'partial_dependence',
-    'plot_partial_dependence',
+    "partial_dependence",
+    "plot_partial_dependence",
+    "permutation_importance",
+    "PartialDependenceDisplay",
+    "DecisionBoundaryDisplay",
 ]
('sklearn/svm', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -10,19 +10,16 @@
 #         of their respective owners.
 # License: BSD 3 clause (C) INRIA 2010

-from .classes import SVC, NuSVC, SVR, NuSVR, OneClassSVM, LinearSVC, \
-        LinearSVR
-from .bounds import l1_min_c
-from . import libsvm, liblinear, libsvm_sparse
+from ._classes import SVC, NuSVC, SVR, NuSVR, OneClassSVM, LinearSVC, LinearSVR
+from ._bounds import l1_min_c

-__all__ = ['LinearSVC',
-           'LinearSVR',
-           'NuSVC',
-           'NuSVR',
-           'OneClassSVM',
-           'SVC',
-           'SVR',
-           'l1_min_c',
-           'liblinear',
-           'libsvm',
-           'libsvm_sparse']
+__all__ = [
+    "LinearSVC",
+    "LinearSVR",
+    "NuSVC",
+    "NuSVR",
+    "OneClassSVM",
+    "SVC",
+    "SVR",
+    "l1_min_c",
+]
('sklearn/svm', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,75 +3,132 @@
 import numpy


-def configuration(parent_package='', top_path=None):
+def configuration(parent_package="", top_path=None):
     from numpy.distutils.misc_util import Configuration

-    config = Configuration('svm', parent_package, top_path)
+    config = Configuration("svm", parent_package, top_path)

-    config.add_subpackage('tests')
+    config.add_subpackage("tests")
+
+    # newrand wrappers
+    config.add_extension(
+        "_newrand",
+        sources=["_newrand.pyx"],
+        include_dirs=[numpy.get_include(), join("src", "newrand")],
+        depends=[join("src", "newrand", "newrand.h")],
+        language="c++",
+        # Use C++11 random number generator fix
+        extra_compile_args=["-std=c++11"],
+    )

     # Section LibSVM

     # we compile both libsvm and libsvm_sparse
-    config.add_library('libsvm-skl',
-                       sources=[join('src', 'libsvm', 'libsvm_template.cpp')],
-                       depends=[join('src', 'libsvm', 'svm.cpp'),
-                                join('src', 'libsvm', 'svm.h')],
-                       # Force C++ linking in case gcc is picked up instead
-                       # of g++ under windows with some versions of MinGW
-                       extra_link_args=['-lstdc++'],
-                       )
+    config.add_library(
+        "libsvm-skl",
+        sources=[join("src", "libsvm", "libsvm_template.cpp")],
+        depends=[
+            join("src", "libsvm", "svm.cpp"),
+            join("src", "libsvm", "svm.h"),
+            join("src", "newrand", "newrand.h"),
+        ],
+        # Force C++ linking in case gcc is picked up instead
+        # of g++ under windows with some versions of MinGW
+        extra_link_args=["-lstdc++"],
+        # Use C++11 to use the random number generator fix
+        extra_compiler_args=["-std=c++11"],
+    )

-    libsvm_sources = ['libsvm.pyx']
-    libsvm_depends = [join('src', 'libsvm', 'libsvm_helper.c'),
-                      join('src', 'libsvm', 'libsvm_template.cpp'),
-                      join('src', 'libsvm', 'svm.cpp'),
-                      join('src', 'libsvm', 'svm.h')]
+    libsvm_sources = ["_libsvm.pyx"]
+    libsvm_depends = [
+        join("src", "libsvm", "libsvm_helper.c"),
+        join("src", "libsvm", "libsvm_template.cpp"),
+        join("src", "libsvm", "svm.cpp"),
+        join("src", "libsvm", "svm.h"),
+        join("src", "newrand", "newrand.h"),
+    ]

-    config.add_extension('libsvm',
-                         sources=libsvm_sources,
-                         include_dirs=[numpy.get_include(),
-                                       join('src', 'libsvm')],
-                         libraries=['libsvm-skl'],
-                         depends=libsvm_depends,
-                         )
+    config.add_extension(
+        "_libsvm",
+        sources=libsvm_sources,
+        include_dirs=[
+            numpy.get_include(),
+            join("src", "libsvm"),
+            join("src", "newrand"),
+        ],
+        libraries=["libsvm-skl"],
+        depends=libsvm_depends,
+    )

     # liblinear module
     libraries = []
-    if os.name == 'posix':
-        libraries.append('m')
+    if os.name == "posix":
+        libraries.append("m")

-    liblinear_sources = ['liblinear.pyx',
-                         join('src', 'liblinear', '*.cpp')]
+    # precompile liblinear to use C++11 flag
+    config.add_library(
+        "liblinear-skl",
+        sources=[
+            join("src", "liblinear", "linear.cpp"),
+            join("src", "liblinear", "tron.cpp"),
+        ],
+        depends=[
+            join("src", "liblinear", "linear.h"),
+            join("src", "liblinear", "tron.h"),
+            join("src", "newrand", "newrand.h"),
+        ],
+        # Force C++ linking in case gcc is picked up instead
+        # of g++ under windows with some versions of MinGW
+        extra_link_args=["-lstdc++"],
+        # Use C++11 to use the random number generator fix
+        extra_compiler_args=["-std=c++11"],
+    )

-    liblinear_depends = [join('src', 'liblinear', '*.h'),
-                         join('src', 'liblinear', 'liblinear_helper.c')]
+    liblinear_sources = ["_liblinear.pyx"]
+    liblinear_depends = [
+        join("src", "liblinear", "*.h"),
+        join("src", "newrand", "newrand.h"),
+        join("src", "liblinear", "liblinear_helper.c"),
+    ]

-    config.add_extension('liblinear',
-                         sources=liblinear_sources,
-                         libraries=libraries,
-                         include_dirs=[join('.', 'src', 'liblinear'),
-                                       join('..', 'utils'),
-                                       numpy.get_include()],
-                         depends=liblinear_depends,
-                         # extra_compile_args=['-O0 -fno-inline'],
-                         )
+    config.add_extension(
+        "_liblinear",
+        sources=liblinear_sources,
+        libraries=["liblinear-skl"] + libraries,
+        include_dirs=[
+            join(".", "src", "liblinear"),
+            join(".", "src", "newrand"),
+            join("..", "utils"),
+            numpy.get_include(),
+        ],
+        depends=liblinear_depends,
+        # extra_compile_args=['-O0 -fno-inline'],
+    )

     # end liblinear module

     # this should go *after* libsvm-skl
-    libsvm_sparse_sources = ['libsvm_sparse.pyx']
-    config.add_extension('libsvm_sparse', libraries=['libsvm-skl'],
-                         sources=libsvm_sparse_sources,
-                         include_dirs=[numpy.get_include(),
-                                       join("src", "libsvm")],
-                         depends=[join("src", "libsvm", "svm.h"),
-                                  join("src", "libsvm",
-                                       "libsvm_sparse_helper.c")])
+    libsvm_sparse_sources = ["_libsvm_sparse.pyx"]
+    config.add_extension(
+        "_libsvm_sparse",
+        libraries=["libsvm-skl"],
+        sources=libsvm_sparse_sources,
+        include_dirs=[
+            numpy.get_include(),
+            join("src", "libsvm"),
+            join("src", "newrand"),
+        ],
+        depends=[
+            join("src", "libsvm", "svm.h"),
+            join("src", "newrand", "newrand.h"),
+            join("src", "libsvm", "libsvm_sparse_helper.c"),
+        ],
+    )

     return config


-if __name__ == '__main__':
+if __name__ == "__main__":
     from numpy.distutils.core import setup
-    setup(**configuration(top_path='').todict())
+
+    setup(**configuration(top_path="").todict())
('sklearn/svm/src/liblinear', 'linear.cpp')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,8 +1,8 @@
-/*
+/*
    Modified 2011:

    - Make labels sorted in group_classes, Dan Yamins.
-
+
    Modified 2012:

    - Changes roles of +1 and -1 to match scikit API, Andreas Mueller
@@ -22,6 +22,13 @@
    Modified 2015:
    - Patched liblinear for sample_weights - Manoj Kumar
      See https://github.com/scikit-learn/scikit-learn/pull/5274
+
+   Modified 2020:
+   - Improved random number generator by using a mersenne twister + tweaked
+     lemire postprocessor. This fixed a convergence issue on windows targets.
+     Sylvain Marie, Schneider Electric
+     See <https://github.com/scikit-learn/scikit-learn/pull/13511#issuecomment-481729756>
+
  */

 #include <math.h>
@@ -32,6 +39,10 @@
 #include <locale.h>
 #include "linear.h"
 #include "tron.h"
+#include <climits>
+#include <random>
+#include "../newrand/newrand.h"
+
 typedef signed char schar;
 template <class T> static inline void swap(T& x, T& y) { T t=x; x=y; y=t; }
 #ifndef min
@@ -456,19 +467,19 @@
 		g[i] = w[i] + 2*g[i];
 }

-// A coordinate descent algorithm for
+// A coordinate descent algorithm for
 // multi-class support vector machines by Crammer and Singer
 //
 //  min_{\alpha}  0.5 \sum_m ||w_m(\alpha)||^2 + \sum_i \sum_m e^m_i alpha^m_i
 //    s.t.     \alpha^m_i <= C^m_i \forall m,i , \sum_m \alpha^m_i=0 \forall i
-//
+//
 //  where e^m_i = 0 if y_i  = m,
 //        e^m_i = 1 if y_i != m,
-//  C^m_i = C if m  = y_i,
-//  C^m_i = 0 if m != y_i,
-//  and w_m(\alpha) = \sum_i \alpha^m_i x_i
-//
-// Given:
+//  C^m_i = C if m  = y_i,
+//  C^m_i = 0 if m != y_i,
+//  and w_m(\alpha) = \sum_i \alpha^m_i x_i
+//
+// Given:
 // x, y, C
 // eps is the stopping tolerance
 //
@@ -476,7 +487,7 @@
 //
 // See Appendix of LIBLINEAR paper, Fan et al. (2008)

-#define GETI(i) ((int) prob->y[i])
+#define GETI(i) (i)
 // To support weights for instances, use GETI(i) (i)

 class Solver_MCSVM_CS
@@ -506,13 +517,16 @@
 	this->prob = prob;
 	this->B = new double[nr_class];
 	this->G = new double[nr_class];
-	this->C = weighted_C;
+	this->C = new double[prob->l];
+	for(int i = 0; i < prob->l; i++)
+		this->C[i] = prob->W[i] * weighted_C[(int)prob->y[i]];
 }

 Solver_MCSVM_CS::~Solver_MCSVM_CS()
 {
 	delete[] B;
 	delete[] G;
+	delete[] C;
 }

 int compare_double(const void *a, const void *b)
@@ -576,7 +590,7 @@
 	double eps_shrink = max(10.0*eps, 1.0); // stopping tolerance for shrinking
 	bool start_from_all = true;

-	// Initial alpha can be set here. Note that
+	// Initial alpha can be set here. Note that
 	// sum_m alpha[i*nr_class+m] = 0, for all i=1,...,l-1
 	// alpha[i*nr_class+m] <= C[GETI(i)] if prob->y[i] == m
 	// alpha[i*nr_class+m] <= 0 if prob->y[i] != m
@@ -612,7 +626,7 @@
 		double stopping = -INF;
 		for(i=0;i<active_size;i++)
 		{
-			int j = i+rand()%(active_size-i);
+			int j = i+bounded_rand_int(active_size-i);
 			swap(index[i], index[j]);
 		}
 		for(s=0;s<active_size;s++)
@@ -772,14 +786,14 @@
 	return iter;
 }

-// A coordinate descent algorithm for
+// A coordinate descent algorithm for
 // L1-loss and L2-loss SVM dual problems
 //
 //  min_\alpha  0.5(\alpha^T (Q + D)\alpha) - e^T \alpha,
 //    s.t.      0 <= \alpha_i <= upper_bound_i,
-//
+//
 //  where Qij = yi yj xi^T xj and
-//  D is a diagonal matrix
+//  D is a diagonal matrix
 //
 // In L1-SVM case:
 // 		upper_bound_i = Cp if y_i = 1
@@ -790,16 +804,16 @@
 // 		D_ii = 1/(2*Cp)	if y_i = 1
 // 		D_ii = 1/(2*Cn)	if y_i = -1
 //
-// Given:
+// Given:
 // x, y, Cp, Cn
 // eps is the stopping tolerance
 //
 // solution will be put in w
-//
+//
 // See Algorithm 3 of Hsieh et al., ICML 2008

 #undef GETI
-#define GETI(i) (y[i]+1)
+#define GETI(i) (i)
 // To support weights for instances, use GETI(i) (i)

 static int solve_l2r_l1l2_svc(
@@ -823,14 +837,25 @@
 	double PGmax_new, PGmin_new;

 	// default solver_type: L2R_L2LOSS_SVC_DUAL
-	double diag[3] = {0.5/Cn, 0, 0.5/Cp};
-	double upper_bound[3] = {INF, 0, INF};
+	double *diag = new double[l];
+	double *upper_bound = new double[l];
+	double *C_ = new double[l];
+	for(i=0; i<l; i++)
+	{
+		if(prob->y[i]>0)
+			C_[i] = prob->W[i] * Cp;
+		else
+			C_[i] = prob->W[i] * Cn;
+		diag[i] = 0.5/C_[i];
+		upper_bound[i] = INF;
+	}
 	if(solver_type == L2R_L1LOSS_SVC_DUAL)
 	{
-		diag[0] = 0;
-		diag[2] = 0;
-		upper_bound[0] = Cn;
-		upper_bound[2] = Cp;
+		for(i=0; i<l; i++)
+		{
+			diag[i] = 0;
+			upper_bound[i] = C_[i];
+		}
 	}

 	for(i=0; i<l; i++)
@@ -874,7 +899,7 @@

 		for (i=0; i<active_size; i++)
 		{
-			int j = i+rand()%(active_size-i);
+			int j = i+bounded_rand_int(active_size-i);
 			swap(index[i], index[j]);
 		}

@@ -988,18 +1013,21 @@
 	delete [] alpha;
 	delete [] y;
 	delete [] index;
+	delete [] diag;
+	delete [] upper_bound;
+	delete [] C_;
 	return iter;
 }


-// A coordinate descent algorithm for
+// A coordinate descent algorithm for
 // L1-loss and L2-loss epsilon-SVR dual problem
 //
 //  min_\beta  0.5\beta^T (Q + diag(lambda)) \beta - p \sum_{i=1}^l|\beta_i| + \sum_{i=1}^l yi\beta_i,
 //    s.t.      -upper_bound_i <= \beta_i <= upper_bound_i,
-//
+//
 //  where Qij = xi^T xj and
-//  D is a diagonal matrix
+//  D is a diagonal matrix
 //
 // In L1-SVM case:
 // 		upper_bound_i = C
@@ -1008,16 +1036,16 @@
 // 		upper_bound_i = INF
 // 		lambda_i = 1/(2*C)
 //
-// Given:
+// Given:
 // x, y, p, C
 // eps is the stopping tolerance
 //
 // solution will be put in w
 //
-// See Algorithm 4 of Ho and Lin, 2012
+// See Algorithm 4 of Ho and Lin, 2012

 #undef GETI
-#define GETI(i) (0)
+#define GETI(i) (i)
 // To support weights for instances, use GETI(i) (i)

 static int solve_l2r_l1l2_svr(
@@ -1042,14 +1070,22 @@
 	double *y = prob->y;

 	// L2R_L2LOSS_SVR_DUAL
-	double lambda[1], upper_bound[1];
-	lambda[0] = 0.5/C;
-	upper_bound[0] = INF;
-
+	double *lambda = new double[l];
+	double *upper_bound = new double[l];
+	double *C_ = new double[l];
+	for (i=0; i<l; i++)
+	{
+		C_[i] = prob->W[i] * C;
+		lambda[i] = 0.5/C_[i];
+		upper_bound[i] = INF;
+	}
 	if(solver_type == L2R_L1LOSS_SVR_DUAL)
 	{
-		lambda[0] = 0;
-		upper_bound[0] = C;
+		for (i=0; i<l; i++)
+		{
+			lambda[i] = 0;
+			upper_bound[i] = C_[i];
+		}
 	}

 	// Initial beta can be set here. Note that
@@ -1082,7 +1118,7 @@

 		for(i=0; i<active_size; i++)
 		{
-			int j = i+rand()%(active_size-i);
+			int j = i+bounded_rand_int(active_size-i);
 			swap(index[i], index[j]);
 		}

@@ -1221,21 +1257,24 @@
 	delete [] beta;
 	delete [] QD;
 	delete [] index;
+	delete [] lambda;
+	delete [] upper_bound;
+	delete [] C_;
 	return iter;
 }


-// A coordinate descent algorithm for
+// A coordinate descent algorithm for
 // the dual of L2-regularized logistic regression problems
 //
 //  min_\alpha  0.5(\alpha^T Q \alpha) + \sum \alpha_i log (\alpha_i) + (upper_bound_i - \alpha_i) log (upper_bound_i - \alpha_i),
 //    s.t.      0 <= \alpha_i <= upper_bound_i,
-//
-//  where Qij = yi yj xi^T xj and
+//
+//  where Qij = yi yj xi^T xj and
 //  upper_bound_i = Cp if y_i = 1
 //  upper_bound_i = Cn if y_i = -1
 //
-// Given:
+// Given:
 // x, y, Cp, Cn
 // eps is the stopping tolerance
 //
@@ -1243,10 +1282,9 @@
 //
 // See Algorithm 5 of Yu et al., MLJ 2010

-
-#define SAMPLE_WEIGHT(i) upper_bound[y[i]+1]*sample_weight[i]
-// To support weights for instances, use SAMPLE_WEIGHT(i)
-// Each instance is weighted by sample_weight*class_weight)
+#undef GETI
+#define GETI(i) (i)
+// To support weights for instances, use GETI(i) (i)

 int solve_l2r_lr_dual(const problem *prob, double *w, double eps, double Cp, double Cn,
 					   int max_iter)
@@ -1261,28 +1299,29 @@
 	int max_inner_iter = 100; // for inner Newton
 	double innereps = 1e-2;
 	double innereps_min = min(1e-8, eps);
-	double upper_bound[3] = {Cn, 0, Cp};
-	double *sample_weight = prob->sample_weight;
+	double *upper_bound = new double [l];

 	for(i=0; i<l; i++)
 	{
 		if(prob->y[i] > 0)
 		{
+			upper_bound[i] = prob->W[i] * Cp;
 			y[i] = +1;
 		}
 		else
 		{
+			upper_bound[i] = prob->W[i] * Cn;
 			y[i] = -1;
 		}
 	}

 	// Initial alpha can be set here. Note that
-	// 0 < alpha[i] < SAMPLE_WEIGHT(i)
-	// alpha[2*i] + alpha[2*i+1] = SAMPLE_WEIGHT(i)
+	// 0 < alpha[i] < upper_bound[GETI(i)]
+	// alpha[2*i] + alpha[2*i+1] = upper_bound[GETI(i)]
 	for(i=0; i<l; i++)
 	{
-		alpha[2*i] = min(0.001*SAMPLE_WEIGHT(i), 1e-8);
-		alpha[2*i+1] = SAMPLE_WEIGHT(i) - alpha[2*i];
+		alpha[2*i] = min(0.001*upper_bound[GETI(i)], 1e-8);
+		alpha[2*i+1] = upper_bound[GETI(i)] - alpha[2*i];
 	}

 	for(i=0; i<w_size; i++)
@@ -1305,7 +1344,7 @@
 	{
 		for (i=0; i<l; i++)
 		{
-			int j = i+rand()%(l-i);
+			int j = i+bounded_rand_int(l-i);
 			swap(index[i], index[j]);
 		}
 		int newton_iter = 0;
@@ -1314,7 +1353,7 @@
 		{
 			i = index[s];
 			schar yi = y[i];
-			double C = SAMPLE_WEIGHT(i);
+			double C = upper_bound[GETI(i)];
 			double ywTx = 0, xisq = xTx[i];
 			feature_node *xi = prob->x[i];
 			while (xi->index != -1)
@@ -1397,13 +1436,14 @@
 	v *= 0.5;
 	for(i=0; i<l; i++)
 		v += alpha[2*i] * log(alpha[2*i]) + alpha[2*i+1] * log(alpha[2*i+1])
-			- SAMPLE_WEIGHT(i) * log(SAMPLE_WEIGHT(i));
+			- upper_bound[GETI(i)] * log(upper_bound[GETI(i)]);
 	info("Objective value = %lf\n", v);

 	delete [] xTx;
 	delete [] alpha;
 	delete [] y;
 	delete [] index;
+	delete [] upper_bound;
 	return iter;
 }

@@ -1421,7 +1461,7 @@
 // See Yuan et al. (2010) and appendix of LIBLINEAR paper, Fan et al. (2008)

 #undef GETI
-#define GETI(i) (y[i]+1)
+#define GETI(i) (i)
 // To support weights for instances, use GETI(i) (i)

 static int solve_l1r_l2_svc(
@@ -1449,7 +1489,7 @@
 	double *xj_sq = new double[w_size];
 	feature_node *x;

-	double C[3] = {Cn,0,Cp};
+	double *C = new double[l];

 	// Initial w can be set here.
 	for(j=0; j<w_size; j++)
@@ -1459,9 +1499,15 @@
 	{
 		b[j] = 1;
 		if(prob_col->y[j] > 0)
+		{
 			y[j] = 1;
+			C[j] = prob_col->W[j] * Cp;
+		}
 		else
+		{
 			y[j] = -1;
+			C[j] = prob_col->W[j] * Cn;
+		}
 	}
 	for(j=0; j<w_size; j++)
 	{
@@ -1486,7 +1532,7 @@

 		for(j=0; j<active_size; j++)
 		{
-			int i = j+rand()%(active_size-j);
+			int i = j+bounded_rand_int(active_size-j);
 			swap(index[i], index[j]);
 		}

@@ -1691,6 +1737,7 @@
 	delete [] y;
 	delete [] b;
 	delete [] xj_sq;
+	delete [] C;
 	return iter;
 }

@@ -1706,10 +1753,10 @@
 // solution will be put in w
 //
 // See Yuan et al. (2011) and appendix of LIBLINEAR paper, Fan et al. (2008)
-#undef SAMPLE_WEIGHT
-#define SAMPLE_WEIGHT(i) C[y[i]+1]*sample_weight[i]
-// To support weights for instances, use SAMPLE_WEIGHT(i)
-// Each instance is weighted by (class_weight*sample_weight)
+
+#undef GETI
+#define GETI(i) (i)
+// To support weights for instances, use GETI(i) (i)

 static int solve_l1r_lr(
 	const problem *prob_col, double *w, double eps,
@@ -1746,10 +1793,9 @@
 	double *exp_wTx_new = new double[l];
 	double *tau = new double[l];
 	double *D = new double[l];
-	double *sample_weight = prob_col->sample_weight;
 	feature_node *x;

-	double C[3] = {Cn,0,Cp};
+	double *C = new double[l];

 	// Initial w can be set here.
 	for(j=0; j<w_size; j++)
@@ -1758,9 +1804,15 @@
 	for(j=0; j<l; j++)
 	{
 		if(prob_col->y[j] > 0)
+		{
 			y[j] = 1;
+			C[j] = prob_col->W[j] * Cp;
+		}
 		else
+		{
 			y[j] = -1;
+			C[j] = prob_col->W[j] * Cn;
+		}

 		exp_wTx[j] = 0;
 	}
@@ -1779,7 +1831,7 @@
 			double val = x->value;
 			exp_wTx[ind] += w[j]*val;
 			if(y[ind] == -1)
-				xjneg_sum[j] += SAMPLE_WEIGHT(ind)*val;
+				xjneg_sum[j] += C[GETI(ind)]*val;
 			x++;
 		}
 	}
@@ -1787,8 +1839,8 @@
 	{
 		exp_wTx[j] = exp(exp_wTx[j]);
 		double tau_tmp = 1/(1+exp_wTx[j]);
-		tau[j] = SAMPLE_WEIGHT(j)*tau_tmp;
-		D[j] = SAMPLE_WEIGHT(j)*exp_wTx[j]*tau_tmp*tau_tmp;
+		tau[j] = C[GETI(j)]*tau_tmp;
+		D[j] = C[GETI(j)]*exp_wTx[j]*tau_tmp*tau_tmp;
 	}

 	while(newton_iter < max_newton_iter)
@@ -1862,7 +1914,7 @@

 			for(j=0; j<QP_active_size; j++)
 			{
-				int i = j+rand()%(QP_active_size-j);
+				int i = j+bounded_rand_int(QP_active_size-j);
 				swap(index[i], index[j]);
 			}

@@ -1964,7 +2016,7 @@
 		negsum_xTd = 0;
 		for(int i=0; i<l; i++)
 			if(y[i] == -1)
-				negsum_xTd += SAMPLE_WEIGHT(i)*xTd[i];
+				negsum_xTd += C[GETI(i)]*xTd[i];

 		int num_linesearch;
 		for(num_linesearch=0; num_linesearch < max_num_linesearch; num_linesearch++)
@@ -1975,7 +2027,7 @@
 			{
 				double exp_xTd = exp(xTd[i]);
 				exp_wTx_new[i] = exp_wTx[i]*exp_xTd;
-				cond += SAMPLE_WEIGHT(i)*log((1+exp_wTx_new[i])/(exp_xTd+exp_wTx_new[i]));
+				cond += C[GETI(i)]*log((1+exp_wTx_new[i])/(exp_xTd+exp_wTx_new[i]));
 			}

 			if(cond <= 0)
@@ -1987,8 +2039,8 @@
 				{
 					exp_wTx[i] = exp_wTx_new[i];
 					double tau_tmp = 1/(1+exp_wTx[i]);
-					tau[i] = SAMPLE_WEIGHT(i)*tau_tmp;
-					D[i] = SAMPLE_WEIGHT(i)*exp_wTx[i]*tau_tmp*tau_tmp;
+					tau[i] = C[GETI(i)]*tau_tmp;
+					D[i] = C[GETI(i)]*exp_wTx[i]*tau_tmp*tau_tmp;
 				}
 				break;
 			}
@@ -2055,9 +2107,9 @@
 		}
 	for(j=0; j<l; j++)
 		if(y[j] == 1)
-			v += SAMPLE_WEIGHT(j)*log(1+1/exp_wTx[j]);
+			v += C[GETI(j)]*log(1+1/exp_wTx[j]);
 		else
-			v += SAMPLE_WEIGHT(j)*log(1+exp_wTx[j]);
+			v += C[GETI(j)]*log(1+exp_wTx[j]);

 	info("Objective value = %lf\n", v);
 	info("#nonzeros/#features = %d/%d\n", nnz, w_size);
@@ -2073,6 +2125,7 @@
 	delete [] exp_wTx_new;
 	delete [] tau;
 	delete [] D;
+	delete [] C;
 	return newton_iter;
 }

@@ -2089,10 +2142,13 @@
 	prob_col->n = n;
 	prob_col->y = new double[l];
 	prob_col->x = new feature_node*[n];
-	prob_col->sample_weight=prob->sample_weight;
+	prob_col->W = new double[l];

 	for(i=0; i<l; i++)
+	{
 		prob_col->y[i] = prob->y[i];
+		prob_col->W[i] = prob->W[i];
+	}

 	for(i=0; i<n+1; i++)
 		col_ptr[i] = 0;
@@ -2189,14 +2245,14 @@
                 label[i+1] = this_label;
                 count[i+1] = this_count;
         }
-
+
         for (i=0; i <l; i++)
         {
                 j = 0;
                 int this_label = (int)prob->y[i];
                 while(this_label != label[j])
                 {
-                        j++;
+                        j++;
                 }
                 data_label[i] = j;

@@ -2247,7 +2303,6 @@
 static int train_one(const problem *prob, const parameter *param, double *w, double Cp, double Cn, BlasFunctions *blas_functions)
 {
 	double eps=param->eps;
-	double* sample_weight=prob->sample_weight;
 	int max_iter=param->max_iter;
 	int pos = 0;
 	int neg = 0;
@@ -2268,9 +2323,9 @@
 			for(int i = 0; i < prob->l; i++)
 			{
 				if(prob->y[i] > 0)
-					C[i] = sample_weight[i]*Cp;
+					C[i] = prob->W[i] * Cp;
 				else
-					C[i] = sample_weight[i]*Cn;
+					C[i] = prob->W[i] * Cn;
 			}

 			fun_obj=new l2r_lr_fun(prob, C);
@@ -2287,9 +2342,9 @@
 			for(int i = 0; i < prob->l; i++)
 			{
 				if(prob->y[i] > 0)
-					C[i] = Cp;
+					C[i] = prob->W[i] * Cp;
 				else
-					C[i] = Cn;
+					C[i] = prob->W[i] * Cn;
 			}
 			fun_obj=new l2r_l2_svc_fun(prob, C);
 			TRON tron_obj(fun_obj, primal_solver_tol, max_iter, blas_functions);
@@ -2313,6 +2368,7 @@
 			n_iter=solve_l1r_l2_svc(&prob_col, w, primal_solver_tol, Cp, Cn, max_iter);
 			delete [] prob_col.y;
 			delete [] prob_col.x;
+			delete [] prob_col.W;
 			delete [] x_space;
 			break;
 		}
@@ -2324,6 +2380,7 @@
 			n_iter=solve_l1r_lr(&prob_col, w, primal_solver_tol, Cp, Cn, max_iter);
 			delete [] prob_col.y;
 			delete [] prob_col.x;
+			delete [] prob_col.W;
 			delete [] x_space;
 			break;
 		}
@@ -2334,7 +2391,7 @@
 		{
 			double *C = new double[prob->l];
 			for(int i = 0; i < prob->l; i++)
-				C[i] = param->C;
+				C[i] = prob->W[i] * param->C;

 			fun_obj=new l2r_l2_svr_fun(prob, C, param->p);
 			TRON tron_obj(fun_obj, param->eps, max_iter, blas_functions);
@@ -2359,10 +2416,39 @@
 }

 //
+// Remove zero weighed data as libsvm and some liblinear solvers require C > 0.
+//
+static void remove_zero_weight(problem *newprob, const problem *prob)
+{
+	int i;
+	int l = 0;
+	for(i=0;i<prob->l;i++)
+		if(prob->W[i] > 0) l++;
+	*newprob = *prob;
+	newprob->l = l;
+	newprob->x = Malloc(feature_node*,l);
+	newprob->y = Malloc(double,l);
+	newprob->W = Malloc(double,l);
+
+	int j = 0;
+	for(i=0;i<prob->l;i++)
+		if(prob->W[i] > 0)
+		{
+			newprob->x[j] = prob->x[i];
+			newprob->y[j] = prob->y[i];
+			newprob->W[j] = prob->W[i];
+			j++;
+		}
+}
+
+//
 // Interface functions
 //
 model* train(const problem *prob, const parameter *param, BlasFunctions *blas_functions)
 {
+	problem newprob;
+	remove_zero_weight(&newprob, prob);
+	prob = &newprob;
 	int i,j;
 	int l = prob->l;
 	int n = prob->n;
@@ -2418,12 +2504,8 @@

 		// constructing the subproblem
 		feature_node **x = Malloc(feature_node *,l);
-		double *sample_weight = new double[l];
 		for(i=0;i<l;i++)
-		{
 			x[i] = prob->x[perm[i]];
-			sample_weight[i] = prob->sample_weight[perm[i]];
-		}

 		int k;
 		problem sub_prob;
@@ -2431,10 +2513,11 @@
 		sub_prob.n = n;
 		sub_prob.x = Malloc(feature_node *,sub_prob.l);
 		sub_prob.y = Malloc(double,sub_prob.l);
-		sub_prob.sample_weight = sample_weight;
-
-		for(k=0; k<sub_prob.l; k++)
+		sub_prob.W = Malloc(double,sub_prob.l);
+		for(k=0; k<sub_prob.l; k++){
 			sub_prob.x[k] = x[k];
+			sub_prob.W[k] = prob->W[perm[k]];
+		}

 		// multi-class svm by Crammer and Singer
 		if(param->solver_type == MCSVM_CS)
@@ -2497,8 +2580,11 @@
 		free(perm);
 		free(sub_prob.x);
 		free(sub_prob.y);
+		free(sub_prob.W);
 		free(weighted_C);
-		delete[] sample_weight;
+		free(newprob.x);
+		free(newprob.y);
+		free(newprob.W);
 	}
 	return model_;
 }
@@ -2519,7 +2605,7 @@
 	for(i=0;i<l;i++) perm[i]=i;
 	for(i=0;i<l;i++)
 	{
-		int j = i+rand()%(l-i);
+		int j = i+bounded_rand_int(l-i);
 		swap(perm[i],perm[j]);
 	}
 	for(i=0;i<=nr_fold;i++)
@@ -2982,4 +3068,3 @@
 	else
 		liblinear_print_string = print_func;
 }
-
('sklearn/svm/src/liblinear', 'linear.h')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -19,7 +19,7 @@
 	double *y;
 	struct feature_node **x;
 	double bias;            /* < 0 if no bias term */
-	double *sample_weight;
+	double *W;
 };

 enum { L2R_LR, L2R_L2LOSS_SVC_DUAL, L2R_L2LOSS_SVC, L2R_L1LOSS_SVC_DUAL, MCSVM_CS, L1R_L2LOSS_SVC, L1R_LR, L2R_LR_DUAL, L2R_L2LOSS_SVR = 11, L2R_L2LOSS_SVR_DUAL, L2R_L1LOSS_SVR_DUAL }; /* solver_type */
@@ -48,6 +48,8 @@
 	double bias;
 	int *n_iter;    /* no. of iterations of each class */
 };
+
+void set_seed(unsigned seed);

 struct model* train(const struct problem *prob, const struct parameter *param, BlasFunctions *blas_functions);
 void cross_validation(const struct problem *prob, const struct parameter *param, int nr_fold, double *target);
('sklearn/svm/src/liblinear', 'liblinear_helper.c')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,49 +3,62 @@
 #include "linear.h"

 /*
- * Convert matrix to sparse representation suitable for libsvm. x is
- * expected to be an array of length nrow*ncol.
+ * Convert matrix to sparse representation suitable for liblinear. x is
+ * expected to be an array of length n_samples*n_features.
  *
- * Typically the matrix will be dense, so we speed up the routine for
- * this case. We create a temporary array temp that collects non-zero
- * elements and after we just memcpy that to the proper array.
+ * Whether the matrix is densely or sparsely populated, the fastest way to
+ * convert it to liblinear's sparse format is to calculate the amount of memory
+ * needed and allocate a single big block.
  *
- * Special care must be taken with indices, since libsvm indices start
- * at 1 and not at 0.
+ * Special care must be taken with indices, since liblinear indices start at 1
+ * and not at 0.
  *
  * If bias is > 0, we append an item at the end.
  */
-static struct feature_node **dense_to_sparse(double *x, npy_intp *dims,
-                                             double bias)
-{
+static struct feature_node **dense_to_sparse(char *x, int double_precision,
+        int n_samples, int n_features, int n_nonzero, double bias)
+{
+    float *x32 = (float *)x;
+    double *x64 = (double *)x;
     struct feature_node **sparse;
     int i, j;                           /* number of nonzero elements in row i */
-    struct feature_node *temp;          /* stack for nonzero elements */
     struct feature_node *T;             /* pointer to the top of the stack */
-    int count;
-
-    sparse = malloc (dims[0] * sizeof(struct feature_node *));
+    int have_bias = (bias > 0);
+
+    sparse = malloc (n_samples * sizeof(struct feature_node *));
     if (sparse == NULL)
-        goto sparse_error;
-
-    temp = malloc ((dims[1]+2) * sizeof(struct feature_node));
-    if (temp == NULL)
-        goto temp_error;
-
-    for (i=0; i<dims[0]; ++i) {
-        T = temp; /* reset stack pointer */
-
-        for (j=1; j<=dims[1]; ++j) {
-            if (*x != 0) {
-                T->value = *x;
-                T->index = j;
-                ++ T;
+        return NULL;
+
+    n_nonzero += (have_bias+1) * n_samples;
+    T = malloc (n_nonzero * sizeof(struct feature_node));
+    if (T == NULL) {
+        free(sparse);
+        return NULL;
+    }
+
+    for (i=0; i<n_samples; ++i) {
+        sparse[i] = T;
+
+        for (j=1; j<=n_features; ++j) {
+            if (double_precision) {
+                if (*x64 != 0) {
+                    T->value = *x64;
+                    T->index = j;
+                    ++ T;
+                }
+                ++ x64; /* go to next element */
+            } else {
+                if (*x32 != 0) {
+                    T->value = *x32;
+                    T->index = j;
+                    ++ T;
+                }
+                ++ x32; /* go to next element */
             }
-            ++ x; /* go to next element */
         }

         /* set bias element */
-        if (bias > 0) {
+        if (have_bias) {
                 T->value = bias;
                 T->index = j;
                 ++ T;
@@ -54,95 +67,79 @@
         /* set sentinel */
         T->index = -1;
         ++ T;
-
-        /* allocate memory and copy collected items*/
-        count = T - temp;
-        sparse[i] = malloc(count * sizeof(struct feature_node));
-        if (sparse[i] == NULL) {
-            int k;
-            for (k=0; k<i; k++)
-                free(sparse[k]);
-            goto sparse_i_error;
-        }
-        memcpy(sparse[i], temp, count * sizeof(struct feature_node));
-    }
-
-    free(temp);
+    }
+
     return sparse;
-
-sparse_i_error:
-    free(temp);
-temp_error:
-    free(sparse);
-sparse_error:
-    return NULL;
 }


 /*
- * Convert scipy.sparse.csr to libsvm's sparse data structure
+ * Convert scipy.sparse.csr to liblinear's sparse data structure
  */
-static struct feature_node **csr_to_sparse(double *values,
-        npy_intp *shape_indices, int *indices, npy_intp *shape_indptr,
-        int *indptr, double bias, int n_features)
-{
-    struct feature_node **sparse, *temp;
+static struct feature_node **csr_to_sparse(char *x, int double_precision,
+        int *indices, int *indptr, int n_samples, int n_features, int n_nonzero,
+        double bias)
+{
+    float *x32 = (float *)x;
+    double *x64 = (double *)x;
+    struct feature_node **sparse;
     int i, j=0, k=0, n;
-
-    sparse = malloc ((shape_indptr[0]-1)* sizeof(struct feature_node *));
+    struct feature_node *T;
+    int have_bias = (bias > 0);
+
+    sparse = malloc (n_samples * sizeof(struct feature_node *));
     if (sparse == NULL)
         return NULL;

-    for (i=0; i<shape_indptr[0]-1; ++i) {
+    n_nonzero += (have_bias+1) * n_samples;
+    T = malloc (n_nonzero * sizeof(struct feature_node));
+    if (T == NULL) {
+        free(sparse);
+        return NULL;
+    }
+
+    for (i=0; i<n_samples; ++i) {
+        sparse[i] = T;
         n = indptr[i+1] - indptr[i]; /* count elements in row i */

-        sparse[i] = malloc ((n+2) * sizeof(struct feature_node));
-        if (sparse[i] == NULL) {
-            int l;
-            for (l=0; l<i; l++)
-                free(sparse[l]);
-            break;
-        }
-
-        temp = sparse[i];
         for (j=0; j<n; ++j) {
-            temp[j].value = values[k];
-            temp[j].index = indices[k] + 1; /* libsvm uses 1-based indexing */
+            T->value = double_precision ? x64[k] : x32[k];
+            T->index = indices[k] + 1; /* liblinear uses 1-based indexing */
+            ++T;
             ++k;
         }

-        if (bias > 0) {
-            temp[j].value = bias;
-            temp[j].index = n_features + 1;
+        if (have_bias) {
+            T->value = bias;
+            T->index = n_features + 1;
+            ++T;
             ++j;
         }

         /* set sentinel */
-        temp[j].index = -1;
+        T->index = -1;
+        ++T;
     }

     return sparse;
 }

-struct problem * set_problem(char *X,char *Y, npy_intp *dims, double bias, char* sample_weight)
+struct problem * set_problem(char *X, int double_precision_X, int n_samples,
+        int n_features, int n_nonzero, double bias, char* sample_weight,
+        char *Y)
 {
     struct problem *problem;
     /* not performant but simple */
     problem = malloc(sizeof(struct problem));
     if (problem == NULL) return NULL;
-    problem->l = (int) dims[0];
-
-    if (bias > 0) {
-        problem->n = (int) dims[1] + 1;
-    } else {
-        problem->n = (int) dims[1];
-    }
-
+    problem->l = n_samples;
+    problem->n = n_features + (bias > 0);
     problem->y = (double *) Y;
-    problem->sample_weight = (double *) sample_weight;
-    problem->x = dense_to_sparse((double *) X, dims, bias);
+    problem->W = (double *) sample_weight;
+    problem->x = dense_to_sparse(X, double_precision_X, n_samples, n_features,
+                        n_nonzero, bias);
     problem->bias = bias;
-    problem->sample_weight = sample_weight;
+
     if (problem->x == NULL) {
         free(problem);
         return NULL;
@@ -151,27 +148,20 @@
     return problem;
 }

-struct problem * csr_set_problem (char *values, npy_intp *n_indices,
-	char *indices, npy_intp *n_indptr, char *indptr, char *Y,
-        npy_intp n_features, double bias, char *sample_weight) {
-
+struct problem * csr_set_problem (char *X, int double_precision_X,
+        char *indices, char *indptr, int n_samples, int n_features,
+        int n_nonzero, double bias, char *sample_weight, char *Y)
+{
     struct problem *problem;
     problem = malloc (sizeof (struct problem));
     if (problem == NULL) return NULL;
-    problem->l = (int) n_indptr[0] -1;
-    problem->sample_weight = (double *) sample_weight;
-
-    if (bias > 0){
-        problem->n = (int) n_features + 1;
-    } else {
-        problem->n = (int) n_features;
-    }
-
+    problem->l = n_samples;
+    problem->n = n_features + (bias > 0);
     problem->y = (double *) Y;
-    problem->x = csr_to_sparse((double *) values, n_indices, (int *) indices,
-			n_indptr, (int *) indptr, bias, n_features);
+    problem->W = (double *) sample_weight;
+    problem->x = csr_to_sparse(X, double_precision_X, (int *) indices,
+                        (int *) indptr, n_samples, n_features, n_nonzero, bias);
     problem->bias = bias;
-    problem->sample_weight = sample_weight;

     if (problem->x == NULL) {
         free(problem);
@@ -182,7 +172,7 @@
 }


-/* Create a paramater struct with and return it */
+/* Create a parameter struct with and return it */
 struct parameter *set_parameter(int solver_type, double eps, double C,
                                 npy_intp nr_weight, char *weight_label,
                                 char *weight, int max_iter, unsigned seed,
@@ -192,7 +182,7 @@
     if (param == NULL)
         return NULL;

-    srand(seed);
+    set_seed(seed);
     param->solver_type = solver_type;
     param->eps = eps;
     param->C = C;
@@ -216,8 +206,7 @@

 void free_problem(struct problem *problem)
 {
-    int i;
-    for(i=problem->l-1; i>=0; --i) free(problem->x[i]);
+    free(problem->x[0]);
     free(problem->x);
     free(problem);
 }
('sklearn/svm/src/libsvm', 'libsvm_sparse_helper.c')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,12 @@
 #include <stdlib.h>
 #include <numpy/arrayobject.h>
 #include "svm.h"
+#include "_svm_cython_blas_helpers.h"
+
+
+#ifndef MAX
+    #define MAX(x, y) (((x) > (y)) ? (x) : (y))
+#endif


 /*
@@ -122,6 +128,9 @@
     if ((model->rho = malloc( m * sizeof(double))) == NULL)
         goto rho_error;

+    // This is only allocated in dynamic memory while training.
+    model->n_iter = NULL;
+
     /* in the case of precomputed kernels we do not use
        dense_to_precomputed because we don't want the leading 0. As
        indices start at 1 (not at 0) this will work */
@@ -242,11 +251,11 @@


 /*
- * Predict using a model, where data is expected to be enconded into a csr matrix.
+ * Predict using a model, where data is expected to be encoded into a csr matrix.
  */
 int csr_copy_predict (npy_intp *data_size, char *data, npy_intp *index_size,
 		char *index, npy_intp *intptr_size, char *intptr, struct svm_csr_model *model,
-		char *dec_values) {
+		char *dec_values, BlasFunctions *blas_functions) {
     double *t = (double *) dec_values;
     struct svm_csr_node **predict_nodes;
     npy_intp i;
@@ -257,7 +266,7 @@
     if (predict_nodes == NULL)
         return -1;
     for(i=0; i < intptr_size[0] - 1; ++i) {
-        *t = svm_csr_predict(model, predict_nodes[i]);
+        *t = svm_csr_predict(model, predict_nodes[i], blas_functions);
         free(predict_nodes[i]);
         ++t;
     }
@@ -267,7 +276,7 @@

 int csr_copy_predict_values (npy_intp *data_size, char *data, npy_intp *index_size,
                 char *index, npy_intp *intptr_size, char *intptr, struct svm_csr_model *model,
-                char *dec_values, int nr_class) {
+                char *dec_values, int nr_class, BlasFunctions *blas_functions) {
     struct svm_csr_node **predict_nodes;
     npy_intp i;

@@ -278,7 +287,8 @@
         return -1;
     for(i=0; i < intptr_size[0] - 1; ++i) {
         svm_csr_predict_values(model, predict_nodes[i],
-                               ((double *) dec_values) + i*nr_class);
+                               ((double *) dec_values) + i*nr_class,
+			       blas_functions);
         free(predict_nodes[i]);
     }
     free(predict_nodes);
@@ -288,7 +298,7 @@

 int csr_copy_predict_proba (npy_intp *data_size, char *data, npy_intp *index_size,
 		char *index, npy_intp *intptr_size, char *intptr, struct svm_csr_model *model,
-		char *dec_values) {
+		char *dec_values, BlasFunctions *blas_functions) {

     struct svm_csr_node **predict_nodes;
     npy_intp i;
@@ -301,7 +311,7 @@
         return -1;
     for(i=0; i < intptr_size[0] - 1; ++i) {
         svm_csr_predict_probability(
-		model, predict_nodes[i], ((double *) dec_values) + i*m);
+		model, predict_nodes[i], ((double *) dec_values) + i*m, blas_functions);
         free(predict_nodes[i]);
     }
     free(predict_nodes);
@@ -348,6 +358,15 @@
 }

 /*
+ * Get the number of iterations run in optimization
+ */
+void copy_n_iter(char *data, struct svm_csr_model *model)
+{
+    const int n_models = MAX(1, model->nr_class * (model->nr_class-1) / 2);
+    memcpy(data, model->n_iter, n_models * sizeof(int));
+}
+
+/*
  * Get the number of support vectors in a model.
  */
 npy_intp get_l(struct svm_csr_model *model)
@@ -401,6 +420,7 @@
 int free_model(struct svm_csr_model *model)
 {
     /* like svm_free_and_destroy_model, but does not free sv_coef[i] */
+    /* We don't free n_iter, since we did not create them in set_model. */
     if (model == NULL) return -1;
     free(model->SV);
     free(model->sv_coef);
('sklearn/svm/src/libsvm', 'svm.h')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,6 +6,7 @@
 #ifdef __cplusplus
 extern "C" {
 #endif
+#include "_svm_cython_blas_helpers.h"

 struct svm_node
 {
@@ -75,11 +76,12 @@
 	int l;			/* total #SV */
 	struct svm_node *SV;		/* SVs (SV[l]) */
 	double **sv_coef;	/* coefficients for SVs in decision functions (sv_coef[k-1][l]) */
+	int *n_iter;		/* number of iterations run by the optimization routine to fit the model */

 	int *sv_ind;            /* index of support vectors */

 	double *rho;		/* constants in decision functions (rho[k*(k-1)/2]) */
-	double *probA;		/* pariwise probability information */
+	double *probA;		/* pairwise probability information */
 	double *probB;

 	/* for classification only */
@@ -100,11 +102,12 @@
 	int l;			/* total #SV */
 	struct svm_csr_node **SV;		/* SVs (SV[l]) */
 	double **sv_coef;	/* coefficients for SVs in decision functions (sv_coef[k-1][l]) */
+	int *n_iter;		/* number of iterations run by the optimization routine to fit the model */

         int *sv_ind;            /* index of support vectors */

 	double *rho;		/* constants in decision functions (rho[k*(k-1)/2]) */
-	double *probA;		/* pariwise probability information */
+	double *probA;		/* pairwise probability information */
 	double *probB;

 	/* for classification only */
@@ -117,9 +120,9 @@
 				/* 0 if svm_model is created by svm_train */
 };

-
-struct svm_model *svm_train(const struct svm_problem *prob, const struct svm_parameter *param, int *status);
-void svm_cross_validation(const struct svm_problem *prob, const struct svm_parameter *param, int nr_fold, double *target);
+/* svm_ functions are defined by libsvm_template.cpp from generic versions in svm.cpp */
+struct svm_model *svm_train(const struct svm_problem *prob, const struct svm_parameter *param, int *status, BlasFunctions *blas_functions);
+void svm_cross_validation(const struct svm_problem *prob, const struct svm_parameter *param, int nr_fold, double *target, BlasFunctions *blas_functions);

 int svm_save_model(const char *model_file_name, const struct svm_model *model);
 struct svm_model *svm_load_model(const char *model_file_name);
@@ -129,9 +132,9 @@
 void svm_get_labels(const struct svm_model *model, int *label);
 double svm_get_svr_probability(const struct svm_model *model);

-double svm_predict_values(const struct svm_model *model, const struct svm_node *x, double* dec_values);
-double svm_predict(const struct svm_model *model, const struct svm_node *x);
-double svm_predict_probability(const struct svm_model *model, const struct svm_node *x, double* prob_estimates);
+double svm_predict_values(const struct svm_model *model, const struct svm_node *x, double* dec_values, BlasFunctions *blas_functions);
+double svm_predict(const struct svm_model *model, const struct svm_node *x, BlasFunctions *blas_functions);
+double svm_predict_probability(const struct svm_model *model, const struct svm_node *x, double* prob_estimates, BlasFunctions *blas_functions);

 void svm_free_model_content(struct svm_model *model_ptr);
 void svm_free_and_destroy_model(struct svm_model **model_ptr_ptr);
@@ -144,17 +147,18 @@

 /* sparse version */

-struct svm_csr_model *svm_csr_train(const struct svm_csr_problem *prob, const struct svm_parameter *param, int *status);
-void svm_csr_cross_validation(const struct svm_csr_problem *prob, const struct svm_parameter *param, int nr_fold, double *target);
+/* svm_csr_ functions are defined by libsvm_template.cpp from generic versions in svm.cpp */
+struct svm_csr_model *svm_csr_train(const struct svm_csr_problem *prob, const struct svm_parameter *param, int *status, BlasFunctions *blas_functions);
+void svm_csr_cross_validation(const struct svm_csr_problem *prob, const struct svm_parameter *param, int nr_fold, double *target, BlasFunctions *blas_functions);

 int svm_csr_get_svm_type(const struct svm_csr_model *model);
 int svm_csr_get_nr_class(const struct svm_csr_model *model);
 void svm_csr_get_labels(const struct svm_csr_model *model, int *label);
 double svm_csr_get_svr_probability(const struct svm_csr_model *model);

-double svm_csr_predict_values(const struct svm_csr_model *model, const struct svm_csr_node *x, double* dec_values);
-double svm_csr_predict(const struct svm_csr_model *model, const struct svm_csr_node *x);
-double svm_csr_predict_probability(const struct svm_csr_model *model, const struct svm_csr_node *x, double* prob_estimates);
+double svm_csr_predict_values(const struct svm_csr_model *model, const struct svm_csr_node *x, double* dec_values, BlasFunctions *blas_functions);
+double svm_csr_predict(const struct svm_csr_model *model, const struct svm_csr_node *x, BlasFunctions *blas_functions);
+double svm_csr_predict_probability(const struct svm_csr_model *model, const struct svm_csr_node *x, double* prob_estimates, BlasFunctions *blas_functions);

 void svm_csr_free_model_content(struct svm_csr_model *model_ptr);
 void svm_csr_free_and_destroy_model(struct svm_csr_model **model_ptr_ptr);
('sklearn/svm/src/libsvm', 'LIBSVM_CHANGES')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,5 +4,8 @@

   * Add copyright to files svm.cpp and svm.h
   * Add random_seed support and call to srand in fit function
-
+  * Improved random number generator (fix on windows, enhancement on other
+    platforms). See <https://github.com/scikit-learn/scikit-learn/pull/13511#issuecomment-481729756>
+  * invoke scipy blas api for svm kernel function to improve performance with speedup rate of 1.5X to 2X for dense data only. See <https://github.com/scikit-learn/scikit-learn/pull/16530>
+  * Expose the number of iterations run in optimization. See <https://github.com/scikit-learn/scikit-learn/pull/21408>
 The changes made with respect to upstream are detailed in the heading of svm.cpp
('sklearn/svm/src/libsvm', 'libsvm_helper.c')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,13 @@
 #include <stdlib.h>
 #include <numpy/arrayobject.h>
 #include "svm.h"
+#include "_svm_cython_blas_helpers.h"
+
+
+#ifndef MAX
+    #define MAX(x, y) (((x) > (y)) ? (x) : (y))
+#endif
+

 /*
  * Some helper methods for libsvm bindings.
@@ -128,6 +135,9 @@
     if ((model->rho = malloc( m * sizeof(double))) == NULL)
         goto rho_error;

+    // This is only allocated in dynamic memory while training.
+    model->n_iter = NULL;
+
     model->nr_class = nr_class;
     model->param = *param;
     model->l = (int) support_dims[0];
@@ -219,6 +229,15 @@
 }

 /*
+ * Get the number of iterations run in optimization
+ */
+void copy_n_iter(char *data, struct svm_model *model)
+{
+    const int n_models = MAX(1, model->nr_class * (model->nr_class-1) / 2);
+    memcpy(data, model->n_iter, n_models * sizeof(int));
+}
+
+/*
  * Some helpers to convert from libsvm sparse data structures
  * model->sv_coef is a double **, whereas data is just a double *,
  * so we have to do some stupid copying.
@@ -293,7 +312,7 @@
  *  It will return -1 if we run out of memory.
  */
 int copy_predict(char *predict, struct svm_model *model, npy_intp *predict_dims,
-                 char *dec_values)
+                 char *dec_values, BlasFunctions *blas_functions)
 {
     double *t = (double *) dec_values;
     struct svm_node *predict_nodes;
@@ -304,7 +323,7 @@
     if (predict_nodes == NULL)
         return -1;
     for(i=0; i<predict_dims[0]; ++i) {
-        *t = svm_predict(model, &predict_nodes[i]);
+        *t = svm_predict(model, &predict_nodes[i], blas_functions);
         ++t;
     }
     free(predict_nodes);
@@ -312,7 +331,7 @@
 }

 int copy_predict_values(char *predict, struct svm_model *model,
-                        npy_intp *predict_dims, char *dec_values, int nr_class)
+                        npy_intp *predict_dims, char *dec_values, int nr_class, BlasFunctions *blas_functions)
 {
     npy_intp i;
     struct svm_node *predict_nodes;
@@ -321,7 +340,8 @@
         return -1;
     for(i=0; i<predict_dims[0]; ++i) {
         svm_predict_values(model, &predict_nodes[i],
-                                ((double *) dec_values) + i*nr_class);
+                                ((double *) dec_values) + i*nr_class,
+				blas_functions);
     }

     free(predict_nodes);
@@ -331,7 +351,7 @@


 int copy_predict_proba(char *predict, struct svm_model *model, npy_intp *predict_dims,
-                 char *dec_values)
+                 char *dec_values, BlasFunctions *blas_functions)
 {
     npy_intp i, n, m;
     struct svm_node *predict_nodes;
@@ -342,7 +362,8 @@
         return -1;
     for(i=0; i<n; ++i) {
         svm_predict_probability(model, &predict_nodes[i],
-                                ((double *) dec_values) + i*m);
+                                ((double *) dec_values) + i*m,
+				blas_functions);
     }
     free(predict_nodes);
     return 0;
@@ -361,9 +382,11 @@
     if (model == NULL) return -1;
     free(model->SV);

-    /* We don't free sv_ind, since we did not create them in
+    /* We don't free sv_ind and n_iter, since we did not create them in
        set_model */
-    /* free(model->sv_ind); */
+    /* free(model->sv_ind);
+     * free(model->n_iter);
+     */
     free(model->sv_coef);
     free(model->rho);
     free(model->label);
('sklearn/svm/src/libsvm', 'svm.cpp')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -48,6 +48,17 @@

    - Make labels sorted in svm_group_classes, Fabian Pedregosa.

+   Modified 2020:
+
+   - Improved random number generator by using a mersenne twister + tweaked
+     lemire postprocessor. This fixed a convergence issue on windows targets.
+     Sylvain Marie, Schneider Electric
+     see <https://github.com/scikit-learn/scikit-learn/pull/13511#issuecomment-481729756>
+
+   Modified 2021:
+
+   - Exposed number of iterations run in optimization, Juan Martín Loyola.
+     See <https://github.com/scikit-learn/scikit-learn/pull/21408/>
  */

 #include <math.h>
@@ -57,7 +68,12 @@
 #include <float.h>
 #include <string.h>
 #include <stdarg.h>
+#include <climits>
+#include <random>
 #include "svm.h"
+#include "_svm_cython_blas_helpers.h"
+#include "../newrand/newrand.h"
+

 #ifndef _LIBSVM_CPP
 typedef float Qfloat;
@@ -279,14 +295,14 @@
 class Kernel: public QMatrix {
 public:
 #ifdef _DENSE_REP
-	Kernel(int l, PREFIX(node) * x, const svm_parameter& param);
+	Kernel(int l, PREFIX(node) * x, const svm_parameter& param, BlasFunctions *blas_functions);
 #else
-	Kernel(int l, PREFIX(node) * const * x, const svm_parameter& param);
+	Kernel(int l, PREFIX(node) * const * x, const svm_parameter& param, BlasFunctions *blas_functions);
 #endif
 	virtual ~Kernel();

 	static double k_function(const PREFIX(node) *x, const PREFIX(node) *y,
-				 const svm_parameter& param);
+				 const svm_parameter& param, BlasFunctions *blas_functions);
 	virtual Qfloat *get_Q(int column, int len) const = 0;
 	virtual double *get_QD() const = 0;
 	virtual void swap_index(int i, int j) const	// no so const...
@@ -305,6 +321,8 @@
 	const PREFIX(node) **x;
 #endif
 	double *x_square;
+	// scipy blas pointer
+	BlasFunctions *m_blas;

 	// svm_parameter
 	const int kernel_type;
@@ -312,26 +330,26 @@
 	const double gamma;
 	const double coef0;

-	static double dot(const PREFIX(node) *px, const PREFIX(node) *py);
+	static double dot(const PREFIX(node) *px, const PREFIX(node) *py, BlasFunctions *blas_functions);
 #ifdef _DENSE_REP
-	static double dot(const PREFIX(node) &px, const PREFIX(node) &py);
+	static double dot(const PREFIX(node) &px, const PREFIX(node) &py, BlasFunctions *blas_functions);
 #endif

 	double kernel_linear(int i, int j) const
 	{
-		return dot(x[i],x[j]);
+		return dot(x[i],x[j],m_blas);
 	}
 	double kernel_poly(int i, int j) const
 	{
-		return powi(gamma*dot(x[i],x[j])+coef0,degree);
+		return powi(gamma*dot(x[i],x[j],m_blas)+coef0,degree);
 	}
 	double kernel_rbf(int i, int j) const
 	{
-		return exp(-gamma*(x_square[i]+x_square[j]-2*dot(x[i],x[j])));
+		return exp(-gamma*(x_square[i]+x_square[j]-2*dot(x[i],x[j],m_blas)));
 	}
 	double kernel_sigmoid(int i, int j) const
 	{
-		return tanh(gamma*dot(x[i],x[j])+coef0);
+		return tanh(gamma*dot(x[i],x[j],m_blas)+coef0);
 	}
 	double kernel_precomputed(int i, int j) const
 	{
@@ -344,13 +362,14 @@
 };

 #ifdef _DENSE_REP
-Kernel::Kernel(int l, PREFIX(node) * x_, const svm_parameter& param)
+Kernel::Kernel(int l, PREFIX(node) * x_, const svm_parameter& param, BlasFunctions *blas_functions)
 #else
-Kernel::Kernel(int l, PREFIX(node) * const * x_, const svm_parameter& param)
+Kernel::Kernel(int l, PREFIX(node) * const * x_, const svm_parameter& param, BlasFunctions *blas_functions)
 #endif
 :kernel_type(param.kernel_type), degree(param.degree),
  gamma(param.gamma), coef0(param.coef0)
 {
+	m_blas = blas_functions;
 	switch(kernel_type)
 	{
 		case LINEAR:
@@ -376,7 +395,7 @@
 	{
 		x_square = new double[l];
 		for(int i=0;i<l;i++)
-			x_square[i] = dot(x[i],x[i]);
+			x_square[i] = dot(x[i],x[i],blas_functions);
 	}
 	else
 		x_square = 0;
@@ -389,27 +408,25 @@
 }

 #ifdef _DENSE_REP
-double Kernel::dot(const PREFIX(node) *px, const PREFIX(node) *py)
+double Kernel::dot(const PREFIX(node) *px, const PREFIX(node) *py, BlasFunctions *blas_functions)
 {
 	double sum = 0;

 	int dim = min(px->dim, py->dim);
-	for (int i = 0; i < dim; i++)
-		sum += (px->values)[i] * (py->values)[i];
+	sum = blas_functions->dot(dim, px->values, 1, py->values, 1);
 	return sum;
 }

-double Kernel::dot(const PREFIX(node) &px, const PREFIX(node) &py)
+double Kernel::dot(const PREFIX(node) &px, const PREFIX(node) &py, BlasFunctions *blas_functions)
 {
 	double sum = 0;

 	int dim = min(px.dim, py.dim);
-	for (int i = 0; i < dim; i++)
-		sum += px.values[i] * py.values[i];
+	sum = blas_functions->dot(dim, px.values, 1, py.values, 1);
 	return sum;
 }
 #else
-double Kernel::dot(const PREFIX(node) *px, const PREFIX(node) *py)
+double Kernel::dot(const PREFIX(node) *px, const PREFIX(node) *py, BlasFunctions *blas_functions)
 {
 	double sum = 0;
 	while(px->index != -1 && py->index != -1)
@@ -433,24 +450,26 @@
 #endif

 double Kernel::k_function(const PREFIX(node) *x, const PREFIX(node) *y,
-			  const svm_parameter& param)
+			  const svm_parameter& param, BlasFunctions *blas_functions)
 {
 	switch(param.kernel_type)
 	{
 		case LINEAR:
-			return dot(x,y);
+			return dot(x,y,blas_functions);
 		case POLY:
-			return powi(param.gamma*dot(x,y)+param.coef0,param.degree);
+			return powi(param.gamma*dot(x,y,blas_functions)+param.coef0,param.degree);
 		case RBF:
 		{
 			double sum = 0;
 #ifdef _DENSE_REP
 			int dim = min(x->dim, y->dim), i;
+			double* m_array = (double*)malloc(sizeof(double)*dim);
 			for (i = 0; i < dim; i++)
 			{
-				double d = x->values[i] - y->values[i];
-				sum += d*d;
-			}
+				m_array[i] = x->values[i] - y->values[i];
+			}
+			sum = blas_functions->dot(dim, m_array, 1, m_array, 1);
+			free(m_array);
 			for (; i < x->dim; i++)
 				sum += x->values[i] * x->values[i];
 			for (; i < y->dim; i++)
@@ -495,7 +514,7 @@
 			return exp(-param.gamma*sum);
 		}
 		case SIGMOID:
-			return tanh(param.gamma*dot(x,y)+param.coef0);
+			return tanh(param.gamma*dot(x,y,blas_functions)+param.coef0);
 		case PRECOMPUTED:  //x: test (validation), y: SV
                     {
 #ifdef _DENSE_REP
@@ -508,7 +527,6 @@
 			return 0;  // Unreachable
 	}
 }
-
 // An SMO algorithm in Fan et al., JMLR 6(2005), p. 1889--1918
 // Solves:
 //
@@ -539,6 +557,7 @@
                 double *upper_bound;
 		double r;	// for Solver_NU
                 bool solve_timed_out;
+		int n_iter;
 	};

 	void Solve(int l, const QMatrix& Q, const double *p_, const schar *y_,
@@ -905,6 +924,9 @@
 	for(int i=0;i<l;i++)
 		si->upper_bound[i] = C[i];

+	// store number of iterations
+	si->n_iter = iter;
+
 	info("\noptimization finished, #iter = %d\n",iter);

 	delete[] p;
@@ -923,7 +945,7 @@
 	// return i,j such that
 	// i: maximizes -y_i * grad(f)_i, i in I_up(\alpha)
 	// j: minimizes the decrease of obj value
-	//    (if quadratic coefficeint <= 0, replace it with tau)
+	//    (if quadratic coefficient <= 0, replace it with tau)
 	//    -y_j*grad(f)_j < -y_i*grad(f)_i, j in I_low(\alpha)

 	double Gmax = -INF;
@@ -1166,7 +1188,7 @@
 	// return i,j such that y_i = y_j and
 	// i: maximizes -y_i * grad(f)_i, i in I_up(\alpha)
 	// j: minimizes the decrease of obj value
-	//    (if quadratic coefficeint <= 0, replace it with tau)
+	//    (if quadratic coefficient <= 0, replace it with tau)
 	//    -y_j*grad(f)_j < -y_i*grad(f)_i, j in I_low(\alpha)

 	double Gmaxp = -INF;
@@ -1401,8 +1423,8 @@
 class SVC_Q: public Kernel
 {
 public:
-	SVC_Q(const PREFIX(problem)& prob, const svm_parameter& param, const schar *y_)
-	:Kernel(prob.l, prob.x, param)
+	SVC_Q(const PREFIX(problem)& prob, const svm_parameter& param, const schar *y_, BlasFunctions *blas_functions)
+	:Kernel(prob.l, prob.x, param, blas_functions)
 	{
 		clone(y,y_,prob.l);
 		cache = new Cache(prob.l,(long int)(param.cache_size*(1<<20)));
@@ -1451,8 +1473,8 @@
 class ONE_CLASS_Q: public Kernel
 {
 public:
-	ONE_CLASS_Q(const PREFIX(problem)& prob, const svm_parameter& param)
-	:Kernel(prob.l, prob.x, param)
+	ONE_CLASS_Q(const PREFIX(problem)& prob, const svm_parameter& param, BlasFunctions *blas_functions)
+	:Kernel(prob.l, prob.x, param, blas_functions)
 	{
 		cache = new Cache(prob.l,(long int)(param.cache_size*(1<<20)));
 		QD = new double[prob.l];
@@ -1497,8 +1519,8 @@
 class SVR_Q: public Kernel
 {
 public:
-	SVR_Q(const PREFIX(problem)& prob, const svm_parameter& param)
-	:Kernel(prob.l, prob.x, param)
+	SVR_Q(const PREFIX(problem)& prob, const svm_parameter& param, BlasFunctions *blas_functions)
+	:Kernel(prob.l, prob.x, param, blas_functions)
 	{
 		l = prob.l;
 		cache = new Cache(l,(long int)(param.cache_size*(1<<20)));
@@ -1574,7 +1596,7 @@
 //
 static void solve_c_svc(
 	const PREFIX(problem) *prob, const svm_parameter* param,
-	double *alpha, Solver::SolutionInfo* si, double Cp, double Cn)
+	double *alpha, Solver::SolutionInfo* si, double Cp, double Cn, BlasFunctions *blas_functions)
 {
 	int l = prob->l;
 	double *minus_ones = new double[l];
@@ -1600,7 +1622,7 @@
 	}

 	Solver s;
-	s.Solve(l, SVC_Q(*prob,*param,y), minus_ones, y,
+	s.Solve(l, SVC_Q(*prob,*param,y, blas_functions), minus_ones, y,
 		alpha, C, param->eps, si, param->shrinking,
                 param->max_iter);

@@ -1623,7 +1645,7 @@

 static void solve_nu_svc(
 	const PREFIX(problem) *prob, const svm_parameter *param,
-	double *alpha, Solver::SolutionInfo* si)
+	double *alpha, Solver::SolutionInfo* si, BlasFunctions *blas_functions)
 {
 	int i;
 	int l = prob->l;
@@ -1665,7 +1687,7 @@
 		zeros[i] = 0;

 	Solver_NU s;
-	s.Solve(l, SVC_Q(*prob,*param,y), zeros, y,
+	s.Solve(l, SVC_Q(*prob,*param,y,blas_functions), zeros, y,
 		alpha, C, param->eps, si,  param->shrinking, param->max_iter);
 	double r = si->r;

@@ -1687,7 +1709,7 @@

 static void solve_one_class(
 	const PREFIX(problem) *prob, const svm_parameter *param,
-	double *alpha, Solver::SolutionInfo* si)
+	double *alpha, Solver::SolutionInfo* si, BlasFunctions *blas_functions)
 {
 	int l = prob->l;
 	double *zeros = new double[l];
@@ -1720,7 +1742,7 @@
 	}

 	Solver s;
-	s.Solve(l, ONE_CLASS_Q(*prob,*param), zeros, ones,
+	s.Solve(l, ONE_CLASS_Q(*prob,*param,blas_functions), zeros, ones,
 		alpha, C, param->eps, si, param->shrinking, param->max_iter);

         delete[] C;
@@ -1730,7 +1752,7 @@

 static void solve_epsilon_svr(
 	const PREFIX(problem) *prob, const svm_parameter *param,
-	double *alpha, Solver::SolutionInfo* si)
+	double *alpha, Solver::SolutionInfo* si, BlasFunctions *blas_functions)
 {
 	int l = prob->l;
 	double *alpha2 = new double[2*l];
@@ -1753,7 +1775,7 @@
 	}

 	Solver s;
-	s.Solve(2*l, SVR_Q(*prob,*param), linear_term, y,
+	s.Solve(2*l, SVR_Q(*prob,*param,blas_functions), linear_term, y,
 		alpha2, C, param->eps, si, param->shrinking, param->max_iter);

 	double sum_alpha = 0;
@@ -1772,7 +1794,7 @@

 static void solve_nu_svr(
 	const PREFIX(problem) *prob, const svm_parameter *param,
-	double *alpha, Solver::SolutionInfo* si)
+	double *alpha, Solver::SolutionInfo* si, BlasFunctions *blas_functions)
 {
 	int l = prob->l;
 	double *C = new double[2*l];
@@ -1802,7 +1824,7 @@
 	}

 	Solver_NU s;
-	s.Solve(2*l, SVR_Q(*prob,*param), linear_term, y,
+	s.Solve(2*l, SVR_Q(*prob,*param,blas_functions), linear_term, y,
 		alpha2, C, param->eps, si, param->shrinking, param->max_iter);

 	info("epsilon = %f\n",-si->r);
@@ -1823,11 +1845,12 @@
 {
 	double *alpha;
 	double rho;
+	int n_iter;
 };

 static decision_function svm_train_one(
 	const PREFIX(problem) *prob, const svm_parameter *param,
-	double Cp, double Cn, int *status)
+	double Cp, double Cn, int *status, BlasFunctions *blas_functions)
 {
 	double *alpha = Malloc(double,prob->l);
 	Solver::SolutionInfo si;
@@ -1835,23 +1858,23 @@
 	{
  		case C_SVC:
 			si.upper_bound = Malloc(double,prob->l);
- 			solve_c_svc(prob,param,alpha,&si,Cp,Cn);
+ 			solve_c_svc(prob,param,alpha,&si,Cp,Cn,blas_functions);
  			break;
  		case NU_SVC:
 			si.upper_bound = Malloc(double,prob->l);
- 			solve_nu_svc(prob,param,alpha,&si);
+ 			solve_nu_svc(prob,param,alpha,&si,blas_functions);
  			break;
  		case ONE_CLASS:
 			si.upper_bound = Malloc(double,prob->l);
- 			solve_one_class(prob,param,alpha,&si);
+ 			solve_one_class(prob,param,alpha,&si,blas_functions);
  			break;
  		case EPSILON_SVR:
 			si.upper_bound = Malloc(double,2*prob->l);
- 			solve_epsilon_svr(prob,param,alpha,&si);
+ 			solve_epsilon_svr(prob,param,alpha,&si,blas_functions);
  			break;
  		case NU_SVR:
 			si.upper_bound = Malloc(double,2*prob->l);
- 			solve_nu_svr(prob,param,alpha,&si);
+ 			solve_nu_svr(prob,param,alpha,&si,blas_functions);
  			break;
 	}

@@ -1888,6 +1911,7 @@
 	decision_function f;
 	f.alpha = alpha;
 	f.rho = si.rho;
+	f.n_iter = si.n_iter;
 	return f;
 }

@@ -2082,7 +2106,7 @@
 // Cross-validation decision values for probability estimates
 static void svm_binary_svc_probability(
 	const PREFIX(problem) *prob, const svm_parameter *param,
-	double Cp, double Cn, double& probA, double& probB, int * status)
+	double Cp, double Cn, double& probA, double& probB, int * status, BlasFunctions *blas_functions)
 {
 	int i;
 	int nr_fold = 5;
@@ -2093,7 +2117,7 @@
 	for(i=0;i<prob->l;i++) perm[i]=i;
 	for(i=0;i<prob->l;i++)
 	{
-		int j = i+rand()%(prob->l-i);
+		int j = i+bounded_rand_int(prob->l-i);
 		swap(perm[i],perm[j]);
 	}
 	for(i=0;i<nr_fold;i++)
@@ -2155,13 +2179,13 @@
 			subparam.weight_label[1]=-1;
 			subparam.weight[0]=Cp;
 			subparam.weight[1]=Cn;
-			struct PREFIX(model) *submodel = PREFIX(train)(&subprob,&subparam, status);
+			struct PREFIX(model) *submodel = PREFIX(train)(&subprob,&subparam, status, blas_functions);
 			for(j=begin;j<end;j++)
 			{
 #ifdef _DENSE_REP
-                                PREFIX(predict_values)(submodel,(prob->x+perm[j]),&(dec_values[perm[j]]));
+                                PREFIX(predict_values)(submodel,(prob->x+perm[j]),&(dec_values[perm[j]]), blas_functions);
 #else
-				PREFIX(predict_values)(submodel,prob->x[perm[j]],&(dec_values[perm[j]]));
+				PREFIX(predict_values)(submodel,prob->x[perm[j]],&(dec_values[perm[j]]), blas_functions);
 #endif
 				// ensure +1 -1 order; reason not using CV subroutine
 				dec_values[perm[j]] *= submodel->label[0];
@@ -2180,7 +2204,7 @@

 // Return parameter of a Laplace distribution
 static double svm_svr_probability(
-	const PREFIX(problem) *prob, const svm_parameter *param)
+	const PREFIX(problem) *prob, const svm_parameter *param, BlasFunctions *blas_functions)
 {
 	int i;
 	int nr_fold = 5;
@@ -2191,7 +2215,7 @@
 	newparam.probability = 0;
     newparam.random_seed = -1; // This is called from train, which already sets
                                // the seed.
-	PREFIX(cross_validation)(prob,&newparam,nr_fold,ymv);
+	PREFIX(cross_validation)(prob,&newparam,nr_fold,ymv, blas_functions);
 	for(i=0;i<prob->l;i++)
 	{
 		ymv[i]=prob->y[i]-ymv[i];
@@ -2336,7 +2360,7 @@
 // Interface functions
 //
 PREFIX(model) *PREFIX(train)(const PREFIX(problem) *prob, const svm_parameter *param,
-        int *status)
+        int *status, BlasFunctions *blas_functions)
 {
 	PREFIX(problem) newprob;
 	remove_zero_weight(&newprob, prob);
@@ -2348,7 +2372,7 @@

     if(param->random_seed >= 0)
     {
-        srand(param->random_seed);
+        set_seed(param->random_seed);
     }

 	if(param->svm_type == ONE_CLASS ||
@@ -2367,12 +2391,14 @@
 		    param->svm_type == NU_SVR))
 		{
 			model->probA = Malloc(double,1);
-			model->probA[0] = NAMESPACE::svm_svr_probability(prob,param);
-		}
-
-                NAMESPACE::decision_function f = NAMESPACE::svm_train_one(prob,param,0,0, status);
+			model->probA[0] = NAMESPACE::svm_svr_probability(prob,param,blas_functions);
+		}
+
+                NAMESPACE::decision_function f = NAMESPACE::svm_train_one(prob,param,0,0, status,blas_functions);
 		model->rho = Malloc(double,1);
 		model->rho[0] = f.rho;
+		model->n_iter = Malloc(int,1);
+		model->n_iter[0] = f.n_iter;

 		int nSV = 0;
 		int i;
@@ -2485,9 +2511,9 @@
 				}

 				if(param->probability)
-                                    NAMESPACE::svm_binary_svc_probability(&sub_prob,param,weighted_C[i],weighted_C[j],probA[p],probB[p], status);
-
-				f[p] = NAMESPACE::svm_train_one(&sub_prob,param,weighted_C[i],weighted_C[j], status);
+                                    NAMESPACE::svm_binary_svc_probability(&sub_prob,param,weighted_C[i],weighted_C[j],probA[p],probB[p], status, blas_functions);
+
+				f[p] = NAMESPACE::svm_train_one(&sub_prob,param,weighted_C[i],weighted_C[j], status, blas_functions);
 				for(k=0;k<ci;k++)
 					if(!nonzero[si+k] && fabs(f[p].alpha[k]) > 0)
 						nonzero[si+k] = true;
@@ -2509,8 +2535,12 @@
 			model->label[i] = label[i];

 		model->rho = Malloc(double,nr_class*(nr_class-1)/2);
+		model->n_iter = Malloc(int,nr_class*(nr_class-1)/2);
 		for(i=0;i<nr_class*(nr_class-1)/2;i++)
+		{
 			model->rho[i] = f[i].rho;
+			model->n_iter[i] = f[i].n_iter;
+		}

 		if(param->probability)
 		{
@@ -2619,7 +2649,7 @@
 }

 // Stratified cross validation
-void PREFIX(cross_validation)(const PREFIX(problem) *prob, const svm_parameter *param, int nr_fold, double *target)
+void PREFIX(cross_validation)(const PREFIX(problem) *prob, const svm_parameter *param, int nr_fold, double *target, BlasFunctions *blas_functions)
 {
 	int i;
 	int *fold_start = Malloc(int,nr_fold+1);
@@ -2628,7 +2658,7 @@
 	int nr_class;
     if(param->random_seed >= 0)
     {
-        srand(param->random_seed);
+        set_seed(param->random_seed);
     }

 	// stratified cv may not give leave-one-out rate
@@ -2650,7 +2680,7 @@
 		for (c=0; c<nr_class; c++)
 			for(i=0;i<count[c];i++)
 			{
-				int j = i+rand()%(count[c]-i);
+				int j = i+bounded_rand_int(count[c]-i);
 				swap(index[start[c]+j],index[start[c]+i]);
 			}
 		for(i=0;i<nr_fold;i++)
@@ -2687,7 +2717,7 @@
 		for(i=0;i<l;i++) perm[i]=i;
 		for(i=0;i<l;i++)
 		{
-			int j = i+rand()%(l-i);
+			int j = i+bounded_rand_int(l-i);
 			swap(perm[i],perm[j]);
 		}
 		for(i=0;i<=nr_fold;i++)
@@ -2726,25 +2756,25 @@
 			++k;
 		}
                 int dummy_status = 0; // IGNORES TIMEOUT ERRORS
-		struct PREFIX(model) *submodel = PREFIX(train)(&subprob,param, &dummy_status);
+		struct PREFIX(model) *submodel = PREFIX(train)(&subprob,param, &dummy_status, blas_functions);
 		if(param->probability &&
 		   (param->svm_type == C_SVC || param->svm_type == NU_SVC))
 		{
 			double *prob_estimates=Malloc(double, PREFIX(get_nr_class)(submodel));
 			for(j=begin;j<end;j++)
 #ifdef _DENSE_REP
-				target[perm[j]] = PREFIX(predict_probability)(submodel,(prob->x + perm[j]),prob_estimates);
+				target[perm[j]] = PREFIX(predict_probability)(submodel,(prob->x + perm[j]),prob_estimates, blas_functions);
 #else
-                                target[perm[j]] = PREFIX(predict_probability)(submodel,prob->x[perm[j]],prob_estimates);
+                                target[perm[j]] = PREFIX(predict_probability)(submodel,prob->x[perm[j]],prob_estimates, blas_functions);
 #endif
 			free(prob_estimates);
 		}
 		else
 			for(j=begin;j<end;j++)
 #ifdef _DENSE_REP
-				target[perm[j]] = PREFIX(predict)(submodel,prob->x+perm[j]);
+				target[perm[j]] = PREFIX(predict)(submodel,prob->x+perm[j],blas_functions);
 #else
-                target[perm[j]] = PREFIX(predict)(submodel,prob->x[perm[j]]);
+                target[perm[j]] = PREFIX(predict)(submodel,prob->x[perm[j]],blas_functions);
 #endif
 		PREFIX(free_and_destroy_model)(&submodel);
 		free(subprob.x);
@@ -2785,7 +2815,7 @@
 	}
 }

-double PREFIX(predict_values)(const PREFIX(model) *model, const PREFIX(node) *x, double* dec_values)
+double PREFIX(predict_values)(const PREFIX(model) *model, const PREFIX(node) *x, double* dec_values, BlasFunctions *blas_functions)
 {
 	int i;
 	if(model->param.svm_type == ONE_CLASS ||
@@ -2797,9 +2827,9 @@

 		for(i=0;i<model->l;i++)
 #ifdef _DENSE_REP
-                    sum += sv_coef[i] * NAMESPACE::Kernel::k_function(x,model->SV+i,model->param);
+                    sum += sv_coef[i] * NAMESPACE::Kernel::k_function(x,model->SV+i,model->param,blas_functions);
 #else
-                sum += sv_coef[i] * NAMESPACE::Kernel::k_function(x,model->SV[i],model->param);
+                sum += sv_coef[i] * NAMESPACE::Kernel::k_function(x,model->SV[i],model->param,blas_functions);
 #endif
 		sum -= model->rho[0];
 		*dec_values = sum;
@@ -2817,9 +2847,9 @@
 		double *kvalue = Malloc(double,l);
 		for(i=0;i<l;i++)
 #ifdef _DENSE_REP
-                    kvalue[i] = NAMESPACE::Kernel::k_function(x,model->SV+i,model->param);
+                    kvalue[i] = NAMESPACE::Kernel::k_function(x,model->SV+i,model->param,blas_functions);
 #else
-                kvalue[i] = NAMESPACE::Kernel::k_function(x,model->SV[i],model->param);
+                kvalue[i] = NAMESPACE::Kernel::k_function(x,model->SV[i],model->param,blas_functions);
 #endif

 		int *start = Malloc(int,nr_class);
@@ -2870,7 +2900,7 @@
 	}
 }

-double PREFIX(predict)(const PREFIX(model) *model, const PREFIX(node) *x)
+double PREFIX(predict)(const PREFIX(model) *model, const PREFIX(node) *x, BlasFunctions *blas_functions)
 {
 	int nr_class = model->nr_class;
 	double *dec_values;
@@ -2880,13 +2910,13 @@
 		dec_values = Malloc(double, 1);
 	else
 		dec_values = Malloc(double, nr_class*(nr_class-1)/2);
-	double pred_result = PREFIX(predict_values)(model, x, dec_values);
+	double pred_result = PREFIX(predict_values)(model, x, dec_values, blas_functions);
 	free(dec_values);
 	return pred_result;
 }

 double PREFIX(predict_probability)(
-	const PREFIX(model) *model, const PREFIX(node) *x, double *prob_estimates)
+	const PREFIX(model) *model, const PREFIX(node) *x, double *prob_estimates, BlasFunctions *blas_functions)
 {
 	if ((model->param.svm_type == C_SVC || model->param.svm_type == NU_SVC) &&
 	    model->probA!=NULL && model->probB!=NULL)
@@ -2894,7 +2924,7 @@
 		int i;
 		int nr_class = model->nr_class;
 		double *dec_values = Malloc(double, nr_class*(nr_class-1)/2);
-		PREFIX(predict_values)(model, x, dec_values);
+		PREFIX(predict_values)(model, x, dec_values, blas_functions);

 		double min_prob=1e-7;
 		double **pairwise_prob=Malloc(double *,nr_class);
@@ -2921,7 +2951,7 @@
 		return model->label[prob_max_idx];
 	}
 	else
-		return PREFIX(predict)(model, x);
+		return PREFIX(predict)(model, x, blas_functions);
 }


@@ -2964,6 +2994,9 @@

 	free(model_ptr->nSV);
 	model_ptr->nSV = NULL;
+
+	free(model_ptr->n_iter);
+	model_ptr->n_iter = NULL;
 }

 void PREFIX(free_and_destroy_model)(PREFIX(model)** model_ptr_ptr)
@@ -3101,6 +3134,42 @@
 		free(count);
 	}

+	if(svm_type == C_SVC ||
+	   svm_type == EPSILON_SVR ||
+	   svm_type == NU_SVR ||
+	   svm_type == ONE_CLASS)
+	{
+		PREFIX(problem) newprob;
+		// filter samples with negative and null weights
+		remove_zero_weight(&newprob, prob);
+
+		char* msg = NULL;
+		// all samples were removed
+		if(newprob.l == 0)
+			msg =  "Invalid input - all samples have zero or negative weights.";
+		else if(prob->l != newprob.l &&
+		        svm_type == C_SVC)
+		{
+			bool only_one_label = true;
+			int first_label = newprob.y[0];
+			for(int i=1;i<newprob.l;i++)
+			{
+				if(newprob.y[i] != first_label)
+				{
+					only_one_label = false;
+					break;
+				}
+			}
+			if(only_one_label == true)
+				msg = "Invalid input - all samples with positive weights have the same label.";
+		}
+
+		free(newprob.x);
+		free(newprob.y);
+		free(newprob.W);
+		if(msg != NULL)
+			return msg;
+	}
 	return NULL;
 }

('sklearn/manifold', '_utils.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,10 +1,12 @@
-# cython: boundscheck=False
-
 from libc cimport math
 cimport cython
 import numpy as np
 cimport numpy as np
 from libc.stdio cimport printf
+
+np.import_array()
+
+
 cdef extern from "numpy/npy_math.h":
     float NPY_INFINITY

@@ -13,24 +15,21 @@
 cdef float PERPLEXITY_TOLERANCE = 1e-5

 cpdef np.ndarray[np.float32_t, ndim=2] _binary_search_perplexity(
-        np.ndarray[np.float32_t, ndim=2] affinities,
-        np.ndarray[np.int64_t, ndim=2] neighbors,
+        np.ndarray[np.float32_t, ndim=2] sqdistances,
         float desired_perplexity,
         int verbose):
     """Binary search for sigmas of conditional Gaussians.

     This approximation reduces the computational complexity from O(N^2) to
-    O(uN). See the exact method '_binary_search_perplexity' for more details.
+    O(uN).

     Parameters
     ----------
-    affinities : array-like, shape (n_samples, k)
-        Distances between training samples and its k nearest neighbors.
-
-    neighbors : array-like, shape (n_samples, k) or None
-        Each row contains the indices to the k nearest neigbors. If this
-        array is None, then the perplexity is estimated over all data
-        not just the nearest neighbors.
+    sqdistances : array-like, shape (n_samples, n_neighbors)
+        Distances between training samples and their k nearest neighbors.
+        When using the exact method, this is a square (n_samples, n_samples)
+        distance matrix. The TSNE default metric is "euclidean" which is
+        interpreted as squared euclidean distance.

     desired_perplexity : float
         Desired perplexity (2^entropy) of the conditional Gaussians.
@@ -46,26 +45,23 @@
     # Maximum number of binary search steps
     cdef long n_steps = 100

-    cdef long n_samples = affinities.shape[0]
+    cdef long n_samples = sqdistances.shape[0]
+    cdef long n_neighbors = sqdistances.shape[1]
+    cdef int using_neighbors = n_neighbors < n_samples
     # Precisions of conditional Gaussian distributions
-    cdef float beta
-    cdef float beta_min
-    cdef float beta_max
-    cdef float beta_sum = 0.0
+    cdef double beta
+    cdef double beta_min
+    cdef double beta_max
+    cdef double beta_sum = 0.0

     # Use log scale
-    cdef float desired_entropy = math.log(desired_perplexity)
-    cdef float entropy_diff
+    cdef double desired_entropy = math.log(desired_perplexity)
+    cdef double entropy_diff

-    cdef float entropy
-    cdef float sum_Pi
-    cdef float sum_disti_Pi
+    cdef double entropy
+    cdef double sum_Pi
+    cdef double sum_disti_Pi
     cdef long i, j, k, l
-    cdef long n_neighbors = n_samples
-    cdef int using_neighbors = neighbors is not None
-
-    if using_neighbors:
-        n_neighbors = neighbors.shape[1]

     # This array is later used as a 32bit array. It has multiple intermediate
     # floating point additions that benefit from the extra precision
@@ -85,7 +81,7 @@
             sum_Pi = 0.0
             for j in range(n_neighbors):
                 if j != i or using_neighbors:
-                    P[i, j] = math.exp(-affinities[i, j] * beta)
+                    P[i, j] = math.exp(-sqdistances[i, j] * beta)
                     sum_Pi += P[i, j]

             if sum_Pi == 0.0:
@@ -94,7 +90,7 @@

             for j in range(n_neighbors):
                 P[i, j] /= sum_Pi
-                sum_disti_Pi += affinities[i, j] * P[i, j]
+                sum_disti_Pi += sqdistances[i, j] * P[i, j]

             entropy = math.log(sum_Pi) + beta * sum_disti_Pi
             entropy_diff = entropy - desired_entropy
('sklearn/manifold', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -2,11 +2,20 @@
 The :mod:`sklearn.manifold` module implements data embedding techniques.
 """

-from .locally_linear import locally_linear_embedding, LocallyLinearEmbedding
-from .isomap import Isomap
-from .mds import MDS, smacof
-from .spectral_embedding_ import SpectralEmbedding, spectral_embedding
-from .t_sne import TSNE
+from ._locally_linear import locally_linear_embedding, LocallyLinearEmbedding
+from ._isomap import Isomap
+from ._mds import MDS, smacof
+from ._spectral_embedding import SpectralEmbedding, spectral_embedding
+from ._t_sne import TSNE, trustworthiness

-__all__ = ['locally_linear_embedding', 'LocallyLinearEmbedding', 'Isomap',
-           'MDS', 'smacof', 'SpectralEmbedding', 'spectral_embedding', "TSNE"]
+__all__ = [
+    "locally_linear_embedding",
+    "LocallyLinearEmbedding",
+    "Isomap",
+    "MDS",
+    "smacof",
+    "SpectralEmbedding",
+    "spectral_embedding",
+    "TSNE",
+    "trustworthiness",
+]
('sklearn/manifold', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -9,26 +9,31 @@
     config = Configuration("manifold", parent_package, top_path)

     libraries = []
-    if os.name == 'posix':
-        libraries.append('m')
+    if os.name == "posix":
+        libraries.append("m")

-    config.add_extension("_utils",
-                         sources=["_utils.pyx"],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries,
-                         extra_compile_args=["-O3"])
+    config.add_extension(
+        "_utils",
+        sources=["_utils.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+        extra_compile_args=["-O3"],
+    )

-    config.add_extension("_barnes_hut_tsne",
-                         sources=["_barnes_hut_tsne.pyx"],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries,
-                         extra_compile_args=['-O3'])
+    config.add_extension(
+        "_barnes_hut_tsne",
+        sources=["_barnes_hut_tsne.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+        extra_compile_args=["-O3"],
+    )

-    config.add_subpackage('tests')
+    config.add_subpackage("tests")

     return config


 if __name__ == "__main__":
     from numpy.distutils.core import setup
+
     setup(**configuration().todict())
('sklearn/manifold', '_barnes_hut_tsne.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,7 +1,3 @@
-# cython: boundscheck=False
-# cython: wraparound=False
-# cython: cdivision=True
-#
 # Author: Christopher Moody <chrisemoody@gmail.com>
 # Author: Nick Travers <nickt@squareup.com>
 # Implementation by Chris Moody & Nick Travers
@@ -9,13 +5,17 @@
 # implementations and papers describing the technique


-from libc.stdlib cimport malloc, free
+import numpy as np
+cimport numpy as np
 from libc.stdio cimport printf
 from libc.math cimport sqrt, log
-import numpy as np
-cimport numpy as np
-
-from ..neighbors.quad_tree cimport _QuadTree
+from libc.stdlib cimport malloc, free
+from cython.parallel cimport prange, parallel
+
+from ..neighbors._quad_tree cimport _QuadTree
+
+np.import_array()
+

 cdef char* EMPTY_STRING = ""

@@ -53,7 +53,8 @@
                             int dof,
                             long start,
                             long stop,
-                            bint compute_error) nogil:
+                            bint compute_error,
+                            int num_threads) nogil:
     # Having created the tree, calculate the gradient
     # in two components, the positive and negative forces
     cdef:
@@ -61,9 +62,9 @@
         int ax
         long n_samples = pos_reference.shape[0]
         int n_dimensions = qt.n_dimensions
-        double[1] sum_Q
         clock_t t1 = 0, t2 = 0
-        float sQ, error
+        double sQ
+        float error
         int take_timing = 1 if qt.verbose > 15 else 0

     if qt.verbose > 11:
@@ -72,25 +73,25 @@
     cdef float* neg_f = <float*> malloc(sizeof(float) * n_samples * n_dimensions)
     cdef float* pos_f = <float*> malloc(sizeof(float) * n_samples * n_dimensions)

-    sum_Q[0] = 0.0
     if take_timing:
         t1 = clock()
-    compute_gradient_negative(pos_reference, neg_f, qt, sum_Q,
-                              dof, theta, start, stop)
+    sQ = compute_gradient_negative(pos_reference, neg_f, qt, dof, theta, start,
+                                   stop, num_threads)
     if take_timing:
         t2 = clock()
         printf("[t-SNE] Computing negative gradient: %e ticks\n", ((float) (t2 - t1)))
-    sQ = sum_Q[0]

     if take_timing:
         t1 = clock()
     error = compute_gradient_positive(val_P, pos_reference, neighbors, indptr,
                                       pos_f, n_dimensions, dof, sQ, start,
-                                      qt.verbose, compute_error)
+                                      qt.verbose, compute_error, num_threads)
     if take_timing:
         t2 = clock()
-        printf("[t-SNE] Computing positive gradient: %e ticks\n", ((float) (t2 - t1)))
-    for i in range(start, n_samples):
+        printf("[t-SNE] Computing positive gradient: %e ticks\n",
+               ((float) (t2 - t1)))
+    for i in prange(start, n_samples, nogil=True, num_threads=num_threads,
+                    schedule='static'):
         for ax in range(n_dimensions):
             coord = i * n_dimensions + ax
             tot_force[i, ax] = pos_f[coord] - (neg_f[coord] / sQ)
@@ -110,7 +111,8 @@
                                      double sum_Q,
                                      np.int64_t start,
                                      int verbose,
-                                     bint compute_error) nogil:
+                                     bint compute_error,
+                                     int num_threads) nogil:
     # Sum over the following expression for i not equal to j
     # grad_i = p_ij (1 + ||y_i - y_j||^2)^-1 (y_i - y_j)
     # This is equivalent to compute_edge_forces in the authors' code
@@ -120,39 +122,47 @@
         int ax
         long i, j, k
         long n_samples = indptr.shape[0] - 1
+        float C = 0.0
         float dij, qij, pij
-        float C = 0.0
         float exponent = (dof + 1.0) / 2.0
         float float_dof = (float) (dof)
-        float[3] buff
+        float* buff
         clock_t t1 = 0, t2 = 0
         float dt

     if verbose > 10:
         t1 = clock()
-    for i in range(start, n_samples):
-        # Init the gradient vector
-        for ax in range(n_dimensions):
-            pos_f[i * n_dimensions + ax] = 0.0
-        # Compute the positive interaction for the nearest neighbors
-        for k in range(indptr[i], indptr[i+1]):
-            j = neighbors[k]
-            dij = 0.0
-            pij = val_P[k]
+
+    with nogil, parallel(num_threads=num_threads):
+        # Define private buffer variables
+        buff = <float *> malloc(sizeof(float) * n_dimensions)
+
+        for i in prange(start, n_samples, schedule='static'):
+            # Init the gradient vector
             for ax in range(n_dimensions):
-                buff[ax] = pos_reference[i, ax] - pos_reference[j, ax]
-                dij += buff[ax] * buff[ax]
-            qij = float_dof / (float_dof + dij)
-            if dof != 1:  # i.e. exponent != 1
-                qij **= exponent
-            dij = pij * qij
-
-            # only compute the error when needed
-            if compute_error:
-                qij /= sum_Q
-                C += pij * log(max(pij, FLOAT32_TINY) / max(qij, FLOAT32_TINY))
-            for ax in range(n_dimensions):
-                pos_f[i * n_dimensions + ax] += dij * buff[ax]
+                pos_f[i * n_dimensions + ax] = 0.0
+            # Compute the positive interaction for the nearest neighbors
+            for k in range(indptr[i], indptr[i+1]):
+                j = neighbors[k]
+                dij = 0.0
+                pij = val_P[k]
+                for ax in range(n_dimensions):
+                    buff[ax] = pos_reference[i, ax] - pos_reference[j, ax]
+                    dij += buff[ax] * buff[ax]
+                qij = float_dof / (float_dof + dij)
+                if dof != 1:  # i.e. exponent != 1
+                    qij = qij ** exponent
+                dij = pij * qij
+
+                # only compute the error when needed
+                if compute_error:
+                    qij = qij / sum_Q
+                    C += pij * log(max(pij, FLOAT32_TINY) \
+                        / max(qij, FLOAT32_TINY))
+                for ax in range(n_dimensions):
+                    pos_f[i * n_dimensions + ax] += dij * buff[ax]
+
+        free(buff)
     if verbose > 10:
         t2 = clock()
         dt = ((float) (t2 - t1))
@@ -160,78 +170,90 @@
     return C


-cdef void compute_gradient_negative(float[:, :] pos_reference,
-                                    float* neg_f,
-                                    _QuadTree qt,
-                                    double* sum_Q,
-                                    int dof,
-                                    float theta,
-                                    long start,
-                                    long stop) nogil:
+cdef double compute_gradient_negative(float[:, :] pos_reference,
+                                      float* neg_f,
+                                      _QuadTree qt,
+                                      int dof,
+                                      float theta,
+                                      long start,
+                                      long stop,
+                                      int num_threads) nogil:
     if stop == -1:
         stop = pos_reference.shape[0]
     cdef:
         int ax
         int n_dimensions = qt.n_dimensions
+        int offset = n_dimensions + 2
         long i, j, idx
         long n = stop - start
         long dta = 0
         long dtb = 0
-        long offset = n_dimensions + 2
         float size, dist2s, mult
         float exponent = (dof + 1.0) / 2.0
         float float_dof = (float) (dof)
-        double qijZ
-        float[1] iQ
-        float[3] force, neg_force, pos
+        double qijZ, sum_Q = 0.0
+        float* force
+        float* neg_force
+        float* pos
         clock_t t1 = 0, t2 = 0, t3 = 0
         int take_timing = 1 if qt.verbose > 20 else 0

-    summary = <float*> malloc(sizeof(float) * n * offset)
-
-    for i in range(start, stop):
-        # Clear the arrays
-        for ax in range(n_dimensions):
-            force[ax] = 0.0
-            neg_force[ax] = 0.0
-            pos[ax] = pos_reference[i, ax]
-        iQ[0] = 0.0
-        # Find which nodes are summarizing and collect their centers of mass
-        # deltas, and sizes, into vectorized arrays
-        if take_timing:
-            t1 = clock()
-        idx = qt.summarize(pos, summary, theta*theta)
-        if take_timing:
-            t2 = clock()
-        # Compute the t-SNE negative force
-        # for the digits dataset, walking the tree
-        # is about 10-15x more expensive than the
-        # following for loop
-        for j in range(idx // offset):
-
-            dist2s = summary[j * offset + n_dimensions]
-            size = summary[j * offset + n_dimensions + 1]
-            qijZ = float_dof / (float_dof + dist2s)  # 1/(1+dist)
-            if dof != 1:  # i.e. exponent != 1
-                qijZ **= exponent
-            sum_Q[0] += size * qijZ   # size of the node * q
-            mult = size * qijZ * qijZ
+
+    with nogil, parallel(num_threads=num_threads):
+        # Define thread-local buffers
+        summary = <float*> malloc(sizeof(float) * n * offset)
+        pos = <float *> malloc(sizeof(float) * n_dimensions)
+        force = <float *> malloc(sizeof(float) * n_dimensions)
+        neg_force = <float *> malloc(sizeof(float) * n_dimensions)
+
+        for i in prange(start, stop, schedule='static'):
+            # Clear the arrays
             for ax in range(n_dimensions):
-                neg_force[ax] += mult * summary[j * offset + ax]
-        if take_timing:
-            t3 = clock()
-        for ax in range(n_dimensions):
-            neg_f[i * n_dimensions + ax] = neg_force[ax]
-        if take_timing:
-            dta += t2 - t1
-            dtb += t3 - t2
+                force[ax] = 0.0
+                neg_force[ax] = 0.0
+                pos[ax] = pos_reference[i, ax]
+
+            # Find which nodes are summarizing and collect their centers of mass
+            # deltas, and sizes, into vectorized arrays
+            if take_timing:
+                t1 = clock()
+            idx = qt.summarize(pos, summary, theta*theta)
+            if take_timing:
+                t2 = clock()
+            # Compute the t-SNE negative force
+            # for the digits dataset, walking the tree
+            # is about 10-15x more expensive than the
+            # following for loop
+            for j in range(idx // offset):
+
+                dist2s = summary[j * offset + n_dimensions]
+                size = summary[j * offset + n_dimensions + 1]
+                qijZ = float_dof / (float_dof + dist2s)  # 1/(1+dist)
+                if dof != 1:  # i.e. exponent != 1
+                    qijZ = qijZ ** exponent
+
+                sum_Q += size * qijZ   # size of the node * q
+                mult = size * qijZ * qijZ
+                for ax in range(n_dimensions):
+                    neg_force[ax] += mult * summary[j * offset + ax]
+            if take_timing:
+                t3 = clock()
+            for ax in range(n_dimensions):
+                neg_f[i * n_dimensions + ax] = neg_force[ax]
+            if take_timing:
+                dta += t2 - t1
+                dtb += t3 - t2
+        free(pos)
+        free(force)
+        free(neg_force)
+        free(summary)
     if take_timing:
         printf("[t-SNE] Tree: %li clock ticks | ", dta)
         printf("Force computation: %li clock ticks\n", dtb)

     # Put sum_Q to machine EPSILON to avoid divisions by 0
-    sum_Q[0] = max(sum_Q[0], FLOAT64_EPS)
-    free(summary)
+    sum_Q = max(sum_Q, FLOAT64_EPS)
+    return sum_Q


 def gradient(float[:] val_P,
@@ -244,9 +266,10 @@
              int verbose,
              int dof=1,
              long skip_num_points=0,
-             bint compute_error=1):
+             bint compute_error=1,
+             int num_threads=1):
     # This function is designed to be called from external Python
-    # it passes the 'forces' array by reference and fills thats array
+    # it passes the 'forces' array by reference and fills that's array
     # up in-place
     cdef float C
     cdef int n
@@ -269,8 +292,11 @@
         # in the generated C code that triggers error with gcc 4.9
         # and -Werror=format-security
         printf("[t-SNE] Computing gradient\n%s", EMPTY_STRING)
+
     C = compute_gradient(val_P, pos_output, neighbors, indptr, forces,
-                         qt, theta, dof, skip_num_points, -1, compute_error)
+                         qt, theta, dof, skip_num_points, -1, compute_error,
+                         num_threads)
+
     if verbose > 10:
         # XXX: format hack to workaround lack of `const char *` type
         # in the generated C code
('sklearn/mixture', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -2,9 +2,8 @@
 The :mod:`sklearn.mixture` module implements mixture modeling algorithms.
 """

-from .gaussian_mixture import GaussianMixture
-from .bayesian_mixture import BayesianGaussianMixture
+from ._gaussian_mixture import GaussianMixture
+from ._bayesian_mixture import BayesianGaussianMixture


-__all__ = ['GaussianMixture',
-           'BayesianGaussianMixture']
+__all__ = ["GaussianMixture", "BayesianGaussianMixture"]
('sklearn/preprocessing', '_encoders.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,31 +8,27 @@
 import numpy as np
 from scipy import sparse

-from .. import get_config as _get_config
-from ..base import BaseEstimator, TransformerMixin
-from ..utils import check_array
-from ..utils import deprecated
-from ..utils.fixes import _argmax, _object_dtype_isnan
+from ..base import BaseEstimator, TransformerMixin, _OneToOneFeatureMixin
+from ..utils import check_array, is_scalar_nan
+from ..utils.deprecation import deprecated
 from ..utils.validation import check_is_fitted
-
-from .base import _transform_selected
-from .label import _encode, _encode_check_unknown
-
-
-__all__ = [
-    'OneHotEncoder',
-    'OrdinalEncoder'
-]
-
-
-class _BaseEncoder(BaseEstimator, TransformerMixin):
+from ..utils.validation import _check_feature_names_in
+from ..utils._mask import _get_mask
+
+from ..utils._encode import _encode, _check_unknown, _unique, _get_counts
+
+
+__all__ = ["OneHotEncoder", "OrdinalEncoder"]
+
+
+class _BaseEncoder(TransformerMixin, BaseEstimator):
     """
     Base class for encoders that includes the code to categorize and
     transform the input features.

     """

-    def _check_X(self, X):
+    def _check_X(self, X, force_all_finite=True):
         """
         Perform custom check_array:
         - convert list of strings to object dtype
@@ -41,119 +37,181 @@
         - return list of features (arrays): this list of features is
           constructed feature by feature to preserve the data types
           of pandas DataFrame columns, as otherwise information is lost
-          and cannot be used, eg for the `categories_` attribute.
-
-        """
-        if not (hasattr(X, 'iloc') and getattr(X, 'ndim', 0) == 2):
+          and cannot be used, e.g. for the `categories_` attribute.
+
+        """
+        if not (hasattr(X, "iloc") and getattr(X, "ndim", 0) == 2):
             # if not a dataframe, do normal check_array validation
-            X_temp = check_array(X, dtype=None)
-            if (not hasattr(X, 'dtype')
-                    and np.issubdtype(X_temp.dtype, np.str_)):
-                X = check_array(X, dtype=np.object)
+            X_temp = check_array(X, dtype=None, force_all_finite=force_all_finite)
+            if not hasattr(X, "dtype") and np.issubdtype(X_temp.dtype, np.str_):
+                X = check_array(X, dtype=object, force_all_finite=force_all_finite)
             else:
                 X = X_temp
             needs_validation = False
         else:
             # pandas dataframe, do validation later column by column, in order
             # to keep the dtype information to be used in the encoder.
-            needs_validation = True
+            needs_validation = force_all_finite

         n_samples, n_features = X.shape
         X_columns = []

         for i in range(n_features):
             Xi = self._get_feature(X, feature_idx=i)
-            Xi = check_array(Xi, ensure_2d=False, dtype=None,
-                             force_all_finite=needs_validation)
+            Xi = check_array(
+                Xi, ensure_2d=False, dtype=None, force_all_finite=needs_validation
+            )
             X_columns.append(Xi)

         return X_columns, n_samples, n_features

     def _get_feature(self, X, feature_idx):
-        if hasattr(X, 'iloc'):
+        if hasattr(X, "iloc"):
             # pandas dataframes
             return X.iloc[:, feature_idx]
         # numpy arrays, sparse arrays
         return X[:, feature_idx]

-    def _fit(self, X, handle_unknown='error'):
-        X_list, n_samples, n_features = self._check_X(X)
-
-        if self._categories != 'auto':
-            if len(self._categories) != n_features:
-                raise ValueError("Shape mismatch: if categories is an array,"
-                                 " it has to be of shape (n_features,).")
+    def _fit(
+        self, X, handle_unknown="error", force_all_finite=True, return_counts=False
+    ):
+        self._check_n_features(X, reset=True)
+        self._check_feature_names(X, reset=True)
+        X_list, n_samples, n_features = self._check_X(
+            X, force_all_finite=force_all_finite
+        )
+        self.n_features_in_ = n_features
+
+        if self.categories != "auto":
+            if len(self.categories) != n_features:
+                raise ValueError(
+                    "Shape mismatch: if categories is an array,"
+                    " it has to be of shape (n_features,)."
+                )

         self.categories_ = []
+        category_counts = []

         for i in range(n_features):
             Xi = X_list[i]
-            if self._categories == 'auto':
-                cats = _encode(Xi)
+
+            if self.categories == "auto":
+                result = _unique(Xi, return_counts=return_counts)
+                if return_counts:
+                    cats, counts = result
+                    category_counts.append(counts)
+                else:
+                    cats = result
             else:
-                cats = np.array(self._categories[i], dtype=Xi.dtype)
-                if Xi.dtype != object:
-                    if not np.all(np.sort(cats) == cats):
-                        raise ValueError("Unsorted categories are not "
-                                         "supported for numerical categories")
-                if handle_unknown == 'error':
-                    diff = _encode_check_unknown(Xi, cats)
+                cats = np.array(self.categories[i], dtype=Xi.dtype)
+                if Xi.dtype.kind not in "OUS":
+                    sorted_cats = np.sort(cats)
+                    error_msg = (
+                        "Unsorted categories are not supported for numerical categories"
+                    )
+                    # if there are nans, nan should be the last element
+                    stop_idx = -1 if np.isnan(sorted_cats[-1]) else None
+                    if np.any(sorted_cats[:stop_idx] != cats[:stop_idx]) or (
+                        np.isnan(sorted_cats[-1]) and not np.isnan(sorted_cats[-1])
+                    ):
+                        raise ValueError(error_msg)
+
+                if handle_unknown == "error":
+                    diff = _check_unknown(Xi, cats)
                     if diff:
-                        msg = ("Found unknown categories {0} in column {1}"
-                               " during fit".format(diff, i))
+                        msg = (
+                            "Found unknown categories {0} in column {1}"
+                            " during fit".format(diff, i)
+                        )
                         raise ValueError(msg)
+                if return_counts:
+                    category_counts.append(_get_counts(Xi, cats))
+
             self.categories_.append(cats)

-    def _transform(self, X, handle_unknown='error'):
-        X_list, n_samples, n_features = self._check_X(X)
-
-        X_int = np.zeros((n_samples, n_features), dtype=np.int)
-        X_mask = np.ones((n_samples, n_features), dtype=np.bool)
-
+        output = {"n_samples": n_samples}
+        if return_counts:
+            output["category_counts"] = category_counts
+        return output
+
+    def _transform(
+        self, X, handle_unknown="error", force_all_finite=True, warn_on_unknown=False
+    ):
+        self._check_feature_names(X, reset=False)
+        self._check_n_features(X, reset=False)
+        X_list, n_samples, n_features = self._check_X(
+            X, force_all_finite=force_all_finite
+        )
+
+        X_int = np.zeros((n_samples, n_features), dtype=int)
+        X_mask = np.ones((n_samples, n_features), dtype=bool)
+
+        columns_with_unknown = []
         for i in range(n_features):
             Xi = X_list[i]
-            diff, valid_mask = _encode_check_unknown(Xi, self.categories_[i],
-                                                     return_mask=True)
+            diff, valid_mask = _check_unknown(Xi, self.categories_[i], return_mask=True)

             if not np.all(valid_mask):
-                if handle_unknown == 'error':
-                    msg = ("Found unknown categories {0} in column {1}"
-                           " during transform".format(diff, i))
+                if handle_unknown == "error":
+                    msg = (
+                        "Found unknown categories {0} in column {1}"
+                        " during transform".format(diff, i)
+                    )
                     raise ValueError(msg)
                 else:
+                    if warn_on_unknown:
+                        columns_with_unknown.append(i)
                     # Set the problematic rows to an acceptable value and
                     # continue `The rows are marked `X_mask` and will be
                     # removed later.
                     X_mask[:, i] = valid_mask
                     # cast Xi into the largest string type necessary
                     # to handle different lengths of numpy strings
-                    if (self.categories_[i].dtype.kind in ('U', 'S')
-                            and self.categories_[i].itemsize > Xi.itemsize):
+                    if (
+                        self.categories_[i].dtype.kind in ("U", "S")
+                        and self.categories_[i].itemsize > Xi.itemsize
+                    ):
                         Xi = Xi.astype(self.categories_[i].dtype)
+                    elif self.categories_[i].dtype.kind == "O" and Xi.dtype.kind == "U":
+                        # categories are objects and Xi are numpy strings.
+                        # Cast Xi to an object dtype to prevent truncation
+                        # when setting invalid values.
+                        Xi = Xi.astype("O")
                     else:
                         Xi = Xi.copy()

                     Xi[~valid_mask] = self.categories_[i][0]
-            _, encoded = _encode(Xi, self.categories_[i], encode=True)
-            X_int[:, i] = encoded
+            # We use check_unknown=False, since _check_unknown was
+            # already called above.
+            X_int[:, i] = _encode(Xi, uniques=self.categories_[i], check_unknown=False)
+        if columns_with_unknown:
+            warnings.warn(
+                "Found unknown categories in columns "
+                f"{columns_with_unknown} during transform. These "
+                "unknown categories will be encoded as all zeros",
+                UserWarning,
+            )

         return X_int, X_mask

+    def _more_tags(self):
+        return {"X_types": ["categorical"]}
+

 class OneHotEncoder(_BaseEncoder):
-    """Encode categorical integer features as a one-hot numeric array.
+    """
+    Encode categorical features as a one-hot numeric array.

     The input to this transformer should be an array-like of integers or
     strings, denoting the values taken on by categorical (discrete) features.
     The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')
     encoding scheme. This creates a binary column for each category and
-    returns a sparse matrix or dense array.
+    returns a sparse matrix or dense array (depending on the ``sparse``
+    parameter)

     By default, the encoder derives the categories based on the unique values
     in each feature. Alternatively, you can also specify the `categories`
     manually.
-    The OneHotEncoder previously assumed that the input features take on
-    values in the range [0, max(values)). This behaviour is deprecated.

     This encoding is needed for feeding categorical data to many scikit-learn
     estimators, notably linear models and SVMs with the standard kernels.
@@ -165,7 +223,7 @@

     Parameters
     ----------
-    categories : 'auto' or a list of lists/arrays of values, default='auto'.
+    categories : 'auto' or a list of array-like, default='auto'
         Categories (unique values) per feature:

         - 'auto' : Determine categories automatically from the training data.
@@ -176,59 +234,90 @@

         The used categories can be found in the ``categories_`` attribute.

-    drop : 'first' or a list/array of shape (n_features,), default=None.
+        .. versionadded:: 0.20
+
+    drop : {'first', 'if_binary'} or an array-like of shape (n_features,), \
+            default=None
         Specifies a methodology to use to drop one of the categories per
         feature. This is useful in situations where perfectly collinear
         features cause problems, such as when feeding the resulting data
-        into a neural network or an unregularized regression.
+        into an unregularized linear regression model.
+
+        However, dropping one category breaks the symmetry of the original
+        representation and can therefore induce a bias in downstream models,
+        for instance for penalized linear classification or regression models.

         - None : retain all features (the default).
         - 'first' : drop the first category in each feature. If only one
           category is present, the feature will be dropped entirely.
+        - 'if_binary' : drop the first category in each feature with two
+          categories. Features with 1 or more than 2 categories are
+          left intact.
         - array : ``drop[i]`` is the category in feature ``X[:, i]`` that
           should be dropped.

-    sparse : boolean, default=True
+        .. versionadded:: 0.21
+           The parameter `drop` was added in 0.21.
+
+        .. versionchanged:: 0.23
+           The option `drop='if_binary'` was added in 0.23.
+
+        .. versionchanged:: 1.1
+            Support for dropping infrequent categories.
+
+    sparse : bool, default=True
         Will return sparse matrix if set True else will return an array.

-    dtype : number type, default=np.float
+    dtype : number type, default=float
         Desired dtype of output.

-    handle_unknown : 'error' or 'ignore', default='error'.
-        Whether to raise an error or ignore if an unknown categorical feature
-        is present during transform (default is to raise). When this parameter
-        is set to 'ignore' and an unknown category is encountered during
-        transform, the resulting one-hot encoded columns for this feature
-        will be all zeros. In the inverse transform, an unknown category
-        will be denoted as None.
-
-    n_values : 'auto', int or array of ints, default='auto'
-        Number of values per feature.
-
-        - 'auto' : determine value range from training data.
-        - int : number of categorical values per feature.
-                Each feature value should be in ``range(n_values)``
-        - array : ``n_values[i]`` is the number of categorical values in
-                  ``X[:, i]``. Each feature value should be
-                  in ``range(n_values[i])``
-
-        .. deprecated:: 0.20
-            The `n_values` keyword was deprecated in version 0.20 and will
-            be removed in 0.22. Use `categories` instead.
-
-    categorical_features : 'all' or array of indices or mask, default='all'
-        Specify what features are treated as categorical.
-
-        - 'all': All features are treated as categorical.
-        - array of indices: Array of categorical feature indices.
-        - mask: Array of length n_features and with dtype=bool.
-
-        Non-categorical features are always stacked to the right of the matrix.
-
-        .. deprecated:: 0.20
-            The `categorical_features` keyword was deprecated in version
-            0.20 and will be removed in 0.22.
-            You can use the ``ColumnTransformer`` instead.
+    handle_unknown : {'error', 'ignore', 'infrequent_if_exist'}, \
+                     default='error'
+        Specifies the way unknown categories are handled during :meth:`transform`.
+
+        - 'error' : Raise an error if an unknown category is present during transform.
+        - 'ignore' : When an unknown category is encountered during
+          transform, the resulting one-hot encoded columns for this feature
+          will be all zeros. In the inverse transform, an unknown category
+          will be denoted as None.
+        - 'infrequent_if_exist' : When an unknown category is encountered
+          during transform, the resulting one-hot encoded columns for this
+          feature will map to the infrequent category if it exists. The
+          infrequent category will be mapped to the last position in the
+          encoding. During inverse transform, an unknown category will be
+          mapped to the category denoted `'infrequent'` if it exists. If the
+          `'infrequent'` category does not exist, then :meth:`transform` and
+          :meth:`inverse_transform` will handle an unknown category as with
+          `handle_unknown='ignore'`. Infrequent categories exist based on
+          `min_frequency` and `max_categories`. Read more in the
+          :ref:`User Guide <one_hot_encoder_infrequent_categories>`.
+
+        .. versionchanged:: 1.1
+            `'infrequent_if_exist'` was added to automatically handle unknown
+            categories and infrequent categories.
+
+    min_frequency : int or float, default=None
+        Specifies the minimum frequency below which a category will be
+        considered infrequent.
+
+        - If `int`, categories with a smaller cardinality will be considered
+          infrequent.
+
+        - If `float`, categories with a smaller cardinality than
+          `min_frequency * n_samples`  will be considered infrequent.
+
+        .. versionadded:: 1.1
+            Read more in the :ref:`User Guide <one_hot_encoder_infrequent_categories>`.
+
+    max_categories : int, default=None
+        Specifies an upper limit to the number of output features for each input
+        feature when considering infrequent categories. If there are infrequent
+        categories, `max_categories` includes the category representing the
+        infrequent categories along with the frequent categories. If `None`,
+        there is no limit to the number of output features.
+
+        .. versionadded:: 1.1
+            Read more in the :ref:`User Guide <one_hot_encoder_infrequent_categories>`.

     Attributes
     ----------
@@ -239,34 +328,55 @@
         (if any).

     drop_idx_ : array of shape (n_features,)
-        ``drop_idx_[i]`` is the index in ``categories_[i]`` of the category to
-        be dropped for each feature. None if all the transformed features will
-        be retained.
-
-    active_features_ : array
-        Indices for active features, meaning values that actually occur
-        in the training set. Only available when n_values is ``'auto'``.
-
-        .. deprecated:: 0.20
-            The ``active_features_`` attribute was deprecated in version
-            0.20 and will be removed in 0.22.
-
-    feature_indices_ : array of shape (n_features,)
-        Indices to feature ranges.
-        Feature ``i`` in the original data is mapped to features
-        from ``feature_indices_[i]`` to ``feature_indices_[i+1]``
-        (and then potentially masked by ``active_features_`` afterwards)
-
-        .. deprecated:: 0.20
-            The ``feature_indices_`` attribute was deprecated in version
-            0.20 and will be removed in 0.22.
-
-    n_values_ : array of shape (n_features,)
-        Maximum number of values per feature.
-
-        .. deprecated:: 0.20
-            The ``n_values_`` attribute was deprecated in version
-            0.20 and will be removed in 0.22.
+        - ``drop_idx_[i]`` is the index in ``categories_[i]`` of the category
+          to be dropped for each feature.
+        - ``drop_idx_[i] = None`` if no category is to be dropped from the
+          feature with index ``i``, e.g. when `drop='if_binary'` and the
+          feature isn't binary.
+        - ``drop_idx_ = None`` if all the transformed features will be
+          retained.
+
+        If infrequent categories are enabled by setting `min_frequency` or
+        `max_categories` to a non-default value and `drop_idx[i]` corresponds
+        to a infrequent category, then the entire infrequent category is
+        dropped.
+
+        .. versionchanged:: 0.23
+           Added the possibility to contain `None` values.
+
+    infrequent_categories_ : list of ndarray
+        Defined only if infrequent categories are enabled by setting
+        `min_frequency` or `max_categories` to a non-default value.
+        `infrequent_categories_[i]` are the infrequent categories for feature
+        `i`. If the feature `i` has no infrequent categories
+        `infrequent_categories_[i]` is None.
+
+        .. versionadded:: 1.1
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 1.0
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    OrdinalEncoder : Performs an ordinal (integer)
+      encoding of the categorical features.
+    sklearn.feature_extraction.DictVectorizer : Performs a one-hot encoding of
+      dictionary items (also handles string-valued features).
+    sklearn.feature_extraction.FeatureHasher : Performs an approximate one-hot
+      encoding of dictionary items or strings.
+    LabelBinarizer : Binarizes labels in a one-vs-all
+      fashion.
+    MultiLabelBinarizer : Transforms between iterable of
+      iterables and a multilabel format, e.g. a (samples x classes) binary
+      matrix indicating the presence of a class label.

     Examples
     --------
@@ -274,15 +384,13 @@
     values per feature and transform the data to a binary one-hot encoding.

     >>> from sklearn.preprocessing import OneHotEncoder
+
+    One can discard categories not seen during `fit`:
+
     >>> enc = OneHotEncoder(handle_unknown='ignore')
     >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]
     >>> enc.fit(X)
-    ... # doctest: +ELLIPSIS
-    ... # doctest: +NORMALIZE_WHITESPACE
-    OneHotEncoder(categorical_features=None, categories=None, drop=None,
-       dtype=<... 'numpy.float64'>, handle_unknown='ignore',
-       n_values=None, sparse=True)
-
+    OneHotEncoder(handle_unknown='ignore')
     >>> enc.categories_
     [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]
     >>> enc.transform([['Female', 1], ['Male', 4]]).toarray()
@@ -291,8 +399,11 @@
     >>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])
     array([['Male', 1],
            [None, 2]], dtype=object)
-    >>> enc.get_feature_names()
-    array(['x0_Female', 'x0_Male', 'x1_1', 'x1_2', 'x1_3'], dtype=object)
+    >>> enc.get_feature_names_out(['gender', 'group'])
+    array(['gender_Female', 'gender_Male', 'group_1', 'group_2', 'group_3'], ...)
+
+    One can always drop the first column for each feature:
+
     >>> drop_enc = OneHotEncoder(drop='first').fit(X)
     >>> drop_enc.categories_
     [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]
@@ -300,487 +411,579 @@
     array([[0., 0., 0.],
            [1., 1., 0.]])

-    See also
-    --------
-    sklearn.preprocessing.OrdinalEncoder : performs an ordinal (integer)
-      encoding of the categorical features.
-    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of
-      dictionary items (also handles string-valued features).
-    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot
-      encoding of dictionary items or strings.
-    sklearn.preprocessing.LabelBinarizer : binarizes labels in a one-vs-all
-      fashion.
-    sklearn.preprocessing.MultiLabelBinarizer : transforms between iterable of
-      iterables and a multilabel format, e.g. a (samples x classes) binary
-      matrix indicating the presence of a class label.
+    Or drop a column for feature only having 2 categories:
+
+    >>> drop_binary_enc = OneHotEncoder(drop='if_binary').fit(X)
+    >>> drop_binary_enc.transform([['Female', 1], ['Male', 2]]).toarray()
+    array([[0., 1., 0., 0.],
+           [1., 0., 1., 0.]])
+
+    Infrequent categories are enabled by setting `max_categories` or `min_frequency`.
+
+    >>> import numpy as np
+    >>> X = np.array([["a"] * 5 + ["b"] * 20 + ["c"] * 10 + ["d"] * 3], dtype=object).T
+    >>> ohe = OneHotEncoder(max_categories=3, sparse=False).fit(X)
+    >>> ohe.infrequent_categories_
+    [array(['a', 'd'], dtype=object)]
+    >>> ohe.transform([["a"], ["b"]])
+    array([[0., 0., 1.],
+           [1., 0., 0.]])
     """

-    def __init__(self, n_values=None, categorical_features=None,
-                 categories=None, drop=None, sparse=True, dtype=np.float64,
-                 handle_unknown='error'):
+    def __init__(
+        self,
+        *,
+        categories="auto",
+        drop=None,
+        sparse=True,
+        dtype=np.float64,
+        handle_unknown="error",
+        min_frequency=None,
+        max_categories=None,
+    ):
         self.categories = categories
         self.sparse = sparse
         self.dtype = dtype
         self.handle_unknown = handle_unknown
-        self.n_values = n_values
-        self.categorical_features = categorical_features
         self.drop = drop
-
-    # Deprecated attributes
-
-    @deprecated("The ``active_features_`` attribute was deprecated in version "
-                "0.20 and will be removed 0.22.")
+        self.min_frequency = min_frequency
+        self.max_categories = max_categories
+
     @property
-    def active_features_(self):
-        check_is_fitted(self, 'categories_')
-        return self._active_features_
-
-    @deprecated("The ``feature_indices_`` attribute was deprecated in version "
-                "0.20 and will be removed 0.22.")
-    @property
-    def feature_indices_(self):
-        check_is_fitted(self, 'categories_')
-        return self._feature_indices_
-
-    @deprecated("The ``n_values_`` attribute was deprecated in version "
-                "0.20 and will be removed 0.22.")
-    @property
-    def n_values_(self):
-        check_is_fitted(self, 'categories_')
-        return self._n_values_
-
-    def _handle_deprecations(self, X):
-        # internal version of the attributes to handle deprecations
-        self._n_values = self.n_values
-        self._categories = getattr(self, '_categories', None)
-        self._categorical_features = getattr(self, '_categorical_features',
-                                             None)
-
-        # user manually set the categories or second fit -> never legacy mode
-        if self.categories is not None or self._categories is not None:
-            self._legacy_mode = False
-            if self.categories is not None:
-                self._categories = self.categories
-
-        # categories not set -> infer if we need legacy mode or not
-        elif self.n_values is not None and self.n_values != 'auto':
+    def infrequent_categories_(self):
+        """Infrequent categories for each feature."""
+        # raises an AttributeError if `_infrequent_indices` is not defined
+        infrequent_indices = self._infrequent_indices
+        return [
+            None if indices is None else category[indices]
+            for category, indices in zip(self.categories_, infrequent_indices)
+        ]
+
+    def _validate_keywords(self):
+
+        if self.handle_unknown not in {"error", "ignore", "infrequent_if_exist"}:
             msg = (
-                "Passing 'n_values' is deprecated in version 0.20 and will be "
-                "removed in 0.22. You can use the 'categories' keyword "
-                "instead. 'n_values=n' corresponds to 'categories=[range(n)]'."
+                "handle_unknown should be one of 'error', 'ignore', "
+                f"'infrequent_if_exist' got {self.handle_unknown}."
             )
-            warnings.warn(msg, DeprecationWarning)
-            self._legacy_mode = True
-
-        else:  # n_values = 'auto'
-            # n_values can also be None (default to catch usage), so set
-            # _n_values to 'auto' explicitly
-            self._n_values = 'auto'
-            if self.handle_unknown == 'ignore':
-                # no change in behaviour, no need to raise deprecation warning
-                self._legacy_mode = False
-                self._categories = 'auto'
-                if self.n_values == 'auto':
-                    # user manually specified this
-                    msg = (
-                        "Passing 'n_values' is deprecated in version 0.20 and "
-                        "will be removed in 0.22. n_values='auto' can be "
-                        "replaced with categories='auto'."
+            raise ValueError(msg)
+
+        if self.max_categories is not None and self.max_categories < 1:
+            raise ValueError("max_categories must be greater than 1")
+
+        if isinstance(self.min_frequency, numbers.Integral):
+            if not self.min_frequency >= 1:
+                raise ValueError(
+                    "min_frequency must be an integer at least "
+                    "1 or a float in (0.0, 1.0); got the "
+                    f"integer {self.min_frequency}"
+                )
+        elif isinstance(self.min_frequency, numbers.Real):
+            if not (0.0 < self.min_frequency < 1.0):
+                raise ValueError(
+                    "min_frequency must be an integer at least "
+                    "1 or a float in (0.0, 1.0); got the "
+                    f"float {self.min_frequency}"
+                )
+
+        self._infrequent_enabled = (
+            self.max_categories is not None and self.max_categories >= 1
+        ) or self.min_frequency is not None
+
+    def _map_drop_idx_to_infrequent(self, feature_idx, drop_idx):
+        """Convert `drop_idx` into the index for infrequent categories.
+
+        If there are no infrequent categories, then `drop_idx` is
+        returned. This method is called in `_compute_drop_idx` when the `drop`
+        parameter is an array-like.
+        """
+        if not self._infrequent_enabled:
+            return drop_idx
+
+        default_to_infrequent = self._default_to_infrequent_mappings[feature_idx]
+        if default_to_infrequent is None:
+            return drop_idx
+
+        # Raise error when explicitly dropping a category that is infrequent
+        infrequent_indices = self._infrequent_indices[feature_idx]
+        if infrequent_indices is not None and drop_idx in infrequent_indices:
+            categories = self.categories_[feature_idx]
+            raise ValueError(
+                f"Unable to drop category {categories[drop_idx]!r} from feature"
+                f" {feature_idx} because it is infrequent"
+            )
+        return default_to_infrequent[drop_idx]
+
+    def _compute_drop_idx(self):
+        """Compute the drop indices associated with `self.categories_`.
+
+        If `self.drop` is:
+        - `None`, returns `None`.
+        - `'first'`, returns all zeros to drop the first category.
+        - `'if_binary'`, returns zero if the category is binary and `None`
+          otherwise.
+        - array-like, returns the indices of the categories that match the
+          categories in `self.drop`. If the dropped category is an infrequent
+          category, then the index for the infrequent category is used. This
+          means that the entire infrequent category is dropped.
+        """
+        if self.drop is None:
+            return None
+        elif isinstance(self.drop, str):
+            if self.drop == "first":
+                return np.zeros(len(self.categories_), dtype=object)
+            elif self.drop == "if_binary":
+                n_features_out_no_drop = [len(cat) for cat in self.categories_]
+                if self._infrequent_enabled:
+                    for i, infreq_idx in enumerate(self._infrequent_indices):
+                        if infreq_idx is None:
+                            continue
+                        n_features_out_no_drop[i] -= infreq_idx.size - 1
+
+                return np.array(
+                    [
+                        0 if n_features_out == 2 else None
+                        for n_features_out in n_features_out_no_drop
+                    ],
+                    dtype=object,
+                )
+            else:
+                msg = (
+                    "Wrong input for parameter `drop`. Expected "
+                    "'first', 'if_binary', None or array of objects, got {}"
+                )
+                raise ValueError(msg.format(type(self.drop)))
+
+        else:
+            try:
+                drop_array = np.asarray(self.drop, dtype=object)
+                droplen = len(drop_array)
+            except (ValueError, TypeError):
+                msg = (
+                    "Wrong input for parameter `drop`. Expected "
+                    "'first', 'if_binary', None or array of objects, got {}"
+                )
+                raise ValueError(msg.format(type(drop_array)))
+            if droplen != len(self.categories_):
+                msg = (
+                    "`drop` should have length equal to the number "
+                    "of features ({}), got {}"
+                )
+                raise ValueError(msg.format(len(self.categories_), droplen))
+            missing_drops = []
+            drop_indices = []
+            for feature_idx, (drop_val, cat_list) in enumerate(
+                zip(drop_array, self.categories_)
+            ):
+                if not is_scalar_nan(drop_val):
+                    drop_idx = np.where(cat_list == drop_val)[0]
+                    if drop_idx.size:  # found drop idx
+                        drop_indices.append(
+                            self._map_drop_idx_to_infrequent(feature_idx, drop_idx[0])
+                        )
+                    else:
+                        missing_drops.append((feature_idx, drop_val))
+                    continue
+
+                # drop_val is nan, find nan in categories manually
+                for cat_idx, cat in enumerate(cat_list):
+                    if is_scalar_nan(cat):
+                        drop_indices.append(
+                            self._map_drop_idx_to_infrequent(feature_idx, cat_idx)
+                        )
+                        break
+                else:  # loop did not break thus drop is missing
+                    missing_drops.append((feature_idx, drop_val))
+
+            if any(missing_drops):
+                msg = (
+                    "The following categories were supposed to be "
+                    "dropped, but were not found in the training "
+                    "data.\n{}".format(
+                        "\n".join(
+                            [
+                                "Category: {}, Feature: {}".format(c, v)
+                                for c, v in missing_drops
+                            ]
+                        )
                     )
-                    warnings.warn(msg, DeprecationWarning)
-            else:
-                # check if we have integer or categorical input
-                try:
-                    check_array(X, dtype=np.int)
-                except ValueError:
-                    self._legacy_mode = False
-                    self._categories = 'auto'
-                else:
-                    if self.drop is None:
-                        msg = (
-                            "The handling of integer data will change in "
-                            "version 0.22. Currently, the categories are "
-                            "determined based on the range "
-                            "[0, max(values)], while in the future they "
-                            "will be determined based on the unique "
-                            "values.\nIf you want the future behaviour "
-                            "and silence this warning, you can specify "
-                            "\"categories='auto'\".\n"
-                            "In case you used a LabelEncoder before this "
-                            "OneHotEncoder to convert the categories to "
-                            "integers, then you can now use the "
-                            "OneHotEncoder directly."
-                        )
-                        warnings.warn(msg, FutureWarning)
-                        self._legacy_mode = True
-                    else:
-                        msg = (
-                            "The handling of integer data will change in "
-                            "version 0.22. Currently, the categories are "
-                            "determined based on the range "
-                            "[0, max(values)], while in the future they "
-                            "will be determined based on the unique "
-                            "values.\n The old behavior is not compatible "
-                            "with the `drop` parameter. Instead, you "
-                            "must manually specify \"categories='auto'\" "
-                            "if you wish to use the `drop` parameter on "
-                            "an array of entirely integer data. This will "
-                            "enable the future behavior."
-                        )
-                        raise ValueError(msg)
-
-        # if user specified categorical_features -> always use legacy mode
-        if self.categorical_features is not None:
-            if (isinstance(self.categorical_features, str)
-                    and self.categorical_features == 'all'):
-                warnings.warn(
-                    "The 'categorical_features' keyword is deprecated in "
-                    "version 0.20 and will be removed in 0.22. The passed "
-                    "value of 'all' is the default and can simply be removed.",
-                    DeprecationWarning)
-            else:
-                if self.categories is not None:
-                    raise ValueError(
-                        "The 'categorical_features' keyword is deprecated, "
-                        "and cannot be used together with specifying "
-                        "'categories'.")
-                warnings.warn(
-                    "The 'categorical_features' keyword is deprecated in "
-                    "version 0.20 and will be removed in 0.22. You can "
-                    "use the ColumnTransformer instead.", DeprecationWarning)
-                # Set categories_ to empty list if no categorical columns exist
-                n_features = X.shape[1]
-                sel = np.zeros(n_features, dtype=bool)
-                sel[np.asarray(self.categorical_features)] = True
-                if sum(sel) == 0:
-                    self.categories_ = []
-                self._legacy_mode = True
-            self._categorical_features = self.categorical_features
-        else:
-            self._categorical_features = 'all'
-
-        # Prevents new drop functionality from being used in legacy mode
-        if self._legacy_mode and self.drop is not None:
-            raise ValueError(
-                "The `categorical_features` and `n_values` keywords "
-                "are deprecated, and cannot be used together "
-                "with 'drop'.")
-
-    def fit(self, X, y=None):
-        """Fit OneHotEncoder to X.
+                )
+                raise ValueError(msg)
+            return np.array(drop_indices, dtype=object)
+
+    def _identify_infrequent(self, category_count, n_samples, col_idx):
+        """Compute the infrequent indices.

         Parameters
         ----------
-        X : array-like, shape [n_samples, n_features]
+        category_count : ndarray of shape (n_cardinality,)
+            Category counts.
+
+        n_samples : int
+            Number of samples.
+
+        col_idx : int
+            Index of the current category. Only used for the error message.
+
+        Returns
+        -------
+        output : ndarray of shape (n_infrequent_categories,) or None
+            If there are infrequent categories, indices of infrequent
+            categories. Otherwise None.
+        """
+        if isinstance(self.min_frequency, numbers.Integral):
+            infrequent_mask = category_count < self.min_frequency
+        elif isinstance(self.min_frequency, numbers.Real):
+            min_frequency_abs = n_samples * self.min_frequency
+            infrequent_mask = category_count < min_frequency_abs
+        else:
+            infrequent_mask = np.zeros(category_count.shape[0], dtype=bool)
+
+        n_current_features = category_count.size - infrequent_mask.sum() + 1
+        if self.max_categories is not None and self.max_categories < n_current_features:
+            # stable sort to preserve original count order
+            smallest_levels = np.argsort(category_count, kind="mergesort")[
+                : -self.max_categories + 1
+            ]
+            infrequent_mask[smallest_levels] = True
+
+        output = np.flatnonzero(infrequent_mask)
+        return output if output.size > 0 else None
+
+    def _fit_infrequent_category_mapping(self, n_samples, category_counts):
+        """Fit infrequent categories.
+
+        Defines the private attribute: `_default_to_infrequent_mappings`. For
+        feature `i`, `_default_to_infrequent_mappings[i]` defines the mapping
+        from the integer encoding returned by `super().transform()` into
+        infrequent categories. If `_default_to_infrequent_mappings[i]` is None,
+        there were no infrequent categories in the training set.
+
+        For example if categories 0, 2 and 4 were frequent, while categories
+        1, 3, 5 were infrequent for feature 7, then these categories are mapped
+        to a single output:
+        `_default_to_infrequent_mappings[7] = array([0, 3, 1, 3, 2, 3])`
+
+        Defines private attrite: `_infrequent_indices`. `_infrequent_indices[i]`
+        is an array of indices such that
+        `categories_[i][_infrequent_indices[i]]` are all the infrequent category
+        labels. If the feature `i` has no infrequent categories
+        `_infrequent_indices[i]` is None.
+
+        .. versionadded:: 1.1
+
+        Parameters
+        ----------
+        n_samples : int
+            Number of samples in training set.
+        category_counts: list of ndarray
+            `category_counts[i]` is the category counts corresponding to
+            `self.categories_[i]`.
+        """
+        self._infrequent_indices = [
+            self._identify_infrequent(category_count, n_samples, col_idx)
+            for col_idx, category_count in enumerate(category_counts)
+        ]
+
+        # compute mapping from default mapping to infrequent mapping
+        self._default_to_infrequent_mappings = []
+
+        for cats, infreq_idx in zip(self.categories_, self._infrequent_indices):
+            # no infrequent categories
+            if infreq_idx is None:
+                self._default_to_infrequent_mappings.append(None)
+                continue
+
+            n_cats = len(cats)
+            # infrequent indices exist
+            mapping = np.empty(n_cats, dtype=np.int64)
+            n_infrequent_cats = infreq_idx.size
+
+            # infrequent categories are mapped to the last element.
+            n_frequent_cats = n_cats - n_infrequent_cats
+            mapping[infreq_idx] = n_frequent_cats
+
+            frequent_indices = np.setdiff1d(np.arange(n_cats), infreq_idx)
+            mapping[frequent_indices] = np.arange(n_frequent_cats)
+
+            self._default_to_infrequent_mappings.append(mapping)
+
+    def _map_infrequent_categories(self, X_int, X_mask):
+        """Map infrequent categories to integer representing the infrequent category.
+
+        This modifies X_int in-place. Values that were invalid based on `X_mask`
+        are mapped to the infrequent category if there was an infrequent
+        category for that feature.
+
+        Parameters
+        ----------
+        X_int: ndarray of shape (n_samples, n_features)
+            Integer encoded categories.
+
+        X_mask: ndarray of shape (n_samples, n_features)
+            Bool mask for valid values in `X_int`.
+        """
+        if not self._infrequent_enabled:
+            return
+
+        for col_idx in range(X_int.shape[1]):
+            infrequent_idx = self._infrequent_indices[col_idx]
+            if infrequent_idx is None:
+                continue
+
+            X_int[~X_mask[:, col_idx], col_idx] = infrequent_idx[0]
+            if self.handle_unknown == "infrequent_if_exist":
+                # All the unknown values are now mapped to the
+                # infrequent_idx[0], which makes the unknown values valid
+                # This is needed in `transform` when the encoding is formed
+                # using `X_mask`.
+                X_mask[:, col_idx] = True
+
+        # Remaps encoding in `X_int` where the infrequent categories are
+        # grouped together.
+        for i, mapping in enumerate(self._default_to_infrequent_mappings):
+            if mapping is None:
+                continue
+            X_int[:, i] = np.take(mapping, X_int[:, i])
+
+    def _compute_transformed_categories(self, i, remove_dropped=True):
+        """Compute the transformed categories used for column `i`.
+
+        1. If there are infrequent categories, the category is named
+        'infrequent_sklearn'.
+        2. Dropped columns are removed when remove_dropped=True.
+        """
+        cats = self.categories_[i]
+
+        if self._infrequent_enabled:
+            infreq_map = self._default_to_infrequent_mappings[i]
+            if infreq_map is not None:
+                frequent_mask = infreq_map < infreq_map.max()
+                infrequent_cat = "infrequent_sklearn"
+                # infrequent category is always at the end
+                cats = np.concatenate(
+                    (cats[frequent_mask], np.array([infrequent_cat], dtype=object))
+                )
+
+        if remove_dropped:
+            cats = self._remove_dropped_categories(cats, i)
+        return cats
+
+    def _remove_dropped_categories(self, categories, i):
+        """Remove dropped categories."""
+        if self.drop_idx_ is not None and self.drop_idx_[i] is not None:
+            return np.delete(categories, self.drop_idx_[i])
+        return categories
+
+    def _compute_n_features_outs(self):
+        """Compute the n_features_out for each input feature."""
+        output = [len(cats) for cats in self.categories_]
+
+        if self.drop_idx_ is not None:
+            for i, drop_idx in enumerate(self.drop_idx_):
+                if drop_idx is not None:
+                    output[i] -= 1
+
+        if not self._infrequent_enabled:
+            return output
+
+        # infrequent is enabled, the number of features out are reduced
+        # because the infrequent categories are grouped together
+        for i, infreq_idx in enumerate(self._infrequent_indices):
+            if infreq_idx is None:
+                continue
+            output[i] -= infreq_idx.size - 1
+
+        return output
+
+    def fit(self, X, y=None):
+        """
+        Fit OneHotEncoder to X.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
             The data to determine the categories of each feature.
+
+        y : None
+            Ignored. This parameter exists only for compatibility with
+            :class:`~sklearn.pipeline.Pipeline`.

         Returns
         -------
         self
-        """
-
+            Fitted encoder.
+        """
         self._validate_keywords()
-
-        self._handle_deprecations(X)
-
-        if self._legacy_mode:
-            _transform_selected(X, self._legacy_fit_transform, self.dtype,
-                                self._categorical_features,
-                                copy=True)
-            return self
-        else:
-            self._fit(X, handle_unknown=self.handle_unknown)
-            self.drop_idx_ = self._compute_drop_idx()
-            return self
-
-    def _compute_drop_idx(self):
-        if self.drop is None:
-            return None
-        elif (isinstance(self.drop, str) and self.drop == 'first'):
-            return np.zeros(len(self.categories_), dtype=np.int_)
-        elif not isinstance(self.drop, str):
-            try:
-                self.drop = np.asarray(self.drop, dtype=object)
-                droplen = len(self.drop)
-            except (ValueError, TypeError):
-                msg = ("Wrong input for parameter `drop`. Expected "
-                       "'first', None or array of objects, got {}")
-                raise ValueError(msg.format(type(self.drop)))
-            if droplen != len(self.categories_):
-                msg = ("`drop` should have length equal to the number "
-                       "of features ({}), got {}")
-                raise ValueError(msg.format(len(self.categories_),
-                                            len(self.drop)))
-            missing_drops = [(i, val) for i, val in enumerate(self.drop)
-                             if val not in self.categories_[i]]
-            if any(missing_drops):
-                msg = ("The following categories were supposed to be "
-                       "dropped, but were not found in the training "
-                       "data.\n{}".format(
-                           "\n".join(
-                                ["Category: {}, Feature: {}".format(c, v)
-                                    for c, v in missing_drops])))
-                raise ValueError(msg)
-            return np.array([np.where(cat_list == val)[0][0]
-                             for (val, cat_list) in
-                             zip(self.drop, self.categories_)], dtype=np.int_)
-        else:
-            msg = ("Wrong input for parameter `drop`. Expected "
-                   "'first', None or array of objects, got {}")
-            raise ValueError(msg.format(type(self.drop)))
-
-    def _validate_keywords(self):
-        if self.handle_unknown not in ('error', 'ignore'):
-            msg = ("handle_unknown should be either 'error' or 'ignore', "
-                   "got {0}.".format(self.handle_unknown))
-            raise ValueError(msg)
-        # If we have both dropped columns and ignored unknown
-        # values, there will be ambiguous cells. This creates difficulties
-        # in interpreting the model.
-        if self.drop is not None and self.handle_unknown != 'error':
-            raise ValueError(
-                "`handle_unknown` must be 'error' when the drop parameter is "
-                "specified, as both would create categories that are all "
-                "zero.")
-
-    def _legacy_fit_transform(self, X):
-        """Assumes X contains only categorical features."""
-        dtype = getattr(X, 'dtype', None)
-        X = check_array(X, dtype=np.int)
-        if np.any(X < 0):
-            raise ValueError("OneHotEncoder in legacy mode cannot handle "
-                             "categories encoded as negative integers. "
-                             "Please set categories='auto' explicitly to "
-                             "be able to use arbitrary integer values as "
-                             "category identifiers.")
-        n_samples, n_features = X.shape
-        if (isinstance(self._n_values, str) and
-                self._n_values == 'auto'):
-            n_values = np.max(X, axis=0) + 1
-        elif isinstance(self._n_values, numbers.Integral):
-            if (np.max(X, axis=0) >= self._n_values).any():
-                raise ValueError("Feature out of bounds for n_values=%d"
-                                 % self._n_values)
-            n_values = np.empty(n_features, dtype=np.int)
-            n_values.fill(self._n_values)
-        else:
-            try:
-                n_values = np.asarray(self._n_values, dtype=int)
-            except (ValueError, TypeError):
-                raise TypeError("Wrong type for parameter `n_values`. Expected"
-                                " 'auto', int or array of ints, got %r"
-                                % type(self._n_values))
-            if n_values.ndim < 1 or n_values.shape[0] != X.shape[1]:
-                raise ValueError("Shape mismatch: if n_values is an array,"
-                                 " it has to be of shape (n_features,).")
-
-        self._n_values_ = n_values
-        self.categories_ = [np.arange(n_val - 1, dtype=dtype)
-                            for n_val in n_values]
-        n_values = np.hstack([[0], n_values])
-        indices = np.cumsum(n_values)
-        self._feature_indices_ = indices
-
-        column_indices = (X + indices[:-1]).ravel()
-        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),
-                                n_features)
-        data = np.ones(n_samples * n_features)
-        out = sparse.coo_matrix((data, (row_indices, column_indices)),
-                                shape=(n_samples, indices[-1]),
-                                dtype=self.dtype).tocsr()
-
-        if (isinstance(self._n_values, str) and
-                self._n_values == 'auto'):
-            mask = np.array(out.sum(axis=0)).ravel() != 0
-            active_features = np.where(mask)[0]
-            out = out[:, active_features]
-            self._active_features_ = active_features
-
-            self.categories_ = [
-                np.unique(X[:, i]).astype(dtype) if dtype
-                else np.unique(X[:, i]) for i in range(n_features)]
-
-        return out if self.sparse else out.toarray()
+        fit_results = self._fit(
+            X,
+            handle_unknown=self.handle_unknown,
+            force_all_finite="allow-nan",
+            return_counts=self._infrequent_enabled,
+        )
+        if self._infrequent_enabled:
+            self._fit_infrequent_category_mapping(
+                fit_results["n_samples"], fit_results["category_counts"]
+            )
+        self.drop_idx_ = self._compute_drop_idx()
+        self._n_features_outs = self._compute_n_features_outs()
+        return self

     def fit_transform(self, X, y=None):
-        """Fit OneHotEncoder to X, then transform X.
+        """
+        Fit OneHotEncoder to X, then transform X.

         Equivalent to fit(X).transform(X) but more convenient.

         Parameters
         ----------
-        X : array-like, shape [n_samples, n_features]
+        X : array-like of shape (n_samples, n_features)
             The data to encode.
+
+        y : None
+            Ignored. This parameter exists only for compatibility with
+            :class:`~sklearn.pipeline.Pipeline`.

         Returns
         -------
-        X_out : sparse matrix if sparse=True else a 2-d array
-            Transformed input.
-        """
-
+        X_out : {ndarray, sparse matrix} of shape \
+                (n_samples, n_encoded_features)
+            Transformed input. If `sparse=True`, a sparse matrix will be
+            returned.
+        """
         self._validate_keywords()
-
-        self._handle_deprecations(X)
-
-        if self._legacy_mode:
-            return _transform_selected(
-                X, self._legacy_fit_transform, self.dtype,
-                self._categorical_features, copy=True)
-        else:
-            return self.fit(X).transform(X)
-
-    def _legacy_transform(self, X):
-        """Assumes X contains only categorical features."""
-        X = check_array(X, dtype=np.int)
-        if np.any(X < 0):
-            raise ValueError("OneHotEncoder in legacy mode cannot handle "
-                             "categories encoded as negative integers. "
-                             "Please set categories='auto' explicitly to "
-                             "be able to use arbitrary integer values as "
-                             "category identifiers.")
-        n_samples, n_features = X.shape
-
-        indices = self._feature_indices_
-        if n_features != indices.shape[0] - 1:
-            raise ValueError("X has different shape than during fitting."
-                             " Expected %d, got %d."
-                             % (indices.shape[0] - 1, n_features))
-
-        # We use only those categorical features of X that are known using fit.
-        # i.e lesser than n_values_ using mask.
-        # This means, if self.handle_unknown is "ignore", the row_indices and
-        # col_indices corresponding to the unknown categorical feature are
-        # ignored.
-        mask = (X < self._n_values_).ravel()
-        if np.any(~mask):
-            if self.handle_unknown not in ['error', 'ignore']:
-                raise ValueError("handle_unknown should be either error or "
-                                 "unknown got %s" % self.handle_unknown)
-            if self.handle_unknown == 'error':
-                raise ValueError("unknown categorical feature present %s "
-                                 "during transform." % X.ravel()[~mask])
-
-        column_indices = (X + indices[:-1]).ravel()[mask]
-        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),
-                                n_features)[mask]
-        data = np.ones(np.sum(mask))
-        out = sparse.coo_matrix((data, (row_indices, column_indices)),
-                                shape=(n_samples, indices[-1]),
-                                dtype=self.dtype).tocsr()
-        if (isinstance(self._n_values, str) and
-                self._n_values == 'auto'):
-            out = out[:, self._active_features_]
-
-        return out if self.sparse else out.toarray()
-
-    def _transform_new(self, X):
-        """New implementation assuming categorical input"""
+        return super().fit_transform(X, y)
+
+    def transform(self, X):
+        """
+        Transform X using one-hot encoding.
+
+        If there are infrequent categories for a feature, the infrequent
+        categories will be grouped into a single category.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features)
+            The data to encode.
+
+        Returns
+        -------
+        X_out : {ndarray, sparse matrix} of shape \
+                (n_samples, n_encoded_features)
+            Transformed input. If `sparse=True`, a sparse matrix will be
+            returned.
+        """
+        check_is_fitted(self)
         # validation of X happens in _check_X called by _transform
-        X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown)
+        warn_on_unknown = self.drop is not None and self.handle_unknown in {
+            "ignore",
+            "infrequent_if_exist",
+        }
+        X_int, X_mask = self._transform(
+            X,
+            handle_unknown=self.handle_unknown,
+            force_all_finite="allow-nan",
+            warn_on_unknown=warn_on_unknown,
+        )
+        self._map_infrequent_categories(X_int, X_mask)

         n_samples, n_features = X_int.shape

-        if self.drop is not None:
-            to_drop = self.drop_idx_.reshape(1, -1)
-
+        if self.drop_idx_ is not None:
+            to_drop = self.drop_idx_.copy()
             # We remove all the dropped categories from mask, and decrement all
             # categories that occur after them to avoid an empty column.
-
             keep_cells = X_int != to_drop
+            for i, cats in enumerate(self.categories_):
+                # drop='if_binary' but feature isn't binary
+                if to_drop[i] is None:
+                    # set to cardinality to not drop from X_int
+                    to_drop[i] = len(cats)
+
+            to_drop = to_drop.reshape(1, -1)
+            X_int[X_int > to_drop] -= 1
             X_mask &= keep_cells
-            X_int[X_int > to_drop] -= 1
-            n_values = [len(cats) - 1 for cats in self.categories_]
-        else:
-            n_values = [len(cats) for cats in self.categories_]

         mask = X_mask.ravel()
-        n_values = np.array([0] + n_values)
-        feature_indices = np.cumsum(n_values)
+        feature_indices = np.cumsum([0] + self._n_features_outs)
         indices = (X_int + feature_indices[:-1]).ravel()[mask]
-        indptr = X_mask.sum(axis=1).cumsum()
-        indptr = np.insert(indptr, 0, 0)
-        data = np.ones(n_samples * n_features)[mask]
-
-        out = sparse.csr_matrix((data, indices, indptr),
-                                shape=(n_samples, feature_indices[-1]),
-                                dtype=self.dtype)
+
+        indptr = np.empty(n_samples + 1, dtype=int)
+        indptr[0] = 0
+        np.sum(X_mask, axis=1, out=indptr[1:], dtype=indptr.dtype)
+        np.cumsum(indptr[1:], out=indptr[1:])
+        data = np.ones(indptr[-1])
+
+        out = sparse.csr_matrix(
+            (data, indices, indptr),
+            shape=(n_samples, feature_indices[-1]),
+            dtype=self.dtype,
+        )
         if not self.sparse:
             return out.toarray()
         else:
             return out

-    def transform(self, X):
-        """Transform X using one-hot encoding.
+    def inverse_transform(self, X):
+        """
+        Convert the data back to the original representation.
+
+        When unknown categories are encountered (all zeros in the
+        one-hot encoding), ``None`` is used to represent this category. If the
+        feature with the unknown category has a dropped category, the dropped
+        category will be its inverse.
+
+        For a given input feature, if there is an infrequent category,
+        'infrequent_sklearn' will be used to represent the infrequent category.

         Parameters
         ----------
-        X : array-like, shape [n_samples, n_features]
-            The data to encode.
+        X : {array-like, sparse matrix} of shape \
+                (n_samples, n_encoded_features)
+            The transformed data.

         Returns
         -------
-        X_out : sparse matrix if sparse=True else a 2-d array
-            Transformed input.
-        """
-        check_is_fitted(self, 'categories_')
-        if self._legacy_mode:
-            return _transform_selected(X, self._legacy_transform, self.dtype,
-                                       self._categorical_features,
-                                       copy=True)
-        else:
-            return self._transform_new(X)
-
-    def inverse_transform(self, X):
-        """Convert the back data to the original representation.
-
-        In case unknown categories are encountered (all zeros in the
-        one-hot encoding), ``None`` is used to represent this category.
-
-        Parameters
-        ----------
-        X : array-like or sparse matrix, shape [n_samples, n_encoded_features]
-            The transformed data.
-
-        Returns
-        -------
-        X_tr : array-like, shape [n_samples, n_features]
+        X_tr : ndarray of shape (n_samples, n_features)
             Inverse transformed array.
-
-        """
-        # if self._legacy_mode:
-        #     raise ValueError("only supported for categorical features")
-
-        check_is_fitted(self, 'categories_')
-        X = check_array(X, accept_sparse='csr')
+        """
+        check_is_fitted(self)
+        X = check_array(X, accept_sparse="csr")

         n_samples, _ = X.shape
         n_features = len(self.categories_)
-        if self.drop is None:
-            n_transformed_features = sum(len(cats)
-                                         for cats in self.categories_)
-        else:
-            n_transformed_features = sum(len(cats) - 1
-                                         for cats in self.categories_)
+
+        n_features_out = np.sum(self._n_features_outs)

         # validate shape of passed X
-        msg = ("Shape of the passed X data is not correct. Expected {0} "
-               "columns, got {1}.")
-        if X.shape[1] != n_transformed_features:
-            raise ValueError(msg.format(n_transformed_features, X.shape[1]))
+        msg = (
+            "Shape of the passed X data is not correct. Expected {0} columns, got {1}."
+        )
+        if X.shape[1] != n_features_out:
+            raise ValueError(msg.format(n_features_out, X.shape[1]))
+
+        transformed_features = [
+            self._compute_transformed_categories(i, remove_dropped=False)
+            for i, _ in enumerate(self.categories_)
+        ]

         # create resulting array of appropriate dtype
-        dt = np.find_common_type([cat.dtype for cat in self.categories_], [])
+        dt = np.find_common_type([cat.dtype for cat in transformed_features], [])
         X_tr = np.empty((n_samples, n_features), dtype=dt)

         j = 0
         found_unknown = {}

+        if self._infrequent_enabled:
+            infrequent_indices = self._infrequent_indices
+        else:
+            infrequent_indices = [None] * n_features
+
         for i in range(n_features):
-            if self.drop is None:
-                cats = self.categories_[i]
-            else:
-                cats = np.delete(self.categories_[i], self.drop_idx_[i])
-            n_categories = len(cats)
+            cats_wo_dropped = self._remove_dropped_categories(
+                transformed_features[i], i
+            )
+            n_categories = cats_wo_dropped.shape[0]

             # Only happens if there was a column with a unique
             # category. In this case we just fill the column with this
@@ -789,22 +992,38 @@
                 X_tr[:, i] = self.categories_[i][self.drop_idx_[i]]
                 j += n_categories
                 continue
-            sub = X[:, j:j + n_categories]
+            sub = X[:, j : j + n_categories]
             # for sparse X argmax returns 2D matrix, ensure 1D array
-            labels = np.asarray(_argmax(sub, axis=1)).flatten()
-            X_tr[:, i] = cats[labels]
-            if self.handle_unknown == 'ignore':
+            labels = np.asarray(sub.argmax(axis=1)).flatten()
+            X_tr[:, i] = cats_wo_dropped[labels]
+
+            if self.handle_unknown == "ignore" or (
+                self.handle_unknown == "infrequent_if_exist"
+                and infrequent_indices[i] is None
+            ):
                 unknown = np.asarray(sub.sum(axis=1) == 0).flatten()
                 # ignored unknown categories: we have a row of all zero
                 if unknown.any():
-                    found_unknown[i] = unknown
-            # drop will either be None or handle_unknown will be error. If
-            # self.drop is not None, then we can safely assume that all of
-            # the nulls in each column are the dropped value
-            elif self.drop is not None:
+                    # if categories were dropped then unknown categories will
+                    # be mapped to the dropped category
+                    if self.drop_idx_ is None or self.drop_idx_[i] is None:
+                        found_unknown[i] = unknown
+                    else:
+                        X_tr[unknown, i] = self.categories_[i][self.drop_idx_[i]]
+            else:
                 dropped = np.asarray(sub.sum(axis=1) == 0).flatten()
                 if dropped.any():
-                    X_tr[dropped, i] = self.categories_[i][self.drop_idx_[i]]
+                    if self.drop_idx_ is None:
+                        all_zero_samples = np.flatnonzero(dropped)
+                        raise ValueError(
+                            f"Samples {all_zero_samples} can not be inverted "
+                            "when drop=None and handle_unknown='error' "
+                            "because they contain all zeros"
+                        )
+                    # we can safely assume that all of the nulls in each column
+                    # are the dropped value
+                    drop_idx = self.drop_idx_[i]
+                    X_tr[dropped, i] = transformed_features[i][drop_idx]

             j += n_categories

@@ -819,41 +1038,85 @@

         return X_tr

+    @deprecated(
+        "get_feature_names is deprecated in 1.0 and will be removed "
+        "in 1.2. Please use get_feature_names_out instead."
+    )
     def get_feature_names(self, input_features=None):
         """Return feature names for output features.

+        For a given input feature, if there is an infrequent category, the most
+        'infrequent_sklearn' will be used as a feature name.
+
         Parameters
         ----------
-        input_features : list of string, length n_features, optional
+        input_features : list of str of shape (n_features,)
             String names for input features if available. By default,
             "x0", "x1", ... "xn_features" is used.

         Returns
         -------
-        output_feature_names : array of string, length n_output_features
-
-        """
-        check_is_fitted(self, 'categories_')
-        cats = self.categories_
+        output_feature_names : ndarray of shape (n_output_features,)
+            Array of feature names.
+        """
+        check_is_fitted(self)
+        cats = [
+            self._compute_transformed_categories(i)
+            for i, _ in enumerate(self.categories_)
+        ]
         if input_features is None:
-            input_features = ['x%d' % i for i in range(len(cats))]
-        elif len(input_features) != len(self.categories_):
+            input_features = ["x%d" % i for i in range(len(cats))]
+        elif len(input_features) != len(cats):
             raise ValueError(
                 "input_features should have length equal to number of "
-                "features ({}), got {}".format(len(self.categories_),
-                                               len(input_features)))
+                "features ({}), got {}".format(len(cats), len(input_features))
+            )

         feature_names = []
         for i in range(len(cats)):
-            names = [
-                input_features[i] + '_' + str(t) for t in cats[i]]
+            names = [input_features[i] + "_" + str(t) for t in cats[i]]
             feature_names.extend(names)

         return np.array(feature_names, dtype=object)

-
-class OrdinalEncoder(_BaseEncoder):
-    """Encode categorical features as an integer array.
+    def get_feature_names_out(self, input_features=None):
+        """Get output feature names for transformation.
+
+        Parameters
+        ----------
+        input_features : array-like of str or None, default=None
+            Input features.
+
+            - If `input_features` is `None`, then `feature_names_in_` is
+              used as feature names in. If `feature_names_in_` is not defined,
+              then the following input feature names are generated:
+              `["x0", "x1", ..., "x(n_features_in_ - 1)"]`.
+            - If `input_features` is an array-like, then `input_features` must
+              match `feature_names_in_` if `feature_names_in_` is defined.
+
+        Returns
+        -------
+        feature_names_out : ndarray of str objects
+            Transformed feature names.
+        """
+        check_is_fitted(self)
+        input_features = _check_feature_names_in(self, input_features)
+        cats = [
+            self._compute_transformed_categories(i)
+            for i, _ in enumerate(self.categories_)
+        ]
+
+        feature_names = []
+        for i in range(len(cats)):
+            names = [input_features[i] + "_" + str(t) for t in cats[i]]
+            feature_names.extend(names)
+
+        return np.array(feature_names, dtype=object)
+
+
+class OrdinalEncoder(_OneToOneFeatureMixin, _BaseEncoder):
+    """
+    Encode categorical features as an integer array.

     The input to this transformer should be an array-like of integers or
     strings, denoting the values taken on by categorical (discrete) features.
@@ -862,9 +1125,11 @@

     Read more in the :ref:`User Guide <preprocessing_categorical_features>`.

+    .. versionadded:: 0.20
+
     Parameters
     ----------
-    categories : 'auto' or a list of lists/arrays of values.
+    categories : 'auto' or a list of array-like, default='auto'
         Categories (unique values) per feature:

         - 'auto' : Determine categories automatically from the training data.
@@ -877,12 +1142,53 @@
     dtype : number type, default np.float64
         Desired dtype of output.

+    handle_unknown : {'error', 'use_encoded_value'}, default='error'
+        When set to 'error' an error will be raised in case an unknown
+        categorical feature is present during transform. When set to
+        'use_encoded_value', the encoded value of unknown categories will be
+        set to the value given for the parameter `unknown_value`. In
+        :meth:`inverse_transform`, an unknown category will be denoted as None.
+
+        .. versionadded:: 0.24
+
+    unknown_value : int or np.nan, default=None
+        When the parameter handle_unknown is set to 'use_encoded_value', this
+        parameter is required and will set the encoded value of unknown
+        categories. It has to be distinct from the values used to encode any of
+        the categories in `fit`. If set to np.nan, the `dtype` parameter must
+        be a float dtype.
+
+        .. versionadded:: 0.24
+
+    encoded_missing_value : int or np.nan, default=np.nan
+        Encoded value of missing categories. If set to `np.nan`, then the `dtype`
+        parameter must be a float dtype.
+
+        .. versionadded:: 1.1
+
     Attributes
     ----------
     categories_ : list of arrays
-        The categories of each feature determined during fitting
-        (in order of the features in X and corresponding with the output
-        of ``transform``).
+        The categories of each feature determined during ``fit`` (in order of
+        the features in X and corresponding with the output of ``transform``).
+        This does not include categories that weren't seen during ``fit``.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 1.0
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    OneHotEncoder : Performs a one-hot encoding of categorical features.
+    LabelEncoder : Encodes target labels with values between 0 and
+        ``n_classes-1``.

     Examples
     --------
@@ -893,8 +1199,7 @@
     >>> enc = OrdinalEncoder()
     >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]
     >>> enc.fit(X)
-    ... # doctest: +ELLIPSIS
-    OrdinalEncoder(categories='auto', dtype=<... 'numpy.float64'>)
+    OrdinalEncoder()
     >>> enc.categories_
     [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]
     >>> enc.transform([['Female', 3], ['Male', 1]])
@@ -905,78 +1210,194 @@
     array([['Male', 1],
            ['Female', 2]], dtype=object)

-    See also
-    --------
-    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of
-      categorical features.
-    sklearn.preprocessing.LabelEncoder : encodes target labels with values
-      between 0 and n_classes-1.
+    By default, :class:`OrdinalEncoder` is lenient towards missing values by
+    propagating them.
+
+    >>> import numpy as np
+    >>> X = [['Male', 1], ['Female', 3], ['Female', np.nan]]
+    >>> enc.fit_transform(X)
+    array([[ 1.,  0.],
+           [ 0.,  1.],
+           [ 0., nan]])
+
+    You can use the parameter `encoded_missing_value` to encode missing values.
+
+    >>> enc.set_params(encoded_missing_value=-1).fit_transform(X)
+    array([[ 1.,  0.],
+           [ 0.,  1.],
+           [ 0., -1.]])
     """

-    def __init__(self, categories='auto', dtype=np.float64):
+    def __init__(
+        self,
+        *,
+        categories="auto",
+        dtype=np.float64,
+        handle_unknown="error",
+        unknown_value=None,
+        encoded_missing_value=np.nan,
+    ):
         self.categories = categories
         self.dtype = dtype
+        self.handle_unknown = handle_unknown
+        self.unknown_value = unknown_value
+        self.encoded_missing_value = encoded_missing_value

     def fit(self, X, y=None):
-        """Fit the OrdinalEncoder to X.
+        """
+        Fit the OrdinalEncoder to X.

         Parameters
         ----------
-        X : array-like, shape [n_samples, n_features]
+        X : array-like of shape (n_samples, n_features)
             The data to determine the categories of each feature.
+
+        y : None
+            Ignored. This parameter exists only for compatibility with
+            :class:`~sklearn.pipeline.Pipeline`.

         Returns
         -------
-        self
-
-        """
-        # base classes uses _categories to deal with deprecations in
-        # OneHoteEncoder: can be removed once deprecations are removed
-        self._categories = self.categories
-        self._fit(X)
+        self : object
+            Fitted encoder.
+        """
+        handle_unknown_strategies = ("error", "use_encoded_value")
+        if self.handle_unknown not in handle_unknown_strategies:
+            raise ValueError(
+                "handle_unknown should be either 'error' or "
+                f"'use_encoded_value', got {self.handle_unknown}."
+            )
+
+        if self.handle_unknown == "use_encoded_value":
+            if is_scalar_nan(self.unknown_value):
+                if np.dtype(self.dtype).kind != "f":
+                    raise ValueError(
+                        "When unknown_value is np.nan, the dtype "
+                        "parameter should be "
+                        f"a float dtype. Got {self.dtype}."
+                    )
+            elif not isinstance(self.unknown_value, numbers.Integral):
+                raise TypeError(
+                    "unknown_value should be an integer or "
+                    "np.nan when "
+                    "handle_unknown is 'use_encoded_value', "
+                    f"got {self.unknown_value}."
+                )
+        elif self.unknown_value is not None:
+            raise TypeError(
+                "unknown_value should only be set when "
+                "handle_unknown is 'use_encoded_value', "
+                f"got {self.unknown_value}."
+            )
+
+        # `_fit` will only raise an error when `self.handle_unknown="error"`
+        self._fit(X, handle_unknown=self.handle_unknown, force_all_finite="allow-nan")
+
+        if self.handle_unknown == "use_encoded_value":
+            for feature_cats in self.categories_:
+                if 0 <= self.unknown_value < len(feature_cats):
+                    raise ValueError(
+                        "The used value for unknown_value "
+                        f"{self.unknown_value} is one of the "
+                        "values already used for encoding the "
+                        "seen categories."
+                    )
+
+        # stores the missing indices per category
+        self._missing_indices = {}
+        for cat_idx, categories_for_idx in enumerate(self.categories_):
+            for i, cat in enumerate(categories_for_idx):
+                if is_scalar_nan(cat):
+                    self._missing_indices[cat_idx] = i
+                    continue
+
+        if self._missing_indices:
+            if np.dtype(self.dtype).kind != "f" and is_scalar_nan(
+                self.encoded_missing_value
+            ):
+                raise ValueError(
+                    "There are missing values in features "
+                    f"{list(self._missing_indices)}. For OrdinalEncoder to "
+                    f"encode missing values with dtype: {self.dtype}, set "
+                    "encoded_missing_value to a non-nan value, or "
+                    "set dtype to a float"
+                )
+
+            if not is_scalar_nan(self.encoded_missing_value):
+                # Features are invalid when they contain a missing category
+                # and encoded_missing_value was already used to encode a
+                # known category
+                invalid_features = [
+                    cat_idx
+                    for cat_idx, categories_for_idx in enumerate(self.categories_)
+                    if cat_idx in self._missing_indices
+                    and 0 <= self.encoded_missing_value < len(categories_for_idx)
+                ]
+
+                if invalid_features:
+                    # Use feature names if they are avaliable
+                    if hasattr(self, "feature_names_in_"):
+                        invalid_features = self.feature_names_in_[invalid_features]
+                    raise ValueError(
+                        f"encoded_missing_value ({self.encoded_missing_value}) "
+                        "is already used to encode a known category in features: "
+                        f"{invalid_features}"
+                    )

         return self

     def transform(self, X):
-        """Transform X to ordinal codes.
+        """
+        Transform X to ordinal codes.

         Parameters
         ----------
-        X : array-like, shape [n_samples, n_features]
+        X : array-like of shape (n_samples, n_features)
             The data to encode.

         Returns
         -------
-        X_out : sparse matrix or a 2-d array
+        X_out : ndarray of shape (n_samples, n_features)
             Transformed input.
-
-        """
-        X_int, _ = self._transform(X)
-        return X_int.astype(self.dtype, copy=False)
+        """
+        X_int, X_mask = self._transform(
+            X, handle_unknown=self.handle_unknown, force_all_finite="allow-nan"
+        )
+        X_trans = X_int.astype(self.dtype, copy=False)
+
+        for cat_idx, missing_idx in self._missing_indices.items():
+            X_missing_mask = X_int[:, cat_idx] == missing_idx
+            X_trans[X_missing_mask, cat_idx] = self.encoded_missing_value
+
+        # create separate category for unknown values
+        if self.handle_unknown == "use_encoded_value":
+            X_trans[~X_mask] = self.unknown_value
+        return X_trans

     def inverse_transform(self, X):
-        """Convert the data back to the original representation.
+        """
+        Convert the data back to the original representation.

         Parameters
         ----------
-        X : array-like or sparse matrix, shape [n_samples, n_encoded_features]
+        X : array-like of shape (n_samples, n_encoded_features)
             The transformed data.

         Returns
         -------
-        X_tr : array-like, shape [n_samples, n_features]
+        X_tr : ndarray of shape (n_samples, n_features)
             Inverse transformed array.
-
-        """
-        check_is_fitted(self, 'categories_')
-        X = check_array(X, accept_sparse='csr')
+        """
+        check_is_fitted(self)
+        X = check_array(X, force_all_finite="allow-nan")

         n_samples, _ = X.shape
         n_features = len(self.categories_)

         # validate shape of passed X
-        msg = ("Shape of the passed X data is not correct. Expected {0} "
-               "columns, got {1}.")
+        msg = (
+            "Shape of the passed X data is not correct. Expected {0} columns, got {1}."
+        )
         if X.shape[1] != n_features:
             raise ValueError(msg.format(n_features, X.shape[1]))

@@ -984,11 +1405,28 @@
         dt = np.find_common_type([cat.dtype for cat in self.categories_], [])
         X_tr = np.empty((n_samples, n_features), dtype=dt)

+        found_unknown = {}
+
         for i in range(n_features):
-            labels = X[:, i].astype('int64', copy=False)
-            X_tr[:, i] = self.categories_[i][labels]
+            labels = X[:, i].astype("int64", copy=False)
+
+            # replace values of X[:, i] that were nan with actual indices
+            if i in self._missing_indices:
+                X_i_mask = _get_mask(X[:, i], self.encoded_missing_value)
+                labels[X_i_mask] = self._missing_indices[i]
+
+            if self.handle_unknown == "use_encoded_value":
+                unknown_labels = labels == self.unknown_value
+                X_tr[:, i] = self.categories_[i][np.where(unknown_labels, 0, labels)]
+                found_unknown[i] = unknown_labels
+            else:
+                X_tr[:, i] = self.categories_[i][labels]
+
+        # insert None values for unknown values
+        if found_unknown:
+            X_tr = X_tr.astype(object, copy=False)
+
+            for idx, mask in found_unknown.items():
+                X_tr[mask, idx] = None

         return X_tr
-
-    def _more_tags(self):
-        return {'X_types': ['categorical']}
('sklearn/preprocessing', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,70 +1,70 @@
 """
 The :mod:`sklearn.preprocessing` module includes scaling, centering,
-normalization, binarization and imputation methods.
+normalization, binarization methods.
 """

 from ._function_transformer import FunctionTransformer

-from .data import Binarizer
-from .data import KernelCenterer
-from .data import MinMaxScaler
-from .data import MaxAbsScaler
-from .data import Normalizer
-from .data import RobustScaler
-from .data import StandardScaler
-from .data import QuantileTransformer
-from .data import add_dummy_feature
-from .data import binarize
-from .data import normalize
-from .data import scale
-from .data import robust_scale
-from .data import maxabs_scale
-from .data import minmax_scale
-from .data import quantile_transform
-from .data import power_transform
-from .data import PowerTransformer
-from .data import PolynomialFeatures
+from ._data import Binarizer
+from ._data import KernelCenterer
+from ._data import MinMaxScaler
+from ._data import MaxAbsScaler
+from ._data import Normalizer
+from ._data import RobustScaler
+from ._data import StandardScaler
+from ._data import QuantileTransformer
+from ._data import add_dummy_feature
+from ._data import binarize
+from ._data import normalize
+from ._data import scale
+from ._data import robust_scale
+from ._data import maxabs_scale
+from ._data import minmax_scale
+from ._data import quantile_transform
+from ._data import power_transform
+from ._data import PowerTransformer

 from ._encoders import OneHotEncoder
 from ._encoders import OrdinalEncoder

-from .label import label_binarize
-from .label import LabelBinarizer
-from .label import LabelEncoder
-from .label import MultiLabelBinarizer
+from ._label import label_binarize
+from ._label import LabelBinarizer
+from ._label import LabelEncoder
+from ._label import MultiLabelBinarizer

 from ._discretization import KBinsDiscretizer

-from .imputation import Imputer
+from ._polynomial import PolynomialFeatures
+from ._polynomial import SplineTransformer


 __all__ = [
-    'Binarizer',
-    'FunctionTransformer',
-    'Imputer',
-    'KBinsDiscretizer',
-    'KernelCenterer',
-    'LabelBinarizer',
-    'LabelEncoder',
-    'MultiLabelBinarizer',
-    'MinMaxScaler',
-    'MaxAbsScaler',
-    'QuantileTransformer',
-    'Normalizer',
-    'OneHotEncoder',
-    'OrdinalEncoder',
-    'PowerTransformer',
-    'RobustScaler',
-    'StandardScaler',
-    'add_dummy_feature',
-    'PolynomialFeatures',
-    'binarize',
-    'normalize',
-    'scale',
-    'robust_scale',
-    'maxabs_scale',
-    'minmax_scale',
-    'label_binarize',
-    'quantile_transform',
-    'power_transform',
+    "Binarizer",
+    "FunctionTransformer",
+    "KBinsDiscretizer",
+    "KernelCenterer",
+    "LabelBinarizer",
+    "LabelEncoder",
+    "MultiLabelBinarizer",
+    "MinMaxScaler",
+    "MaxAbsScaler",
+    "QuantileTransformer",
+    "Normalizer",
+    "OneHotEncoder",
+    "OrdinalEncoder",
+    "PowerTransformer",
+    "RobustScaler",
+    "SplineTransformer",
+    "StandardScaler",
+    "add_dummy_feature",
+    "PolynomialFeatures",
+    "binarize",
+    "normalize",
+    "scale",
+    "robust_scale",
+    "maxabs_scale",
+    "minmax_scale",
+    "label_binarize",
+    "quantile_transform",
+    "power_transform",
 ]
('sklearn/preprocessing', '_discretization.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,5 +1,3 @@
-# -*- coding: utf-8 -*-
-
 # Author: Henry Lin <hlin117@gmail.com>
 #         Tom Dupré la Tour

@@ -15,62 +13,123 @@
 from ..base import BaseEstimator, TransformerMixin
 from ..utils.validation import check_array
 from ..utils.validation import check_is_fitted
-from ..utils.validation import FLOAT_DTYPES
-
-
-class KBinsDiscretizer(BaseEstimator, TransformerMixin):
-    """Bin continuous data into intervals.
+from ..utils.validation import check_random_state
+from ..utils.validation import _check_feature_names_in
+from ..utils.validation import check_scalar
+from ..utils import _safe_indexing
+
+
+class KBinsDiscretizer(TransformerMixin, BaseEstimator):
+    """
+    Bin continuous data into intervals.

     Read more in the :ref:`User Guide <preprocessing_discretization>`.
+
+    .. versionadded:: 0.20

     Parameters
     ----------
-    n_bins : int or array-like, shape (n_features,) (default=5)
+    n_bins : int or array-like of shape (n_features,), default=5
         The number of bins to produce. Raises ValueError if ``n_bins < 2``.

-    encode : {'onehot', 'onehot-dense', 'ordinal'}, (default='onehot')
+    encode : {'onehot', 'onehot-dense', 'ordinal'}, default='onehot'
         Method used to encode the transformed result.

-        onehot
-            Encode the transformed result with one-hot encoding
-            and return a sparse matrix. Ignored features are always
-            stacked to the right.
-        onehot-dense
-            Encode the transformed result with one-hot encoding
-            and return a dense array. Ignored features are always
-            stacked to the right.
-        ordinal
-            Return the bin identifier encoded as an integer value.
-
-    strategy : {'uniform', 'quantile', 'kmeans'}, (default='quantile')
+        - 'onehot': Encode the transformed result with one-hot encoding
+          and return a sparse matrix. Ignored features are always
+          stacked to the right.
+        - 'onehot-dense': Encode the transformed result with one-hot encoding
+          and return a dense array. Ignored features are always
+          stacked to the right.
+        - 'ordinal': Return the bin identifier encoded as an integer value.
+
+    strategy : {'uniform', 'quantile', 'kmeans'}, default='quantile'
         Strategy used to define the widths of the bins.

-        uniform
-            All bins in each feature have identical widths.
-        quantile
-            All bins in each feature have the same number of points.
-        kmeans
-            Values in each bin have the same nearest center of a 1D k-means
-            cluster.
+        - 'uniform': All bins in each feature have identical widths.
+        - 'quantile': All bins in each feature have the same number of points.
+        - 'kmeans': Values in each bin have the same nearest center of a 1D
+          k-means cluster.
+
+    dtype : {np.float32, np.float64}, default=None
+        The desired data-type for the output. If None, output dtype is
+        consistent with input dtype. Only np.float32 and np.float64 are
+        supported.
+
+        .. versionadded:: 0.24
+
+    subsample : int or None (default='warn')
+        Maximum number of samples, used to fit the model, for computational
+        efficiency. Used when `strategy="quantile"`.
+        `subsample=None` means that all the training samples are used when
+        computing the quantiles that determine the binning thresholds.
+        Since quantile computation relies on sorting each column of `X` and
+        that sorting has an `n log(n)` time complexity,
+        it is recommended to use subsampling on datasets with a
+        very large number of samples.
+
+        .. deprecated:: 1.1
+           In version 1.3 and onwards, `subsample=2e5` will be the default.
+
+    random_state : int, RandomState instance or None, default=None
+        Determines random number generation for subsampling.
+        Pass an int for reproducible results across multiple function calls.
+        See the `subsample` parameter for more details.
+        See :term:`Glossary <random_state>`.
+
+        .. versionadded:: 1.1

     Attributes
     ----------
-    n_bins_ : int array, shape (n_features,)
+    bin_edges_ : ndarray of ndarray of shape (n_features,)
+        The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``
+        Ignored features will have empty arrays.
+
+    n_bins_ : ndarray of shape (n_features,), dtype=np.int_
         Number of bins per feature. Bins whose width are too small
         (i.e., <= 1e-8) are removed with a warning.

-    bin_edges_ : array of arrays, shape (n_features, )
-        The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``
-        Ignored features will have empty arrays.
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    Binarizer : Class used to bin values as ``0`` or
+        ``1`` based on a parameter ``threshold``.
+
+    Notes
+    -----
+    In bin edges for feature ``i``, the first and last values are used only for
+    ``inverse_transform``. During transform, bin edges are extended to::
+
+      np.concatenate([-np.inf, bin_edges_[i][1:-1], np.inf])
+
+    You can combine ``KBinsDiscretizer`` with
+    :class:`~sklearn.compose.ColumnTransformer` if you only want to preprocess
+    part of the features.
+
+    ``KBinsDiscretizer`` might produce constant features (e.g., when
+    ``encode = 'onehot'`` and certain bins do not contain any data).
+    These features can be removed with feature selection algorithms
+    (e.g., :class:`~sklearn.feature_selection.VarianceThreshold`).

     Examples
     --------
+    >>> from sklearn.preprocessing import KBinsDiscretizer
     >>> X = [[-2, 1, -4,   -1],
     ...      [-1, 2, -3, -0.5],
     ...      [ 0, 3, -2,  0.5],
     ...      [ 1, 4, -1,    2]]
     >>> est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
-    >>> est.fit(X)  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
+    >>> est.fit(X)
     KBinsDiscretizer(...)
     >>> Xt = est.transform(X)
     >>> Xt  # doctest: +SKIP
@@ -91,60 +150,100 @@
            [-0.5,  2.5, -2.5, -0.5],
            [ 0.5,  3.5, -1.5,  0.5],
            [ 0.5,  3.5, -1.5,  1.5]])
-
-    Notes
-    -----
-    In bin edges for feature ``i``, the first and last values are used only for
-    ``inverse_transform``. During transform, bin edges are extended to::
-
-      np.concatenate([-np.inf, bin_edges_[i][1:-1], np.inf])
-
-    You can combine ``KBinsDiscretizer`` with
-    :class:`sklearn.compose.ColumnTransformer` if you only want to preprocess
-    part of the features.
-
-    ``KBinsDiscretizer`` might produce constant features (e.g., when
-    ``encode = 'onehot'`` and certain bins do not contain any data).
-    These features can be removed with feature selection algorithms
-    (e.g., :class:`sklearn.feature_selection.VarianceThreshold`).
-
-    See also
-    --------
-     sklearn.preprocessing.Binarizer : class used to bin values as ``0`` or
-        ``1`` based on a parameter ``threshold``.
     """

-    def __init__(self, n_bins=5, encode='onehot', strategy='quantile'):
+    def __init__(
+        self,
+        n_bins=5,
+        *,
+        encode="onehot",
+        strategy="quantile",
+        dtype=None,
+        subsample="warn",
+        random_state=None,
+    ):
         self.n_bins = n_bins
         self.encode = encode
         self.strategy = strategy
+        self.dtype = dtype
+        self.subsample = subsample
+        self.random_state = random_state

     def fit(self, X, y=None):
-        """Fits the estimator.
+        """
+        Fit the estimator.

         Parameters
         ----------
-        X : numeric array-like, shape (n_samples, n_features)
+        X : array-like of shape (n_samples, n_features)
             Data to be discretized.

-        y : ignored
+        y : None
+            Ignored. This parameter exists only for compatibility with
+            :class:`~sklearn.pipeline.Pipeline`.

         Returns
         -------
-        self
-        """
-        X = check_array(X, dtype='numeric')
-
-        valid_encode = ('onehot', 'onehot-dense', 'ordinal')
+        self : object
+            Returns the instance itself.
+        """
+        X = self._validate_data(X, dtype="numeric")
+
+        supported_dtype = (np.float64, np.float32)
+        if self.dtype in supported_dtype:
+            output_dtype = self.dtype
+        elif self.dtype is None:
+            output_dtype = X.dtype
+        else:
+            raise ValueError(
+                "Valid options for 'dtype' are "
+                f"{supported_dtype + (None,)}. Got dtype={self.dtype} "
+                " instead."
+            )
+
+        n_samples, n_features = X.shape
+
+        if self.strategy == "quantile" and self.subsample is not None:
+            if self.subsample == "warn":
+                if n_samples > 2e5:
+                    warnings.warn(
+                        "In version 1.3 onwards, subsample=2e5 "
+                        "will be used by default. Set subsample explicitly to "
+                        "silence this warning in the mean time. Set "
+                        "subsample=None to disable subsampling explicitly.",
+                        FutureWarning,
+                    )
+            else:
+                self.subsample = check_scalar(
+                    self.subsample, "subsample", numbers.Integral, min_val=1
+                )
+                rng = check_random_state(self.random_state)
+                if n_samples > self.subsample:
+                    subsample_idx = rng.choice(
+                        n_samples, size=self.subsample, replace=False
+                    )
+                    X = _safe_indexing(X, subsample_idx)
+        elif self.strategy != "quantile" and isinstance(
+            self.subsample, numbers.Integral
+        ):
+            raise ValueError(
+                f"Invalid parameter for `strategy`: {self.strategy}. "
+                '`subsample` must be used with `strategy="quantile"`.'
+            )
+
+        valid_encode = ("onehot", "onehot-dense", "ordinal")
         if self.encode not in valid_encode:
-            raise ValueError("Valid options for 'encode' are {}. "
-                             "Got encode={!r} instead."
-                             .format(valid_encode, self.encode))
-        valid_strategy = ('uniform', 'quantile', 'kmeans')
+            raise ValueError(
+                "Valid options for 'encode' are {}. Got encode={!r} instead.".format(
+                    valid_encode, self.encode
+                )
+            )
+        valid_strategy = ("uniform", "quantile", "kmeans")
         if self.strategy not in valid_strategy:
-            raise ValueError("Valid options for 'strategy' are {}. "
-                             "Got strategy={!r} instead."
-                             .format(valid_strategy, self.strategy))
+            raise ValueError(
+                "Valid options for 'strategy' are {}. "
+                "Got strategy={!r} instead.".format(valid_strategy, self.strategy)
+            )

         n_features = X.shape[1]
         n_bins = self._validate_n_bins(n_features)
@@ -155,20 +254,21 @@
             col_min, col_max = column.min(), column.max()

             if col_min == col_max:
-                warnings.warn("Feature %d is constant and will be "
-                              "replaced with 0." % jj)
+                warnings.warn(
+                    "Feature %d is constant and will be replaced with 0." % jj
+                )
                 n_bins[jj] = 1
                 bin_edges[jj] = np.array([-np.inf, np.inf])
                 continue

-            if self.strategy == 'uniform':
+            if self.strategy == "uniform":
                 bin_edges[jj] = np.linspace(col_min, col_max, n_bins[jj] + 1)

-            elif self.strategy == 'quantile':
+            elif self.strategy == "quantile":
                 quantiles = np.linspace(0, 100, n_bins[jj] + 1)
                 bin_edges[jj] = np.asarray(np.percentile(column, quantiles))

-            elif self.strategy == 'kmeans':
+            elif self.strategy == "kmeans":
                 from ..cluster import KMeans  # fixes import loops

                 # Deterministic initialization with uniform spacing
@@ -184,126 +284,140 @@
                 bin_edges[jj] = np.r_[col_min, bin_edges[jj], col_max]

             # Remove bins whose width are too small (i.e., <= 1e-8)
-            if self.strategy in ('quantile', 'kmeans'):
+            if self.strategy in ("quantile", "kmeans"):
                 mask = np.ediff1d(bin_edges[jj], to_begin=np.inf) > 1e-8
                 bin_edges[jj] = bin_edges[jj][mask]
                 if len(bin_edges[jj]) - 1 != n_bins[jj]:
-                    warnings.warn('Bins whose width are too small (i.e., <= '
-                                  '1e-8) in feature %d are removed. Consider '
-                                  'decreasing the number of bins.' % jj)
+                    warnings.warn(
+                        "Bins whose width are too small (i.e., <= "
+                        "1e-8) in feature %d are removed. Consider "
+                        "decreasing the number of bins." % jj
+                    )
                     n_bins[jj] = len(bin_edges[jj]) - 1

         self.bin_edges_ = bin_edges
         self.n_bins_ = n_bins

-        if 'onehot' in self.encode:
+        if "onehot" in self.encode:
             self._encoder = OneHotEncoder(
                 categories=[np.arange(i) for i in self.n_bins_],
-                sparse=self.encode == 'onehot')
+                sparse=self.encode == "onehot",
+                dtype=output_dtype,
+            )
             # Fit the OneHotEncoder with toy datasets
             # so that it's ready for use after the KBinsDiscretizer is fitted
-            self._encoder.fit(np.zeros((1, len(self.n_bins_)), dtype=int))
+            self._encoder.fit(np.zeros((1, len(self.n_bins_))))

         return self

     def _validate_n_bins(self, n_features):
-        """Returns n_bins_, the number of bins per feature.
-        """
+        """Returns n_bins_, the number of bins per feature."""
         orig_bins = self.n_bins
         if isinstance(orig_bins, numbers.Number):
-            if not isinstance(orig_bins, (numbers.Integral, np.integer)):
-                raise ValueError("{} received an invalid n_bins type. "
-                                 "Received {}, expected int."
-                                 .format(KBinsDiscretizer.__name__,
-                                         type(orig_bins).__name__))
+            if not isinstance(orig_bins, numbers.Integral):
+                raise ValueError(
+                    "{} received an invalid n_bins type. "
+                    "Received {}, expected int.".format(
+                        KBinsDiscretizer.__name__, type(orig_bins).__name__
+                    )
+                )
             if orig_bins < 2:
-                raise ValueError("{} received an invalid number "
-                                 "of bins. Received {}, expected at least 2."
-                                 .format(KBinsDiscretizer.__name__, orig_bins))
-            return np.full(n_features, orig_bins, dtype=np.int)
-
-        n_bins = check_array(orig_bins, dtype=np.int, copy=True,
-                             ensure_2d=False)
+                raise ValueError(
+                    "{} received an invalid number "
+                    "of bins. Received {}, expected at least 2.".format(
+                        KBinsDiscretizer.__name__, orig_bins
+                    )
+                )
+            return np.full(n_features, orig_bins, dtype=int)
+
+        n_bins = check_array(orig_bins, dtype=int, copy=True, ensure_2d=False)

         if n_bins.ndim > 1 or n_bins.shape[0] != n_features:
-            raise ValueError("n_bins must be a scalar or array "
-                             "of shape (n_features,).")
+            raise ValueError("n_bins must be a scalar or array of shape (n_features,).")

         bad_nbins_value = (n_bins < 2) | (n_bins != orig_bins)

         violating_indices = np.where(bad_nbins_value)[0]
         if violating_indices.shape[0] > 0:
             indices = ", ".join(str(i) for i in violating_indices)
-            raise ValueError("{} received an invalid number "
-                             "of bins at indices {}. Number of bins "
-                             "must be at least 2, and must be an int."
-                             .format(KBinsDiscretizer.__name__, indices))
+            raise ValueError(
+                "{} received an invalid number "
+                "of bins at indices {}. Number of bins "
+                "must be at least 2, and must be an int.".format(
+                    KBinsDiscretizer.__name__, indices
+                )
+            )
         return n_bins

     def transform(self, X):
-        """Discretizes the data.
+        """
+        Discretize the data.

         Parameters
         ----------
-        X : numeric array-like, shape (n_samples, n_features)
+        X : array-like of shape (n_samples, n_features)
             Data to be discretized.

         Returns
         -------
-        Xt : numeric array-like or sparse matrix
-            Data in the binned space.
-        """
-        check_is_fitted(self, ["bin_edges_"])
-
-        Xt = check_array(X, copy=True, dtype=FLOAT_DTYPES)
-        n_features = self.n_bins_.shape[0]
-        if Xt.shape[1] != n_features:
-            raise ValueError("Incorrect number of features. Expecting {}, "
-                             "received {}.".format(n_features, Xt.shape[1]))
+        Xt : {ndarray, sparse matrix}, dtype={np.float32, np.float64}
+            Data in the binned space. Will be a sparse matrix if
+            `self.encode='onehot'` and ndarray otherwise.
+        """
+        check_is_fitted(self)
+
+        # check input and attribute dtypes
+        dtype = (np.float64, np.float32) if self.dtype is None else self.dtype
+        Xt = self._validate_data(X, copy=True, dtype=dtype, reset=False)

         bin_edges = self.bin_edges_
         for jj in range(Xt.shape[1]):
-            # Values which are close to a bin edge are susceptible to numeric
-            # instability. Add eps to X so these values are binned correctly
-            # with respect to their decimal truncation. See documentation of
-            # numpy.isclose for an explanation of ``rtol`` and ``atol``.
-            rtol = 1.e-5
-            atol = 1.e-8
-            eps = atol + rtol * np.abs(Xt[:, jj])
-            Xt[:, jj] = np.digitize(Xt[:, jj] + eps, bin_edges[jj][1:])
-        np.clip(Xt, 0, self.n_bins_ - 1, out=Xt)
-
-        if self.encode == 'ordinal':
+            Xt[:, jj] = np.searchsorted(bin_edges[jj][1:-1], Xt[:, jj], side="right")
+
+        if self.encode == "ordinal":
             return Xt

-        return self._encoder.transform(Xt)
+        dtype_init = None
+        if "onehot" in self.encode:
+            dtype_init = self._encoder.dtype
+            self._encoder.dtype = Xt.dtype
+        try:
+            Xt_enc = self._encoder.transform(Xt)
+        finally:
+            # revert the initial dtype to avoid modifying self.
+            self._encoder.dtype = dtype_init
+        return Xt_enc

     def inverse_transform(self, Xt):
-        """Transforms discretized data back to original feature space.
+        """
+        Transform discretized data back to original feature space.

         Note that this function does not regenerate the original data
         due to discretization rounding.

         Parameters
         ----------
-        Xt : numeric array-like, shape (n_sample, n_features)
+        Xt : array-like of shape (n_samples, n_features)
             Transformed data in the binned space.

         Returns
         -------
-        Xinv : numeric array-like
+        Xinv : ndarray, dtype={np.float32, np.float64}
             Data in the original feature space.
         """
-        check_is_fitted(self, ["bin_edges_"])
-
-        if 'onehot' in self.encode:
+        check_is_fitted(self)
+
+        if "onehot" in self.encode:
             Xt = self._encoder.inverse_transform(Xt)

-        Xinv = check_array(Xt, copy=True, dtype=FLOAT_DTYPES)
+        Xinv = check_array(Xt, copy=True, dtype=(np.float64, np.float32))
         n_features = self.n_bins_.shape[0]
         if Xinv.shape[1] != n_features:
-            raise ValueError("Incorrect number of features. Expecting {}, "
-                             "received {}.".format(n_features, Xinv.shape[1]))
+            raise ValueError(
+                "Incorrect number of features. Expecting {}, received {}.".format(
+                    n_features, Xinv.shape[1]
+                )
+            )

         for jj in range(n_features):
             bin_edges = self.bin_edges_[jj]
@@ -311,3 +425,30 @@
             Xinv[:, jj] = bin_centers[np.int_(Xinv[:, jj])]

         return Xinv
+
+    def get_feature_names_out(self, input_features=None):
+        """Get output feature names.
+
+        Parameters
+        ----------
+        input_features : array-like of str or None, default=None
+            Input features.
+
+            - If `input_features` is `None`, then `feature_names_in_` is
+              used as feature names in. If `feature_names_in_` is not defined,
+              then the following input feature names are generated:
+              `["x0", "x1", ..., "x(n_features_in_ - 1)"]`.
+            - If `input_features` is an array-like, then `input_features` must
+              match `feature_names_in_` if `feature_names_in_` is defined.
+
+        Returns
+        -------
+        feature_names_out : ndarray of str objects
+            Transformed feature names.
+        """
+        input_features = _check_feature_names_in(self, input_features)
+        if hasattr(self, "_encoder"):
+            return self._encoder.get_feature_names_out(input_features)
+
+        # ordinal encoding
+        return input_features
('sklearn/preprocessing', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,20 +1,22 @@
 import os


-def configuration(parent_package='', top_path=None):
+def configuration(parent_package="", top_path=None):
     import numpy
     from numpy.distutils.misc_util import Configuration

-    config = Configuration('preprocessing', parent_package, top_path)
+    config = Configuration("preprocessing", parent_package, top_path)
     libraries = []
-    if os.name == 'posix':
-        libraries.append('m')
+    if os.name == "posix":
+        libraries.append("m")

-    config.add_extension('_csr_polynomial_expansion',
-                         sources=['_csr_polynomial_expansion.pyx'],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
+    config.add_extension(
+        "_csr_polynomial_expansion",
+        sources=["_csr_polynomial_expansion.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )

-    config.add_subpackage('tests')
+    config.add_subpackage("tests")

     return config
('sklearn/preprocessing', '_csr_polynomial_expansion.pyx')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,13 +1,10 @@
-# cython: cdivision=True
-# cython: boundscheck=False
-# cython: wraparound=False
-
 # Author: Andrew nystrom <awnystrom@gmail.com>

 from scipy.sparse import csr_matrix
 from numpy cimport ndarray
 cimport numpy as np

+np.import_array()
 ctypedef np.int32_t INDEX_T

 ctypedef fused DATA_T:
('sklearn/preprocessing', '_function_transformer.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,17 +1,22 @@
 import warnings

+import numpy as np
+
 from ..base import BaseEstimator, TransformerMixin
-from ..utils import check_array
-from ..utils.testing import assert_allclose_dense_sparse
+from ..utils.metaestimators import available_if
+from ..utils.validation import (
+    _allclose_dense_sparse,
+    _check_feature_names_in,
+    check_array,
+)


 def _identity(X):
-    """The identity function.
-    """
+    """The identity function."""
     return X


-class FunctionTransformer(BaseEstimator, TransformerMixin):
+class FunctionTransformer(TransformerMixin, BaseEstimator):
     """Constructs a transformer from an arbitrary callable.

     A FunctionTransformer forwards its X (and optionally y) arguments to a
@@ -28,18 +33,18 @@

     Parameters
     ----------
-    func : callable, optional default=None
+    func : callable, default=None
         The callable to use for the transformation. This will be passed
         the same arguments as transform, with args and kwargs forwarded.
         If func is None, then func will be the identity function.

-    inverse_func : callable, optional default=None
+    inverse_func : callable, default=None
         The callable to use for the inverse transformation. This will be
         passed the same arguments as inverse transform, with args and
         kwargs forwarded. If inverse_func is None, then inverse_func
         will be the identity function.

-    validate : bool, optional default=True
+    validate : bool, default=False
         Indicate that the input X array should be checked before calling
         ``func``. The possibilities are:

@@ -48,20 +53,13 @@
           sparse matrix. If the conversion is not possible an exception is
           raised.

-        .. deprecated:: 0.20
-           ``validate=True`` as default will be replaced by
-           ``validate=False`` in 0.22.
-
-    accept_sparse : boolean, optional
+        .. versionchanged:: 0.22
+           The default of ``validate`` changed from True to False.
+
+    accept_sparse : bool, default=False
         Indicate that func accepts a sparse matrix as input. If validate is
         False, this has no effect. Otherwise, if accept_sparse is false,
         sparse matrix inputs will cause an exception to be raised.
-
-    pass_y : bool, optional default=False
-        Indicate that transform should forward the y argument to the
-        inner callable.
-
-        .. deprecated:: 0.19

     check_inverse : bool, default=True
        Whether to check that or ``func`` followed by ``inverse_func`` leads to
@@ -70,50 +68,109 @@

        .. versionadded:: 0.20

-    kw_args : dict, optional
+    feature_names_out : callable, 'one-to-one' or None, default=None
+        Determines the list of feature names that will be returned by the
+        `get_feature_names_out` method. If it is 'one-to-one', then the output
+        feature names will be equal to the input feature names. If it is a
+        callable, then it must take two positional arguments: this
+        `FunctionTransformer` (`self`) and an array-like of input feature names
+        (`input_features`). It must return an array-like of output feature
+        names. The `get_feature_names_out` method is only defined if
+        `feature_names_out` is not None.
+
+        See ``get_feature_names_out`` for more details.
+
+        .. versionadded:: 1.1
+
+    kw_args : dict, default=None
         Dictionary of additional keyword arguments to pass to func.

-    inv_kw_args : dict, optional
+        .. versionadded:: 0.18
+
+    inv_kw_args : dict, default=None
         Dictionary of additional keyword arguments to pass to inverse_func.

+        .. versionadded:: 0.18
+
+    Attributes
+    ----------
+    n_features_in_ : int
+        Number of features seen during :term:`fit`. Defined only when
+        `validate=True`.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `validate=True`
+        and `X` has feature names that are all strings.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    MaxAbsScaler : Scale each feature by its maximum absolute value.
+    StandardScaler : Standardize features by removing the mean and
+        scaling to unit variance.
+    LabelBinarizer : Binarize labels in a one-vs-all fashion.
+    MultiLabelBinarizer : Transform between iterable of iterables
+        and a multilabel format.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.preprocessing import FunctionTransformer
+    >>> transformer = FunctionTransformer(np.log1p)
+    >>> X = np.array([[0, 1], [2, 3]])
+    >>> transformer.transform(X)
+    array([[0.       , 0.6931...],
+           [1.0986..., 1.3862...]])
     """
-    def __init__(self, func=None, inverse_func=None, validate=None,
-                 accept_sparse=False, pass_y='deprecated', check_inverse=True,
-                 kw_args=None, inv_kw_args=None):
+
+    def __init__(
+        self,
+        func=None,
+        inverse_func=None,
+        *,
+        validate=False,
+        accept_sparse=False,
+        check_inverse=True,
+        feature_names_out=None,
+        kw_args=None,
+        inv_kw_args=None,
+    ):
         self.func = func
         self.inverse_func = inverse_func
         self.validate = validate
         self.accept_sparse = accept_sparse
-        self.pass_y = pass_y
         self.check_inverse = check_inverse
+        self.feature_names_out = feature_names_out
         self.kw_args = kw_args
         self.inv_kw_args = inv_kw_args

-    def _check_input(self, X):
-        # FIXME: Future warning to be removed in 0.22
-        if self.validate is None:
-            self._validate = True
-            warnings.warn("The default validate=True will be replaced by "
-                          "validate=False in 0.22.", FutureWarning)
-        else:
-            self._validate = self.validate
-
-        if self._validate:
-            return check_array(X, accept_sparse=self.accept_sparse)
+    def _check_input(self, X, *, reset):
+        if self.validate:
+            return self._validate_data(X, accept_sparse=self.accept_sparse, reset=reset)
         return X

     def _check_inverse_transform(self, X):
         """Check that func and inverse_func are the inverse."""
         idx_selected = slice(None, None, max(1, X.shape[0] // 100))
-        try:
-            assert_allclose_dense_sparse(
-                X[idx_selected],
-                self.inverse_transform(self.transform(X[idx_selected])))
-        except AssertionError:
-            warnings.warn("The provided functions are not strictly"
-                          " inverse of each other. If you are sure you"
-                          " want to proceed regardless, set"
-                          " 'check_inverse=False'.", UserWarning)
+        X_round_trip = self.inverse_transform(self.transform(X[idx_selected]))
+
+        if not np.issubdtype(X.dtype, np.number):
+            raise ValueError(
+                "'check_inverse' is only supported when all the elements in `X` is"
+                " numerical."
+            )
+
+        if not _allclose_dense_sparse(X[idx_selected], X_round_trip):
+            warnings.warn(
+                "The provided functions are not strictly"
+                " inverse of each other. If you are sure you"
+                " want to proceed regardless, set"
+                " 'check_inverse=False'.",
+                UserWarning,
+            )

     def fit(self, X, y=None):
         """Fit transformer by checking X.
@@ -125,13 +182,16 @@
         X : array-like, shape (n_samples, n_features)
             Input array.

-        Returns
-        -------
-        self
-        """
-        X = self._check_input(X)
-        if (self.check_inverse and not (self.func is None or
-                                        self.inverse_func is None)):
+        y : Ignored
+            Not used, present here for API consistency by convention.
+
+        Returns
+        -------
+        self : object
+            FunctionTransformer class instance.
+        """
+        X = self._check_input(X, reset=True)
+        if self.check_inverse and not (self.func is None or self.inverse_func is None):
             self._check_inverse_transform(X)
         return self

@@ -143,13 +203,12 @@
         X : array-like, shape (n_samples, n_features)
             Input array.

-
-
         Returns
         -------
         X_out : array-like, shape (n_samples, n_features)
             Transformed input.
         """
+        X = self._check_input(X, reset=False)
         return self._transform(X, func=self.func, kw_args=self.kw_args)

     def inverse_transform(self, X):
@@ -160,24 +219,81 @@
         X : array-like, shape (n_samples, n_features)
             Input array.

-
-
         Returns
         -------
         X_out : array-like, shape (n_samples, n_features)
             Transformed input.
         """
-        return self._transform(X, func=self.inverse_func,
-                               kw_args=self.inv_kw_args)
+        if self.validate:
+            X = check_array(X, accept_sparse=self.accept_sparse)
+        return self._transform(X, func=self.inverse_func, kw_args=self.inv_kw_args)
+
+    @available_if(lambda self: self.feature_names_out is not None)
+    def get_feature_names_out(self, input_features=None):
+        """Get output feature names for transformation.
+
+        This method is only defined if `feature_names_out` is not None.
+
+        Parameters
+        ----------
+        input_features : array-like of str or None, default=None
+            Input feature names.
+
+            - If `input_features` is None, then `feature_names_in_` is
+              used as the input feature names. If `feature_names_in_` is not
+              defined, then names are generated:
+              `[x0, x1, ..., x(n_features_in_ - 1)]`.
+            - If `input_features` is array-like, then `input_features` must
+              match `feature_names_in_` if `feature_names_in_` is defined.
+
+        Returns
+        -------
+        feature_names_out : ndarray of str objects
+            Transformed feature names.
+
+            - If `feature_names_out` is 'one-to-one', the input feature names
+              are returned (see `input_features` above). This requires
+              `feature_names_in_` and/or `n_features_in_` to be defined, which
+              is done automatically if `validate=True`. Alternatively, you can
+              set them in `func`.
+            - If `feature_names_out` is a callable, then it is called with two
+              arguments, `self` and `input_features`, and its return value is
+              returned by this method.
+        """
+        if hasattr(self, "n_features_in_") or input_features is not None:
+            input_features = _check_feature_names_in(self, input_features)
+        if self.feature_names_out == "one-to-one":
+            if input_features is None:
+                raise ValueError(
+                    "When 'feature_names_out' is 'one-to-one', either "
+                    "'input_features' must be passed, or 'feature_names_in_' "
+                    "and/or 'n_features_in_' must be defined. If you set "
+                    "'validate' to 'True', then they will be defined "
+                    "automatically when 'fit' is called. Alternatively, you "
+                    "can set them in 'func'."
+                )
+            names_out = input_features
+        elif callable(self.feature_names_out):
+            names_out = self.feature_names_out(self, input_features)
+        else:
+            raise ValueError(
+                f"feature_names_out={self.feature_names_out!r} is invalid. "
+                'It must either be "one-to-one" or a callable with two '
+                "arguments: the function transformer and an array-like of "
+                "input feature names. The callable must return an array-like "
+                "of output feature names."
+            )
+        return np.asarray(names_out, dtype=object)

     def _transform(self, X, func=None, kw_args=None):
-        X = self._check_input(X)
-
         if func is None:
             func = _identity

         return func(X, **(kw_args if kw_args else {}))

+    def __sklearn_is_fitted__(self):
+        """Return True since FunctionTransfomer is stateless."""
+        return True
+
     def _more_tags(self):
-        return {'no_validation': True,
-                'stateless': True}
+        return {"no_validation": not self.validate, "stateless": True}
('sklearn/model_selection', '_search.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -15,11 +15,13 @@
 from collections.abc import Mapping, Sequence, Iterable
 from functools import partial, reduce
 from itertools import product
+import numbers
 import operator
 import time
 import warnings

 import numpy as np
+from numpy.ma import MaskedArray
 from scipy.stats import rankdata

 from ..base import BaseEstimator, is_classifier, clone
@@ -27,19 +29,21 @@
 from ._split import check_cv
 from ._validation import _fit_and_score
 from ._validation import _aggregate_score_dicts
+from ._validation import _insert_error_scores
+from ._validation import _normalize_score_results
+from ._validation import _warn_or_raise_about_fit_failures
 from ..exceptions import NotFittedError
-from ..utils._joblib import Parallel, delayed
+from joblib import Parallel
 from ..utils import check_random_state
-from ..utils.fixes import MaskedArray
 from ..utils.random import sample_without_replacement
-from ..utils.validation import indexable, check_is_fitted
-from ..utils.metaestimators import if_delegate_has_method
-from ..metrics.scorer import _check_multimetric_scoring
-from ..metrics.scorer import check_scoring
-
-
-__all__ = ['GridSearchCV', 'ParameterGrid', 'fit_grid_point',
-           'ParameterSampler', 'RandomizedSearchCV']
+from ..utils._tags import _safe_tags
+from ..utils.validation import indexable, check_is_fitted, _check_fit_params
+from ..utils.metaestimators import available_if
+from ..utils.fixes import delayed
+from ..metrics._scorer import _check_multimetric_scoring
+from ..metrics import check_scoring
+
+__all__ = ["GridSearchCV", "ParameterGrid", "ParameterSampler", "RandomizedSearchCV"]


 class ParameterGrid:
@@ -47,12 +51,13 @@

     Can be used to iterate over parameter value combinations with the
     Python built-in function iter.
+    The order of the generated parameter combinations is deterministic.

     Read more in the :ref:`User Guide <grid_search>`.

     Parameters
     ----------
-    param_grid : dict of string to sequence, or sequence of such
+    param_grid : dict of str to sequence, or sequence of such
         The parameter grid to explore, as a dictionary mapping estimator
         parameters to sequences of allowed values.

@@ -79,17 +84,18 @@
     >>> ParameterGrid(grid)[1] == {'kernel': 'rbf', 'gamma': 1}
     True

-    See also
+    See Also
     --------
-    :class:`GridSearchCV`:
-        Uses :class:`ParameterGrid` to perform a full parallelized parameter
-        search.
+    GridSearchCV : Uses :class:`ParameterGrid` to perform a full parallelized
+        parameter search.
     """

     def __init__(self, param_grid):
         if not isinstance(param_grid, (Mapping, Iterable)):
-            raise TypeError('Parameter grid is not a dict or '
-                            'a list ({!r})'.format(param_grid))
+            raise TypeError(
+                f"Parameter grid should be a dict or a list, got: {param_grid!r} of"
+                f" type {type(param_grid).__name__}"
+            )

         if isinstance(param_grid, Mapping):
             # wrap dictionary in a singleton list to support either dict
@@ -99,13 +105,27 @@
         # check if all entries are dictionaries of lists
         for grid in param_grid:
             if not isinstance(grid, dict):
-                raise TypeError('Parameter grid is not a '
-                                'dict ({!r})'.format(grid))
-            for key in grid:
-                if not isinstance(grid[key], Iterable):
-                    raise TypeError('Parameter grid value is not iterable '
-                                    '(key={!r}, value={!r})'
-                                    .format(key, grid[key]))
+                raise TypeError(f"Parameter grid is not a dict ({grid!r})")
+            for key, value in grid.items():
+                if isinstance(value, np.ndarray) and value.ndim > 1:
+                    raise ValueError(
+                        f"Parameter array for {key!r} should be one-dimensional, got:"
+                        f" {value!r} with shape {value.shape}"
+                    )
+                if isinstance(value, str) or not isinstance(
+                    value, (np.ndarray, Sequence)
+                ):
+                    raise TypeError(
+                        f"Parameter grid for parameter {key!r} needs to be a list or a"
+                        f" numpy array, but got {value!r} (of type "
+                        f"{type(value).__name__}) instead. Single values "
+                        "need to be wrapped in a list with one element."
+                    )
+                if len(value) == 0:
+                    raise ValueError(
+                        f"Parameter grid for parameter {key!r} need "
+                        f"to be a non-empty sequence, got: {value!r}"
+                    )

         self.param_grid = param_grid

@@ -114,7 +134,7 @@

         Returns
         -------
-        params : iterator over dict of string to any
+        params : iterator over dict of str to any
             Yields dictionaries mapping each estimator parameter to one of its
             allowed values.
         """
@@ -133,8 +153,9 @@
         """Number of points on the grid."""
         # Product function that can handle iterables (np.product can't).
         product = partial(reduce, operator.mul)
-        return sum(product(len(v) for v in p.values()) if p else 1
-                   for p in self.param_grid)
+        return sum(
+            product(len(v) for v in p.values()) if p else 1 for p in self.param_grid
+        )

     def __getitem__(self, ind):
         """Get the parameters that would be ``ind``th in iteration
@@ -146,7 +167,7 @@

         Returns
         -------
-        params : dict of string to any
+        params : dict of str to any
             Equal to list(self)[ind]
         """
         # This is used to make discrete sampling without replacement memory
@@ -175,7 +196,7 @@
                     out[key] = v_list[offset]
                 return out

-        raise IndexError('ParameterGrid index out of range')
+        raise IndexError("ParameterGrid index out of range")


 class ParameterSampler:
@@ -188,38 +209,31 @@
     It is highly recommended to use continuous distributions for continuous
     parameters.

-    Note that before SciPy 0.16, the ``scipy.stats.distributions`` do not
-    accept a custom RNG instance and always use the singleton RNG from
-    ``numpy.random``. Hence setting ``random_state`` will not guarantee a
-    deterministic iteration whenever ``scipy.stats`` distributions are used to
-    define the parameter search space. Deterministic behavior is however
-    guaranteed from SciPy 0.16 onwards.
-
-    Read more in the :ref:`User Guide <search>`.
+    Read more in the :ref:`User Guide <grid_search>`.

     Parameters
     ----------
     param_distributions : dict
-        Dictionary where the keys are parameters and values
-        are distributions from which a parameter is to be sampled.
-        Distributions either have to provide a ``rvs`` function
-        to sample from them, or can be given as a list of values,
-        where a uniform distribution is assumed.
-
-    n_iter : integer
+        Dictionary with parameters names (`str`) as keys and distributions
+        or lists of parameters to try. Distributions must provide a ``rvs``
+        method for sampling (such as those from scipy.stats.distributions).
+        If a list is given, it is sampled uniformly.
+        If a list of dicts is given, first a dict is sampled uniformly, and
+        then a parameter is sampled using that dict as above.
+
+    n_iter : int
         Number of parameter settings that are produced.

-    random_state : int, RandomState instance or None, optional (default=None)
+    random_state : int, RandomState instance or None, default=None
         Pseudo random number generator state used for random uniform sampling
         from lists of possible values instead of scipy.stats distributions.
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+        Pass an int for reproducible output across multiple
+        function calls.
+        See :term:`Glossary <random_state>`.

     Returns
     -------
-    params : dict of string to any
+    params : dict of str to any
         **Yields** dictionaries mapping each estimator parameter to
         as sampled value.

@@ -240,19 +254,49 @@
     ...                  {'b': 1.038159, 'a': 2}]
     True
     """
-    def __init__(self, param_distributions, n_iter, random_state=None):
-        self.param_distributions = param_distributions
+
+    def __init__(self, param_distributions, n_iter, *, random_state=None):
+        if not isinstance(param_distributions, (Mapping, Iterable)):
+            raise TypeError(
+                "Parameter distribution is not a dict or a list,"
+                f" got: {param_distributions!r} of type "
+                f"{type(param_distributions).__name__}"
+            )
+
+        if isinstance(param_distributions, Mapping):
+            # wrap dictionary in a singleton list to support either dict
+            # or list of dicts
+            param_distributions = [param_distributions]
+
+        for dist in param_distributions:
+            if not isinstance(dist, dict):
+                raise TypeError(
+                    "Parameter distribution is not a dict ({!r})".format(dist)
+                )
+            for key in dist:
+                if not isinstance(dist[key], Iterable) and not hasattr(
+                    dist[key], "rvs"
+                ):
+                    raise TypeError(
+                        f"Parameter grid for parameter {key!r} is not iterable "
+                        f"or a distribution (value={dist[key]})"
+                    )
         self.n_iter = n_iter
         self.random_state = random_state
+        self.param_distributions = param_distributions
+
+    def _is_all_lists(self):
+        return all(
+            all(not hasattr(v, "rvs") for v in dist.values())
+            for dist in self.param_distributions
+        )

     def __iter__(self):
-        # check if all distributions are given as lists
-        # in this case we want to sample without replacement
-        all_lists = np.all([not hasattr(v, "rvs")
-                            for v in self.param_distributions.values()])
-        rnd = check_random_state(self.random_state)
-
-        if all_lists:
+        rng = check_random_state(self.random_state)
+
+        # if all distributions are given as lists, we want to sample without
+        # replacement
+        if self._is_all_lists():
             # look up sampled parameter settings in parameter grid
             param_grid = ParameterGrid(self.param_distributions)
             grid_size = len(param_grid)
@@ -260,134 +304,92 @@

             if grid_size < n_iter:
                 warnings.warn(
-                    'The total space of parameters %d is smaller '
-                    'than n_iter=%d. Running %d iterations. For exhaustive '
-                    'searches, use GridSearchCV.'
-                    % (grid_size, self.n_iter, grid_size), UserWarning)
+                    "The total space of parameters %d is smaller "
+                    "than n_iter=%d. Running %d iterations. For exhaustive "
+                    "searches, use GridSearchCV." % (grid_size, self.n_iter, grid_size),
+                    UserWarning,
+                )
                 n_iter = grid_size
-            for i in sample_without_replacement(grid_size, n_iter,
-                                                random_state=rnd):
+            for i in sample_without_replacement(grid_size, n_iter, random_state=rng):
                 yield param_grid[i]

         else:
-            # Always sort the keys of a dictionary, for reproducibility
-            items = sorted(self.param_distributions.items())
             for _ in range(self.n_iter):
+                dist = rng.choice(self.param_distributions)
+                # Always sort the keys of a dictionary, for reproducibility
+                items = sorted(dist.items())
                 params = dict()
                 for k, v in items:
                     if hasattr(v, "rvs"):
-                        params[k] = v.rvs(random_state=rnd)
+                        params[k] = v.rvs(random_state=rng)
                     else:
-                        params[k] = v[rnd.randint(len(v))]
+                        params[k] = v[rng.randint(len(v))]
                 yield params

     def __len__(self):
         """Number of points that will be sampled."""
-        return self.n_iter
-
-
-def fit_grid_point(X, y, estimator, parameters, train, test, scorer,
-                   verbose, error_score='raise-deprecating', **fit_params):
-    """Run fit on one set of parameters.
-
-    Parameters
-    ----------
-    X : array-like, sparse matrix or list
-        Input data.
-
-    y : array-like or None
-        Targets for input data.
-
-    estimator : estimator object
-        A object of that type is instantiated for each grid point.
-        This is assumed to implement the scikit-learn estimator interface.
-        Either estimator needs to provide a ``score`` function,
-        or ``scoring`` must be passed.
-
-    parameters : dict
-        Parameters to be set on estimator for this grid point.
-
-    train : ndarray, dtype int or bool
-        Boolean mask or indices for training set.
-
-    test : ndarray, dtype int or bool
-        Boolean mask or indices for test set.
-
-    scorer : callable or None
-        The scorer callable object / function must have its signature as
-        ``scorer(estimator, X, y)``.
-
-        If ``None`` the estimator's score method is used.
-
-    verbose : int
-        Verbosity level.
-
-    **fit_params : kwargs
-        Additional parameter passed to the fit function of the estimator.
-
-    error_score : 'raise' or numeric
-        Value to assign to the score if an error occurs in estimator fitting.
-        If set to 'raise', the error is raised. If a numeric value is given,
-        FitFailedWarning is raised. This parameter does not affect the refit
-        step, which will always raise the error. Default is 'raise' but from
-        version 0.22 it will change to np.nan.
-
-    Returns
-    -------
-    score : float
-         Score of this parameter setting on given test split.
-
-    parameters : dict
-        The parameters that have been evaluated.
-
-    n_samples_test : int
-        Number of test samples in this split.
+        if self._is_all_lists():
+            grid_size = len(ParameterGrid(self.param_distributions))
+            return min(self.n_iter, grid_size)
+        else:
+            return self.n_iter
+
+
+def _check_refit(search_cv, attr):
+    if not search_cv.refit:
+        raise AttributeError(
+            f"This {type(search_cv).__name__} instance was initialized with "
+            f"`refit=False`. {attr} is available only after refitting on the best "
+            "parameters. You can refit an estimator manually using the "
+            "`best_params_` attribute"
+        )
+
+
+def _estimator_has(attr):
+    """Check if we can delegate a method to the underlying estimator.
+
+    Calling a prediction method will only be available if `refit=True`. In
+    such case, we check first the fitted best estimator. If it is not
+    fitted, we check the unfitted estimator.
+
+    Checking the unfitted estimator allows to use `hasattr` on the `SearchCV`
+    instance even before calling `fit`.
     """
-    # NOTE we are not using the return value as the scorer by itself should be
-    # validated before. We use check_scoring only to reject multimetric scorer
-    check_scoring(estimator, scorer)
-    scores, n_samples_test = _fit_and_score(estimator, X, y,
-                                            scorer, train,
-                                            test, verbose, parameters,
-                                            fit_params=fit_params,
-                                            return_n_test_samples=True,
-                                            error_score=error_score)
-    return scores, parameters, n_samples_test
-
-
-def _check_param_grid(param_grid):
-    if hasattr(param_grid, 'items'):
-        param_grid = [param_grid]
-
-    for p in param_grid:
-        for name, v in p.items():
-            if isinstance(v, np.ndarray) and v.ndim > 1:
-                raise ValueError("Parameter array should be one-dimensional.")
-
-            if (isinstance(v, str) or
-                    not isinstance(v, (np.ndarray, Sequence))):
-                raise ValueError("Parameter values for parameter ({0}) need "
-                                 "to be a sequence(but not a string) or"
-                                 " np.ndarray.".format(name))
-
-            if len(v) == 0:
-                raise ValueError("Parameter values for parameter ({0}) need "
-                                 "to be a non-empty sequence.".format(name))
-
-
-class BaseSearchCV(BaseEstimator, MetaEstimatorMixin, metaclass=ABCMeta):
-    """Abstract base class for hyper parameter search with cross-validation.
-    """
+
+    def check(self):
+        _check_refit(self, attr)
+        if hasattr(self, "best_estimator_"):
+            # raise an AttributeError if `attr` does not exist
+            getattr(self.best_estimator_, attr)
+            return True
+        # raise an AttributeError if `attr` does not exist
+        getattr(self.estimator, attr)
+        return True
+
+    return check
+
+
+class BaseSearchCV(MetaEstimatorMixin, BaseEstimator, metaclass=ABCMeta):
+    """Abstract base class for hyper parameter search with cross-validation."""

     @abstractmethod
-    def __init__(self, estimator, scoring=None, n_jobs=None, iid='warn',
-                 refit=True, cv='warn', verbose=0, pre_dispatch='2*n_jobs',
-                 error_score='raise-deprecating', return_train_score=True):
+    def __init__(
+        self,
+        estimator,
+        *,
+        scoring=None,
+        n_jobs=None,
+        refit=True,
+        cv=None,
+        verbose=0,
+        pre_dispatch="2*n_jobs",
+        error_score=np.nan,
+        return_train_score=True,
+    ):

         self.scoring = scoring
         self.estimator = estimator
         self.n_jobs = n_jobs
-        self.iid = iid
         self.refit = refit
         self.cv = cv
         self.verbose = verbose
@@ -399,47 +401,83 @@
     def _estimator_type(self):
         return self.estimator._estimator_type

+    def _more_tags(self):
+        # allows cross-validation to see 'precomputed' metrics
+        return {
+            "pairwise": _safe_tags(self.estimator, "pairwise"),
+            "_xfail_checks": {
+                "check_supervised_y_2d": "DataConversionWarning not caught"
+            },
+        }
+
     def score(self, X, y=None):
-        """Returns the score on the given data, if the estimator has been refit.
+        """Return the score on the given data, if the estimator has been refit.

         This uses the score defined by ``scoring`` where provided, and the
         ``best_estimator_.score`` method otherwise.

         Parameters
         ----------
-        X : array-like, shape = [n_samples, n_features]
-            Input data, where n_samples is the number of samples and
-            n_features is the number of features.
-
-        y : array-like, shape = [n_samples] or [n_samples, n_output], optional
+        X : array-like of shape (n_samples, n_features)
+            Input data, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+
+        y : array-like of shape (n_samples, n_output) \
+            or (n_samples,), default=None
             Target relative to X for classification or regression;
             None for unsupervised learning.

         Returns
         -------
         score : float
+            The score defined by ``scoring`` if provided, and the
+            ``best_estimator_.score`` method otherwise.
         """
-        self._check_is_fitted('score')
+        _check_refit(self, "score")
+        check_is_fitted(self)
         if self.scorer_ is None:
-            raise ValueError("No score function explicitly defined, "
-                             "and the estimator doesn't provide one %s"
-                             % self.best_estimator_)
-        score = self.scorer_[self.refit] if self.multimetric_ else self.scorer_
-        return score(self.best_estimator_, X, y)
-
-    def _check_is_fitted(self, method_name):
-        if not self.refit:
-            raise NotFittedError('This %s instance was initialized '
-                                 'with refit=False. %s is '
-                                 'available only after refitting on the best '
-                                 'parameters. You can refit an estimator '
-                                 'manually using the ``best_params_`` '
-                                 'attribute'
-                                 % (type(self).__name__, method_name))
-        else:
-            check_is_fitted(self, 'best_estimator_')
-
-    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
+            raise ValueError(
+                "No score function explicitly defined, "
+                "and the estimator doesn't provide one %s"
+                % self.best_estimator_
+            )
+        if isinstance(self.scorer_, dict):
+            if self.multimetric_:
+                scorer = self.scorer_[self.refit]
+            else:
+                scorer = self.scorer_
+            return scorer(self.best_estimator_, X, y)
+
+        # callable
+        score = self.scorer_(self.best_estimator_, X, y)
+        if self.multimetric_:
+            score = score[self.refit]
+        return score
+
+    @available_if(_estimator_has("score_samples"))
+    def score_samples(self, X):
+        """Call score_samples on the estimator with the best found parameters.
+
+        Only available if ``refit=True`` and the underlying estimator supports
+        ``score_samples``.
+
+        .. versionadded:: 0.24
+
+        Parameters
+        ----------
+        X : iterable
+            Data to predict on. Must fulfill input requirements
+            of the underlying estimator.
+
+        Returns
+        -------
+        y_score : ndarray of shape (n_samples,)
+            The ``best_estimator_.score_samples`` method.
+        """
+        check_is_fitted(self)
+        return self.best_estimator_.score_samples(X)
+
+    @available_if(_estimator_has("predict"))
     def predict(self, X):
         """Call predict on the estimator with the best found parameters.

@@ -452,11 +490,16 @@
             Must fulfill the input assumptions of the
             underlying estimator.

+        Returns
+        -------
+        y_pred : ndarray of shape (n_samples,)
+            The predicted labels or values for `X` based on the estimator with
+            the best found parameters.
         """
-        self._check_is_fitted('predict')
+        check_is_fitted(self)
         return self.best_estimator_.predict(X)

-    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
+    @available_if(_estimator_has("predict_proba"))
     def predict_proba(self, X):
         """Call predict_proba on the estimator with the best found parameters.

@@ -469,11 +512,17 @@
             Must fulfill the input assumptions of the
             underlying estimator.

+        Returns
+        -------
+        y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)
+            Predicted class probabilities for `X` based on the estimator with
+            the best found parameters. The order of the classes corresponds
+            to that in the fitted attribute :term:`classes_`.
         """
-        self._check_is_fitted('predict_proba')
+        check_is_fitted(self)
         return self.best_estimator_.predict_proba(X)

-    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
+    @available_if(_estimator_has("predict_log_proba"))
     def predict_log_proba(self, X):
         """Call predict_log_proba on the estimator with the best found parameters.

@@ -486,11 +535,17 @@
             Must fulfill the input assumptions of the
             underlying estimator.

+        Returns
+        -------
+        y_pred : ndarray of shape (n_samples,) or (n_samples, n_classes)
+            Predicted class log-probabilities for `X` based on the estimator
+            with the best found parameters. The order of the classes
+            corresponds to that in the fitted attribute :term:`classes_`.
         """
-        self._check_is_fitted('predict_log_proba')
+        check_is_fitted(self)
         return self.best_estimator_.predict_log_proba(X)

-    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
+    @available_if(_estimator_has("decision_function"))
     def decision_function(self, X):
         """Call decision_function on the estimator with the best found parameters.

@@ -503,11 +558,17 @@
             Must fulfill the input assumptions of the
             underlying estimator.

+        Returns
+        -------
+        y_score : ndarray of shape (n_samples,) or (n_samples, n_classes) \
+                or (n_samples, n_classes * (n_classes-1) / 2)
+            Result of the decision function for `X` based on the estimator with
+            the best found parameters.
         """
-        self._check_is_fitted('decision_function')
+        check_is_fitted(self)
         return self.best_estimator_.decision_function(X)

-    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
+    @available_if(_estimator_has("transform"))
     def transform(self, X):
         """Call transform on the estimator with the best found parameters.

@@ -520,11 +581,16 @@
             Must fulfill the input assumptions of the
             underlying estimator.

+        Returns
+        -------
+        Xt : {ndarray, sparse matrix} of shape (n_samples, n_features)
+            `X` transformed in the new space based on the estimator with
+            the best found parameters.
         """
-        self._check_is_fitted('transform')
+        check_is_fitted(self)
         return self.best_estimator_.transform(X)

-    @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
+    @available_if(_estimator_has("inverse_transform"))
     def inverse_transform(self, Xt):
         """Call inverse_transform on the estimator with the best found params.

@@ -537,20 +603,48 @@
             Must fulfill the input assumptions of the
             underlying estimator.

+        Returns
+        -------
+        X : {ndarray, sparse matrix} of shape (n_samples, n_features)
+            Result of the `inverse_transform` function for `Xt` based on the
+            estimator with the best found parameters.
         """
-        self._check_is_fitted('inverse_transform')
+        check_is_fitted(self)
         return self.best_estimator_.inverse_transform(Xt)
+
+    @property
+    def n_features_in_(self):
+        """Number of features seen during :term:`fit`.
+
+        Only available when `refit=True`.
+        """
+        # For consistency with other estimators we raise a AttributeError so
+        # that hasattr() fails if the search estimator isn't fitted.
+        try:
+            check_is_fitted(self)
+        except NotFittedError as nfe:
+            raise AttributeError(
+                "{} object has no n_features_in_ attribute.".format(
+                    self.__class__.__name__
+                )
+            ) from nfe
+
+        return self.best_estimator_.n_features_in_

     @property
     def classes_(self):
-        self._check_is_fitted("classes_")
+        """Class labels.
+
+        Only available when `refit=True` and the estimator is a classifier.
+        """
+        _estimator_has("classes_")(self)
         return self.best_estimator_.classes_

     def _run_search(self, evaluate_candidates):
         """Repeatedly calls `evaluate_candidates` to conduct a search.

         This method, implemented in sub-classes, makes it possible to
-        customize the the scheduling of evaluations: GridSearchCV and
+        customize the scheduling of evaluations: GridSearchCV and
         RandomizedSearchCV schedule evaluations for their whole parameter
         search space at once but other more sequential approaches are also
         possible: for instance is possible to iteratively schedule evaluations
@@ -558,13 +652,39 @@
         collected evaluation results. This makes it possible to implement
         Bayesian optimization or more generally sequential model-based
         optimization by deriving from the BaseSearchCV abstract base class.
+        For example, Successive Halving is implemented by calling
+        `evaluate_candidates` multiples times (once per iteration of the SH
+        process), each time passing a different set of candidates with `X`
+        and `y` of increasing sizes.

         Parameters
         ----------
         evaluate_candidates : callable
-            This callback accepts a list of candidates, where each candidate is
-            a dict of parameter settings. It returns a dict of all results so
-            far, formatted like ``cv_results_``.
+            This callback accepts:
+                - a list of candidates, where each candidate is a dict of
+                  parameter settings.
+                - an optional `cv` parameter which can be used to e.g.
+                  evaluate candidates on different dataset splits, or
+                  evaluate candidates on subsampled data (as done in the
+                  SucessiveHaling estimators). By default, the original `cv`
+                  parameter is used, and it is available as a private
+                  `_checked_cv_orig` attribute.
+                - an optional `more_results` dict. Each key will be added to
+                  the `cv_results_` attribute. Values should be lists of
+                  length `n_candidates`
+
+            It returns a dict of all results so far, formatted like
+            ``cv_results_``.
+
+            Important note (relevant whether the default cv is used or not):
+            in randomized splitters, and unless the random_state parameter of
+            cv was set to an int, calling cv.split() multiple times will
+            yield different splits. Since cv.split() is called in
+            evaluate_candidates, this means that candidates will be evaluated
+            on different splits each time evaluate_candidates is called. This
+            might be a methodological issue depending on the search strategy
+            that you're implementing. To prevent randomized splitters from
+            being used, you may use _split._yields_constant_splits()

         Examples
         --------
@@ -580,135 +700,211 @@
         """
         raise NotImplementedError("_run_search not implemented.")

-    def fit(self, X, y=None, groups=None, **fit_params):
+    def _check_refit_for_multimetric(self, scores):
+        """Check `refit` is compatible with `scores` is valid"""
+        multimetric_refit_msg = (
+            "For multi-metric scoring, the parameter refit must be set to a "
+            "scorer key or a callable to refit an estimator with the best "
+            "parameter setting on the whole data and make the best_* "
+            "attributes available for that metric. If this is not needed, "
+            f"refit should be set to False explicitly. {self.refit!r} was "
+            "passed."
+        )
+
+        valid_refit_dict = isinstance(self.refit, str) and self.refit in scores
+
+        if (
+            self.refit is not False
+            and not valid_refit_dict
+            and not callable(self.refit)
+        ):
+            raise ValueError(multimetric_refit_msg)
+
+    @staticmethod
+    def _select_best_index(refit, refit_metric, results):
+        """Select index of the best combination of hyperparemeters."""
+        if callable(refit):
+            # If callable, refit is expected to return the index of the best
+            # parameter set.
+            best_index = refit(results)
+            if not isinstance(best_index, numbers.Integral):
+                raise TypeError("best_index_ returned is not an integer")
+            if best_index < 0 or best_index >= len(results["params"]):
+                raise IndexError("best_index_ index out of range")
+        else:
+            best_index = results[f"rank_test_{refit_metric}"].argmin()
+        return best_index
+
+    def fit(self, X, y=None, *, groups=None, **fit_params):
         """Run fit with all sets of parameters.

         Parameters
         ----------

-        X : array-like, shape = [n_samples, n_features]
-            Training vector, where n_samples is the number of samples and
-            n_features is the number of features.
-
-        y : array-like, shape = [n_samples] or [n_samples, n_output], optional
+        X : array-like of shape (n_samples, n_features)
+            Training vector, where `n_samples` is the number of samples and
+            `n_features` is the number of features.
+
+        y : array-like of shape (n_samples, n_output) \
+            or (n_samples,), default=None
             Target relative to X for classification or regression;
             None for unsupervised learning.

-        groups : array-like, with shape (n_samples,), optional
+        groups : array-like of shape (n_samples,), default=None
             Group labels for the samples used while splitting the dataset into
-            train/test set.
-
-        **fit_params : dict of string -> object
-            Parameters passed to the ``fit`` method of the estimator
+            train/test set. Only used in conjunction with a "Group" :term:`cv`
+            instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).
+
+        **fit_params : dict of str -> object
+            Parameters passed to the `fit` method of the estimator.
+
+            If a fit parameter is an array-like whose length is equal to
+            `num_samples` then it will be split across CV groups along with `X`
+            and `y`. For example, the :term:`sample_weight` parameter is split
+            because `len(sample_weights) = len(X)`.
+
+        Returns
+        -------
+        self : object
+            Instance of fitted estimator.
         """
         estimator = self.estimator
-        cv = check_cv(self.cv, y, classifier=is_classifier(estimator))
-
-        scorers, self.multimetric_ = _check_multimetric_scoring(
-            self.estimator, scoring=self.scoring)
-
-        if self.multimetric_:
-            if self.refit is not False and (
-                    not isinstance(self.refit, str) or
-                    # This will work for both dict / list (tuple)
-                    self.refit not in scorers) and not callable(self.refit):
-                raise ValueError("For multi-metric scoring, the parameter "
-                                 "refit must be set to a scorer key or a "
-                                 "callable to refit an estimator with the "
-                                 "best parameter setting on the whole "
-                                 "data and make the best_* attributes "
-                                 "available for that metric. If this is "
-                                 "not needed, refit should be set to "
-                                 "False explicitly. %r was passed."
-                                 % self.refit)
-            else:
-                refit_metric = self.refit
+        refit_metric = "score"
+
+        if callable(self.scoring):
+            scorers = self.scoring
+        elif self.scoring is None or isinstance(self.scoring, str):
+            scorers = check_scoring(self.estimator, self.scoring)
         else:
-            refit_metric = 'score'
+            scorers = _check_multimetric_scoring(self.estimator, self.scoring)
+            self._check_refit_for_multimetric(scorers)
+            refit_metric = self.refit

         X, y, groups = indexable(X, y, groups)
-        n_splits = cv.get_n_splits(X, y, groups)
+        fit_params = _check_fit_params(X, fit_params)
+
+        cv_orig = check_cv(self.cv, y, classifier=is_classifier(estimator))
+        n_splits = cv_orig.get_n_splits(X, y, groups)

         base_estimator = clone(self.estimator)

-        parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
-                            pre_dispatch=self.pre_dispatch)
-
-        fit_and_score_kwargs = dict(scorer=scorers,
-                                    fit_params=fit_params,
-                                    return_train_score=self.return_train_score,
-                                    return_n_test_samples=True,
-                                    return_times=True,
-                                    return_parameters=False,
-                                    error_score=self.error_score,
-                                    verbose=self.verbose)
+        parallel = Parallel(n_jobs=self.n_jobs, pre_dispatch=self.pre_dispatch)
+
+        fit_and_score_kwargs = dict(
+            scorer=scorers,
+            fit_params=fit_params,
+            return_train_score=self.return_train_score,
+            return_n_test_samples=True,
+            return_times=True,
+            return_parameters=False,
+            error_score=self.error_score,
+            verbose=self.verbose,
+        )
         results = {}
         with parallel:
             all_candidate_params = []
             all_out = []
-
-            def evaluate_candidates(candidate_params):
+            all_more_results = defaultdict(list)
+
+            def evaluate_candidates(candidate_params, cv=None, more_results=None):
+                cv = cv or cv_orig
                 candidate_params = list(candidate_params)
                 n_candidates = len(candidate_params)

                 if self.verbose > 0:
-                    print("Fitting {0} folds for each of {1} candidates,"
-                          " totalling {2} fits".format(
-                              n_splits, n_candidates, n_candidates * n_splits))
-
-                out = parallel(delayed(_fit_and_score)(clone(base_estimator),
-                                                       X, y,
-                                                       train=train, test=test,
-                                                       parameters=parameters,
-                                                       **fit_and_score_kwargs)
-                               for parameters, (train, test)
-                               in product(candidate_params,
-                                          cv.split(X, y, groups)))
+                    print(
+                        "Fitting {0} folds for each of {1} candidates,"
+                        " totalling {2} fits".format(
+                            n_splits, n_candidates, n_candidates * n_splits
+                        )
+                    )
+
+                out = parallel(
+                    delayed(_fit_and_score)(
+                        clone(base_estimator),
+                        X,
+                        y,
+                        train=train,
+                        test=test,
+                        parameters=parameters,
+                        split_progress=(split_idx, n_splits),
+                        candidate_progress=(cand_idx, n_candidates),
+                        **fit_and_score_kwargs,
+                    )
+                    for (cand_idx, parameters), (split_idx, (train, test)) in product(
+                        enumerate(candidate_params), enumerate(cv.split(X, y, groups))
+                    )
+                )

                 if len(out) < 1:
-                    raise ValueError('No fits were performed. '
-                                     'Was the CV iterator empty? '
-                                     'Were there no candidates?')
+                    raise ValueError(
+                        "No fits were performed. "
+                        "Was the CV iterator empty? "
+                        "Were there no candidates?"
+                    )
                 elif len(out) != n_candidates * n_splits:
-                    raise ValueError('cv.split and cv.get_n_splits returned '
-                                     'inconsistent results. Expected {} '
-                                     'splits, got {}'
-                                     .format(n_splits,
-                                             len(out) // n_candidates))
+                    raise ValueError(
+                        "cv.split and cv.get_n_splits returned "
+                        "inconsistent results. Expected {} "
+                        "splits, got {}".format(n_splits, len(out) // n_candidates)
+                    )
+
+                _warn_or_raise_about_fit_failures(out, self.error_score)
+
+                # For callable self.scoring, the return type is only know after
+                # calling. If the return type is a dictionary, the error scores
+                # can now be inserted with the correct key. The type checking
+                # of out will be done in `_insert_error_scores`.
+                if callable(self.scoring):
+                    _insert_error_scores(out, self.error_score)

                 all_candidate_params.extend(candidate_params)
                 all_out.extend(out)

+                if more_results is not None:
+                    for key, value in more_results.items():
+                        all_more_results[key].extend(value)
+
                 nonlocal results
                 results = self._format_results(
-                    all_candidate_params, scorers, n_splits, all_out)
+                    all_candidate_params, n_splits, all_out, all_more_results
+                )
+
                 return results

             self._run_search(evaluate_candidates)
+
+            # multimetric is determined here because in the case of a callable
+            # self.scoring the return type is only known after calling
+            first_test_score = all_out[0]["test_scores"]
+            self.multimetric_ = isinstance(first_test_score, dict)
+
+            # check refit_metric now for a callabe scorer that is multimetric
+            if callable(self.scoring) and self.multimetric_:
+                self._check_refit_for_multimetric(first_test_score)
+                refit_metric = self.refit

         # For multi-metric evaluation, store the best_index_, best_params_ and
         # best_score_ iff refit is one of the scorer names
         # In single metric evaluation, refit_metric is "score"
         if self.refit or not self.multimetric_:
-            # If callable, refit is expected to return the index of the best
-            # parameter set.
-            if callable(self.refit):
-                self.best_index_ = self.refit(results)
-                if not isinstance(self.best_index_, (int, np.integer)):
-                    raise TypeError('best_index_ returned is not an integer')
-                if (self.best_index_ < 0 or
-                   self.best_index_ >= len(results["params"])):
-                    raise IndexError('best_index_ index out of range')
-            else:
-                self.best_index_ = results["rank_test_%s"
-                                           % refit_metric].argmin()
-                self.best_score_ = results["mean_test_%s" % refit_metric][
-                                           self.best_index_]
+            self.best_index_ = self._select_best_index(
+                self.refit, refit_metric, results
+            )
+            if not callable(self.refit):
+                # With a non-custom callable, we can select the best score
+                # based on the best index
+                self.best_score_ = results[f"mean_test_{refit_metric}"][
+                    self.best_index_
+                ]
             self.best_params_ = results["params"][self.best_index_]

         if self.refit:
-            self.best_estimator_ = clone(base_estimator).set_params(
-                **self.best_params_)
+            # we clone again after setting params in case some
+            # of the params are estimators as well.
+            self.best_estimator_ = clone(
+                clone(base_estimator).set_params(**self.best_params_)
+            )
             refit_start_time = time.time()
             if y is not None:
                 self.best_estimator_.fit(X, y, **fit_params)
@@ -717,110 +913,107 @@
             refit_end_time = time.time()
             self.refit_time_ = refit_end_time - refit_start_time

+            if hasattr(self.best_estimator_, "feature_names_in_"):
+                self.feature_names_in_ = self.best_estimator_.feature_names_in_
+
         # Store the only scorer not as a dict for single metric evaluation
-        self.scorer_ = scorers if self.multimetric_ else scorers['score']
+        self.scorer_ = scorers

         self.cv_results_ = results
         self.n_splits_ = n_splits

         return self

-    def _format_results(self, candidate_params, scorers, n_splits, out):
+    def _format_results(self, candidate_params, n_splits, out, more_results=None):
         n_candidates = len(candidate_params)
-
-        # if one choose to see train score, "out" will contain train score info
-        if self.return_train_score:
-            (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,
-             score_time) = zip(*out)
-        else:
-            (test_score_dicts, test_sample_counts, fit_time,
-             score_time) = zip(*out)
-
-        # test_score_dicts and train_score dicts are lists of dictionaries and
-        # we make them into dict of lists
-        test_scores = _aggregate_score_dicts(test_score_dicts)
-        if self.return_train_score:
-            train_scores = _aggregate_score_dicts(train_score_dicts)
-
-        results = {}
+        out = _aggregate_score_dicts(out)
+
+        results = dict(more_results or {})
+        for key, val in results.items():
+            # each value is a list (as per evaluate_candidate's convention)
+            # we convert it to an array for consistency with the other keys
+            results[key] = np.asarray(val)

         def _store(key_name, array, weights=None, splits=False, rank=False):
             """A small helper to store the scores/times to the cv_results_"""
             # When iterated first by splits, then by parameters
             # We want `array` to have `n_candidates` rows and `n_splits` cols.
-            array = np.array(array, dtype=np.float64).reshape(n_candidates,
-                                                              n_splits)
+            array = np.array(array, dtype=np.float64).reshape(n_candidates, n_splits)
             if splits:
-                for split_i in range(n_splits):
+                for split_idx in range(n_splits):
                     # Uses closure to alter the results
-                    results["split%d_%s"
-                            % (split_i, key_name)] = array[:, split_i]
+                    results["split%d_%s" % (split_idx, key_name)] = array[:, split_idx]

             array_means = np.average(array, axis=1, weights=weights)
-            results['mean_%s' % key_name] = array_means
+            results["mean_%s" % key_name] = array_means
+
+            if key_name.startswith(("train_", "test_")) and np.any(
+                ~np.isfinite(array_means)
+            ):
+                warnings.warn(
+                    f"One or more of the {key_name.split('_')[0]} scores "
+                    f"are non-finite: {array_means}",
+                    category=UserWarning,
+                )
+
             # Weighted std is not directly available in numpy
-            array_stds = np.sqrt(np.average((array -
-                                             array_means[:, np.newaxis]) ** 2,
-                                            axis=1, weights=weights))
-            results['std_%s' % key_name] = array_stds
+            array_stds = np.sqrt(
+                np.average(
+                    (array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights
+                )
+            )
+            results["std_%s" % key_name] = array_stds

             if rank:
                 results["rank_%s" % key_name] = np.asarray(
-                    rankdata(-array_means, method='min'), dtype=np.int32)
-
-        _store('fit_time', fit_time)
-        _store('score_time', score_time)
+                    rankdata(-array_means, method="min"), dtype=np.int32
+                )
+
+        _store("fit_time", out["fit_time"])
+        _store("score_time", out["score_time"])
         # Use one MaskedArray and mask all the places where the param is not
         # applicable for that candidate. Use defaultdict as each candidate may
         # not contain all the params
-        param_results = defaultdict(partial(MaskedArray,
-                                            np.empty(n_candidates,),
-                                            mask=True,
-                                            dtype=object))
-        for cand_i, params in enumerate(candidate_params):
+        param_results = defaultdict(
+            partial(
+                MaskedArray,
+                np.empty(
+                    n_candidates,
+                ),
+                mask=True,
+                dtype=object,
+            )
+        )
+        for cand_idx, params in enumerate(candidate_params):
             for name, value in params.items():
                 # An all masked empty array gets created for the key
                 # `"param_%s" % name` at the first occurrence of `name`.
                 # Setting the value at an index also unmasks that index
-                param_results["param_%s" % name][cand_i] = value
+                param_results["param_%s" % name][cand_idx] = value

         results.update(param_results)
         # Store a list of param dicts at the key 'params'
-        results['params'] = candidate_params
-
-        # NOTE test_sample counts (weights) remain the same for all candidates
-        test_sample_counts = np.array(test_sample_counts[:n_splits],
-                                      dtype=np.int)
-        iid = self.iid
-        if self.iid == 'warn':
-            warn = False
-            for scorer_name in scorers.keys():
-                scores = test_scores[scorer_name].reshape(n_candidates,
-                                                          n_splits)
-                means_weighted = np.average(scores, axis=1,
-                                            weights=test_sample_counts)
-                means_unweighted = np.average(scores, axis=1)
-                if not np.allclose(means_weighted, means_unweighted,
-                                   rtol=1e-4, atol=1e-4):
-                    warn = True
-                    break
-
-            if warn:
-                warnings.warn("The default of the `iid` parameter will change "
-                              "from True to False in version 0.22 and will be"
-                              " removed in 0.24. This will change numeric"
-                              " results when test-set sizes are unequal.",
-                              DeprecationWarning)
-            iid = True
-
-        for scorer_name in scorers.keys():
+        results["params"] = candidate_params
+
+        test_scores_dict = _normalize_score_results(out["test_scores"])
+        if self.return_train_score:
+            train_scores_dict = _normalize_score_results(out["train_scores"])
+
+        for scorer_name in test_scores_dict:
             # Computed the (weighted) mean and std for test scores alone
-            _store('test_%s' % scorer_name, test_scores[scorer_name],
-                   splits=True, rank=True,
-                   weights=test_sample_counts if iid else None)
+            _store(
+                "test_%s" % scorer_name,
+                test_scores_dict[scorer_name],
+                splits=True,
+                rank=True,
+                weights=None,
+            )
             if self.return_train_score:
-                _store('train_%s' % scorer_name, train_scores[scorer_name],
-                       splits=True)
+                _store(
+                    "train_%s" % scorer_name,
+                    train_scores_dict[scorer_name],
+                    splits=True,
+                )

         return results

@@ -831,9 +1024,9 @@
     Important members are fit, predict.

     GridSearchCV implements a "fit" and a "score" method.
-    It also implements "predict", "predict_proba", "decision_function",
-    "transform" and "inverse_transform" if they are implemented in the
-    estimator used.
+    It also implements "score_samples", "predict", "predict_proba",
+    "decision_function", "transform" and "inverse_transform" if they are
+    implemented in the estimator used.

     The parameters of the estimator used to apply these methods are optimized
     by cross-validated grid-search over a parameter grid.
@@ -842,40 +1035,105 @@

     Parameters
     ----------
-    estimator : estimator object.
+    estimator : estimator object
         This is assumed to implement the scikit-learn estimator interface.
         Either estimator needs to provide a ``score`` function,
         or ``scoring`` must be passed.

     param_grid : dict or list of dictionaries
-        Dictionary with parameters names (string) as keys and lists of
+        Dictionary with parameters names (`str`) as keys and lists of
         parameter settings to try as values, or a list of such
         dictionaries, in which case the grids spanned by each dictionary
         in the list are explored. This enables searching over any sequence
         of parameter settings.

-    scoring : string, callable, list/tuple, dict or None, default: None
-        A single string (see :ref:`scoring_parameter`) or a callable
-        (see :ref:`scoring`) to evaluate the predictions on the test set.
-
-        For evaluating multiple metrics, either give a list of (unique) strings
-        or a dict with names as keys and callables as values.
-
-        NOTE that when using custom scorers, each scorer should return a single
-        value. Metric functions returning a list/array of values can be wrapped
-        into multiple scorers that return one value each.
+    scoring : str, callable, list, tuple or dict, default=None
+        Strategy to evaluate the performance of the cross-validated model on
+        the test set.
+
+        If `scoring` represents a single score, one can use:
+
+        - a single string (see :ref:`scoring_parameter`);
+        - a callable (see :ref:`scoring`) that returns a single value.
+
+        If `scoring` represents multiple scores, one can use:
+
+        - a list or tuple of unique strings;
+        - a callable returning a dictionary where the keys are the metric
+          names and the values are the metric scores;
+        - a dictionary with metric names as keys and callables a values.

         See :ref:`multimetric_grid_search` for an example.

-        If None, the estimator's score method is used.
-
-    n_jobs : int or None, optional (default=None)
+    n_jobs : int, default=None
         Number of jobs to run in parallel.
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.

-    pre_dispatch : int, or string, optional
+        .. versionchanged:: v0.20
+           `n_jobs` default changed from 1 to None
+
+    refit : bool, str, or callable, default=True
+        Refit an estimator using the best found parameters on the whole
+        dataset.
+
+        For multiple metric evaluation, this needs to be a `str` denoting the
+        scorer that would be used to find the best parameters for refitting
+        the estimator at the end.
+
+        Where there are considerations other than maximum score in
+        choosing a best estimator, ``refit`` can be set to a function which
+        returns the selected ``best_index_`` given ``cv_results_``. In that
+        case, the ``best_estimator_`` and ``best_params_`` will be set
+        according to the returned ``best_index_`` while the ``best_score_``
+        attribute will not be available.
+
+        The refitted estimator is made available at the ``best_estimator_``
+        attribute and permits using ``predict`` directly on this
+        ``GridSearchCV`` instance.
+
+        Also for multiple metric evaluation, the attributes ``best_index_``,
+        ``best_score_`` and ``best_params_`` will only be available if
+        ``refit`` is set and all of them will be determined w.r.t this specific
+        scorer.
+
+        See ``scoring`` parameter to know more about multiple metric
+        evaluation.
+
+        .. versionchanged:: 0.20
+            Support for callable added.
+
+    cv : int, cross-validation generator or an iterable, default=None
+        Determines the cross-validation splitting strategy.
+        Possible inputs for cv are:
+
+        - None, to use the default 5-fold cross validation,
+        - integer, to specify the number of folds in a `(Stratified)KFold`,
+        - :term:`CV splitter`,
+        - An iterable yielding (train, test) splits as arrays of indices.
+
+        For integer/None inputs, if the estimator is a classifier and ``y`` is
+        either binary or multiclass, :class:`StratifiedKFold` is used. In all
+        other cases, :class:`KFold` is used. These splitters are instantiated
+        with `shuffle=False` so the splits will be the same across calls.
+
+        Refer :ref:`User Guide <cross_validation>` for the various
+        cross-validation strategies that can be used here.
+
+        .. versionchanged:: 0.22
+            ``cv`` default value if None changed from 3-fold to 5-fold.
+
+    verbose : int
+        Controls the verbosity: the higher, the more messages.
+
+        - >1 : the computation time for each fold and parameter candidate is
+          displayed;
+        - >2 : the score is also displayed;
+        - >3 : the fold and candidate parameter indexes are also displayed
+          together with the starting time of the computation.
+
+    pre_dispatch : int, or str, default='2*n_jobs'
         Controls the number of jobs that get dispatched during parallel
         execution. Reducing this number can be useful to avoid an
         explosion of memory consumption when more jobs get dispatched
@@ -889,80 +1147,16 @@
             - An int, giving the exact number of total jobs that are
               spawned

-            - A string, giving an expression as a function of n_jobs,
+            - A str, giving an expression as a function of n_jobs,
               as in '2*n_jobs'

-    iid : boolean, default='warn'
-        If True, return the average score across folds, weighted by the number
-        of samples in each test set. In this case, the data is assumed to be
-        identically distributed across the folds, and the loss minimized is
-        the total loss per sample, and not the mean loss across the folds. If
-        False, return the average score across folds. Default is True, but
-        will change to False in version 0.22, to correspond to the standard
-        definition of cross-validation.
-
-        .. versionchanged:: 0.20
-            Parameter ``iid`` will change from True to False by default in
-            version 0.22, and will be removed in 0.24.
-
-    cv : int, cross-validation generator or an iterable, optional
-        Determines the cross-validation splitting strategy.
-        Possible inputs for cv are:
-
-        - None, to use the default 3-fold cross validation,
-        - integer, to specify the number of folds in a `(Stratified)KFold`,
-        - :term:`CV splitter`,
-        - An iterable yielding (train, test) splits as arrays of indices.
-
-        For integer/None inputs, if the estimator is a classifier and ``y`` is
-        either binary or multiclass, :class:`StratifiedKFold` is used. In all
-        other cases, :class:`KFold` is used.
-
-        Refer :ref:`User Guide <cross_validation>` for the various
-        cross-validation strategies that can be used here.
-
-        .. versionchanged:: 0.20
-            ``cv`` default value if None will change from 3-fold to 5-fold
-            in v0.22.
-
-    refit : boolean, string, or callable, default=True
-        Refit an estimator using the best found parameters on the whole
-        dataset.
-
-        For multiple metric evaluation, this needs to be a string denoting the
-        scorer that would be used to find the best parameters for refitting
-        the estimator at the end.
-
-        Where there are considerations other than maximum score in
-        choosing a best estimator, ``refit`` can be set to a function which
-        returns the selected ``best_index_`` given ``cv_results_``.
-
-        The refitted estimator is made available at the ``best_estimator_``
-        attribute and permits using ``predict`` directly on this
-        ``GridSearchCV`` instance.
-
-        Also for multiple metric evaluation, the attributes ``best_index_``,
-        ``best_score_`` and ``best_params_`` will only be available if
-        ``refit`` is set and all of them will be determined w.r.t this specific
-        scorer. ``best_score_`` is not returned if refit is callable.
-
-        See ``scoring`` parameter to know more about multiple metric
-        evaluation.
-
-        .. versionchanged:: 0.20
-            Support for callable added.
-
-    verbose : integer
-        Controls the verbosity: the higher, the more messages.
-
-    error_score : 'raise' or numeric
+    error_score : 'raise' or numeric, default=np.nan
         Value to assign to the score if an error occurs in estimator fitting.
         If set to 'raise', the error is raised. If a numeric value is given,
         FitFailedWarning is raised. This parameter does not affect the refit
-        step, which will always raise the error. Default is 'raise' but from
-        version 0.22 it will change to np.nan.
-
-    return_train_score : boolean, default=False
+        step, which will always raise the error.
+
+    return_train_score : bool, default=False
         If ``False``, the ``cv_results_`` attribute will not include training
         scores.
         Computing training scores is used to get insights on how different
@@ -971,33 +1165,10 @@
         expensive and is not strictly required to select the parameters that
         yield the best generalization performance.

-
-    Examples
-    --------
-    >>> from sklearn import svm, datasets
-    >>> from sklearn.model_selection import GridSearchCV
-    >>> iris = datasets.load_iris()
-    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
-    >>> svc = svm.SVC(gamma="scale")
-    >>> clf = GridSearchCV(svc, parameters, cv=5)
-    >>> clf.fit(iris.data, iris.target)
-    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-    GridSearchCV(cv=5, error_score=...,
-           estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,
-                         decision_function_shape='ovr', degree=..., gamma=...,
-                         kernel='rbf', max_iter=-1, probability=False,
-                         random_state=None, shrinking=True, tol=...,
-                         verbose=False),
-           iid=..., n_jobs=None,
-           param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,
-           scoring=..., verbose=...)
-    >>> sorted(clf.cv_results_.keys())
-    ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
-    ['mean_fit_time', 'mean_score_time', 'mean_test_score',...
-     'param_C', 'param_kernel', 'params',...
-     'rank_test_score', 'split0_test_score',...
-     'split2_test_score', ...
-     'std_fit_time', 'std_score_time', 'std_test_score']
+        .. versionadded:: 0.19
+
+        .. versionchanged:: 0.21
+            Default value was changed from ``True`` to ``False``

     Attributes
     ----------
@@ -1057,7 +1228,7 @@
         scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown
         above. ('split0_test_precision', 'mean_train_precision' etc.)

-    best_estimator_ : estimator or dict
+    best_estimator_ : estimator
         Estimator that was chosen by the search, i.e. estimator
         which gave highest score (or smallest loss if specified)
         on the left out data. Not available if ``refit=False``.
@@ -1070,6 +1241,8 @@
         For multi-metric evaluation, this is present only if ``refit`` is
         specified.

+        This attribute is not available if ``refit`` is a function.
+
     best_params_ : dict
         Parameter setting that gave the best results on the hold out data.

@@ -1101,6 +1274,40 @@
         Seconds used for refitting the best model on the whole dataset.

         This is present only if ``refit`` is not False.
+
+        .. versionadded:: 0.20
+
+    multimetric_ : bool
+        Whether or not the scorers compute several metrics.
+
+    classes_ : ndarray of shape (n_classes,)
+        The classes labels. This is present only if ``refit`` is specified and
+        the underlying estimator is a classifier.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`. Only defined if
+        `best_estimator_` is defined (see the documentation for the `refit`
+        parameter for more details) and that `best_estimator_` exposes
+        `n_features_in_` when fit.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Only defined if
+        `best_estimator_` is defined (see the documentation for the `refit`
+        parameter for more details) and that `best_estimator_` exposes
+        `feature_names_in_` when fit.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    ParameterGrid : Generates all the combinations of a hyperparameter grid.
+    train_test_split : Utility function to split the data into a development
+        set usable for fitting a GridSearchCV instance and an evaluation set
+        for its final evaluation.
+    sklearn.metrics.make_scorer : Make a scorer from a performance metric or
+        loss function.

     Notes
     -----
@@ -1115,33 +1322,53 @@
     `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *
     n_jobs`.

-    See Also
-    ---------
-    :class:`ParameterGrid`:
-        generates all the combinations of a hyperparameter grid.
-
-    :func:`sklearn.model_selection.train_test_split`:
-        utility function to split the data into a development set usable
-        for fitting a GridSearchCV instance and an evaluation set for
-        its final evaluation.
-
-    :func:`sklearn.metrics.make_scorer`:
-        Make a scorer from a performance metric or loss function.
-
+    Examples
+    --------
+    >>> from sklearn import svm, datasets
+    >>> from sklearn.model_selection import GridSearchCV
+    >>> iris = datasets.load_iris()
+    >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
+    >>> svc = svm.SVC()
+    >>> clf = GridSearchCV(svc, parameters)
+    >>> clf.fit(iris.data, iris.target)
+    GridSearchCV(estimator=SVC(),
+                 param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})
+    >>> sorted(clf.cv_results_.keys())
+    ['mean_fit_time', 'mean_score_time', 'mean_test_score',...
+     'param_C', 'param_kernel', 'params',...
+     'rank_test_score', 'split0_test_score',...
+     'split2_test_score', ...
+     'std_fit_time', 'std_score_time', 'std_test_score']
     """
+
     _required_parameters = ["estimator", "param_grid"]

-    def __init__(self, estimator, param_grid, scoring=None,
-                 n_jobs=None, iid='warn', refit=True, cv='warn', verbose=0,
-                 pre_dispatch='2*n_jobs', error_score='raise-deprecating',
-                 return_train_score=False):
+    def __init__(
+        self,
+        estimator,
+        param_grid,
+        *,
+        scoring=None,
+        n_jobs=None,
+        refit=True,
+        cv=None,
+        verbose=0,
+        pre_dispatch="2*n_jobs",
+        error_score=np.nan,
+        return_train_score=False,
+    ):
         super().__init__(
-            estimator=estimator, scoring=scoring,
-            n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,
-            pre_dispatch=pre_dispatch, error_score=error_score,
-            return_train_score=return_train_score)
+            estimator=estimator,
+            scoring=scoring,
+            n_jobs=n_jobs,
+            refit=refit,
+            cv=cv,
+            verbose=verbose,
+            pre_dispatch=pre_dispatch,
+            error_score=error_score,
+            return_train_score=return_train_score,
+        )
         self.param_grid = param_grid
-        _check_param_grid(param_grid)

     def _run_search(self, evaluate_candidates):
         """Search all candidates in param_grid"""
@@ -1152,9 +1379,9 @@
     """Randomized search on hyper parameters.

     RandomizedSearchCV implements a "fit" and a "score" method.
-    It also implements "predict", "predict_proba", "decision_function",
-    "transform" and "inverse_transform" if they are implemented in the
-    estimator used.
+    It also implements "score_samples", "predict", "predict_proba",
+    "decision_function", "transform" and "inverse_transform" if they are
+    implemented in the estimator used.

     The parameters of the estimator used to apply these methods are optimized
     by cross-validated search over parameter settings.
@@ -1170,54 +1397,113 @@
     It is highly recommended to use continuous distributions for continuous
     parameters.

-    Note that before SciPy 0.16, the ``scipy.stats.distributions`` do not
-    accept a custom RNG instance and always use the singleton RNG from
-    ``numpy.random``. Hence setting ``random_state`` will not guarantee a
-    deterministic iteration whenever ``scipy.stats`` distributions are used to
-    define the parameter search space.
-
     Read more in the :ref:`User Guide <randomized_parameter_search>`.
+
+    .. versionadded:: 0.14

     Parameters
     ----------
-    estimator : estimator object.
+    estimator : estimator object
         A object of that type is instantiated for each grid point.
         This is assumed to implement the scikit-learn estimator interface.
         Either estimator needs to provide a ``score`` function,
         or ``scoring`` must be passed.

-    param_distributions : dict
-        Dictionary with parameters names (string) as keys and distributions
+    param_distributions : dict or list of dicts
+        Dictionary with parameters names (`str`) as keys and distributions
         or lists of parameters to try. Distributions must provide a ``rvs``
         method for sampling (such as those from scipy.stats.distributions).
         If a list is given, it is sampled uniformly.
+        If a list of dicts is given, first a dict is sampled uniformly, and
+        then a parameter is sampled using that dict as above.

     n_iter : int, default=10
         Number of parameter settings that are sampled. n_iter trades
         off runtime vs quality of the solution.

-    scoring : string, callable, list/tuple, dict or None, default: None
-        A single string (see :ref:`scoring_parameter`) or a callable
-        (see :ref:`scoring`) to evaluate the predictions on the test set.
-
-        For evaluating multiple metrics, either give a list of (unique) strings
-        or a dict with names as keys and callables as values.
-
-        NOTE that when using custom scorers, each scorer should return a single
-        value. Metric functions returning a list/array of values can be wrapped
-        into multiple scorers that return one value each.
+    scoring : str, callable, list, tuple or dict, default=None
+        Strategy to evaluate the performance of the cross-validated model on
+        the test set.
+
+        If `scoring` represents a single score, one can use:
+
+        - a single string (see :ref:`scoring_parameter`);
+        - a callable (see :ref:`scoring`) that returns a single value.
+
+        If `scoring` represents multiple scores, one can use:
+
+        - a list or tuple of unique strings;
+        - a callable returning a dictionary where the keys are the metric
+          names and the values are the metric scores;
+        - a dictionary with metric names as keys and callables a values.

         See :ref:`multimetric_grid_search` for an example.

         If None, the estimator's score method is used.

-    n_jobs : int or None, optional (default=None)
+    n_jobs : int, default=None
         Number of jobs to run in parallel.
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.

-    pre_dispatch : int, or string, optional
+        .. versionchanged:: v0.20
+           `n_jobs` default changed from 1 to None
+
+    refit : bool, str, or callable, default=True
+        Refit an estimator using the best found parameters on the whole
+        dataset.
+
+        For multiple metric evaluation, this needs to be a `str` denoting the
+        scorer that would be used to find the best parameters for refitting
+        the estimator at the end.
+
+        Where there are considerations other than maximum score in
+        choosing a best estimator, ``refit`` can be set to a function which
+        returns the selected ``best_index_`` given the ``cv_results``. In that
+        case, the ``best_estimator_`` and ``best_params_`` will be set
+        according to the returned ``best_index_`` while the ``best_score_``
+        attribute will not be available.
+
+        The refitted estimator is made available at the ``best_estimator_``
+        attribute and permits using ``predict`` directly on this
+        ``RandomizedSearchCV`` instance.
+
+        Also for multiple metric evaluation, the attributes ``best_index_``,
+        ``best_score_`` and ``best_params_`` will only be available if
+        ``refit`` is set and all of them will be determined w.r.t this specific
+        scorer.
+
+        See ``scoring`` parameter to know more about multiple metric
+        evaluation.
+
+        .. versionchanged:: 0.20
+            Support for callable added.
+
+    cv : int, cross-validation generator or an iterable, default=None
+        Determines the cross-validation splitting strategy.
+        Possible inputs for cv are:
+
+        - None, to use the default 5-fold cross validation,
+        - integer, to specify the number of folds in a `(Stratified)KFold`,
+        - :term:`CV splitter`,
+        - An iterable yielding (train, test) splits as arrays of indices.
+
+        For integer/None inputs, if the estimator is a classifier and ``y`` is
+        either binary or multiclass, :class:`StratifiedKFold` is used. In all
+        other cases, :class:`KFold` is used. These splitters are instantiated
+        with `shuffle=False` so the splits will be the same across calls.
+
+        Refer :ref:`User Guide <cross_validation>` for the various
+        cross-validation strategies that can be used here.
+
+        .. versionchanged:: 0.22
+            ``cv`` default value if None changed from 3-fold to 5-fold.
+
+    verbose : int
+        Controls the verbosity: the higher, the more messages.
+
+    pre_dispatch : int, or str, default='2*n_jobs'
         Controls the number of jobs that get dispatched during parallel
         execution. Reducing this number can be useful to avoid an
         explosion of memory consumption when more jobs get dispatched
@@ -1231,88 +1517,23 @@
             - An int, giving the exact number of total jobs that are
               spawned

-            - A string, giving an expression as a function of n_jobs,
+            - A str, giving an expression as a function of n_jobs,
               as in '2*n_jobs'

-    iid : boolean, default='warn'
-        If True, return the average score across folds, weighted by the number
-        of samples in each test set. In this case, the data is assumed to be
-        identically distributed across the folds, and the loss minimized is
-        the total loss per sample, and not the mean loss across the folds. If
-        False, return the average score across folds. Default is True, but
-        will change to False in version 0.22, to correspond to the standard
-        definition of cross-validation.
-
-        .. versionchanged:: 0.20
-            Parameter ``iid`` will change from True to False by default in
-            version 0.22, and will be removed in 0.24.
-
-    cv : int, cross-validation generator or an iterable, optional
-        Determines the cross-validation splitting strategy.
-        Possible inputs for cv are:
-
-        - None, to use the default 3-fold cross validation,
-        - integer, to specify the number of folds in a `(Stratified)KFold`,
-        - :term:`CV splitter`,
-        - An iterable yielding (train, test) splits as arrays of indices.
-
-        For integer/None inputs, if the estimator is a classifier and ``y`` is
-        either binary or multiclass, :class:`StratifiedKFold` is used. In all
-        other cases, :class:`KFold` is used.
-
-        Refer :ref:`User Guide <cross_validation>` for the various
-        cross-validation strategies that can be used here.
-
-        .. versionchanged:: 0.20
-            ``cv`` default value if None will change from 3-fold to 5-fold
-            in v0.22.
-
-    refit : boolean, string, or callable, default=True
-        Refit an estimator using the best found parameters on the whole
-        dataset.
-
-        For multiple metric evaluation, this needs to be a string denoting the
-        scorer that would be used to find the best parameters for refitting
-        the estimator at the end.
-
-        Where there are considerations other than maximum score in
-        choosing a best estimator, ``refit`` can be set to a function which
-        returns the selected ``best_index_`` given the ``cv_results``.
-
-        The refitted estimator is made available at the ``best_estimator_``
-        attribute and permits using ``predict`` directly on this
-        ``RandomizedSearchCV`` instance.
-
-        Also for multiple metric evaluation, the attributes ``best_index_``,
-        ``best_score_`` and ``best_params_`` will only be available if
-        ``refit`` is set and all of them will be determined w.r.t this specific
-        scorer. When refit is callable, ``best_score_`` is disabled.
-
-        See ``scoring`` parameter to know more about multiple metric
-        evaluation.
-
-        .. versionchanged:: 0.20
-            Support for callable added.
-
-    verbose : integer
-        Controls the verbosity: the higher, the more messages.
-
-    random_state : int, RandomState instance or None, optional, default=None
+    random_state : int, RandomState instance or None, default=None
         Pseudo random number generator state used for random uniform sampling
         from lists of possible values instead of scipy.stats distributions.
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-    error_score : 'raise' or numeric
+        Pass an int for reproducible output across multiple
+        function calls.
+        See :term:`Glossary <random_state>`.
+
+    error_score : 'raise' or numeric, default=np.nan
         Value to assign to the score if an error occurs in estimator fitting.
         If set to 'raise', the error is raised. If a numeric value is given,
         FitFailedWarning is raised. This parameter does not affect the refit
-        step, which will always raise the error. Default is 'raise' but from
-        version 0.22 it will change to np.nan.
-
-    return_train_score : boolean, default=False
+        step, which will always raise the error.
+
+    return_train_score : bool, default=False
         If ``False``, the ``cv_results_`` attribute will not include training
         scores.
         Computing training scores is used to get insights on how different
@@ -1321,6 +1542,11 @@
         expensive and is not strictly required to select the parameters that
         yield the best generalization performance.

+        .. versionadded:: 0.19
+
+        .. versionchanged:: 0.21
+            Default value was changed from ``True`` to ``False``
+
     Attributes
     ----------
     cv_results_ : dict of numpy (masked) ndarrays
@@ -1332,11 +1558,11 @@
         +--------------+-------------+-------------------+---+---------------+
         | param_kernel | param_gamma | split0_test_score |...|rank_test_score|
         +==============+=============+===================+===+===============+
-        |    'rbf'     |     0.1     |       0.80        |...|       2       |
+        |    'rbf'     |     0.1     |       0.80        |...|       1       |
         +--------------+-------------+-------------------+---+---------------+
-        |    'rbf'     |     0.2     |       0.90        |...|       1       |
+        |    'rbf'     |     0.2     |       0.84        |...|       3       |
         +--------------+-------------+-------------------+---+---------------+
-        |    'rbf'     |     0.3     |       0.70        |...|       1       |
+        |    'rbf'     |     0.3     |       0.70        |...|       2       |
         +--------------+-------------+-------------------+---+---------------+

         will be represented by a ``cv_results_`` dict of::
@@ -1345,11 +1571,11 @@
             'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],
                                           mask = False),
             'param_gamma'  : masked_array(data = [0.1 0.2 0.3], mask = False),
-            'split0_test_score'  : [0.80, 0.90, 0.70],
+            'split0_test_score'  : [0.80, 0.84, 0.70],
             'split1_test_score'  : [0.82, 0.50, 0.70],
-            'mean_test_score'    : [0.81, 0.70, 0.70],
-            'std_test_score'     : [0.01, 0.20, 0.00],
-            'rank_test_score'    : [3, 1, 1],
+            'mean_test_score'    : [0.81, 0.67, 0.70],
+            'std_test_score'     : [0.01, 0.24, 0.00],
+            'rank_test_score'    : [1, 3, 2],
             'split0_train_score' : [0.80, 0.92, 0.70],
             'split1_train_score' : [0.82, 0.55, 0.70],
             'mean_train_score'   : [0.81, 0.74, 0.70],
@@ -1374,7 +1600,7 @@
         scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown
         above. ('split0_test_precision', 'mean_train_precision' etc.)

-    best_estimator_ : estimator or dict
+    best_estimator_ : estimator
         Estimator that was chosen by the search, i.e. estimator
         which gave highest score (or smallest loss if specified)
         on the left out data. Not available if ``refit=False``.
@@ -1390,6 +1616,8 @@
         For multi-metric evaluation, this is not available if ``refit`` is
         ``False``. See ``refit`` parameter for more information.

+        This attribute is not available if ``refit`` is a function.
+
     best_params_ : dict
         Parameter setting that gave the best results on the hold out data.

@@ -1421,6 +1649,37 @@
         Seconds used for refitting the best model on the whole dataset.

         This is present only if ``refit`` is not False.
+
+        .. versionadded:: 0.20
+
+    multimetric_ : bool
+        Whether or not the scorers compute several metrics.
+
+    classes_ : ndarray of shape (n_classes,)
+        The classes labels. This is present only if ``refit`` is specified and
+        the underlying estimator is a classifier.
+
+    n_features_in_ : int
+        Number of features seen during :term:`fit`. Only defined if
+        `best_estimator_` is defined (see the documentation for the `refit`
+        parameter for more details) and that `best_estimator_` exposes
+        `n_features_in_` when fit.
+
+        .. versionadded:: 0.24
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Only defined if
+        `best_estimator_` is defined (see the documentation for the `refit`
+        parameter for more details) and that `best_estimator_` exposes
+        `feature_names_in_` when fit.
+
+        .. versionadded:: 1.0
+
+    See Also
+    --------
+    GridSearchCV : Does exhaustive search over a grid of parameters.
+    ParameterSampler : A generator over parameter settings, constructed from
+        param_distributions.

     Notes
     -----
@@ -1435,34 +1694,60 @@
     `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *
     n_jobs`.

-    See Also
+    Examples
     --------
-    :class:`GridSearchCV`:
-        Does exhaustive search over a grid of parameters.
-
-    :class:`ParameterSampler`:
-        A generator over parameter settings, constructed from
-        param_distributions.
-
+    >>> from sklearn.datasets import load_iris
+    >>> from sklearn.linear_model import LogisticRegression
+    >>> from sklearn.model_selection import RandomizedSearchCV
+    >>> from scipy.stats import uniform
+    >>> iris = load_iris()
+    >>> logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,
+    ...                               random_state=0)
+    >>> distributions = dict(C=uniform(loc=0, scale=4),
+    ...                      penalty=['l2', 'l1'])
+    >>> clf = RandomizedSearchCV(logistic, distributions, random_state=0)
+    >>> search = clf.fit(iris.data, iris.target)
+    >>> search.best_params_
+    {'C': 2..., 'penalty': 'l1'}
     """
+
     _required_parameters = ["estimator", "param_distributions"]

-    def __init__(self, estimator, param_distributions, n_iter=10, scoring=None,
-                 n_jobs=None, iid='warn', refit=True,
-                 cv='warn', verbose=0, pre_dispatch='2*n_jobs',
-                 random_state=None, error_score='raise-deprecating',
-                 return_train_score=False):
+    def __init__(
+        self,
+        estimator,
+        param_distributions,
+        *,
+        n_iter=10,
+        scoring=None,
+        n_jobs=None,
+        refit=True,
+        cv=None,
+        verbose=0,
+        pre_dispatch="2*n_jobs",
+        random_state=None,
+        error_score=np.nan,
+        return_train_score=False,
+    ):
         self.param_distributions = param_distributions
         self.n_iter = n_iter
         self.random_state = random_state
         super().__init__(
-            estimator=estimator, scoring=scoring,
-            n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,
-            pre_dispatch=pre_dispatch, error_score=error_score,
-            return_train_score=return_train_score)
+            estimator=estimator,
+            scoring=scoring,
+            n_jobs=n_jobs,
+            refit=refit,
+            cv=cv,
+            verbose=verbose,
+            pre_dispatch=pre_dispatch,
+            error_score=error_score,
+            return_train_score=return_train_score,
+        )

     def _run_search(self, evaluate_candidates):
         """Search n_iter candidates from param_distributions"""
-        evaluate_candidates(ParameterSampler(
-            self.param_distributions, self.n_iter,
-            random_state=self.random_state))
+        evaluate_candidates(
+            ParameterSampler(
+                self.param_distributions, self.n_iter, random_state=self.random_state
+            )
+        )
('sklearn/model_selection', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,4 +1,7 @@
+import typing
+
 from ._split import BaseCrossValidator
+from ._split import BaseShuffleSplit
 from ._split import KFold
 from ._split import GroupKFold
 from ._split import StratifiedKFold
@@ -12,6 +15,7 @@
 from ._split import ShuffleSplit
 from ._split import GroupShuffleSplit
 from ._split import StratifiedShuffleSplit
+from ._split import StratifiedGroupKFold
 from ._split import PredefinedSplit
 from ._split import train_test_split
 from ._split import check_cv
@@ -27,33 +31,44 @@
 from ._search import RandomizedSearchCV
 from ._search import ParameterGrid
 from ._search import ParameterSampler
-from ._search import fit_grid_point

-__all__ = ('BaseCrossValidator',
-           'GridSearchCV',
-           'TimeSeriesSplit',
-           'KFold',
-           'GroupKFold',
-           'GroupShuffleSplit',
-           'LeaveOneGroupOut',
-           'LeaveOneOut',
-           'LeavePGroupsOut',
-           'LeavePOut',
-           'RepeatedKFold',
-           'RepeatedStratifiedKFold',
-           'ParameterGrid',
-           'ParameterSampler',
-           'PredefinedSplit',
-           'RandomizedSearchCV',
-           'ShuffleSplit',
-           'StratifiedKFold',
-           'StratifiedShuffleSplit',
-           'check_cv',
-           'cross_val_predict',
-           'cross_val_score',
-           'cross_validate',
-           'fit_grid_point',
-           'learning_curve',
-           'permutation_test_score',
-           'train_test_split',
-           'validation_curve')
+if typing.TYPE_CHECKING:
+    # Avoid errors in type checkers (e.g. mypy) for experimental estimators.
+    # TODO: remove this check once the estimator is no longer experimental.
+    from ._search_successive_halving import (  # noqa
+        HalvingGridSearchCV,
+        HalvingRandomSearchCV,
+    )
+
+
+__all__ = [
+    "BaseCrossValidator",
+    "BaseShuffleSplit",
+    "GridSearchCV",
+    "TimeSeriesSplit",
+    "KFold",
+    "GroupKFold",
+    "GroupShuffleSplit",
+    "LeaveOneGroupOut",
+    "LeaveOneOut",
+    "LeavePGroupsOut",
+    "LeavePOut",
+    "RepeatedKFold",
+    "RepeatedStratifiedKFold",
+    "ParameterGrid",
+    "ParameterSampler",
+    "PredefinedSplit",
+    "RandomizedSearchCV",
+    "ShuffleSplit",
+    "StratifiedKFold",
+    "StratifiedGroupKFold",
+    "StratifiedShuffleSplit",
+    "check_cv",
+    "cross_val_predict",
+    "cross_val_score",
+    "cross_validate",
+    "learning_curve",
+    "permutation_test_score",
+    "train_test_split",
+    "validation_curve",
+]
('sklearn/model_selection', '_validation.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,37 +7,61 @@
 #         Gael Varoquaux <gael.varoquaux@normalesup.org>
 #         Olivier Grisel <olivier.grisel@ensta.org>
 #         Raghav RV <rvraghav93@gmail.com>
+#         Michal Karbownik <michakarbownik@gmail.com>
 # License: BSD 3 clause


 import warnings
 import numbers
 import time
-from traceback import format_exception_only
+from functools import partial
+from traceback import format_exc
+from contextlib import suppress
+from collections import Counter

 import numpy as np
 import scipy.sparse as sp
+from joblib import Parallel, logger

 from ..base import is_classifier, clone
-from ..utils import (indexable, check_random_state, safe_indexing,
-                     _message_with_time)
-from ..utils.validation import _is_arraylike, _num_samples
+from ..utils import indexable, check_random_state, _safe_indexing
+from ..utils.validation import _check_fit_params
+from ..utils.validation import _num_samples
+from ..utils.fixes import delayed
 from ..utils.metaestimators import _safe_split
-from ..utils._joblib import Parallel, delayed
-from ..metrics.scorer import check_scoring, _check_multimetric_scoring
+from ..metrics import check_scoring
+from ..metrics._scorer import _check_multimetric_scoring, _MultimetricScorer
 from ..exceptions import FitFailedWarning
 from ._split import check_cv
 from ..preprocessing import LabelEncoder


-__all__ = ['cross_validate', 'cross_val_score', 'cross_val_predict',
-           'permutation_test_score', 'learning_curve', 'validation_curve']
-
-
-def cross_validate(estimator, X, y=None, groups=None, scoring=None, cv='warn',
-                   n_jobs=None, verbose=0, fit_params=None,
-                   pre_dispatch='2*n_jobs', return_train_score=False,
-                   return_estimator=False, error_score='raise-deprecating'):
+__all__ = [
+    "cross_validate",
+    "cross_val_score",
+    "cross_val_predict",
+    "permutation_test_score",
+    "learning_curve",
+    "validation_curve",
+]
+
+
+def cross_validate(
+    estimator,
+    X,
+    y=None,
+    *,
+    groups=None,
+    scoring=None,
+    cv=None,
+    n_jobs=None,
+    verbose=0,
+    fit_params=None,
+    pre_dispatch="2*n_jobs",
+    return_train_score=False,
+    return_estimator=False,
+    error_score=np.nan,
+):
     """Evaluate metric(s) by cross-validation and also record fit/score times.

     Read more in the :ref:`User Guide <multimetric_cross_validation>`.
@@ -47,65 +71,70 @@
     estimator : estimator object implementing 'fit'
         The object to use to fit the data.

-    X : array-like
+    X : array-like of shape (n_samples, n_features)
         The data to fit. Can be for example a list, or an array.

-    y : array-like, optional, default: None
+    y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None
         The target variable to try to predict in the case of
         supervised learning.

-    groups : array-like, with shape (n_samples,), optional
+    groups : array-like of shape (n_samples,), default=None
         Group labels for the samples used while splitting the dataset into
-        train/test set.
-
-    scoring : string, callable, list/tuple, dict or None, default: None
-        A single string (see :ref:`scoring_parameter`) or a callable
-        (see :ref:`scoring`) to evaluate the predictions on the test set.
-
-        For evaluating multiple metrics, either give a list of (unique) strings
-        or a dict with names as keys and callables as values.
-
-        NOTE that when using custom scorers, each scorer should return a single
-        value. Metric functions returning a list/array of values can be wrapped
-        into multiple scorers that return one value each.
+        train/test set. Only used in conjunction with a "Group" :term:`cv`
+        instance (e.g., :class:`GroupKFold`).
+
+    scoring : str, callable, list, tuple, or dict, default=None
+        Strategy to evaluate the performance of the cross-validated model on
+        the test set.
+
+        If `scoring` represents a single score, one can use:
+
+        - a single string (see :ref:`scoring_parameter`);
+        - a callable (see :ref:`scoring`) that returns a single value.
+
+        If `scoring` represents multiple scores, one can use:
+
+        - a list or tuple of unique strings;
+        - a callable returning a dictionary where the keys are the metric
+          names and the values are the metric scores;
+        - a dictionary with metric names as keys and callables a values.

         See :ref:`multimetric_grid_search` for an example.

-        If None, the estimator's score method is used.
-
-    cv : int, cross-validation generator or an iterable, optional
+    cv : int, cross-validation generator or an iterable, default=None
         Determines the cross-validation splitting strategy.
         Possible inputs for cv are:

-        - None, to use the default 3-fold cross validation,
-        - integer, to specify the number of folds in a `(Stratified)KFold`,
+        - None, to use the default 5-fold cross validation,
+        - int, to specify the number of folds in a `(Stratified)KFold`,
         - :term:`CV splitter`,
         - An iterable yielding (train, test) splits as arrays of indices.

-        For integer/None inputs, if the estimator is a classifier and ``y`` is
+        For int/None inputs, if the estimator is a classifier and ``y`` is
         either binary or multiclass, :class:`StratifiedKFold` is used. In all
-        other cases, :class:`KFold` is used.
+        other cases, :class:`.Fold` is used. These splitters are instantiated
+        with `shuffle=False` so the splits will be the same across calls.

         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.

-        .. versionchanged:: 0.20
-            ``cv`` default value if None will change from 3-fold to 5-fold
-            in v0.22.
-
-    n_jobs : int or None, optional (default=None)
-        The number of CPUs to use to do the computation.
+        .. versionchanged:: 0.22
+            ``cv`` default value if None changed from 3-fold to 5-fold.
+
+    n_jobs : int, default=None
+        Number of jobs to run in parallel. Training the estimator and computing
+        the score are parallelized over the cross-validation splits.
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.

-    verbose : integer, optional
+    verbose : int, default=0
         The verbosity level.

-    fit_params : dict, optional
+    fit_params : dict, default=None
         Parameters to pass to the fit method of the estimator.

-    pre_dispatch : int, or string, optional
+    pre_dispatch : int or str, default='2*n_jobs'
         Controls the number of jobs that get dispatched during parallel
         execution. Reducing this number can be useful to avoid an
         explosion of memory consumption when more jobs get dispatched
@@ -119,10 +148,10 @@
             - An int, giving the exact number of total jobs that are
               spawned

-            - A string, giving an expression as a function of n_jobs,
+            - A str, giving an expression as a function of n_jobs,
               as in '2*n_jobs'

-    return_train_score : boolean, default=False
+    return_train_score : bool, default=False
         Whether to include train scores.
         Computing training scores is used to get insights on how different
         parameter settings impact the overfitting/underfitting trade-off.
@@ -130,22 +159,26 @@
         expensive and is not strictly required to select the parameters that
         yield the best generalization performance.

-    return_estimator : boolean, default False
+        .. versionadded:: 0.19
+
+        .. versionchanged:: 0.21
+            Default value was changed from ``True`` to ``False``
+
+    return_estimator : bool, default=False
         Whether to return the estimators fitted on each split.

-    error_score : 'raise' | 'raise-deprecating' or numeric
+        .. versionadded:: 0.20
+
+    error_score : 'raise' or numeric, default=np.nan
         Value to assign to the score if an error occurs in estimator fitting.
         If set to 'raise', the error is raised.
-        If set to 'raise-deprecating', a FutureWarning is printed before the
-        error is raised.
-        If a numeric value is given, FitFailedWarning is raised. This parameter
-        does not affect the refit step, which will always raise the error.
-        Default is 'raise-deprecating' but from version 0.22 it will change
-        to np.nan.
+        If a numeric value is given, FitFailedWarning is raised.
+
+        .. versionadded:: 0.20

     Returns
     -------
-    scores : dict of float arrays of shape=(n_splits,)
+    scores : dict of float arrays of shape (n_splits,)
         Array of scores of the estimator for each run of the cross validation.

         A dict of arrays containing the score/time arrays for each scorer is
@@ -153,8 +186,14 @@

             ``test_score``
                 The score array for test scores on each cv split.
+                Suffix ``_score`` in ``test_score`` changes to a specific
+                metric like ``test_r2`` or ``test_auc`` if there are
+                multiple scoring metrics in the scoring parameter.
             ``train_score``
                 The score array for train scores on each cv split.
+                Suffix ``_score`` in ``train_score`` changes to a specific
+                metric like ``train_r2`` or ``train_auc`` if there are
+                multiple scoring metrics in the scoring parameter.
                 This is available only if ``return_train_score`` parameter
                 is ``True``.
             ``fit_time``
@@ -169,11 +208,21 @@
                 This is available only if ``return_estimator`` parameter
                 is set to ``True``.

+    See Also
+    --------
+    cross_val_score : Run cross-validation for single metric evaluation.
+
+    cross_val_predict : Get predictions from each split of cross-validation for
+        diagnostic purposes.
+
+    sklearn.metrics.make_scorer : Make a scorer from a performance metric or
+        loss function.
+
     Examples
     --------
     >>> from sklearn import datasets, linear_model
     >>> from sklearn.model_selection import cross_validate
-    >>> from sklearn.metrics.scorer import make_scorer
+    >>> from sklearn.metrics import make_scorer
     >>> from sklearn.metrics import confusion_matrix
     >>> from sklearn.svm import LinearSVC
     >>> diabetes = datasets.load_diabetes()
@@ -184,10 +233,10 @@
     Single metric evaluation using ``cross_validate``

     >>> cv_results = cross_validate(lasso, X, y, cv=3)
-    >>> sorted(cv_results.keys())                         # doctest: +ELLIPSIS
+    >>> sorted(cv_results.keys())
     ['fit_time', 'score_time', 'test_score']
-    >>> cv_results['test_score']    # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
-    array([0.33150734, 0.08022311, 0.03531764])
+    >>> cv_results['test_score']
+    array([0.3315057 , 0.08022103, 0.03531816])

     Multiple metric evaluation using ``cross_validate``
     (please refer the ``scoring`` parameter doc for more information)
@@ -195,70 +244,155 @@
     >>> scores = cross_validate(lasso, X, y, cv=3,
     ...                         scoring=('r2', 'neg_mean_squared_error'),
     ...                         return_train_score=True)
-    >>> print(scores['test_neg_mean_squared_error'])      # doctest: +ELLIPSIS
+    >>> print(scores['test_neg_mean_squared_error'])
     [-3635.5... -3573.3... -6114.7...]
-    >>> print(scores['train_r2'])                         # doctest: +ELLIPSIS
-    [0.28010158 0.39088426 0.22784852]
-
-    See Also
-    ---------
-    :func:`sklearn.model_selection.cross_val_score`:
-        Run cross-validation for single metric evaluation.
-
-    :func:`sklearn.model_selection.cross_val_predict`:
-        Get predictions from each split of cross-validation for diagnostic
-        purposes.
-
-    :func:`sklearn.metrics.make_scorer`:
-        Make a scorer from a performance metric or loss function.
-
+    >>> print(scores['train_r2'])
+    [0.28009951 0.3908844  0.22784907]
     """
     X, y, groups = indexable(X, y, groups)

     cv = check_cv(cv, y, classifier=is_classifier(estimator))
-    scorers, _ = _check_multimetric_scoring(estimator, scoring=scoring)
+
+    if callable(scoring):
+        scorers = scoring
+    elif scoring is None or isinstance(scoring, str):
+        scorers = check_scoring(estimator, scoring)
+    else:
+        scorers = _check_multimetric_scoring(estimator, scoring)

     # We clone the estimator to make sure that all the folds are
     # independent, and that it is pickle-able.
-    parallel = Parallel(n_jobs=n_jobs, verbose=verbose,
-                        pre_dispatch=pre_dispatch)
-    scores = parallel(
+    parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
+    results = parallel(
         delayed(_fit_and_score)(
-            clone(estimator), X, y, scorers, train, test, verbose, None,
-            fit_params, return_train_score=return_train_score,
-            return_times=True, return_estimator=return_estimator,
-            error_score=error_score)
-        for train, test in cv.split(X, y, groups))
-
-    zipped_scores = list(zip(*scores))
+            clone(estimator),
+            X,
+            y,
+            scorers,
+            train,
+            test,
+            verbose,
+            None,
+            fit_params,
+            return_train_score=return_train_score,
+            return_times=True,
+            return_estimator=return_estimator,
+            error_score=error_score,
+        )
+        for train, test in cv.split(X, y, groups)
+    )
+
+    _warn_or_raise_about_fit_failures(results, error_score)
+
+    # For callabe scoring, the return type is only know after calling. If the
+    # return type is a dictionary, the error scores can now be inserted with
+    # the correct key.
+    if callable(scoring):
+        _insert_error_scores(results, error_score)
+
+    results = _aggregate_score_dicts(results)
+
+    ret = {}
+    ret["fit_time"] = results["fit_time"]
+    ret["score_time"] = results["score_time"]
+
+    if return_estimator:
+        ret["estimator"] = results["estimator"]
+
+    test_scores_dict = _normalize_score_results(results["test_scores"])
     if return_train_score:
-        train_scores = zipped_scores.pop(0)
-        train_scores = _aggregate_score_dicts(train_scores)
-    if return_estimator:
-        fitted_estimators = zipped_scores.pop()
-    test_scores, fit_times, score_times = zipped_scores
-    test_scores = _aggregate_score_dicts(test_scores)
-
-    ret = {}
-    ret['fit_time'] = np.array(fit_times)
-    ret['score_time'] = np.array(score_times)
-
-    if return_estimator:
-        ret['estimator'] = fitted_estimators
-
-    for name in scorers:
-        ret['test_%s' % name] = np.array(test_scores[name])
+        train_scores_dict = _normalize_score_results(results["train_scores"])
+
+    for name in test_scores_dict:
+        ret["test_%s" % name] = test_scores_dict[name]
         if return_train_score:
-            key = 'train_%s' % name
-            ret[key] = np.array(train_scores[name])
+            key = "train_%s" % name
+            ret[key] = train_scores_dict[name]

     return ret


-def cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv='warn',
-                    n_jobs=None, verbose=0, fit_params=None,
-                    pre_dispatch='2*n_jobs', error_score='raise-deprecating'):
-    """Evaluate a score by cross-validation
+def _insert_error_scores(results, error_score):
+    """Insert error in `results` by replacing them inplace with `error_score`.
+
+    This only applies to multimetric scores because `_fit_and_score` will
+    handle the single metric case.
+    """
+    successful_score = None
+    failed_indices = []
+    for i, result in enumerate(results):
+        if result["fit_error"] is not None:
+            failed_indices.append(i)
+        elif successful_score is None:
+            successful_score = result["test_scores"]
+
+    if isinstance(successful_score, dict):
+        formatted_error = {name: error_score for name in successful_score}
+        for i in failed_indices:
+            results[i]["test_scores"] = formatted_error.copy()
+            if "train_scores" in results[i]:
+                results[i]["train_scores"] = formatted_error.copy()
+
+
+def _normalize_score_results(scores, scaler_score_key="score"):
+    """Creates a scoring dictionary based on the type of `scores`"""
+    if isinstance(scores[0], dict):
+        # multimetric scoring
+        return _aggregate_score_dicts(scores)
+    # scaler
+    return {scaler_score_key: scores}
+
+
+def _warn_or_raise_about_fit_failures(results, error_score):
+    fit_errors = [
+        result["fit_error"] for result in results if result["fit_error"] is not None
+    ]
+    if fit_errors:
+        num_failed_fits = len(fit_errors)
+        num_fits = len(results)
+        fit_errors_counter = Counter(fit_errors)
+        delimiter = "-" * 80 + "\n"
+        fit_errors_summary = "\n".join(
+            f"{delimiter}{n} fits failed with the following error:\n{error}"
+            for error, n in fit_errors_counter.items()
+        )
+
+        if num_failed_fits == num_fits:
+            all_fits_failed_message = (
+                f"\nAll the {num_fits} fits failed.\n"
+                "It is is very likely that your model is misconfigured.\n"
+                "You can try to debug the error by setting error_score='raise'.\n\n"
+                f"Below are more details about the failures:\n{fit_errors_summary}"
+            )
+            raise ValueError(all_fits_failed_message)
+
+        else:
+            some_fits_failed_message = (
+                f"\n{num_failed_fits} fits failed out of a total of {num_fits}.\n"
+                "The score on these train-test partitions for these parameters"
+                f" will be set to {error_score}.\n"
+                "If these failures are not expected, you can try to debug them "
+                "by setting error_score='raise'.\n\n"
+                f"Below are more details about the failures:\n{fit_errors_summary}"
+            )
+            warnings.warn(some_fits_failed_message, FitFailedWarning)
+
+
+def cross_val_score(
+    estimator,
+    X,
+    y=None,
+    *,
+    groups=None,
+    scoring=None,
+    cv=None,
+    n_jobs=None,
+    verbose=0,
+    fit_params=None,
+    pre_dispatch="2*n_jobs",
+    error_score=np.nan,
+):
+    """Evaluate a score by cross-validation.

     Read more in the :ref:`User Guide <cross_validation>`.

@@ -267,19 +401,21 @@
     estimator : estimator object implementing 'fit'
         The object to use to fit the data.

-    X : array-like
+    X : array-like of shape (n_samples, n_features)
         The data to fit. Can be for example a list, or an array.

-    y : array-like, optional, default: None
+    y : array-like of shape (n_samples,) or (n_samples, n_outputs), \
+            default=None
         The target variable to try to predict in the case of
         supervised learning.

-    groups : array-like, with shape (n_samples,), optional
+    groups : array-like of shape (n_samples,), default=None
         Group labels for the samples used while splitting the dataset into
-        train/test set.
-
-    scoring : string, callable or None, optional, default: None
-        A string (see model evaluation documentation) or
+        train/test set. Only used in conjunction with a "Group" :term:`cv`
+        instance (e.g., :class:`GroupKFold`).
+
+    scoring : str or callable, default=None
+        A str (see model evaluation documentation) or
         a scorer callable object / function with signature
         ``scorer(estimator, X, y)`` which should return only
         a single value.
@@ -287,47 +423,48 @@
         Similar to :func:`cross_validate`
         but only a single metric is permitted.

-        If None, the estimator's default scorer (if available) is used.
-
-    cv : int, cross-validation generator or an iterable, optional
+        If `None`, the estimator's default scorer (if available) is used.
+
+    cv : int, cross-validation generator or an iterable, default=None
         Determines the cross-validation splitting strategy.
         Possible inputs for cv are:

-        - None, to use the default 3-fold cross validation,
-        - integer, to specify the number of folds in a `(Stratified)KFold`,
+        - `None`, to use the default 5-fold cross validation,
+        - int, to specify the number of folds in a `(Stratified)KFold`,
         - :term:`CV splitter`,
-        - An iterable yielding (train, test) splits as arrays of indices.
-
-        For integer/None inputs, if the estimator is a classifier and ``y`` is
+        - An iterable that generates (train, test) splits as arrays of indices.
+
+        For `int`/`None` inputs, if the estimator is a classifier and `y` is
         either binary or multiclass, :class:`StratifiedKFold` is used. In all
-        other cases, :class:`KFold` is used.
+        other cases, :class:`KFold` is used. These splitters are instantiated
+        with `shuffle=False` so the splits will be the same across calls.

         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.

-        .. versionchanged:: 0.20
-            ``cv`` default value if None will change from 3-fold to 5-fold
-            in v0.22.
-
-    n_jobs : int or None, optional (default=None)
-        The number of CPUs to use to do the computation.
+        .. versionchanged:: 0.22
+            `cv` default value if `None` changed from 3-fold to 5-fold.
+
+    n_jobs : int, default=None
+        Number of jobs to run in parallel. Training the estimator and computing
+        the score are parallelized over the cross-validation splits.
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.

-    verbose : integer, optional
+    verbose : int, default=0
         The verbosity level.

-    fit_params : dict, optional
+    fit_params : dict, default=None
         Parameters to pass to the fit method of the estimator.

-    pre_dispatch : int, or string, optional
+    pre_dispatch : int or str, default='2*n_jobs'
         Controls the number of jobs that get dispatched during parallel
         execution. Reducing this number can be useful to avoid an
         explosion of memory consumption when more jobs get dispatched
         than CPUs can process. This parameter can be:

-            - None, in which case all the jobs are immediately
+            - ``None``, in which case all the jobs are immediately
               created and spawned. Use this for lightweight and
               fast-running jobs, to avoid delays due to on-demand
               spawning of the jobs
@@ -335,23 +472,31 @@
             - An int, giving the exact number of total jobs that are
               spawned

-            - A string, giving an expression as a function of n_jobs,
+            - A str, giving an expression as a function of n_jobs,
               as in '2*n_jobs'

-    error_score : 'raise' | 'raise-deprecating' or numeric
+    error_score : 'raise' or numeric, default=np.nan
         Value to assign to the score if an error occurs in estimator fitting.
         If set to 'raise', the error is raised.
-        If set to 'raise-deprecating', a FutureWarning is printed before the
-        error is raised.
-        If a numeric value is given, FitFailedWarning is raised. This parameter
-        does not affect the refit step, which will always raise the error.
-        Default is 'raise-deprecating' but from version 0.22 it will change
-        to np.nan.
+        If a numeric value is given, FitFailedWarning is raised.
+
+        .. versionadded:: 0.20

     Returns
     -------
-    scores : array of float, shape=(len(list(cv)),)
+    scores : ndarray of float of shape=(len(list(cv)),)
         Array of scores of the estimator for each run of the cross validation.
+
+    See Also
+    --------
+    cross_validate : To run cross-validation on multiple metrics and also to
+        return train scores, fit times and score times.
+
+    cross_val_predict : Get predictions from each split of cross-validation for
+        diagnostic purposes.
+
+    sklearn.metrics.make_scorer : Make a scorer from a performance metric or
+        loss function.

     Examples
     --------
@@ -361,40 +506,48 @@
     >>> X = diabetes.data[:150]
     >>> y = diabetes.target[:150]
     >>> lasso = linear_model.Lasso()
-    >>> print(cross_val_score(lasso, X, y, cv=3))  # doctest: +ELLIPSIS
-    [0.33150734 0.08022311 0.03531764]
-
-    See Also
-    ---------
-    :func:`sklearn.model_selection.cross_validate`:
-        To run cross-validation on multiple metrics and also to return
-        train scores, fit times and score times.
-
-    :func:`sklearn.model_selection.cross_val_predict`:
-        Get predictions from each split of cross-validation for diagnostic
-        purposes.
-
-    :func:`sklearn.metrics.make_scorer`:
-        Make a scorer from a performance metric or loss function.
-
+    >>> print(cross_val_score(lasso, X, y, cv=3))
+    [0.3315057  0.08022103 0.03531816]
     """
     # To ensure multimetric format is not supported
     scorer = check_scoring(estimator, scoring=scoring)

-    cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,
-                                scoring={'score': scorer}, cv=cv,
-                                n_jobs=n_jobs, verbose=verbose,
-                                fit_params=fit_params,
-                                pre_dispatch=pre_dispatch,
-                                error_score=error_score)
-    return cv_results['test_score']
-
-
-def _fit_and_score(estimator, X, y, scorer, train, test, verbose,
-                   parameters, fit_params, return_train_score=False,
-                   return_parameters=False, return_n_test_samples=False,
-                   return_times=False, return_estimator=False,
-                   error_score='raise-deprecating'):
+    cv_results = cross_validate(
+        estimator=estimator,
+        X=X,
+        y=y,
+        groups=groups,
+        scoring={"score": scorer},
+        cv=cv,
+        n_jobs=n_jobs,
+        verbose=verbose,
+        fit_params=fit_params,
+        pre_dispatch=pre_dispatch,
+        error_score=error_score,
+    )
+    return cv_results["test_score"]
+
+
+def _fit_and_score(
+    estimator,
+    X,
+    y,
+    scorer,
+    train,
+    test,
+    verbose,
+    parameters,
+    fit_params,
+    return_train_score=False,
+    return_parameters=False,
+    return_n_test_samples=False,
+    return_times=False,
+    return_estimator=False,
+    split_progress=None,
+    candidate_progress=None,
+    error_score=np.nan,
+):
+
     """Fit estimator and compute scores for a given dataset split.

     Parameters
@@ -402,10 +555,10 @@
     estimator : estimator object implementing 'fit'
         The object to use to fit the data.

-    X : array-like of shape at least 2D
+    X : array-like of shape (n_samples, n_features)
         The data to fit.

-    y : array-like, optional, default: None
+    y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None
         The target variable to try to predict in the case of
         supervised learning.

@@ -419,24 +572,19 @@
         The callable object / fn should have signature
         ``scorer(estimator, X, y)``.

-    train : array-like, shape (n_train_samples,)
+    train : array-like of shape (n_train_samples,)
         Indices of training samples.

-    test : array-like, shape (n_test_samples,)
+    test : array-like of shape (n_test_samples,)
         Indices of test samples.

-    verbose : integer
+    verbose : int
         The verbosity level.

-    error_score : 'raise' | 'raise-deprecating' or numeric
+    error_score : 'raise' or numeric, default=np.nan
         Value to assign to the score if an error occurs in estimator fitting.
         If set to 'raise', the error is raised.
-        If set to 'raise-deprecating', a FutureWarning is printed before the
-        error is raised.
-        If a numeric value is given, FitFailedWarning is raised. This parameter
-        does not affect the refit step, which will always raise the error.
-        Default is 'raise-deprecating' but from version 0.22 it will change
-        to np.nan.
+        If a numeric value is given, FitFailedWarning is raised.

     parameters : dict or None
         Parameters to be set on the estimator.
@@ -444,211 +592,237 @@
     fit_params : dict or None
         Parameters that will be passed to ``estimator.fit``.

-    return_train_score : boolean, optional, default: False
+    return_train_score : bool, default=False
         Compute and return score on training set.

-    return_parameters : boolean, optional, default: False
+    return_parameters : bool, default=False
         Return parameters that has been used for the estimator.

-    return_n_test_samples : boolean, optional, default: False
-        Whether to return the ``n_test_samples``
-
-    return_times : boolean, optional, default: False
+    split_progress : {list, tuple} of int, default=None
+        A list or tuple of format (<current_split_id>, <total_num_of_splits>).
+
+    candidate_progress : {list, tuple} of int, default=None
+        A list or tuple of format
+        (<current_candidate_id>, <total_number_of_candidates>).
+
+    return_n_test_samples : bool, default=False
+        Whether to return the ``n_test_samples``.
+
+    return_times : bool, default=False
         Whether to return the fit/score times.

-    return_estimator : boolean, optional, default: False
+    return_estimator : bool, default=False
         Whether to return the fitted estimator.

     Returns
     -------
-    train_scores : dict of scorer name -> float, optional
-        Score on training set (for all the scorers),
-        returned only if `return_train_score` is `True`.
-
-    test_scores : dict of scorer name -> float, optional
-        Score on testing set (for all the scorers).
-
-    n_test_samples : int
-        Number of test samples.
-
-    fit_time : float
-        Time spent for fitting in seconds.
-
-    score_time : float
-        Time spent for scoring in seconds.
-
-    parameters : dict or None, optional
-        The parameters that have been evaluated.
-
-    estimator : estimator object
-        The fitted estimator
+    result : dict with the following attributes
+        train_scores : dict of scorer name -> float
+            Score on training set (for all the scorers),
+            returned only if `return_train_score` is `True`.
+        test_scores : dict of scorer name -> float
+            Score on testing set (for all the scorers).
+        n_test_samples : int
+            Number of test samples.
+        fit_time : float
+            Time spent for fitting in seconds.
+        score_time : float
+            Time spent for scoring in seconds.
+        parameters : dict or None
+            The parameters that have been evaluated.
+        estimator : estimator object
+            The fitted estimator.
+        fit_error : str or None
+            Traceback str if the fit failed, None if the fit succeeded.
     """
+    if not isinstance(error_score, numbers.Number) and error_score != "raise":
+        raise ValueError(
+            "error_score must be the string 'raise' or a numeric value. "
+            "(Hint: if using 'raise', please make sure that it has been "
+            "spelled correctly.)"
+        )
+
+    progress_msg = ""
+    if verbose > 2:
+        if split_progress is not None:
+            progress_msg = f" {split_progress[0]+1}/{split_progress[1]}"
+        if candidate_progress and verbose > 9:
+            progress_msg += f"; {candidate_progress[0]+1}/{candidate_progress[1]}"
+
     if verbose > 1:
         if parameters is None:
-            msg = ''
+            params_msg = ""
         else:
-            msg = '%s' % (', '.join('%s=%s' % (k, v)
-                          for k, v in parameters.items()))
-        print("[CV] %s %s" % (msg, (64 - len(msg)) * '.'))
+            sorted_keys = sorted(parameters)  # Ensure deterministic o/p
+            params_msg = ", ".join(f"{k}={parameters[k]}" for k in sorted_keys)
+    if verbose > 9:
+        start_msg = f"[CV{progress_msg}] START {params_msg}"
+        print(f"{start_msg}{(80 - len(start_msg)) * '.'}")

     # Adjust length of sample weights
     fit_params = fit_params if fit_params is not None else {}
-    fit_params = {k: _index_param_value(X, v, train)
-                  for k, v in fit_params.items()}
-
-    train_scores = {}
+    fit_params = _check_fit_params(X, fit_params, train)
+
     if parameters is not None:
-        estimator.set_params(**parameters)
+        # clone after setting parameters in case any parameters
+        # are estimators (like pipeline steps)
+        # because pipeline doesn't clone steps in fit
+        cloned_parameters = {}
+        for k, v in parameters.items():
+            cloned_parameters[k] = clone(v, safe=False)
+
+        estimator = estimator.set_params(**cloned_parameters)

     start_time = time.time()

     X_train, y_train = _safe_split(estimator, X, y, train)
     X_test, y_test = _safe_split(estimator, X, y, test, train)

-    is_multimetric = not callable(scorer)
-    n_scorers = len(scorer.keys()) if is_multimetric else 1
+    result = {}
     try:
         if y_train is None:
             estimator.fit(X_train, **fit_params)
         else:
             estimator.fit(X_train, y_train, **fit_params)

-    except Exception as e:
+    except Exception:
         # Note fit time as time until error
         fit_time = time.time() - start_time
         score_time = 0.0
-        if error_score == 'raise':
-            raise
-        elif error_score == 'raise-deprecating':
-            warnings.warn("From version 0.22, errors during fit will result "
-                          "in a cross validation score of NaN by default. Use "
-                          "error_score='raise' if you want an exception "
-                          "raised or error_score=np.nan to adopt the "
-                          "behavior from version 0.22.",
-                          FutureWarning)
+        if error_score == "raise":
             raise
         elif isinstance(error_score, numbers.Number):
-            if is_multimetric:
-                test_scores = dict(zip(scorer.keys(),
-                                   [error_score, ] * n_scorers))
+            if isinstance(scorer, dict):
+                test_scores = {name: error_score for name in scorer}
                 if return_train_score:
-                    train_scores = dict(zip(scorer.keys(),
-                                        [error_score, ] * n_scorers))
+                    train_scores = test_scores.copy()
             else:
                 test_scores = error_score
                 if return_train_score:
                     train_scores = error_score
-            warnings.warn("Estimator fit failed. The score on this train-test"
-                          " partition for these parameters will be set to %f. "
-                          "Details: \n%s" %
-                          (error_score, format_exception_only(type(e), e)[0]),
-                          FitFailedWarning)
-        else:
-            raise ValueError("error_score must be the string 'raise' or a"
-                             " numeric value. (Hint: if using 'raise', please"
-                             " make sure that it has been spelled correctly.)")
-
+        result["fit_error"] = format_exc()
     else:
+        result["fit_error"] = None
+
         fit_time = time.time() - start_time
-        # _score will return dict if is_multimetric is True
-        test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)
+        test_scores = _score(estimator, X_test, y_test, scorer, error_score)
         score_time = time.time() - start_time - fit_time
         if return_train_score:
-            train_scores = _score(estimator, X_train, y_train, scorer,
-                                  is_multimetric)
-    if verbose > 2:
-        if is_multimetric:
-            for scorer_name in sorted(test_scores):
-                msg += ", %s=" % scorer_name
-                if return_train_score:
-                    msg += "(train=%.3f," % train_scores[scorer_name]
-                    msg += " test=%.3f)" % test_scores[scorer_name]
-                else:
-                    msg += "%.3f" % test_scores[scorer_name]
-        else:
-            msg += ", score="
-            msg += ("%.3f" % test_scores if not return_train_score else
-                    "(train=%.3f, test=%.3f)" % (train_scores, test_scores))
+            train_scores = _score(estimator, X_train, y_train, scorer, error_score)

     if verbose > 1:
         total_time = score_time + fit_time
-        print(_message_with_time('CV', msg, total_time))
-
-    ret = [train_scores, test_scores] if return_train_score else [test_scores]
-
+        end_msg = f"[CV{progress_msg}] END "
+        result_msg = params_msg + (";" if params_msg else "")
+        if verbose > 2:
+            if isinstance(test_scores, dict):
+                for scorer_name in sorted(test_scores):
+                    result_msg += f" {scorer_name}: ("
+                    if return_train_score:
+                        scorer_scores = train_scores[scorer_name]
+                        result_msg += f"train={scorer_scores:.3f}, "
+                    result_msg += f"test={test_scores[scorer_name]:.3f})"
+            else:
+                result_msg += ", score="
+                if return_train_score:
+                    result_msg += f"(train={train_scores:.3f}, test={test_scores:.3f})"
+                else:
+                    result_msg += f"{test_scores:.3f}"
+        result_msg += f" total time={logger.short_format_time(total_time)}"
+
+        # Right align the result_msg
+        end_msg += "." * (80 - len(end_msg) - len(result_msg))
+        end_msg += result_msg
+        print(end_msg)
+
+    result["test_scores"] = test_scores
+    if return_train_score:
+        result["train_scores"] = train_scores
     if return_n_test_samples:
-        ret.append(_num_samples(X_test))
+        result["n_test_samples"] = _num_samples(X_test)
     if return_times:
-        ret.extend([fit_time, score_time])
+        result["fit_time"] = fit_time
+        result["score_time"] = score_time
     if return_parameters:
-        ret.append(parameters)
+        result["parameters"] = parameters
     if return_estimator:
-        ret.append(estimator)
-    return ret
-
-
-def _score(estimator, X_test, y_test, scorer, is_multimetric=False):
+        result["estimator"] = estimator
+    return result
+
+
+def _score(estimator, X_test, y_test, scorer, error_score="raise"):
     """Compute the score(s) of an estimator on a given test set.

-    Will return a single float if is_multimetric is False and a dict of floats,
-    if is_multimetric is True
+    Will return a dict of floats if `scorer` is a dict, otherwise a single
+    float is returned.
     """
-    if is_multimetric:
-        return _multimetric_score(estimator, X_test, y_test, scorer)
-    else:
+    if isinstance(scorer, dict):
+        # will cache method calls if needed. scorer() returns a dict
+        scorer = _MultimetricScorer(**scorer)
+
+    try:
         if y_test is None:
-            score = scorer(estimator, X_test)
+            scores = scorer(estimator, X_test)
         else:
-            score = scorer(estimator, X_test, y_test)
-
-        if hasattr(score, 'item'):
-            try:
+            scores = scorer(estimator, X_test, y_test)
+    except Exception:
+        if error_score == "raise":
+            raise
+        else:
+            if isinstance(scorer, _MultimetricScorer):
+                scores = {name: error_score for name in scorer._scorers}
+            else:
+                scores = error_score
+            warnings.warn(
+                "Scoring failed. The score on this train-test partition for "
+                f"these parameters will be set to {error_score}. Details: \n"
+                f"{format_exc()}",
+                UserWarning,
+            )
+
+    error_msg = "scoring must return a number, got %s (%s) instead. (scorer=%s)"
+    if isinstance(scores, dict):
+        for name, score in scores.items():
+            if hasattr(score, "item"):
+                with suppress(ValueError):
+                    # e.g. unwrap memmapped scalars
+                    score = score.item()
+            if not isinstance(score, numbers.Number):
+                raise ValueError(error_msg % (score, type(score), name))
+            scores[name] = score
+    else:  # scalar
+        if hasattr(scores, "item"):
+            with suppress(ValueError):
                 # e.g. unwrap memmapped scalars
-                score = score.item()
-            except ValueError:
-                # non-scalar?
-                pass
-
-        if not isinstance(score, numbers.Number):
-            raise ValueError("scoring must return a number, got %s (%s) "
-                             "instead. (scorer=%r)"
-                             % (str(score), type(score), scorer))
-    return score
-
-
-def _multimetric_score(estimator, X_test, y_test, scorers):
-    """Return a dict of score for multimetric scoring"""
-    scores = {}
-
-    for name, scorer in scorers.items():
-        if y_test is None:
-            score = scorer(estimator, X_test)
-        else:
-            score = scorer(estimator, X_test, y_test)
-
-        if hasattr(score, 'item'):
-            try:
-                # e.g. unwrap memmapped scalars
-                score = score.item()
-            except ValueError:
-                # non-scalar?
-                pass
-        scores[name] = score
-
-        if not isinstance(score, numbers.Number):
-            raise ValueError("scoring must return a number, got %s (%s) "
-                             "instead. (scorer=%s)"
-                             % (str(score), type(score), name))
+                scores = scores.item()
+        if not isinstance(scores, numbers.Number):
+            raise ValueError(error_msg % (scores, type(scores), scorer))
     return scores


-def cross_val_predict(estimator, X, y=None, groups=None, cv='warn',
-                      n_jobs=None, verbose=0, fit_params=None,
-                      pre_dispatch='2*n_jobs', method='predict'):
-    """Generate cross-validated estimates for each input data point
-
-    It is not appropriate to pass these predictions into an evaluation
-    metric. Use :func:`cross_validate` to measure generalization error.
+def cross_val_predict(
+    estimator,
+    X,
+    y=None,
+    *,
+    groups=None,
+    cv=None,
+    n_jobs=None,
+    verbose=0,
+    fit_params=None,
+    pre_dispatch="2*n_jobs",
+    method="predict",
+):
+    """Generate cross-validated estimates for each input data point.
+
+    The data is split according to the cv parameter. Each sample belongs
+    to exactly one test set, and its prediction is computed with an
+    estimator fitted on the corresponding training set.
+
+    Passing these predictions into an evaluation metric may not be a valid
+    way to measure generalization performance. Results can differ from
+    :func:`cross_validate` and :func:`cross_val_score` unless all tests sets
+    have equal size and the metric decomposes over samples.

     Read more in the :ref:`User Guide <cross_validation>`.

@@ -657,50 +831,53 @@
     estimator : estimator object implementing 'fit' and 'predict'
         The object to use to fit the data.

-    X : array-like
+    X : array-like of shape (n_samples, n_features)
         The data to fit. Can be, for example a list, or an array at least 2d.

-    y : array-like, optional, default: None
+    y : array-like of shape (n_samples,) or (n_samples, n_outputs), \
+            default=None
         The target variable to try to predict in the case of
         supervised learning.

-    groups : array-like, with shape (n_samples,), optional
+    groups : array-like of shape (n_samples,), default=None
         Group labels for the samples used while splitting the dataset into
-        train/test set.
-
-    cv : int, cross-validation generator or an iterable, optional
+        train/test set. Only used in conjunction with a "Group" :term:`cv`
+        instance (e.g., :class:`GroupKFold`).
+
+    cv : int, cross-validation generator or an iterable, default=None
         Determines the cross-validation splitting strategy.
         Possible inputs for cv are:

-        - None, to use the default 3-fold cross validation,
-        - integer, to specify the number of folds in a `(Stratified)KFold`,
+        - None, to use the default 5-fold cross validation,
+        - int, to specify the number of folds in a `(Stratified)KFold`,
         - :term:`CV splitter`,
-        - An iterable yielding (train, test) splits as arrays of indices.
-
-        For integer/None inputs, if the estimator is a classifier and ``y`` is
+        - An iterable that generates (train, test) splits as arrays of indices.
+
+        For int/None inputs, if the estimator is a classifier and ``y`` is
         either binary or multiclass, :class:`StratifiedKFold` is used. In all
-        other cases, :class:`KFold` is used.
+        other cases, :class:`KFold` is used. These splitters are instantiated
+        with `shuffle=False` so the splits will be the same across calls.

         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.

-        .. versionchanged:: 0.20
-            ``cv`` default value if None will change from 3-fold to 5-fold
-            in v0.22.
-
-    n_jobs : int or None, optional (default=None)
-        The number of CPUs to use to do the computation.
+        .. versionchanged:: 0.22
+            ``cv`` default value if None changed from 3-fold to 5-fold.
+
+    n_jobs : int, default=None
+        Number of jobs to run in parallel. Training the estimator and
+        predicting are parallelized over the cross-validation splits.
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.

-    verbose : integer, optional
+    verbose : int, default=0
         The verbosity level.

-    fit_params : dict, optional
+    fit_params : dict, default=None
         Parameters to pass to the fit method of the estimator.

-    pre_dispatch : int, or string, optional
+    pre_dispatch : int or str, default='2*n_jobs'
         Controls the number of jobs that get dispatched during parallel
         execution. Reducing this number can be useful to avoid an
         explosion of memory consumption when more jobs get dispatched
@@ -714,24 +891,31 @@
             - An int, giving the exact number of total jobs that are
               spawned

-            - A string, giving an expression as a function of n_jobs,
+            - A str, giving an expression as a function of n_jobs,
               as in '2*n_jobs'

-    method : string, optional, default: 'predict'
-        Invokes the passed method name of the passed estimator. For
-        method='predict_proba', the columns correspond to the classes
-        in sorted order.
+    method : {'predict', 'predict_proba', 'predict_log_proba', \
+              'decision_function'}, default='predict'
+        The method to be invoked by `estimator`.

     Returns
     -------
     predictions : ndarray
-        This is the result of calling ``method``
-
-    See also
+        This is the result of calling `method`. Shape:
+
+            - When `method` is 'predict' and in special case where `method` is
+              'decision_function' and the target is binary: (n_samples,)
+            - When `method` is one of {'predict_proba', 'predict_log_proba',
+              'decision_function'} (unless special case above):
+              (n_samples, n_classes)
+            - If `estimator` is :term:`multioutput`, an extra dimension
+              'n_outputs' is added to the end of each shape above.
+
+    See Also
     --------
-    cross_val_score : calculate score for each CV split
-
-    cross_validate : calculate one or more scores and timings for each CV split
+    cross_val_score : Calculate score for each CV split.
+    cross_validate : Calculate one or more scores and timings for each CV
+        split.

     Notes
     -----
@@ -755,37 +939,38 @@
     X, y, groups = indexable(X, y, groups)

     cv = check_cv(cv, y, classifier=is_classifier(estimator))
+    splits = list(cv.split(X, y, groups))
+
+    test_indices = np.concatenate([test for _, test in splits])
+    if not _check_is_permutation(test_indices, _num_samples(X)):
+        raise ValueError("cross_val_predict only works for partitions")

     # If classification methods produce multiple columns of output,
     # we need to manually encode classes to ensure consistent column ordering.
-    encode = method in ['decision_function', 'predict_proba',
-                        'predict_log_proba']
+    encode = (
+        method in ["decision_function", "predict_proba", "predict_log_proba"]
+        and y is not None
+    )
     if encode:
         y = np.asarray(y)
         if y.ndim == 1:
             le = LabelEncoder()
             y = le.fit_transform(y)
         elif y.ndim == 2:
-            y_enc = np.zeros_like(y, dtype=np.int)
+            y_enc = np.zeros_like(y, dtype=int)
             for i_label in range(y.shape[1]):
                 y_enc[:, i_label] = LabelEncoder().fit_transform(y[:, i_label])
             y = y_enc

     # We clone the estimator to make sure that all the folds are
     # independent, and that it is pickle-able.
-    parallel = Parallel(n_jobs=n_jobs, verbose=verbose,
-                        pre_dispatch=pre_dispatch)
-    prediction_blocks = parallel(delayed(_fit_and_predict)(
-        clone(estimator), X, y, train, test, verbose, fit_params, method)
-        for train, test in cv.split(X, y, groups))
-
-    # Concatenate the predictions
-    predictions = [pred_block_i for pred_block_i, _ in prediction_blocks]
-    test_indices = np.concatenate([indices_i
-                                   for _, indices_i in prediction_blocks])
-
-    if not _check_is_permutation(test_indices, _num_samples(X)):
-        raise ValueError('cross_val_predict only works for partitions')
+    parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
+    predictions = parallel(
+        delayed(_fit_and_predict)(
+            clone(estimator), X, y, train, test, verbose, fit_params, method
+        )
+        for train, test in splits
+    )

     inv_test_indices = np.empty(len(test_indices), dtype=int)
     inv_test_indices[test_indices] = np.arange(len(test_indices))
@@ -812,8 +997,7 @@
         return predictions[inv_test_indices]


-def _fit_and_predict(estimator, X, y, train, test, verbose, fit_params,
-                     method):
+def _fit_and_predict(estimator, X, y, train, test, verbose, fit_params, method):
     """Fit estimator and predict values for a given dataset split.

     Read more in the :ref:`User Guide <cross_validation>`.
@@ -823,40 +1007,39 @@
     estimator : estimator object implementing 'fit' and 'predict'
         The object to use to fit the data.

-    X : array-like of shape at least 2D
+    X : array-like of shape (n_samples, n_features)
         The data to fit.

-    y : array-like, optional, default: None
+        .. versionchanged:: 0.20
+            X is only required to be an object with finite length or shape now
+
+    y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None
         The target variable to try to predict in the case of
         supervised learning.

-    train : array-like, shape (n_train_samples,)
+    train : array-like of shape (n_train_samples,)
         Indices of training samples.

-    test : array-like, shape (n_test_samples,)
+    test : array-like of shape (n_test_samples,)
         Indices of test samples.

-    verbose : integer
+    verbose : int
         The verbosity level.

     fit_params : dict or None
         Parameters that will be passed to ``estimator.fit``.

-    method : string
+    method : str
         Invokes the passed method name of the passed estimator.

     Returns
     -------
     predictions : sequence
         Result of calling 'estimator.method'
-
-    test : array-like
-        This is the value of the test parameter
     """
     # Adjust length of sample weights
     fit_params = fit_params if fit_params is not None else {}
-    fit_params = {k: _index_param_value(X, v, train)
-                  for k, v in fit_params.items()}
+    fit_params = _check_fit_params(X, fit_params, train)

     X_train, y_train = _safe_split(estimator, X, y, train)
     X_test, _ = _safe_split(estimator, X, y, test, train)
@@ -867,18 +1050,30 @@
         estimator.fit(X_train, y_train, **fit_params)
     func = getattr(estimator, method)
     predictions = func(X_test)
-    if method in ['decision_function', 'predict_proba', 'predict_log_proba']:
+
+    encode = (
+        method in ["decision_function", "predict_proba", "predict_log_proba"]
+        and y is not None
+    )
+
+    if encode:
         if isinstance(predictions, list):
-            predictions = [_enforce_prediction_order(
-                estimator.classes_[i_label], predictions[i_label],
-                n_classes=len(set(y[:, i_label])), method=method)
-                for i_label in range(len(predictions))]
+            predictions = [
+                _enforce_prediction_order(
+                    estimator.classes_[i_label],
+                    predictions[i_label],
+                    n_classes=len(set(y[:, i_label])),
+                    method=method,
+                )
+                for i_label in range(len(predictions))
+            ]
         else:
             # A 2D y array should be a binary label indicator matrix
             n_classes = len(set(y)) if y.ndim == 1 else y.shape[1]
             predictions = _enforce_prediction_order(
-                estimator.classes_, predictions, n_classes, method)
-    return predictions, test
+                estimator.classes_, predictions, n_classes, method
+            )
+    return predictions


 def _enforce_prediction_order(classes, predictions, n_classes, method):
@@ -888,7 +1083,7 @@
     not present in the subset of data used for training,
     then the output prediction array might not have the same
     columns as other folds. Use the list of class names
-    (assumed to be integers) to enforce the correct column order.
+    (assumed to be ints) to enforce the correct column order.

     Note that `classes` is the list of classes in this fold
     (a subset of the classes in the full training set)
@@ -896,43 +1091,52 @@
     """
     if n_classes != len(classes):
         recommendation = (
-            'To fix this, use a cross-validation '
-            'technique resulting in properly '
-            'stratified folds')
-        warnings.warn('Number of classes in training fold ({}) does '
-                      'not match total number of classes ({}). '
-                      'Results may not be appropriate for your use case. '
-                      '{}'.format(len(classes), n_classes, recommendation),
-                      RuntimeWarning)
-        if method == 'decision_function':
-            if (predictions.ndim == 2 and
-                    predictions.shape[1] != len(classes)):
+            "To fix this, use a cross-validation "
+            "technique resulting in properly "
+            "stratified folds"
+        )
+        warnings.warn(
+            "Number of classes in training fold ({}) does "
+            "not match total number of classes ({}). "
+            "Results may not be appropriate for your use case. "
+            "{}".format(len(classes), n_classes, recommendation),
+            RuntimeWarning,
+        )
+        if method == "decision_function":
+            if predictions.ndim == 2 and predictions.shape[1] != len(classes):
                 # This handles the case when the shape of predictions
                 # does not match the number of classes used to train
                 # it with. This case is found when sklearn.svm.SVC is
                 # set to `decision_function_shape='ovo'`.
-                raise ValueError('Output shape {} of {} does not match '
-                                 'number of classes ({}) in fold. '
-                                 'Irregular decision_function outputs '
-                                 'are not currently supported by '
-                                 'cross_val_predict'.format(
-                                    predictions.shape, method, len(classes)))
+                raise ValueError(
+                    "Output shape {} of {} does not match "
+                    "number of classes ({}) in fold. "
+                    "Irregular decision_function outputs "
+                    "are not currently supported by "
+                    "cross_val_predict".format(predictions.shape, method, len(classes))
+                )
             if len(classes) <= 2:
                 # In this special case, `predictions` contains a 1D array.
-                raise ValueError('Only {} class/es in training fold, but {} '
-                                 'in overall dataset. This '
-                                 'is not supported for decision_function '
-                                 'with imbalanced folds. {}'.format(
-                                    len(classes), n_classes, recommendation))
+                raise ValueError(
+                    "Only {} class/es in training fold, but {} "
+                    "in overall dataset. This "
+                    "is not supported for decision_function "
+                    "with imbalanced folds. {}".format(
+                        len(classes), n_classes, recommendation
+                    )
+                )

         float_min = np.finfo(predictions.dtype).min
-        default_values = {'decision_function': float_min,
-                          'predict_log_proba': float_min,
-                          'predict_proba': 0}
-        predictions_for_all_classes = np.full((_num_samples(predictions),
-                                               n_classes),
-                                              default_values[method],
-                                              dtype=predictions.dtype)
+        default_values = {
+            "decision_function": float_min,
+            "predict_log_proba": float_min,
+            "predict_proba": 0,
+        }
+        predictions_for_all_classes = np.full(
+            (_num_samples(predictions), n_classes),
+            default_values[method],
+            dtype=predictions.dtype,
+        )
         predictions_for_all_classes[:, classes] = predictions
         predictions = predictions_for_all_classes
     return predictions
@@ -944,7 +1148,7 @@
     Parameters
     ----------
     indices : ndarray
-        integer array to test
+        int array to test
     n_samples : int
         number of expected elements

@@ -962,22 +1166,35 @@
     return True


-def _index_param_value(X, v, indices):
-    """Private helper function for parameter value indexing."""
-    if not _is_arraylike(v) or _num_samples(v) != _num_samples(X):
-        # pass through: skip indexing
-        return v
-    if sp.issparse(v):
-        v = v.tocsr()
-    return safe_indexing(v, indices)
-
-
-def permutation_test_score(estimator, X, y, groups=None, cv='warn',
-                           n_permutations=100, n_jobs=None, random_state=0,
-                           verbose=0, scoring=None):
+def permutation_test_score(
+    estimator,
+    X,
+    y,
+    *,
+    groups=None,
+    cv=None,
+    n_permutations=100,
+    n_jobs=None,
+    random_state=0,
+    verbose=0,
+    scoring=None,
+    fit_params=None,
+):
     """Evaluate the significance of a cross-validated score with permutations

-    Read more in the :ref:`User Guide <cross_validation>`.
+    Permutes targets to generate 'randomized data' and compute the empirical
+    p-value against the null hypothesis that features and targets are
+    independent.
+
+    The p-value represents the fraction of randomized data sets where the
+    estimator performed as well or better than in the original data. A small
+    p-value suggests that there is a real dependency between features and
+    targets which has been used by the estimator to give good predictions.
+    A large p-value may be due to lack of real dependency between features
+    and targets or the estimator was not able to use the dependency to
+    give good predictions.
+
+    Read more in the :ref:`User Guide <permutation_test_score>`.

     Parameters
     ----------
@@ -987,11 +1204,11 @@
     X : array-like of shape at least 2D
         The data to fit.

-    y : array-like
+    y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None
         The target variable to try to predict in the case of
         supervised learning.

-    groups : array-like, with shape (n_samples,), optional
+    groups : array-like of shape (n_samples,), default=None
         Labels to constrain permutation within groups, i.e. ``y`` values
         are permuted among samples with the same group identifier.
         When not specified, ``y`` values are permuted among all samples.
@@ -1001,56 +1218,60 @@
         cross-validator uses them for grouping the samples  while splitting
         the dataset into train/test set.

-    scoring : string, callable or None, optional, default: None
-        A single string (see :ref:`scoring_parameter`) or a callable
+    scoring : str or callable, default=None
+        A single str (see :ref:`scoring_parameter`) or a callable
         (see :ref:`scoring`) to evaluate the predictions on the test set.

-        If None the estimator's score method is used.
-
-    cv : int, cross-validation generator or an iterable, optional
+        If `None` the estimator's score method is used.
+
+    cv : int, cross-validation generator or an iterable, default=None
         Determines the cross-validation splitting strategy.
         Possible inputs for cv are:

-        - None, to use the default 3-fold cross validation,
-        - integer, to specify the number of folds in a `(Stratified)KFold`,
+        - `None`, to use the default 5-fold cross validation,
+        - int, to specify the number of folds in a `(Stratified)KFold`,
         - :term:`CV splitter`,
         - An iterable yielding (train, test) splits as arrays of indices.

-        For integer/None inputs, if the estimator is a classifier and ``y`` is
+        For `int`/`None` inputs, if the estimator is a classifier and `y` is
         either binary or multiclass, :class:`StratifiedKFold` is used. In all
-        other cases, :class:`KFold` is used.
+        other cases, :class:`KFold` is used. These splitters are instantiated
+        with `shuffle=False` so the splits will be the same across calls.

         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.

-        .. versionchanged:: 0.20
-            ``cv`` default value if None will change from 3-fold to 5-fold
-            in v0.22.
-
-    n_permutations : integer, optional
+        .. versionchanged:: 0.22
+            `cv` default value if `None` changed from 3-fold to 5-fold.
+
+    n_permutations : int, default=100
         Number of times to permute ``y``.

-    n_jobs : int or None, optional (default=None)
-        The number of CPUs to use to do the computation.
+    n_jobs : int, default=None
+        Number of jobs to run in parallel. Training the estimator and computing
+        the cross-validated score are parallelized over the permutations.
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.

-    random_state : int, RandomState instance or None, optional (default=0)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-    verbose : integer, optional
+    random_state : int, RandomState instance or None, default=0
+        Pass an int for reproducible output for permutation of
+        ``y`` values among samples. See :term:`Glossary <random_state>`.
+
+    verbose : int, default=0
         The verbosity level.
+
+    fit_params : dict, default=None
+        Parameters to pass to the fit method of the estimator.
+
+        .. versionadded:: 0.24

     Returns
     -------
     score : float
         The true score without permuting targets.

-    permutation_scores : array, shape (n_permutations,)
+    permutation_scores : array of shape (n_permutations,)
         The scores obtained for each permutations.

     pvalue : float
@@ -1067,9 +1288,10 @@
     -----
     This function implements Test 1 in:

-        Ojala and Garriga. Permutation Tests for Studying Classifier
-        Performance.  The Journal of Machine Learning Research (2010)
-        vol. 11
+        Ojala and Garriga. `Permutation Tests for Studying Classifier
+        Performance
+        <http://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf>`_. The
+        Journal of Machine Learning Research (2010) vol. 11

     """
     X, y, groups = indexable(X, y, groups)
@@ -1080,24 +1302,36 @@

     # We clone the estimator to make sure that all the folds are
     # independent, and that it is pickle-able.
-    score = _permutation_test_score(clone(estimator), X, y, groups, cv, scorer)
+    score = _permutation_test_score(
+        clone(estimator), X, y, groups, cv, scorer, fit_params=fit_params
+    )
     permutation_scores = Parallel(n_jobs=n_jobs, verbose=verbose)(
         delayed(_permutation_test_score)(
-            clone(estimator), X, _shuffle(y, groups, random_state),
-            groups, cv, scorer)
-        for _ in range(n_permutations))
+            clone(estimator),
+            X,
+            _shuffle(y, groups, random_state),
+            groups,
+            cv,
+            scorer,
+            fit_params=fit_params,
+        )
+        for _ in range(n_permutations)
+    )
     permutation_scores = np.array(permutation_scores)
     pvalue = (np.sum(permutation_scores >= score) + 1.0) / (n_permutations + 1)
     return score, permutation_scores, pvalue


-def _permutation_test_score(estimator, X, y, groups, cv, scorer):
+def _permutation_test_score(estimator, X, y, groups, cv, scorer, fit_params):
     """Auxiliary function for permutation_test_score"""
+    # Adjust length of sample weights
+    fit_params = fit_params if fit_params is not None else {}
     avg_score = []
     for train, test in cv.split(X, y, groups):
         X_train, y_train = _safe_split(estimator, X, y, train)
         X_test, y_test = _safe_split(estimator, X, y, test, train)
-        estimator.fit(X_train, y_train)
+        fit_params = _check_fit_params(X, fit_params, train)
+        estimator.fit(X_train, y_train, **fit_params)
         avg_score.append(scorer(estimator, X_test, y_test))
     return np.mean(avg_score)

@@ -1109,16 +1343,30 @@
     else:
         indices = np.arange(len(groups))
         for group in np.unique(groups):
-            this_mask = (groups == group)
+            this_mask = groups == group
             indices[this_mask] = random_state.permutation(indices[this_mask])
-    return safe_indexing(y, indices)
-
-
-def learning_curve(estimator, X, y, groups=None,
-                   train_sizes=np.linspace(0.1, 1.0, 5), cv='warn',
-                   scoring=None, exploit_incremental_learning=False,
-                   n_jobs=None, pre_dispatch="all", verbose=0, shuffle=False,
-                   random_state=None, error_score='raise-deprecating'):
+    return _safe_indexing(y, indices)
+
+
+def learning_curve(
+    estimator,
+    X,
+    y,
+    *,
+    groups=None,
+    train_sizes=np.linspace(0.1, 1.0, 5),
+    cv=None,
+    scoring=None,
+    exploit_incremental_learning=False,
+    n_jobs=None,
+    pre_dispatch="all",
+    verbose=0,
+    shuffle=False,
+    random_state=None,
+    error_score=np.nan,
+    return_times=False,
+    fit_params=None,
+):
     """Learning curve.

     Determines cross-validated training and test scores for different training
@@ -1137,19 +1385,21 @@
     estimator : object type that implements the "fit" and "predict" methods
         An object of that type which is cloned for each validation.

-    X : array-like, shape (n_samples, n_features)
-        Training vector, where n_samples is the number of samples and
-        n_features is the number of features.
-
-    y : array-like, shape (n_samples) or (n_samples, n_features), optional
+    X : array-like of shape (n_samples, n_features)
+        Training vector, where `n_samples` is the number of samples and
+        `n_features` is the number of features.
+
+    y : array-like of shape (n_samples,) or (n_samples, n_outputs)
         Target relative to X for classification or regression;
         None for unsupervised learning.

-    groups : array-like, with shape (n_samples,), optional
+    groups : array-like of  shape (n_samples,), default=None
         Group labels for the samples used while splitting the dataset into
-        train/test set.
-
-    train_sizes : array-like, shape (n_ticks,), dtype float or int
+        train/test set. Only used in conjunction with a "Group" :term:`cv`
+        instance (e.g., :class:`GroupKFold`).
+
+    train_sizes : array-like of shape (n_ticks,), \
+            default=np.linspace(0.1, 1.0, 5)
         Relative or absolute numbers of training examples that will be used to
         generate the learning curve. If the dtype is float, it is regarded as a
         fraction of the maximum size of the training set (that is determined
@@ -1157,83 +1407,95 @@
         Otherwise it is interpreted as absolute sizes of the training sets.
         Note that for classification the number of samples usually have to
         be big enough to contain at least one sample from each class.
-        (default: np.linspace(0.1, 1.0, 5))
-
-    cv : int, cross-validation generator or an iterable, optional
+
+    cv : int, cross-validation generator or an iterable, default=None
         Determines the cross-validation splitting strategy.
         Possible inputs for cv are:

-        - None, to use the default 3-fold cross validation,
-        - integer, to specify the number of folds in a `(Stratified)KFold`,
+        - None, to use the default 5-fold cross validation,
+        - int, to specify the number of folds in a `(Stratified)KFold`,
         - :term:`CV splitter`,
         - An iterable yielding (train, test) splits as arrays of indices.

-        For integer/None inputs, if the estimator is a classifier and ``y`` is
+        For int/None inputs, if the estimator is a classifier and ``y`` is
         either binary or multiclass, :class:`StratifiedKFold` is used. In all
-        other cases, :class:`KFold` is used.
+        other cases, :class:`KFold` is used. These splitters are instantiated
+        with `shuffle=False` so the splits will be the same across calls.

         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.

-        .. versionchanged:: 0.20
-            ``cv`` default value if None will change from 3-fold to 5-fold
-            in v0.22.
-
-    scoring : string, callable or None, optional, default: None
-        A string (see model evaluation documentation) or
+        .. versionchanged:: 0.22
+            ``cv`` default value if None changed from 3-fold to 5-fold.
+
+    scoring : str or callable, default=None
+        A str (see model evaluation documentation) or
         a scorer callable object / function with signature
         ``scorer(estimator, X, y)``.

-    exploit_incremental_learning : boolean, optional, default: False
+    exploit_incremental_learning : bool, default=False
         If the estimator supports incremental learning, this will be
         used to speed up fitting for different training set sizes.

-    n_jobs : int or None, optional (default=None)
-        Number of jobs to run in parallel.
+    n_jobs : int, default=None
+        Number of jobs to run in parallel. Training the estimator and computing
+        the score are parallelized over the different training and test sets.
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.

-    pre_dispatch : integer or string, optional
+    pre_dispatch : int or str, default='all'
         Number of predispatched jobs for parallel execution (default is
-        all). The option can reduce the allocated memory. The string can
+        all). The option can reduce the allocated memory. The str can
         be an expression like '2*n_jobs'.

-    verbose : integer, optional
+    verbose : int, default=0
         Controls the verbosity: the higher, the more messages.

-    shuffle : boolean, optional
+    shuffle : bool, default=False
         Whether to shuffle training data before taking prefixes of it
         based on``train_sizes``.

-    random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`. Used when ``shuffle`` is True.
-
-    error_score : 'raise' | 'raise-deprecating' or numeric
+    random_state : int, RandomState instance or None, default=None
+        Used when ``shuffle`` is True. Pass an int for reproducible
+        output across multiple function calls.
+        See :term:`Glossary <random_state>`.
+
+    error_score : 'raise' or numeric, default=np.nan
         Value to assign to the score if an error occurs in estimator fitting.
         If set to 'raise', the error is raised.
-        If set to 'raise-deprecating', a FutureWarning is printed before the
-        error is raised.
-        If a numeric value is given, FitFailedWarning is raised. This parameter
-        does not affect the refit step, which will always raise the error.
-        Default is 'raise-deprecating' but from version 0.22 it will change
-        to np.nan.
+        If a numeric value is given, FitFailedWarning is raised.
+
+        .. versionadded:: 0.20
+
+    return_times : bool, default=False
+        Whether to return the fit and score times.
+
+    fit_params : dict, default=None
+        Parameters to pass to the fit method of the estimator.
+
+        .. versionadded:: 0.24

     Returns
     -------
-    train_sizes_abs : array, shape (n_unique_ticks,), dtype int
+    train_sizes_abs : array of shape (n_unique_ticks,)
         Numbers of training examples that has been used to generate the
         learning curve. Note that the number of ticks might be less
         than n_ticks because duplicate entries will be removed.

-    train_scores : array, shape (n_ticks, n_cv_folds)
+    train_scores : array of shape (n_ticks, n_cv_folds)
         Scores on training sets.

-    test_scores : array, shape (n_ticks, n_cv_folds)
+    test_scores : array of shape (n_ticks, n_cv_folds)
         Scores on test set.
+
+    fit_times : array of shape (n_ticks, n_cv_folds)
+        Times spent for fitting in seconds. Only present if ``return_times``
+        is True.
+
+    score_times : array of shape (n_ticks, n_cv_folds)
+        Times spent for scoring in seconds. Only present if ``return_times``
+        is True.

     Notes
     -----
@@ -1241,8 +1503,10 @@
     <sphx_glr_auto_examples_model_selection_plot_learning_curve.py>`
     """
     if exploit_incremental_learning and not hasattr(estimator, "partial_fit"):
-        raise ValueError("An estimator must support the partial_fit interface "
-                         "to exploit incremental learning")
+        raise ValueError(
+            "An estimator must support the partial_fit interface "
+            "to exploit incremental learning"
+        )
     X, y, groups = indexable(X, y, groups)

     cv = check_cv(cv, y, classifier=is_classifier(estimator))
@@ -1255,14 +1519,12 @@
     # Because the lengths of folds can be significantly different, it is
     # not guaranteed that we use all of the available training data when we
     # use the first 'n_max_training_samples' samples.
-    train_sizes_abs = _translate_train_sizes(train_sizes,
-                                             n_max_training_samples)
+    train_sizes_abs = _translate_train_sizes(train_sizes, n_max_training_samples)
     n_unique_ticks = train_sizes_abs.shape[0]
     if verbose > 0:
         print("[learning_curve] Training set sizes: " + str(train_sizes_abs))

-    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,
-                        verbose=verbose)
+    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch, verbose=verbose)

     if shuffle:
         rng = check_random_state(random_state)
@@ -1270,27 +1532,63 @@

     if exploit_incremental_learning:
         classes = np.unique(y) if is_classifier(estimator) else None
-        out = parallel(delayed(_incremental_fit_estimator)(
-            clone(estimator), X, y, classes, train, test, train_sizes_abs,
-            scorer, verbose) for train, test in cv_iter)
+        out = parallel(
+            delayed(_incremental_fit_estimator)(
+                clone(estimator),
+                X,
+                y,
+                classes,
+                train,
+                test,
+                train_sizes_abs,
+                scorer,
+                verbose,
+                return_times,
+                error_score=error_score,
+                fit_params=fit_params,
+            )
+            for train, test in cv_iter
+        )
+        out = np.asarray(out).transpose((2, 1, 0))
     else:
         train_test_proportions = []
         for train, test in cv_iter:
             for n_train_samples in train_sizes_abs:
                 train_test_proportions.append((train[:n_train_samples], test))

-        out = parallel(delayed(_fit_and_score)(
-            clone(estimator), X, y, scorer, train, test, verbose,
-            parameters=None, fit_params=None, return_train_score=True,
-            error_score=error_score)
-            for train, test in train_test_proportions)
-        out = np.array(out)
-        n_cv_folds = out.shape[0] // n_unique_ticks
-        out = out.reshape(n_cv_folds, n_unique_ticks, 2)
-
-    out = np.asarray(out).transpose((2, 1, 0))
-
-    return train_sizes_abs, out[0], out[1]
+        results = parallel(
+            delayed(_fit_and_score)(
+                clone(estimator),
+                X,
+                y,
+                scorer,
+                train,
+                test,
+                verbose,
+                parameters=None,
+                fit_params=fit_params,
+                return_train_score=True,
+                error_score=error_score,
+                return_times=return_times,
+            )
+            for train, test in train_test_proportions
+        )
+        results = _aggregate_score_dicts(results)
+        train_scores = results["train_scores"].reshape(-1, n_unique_ticks).T
+        test_scores = results["test_scores"].reshape(-1, n_unique_ticks).T
+        out = [train_scores, test_scores]
+
+        if return_times:
+            fit_times = results["fit_time"].reshape(-1, n_unique_ticks).T
+            score_times = results["score_time"].reshape(-1, n_unique_ticks).T
+            out.extend([fit_times, score_times])
+
+    ret = train_sizes_abs, out[0], out[1]
+
+    if return_times:
+        ret = ret + (out[2], out[3])
+
+    return ret


 def _translate_train_sizes(train_sizes, n_max_training_samples):
@@ -1302,7 +1600,7 @@

     Parameters
     ----------
-    train_sizes : array-like, shape (n_ticks,), dtype float or int
+    train_sizes : array-like of shape (n_ticks,)
         Numbers of training examples that will be used to generate the
         learning curve. If the dtype is float, it is regarded as a
         fraction of 'n_max_training_samples', i.e. it has to be within (0, 1].
@@ -1312,7 +1610,7 @@

     Returns
     -------
-    train_sizes_abs : array, shape (n_unique_ticks,), dtype int
+    train_sizes_abs : array of shape (n_unique_ticks,)
         Numbers of training examples that will be used to generate the
         learning curve. Note that the number of ticks might be less
         than n_ticks because duplicate entries will be removed.
@@ -1323,59 +1621,114 @@
     n_max_required_samples = np.max(train_sizes_abs)
     if np.issubdtype(train_sizes_abs.dtype, np.floating):
         if n_min_required_samples <= 0.0 or n_max_required_samples > 1.0:
-            raise ValueError("train_sizes has been interpreted as fractions "
-                             "of the maximum number of training samples and "
-                             "must be within (0, 1], but is within [%f, %f]."
-                             % (n_min_required_samples,
-                                n_max_required_samples))
+            raise ValueError(
+                "train_sizes has been interpreted as fractions "
+                "of the maximum number of training samples and "
+                "must be within (0, 1], but is within [%f, %f]."
+                % (n_min_required_samples, n_max_required_samples)
+            )
         train_sizes_abs = (train_sizes_abs * n_max_training_samples).astype(
-                             dtype=np.int, copy=False)
-        train_sizes_abs = np.clip(train_sizes_abs, 1,
-                                  n_max_training_samples)
+            dtype=int, copy=False
+        )
+        train_sizes_abs = np.clip(train_sizes_abs, 1, n_max_training_samples)
     else:
-        if (n_min_required_samples <= 0 or
-                n_max_required_samples > n_max_training_samples):
-            raise ValueError("train_sizes has been interpreted as absolute "
-                             "numbers of training samples and must be within "
-                             "(0, %d], but is within [%d, %d]."
-                             % (n_max_training_samples,
-                                n_min_required_samples,
-                                n_max_required_samples))
+        if (
+            n_min_required_samples <= 0
+            or n_max_required_samples > n_max_training_samples
+        ):
+            raise ValueError(
+                "train_sizes has been interpreted as absolute "
+                "numbers of training samples and must be within "
+                "(0, %d], but is within [%d, %d]."
+                % (
+                    n_max_training_samples,
+                    n_min_required_samples,
+                    n_max_required_samples,
+                )
+            )

     train_sizes_abs = np.unique(train_sizes_abs)
     if n_ticks > train_sizes_abs.shape[0]:
-        warnings.warn("Removed duplicate entries from 'train_sizes'. Number "
-                      "of ticks will be less than the size of "
-                      "'train_sizes' %d instead of %d)."
-                      % (train_sizes_abs.shape[0], n_ticks), RuntimeWarning)
+        warnings.warn(
+            "Removed duplicate entries from 'train_sizes'. Number "
+            "of ticks will be less than the size of "
+            "'train_sizes': %d instead of %d." % (train_sizes_abs.shape[0], n_ticks),
+            RuntimeWarning,
+        )

     return train_sizes_abs


-def _incremental_fit_estimator(estimator, X, y, classes, train, test,
-                               train_sizes, scorer, verbose):
+def _incremental_fit_estimator(
+    estimator,
+    X,
+    y,
+    classes,
+    train,
+    test,
+    train_sizes,
+    scorer,
+    verbose,
+    return_times,
+    error_score,
+    fit_params,
+):
     """Train estimator on training subsets incrementally and compute scores."""
-    train_scores, test_scores = [], []
+    train_scores, test_scores, fit_times, score_times = [], [], [], []
     partitions = zip(train_sizes, np.split(train, train_sizes)[:-1])
+    if fit_params is None:
+        fit_params = {}
+    if classes is None:
+        partial_fit_func = partial(estimator.partial_fit, **fit_params)
+    else:
+        partial_fit_func = partial(estimator.partial_fit, classes=classes, **fit_params)
+
     for n_train_samples, partial_train in partitions:
         train_subset = train[:n_train_samples]
         X_train, y_train = _safe_split(estimator, X, y, train_subset)
-        X_partial_train, y_partial_train = _safe_split(estimator, X, y,
-                                                       partial_train)
+        X_partial_train, y_partial_train = _safe_split(estimator, X, y, partial_train)
         X_test, y_test = _safe_split(estimator, X, y, test, train_subset)
+        start_fit = time.time()
         if y_partial_train is None:
-            estimator.partial_fit(X_partial_train, classes=classes)
+            partial_fit_func(X_partial_train)
         else:
-            estimator.partial_fit(X_partial_train, y_partial_train,
-                                  classes=classes)
-        train_scores.append(_score(estimator, X_train, y_train, scorer))
-        test_scores.append(_score(estimator, X_test, y_test, scorer))
-    return np.array((train_scores, test_scores)).T
-
-
-def validation_curve(estimator, X, y, param_name, param_range, groups=None,
-                     cv='warn', scoring=None, n_jobs=None, pre_dispatch="all",
-                     verbose=0, error_score='raise-deprecating'):
+            partial_fit_func(X_partial_train, y_partial_train)
+        fit_time = time.time() - start_fit
+        fit_times.append(fit_time)
+
+        start_score = time.time()
+
+        test_scores.append(_score(estimator, X_test, y_test, scorer, error_score))
+        train_scores.append(_score(estimator, X_train, y_train, scorer, error_score))
+
+        score_time = time.time() - start_score
+        score_times.append(score_time)
+
+    ret = (
+        (train_scores, test_scores, fit_times, score_times)
+        if return_times
+        else (train_scores, test_scores)
+    )
+
+    return np.array(ret).T
+
+
+def validation_curve(
+    estimator,
+    X,
+    y,
+    *,
+    param_name,
+    param_range,
+    groups=None,
+    cv=None,
+    scoring=None,
+    n_jobs=None,
+    pre_dispatch="all",
+    verbose=0,
+    error_score=np.nan,
+    fit_params=None,
+):
     """Validation curve.

     Determine training and test scores for varying parameter values.
@@ -1385,86 +1738,91 @@
     will also compute training scores and is merely a utility for plotting the
     results.

-    Read more in the :ref:`User Guide <learning_curve>`.
+    Read more in the :ref:`User Guide <validation_curve>`.

     Parameters
     ----------
     estimator : object type that implements the "fit" and "predict" methods
         An object of that type which is cloned for each validation.

-    X : array-like, shape (n_samples, n_features)
-        Training vector, where n_samples is the number of samples and
-        n_features is the number of features.
-
-    y : array-like, shape (n_samples) or (n_samples, n_features), optional
+    X : array-like of shape (n_samples, n_features)
+        Training vector, where `n_samples` is the number of samples and
+        `n_features` is the number of features.
+
+    y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None
         Target relative to X for classification or regression;
         None for unsupervised learning.

-    param_name : string
+    param_name : str
         Name of the parameter that will be varied.

-    param_range : array-like, shape (n_values,)
+    param_range : array-like of shape (n_values,)
         The values of the parameter that will be evaluated.

-    groups : array-like, with shape (n_samples,), optional
+    groups : array-like of shape (n_samples,), default=None
         Group labels for the samples used while splitting the dataset into
-        train/test set.
-
-    cv : int, cross-validation generator or an iterable, optional
+        train/test set. Only used in conjunction with a "Group" :term:`cv`
+        instance (e.g., :class:`GroupKFold`).
+
+    cv : int, cross-validation generator or an iterable, default=None
         Determines the cross-validation splitting strategy.
         Possible inputs for cv are:

-        - None, to use the default 3-fold cross validation,
-        - integer, to specify the number of folds in a `(Stratified)KFold`,
+        - None, to use the default 5-fold cross validation,
+        - int, to specify the number of folds in a `(Stratified)KFold`,
         - :term:`CV splitter`,
         - An iterable yielding (train, test) splits as arrays of indices.

-        For integer/None inputs, if the estimator is a classifier and ``y`` is
+        For int/None inputs, if the estimator is a classifier and ``y`` is
         either binary or multiclass, :class:`StratifiedKFold` is used. In all
-        other cases, :class:`KFold` is used.
+        other cases, :class:`KFold` is used. These splitters are instantiated
+        with `shuffle=False` so the splits will be the same across calls.

         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.

-        .. versionchanged:: 0.20
-            ``cv`` default value if None will change from 3-fold to 5-fold
-            in v0.22.
-
-    scoring : string, callable or None, optional, default: None
-        A string (see model evaluation documentation) or
+        .. versionchanged:: 0.22
+            ``cv`` default value if None changed from 3-fold to 5-fold.
+
+    scoring : str or callable, default=None
+        A str (see model evaluation documentation) or
         a scorer callable object / function with signature
         ``scorer(estimator, X, y)``.

-    n_jobs : int or None, optional (default=None)
-        Number of jobs to run in parallel.
+    n_jobs : int, default=None
+        Number of jobs to run in parallel. Training the estimator and computing
+        the score are parallelized over the combinations of each parameter
+        value and each cross-validation split.
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.

-    pre_dispatch : integer or string, optional
+    pre_dispatch : int or str, default='all'
         Number of predispatched jobs for parallel execution (default is
-        all). The option can reduce the allocated memory. The string can
+        all). The option can reduce the allocated memory. The str can
         be an expression like '2*n_jobs'.

-    verbose : integer, optional
+    verbose : int, default=0
         Controls the verbosity: the higher, the more messages.

-    error_score : 'raise' | 'raise-deprecating' or numeric
+    fit_params : dict, default=None
+        Parameters to pass to the fit method of the estimator.
+
+        .. versionadded:: 0.24
+
+    error_score : 'raise' or numeric, default=np.nan
         Value to assign to the score if an error occurs in estimator fitting.
         If set to 'raise', the error is raised.
-        If set to 'raise-deprecating', a FutureWarning is printed before the
-        error is raised.
-        If a numeric value is given, FitFailedWarning is raised. This parameter
-        does not affect the refit step, which will always raise the error.
-        Default is 'raise-deprecating' but from version 0.22 it will change
-        to np.nan.
+        If a numeric value is given, FitFailedWarning is raised.
+
+        .. versionadded:: 0.20

     Returns
     -------
-    train_scores : array, shape (n_ticks, n_cv_folds)
+    train_scores : array of shape (n_ticks, n_cv_folds)
         Scores on training sets.

-    test_scores : array, shape (n_ticks, n_cv_folds)
+    test_scores : array of shape (n_ticks, n_cv_folds)
         Scores on test set.

     Notes
@@ -1477,26 +1835,38 @@
     cv = check_cv(cv, y, classifier=is_classifier(estimator))
     scorer = check_scoring(estimator, scoring=scoring)

-    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,
-                        verbose=verbose)
-    out = parallel(delayed(_fit_and_score)(
-        clone(estimator), X, y, scorer, train, test, verbose,
-        parameters={param_name: v}, fit_params=None, return_train_score=True,
-        error_score=error_score)
+    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch, verbose=verbose)
+    results = parallel(
+        delayed(_fit_and_score)(
+            clone(estimator),
+            X,
+            y,
+            scorer,
+            train,
+            test,
+            verbose,
+            parameters={param_name: v},
+            fit_params=fit_params,
+            return_train_score=True,
+            error_score=error_score,
+        )
         # NOTE do not change order of iteration to allow one time cv splitters
-        for train, test in cv.split(X, y, groups) for v in param_range)
-    out = np.asarray(out)
+        for train, test in cv.split(X, y, groups)
+        for v in param_range
+    )
     n_params = len(param_range)
-    n_cv_folds = out.shape[0] // n_params
-    out = out.reshape(n_cv_folds, n_params, 2).transpose((2, 1, 0))
-
-    return out[0], out[1]
+
+    results = _aggregate_score_dicts(results)
+    train_scores = results["train_scores"].reshape(-1, n_params).T
+    test_scores = results["test_scores"].reshape(-1, n_params).T
+
+    return train_scores, test_scores


 def _aggregate_score_dicts(scores):
     """Aggregate the list of dict to dict of np ndarray

-    The aggregated output of _fit_and_score will be a list of dict
+    The aggregated output of _aggregate_score_dicts will be a list of dict
     of form [{'prec': 0.1, 'acc':1.0}, {'prec': 0.1, 'acc':1.0}, ...]
     Convert it to a dict of array {'prec': np.array([0.1 ...]), ...}

@@ -1516,5 +1886,9 @@
     {'a': array([1, 2, 3, 10]),
      'b': array([10, 2, 3, 10])}
     """
-    return {key: np.asarray([score[key] for score in scores])
-            for key in scores[0]}
+    return {
+        key: np.asarray([score[key] for score in scores])
+        if isinstance(scores[0][key], numbers.Number)
+        else [score[key] for score in scores]
+        for key in scores[0]
+    }
('sklearn/model_selection', '_split.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,13 +3,16 @@
 functions to split the data based on a preset strategy.
 """

-# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>,
-#         Gael Varoquaux <gael.varoquaux@normalesup.org>,
+# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
+#         Gael Varoquaux <gael.varoquaux@normalesup.org>
 #         Olivier Grisel <olivier.grisel@ensta.org>
 #         Raghav RV <rvraghav93@gmail.com>
+#         Leandro Hermida <hermidal@cs.umd.edu>
+#         Rodion Martynov <marrodion@gmail.com>
 # License: BSD 3 clause

 from collections.abc import Iterable
+from collections import defaultdict
 import warnings
 from itertools import chain, combinations
 from math import ceil, floor
@@ -18,40 +21,34 @@
 from inspect import signature

 import numpy as np
-
-from ..utils import indexable, check_random_state, safe_indexing
+from scipy.special import comb
+
+from ..utils import indexable, check_random_state, _safe_indexing
 from ..utils import _approximate_mode
 from ..utils.validation import _num_samples, column_or_1d
 from ..utils.validation import check_array
 from ..utils.multiclass import type_of_target
-from ..utils.fixes import comb
 from ..base import _pprint

-__all__ = ['BaseCrossValidator',
-           'KFold',
-           'GroupKFold',
-           'LeaveOneGroupOut',
-           'LeaveOneOut',
-           'LeavePGroupsOut',
-           'LeavePOut',
-           'RepeatedStratifiedKFold',
-           'RepeatedKFold',
-           'ShuffleSplit',
-           'GroupShuffleSplit',
-           'StratifiedKFold',
-           'StratifiedShuffleSplit',
-           'PredefinedSplit',
-           'train_test_split',
-           'check_cv']
-
-
-NSPLIT_WARNING = (
-    "The default value of n_split will change from 3 to 5 "
-    "in version 0.22. Specify it explicitly to silence this warning.")
-
-CV_WARNING = (
-    "The default value of cv will change from 3 to 5 "
-    "in version 0.22. Specify it explicitly to silence this warning.")
+__all__ = [
+    "BaseCrossValidator",
+    "KFold",
+    "GroupKFold",
+    "LeaveOneGroupOut",
+    "LeaveOneOut",
+    "LeavePGroupsOut",
+    "LeavePOut",
+    "RepeatedStratifiedKFold",
+    "RepeatedKFold",
+    "ShuffleSplit",
+    "GroupShuffleSplit",
+    "StratifiedKFold",
+    "StratifiedGroupKFold",
+    "StratifiedShuffleSplit",
+    "PredefinedSplit",
+    "train_test_split",
+    "check_cv",
+]


 class BaseCrossValidator(metaclass=ABCMeta):
@@ -65,14 +62,14 @@

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples is the number of samples
-            and n_features is the number of features.
-
-        y : array-like, of length n_samples
+        X : array-like of shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,)
             The target variable for supervised learning problems.

-        groups : array-like, with shape (n_samples,), optional
+        groups : array-like of shape (n_samples,), default=None
             Group labels for the samples used while splitting the dataset into
             train/test set.

@@ -99,7 +96,7 @@
         By default, delegates to _iter_test_indices(X, y, groups)
         """
         for test_index in self._iter_test_indices(X, y, groups):
-            test_mask = np.zeros(_num_samples(X), dtype=np.bool)
+            test_mask = np.zeros(_num_samples(X), dtype=bool)
             test_mask[test_index] = True
             yield test_mask

@@ -130,7 +127,7 @@
     For large datasets one should favor :class:`KFold`, :class:`ShuffleSplit`
     or :class:`StratifiedKFold`.

-    Read more in the :ref:`User Guide <cross_validation>`.
+    Read more in the :ref:`User Guide <leave_one_out>`.

     Examples
     --------
@@ -144,30 +141,27 @@
     >>> print(loo)
     LeaveOneOut()
     >>> for train_index, test_index in loo.split(X):
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
-    ...    print(X_train, X_test, y_train, y_test)
+    ...     print("TRAIN:", train_index, "TEST:", test_index)
+    ...     X_train, X_test = X[train_index], X[test_index]
+    ...     y_train, y_test = y[train_index], y[test_index]
+    ...     print(X_train, X_test, y_train, y_test)
     TRAIN: [1] TEST: [0]
     [[3 4]] [[1 2]] [2] [1]
     TRAIN: [0] TEST: [1]
     [[1 2]] [[3 4]] [1] [2]

-    See also
+    See Also
     --------
-    LeaveOneGroupOut
-        For splitting the data according to explicit, domain-specific
-        stratification of the dataset.
-
-    GroupKFold: K-fold iterator variant with non-overlapping groups.
+    LeaveOneGroupOut : For splitting the data according to explicit,
+        domain-specific stratification of the dataset.
+    GroupKFold : K-fold iterator variant with non-overlapping groups.
     """

     def _iter_test_indices(self, X, y=None, groups=None):
         n_samples = _num_samples(X)
         if n_samples <= 1:
             raise ValueError(
-                'Cannot perform LeaveOneOut with n_samples={}.'.format(
-                    n_samples)
+                "Cannot perform LeaveOneOut with n_samples={}.".format(n_samples)
             )
         return range(n_samples)

@@ -176,9 +170,9 @@

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples is the number of samples
-            and n_features is the number of features.
+        X : array-like of shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.

         y : object
             Always ignored, exists for compatibility.
@@ -211,12 +205,12 @@
     large datasets one should favor :class:`KFold`, :class:`StratifiedKFold`
     or :class:`ShuffleSplit`.

-    Read more in the :ref:`User Guide <cross_validation>`.
+    Read more in the :ref:`User Guide <leave_p_out>`.

     Parameters
     ----------
     p : int
-        Size of the test sets. Must be strictly greater than the number of
+        Size of the test sets. Must be strictly less than the number of
         samples.

     Examples
@@ -231,9 +225,9 @@
     >>> print(lpo)
     LeavePOut(p=2)
     >>> for train_index, test_index in lpo.split(X):
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
+    ...     print("TRAIN:", train_index, "TEST:", test_index)
+    ...     X_train, X_test = X[train_index], X[test_index]
+    ...     y_train, y_test = y[train_index], y[test_index]
     TRAIN: [2 3] TEST: [0 1]
     TRAIN: [1 3] TEST: [0 2]
     TRAIN: [1 2] TEST: [0 3]
@@ -249,8 +243,9 @@
         n_samples = _num_samples(X)
         if n_samples <= self.p:
             raise ValueError(
-                'p={} must be strictly less than the number of '
-                'samples={}'.format(self.p, n_samples)
+                "p={} must be strictly less than the number of samples={}".format(
+                    self.p, n_samples
+                )
             )
         for combination in combinations(range(n_samples), self.p):
             yield np.array(combination)
@@ -260,9 +255,9 @@

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples is the number of samples
-            and n_features is the number of features.
+        X : array-like of shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.

         y : object
             Always ignored, exists for compatibility.
@@ -279,22 +274,30 @@
     """Base class for KFold, GroupKFold, and StratifiedKFold"""

     @abstractmethod
-    def __init__(self, n_splits, shuffle, random_state):
+    def __init__(self, n_splits, *, shuffle, random_state):
         if not isinstance(n_splits, numbers.Integral):
-            raise ValueError('The number of folds must be of Integral type. '
-                             '%s of type %s was passed.'
-                             % (n_splits, type(n_splits)))
+            raise ValueError(
+                "The number of folds must be of Integral type. "
+                "%s of type %s was passed." % (n_splits, type(n_splits))
+            )
         n_splits = int(n_splits)

         if n_splits <= 1:
             raise ValueError(
                 "k-fold cross-validation requires at least one"
                 " train/test split by setting n_splits=2 or more,"
-                " got n_splits={0}.".format(n_splits))
+                " got n_splits={0}.".format(n_splits)
+            )

         if not isinstance(shuffle, bool):
-            raise TypeError("shuffle must be True or False;"
-                            " got {0}".format(shuffle))
+            raise TypeError("shuffle must be True or False; got {0}".format(shuffle))
+
+        if not shuffle and random_state is not None:  # None is the default
+            raise ValueError(
+                "Setting a random_state has no effect since shuffle is "
+                "False. You should leave "
+                "random_state to its default (None), or set shuffle=True.",
+            )

         self.n_splits = n_splits
         self.shuffle = shuffle
@@ -305,14 +308,14 @@

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples is the number of samples
-            and n_features is the number of features.
-
-        y : array-like, shape (n_samples,)
+        X : array-like of shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,), default=None
             The target variable for supervised learning problems.

-        groups : array-like, with shape (n_samples,), optional
+        groups : array-like of shape (n_samples,), default=None
             Group labels for the samples used while splitting the dataset into
             train/test set.

@@ -328,9 +331,11 @@
         n_samples = _num_samples(X)
         if self.n_splits > n_samples:
             raise ValueError(
-                ("Cannot have number of splits n_splits={0} greater"
-                 " than the number of samples: n_samples={1}.")
-                .format(self.n_splits, n_samples))
+                (
+                    "Cannot have number of splits n_splits={0} greater"
+                    " than the number of samples: n_samples={1}."
+                ).format(self.n_splits, n_samples)
+            )

         for train, test in super().split(X, y, groups):
             yield train, test
@@ -366,24 +371,26 @@
     Each fold is then used once as a validation while the k - 1 remaining
     folds form the training set.

-    Read more in the :ref:`User Guide <cross_validation>`.
+    Read more in the :ref:`User Guide <k_fold>`.

     Parameters
     ----------
-    n_splits : int, default=3
+    n_splits : int, default=5
         Number of folds. Must be at least 2.

-        .. versionchanged:: 0.20
-            ``n_splits`` default value will change from 3 to 5 in v0.22.
-
-    shuffle : boolean, optional
+        .. versionchanged:: 0.22
+            ``n_splits`` default value changed from 3 to 5.
+
+    shuffle : bool, default=False
         Whether to shuffle the data before splitting into batches.
-
-    random_state : int, RandomState instance or None, optional, default=None
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`. Used when ``shuffle`` == True.
+        Note that the samples within each split will not be shuffled.
+
+    random_state : int, RandomState instance or None, default=None
+        When `shuffle` is True, `random_state` affects the ordering of the
+        indices, which controls the randomness of each fold. Otherwise, this
+        parameter has no effect.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.

     Examples
     --------
@@ -394,12 +401,12 @@
     >>> kf = KFold(n_splits=2)
     >>> kf.get_n_splits(X)
     2
-    >>> print(kf)  # doctest: +NORMALIZE_WHITESPACE
+    >>> print(kf)
     KFold(n_splits=2, random_state=None, shuffle=False)
     >>> for train_index, test_index in kf.split(X):
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
+    ...     print("TRAIN:", train_index, "TEST:", test_index)
+    ...     X_train, X_test = X[train_index], X[test_index]
+    ...     y_train, y_test = y[train_index], y[test_index]
     TRAIN: [2 3] TEST: [0 1]
     TRAIN: [0 1] TEST: [2 3]

@@ -410,27 +417,22 @@
     ``n_samples // n_splits``, where ``n_samples`` is the number of samples.

     Randomized CV splitters may return different results for each call of
-    split. You can make the results identical by setting ``random_state``
+    split. You can make the results identical by setting `random_state`
     to an integer.

-    See also
+    See Also
     --------
-    StratifiedKFold
-        Takes group information into account to avoid building folds with
-        imbalanced class distributions (for binary or multiclass
+    StratifiedKFold : Takes group information into account to avoid building
+        folds with imbalanced class distributions (for binary or multiclass
         classification tasks).

-    GroupKFold: K-fold iterator variant with non-overlapping groups.
-
-    RepeatedKFold: Repeats K-Fold n times.
+    GroupKFold : K-fold iterator variant with non-overlapping groups.
+
+    RepeatedKFold : Repeats K-Fold n times.
     """

-    def __init__(self, n_splits='warn', shuffle=False,
-                 random_state=None):
-        if n_splits == 'warn':
-            warnings.warn(NSPLIT_WARNING, FutureWarning)
-            n_splits = 3
-        super().__init__(n_splits, shuffle, random_state)
+    def __init__(self, n_splits=5, *, shuffle=False, random_state=None):
+        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)

     def _iter_test_indices(self, X, y=None, groups=None):
         n_samples = _num_samples(X)
@@ -439,8 +441,8 @@
             check_random_state(self.random_state).shuffle(indices)

         n_splits = self.n_splits
-        fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)
-        fold_sizes[:n_samples % n_splits] += 1
+        fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=int)
+        fold_sizes[: n_samples % n_splits] += 1
         current = 0
         for fold_size in fold_sizes:
             start, stop = current, current + fold_size
@@ -457,13 +459,15 @@
     The folds are approximately balanced in the sense that the number of
     distinct groups is approximately the same in each fold.

+    Read more in the :ref:`User Guide <group_k_fold>`.
+
     Parameters
     ----------
-    n_splits : int, default=3
+    n_splits : int, default=5
         Number of folds. Must be at least 2.

-        .. versionchanged:: 0.20
-            ``n_splits`` default value will change from 3 to 5 in v0.22.
+        .. versionchanged:: 0.22
+            ``n_splits`` default value changed from 3 to 5.

     Examples
     --------
@@ -492,30 +496,28 @@
      [7 8]] [[1 2]
      [3 4]] [3 4] [1 2]

-    See also
+    See Also
     --------
-    LeaveOneGroupOut
-        For splitting the data according to explicit domain-specific
-        stratification of the dataset.
+    LeaveOneGroupOut : For splitting the data according to explicit
+        domain-specific stratification of the dataset.
     """
-    def __init__(self, n_splits='warn'):
-        if n_splits == 'warn':
-            warnings.warn(NSPLIT_WARNING, FutureWarning)
-            n_splits = 3
+
+    def __init__(self, n_splits=5):
         super().__init__(n_splits, shuffle=False, random_state=None)

     def _iter_test_indices(self, X, y, groups):
         if groups is None:
             raise ValueError("The 'groups' parameter should not be None.")
-        groups = check_array(groups, ensure_2d=False, dtype=None)
+        groups = check_array(groups, input_name="groups", ensure_2d=False, dtype=None)

         unique_groups, groups = np.unique(groups, return_inverse=True)
         n_groups = len(unique_groups)

         if self.n_splits > n_groups:
-            raise ValueError("Cannot have number of splits n_splits=%d greater"
-                             " than the number of groups: %d."
-                             % (self.n_splits, n_groups))
+            raise ValueError(
+                "Cannot have number of splits n_splits=%d greater"
+                " than the number of groups: %d." % (self.n_splits, n_groups)
+            )

         # Weight groups by their number of occurrences
         n_samples_per_group = np.bincount(groups)
@@ -546,14 +548,14 @@

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples is the number of samples
-            and n_features is the number of features.
-
-        y : array-like, shape (n_samples,), optional
+        X : array-like of shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,), default=None
             The target variable for supervised learning problems.

-        groups : array-like, with shape (n_samples,)
+        groups : array-like of shape (n_samples,)
             Group labels for the samples used while splitting the dataset into
             train/test set.

@@ -569,7 +571,7 @@


 class StratifiedKFold(_BaseKFold):
-    """Stratified K-Folds cross-validator
+    """Stratified K-Folds cross-validator.

     Provides train/test indices to split data in train/test sets.

@@ -577,24 +579,26 @@
     stratified folds. The folds are made by preserving the percentage of
     samples for each class.

-    Read more in the :ref:`User Guide <cross_validation>`.
+    Read more in the :ref:`User Guide <stratified_k_fold>`.

     Parameters
     ----------
-    n_splits : int, default=3
+    n_splits : int, default=5
         Number of folds. Must be at least 2.

-        .. versionchanged:: 0.20
-            ``n_splits`` default value will change from 3 to 5 in v0.22.
-
-    shuffle : boolean, optional
+        .. versionchanged:: 0.22
+            ``n_splits`` default value changed from 3 to 5.
+
+    shuffle : bool, default=False
         Whether to shuffle each class's samples before splitting into batches.
-
-    random_state : int, RandomState instance or None, optional, default=None
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`. Used when ``shuffle`` == True.
+        Note that the samples within each split will not be shuffled.
+
+    random_state : int, RandomState instance or None, default=None
+        When `shuffle` is True, `random_state` affects the ordering of the
+        indices, which controls the randomness of each fold for each class.
+        Otherwise, leave `random_state` as `None`.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.

     Examples
     --------
@@ -605,81 +609,100 @@
     >>> skf = StratifiedKFold(n_splits=2)
     >>> skf.get_n_splits(X, y)
     2
-    >>> print(skf)  # doctest: +NORMALIZE_WHITESPACE
+    >>> print(skf)
     StratifiedKFold(n_splits=2, random_state=None, shuffle=False)
     >>> for train_index, test_index in skf.split(X, y):
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
+    ...     print("TRAIN:", train_index, "TEST:", test_index)
+    ...     X_train, X_test = X[train_index], X[test_index]
+    ...     y_train, y_test = y[train_index], y[test_index]
     TRAIN: [1 3] TEST: [0 2]
     TRAIN: [0 2] TEST: [1 3]

     Notes
     -----
-    Train and test sizes may be different in each fold, with a difference of at
-    most ``n_classes``.
-
-    See also
+    The implementation is designed to:
+
+    * Generate test sets such that all contain the same distribution of
+      classes, or as close as possible.
+    * Be invariant to class label: relabelling ``y = ["Happy", "Sad"]`` to
+      ``y = [1, 0]`` should not change the indices generated.
+    * Preserve order dependencies in the dataset ordering, when
+      ``shuffle=False``: all samples from class k in some test set were
+      contiguous in y, or separated in y by samples from classes other than k.
+    * Generate test sets where the smallest and largest differ by at most one
+      sample.
+
+    .. versionchanged:: 0.22
+        The previous implementation did not follow the last constraint.
+
+    See Also
     --------
-    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.
+    RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.
     """

-    def __init__(self, n_splits='warn', shuffle=False, random_state=None):
-        if n_splits == 'warn':
-            warnings.warn(NSPLIT_WARNING, FutureWarning)
-            n_splits = 3
-        super().__init__(n_splits, shuffle, random_state)
+    def __init__(self, n_splits=5, *, shuffle=False, random_state=None):
+        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)

     def _make_test_folds(self, X, y=None):
         rng = check_random_state(self.random_state)
         y = np.asarray(y)
         type_of_target_y = type_of_target(y)
-        allowed_target_types = ('binary', 'multiclass')
+        allowed_target_types = ("binary", "multiclass")
         if type_of_target_y not in allowed_target_types:
             raise ValueError(
-                'Supported target types are: {}. Got {!r} instead.'.format(
-                    allowed_target_types, type_of_target_y))
+                "Supported target types are: {}. Got {!r} instead.".format(
+                    allowed_target_types, type_of_target_y
+                )
+            )

         y = column_or_1d(y)
-        n_samples = y.shape[0]
-        unique_y, y_inversed = np.unique(y, return_inverse=True)
-        y_counts = np.bincount(y_inversed)
+
+        _, y_idx, y_inv = np.unique(y, return_index=True, return_inverse=True)
+        # y_inv encodes y according to lexicographic order. We invert y_idx to
+        # map the classes so that they are encoded by order of appearance:
+        # 0 represents the first label appearing in y, 1 the second, etc.
+        _, class_perm = np.unique(y_idx, return_inverse=True)
+        y_encoded = class_perm[y_inv]
+
+        n_classes = len(y_idx)
+        y_counts = np.bincount(y_encoded)
         min_groups = np.min(y_counts)
         if np.all(self.n_splits > y_counts):
-            raise ValueError("n_splits=%d cannot be greater than the"
-                             " number of members in each class."
-                             % (self.n_splits))
+            raise ValueError(
+                "n_splits=%d cannot be greater than the"
+                " number of members in each class." % (self.n_splits)
+            )
         if self.n_splits > min_groups:
-            warnings.warn(("The least populated class in y has only %d"
-                           " members, which is too few. The minimum"
-                           " number of members in any class cannot"
-                           " be less than n_splits=%d."
-                           % (min_groups, self.n_splits)), Warning)
-
-        # pre-assign each sample to a test fold index using individual KFold
-        # splitting strategies for each class so as to respect the balance of
-        # classes
-        # NOTE: Passing the data corresponding to ith class say X[y==class_i]
-        # will break when the data is not 100% stratifiable for all classes.
-        # So we pass np.zeroes(max(c, n_splits)) as data to the KFold
-        per_cls_cvs = [
-            KFold(self.n_splits, shuffle=self.shuffle,
-                  random_state=rng).split(np.zeros(max(count, self.n_splits)))
-            for count in y_counts]
-
-        test_folds = np.zeros(n_samples, dtype=np.int)
-        for test_fold_indices, per_cls_splits in enumerate(zip(*per_cls_cvs)):
-            for cls, (_, test_split) in zip(unique_y, per_cls_splits):
-                cls_test_folds = test_folds[y == cls]
-                # the test split can be too big because we used
-                # KFold(...).split(X[:max(c, n_splits)]) when data is not 100%
-                # stratifiable for all the classes
-                # (we use a warning instead of raising an exception)
-                # If this is the case, let's trim it:
-                test_split = test_split[test_split < len(cls_test_folds)]
-                cls_test_folds[test_split] = test_fold_indices
-                test_folds[y == cls] = cls_test_folds
-
+            warnings.warn(
+                "The least populated class in y has only %d"
+                " members, which is less than n_splits=%d."
+                % (min_groups, self.n_splits),
+                UserWarning,
+            )
+
+        # Determine the optimal number of samples from each class in each fold,
+        # using round robin over the sorted y. (This can be done direct from
+        # counts, but that code is unreadable.)
+        y_order = np.sort(y_encoded)
+        allocation = np.asarray(
+            [
+                np.bincount(y_order[i :: self.n_splits], minlength=n_classes)
+                for i in range(self.n_splits)
+            ]
+        )
+
+        # To maintain the data order dependencies as best as possible within
+        # the stratification constraint, we assign samples from each class in
+        # blocks (and then mess that up when shuffle=True).
+        test_folds = np.empty(len(y), dtype="i")
+        for k in range(n_classes):
+            # since the kth column of allocation stores the number of samples
+            # of class k in each test set, this generates blocks of fold
+            # indices corresponding to the allocation for class k.
+            folds_for_class = np.arange(self.n_splits).repeat(allocation[:, k])
+            if self.shuffle:
+                rng.shuffle(folds_for_class)
+            test_folds[y_encoded == k] = folds_for_class
         return test_folds

     def _iter_test_masks(self, X, y=None, groups=None):
@@ -692,15 +715,15 @@

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples is the number of samples
-            and n_features is the number of features.
+        X : array-like of shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.

             Note that providing ``y`` is sufficient to generate the splits and
             hence ``np.zeros(n_samples)`` may be used as a placeholder for
             ``X`` instead of actual training data.

-        y : array-like, shape (n_samples,)
+        y : array-like of shape (n_samples,)
             The target variable for supervised learning problems.
             Stratification is done based on the y labels.

@@ -718,11 +741,204 @@
         Notes
         -----
         Randomized CV splitters may return different results for each call of
-        split. You can make the results identical by setting ``random_state``
+        split. You can make the results identical by setting `random_state`
         to an integer.
         """
-        y = check_array(y, ensure_2d=False, dtype=None)
+        y = check_array(y, input_name="y", ensure_2d=False, dtype=None)
         return super().split(X, y, groups)
+
+
+class StratifiedGroupKFold(_BaseKFold):
+    """Stratified K-Folds iterator variant with non-overlapping groups.
+
+    This cross-validation object is a variation of StratifiedKFold attempts to
+    return stratified folds with non-overlapping groups. The folds are made by
+    preserving the percentage of samples for each class.
+
+    The same group will not appear in two different folds (the number of
+    distinct groups has to be at least equal to the number of folds).
+
+    The difference between GroupKFold and StratifiedGroupKFold is that
+    the former attempts to create balanced folds such that the number of
+    distinct groups is approximately the same in each fold, whereas
+    StratifiedGroupKFold attempts to create folds which preserve the
+    percentage of samples for each class as much as possible given the
+    constraint of non-overlapping groups between splits.
+
+    Read more in the :ref:`User Guide <cross_validation>`.
+
+    Parameters
+    ----------
+    n_splits : int, default=5
+        Number of folds. Must be at least 2.
+
+    shuffle : bool, default=False
+        Whether to shuffle each class's samples before splitting into batches.
+        Note that the samples within each split will not be shuffled.
+        This implementation can only shuffle groups that have approximately the
+        same y distribution, no global shuffle will be performed.
+
+    random_state : int or RandomState instance, default=None
+        When `shuffle` is True, `random_state` affects the ordering of the
+        indices, which controls the randomness of each fold for each class.
+        Otherwise, leave `random_state` as `None`.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.model_selection import StratifiedGroupKFold
+    >>> X = np.ones((17, 2))
+    >>> y = np.array([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])
+    >>> groups = np.array([1, 1, 2, 2, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 7, 8, 8])
+    >>> cv = StratifiedGroupKFold(n_splits=3)
+    >>> for train_idxs, test_idxs in cv.split(X, y, groups):
+    ...     print("TRAIN:", groups[train_idxs])
+    ...     print("      ", y[train_idxs])
+    ...     print(" TEST:", groups[test_idxs])
+    ...     print("      ", y[test_idxs])
+    TRAIN: [1 1 2 2 4 5 5 5 5 8 8]
+           [0 0 1 1 1 0 0 0 0 0 0]
+     TEST: [3 3 3 6 6 7]
+           [1 1 1 0 0 0]
+    TRAIN: [3 3 3 4 5 5 5 5 6 6 7]
+           [1 1 1 1 0 0 0 0 0 0 0]
+     TEST: [1 1 2 2 8 8]
+           [0 0 1 1 0 0]
+    TRAIN: [1 1 2 2 3 3 3 6 6 7 8 8]
+           [0 0 1 1 1 1 1 0 0 0 0 0]
+     TEST: [4 5 5 5 5]
+           [1 0 0 0 0]
+
+    Notes
+    -----
+    The implementation is designed to:
+
+    * Mimic the behavior of StratifiedKFold as much as possible for trivial
+      groups (e.g. when each group contains only one sample).
+    * Be invariant to class label: relabelling ``y = ["Happy", "Sad"]`` to
+      ``y = [1, 0]`` should not change the indices generated.
+    * Stratify based on samples as much as possible while keeping
+      non-overlapping groups constraint. That means that in some cases when
+      there is a small number of groups containing a large number of samples
+      the stratification will not be possible and the behavior will be close
+      to GroupKFold.
+
+    See also
+    --------
+    StratifiedKFold: Takes class information into account to build folds which
+        retain class distributions (for binary or multiclass classification
+        tasks).
+
+    GroupKFold: K-fold iterator variant with non-overlapping groups.
+    """
+
+    def __init__(self, n_splits=5, shuffle=False, random_state=None):
+        super().__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)
+
+    def _iter_test_indices(self, X, y, groups):
+        # Implementation is based on this kaggle kernel:
+        # https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation
+        # and is a subject to Apache 2.0 License. You may obtain a copy of the
+        # License at http://www.apache.org/licenses/LICENSE-2.0
+        # Changelist:
+        # - Refactored function to a class following scikit-learn KFold
+        #   interface.
+        # - Added heuristic for assigning group to the least populated fold in
+        #   cases when all other criteria are equal
+        # - Swtch from using python ``Counter`` to ``np.unique`` to get class
+        #   distribution
+        # - Added scikit-learn checks for input: checking that target is binary
+        #   or multiclass, checking passed random state, checking that number
+        #   of splits is less than number of members in each class, checking
+        #   that least populated class has more members than there are splits.
+        rng = check_random_state(self.random_state)
+        y = np.asarray(y)
+        type_of_target_y = type_of_target(y)
+        allowed_target_types = ("binary", "multiclass")
+        if type_of_target_y not in allowed_target_types:
+            raise ValueError(
+                "Supported target types are: {}. Got {!r} instead.".format(
+                    allowed_target_types, type_of_target_y
+                )
+            )
+
+        y = column_or_1d(y)
+        _, y_inv, y_cnt = np.unique(y, return_inverse=True, return_counts=True)
+        if np.all(self.n_splits > y_cnt):
+            raise ValueError(
+                "n_splits=%d cannot be greater than the"
+                " number of members in each class." % (self.n_splits)
+            )
+        n_smallest_class = np.min(y_cnt)
+        if self.n_splits > n_smallest_class:
+            warnings.warn(
+                "The least populated class in y has only %d"
+                " members, which is less than n_splits=%d."
+                % (n_smallest_class, self.n_splits),
+                UserWarning,
+            )
+        n_classes = len(y_cnt)
+
+        _, groups_inv, groups_cnt = np.unique(
+            groups, return_inverse=True, return_counts=True
+        )
+        y_counts_per_group = np.zeros((len(groups_cnt), n_classes))
+        for class_idx, group_idx in zip(y_inv, groups_inv):
+            y_counts_per_group[group_idx, class_idx] += 1
+
+        y_counts_per_fold = np.zeros((self.n_splits, n_classes))
+        groups_per_fold = defaultdict(set)
+
+        if self.shuffle:
+            rng.shuffle(y_counts_per_group)
+
+        # Stable sort to keep shuffled order for groups with the same
+        # class distribution variance
+        sorted_groups_idx = np.argsort(
+            -np.std(y_counts_per_group, axis=1), kind="mergesort"
+        )
+
+        for group_idx in sorted_groups_idx:
+            group_y_counts = y_counts_per_group[group_idx]
+            best_fold = self._find_best_fold(
+                y_counts_per_fold=y_counts_per_fold,
+                y_cnt=y_cnt,
+                group_y_counts=group_y_counts,
+            )
+            y_counts_per_fold[best_fold] += group_y_counts
+            groups_per_fold[best_fold].add(group_idx)
+
+        for i in range(self.n_splits):
+            test_indices = [
+                idx
+                for idx, group_idx in enumerate(groups_inv)
+                if group_idx in groups_per_fold[i]
+            ]
+            yield test_indices
+
+    def _find_best_fold(self, y_counts_per_fold, y_cnt, group_y_counts):
+        best_fold = None
+        min_eval = np.inf
+        min_samples_in_fold = np.inf
+        for i in range(self.n_splits):
+            y_counts_per_fold[i] += group_y_counts
+            # Summarise the distribution over classes in each proposed fold
+            std_per_class = np.std(y_counts_per_fold / y_cnt.reshape(1, -1), axis=0)
+            y_counts_per_fold[i] -= group_y_counts
+            fold_eval = np.mean(std_per_class)
+            samples_in_fold = np.sum(y_counts_per_fold[i])
+            is_current_fold_better = (
+                fold_eval < min_eval
+                or np.isclose(fold_eval, min_eval)
+                and samples_in_fold < min_samples_in_fold
+            )
+            if is_current_fold_better:
+                min_eval = fold_eval
+                min_samples_in_fold = samples_in_fold
+                best_fold = i
+        return best_fold


 class TimeSeriesSplit(_BaseKFold):
@@ -740,18 +956,33 @@
     Note that unlike standard cross-validation methods, successive
     training sets are supersets of those that come before them.

-    Read more in the :ref:`User Guide <cross_validation>`.
+    Read more in the :ref:`User Guide <time_series_split>`.
+
+    .. versionadded:: 0.18

     Parameters
     ----------
-    n_splits : int, default=3
+    n_splits : int, default=5
         Number of splits. Must be at least 2.

-        .. versionchanged:: 0.20
-            ``n_splits`` default value will change from 3 to 5 in v0.22.
-
-    max_train_size : int, optional
+        .. versionchanged:: 0.22
+            ``n_splits`` default value changed from 3 to 5.
+
+    max_train_size : int, default=None
         Maximum size for a single training set.
+
+    test_size : int, default=None
+        Used to limit the size of the test set. Defaults to
+        ``n_samples // (n_splits + 1)``, which is the maximum allowed value
+        with ``gap=0``.
+
+        .. versionadded:: 0.24
+
+    gap : int, default=0
+        Number of samples to exclude from the end of each train set before
+        the test set.
+
+        .. versionadded:: 0.24

     Examples
     --------
@@ -759,46 +990,66 @@
     >>> from sklearn.model_selection import TimeSeriesSplit
     >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
     >>> y = np.array([1, 2, 3, 4, 5, 6])
-    >>> tscv = TimeSeriesSplit(n_splits=5)
-    >>> print(tscv)  # doctest: +NORMALIZE_WHITESPACE
-    TimeSeriesSplit(max_train_size=None, n_splits=5)
+    >>> tscv = TimeSeriesSplit()
+    >>> print(tscv)
+    TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)
     >>> for train_index, test_index in tscv.split(X):
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
+    ...     print("TRAIN:", train_index, "TEST:", test_index)
+    ...     X_train, X_test = X[train_index], X[test_index]
+    ...     y_train, y_test = y[train_index], y[test_index]
     TRAIN: [0] TEST: [1]
     TRAIN: [0 1] TEST: [2]
     TRAIN: [0 1 2] TEST: [3]
     TRAIN: [0 1 2 3] TEST: [4]
     TRAIN: [0 1 2 3 4] TEST: [5]
+    >>> # Fix test_size to 2 with 12 samples
+    >>> X = np.random.randn(12, 2)
+    >>> y = np.random.randint(0, 2, 12)
+    >>> tscv = TimeSeriesSplit(n_splits=3, test_size=2)
+    >>> for train_index, test_index in tscv.split(X):
+    ...    print("TRAIN:", train_index, "TEST:", test_index)
+    ...    X_train, X_test = X[train_index], X[test_index]
+    ...    y_train, y_test = y[train_index], y[test_index]
+    TRAIN: [0 1 2 3 4 5] TEST: [6 7]
+    TRAIN: [0 1 2 3 4 5 6 7] TEST: [8 9]
+    TRAIN: [0 1 2 3 4 5 6 7 8 9] TEST: [10 11]
+    >>> # Add in a 2 period gap
+    >>> tscv = TimeSeriesSplit(n_splits=3, test_size=2, gap=2)
+    >>> for train_index, test_index in tscv.split(X):
+    ...    print("TRAIN:", train_index, "TEST:", test_index)
+    ...    X_train, X_test = X[train_index], X[test_index]
+    ...    y_train, y_test = y[train_index], y[test_index]
+    TRAIN: [0 1 2 3] TEST: [6 7]
+    TRAIN: [0 1 2 3 4 5] TEST: [8 9]
+    TRAIN: [0 1 2 3 4 5 6 7] TEST: [10 11]

     Notes
     -----
     The training set has size ``i * n_samples // (n_splits + 1)
-    + n_samples % (n_splits + 1)`` in the ``i``th split,
-    with a test set of size ``n_samples//(n_splits + 1)``,
+    + n_samples % (n_splits + 1)`` in the ``i`` th split,
+    with a test set of size ``n_samples//(n_splits + 1)`` by default,
     where ``n_samples`` is the number of samples.
     """
-    def __init__(self, n_splits='warn', max_train_size=None):
-        if n_splits == 'warn':
-            warnings.warn(NSPLIT_WARNING, FutureWarning)
-            n_splits = 3
+
+    def __init__(self, n_splits=5, *, max_train_size=None, test_size=None, gap=0):
         super().__init__(n_splits, shuffle=False, random_state=None)
         self.max_train_size = max_train_size
+        self.test_size = test_size
+        self.gap = gap

     def split(self, X, y=None, groups=None):
         """Generate indices to split data into training and test set.

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples is the number of samples
-            and n_features is the number of features.
-
-        y : array-like, shape (n_samples,)
-            Always ignored, exists for compatibility.
-
-        groups : array-like, with shape (n_samples,)
+        X : array-like of shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,)
+            Always ignored, exists for compatibility.
+
+        groups : array-like of shape (n_samples,)
             Always ignored, exists for compatibility.

         Yields
@@ -813,22 +1064,38 @@
         n_samples = _num_samples(X)
         n_splits = self.n_splits
         n_folds = n_splits + 1
+        gap = self.gap
+        test_size = (
+            self.test_size if self.test_size is not None else n_samples // n_folds
+        )
+
+        # Make sure we have enough samples for the given split parameters
         if n_folds > n_samples:
             raise ValueError(
-                ("Cannot have number of folds ={0} greater"
-                 " than the number of samples: {1}.").format(n_folds,
-                                                             n_samples))
+                f"Cannot have number of folds={n_folds} greater"
+                f" than the number of samples={n_samples}."
+            )
+        if n_samples - gap - (test_size * n_splits) <= 0:
+            raise ValueError(
+                f"Too many splits={n_splits} for number of samples"
+                f"={n_samples} with test_size={test_size} and gap={gap}."
+            )
+
         indices = np.arange(n_samples)
-        test_size = (n_samples // n_folds)
-        test_starts = range(test_size + n_samples % n_folds,
-                            n_samples, test_size)
+        test_starts = range(n_samples - n_splits * test_size, n_samples, test_size)
+
         for test_start in test_starts:
-            if self.max_train_size and self.max_train_size < test_start:
-                yield (indices[test_start - self.max_train_size:test_start],
-                       indices[test_start:test_start + test_size])
+            train_end = test_start - gap
+            if self.max_train_size and self.max_train_size < train_end:
+                yield (
+                    indices[train_end - self.max_train_size : train_end],
+                    indices[test_start : test_start + test_size],
+                )
             else:
-                yield (indices[:test_start],
-                       indices[test_start:test_start + test_size])
+                yield (
+                    indices[:train_end],
+                    indices[test_start : test_start + test_size],
+                )


 class LeaveOneGroupOut(BaseCrossValidator):
@@ -841,7 +1108,7 @@
     For instance the groups could be the year of collection of the samples
     and thus allow for cross-validation against time-based splits.

-    Read more in the :ref:`User Guide <cross_validation>`.
+    Read more in the :ref:`User Guide <leave_one_group_out>`.

     Examples
     --------
@@ -858,10 +1125,10 @@
     >>> print(logo)
     LeaveOneGroupOut()
     >>> for train_index, test_index in logo.split(X, y, groups):
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
-    ...    print(X_train, X_test, y_train, y_test)
+    ...     print("TRAIN:", train_index, "TEST:", test_index)
+    ...     X_train, X_test = X[train_index], X[test_index]
+    ...     y_train, y_test = y[train_index], y[test_index]
+    ...     print(X_train, X_test, y_train, y_test)
     TRAIN: [2 3] TEST: [0 1]
     [[5 6]
      [7 8]] [[1 2]
@@ -877,12 +1144,15 @@
         if groups is None:
             raise ValueError("The 'groups' parameter should not be None.")
         # We make a copy of groups to avoid side-effects during iteration
-        groups = check_array(groups, copy=True, ensure_2d=False, dtype=None)
+        groups = check_array(
+            groups, input_name="groups", copy=True, ensure_2d=False, dtype=None
+        )
         unique_groups = np.unique(groups)
         if len(unique_groups) <= 1:
             raise ValueError(
                 "The groups parameter contains fewer than 2 unique groups "
-                "(%s). LeaveOneGroupOut expects at least 2." % unique_groups)
+                "(%s). LeaveOneGroupOut expects at least 2." % unique_groups
+            )
         for i in unique_groups:
             yield groups == i

@@ -897,7 +1167,7 @@
         y : object
             Always ignored, exists for compatibility.

-        groups : array-like, with shape (n_samples,)
+        groups : array-like of shape (n_samples,)
             Group labels for the samples used while splitting the dataset into
             train/test set. This 'groups' parameter must always be specified to
             calculate the number of splits, though the other parameters can be
@@ -910,7 +1180,7 @@
         """
         if groups is None:
             raise ValueError("The 'groups' parameter should not be None.")
-        groups = check_array(groups, ensure_2d=False, dtype=None)
+        groups = check_array(groups, input_name="groups", ensure_2d=False, dtype=None)
         return len(np.unique(groups))

     def split(self, X, y=None, groups=None):
@@ -918,14 +1188,14 @@

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples is the number of samples
-            and n_features is the number of features.
-
-        y : array-like, of length n_samples, optional
+        X : array-like of shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,), default=None
             The target variable for supervised learning problems.

-        groups : array-like, with shape (n_samples,)
+        groups : array-like of shape (n_samples,)
             Group labels for the samples used while splitting the dataset into
             train/test set.

@@ -955,7 +1225,7 @@
     ``p`` different values of the groups while the latter uses samples
     all assigned the same groups.

-    Read more in the :ref:`User Guide <cross_validation>`.
+    Read more in the :ref:`User Guide <leave_p_groups_out>`.

     Parameters
     ----------
@@ -977,10 +1247,10 @@
     >>> print(lpgo)
     LeavePGroupsOut(n_groups=2)
     >>> for train_index, test_index in lpgo.split(X, y, groups):
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
-    ...    print(X_train, X_test, y_train, y_test)
+    ...     print("TRAIN:", train_index, "TEST:", test_index)
+    ...     X_train, X_test = X[train_index], X[test_index]
+    ...     y_train, y_test = y[train_index], y[test_index]
+    ...     print(X_train, X_test, y_train, y_test)
     TRAIN: [2] TEST: [0 1]
     [[5 6]] [[1 2]
      [3 4]] [1] [1 2]
@@ -991,9 +1261,9 @@
     [[1 2]] [[3 4]
      [5 6]] [1] [2 1]

-    See also
+    See Also
     --------
-    GroupKFold: K-fold iterator variant with non-overlapping groups.
+    GroupKFold : K-fold iterator variant with non-overlapping groups.
     """

     def __init__(self, n_groups):
@@ -1002,17 +1272,20 @@
     def _iter_test_masks(self, X, y, groups):
         if groups is None:
             raise ValueError("The 'groups' parameter should not be None.")
-        groups = check_array(groups, copy=True, ensure_2d=False, dtype=None)
+        groups = check_array(
+            groups, input_name="groups", copy=True, ensure_2d=False, dtype=None
+        )
         unique_groups = np.unique(groups)
         if self.n_groups >= len(unique_groups):
             raise ValueError(
                 "The groups parameter contains fewer than (or equal to) "
                 "n_groups (%d) numbers of unique groups (%s). LeavePGroupsOut "
                 "expects that at least n_groups + 1 (%d) unique groups be "
-                "present" % (self.n_groups, unique_groups, self.n_groups + 1))
+                "present" % (self.n_groups, unique_groups, self.n_groups + 1)
+            )
         combi = combinations(range(len(unique_groups)), self.n_groups)
         for indices in combi:
-            test_index = np.zeros(_num_samples(X), dtype=np.bool)
+            test_index = np.zeros(_num_samples(X), dtype=bool)
             for l in unique_groups[np.array(indices)]:
                 test_index[groups == l] = True
             yield test_index
@@ -1028,7 +1301,7 @@
         y : object
             Always ignored, exists for compatibility.

-        groups : array-like, with shape (n_samples,)
+        groups : array-like of shape (n_samples,)
             Group labels for the samples used while splitting the dataset into
             train/test set. This 'groups' parameter must always be specified to
             calculate the number of splits, though the other parameters can be
@@ -1041,7 +1314,7 @@
         """
         if groups is None:
             raise ValueError("The 'groups' parameter should not be None.")
-        groups = check_array(groups, ensure_2d=False, dtype=None)
+        groups = check_array(groups, input_name="groups", ensure_2d=False, dtype=None)
         return int(comb(len(np.unique(groups)), self.n_groups, exact=True))

     def split(self, X, y=None, groups=None):
@@ -1049,14 +1322,14 @@

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples is the number of samples
-            and n_features is the number of features.
-
-        y : array-like, of length n_samples, optional
+        X : array-like of shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,), default=None
             The target variable for supervised learning problems.

-        groups : array-like, with shape (n_samples,)
+        groups : array-like of shape (n_samples,)
             Group labels for the samples used while splitting the dataset into
             train/test set.

@@ -1085,26 +1358,25 @@
     n_repeats : int, default=10
         Number of times cross-validator needs to be repeated.

-    random_state : int, RandomState instance or None, optional, default=None
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None, default=None
+        Passes `random_state` to the arbitrary repeating cross validator.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.

     **cvargs : additional params
         Constructor parameters for cv. Must not contain random_state
         and shuffle.
     """
-    def __init__(self, cv, n_repeats=10, random_state=None, **cvargs):
-        if not isinstance(n_repeats, (np.integer, numbers.Integral)):
+
+    def __init__(self, cv, *, n_repeats=10, random_state=None, **cvargs):
+        if not isinstance(n_repeats, numbers.Integral):
             raise ValueError("Number of repetitions must be of Integral type.")

         if n_repeats <= 0:
             raise ValueError("Number of repetitions must be greater than 0.")

-        if any(key in cvargs for key in ('random_state', 'shuffle')):
-            raise ValueError(
-                "cvargs must not contain random_state or shuffle.")
+        if any(key in cvargs for key in ("random_state", "shuffle")):
+            raise ValueError("cvargs must not contain random_state or shuffle.")

         self.cv = cv
         self.n_repeats = n_repeats
@@ -1116,14 +1388,14 @@

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples is the number of samples
-            and n_features is the number of features.
-
-        y : array-like, of length n_samples
+        X : array-like of shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,)
             The target variable for supervised learning problems.

-        groups : array-like, with shape (n_samples,), optional
+        groups : array-like of shape (n_samples,), default=None
             Group labels for the samples used while splitting the dataset into
             train/test set.

@@ -1139,8 +1411,7 @@
         rng = check_random_state(self.random_state)

         for idx in range(n_repeats):
-            cv = self.cv(random_state=rng, shuffle=True,
-                         **self.cvargs)
+            cv = self.cv(random_state=rng, shuffle=True, **self.cvargs)
             for train_index, test_index in cv.split(X, y, groups):
                 yield train_index, test_index

@@ -1157,7 +1428,7 @@
             Always ignored, exists for compatibility.
             ``np.zeros(n_samples)`` may be used as a placeholder.

-        groups : array-like, with shape (n_samples,), optional
+        groups : array-like of shape (n_samples,), default=None
             Group labels for the samples used while splitting the dataset into
             train/test set.

@@ -1167,9 +1438,11 @@
             Returns the number of splitting iterations in the cross-validator.
         """
         rng = check_random_state(self.random_state)
-        cv = self.cv(random_state=rng, shuffle=True,
-                     **self.cvargs)
+        cv = self.cv(random_state=rng, shuffle=True, **self.cvargs)
         return cv.get_n_splits(X, y, groups) * self.n_repeats
+
+    def __repr__(self):
+        return _build_repr(self)


 class RepeatedKFold(_RepeatedSplits):
@@ -1177,7 +1450,7 @@

     Repeats K-Fold n times with different randomization in each repetition.

-    Read more in the :ref:`User Guide <cross_validation>`.
+    Read more in the :ref:`User Guide <repeated_k_fold>`.

     Parameters
     ----------
@@ -1187,11 +1460,10 @@
     n_repeats : int, default=10
         Number of times cross-validator needs to be repeated.

-    random_state : int, RandomState instance or None, optional, default=None
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None, default=None
+        Controls the randomness of each repeated cross-validation instance.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.

     Examples
     --------
@@ -1213,16 +1485,18 @@
     Notes
     -----
     Randomized CV splitters may return different results for each call of
-    split. You can make the results identical by setting ``random_state``
+    split. You can make the results identical by setting `random_state`
     to an integer.

-    See also
+    See Also
     --------
-    RepeatedStratifiedKFold: Repeats Stratified K-Fold n times.
+    RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.
     """
-    def __init__(self, n_splits=5, n_repeats=10, random_state=None):
+
+    def __init__(self, *, n_splits=5, n_repeats=10, random_state=None):
         super().__init__(
-            KFold, n_repeats, random_state, n_splits=n_splits)
+            KFold, n_repeats=n_repeats, random_state=random_state, n_splits=n_splits
+        )


 class RepeatedStratifiedKFold(_RepeatedSplits):
@@ -1231,7 +1505,7 @@
     Repeats Stratified K-Fold n times with different randomization in each
     repetition.

-    Read more in the :ref:`User Guide <cross_validation>`.
+    Read more in the :ref:`User Guide <repeated_k_fold>`.

     Parameters
     ----------
@@ -1241,9 +1515,10 @@
     n_repeats : int, default=10
         Number of times cross-validator needs to be repeated.

-    random_state : None, int or RandomState, default=None
-        Random state to be used to generate random state for each
-        repetition.
+    random_state : int, RandomState instance or None, default=None
+        Controls the generation of the random states for each repetition.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.

     Examples
     --------
@@ -1266,23 +1541,29 @@
     Notes
     -----
     Randomized CV splitters may return different results for each call of
-    split. You can make the results identical by setting ``random_state``
+    split. You can make the results identical by setting `random_state`
     to an integer.

-    See also
+    See Also
     --------
-    RepeatedKFold: Repeats K-Fold n times.
+    RepeatedKFold : Repeats K-Fold n times.
     """
-    def __init__(self, n_splits=5, n_repeats=10, random_state=None):
+
+    def __init__(self, *, n_splits=5, n_repeats=10, random_state=None):
         super().__init__(
-            StratifiedKFold, n_repeats, random_state, n_splits=n_splits)
+            StratifiedKFold,
+            n_repeats=n_repeats,
+            random_state=random_state,
+            n_splits=n_splits,
+        )


 class BaseShuffleSplit(metaclass=ABCMeta):
     """Base class for ShuffleSplit and StratifiedShuffleSplit"""

-    def __init__(self, n_splits=10, test_size=None, train_size=None,
-                 random_state=None):
+    def __init__(
+        self, n_splits=10, *, test_size=None, train_size=None, random_state=None
+    ):
         self.n_splits = n_splits
         self.test_size = test_size
         self.train_size = train_size
@@ -1294,14 +1575,14 @@

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples is the number of samples
-            and n_features is the number of features.
-
-        y : array-like, shape (n_samples,)
+        X : array-like of shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,)
             The target variable for supervised learning problems.

-        groups : array-like, with shape (n_samples,), optional
+        groups : array-like of shape (n_samples,), default=None
             Group labels for the samples used while splitting the dataset into
             train/test set.

@@ -1316,7 +1597,7 @@
         Notes
         -----
         Randomized CV splitters may return different results for each call of
-        split. You can make the results identical by setting ``random_state``
+        split. You can make the results identical by setting `random_state`
         to an integer.
         """
         X, y, groups = indexable(X, y, groups)
@@ -1361,31 +1642,30 @@
     do not guarantee that all folds will be different, although this is
     still very likely for sizeable datasets.

-    Read more in the :ref:`User Guide <cross_validation>`.
+    Read more in the :ref:`User Guide <ShuffleSplit>`.

     Parameters
     ----------
-    n_splits : int, default 10
+    n_splits : int, default=10
         Number of re-shuffling & splitting iterations.

-    test_size : float, int, None, default=None
+    test_size : float or int, default=None
         If float, should be between 0.0 and 1.0 and represent the proportion
         of the dataset to include in the test split. If int, represents the
         absolute number of test samples. If None, the value is set to the
         complement of the train size. If ``train_size`` is also None, it will
         be set to 0.1.

-    train_size : float, int, or None, default=None
+    train_size : float or int, default=None
         If float, should be between 0.0 and 1.0 and represent the
         proportion of the dataset to include in the train split. If
         int, represents the absolute number of train samples. If None,
         the value is automatically set to the complement of the test size.

-    random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None, default=None
+        Controls the randomness of the training and testing indices produced.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.

     Examples
     --------
@@ -1399,8 +1679,7 @@
     >>> print(rs)
     ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)
     >>> for train_index, test_index in rs.split(X):
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...  # doctest: +ELLIPSIS
+    ...     print("TRAIN:", train_index, "TEST:", test_index)
     TRAIN: [1 3 0 4] TEST: [5 2]
     TRAIN: [4 0 2 5] TEST: [1 3]
     TRAIN: [1 2 4 0] TEST: [3 5]
@@ -1409,40 +1688,45 @@
     >>> rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25,
     ...                   random_state=0)
     >>> for train_index, test_index in rs.split(X):
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...  # doctest: +ELLIPSIS
+    ...     print("TRAIN:", train_index, "TEST:", test_index)
     TRAIN: [1 3 0] TEST: [5 2]
     TRAIN: [4 0 2] TEST: [1 3]
     TRAIN: [1 2 4] TEST: [3 5]
     TRAIN: [3 4 1] TEST: [5 2]
     TRAIN: [3 5 1] TEST: [2 4]
     """
-    def __init__(self, n_splits=10, test_size=None, train_size=None,
-                 random_state=None):
+
+    def __init__(
+        self, n_splits=10, *, test_size=None, train_size=None, random_state=None
+    ):
         super().__init__(
             n_splits=n_splits,
             test_size=test_size,
             train_size=train_size,
-            random_state=random_state)
+            random_state=random_state,
+        )
         self._default_test_size = 0.1

     def _iter_indices(self, X, y=None, groups=None):
         n_samples = _num_samples(X)
         n_train, n_test = _validate_shuffle_split(
-            n_samples, self.test_size, self.train_size,
-            default_test_size=self._default_test_size)
+            n_samples,
+            self.test_size,
+            self.train_size,
+            default_test_size=self._default_test_size,
+        )

         rng = check_random_state(self.random_state)
         for i in range(self.n_splits):
             # random partition
             permutation = rng.permutation(n_samples)
             ind_test = permutation[:n_test]
-            ind_train = permutation[n_test:(n_test + n_train)]
+            ind_train = permutation[n_test : (n_test + n_train)]
             yield ind_train, ind_test


 class GroupShuffleSplit(ShuffleSplit):
-    '''Shuffle-Group(s)-Out cross-validation iterator
+    """Shuffle-Group(s)-Out cross-validation iterator

     Provides randomized train/test indices to split data according to a
     third-party provided group. This group information can be used to encode
@@ -1463,46 +1747,66 @@
     Note: The parameters ``test_size`` and ``train_size`` refer to groups, and
     not to samples, as in ShuffleSplit.

+    Read more in the :ref:`User Guide <group_shuffle_split>`.

     Parameters
     ----------
-    n_splits : int (default 5)
+    n_splits : int, default=5
         Number of re-shuffling & splitting iterations.

-    test_size : float, int, None, optional (default=None)
+    test_size : float, int, default=0.2
         If float, should be between 0.0 and 1.0 and represent the proportion
-        of the dataset to include in the test split. If int, represents the
-        absolute number of test groups. If None, the value is set to the
-        complement of the train size. If ``train_size`` is also None, it will
-        be set to 0.2.
-
-    train_size : float, int, or None, default is None
+        of groups to include in the test split (rounded up). If int,
+        represents the absolute number of test groups. If None, the value is
+        set to the complement of the train size.
+        The default will change in version 0.21. It will remain 0.2 only
+        if ``train_size`` is unspecified, otherwise it will complement
+        the specified ``train_size``.
+
+    train_size : float or int, default=None
         If float, should be between 0.0 and 1.0 and represent the
         proportion of the groups to include in the train split. If
         int, represents the absolute number of train groups. If None,
         the value is automatically set to the complement of the test size.

-    random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-    '''
-
-    def __init__(self, n_splits=5, test_size=None, train_size=None,
-                 random_state=None):
+    random_state : int, RandomState instance or None, default=None
+        Controls the randomness of the training and testing indices produced.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.
+
+    Examples
+    --------
+    >>> import numpy as np
+    >>> from sklearn.model_selection import GroupShuffleSplit
+    >>> X = np.ones(shape=(8, 2))
+    >>> y = np.ones(shape=(8, 1))
+    >>> groups = np.array([1, 1, 2, 2, 2, 3, 3, 3])
+    >>> print(groups.shape)
+    (8,)
+    >>> gss = GroupShuffleSplit(n_splits=2, train_size=.7, random_state=42)
+    >>> gss.get_n_splits()
+    2
+    >>> for train_idx, test_idx in gss.split(X, y, groups):
+    ...     print("TRAIN:", train_idx, "TEST:", test_idx)
+    TRAIN: [2 3 4 5 6 7] TEST: [0 1]
+    TRAIN: [0 1 5 6 7] TEST: [2 3 4]
+    """
+
+    def __init__(
+        self, n_splits=5, *, test_size=None, train_size=None, random_state=None
+    ):
         super().__init__(
             n_splits=n_splits,
             test_size=test_size,
             train_size=train_size,
-            random_state=random_state)
+            random_state=random_state,
+        )
         self._default_test_size = 0.2

     def _iter_indices(self, X, y, groups):
         if groups is None:
             raise ValueError("The 'groups' parameter should not be None.")
-        groups = check_array(groups, ensure_2d=False, dtype=None)
+        groups = check_array(groups, input_name="groups", ensure_2d=False, dtype=None)
         classes, group_indices = np.unique(groups, return_inverse=True)
         for group_train, group_test in super()._iter_indices(X=classes):
             # these are the indices of classes in the partition
@@ -1518,14 +1822,14 @@

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples is the number of samples
-            and n_features is the number of features.
-
-        y : array-like, shape (n_samples,), optional
+        X : array-like of shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.
+
+        y : array-like of shape (n_samples,), default=None
             The target variable for supervised learning problems.

-        groups : array-like, with shape (n_samples,)
+        groups : array-like of shape (n_samples,)
             Group labels for the samples used while splitting the dataset into
             train/test set.

@@ -1540,7 +1844,7 @@
         Notes
         -----
         Randomized CV splitters may return different results for each call of
-        split. You can make the results identical by setting ``random_state``
+        split. You can make the results identical by setting `random_state`
         to an integer.
         """
         return super().split(X, y, groups)
@@ -1559,31 +1863,30 @@
     do not guarantee that all folds will be different, although this is
     still very likely for sizeable datasets.

-    Read more in the :ref:`User Guide <cross_validation>`.
+    Read more in the :ref:`User Guide <stratified_shuffle_split>`.

     Parameters
     ----------
-    n_splits : int, default 10
+    n_splits : int, default=10
         Number of re-shuffling & splitting iterations.

-    test_size : float, int, None, optional (default=None)
+    test_size : float or int, default=None
         If float, should be between 0.0 and 1.0 and represent the proportion
         of the dataset to include in the test split. If int, represents the
         absolute number of test samples. If None, the value is set to the
         complement of the train size. If ``train_size`` is also None, it will
         be set to 0.1.

-    train_size : float, int, or None, default is None
+    train_size : float or int, default=None
         If float, should be between 0.0 and 1.0 and represent the
         proportion of the dataset to include in the train split. If
         int, represents the absolute number of train samples. If None,
         the value is automatically set to the complement of the test size.

-    random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
+    random_state : int, RandomState instance or None, default=None
+        Controls the randomness of the training and testing indices produced.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.

     Examples
     --------
@@ -1594,12 +1897,12 @@
     >>> sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)
     >>> sss.get_n_splits(X, y)
     5
-    >>> print(sss)       # doctest: +ELLIPSIS
+    >>> print(sss)
     StratifiedShuffleSplit(n_splits=5, random_state=0, ...)
     >>> for train_index, test_index in sss.split(X, y):
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
+    ...     print("TRAIN:", train_index, "TEST:", test_index)
+    ...     X_train, X_test = X[train_index], X[test_index]
+    ...     y_train, y_test = y[train_index], y[test_index]
     TRAIN: [5 2 3] TEST: [4 1 0]
     TRAIN: [5 1 4] TEST: [0 2 3]
     TRAIN: [5 0 2] TEST: [4 3 1]
@@ -1607,50 +1910,60 @@
     TRAIN: [0 5 1] TEST: [3 4 2]
     """

-    def __init__(self, n_splits=10, test_size=None, train_size=None,
-                 random_state=None):
+    def __init__(
+        self, n_splits=10, *, test_size=None, train_size=None, random_state=None
+    ):
         super().__init__(
             n_splits=n_splits,
             test_size=test_size,
             train_size=train_size,
-            random_state=random_state)
+            random_state=random_state,
+        )
         self._default_test_size = 0.1

     def _iter_indices(self, X, y, groups=None):
         n_samples = _num_samples(X)
-        y = check_array(y, ensure_2d=False, dtype=None)
+        y = check_array(y, input_name="y", ensure_2d=False, dtype=None)
         n_train, n_test = _validate_shuffle_split(
-            n_samples, self.test_size, self.train_size,
-            default_test_size=self._default_test_size)
+            n_samples,
+            self.test_size,
+            self.train_size,
+            default_test_size=self._default_test_size,
+        )

         if y.ndim == 2:
             # for multi-label y, map each distinct row to a string repr
             # using join because str(row) uses an ellipsis if len(row) > 1000
-            y = np.array([' '.join(row.astype('str')) for row in y])
+            y = np.array([" ".join(row.astype("str")) for row in y])

         classes, y_indices = np.unique(y, return_inverse=True)
         n_classes = classes.shape[0]

         class_counts = np.bincount(y_indices)
         if np.min(class_counts) < 2:
-            raise ValueError("The least populated class in y has only 1"
-                             " member, which is too few. The minimum"
-                             " number of groups for any class cannot"
-                             " be less than 2.")
+            raise ValueError(
+                "The least populated class in y has only 1"
+                " member, which is too few. The minimum"
+                " number of groups for any class cannot"
+                " be less than 2."
+            )

         if n_train < n_classes:
-            raise ValueError('The train_size = %d should be greater or '
-                             'equal to the number of classes = %d' %
-                             (n_train, n_classes))
+            raise ValueError(
+                "The train_size = %d should be greater or "
+                "equal to the number of classes = %d" % (n_train, n_classes)
+            )
         if n_test < n_classes:
-            raise ValueError('The test_size = %d should be greater or '
-                             'equal to the number of classes = %d' %
-                             (n_test, n_classes))
+            raise ValueError(
+                "The test_size = %d should be greater or "
+                "equal to the number of classes = %d" % (n_test, n_classes)
+            )

         # Find the sorted list of instances for each class:
         # (np.unique above performs a sort, so code is O(n logn) already)
-        class_indices = np.split(np.argsort(y_indices, kind='mergesort'),
-                                 np.cumsum(class_counts)[:-1])
+        class_indices = np.split(
+            np.argsort(y_indices, kind="mergesort"), np.cumsum(class_counts)[:-1]
+        )

         rng = check_random_state(self.random_state)

@@ -1666,11 +1979,10 @@

             for i in range(n_classes):
                 permutation = rng.permutation(class_counts[i])
-                perm_indices_class_i = class_indices[i].take(permutation,
-                                                             mode='clip')
-
-                train.extend(perm_indices_class_i[:n_i[i]])
-                test.extend(perm_indices_class_i[n_i[i]:n_i[i] + t_i[i]])
+                perm_indices_class_i = class_indices[i].take(permutation, mode="clip")
+
+                train.extend(perm_indices_class_i[: n_i[i]])
+                test.extend(perm_indices_class_i[n_i[i] : n_i[i] + t_i[i]])

             train = rng.permutation(train)
             test = rng.permutation(test)
@@ -1682,15 +1994,15 @@

         Parameters
         ----------
-        X : array-like, shape (n_samples, n_features)
-            Training data, where n_samples is the number of samples
-            and n_features is the number of features.
+        X : array-like of shape (n_samples, n_features)
+            Training data, where `n_samples` is the number of samples
+            and `n_features` is the number of features.

             Note that providing ``y`` is sufficient to generate the splits and
             hence ``np.zeros(n_samples)`` may be used as a placeholder for
             ``X`` instead of actual training data.

-        y : array-like, shape (n_samples,)
+        y : array-like of shape (n_samples,) or (n_samples, n_labels)
             The target variable for supervised learning problems.
             Stratification is done based on the y labels.

@@ -1708,15 +2020,14 @@
         Notes
         -----
         Randomized CV splitters may return different results for each call of
-        split. You can make the results identical by setting ``random_state``
+        split. You can make the results identical by setting `random_state`
         to an integer.
         """
-        y = check_array(y, ensure_2d=False, dtype=None)
+        y = check_array(y, input_name="y", ensure_2d=False, dtype=None)
         return super().split(X, y, groups)


-def _validate_shuffle_split(n_samples, test_size, train_size,
-                            default_test_size=None):
+def _validate_shuffle_split(n_samples, test_size, train_size, default_test_size=None):
     """
     Validation helper to check if the test/test sizes are meaningful wrt to the
     size of the data (n_samples)
@@ -1727,38 +2038,49 @@
     test_size_type = np.asarray(test_size).dtype.kind
     train_size_type = np.asarray(train_size).dtype.kind

-    if (test_size_type == 'i' and (test_size >= n_samples or test_size <= 0)
-       or test_size_type == 'f' and (test_size <= 0 or test_size >= 1)):
-        raise ValueError('test_size={0} should be either positive and smaller'
-                         ' than the number of samples {1} or a float in the '
-                         '(0, 1) range'.format(test_size, n_samples))
-
-    if (train_size_type == 'i' and (train_size >= n_samples or train_size <= 0)
-       or train_size_type == 'f' and (train_size <= 0 or train_size >= 1)):
-        raise ValueError('train_size={0} should be either positive and smaller'
-                         ' than the number of samples {1} or a float in the '
-                         '(0, 1) range'.format(train_size, n_samples))
-
-    if train_size is not None and train_size_type not in ('i', 'f'):
+    if (
+        test_size_type == "i"
+        and (test_size >= n_samples or test_size <= 0)
+        or test_size_type == "f"
+        and (test_size <= 0 or test_size >= 1)
+    ):
+        raise ValueError(
+            "test_size={0} should be either positive and smaller"
+            " than the number of samples {1} or a float in the "
+            "(0, 1) range".format(test_size, n_samples)
+        )
+
+    if (
+        train_size_type == "i"
+        and (train_size >= n_samples or train_size <= 0)
+        or train_size_type == "f"
+        and (train_size <= 0 or train_size >= 1)
+    ):
+        raise ValueError(
+            "train_size={0} should be either positive and smaller"
+            " than the number of samples {1} or a float in the "
+            "(0, 1) range".format(train_size, n_samples)
+        )
+
+    if train_size is not None and train_size_type not in ("i", "f"):
         raise ValueError("Invalid value for train_size: {}".format(train_size))
-    if test_size is not None and test_size_type not in ('i', 'f'):
+    if test_size is not None and test_size_type not in ("i", "f"):
         raise ValueError("Invalid value for test_size: {}".format(test_size))

-    if (train_size_type == 'f' and test_size_type == 'f' and
-            train_size + test_size > 1):
+    if train_size_type == "f" and test_size_type == "f" and train_size + test_size > 1:
         raise ValueError(
-            'The sum of test_size and train_size = {}, should be in the (0, 1)'
-            ' range. Reduce test_size and/or train_size.'
-            .format(train_size + test_size))
-
-    if test_size_type == 'f':
+            "The sum of test_size and train_size = {}, should be in the (0, 1)"
+            " range. Reduce test_size and/or train_size.".format(train_size + test_size)
+        )
+
+    if test_size_type == "f":
         n_test = ceil(test_size * n_samples)
-    elif test_size_type == 'i':
+    elif test_size_type == "i":
         n_test = float(test_size)

-    if train_size_type == 'f':
+    if train_size_type == "f":
         n_train = floor(train_size * n_samples)
-    elif train_size_type == 'i':
+    elif train_size_type == "i":
         n_train = float(train_size)

     if train_size is None:
@@ -1767,19 +2089,20 @@
         n_test = n_samples - n_train

     if n_train + n_test > n_samples:
-        raise ValueError('The sum of train_size and test_size = %d, '
-                         'should be smaller than the number of '
-                         'samples %d. Reduce test_size and/or '
-                         'train_size.' % (n_train + n_test, n_samples))
+        raise ValueError(
+            "The sum of train_size and test_size = %d, "
+            "should be smaller than the number of "
+            "samples %d. Reduce test_size and/or "
+            "train_size." % (n_train + n_test, n_samples)
+        )

     n_train, n_test = int(n_train), int(n_test)

     if n_train == 0:
         raise ValueError(
-            'With n_samples={}, test_size={} and train_size={}, the '
-            'resulting train set will be empty. Adjust any of the '
-            'aforementioned parameters.'.format(n_samples, test_size,
-                                                train_size)
+            "With n_samples={}, test_size={} and train_size={}, the "
+            "resulting train set will be empty. Adjust any of the "
+            "aforementioned parameters.".format(n_samples, test_size, train_size)
         )

     return n_train, n_test
@@ -1791,11 +2114,13 @@
     Provides train/test indices to split data into train/test sets using a
     predefined scheme specified by the user with the ``test_fold`` parameter.

-    Read more in the :ref:`User Guide <cross_validation>`.
+    Read more in the :ref:`User Guide <predefined_split>`.
+
+    .. versionadded:: 0.16

     Parameters
     ----------
-    test_fold : array-like, shape (n_samples,)
+    test_fold : array-like of shape (n_samples,)
         The entry ``test_fold[i]`` represents the index of the test set that
         sample ``i`` belongs to. It is possible to exclude sample ``i`` from
         any test set (i.e. include sample ``i`` in every training set) by
@@ -1811,18 +2136,18 @@
     >>> ps = PredefinedSplit(test_fold)
     >>> ps.get_n_splits()
     2
-    >>> print(ps)       # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS
+    >>> print(ps)
     PredefinedSplit(test_fold=array([ 0,  1, -1,  1]))
     >>> for train_index, test_index in ps.split():
-    ...    print("TRAIN:", train_index, "TEST:", test_index)
-    ...    X_train, X_test = X[train_index], X[test_index]
-    ...    y_train, y_test = y[train_index], y[test_index]
+    ...     print("TRAIN:", train_index, "TEST:", test_index)
+    ...     X_train, X_test = X[train_index], X[test_index]
+    ...     y_train, y_test = y[train_index], y[test_index]
     TRAIN: [1 2 3] TEST: [0]
     TRAIN: [0 2] TEST: [1 3]
     """

     def __init__(self, test_fold):
-        self.test_fold = np.array(test_fold, dtype=np.int)
+        self.test_fold = np.array(test_fold, dtype=int)
         self.test_fold = column_or_1d(self.test_fold)
         self.unique_folds = np.unique(self.test_fold)
         self.unique_folds = self.unique_folds[self.unique_folds != -1]
@@ -1859,7 +2184,7 @@
         """Generates boolean masks corresponding to test sets."""
         for f in self.unique_folds:
             test_index = np.where(self.test_fold == f)[0]
-            test_mask = np.zeros(len(self.test_fold), dtype=np.bool)
+            test_mask = np.zeros(len(self.test_fold), dtype=bool)
             test_mask[test_index] = True
             yield test_mask

@@ -1887,6 +2212,7 @@

 class _CVIterableWrapper(BaseCrossValidator):
     """Wrapper class for old style cv objects and iterables."""
+
     def __init__(self, cv):
         self.cv = list(cv)

@@ -1937,19 +2263,18 @@
             yield train, test


-def check_cv(cv='warn', y=None, classifier=False):
-    """Input checker utility for building a cross-validator
+def check_cv(cv=5, y=None, *, classifier=False):
+    """Input checker utility for building a cross-validator.

     Parameters
     ----------
-    cv : int, cross-validation generator or an iterable, optional
+    cv : int, cross-validation generator or an iterable, default=None
         Determines the cross-validation splitting strategy.
         Possible inputs for cv are:
-
-        - None, to use the default 3-fold cross-validation,
+        - None, to use the default 5-fold cross validation,
         - integer, to specify the number of folds.
         - :term:`CV splitter`,
-        - An iterable yielding (train, test) splits as arrays of indices.
+        - An iterable that generates (train, test) splits as arrays of indices.

         For integer/None inputs, if classifier is True and ``y`` is either
         binary or multiclass, :class:`StratifiedKFold` is used. In all other
@@ -1958,13 +2283,13 @@
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validation strategies that can be used here.

-        .. versionchanged:: 0.20
-            ``cv`` default value will change from 3-fold to 5-fold in v0.22.
-
-    y : array-like, optional
+        .. versionchanged:: 0.22
+            ``cv`` default value changed from 3-fold to 5-fold.
+
+    y : array-like, default=None
         The target variable for supervised learning problems.

-    classifier : boolean, optional, default False
+    classifier : bool, default=False
         Whether the task is a classification task, in which case
         stratified KFold will be used.

@@ -1974,29 +2299,38 @@
         The return value is a cross-validator which generates the train/test
         splits via the ``split`` method.
     """
-    if cv is None or cv == 'warn':
-        warnings.warn(CV_WARNING, FutureWarning)
-        cv = 3
-
+    cv = 5 if cv is None else cv
     if isinstance(cv, numbers.Integral):
-        if (classifier and (y is not None) and
-                (type_of_target(y) in ('binary', 'multiclass'))):
+        if (
+            classifier
+            and (y is not None)
+            and (type_of_target(y, input_name="y") in ("binary", "multiclass"))
+        ):
             return StratifiedKFold(cv)
         else:
             return KFold(cv)

-    if not hasattr(cv, 'split') or isinstance(cv, str):
+    if not hasattr(cv, "split") or isinstance(cv, str):
         if not isinstance(cv, Iterable) or isinstance(cv, str):
-            raise ValueError("Expected cv as an integer, cross-validation "
-                             "object (from sklearn.model_selection) "
-                             "or an iterable. Got %s." % cv)
+            raise ValueError(
+                "Expected cv as an integer, cross-validation "
+                "object (from sklearn.model_selection) "
+                "or an iterable. Got %s." % cv
+            )
         return _CVIterableWrapper(cv)

     return cv  # New style cv objects are passed without any modification


-def train_test_split(*arrays, **options):
-    """Split arrays or matrices into random train and test subsets
+def train_test_split(
+    *arrays,
+    test_size=None,
+    train_size=None,
+    random_state=None,
+    shuffle=True,
+    stratify=None,
+):
+    """Split arrays or matrices into random train and test subsets.

     Quick utility that wraps input validation and
     ``next(ShuffleSplit().split(X, y))`` and application to input data
@@ -2011,32 +2345,32 @@
         Allowed inputs are lists, numpy arrays, scipy-sparse
         matrices or pandas dataframes.

-    test_size : float, int or None, optional (default=None)
+    test_size : float or int, default=None
         If float, should be between 0.0 and 1.0 and represent the proportion
         of the dataset to include in the test split. If int, represents the
         absolute number of test samples. If None, the value is set to the
         complement of the train size. If ``train_size`` is also None, it will
         be set to 0.25.

-    train_size : float, int, or None, (default=None)
+    train_size : float or int, default=None
         If float, should be between 0.0 and 1.0 and represent the
         proportion of the dataset to include in the train split. If
         int, represents the absolute number of train samples. If None,
         the value is automatically set to the complement of the test size.

-    random_state : int, RandomState instance or None, optional (default=None)
-        If int, random_state is the seed used by the random number generator;
-        If RandomState instance, random_state is the random number generator;
-        If None, the random number generator is the RandomState instance used
-        by `np.random`.
-
-    shuffle : boolean, optional (default=True)
+    random_state : int, RandomState instance or None, default=None
+        Controls the shuffling applied to the data before applying the split.
+        Pass an int for reproducible output across multiple function calls.
+        See :term:`Glossary <random_state>`.
+
+    shuffle : bool, default=True
         Whether or not to shuffle the data before splitting. If shuffle=False
         then stratify must be None.

-    stratify : array-like or None (default=None)
+    stratify : array-like, default=None
         If not None, data is split in a stratified fashion, using this as
         the class labels.
+        Read more in the :ref:`User Guide <stratification>`.

     Returns
     -------
@@ -2079,31 +2413,23 @@

     >>> train_test_split(y, shuffle=False)
     [[0, 1, 2], [3, 4]]
-
     """
     n_arrays = len(arrays)
     if n_arrays == 0:
         raise ValueError("At least one array required as input")
-    test_size = options.pop('test_size', None)
-    train_size = options.pop('train_size', None)
-    random_state = options.pop('random_state', None)
-    stratify = options.pop('stratify', None)
-    shuffle = options.pop('shuffle', True)
-
-    if options:
-        raise TypeError("Invalid parameters passed: %s" % str(options))

     arrays = indexable(*arrays)

     n_samples = _num_samples(arrays[0])
-    n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,
-                                              default_test_size=0.25)
+    n_train, n_test = _validate_shuffle_split(
+        n_samples, test_size, train_size, default_test_size=0.25
+    )

     if shuffle is False:
         if stratify is not None:
             raise ValueError(
-                "Stratified train/test split is not implemented for "
-                "shuffle=False")
+                "Stratified train/test split is not implemented for shuffle=False"
+            )

         train = np.arange(n_train)
         test = np.arange(n_train, n_train + n_test)
@@ -2114,28 +2440,40 @@
         else:
             CVClass = ShuffleSplit

-        cv = CVClass(test_size=n_test,
-                     train_size=n_train,
-                     random_state=random_state)
+        cv = CVClass(test_size=n_test, train_size=n_train, random_state=random_state)

         train, test = next(cv.split(X=arrays[0], y=stratify))

-    return list(chain.from_iterable((safe_indexing(a, train),
-                                     safe_indexing(a, test)) for a in arrays))
+    return list(
+        chain.from_iterable(
+            (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays
+        )
+    )
+
+
+# Tell nose that train_test_split is not a test.
+# (Needed for external libraries that may use nose.)
+# Use setattr to avoid mypy errors when monkeypatching.
+setattr(train_test_split, "__test__", False)


 def _build_repr(self):
     # XXX This is copied from BaseEstimator's get_params
     cls = self.__class__
-    init = getattr(cls.__init__, 'deprecated_original', cls.__init__)
+    init = getattr(cls.__init__, "deprecated_original", cls.__init__)
     # Ignore varargs, kw and default values and pop self
     init_signature = signature(init)
     # Consider the constructor parameters excluding 'self'
     if init is object.__init__:
         args = []
     else:
-        args = sorted([p.name for p in init_signature.parameters.values()
-                       if p.name != 'self' and p.kind != p.VAR_KEYWORD])
+        args = sorted(
+            [
+                p.name
+                for p in init_signature.parameters.values()
+                if p.name != "self" and p.kind != p.VAR_KEYWORD
+            ]
+        )
     class_name = self.__class__.__name__
     params = dict()
     for key in args:
@@ -2143,15 +2481,28 @@
         # catch deprecated param values.
         # This is set in utils/__init__.py but it gets overwritten
         # when running under python3 somehow.
-        warnings.simplefilter("always", DeprecationWarning)
+        warnings.simplefilter("always", FutureWarning)
         try:
             with warnings.catch_warnings(record=True) as w:
                 value = getattr(self, key, None)
-            if len(w) and w[0].category == DeprecationWarning:
+                if value is None and hasattr(self, "cvargs"):
+                    value = self.cvargs.get(key, None)
+            if len(w) and w[0].category == FutureWarning:
                 # if the parameter is deprecated, don't show it
                 continue
         finally:
             warnings.filters.pop(0)
         params[key] = value

-    return '%s(%s)' % (class_name, _pprint(params, offset=len(class_name)))
+    return "%s(%s)" % (class_name, _pprint(params, offset=len(class_name)))
+
+
+def _yields_constant_splits(cv):
+    # Return True if calling cv.split() always returns the same splits
+    # We assume that if a cv doesn't have a shuffle parameter, it shuffles by
+    # default (e.g. ShuffleSplit). If it actually doesn't shuffle (e.g.
+    # LeaveOneOut), then it won't have a random_state parameter anyway, in
+    # which case it will default to 0, leading to output=True
+    shuffle = getattr(cv, "shuffle", True)
+    random_state = getattr(cv, "random_state", 0)
+    return isinstance(random_state, numbers.Integral) or not shuffle
('sklearn/_build_utils', 'openmp_helpers.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,46 +1,33 @@
 """Helpers for OpenMP support during the build."""

 # This code is adapted for a large part from the astropy openmp helpers, which
-# can be found at: https://github.com/astropy/astropy-helpers/blob/master/astropy_helpers/openmp_helpers.py  # noqa
+# can be found at: https://github.com/astropy/extension-helpers/blob/master/extension_helpers/_openmp_helpers.py  # noqa


 import os
 import sys
-import glob
-import tempfile
 import textwrap
+import warnings
 import subprocess

-from numpy.distutils.ccompiler import new_compiler
-from distutils.sysconfig import customize_compiler
 from distutils.errors import CompileError, LinkError

-
-CCODE = textwrap.dedent(
-    """\
-    #include <omp.h>
-    #include <stdio.h>
-    int main(void) {
-    #pragma omp parallel
-    printf("nthreads=%d\\n", omp_get_num_threads());
-    return 0;
-    }
-    """)
+from .pre_build_helpers import compile_test_program


 def get_openmp_flag(compiler):
-    if hasattr(compiler, 'compiler'):
+    if hasattr(compiler, "compiler"):
         compiler = compiler.compiler[0]
     else:
         compiler = compiler.__class__.__name__

-    if sys.platform == "win32" and ('icc' in compiler or 'icl' in compiler):
-        return ['/Qopenmp']
+    if sys.platform == "win32" and ("icc" in compiler or "icl" in compiler):
+        return ["/Qopenmp"]
     elif sys.platform == "win32":
-        return ['/openmp']
-    elif sys.platform == "darwin" and ('icc' in compiler or 'icl' in compiler):
-        return ['-openmp']
-    elif sys.platform == "darwin" and 'openmp' in os.getenv('CPPFLAGS', ''):
+        return ["/openmp"]
+    elif sys.platform in ("darwin", "linux") and "icc" in compiler:
+        return ["-qopenmp"]
+    elif sys.platform == "darwin" and "openmp" in os.getenv("CPPFLAGS", ""):
         # -fopenmp can't be passed as compile flag when using Apple-clang.
         # OpenMP support has to be enabled during preprocessing.
         #
@@ -50,93 +37,93 @@
         # export CPPFLAGS="$CPPFLAGS -Xpreprocessor -fopenmp"
         # export CFLAGS="$CFLAGS -I/usr/local/opt/libomp/include"
         # export CXXFLAGS="$CXXFLAGS -I/usr/local/opt/libomp/include"
-        # export LDFLAGS="$LDFLAGS -L/usr/local/opt/libomp/lib -lomp"
-        # export DYLD_LIBRARY_PATH=/usr/local/opt/libomp/lib
+        # export LDFLAGS="$LDFLAGS -Wl,-rpath,/usr/local/opt/libomp/lib
+        #                          -L/usr/local/opt/libomp/lib -lomp"
         return []
     # Default flag for GCC and clang:
-    return ['-fopenmp']
+    return ["-fopenmp"]


 def check_openmp_support():
     """Check whether OpenMP test code can be compiled and run"""
-    ccompiler = new_compiler()
-    customize_compiler(ccompiler)
-
-    if os.getenv('SKLEARN_NO_OPENMP'):
-        # Build explicitly without OpenMP support
+    if "PYODIDE_PACKAGE_ABI" in os.environ:
+        # Pyodide doesn't support OpenMP
         return False

-    start_dir = os.path.abspath('.')
+    code = textwrap.dedent(
+        """\
+        #include <omp.h>
+        #include <stdio.h>
+        int main(void) {
+        #pragma omp parallel
+        printf("nthreads=%d\\n", omp_get_num_threads());
+        return 0;
+        }
+        """
+    )

-    with tempfile.TemporaryDirectory() as tmp_dir:
-        try:
-            os.chdir(tmp_dir)
+    extra_preargs = os.getenv("LDFLAGS", None)
+    if extra_preargs is not None:
+        extra_preargs = extra_preargs.strip().split(" ")
+        # FIXME: temporary fix to link against system libraries on linux
+        # "-Wl,--sysroot=/" should be removed
+        extra_preargs = [
+            flag
+            for flag in extra_preargs
+            if flag.startswith(("-L", "-Wl,-rpath", "-l", "-Wl,--sysroot=/"))
+        ]

-            # Write test program
-            with open('test_openmp.c', 'w') as f:
-                f.write(CCODE)
+    extra_postargs = get_openmp_flag

-            os.mkdir('objects')
+    try:
+        output = compile_test_program(
+            code, extra_preargs=extra_preargs, extra_postargs=extra_postargs
+        )

-            # Compile, test program
-            openmp_flags = get_openmp_flag(ccompiler)
-            ccompiler.compile(['test_openmp.c'], output_dir='objects',
-                              extra_postargs=openmp_flags)
-
-            # Link test program
-            extra_preargs = os.getenv('LDFLAGS', None)
-            if extra_preargs is not None:
-                extra_preargs = extra_preargs.split(" ")
-            else:
-                extra_preargs = []
-
-            objects = glob.glob(
-                os.path.join('objects', '*' + ccompiler.obj_extension))
-            ccompiler.link_executable(objects, 'test_openmp',
-                                      extra_preargs=extra_preargs,
-                                      extra_postargs=openmp_flags)
-
-            # Run test program
-            output = subprocess.check_output('./test_openmp')
-            output = output.decode(sys.stdout.encoding or 'utf-8').splitlines()
-
-            # Check test program output
-            if 'nthreads=' in output[0]:
-                nthreads = int(output[0].strip().split('=')[1])
-                openmp_supported = (len(output) == nthreads)
-            else:
-                openmp_supported = False
-
-        except (CompileError, LinkError, subprocess.CalledProcessError):
+        if output and "nthreads=" in output[0]:
+            nthreads = int(output[0].strip().split("=")[1])
+            openmp_supported = len(output) == nthreads
+        elif "PYTHON_CROSSENV" in os.environ:
+            # Since we can't run the test program when cross-compiling
+            # assume that openmp is supported if the program can be
+            # compiled.
+            openmp_supported = True
+        else:
             openmp_supported = False

-        finally:
-            os.chdir(start_dir)
-
-    err_message = textwrap.dedent(
-        """
-                            ***
-
-        It seems that scikit-learn cannot be built with OpenMP support.
-
-        - Make sure you have followed the installation instructions:
-
-            https://scikit-learn.org/dev/developers/advanced_installation.html
-
-        - If your compiler supports OpenMP but the build still fails, please
-          submit a bug report at:
-
-            https://github.com/scikit-learn/scikit-learn/issues
-
-        - If you want to build scikit-learn without OpenMP support, you can set
-          the environment variable SKLEARN_NO_OPENMP and rerun the build
-          command. Note however that some estimators will run in sequential
-          mode and their `n_jobs` parameter will have no effect anymore.
-
-                            ***
-        """)
+    except (CompileError, LinkError, subprocess.CalledProcessError):
+        openmp_supported = False

     if not openmp_supported:
-        raise CompileError(err_message)
+        if os.getenv("SKLEARN_FAIL_NO_OPENMP"):
+            raise CompileError("Failed to build with OpenMP")
+        else:
+            message = textwrap.dedent(
+                """

-    return True
+                                ***********
+                                * WARNING *
+                                ***********
+
+                It seems that scikit-learn cannot be built with OpenMP.
+
+                - Make sure you have followed the installation instructions:
+
+                    https://scikit-learn.org/dev/developers/advanced_installation.html
+
+                - If your compiler supports OpenMP but you still see this
+                  message, please submit a bug report at:
+
+                    https://github.com/scikit-learn/scikit-learn/issues
+
+                - The build will continue with OpenMP-based parallelism
+                  disabled. Note however that some estimators will run in
+                  sequential mode instead of leveraging thread-based
+                  parallelism.
+
+                                    ***
+                """
+            )
+            warnings.warn(message)
+
+    return openmp_supported
('sklearn/_build_utils', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,87 +6,106 @@


 import os
+import sklearn
+import contextlib

 from distutils.version import LooseVersion

-from numpy.distutils.system_info import get_info
-
+from .pre_build_helpers import basic_check_build
 from .openmp_helpers import check_openmp_support
+from .._min_dependencies import CYTHON_MIN_VERSION


-DEFAULT_ROOT = 'sklearn'
-# on conda, this is the latest for python 3.5
-CYTHON_MIN_VERSION = '0.28.5'
+DEFAULT_ROOT = "sklearn"


-def get_blas_info():
-    def atlas_not_found(blas_info_):
-        def_macros = blas_info.get('define_macros', [])
-        for x in def_macros:
-            if x[0] == "NO_ATLAS_INFO":
-                # if x[1] != 1 we should have lapack
-                # how do we do that now?
-                return True
-            if x[0] == "ATLAS_INFO":
-                if "None" in x[1]:
-                    # this one turned up on FreeBSD
-                    return True
-        return False
+def _check_cython_version():
+    message = (
+        "Please install Cython with a version >= {0} in order "
+        "to build a scikit-learn from source."
+    ).format(CYTHON_MIN_VERSION)
+    try:
+        import Cython
+    except ModuleNotFoundError as e:
+        # Re-raise with more informative error message instead:
+        raise ModuleNotFoundError(message) from e

-    blas_info = get_info('blas_opt', 0)
-    if (not blas_info) or atlas_not_found(blas_info):
-        cblas_libs = ['cblas']
-        blas_info.pop('libraries', None)
-    else:
-        cblas_libs = blas_info.pop('libraries', [])
-
-    return cblas_libs, blas_info
+    if LooseVersion(Cython.__version__) < CYTHON_MIN_VERSION:
+        message += " The current version of Cython is {} installed in {}.".format(
+            Cython.__version__, Cython.__path__
+        )
+        raise ValueError(message)


-def build_from_c_and_cpp_files(extensions):
-    """Modify the extensions to build from the .c and .cpp files.
+def cythonize_extensions(top_path, config):
+    """Check that a recent Cython is available and cythonize extensions"""
+    _check_cython_version()
+    from Cython.Build import cythonize

-    This is useful for releases, this way cython is not required to
-    run python setup.py install.
-    """
-    for extension in extensions:
-        sources = []
-        for sfile in extension.sources:
-            path, ext = os.path.splitext(sfile)
-            if ext in ('.pyx', '.py'):
-                if extension.language == 'c++':
-                    ext = '.cpp'
-                else:
-                    ext = '.c'
-                sfile = path + ext
-            sources.append(sfile)
-        extension.sources = sources
+    # Fast fail before cythonization if compiler fails compiling basic test
+    # code even without OpenMP
+    basic_check_build()
+
+    # check simple compilation with OpenMP. If it fails scikit-learn will be
+    # built without OpenMP and the test test_openmp_supported in the test suite
+    # will fail.
+    # `check_openmp_support` compiles a small test program to see if the
+    # compilers are properly configured to build with OpenMP. This is expensive
+    # and we only want to call this function once.
+    # The result of this check is cached as a private attribute on the sklearn
+    # module (only at build-time) to be used twice:
+    # - First to set the value of SKLEARN_OPENMP_PARALLELISM_ENABLED, the
+    #   cython build-time variable passed to the cythonize() call.
+    # - Then in the build_ext subclass defined in the top-level setup.py file
+    #   to actually build the compiled extensions with OpenMP flags if needed.
+    sklearn._OPENMP_SUPPORTED = check_openmp_support()
+
+    n_jobs = 1
+    with contextlib.suppress(ImportError):
+        import joblib
+
+        n_jobs = joblib.cpu_count()
+
+    # Additional checks for Cython
+    cython_enable_debug_directives = (
+        os.environ.get("SKLEARN_ENABLE_DEBUG_CYTHON_DIRECTIVES", "0") != "0"
+    )
+
+    config.ext_modules = cythonize(
+        config.ext_modules,
+        nthreads=n_jobs,
+        compile_time_env={
+            "SKLEARN_OPENMP_PARALLELISM_ENABLED": sklearn._OPENMP_SUPPORTED
+        },
+        compiler_directives={
+            "language_level": 3,
+            "boundscheck": cython_enable_debug_directives,
+            "wraparound": False,
+            "initializedcheck": False,
+            "nonecheck": False,
+            "cdivision": True,
+        },
+    )


-def maybe_cythonize_extensions(top_path, config):
-    """Tweaks for building extensions between release and development mode."""
-    with_openmp = check_openmp_support()
+def gen_from_templates(templates):
+    """Generate cython files from a list of templates"""
+    # Lazy import because cython is not a runtime dependency.
+    from Cython import Tempita

-    is_release = os.path.exists(os.path.join(top_path, 'PKG-INFO'))
+    for template in templates:
+        outfile = template.replace(".tp", "")

-    if is_release:
-        build_from_c_and_cpp_files(config.ext_modules)
-    else:
-        message = ('Please install cython with a version >= {0} in order '
-                   'to build a scikit-learn development version.').format(
-                       CYTHON_MIN_VERSION)
-        try:
-            import Cython
-            if LooseVersion(Cython.__version__) < CYTHON_MIN_VERSION:
-                message += ' Your version of Cython was {0}.'.format(
-                    Cython.__version__)
-                raise ValueError(message)
-            from Cython.Build import cythonize
-        except ImportError as exc:
-            exc.args += (message,)
-            raise
+        # if the template is not updated, no need to output the cython file
+        if not (
+            os.path.exists(outfile)
+            and os.stat(template).st_mtime < os.stat(outfile).st_mtime
+        ):

-        config.ext_modules = cythonize(
-            config.ext_modules,
-            compile_time_env={'SKLEARN_OPENMP_SUPPORTED': with_openmp},
-            compiler_directives={'language_level': 3})
+            with open(template, "r") as f:
+                tmpl = f.read()
+
+            tmpl_ = Tempita.sub(tmpl)
+
+            with open(outfile, "w") as f:
+                f.write(tmpl_)
('sklearn/decomposition', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,36 +4,50 @@
 this module can be regarded as dimensionality reduction techniques.
 """

-from .nmf import NMF, non_negative_factorization
-from .pca import PCA
-from .incremental_pca import IncrementalPCA
-from .kernel_pca import KernelPCA
-from .sparse_pca import SparsePCA, MiniBatchSparsePCA
-from .truncated_svd import TruncatedSVD
-from .fastica_ import FastICA, fastica
-from .dict_learning import (dict_learning, dict_learning_online, sparse_encode,
-                            DictionaryLearning, MiniBatchDictionaryLearning,
-                            SparseCoder)
-from .factor_analysis import FactorAnalysis
+
+from ._nmf import (
+    NMF,
+    MiniBatchNMF,
+    non_negative_factorization,
+)
+from ._pca import PCA
+from ._incremental_pca import IncrementalPCA
+from ._kernel_pca import KernelPCA
+from ._sparse_pca import SparsePCA, MiniBatchSparsePCA
+from ._truncated_svd import TruncatedSVD
+from ._fastica import FastICA, fastica
+from ._dict_learning import (
+    dict_learning,
+    dict_learning_online,
+    sparse_encode,
+    DictionaryLearning,
+    MiniBatchDictionaryLearning,
+    SparseCoder,
+)
+from ._factor_analysis import FactorAnalysis
 from ..utils.extmath import randomized_svd
-from .online_lda import LatentDirichletAllocation
+from ._lda import LatentDirichletAllocation

-__all__ = ['DictionaryLearning',
-           'FastICA',
-           'IncrementalPCA',
-           'KernelPCA',
-           'MiniBatchDictionaryLearning',
-           'MiniBatchSparsePCA',
-           'NMF',
-           'PCA',
-           'SparseCoder',
-           'SparsePCA',
-           'dict_learning',
-           'dict_learning_online',
-           'fastica',
-           'non_negative_factorization',
-           'randomized_svd',
-           'sparse_encode',
-           'FactorAnalysis',
-           'TruncatedSVD',
-           'LatentDirichletAllocation']
+
+__all__ = [
+    "DictionaryLearning",
+    "FastICA",
+    "IncrementalPCA",
+    "KernelPCA",
+    "MiniBatchDictionaryLearning",
+    "MiniBatchNMF",
+    "MiniBatchSparsePCA",
+    "NMF",
+    "PCA",
+    "SparseCoder",
+    "SparsePCA",
+    "dict_learning",
+    "dict_learning_online",
+    "fastica",
+    "non_negative_factorization",
+    "randomized_svd",
+    "sparse_encode",
+    "FactorAnalysis",
+    "TruncatedSVD",
+    "LatentDirichletAllocation",
+]
('sklearn/decomposition', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,23 +7,29 @@
     config = Configuration("decomposition", parent_package, top_path)

     libraries = []
-    if os.name == 'posix':
-        libraries.append('m')
+    if os.name == "posix":
+        libraries.append("m")

-    config.add_extension("_online_lda",
-                         sources=["_online_lda.pyx"],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
+    config.add_extension(
+        "_online_lda_fast",
+        sources=["_online_lda_fast.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )

-    config.add_extension('cdnmf_fast',
-                         sources=['cdnmf_fast.pyx'],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
+    config.add_extension(
+        "_cdnmf_fast",
+        sources=["_cdnmf_fast.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )

     config.add_subpackage("tests")

     return config

+
 if __name__ == "__main__":
     from numpy.distutils.core import setup
+
     setup(**configuration().todict())
('sklearn/cross_decomposition', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,2 +1,3 @@
-from .pls_ import *  # noqa
-from .cca_ import *  # noqa
+from ._pls import PLSCanonical, PLSRegression, PLSSVD, CCA
+
+__all__ = ["PLSCanonical", "PLSRegression", "PLSSVD", "CCA"]
('sklearn/neighbors', '__init__.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,32 +3,37 @@
 algorithm.
 """

-from .ball_tree import BallTree
-from .kd_tree import KDTree
-from .dist_metrics import DistanceMetric
-from .graph import kneighbors_graph, radius_neighbors_graph
-from .unsupervised import NearestNeighbors
-from .classification import KNeighborsClassifier, RadiusNeighborsClassifier
-from .regression import KNeighborsRegressor, RadiusNeighborsRegressor
-from .nearest_centroid import NearestCentroid
-from .kde import KernelDensity
-from .lof import LocalOutlierFactor
-from .nca import NeighborhoodComponentsAnalysis
-from .base import VALID_METRICS, VALID_METRICS_SPARSE
+from ._ball_tree import BallTree
+from ._kd_tree import KDTree
+from ._distance_metric import DistanceMetric
+from ._graph import kneighbors_graph, radius_neighbors_graph
+from ._graph import KNeighborsTransformer, RadiusNeighborsTransformer
+from ._unsupervised import NearestNeighbors
+from ._classification import KNeighborsClassifier, RadiusNeighborsClassifier
+from ._regression import KNeighborsRegressor, RadiusNeighborsRegressor
+from ._nearest_centroid import NearestCentroid
+from ._kde import KernelDensity
+from ._lof import LocalOutlierFactor
+from ._nca import NeighborhoodComponentsAnalysis
+from ._base import VALID_METRICS, VALID_METRICS_SPARSE

-__all__ = ['BallTree',
-           'DistanceMetric',
-           'KDTree',
-           'KNeighborsClassifier',
-           'KNeighborsRegressor',
-           'NearestCentroid',
-           'NearestNeighbors',
-           'RadiusNeighborsClassifier',
-           'RadiusNeighborsRegressor',
-           'kneighbors_graph',
-           'radius_neighbors_graph',
-           'KernelDensity',
-           'LocalOutlierFactor',
-           'NeighborhoodComponentsAnalysis',
-           'VALID_METRICS',
-           'VALID_METRICS_SPARSE']
+__all__ = [
+    "BallTree",
+    "DistanceMetric",
+    "KDTree",
+    "KNeighborsClassifier",
+    "KNeighborsRegressor",
+    "KNeighborsTransformer",
+    "NearestCentroid",
+    "NearestNeighbors",
+    "RadiusNeighborsClassifier",
+    "RadiusNeighborsRegressor",
+    "RadiusNeighborsTransformer",
+    "kneighbors_graph",
+    "radius_neighbors_graph",
+    "KernelDensity",
+    "LocalOutlierFactor",
+    "NeighborhoodComponentsAnalysis",
+    "VALID_METRICS",
+    "VALID_METRICS_SPARSE",
+]
('sklearn/neighbors', 'setup.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,41 +1,44 @@
 import os


-def configuration(parent_package='', top_path=None):
+def configuration(parent_package="", top_path=None):
     import numpy
     from numpy.distutils.misc_util import Configuration

-    config = Configuration('neighbors', parent_package, top_path)
+    config = Configuration("neighbors", parent_package, top_path)
     libraries = []
-    if os.name == 'posix':
-        libraries.append('m')
+    if os.name == "posix":
+        libraries.append("m")

-    config.add_extension('ball_tree',
-                         sources=['ball_tree.pyx'],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
+    config.add_extension(
+        "_ball_tree",
+        sources=["_ball_tree.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )

-    config.add_extension('kd_tree',
-                         sources=['kd_tree.pyx'],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
+    config.add_extension(
+        "_kd_tree",
+        sources=["_kd_tree.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )

-    config.add_extension('dist_metrics',
-                         sources=['dist_metrics.pyx'],
-                         include_dirs=[numpy.get_include(),
-                                       os.path.join(numpy.get_include(),
-                                                    'numpy')],
-                         libraries=libraries)
+    config.add_extension(
+        "_partition_nodes",
+        sources=["_partition_nodes.pyx"],
+        include_dirs=[numpy.get_include()],
+        language="c++",
+        libraries=libraries,
+    )

-    config.add_extension('typedefs',
-                         sources=['typedefs.pyx'],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
-    config.add_extension("quad_tree",
-                         sources=["quad_tree.pyx"],
-                         include_dirs=[numpy.get_include()],
-                         libraries=libraries)
+    config.add_extension(
+        "_quad_tree",
+        sources=["_quad_tree.pyx"],
+        include_dirs=[numpy.get_include()],
+        libraries=libraries,
+    )

-    config.add_subpackage('tests')
+    config.add_subpackage("tests")

     return config
('maint_tools', 'whats_missing.sh')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -11,7 +11,7 @@
 to_file=$2

 logged_prs() {
-	git log --oneline $from_branch..master sklearn/ |
+	git log --oneline $from_branch..main sklearn/ |
 		grep -wv -e CLN -e TST -e CI -e DOC -e doc -e MNT -e MAINT -e BLD -e COSMIT -e EXA -e examples -e example -e minor -e STY -e Style -e docstring |
 		grep -o '(#[0-9][0-9]\+)$' |
 		grep -o '[0-9]\+'
@@ -19,7 +19,7 @@

 mentioned_issues() {
 	cat doc/whats_new/v$to_file.rst |
-			grep -o 'issue:`[0-9]\+`' |
+			grep -o 'issue:`[0-9]\+`\|pr:`[0-9]\+`' |
 			grep -o '[0-9]\+'
 }

('maint_tools', 'sort_whats_new.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,40 +6,38 @@
 import re
 from collections import defaultdict

-LABEL_ORDER = ['MajorFeature', 'Feature', 'Enhancement', 'Efficiency',
-               'Fix', 'API']
+LABEL_ORDER = ["MajorFeature", "Feature", "Enhancement", "Efficiency", "Fix", "API"]


 def entry_sort_key(s):
-    if s.startswith('- |'):
-        return LABEL_ORDER.index(s.split('|')[1])
+    if s.startswith("- |"):
+        return LABEL_ORDER.index(s.split("|")[1])
     else:
         return -1


 # discard headings and other non-entry lines
-text = ''.join(l for l in sys.stdin
-               if l.startswith('- ') or l.startswith(' '))
+text = "".join(l for l in sys.stdin if l.startswith("- ") or l.startswith(" "))

 bucketed = defaultdict(list)

-for entry in re.split('\n(?=- )', text.strip()):
-    modules = re.findall(r':(?:func|meth|mod|class):'
-                         r'`(?:[^<`]*<|~)?(?:sklearn.)?([a-z]\w+)',
-                         entry)
+for entry in re.split("\n(?=- )", text.strip()):
+    modules = re.findall(
+        r":(?:func|meth|mod|class):" r"`(?:[^<`]*<|~)?(?:sklearn.)?([a-z]\w+)", entry
+    )
     modules = set(modules)
     if len(modules) > 1:
-        key = 'Multiple modules'
+        key = "Multiple modules"
     elif modules:
-        key = ':mod:`sklearn.%s`' % next(iter(modules))
+        key = ":mod:`sklearn.%s`" % next(iter(modules))
     else:
-        key = 'Miscellaneous'
+        key = "Miscellaneous"
     bucketed[key].append(entry)
-    entry = entry.strip() + '\n'
+    entry = entry.strip() + "\n"

 everything = []
 for key, bucket in sorted(bucketed.items()):
-    everything.append(key + '\n' + '.' * len(key))
+    everything.append(key + "\n" + "." * len(key))
     bucket.sort(key=entry_sort_key)
     everything.extend(bucket)
-print('\n\n'.join(everything))
+print("\n\n".join(everything))
('examples/bicluster', 'plot_bicluster_newsgroups.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -22,23 +22,22 @@
 achieve a better V-measure than clusters found by MiniBatchKMeans.

 """
+
 from collections import defaultdict
 import operator
 from time import time

 import numpy as np

-from sklearn.cluster.bicluster import SpectralCoclustering
+from sklearn.cluster import SpectralCoclustering
 from sklearn.cluster import MiniBatchKMeans
-from sklearn.datasets.twenty_newsgroups import fetch_20newsgroups
+from sklearn.datasets import fetch_20newsgroups
 from sklearn.feature_extraction.text import TfidfVectorizer
 from sklearn.metrics.cluster import v_measure_score

-print(__doc__)
-

 def number_normalizer(tokens):
-    """ Map all numeric tokens to a placeholder.
+    """Map all numeric tokens to a placeholder.

     For many applications, tokens that begin with a number are not directly
     useful, but the fact that such a token exists can be relevant.  By applying
@@ -54,22 +53,35 @@


 # exclude 'comp.os.ms-windows.misc'
-categories = ['alt.atheism', 'comp.graphics',
-              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',
-              'comp.windows.x', 'misc.forsale', 'rec.autos',
-              'rec.motorcycles', 'rec.sport.baseball',
-              'rec.sport.hockey', 'sci.crypt', 'sci.electronics',
-              'sci.med', 'sci.space', 'soc.religion.christian',
-              'talk.politics.guns', 'talk.politics.mideast',
-              'talk.politics.misc', 'talk.religion.misc']
+categories = [
+    "alt.atheism",
+    "comp.graphics",
+    "comp.sys.ibm.pc.hardware",
+    "comp.sys.mac.hardware",
+    "comp.windows.x",
+    "misc.forsale",
+    "rec.autos",
+    "rec.motorcycles",
+    "rec.sport.baseball",
+    "rec.sport.hockey",
+    "sci.crypt",
+    "sci.electronics",
+    "sci.med",
+    "sci.space",
+    "soc.religion.christian",
+    "talk.politics.guns",
+    "talk.politics.mideast",
+    "talk.politics.misc",
+    "talk.religion.misc",
+]
 newsgroups = fetch_20newsgroups(categories=categories)
 y_true = newsgroups.target

-vectorizer = NumberNormalizingVectorizer(stop_words='english', min_df=5)
-cocluster = SpectralCoclustering(n_clusters=len(categories),
-                                 svd_method='arpack', random_state=0)
-kmeans = MiniBatchKMeans(n_clusters=len(categories), batch_size=20000,
-                         random_state=0)
+vectorizer = NumberNormalizingVectorizer(stop_words="english", min_df=5)
+cocluster = SpectralCoclustering(
+    n_clusters=len(categories), svd_method="arpack", random_state=0
+)
+kmeans = MiniBatchKMeans(n_clusters=len(categories), batch_size=20000, random_state=0)

 print("Vectorizing...")
 X = vectorizer.fit_transform(newsgroups.data)
@@ -78,18 +90,22 @@
 start_time = time()
 cocluster.fit(X)
 y_cocluster = cocluster.row_labels_
-print("Done in {:.2f}s. V-measure: {:.4f}".format(
-    time() - start_time,
-    v_measure_score(y_cocluster, y_true)))
+print(
+    "Done in {:.2f}s. V-measure: {:.4f}".format(
+        time() - start_time, v_measure_score(y_cocluster, y_true)
+    )
+)

 print("MiniBatchKMeans...")
 start_time = time()
 y_kmeans = kmeans.fit_predict(X)
-print("Done in {:.2f}s. V-measure: {:.4f}".format(
-    time() - start_time,
-    v_measure_score(y_kmeans, y_true)))
+print(
+    "Done in {:.2f}s. V-measure: {:.4f}".format(
+        time() - start_time, v_measure_score(y_kmeans, y_true)
+    )
+)

-feature_names = vectorizer.get_feature_names()
+feature_names = vectorizer.get_feature_names_out()
 document_names = list(newsgroups.target_names[i] for i in newsgroups.target)


@@ -97,14 +113,14 @@
     rows, cols = cocluster.get_indices(i)
     if not (np.any(rows) and np.any(cols)):
         import sys
+
         return sys.float_info.max
     row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]
     col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]
     # Note: the following is identical to X[rows[:, np.newaxis],
     # cols].sum() but much faster in scipy <= 0.16
     weight = X[rows][:, cols].sum()
-    cut = (X[row_complement][:, cols].sum() +
-           X[rows][:, col_complement].sum())
+    cut = X[row_complement][:, cols].sum() + X[rows][:, col_complement].sum()
     return cut / weight


@@ -116,8 +132,7 @@
     return sorted(d.items(), key=operator.itemgetter(1), reverse=True)


-bicluster_ncuts = list(bicluster_ncut(i)
-                       for i in range(len(newsgroups.target_names)))
+bicluster_ncuts = list(bicluster_ncut(i) for i in range(len(newsgroups.target_names)))
 best_idx = np.argsort(bicluster_ncuts)[:5]

 print()
@@ -133,20 +148,24 @@
     counter = defaultdict(int)
     for i in cluster_docs:
         counter[document_names[i]] += 1
-    cat_string = ", ".join("{:.0f}% {}".format(float(c) / n_rows * 100, name)
-                           for name, c in most_common(counter)[:3])
+    cat_string = ", ".join(
+        "{:.0f}% {}".format(float(c) / n_rows * 100, name)
+        for name, c in most_common(counter)[:3]
+    )

     # words
     out_of_cluster_docs = cocluster.row_labels_ != cluster
     out_of_cluster_docs = np.where(out_of_cluster_docs)[0]
     word_col = X[:, cluster_words]
-    word_scores = np.array(word_col[cluster_docs, :].sum(axis=0) -
-                           word_col[out_of_cluster_docs, :].sum(axis=0))
+    word_scores = np.array(
+        word_col[cluster_docs, :].sum(axis=0)
+        - word_col[out_of_cluster_docs, :].sum(axis=0)
+    )
     word_scores = word_scores.ravel()
-    important_words = list(feature_names[cluster_words[i]]
-                           for i in word_scores.argsort()[:-11:-1])
+    important_words = list(
+        feature_names[cluster_words[i]] for i in word_scores.argsort()[:-11:-1]
+    )

-    print("bicluster {} : {} documents, {} words".format(
-        idx, n_rows, n_cols))
+    print("bicluster {} : {} documents, {} words".format(idx, n_rows, n_cols))
     print("categories   : {}".format(cat_string))
-    print("words        : {}\n".format(', '.join(important_words)))
+    print("words        : {}\n".format(", ".join(important_words)))
('examples/bicluster', 'plot_spectral_biclustering.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -15,7 +15,6 @@
 representation of the checkerboard structure.

 """
-print(__doc__)

 # Author: Kemal Eren <kemal@kemaleren.com>
 # License: BSD 3 clause
@@ -24,27 +23,30 @@
 from matplotlib import pyplot as plt

 from sklearn.datasets import make_checkerboard
-from sklearn.datasets import samples_generator as sg
-from sklearn.cluster.bicluster import SpectralBiclustering
+from sklearn.cluster import SpectralBiclustering
 from sklearn.metrics import consensus_score
+

 n_clusters = (4, 3)
 data, rows, columns = make_checkerboard(
-    shape=(300, 300), n_clusters=n_clusters, noise=10,
-    shuffle=False, random_state=0)
+    shape=(300, 300), n_clusters=n_clusters, noise=10, shuffle=False, random_state=0
+)

 plt.matshow(data, cmap=plt.cm.Blues)
 plt.title("Original dataset")

-data, row_idx, col_idx = sg._shuffle(data, random_state=0)
+# shuffle clusters
+rng = np.random.RandomState(0)
+row_idx = rng.permutation(data.shape[0])
+col_idx = rng.permutation(data.shape[1])
+data = data[row_idx][:, col_idx]
+
 plt.matshow(data, cmap=plt.cm.Blues)
 plt.title("Shuffled dataset")

-model = SpectralBiclustering(n_clusters=n_clusters, method='log',
-                             random_state=0)
+model = SpectralBiclustering(n_clusters=n_clusters, method="log", random_state=0)
 model.fit(data)
-score = consensus_score(model.biclusters_,
-                        (rows[:, row_idx], columns[:, col_idx]))
+score = consensus_score(model.biclusters_, (rows[:, row_idx], columns[:, col_idx]))

 print("consensus score: {:.1f}".format(score))

@@ -54,9 +56,10 @@
 plt.matshow(fit_data, cmap=plt.cm.Blues)
 plt.title("After biclustering; rearranged to show biclusters")

-plt.matshow(np.outer(np.sort(model.row_labels_) + 1,
-                     np.sort(model.column_labels_) + 1),
-            cmap=plt.cm.Blues)
+plt.matshow(
+    np.outer(np.sort(model.row_labels_) + 1, np.sort(model.column_labels_) + 1),
+    cmap=plt.cm.Blues,
+)
 plt.title("Checkerboard structure of rearranged data")

 plt.show()
('examples/bicluster', 'plot_spectral_coclustering.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -14,7 +14,6 @@
 the biclusters.

 """
-print(__doc__)

 # Author: Kemal Eren <kemal@kemaleren.com>
 # License: BSD 3 clause
@@ -23,25 +22,28 @@
 from matplotlib import pyplot as plt

 from sklearn.datasets import make_biclusters
-from sklearn.datasets import samples_generator as sg
-from sklearn.cluster.bicluster import SpectralCoclustering
+from sklearn.cluster import SpectralCoclustering
 from sklearn.metrics import consensus_score

 data, rows, columns = make_biclusters(
-    shape=(300, 300), n_clusters=5, noise=5,
-    shuffle=False, random_state=0)
+    shape=(300, 300), n_clusters=5, noise=5, shuffle=False, random_state=0
+)

 plt.matshow(data, cmap=plt.cm.Blues)
 plt.title("Original dataset")

-data, row_idx, col_idx = sg._shuffle(data, random_state=0)
+# shuffle clusters
+rng = np.random.RandomState(0)
+row_idx = rng.permutation(data.shape[0])
+col_idx = rng.permutation(data.shape[1])
+data = data[row_idx][:, col_idx]
+
 plt.matshow(data, cmap=plt.cm.Blues)
 plt.title("Shuffled dataset")

 model = SpectralCoclustering(n_clusters=5, random_state=0)
 model.fit(data)
-score = consensus_score(model.biclusters_,
-                        (rows[:, row_idx], columns[:, col_idx]))
+score = consensus_score(model.biclusters_, (rows[:, row_idx], columns[:, col_idx]))

 print("consensus score: {:.3f}".format(score))

('examples/classification', 'plot_classifier_comparison.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =====================
 Classifier comparison
@@ -19,9 +17,8 @@
 The plots show training points in solid colors and testing points
 semi-transparent. The lower right shows the classification accuracy on the test
 set.
+
 """
-print(__doc__)
-

 # Code source: Gaël Varoquaux
 #              Andreas Müller
@@ -43,12 +40,20 @@
 from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
 from sklearn.naive_bayes import GaussianNB
 from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
+from sklearn.inspection import DecisionBoundaryDisplay

-h = .02  # step size in the mesh
-
-names = ["Nearest Neighbors", "Linear SVM", "RBF SVM", "Gaussian Process",
-         "Decision Tree", "Random Forest", "Neural Net", "AdaBoost",
-         "Naive Bayes", "QDA"]
+names = [
+    "Nearest Neighbors",
+    "Linear SVM",
+    "RBF SVM",
+    "Gaussian Process",
+    "Decision Tree",
+    "Random Forest",
+    "Neural Net",
+    "AdaBoost",
+    "Naive Bayes",
+    "QDA",
+]

 classifiers = [
     KNeighborsClassifier(3),
@@ -60,18 +65,21 @@
     MLPClassifier(alpha=1, max_iter=1000),
     AdaBoostClassifier(),
     GaussianNB(),
-    QuadraticDiscriminantAnalysis()]
+    QuadraticDiscriminantAnalysis(),
+]

-X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
-                           random_state=1, n_clusters_per_class=1)
+X, y = make_classification(
+    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1
+)
 rng = np.random.RandomState(2)
 X += 2 * rng.uniform(size=X.shape)
 linearly_separable = (X, y)

-datasets = [make_moons(noise=0.3, random_state=0),
-            make_circles(noise=0.2, factor=0.5, random_state=1),
-            linearly_separable
-            ]
+datasets = [
+    make_moons(noise=0.3, random_state=0),
+    make_circles(noise=0.2, factor=0.5, random_state=1),
+    linearly_separable,
+]

 figure = plt.figure(figsize=(27, 9))
 i = 1
@@ -80,28 +88,27 @@
     # preprocess dataset, split into training and test part
     X, y = ds
     X = StandardScaler().fit_transform(X)
-    X_train, X_test, y_train, y_test = \
-        train_test_split(X, y, test_size=.4, random_state=42)
+    X_train, X_test, y_train, y_test = train_test_split(
+        X, y, test_size=0.4, random_state=42
+    )

-    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
-    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
-    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
-                         np.arange(y_min, y_max, h))
+    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
+    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5

     # just plot the dataset first
     cm = plt.cm.RdBu
-    cm_bright = ListedColormap(['#FF0000', '#0000FF'])
+    cm_bright = ListedColormap(["#FF0000", "#0000FF"])
     ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
     if ds_cnt == 0:
         ax.set_title("Input data")
     # Plot the training points
-    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
-               edgecolors='k')
+    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors="k")
     # Plot the testing points
-    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,
-               edgecolors='k')
-    ax.set_xlim(xx.min(), xx.max())
-    ax.set_ylim(yy.min(), yy.max())
+    ax.scatter(
+        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors="k"
+    )
+    ax.set_xlim(x_min, x_max)
+    ax.set_ylim(y_min, y_max)
     ax.set_xticks(())
     ax.set_yticks(())
     i += 1
@@ -111,33 +118,37 @@
         ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
         clf.fit(X_train, y_train)
         score = clf.score(X_test, y_test)
-
-        # Plot the decision boundary. For that, we will assign a color to each
-        # point in the mesh [x_min, x_max]x[y_min, y_max].
-        if hasattr(clf, "decision_function"):
-            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
-        else:
-            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
-
-        # Put the result into a color plot
-        Z = Z.reshape(xx.shape)
-        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)
+        DecisionBoundaryDisplay.from_estimator(
+            clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5
+        )

         # Plot the training points
-        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
-                   edgecolors='k')
+        ax.scatter(
+            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors="k"
+        )
         # Plot the testing points
-        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
-                   edgecolors='k', alpha=0.6)
+        ax.scatter(
+            X_test[:, 0],
+            X_test[:, 1],
+            c=y_test,
+            cmap=cm_bright,
+            edgecolors="k",
+            alpha=0.6,
+        )

-        ax.set_xlim(xx.min(), xx.max())
-        ax.set_ylim(yy.min(), yy.max())
+        ax.set_xlim(x_min, x_max)
+        ax.set_ylim(y_min, y_max)
         ax.set_xticks(())
         ax.set_yticks(())
         if ds_cnt == 0:
             ax.set_title(name)
-        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),
-                size=15, horizontalalignment='right')
+        ax.text(
+            x_max - 0.3,
+            y_min + 0.3,
+            ("%.2f" % score).lstrip("0"),
+            size=15,
+            horizontalalignment="right",
+        )
         i += 1

 plt.tight_layout()
('examples/classification', 'plot_lda.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,15 +1,19 @@
 """
-====================================================================
-Normal and Shrinkage Linear Discriminant Analysis for classification
-====================================================================
+===========================================================================
+Normal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification
+===========================================================================

-Shows how shrinkage improves classification.
+This example illustrates how the Ledoit-Wolf and Oracle Shrinkage
+Approximating (OAS) estimators of covariance can improve classification.
+
 """
+
 import numpy as np
 import matplotlib.pyplot as plt

 from sklearn.datasets import make_blobs
 from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
+from sklearn.covariance import OAS


 n_train = 20  # samples for training
@@ -35,34 +39,63 @@
         X = np.hstack([X, np.random.randn(n_samples, n_features - 1)])
     return X, y

-acc_clf1, acc_clf2 = [], []
+
+acc_clf1, acc_clf2, acc_clf3 = [], [], []
 n_features_range = range(1, n_features_max + 1, step)
 for n_features in n_features_range:
-    score_clf1, score_clf2 = 0, 0
+    score_clf1, score_clf2, score_clf3 = 0, 0, 0
     for _ in range(n_averages):
         X, y = generate_data(n_train, n_features)

-        clf1 = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto').fit(X, y)
-        clf2 = LinearDiscriminantAnalysis(solver='lsqr', shrinkage=None).fit(X, y)
+        clf1 = LinearDiscriminantAnalysis(solver="lsqr", shrinkage="auto").fit(X, y)
+        clf2 = LinearDiscriminantAnalysis(solver="lsqr", shrinkage=None).fit(X, y)
+        oa = OAS(store_precision=False, assume_centered=False)
+        clf3 = LinearDiscriminantAnalysis(solver="lsqr", covariance_estimator=oa).fit(
+            X, y
+        )

         X, y = generate_data(n_test, n_features)
         score_clf1 += clf1.score(X, y)
         score_clf2 += clf2.score(X, y)
+        score_clf3 += clf3.score(X, y)

     acc_clf1.append(score_clf1 / n_averages)
     acc_clf2.append(score_clf2 / n_averages)
+    acc_clf3.append(score_clf3 / n_averages)

 features_samples_ratio = np.array(n_features_range) / n_train

-plt.plot(features_samples_ratio, acc_clf1, linewidth=2,
-         label="Linear Discriminant Analysis with shrinkage", color='navy')
-plt.plot(features_samples_ratio, acc_clf2, linewidth=2,
-         label="Linear Discriminant Analysis", color='gold')
+plt.plot(
+    features_samples_ratio,
+    acc_clf1,
+    linewidth=2,
+    label="Linear Discriminant Analysis with Ledoit Wolf",
+    color="navy",
+)
+plt.plot(
+    features_samples_ratio,
+    acc_clf2,
+    linewidth=2,
+    label="Linear Discriminant Analysis",
+    color="gold",
+)
+plt.plot(
+    features_samples_ratio,
+    acc_clf3,
+    linewidth=2,
+    label="Linear Discriminant Analysis with OAS",
+    color="red",
+)

-plt.xlabel('n_features / n_samples')
-plt.ylabel('Classification accuracy')
+plt.xlabel("n_features / n_samples")
+plt.ylabel("Classification accuracy")

-plt.legend(loc=1, prop={'size': 12})
-plt.suptitle('Linear Discriminant Analysis vs. \
-shrinkage Linear Discriminant Analysis (1 discriminative feature)')
+plt.legend(loc=3, prop={"size": 12})
+plt.suptitle(
+    "Linear Discriminant Analysis vs. "
+    + "\n"
+    + "Shrinkage Linear Discriminant Analysis vs. "
+    + "\n"
+    + "OAS Linear Discriminant Analysis (1 discriminative feature)"
+)
 plt.show()
('examples/classification', 'plot_lda_qda.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,97 +8,123 @@
 the double standard deviation for each class. With LDA, the
 standard deviation is the same for all the classes, while each
 class has its own standard deviation with QDA.
+
 """
-print(__doc__)

-from scipy import linalg
-import numpy as np
+# %%
+# Colormap
+# --------
+
 import matplotlib.pyplot as plt
 import matplotlib as mpl
 from matplotlib import colors

-from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
-from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
-
-# #############################################################################
-# Colormap
 cmap = colors.LinearSegmentedColormap(
-    'red_blue_classes',
-    {'red': [(0, 1, 1), (1, 0.7, 0.7)],
-     'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],
-     'blue': [(0, 0.7, 0.7), (1, 1, 1)]})
+    "red_blue_classes",
+    {
+        "red": [(0, 1, 1), (1, 0.7, 0.7)],
+        "green": [(0, 0.7, 0.7), (1, 0.7, 0.7)],
+        "blue": [(0, 0.7, 0.7), (1, 1, 1)],
+    },
+)
 plt.cm.register_cmap(cmap=cmap)


-# #############################################################################
-# Generate datasets
+# %%
+# Datasets generation functions
+# -----------------------------
+
+import numpy as np
+
+
 def dataset_fixed_cov():
-    '''Generate 2 Gaussians samples with the same covariance matrix'''
+    """Generate 2 Gaussians samples with the same covariance matrix"""
     n, dim = 300, 2
     np.random.seed(0)
-    C = np.array([[0., -0.23], [0.83, .23]])
-    X = np.r_[np.dot(np.random.randn(n, dim), C),
-              np.dot(np.random.randn(n, dim), C) + np.array([1, 1])]
+    C = np.array([[0.0, -0.23], [0.83, 0.23]])
+    X = np.r_[
+        np.dot(np.random.randn(n, dim), C),
+        np.dot(np.random.randn(n, dim), C) + np.array([1, 1]),
+    ]
     y = np.hstack((np.zeros(n), np.ones(n)))
     return X, y


 def dataset_cov():
-    '''Generate 2 Gaussians samples with different covariance matrices'''
+    """Generate 2 Gaussians samples with different covariance matrices"""
     n, dim = 300, 2
     np.random.seed(0)
-    C = np.array([[0., -1.], [2.5, .7]]) * 2.
-    X = np.r_[np.dot(np.random.randn(n, dim), C),
-              np.dot(np.random.randn(n, dim), C.T) + np.array([1, 4])]
+    C = np.array([[0.0, -1.0], [2.5, 0.7]]) * 2.0
+    X = np.r_[
+        np.dot(np.random.randn(n, dim), C),
+        np.dot(np.random.randn(n, dim), C.T) + np.array([1, 4]),
+    ]
     y = np.hstack((np.zeros(n), np.ones(n)))
     return X, y


-# #############################################################################
+# %%
 # Plot functions
+# --------------
+
+from scipy import linalg
+
+
 def plot_data(lda, X, y, y_pred, fig_index):
     splot = plt.subplot(2, 2, fig_index)
     if fig_index == 1:
-        plt.title('Linear Discriminant Analysis')
-        plt.ylabel('Data with\n fixed covariance')
+        plt.title("Linear Discriminant Analysis")
+        plt.ylabel("Data with\n fixed covariance")
     elif fig_index == 2:
-        plt.title('Quadratic Discriminant Analysis')
+        plt.title("Quadratic Discriminant Analysis")
     elif fig_index == 3:
-        plt.ylabel('Data with\n varying covariances')
+        plt.ylabel("Data with\n varying covariances")

-    tp = (y == y_pred)  # True Positive
+    tp = y == y_pred  # True Positive
     tp0, tp1 = tp[y == 0], tp[y == 1]
     X0, X1 = X[y == 0], X[y == 1]
     X0_tp, X0_fp = X0[tp0], X0[~tp0]
     X1_tp, X1_fp = X1[tp1], X1[~tp1]

     # class 0: dots
-    plt.scatter(X0_tp[:, 0], X0_tp[:, 1], marker='.', color='red')
-    plt.scatter(X0_fp[:, 0], X0_fp[:, 1], marker='x',
-                s=20, color='#990000')  # dark red
+    plt.scatter(X0_tp[:, 0], X0_tp[:, 1], marker=".", color="red")
+    plt.scatter(X0_fp[:, 0], X0_fp[:, 1], marker="x", s=20, color="#990000")  # dark red

     # class 1: dots
-    plt.scatter(X1_tp[:, 0], X1_tp[:, 1], marker='.', color='blue')
-    plt.scatter(X1_fp[:, 0], X1_fp[:, 1], marker='x',
-                s=20, color='#000099')  # dark blue
+    plt.scatter(X1_tp[:, 0], X1_tp[:, 1], marker=".", color="blue")
+    plt.scatter(
+        X1_fp[:, 0], X1_fp[:, 1], marker="x", s=20, color="#000099"
+    )  # dark blue

     # class 0 and 1 : areas
     nx, ny = 200, 100
     x_min, x_max = plt.xlim()
     y_min, y_max = plt.ylim()
-    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),
-                         np.linspace(y_min, y_max, ny))
+    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx), np.linspace(y_min, y_max, ny))
     Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])
     Z = Z[:, 1].reshape(xx.shape)
-    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',
-                   norm=colors.Normalize(0., 1.), zorder=0)
-    plt.contour(xx, yy, Z, [0.5], linewidths=2., colors='white')
+    plt.pcolormesh(
+        xx, yy, Z, cmap="red_blue_classes", norm=colors.Normalize(0.0, 1.0), zorder=0
+    )
+    plt.contour(xx, yy, Z, [0.5], linewidths=2.0, colors="white")

     # means
-    plt.plot(lda.means_[0][0], lda.means_[0][1],
-             '*', color='yellow', markersize=15, markeredgecolor='grey')
-    plt.plot(lda.means_[1][0], lda.means_[1][1],
-             '*', color='yellow', markersize=15, markeredgecolor='grey')
+    plt.plot(
+        lda.means_[0][0],
+        lda.means_[0][1],
+        "*",
+        color="yellow",
+        markersize=15,
+        markeredgecolor="grey",
+    )
+    plt.plot(
+        lda.means_[1][0],
+        lda.means_[1][1],
+        "*",
+        color="yellow",
+        markersize=15,
+        markeredgecolor="grey",
+    )

     return splot

@@ -109,9 +135,15 @@
     angle = np.arctan(u[1] / u[0])
     angle = 180 * angle / np.pi  # convert to degrees
     # filled Gaussian at 2 standard deviation
-    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,
-                              180 + angle, facecolor=color,
-                              edgecolor='black', linewidth=2)
+    ell = mpl.patches.Ellipse(
+        mean,
+        2 * v[0] ** 0.5,
+        2 * v[1] ** 0.5,
+        180 + angle,
+        facecolor=color,
+        edgecolor="black",
+        linewidth=2,
+    )
     ell.set_clip_box(splot.bbox)
     ell.set_alpha(0.2)
     splot.add_artist(ell)
@@ -120,31 +152,44 @@


 def plot_lda_cov(lda, splot):
-    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')
-    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')
+    plot_ellipse(splot, lda.means_[0], lda.covariance_, "red")
+    plot_ellipse(splot, lda.means_[1], lda.covariance_, "blue")


 def plot_qda_cov(qda, splot):
-    plot_ellipse(splot, qda.means_[0], qda.covariance_[0], 'red')
-    plot_ellipse(splot, qda.means_[1], qda.covariance_[1], 'blue')
+    plot_ellipse(splot, qda.means_[0], qda.covariance_[0], "red")
+    plot_ellipse(splot, qda.means_[1], qda.covariance_[1], "blue")


-plt.figure(figsize=(10, 8), facecolor='white')
+# %%
+# Plot
+# ----
+
+plt.figure(figsize=(10, 8), facecolor="white")
+plt.suptitle(
+    "Linear Discriminant Analysis vs Quadratic Discriminant Analysis",
+    y=0.98,
+    fontsize=15,
+)
+
+from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
+from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
+
 for i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):
     # Linear Discriminant Analysis
     lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True)
     y_pred = lda.fit(X, y).predict(X)
     splot = plot_data(lda, X, y, y_pred, fig_index=2 * i + 1)
     plot_lda_cov(lda, splot)
-    plt.axis('tight')
+    plt.axis("tight")

     # Quadratic Discriminant Analysis
     qda = QuadraticDiscriminantAnalysis(store_covariance=True)
     y_pred = qda.fit(X, y).predict(X)
     splot = plot_data(qda, X, y, y_pred, fig_index=2 * i + 2)
     plot_qda_cov(qda, splot)
-    plt.axis('tight')
-plt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant Analysis',
-             y=1.02, fontsize=15)
+    plt.axis("tight")
+
 plt.tight_layout()
+plt.subplots_adjust(top=0.92)
 plt.show()
('examples/classification', 'plot_classification_probability.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -14,8 +14,8 @@
 The logistic regression with One-Vs-Rest is not a multiclass classifier out of
 the box. As a result it has more trouble in separating class 2 and 3 than the
 other estimators.
+
 """
-print(__doc__)

 # Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
 # License: BSD 3 clause
@@ -41,27 +41,23 @@

 # Create different classifiers.
 classifiers = {
-    'L1 logistic': LogisticRegression(C=C, penalty='l1',
-                                      solver='saga',
-                                      multi_class='multinomial',
-                                      max_iter=10000),
-    'L2 logistic (Multinomial)': LogisticRegression(C=C, penalty='l2',
-                                                    solver='saga',
-                                                    multi_class='multinomial',
-                                                    max_iter=10000),
-    'L2 logistic (OvR)': LogisticRegression(C=C, penalty='l2',
-                                            solver='saga',
-                                            multi_class='ovr',
-                                            max_iter=10000),
-    'Linear SVC': SVC(kernel='linear', C=C, probability=True,
-                      random_state=0),
-    'GPC': GaussianProcessClassifier(kernel)
+    "L1 logistic": LogisticRegression(
+        C=C, penalty="l1", solver="saga", multi_class="multinomial", max_iter=10000
+    ),
+    "L2 logistic (Multinomial)": LogisticRegression(
+        C=C, penalty="l2", solver="saga", multi_class="multinomial", max_iter=10000
+    ),
+    "L2 logistic (OvR)": LogisticRegression(
+        C=C, penalty="l2", solver="saga", multi_class="ovr", max_iter=10000
+    ),
+    "Linear SVC": SVC(kernel="linear", C=C, probability=True, random_state=0),
+    "GPC": GaussianProcessClassifier(kernel),
 }

 n_classifiers = len(classifiers)

 plt.figure(figsize=(3 * 2, n_classifiers * 2))
-plt.subplots_adjust(bottom=.2, top=.95)
+plt.subplots_adjust(bottom=0.2, top=0.95)

 xx = np.linspace(3, 9, 100)
 yy = np.linspace(1, 5, 100).T
@@ -83,16 +79,17 @@
         plt.title("Class %d" % k)
         if k == 0:
             plt.ylabel(name)
-        imshow_handle = plt.imshow(probas[:, k].reshape((100, 100)),
-                                   extent=(3, 9, 1, 5), origin='lower')
+        imshow_handle = plt.imshow(
+            probas[:, k].reshape((100, 100)), extent=(3, 9, 1, 5), origin="lower"
+        )
         plt.xticks(())
         plt.yticks(())
-        idx = (y_pred == k)
+        idx = y_pred == k
         if idx.any():
-            plt.scatter(X[idx, 0], X[idx, 1], marker='o', c='w', edgecolor='k')
+            plt.scatter(X[idx, 0], X[idx, 1], marker="o", c="w", edgecolor="k")

 ax = plt.axes([0.15, 0.04, 0.7, 0.05])
 plt.title("Probability")
-plt.colorbar(imshow_handle, cax=ax, orientation='horizontal')
+plt.colorbar(imshow_handle, cax=ax, orientation="horizontal")

 plt.show()
('examples/classification', 'plot_digits_classification.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,14 +3,10 @@
 Recognizing hand-written digits
 ================================

-An example showing how the scikit-learn can be used to recognize images of
-hand-written digits.
-
-This example is commented in the
-:ref:`tutorial section of the user manual <introduction>`.
+This example shows how scikit-learn can be used to recognize images of
+hand-written digits, from 0-9.

 """
-print(__doc__)

 # Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>
 # License: BSD 3 clause
@@ -20,47 +16,89 @@

 # Import datasets, classifiers and performance metrics
 from sklearn import datasets, svm, metrics
+from sklearn.model_selection import train_test_split

-# The digits dataset
+###############################################################################
+# Digits dataset
+# --------------
+#
+# The digits dataset consists of 8x8
+# pixel images of digits. The ``images`` attribute of the dataset stores
+# 8x8 arrays of grayscale values for each image. We will use these arrays to
+# visualize the first 4 images. The ``target`` attribute of the dataset stores
+# the digit each image represents and this is included in the title of the 4
+# plots below.
+#
+# Note: if we were working from image files (e.g., 'png' files), we would load
+# them using :func:`matplotlib.pyplot.imread`.
+
 digits = datasets.load_digits()

-# The data that we are interested in is made of 8x8 images of digits, let's
-# have a look at the first 4 images, stored in the `images` attribute of the
-# dataset.  If we were working from image files, we could load them using
-# matplotlib.pyplot.imread.  Note that each image must have the same size. For these
-# images, we know which digit they represent: it is given in the 'target' of
-# the dataset.
-images_and_labels = list(zip(digits.images, digits.target))
-for index, (image, label) in enumerate(images_and_labels[:4]):
-    plt.subplot(2, 4, index + 1)
-    plt.axis('off')
-    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
-    plt.title('Training: %i' % label)
+_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))
+for ax, image, label in zip(axes, digits.images, digits.target):
+    ax.set_axis_off()
+    ax.imshow(image, cmap=plt.cm.gray_r, interpolation="nearest")
+    ax.set_title("Training: %i" % label)

-# To apply a classifier on this data, we need to flatten the image, to
-# turn the data in a (samples, feature) matrix:
+###############################################################################
+# Classification
+# --------------
+#
+# To apply a classifier on this data, we need to flatten the images, turning
+# each 2-D array of grayscale values from shape ``(8, 8)`` into shape
+# ``(64,)``. Subsequently, the entire dataset will be of shape
+# ``(n_samples, n_features)``, where ``n_samples`` is the number of images and
+# ``n_features`` is the total number of pixels in each image.
+#
+# We can then split the data into train and test subsets and fit a support
+# vector classifier on the train samples. The fitted classifier can
+# subsequently be used to predict the value of the digit for the samples
+# in the test subset.
+
+# flatten the images
 n_samples = len(digits.images)
 data = digits.images.reshape((n_samples, -1))

 # Create a classifier: a support vector classifier
-classifier = svm.SVC(gamma=0.001)
+clf = svm.SVC(gamma=0.001)

-# We learn the digits on the first half of the digits
-classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])
+# Split data into 50% train and 50% test subsets
+X_train, X_test, y_train, y_test = train_test_split(
+    data, digits.target, test_size=0.5, shuffle=False
+)

-# Now predict the value of the digit on the second half:
-expected = digits.target[n_samples // 2:]
-predicted = classifier.predict(data[n_samples // 2:])
+# Learn the digits on the train subset
+clf.fit(X_train, y_train)

-print("Classification report for classifier %s:\n%s\n"
-      % (classifier, metrics.classification_report(expected, predicted)))
-print("Confusion matrix:\n%s" % metrics.confusion_matrix(expected, predicted))
+# Predict the value of the digit on the test subset
+predicted = clf.predict(X_test)

-images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))
-for index, (image, prediction) in enumerate(images_and_predictions[:4]):
-    plt.subplot(2, 4, index + 5)
-    plt.axis('off')
-    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
-    plt.title('Prediction: %i' % prediction)
+###############################################################################
+# Below we visualize the first 4 test samples and show their predicted
+# digit value in the title.
+
+_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))
+for ax, image, prediction in zip(axes, X_test, predicted):
+    ax.set_axis_off()
+    image = image.reshape(8, 8)
+    ax.imshow(image, cmap=plt.cm.gray_r, interpolation="nearest")
+    ax.set_title(f"Prediction: {prediction}")
+
+###############################################################################
+# :func:`~sklearn.metrics.classification_report` builds a text report showing
+# the main classification metrics.
+
+print(
+    f"Classification report for classifier {clf}:\n"
+    f"{metrics.classification_report(y_test, predicted)}\n"
+)
+
+###############################################################################
+# We can also plot a :ref:`confusion matrix <confusion_matrix>` of the
+# true digit values and the predicted digit values.
+
+disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, predicted)
+disp.figure_.suptitle("Confusion Matrix")
+print(f"Confusion matrix:\n{disp.confusion_matrix}")

 plt.show()
('examples/tree', 'plot_unveil_tree_structure.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -15,120 +15,180 @@
 - the decision path shared by a group of samples.

 """
+
 import numpy as np
+from matplotlib import pyplot as plt

 from sklearn.model_selection import train_test_split
 from sklearn.datasets import load_iris
 from sklearn.tree import DecisionTreeClassifier
+from sklearn import tree
+
+##############################################################################
+# Train tree classifier
+# ---------------------
+# First, we fit a :class:`~sklearn.tree.DecisionTreeClassifier` using the
+# :func:`~sklearn.datasets.load_iris` dataset.

 iris = load_iris()
 X = iris.data
 y = iris.target
 X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

-estimator = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)
-estimator.fit(X_train, y_train)
+clf = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)
+clf.fit(X_train, y_train)

-# The decision estimator has an attribute called tree_  which stores the entire
-# tree structure and allows access to low level attributes. The binary tree
-# tree_ is represented as a number of parallel arrays. The i-th element of each
-# array holds information about the node `i`. Node 0 is the tree's root. NOTE:
-# Some of the arrays only apply to either leaves or split nodes, resp. In this
-# case the values of nodes of the other type are arbitrary!
+##############################################################################
+# Tree structure
+# --------------
 #
-# Among those arrays, we have:
-#   - left_child, id of the left child of the node
-#   - right_child, id of the right child of the node
-#   - feature, feature used for splitting the node
-#   - threshold, threshold value at the node
+# The decision classifier has an attribute called ``tree_`` which allows access
+# to low level attributes such as ``node_count``, the total number of nodes,
+# and ``max_depth``, the maximal depth of the tree. It also stores the
+# entire binary tree structure, represented as a number of parallel arrays. The
+# i-th element of each array holds information about the node ``i``. Node 0 is
+# the tree's root. Some of the arrays only apply to either leaves or split
+# nodes. In this case the values of the nodes of the other type is arbitrary.
+# For example, the arrays ``feature`` and ``threshold`` only apply to split
+# nodes. The values for leaf nodes in these arrays are therefore arbitrary.
 #
+# Among these arrays, we have:
+#
+#   - ``children_left[i]``: id of the left child of node ``i`` or -1 if leaf
+#     node
+#   - ``children_right[i]``: id of the right child of node ``i`` or -1 if leaf
+#     node
+#   - ``feature[i]``: feature used for splitting node ``i``
+#   - ``threshold[i]``: threshold value at node ``i``
+#   - ``n_node_samples[i]``: the number of of training samples reaching node
+#     ``i``
+#   - ``impurity[i]``: the impurity at node ``i``
+#
+# Using the arrays, we can traverse the tree structure to compute various
+# properties. Below, we will compute the depth of each node and whether or not
+# it is a leaf.

-# Using those arrays, we can parse the tree structure:
+n_nodes = clf.tree_.node_count
+children_left = clf.tree_.children_left
+children_right = clf.tree_.children_right
+feature = clf.tree_.feature
+threshold = clf.tree_.threshold

-n_nodes = estimator.tree_.node_count
-children_left = estimator.tree_.children_left
-children_right = estimator.tree_.children_right
-feature = estimator.tree_.feature
-threshold = estimator.tree_.threshold
-
-
-# The tree structure can be traversed to compute various properties such
-# as the depth of each node and whether or not it is a leaf.
 node_depth = np.zeros(shape=n_nodes, dtype=np.int64)
 is_leaves = np.zeros(shape=n_nodes, dtype=bool)
-stack = [(0, -1)]  # seed is the root node id and its parent depth
+stack = [(0, 0)]  # start with the root node id (0) and its depth (0)
 while len(stack) > 0:
-    node_id, parent_depth = stack.pop()
-    node_depth[node_id] = parent_depth + 1
+    # `pop` ensures each node is only visited once
+    node_id, depth = stack.pop()
+    node_depth[node_id] = depth

-    # If we have a test node
-    if (children_left[node_id] != children_right[node_id]):
-        stack.append((children_left[node_id], parent_depth + 1))
-        stack.append((children_right[node_id], parent_depth + 1))
+    # If the left and right child of a node is not the same we have a split
+    # node
+    is_split_node = children_left[node_id] != children_right[node_id]
+    # If a split node, append left and right children and depth to `stack`
+    # so we can loop through them
+    if is_split_node:
+        stack.append((children_left[node_id], depth + 1))
+        stack.append((children_right[node_id], depth + 1))
     else:
         is_leaves[node_id] = True

-print("The binary tree structure has %s nodes and has "
-      "the following tree structure:"
-      % n_nodes)
+print(
+    "The binary tree structure has {n} nodes and has "
+    "the following tree structure:\n".format(n=n_nodes)
+)
 for i in range(n_nodes):
     if is_leaves[i]:
-        print("%snode=%s leaf node." % (node_depth[i] * "\t", i))
+        print(
+            "{space}node={node} is a leaf node.".format(
+                space=node_depth[i] * "\t", node=i
+            )
+        )
     else:
-        print("%snode=%s test node: go to node %s if X[:, %s] <= %s else to "
-              "node %s."
-              % (node_depth[i] * "\t",
-                 i,
-                 children_left[i],
-                 feature[i],
-                 threshold[i],
-                 children_right[i],
-                 ))
-print()
+        print(
+            "{space}node={node} is a split node: "
+            "go to node {left} if X[:, {feature}] <= {threshold} "
+            "else to node {right}.".format(
+                space=node_depth[i] * "\t",
+                node=i,
+                left=children_left[i],
+                feature=feature[i],
+                threshold=threshold[i],
+                right=children_right[i],
+            )
+        )

-# First let's retrieve the decision path of each sample. The decision_path
-# method allows to retrieve the node indicator functions. A non zero element of
-# indicator matrix at the position (i, j) indicates that the sample i goes
-# through the node j.
+##############################################################################
+# We can compare the above output to the plot of the decision tree.

-node_indicator = estimator.decision_path(X_test)
+tree.plot_tree(clf)
+plt.show()

-# Similarly, we can also have the leaves ids reached by each sample.
+##############################################################################
+# Decision path
+# -------------
+#
+# We can also retrieve the decision path of samples of interest. The
+# ``decision_path`` method outputs an indicator matrix that allows us to
+# retrieve the nodes the samples of interest traverse through. A non zero
+# element in the indicator matrix at position ``(i, j)`` indicates that
+# the sample ``i`` goes through the node ``j``. Or, for one sample ``i``, the
+# positions of the non zero elements in row ``i`` of the indicator matrix
+# designate the ids of the nodes that sample goes through.
+#
+# The leaf ids reached by samples of interest can be obtained with the
+# ``apply`` method. This returns an array of the node ids of the leaves
+# reached by each sample of interest. Using the leaf ids and the
+# ``decision_path`` we can obtain the splitting conditions that were used to
+# predict a sample or a group of samples. First, let's do it for one sample.
+# Note that ``node_index`` is a sparse matrix.

-leave_id = estimator.apply(X_test)
-
-# Now, it's possible to get the tests that were used to predict a sample or
-# a group of samples. First, let's make it for the sample.
+node_indicator = clf.decision_path(X_test)
+leaf_id = clf.apply(X_test)

 sample_id = 0
-node_index = node_indicator.indices[node_indicator.indptr[sample_id]:
-                                    node_indicator.indptr[sample_id + 1]]
+# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`
+node_index = node_indicator.indices[
+    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]
+]

-print('Rules used to predict sample %s: ' % sample_id)
+print("Rules used to predict sample {id}:\n".format(id=sample_id))
 for node_id in node_index:
-    if leave_id[sample_id] == node_id:
+    # continue to the next node if it is a leaf node
+    if leaf_id[sample_id] == node_id:
         continue

-    if (X_test[sample_id, feature[node_id]] <= threshold[node_id]):
+    # check if value of the split feature for sample 0 is below threshold
+    if X_test[sample_id, feature[node_id]] <= threshold[node_id]:
         threshold_sign = "<="
     else:
         threshold_sign = ">"

-    print("decision id node %s : (X_test[%s, %s] (= %s) %s %s)"
-          % (node_id,
-             sample_id,
-             feature[node_id],
-             X_test[sample_id, feature[node_id]],
-             threshold_sign,
-             threshold[node_id]))
+    print(
+        "decision node {node} : (X_test[{sample}, {feature}] = {value}) "
+        "{inequality} {threshold})".format(
+            node=node_id,
+            sample=sample_id,
+            feature=feature[node_id],
+            value=X_test[sample_id, feature[node_id]],
+            inequality=threshold_sign,
+            threshold=threshold[node_id],
+        )
+    )

-# For a group of samples, we have the following common node.
+##############################################################################
+# For a group of samples, we can determine the common nodes the samples go
+# through.
+
 sample_ids = [0, 1]
-common_nodes = (node_indicator.toarray()[sample_ids].sum(axis=0) ==
-                len(sample_ids))
-
+# boolean array indicating the nodes both samples go through
+common_nodes = node_indicator.toarray()[sample_ids].sum(axis=0) == len(sample_ids)
+# obtain node ids using position in array
 common_node_id = np.arange(n_nodes)[common_nodes]

-print("\nThe following samples %s share the node %s in the tree"
-      % (sample_ids, common_node_id))
-print("It is %s %% of all nodes." % (100 * len(common_node_id) / n_nodes,))
+print(
+    "\nThe following samples {samples} share the node(s) {nodes} in the tree.".format(
+        samples=sample_ids, nodes=common_node_id
+    )
+)
+print("This is {prop}% of all nodes.".format(prop=100 * len(common_node_id) / n_nodes))
('examples/tree', 'plot_iris_dtc.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,7 +1,7 @@
 """
-================================================================
-Plot the decision surface of a decision tree on the iris dataset
-================================================================
+=======================================================================
+Plot the decision surface of decision trees trained on the iris dataset
+=======================================================================

 Plot the decision surface of a decision tree trained on pairs
 of features of the iris dataset.
@@ -14,24 +14,30 @@

 We also show the tree structure of a model built on all of the features.
 """
-print(__doc__)
+# %%
+# First load the copy of the Iris dataset shipped with scikit-learn:
+from sklearn.datasets import load_iris

+iris = load_iris()
+
+
+# %%
+# Display the decision functions of trees trained on all pairs of features.
 import numpy as np
 import matplotlib.pyplot as plt

 from sklearn.datasets import load_iris
-from sklearn.tree import DecisionTreeClassifier, plot_tree
+from sklearn.tree import DecisionTreeClassifier
+from sklearn.inspection import DecisionBoundaryDisplay
+

 # Parameters
 n_classes = 3
 plot_colors = "ryb"
 plot_step = 0.02

-# Load data
-iris = load_iris()

-for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],
-                                [1, 2], [1, 3], [2, 3]]):
+for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]):
     # We only take the two corresponding features
     X = iris.data[:, pair]
     y = iris.target
@@ -40,32 +46,42 @@
     clf = DecisionTreeClassifier().fit(X, y)

     # Plot the decision boundary
-    plt.subplot(2, 3, pairidx + 1)
-
-    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
-    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
-    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
-                         np.arange(y_min, y_max, plot_step))
+    ax = plt.subplot(2, 3, pairidx + 1)
     plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)
-
-    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
-    Z = Z.reshape(xx.shape)
-    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)
-
-    plt.xlabel(iris.feature_names[pair[0]])
-    plt.ylabel(iris.feature_names[pair[1]])
+    DecisionBoundaryDisplay.from_estimator(
+        clf,
+        X,
+        cmap=plt.cm.RdYlBu,
+        response_method="predict",
+        ax=ax,
+        xlabel=iris.feature_names[pair[0]],
+        ylabel=iris.feature_names[pair[1]],
+    )

     # Plot the training points
     for i, color in zip(range(n_classes), plot_colors):
         idx = np.where(y == i)
-        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],
-                    cmap=plt.cm.RdYlBu, edgecolor='black', s=15)
+        plt.scatter(
+            X[idx, 0],
+            X[idx, 1],
+            c=color,
+            label=iris.target_names[i],
+            cmap=plt.cm.RdYlBu,
+            edgecolor="black",
+            s=15,
+        )

-plt.suptitle("Decision surface of a decision tree using paired features")
-plt.legend(loc='lower right', borderpad=0, handletextpad=0)
-plt.axis("tight")
+plt.suptitle("Decision surface of decision trees trained on pairs of features")
+plt.legend(loc="lower right", borderpad=0, handletextpad=0)
+_ = plt.axis("tight")
+
+# %%
+# Display the structure of a single decision tree trained on all the features
+# together.
+from sklearn.tree import plot_tree

 plt.figure()
 clf = DecisionTreeClassifier().fit(iris.data, iris.target)
 plot_tree(clf, filled=True)
+plt.title("Decision tree trained on all the iris features")
 plt.show()
('examples/tree', 'plot_tree_regression_multioutput.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -14,7 +14,6 @@
 `max_depth` parameter) is set too high, the decision trees learn too fine
 details of the training data and learn from the noise, i.e. they overfit.
 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -24,7 +23,7 @@
 rng = np.random.RandomState(1)
 X = np.sort(200 * rng.rand(100, 1) - 100, axis=0)
 y = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T
-y[::5, :] += (0.5 - rng.rand(20, 2))
+y[::5, :] += 0.5 - rng.rand(20, 2)

 # Fit regression model
 regr_1 = DecisionTreeRegressor(max_depth=2)
@@ -43,14 +42,19 @@
 # Plot the results
 plt.figure()
 s = 25
-plt.scatter(y[:, 0], y[:, 1], c="navy", s=s,
-            edgecolor="black", label="data")
-plt.scatter(y_1[:, 0], y_1[:, 1], c="cornflowerblue", s=s,
-            edgecolor="black", label="max_depth=2")
-plt.scatter(y_2[:, 0], y_2[:, 1], c="red", s=s,
-            edgecolor="black", label="max_depth=5")
-plt.scatter(y_3[:, 0], y_3[:, 1], c="orange", s=s,
-            edgecolor="black", label="max_depth=8")
+plt.scatter(y[:, 0], y[:, 1], c="navy", s=s, edgecolor="black", label="data")
+plt.scatter(
+    y_1[:, 0],
+    y_1[:, 1],
+    c="cornflowerblue",
+    s=s,
+    edgecolor="black",
+    label="max_depth=2",
+)
+plt.scatter(y_2[:, 0], y_2[:, 1], c="red", s=s, edgecolor="black", label="max_depth=5")
+plt.scatter(
+    y_3[:, 0], y_3[:, 1], c="orange", s=s, edgecolor="black", label="max_depth=8"
+)
 plt.xlim([-6, 6])
 plt.ylim([-6, 6])
 plt.xlabel("target 1")
('examples/tree', 'plot_tree_regression.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -13,7 +13,6 @@
 `max_depth` parameter) is set too high, the decision trees learn too fine
 details of the training data and learn from the noise, i.e. they overfit.
 """
-print(__doc__)

 # Import the necessary modules and libraries
 import numpy as np
@@ -39,10 +38,8 @@

 # Plot the results
 plt.figure()
-plt.scatter(X, y, s=20, edgecolor="black",
-            c="darkorange", label="data")
-plt.plot(X_test, y_1, color="cornflowerblue",
-         label="max_depth=2", linewidth=2)
+plt.scatter(X, y, s=20, edgecolor="black", c="darkorange", label="data")
+plt.plot(X_test, y_1, color="cornflowerblue", label="max_depth=2", linewidth=2)
 plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=5", linewidth=2)
 plt.xlabel("data")
 plt.ylabel("target")
('examples/ensemble', 'plot_forest_importances.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,54 +1,116 @@
 """
-=========================================
-Feature importances with forests of trees
-=========================================
+==========================================
+Feature importances with a forest of trees
+==========================================

-This examples shows the use of forests of trees to evaluate the importance of
-features on an artificial classification task. The red bars are the feature
-importances of the forest, along with their inter-trees variability.
+This example shows the use of a forest of trees to evaluate the importance of
+features on an artificial classification task. The blue bars are the feature
+importances of the forest, along with their inter-trees variability represented
+by the error bars.

 As expected, the plot suggests that 3 features are informative, while the
 remaining are not.
+
 """
-print(__doc__)

-import numpy as np
 import matplotlib.pyplot as plt

+# %%
+# Data generation and model fitting
+# ---------------------------------
+# We generate a synthetic dataset with only 3 informative features. We will
+# explicitly not shuffle the dataset to ensure that the informative features
+# will correspond to the three first columns of X. In addition, we will split
+# our dataset into training and testing subsets.
 from sklearn.datasets import make_classification
-from sklearn.ensemble import ExtraTreesClassifier
+from sklearn.model_selection import train_test_split

-# Build a classification task using 3 informative features
-X, y = make_classification(n_samples=1000,
-                           n_features=10,
-                           n_informative=3,
-                           n_redundant=0,
-                           n_repeated=0,
-                           n_classes=2,
-                           random_state=0,
-                           shuffle=False)
+X, y = make_classification(
+    n_samples=1000,
+    n_features=10,
+    n_informative=3,
+    n_redundant=0,
+    n_repeated=0,
+    n_classes=2,
+    random_state=0,
+    shuffle=False,
+)
+X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

-# Build a forest and compute the feature importances
-forest = ExtraTreesClassifier(n_estimators=250,
-                              random_state=0)
+# %%
+# A random forest classifier will be fitted to compute the feature importances.
+from sklearn.ensemble import RandomForestClassifier

-forest.fit(X, y)
+feature_names = [f"feature {i}" for i in range(X.shape[1])]
+forest = RandomForestClassifier(random_state=0)
+forest.fit(X_train, y_train)
+
+# %%
+# Feature importance based on mean decrease in impurity
+# -----------------------------------------------------
+# Feature importances are provided by the fitted attribute
+# `feature_importances_` and they are computed as the mean and standard
+# deviation of accumulation of the impurity decrease within each tree.
+#
+# .. warning::
+#     Impurity-based feature importances can be misleading for **high
+#     cardinality** features (many unique values). See
+#     :ref:`permutation_importance` as an alternative below.
+import time
+import numpy as np
+
+start_time = time.time()
 importances = forest.feature_importances_
-std = np.std([tree.feature_importances_ for tree in forest.estimators_],
-             axis=0)
-indices = np.argsort(importances)[::-1]
+std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)
+elapsed_time = time.time() - start_time

-# Print the feature ranking
-print("Feature ranking:")
+print(f"Elapsed time to compute the importances: {elapsed_time:.3f} seconds")

-for f in range(X.shape[1]):
-    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))
+# %%
+# Let's plot the impurity-based importance.
+import pandas as pd

-# Plot the feature importances of the forest
-plt.figure()
-plt.title("Feature importances")
-plt.bar(range(X.shape[1]), importances[indices],
-       color="r", yerr=std[indices], align="center")
-plt.xticks(range(X.shape[1]), indices)
-plt.xlim([-1, X.shape[1]])
+forest_importances = pd.Series(importances, index=feature_names)
+
+fig, ax = plt.subplots()
+forest_importances.plot.bar(yerr=std, ax=ax)
+ax.set_title("Feature importances using MDI")
+ax.set_ylabel("Mean decrease in impurity")
+fig.tight_layout()
+
+# %%
+# We observe that, as expected, the three first features are found important.
+#
+# Feature importance based on feature permutation
+# -----------------------------------------------
+# Permutation feature importance overcomes limitations of the impurity-based
+# feature importance: they do not have a bias toward high-cardinality features
+# and can be computed on a left-out test set.
+from sklearn.inspection import permutation_importance
+
+start_time = time.time()
+result = permutation_importance(
+    forest, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2
+)
+elapsed_time = time.time() - start_time
+print(f"Elapsed time to compute the importances: {elapsed_time:.3f} seconds")
+
+forest_importances = pd.Series(result.importances_mean, index=feature_names)
+
+# %%
+# The computation for full permutation importance is more costly. Features are
+# shuffled n times and the model refitted to estimate the importance of it.
+# Please see :ref:`permutation_importance` for more details. We can now plot
+# the importance ranking.
+
+fig, ax = plt.subplots()
+forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
+ax.set_title("Feature importances using permutation on full model")
+ax.set_ylabel("Mean accuracy decrease")
+fig.tight_layout()
 plt.show()
+
+# %%
+# The same features are detected as most important using both methods. Although
+# the relative importances vary. As seen on the plots, MDI is less likely than
+# permutation importance to fully omit a feature.
('examples/ensemble', 'plot_adaboost_multiclass.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -23,7 +23,6 @@
 .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.

 """
-print(__doc__)

 # Author: Noel Dawe <noel.dawe@gmail.com>
 #
@@ -37,8 +36,9 @@
 from sklearn.tree import DecisionTreeClassifier


-X, y = make_gaussian_quantiles(n_samples=13000, n_features=10,
-                               n_classes=3, random_state=1)
+X, y = make_gaussian_quantiles(
+    n_samples=13000, n_features=10, n_classes=3, random_state=1
+)

 n_split = 3000

@@ -46,15 +46,15 @@
 y_train, y_test = y[:n_split], y[n_split:]

 bdt_real = AdaBoostClassifier(
-    DecisionTreeClassifier(max_depth=2),
-    n_estimators=600,
-    learning_rate=1)
+    DecisionTreeClassifier(max_depth=2), n_estimators=300, learning_rate=1
+)

 bdt_discrete = AdaBoostClassifier(
     DecisionTreeClassifier(max_depth=2),
-    n_estimators=600,
+    n_estimators=300,
     learning_rate=1.5,
-    algorithm="SAMME")
+    algorithm="SAMME",
+)

 bdt_real.fit(X_train, y_train)
 bdt_discrete.fit(X_train, y_train)
@@ -63,11 +63,10 @@
 discrete_test_errors = []

 for real_test_predict, discrete_train_predict in zip(
-        bdt_real.staged_predict(X_test), bdt_discrete.staged_predict(X_test)):
-    real_test_errors.append(
-        1. - accuracy_score(real_test_predict, y_test))
-    discrete_test_errors.append(
-        1. - accuracy_score(discrete_train_predict, y_test))
+    bdt_real.staged_predict(X_test), bdt_discrete.staged_predict(X_test)
+):
+    real_test_errors.append(1.0 - accuracy_score(real_test_predict, y_test))
+    discrete_test_errors.append(1.0 - accuracy_score(discrete_train_predict, y_test))

 n_trees_discrete = len(bdt_discrete)
 n_trees_real = len(bdt_real)
@@ -81,35 +80,41 @@
 plt.figure(figsize=(15, 5))

 plt.subplot(131)
-plt.plot(range(1, n_trees_discrete + 1),
-         discrete_test_errors, c='black', label='SAMME')
-plt.plot(range(1, n_trees_real + 1),
-         real_test_errors, c='black',
-         linestyle='dashed', label='SAMME.R')
+plt.plot(range(1, n_trees_discrete + 1), discrete_test_errors, c="black", label="SAMME")
+plt.plot(
+    range(1, n_trees_real + 1),
+    real_test_errors,
+    c="black",
+    linestyle="dashed",
+    label="SAMME.R",
+)
 plt.legend()
 plt.ylim(0.18, 0.62)
-plt.ylabel('Test Error')
-plt.xlabel('Number of Trees')
+plt.ylabel("Test Error")
+plt.xlabel("Number of Trees")

 plt.subplot(132)
-plt.plot(range(1, n_trees_discrete + 1), discrete_estimator_errors,
-         "b", label='SAMME', alpha=.5)
-plt.plot(range(1, n_trees_real + 1), real_estimator_errors,
-         "r", label='SAMME.R', alpha=.5)
+plt.plot(
+    range(1, n_trees_discrete + 1),
+    discrete_estimator_errors,
+    "b",
+    label="SAMME",
+    alpha=0.5,
+)
+plt.plot(
+    range(1, n_trees_real + 1), real_estimator_errors, "r", label="SAMME.R", alpha=0.5
+)
 plt.legend()
-plt.ylabel('Error')
-plt.xlabel('Number of Trees')
-plt.ylim((.2,
-         max(real_estimator_errors.max(),
-             discrete_estimator_errors.max()) * 1.2))
+plt.ylabel("Error")
+plt.xlabel("Number of Trees")
+plt.ylim((0.2, max(real_estimator_errors.max(), discrete_estimator_errors.max()) * 1.2))
 plt.xlim((-20, len(bdt_discrete) + 20))

 plt.subplot(133)
-plt.plot(range(1, n_trees_discrete + 1), discrete_estimator_weights,
-         "b", label='SAMME')
+plt.plot(range(1, n_trees_discrete + 1), discrete_estimator_weights, "b", label="SAMME")
 plt.legend()
-plt.ylabel('Weight')
-plt.xlabel('Number of Trees')
+plt.ylabel("Weight")
+plt.xlabel("Number of Trees")
 plt.ylim((0, discrete_estimator_weights.max() * 1.2))
 plt.xlim((-20, n_trees_discrete + 20))

('examples/ensemble', 'plot_adaboost_twoclass.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -16,7 +16,6 @@
 with a decision score above some value.

 """
-print(__doc__)

 # Author: Noel Dawe <noel.dawe@gmail.com>
 #
@@ -28,22 +27,23 @@
 from sklearn.ensemble import AdaBoostClassifier
 from sklearn.tree import DecisionTreeClassifier
 from sklearn.datasets import make_gaussian_quantiles
+from sklearn.inspection import DecisionBoundaryDisplay


 # Construct dataset
-X1, y1 = make_gaussian_quantiles(cov=2.,
-                                 n_samples=200, n_features=2,
-                                 n_classes=2, random_state=1)
-X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,
-                                 n_samples=300, n_features=2,
-                                 n_classes=2, random_state=1)
+X1, y1 = make_gaussian_quantiles(
+    cov=2.0, n_samples=200, n_features=2, n_classes=2, random_state=1
+)
+X2, y2 = make_gaussian_quantiles(
+    mean=(3, 3), cov=1.5, n_samples=300, n_features=2, n_classes=2, random_state=1
+)
 X = np.concatenate((X1, X2))
-y = np.concatenate((y1, - y2 + 1))
+y = np.concatenate((y1, -y2 + 1))

 # Create and fit an AdaBoosted decision tree
-bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),
-                         algorithm="SAMME",
-                         n_estimators=200)
+bdt = AdaBoostClassifier(
+    DecisionTreeClassifier(max_depth=1), algorithm="SAMME", n_estimators=200
+)

 bdt.fit(X, y)

@@ -54,49 +54,58 @@
 plt.figure(figsize=(10, 5))

 # Plot the decision boundaries
-plt.subplot(121)
-x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
-y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
-xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
-                     np.arange(y_min, y_max, plot_step))
-
-Z = bdt.predict(np.c_[xx.ravel(), yy.ravel()])
-Z = Z.reshape(xx.shape)
-cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
+ax = plt.subplot(121)
+disp = DecisionBoundaryDisplay.from_estimator(
+    bdt,
+    X,
+    cmap=plt.cm.Paired,
+    response_method="predict",
+    ax=ax,
+    xlabel="x",
+    ylabel="y",
+)
+x_min, x_max = disp.xx0.min(), disp.xx0.max()
+y_min, y_max = disp.xx1.min(), disp.xx1.max()
 plt.axis("tight")

 # Plot the training points
 for i, n, c in zip(range(2), class_names, plot_colors):
     idx = np.where(y == i)
-    plt.scatter(X[idx, 0], X[idx, 1],
-                c=c, cmap=plt.cm.Paired,
-                s=20, edgecolor='k',
-                label="Class %s" % n)
+    plt.scatter(
+        X[idx, 0],
+        X[idx, 1],
+        c=c,
+        cmap=plt.cm.Paired,
+        s=20,
+        edgecolor="k",
+        label="Class %s" % n,
+    )
 plt.xlim(x_min, x_max)
 plt.ylim(y_min, y_max)
-plt.legend(loc='upper right')
-plt.xlabel('x')
-plt.ylabel('y')
-plt.title('Decision Boundary')
+plt.legend(loc="upper right")
+
+plt.title("Decision Boundary")

 # Plot the two-class decision scores
 twoclass_output = bdt.decision_function(X)
 plot_range = (twoclass_output.min(), twoclass_output.max())
 plt.subplot(122)
 for i, n, c in zip(range(2), class_names, plot_colors):
-    plt.hist(twoclass_output[y == i],
-             bins=10,
-             range=plot_range,
-             facecolor=c,
-             label='Class %s' % n,
-             alpha=.5,
-             edgecolor='k')
+    plt.hist(
+        twoclass_output[y == i],
+        bins=10,
+        range=plot_range,
+        facecolor=c,
+        label="Class %s" % n,
+        alpha=0.5,
+        edgecolor="k",
+    )
 x1, x2, y1, y2 = plt.axis()
 plt.axis((x1, x2, y1, y2 * 1.2))
-plt.legend(loc='upper right')
-plt.ylabel('Samples')
-plt.xlabel('Score')
-plt.title('Decision Scores')
+plt.legend(loc="upper right")
+plt.ylabel("Samples")
+plt.xlabel("Score")
+plt.title("Decision Scores")

 plt.tight_layout()
 plt.subplots_adjust(wspace=0.35)
('examples/ensemble', 'plot_ensemble_oob.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -19,11 +19,6 @@
        Learning Ed. 2", p592-593, Springer, 2009.

 """
-import matplotlib.pyplot as plt
-
-from collections import OrderedDict
-from sklearn.datasets import make_classification
-from sklearn.ensemble import RandomForestClassifier

 # Author: Kian Ho <hui.kian.ho@gmail.com>
 #         Gilles Louppe <g.louppe@gmail.com>
@@ -31,34 +26,54 @@
 #
 # License: BSD 3 Clause

-print(__doc__)
+import matplotlib.pyplot as plt
+
+from collections import OrderedDict
+from sklearn.datasets import make_classification
+from sklearn.ensemble import RandomForestClassifier

 RANDOM_STATE = 123

 # Generate a binary classification dataset.
-X, y = make_classification(n_samples=500, n_features=25,
-                           n_clusters_per_class=1, n_informative=15,
-                           random_state=RANDOM_STATE)
+X, y = make_classification(
+    n_samples=500,
+    n_features=25,
+    n_clusters_per_class=1,
+    n_informative=15,
+    random_state=RANDOM_STATE,
+)

 # NOTE: Setting the `warm_start` construction parameter to `True` disables
 # support for parallelized ensembles but is necessary for tracking the OOB
 # error trajectory during training.
 ensemble_clfs = [
-    ("RandomForestClassifier, max_features='sqrt'",
-        RandomForestClassifier(n_estimators=100,
-                               warm_start=True, oob_score=True,
-                               max_features="sqrt",
-                               random_state=RANDOM_STATE)),
-    ("RandomForestClassifier, max_features='log2'",
-        RandomForestClassifier(n_estimators=100,
-                               warm_start=True, max_features='log2',
-                               oob_score=True,
-                               random_state=RANDOM_STATE)),
-    ("RandomForestClassifier, max_features=None",
-        RandomForestClassifier(n_estimators=100,
-                               warm_start=True, max_features=None,
-                               oob_score=True,
-                               random_state=RANDOM_STATE))
+    (
+        "RandomForestClassifier, max_features='sqrt'",
+        RandomForestClassifier(
+            warm_start=True,
+            oob_score=True,
+            max_features="sqrt",
+            random_state=RANDOM_STATE,
+        ),
+    ),
+    (
+        "RandomForestClassifier, max_features='log2'",
+        RandomForestClassifier(
+            warm_start=True,
+            max_features="log2",
+            oob_score=True,
+            random_state=RANDOM_STATE,
+        ),
+    ),
+    (
+        "RandomForestClassifier, max_features=None",
+        RandomForestClassifier(
+            warm_start=True,
+            max_features=None,
+            oob_score=True,
+            random_state=RANDOM_STATE,
+        ),
+    ),
 ]

 # Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.
@@ -66,10 +81,10 @@

 # Range of `n_estimators` values to explore.
 min_estimators = 15
-max_estimators = 175
+max_estimators = 150

 for label, clf in ensemble_clfs:
-    for i in range(min_estimators, max_estimators + 1):
+    for i in range(min_estimators, max_estimators + 1, 5):
         clf.set_params(n_estimators=i)
         clf.fit(X, y)

('examples/ensemble', 'plot_adaboost_regression.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -12,7 +12,6 @@
 .. [1] H. Drucker, "Improving Regressors using Boosting Techniques", 1997.

 """
-print(__doc__)

 # Author: Noel Dawe <noel.dawe@gmail.com>
 #
@@ -32,8 +31,9 @@
 # Fit regression model
 regr_1 = DecisionTreeRegressor(max_depth=4)

-regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),
-                          n_estimators=300, random_state=rng)
+regr_2 = AdaBoostRegressor(
+    DecisionTreeRegressor(max_depth=4), n_estimators=300, random_state=rng
+)

 regr_1.fit(X, y)
 regr_2.fit(X, y)
('examples/ensemble', 'plot_forest_importances_faces.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,47 +3,86 @@
 Pixel importances with a parallel forest of trees
 =================================================

-This example shows the use of forests of trees to evaluate the importance
-of the pixels in an image classification task (faces). The hotter the pixel,
-the more important.
+This example shows the use of a forest of trees to evaluate the impurity
+based importance of the pixels in an image classification task on the faces
+dataset. The hotter the pixel, the more important it is.

 The code below also illustrates how the construction and the computation
 of the predictions can be parallelized within multiple jobs.
+
 """
-print(__doc__)

-from time import time
-import matplotlib.pyplot as plt
+# %%
+# Loading the data and model fitting
+# ----------------------------------
+# First, we load the olivetti faces dataset and limit the dataset to contain
+# only the first five classes. Then we train a random forest on the dataset
+# and evaluate the impurity-based feature importance. One drawback of this
+# method is that it cannot be evaluated on a separate test set. For this
+# example, we are interested in representing the information learned from
+# the full dataset. Also, we'll set the number of cores to use for the tasks.
+from sklearn.datasets import fetch_olivetti_faces

-from sklearn.datasets import fetch_olivetti_faces
-from sklearn.ensemble import ExtraTreesClassifier
+# %%
+# We select the number of cores to use to perform parallel fitting of
+# the forest model. `-1` means use all available cores.
+n_jobs = -1

-# Number of cores to use to perform parallel fitting of the forest model
-n_jobs = 1
-
+# %%
 # Load the faces dataset
 data = fetch_olivetti_faces()
-X = data.images.reshape((len(data.images), -1))
-y = data.target
+X, y = data.data, data.target

-mask = y < 5  # Limit to 5 classes
+# %%
+# Limit the dataset to 5 classes.
+mask = y < 5
 X = X[mask]
 y = y[mask]

-# Build a forest and compute the pixel importances
-print("Fitting ExtraTreesClassifier on faces data with %d cores..." % n_jobs)
-t0 = time()
-forest = ExtraTreesClassifier(n_estimators=1000,
-                              max_features=128,
-                              n_jobs=n_jobs,
-                              random_state=0)
+# %%
+# A random forest classifier will be fitted to compute the feature importances.
+from sklearn.ensemble import RandomForestClassifier
+
+forest = RandomForestClassifier(n_estimators=750, n_jobs=n_jobs, random_state=42)

 forest.fit(X, y)
-print("done in %0.3fs" % (time() - t0))
+
+# %%
+# Feature importance based on mean decrease in impurity (MDI)
+# -----------------------------------------------------------
+# Feature importances are provided by the fitted attribute
+# `feature_importances_` and they are computed as the mean and standard
+# deviation of accumulation of the impurity decrease within each tree.
+#
+# .. warning::
+#     Impurity-based feature importances can be misleading for **high
+#     cardinality** features (many unique values). See
+#     :ref:`permutation_importance` as an alternative.
+import time
+import matplotlib.pyplot as plt
+
+start_time = time.time()
+img_shape = data.images[0].shape
 importances = forest.feature_importances_
-importances = importances.reshape(data.images[0].shape)
+elapsed_time = time.time() - start_time

-# Plot pixel importances
-plt.matshow(importances, cmap=plt.cm.hot)
-plt.title("Pixel importances with forests of trees")
+print(f"Elapsed time to compute the importances: {elapsed_time:.3f} seconds")
+imp_reshaped = importances.reshape(img_shape)
+plt.matshow(imp_reshaped, cmap=plt.cm.hot)
+plt.title("Pixel importances using impurity values")
+plt.colorbar()
 plt.show()
+
+# %%
+# Can you still recognize a face?
+
+# %%
+# The limitations of MDI is not a problem for this dataset because:
+#
+#  1. All features are (ordered) numeric and will thus not suffer the
+#     cardinality bias
+#  2. We are only interested to represent knowledge of the forest acquired
+#     on the training set.
+#
+# If these two conditions are not met, it is recommended to instead use
+# the :func:`~sklearn.inspection.permutation_importance`.
('examples/ensemble', 'plot_gradient_boosting_oob.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -22,8 +22,8 @@
 The figure also shows the performance of 3-fold cross validation which
 usually gives a better estimate of the test loss
 but is computationally more demanding.
+
 """
-print(__doc__)

 # Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>
 #
@@ -51,24 +51,29 @@
 X = np.c_[x1, x2, x3]

 X = X.astype(np.float32)
-X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,
-                                                    random_state=9)
+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=9)

 # Fit classifier with out-of-bag estimates
-params = {'n_estimators': 1200, 'max_depth': 3, 'subsample': 0.5,
-          'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}
+params = {
+    "n_estimators": 1200,
+    "max_depth": 3,
+    "subsample": 0.5,
+    "learning_rate": 0.01,
+    "min_samples_leaf": 1,
+    "random_state": 3,
+}
 clf = ensemble.GradientBoostingClassifier(**params)

 clf.fit(X_train, y_train)
 acc = clf.score(X_test, y_test)
 print("Accuracy: {:.4f}".format(acc))

-n_estimators = params['n_estimators']
+n_estimators = params["n_estimators"]
 x = np.arange(n_estimators) + 1


 def heldout_score(clf, X_test, y_test):
-    """compute deviance scores on ``X_test`` and ``y_test``. """
+    """compute deviance scores on ``X_test`` and ``y_test``."""
     score = np.zeros((n_estimators,), dtype=np.float64)
     for i, y_pred in enumerate(clf.staged_decision_function(X_test)):
         score[i] = clf.loss_(y_test, y_pred)
@@ -112,26 +117,26 @@
 cv_color = list(map(lambda x: x / 256.0, (253, 192, 134)))

 # plot curves and vertical lines for best iterations
-plt.plot(x, cumsum, label='OOB loss', color=oob_color)
-plt.plot(x, test_score, label='Test loss', color=test_color)
-plt.plot(x, cv_score, label='CV loss', color=cv_color)
+plt.plot(x, cumsum, label="OOB loss", color=oob_color)
+plt.plot(x, test_score, label="Test loss", color=test_color)
+plt.plot(x, cv_score, label="CV loss", color=cv_color)
 plt.axvline(x=oob_best_iter, color=oob_color)
 plt.axvline(x=test_best_iter, color=test_color)
 plt.axvline(x=cv_best_iter, color=cv_color)

 # add three vertical lines to xticks
 xticks = plt.xticks()
-xticks_pos = np.array(xticks[0].tolist() +
-                      [oob_best_iter, cv_best_iter, test_best_iter])
-xticks_label = np.array(list(map(lambda t: int(t), xticks[0])) +
-                        ['OOB', 'CV', 'Test'])
+xticks_pos = np.array(
+    xticks[0].tolist() + [oob_best_iter, cv_best_iter, test_best_iter]
+)
+xticks_label = np.array(list(map(lambda t: int(t), xticks[0])) + ["OOB", "CV", "Test"])
 ind = np.argsort(xticks_pos)
 xticks_pos = xticks_pos[ind]
 xticks_label = xticks_label[ind]
 plt.xticks(xticks_pos, xticks_label)

-plt.legend(loc='upper right')
-plt.ylabel('normalized loss')
-plt.xlabel('number of iterations')
+plt.legend(loc="upper right")
+plt.ylabel("normalized loss")
+plt.xlabel("number of iterations")

 plt.show()
('examples/ensemble', 'plot_gradient_boosting_regression.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,74 +3,152 @@
 Gradient Boosting regression
 ============================

-Demonstrate Gradient Boosting on the Boston housing dataset.
+This example demonstrates Gradient Boosting to produce a predictive
+model from an ensemble of weak predictive models. Gradient boosting can be used
+for regression and classification problems. Here, we will train a model to
+tackle a diabetes regression task. We will obtain the results from
+:class:`~sklearn.ensemble.GradientBoostingRegressor` with least squares loss
+and 500 regression trees of depth 4.

-This example fits a Gradient Boosting model with least squares loss and
-500 regression trees of depth 4.
+Note: For larger datasets (n_samples >= 10000), please refer to
+:class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
+
 """
-print(__doc__)

 # Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>
+#         Maria Telenczuk <https://github.com/maikia>
+#         Katrina Ni <https://github.com/nilichen>
 #
 # License: BSD 3 clause

+import matplotlib.pyplot as plt
 import numpy as np
-import matplotlib.pyplot as plt
+from sklearn import datasets, ensemble
+from sklearn.inspection import permutation_importance
+from sklearn.metrics import mean_squared_error
+from sklearn.model_selection import train_test_split

-from sklearn import ensemble
-from sklearn import datasets
-from sklearn.utils import shuffle
-from sklearn.metrics import mean_squared_error
+# %%
+# Load the data
+# -------------------------------------
+#
+# First we need to load the data.

-# #############################################################################
-# Load data
-boston = datasets.load_boston()
-X, y = shuffle(boston.data, boston.target, random_state=13)
-X = X.astype(np.float32)
-offset = int(X.shape[0] * 0.9)
-X_train, y_train = X[:offset], y[:offset]
-X_test, y_test = X[offset:], y[offset:]
+diabetes = datasets.load_diabetes()
+X, y = diabetes.data, diabetes.target

-# #############################################################################
+# %%
+# Data preprocessing
+# -------------------------------------
+#
+# Next, we will split our dataset to use 90% for training and leave the rest
+# for testing. We will also set the regression model parameters. You can play
+# with these parameters to see how the results change.
+#
+# `n_estimators` : the number of boosting stages that will be performed.
+# Later, we will plot deviance against boosting iterations.
+#
+# `max_depth` : limits the number of nodes in the tree.
+# The best value depends on the interaction of the input variables.
+#
+# `min_samples_split` : the minimum number of samples required to split an
+# internal node.
+#
+# `learning_rate` : how much the contribution of each tree will shrink.
+#
+# `loss` : loss function to optimize. The least squares function is  used in
+# this case however, there are many other options (see
+# :class:`~sklearn.ensemble.GradientBoostingRegressor` ).
+
+X_train, X_test, y_train, y_test = train_test_split(
+    X, y, test_size=0.1, random_state=13
+)
+
+params = {
+    "n_estimators": 500,
+    "max_depth": 4,
+    "min_samples_split": 5,
+    "learning_rate": 0.01,
+    "loss": "squared_error",
+}
+
+# %%
 # Fit regression model
-params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,
-          'learning_rate': 0.01, 'loss': 'ls'}
-clf = ensemble.GradientBoostingRegressor(**params)
+# --------------------
+#
+# Now we will initiate the gradient boosting regressors and fit it with our
+# training data. Let's also look and the mean squared error on the test data.

-clf.fit(X_train, y_train)
-mse = mean_squared_error(y_test, clf.predict(X_test))
-print("MSE: %.4f" % mse)
+reg = ensemble.GradientBoostingRegressor(**params)
+reg.fit(X_train, y_train)

-# #############################################################################
+mse = mean_squared_error(y_test, reg.predict(X_test))
+print("The mean squared error (MSE) on test set: {:.4f}".format(mse))
+
+# %%
 # Plot training deviance
+# ----------------------
+#
+# Finally, we will visualize the results. To do that we will first compute the
+# test set deviance and then plot it against boosting iterations.

-# compute test set deviance
-test_score = np.zeros((params['n_estimators'],), dtype=np.float64)
+test_score = np.zeros((params["n_estimators"],), dtype=np.float64)
+for i, y_pred in enumerate(reg.staged_predict(X_test)):
+    test_score[i] = reg.loss_(y_test, y_pred)

-for i, y_pred in enumerate(clf.staged_predict(X_test)):
-    test_score[i] = clf.loss_(y_test, y_pred)
+fig = plt.figure(figsize=(6, 6))
+plt.subplot(1, 1, 1)
+plt.title("Deviance")
+plt.plot(
+    np.arange(params["n_estimators"]) + 1,
+    reg.train_score_,
+    "b-",
+    label="Training Set Deviance",
+)
+plt.plot(
+    np.arange(params["n_estimators"]) + 1, test_score, "r-", label="Test Set Deviance"
+)
+plt.legend(loc="upper right")
+plt.xlabel("Boosting Iterations")
+plt.ylabel("Deviance")
+fig.tight_layout()
+plt.show()

-plt.figure(figsize=(12, 6))
+# %%
+# Plot feature importance
+# -----------------------
+#
+# .. warning::
+#    Careful, impurity-based feature importances can be misleading for
+#    **high cardinality** features (many unique values). As an alternative,
+#    the permutation importances of ``reg`` can be computed on a
+#    held out test set. See :ref:`permutation_importance` for more details.
+#
+# For this example, the impurity-based and permutation methods identify the
+# same 2 strongly predictive features but not in the same order. The third most
+# predictive feature, "bp", is also the same for the 2 methods. The remaining
+# features are less predictive and the error bars of the permutation plot
+# show that they overlap with 0.
+
+feature_importance = reg.feature_importances_
+sorted_idx = np.argsort(feature_importance)
+pos = np.arange(sorted_idx.shape[0]) + 0.5
+fig = plt.figure(figsize=(12, 6))
 plt.subplot(1, 2, 1)
-plt.title('Deviance')
-plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',
-         label='Training Set Deviance')
-plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',
-         label='Test Set Deviance')
-plt.legend(loc='upper right')
-plt.xlabel('Boosting Iterations')
-plt.ylabel('Deviance')
+plt.barh(pos, feature_importance[sorted_idx], align="center")
+plt.yticks(pos, np.array(diabetes.feature_names)[sorted_idx])
+plt.title("Feature Importance (MDI)")

-# #############################################################################
-# Plot feature importance
-feature_importance = clf.feature_importances_
-# make importances relative to max importance
-feature_importance = 100.0 * (feature_importance / feature_importance.max())
-sorted_idx = np.argsort(feature_importance)
-pos = np.arange(sorted_idx.shape[0]) + .5
+result = permutation_importance(
+    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2
+)
+sorted_idx = result.importances_mean.argsort()
 plt.subplot(1, 2, 2)
-plt.barh(pos, feature_importance[sorted_idx], align='center')
-plt.yticks(pos, boston.feature_names[sorted_idx])
-plt.xlabel('Relative Importance')
-plt.title('Variable Importance')
+plt.boxplot(
+    result.importances[sorted_idx].T,
+    vert=False,
+    labels=np.array(diabetes.feature_names)[sorted_idx],
+)
+plt.title("Permutation Importance (test set)")
+fig.tight_layout()
 plt.show()
('examples/ensemble', 'plot_isolation_forest.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,7 +3,7 @@
 IsolationForest example
 ==========================================

-An example using :class:`sklearn.ensemble.IsolationForest` for anomaly
+An example using :class:`~sklearn.ensemble.IsolationForest` for anomaly
 detection.

 The IsolationForest 'isolates' observations by randomly selecting a feature
@@ -22,7 +22,6 @@
 for particular samples, they are highly likely to be anomalies.

 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -40,8 +39,7 @@
 X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))

 # fit the model
-clf = IsolationForest(behaviour='new', max_samples=100,
-                      random_state=rng, contamination='auto')
+clf = IsolationForest(max_samples=100, random_state=rng)
 clf.fit(X_train)
 y_pred_train = clf.predict(X_train)
 y_pred_test = clf.predict(X_test)
@@ -55,17 +53,15 @@
 plt.title("IsolationForest")
 plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)

-b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white',
-                 s=20, edgecolor='k')
-b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green',
-                 s=20, edgecolor='k')
-c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red',
-                s=20, edgecolor='k')
-plt.axis('tight')
+b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c="white", s=20, edgecolor="k")
+b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c="green", s=20, edgecolor="k")
+c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c="red", s=20, edgecolor="k")
+plt.axis("tight")
 plt.xlim((-5, 5))
 plt.ylim((-5, 5))
-plt.legend([b1, b2, c],
-           ["training observations",
-            "new regular observations", "new abnormal observations"],
-           loc="upper left")
+plt.legend(
+    [b1, b2, c],
+    ["training observations", "new regular observations", "new abnormal observations"],
+    loc="upper left",
+)
 plt.show()
('examples/ensemble', 'plot_bias_variance.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -12,7 +12,8 @@
 predictions of the estimator differ from the predictions of the best possible
 estimator for the problem (i.e., the Bayes model). The variance term measures
 the variability of the predictions of the estimator when fit over different
-instances LS of the problem. Finally, the noise measures the irreducible part
+random instances of the same problem. Each problem instance is noted "LS", for
+"Learning Sample", in the following. Finally, the noise measures the irreducible part
 of the error which is due the variability in the data.

 The upper left figure illustrates the predictions (in dark red) of a single
@@ -61,7 +62,6 @@
        "Elements of Statistical Learning", Springer, 2009.

 """
-print(__doc__)

 # Author: Gilles Louppe <g.louppe@gmail.com>
 # License: BSD 3 clause
@@ -73,18 +73,20 @@
 from sklearn.tree import DecisionTreeRegressor

 # Settings
-n_repeat = 50       # Number of iterations for computing expectations
-n_train = 50        # Size of the training set
-n_test = 1000       # Size of the test set
-noise = 0.1         # Standard deviation of the noise
+n_repeat = 50  # Number of iterations for computing expectations
+n_train = 50  # Size of the training set
+n_test = 1000  # Size of the test set
+noise = 0.1  # Standard deviation of the noise
 np.random.seed(0)

 # Change this for exploring the bias-variance decomposition of other
 # estimators. This should work well for estimators with high variance (e.g.,
 # decision trees or KNN), but poorly for estimators with low variance (e.g.,
 # linear models).
-estimators = [("Tree", DecisionTreeRegressor()),
-              ("Bagging(Tree)", BaggingRegressor(DecisionTreeRegressor()))]
+estimators = [
+    ("Tree", DecisionTreeRegressor()),
+    ("Bagging(Tree)", BaggingRegressor(DecisionTreeRegressor())),
+]

 n_estimators = len(estimators)

@@ -93,7 +95,7 @@
 def f(x):
     x = x.ravel()

-    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)
+    return np.exp(-(x**2)) + 1.5 * np.exp(-((x - 2) ** 2))


 def generate(n_samples, noise, n_repeat=1):
@@ -141,18 +143,18 @@
         for j in range(n_repeat):
             y_error += (y_test[:, j] - y_predict[:, i]) ** 2

-    y_error /= (n_repeat * n_repeat)
+    y_error /= n_repeat * n_repeat

     y_noise = np.var(y_test, axis=1)
     y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 2
     y_var = np.var(y_predict, axis=1)

-    print("{0}: {1:.4f} (error) = {2:.4f} (bias^2) "
-          " + {3:.4f} (var) + {4:.4f} (noise)".format(name,
-                                                      np.mean(y_error),
-                                                      np.mean(y_bias),
-                                                      np.mean(y_var),
-                                                      np.mean(y_noise)))
+    print(
+        "{0}: {1:.4f} (error) = {2:.4f} (bias^2) "
+        " + {3:.4f} (var) + {4:.4f} (noise)".format(
+            name, np.mean(y_error), np.mean(y_bias), np.mean(y_var), np.mean(y_noise)
+        )
+    )

     # Plot figures
     plt.subplot(2, n_estimators, n + 1)
@@ -165,14 +167,13 @@
         else:
             plt.plot(X_test, y_predict[:, i], "r", alpha=0.05)

-    plt.plot(X_test, np.mean(y_predict, axis=1), "c",
-             label=r"$\mathbb{E}_{LS} \^y(x)$")
+    plt.plot(X_test, np.mean(y_predict, axis=1), "c", label=r"$\mathbb{E}_{LS} \^y(x)$")

     plt.xlim([-5, 5])
     plt.title(name)

     if n == n_estimators - 1:
-        plt.legend(loc=(1.1, .5))
+        plt.legend(loc=(1.1, 0.5))

     plt.subplot(2, n_estimators, n_estimators + n + 1)
     plt.plot(X_test, y_error, "r", label="$error(x)$")
@@ -185,7 +186,7 @@

     if n == n_estimators - 1:

-        plt.legend(loc=(1.1, .5))
+        plt.legend(loc=(1.1, 0.5))

-plt.subplots_adjust(right=.75)
+plt.subplots_adjust(right=0.75)
 plt.show()
('examples/ensemble', 'plot_feature_transformation.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,118 +3,165 @@
 Feature transformations with ensembles of trees
 ===============================================

-Transform your features into a higher dimensional, sparse space. Then
-train a linear model on these features.
+Transform your features into a higher dimensional, sparse space. Then train a
+linear model on these features.

-First fit an ensemble of trees (totally random trees, a random
-forest, or gradient boosted trees) on the training set. Then each leaf
-of each tree in the ensemble is assigned a fixed arbitrary feature
-index in a new feature space. These leaf indices are then encoded in a
-one-hot fashion.
+First fit an ensemble of trees (totally random trees, a random forest, or
+gradient boosted trees) on the training set. Then each leaf of each tree in the
+ensemble is assigned a fixed arbitrary feature index in a new feature space.
+These leaf indices are then encoded in a one-hot fashion.

-Each sample goes through the decisions of each tree of the ensemble
-and ends up in one leaf per tree. The sample is encoded by setting
-feature values for these leaves to 1 and the other feature values to 0.
+Each sample goes through the decisions of each tree of the ensemble and ends up
+in one leaf per tree. The sample is encoded by setting feature values for these
+leaves to 1 and the other feature values to 0.

 The resulting transformer has then learned a supervised, sparse,
 high-dimensional categorical embedding of the data.

 """

+
 # Author: Tim Head <betatim@gmail.com>
 #
 # License: BSD 3 clause

-import numpy as np
-np.random.seed(10)
+# %%
+# First, we will create a large dataset and split it into three sets:
+#
+# - a set to train the ensemble methods which are later used to as a feature
+#   engineering transformer;
+# - a set to train the linear model;
+# - a set to test the linear model.
+#
+# It is important to split the data in such way to avoid overfitting by leaking
+# data.
+
+from sklearn.datasets import make_classification
+from sklearn.model_selection import train_test_split
+
+X, y = make_classification(n_samples=80000, random_state=10)
+
+X_full_train, X_test, y_full_train, y_test = train_test_split(
+    X, y, test_size=0.5, random_state=10
+)
+X_train_ensemble, X_train_linear, y_train_ensemble, y_train_linear = train_test_split(
+    X_full_train, y_full_train, test_size=0.5, random_state=10
+)
+
+# %%
+# For each of the ensemble methods, we will use 10 estimators and a maximum
+# depth of 3 levels.
+
+n_estimators = 10
+max_depth = 3
+
+# %%
+# First, we will start by training the random forest and gradient boosting on
+# the separated training set
+
+from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
+
+random_forest = RandomForestClassifier(
+    n_estimators=n_estimators, max_depth=max_depth, random_state=10
+)
+random_forest.fit(X_train_ensemble, y_train_ensemble)
+
+gradient_boosting = GradientBoostingClassifier(
+    n_estimators=n_estimators, max_depth=max_depth, random_state=10
+)
+_ = gradient_boosting.fit(X_train_ensemble, y_train_ensemble)
+
+# %%
+# The :class:`~sklearn.ensemble.RandomTreesEmbedding` is an unsupervised method
+# and thus does not required to be trained independently.
+
+from sklearn.ensemble import RandomTreesEmbedding
+
+random_tree_embedding = RandomTreesEmbedding(
+    n_estimators=n_estimators, max_depth=max_depth, random_state=0
+)
+
+# %%
+# Now, we will create three pipelines that will use the above embedding as
+# a preprocessing stage.
+#
+# The random trees embedding can be directly pipelined with the logistic
+# regression because it is a standard scikit-learn transformer.
+
+from sklearn.linear_model import LogisticRegression
+from sklearn.pipeline import make_pipeline
+
+rt_model = make_pipeline(random_tree_embedding, LogisticRegression(max_iter=1000))
+rt_model.fit(X_train_linear, y_train_linear)
+
+# %%
+# Then, we can pipeline random forest or gradient boosting with a logistic
+# regression. However, the feature transformation will happen by calling the
+# method `apply`. The pipeline in scikit-learn expects a call to `transform`.
+# Therefore, we wrapped the call to `apply` within a `FunctionTransformer`.
+
+from sklearn.preprocessing import FunctionTransformer
+from sklearn.preprocessing import OneHotEncoder
+
+
+def rf_apply(X, model):
+    return model.apply(X)
+
+
+rf_leaves_yielder = FunctionTransformer(rf_apply, kw_args={"model": random_forest})
+
+rf_model = make_pipeline(
+    rf_leaves_yielder,
+    OneHotEncoder(handle_unknown="ignore"),
+    LogisticRegression(max_iter=1000),
+)
+rf_model.fit(X_train_linear, y_train_linear)
+
+
+# %%
+def gbdt_apply(X, model):
+    return model.apply(X)[:, :, 0]
+
+
+gbdt_leaves_yielder = FunctionTransformer(
+    gbdt_apply, kw_args={"model": gradient_boosting}
+)
+
+gbdt_model = make_pipeline(
+    gbdt_leaves_yielder,
+    OneHotEncoder(handle_unknown="ignore"),
+    LogisticRegression(max_iter=1000),
+)
+gbdt_model.fit(X_train_linear, y_train_linear)
+
+# %%
+# We can finally show the different ROC curves for all the models.

 import matplotlib.pyplot as plt
+from sklearn.metrics import RocCurveDisplay

-from sklearn.datasets import make_classification
-from sklearn.linear_model import LogisticRegression
-from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,
-                              GradientBoostingClassifier)
-from sklearn.preprocessing import OneHotEncoder
-from sklearn.model_selection import train_test_split
-from sklearn.metrics import roc_curve
-from sklearn.pipeline import make_pipeline
+fig, ax = plt.subplots()

-n_estimator = 10
-X, y = make_classification(n_samples=80000)
-X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)
+models = [
+    ("RT embedding -> LR", rt_model),
+    ("RF", random_forest),
+    ("RF embedding -> LR", rf_model),
+    ("GBDT", gradient_boosting),
+    ("GBDT embedding -> LR", gbdt_model),
+]

-# It is important to train the ensemble of trees on a different subset
-# of the training data than the linear regression model to avoid
-# overfitting, in particular if the total number of leaves is
-# similar to the number of training samples
-X_train, X_train_lr, y_train, y_train_lr = train_test_split(
-    X_train, y_train, test_size=0.5)
+model_displays = {}
+for name, pipeline in models:
+    model_displays[name] = RocCurveDisplay.from_estimator(
+        pipeline, X_test, y_test, ax=ax, name=name
+    )
+_ = ax.set_title("ROC curve")

-# Unsupervised transformation based on totally random trees
-rt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator,
-                          random_state=0)
+# %%
+fig, ax = plt.subplots()
+for name, pipeline in models:
+    model_displays[name].plot(ax=ax)

-rt_lm = LogisticRegression(solver='lbfgs', max_iter=1000)
-pipeline = make_pipeline(rt, rt_lm)
-pipeline.fit(X_train, y_train)
-y_pred_rt = pipeline.predict_proba(X_test)[:, 1]
-fpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_test, y_pred_rt)
-
-# Supervised transformation based on random forests
-rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)
-rf_enc = OneHotEncoder(categories='auto')
-rf_lm = LogisticRegression(solver='lbfgs', max_iter=1000)
-rf.fit(X_train, y_train)
-rf_enc.fit(rf.apply(X_train))
-rf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), y_train_lr)
-
-y_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(X_test)))[:, 1]
-fpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm)
-
-# Supervised transformation based on gradient boosted trees
-grd = GradientBoostingClassifier(n_estimators=n_estimator)
-grd_enc = OneHotEncoder(categories='auto')
-grd_lm = LogisticRegression(solver='lbfgs', max_iter=1000)
-grd.fit(X_train, y_train)
-grd_enc.fit(grd.apply(X_train)[:, :, 0])
-grd_lm.fit(grd_enc.transform(grd.apply(X_train_lr)[:, :, 0]), y_train_lr)
-
-y_pred_grd_lm = grd_lm.predict_proba(
-    grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]
-fpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm)
-
-# The gradient boosted model by itself
-y_pred_grd = grd.predict_proba(X_test)[:, 1]
-fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)
-
-# The random forest model by itself
-y_pred_rf = rf.predict_proba(X_test)[:, 1]
-fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)
-
-plt.figure(1)
-plt.plot([0, 1], [0, 1], 'k--')
-plt.plot(fpr_rt_lm, tpr_rt_lm, label='RT + LR')
-plt.plot(fpr_rf, tpr_rf, label='RF')
-plt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')
-plt.plot(fpr_grd, tpr_grd, label='GBT')
-plt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')
-plt.xlabel('False positive rate')
-plt.ylabel('True positive rate')
-plt.title('ROC curve')
-plt.legend(loc='best')
-plt.show()
-
-plt.figure(2)
-plt.xlim(0, 0.2)
-plt.ylim(0.8, 1)
-plt.plot([0, 1], [0, 1], 'k--')
-plt.plot(fpr_rt_lm, tpr_rt_lm, label='RT + LR')
-plt.plot(fpr_rf, tpr_rf, label='RF')
-plt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')
-plt.plot(fpr_grd, tpr_grd, label='GBT')
-plt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')
-plt.xlabel('False positive rate')
-plt.ylabel('True positive rate')
-plt.title('ROC curve (zoomed in at top left)')
-plt.legend(loc='best')
-plt.show()
+ax.set_xlim(0, 0.2)
+ax.set_ylim(0.8, 1)
+_ = ax.set_title("ROC curve (zoomed in at top left)")
('examples/ensemble', 'plot_random_forest_embedding.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -23,7 +23,9 @@
 decision boundary obtained by BernoulliNB in the transformed
 space with an ExtraTreesClassifier forests learned on the
 original data.
+
 """
+
 import numpy as np
 import matplotlib.pyplot as plt

@@ -57,23 +59,24 @@
 fig = plt.figure(figsize=(9, 8))

 ax = plt.subplot(221)
-ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k')
+ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor="k")
 ax.set_title("Original Data (2d)")
 ax.set_xticks(())
 ax.set_yticks(())

 ax = plt.subplot(222)
-ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=50, edgecolor='k')
-ax.set_title("Truncated SVD reduction (2d) of transformed data (%dd)" %
-             X_transformed.shape[1])
+ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=50, edgecolor="k")
+ax.set_title(
+    "Truncated SVD reduction (2d) of transformed data (%dd)" % X_transformed.shape[1]
+)
 ax.set_xticks(())
 ax.set_yticks(())

 # Plot the decision in original space. For that, we will assign a color
 # to each point in the mesh [x_min, x_max]x[y_min, y_max].
-h = .01
-x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
-y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
+h = 0.01
+x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
+y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

 # transform grid using RandomTreesEmbedding
@@ -83,7 +86,7 @@
 ax = plt.subplot(223)
 ax.set_title("Naive Bayes on Transformed data")
 ax.pcolormesh(xx, yy, y_grid_pred.reshape(xx.shape))
-ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k')
+ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor="k")
 ax.set_ylim(-1.4, 1.4)
 ax.set_xlim(-1.4, 1.4)
 ax.set_xticks(())
@@ -95,7 +98,7 @@
 ax = plt.subplot(224)
 ax.set_title("ExtraTrees predictions")
 ax.pcolormesh(xx, yy, y_grid_pred.reshape(xx.shape))
-ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor='k')
+ax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor="k")
 ax.set_ylim(-1.4, 1.4)
 ax.set_xlim(-1.4, 1.4)
 ax.set_xticks(())
('examples/ensemble', 'plot_random_forest_regression_multioutput.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -20,7 +20,6 @@
 x and y coordinate as output.

 """
-print(__doc__)

 # Author: Tim Head <betatim@gmail.com>
 #
@@ -37,19 +36,19 @@
 rng = np.random.RandomState(1)
 X = np.sort(200 * rng.rand(600, 1) - 100, axis=0)
 y = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T
-y += (0.5 - rng.rand(*y.shape))
+y += 0.5 - rng.rand(*y.shape)

 X_train, X_test, y_train, y_test = train_test_split(
-    X, y, train_size=400, test_size=200, random_state=4)
+    X, y, train_size=400, test_size=200, random_state=4
+)

 max_depth = 30
-regr_multirf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100,
-                                                          max_depth=max_depth,
-                                                          random_state=0))
+regr_multirf = MultiOutputRegressor(
+    RandomForestRegressor(n_estimators=100, max_depth=max_depth, random_state=0)
+)
 regr_multirf.fit(X_train, y_train)

-regr_rf = RandomForestRegressor(n_estimators=100, max_depth=max_depth,
-                                random_state=2)
+regr_rf = RandomForestRegressor(n_estimators=100, max_depth=max_depth, random_state=2)
 regr_rf.fit(X_train, y_train)

 # Predict on new data
@@ -60,14 +59,35 @@
 plt.figure()
 s = 50
 a = 0.4
-plt.scatter(y_test[:, 0], y_test[:, 1], edgecolor='k',
-            c="navy", s=s, marker="s", alpha=a, label="Data")
-plt.scatter(y_multirf[:, 0], y_multirf[:, 1], edgecolor='k',
-            c="cornflowerblue", s=s, alpha=a,
-            label="Multi RF score=%.2f" % regr_multirf.score(X_test, y_test))
-plt.scatter(y_rf[:, 0], y_rf[:, 1], edgecolor='k',
-            c="c", s=s, marker="^", alpha=a,
-            label="RF score=%.2f" % regr_rf.score(X_test, y_test))
+plt.scatter(
+    y_test[:, 0],
+    y_test[:, 1],
+    edgecolor="k",
+    c="navy",
+    s=s,
+    marker="s",
+    alpha=a,
+    label="Data",
+)
+plt.scatter(
+    y_multirf[:, 0],
+    y_multirf[:, 1],
+    edgecolor="k",
+    c="cornflowerblue",
+    s=s,
+    alpha=a,
+    label="Multi RF score=%.2f" % regr_multirf.score(X_test, y_test),
+)
+plt.scatter(
+    y_rf[:, 0],
+    y_rf[:, 1],
+    edgecolor="k",
+    c="c",
+    s=s,
+    marker="^",
+    alpha=a,
+    label="RF score=%.2f" % regr_rf.score(X_test, y_test),
+)
 plt.xlim([-6, 6])
 plt.ylim([-6, 6])
 plt.xlabel("target 1")
('examples/ensemble', 'plot_gradient_boosting_quantile.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,77 +3,328 @@
 Prediction Intervals for Gradient Boosting Regression
 =====================================================

-This example shows how quantile regression can be used
-to create prediction intervals.
+This example shows how quantile regression can be used to create prediction
+intervals.
+
 """

+# %%
+# Generate some data for a synthetic regression problem by applying the
+# function f to uniformly sampled random inputs.
 import numpy as np
-import matplotlib.pyplot as plt
-
-from sklearn.ensemble import GradientBoostingRegressor
-
-np.random.seed(1)
+from sklearn.model_selection import train_test_split


 def f(x):
     """The function to predict."""
     return x * np.sin(x)

-#----------------------------------------------------------------------
-#  First the noiseless case
-X = np.atleast_2d(np.random.uniform(0, 10.0, size=100)).T
-X = X.astype(np.float32)
-
-# Observations
-y = f(X).ravel()
-
-dy = 1.5 + 1.0 * np.random.random(y.shape)
-noise = np.random.normal(0, dy)
-y += noise
-y = y.astype(np.float32)
-
-# Mesh the input space for evaluations of the real function, the prediction and
-# its MSE
+
+rng = np.random.RandomState(42)
+X = np.atleast_2d(rng.uniform(0, 10.0, size=1000)).T
+expected_y = f(X).ravel()
+
+# %%
+# To make the problem interesting, we generate observations of the target y as
+# the sum of a deterministic term computed by the function f and a random noise
+# term that follows a centered `log-normal
+# <https://en.wikipedia.org/wiki/Log-normal_distribution>`_. To make this even
+# more interesting we consider the case where the amplitude of the noise
+# depends on the input variable x (heteroscedastic noise).
+#
+# The lognormal distribution is non-symmetric and long tailed: observing large
+# outliers is likely but it is impossible to observe small outliers.
+sigma = 0.5 + X.ravel() / 10
+noise = rng.lognormal(sigma=sigma) - np.exp(sigma**2 / 2)
+y = expected_y + noise
+
+# %%
+# Split into train, test datasets:
+X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
+
+# %%
+# Fitting non-linear quantile and least squares regressors
+# --------------------------------------------------------
+#
+# Fit gradient boosting models trained with the quantile loss and
+# alpha=0.05, 0.5, 0.95.
+#
+# The models obtained for alpha=0.05 and alpha=0.95 produce a 90% confidence
+# interval (95% - 5% = 90%).
+#
+# The model trained with alpha=0.5 produces a regression of the median: on
+# average, there should be the same number of target observations above and
+# below the predicted values.
+from sklearn.ensemble import GradientBoostingRegressor
+from sklearn.metrics import mean_pinball_loss, mean_squared_error
+
+
+all_models = {}
+common_params = dict(
+    learning_rate=0.05,
+    n_estimators=200,
+    max_depth=2,
+    min_samples_leaf=9,
+    min_samples_split=9,
+)
+for alpha in [0.05, 0.5, 0.95]:
+    gbr = GradientBoostingRegressor(loss="quantile", alpha=alpha, **common_params)
+    all_models["q %1.2f" % alpha] = gbr.fit(X_train, y_train)
+
+# %%
+# For the sake of comparison, we also fit a baseline model trained with the
+# usual (mean) squared error (MSE).
+gbr_ls = GradientBoostingRegressor(loss="squared_error", **common_params)
+all_models["mse"] = gbr_ls.fit(X_train, y_train)
+
+# %%
+# Create an evenly spaced evaluation set of input values spanning the [0, 10]
+# range.
 xx = np.atleast_2d(np.linspace(0, 10, 1000)).T
-xx = xx.astype(np.float32)
+
+# %%
+# Plot the true conditional mean function f, the predictions of the conditional
+# mean (loss equals squared error), the conditional median and the conditional
+# 90% interval (from 5th to 95th conditional percentiles).
+import matplotlib.pyplot as plt
+
+
+y_pred = all_models["mse"].predict(xx)
+y_lower = all_models["q 0.05"].predict(xx)
+y_upper = all_models["q 0.95"].predict(xx)
+y_med = all_models["q 0.50"].predict(xx)
+
+fig = plt.figure(figsize=(10, 10))
+plt.plot(xx, f(xx), "g:", linewidth=3, label=r"$f(x) = x\,\sin(x)$")
+plt.plot(X_test, y_test, "b.", markersize=10, label="Test observations")
+plt.plot(xx, y_med, "r-", label="Predicted median")
+plt.plot(xx, y_pred, "r-", label="Predicted mean")
+plt.plot(xx, y_upper, "k-")
+plt.plot(xx, y_lower, "k-")
+plt.fill_between(
+    xx.ravel(), y_lower, y_upper, alpha=0.4, label="Predicted 90% interval"
+)
+plt.xlabel("$x$")
+plt.ylabel("$f(x)$")
+plt.ylim(-10, 25)
+plt.legend(loc="upper left")
+plt.show()
+
+# %%
+# Comparing the predicted median with the predicted mean, we note that the
+# median is on average below the mean as the noise is skewed towards high
+# values (large outliers). The median estimate also seems to be smoother
+# because of its natural robustness to outliers.
+#
+# Also observe that the inductive bias of gradient boosting trees is
+# unfortunately preventing our 0.05 quantile to fully capture the sinoisoidal
+# shape of the signal, in particular around x=8. Tuning hyper-parameters can
+# reduce this effect as shown in the last part of this notebook.
+#
+# Analysis of the error metrics
+# -----------------------------
+#
+# Measure the models with :func:`mean_squared_error` and
+# :func:`mean_pinball_loss` metrics on the training dataset.
+import pandas as pd
+
+
+def highlight_min(x):
+    x_min = x.min()
+    return ["font-weight: bold" if v == x_min else "" for v in x]
+
+
+results = []
+for name, gbr in sorted(all_models.items()):
+    metrics = {"model": name}
+    y_pred = gbr.predict(X_train)
+    for alpha in [0.05, 0.5, 0.95]:
+        metrics["pbl=%1.2f" % alpha] = mean_pinball_loss(y_train, y_pred, alpha=alpha)
+    metrics["MSE"] = mean_squared_error(y_train, y_pred)
+    results.append(metrics)
+
+pd.DataFrame(results).set_index("model").style.apply(highlight_min)
+
+# %%
+# One column shows all models evaluated by the same metric. The minimum number
+# on a column should be obtained when the model is trained and measured with
+# the same metric. This should be always the case on the training set if the
+# training converged.
+#
+# Note that because the target distribution is asymmetric, the expected
+# conditional mean and conditional median are signficiantly different and
+# therefore one could not use the squared error model get a good estimation of
+# the conditional median nor the converse.
+#
+# If the target distribution were symmetric and had no outliers (e.g. with a
+# Gaussian noise), then median estimator and the least squares estimator would
+# have yielded similar predictions.
+#
+# We then do the same on the test set.
+results = []
+for name, gbr in sorted(all_models.items()):
+    metrics = {"model": name}
+    y_pred = gbr.predict(X_test)
+    for alpha in [0.05, 0.5, 0.95]:
+        metrics["pbl=%1.2f" % alpha] = mean_pinball_loss(y_test, y_pred, alpha=alpha)
+    metrics["MSE"] = mean_squared_error(y_test, y_pred)
+    results.append(metrics)
+
+pd.DataFrame(results).set_index("model").style.apply(highlight_min)
+
+
+# %%
+# Errors are higher meaning the models slightly overfitted the data. It still
+# shows that the best test metric is obtained when the model is trained by
+# minimizing this same metric.
+#
+# Note that the conditional median estimator is competitive with the squared
+# error estimator in terms of MSE on the test set: this can be explained by
+# the fact the squared error estimator is very sensitive to large outliers
+# which can cause significant overfitting. This can be seen on the right hand
+# side of the previous plot. The conditional median estimator is biased
+# (underestimation for this asymmetric noise) but is also naturally robust to
+# outliers and overfits less.
+#
+# Calibration of the confidence interval
+# --------------------------------------
+#
+# We can also evaluate the ability of the two extreme quantile estimators at
+# producing a well-calibrated conditational 90%-confidence interval.
+#
+# To do this we can compute the fraction of observations that fall between the
+# predictions:
+def coverage_fraction(y, y_low, y_high):
+    return np.mean(np.logical_and(y >= y_low, y <= y_high))
+
+
+coverage_fraction(
+    y_train,
+    all_models["q 0.05"].predict(X_train),
+    all_models["q 0.95"].predict(X_train),
+)
+
+# %%
+# On the training set the calibration is very close to the expected coverage
+# value for a 90% confidence interval.
+coverage_fraction(
+    y_test, all_models["q 0.05"].predict(X_test), all_models["q 0.95"].predict(X_test)
+)
+
+
+# %%
+# On the test set, the estimated confidence interval is slightly too narrow.
+# Note, however, that we would need to wrap those metrics in a cross-validation
+# loop to assess their variability under data resampling.
+#
+# Tuning the hyper-parameters of the quantile regressors
+# ------------------------------------------------------
+#
+# In the plot above, we observed that the 5th percentile regressor seems to
+# underfit and could not adapt to sinusoidal shape of the signal.
+#
+# The hyper-parameters of the model were approximately hand-tuned for the
+# median regressor and there is no reason that the same hyper-parameters are
+# suitable for the 5th percentile regressor.
+#
+# To confirm this hypothesis, we tune the hyper-parameters of a new regressor
+# of the 5th percentile by selecting the best model parameters by
+# cross-validation on the pinball loss with alpha=0.05:
+
+# %%
+from sklearn.experimental import enable_halving_search_cv  # noqa
+from sklearn.model_selection import HalvingRandomSearchCV
+from sklearn.metrics import make_scorer
+from pprint import pprint
+
+param_grid = dict(
+    learning_rate=[0.05, 0.1, 0.2],
+    max_depth=[2, 5, 10],
+    min_samples_leaf=[1, 5, 10, 20],
+    min_samples_split=[5, 10, 20, 30, 50],
+)
+alpha = 0.05
+neg_mean_pinball_loss_05p_scorer = make_scorer(
+    mean_pinball_loss,
+    alpha=alpha,
+    greater_is_better=False,  # maximize the negative loss
+)
+gbr = GradientBoostingRegressor(loss="quantile", alpha=alpha, random_state=0)
+search_05p = HalvingRandomSearchCV(
+    gbr,
+    param_grid,
+    resource="n_estimators",
+    max_resources=250,
+    min_resources=50,
+    scoring=neg_mean_pinball_loss_05p_scorer,
+    n_jobs=2,
+    random_state=0,
+).fit(X_train, y_train)
+pprint(search_05p.best_params_)
+
+# %%
+# We observe that the hyper-parameters that were hand-tuned for the median
+# regressor are in the same range as the hyper-parameters suitable for the 5th
+# percentile regressor.
+#
+# Let's now tune the hyper-parameters for the 95th percentile regressor. We
+# need to redefine the `scoring` metric used to select the best model, along
+# with adjusting the alpha parameter of the inner gradient boosting estimator
+# itself:
+from sklearn.base import clone

 alpha = 0.95
-
-clf = GradientBoostingRegressor(loss='quantile', alpha=alpha,
-                                n_estimators=250, max_depth=3,
-                                learning_rate=.1, min_samples_leaf=9,
-                                min_samples_split=9)
-
-clf.fit(X, y)
-
-# Make the prediction on the meshed x-axis
-y_upper = clf.predict(xx)
-
-clf.set_params(alpha=1.0 - alpha)
-clf.fit(X, y)
-
-# Make the prediction on the meshed x-axis
-y_lower = clf.predict(xx)
-
-clf.set_params(loss='ls')
-clf.fit(X, y)
-
-# Make the prediction on the meshed x-axis
-y_pred = clf.predict(xx)
-
-# Plot the function, the prediction and the 90% confidence interval based on
-# the MSE
-fig = plt.figure()
-plt.plot(xx, f(xx), 'g:', label=r'$f(x) = x\,\sin(x)$')
-plt.plot(X, y, 'b.', markersize=10, label=u'Observations')
-plt.plot(xx, y_pred, 'r-', label=u'Prediction')
-plt.plot(xx, y_upper, 'k-')
-plt.plot(xx, y_lower, 'k-')
-plt.fill(np.concatenate([xx, xx[::-1]]),
-         np.concatenate([y_upper, y_lower[::-1]]),
-         alpha=.5, fc='b', ec='None', label='90% prediction interval')
-plt.xlabel('$x$')
-plt.ylabel('$f(x)$')
-plt.ylim(-10, 20)
-plt.legend(loc='upper left')
+neg_mean_pinball_loss_95p_scorer = make_scorer(
+    mean_pinball_loss,
+    alpha=alpha,
+    greater_is_better=False,  # maximize the negative loss
+)
+search_95p = clone(search_05p).set_params(
+    estimator__alpha=alpha,
+    scoring=neg_mean_pinball_loss_95p_scorer,
+)
+search_95p.fit(X_train, y_train)
+pprint(search_95p.best_params_)
+
+# %%
+# The result shows that the hyper-parameters for the 95th percentile regressor
+# identified by the search procedure are roughly in the same range as the hand-
+# tuned hyper-parameters for the median regressor and the hyper-parameters
+# identified by the search procedure for the 5th percentile regressor. However,
+# the hyper-parameter searches did lead to an improved 90% confidence interval
+# that is comprised by the predictions of those two tuned quantile regressors.
+# Note that the prediction of the upper 95th percentile has a much coarser shape
+# than the prediction of the lower 5th percentile because of the outliers:
+y_lower = search_05p.predict(xx)
+y_upper = search_95p.predict(xx)
+
+fig = plt.figure(figsize=(10, 10))
+plt.plot(xx, f(xx), "g:", linewidth=3, label=r"$f(x) = x\,\sin(x)$")
+plt.plot(X_test, y_test, "b.", markersize=10, label="Test observations")
+plt.plot(xx, y_upper, "k-")
+plt.plot(xx, y_lower, "k-")
+plt.fill_between(
+    xx.ravel(), y_lower, y_upper, alpha=0.4, label="Predicted 90% interval"
+)
+plt.xlabel("$x$")
+plt.ylabel("$f(x)$")
+plt.ylim(-10, 25)
+plt.legend(loc="upper left")
+plt.title("Prediction with tuned hyper-parameters")
 plt.show()
+
+# %%
+# The plot looks qualitatively better than for the untuned models, especially
+# for the shape of the of lower quantile.
+#
+# We now quantitatively evaluate the joint-calibration of the pair of
+# estimators:
+coverage_fraction(y_train, search_05p.predict(X_train), search_95p.predict(X_train))
+# %%
+coverage_fraction(y_test, search_05p.predict(X_test), search_95p.predict(X_test))
+# %%
+# The calibration of the tuned pair is sadly not better on the test set: the
+# width of the estimated confidence interval is still too narrow.
+#
+# Again, we would need to wrap this study in a cross-validation loop to
+# better assess the variability of those estimates.
('examples/ensemble', 'plot_adaboost_hastie_10_2.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -18,7 +18,6 @@
 .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.

 """
-print(__doc__)

 # Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,
 #         Noel Dawe <noel.dawe@gmail.com>
@@ -36,7 +35,7 @@

 n_estimators = 400
 # A learning rate of 1. may not be optimal for both SAMME and SAMME.R
-learning_rate = 1.
+learning_rate = 1.0

 X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)

@@ -55,23 +54,23 @@
     base_estimator=dt_stump,
     learning_rate=learning_rate,
     n_estimators=n_estimators,
-    algorithm="SAMME")
+    algorithm="SAMME",
+)
 ada_discrete.fit(X_train, y_train)

 ada_real = AdaBoostClassifier(
     base_estimator=dt_stump,
     learning_rate=learning_rate,
     n_estimators=n_estimators,
-    algorithm="SAMME.R")
+    algorithm="SAMME.R",
+)
 ada_real.fit(X_train, y_train)

 fig = plt.figure()
 ax = fig.add_subplot(111)

-ax.plot([1, n_estimators], [dt_stump_err] * 2, 'k-',
-        label='Decision Stump Error')
-ax.plot([1, n_estimators], [dt_err] * 2, 'k--',
-        label='Decision Tree Error')
+ax.plot([1, n_estimators], [dt_stump_err] * 2, "k-", label="Decision Stump Error")
+ax.plot([1, n_estimators], [dt_err] * 2, "k--", label="Decision Tree Error")

 ada_discrete_err = np.zeros((n_estimators,))
 for i, y_pred in enumerate(ada_discrete.staged_predict(X_test)):
@@ -89,24 +88,36 @@
 for i, y_pred in enumerate(ada_real.staged_predict(X_train)):
     ada_real_err_train[i] = zero_one_loss(y_pred, y_train)

-ax.plot(np.arange(n_estimators) + 1, ada_discrete_err,
-        label='Discrete AdaBoost Test Error',
-        color='red')
-ax.plot(np.arange(n_estimators) + 1, ada_discrete_err_train,
-        label='Discrete AdaBoost Train Error',
-        color='blue')
-ax.plot(np.arange(n_estimators) + 1, ada_real_err,
-        label='Real AdaBoost Test Error',
-        color='orange')
-ax.plot(np.arange(n_estimators) + 1, ada_real_err_train,
-        label='Real AdaBoost Train Error',
-        color='green')
+ax.plot(
+    np.arange(n_estimators) + 1,
+    ada_discrete_err,
+    label="Discrete AdaBoost Test Error",
+    color="red",
+)
+ax.plot(
+    np.arange(n_estimators) + 1,
+    ada_discrete_err_train,
+    label="Discrete AdaBoost Train Error",
+    color="blue",
+)
+ax.plot(
+    np.arange(n_estimators) + 1,
+    ada_real_err,
+    label="Real AdaBoost Test Error",
+    color="orange",
+)
+ax.plot(
+    np.arange(n_estimators) + 1,
+    ada_real_err_train,
+    label="Real AdaBoost Train Error",
+    color="green",
+)

 ax.set_ylim((0.0, 0.5))
-ax.set_xlabel('n_estimators')
-ax.set_ylabel('error rate')
+ax.set_xlabel("n_estimators")
+ax.set_ylabel("error rate")

-leg = ax.legend(loc='upper right', fancybox=True)
+leg = ax.legend(loc="upper right", fancybox=True)
 leg.get_frame().set_alpha(0.7)

 plt.show()
('examples/ensemble', 'plot_gradient_boosting_early_stopping.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -17,7 +17,7 @@
 model is trained using the training set and evaluated using the validation set.
 When each additional stage of regression tree is added, the validation set is
 used to score the model.  This is continued until the scores of the model in
-the last ``n_iter_no_change`` stages do not improve by atleast `tol`. After
+the last ``n_iter_no_change`` stages do not improve by at least `tol`. After
 that the model is considered to have converged and further addition of stages
 is "stopped early".

@@ -25,10 +25,11 @@
 ``n_estimators_``.

 This example illustrates how the early stopping can used in the
-:class:`sklearn.ensemble.GradientBoostingClassifier` model to achieve
+:class:`~sklearn.ensemble.GradientBoostingClassifier` model to achieve
 almost the same accuracy as compared to a model built without early stopping
 using many fewer estimators. This can significantly reduce training time,
 memory usage and prediction latency.
+
 """

 # Authors: Vighnesh Birodkar <vighneshbirodkar@nyu.edu>
@@ -44,12 +45,12 @@
 from sklearn import datasets
 from sklearn.model_selection import train_test_split

-print(__doc__)
-
-data_list = [datasets.load_iris(), datasets.load_digits()]
-data_list = [(d.data, d.target) for d in data_list]
-data_list += [datasets.make_hastie_10_2()]
-names = ['Iris Data', 'Digits Data', 'Hastie Data']
+data_list = [
+    datasets.load_iris(return_X_y=True),
+    datasets.make_classification(n_samples=800, random_state=0),
+    datasets.make_hastie_10_2(n_samples=2000, random_state=0),
+]
+names = ["Iris Data", "Classification Data", "Hastie Data"]

 n_gb = []
 score_gb = []
@@ -58,20 +59,23 @@
 score_gbes = []
 time_gbes = []

-n_estimators = 500
+n_estimators = 200

 for X, y in data_list:
-    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
-                                                        random_state=0)
+    X_train, X_test, y_train, y_test = train_test_split(
+        X, y, test_size=0.2, random_state=0
+    )

-    # We specify that if the scores don't improve by atleast 0.01 for the last
+    # We specify that if the scores don't improve by at least 0.01 for the last
     # 10 stages, stop fitting additional stages
-    gbes = ensemble.GradientBoostingClassifier(n_estimators=n_estimators,
-                                               validation_fraction=0.2,
-                                               n_iter_no_change=5, tol=0.01,
-                                               random_state=0)
-    gb = ensemble.GradientBoostingClassifier(n_estimators=n_estimators,
-                                             random_state=0)
+    gbes = ensemble.GradientBoostingClassifier(
+        n_estimators=n_estimators,
+        validation_fraction=0.2,
+        n_iter_no_change=5,
+        tol=0.01,
+        random_state=0,
+    )
+    gb = ensemble.GradientBoostingClassifier(n_estimators=n_estimators, random_state=0)
     start = time.time()
     gb.fit(X_train, y_train)
     time_gb.append(time.time() - start)
@@ -91,16 +95,18 @@
 index = np.arange(0, n * bar_width, bar_width) * 2.5
 index = index[0:n]

-#######################################################################
+# %%
 # Compare scores with and without early stopping
 # ----------------------------------------------

 plt.figure(figsize=(9, 5))

-bar1 = plt.bar(index, score_gb, bar_width, label='Without early stopping',
-               color='crimson')
-bar2 = plt.bar(index + bar_width, score_gbes, bar_width,
-               label='With early stopping', color='coral')
+bar1 = plt.bar(
+    index, score_gb, bar_width, label="Without early stopping", color="crimson"
+)
+bar2 = plt.bar(
+    index + bar_width, score_gbes, bar_width, label="With early stopping", color="coral"
+)

 plt.xticks(index + bar_width, names)
 plt.yticks(np.arange(0, 1.3, 0.1))
@@ -111,34 +117,40 @@
     Attach a text label above each bar displaying n_estimators of each model
     """
     for i, rect in enumerate(rects):
-        plt.text(rect.get_x() + rect.get_width() / 2.,
-                 1.05 * rect.get_height(), 'n_est=%d' % n_estimators[i],
-                 ha='center', va='bottom')
+        plt.text(
+            rect.get_x() + rect.get_width() / 2.0,
+            1.05 * rect.get_height(),
+            "n_est=%d" % n_estimators[i],
+            ha="center",
+            va="bottom",
+        )


 autolabel(bar1, n_gb)
 autolabel(bar2, n_gbes)

 plt.ylim([0, 1.3])
-plt.legend(loc='best')
+plt.legend(loc="best")
 plt.grid(True)

-plt.xlabel('Datasets')
-plt.ylabel('Test score')
+plt.xlabel("Datasets")
+plt.ylabel("Test score")

 plt.show()


-#######################################################################
+# %%
 # Compare fit times with and without early stopping
 # -------------------------------------------------

 plt.figure(figsize=(9, 5))

-bar1 = plt.bar(index, time_gb, bar_width, label='Without early stopping',
-               color='crimson')
-bar2 = plt.bar(index + bar_width, time_gbes, bar_width,
-               label='With early stopping', color='coral')
+bar1 = plt.bar(
+    index, time_gb, bar_width, label="Without early stopping", color="crimson"
+)
+bar2 = plt.bar(
+    index + bar_width, time_gbes, bar_width, label="With early stopping", color="coral"
+)

 max_y = np.amax(np.maximum(time_gb, time_gbes))

@@ -149,10 +161,10 @@
 autolabel(bar2, n_gbes)

 plt.ylim([0, 1.3 * max_y])
-plt.legend(loc='best')
+plt.legend(loc="best")
 plt.grid(True)

-plt.xlabel('Datasets')
-plt.ylabel('Fit Time')
+plt.xlabel("Datasets")
+plt.ylabel("Fit Time")

 plt.show()
('examples/ensemble', 'plot_forest_iris.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -39,16 +39,19 @@
 It is worth noting that RandomForests and ExtraTrees can be fitted in parallel
 on many cores as each tree is built independently of the others. AdaBoost's
 samples are built sequentially and so do not use multiple cores.
+
 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
 from matplotlib.colors import ListedColormap

 from sklearn.datasets import load_iris
-from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,
-                              AdaBoostClassifier)
+from sklearn.ensemble import (
+    RandomForestClassifier,
+    ExtraTreesClassifier,
+    AdaBoostClassifier,
+)
 from sklearn.tree import DecisionTreeClassifier

 # Parameters
@@ -64,11 +67,12 @@

 plot_idx = 1

-models = [DecisionTreeClassifier(max_depth=None),
-          RandomForestClassifier(n_estimators=n_estimators),
-          ExtraTreesClassifier(n_estimators=n_estimators),
-          AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),
-                             n_estimators=n_estimators)]
+models = [
+    DecisionTreeClassifier(max_depth=None),
+    RandomForestClassifier(n_estimators=n_estimators),
+    ExtraTreesClassifier(n_estimators=n_estimators),
+    AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=n_estimators),
+]

 for pair in ([0, 1], [0, 2], [2, 3]):
     for model in models:
@@ -94,15 +98,12 @@
         scores = model.score(X, y)
         # Create a title for each column and the console by using str() and
         # slicing away useless parts of the string
-        model_title = str(type(model)).split(
-            ".")[-1][:-2][:-len("Classifier")]
+        model_title = str(type(model)).split(".")[-1][:-2][: -len("Classifier")]

         model_details = model_title
         if hasattr(model, "estimators_"):
-            model_details += " with {} estimators".format(
-                len(model.estimators_))
-        print(model_details + " with features", pair,
-              "has a score of", scores)
+            model_details += " with {} estimators".format(len(model.estimators_))
+        print(model_details + " with features", pair, "has a score of", scores)

         plt.subplot(3, 4, plot_idx)
         if plot_idx <= len(models):
@@ -113,8 +114,9 @@
         # filled contour plot
         x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
         y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
-        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
-                             np.arange(y_min, y_max, plot_step))
+        xx, yy = np.meshgrid(
+            np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)
+        )

         # Plot either a single DecisionTreeClassifier or alpha blend the
         # decision surfaces of the ensemble of classifiers
@@ -139,19 +141,30 @@
         # black outline
         xx_coarser, yy_coarser = np.meshgrid(
             np.arange(x_min, x_max, plot_step_coarser),
-            np.arange(y_min, y_max, plot_step_coarser))
-        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(),
-                                         yy_coarser.ravel()]
-                                         ).reshape(xx_coarser.shape)
-        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15,
-                                c=Z_points_coarser, cmap=cmap,
-                                edgecolors="none")
+            np.arange(y_min, y_max, plot_step_coarser),
+        )
+        Z_points_coarser = model.predict(
+            np.c_[xx_coarser.ravel(), yy_coarser.ravel()]
+        ).reshape(xx_coarser.shape)
+        cs_points = plt.scatter(
+            xx_coarser,
+            yy_coarser,
+            s=15,
+            c=Z_points_coarser,
+            cmap=cmap,
+            edgecolors="none",
+        )

         # Plot the training points, these are clustered together and have a
         # black outline
-        plt.scatter(X[:, 0], X[:, 1], c=y,
-                    cmap=ListedColormap(['r', 'y', 'b']),
-                    edgecolor='k', s=20)
+        plt.scatter(
+            X[:, 0],
+            X[:, 1],
+            c=y,
+            cmap=ListedColormap(["r", "y", "b"]),
+            edgecolor="k",
+            s=20,
+        )
         plot_idx += 1  # move on to the next plot in sequence

 plt.suptitle("Classifiers on feature subsets of the Iris dataset", fontsize=12)
('examples/ensemble', 'plot_voting_probas.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,23 +3,25 @@
 Plot class probabilities calculated by the VotingClassifier
 ===========================================================

-Plot the class probabilities of the first sample in a toy dataset
-predicted by three different classifiers and averaged by the
-`VotingClassifier`.
+.. currentmodule:: sklearn

-First, three examplary classifiers are initialized (`LogisticRegression`,
-`GaussianNB`, and `RandomForestClassifier`) and used to initialize a
-soft-voting `VotingClassifier` with weights `[1, 1, 5]`, which means that
-the predicted probabilities of the `RandomForestClassifier` count 5 times
-as much as the weights of the other classifiers when the averaged probability
-is calculated.
+Plot the class probabilities of the first sample in a toy dataset predicted by
+three different classifiers and averaged by the
+:class:`~ensemble.VotingClassifier`.
+
+First, three examplary classifiers are initialized
+(:class:`~linear_model.LogisticRegression`, :class:`~naive_bayes.GaussianNB`,
+and :class:`~ensemble.RandomForestClassifier`) and used to initialize a
+soft-voting :class:`~ensemble.VotingClassifier` with weights `[1, 1, 5]`, which
+means that the predicted probabilities of the
+:class:`~ensemble.RandomForestClassifier` count 5 times as much as the weights
+of the other classifiers when the averaged probability is calculated.

 To visualize the probability weighting, we fit each classifier on the training
 set and plot the predicted class probabilities for the first sample in this
 example dataset.

 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -29,15 +31,17 @@
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.ensemble import VotingClassifier

-clf1 = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=123)
+clf1 = LogisticRegression(max_iter=1000, random_state=123)
 clf2 = RandomForestClassifier(n_estimators=100, random_state=123)
 clf3 = GaussianNB()
 X = np.array([[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])
 y = np.array([1, 1, 2, 2])

-eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
-                        voting='soft',
-                        weights=[1, 1, 5])
+eclf = VotingClassifier(
+    estimators=[("lr", clf1), ("rf", clf2), ("gnb", clf3)],
+    voting="soft",
+    weights=[1, 1, 5],
+)

 # predict class probabilities for all classifiers
 probas = [c.fit(X, y).predict_proba(X) for c in (clf1, clf2, clf3, eclf)]
@@ -56,28 +60,36 @@
 fig, ax = plt.subplots()

 # bars for classifier 1-3
-p1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), width,
-            color='green', edgecolor='k')
-p2 = ax.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width,
-            color='lightgreen', edgecolor='k')
+p1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), width, color="green", edgecolor="k")
+p2 = ax.bar(
+    ind + width,
+    np.hstack(([class2_1[:-1], [0]])),
+    width,
+    color="lightgreen",
+    edgecolor="k",
+)

 # bars for VotingClassifier
-p3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width,
-            color='blue', edgecolor='k')
-p4 = ax.bar(ind + width, [0, 0, 0, class2_1[-1]], width,
-            color='steelblue', edgecolor='k')
+p3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width, color="blue", edgecolor="k")
+p4 = ax.bar(
+    ind + width, [0, 0, 0, class2_1[-1]], width, color="steelblue", edgecolor="k"
+)

 # plot annotations
-plt.axvline(2.8, color='k', linestyle='dashed')
+plt.axvline(2.8, color="k", linestyle="dashed")
 ax.set_xticks(ind + width)
-ax.set_xticklabels(['LogisticRegression\nweight 1',
-                    'GaussianNB\nweight 1',
-                    'RandomForestClassifier\nweight 5',
-                    'VotingClassifier\n(average probabilities)'],
-                   rotation=40,
-                   ha='right')
+ax.set_xticklabels(
+    [
+        "LogisticRegression\nweight 1",
+        "GaussianNB\nweight 1",
+        "RandomForestClassifier\nweight 5",
+        "VotingClassifier\n(average probabilities)",
+    ],
+    rotation=40,
+    ha="right",
+)
 plt.ylim([0, 1])
-plt.title('Class probabilities for sample 1 by different classifiers')
-plt.legend([p1[0], p2[0]], ['class 1', 'class 2'], loc='upper left')
+plt.title("Class probabilities for sample 1 by different classifiers")
+plt.legend([p1[0], p2[0]], ["class 1", "class 2"], loc="upper left")
 plt.tight_layout()
 plt.show()
('examples/ensemble', 'plot_gradient_boosting_regularization.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -18,8 +18,8 @@

 .. [1] T. Hastie, R. Tibshirani and J. Friedman, "Elements of Statistical
     Learning Ed. 2", Springer, 2009.
+
 """
-print(__doc__)

 # Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>
 #
@@ -31,31 +31,40 @@
 from sklearn import ensemble
 from sklearn import datasets

+from sklearn.model_selection import train_test_split

-X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)
-X = X.astype(np.float32)
+X, y = datasets.make_hastie_10_2(n_samples=4000, random_state=1)

 # map labels from {-1, 1} to {0, 1}
 labels, y = np.unique(y, return_inverse=True)

-X_train, X_test = X[:2000], X[2000:]
-y_train, y_test = y[:2000], y[2000:]
+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=0)

-original_params = {'n_estimators': 1000, 'max_leaf_nodes': 4, 'max_depth': None, 'random_state': 2,
-                   'min_samples_split': 5}
+original_params = {
+    "n_estimators": 400,
+    "max_leaf_nodes": 4,
+    "max_depth": None,
+    "random_state": 2,
+    "min_samples_split": 5,
+}

 plt.figure()

-for label, color, setting in [('No shrinkage', 'orange',
-                               {'learning_rate': 1.0, 'subsample': 1.0}),
-                              ('learning_rate=0.1', 'turquoise',
-                               {'learning_rate': 0.1, 'subsample': 1.0}),
-                              ('subsample=0.5', 'blue',
-                               {'learning_rate': 1.0, 'subsample': 0.5}),
-                              ('learning_rate=0.1, subsample=0.5', 'gray',
-                               {'learning_rate': 0.1, 'subsample': 0.5}),
-                              ('learning_rate=0.1, max_features=2', 'magenta',
-                               {'learning_rate': 0.1, 'max_features': 2})]:
+for label, color, setting in [
+    ("No shrinkage", "orange", {"learning_rate": 1.0, "subsample": 1.0}),
+    ("learning_rate=0.2", "turquoise", {"learning_rate": 0.2, "subsample": 1.0}),
+    ("subsample=0.5", "blue", {"learning_rate": 1.0, "subsample": 0.5}),
+    (
+        "learning_rate=0.2, subsample=0.5",
+        "gray",
+        {"learning_rate": 0.2, "subsample": 0.5},
+    ),
+    (
+        "learning_rate=0.2, max_features=2",
+        "magenta",
+        {"learning_rate": 0.2, "max_features": 2},
+    ),
+]:
     params = dict(original_params)
     params.update(setting)

@@ -63,17 +72,22 @@
     clf.fit(X_train, y_train)

     # compute test set deviance
-    test_deviance = np.zeros((params['n_estimators'],), dtype=np.float64)
+    test_deviance = np.zeros((params["n_estimators"],), dtype=np.float64)

     for i, y_pred in enumerate(clf.staged_decision_function(X_test)):
         # clf.loss_ assumes that y_test[i] in {0, 1}
         test_deviance[i] = clf.loss_(y_test, y_pred)

-    plt.plot((np.arange(test_deviance.shape[0]) + 1)[::5], test_deviance[::5],
-            '-', color=color, label=label)
+    plt.plot(
+        (np.arange(test_deviance.shape[0]) + 1)[::5],
+        test_deviance[::5],
+        "-",
+        color=color,
+        label=label,
+    )

-plt.legend(loc='upper left')
-plt.xlabel('Boosting Iterations')
-plt.ylabel('Test Set Deviance')
+plt.legend(loc="upper left")
+plt.xlabel("Boosting Iterations")
+plt.ylabel("Test Set Deviance")

 plt.show()
('examples/ensemble', 'plot_voting_decision_regions.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,26 +3,28 @@
 Plot the decision boundaries of a VotingClassifier
 ==================================================

-Plot the decision boundaries of a `VotingClassifier` for
-two features of the Iris dataset.
+.. currentmodule:: sklearn

-Plot the class probabilities of the first sample in a toy dataset
-predicted by three different classifiers and averaged by the
-`VotingClassifier`.
+Plot the decision boundaries of a :class:`~ensemble.VotingClassifier` for two
+features of the Iris dataset.

-First, three exemplary classifiers are initialized (`DecisionTreeClassifier`,
-`KNeighborsClassifier`, and `SVC`) and used to initialize a
-soft-voting `VotingClassifier` with weights `[2, 1, 2]`, which means that
-the predicted probabilities of the `DecisionTreeClassifier` and `SVC`
-count 5 times as much as the weights of the `KNeighborsClassifier` classifier
-when the averaged probability is calculated.
+Plot the class probabilities of the first sample in a toy dataset predicted by
+three different classifiers and averaged by the
+:class:`~ensemble.VotingClassifier`.
+
+First, three exemplary classifiers are initialized
+(:class:`~tree.DecisionTreeClassifier`,
+:class:`~neighbors.KNeighborsClassifier`, and :class:`~svm.SVC`) and used to
+initialize a soft-voting :class:`~ensemble.VotingClassifier` with weights `[2,
+1, 2]`, which means that the predicted probabilities of the
+:class:`~tree.DecisionTreeClassifier` and :class:`~svm.SVC` each count 2 times
+as much as the weights of the :class:`~neighbors.KNeighborsClassifier`
+classifier when the averaged probability is calculated.

 """
-print(__doc__)

 from itertools import product

-import numpy as np
 import matplotlib.pyplot as plt

 from sklearn import datasets
@@ -30,6 +32,7 @@
 from sklearn.neighbors import KNeighborsClassifier
 from sklearn.svm import SVC
 from sklearn.ensemble import VotingClassifier
+from sklearn.inspection import DecisionBoundaryDisplay

 # Loading some example data
 iris = datasets.load_iris()
@@ -39,10 +42,12 @@
 # Training classifiers
 clf1 = DecisionTreeClassifier(max_depth=4)
 clf2 = KNeighborsClassifier(n_neighbors=7)
-clf3 = SVC(gamma=.1, kernel='rbf', probability=True)
-eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2),
-                                    ('svc', clf3)],
-                        voting='soft', weights=[2, 1, 2])
+clf3 = SVC(gamma=0.1, kernel="rbf", probability=True)
+eclf = VotingClassifier(
+    estimators=[("dt", clf1), ("knn", clf2), ("svc", clf3)],
+    voting="soft",
+    weights=[2, 1, 2],
+)

 clf1.fit(X, y)
 clf2.fit(X, y)
@@ -50,24 +55,16 @@
 eclf.fit(X, y)

 # Plotting decision regions
-x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
-y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
-xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
-                     np.arange(y_min, y_max, 0.1))
-
-f, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10, 8))
-
-for idx, clf, tt in zip(product([0, 1], [0, 1]),
-                        [clf1, clf2, clf3, eclf],
-                        ['Decision Tree (depth=4)', 'KNN (k=7)',
-                         'Kernel SVM', 'Soft Voting']):
-
-    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
-    Z = Z.reshape(xx.shape)
-
-    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)
-    axarr[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y,
-                                  s=20, edgecolor='k')
+f, axarr = plt.subplots(2, 2, sharex="col", sharey="row", figsize=(10, 8))
+for idx, clf, tt in zip(
+    product([0, 1], [0, 1]),
+    [clf1, clf2, clf3, eclf],
+    ["Decision Tree (depth=4)", "KNN (k=7)", "Kernel SVM", "Soft Voting"],
+):
+    DecisionBoundaryDisplay.from_estimator(
+        clf, X, alpha=0.4, ax=axarr[idx[0], idx[1]], response_method="predict"
+    )
+    axarr[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor="k")
     axarr[idx[0], idx[1]].set_title(tt)

 plt.show()
('examples/ensemble', 'plot_voting_regressor.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,51 +3,86 @@
 Plot individual and voting regression predictions
 =================================================

-Plot individual and averaged regression predictions for Boston dataset.
+.. currentmodule:: sklearn

-First, three exemplary regressors are initialized (`GradientBoostingRegressor`,
-`RandomForestRegressor`, and `LinearRegression`) and used to initialize a
-`VotingRegressor`.
+A voting regressor is an ensemble meta-estimator that fits several base
+regressors, each on the whole dataset. Then it averages the individual
+predictions to form a final prediction.
+We will use three different regressors to predict the data:
+:class:`~ensemble.GradientBoostingRegressor`,
+:class:`~ensemble.RandomForestRegressor`, and
+:class:`~linear_model.LinearRegression`).
+Then the above 3 regressors will be used for the
+:class:`~ensemble.VotingRegressor`.

-The red starred dots are the averaged predictions.
+Finally, we will plot the predictions made by all models for comparison.
+
+We will work with the diabetes dataset which consists of 10 features
+collected from a cohort of diabetes patients. The target is a quantitative
+measure of disease progression one year after baseline.

 """
-print(__doc__)

 import matplotlib.pyplot as plt

-from sklearn import datasets
+from sklearn.datasets import load_diabetes
 from sklearn.ensemble import GradientBoostingRegressor
 from sklearn.ensemble import RandomForestRegressor
 from sklearn.linear_model import LinearRegression
 from sklearn.ensemble import VotingRegressor

-# Loading some example data
-boston = datasets.load_boston()
-X = boston.data
-y = boston.target
+# %%
+# Training classifiers
+# --------------------------------
+#
+# First, we will load the diabetes dataset and initiate a gradient boosting
+# regressor, a random forest regressor and a linear regression. Next, we will
+# use the 3 regressors to build the voting regressor:

-# Training classifiers
-reg1 = GradientBoostingRegressor(random_state=1, n_estimators=10)
-reg2 = RandomForestRegressor(random_state=1, n_estimators=10)
+X, y = load_diabetes(return_X_y=True)
+
+# Train classifiers
+reg1 = GradientBoostingRegressor(random_state=1)
+reg2 = RandomForestRegressor(random_state=1)
 reg3 = LinearRegression()
-ereg = VotingRegressor([('gb', reg1), ('rf', reg2), ('lr', reg3)])
+
 reg1.fit(X, y)
 reg2.fit(X, y)
 reg3.fit(X, y)
+
+ereg = VotingRegressor([("gb", reg1), ("rf", reg2), ("lr", reg3)])
 ereg.fit(X, y)
+
+# %%
+# Making predictions
+# --------------------------------
+#
+# Now we will use each of the regressors to make the 20 first predictions.

 xt = X[:20]

+pred1 = reg1.predict(xt)
+pred2 = reg2.predict(xt)
+pred3 = reg3.predict(xt)
+pred4 = ereg.predict(xt)
+
+# %%
+# Plot the results
+# --------------------------------
+#
+# Finally, we will visualize the 20 predictions. The red stars show the average
+# prediction made by :class:`~ensemble.VotingRegressor`.
+
 plt.figure()
-plt.plot(reg1.predict(xt), 'gd', label='GradientBoostingRegressor')
-plt.plot(reg2.predict(xt), 'b^', label='RandomForestRegressor')
-plt.plot(reg3.predict(xt), 'ys', label='LinearRegression')
-plt.plot(ereg.predict(xt), 'r*', label='VotingRegressor')
-plt.tick_params(axis='x', which='both', bottom=False, top=False,
-                labelbottom=False)
-plt.ylabel('predicted')
-plt.xlabel('training samples')
+plt.plot(pred1, "gd", label="GradientBoostingRegressor")
+plt.plot(pred2, "b^", label="RandomForestRegressor")
+plt.plot(pred3, "ys", label="LinearRegression")
+plt.plot(pred4, "r*", ms=10, label="VotingRegressor")
+
+plt.tick_params(axis="x", which="both", bottom=False, top=False, labelbottom=False)
+plt.ylabel("predicted")
+plt.xlabel("training samples")
 plt.legend(loc="best")
-plt.title('Comparison of individual predictions with averaged')
+plt.title("Regressor predictions and their average")
+
 plt.show()
('examples/cluster', 'plot_affinity_propagation.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,56 +8,67 @@
 Between Data Points", Science Feb. 2007

 """
-print(__doc__)

 from sklearn.cluster import AffinityPropagation
 from sklearn import metrics
-from sklearn.datasets.samples_generator import make_blobs
+from sklearn.datasets import make_blobs

-# #############################################################################
+# %%
 # Generate sample data
+# --------------------
 centers = [[1, 1], [-1, -1], [1, -1]]
-X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5,
-                            random_state=0)
+X, labels_true = make_blobs(
+    n_samples=300, centers=centers, cluster_std=0.5, random_state=0
+)

-# #############################################################################
+# %%
 # Compute Affinity Propagation
-af = AffinityPropagation(preference=-50).fit(X)
+# ----------------------------
+af = AffinityPropagation(preference=-50, random_state=0).fit(X)
 cluster_centers_indices = af.cluster_centers_indices_
 labels = af.labels_

 n_clusters_ = len(cluster_centers_indices)

-print('Estimated number of clusters: %d' % n_clusters_)
+print("Estimated number of clusters: %d" % n_clusters_)
 print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))
 print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))
 print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))
-print("Adjusted Rand Index: %0.3f"
-      % metrics.adjusted_rand_score(labels_true, labels))
-print("Adjusted Mutual Information: %0.3f"
-      % metrics.adjusted_mutual_info_score(labels_true, labels,
-                                           average_method='arithmetic'))
-print("Silhouette Coefficient: %0.3f"
-      % metrics.silhouette_score(X, labels, metric='sqeuclidean'))
+print("Adjusted Rand Index: %0.3f" % metrics.adjusted_rand_score(labels_true, labels))
+print(
+    "Adjusted Mutual Information: %0.3f"
+    % metrics.adjusted_mutual_info_score(labels_true, labels)
+)
+print(
+    "Silhouette Coefficient: %0.3f"
+    % metrics.silhouette_score(X, labels, metric="sqeuclidean")
+)

-# #############################################################################
+# %%
 # Plot result
+# -----------
 import matplotlib.pyplot as plt
 from itertools import cycle

-plt.close('all')
+plt.close("all")
 plt.figure(1)
 plt.clf()

-colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')
+colors = cycle("bgrcmykbgrcmykbgrcmykbgrcmyk")
 for k, col in zip(range(n_clusters_), colors):
     class_members = labels == k
     cluster_center = X[cluster_centers_indices[k]]
-    plt.plot(X[class_members, 0], X[class_members, 1], col + '.')
-    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,
-             markeredgecolor='k', markersize=14)
+    plt.plot(X[class_members, 0], X[class_members, 1], col + ".")
+    plt.plot(
+        cluster_center[0],
+        cluster_center[1],
+        "o",
+        markerfacecolor=col,
+        markeredgecolor="k",
+        markersize=14,
+    )
     for x in X[class_members]:
         plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)

-plt.title('Estimated number of clusters: %d' % n_clusters_)
+plt.title("Estimated number of clusters: %d" % n_clusters_)
 plt.show()
('examples/cluster', 'plot_inductive_clustering.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,7 +1,7 @@
 """
-==============================================
+====================
 Inductive Clustering
-==============================================
+====================

 Clustering can be expensive, especially when our dataset contains millions
 of datapoints. Many clustering algorithms are not :term:`inductive` and so
@@ -17,22 +17,37 @@

 This example illustrates a generic implementation of a meta-estimator which
 extends clustering by inducing a classifier from the cluster labels.
+
 """
+
 # Authors: Chirag Nagpal
 #          Christos Aridas
-print(__doc__)

-import numpy as np
 import matplotlib.pyplot as plt
 from sklearn.base import BaseEstimator, clone
 from sklearn.cluster import AgglomerativeClustering
 from sklearn.datasets import make_blobs
 from sklearn.ensemble import RandomForestClassifier
-from sklearn.utils.metaestimators import if_delegate_has_method
+from sklearn.inspection import DecisionBoundaryDisplay
+from sklearn.utils.metaestimators import available_if
+from sklearn.utils.validation import check_is_fitted


 N_SAMPLES = 5000
 RANDOM_STATE = 42
+
+
+def _classifier_has(attr):
+    """Check if we can delegate a method to the underlying classifier.
+
+    First, we check the first fitted classifier if available, otherwise we
+    check the unfitted classifier.
+    """
+    return lambda estimator: (
+        hasattr(estimator.classifier_, attr)
+        if hasattr(estimator, "classifier_")
+        else hasattr(estimator.classifier, attr)
+    )


 class InductiveClusterer(BaseEstimator):
@@ -47,28 +62,28 @@
         self.classifier_.fit(X, y)
         return self

-    @if_delegate_has_method(delegate='classifier_')
+    @available_if(_classifier_has("predict"))
     def predict(self, X):
+        check_is_fitted(self)
         return self.classifier_.predict(X)

-    @if_delegate_has_method(delegate='classifier_')
+    @available_if(_classifier_has("decision_function"))
     def decision_function(self, X):
+        check_is_fitted(self)
         return self.classifier_.decision_function(X)


-def plot_scatter(X,  color, alpha=0.5):
-    return plt.scatter(X[:, 0],
-                       X[:, 1],
-                       c=color,
-                       alpha=alpha,
-                       edgecolor='k')
+def plot_scatter(X, color, alpha=0.5):
+    return plt.scatter(X[:, 0], X[:, 1], c=color, alpha=alpha, edgecolor="k")


 # Generate some training data from clustering
-X, y = make_blobs(n_samples=N_SAMPLES,
-                  cluster_std=[1.0, 1.0, 0.5],
-                  centers=[(-5, -5), (0, 0), (5, 5)],
-                  random_state=RANDOM_STATE)
+X, y = make_blobs(
+    n_samples=N_SAMPLES,
+    cluster_std=[1.0, 1.0, 0.5],
+    centers=[(-5, -5), (0, 0), (5, 5)],
+    random_state=RANDOM_STATE,
+)


 # Train a clustering algorithm on the training data and get the cluster labels
@@ -83,13 +98,13 @@


 # Generate new samples and plot them along with the original dataset
-X_new, y_new = make_blobs(n_samples=10,
-                          centers=[(-7, -1), (-2, 4), (3, 6)],
-                          random_state=RANDOM_STATE)
+X_new, y_new = make_blobs(
+    n_samples=10, centers=[(-7, -1), (-2, 4), (3, 6)], random_state=RANDOM_STATE
+)

 plt.subplot(132)
 plot_scatter(X, cluster_labels)
-plot_scatter(X_new, 'black', 1)
+plot_scatter(X_new, "black", 1)
 plt.title("Unknown instances")


@@ -101,20 +116,14 @@
 probable_clusters = inductive_learner.predict(X_new)


-plt.subplot(133)
+ax = plt.subplot(133)
 plot_scatter(X, cluster_labels)
 plot_scatter(X_new, probable_clusters)

 # Plotting decision regions
-x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
-y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
-xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
-                     np.arange(y_min, y_max, 0.1))
-
-Z = inductive_learner.predict(np.c_[xx.ravel(), yy.ravel()])
-Z = Z.reshape(xx.shape)
-
-plt.contourf(xx, yy, Z, alpha=0.4)
+DecisionBoundaryDisplay.from_estimator(
+    inductive_learner, X, response_method="predict", alpha=0.4, ax=ax
+)
 plt.title("Classify unknown instances")

 plt.show()
('examples/cluster', 'plot_feature_agglomeration_vs_univariate_selection.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -11,13 +11,13 @@

 Both methods are compared in a regression problem using
 a BayesianRidge as supervised estimator.
+
 """

 # Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
 # License: BSD 3 clause

-print(__doc__)
-
+# %%
 import shutil
 import tempfile

@@ -34,63 +34,68 @@
 from sklearn.model_selection import GridSearchCV
 from sklearn.model_selection import KFold

-# #############################################################################
-# Generate data
+# %%
+# Set parameters
 n_samples = 200
 size = 40  # image size
 roi_size = 15
-snr = 5.
+snr = 5.0
 np.random.seed(0)
-mask = np.ones([size, size], dtype=np.bool)

+# %%
+# Generate data
 coef = np.zeros((size, size))
-coef[0:roi_size, 0:roi_size] = -1.
-coef[-roi_size:, -roi_size:] = 1.
+coef[0:roi_size, 0:roi_size] = -1.0
+coef[-roi_size:, -roi_size:] = 1.0

-X = np.random.randn(n_samples, size ** 2)
+X = np.random.randn(n_samples, size**2)
 for x in X:  # smooth data
     x[:] = ndimage.gaussian_filter(x.reshape(size, size), sigma=1.0).ravel()
 X -= X.mean(axis=0)
 X /= X.std(axis=0)

 y = np.dot(X, coef.ravel())
+
+# %%
+# add noise
 noise = np.random.randn(y.shape[0])
-noise_coef = (linalg.norm(y, 2) / np.exp(snr / 20.)) / linalg.norm(noise, 2)
-y += noise_coef * noise  # add noise
+noise_coef = (linalg.norm(y, 2) / np.exp(snr / 20.0)) / linalg.norm(noise, 2)
+y += noise_coef * noise

-# #############################################################################
+# %%
 # Compute the coefs of a Bayesian Ridge with GridSearch
 cv = KFold(2)  # cross-validation generator for model selection
 ridge = BayesianRidge()
 cachedir = tempfile.mkdtemp()
 mem = Memory(location=cachedir, verbose=1)

+# %%
 # Ward agglomeration followed by BayesianRidge
 connectivity = grid_to_graph(n_x=size, n_y=size)
-ward = FeatureAgglomeration(n_clusters=10, connectivity=connectivity,
-                            memory=mem)
-clf = Pipeline([('ward', ward), ('ridge', ridge)])
+ward = FeatureAgglomeration(n_clusters=10, connectivity=connectivity, memory=mem)
+clf = Pipeline([("ward", ward), ("ridge", ridge)])
 # Select the optimal number of parcels with grid search
-clf = GridSearchCV(clf, {'ward__n_clusters': [10, 20, 30]}, n_jobs=1, cv=cv)
+clf = GridSearchCV(clf, {"ward__n_clusters": [10, 20, 30]}, n_jobs=1, cv=cv)
 clf.fit(X, y)  # set the best parameters
 coef_ = clf.best_estimator_.steps[-1][1].coef_
 coef_ = clf.best_estimator_.steps[0][1].inverse_transform(coef_)
 coef_agglomeration_ = coef_.reshape(size, size)

+# %%
 # Anova univariate feature selection followed by BayesianRidge
 f_regression = mem.cache(feature_selection.f_regression)  # caching function
 anova = feature_selection.SelectPercentile(f_regression)
-clf = Pipeline([('anova', anova), ('ridge', ridge)])
+clf = Pipeline([("anova", anova), ("ridge", ridge)])
 # Select the optimal percentage of features with grid search
-clf = GridSearchCV(clf, {'anova__percentile': [5, 10, 20]}, cv=cv)
+clf = GridSearchCV(clf, {"anova__percentile": [5, 10, 20]}, cv=cv)
 clf.fit(X, y)  # set the best parameters
 coef_ = clf.best_estimator_.steps[-1][1].coef_
 coef_ = clf.best_estimator_.steps[0][1].inverse_transform(coef_.reshape(1, -1))
 coef_selection_ = coef_.reshape(size, size)

-# #############################################################################
+# %%
 # Inverse the transformation to plot the results on an image
-plt.close('all')
+plt.close("all")
 plt.figure(figsize=(7.3, 2.7))
 plt.subplot(1, 3, 1)
 plt.imshow(coef, interpolation="nearest", cmap=plt.cm.RdBu_r)
@@ -104,5 +109,6 @@
 plt.subplots_adjust(0.04, 0.0, 0.98, 0.94, 0.16, 0.26)
 plt.show()

+# %%
 # Attempt to remove the temporary cachedir, but don't worry if it fails
 shutil.rmtree(cachedir, ignore_errors=True)
('examples/cluster', 'plot_digits_linkage.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -12,68 +12,63 @@

 What this example shows us is the behavior "rich getting richer" of
 agglomerative clustering that tends to create uneven cluster sizes.
+
 This behavior is pronounced for the average linkage strategy,
-that ends up with a couple of singleton clusters, while in the case
-of single linkage we get a single central cluster with all other clusters
-being drawn from noise points around the fringes.
+that ends up with a couple of clusters with few datapoints.
+
+The case of single linkage is even more pathologic with a very
+large cluster covering most digits, an intermediate size (clean)
+cluster with most zero digits and all other clusters being drawn
+from noise points around the fringes.
+
+The other linkage strategies lead to more evenly distributed
+clusters that are therefore likely to be less sensible to a
+random resampling of the dataset.
+
 """

 # Authors: Gael Varoquaux
 # License: BSD 3 clause (C) INRIA 2014

-print(__doc__)
 from time import time

 import numpy as np
-from scipy import ndimage
 from matplotlib import pyplot as plt

 from sklearn import manifold, datasets

-digits = datasets.load_digits(n_class=10)
-X = digits.data
-y = digits.target
+digits = datasets.load_digits()
+X, y = digits.data, digits.target
 n_samples, n_features = X.shape

 np.random.seed(0)

-def nudge_images(X, y):
-    # Having a larger dataset shows more clearly the behavior of the
-    # methods, but we multiply the size of the dataset only by 2, as the
-    # cost of the hierarchical clustering methods are strongly
-    # super-linear in n_samples
-    shift = lambda x: ndimage.shift(x.reshape((8, 8)),
-                                  .3 * np.random.normal(size=2),
-                                  mode='constant',
-                                  ).ravel()
-    X = np.concatenate([X, np.apply_along_axis(shift, 1, X)])
-    Y = np.concatenate([y, y], axis=0)
-    return X, Y

-
-X, y = nudge_images(X, y)
-
-
-#----------------------------------------------------------------------
+# ----------------------------------------------------------------------
 # Visualize the clustering
 def plot_clustering(X_red, labels, title=None):
     x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)
     X_red = (X_red - x_min) / (x_max - x_min)

     plt.figure(figsize=(6, 4))
-    for i in range(X_red.shape[0]):
-        plt.text(X_red[i, 0], X_red[i, 1], str(y[i]),
-                 color=plt.cm.nipy_spectral(labels[i] / 10.),
-                 fontdict={'weight': 'bold', 'size': 9})
+    for digit in digits.target_names:
+        plt.scatter(
+            *X_red[y == digit].T,
+            marker=f"${digit}$",
+            s=50,
+            c=plt.cm.nipy_spectral(labels[y == digit] / 10),
+            alpha=0.5,
+        )

     plt.xticks([])
     plt.yticks([])
     if title is not None:
         plt.title(title, size=17)
-    plt.axis('off')
+    plt.axis("off")
     plt.tight_layout(rect=[0, 0.03, 1, 0.95])

-#----------------------------------------------------------------------
+
+# ----------------------------------------------------------------------
 # 2D embedding of the digits dataset
 print("Computing embedding")
 X_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X)
@@ -81,7 +76,7 @@

 from sklearn.cluster import AgglomerativeClustering

-for linkage in ('ward', 'average', 'complete', 'single'):
+for linkage in ("ward", "average", "complete", "single"):
     clustering = AgglomerativeClustering(linkage=linkage, n_clusters=10)
     t0 = time()
     clustering.fit(X_red)
('examples/cluster', 'plot_linkage_comparison.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -20,8 +20,8 @@
 While these examples give some intuition about the
 algorithms, this intuition might not apply to very high
 dimensional data.
+
 """
-print(__doc__)

 import time
 import warnings
@@ -35,14 +35,13 @@

 np.random.seed(0)

-######################################################################
+# %%
 # Generate datasets. We choose the size big enough to see the scalability
 # of the algorithms, but not too big to avoid too long running times

 n_samples = 1500
-noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,
-                                      noise=.05)
-noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)
+noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)
+noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)
 blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)
 no_structure = np.random.rand(n_samples, 2), None

@@ -54,30 +53,31 @@
 aniso = (X_aniso, y)

 # blobs with varied variances
-varied = datasets.make_blobs(n_samples=n_samples,
-                             cluster_std=[1.0, 2.5, 0.5],
-                             random_state=random_state)
+varied = datasets.make_blobs(
+    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state
+)

-######################################################################
+# %%
 # Run the clustering and plot

 # Set up cluster parameters
 plt.figure(figsize=(9 * 1.3 + 2, 14.5))
-plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,
-                    hspace=.01)
+plt.subplots_adjust(
+    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01
+)

 plot_num = 1

-default_base = {'n_neighbors': 10,
-                'n_clusters': 3}
+default_base = {"n_neighbors": 10, "n_clusters": 3}

 datasets = [
-    (noisy_circles, {'n_clusters': 2}),
-    (noisy_moons, {'n_clusters': 2}),
-    (varied, {'n_neighbors': 2}),
-    (aniso, {'n_neighbors': 2}),
+    (noisy_circles, {"n_clusters": 2}),
+    (noisy_moons, {"n_clusters": 2}),
+    (varied, {"n_neighbors": 2}),
+    (aniso, {"n_neighbors": 2}),
     (blobs, {}),
-    (no_structure, {})]
+    (no_structure, {}),
+]

 for i_dataset, (dataset, algo_params) in enumerate(datasets):
     # update parameters with dataset-specific values
@@ -93,19 +93,23 @@
     # Create cluster objects
     # ============
     ward = cluster.AgglomerativeClustering(
-        n_clusters=params['n_clusters'], linkage='ward')
+        n_clusters=params["n_clusters"], linkage="ward"
+    )
     complete = cluster.AgglomerativeClustering(
-        n_clusters=params['n_clusters'], linkage='complete')
+        n_clusters=params["n_clusters"], linkage="complete"
+    )
     average = cluster.AgglomerativeClustering(
-        n_clusters=params['n_clusters'], linkage='average')
+        n_clusters=params["n_clusters"], linkage="average"
+    )
     single = cluster.AgglomerativeClustering(
-        n_clusters=params['n_clusters'], linkage='single')
+        n_clusters=params["n_clusters"], linkage="single"
+    )

     clustering_algorithms = (
-        ('Single Linkage', single),
-        ('Average Linkage', average),
-        ('Complete Linkage', complete),
-        ('Ward Linkage', ward),
+        ("Single Linkage", single),
+        ("Average Linkage", average),
+        ("Complete Linkage", complete),
+        ("Ward Linkage", ward),
     )

     for name, algorithm in clustering_algorithms:
@@ -115,15 +119,16 @@
         with warnings.catch_warnings():
             warnings.filterwarnings(
                 "ignore",
-                message="the number of connected components of the " +
-                "connectivity matrix is [0-9]{1,2}" +
-                " > 1. Completing it to avoid stopping the tree early.",
-                category=UserWarning)
+                message="the number of connected components of the "
+                + "connectivity matrix is [0-9]{1,2}"
+                + " > 1. Completing it to avoid stopping the tree early.",
+                category=UserWarning,
+            )
             algorithm.fit(X)

         t1 = time.time()
-        if hasattr(algorithm, 'labels_'):
-            y_pred = algorithm.labels_.astype(np.int)
+        if hasattr(algorithm, "labels_"):
+            y_pred = algorithm.labels_.astype(int)
         else:
             y_pred = algorithm.predict(X)

@@ -131,19 +136,40 @@
         if i_dataset == 0:
             plt.title(name, size=18)

-        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',
-                                             '#f781bf', '#a65628', '#984ea3',
-                                             '#999999', '#e41a1c', '#dede00']),
-                                      int(max(y_pred) + 1))))
+        colors = np.array(
+            list(
+                islice(
+                    cycle(
+                        [
+                            "#377eb8",
+                            "#ff7f00",
+                            "#4daf4a",
+                            "#f781bf",
+                            "#a65628",
+                            "#984ea3",
+                            "#999999",
+                            "#e41a1c",
+                            "#dede00",
+                        ]
+                    ),
+                    int(max(y_pred) + 1),
+                )
+            )
+        )
         plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])

         plt.xlim(-2.5, 2.5)
         plt.ylim(-2.5, 2.5)
         plt.xticks(())
         plt.yticks(())
-        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),
-                 transform=plt.gca().transAxes, size=15,
-                 horizontalalignment='right')
+        plt.text(
+            0.99,
+            0.01,
+            ("%.2fs" % (t1 - t0)).lstrip("0"),
+            transform=plt.gca().transAxes,
+            size=15,
+            horizontalalignment="right",
+        )
         plot_num += 1

 plt.show()
('examples/cluster', 'plot_color_quantization.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -17,14 +17,15 @@

 For comparison, a quantized image using a random codebook (colors picked up
 randomly) is also shown.
+
 """
+
 # Authors: Robert Layton <robertlayton@gmail.com>
 #          Olivier Grisel <olivier.grisel@ensta.org>
 #          Mathieu Blondel <mathieu@mblondel.org>
 #
 # License: BSD 3 clause

-print(__doc__)
 import numpy as np
 import matplotlib.pyplot as plt
 from sklearn.cluster import KMeans
@@ -50,53 +51,45 @@

 print("Fitting model on a small sub-sample of the data")
 t0 = time()
-image_array_sample = shuffle(image_array, random_state=0)[:1000]
+image_array_sample = shuffle(image_array, random_state=0, n_samples=1_000)
 kmeans = KMeans(n_clusters=n_colors, random_state=0).fit(image_array_sample)
-print("done in %0.3fs." % (time() - t0))
+print(f"done in {time() - t0:0.3f}s.")

 # Get labels for all points
 print("Predicting color indices on the full image (k-means)")
 t0 = time()
 labels = kmeans.predict(image_array)
-print("done in %0.3fs." % (time() - t0))
+print(f"done in {time() - t0:0.3f}s.")


-codebook_random = shuffle(image_array, random_state=0)[:n_colors]
+codebook_random = shuffle(image_array, random_state=0, n_samples=n_colors)
 print("Predicting color indices on the full image (random)")
 t0 = time()
-labels_random = pairwise_distances_argmin(codebook_random,
-                                          image_array,
-                                          axis=0)
-print("done in %0.3fs." % (time() - t0))
+labels_random = pairwise_distances_argmin(codebook_random, image_array, axis=0)
+print(f"done in {time() - t0:0.3f}s.")


 def recreate_image(codebook, labels, w, h):
     """Recreate the (compressed) image from the code book & labels"""
-    d = codebook.shape[1]
-    image = np.zeros((w, h, d))
-    label_idx = 0
-    for i in range(w):
-        for j in range(h):
-            image[i][j] = codebook[labels[label_idx]]
-            label_idx += 1
-    return image
+    return codebook[labels].reshape(w, h, -1)
+

 # Display all results, alongside original image
 plt.figure(1)
 plt.clf()
-plt.axis('off')
-plt.title('Original image (96,615 colors)')
+plt.axis("off")
+plt.title("Original image (96,615 colors)")
 plt.imshow(china)

 plt.figure(2)
 plt.clf()
-plt.axis('off')
-plt.title('Quantized image (64 colors, K-Means)')
+plt.axis("off")
+plt.title(f"Quantized image ({n_colors} colors, K-Means)")
 plt.imshow(recreate_image(kmeans.cluster_centers_, labels, w, h))

 plt.figure(3)
 plt.clf()
-plt.axis('off')
-plt.title('Quantized image (64 colors, Random)')
+plt.axis("off")
+plt.title(f"Quantized image ({n_colors} colors, Random)")
 plt.imshow(recreate_image(codebook_random, labels_random, w, h))
 plt.show()
('examples/cluster', 'plot_cluster_comparison.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -21,8 +21,8 @@
 While these examples give some intuition about the
 algorithms, this intuition might not apply to very high
 dimensional data.
+
 """
-print(__doc__)

 import time
 import warnings
@@ -41,10 +41,9 @@
 # Generate datasets. We choose the size big enough to see the scalability
 # of the algorithms, but not too big to avoid too long running times
 # ============
-n_samples = 1500
-noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,
-                                      noise=.05)
-noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)
+n_samples = 500
+noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)
+noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)
 blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)
 no_structure = np.random.rand(n_samples, 2), None

@@ -56,40 +55,77 @@
 aniso = (X_aniso, y)

 # blobs with varied variances
-varied = datasets.make_blobs(n_samples=n_samples,
-                             cluster_std=[1.0, 2.5, 0.5],
-                             random_state=random_state)
+varied = datasets.make_blobs(
+    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state
+)

 # ============
 # Set up cluster parameters
 # ============
-plt.figure(figsize=(9 * 2 + 3, 12.5))
-plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,
-                    hspace=.01)
+plt.figure(figsize=(9 * 2 + 3, 13))
+plt.subplots_adjust(
+    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01
+)

 plot_num = 1

-default_base = {'quantile': .3,
-                'eps': .3,
-                'damping': .9,
-                'preference': -200,
-                'n_neighbors': 10,
-                'n_clusters': 3,
-                'min_samples': 20,
-                'xi': 0.05,
-                'min_cluster_size': 0.1}
+default_base = {
+    "quantile": 0.3,
+    "eps": 0.3,
+    "damping": 0.9,
+    "preference": -200,
+    "n_neighbors": 3,
+    "n_clusters": 3,
+    "min_samples": 7,
+    "xi": 0.05,
+    "min_cluster_size": 0.1,
+}

 datasets = [
-    (noisy_circles, {'damping': .77, 'preference': -240,
-                     'quantile': .2, 'n_clusters': 2,
-                     'min_samples': 20, 'xi': 0.25}),
-    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),
-    (varied, {'eps': .18, 'n_neighbors': 2,
-              'min_samples': 5, 'xi': 0.035, 'min_cluster_size': .2}),
-    (aniso, {'eps': .15, 'n_neighbors': 2,
-             'min_samples': 20, 'xi': 0.1, 'min_cluster_size': .2}),
-    (blobs, {}),
-    (no_structure, {})]
+    (
+        noisy_circles,
+        {
+            "damping": 0.77,
+            "preference": -240,
+            "quantile": 0.2,
+            "n_clusters": 2,
+            "min_samples": 7,
+            "xi": 0.08,
+        },
+    ),
+    (
+        noisy_moons,
+        {
+            "damping": 0.75,
+            "preference": -220,
+            "n_clusters": 2,
+            "min_samples": 7,
+            "xi": 0.1,
+        },
+    ),
+    (
+        varied,
+        {
+            "eps": 0.18,
+            "n_neighbors": 2,
+            "min_samples": 7,
+            "xi": 0.01,
+            "min_cluster_size": 0.2,
+        },
+    ),
+    (
+        aniso,
+        {
+            "eps": 0.15,
+            "n_neighbors": 2,
+            "min_samples": 7,
+            "xi": 0.1,
+            "min_cluster_size": 0.2,
+        },
+    ),
+    (blobs, {"min_samples": 7, "xi": 0.1, "min_cluster_size": 0.2}),
+    (no_structure, {}),
+]

 for i_dataset, (dataset, algo_params) in enumerate(datasets):
     # update parameters with dataset-specific values
@@ -102,11 +138,12 @@
     X = StandardScaler().fit_transform(X)

     # estimate bandwidth for mean shift
-    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])
+    bandwidth = cluster.estimate_bandwidth(X, quantile=params["quantile"])

     # connectivity matrix for structured Ward
     connectivity = kneighbors_graph(
-        X, n_neighbors=params['n_neighbors'], include_self=False)
+        X, n_neighbors=params["n_neighbors"], include_self=False
+    )
     # make connectivity symmetric
     connectivity = 0.5 * (connectivity + connectivity.T)

@@ -114,37 +151,46 @@
     # Create cluster objects
     # ============
     ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)
-    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])
+    two_means = cluster.MiniBatchKMeans(n_clusters=params["n_clusters"])
     ward = cluster.AgglomerativeClustering(
-        n_clusters=params['n_clusters'], linkage='ward',
-        connectivity=connectivity)
+        n_clusters=params["n_clusters"], linkage="ward", connectivity=connectivity
+    )
     spectral = cluster.SpectralClustering(
-        n_clusters=params['n_clusters'], eigen_solver='arpack',
-        affinity="nearest_neighbors")
-    dbscan = cluster.DBSCAN(eps=params['eps'])
-    optics = cluster.OPTICS(min_samples=params['min_samples'],
-                            xi=params['xi'],
-                            min_cluster_size=params['min_cluster_size'])
+        n_clusters=params["n_clusters"],
+        eigen_solver="arpack",
+        affinity="nearest_neighbors",
+    )
+    dbscan = cluster.DBSCAN(eps=params["eps"])
+    optics = cluster.OPTICS(
+        min_samples=params["min_samples"],
+        xi=params["xi"],
+        min_cluster_size=params["min_cluster_size"],
+    )
     affinity_propagation = cluster.AffinityPropagation(
-        damping=params['damping'], preference=params['preference'])
+        damping=params["damping"], preference=params["preference"], random_state=0
+    )
     average_linkage = cluster.AgglomerativeClustering(
-        linkage="average", affinity="cityblock",
-        n_clusters=params['n_clusters'], connectivity=connectivity)
-    birch = cluster.Birch(n_clusters=params['n_clusters'])
+        linkage="average",
+        affinity="cityblock",
+        n_clusters=params["n_clusters"],
+        connectivity=connectivity,
+    )
+    birch = cluster.Birch(n_clusters=params["n_clusters"])
     gmm = mixture.GaussianMixture(
-        n_components=params['n_clusters'], covariance_type='full')
+        n_components=params["n_clusters"], covariance_type="full"
+    )

     clustering_algorithms = (
-        ('MiniBatchKMeans', two_means),
-        ('AffinityPropagation', affinity_propagation),
-        ('MeanShift', ms),
-        ('SpectralClustering', spectral),
-        ('Ward', ward),
-        ('AgglomerativeClustering', average_linkage),
-        ('DBSCAN', dbscan),
-        ('OPTICS', optics),
-        ('Birch', birch),
-        ('GaussianMixture', gmm)
+        ("MiniBatch\nKMeans", two_means),
+        ("Affinity\nPropagation", affinity_propagation),
+        ("MeanShift", ms),
+        ("Spectral\nClustering", spectral),
+        ("Ward", ward),
+        ("Agglomerative\nClustering", average_linkage),
+        ("DBSCAN", dbscan),
+        ("OPTICS", optics),
+        ("BIRCH", birch),
+        ("Gaussian\nMixture", gmm),
     )

     for name, algorithm in clustering_algorithms:
@@ -154,20 +200,22 @@
         with warnings.catch_warnings():
             warnings.filterwarnings(
                 "ignore",
-                message="the number of connected components of the " +
-                "connectivity matrix is [0-9]{1,2}" +
-                " > 1. Completing it to avoid stopping the tree early.",
-                category=UserWarning)
+                message="the number of connected components of the "
+                + "connectivity matrix is [0-9]{1,2}"
+                + " > 1. Completing it to avoid stopping the tree early.",
+                category=UserWarning,
+            )
             warnings.filterwarnings(
                 "ignore",
-                message="Graph is not fully connected, spectral embedding" +
-                " may not work as expected.",
-                category=UserWarning)
+                message="Graph is not fully connected, spectral embedding"
+                + " may not work as expected.",
+                category=UserWarning,
+            )
             algorithm.fit(X)

         t1 = time.time()
-        if hasattr(algorithm, 'labels_'):
-            y_pred = algorithm.labels_.astype(np.int)
+        if hasattr(algorithm, "labels_"):
+            y_pred = algorithm.labels_.astype(int)
         else:
             y_pred = algorithm.predict(X)

@@ -175,10 +223,26 @@
         if i_dataset == 0:
             plt.title(name, size=18)

-        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',
-                                             '#f781bf', '#a65628', '#984ea3',
-                                             '#999999', '#e41a1c', '#dede00']),
-                                      int(max(y_pred) + 1))))
+        colors = np.array(
+            list(
+                islice(
+                    cycle(
+                        [
+                            "#377eb8",
+                            "#ff7f00",
+                            "#4daf4a",
+                            "#f781bf",
+                            "#a65628",
+                            "#984ea3",
+                            "#999999",
+                            "#e41a1c",
+                            "#dede00",
+                        ]
+                    ),
+                    int(max(y_pred) + 1),
+                )
+            )
+        )
         # add black color for outliers (if any)
         colors = np.append(colors, ["#000000"])
         plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])
@@ -187,9 +251,14 @@
         plt.ylim(-2.5, 2.5)
         plt.xticks(())
         plt.yticks(())
-        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),
-                 transform=plt.gca().transAxes, size=15,
-                 horizontalalignment='right')
+        plt.text(
+            0.99,
+            0.01,
+            ("%.2fs" % (t1 - t0)).lstrip("0"),
+            transform=plt.gca().transAxes,
+            size=15,
+            horizontalalignment="right",
+        )
         plot_num += 1

 plt.show()
('examples/cluster', 'plot_digits_agglomeration.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =========================================================
 Feature agglomeration
@@ -8,8 +6,8 @@

 These images how similar features are merged together using
 feature agglomeration.
+
 """
-print(__doc__)

 # Code source: Gaël Varoquaux
 # Modified for documentation by Jaques Grobler
@@ -26,8 +24,7 @@
 X = np.reshape(images, (len(images), -1))
 connectivity = grid_to_graph(*images[0].shape)

-agglo = cluster.FeatureAgglomeration(connectivity=connectivity,
-                                     n_clusters=32)
+agglo = cluster.FeatureAgglomeration(connectivity=connectivity, n_clusters=32)

 agglo.fit(X)
 X_reduced = agglo.transform(X)
@@ -36,26 +33,28 @@
 images_restored = np.reshape(X_restored, images.shape)
 plt.figure(1, figsize=(4, 3.5))
 plt.clf()
-plt.subplots_adjust(left=.01, right=.99, bottom=.01, top=.91)
+plt.subplots_adjust(left=0.01, right=0.99, bottom=0.01, top=0.91)
 for i in range(4):
     plt.subplot(3, 4, i + 1)
-    plt.imshow(images[i], cmap=plt.cm.gray, vmax=16, interpolation='nearest')
+    plt.imshow(images[i], cmap=plt.cm.gray, vmax=16, interpolation="nearest")
     plt.xticks(())
     plt.yticks(())
     if i == 1:
-        plt.title('Original data')
+        plt.title("Original data")
     plt.subplot(3, 4, 4 + i + 1)
-    plt.imshow(images_restored[i], cmap=plt.cm.gray, vmax=16,
-               interpolation='nearest')
+    plt.imshow(images_restored[i], cmap=plt.cm.gray, vmax=16, interpolation="nearest")
     if i == 1:
-        plt.title('Agglomerated data')
+        plt.title("Agglomerated data")
     plt.xticks(())
     plt.yticks(())

 plt.subplot(3, 4, 10)
-plt.imshow(np.reshape(agglo.labels_, images[0].shape),
-           interpolation='nearest', cmap=plt.cm.nipy_spectral)
+plt.imshow(
+    np.reshape(agglo.labels_, images[0].shape),
+    interpolation="nearest",
+    cmap=plt.cm.nipy_spectral,
+)
 plt.xticks(())
 plt.yticks(())
-plt.title('Labels')
+plt.title("Labels")
 plt.show()
('examples/cluster', 'plot_birch_vs_minibatchkmeans.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,22 +3,29 @@
 Compare BIRCH and MiniBatchKMeans
 =================================

-This example compares the timing of Birch (with and without the global
+This example compares the timing of BIRCH (with and without the global
 clustering step) and MiniBatchKMeans on a synthetic dataset having
-100,000 samples and 2 features generated using make_blobs.
+25,000 samples and 2 features generated using make_blobs.

-If ``n_clusters`` is set to None, the data is reduced from 100,000
+Both ``MiniBatchKMeans`` and ``BIRCH`` are very scalable algorithms and could
+run efficiently on hundreds of thousands or even millions of datapoints. We
+chose to limit the dataset size of this example in the interest of keeping
+our Continuous Integration resource usage reasonable but the interested
+reader might enjoy editing this script to rerun it with a larger value for
+`n_samples`.
+
+If ``n_clusters`` is set to None, the data is reduced from 25,000
 samples to a set of 158 clusters. This can be viewed as a preprocessing
 step before the final (global) clustering step that further reduces these
 158 clusters to 100 clusters.
+
 """

 # Authors: Manoj Kumar <manojkumarsivaraj334@gmail.com
 #          Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>
 # License: BSD 3 clause

-print(__doc__)
-
+from joblib import cpu_count
 from itertools import cycle
 from time import time
 import numpy as np
@@ -26,18 +33,17 @@
 import matplotlib.colors as colors

 from sklearn.cluster import Birch, MiniBatchKMeans
-from sklearn.datasets.samples_generator import make_blobs
+from sklearn.datasets import make_blobs


 # Generate centers for the blobs so that it forms a 10 X 10 grid.
 xx = np.linspace(-22, 22, 10)
 yy = np.linspace(-22, 22, 10)
 xx, yy = np.meshgrid(xx, yy)
-n_centres = np.hstack((np.ravel(xx)[:, np.newaxis],
-                       np.ravel(yy)[:, np.newaxis]))
+n_centers = np.hstack((np.ravel(xx)[:, np.newaxis], np.ravel(yy)[:, np.newaxis]))

-# Generate blobs to do a comparison between MiniBatchKMeans and Birch.
-X, y = make_blobs(n_samples=100000, centers=n_centres, random_state=0)
+# Generate blobs to do a comparison between MiniBatchKMeans and BIRCH.
+X, y = make_blobs(n_samples=25000, centers=n_centers, random_state=0)

 # Use all colors that matplotlib provides by default.
 colors_ = cycle(colors.cnames.keys())
@@ -45,18 +51,19 @@
 fig = plt.figure(figsize=(12, 4))
 fig.subplots_adjust(left=0.04, right=0.98, bottom=0.1, top=0.9)

-# Compute clustering with Birch with and without the final clustering step
+# Compute clustering with BIRCH with and without the final clustering step
 # and plot.
-birch_models = [Birch(threshold=1.7, n_clusters=None),
-                Birch(threshold=1.7, n_clusters=100)]
-final_step = ['without global clustering', 'with global clustering']
+birch_models = [
+    Birch(threshold=1.7, n_clusters=None),
+    Birch(threshold=1.7, n_clusters=100),
+]
+final_step = ["without global clustering", "with global clustering"]

 for ind, (birch_model, info) in enumerate(zip(birch_models, final_step)):
     t = time()
     birch_model.fit(X)
     time_ = time() - t
-    print("Birch %s as the final step took %0.2f seconds" % (
-          info, (time() - t)))
+    print("BIRCH %s as the final step took %0.2f seconds" % (info, (time() - t)))

     # Plot result
     labels = birch_model.labels_
@@ -67,20 +74,24 @@
     ax = fig.add_subplot(1, 3, ind + 1)
     for this_centroid, k, col in zip(centroids, range(n_clusters), colors_):
         mask = labels == k
-        ax.scatter(X[mask, 0], X[mask, 1],
-                   c='w', edgecolor=col, marker='.', alpha=0.5)
+        ax.scatter(X[mask, 0], X[mask, 1], c="w", edgecolor=col, marker=".", alpha=0.5)
         if birch_model.n_clusters is None:
-            ax.scatter(this_centroid[0], this_centroid[1], marker='+',
-                       c='k', s=25)
+            ax.scatter(this_centroid[0], this_centroid[1], marker="+", c="k", s=25)
     ax.set_ylim([-25, 25])
     ax.set_xlim([-25, 25])
     ax.set_autoscaley_on(False)
-    ax.set_title('Birch %s' % info)
+    ax.set_title("BIRCH %s" % info)

 # Compute clustering with MiniBatchKMeans.
-mbk = MiniBatchKMeans(init='k-means++', n_clusters=100, batch_size=100,
-                      n_init=10, max_no_improvement=10, verbose=0,
-                      random_state=0)
+mbk = MiniBatchKMeans(
+    init="k-means++",
+    n_clusters=100,
+    batch_size=256 * cpu_count(),
+    n_init=10,
+    max_no_improvement=10,
+    verbose=0,
+    random_state=0,
+)
 t0 = time()
 mbk.fit(X)
 t_mini_batch = time() - t0
@@ -88,13 +99,10 @@
 mbk_means_labels_unique = np.unique(mbk.labels_)

 ax = fig.add_subplot(1, 3, 3)
-for this_centroid, k, col in zip(mbk.cluster_centers_,
-                                 range(n_clusters), colors_):
+for this_centroid, k, col in zip(mbk.cluster_centers_, range(n_clusters), colors_):
     mask = mbk.labels_ == k
-    ax.scatter(X[mask, 0], X[mask, 1], marker='.',
-               c='w', edgecolor=col, alpha=0.5)
-    ax.scatter(this_centroid[0], this_centroid[1], marker='+',
-               c='k', s=25)
+    ax.scatter(X[mask, 0], X[mask, 1], marker=".", c="w", edgecolor=col, alpha=0.5)
+    ax.scatter(this_centroid[0], this_centroid[1], marker="+", c="k", s=25)
 ax.set_xlim([-25, 25])
 ax.set_ylim([-25, 25])
 ax.set_title("MiniBatchKMeans")
('examples/cluster', 'plot_agglomerative_clustering_metrics.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -31,7 +31,9 @@
 distance, the separation is good and the waveform classes are recovered.
 Finally, the cosine distance does not separate at all waveform 1 and 2,
 thus the clustering puts them in the same cluster.
+
 """
+
 # Author: Gael Varoquaux
 # License: BSD 3-Clause or CC-0

@@ -51,19 +53,24 @@
 def sqr(x):
     return np.sign(np.cos(x))

+
 X = list()
 y = list()
-for i, (phi, a) in enumerate([(.5, .15), (.5, .6), (.3, .2)]):
+for i, (phi, a) in enumerate([(0.5, 0.15), (0.5, 0.6), (0.3, 0.2)]):
     for _ in range(30):
-        phase_noise = .01 * np.random.normal()
-        amplitude_noise = .04 * np.random.normal()
+        phase_noise = 0.01 * np.random.normal()
+        amplitude_noise = 0.04 * np.random.normal()
         additional_noise = 1 - 2 * np.random.rand(n_features)
         # Make the noise sparse
-        additional_noise[np.abs(additional_noise) < .997] = 0
+        additional_noise[np.abs(additional_noise) < 0.997] = 0

-        X.append(12 * ((a + amplitude_noise)
-                 * (sqr(6 * (t + phi + phase_noise)))
-                 + additional_noise))
+        X.append(
+            12
+            * (
+                (a + amplitude_noise) * (sqr(6 * (t + phi + phase_noise)))
+                + additional_noise
+            )
+        )
         y.append(i)

 X = np.array(X)
@@ -71,20 +78,19 @@

 n_clusters = 3

-labels = ('Waveform 1', 'Waveform 2', 'Waveform 3')
+labels = ("Waveform 1", "Waveform 2", "Waveform 3")

 # Plot the ground-truth labelling
 plt.figure()
 plt.axes([0, 0, 1, 1])
-for l, c, n in zip(range(n_clusters), 'rgb',
-                   labels):
-    lines = plt.plot(X[y == l].T, c=c, alpha=.5)
+for l, c, n in zip(range(n_clusters), "rgb", labels):
+    lines = plt.plot(X[y == l].T, c=c, alpha=0.5)
     lines[0].set_label(n)

-plt.legend(loc='best')
+plt.legend(loc="best")

-plt.axis('tight')
-plt.axis('off')
+plt.axis("tight")
+plt.axis("off")
 plt.suptitle("Ground truth", size=20)


@@ -94,17 +100,21 @@
     plt.figure(figsize=(5, 4.5))
     for i in range(n_clusters):
         for j in range(n_clusters):
-            avg_dist[i, j] = pairwise_distances(X[y == i], X[y == j],
-                                                metric=metric).mean()
+            avg_dist[i, j] = pairwise_distances(
+                X[y == i], X[y == j], metric=metric
+            ).mean()
     avg_dist /= avg_dist.max()
     for i in range(n_clusters):
         for j in range(n_clusters):
-            plt.text(i, j, '%5.3f' % avg_dist[i, j],
-                     verticalalignment='center',
-                     horizontalalignment='center')
+            plt.text(
+                i,
+                j,
+                "%5.3f" % avg_dist[i, j],
+                verticalalignment="center",
+                horizontalalignment="center",
+            )

-    plt.imshow(avg_dist, interpolation='nearest', cmap=plt.cm.gnuplot2,
-               vmin=0)
+    plt.imshow(avg_dist, interpolation="nearest", cmap=plt.cm.gnuplot2, vmin=0)
     plt.xticks(range(n_clusters), labels, rotation=45)
     plt.yticks(range(n_clusters), labels)
     plt.colorbar()
@@ -114,15 +124,16 @@

 # Plot clustering results
 for index, metric in enumerate(["cosine", "euclidean", "cityblock"]):
-    model = AgglomerativeClustering(n_clusters=n_clusters,
-                                    linkage="average", affinity=metric)
+    model = AgglomerativeClustering(
+        n_clusters=n_clusters, linkage="average", affinity=metric
+    )
     model.fit(X)
     plt.figure()
     plt.axes([0, 0, 1, 1])
-    for l, c in zip(np.arange(model.n_clusters), 'rgbk'):
-        plt.plot(X[model.labels_ == l].T, c=c, alpha=.5)
-    plt.axis('tight')
-    plt.axis('off')
+    for l, c in zip(np.arange(model.n_clusters), "rgbk"):
+        plt.plot(X[model.labels_ == l].T, c=c, alpha=0.5)
+    plt.axis("tight")
+    plt.axis("off")
     plt.suptitle("AgglomerativeClustering(affinity=%s)" % metric, size=20)


('examples/cluster', 'plot_adjusted_for_chance_measures.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -20,7 +20,6 @@
 value of k on various overlapping sub-samples of the dataset.

 """
-print(__doc__)

 # Author: Olivier Grisel <olivier.grisel@ensta.org>
 # License: BSD 3 clause
@@ -30,8 +29,10 @@
 from time import time
 from sklearn import metrics

-def uniform_labelings_scores(score_func, n_samples, n_clusters_range,
-                             fixed_n_classes=None, n_runs=5, seed=42):
+
+def uniform_labelings_scores(
+    score_func, n_samples, n_clusters_range, fixed_n_classes=None, n_runs=5, seed=42
+):
     """Compute score for 2 random uniform cluster labelings.

     Both random labelings have the same number of clusters for each value
@@ -56,8 +57,8 @@


 def ami_score(U, V):
-    return metrics.adjusted_mutual_info_score(U, V,
-                                              average_method='arithmetic')
+    return metrics.adjusted_mutual_info_score(U, V)
+

 score_funcs = [
     metrics.adjusted_rand_score,
@@ -69,27 +70,31 @@
 # 2 independent random clusterings with equal cluster number

 n_samples = 100
-n_clusters_range = np.linspace(2, n_samples, 10).astype(np.int)
+n_clusters_range = np.linspace(2, n_samples, 10).astype(int)

 plt.figure(1)

 plots = []
 names = []
 for score_func in score_funcs:
-    print("Computing %s for %d values of n_clusters and n_samples=%d"
-          % (score_func.__name__, len(n_clusters_range), n_samples))
+    print(
+        "Computing %s for %d values of n_clusters and n_samples=%d"
+        % (score_func.__name__, len(n_clusters_range), n_samples)
+    )

     t0 = time()
     scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range)
     print("done in %0.3fs" % (time() - t0))
-    plots.append(plt.errorbar(
-        n_clusters_range, np.median(scores, axis=1), scores.std(axis=1))[0])
+    plots.append(
+        plt.errorbar(n_clusters_range, np.median(scores, axis=1), scores.std(axis=1))[0]
+    )
     names.append(score_func.__name__)

-plt.title("Clustering measures for 2 random uniform labelings\n"
-          "with equal number of clusters")
-plt.xlabel('Number of clusters (Number of samples is fixed to %d)' % n_samples)
-plt.ylabel('Score value')
+plt.title(
+    "Clustering measures for 2 random uniform labelings\nwith equal number of clusters"
+)
+plt.xlabel("Number of clusters (Number of samples is fixed to %d)" % n_samples)
+plt.ylabel("Score value")
 plt.legend(plots, names)
 plt.ylim(bottom=-0.05, top=1.05)

@@ -98,7 +103,7 @@
 # with fixed number of clusters

 n_samples = 1000
-n_clusters_range = np.linspace(2, 100, 10).astype(np.int)
+n_clusters_range = np.linspace(2, 100, 10).astype(int)
 n_classes = 10

 plt.figure(2)
@@ -106,21 +111,27 @@
 plots = []
 names = []
 for score_func in score_funcs:
-    print("Computing %s for %d values of n_clusters and n_samples=%d"
-          % (score_func.__name__, len(n_clusters_range), n_samples))
+    print(
+        "Computing %s for %d values of n_clusters and n_samples=%d"
+        % (score_func.__name__, len(n_clusters_range), n_samples)
+    )

     t0 = time()
-    scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range,
-                                      fixed_n_classes=n_classes)
+    scores = uniform_labelings_scores(
+        score_func, n_samples, n_clusters_range, fixed_n_classes=n_classes
+    )
     print("done in %0.3fs" % (time() - t0))
-    plots.append(plt.errorbar(
-        n_clusters_range, scores.mean(axis=1), scores.std(axis=1))[0])
+    plots.append(
+        plt.errorbar(n_clusters_range, scores.mean(axis=1), scores.std(axis=1))[0]
+    )
     names.append(score_func.__name__)

-plt.title("Clustering measures for random uniform labeling\n"
-          "against reference assignment with %d classes" % n_classes)
-plt.xlabel('Number of clusters (Number of samples is fixed to %d)' % n_samples)
-plt.ylabel('Score value')
+plt.title(
+    "Clustering measures for random uniform labeling\n"
+    "against reference assignment with %d classes" % n_classes
+)
+plt.xlabel("Number of clusters (Number of samples is fixed to %d)" % n_samples)
+plt.ylabel("Score value")
 plt.ylim(bottom=-0.05, top=1.05)
 plt.legend(plots, names)
 plt.show()
('examples/cluster', 'plot_segmentation_toy.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -23,8 +23,8 @@
 In addition, we use the mask of the objects to restrict the graph to the
 outline of the objects. In this example, we are interested in
 separating the objects one from the other, and not from the background.
+
 """
-print(__doc__)

 # Authors:  Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>
 #           Gael Varoquaux <gael.varoquaux@normalesup.org>
@@ -46,10 +46,10 @@

 radius1, radius2, radius3, radius4 = 16, 14, 15, 14

-circle1 = (x - center1[0]) ** 2 + (y - center1[1]) ** 2 < radius1 ** 2
-circle2 = (x - center2[0]) ** 2 + (y - center2[1]) ** 2 < radius2 ** 2
-circle3 = (x - center3[0]) ** 2 + (y - center3[1]) ** 2 < radius3 ** 2
-circle4 = (x - center4[0]) ** 2 + (y - center4[1]) ** 2 < radius4 ** 2
+circle1 = (x - center1[0]) ** 2 + (y - center1[1]) ** 2 < radius1**2
+circle2 = (x - center2[0]) ** 2 + (y - center2[1]) ** 2 < radius2**2
+circle3 = (x - center3[0]) ** 2 + (y - center3[1]) ** 2 < radius3**2
+circle4 = (x - center4[0]) ** 2 + (y - center4[1]) ** 2 < radius4**2

 # #############################################################################
 # 4 circles
@@ -73,8 +73,8 @@

 # Force the solver to be arpack, since amg is numerically
 # unstable on this example
-labels = spectral_clustering(graph, n_clusters=4, eigen_solver='arpack')
-label_im = np.full(mask.shape, -1.)
+labels = spectral_clustering(graph, n_clusters=4, eigen_solver="arpack")
+label_im = np.full(mask.shape, -1.0)
 label_im[mask] = labels

 plt.matshow(img)
@@ -91,8 +91,8 @@
 graph = image.img_to_graph(img, mask=mask)
 graph.data = np.exp(-graph.data / graph.data.std())

-labels = spectral_clustering(graph, n_clusters=2, eigen_solver='arpack')
-label_im = np.full(mask.shape, -1.)
+labels = spectral_clustering(graph, n_clusters=2, eigen_solver="arpack")
+label_im = np.full(mask.shape, -1.0)
 label_im[mask] = labels

 plt.matshow(img)
('examples/cluster', 'plot_mini_batch_kmeans.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -11,20 +11,18 @@
 MiniBatchKMeans, and plot the results.
 We will also plot the points that are labelled differently between the two
 algorithms.
+
 """
-print(__doc__)

-import time
+# %%
+# Generate the data
+# -----------------
+#
+# We start by generating the blobs of data to be clustered.

 import numpy as np
-import matplotlib.pyplot as plt
+from sklearn.datasets import make_blobs

-from sklearn.cluster import MiniBatchKMeans, KMeans
-from sklearn.metrics.pairwise import pairwise_distances_argmin
-from sklearn.datasets.samples_generator import make_blobs
-
-# #############################################################################
-# Generate sample data
 np.random.seed(0)

 batch_size = 45
@@ -32,83 +30,112 @@
 n_clusters = len(centers)
 X, labels_true = make_blobs(n_samples=3000, centers=centers, cluster_std=0.7)

-# #############################################################################
-# Compute clustering with Means
+# %%
+# Compute clustering with KMeans
+# ------------------------------

-k_means = KMeans(init='k-means++', n_clusters=3, n_init=10)
+import time
+from sklearn.cluster import KMeans
+
+k_means = KMeans(init="k-means++", n_clusters=3, n_init=10)
 t0 = time.time()
 k_means.fit(X)
 t_batch = time.time() - t0

-# #############################################################################
+# %%
 # Compute clustering with MiniBatchKMeans
+# ---------------------------------------

-mbk = MiniBatchKMeans(init='k-means++', n_clusters=3, batch_size=batch_size,
-                      n_init=10, max_no_improvement=10, verbose=0)
+from sklearn.cluster import MiniBatchKMeans
+
+mbk = MiniBatchKMeans(
+    init="k-means++",
+    n_clusters=3,
+    batch_size=batch_size,
+    n_init=10,
+    max_no_improvement=10,
+    verbose=0,
+)
 t0 = time.time()
 mbk.fit(X)
 t_mini_batch = time.time() - t0

-# #############################################################################
-# Plot result
+# %%
+# Establishing parity between clusters
+# ------------------------------------
+#
+# We want to have the same color for the same cluster from both the
+# MiniBatchKMeans and the KMeans algorithm. Let's pair the cluster centers per
+# closest one.
+
+from sklearn.metrics.pairwise import pairwise_distances_argmin
+
+k_means_cluster_centers = k_means.cluster_centers_
+order = pairwise_distances_argmin(k_means.cluster_centers_, mbk.cluster_centers_)
+mbk_means_cluster_centers = mbk.cluster_centers_[order]
+
+k_means_labels = pairwise_distances_argmin(X, k_means_cluster_centers)
+mbk_means_labels = pairwise_distances_argmin(X, mbk_means_cluster_centers)
+
+# %%
+# Plotting the results
+# --------------------
+
+import matplotlib.pyplot as plt

 fig = plt.figure(figsize=(8, 3))
 fig.subplots_adjust(left=0.02, right=0.98, bottom=0.05, top=0.9)
-colors = ['#4EACC5', '#FF9C34', '#4E9A06']
-
-# We want to have the same colors for the same cluster from the
-# MiniBatchKMeans and the KMeans algorithm. Let's pair the cluster centers per
-# closest one.
-k_means_cluster_centers = np.sort(k_means.cluster_centers_, axis=0)
-mbk_means_cluster_centers = np.sort(mbk.cluster_centers_, axis=0)
-k_means_labels = pairwise_distances_argmin(X, k_means_cluster_centers)
-mbk_means_labels = pairwise_distances_argmin(X, mbk_means_cluster_centers)
-order = pairwise_distances_argmin(k_means_cluster_centers,
-                                  mbk_means_cluster_centers)
+colors = ["#4EACC5", "#FF9C34", "#4E9A06"]

 # KMeans
 ax = fig.add_subplot(1, 3, 1)
 for k, col in zip(range(n_clusters), colors):
     my_members = k_means_labels == k
     cluster_center = k_means_cluster_centers[k]
-    ax.plot(X[my_members, 0], X[my_members, 1], 'w',
-            markerfacecolor=col, marker='.')
-    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,
-            markeredgecolor='k', markersize=6)
-ax.set_title('KMeans')
+    ax.plot(X[my_members, 0], X[my_members, 1], "w", markerfacecolor=col, marker=".")
+    ax.plot(
+        cluster_center[0],
+        cluster_center[1],
+        "o",
+        markerfacecolor=col,
+        markeredgecolor="k",
+        markersize=6,
+    )
+ax.set_title("KMeans")
 ax.set_xticks(())
 ax.set_yticks(())
-plt.text(-3.5, 1.8,  'train time: %.2fs\ninertia: %f' % (
-    t_batch, k_means.inertia_))
+plt.text(-3.5, 1.8, "train time: %.2fs\ninertia: %f" % (t_batch, k_means.inertia_))

 # MiniBatchKMeans
 ax = fig.add_subplot(1, 3, 2)
 for k, col in zip(range(n_clusters), colors):
-    my_members = mbk_means_labels == order[k]
-    cluster_center = mbk_means_cluster_centers[order[k]]
-    ax.plot(X[my_members, 0], X[my_members, 1], 'w',
-            markerfacecolor=col, marker='.')
-    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,
-            markeredgecolor='k', markersize=6)
-ax.set_title('MiniBatchKMeans')
+    my_members = mbk_means_labels == k
+    cluster_center = mbk_means_cluster_centers[k]
+    ax.plot(X[my_members, 0], X[my_members, 1], "w", markerfacecolor=col, marker=".")
+    ax.plot(
+        cluster_center[0],
+        cluster_center[1],
+        "o",
+        markerfacecolor=col,
+        markeredgecolor="k",
+        markersize=6,
+    )
+ax.set_title("MiniBatchKMeans")
 ax.set_xticks(())
 ax.set_yticks(())
-plt.text(-3.5, 1.8, 'train time: %.2fs\ninertia: %f' %
-         (t_mini_batch, mbk.inertia_))
+plt.text(-3.5, 1.8, "train time: %.2fs\ninertia: %f" % (t_mini_batch, mbk.inertia_))

-# Initialise the different array to all False
-different = (mbk_means_labels == 4)
+# Initialize the different array to all False
+different = mbk_means_labels == 4
 ax = fig.add_subplot(1, 3, 3)

 for k in range(n_clusters):
-    different += ((k_means_labels == k) != (mbk_means_labels == order[k]))
+    different += (k_means_labels == k) != (mbk_means_labels == k)

 identic = np.logical_not(different)
-ax.plot(X[identic, 0], X[identic, 1], 'w',
-        markerfacecolor='#bbbbbb', marker='.')
-ax.plot(X[different, 0], X[different, 1], 'w',
-        markerfacecolor='m', marker='.')
-ax.set_title('Difference')
+ax.plot(X[identic, 0], X[identic, 1], "w", markerfacecolor="#bbbbbb", marker=".")
+ax.plot(X[different, 0], X[different, 1], "w", markerfacecolor="m", marker=".")
+ax.set_title("Difference")
 ax.set_xticks(())
 ax.set_yticks(())

('examples/cluster', 'plot_mean_shift.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -10,19 +10,20 @@
 Machine Intelligence. 2002. pp. 603-619.

 """
-print(__doc__)

 import numpy as np
 from sklearn.cluster import MeanShift, estimate_bandwidth
-from sklearn.datasets.samples_generator import make_blobs
+from sklearn.datasets import make_blobs

-# #############################################################################
+# %%
 # Generate sample data
+# --------------------
 centers = [[1, 1], [-1, -1], [1, -1]]
 X, _ = make_blobs(n_samples=10000, centers=centers, cluster_std=0.6)

-# #############################################################################
+# %%
 # Compute clustering with MeanShift
+# ---------------------------------

 # The following bandwidth can be automatically detected using
 bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)
@@ -37,20 +38,27 @@

 print("number of estimated clusters : %d" % n_clusters_)

-# #############################################################################
+# %%
 # Plot result
+# -----------
 import matplotlib.pyplot as plt
 from itertools import cycle

 plt.figure(1)
 plt.clf()

-colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')
+colors = cycle("bgrcmykbgrcmykbgrcmykbgrcmyk")
 for k, col in zip(range(n_clusters_), colors):
     my_members = labels == k
     cluster_center = cluster_centers[k]
-    plt.plot(X[my_members, 0], X[my_members, 1], col + '.')
-    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,
-             markeredgecolor='k', markersize=14)
-plt.title('Estimated number of clusters: %d' % n_clusters_)
+    plt.plot(X[my_members, 0], X[my_members, 1], col + ".")
+    plt.plot(
+        cluster_center[0],
+        cluster_center[1],
+        "o",
+        markerfacecolor=col,
+        markeredgecolor="k",
+        markersize=14,
+    )
+plt.title("Estimated number of clusters: %d" % n_clusters_)
 plt.show()
('examples/cluster', 'plot_dict_face_patches.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,6 @@
 """
 Online learning of a dictionary of parts of faces
-==================================================
+=================================================

 This example uses a large dataset of faces to learn a set of 20 x 20
 images patches that constitute faces.
@@ -18,25 +18,29 @@
 partial-fit. This is because the number of patches that they represent
 has become too low, and it is better to choose a random new
 cluster.
+
 """
-print(__doc__)
+
+# %%
+# Load the data
+# -------------
+
+from sklearn import datasets
+
+faces = datasets.fetch_olivetti_faces()
+
+# %%
+# Learn the dictionary of images
+# ------------------------------

 import time

-import matplotlib.pyplot as plt
 import numpy as np

-
-from sklearn import datasets
 from sklearn.cluster import MiniBatchKMeans
 from sklearn.feature_extraction.image import extract_patches_2d

-faces = datasets.fetch_olivetti_faces()
-
-# #############################################################################
-# Learn the dictionary of images
-
-print('Learning the dictionary... ')
+print("Learning the dictionary... ")
 rng = np.random.RandomState(0)
 kmeans = MiniBatchKMeans(n_clusters=81, random_state=rng, verbose=True)
 patch_size = (20, 20)
@@ -48,8 +52,7 @@
 index = 0
 for _ in range(6):
     for img in faces.images:
-        data = extract_patches_2d(img, patch_size, max_patches=50,
-                                  random_state=rng)
+        data = extract_patches_2d(img, patch_size, max_patches=50, random_state=rng)
         data = np.reshape(data, (len(data), -1))
         buffer.append(data)
         index += 1
@@ -60,25 +63,29 @@
             kmeans.partial_fit(data)
             buffer = []
         if index % 100 == 0:
-            print('Partial fit of %4i out of %i'
-                  % (index, 6 * len(faces.images)))
+            print("Partial fit of %4i out of %i" % (index, 6 * len(faces.images)))

 dt = time.time() - t0
-print('done in %.2fs.' % dt)
+print("done in %.2fs." % dt)

-# #############################################################################
+# %%
 # Plot the results
+# ----------------
+
+import matplotlib.pyplot as plt
+
 plt.figure(figsize=(4.2, 4))
 for i, patch in enumerate(kmeans.cluster_centers_):
     plt.subplot(9, 9, i + 1)
-    plt.imshow(patch.reshape(patch_size), cmap=plt.cm.gray,
-               interpolation='nearest')
+    plt.imshow(patch.reshape(patch_size), cmap=plt.cm.gray, interpolation="nearest")
     plt.xticks(())
     plt.yticks(())


-plt.suptitle('Patches of faces\nTrain time %.1fs on %d patches' %
-             (dt, 8 * len(faces.images)), fontsize=16)
+plt.suptitle(
+    "Patches of faces\nTrain time %.1fs on %d patches" % (dt, 8 * len(faces.images)),
+    fontsize=16,
+)
 plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)

 plt.show()
('examples/cluster', 'plot_coin_segmentation.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -10,36 +10,32 @@
 This procedure (spectral clustering on an image) is an efficient
 approximate solution for finding normalized graph cuts.

-There are two options to assign labels:
+There are three options to assign labels:

-* with 'kmeans' spectral clustering will cluster samples in the embedding space
+* 'kmeans' spectral clustering clusters samples in the embedding space
   using a kmeans algorithm
-* whereas 'discrete' will iteratively search for the closest partition
-  space to the embedding space.
+* 'discrete' iteratively searches for the closest partition
+  space to the embedding space of spectral clustering.
+* 'cluster_qr' assigns labels using the QR factorization with pivoting
+  that directly determines the partition in the embedding space.
 """
-print(__doc__)

-# Author: Gael Varoquaux <gael.varoquaux@normalesup.org>, Brian Cheung
+# Author: Gael Varoquaux <gael.varoquaux@normalesup.org>
+#         Brian Cheung
+#         Andrew Knyazev <Andrew.Knyazev@ucdenver.edu>
 # License: BSD 3 clause

 import time

 import numpy as np
-from distutils.version import LooseVersion
-from scipy.ndimage.filters import gaussian_filter
+from scipy.ndimage import gaussian_filter
 import matplotlib.pyplot as plt
-import skimage
 from skimage.data import coins
 from skimage.transform import rescale

 from sklearn.feature_extraction import image
 from sklearn.cluster import spectral_clustering

-# these were introduced in skimage-0.14
-if LooseVersion(skimage.__version__) >= '0.14':
-    rescale_params = {'anti_aliasing': False, 'multichannel': False}
-else:
-    rescale_params = {}

 # load the coins as a numpy array
 orig_coins = coins()
@@ -48,8 +44,9 @@
 # Applying a Gaussian filter for smoothing prior to down-scaling
 # reduces aliasing artifacts.
 smoothened_coins = gaussian_filter(orig_coins, sigma=2)
-rescaled_coins = rescale(smoothened_coins, 0.2, mode="reflect",
-                         **rescale_params)
+rescaled_coins = rescale(
+    smoothened_coins, 0.2, mode="reflect", anti_aliasing=False, multichannel=False
+)

 # Convert the image into a graph with the value of the gradient on the
 # edges.
@@ -62,28 +59,51 @@
 eps = 1e-6
 graph.data = np.exp(-beta * graph.data / graph.data.std()) + eps

-# Apply spectral clustering (this step goes much faster if you have pyamg
-# installed)
-N_REGIONS = 25
+# The number of segmented regions to display needs to be chosen manually.
+# The current version of 'spectral_clustering' does not support determining
+# the number of good quality clusters automatically.
+n_regions = 26

-#############################################################################
-# Visualize the resulting regions
+# %%
+# Compute and visualize the resulting regions

-for assign_labels in ('kmeans', 'discretize'):
+# Computing a few extra eigenvectors may speed up the eigen_solver.
+# The spectral clustering quality may also benetif from requesting
+# extra regions for segmentation.
+n_regions_plus = 3
+
+# Apply spectral clustering using the default eigen_solver='arpack'.
+# Any implemented solver can be used: eigen_solver='arpack', 'lobpcg', or 'amg'.
+# Choosing eigen_solver='amg' requires an extra package called 'pyamg'.
+# The quality of segmentation and the speed of calculations is mostly determined
+# by the choice of the solver and the value of the tolerance 'eigen_tol'.
+# TODO: varying eigen_tol seems to have no effect for 'lobpcg' and 'amg' #21243.
+for assign_labels in ("kmeans", "discretize", "cluster_qr"):
     t0 = time.time()
-    labels = spectral_clustering(graph, n_clusters=N_REGIONS,
-                                 assign_labels=assign_labels, random_state=42)
+    labels = spectral_clustering(
+        graph,
+        n_clusters=(n_regions + n_regions_plus),
+        eigen_tol=1e-7,
+        assign_labels=assign_labels,
+        random_state=42,
+    )
+
     t1 = time.time()
     labels = labels.reshape(rescaled_coins.shape)
-
     plt.figure(figsize=(5, 5))
     plt.imshow(rescaled_coins, cmap=plt.cm.gray)
-    for l in range(N_REGIONS):
-        plt.contour(labels == l,
-                    colors=[plt.cm.nipy_spectral(l / float(N_REGIONS))])
+
     plt.xticks(())
     plt.yticks(())
-    title = 'Spectral clustering: %s, %.2fs' % (assign_labels, (t1 - t0))
+    title = "Spectral clustering: %s, %.2fs" % (assign_labels, (t1 - t0))
     print(title)
     plt.title(title)
+    for l in range(n_regions):
+        colors = [plt.cm.nipy_spectral((l + 4) / float(n_regions + 4))]
+        plt.contour(labels == l, colors=colors)
+        # To view individual segments as appear comment in plt.pause(0.5)
 plt.show()
+
+# TODO: After #21194 is merged and #21243 is fixed, check which eigen_solver
+# is the best and set eigen_solver='arpack', 'lobpcg', or 'amg' and eigen_tol
+# explicitly in this example.
('examples/cluster', 'plot_ward_structured_vs_unstructured.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -17,6 +17,7 @@
 respect the structure of the swiss roll and extend across different folds of
 the manifolds. On the opposite, when opposing connectivity constraints,
 the clusters form a nice parcellation of the swiss roll.
+
 """

 # Authors : Vincent Michel, 2010
@@ -24,28 +25,32 @@
 #           Gael Varoquaux, 2010
 # License: BSD 3 clause

-print(__doc__)
+import time as time

-import time as time
+import matplotlib.pyplot as plt
+
+# The following import is required
+# for 3D projection to work with matplotlib < 3.2
+import mpl_toolkits.mplot3d  # noqa: F401
+
 import numpy as np
-import matplotlib.pyplot as plt
-import mpl_toolkits.mplot3d.axes3d as p3
+
 from sklearn.cluster import AgglomerativeClustering
-from sklearn.datasets.samples_generator import make_swiss_roll
+from sklearn.datasets import make_swiss_roll

 # #############################################################################
 # Generate data (swiss roll dataset)
 n_samples = 1500
 noise = 0.05
-X, _ = make_swiss_roll(n_samples, noise)
+X, _ = make_swiss_roll(n_samples, noise=noise)
 # Make it thinner
-X[:, 1] *= .5
+X[:, 1] *= 0.5

 # #############################################################################
 # Compute clustering
 print("Compute unstructured hierarchical clustering...")
 st = time.time()
-ward = AgglomerativeClustering(n_clusters=6, linkage='ward').fit(X)
+ward = AgglomerativeClustering(n_clusters=6, linkage="ward").fit(X)
 elapsed_time = time.time() - st
 label = ward.labels_
 print("Elapsed time: %.2fs" % elapsed_time)
@@ -54,26 +59,32 @@
 # #############################################################################
 # Plot result
 fig = plt.figure()
-ax = p3.Axes3D(fig)
-ax.view_init(7, -80)
+ax = fig.add_subplot(111, projection="3d", elev=7, azim=-80)
+ax.set_position([0, 0, 0.95, 1])
 for l in np.unique(label):
-    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],
-               color=plt.cm.jet(np.float(l) / np.max(label + 1)),
-               s=20, edgecolor='k')
-plt.title('Without connectivity constraints (time %.2fs)' % elapsed_time)
-
+    ax.scatter(
+        X[label == l, 0],
+        X[label == l, 1],
+        X[label == l, 2],
+        color=plt.cm.jet(float(l) / np.max(label + 1)),
+        s=20,
+        edgecolor="k",
+    )
+plt.title("Without connectivity constraints (time %.2fs)" % elapsed_time)

 # #############################################################################
 # Define the structure A of the data. Here a 10 nearest neighbors
 from sklearn.neighbors import kneighbors_graph
+
 connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)

 # #############################################################################
 # Compute clustering
 print("Compute structured hierarchical clustering...")
 st = time.time()
-ward = AgglomerativeClustering(n_clusters=6, connectivity=connectivity,
-                               linkage='ward').fit(X)
+ward = AgglomerativeClustering(
+    n_clusters=6, connectivity=connectivity, linkage="ward"
+).fit(X)
 elapsed_time = time.time() - st
 label = ward.labels_
 print("Elapsed time: %.2fs" % elapsed_time)
@@ -82,12 +93,17 @@
 # #############################################################################
 # Plot result
 fig = plt.figure()
-ax = p3.Axes3D(fig)
-ax.view_init(7, -80)
+ax = fig.add_subplot(111, projection="3d", elev=7, azim=-80)
+ax.set_position([0, 0, 0.95, 1])
 for l in np.unique(label):
-    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],
-               color=plt.cm.jet(float(l) / np.max(label + 1)),
-               s=20, edgecolor='k')
-plt.title('With connectivity constraints (time %.2fs)' % elapsed_time)
+    ax.scatter(
+        X[label == l, 0],
+        X[label == l, 1],
+        X[label == l, 2],
+        color=plt.cm.jet(float(l) / np.max(label + 1)),
+        s=20,
+        edgecolor="k",
+    )
+plt.title("With connectivity constraints (time %.2fs)" % elapsed_time)

 plt.show()
('examples/cluster', 'plot_cluster_iris.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =========================================================
 K-means Clustering
@@ -16,8 +14,6 @@
 and finally the ground truth.

 """
-print(__doc__)
-

 # Code source: Gaël Varoquaux
 # Modified for documentation by Jaques Grobler
@@ -25,9 +21,10 @@

 import numpy as np
 import matplotlib.pyplot as plt
+
 # Though the following import is not directly being used, it is required
-# for 3D projection to work
-from mpl_toolkits.mplot3d import Axes3D
+# for 3D projection to work with matplotlib < 3.2
+import mpl_toolkits.mplot3d  # noqa: F401

 from sklearn.cluster import KMeans
 from sklearn import datasets
@@ -38,55 +35,58 @@
 X = iris.data
 y = iris.target

-estimators = [('k_means_iris_8', KMeans(n_clusters=8)),
-              ('k_means_iris_3', KMeans(n_clusters=3)),
-              ('k_means_iris_bad_init', KMeans(n_clusters=3, n_init=1,
-                                               init='random'))]
+estimators = [
+    ("k_means_iris_8", KMeans(n_clusters=8)),
+    ("k_means_iris_3", KMeans(n_clusters=3)),
+    ("k_means_iris_bad_init", KMeans(n_clusters=3, n_init=1, init="random")),
+]

 fignum = 1
-titles = ['8 clusters', '3 clusters', '3 clusters, bad initialization']
+titles = ["8 clusters", "3 clusters", "3 clusters, bad initialization"]
 for name, est in estimators:
     fig = plt.figure(fignum, figsize=(4, 3))
-    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
+    ax = fig.add_subplot(111, projection="3d", elev=48, azim=134)
+    ax.set_position([0, 0, 0.95, 1])
     est.fit(X)
     labels = est.labels_

-    ax.scatter(X[:, 3], X[:, 0], X[:, 2],
-               c=labels.astype(np.float), edgecolor='k')
+    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(float), edgecolor="k")

     ax.w_xaxis.set_ticklabels([])
     ax.w_yaxis.set_ticklabels([])
     ax.w_zaxis.set_ticklabels([])
-    ax.set_xlabel('Petal width')
-    ax.set_ylabel('Sepal length')
-    ax.set_zlabel('Petal length')
+    ax.set_xlabel("Petal width")
+    ax.set_ylabel("Sepal length")
+    ax.set_zlabel("Petal length")
     ax.set_title(titles[fignum - 1])
     ax.dist = 12
     fignum = fignum + 1

 # Plot the ground truth
 fig = plt.figure(fignum, figsize=(4, 3))
-ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
+ax = fig.add_subplot(111, projection="3d", elev=48, azim=134)
+ax.set_position([0, 0, 0.95, 1])

-for name, label in [('Setosa', 0),
-                    ('Versicolour', 1),
-                    ('Virginica', 2)]:
-    ax.text3D(X[y == label, 3].mean(),
-              X[y == label, 0].mean(),
-              X[y == label, 2].mean() + 2, name,
-              horizontalalignment='center',
-              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))
+for name, label in [("Setosa", 0), ("Versicolour", 1), ("Virginica", 2)]:
+    ax.text3D(
+        X[y == label, 3].mean(),
+        X[y == label, 0].mean(),
+        X[y == label, 2].mean() + 2,
+        name,
+        horizontalalignment="center",
+        bbox=dict(alpha=0.2, edgecolor="w", facecolor="w"),
+    )
 # Reorder the labels to have colors matching the cluster results
-y = np.choose(y, [1, 2, 0]).astype(np.float)
-ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor='k')
+y = np.choose(y, [1, 2, 0]).astype(float)
+ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor="k")

 ax.w_xaxis.set_ticklabels([])
 ax.w_yaxis.set_ticklabels([])
 ax.w_zaxis.set_ticklabels([])
-ax.set_xlabel('Petal width')
-ax.set_ylabel('Sepal length')
-ax.set_zlabel('Petal length')
-ax.set_title('Ground Truth')
+ax.set_xlabel("Petal width")
+ax.set_ylabel("Sepal length")
+ax.set_zlabel("Petal length")
+ax.set_title("Ground Truth")
 ax.dist = 12

 fig.show()
('examples/cluster', 'plot_face_compress.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =========================================================
 Vector Quantization Example
@@ -11,8 +9,6 @@
 used for vector quantization.

 """
-print(__doc__)
-

 # Code source: Gaël Varoquaux
 # Modified for documentation by Jaques Grobler
@@ -27,6 +23,7 @@

 try:  # SciPy >= 0.16 have face in misc
     from scipy.misc import face
+
     face = face(gray=True)
 except ImportError:
     face = sp.face(gray=True)
@@ -58,7 +55,7 @@
 # equal bins face
 regular_values = np.linspace(0, 256, n_clusters + 1)
 regular_labels = np.searchsorted(regular_values, face) - 1
-regular_values = .5 * (regular_values[1:] + regular_values[:-1])  # mean
+regular_values = 0.5 * (regular_values[1:] + regular_values[:-1])  # mean
 regular_face = np.choose(regular_labels.ravel(), regular_values, mode="clip")
 regular_face.shape = face.shape
 plt.figure(3, figsize=(3, 2.2))
@@ -67,15 +64,15 @@
 # histogram
 plt.figure(4, figsize=(3, 2.2))
 plt.clf()
-plt.axes([.01, .01, .98, .98])
-plt.hist(X, bins=256, color='.5', edgecolor='.5')
+plt.axes([0.01, 0.01, 0.98, 0.98])
+plt.hist(X, bins=256, color=".5", edgecolor=".5")
 plt.yticks(())
 plt.xticks(regular_values)
 values = np.sort(values)
 for center_1, center_2 in zip(values[:-1], values[1:]):
-    plt.axvline(.5 * (center_1 + center_2), color='b')
+    plt.axvline(0.5 * (center_1 + center_2), color="b")

 for center_1, center_2 in zip(regular_values[:-1], regular_values[1:]):
-    plt.axvline(.5 * (center_1 + center_2), color='b', linestyle='--')
+    plt.axvline(0.5 * (center_1 + center_2), color="b", linestyle="--")

 plt.show()
('examples/cluster', 'plot_kmeans_silhouette_analysis.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -28,7 +28,9 @@
 cluster. However when the ``n_clusters`` is equal to 4, all the plots are more
 or less of similar thickness and hence are of similar sizes as can be also
 verified from the labelled scatter plot on the right.
+
 """
+
 from sklearn.datasets import make_blobs
 from sklearn.cluster import KMeans
 from sklearn.metrics import silhouette_samples, silhouette_score
@@ -37,18 +39,18 @@
 import matplotlib.cm as cm
 import numpy as np

-print(__doc__)
-
 # Generating the sample data from make_blobs
 # This particular setting has one distinct cluster and 3 clusters placed close
 # together.
-X, y = make_blobs(n_samples=500,
-                  n_features=2,
-                  centers=4,
-                  cluster_std=1,
-                  center_box=(-10.0, 10.0),
-                  shuffle=True,
-                  random_state=1)  # For reproducibility
+X, y = make_blobs(
+    n_samples=500,
+    n_features=2,
+    centers=4,
+    cluster_std=1,
+    center_box=(-10.0, 10.0),
+    shuffle=True,
+    random_state=1,
+)  # For reproducibility

 range_n_clusters = [2, 3, 4, 5, 6]

@@ -74,8 +76,12 @@
     # This gives a perspective into the density and separation of the formed
     # clusters
     silhouette_avg = silhouette_score(X, cluster_labels)
-    print("For n_clusters =", n_clusters,
-          "The average silhouette_score is :", silhouette_avg)
+    print(
+        "For n_clusters =",
+        n_clusters,
+        "The average silhouette_score is :",
+        silhouette_avg,
+    )

     # Compute the silhouette scores for each sample
     sample_silhouette_values = silhouette_samples(X, cluster_labels)
@@ -84,8 +90,7 @@
     for i in range(n_clusters):
         # Aggregate the silhouette scores for samples belonging to
         # cluster i, and sort them
-        ith_cluster_silhouette_values = \
-            sample_silhouette_values[cluster_labels == i]
+        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]

         ith_cluster_silhouette_values.sort()

@@ -93,9 +98,14 @@
         y_upper = y_lower + size_cluster_i

         color = cm.nipy_spectral(float(i) / n_clusters)
-        ax1.fill_betweenx(np.arange(y_lower, y_upper),
-                          0, ith_cluster_silhouette_values,
-                          facecolor=color, edgecolor=color, alpha=0.7)
+        ax1.fill_betweenx(
+            np.arange(y_lower, y_upper),
+            0,
+            ith_cluster_silhouette_values,
+            facecolor=color,
+            edgecolor=color,
+            alpha=0.7,
+        )

         # Label the silhouette plots with their cluster numbers at the middle
         ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
@@ -115,25 +125,35 @@

     # 2nd Plot showing the actual clusters formed
     colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
-    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
-                c=colors, edgecolor='k')
+    ax2.scatter(
+        X[:, 0], X[:, 1], marker=".", s=30, lw=0, alpha=0.7, c=colors, edgecolor="k"
+    )

     # Labeling the clusters
     centers = clusterer.cluster_centers_
     # Draw white circles at cluster centers
-    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
-                c="white", alpha=1, s=200, edgecolor='k')
+    ax2.scatter(
+        centers[:, 0],
+        centers[:, 1],
+        marker="o",
+        c="white",
+        alpha=1,
+        s=200,
+        edgecolor="k",
+    )

     for i, c in enumerate(centers):
-        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
-                    s=50, edgecolor='k')
+        ax2.scatter(c[0], c[1], marker="$%d$" % i, alpha=1, s=50, edgecolor="k")

     ax2.set_title("The visualization of the clustered data.")
     ax2.set_xlabel("Feature space for the 1st feature")
     ax2.set_ylabel("Feature space for the 2nd feature")

-    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
-                  "with n_clusters = %d" % n_clusters),
-                 fontsize=14, fontweight='bold')
+    plt.suptitle(
+        "Silhouette analysis for KMeans clustering on sample data with n_clusters = %d"
+        % n_clusters,
+        fontsize=14,
+        fontweight="bold",
+    )

 plt.show()
('examples/cluster', 'plot_agglomerative_clustering.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,8 +6,8 @@
 local structure in the data. The graph is simply the graph of 20 nearest
 neighbors.

-Two consequences of imposing a connectivity can be seen. First clustering
-with a connectivity matrix is much faster.
+Two consequences of imposing a connectivity can be seen. First, clustering
+without a connectivity matrix is much faster.

 Second, when using a connectivity matrix, single, average and complete
 linkage are unstable and tend to create a few clusters that grow very
@@ -20,7 +20,10 @@
 (try decreasing the number of neighbors in kneighbors_graph) and with
 complete linkage. In particular, having a very small number of neighbors in
 the graph, imposes a geometry that is close to that of single linkage,
-which is well known to have this percolation instability. """
+which is well known to have this percolation instability.
+
+"""
+
 # Authors: Gael Varoquaux, Nelle Varoquaux
 # License: BSD 3 clause

@@ -40,7 +43,7 @@


 X = np.concatenate((x, y))
-X += .7 * np.random.randn(2, n_samples)
+X += 0.7 * np.random.randn(2, n_samples)
 X = X.T

 # Create a graph capturing local connectivity. Larger number of neighbors
@@ -53,28 +56,28 @@
 for connectivity in (None, knn_graph):
     for n_clusters in (30, 3):
         plt.figure(figsize=(10, 4))
-        for index, linkage in enumerate(('average',
-                                         'complete',
-                                         'ward',
-                                         'single')):
+        for index, linkage in enumerate(("average", "complete", "ward", "single")):
             plt.subplot(1, 4, index + 1)
-            model = AgglomerativeClustering(linkage=linkage,
-                                            connectivity=connectivity,
-                                            n_clusters=n_clusters)
+            model = AgglomerativeClustering(
+                linkage=linkage, connectivity=connectivity, n_clusters=n_clusters
+            )
             t0 = time.time()
             model.fit(X)
             elapsed_time = time.time() - t0
-            plt.scatter(X[:, 0], X[:, 1], c=model.labels_,
-                        cmap=plt.cm.nipy_spectral)
-            plt.title('linkage=%s\n(time %.2fs)' % (linkage, elapsed_time),
-                      fontdict=dict(verticalalignment='top'))
-            plt.axis('equal')
-            plt.axis('off')
+            plt.scatter(X[:, 0], X[:, 1], c=model.labels_, cmap=plt.cm.nipy_spectral)
+            plt.title(
+                "linkage=%s\n(time %.2fs)" % (linkage, elapsed_time),
+                fontdict=dict(verticalalignment="top"),
+            )
+            plt.axis("equal")
+            plt.axis("off")

-            plt.subplots_adjust(bottom=0, top=.89, wspace=0,
-                                left=0, right=1)
-            plt.suptitle('n_cluster=%i, connectivity=%r' %
-                         (n_clusters, connectivity is not None), size=17)
+            plt.subplots_adjust(bottom=0, top=0.83, wspace=0, left=0, right=1)
+            plt.suptitle(
+                "n_cluster=%i, connectivity=%r"
+                % (n_clusters, connectivity is not None),
+                size=17,
+            )


 plt.show()
('examples/cluster', 'plot_kmeans_stability_low_dim_dense.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -20,8 +20,8 @@

 The dataset used for evaluation is a 2D grid of isotropic Gaussian
 clusters widely spaced.
+
 """
-print(__doc__)

 # Author: Olivier Grisel <olivier.grisel@ensta.org>
 # License: BSD 3 clause
@@ -49,35 +49,35 @@
 n_samples_per_center = 100
 grid_size = 3
 scale = 0.1
-n_clusters = grid_size ** 2
+n_clusters = grid_size**2


 def make_data(random_state, n_samples_per_center, grid_size, scale):
     random_state = check_random_state(random_state)
-    centers = np.array([[i, j]
-                        for i in range(grid_size)
-                        for j in range(grid_size)])
+    centers = np.array([[i, j] for i in range(grid_size) for j in range(grid_size)])
     n_clusters_true, n_features = centers.shape

     noise = random_state.normal(
-        scale=scale, size=(n_samples_per_center, centers.shape[1]))
+        scale=scale, size=(n_samples_per_center, centers.shape[1])
+    )

     X = np.concatenate([c + noise for c in centers])
-    y = np.concatenate([[i] * n_samples_per_center
-                        for i in range(n_clusters_true)])
+    y = np.concatenate([[i] * n_samples_per_center for i in range(n_clusters_true)])
     return shuffle(X, y, random_state=random_state)

+
 # Part 1: Quantitative evaluation of various init methods
+

 plt.figure()
 plots = []
 legends = []

 cases = [
-    (KMeans, 'k-means++', {}),
-    (KMeans, 'random', {}),
-    (MiniBatchKMeans, 'k-means++', {'max_no_improvement': 3}),
-    (MiniBatchKMeans, 'random', {'max_no_improvement': 3, 'init_size': 500}),
+    (KMeans, "k-means++", {}),
+    (KMeans, "random", {}),
+    (MiniBatchKMeans, "k-means++", {"max_no_improvement": 3}),
+    (MiniBatchKMeans, "random", {"max_no_improvement": 3, "init_size": 500}),
 ]

 for factory, init, params in cases:
@@ -87,33 +87,46 @@
     for run_id in range(n_runs):
         X, y = make_data(run_id, n_samples_per_center, grid_size, scale)
         for i, n_init in enumerate(n_init_range):
-            km = factory(n_clusters=n_clusters, init=init, random_state=run_id,
-                         n_init=n_init, **params).fit(X)
+            km = factory(
+                n_clusters=n_clusters,
+                init=init,
+                random_state=run_id,
+                n_init=n_init,
+                **params,
+            ).fit(X)
             inertia[i, run_id] = km.inertia_
     p = plt.errorbar(n_init_range, inertia.mean(axis=1), inertia.std(axis=1))
     plots.append(p[0])
     legends.append("%s with %s init" % (factory.__name__, init))

-plt.xlabel('n_init')
-plt.ylabel('inertia')
+plt.xlabel("n_init")
+plt.ylabel("inertia")
 plt.legend(plots, legends)
 plt.title("Mean inertia for various k-means init across %d runs" % n_runs)

 # Part 2: Qualitative visual inspection of the convergence

 X, y = make_data(random_state, n_samples_per_center, grid_size, scale)
-km = MiniBatchKMeans(n_clusters=n_clusters, init='random', n_init=1,
-                     random_state=random_state).fit(X)
+km = MiniBatchKMeans(
+    n_clusters=n_clusters, init="random", n_init=1, random_state=random_state
+).fit(X)

 plt.figure()
 for k in range(n_clusters):
     my_members = km.labels_ == k
     color = cm.nipy_spectral(float(k) / n_clusters, 1)
-    plt.plot(X[my_members, 0], X[my_members, 1], 'o', marker='.', c=color)
+    plt.plot(X[my_members, 0], X[my_members, 1], "o", marker=".", c=color)
     cluster_center = km.cluster_centers_[k]
-    plt.plot(cluster_center[0], cluster_center[1], 'o',
-             markerfacecolor=color, markeredgecolor='k', markersize=6)
-    plt.title("Example cluster allocation with a single random init\n"
-              "with MiniBatchKMeans")
+    plt.plot(
+        cluster_center[0],
+        cluster_center[1],
+        "o",
+        markerfacecolor=color,
+        markeredgecolor="k",
+        markersize=6,
+    )
+    plt.title(
+        "Example cluster allocation with a single random init\nwith MiniBatchKMeans"
+    )

 plt.show()
('examples/cluster', 'plot_kmeans_digits.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,12 +3,11 @@
 A demo of K-Means clustering on the handwritten digits data
 ===========================================================

-In this example we compare the various initialization strategies for
-K-means in terms of runtime and quality of the results.
-
-As the ground truth is known here, we also apply different cluster
-quality metrics to judge the goodness of fit of the cluster labels to the
-ground truth.
+In this example we compare the various initialization strategies for K-means in
+terms of runtime and quality of the results.
+
+As the ground truth is known here, we also apply different cluster quality
+metrics to judge the goodness of fit of the cluster labels to the ground truth.

 Cluster quality metrics evaluated (see :ref:`clustering_evaluation` for
 definitions and discussions of the metrics):
@@ -25,75 +24,138 @@
 =========== ========================================================

 """
-print(__doc__)
-
+
+# %%
+# Load the dataset
+# ----------------
+#
+# We will start by loading the `digits` dataset. This dataset contains
+# handwritten digits from 0 to 9. In the context of clustering, one would like
+# to group images such that the handwritten digits on the image are the same.
+
+import numpy as np
+from sklearn.datasets import load_digits
+
+data, labels = load_digits(return_X_y=True)
+(n_samples, n_features), n_digits = data.shape, np.unique(labels).size
+
+print(f"# digits: {n_digits}; # samples: {n_samples}; # features {n_features}")
+
+# %%
+# Define our evaluation benchmark
+# -------------------------------
+#
+# We will first our evaluation benchmark. During this benchmark, we intend to
+# compare different initialization methods for KMeans. Our benchmark will:
+#
+# * create a pipeline which will scale the data using a
+#   :class:`~sklearn.preprocessing.StandardScaler`;
+# * train and time the pipeline fitting;
+# * measure the performance of the clustering obtained via different metrics.
 from time import time
-import numpy as np
+from sklearn import metrics
+from sklearn.pipeline import make_pipeline
+from sklearn.preprocessing import StandardScaler
+
+
+def bench_k_means(kmeans, name, data, labels):
+    """Benchmark to evaluate the KMeans initialization methods.
+
+    Parameters
+    ----------
+    kmeans : KMeans instance
+        A :class:`~sklearn.cluster.KMeans` instance with the initialization
+        already set.
+    name : str
+        Name given to the strategy. It will be used to show the results in a
+        table.
+    data : ndarray of shape (n_samples, n_features)
+        The data to cluster.
+    labels : ndarray of shape (n_samples,)
+        The labels used to compute the clustering metrics which requires some
+        supervision.
+    """
+    t0 = time()
+    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)
+    fit_time = time() - t0
+    results = [name, fit_time, estimator[-1].inertia_]
+
+    # Define the metrics which require only the true labels and estimator
+    # labels
+    clustering_metrics = [
+        metrics.homogeneity_score,
+        metrics.completeness_score,
+        metrics.v_measure_score,
+        metrics.adjusted_rand_score,
+        metrics.adjusted_mutual_info_score,
+    ]
+    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]
+
+    # The silhouette score requires the full dataset
+    results += [
+        metrics.silhouette_score(
+            data,
+            estimator[-1].labels_,
+            metric="euclidean",
+            sample_size=300,
+        )
+    ]
+
+    # Show the results
+    formatter_result = (
+        "{:9s}\t{:.3f}s\t{:.0f}\t{:.3f}\t{:.3f}\t{:.3f}\t{:.3f}\t{:.3f}\t{:.3f}"
+    )
+    print(formatter_result.format(*results))
+
+
+# %%
+# Run the benchmark
+# -----------------
+#
+# We will compare three approaches:
+#
+# * an initialization using `kmeans++`. This method is stochastic and we will
+#   run the initialization 4 times;
+# * a random initialization. This method is stochastic as well and we will run
+#   the initialization 4 times;
+# * an initialization based on a :class:`~sklearn.decomposition.PCA`
+#   projection. Indeed, we will use the components of the
+#   :class:`~sklearn.decomposition.PCA` to initialize KMeans. This method is
+#   deterministic and a single initialization suffice.
+from sklearn.cluster import KMeans
+from sklearn.decomposition import PCA
+
+print(82 * "_")
+print("init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\tsilhouette")
+
+kmeans = KMeans(init="k-means++", n_clusters=n_digits, n_init=4, random_state=0)
+bench_k_means(kmeans=kmeans, name="k-means++", data=data, labels=labels)
+
+kmeans = KMeans(init="random", n_clusters=n_digits, n_init=4, random_state=0)
+bench_k_means(kmeans=kmeans, name="random", data=data, labels=labels)
+
+pca = PCA(n_components=n_digits).fit(data)
+kmeans = KMeans(init=pca.components_, n_clusters=n_digits, n_init=1)
+bench_k_means(kmeans=kmeans, name="PCA-based", data=data, labels=labels)
+
+print(82 * "_")
+
+# %%
+# Visualize the results on PCA-reduced data
+# -----------------------------------------
+#
+# :class:`~sklearn.decomposition.PCA` allows to project the data from the
+# original 64-dimensional space into a lower dimensional space. Subsequently,
+# we can use :class:`~sklearn.decomposition.PCA` to project into a
+# 2-dimensional space and plot the data and the clusters in this new space.
 import matplotlib.pyplot as plt

-from sklearn import metrics
-from sklearn.cluster import KMeans
-from sklearn.datasets import load_digits
-from sklearn.decomposition import PCA
-from sklearn.preprocessing import scale
-
-np.random.seed(42)
-
-digits = load_digits()
-data = scale(digits.data)
-
-n_samples, n_features = data.shape
-n_digits = len(np.unique(digits.target))
-labels = digits.target
-
-sample_size = 300
-
-print("n_digits: %d, \t n_samples %d, \t n_features %d"
-      % (n_digits, n_samples, n_features))
-
-
-print(82 * '_')
-print('init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\tsilhouette')
-
-
-def bench_k_means(estimator, name, data):
-    t0 = time()
-    estimator.fit(data)
-    print('%-9s\t%.2fs\t%i\t%.3f\t%.3f\t%.3f\t%.3f\t%.3f\t%.3f'
-          % (name, (time() - t0), estimator.inertia_,
-             metrics.homogeneity_score(labels, estimator.labels_),
-             metrics.completeness_score(labels, estimator.labels_),
-             metrics.v_measure_score(labels, estimator.labels_),
-             metrics.adjusted_rand_score(labels, estimator.labels_),
-             metrics.adjusted_mutual_info_score(labels,  estimator.labels_,
-                                                average_method='arithmetic'),
-             metrics.silhouette_score(data, estimator.labels_,
-                                      metric='euclidean',
-                                      sample_size=sample_size)))
-
-bench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),
-              name="k-means++", data=data)
-
-bench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),
-              name="random", data=data)
-
-# in this case the seeding of the centers is deterministic, hence we run the
-# kmeans algorithm only once with n_init=1
-pca = PCA(n_components=n_digits).fit(data)
-bench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),
-              name="PCA-based",
-              data=data)
-print(82 * '_')
-
-# #############################################################################
-# Visualize the results on PCA-reduced data
-
 reduced_data = PCA(n_components=2).fit_transform(data)
-kmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)
+kmeans = KMeans(init="k-means++", n_clusters=n_digits, n_init=4)
 kmeans.fit(reduced_data)

 # Step size of the mesh. Decrease to increase the quality of the VQ.
-h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].
+h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].

 # Plot the decision boundary. For that, we will assign a color to each
 x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1
@@ -107,19 +169,31 @@
 Z = Z.reshape(xx.shape)
 plt.figure(1)
 plt.clf()
-plt.imshow(Z, interpolation='nearest',
-           extent=(xx.min(), xx.max(), yy.min(), yy.max()),
-           cmap=plt.cm.Paired,
-           aspect='auto', origin='lower')
-
-plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)
+plt.imshow(
+    Z,
+    interpolation="nearest",
+    extent=(xx.min(), xx.max(), yy.min(), yy.max()),
+    cmap=plt.cm.Paired,
+    aspect="auto",
+    origin="lower",
+)
+
+plt.plot(reduced_data[:, 0], reduced_data[:, 1], "k.", markersize=2)
 # Plot the centroids as a white X
 centroids = kmeans.cluster_centers_
-plt.scatter(centroids[:, 0], centroids[:, 1],
-            marker='x', s=169, linewidths=3,
-            color='w', zorder=10)
-plt.title('K-means clustering on the digits dataset (PCA-reduced data)\n'
-          'Centroids are marked with white cross')
+plt.scatter(
+    centroids[:, 0],
+    centroids[:, 1],
+    marker="x",
+    s=169,
+    linewidths=3,
+    color="w",
+    zorder=10,
+)
+plt.title(
+    "K-means clustering on the digits dataset (PCA-reduced data)\n"
+    "Centroids are marked with white cross"
+)
 plt.xlim(x_min, x_max)
 plt.ylim(y_min, y_max)
 plt.xticks(())
('examples/cluster', 'plot_kmeans_assumptions.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,8 +8,8 @@
 input data does not conform to some implicit assumption that k-means makes and
 undesirable clusters are produced as a result. In the last plot, k-means
 returns intuitive clusters despite unevenly sized blobs.
+
 """
-print(__doc__)

 # Author: Phil Roth <mr.phil.roth@gmail.com>
 # License: BSD 3 clause
@@ -43,9 +43,9 @@
 plt.title("Anisotropicly Distributed Blobs")

 # Different variance
-X_varied, y_varied = make_blobs(n_samples=n_samples,
-                                cluster_std=[1.0, 2.5, 0.5],
-                                random_state=random_state)
+X_varied, y_varied = make_blobs(
+    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state
+)
 y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)

 plt.subplot(223)
@@ -54,8 +54,7 @@

 # Unevenly sized blobs
 X_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10]))
-y_pred = KMeans(n_clusters=3,
-                random_state=random_state).fit_predict(X_filtered)
+y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_filtered)

 plt.subplot(224)
 plt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)
('examples/cluster', 'plot_coin_ward_segmentation.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,72 +6,88 @@
 Compute the segmentation of a 2D image with Ward hierarchical
 clustering. The clustering is spatially constrained in order
 for each segmented region to be in one piece.
+
 """

 # Author : Vincent Michel, 2010
 #          Alexandre Gramfort, 2011
 # License: BSD 3 clause

-print(__doc__)
+# %%
+# Generate data
+# -------------
+
+from skimage.data import coins
+
+orig_coins = coins()
+
+# %%
+# Resize it to 20% of the original size to speed up the processing
+# Applying a Gaussian filter for smoothing prior to down-scaling
+# reduces aliasing artifacts.
+
+import numpy as np
+from scipy.ndimage import gaussian_filter
+from skimage.transform import rescale
+
+smoothened_coins = gaussian_filter(orig_coins, sigma=2)
+rescaled_coins = rescale(
+    smoothened_coins,
+    0.2,
+    mode="reflect",
+    anti_aliasing=False,
+)
+
+X = np.reshape(rescaled_coins, (-1, 1))
+
+# %%
+# Define structure of the data
+# ----------------------------
+#
+# Pixels are connected to their neighbors.
+
+from sklearn.feature_extraction.image import grid_to_graph
+
+connectivity = grid_to_graph(*rescaled_coins.shape)
+
+# %%
+# Compute clustering
+# ------------------

 import time as time

-import numpy as np
-from distutils.version import LooseVersion
-from scipy.ndimage.filters import gaussian_filter
+from sklearn.cluster import AgglomerativeClustering
+
+print("Compute structured hierarchical clustering...")
+st = time.time()
+n_clusters = 27  # number of regions
+ward = AgglomerativeClustering(
+    n_clusters=n_clusters, linkage="ward", connectivity=connectivity
+)
+ward.fit(X)
+label = np.reshape(ward.labels_, rescaled_coins.shape)
+print(f"Elapsed time: {time.time() - st:.3f}s")
+print(f"Number of pixels: {label.size}")
+print(f"Number of clusters: {np.unique(label).size}")
+
+# %%
+# Plot the results on an image
+# ----------------------------
+#
+# Agglomerative clustering is able to segment each coin however, we have had to
+# use a ``n_cluster`` larger than the number of coins because the segmentation
+# is finding a large in the background.

 import matplotlib.pyplot as plt

-import skimage
-from skimage.data import coins
-from skimage.transform import rescale
-
-from sklearn.feature_extraction.image import grid_to_graph
-from sklearn.cluster import AgglomerativeClustering
-
-# these were introduced in skimage-0.14
-if LooseVersion(skimage.__version__) >= '0.14':
-    rescale_params = {'anti_aliasing': False, 'multichannel': False}
-else:
-    rescale_params = {}
-
-# #############################################################################
-# Generate data
-orig_coins = coins()
-
-# Resize it to 20% of the original size to speed up the processing
-# Applying a Gaussian filter for smoothing prior to down-scaling
-# reduces aliasing artifacts.
-smoothened_coins = gaussian_filter(orig_coins, sigma=2)
-rescaled_coins = rescale(smoothened_coins, 0.2, mode="reflect",
-                         **rescale_params)
-
-X = np.reshape(rescaled_coins, (-1, 1))
-
-# #############################################################################
-# Define the structure A of the data. Pixels connected to their neighbors.
-connectivity = grid_to_graph(*rescaled_coins.shape)
-
-# #############################################################################
-# Compute clustering
-print("Compute structured hierarchical clustering...")
-st = time.time()
-n_clusters = 27  # number of regions
-ward = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward',
-                               connectivity=connectivity)
-ward.fit(X)
-label = np.reshape(ward.labels_, rescaled_coins.shape)
-print("Elapsed time: ", time.time() - st)
-print("Number of pixels: ", label.size)
-print("Number of clusters: ", np.unique(label).size)
-
-# #############################################################################
-# Plot the results on an image
 plt.figure(figsize=(5, 5))
 plt.imshow(rescaled_coins, cmap=plt.cm.gray)
 for l in range(n_clusters):
-    plt.contour(label == l,
-                colors=[plt.cm.nipy_spectral(l / float(n_clusters)), ])
-plt.xticks(())
-plt.yticks(())
+    plt.contour(
+        label == l,
+        colors=[
+            plt.cm.nipy_spectral(l / float(n_clusters)),
+        ],
+    )
+plt.axis("off")
 plt.show()
('examples/cluster', 'plot_dbscan.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,26 +7,28 @@
 Finds core samples of high density and expands clusters from them.

 """
-print(__doc__)

 import numpy as np

 from sklearn.cluster import DBSCAN
 from sklearn import metrics
-from sklearn.datasets.samples_generator import make_blobs
+from sklearn.datasets import make_blobs
 from sklearn.preprocessing import StandardScaler


-# #############################################################################
+# %%
 # Generate sample data
+# --------------------
 centers = [[1, 1], [-1, -1], [1, -1]]
-X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,
-                            random_state=0)
+X, labels_true = make_blobs(
+    n_samples=750, centers=centers, cluster_std=0.4, random_state=0
+)

 X = StandardScaler().fit_transform(X)

-# #############################################################################
+# %%
 # Compute DBSCAN
+# --------------
 db = DBSCAN(eps=0.3, min_samples=10).fit(X)
 core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
 core_samples_mask[db.core_sample_indices_] = True
@@ -36,41 +38,52 @@
 n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
 n_noise_ = list(labels).count(-1)

-print('Estimated number of clusters: %d' % n_clusters_)
-print('Estimated number of noise points: %d' % n_noise_)
+print("Estimated number of clusters: %d" % n_clusters_)
+print("Estimated number of noise points: %d" % n_noise_)
 print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))
 print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))
 print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))
-print("Adjusted Rand Index: %0.3f"
-      % metrics.adjusted_rand_score(labels_true, labels))
-print("Adjusted Mutual Information: %0.3f"
-      % metrics.adjusted_mutual_info_score(labels_true, labels,
-                                           average_method='arithmetic'))
-print("Silhouette Coefficient: %0.3f"
-      % metrics.silhouette_score(X, labels))
+print("Adjusted Rand Index: %0.3f" % metrics.adjusted_rand_score(labels_true, labels))
+print(
+    "Adjusted Mutual Information: %0.3f"
+    % metrics.adjusted_mutual_info_score(labels_true, labels)
+)
+print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(X, labels))

-# #############################################################################
+# %%
 # Plot result
+# -----------
 import matplotlib.pyplot as plt

 # Black removed and is used for noise instead.
 unique_labels = set(labels)
-colors = [plt.cm.Spectral(each)
-          for each in np.linspace(0, 1, len(unique_labels))]
+colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]
 for k, col in zip(unique_labels, colors):
     if k == -1:
         # Black used for noise.
         col = [0, 0, 0, 1]

-    class_member_mask = (labels == k)
+    class_member_mask = labels == k

     xy = X[class_member_mask & core_samples_mask]
-    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
-             markeredgecolor='k', markersize=14)
+    plt.plot(
+        xy[:, 0],
+        xy[:, 1],
+        "o",
+        markerfacecolor=tuple(col),
+        markeredgecolor="k",
+        markersize=14,
+    )

     xy = X[class_member_mask & ~core_samples_mask]
-    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
-             markeredgecolor='k', markersize=6)
+    plt.plot(
+        xy[:, 0],
+        xy[:, 1],
+        "o",
+        markerfacecolor=tuple(col),
+        markeredgecolor="k",
+        markersize=6,
+    )

-plt.title('Estimated number of clusters: %d' % n_clusters_)
+plt.title("Estimated number of clusters: %d" % n_clusters_)
 plt.show()
('examples/cluster', 'plot_optics.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -2,20 +2,23 @@
 ===================================
 Demo of OPTICS clustering algorithm
 ===================================
+
+.. currentmodule:: sklearn
+
 Finds core samples of high density and expands clusters from them.
 This example uses data that is generated so that the clusters have
 different densities.
-The :class:`sklearn.cluster.OPTICS` is first used with its Xi cluster detection
+The :class:`~cluster.OPTICS` is first used with its Xi cluster detection
 method, and then setting specific thresholds on the reachability, which
-corresponds to :class:`sklearn.cluster.DBSCAN`. We can see that the different
+corresponds to :class:`~cluster.DBSCAN`. We can see that the different
 clusters of OPTICS's Xi method can be recovered with different choices of
 thresholds in DBSCAN.
+
 """

 # Authors: Shane Grigsby <refuge@rocktalus.com>
 #          Adrin Jalali <adrin.jalali@gmail.com>
 # License: BSD 3 clause
-

 from sklearn.cluster import OPTICS, cluster_optics_dbscan
 import matplotlib.gridspec as gridspec
@@ -27,25 +30,31 @@
 np.random.seed(0)
 n_points_per_cluster = 250

-C1 = [-5, -2] + .8 * np.random.randn(n_points_per_cluster, 2)
-C2 = [4, -1] + .1 * np.random.randn(n_points_per_cluster, 2)
-C3 = [1, -2] + .2 * np.random.randn(n_points_per_cluster, 2)
-C4 = [-2, 3] + .3 * np.random.randn(n_points_per_cluster, 2)
+C1 = [-5, -2] + 0.8 * np.random.randn(n_points_per_cluster, 2)
+C2 = [4, -1] + 0.1 * np.random.randn(n_points_per_cluster, 2)
+C3 = [1, -2] + 0.2 * np.random.randn(n_points_per_cluster, 2)
+C4 = [-2, 3] + 0.3 * np.random.randn(n_points_per_cluster, 2)
 C5 = [3, -2] + 1.6 * np.random.randn(n_points_per_cluster, 2)
 C6 = [5, 6] + 2 * np.random.randn(n_points_per_cluster, 2)
 X = np.vstack((C1, C2, C3, C4, C5, C6))

-clust = OPTICS(min_samples=50, xi=.05, min_cluster_size=.05)
+clust = OPTICS(min_samples=50, xi=0.05, min_cluster_size=0.05)

 # Run the fit
 clust.fit(X)

-labels_050 = cluster_optics_dbscan(reachability=clust.reachability_,
-                                   core_distances=clust.core_distances_,
-                                   ordering=clust.ordering_, eps=0.5)
-labels_200 = cluster_optics_dbscan(reachability=clust.reachability_,
-                                   core_distances=clust.core_distances_,
-                                   ordering=clust.ordering_, eps=2)
+labels_050 = cluster_optics_dbscan(
+    reachability=clust.reachability_,
+    core_distances=clust.core_distances_,
+    ordering=clust.ordering_,
+    eps=0.5,
+)
+labels_200 = cluster_optics_dbscan(
+    reachability=clust.reachability_,
+    core_distances=clust.core_distances_,
+    ordering=clust.ordering_,
+    eps=2,
+)

 space = np.arange(len(X))
 reachability = clust.reachability_[clust.ordering_]
@@ -59,40 +68,40 @@
 ax4 = plt.subplot(G[1, 2])

 # Reachability plot
-colors = ['g.', 'r.', 'b.', 'y.', 'c.']
+colors = ["g.", "r.", "b.", "y.", "c."]
 for klass, color in zip(range(0, 5), colors):
     Xk = space[labels == klass]
     Rk = reachability[labels == klass]
     ax1.plot(Xk, Rk, color, alpha=0.3)
-ax1.plot(space[labels == -1], reachability[labels == -1], 'k.', alpha=0.3)
-ax1.plot(space, np.full_like(space, 2., dtype=float), 'k-', alpha=0.5)
-ax1.plot(space, np.full_like(space, 0.5, dtype=float), 'k-.', alpha=0.5)
-ax1.set_ylabel('Reachability (epsilon distance)')
-ax1.set_title('Reachability Plot')
+ax1.plot(space[labels == -1], reachability[labels == -1], "k.", alpha=0.3)
+ax1.plot(space, np.full_like(space, 2.0, dtype=float), "k-", alpha=0.5)
+ax1.plot(space, np.full_like(space, 0.5, dtype=float), "k-.", alpha=0.5)
+ax1.set_ylabel("Reachability (epsilon distance)")
+ax1.set_title("Reachability Plot")

 # OPTICS
-colors = ['g.', 'r.', 'b.', 'y.', 'c.']
+colors = ["g.", "r.", "b.", "y.", "c."]
 for klass, color in zip(range(0, 5), colors):
     Xk = X[clust.labels_ == klass]
     ax2.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)
-ax2.plot(X[clust.labels_ == -1, 0], X[clust.labels_ == -1, 1], 'k+', alpha=0.1)
-ax2.set_title('Automatic Clustering\nOPTICS')
+ax2.plot(X[clust.labels_ == -1, 0], X[clust.labels_ == -1, 1], "k+", alpha=0.1)
+ax2.set_title("Automatic Clustering\nOPTICS")

 # DBSCAN at 0.5
-colors = ['g', 'greenyellow', 'olive', 'r', 'b', 'c']
+colors = ["g", "greenyellow", "olive", "r", "b", "c"]
 for klass, color in zip(range(0, 6), colors):
     Xk = X[labels_050 == klass]
-    ax3.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3, marker='.')
-ax3.plot(X[labels_050 == -1, 0], X[labels_050 == -1, 1], 'k+', alpha=0.1)
-ax3.set_title('Clustering at 0.5 epsilon cut\nDBSCAN')
+    ax3.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3, marker=".")
+ax3.plot(X[labels_050 == -1, 0], X[labels_050 == -1, 1], "k+", alpha=0.1)
+ax3.set_title("Clustering at 0.5 epsilon cut\nDBSCAN")

 # DBSCAN at 2.
-colors = ['g.', 'm.', 'y.', 'c.']
+colors = ["g.", "m.", "y.", "c."]
 for klass, color in zip(range(0, 4), colors):
     Xk = X[labels_200 == klass]
     ax4.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)
-ax4.plot(X[labels_200 == -1, 0], X[labels_200 == -1, 1], 'k+', alpha=0.1)
-ax4.set_title('Clustering at 2.0 epsilon cut\nDBSCAN')
+ax4.plot(X[labels_200 == -1, 0], X[labels_200 == -1, 1], "k+", alpha=0.1)
+ax4.set_title("Clustering at 2.0 epsilon cut\nDBSCAN")

 plt.tight_layout()
 plt.show()
('examples/semi_supervised', 'plot_label_propagation_structure.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,55 +8,98 @@
 labeled "red" and the inner circle "blue". Because both label groups
 lie inside their own distinct shape, we can see that the labels
 propagate correctly around the circle.
+
 """
-print(__doc__)

 # Authors: Clay Woolam <clay@woolam.org>
 #          Andreas Mueller <amueller@ais.uni-bonn.de>
 # License: BSD

+# %%
+# We generate a dataset with two concentric circles. In addition, a label
+# is associated with each sample of the dataset that is: 0 (belonging to
+# the outer circle), 1 (belonging to the inner circle), and -1 (unknown).
+# Here, all labels but two are tagged as unknown.
+
 import numpy as np
-import matplotlib.pyplot as plt
-from sklearn.semi_supervised import label_propagation
 from sklearn.datasets import make_circles

-# generate ring with inner box
 n_samples = 200
 X, y = make_circles(n_samples=n_samples, shuffle=False)
 outer, inner = 0, 1
-labels = np.full(n_samples, -1.)
+labels = np.full(n_samples, -1.0)
 labels[0] = outer
 labels[-1] = inner

-# #############################################################################
-# Learn with LabelSpreading
-label_spread = label_propagation.LabelSpreading(kernel='knn', alpha=0.8)
+# %%
+# Plot raw data
+import matplotlib.pyplot as plt
+
+plt.figure(figsize=(4, 4))
+plt.scatter(
+    X[labels == outer, 0],
+    X[labels == outer, 1],
+    color="navy",
+    marker="s",
+    lw=0,
+    label="outer labeled",
+    s=10,
+)
+plt.scatter(
+    X[labels == inner, 0],
+    X[labels == inner, 1],
+    color="c",
+    marker="s",
+    lw=0,
+    label="inner labeled",
+    s=10,
+)
+plt.scatter(
+    X[labels == -1, 0],
+    X[labels == -1, 1],
+    color="darkorange",
+    marker=".",
+    label="unlabeled",
+)
+plt.legend(scatterpoints=1, shadow=False, loc="upper right")
+plt.title("Raw data (2 classes=outer and inner)")
+
+# %%
+#
+# The aim of :class:`~sklearn.semi_supervised.LabelSpreading` is to associate
+# a label to sample where the label is initially unknown.
+from sklearn.semi_supervised import LabelSpreading
+
+label_spread = LabelSpreading(kernel="knn", alpha=0.8)
 label_spread.fit(X, labels)

-# #############################################################################
-# Plot output labels
+# %%
+# Now, we can check which labels have been associated with each sample
+# when the label was unknown.
 output_labels = label_spread.transduction_
-plt.figure(figsize=(8.5, 4))
-plt.subplot(1, 2, 1)
-plt.scatter(X[labels == outer, 0], X[labels == outer, 1], color='navy',
-            marker='s', lw=0, label="outer labeled", s=10)
-plt.scatter(X[labels == inner, 0], X[labels == inner, 1], color='c',
-            marker='s', lw=0, label='inner labeled', s=10)
-plt.scatter(X[labels == -1, 0], X[labels == -1, 1], color='darkorange',
-            marker='.', label='unlabeled')
-plt.legend(scatterpoints=1, shadow=False, loc='upper right')
-plt.title("Raw data (2 classes=outer and inner)")
-
-plt.subplot(1, 2, 2)
 output_label_array = np.asarray(output_labels)
 outer_numbers = np.where(output_label_array == outer)[0]
 inner_numbers = np.where(output_label_array == inner)[0]
-plt.scatter(X[outer_numbers, 0], X[outer_numbers, 1], color='navy',
-            marker='s', lw=0, s=10, label="outer learned")
-plt.scatter(X[inner_numbers, 0], X[inner_numbers, 1], color='c',
-            marker='s', lw=0, s=10, label="inner learned")
-plt.legend(scatterpoints=1, shadow=False, loc='upper right')
+
+plt.figure(figsize=(4, 4))
+plt.scatter(
+    X[outer_numbers, 0],
+    X[outer_numbers, 1],
+    color="navy",
+    marker="s",
+    lw=0,
+    s=10,
+    label="outer learned",
+)
+plt.scatter(
+    X[inner_numbers, 0],
+    X[inner_numbers, 1],
+    color="c",
+    marker="s",
+    lw=0,
+    s=10,
+    label="inner learned",
+)
+plt.legend(scatterpoints=1, shadow=False, loc="upper right")
 plt.title("Labels learned with Label Spreading (KNN)")
-
-plt.subplots_adjust(left=0.07, bottom=0.07, right=0.93, top=0.92)
 plt.show()
('examples/semi_supervised', 'plot_label_propagation_digits_active_learning.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -17,8 +17,8 @@
 A plot will appear showing the top 5 most uncertain digits for each iteration
 of training. These may or may not contain mistakes, but we will train the next
 model with their true labels.
+
 """
-print(__doc__)

 # Authors: Clay Woolam <clay@woolam.org>
 # License: BSD
@@ -28,7 +28,7 @@
 from scipy import stats

 from sklearn import datasets
-from sklearn.semi_supervised import label_propagation
+from sklearn.semi_supervised import LabelSpreading
 from sklearn.metrics import classification_report, confusion_matrix

 digits = datasets.load_digits()
@@ -54,19 +54,19 @@
     y_train = np.copy(y)
     y_train[unlabeled_indices] = -1

-    lp_model = label_propagation.LabelSpreading(gamma=0.25, max_iter=20)
+    lp_model = LabelSpreading(gamma=0.25, max_iter=20)
     lp_model.fit(X, y_train)

     predicted_labels = lp_model.transduction_[unlabeled_indices]
     true_labels = y[unlabeled_indices]

-    cm = confusion_matrix(true_labels, predicted_labels,
-                          labels=lp_model.classes_)
+    cm = confusion_matrix(true_labels, predicted_labels, labels=lp_model.classes_)

     print("Iteration %i %s" % (i, 70 * "_"))
-    print("Label Spreading model: %d labeled & %d unlabeled (%d total)"
-          % (n_labeled_points, n_total_samples - n_labeled_points,
-             n_total_samples))
+    print(
+        "Label Spreading model: %d labeled & %d unlabeled (%d total)"
+        % (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples)
+    )

     print(classification_report(true_labels, predicted_labels))

@@ -74,42 +74,50 @@
     print(cm)

     # compute the entropies of transduced label distributions
-    pred_entropies = stats.distributions.entropy(
-        lp_model.label_distributions_.T)
+    pred_entropies = stats.distributions.entropy(lp_model.label_distributions_.T)

     # select up to 5 digit examples that the classifier is most uncertain about
     uncertainty_index = np.argsort(pred_entropies)[::-1]
     uncertainty_index = uncertainty_index[
-        np.in1d(uncertainty_index, unlabeled_indices)][:5]
+        np.in1d(uncertainty_index, unlabeled_indices)
+    ][:5]

     # keep track of indices that we get labels for
     delete_indices = np.array([], dtype=int)

     # for more than 5 iterations, visualize the gain only on the first 5
     if i < 5:
-        f.text(.05, (1 - (i + 1) * .183),
-               "model %d\n\nfit with\n%d labels" %
-               ((i + 1), i * 5 + 10), size=10)
+        f.text(
+            0.05,
+            (1 - (i + 1) * 0.183),
+            "model %d\n\nfit with\n%d labels" % ((i + 1), i * 5 + 10),
+            size=10,
+        )
     for index, image_index in enumerate(uncertainty_index):
         image = images[image_index]

         # for more than 5 iterations, visualize the gain only on the first 5
         if i < 5:
             sub = f.add_subplot(5, 5, index + 1 + (5 * i))
-            sub.imshow(image, cmap=plt.cm.gray_r, interpolation='none')
-            sub.set_title("predict: %i\ntrue: %i" % (
-                lp_model.transduction_[image_index], y[image_index]), size=10)
-            sub.axis('off')
+            sub.imshow(image, cmap=plt.cm.gray_r, interpolation="none")
+            sub.set_title(
+                "predict: %i\ntrue: %i"
+                % (lp_model.transduction_[image_index], y[image_index]),
+                size=10,
+            )
+            sub.axis("off")

         # labeling 5 points, remote from labeled set
-        delete_index, = np.where(unlabeled_indices == image_index)
+        (delete_index,) = np.where(unlabeled_indices == image_index)
         delete_indices = np.concatenate((delete_indices, delete_index))

     unlabeled_indices = np.delete(unlabeled_indices, delete_indices)
     n_labeled_points += len(uncertainty_index)

-f.suptitle("Active learning with Label Propagation.\nRows show 5 most "
-           "uncertain labels to learn with the next model.", y=1.15)
-plt.subplots_adjust(left=0.2, bottom=0.03, right=0.9, top=0.9, wspace=0.2,
-                    hspace=0.85)
+f.suptitle(
+    "Active learning with Label Propagation.\nRows show 5 most "
+    "uncertain labels to learn with the next model.",
+    y=1.15,
+)
+plt.subplots_adjust(left=0.2, bottom=0.03, right=0.9, top=0.9, wspace=0.2, hspace=0.85)
 plt.show()
('examples/semi_supervised', 'plot_label_propagation_digits.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -13,27 +13,30 @@
 class will be very good.

 At the end, the top 10 most uncertain predictions will be shown.
+
 """
-print(__doc__)

 # Authors: Clay Woolam <clay@woolam.org>
 # License: BSD

+# %%
+# Data generation
+# ---------------
+#
+# We use the digits dataset. We only use a subset of randomly selected samples.
+from sklearn import datasets
 import numpy as np
-import matplotlib.pyplot as plt
-
-from scipy import stats
-
-from sklearn import datasets
-from sklearn.semi_supervised import label_propagation
-
-from sklearn.metrics import confusion_matrix, classification_report

 digits = datasets.load_digits()
 rng = np.random.RandomState(2)
 indices = np.arange(len(digits.data))
 rng.shuffle(indices)

+# %%
+#
+# We selected 340 samples of which only 40 will be associated with a known label.
+# Therefore, we store the indices of the 300 other samples for which we are not
+# supposed to know their labels.
 X = digits.data[indices[:340]]
 y = digits.target[indices[:340]]
 images = digits.images[indices[:340]]
@@ -45,38 +48,59 @@

 unlabeled_set = indices[n_labeled_points:]

-# #############################################################################
+# %%
 # Shuffle everything around
 y_train = np.copy(y)
 y_train[unlabeled_set] = -1

-# #############################################################################
-# Learn with LabelSpreading
-lp_model = label_propagation.LabelSpreading(gamma=.25, max_iter=20)
+# %%
+# Semi-supervised learning
+# ------------------------
+#
+# We fit a :class:`~sklearn.semi_supervised.LabelSpreading` and use it to predict
+# the unknown labels.
+from sklearn.semi_supervised import LabelSpreading
+from sklearn.metrics import classification_report
+
+lp_model = LabelSpreading(gamma=0.25, max_iter=20)
 lp_model.fit(X, y_train)
 predicted_labels = lp_model.transduction_[unlabeled_set]
 true_labels = y[unlabeled_set]

-cm = confusion_matrix(true_labels, predicted_labels, labels=lp_model.classes_)
+print(
+    "Label Spreading model: %d labeled & %d unlabeled points (%d total)"
+    % (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples)
+)

-print("Label Spreading model: %d labeled & %d unlabeled points (%d total)" %
-      (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples))
-
+# %%
+# Classification report
 print(classification_report(true_labels, predicted_labels))

-print("Confusion matrix")
-print(cm)
+# %%
+# Confusion matrix
+from sklearn.metrics import ConfusionMatrixDisplay

-# #############################################################################
-# Calculate uncertainty values for each transduced distribution
+ConfusionMatrixDisplay.from_predictions(
+    true_labels, predicted_labels, labels=lp_model.classes_
+)
+
+# %%
+# Plot the most uncertain predictions
+# -----------------------------------
+#
+# Here, we will pick and show the 10 most uncertain predictions.
+from scipy import stats
+
 pred_entropies = stats.distributions.entropy(lp_model.label_distributions_.T)

-# #############################################################################
+# %%
 # Pick the top 10 most uncertain labels
 uncertainty_index = np.argsort(pred_entropies)[-10:]

-# #############################################################################
+# %%
 # Plot
+import matplotlib.pyplot as plt
+
 f = plt.figure(figsize=(7, 5))
 for index, image_index in enumerate(uncertainty_index):
     image = images[image_index]
@@ -85,8 +109,9 @@
     sub.imshow(image, cmap=plt.cm.gray_r)
     plt.xticks([])
     plt.yticks([])
-    sub.set_title('predict: %i\ntrue: %i' % (
-        lp_model.transduction_[image_index], y[image_index]))
+    sub.set_title(
+        "predict: %i\ntrue: %i" % (lp_model.transduction_[image_index], y[image_index])
+    )

-f.suptitle('Learning with small amount of labeled data')
+f.suptitle("Learning with small amount of labeled data")
 plt.show()
('examples/calibration', 'plot_calibration.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -20,25 +20,22 @@
 to the expected 0.5 for most of the samples belonging to the middle
 cluster with heterogeneous labels. This results in a significantly improved
 Brier score.
+
 """
-print(__doc__)
-
-# Author: Mathieu Blondel <mathieu@mblondel.org>
-#         Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>
-#         Balazs Kegl <balazs.kegl@gmail.com>
-#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
+# Authors:
+# Mathieu Blondel <mathieu@mblondel.org>
+# Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>
+# Balazs Kegl <balazs.kegl@gmail.com>
+# Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 # License: BSD Style.

+# %%
+# Generate synthetic dataset
+# --------------------------
 import numpy as np
-import matplotlib.pyplot as plt
-from matplotlib import cm

 from sklearn.datasets import make_blobs
-from sklearn.naive_bayes import GaussianNB
-from sklearn.metrics import brier_score_loss
-from sklearn.calibration import CalibratedClassifierCV
 from sklearn.model_selection import train_test_split
-

 n_samples = 50000
 n_bins = 3  # use 3 bins for calibration_curve as we have 3 clusters here
@@ -47,71 +44,99 @@
 # half positive samples and half negative samples. Probability in this
 # blob is therefore 0.5.
 centers = [(-5, -5), (0, 0), (5, 5)]
-X, y = make_blobs(n_samples=n_samples, n_features=2, cluster_std=1.0,
-                  centers=centers, shuffle=False, random_state=42)
+X, y = make_blobs(n_samples=n_samples, centers=centers, shuffle=False, random_state=42)

-y[:n_samples // 2] = 0
-y[n_samples // 2:] = 1
+y[: n_samples // 2] = 0
+y[n_samples // 2 :] = 1
 sample_weight = np.random.RandomState(42).rand(y.shape[0])

 # split train, test for calibration
-X_train, X_test, y_train, y_test, sw_train, sw_test = \
-    train_test_split(X, y, sample_weight, test_size=0.9, random_state=42)
+X_train, X_test, y_train, y_test, sw_train, sw_test = train_test_split(
+    X, y, sample_weight, test_size=0.9, random_state=42
+)

-# Gaussian Naive-Bayes with no calibration
+# %%
+# Gaussian Naive-Bayes
+# --------------------
+from sklearn.calibration import CalibratedClassifierCV
+from sklearn.metrics import brier_score_loss
+from sklearn.naive_bayes import GaussianNB
+
+# With no calibration
 clf = GaussianNB()
 clf.fit(X_train, y_train)  # GaussianNB itself does not support sample-weights
 prob_pos_clf = clf.predict_proba(X_test)[:, 1]

-# Gaussian Naive-Bayes with isotonic calibration
-clf_isotonic = CalibratedClassifierCV(clf, cv=2, method='isotonic')
-clf_isotonic.fit(X_train, y_train, sw_train)
+# With isotonic calibration
+clf_isotonic = CalibratedClassifierCV(clf, cv=2, method="isotonic")
+clf_isotonic.fit(X_train, y_train, sample_weight=sw_train)
 prob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1]

-# Gaussian Naive-Bayes with sigmoid calibration
-clf_sigmoid = CalibratedClassifierCV(clf, cv=2, method='sigmoid')
-clf_sigmoid.fit(X_train, y_train, sw_train)
+# With sigmoid calibration
+clf_sigmoid = CalibratedClassifierCV(clf, cv=2, method="sigmoid")
+clf_sigmoid.fit(X_train, y_train, sample_weight=sw_train)
 prob_pos_sigmoid = clf_sigmoid.predict_proba(X_test)[:, 1]

-print("Brier scores: (the smaller the better)")
+print("Brier score losses: (the smaller the better)")

-clf_score = brier_score_loss(y_test, prob_pos_clf, sw_test)
+clf_score = brier_score_loss(y_test, prob_pos_clf, sample_weight=sw_test)
 print("No calibration: %1.3f" % clf_score)

-clf_isotonic_score = brier_score_loss(y_test, prob_pos_isotonic, sw_test)
+clf_isotonic_score = brier_score_loss(y_test, prob_pos_isotonic, sample_weight=sw_test)
 print("With isotonic calibration: %1.3f" % clf_isotonic_score)

-clf_sigmoid_score = brier_score_loss(y_test, prob_pos_sigmoid, sw_test)
+clf_sigmoid_score = brier_score_loss(y_test, prob_pos_sigmoid, sample_weight=sw_test)
 print("With sigmoid calibration: %1.3f" % clf_sigmoid_score)

-# #############################################################################
-# Plot the data and the predicted probabilities
+# %%
+# Plot data and the predicted probabilities
+# -----------------------------------------
+from matplotlib import cm
+import matplotlib.pyplot as plt
+
 plt.figure()
 y_unique = np.unique(y)
 colors = cm.rainbow(np.linspace(0.0, 1.0, y_unique.size))
 for this_y, color in zip(y_unique, colors):
     this_X = X_train[y_train == this_y]
     this_sw = sw_train[y_train == this_y]
-    plt.scatter(this_X[:, 0], this_X[:, 1], s=this_sw * 50,
-                c=color[np.newaxis, :],
-                alpha=0.5, edgecolor='k',
-                label="Class %s" % this_y)
+    plt.scatter(
+        this_X[:, 0],
+        this_X[:, 1],
+        s=this_sw * 50,
+        c=color[np.newaxis, :],
+        alpha=0.5,
+        edgecolor="k",
+        label="Class %s" % this_y,
+    )
 plt.legend(loc="best")
 plt.title("Data")

 plt.figure()
-order = np.lexsort((prob_pos_clf, ))
-plt.plot(prob_pos_clf[order], 'r', label='No calibration (%1.3f)' % clf_score)
-plt.plot(prob_pos_isotonic[order], 'g', linewidth=3,
-         label='Isotonic calibration (%1.3f)' % clf_isotonic_score)
-plt.plot(prob_pos_sigmoid[order], 'b', linewidth=3,
-         label='Sigmoid calibration (%1.3f)' % clf_sigmoid_score)
-plt.plot(np.linspace(0, y_test.size, 51)[1::2],
-         y_test[order].reshape(25, -1).mean(1),
-         'k', linewidth=3, label=r'Empirical')
+
+order = np.lexsort((prob_pos_clf,))
+plt.plot(prob_pos_clf[order], "r", label="No calibration (%1.3f)" % clf_score)
+plt.plot(
+    prob_pos_isotonic[order],
+    "g",
+    linewidth=3,
+    label="Isotonic calibration (%1.3f)" % clf_isotonic_score,
+)
+plt.plot(
+    prob_pos_sigmoid[order],
+    "b",
+    linewidth=3,
+    label="Sigmoid calibration (%1.3f)" % clf_sigmoid_score,
+)
+plt.plot(
+    np.linspace(0, y_test.size, 51)[1::2],
+    y_test[order].reshape(25, -1).mean(1),
+    "k",
+    linewidth=3,
+    label=r"Empirical",
+)
 plt.ylim([-0.05, 1.05])
-plt.xlabel("Instances sorted according to predicted probability "
-           "(uncalibrated GNB)")
+plt.xlabel("Instances sorted according to predicted probability (uncalibrated GNB)")
 plt.ylabel("P(y=1)")
 plt.legend(loc="upper left")
 plt.title("Gaussian naive Bayes probabilities")
('examples/calibration', 'plot_compare_calibration.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,119 +4,205 @@
 ========================================

 Well calibrated classifiers are probabilistic classifiers for which the output
-of the predict_proba method can be directly interpreted as a confidence level.
-For instance a well calibrated (binary) classifier should classify the samples
-such that among the samples to which it gave a predict_proba value close to
-0.8, approx. 80% actually belong to the positive class.
-
-LogisticRegression returns well calibrated predictions as it directly
-optimizes log-loss. In contrast, the other methods return biased probabilities,
-with different biases per method:
-
-* GaussianNaiveBayes tends to push probabilities to 0 or 1 (note the counts in
-  the histograms). This is mainly because it makes the assumption that features
-  are conditionally independent given the class, which is not the case in this
-  dataset which contains 2 redundant features.
-
-* RandomForestClassifier shows the opposite behavior: the histograms show
-  peaks at approx. 0.2 and 0.9 probability, while probabilities close to 0 or 1
-  are very rare. An explanation for this is given by Niculescu-Mizil and Caruana
-  [1]_: "Methods such as bagging and random forests that average predictions
-  from a base set of models can have difficulty making predictions near 0 and 1
-  because variance in the underlying base models will bias predictions that
-  should be near zero or one away from these values. Because predictions are
-  restricted to the interval [0,1], errors caused by variance tend to be one-
-  sided near zero and one. For example, if a model should predict p = 0 for a
-  case, the only way bagging can achieve this is if all bagged trees predict
-  zero. If we add noise to the trees that bagging is averaging over, this noise
-  will cause some trees to predict values larger than 0 for this case, thus
-  moving the average prediction of the bagged ensemble away from 0. We observe
-  this effect most strongly with random forests because the base-level trees
-  trained with random forests have relatively high variance due to feature
-  subsetting." As a result, the calibration curve shows a characteristic
-  sigmoid shape, indicating that the classifier could trust its "intuition"
-  more and return probabilities closer to 0 or 1 typically.
-
-* Support Vector Classification (SVC) shows an even more sigmoid curve as
-  the  RandomForestClassifier, which is typical for maximum-margin methods
-  (compare Niculescu-Mizil and Caruana [1]_), which focus on hard samples
-  that are close to the decision boundary (the support vectors).
-
-.. topic:: References:
-
-    .. [1] Predicting Good Probabilities with Supervised Learning,
-          A. Niculescu-Mizil & R. Caruana, ICML 2005
+of :term:`predict_proba` can be directly interpreted as a confidence level.
+For instance, a well calibrated (binary) classifier should classify the samples
+such that for the samples to which it gave a :term:`predict_proba` value close
+to 0.8, approximately 80% actually belong to the positive class.
+
+In this example we will compare the calibration of four different
+models: :ref:`Logistic_regression`, :ref:`gaussian_naive_bayes`,
+:ref:`Random Forest Classifier <forest>` and :ref:`Linear SVM
+<svm_classification>`.
+
 """
-print(__doc__)
-
+
+# %%
 # Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
-# License: BSD Style.
+# License: BSD 3 clause.
+#
+# Dataset
+# -------
+#
+# We will use a synthetic binary classification dataset with 100,000 samples
+# and 20 features. Of the 20 features, only 2 are informative, 2 are
+# redundant (random combinations of the informative features) and the
+# remaining 16 are uninformative (random numbers). Of the 100,000 samples,
+# 100 will be used for model fitting and the remaining for testing.
+
+from sklearn.datasets import make_classification
+from sklearn.model_selection import train_test_split
+
+X, y = make_classification(
+    n_samples=100_000, n_features=20, n_informative=2, n_redundant=2, random_state=42
+)
+
+train_samples = 100  # Samples used for training the models
+X_train, X_test, y_train, y_test = train_test_split(
+    X,
+    y,
+    shuffle=False,
+    test_size=100_000 - train_samples,
+)
+
+# %%
+# Calibration curves
+# ------------------
+#
+# Below, we train each of the four models with the small training dataset, then
+# plot calibration curves (also known as reliability diagrams) using
+# predicted probabilities of the test dataset. Calibration curves are created
+# by binning predicted probabilities, then plotting the mean predicted
+# probability in each bin against the observed frequency ('fraction of
+# positives'). Below the calibration curve, we plot a histogram showing
+# the distribution of the predicted probabilities or more specifically,
+# the number of samples in each predicted probability bin.

 import numpy as np
-np.random.seed(0)
+
+from sklearn.svm import LinearSVC
+
+
+class NaivelyCalibratedLinearSVC(LinearSVC):
+    """LinearSVC with `predict_proba` method that naively scales
+    `decision_function` output."""
+
+    def fit(self, X, y):
+        super().fit(X, y)
+        df = self.decision_function(X)
+        self.df_min_ = df.min()
+        self.df_max_ = df.max()
+
+    def predict_proba(self, X):
+        """Min-max scale output of `decision_function` to [0,1]."""
+        df = self.decision_function(X)
+        calibrated_df = (df - self.df_min_) / (self.df_max_ - self.df_min_)
+        proba_pos_class = np.clip(calibrated_df, 0, 1)
+        proba_neg_class = 1 - proba_pos_class
+        proba = np.c_[proba_neg_class, proba_pos_class]
+        return proba
+
+
+# %%
+
+from sklearn.calibration import CalibrationDisplay
+from sklearn.ensemble import RandomForestClassifier
+from sklearn.linear_model import LogisticRegression
+from sklearn.naive_bayes import GaussianNB
+
+# Create classifiers
+lr = LogisticRegression()
+gnb = GaussianNB()
+svc = NaivelyCalibratedLinearSVC(C=1.0)
+rfc = RandomForestClassifier()
+
+clf_list = [
+    (lr, "Logistic"),
+    (gnb, "Naive Bayes"),
+    (svc, "SVC"),
+    (rfc, "Random forest"),
+]
+
+# %%

 import matplotlib.pyplot as plt
-
-from sklearn import datasets
-from sklearn.naive_bayes import GaussianNB
-from sklearn.linear_model import LogisticRegression
-from sklearn.ensemble import RandomForestClassifier
-from sklearn.svm import LinearSVC
-from sklearn.calibration import calibration_curve
-
-X, y = datasets.make_classification(n_samples=100000, n_features=20,
-                                    n_informative=2, n_redundant=2)
-
-train_samples = 100  # Samples used for training the models
-
-X_train = X[:train_samples]
-X_test = X[train_samples:]
-y_train = y[:train_samples]
-y_test = y[train_samples:]
-
-# Create classifiers
-lr = LogisticRegression(solver='lbfgs')
-gnb = GaussianNB()
-svc = LinearSVC(C=1.0)
-rfc = RandomForestClassifier(n_estimators=100)
-
-
-# #############################################################################
-# Plot calibration plots
-
-plt.figure(figsize=(10, 10))
-ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)
-ax2 = plt.subplot2grid((3, 1), (2, 0))
-
-ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
-for clf, name in [(lr, 'Logistic'),
-                  (gnb, 'Naive Bayes'),
-                  (svc, 'Support Vector Classification'),
-                  (rfc, 'Random Forest')]:
+from matplotlib.gridspec import GridSpec
+
+fig = plt.figure(figsize=(10, 10))
+gs = GridSpec(4, 2)
+colors = plt.cm.get_cmap("Dark2")
+
+ax_calibration_curve = fig.add_subplot(gs[:2, :2])
+calibration_displays = {}
+for i, (clf, name) in enumerate(clf_list):
     clf.fit(X_train, y_train)
-    if hasattr(clf, "predict_proba"):
-        prob_pos = clf.predict_proba(X_test)[:, 1]
-    else:  # use decision function
-        prob_pos = clf.decision_function(X_test)
-        prob_pos = \
-            (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())
-    fraction_of_positives, mean_predicted_value = \
-        calibration_curve(y_test, prob_pos, n_bins=10)
-
-    ax1.plot(mean_predicted_value, fraction_of_positives, "s-",
-             label="%s" % (name, ))
-
-    ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,
-             histtype="step", lw=2)
-
-ax1.set_ylabel("Fraction of positives")
-ax1.set_ylim([-0.05, 1.05])
-ax1.legend(loc="lower right")
-ax1.set_title('Calibration plots  (reliability curve)')
-
-ax2.set_xlabel("Mean predicted value")
-ax2.set_ylabel("Count")
-ax2.legend(loc="upper center", ncol=2)
+    display = CalibrationDisplay.from_estimator(
+        clf,
+        X_test,
+        y_test,
+        n_bins=10,
+        name=name,
+        ax=ax_calibration_curve,
+        color=colors(i),
+    )
+    calibration_displays[name] = display
+
+ax_calibration_curve.grid()
+ax_calibration_curve.set_title("Calibration plots")
+
+# Add histogram
+grid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]
+for i, (_, name) in enumerate(clf_list):
+    row, col = grid_positions[i]
+    ax = fig.add_subplot(gs[row, col])
+
+    ax.hist(
+        calibration_displays[name].y_prob,
+        range=(0, 1),
+        bins=10,
+        label=name,
+        color=colors(i),
+    )
+    ax.set(title=name, xlabel="Mean predicted probability", ylabel="Count")

 plt.tight_layout()
 plt.show()
+
+# %%
+# :class:`~sklearn.linear_model.LogisticRegression` returns well calibrated
+# predictions as it directly optimizes log-loss. In contrast, the other methods
+# return biased probabilities, with different biases for each method:
+#
+# * :class:`~sklearn.naive_bayes.GaussianNB` tends to push
+#   probabilities to 0 or 1 (see histogram). This is mainly
+#   because the naive Bayes equation only provides correct estimate of
+#   probabilities when the assumption that features are conditionally
+#   independent holds [2]_. However, features tend to be positively correlated
+#   and is the case with this dataset, which contains 2 features
+#   generated as random linear combinations of the informative features. These
+#   correlated features are effectively being 'counted twice', resulting in
+#   pushing the predicted probabilities towards 0 and 1 [3]_.
+#
+# * :class:`~sklearn.ensemble.RandomForestClassifier` shows the opposite
+#   behavior: the histograms show peaks at approx. 0.2 and 0.9 probability,
+#   while probabilities close to 0 or 1 are very rare. An explanation for this
+#   is given by Niculescu-Mizil and Caruana [1]_: "Methods such as bagging and
+#   random forests that average predictions from a base set of models can have
+#   difficulty making predictions near 0 and 1 because variance in the
+#   underlying base models will bias predictions that should be near zero or
+#   one away from these values. Because predictions are restricted to the
+#   interval [0,1], errors caused by variance tend to be one- sided near zero
+#   and one. For example, if a model should predict p = 0 for a case, the only
+#   way bagging can achieve this is if all bagged trees predict zero. If we add
+#   noise to the trees that bagging is averaging over, this noise will cause
+#   some trees to predict values larger than 0 for this case, thus moving the
+#   average prediction of the bagged ensemble away from 0. We observe this
+#   effect most strongly with random forests because the base-level trees
+#   trained with random forests have relatively high variance due to feature
+#   subsetting." As a result, the calibration curve shows a characteristic
+#   sigmoid shape, indicating that the classifier is under-confident
+#   and could return probabilities closer to 0 or 1.
+#
+# * To show the performance of :class:`~sklearn.svm.LinearSVC`, we naively
+#   scale the output of the :term:`decision_function` into [0, 1] by applying
+#   min-max scaling, since SVC does not output probabilities by default.
+#   :class:`~sklearn.svm.LinearSVC` shows an
+#   even more sigmoid curve than the
+#   :class:`~sklearn.ensemble.RandomForestClassifier`, which is typical for
+#   maximum-margin methods [1]_ as they focus on difficult to classify samples
+#   that are close to the decision boundary (the support vectors).
+#
+# References
+# ----------
+#
+# .. [1] `Predicting Good Probabilities with Supervised Learning
+#        <https://dl.acm.org/doi/pdf/10.1145/1102351.1102430>`_,
+#        A. Niculescu-Mizil & R. Caruana, ICML 2005
+# .. [2] `Beyond independence: Conditions for the optimality of the simple
+#        bayesian classifier
+#        <https://www.ics.uci.edu/~pazzani/Publications/mlc96-pedro.pdf>`_
+#        Domingos, P., & Pazzani, M., Proc. 13th Intl. Conf. Machine Learning.
+#        1996.
+# .. [3] `Obtaining calibrated probability estimates from decision trees and
+#        naive Bayesian classifiers
+#        <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.29.3039&rep=rep1&type=pdf>`_
+#        Zadrozny, Bianca, and Charles Elkan. Icml. Vol. 1. 2001.
('examples/calibration', 'plot_calibration_curve.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,130 +5,332 @@

 When performing classification one often wants to predict not only the class
 label, but also the associated probability. This probability gives some
-kind of confidence on the prediction. This example demonstrates how to display
-how well calibrated the predicted probabilities are and how to calibrate an
-uncalibrated classifier.
-
-The experiment is performed on an artificial dataset for binary classification
-with 100,000 samples (1,000 of them are used for model fitting) with 20
-features. Of the 20 features, only 2 are informative and 10 are redundant. The
-first figure shows the estimated probabilities obtained with logistic
-regression, Gaussian naive Bayes, and Gaussian naive Bayes with both isotonic
-calibration and sigmoid calibration. The calibration performance is evaluated
-with Brier score, reported in the legend (the smaller the better). One can
-observe here that logistic regression is well calibrated while raw Gaussian
-naive Bayes performs very badly. This is because of the redundant features
-which violate the assumption of feature-independence and result in an overly
-confident classifier, which is indicated by the typical transposed-sigmoid
-curve.
-
-Calibration of the probabilities of Gaussian naive Bayes with isotonic
-regression can fix this issue as can be seen from the nearly diagonal
-calibration curve. Sigmoid calibration also improves the brier score slightly,
-albeit not as strongly as the non-parametric isotonic regression. This can be
-attributed to the fact that we have plenty of calibration data such that the
-greater flexibility of the non-parametric model can be exploited.
-
-The second figure shows the calibration curve of a linear support-vector
-classifier (LinearSVC). LinearSVC shows the opposite behavior as Gaussian
-naive Bayes: the calibration curve has a sigmoid curve, which is typical for
-an under-confident classifier. In the case of LinearSVC, this is caused by the
-margin property of the hinge loss, which lets the model focus on hard samples
-that are close to the decision boundary (the support vectors).
-
-Both kinds of calibration can fix this issue and yield nearly identical
-results. This shows that sigmoid calibration can deal with situations where
-the calibration curve of the base classifier is sigmoid (e.g., for LinearSVC)
-but not where it is transposed-sigmoid (e.g., Gaussian naive Bayes).
+kind of confidence on the prediction. This example demonstrates how to
+visualize how well calibrated the predicted probabilities are using calibration
+curves, also known as reliability diagrams. Calibration of an uncalibrated
+classifier will also be demonstrated.
+
 """
-print(__doc__)

 # Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>
 #         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
-# License: BSD Style.
+# License: BSD 3 clause.
+# %%
+# Dataset
+# -------
+#
+# We will use a synthetic binary classification dataset with 100,000 samples
+# and 20 features. Of the 20 features, only 2 are informative, 10 are
+# redundant (random combinations of the informative features) and the
+# remaining 8 are uninformative (random numbers). Of the 100,000 samples, 1,000
+# will be used for model fitting and the rest for testing.
+
+from sklearn.datasets import make_classification
+from sklearn.model_selection import train_test_split
+
+X, y = make_classification(
+    n_samples=100_000, n_features=20, n_informative=2, n_redundant=10, random_state=42
+)
+
+X_train, X_test, y_train, y_test = train_test_split(
+    X, y, test_size=0.99, random_state=42
+)
+
+# %%
+# Calibration curves
+# ------------------
+#
+# Gaussian Naive Bayes
+# ^^^^^^^^^^^^^^^^^^^^
+#
+# First, we will compare:
+#
+# * :class:`~sklearn.linear_model.LogisticRegression` (used as baseline
+#   since very often, properly regularized logistic regression is well
+#   calibrated by default thanks to the use of the log-loss)
+# * Uncalibrated :class:`~sklearn.naive_bayes.GaussianNB`
+# * :class:`~sklearn.naive_bayes.GaussianNB` with isotonic and sigmoid
+#   calibration (see :ref:`User Guide <calibration>`)
+#
+# Calibration curves for all 4 conditions are plotted below, with the average
+# predicted probability for each bin on the x-axis and the fraction of positive
+# classes in each bin on the y-axis.

 import matplotlib.pyplot as plt
-
-from sklearn import datasets
+from matplotlib.gridspec import GridSpec
+
+from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay
+from sklearn.linear_model import LogisticRegression
 from sklearn.naive_bayes import GaussianNB
+
+lr = LogisticRegression(C=1.0)
+gnb = GaussianNB()
+gnb_isotonic = CalibratedClassifierCV(gnb, cv=2, method="isotonic")
+gnb_sigmoid = CalibratedClassifierCV(gnb, cv=2, method="sigmoid")
+
+clf_list = [
+    (lr, "Logistic"),
+    (gnb, "Naive Bayes"),
+    (gnb_isotonic, "Naive Bayes + Isotonic"),
+    (gnb_sigmoid, "Naive Bayes + Sigmoid"),
+]
+
+# %%
+fig = plt.figure(figsize=(10, 10))
+gs = GridSpec(4, 2)
+colors = plt.cm.get_cmap("Dark2")
+
+ax_calibration_curve = fig.add_subplot(gs[:2, :2])
+calibration_displays = {}
+for i, (clf, name) in enumerate(clf_list):
+    clf.fit(X_train, y_train)
+    display = CalibrationDisplay.from_estimator(
+        clf,
+        X_test,
+        y_test,
+        n_bins=10,
+        name=name,
+        ax=ax_calibration_curve,
+        color=colors(i),
+    )
+    calibration_displays[name] = display
+
+ax_calibration_curve.grid()
+ax_calibration_curve.set_title("Calibration plots (Naive Bayes)")
+
+# Add histogram
+grid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]
+for i, (_, name) in enumerate(clf_list):
+    row, col = grid_positions[i]
+    ax = fig.add_subplot(gs[row, col])
+
+    ax.hist(
+        calibration_displays[name].y_prob,
+        range=(0, 1),
+        bins=10,
+        label=name,
+        color=colors(i),
+    )
+    ax.set(title=name, xlabel="Mean predicted probability", ylabel="Count")
+
+plt.tight_layout()
+plt.show()
+
+# %%
+# Uncalibrated :class:`~sklearn.naive_bayes.GaussianNB` is poorly calibrated
+# because of
+# the redundant features which violate the assumption of feature-independence
+# and result in an overly confident classifier, which is indicated by the
+# typical transposed-sigmoid curve. Calibration of the probabilities of
+# :class:`~sklearn.naive_bayes.GaussianNB` with :ref:`isotonic` can fix
+# this issue as can be seen from the nearly diagonal calibration curve.
+# :ref:sigmoid regression `<sigmoid_regressor>` also improves calibration
+# slightly,
+# albeit not as strongly as the non-parametric isotonic regression. This can be
+# attributed to the fact that we have plenty of calibration data such that the
+# greater flexibility of the non-parametric model can be exploited.
+#
+# Below we will make a quantitative analysis considering several classification
+# metrics: :ref:`brier_score_loss`, :ref:`log_loss`,
+# :ref:`precision, recall, F1 score <precision_recall_f_measure_metrics>` and
+# :ref:`ROC AUC <roc_metrics>`.
+
+from collections import defaultdict
+
+import pandas as pd
+
+from sklearn.metrics import (
+    precision_score,
+    recall_score,
+    f1_score,
+    brier_score_loss,
+    log_loss,
+    roc_auc_score,
+)
+
+scores = defaultdict(list)
+for i, (clf, name) in enumerate(clf_list):
+    clf.fit(X_train, y_train)
+    y_prob = clf.predict_proba(X_test)
+    y_pred = clf.predict(X_test)
+    scores["Classifier"].append(name)
+
+    for metric in [brier_score_loss, log_loss]:
+        score_name = metric.__name__.replace("_", " ").replace("score", "").capitalize()
+        scores[score_name].append(metric(y_test, y_prob[:, 1]))
+
+    for metric in [precision_score, recall_score, f1_score, roc_auc_score]:
+        score_name = metric.__name__.replace("_", " ").replace("score", "").capitalize()
+        scores[score_name].append(metric(y_test, y_pred))
+
+    score_df = pd.DataFrame(scores).set_index("Classifier")
+    score_df.round(decimals=3)
+
+score_df
+
+# %%
+# Notice that although calibration improves the :ref:`brier_score_loss` (a
+# metric composed
+# of calibration term and refinement term) and :ref:`log_loss`, it does not
+# significantly alter the prediction accuracy measures (precision, recall and
+# F1 score).
+# This is because calibration should not significantly change prediction
+# probabilities at the location of the decision threshold (at x = 0.5 on the
+# graph). Calibration should however, make the predicted probabilities more
+# accurate and thus more useful for making allocation decisions under
+# uncertainty.
+# Further, ROC AUC, should not change at all because calibration is a
+# monotonic transformation. Indeed, no rank metrics are affected by
+# calibration.
+#
+# Linear support vector classifier
+# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+# Next, we will compare:
+#
+# * :class:`~sklearn.linear_model.LogisticRegression` (baseline)
+# * Uncalibrated :class:`~sklearn.svm.LinearSVC`. Since SVC does not output
+#   probabilities by default, we naively scale the output of the
+#   :term:`decision_function` into [0, 1] by applying min-max scaling.
+# * :class:`~sklearn.svm.LinearSVC` with isotonic and sigmoid
+#   calibration (see :ref:`User Guide <calibration>`)
+
+import numpy as np
+
 from sklearn.svm import LinearSVC
-from sklearn.linear_model import LogisticRegression
-from sklearn.metrics import (brier_score_loss, precision_score, recall_score,
-                             f1_score)
-from sklearn.calibration import CalibratedClassifierCV, calibration_curve
-from sklearn.model_selection import train_test_split
-
-
-# Create dataset of classification task with many redundant and few
-# informative features
-X, y = datasets.make_classification(n_samples=100000, n_features=20,
-                                    n_informative=2, n_redundant=10,
-                                    random_state=42)
-
-X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99,
-                                                    random_state=42)
-
-
-def plot_calibration_curve(est, name, fig_index):
-    """Plot calibration curve for est w/o and with calibration. """
-    # Calibrated with isotonic calibration
-    isotonic = CalibratedClassifierCV(est, cv=2, method='isotonic')
-
-    # Calibrated with sigmoid calibration
-    sigmoid = CalibratedClassifierCV(est, cv=2, method='sigmoid')
-
-    # Logistic regression with no calibration as baseline
-    lr = LogisticRegression(C=1., solver='lbfgs')
-
-    fig = plt.figure(fig_index, figsize=(10, 10))
-    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)
-    ax2 = plt.subplot2grid((3, 1), (2, 0))
-
-    ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
-    for clf, name in [(lr, 'Logistic'),
-                      (est, name),
-                      (isotonic, name + ' + Isotonic'),
-                      (sigmoid, name + ' + Sigmoid')]:
-        clf.fit(X_train, y_train)
-        y_pred = clf.predict(X_test)
-        if hasattr(clf, "predict_proba"):
-            prob_pos = clf.predict_proba(X_test)[:, 1]
-        else:  # use decision function
-            prob_pos = clf.decision_function(X_test)
-            prob_pos = \
-                (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())
-
-        clf_score = brier_score_loss(y_test, prob_pos, pos_label=y.max())
-        print("%s:" % name)
-        print("\tBrier: %1.3f" % (clf_score))
-        print("\tPrecision: %1.3f" % precision_score(y_test, y_pred))
-        print("\tRecall: %1.3f" % recall_score(y_test, y_pred))
-        print("\tF1: %1.3f\n" % f1_score(y_test, y_pred))
-
-        fraction_of_positives, mean_predicted_value = \
-            calibration_curve(y_test, prob_pos, n_bins=10)
-
-        ax1.plot(mean_predicted_value, fraction_of_positives, "s-",
-                 label="%s (%1.3f)" % (name, clf_score))
-
-        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,
-                 histtype="step", lw=2)
-
-    ax1.set_ylabel("Fraction of positives")
-    ax1.set_ylim([-0.05, 1.05])
-    ax1.legend(loc="lower right")
-    ax1.set_title('Calibration plots  (reliability curve)')
-
-    ax2.set_xlabel("Mean predicted value")
-    ax2.set_ylabel("Count")
-    ax2.legend(loc="upper center", ncol=2)
-
-    plt.tight_layout()
-
-# Plot calibration curve for Gaussian Naive Bayes
-plot_calibration_curve(GaussianNB(), "Naive Bayes", 1)
-
-# Plot calibration curve for Linear SVC
-plot_calibration_curve(LinearSVC(max_iter=10000), "SVC", 2)
-
+
+
+class NaivelyCalibratedLinearSVC(LinearSVC):
+    """LinearSVC with `predict_proba` method that naively scales
+    `decision_function` output for binary classification."""
+
+    def fit(self, X, y):
+        super().fit(X, y)
+        df = self.decision_function(X)
+        self.df_min_ = df.min()
+        self.df_max_ = df.max()
+
+    def predict_proba(self, X):
+        """Min-max scale output of `decision_function` to [0, 1]."""
+        df = self.decision_function(X)
+        calibrated_df = (df - self.df_min_) / (self.df_max_ - self.df_min_)
+        proba_pos_class = np.clip(calibrated_df, 0, 1)
+        proba_neg_class = 1 - proba_pos_class
+        proba = np.c_[proba_neg_class, proba_pos_class]
+        return proba
+
+
+# %%
+
+lr = LogisticRegression(C=1.0)
+svc = NaivelyCalibratedLinearSVC(max_iter=10_000)
+svc_isotonic = CalibratedClassifierCV(svc, cv=2, method="isotonic")
+svc_sigmoid = CalibratedClassifierCV(svc, cv=2, method="sigmoid")
+
+clf_list = [
+    (lr, "Logistic"),
+    (svc, "SVC"),
+    (svc_isotonic, "SVC + Isotonic"),
+    (svc_sigmoid, "SVC + Sigmoid"),
+]
+
+# %%
+fig = plt.figure(figsize=(10, 10))
+gs = GridSpec(4, 2)
+
+ax_calibration_curve = fig.add_subplot(gs[:2, :2])
+calibration_displays = {}
+for i, (clf, name) in enumerate(clf_list):
+    clf.fit(X_train, y_train)
+    display = CalibrationDisplay.from_estimator(
+        clf,
+        X_test,
+        y_test,
+        n_bins=10,
+        name=name,
+        ax=ax_calibration_curve,
+        color=colors(i),
+    )
+    calibration_displays[name] = display
+
+ax_calibration_curve.grid()
+ax_calibration_curve.set_title("Calibration plots (SVC)")
+
+# Add histogram
+grid_positions = [(2, 0), (2, 1), (3, 0), (3, 1)]
+for i, (_, name) in enumerate(clf_list):
+    row, col = grid_positions[i]
+    ax = fig.add_subplot(gs[row, col])
+
+    ax.hist(
+        calibration_displays[name].y_prob,
+        range=(0, 1),
+        bins=10,
+        label=name,
+        color=colors(i),
+    )
+    ax.set(title=name, xlabel="Mean predicted probability", ylabel="Count")
+
+plt.tight_layout()
 plt.show()
+
+# %%
+# :class:`~sklearn.svm.LinearSVC` shows the opposite
+# behavior to :class:`~sklearn.naive_bayes.GaussianNB`; the calibration
+# curve has a sigmoid shape, which is typical for an under-confident
+# classifier. In the case of :class:`~sklearn.svm.LinearSVC`, this is caused
+# by the margin property of the hinge loss, which focuses on samples that are
+# close to the decision boundary (support vectors). Samples that are far
+# away from the decision boundary do not impact the hinge loss. It thus makes
+# sense that :class:`~sklearn.svm.LinearSVC` does not try to separate samples
+# in the high confidence region regions. This leads to flatter calibration
+# curves near 0 and 1 and is empirically shown with a variety of datasets
+# in Niculescu-Mizil & Caruana [1]_.
+#
+# Both kinds of calibration (sigmoid and isotonic) can fix this issue and
+# yield similar results.
+#
+# As before, we show the :ref:`brier_score_loss`, :ref:`log_loss`,
+# :ref:`precision, recall, F1 score <precision_recall_f_measure_metrics>` and
+# :ref:`ROC AUC <roc_metrics>`.
+
+scores = defaultdict(list)
+for i, (clf, name) in enumerate(clf_list):
+    clf.fit(X_train, y_train)
+    y_prob = clf.predict_proba(X_test)
+    y_pred = clf.predict(X_test)
+    scores["Classifier"].append(name)
+
+    for metric in [brier_score_loss, log_loss]:
+        score_name = metric.__name__.replace("_", " ").replace("score", "").capitalize()
+        scores[score_name].append(metric(y_test, y_prob[:, 1]))
+
+    for metric in [precision_score, recall_score, f1_score, roc_auc_score]:
+        score_name = metric.__name__.replace("_", " ").replace("score", "").capitalize()
+        scores[score_name].append(metric(y_test, y_pred))
+
+    score_df = pd.DataFrame(scores).set_index("Classifier")
+    score_df.round(decimals=3)
+
+score_df
+
+# %%
+# As with :class:`~sklearn.naive_bayes.GaussianNB` above, calibration improves
+# both :ref:`brier_score_loss` and :ref:`log_loss` but does not alter the
+# prediction accuracy measures (precision, recall and F1 score) much.
+#
+# Summary
+# -------
+#
+# Parametric sigmoid calibration can deal with situations where the calibration
+# curve of the base classifier is sigmoid (e.g., for
+# :class:`~sklearn.svm.LinearSVC`) but not where it is transposed-sigmoid
+# (e.g., :class:`~sklearn.naive_bayes.GaussianNB`). Non-parametric
+# isotonic calibration can deal with both situations but may require more
+# data to produce good results.
+#
+# References
+# ----------
+#
+# .. [1] `Predicting Good Probabilities with Supervised Learning
+#        <https://dl.acm.org/doi/pdf/10.1145/1102351.1102430>`_,
+#        A. Niculescu-Mizil & R. Caruana, ICML 2005
('examples/calibration', 'plot_calibration_multiclass.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,163 +3,269 @@
 Probability Calibration for 3-class classification
 ==================================================

-This example illustrates how sigmoid calibration changes predicted
-probabilities for a 3-class classification problem. Illustrated is the
-standard 2-simplex, where the three corners correspond to the three classes.
-Arrows point from the probability vectors predicted by an uncalibrated
+This example illustrates how sigmoid :ref:`calibration <calibration>` changes
+predicted probabilities for a 3-class classification problem. Illustrated is
+the standard 2-simplex, where the three corners correspond to the three
+classes. Arrows point from the probability vectors predicted by an uncalibrated
 classifier to the probability vectors predicted by the same classifier after
 sigmoid calibration on a hold-out validation set. Colors indicate the true
 class of an instance (red: class 1, green: class 2, blue: class 3).

-The base classifier is a random forest classifier with 25 base estimators
-(trees). If this classifier is trained on all 800 training datapoints, it is
-overly confident in its predictions and thus incurs a large log-loss.
-Calibrating an identical classifier, which was trained on 600 datapoints, with
-method='sigmoid' on the remaining 200 datapoints reduces the confidence of the
-predictions, i.e., moves the probability vectors from the edges of the simplex
-towards the center. This calibration results in a lower log-loss. Note that an
-alternative would have been to increase the number of base estimators which
-would have resulted in a similar decrease in log-loss.
 """
-print(__doc__)
+
+# %%
+# Data
+# ----
+# Below, we generate a classification dataset with 2000 samples, 2 features
+# and 3 target classes. We then split the data as follows:
+#
+# * train: 600 samples (for training the classifier)
+# * valid: 400 samples (for calibrating predicted probabilities)
+# * test: 1000 samples
+#
+# Note that we also create `X_train_valid` and `y_train_valid`, which consists
+# of both the train and valid subsets. This is used when we only want to train
+# the classifier but not calibrate the predicted probabilities.

 # Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 # License: BSD Style.

-
-import matplotlib.pyplot as plt
-
 import numpy as np
-
 from sklearn.datasets import make_blobs
+
+np.random.seed(0)
+
+X, y = make_blobs(
+    n_samples=2000, n_features=2, centers=3, random_state=42, cluster_std=5.0
+)
+X_train, y_train = X[:600], y[:600]
+X_valid, y_valid = X[600:1000], y[600:1000]
+X_train_valid, y_train_valid = X[:1000], y[:1000]
+X_test, y_test = X[1000:], y[1000:]
+
+# %%
+# Fitting and calibration
+# -----------------------
+#
+# First, we will train a :class:`~sklearn.ensemble.RandomForestClassifier`
+# with 25 base estimators (trees) on the concatenated train and validation
+# data (1000 samples). This is the uncalibrated classifier.
+
 from sklearn.ensemble import RandomForestClassifier
-from sklearn.calibration import CalibratedClassifierCV
-from sklearn.metrics import log_loss
-
-np.random.seed(0)
-
-# Generate data
-X, y = make_blobs(n_samples=1000, n_features=2, random_state=42,
-                  cluster_std=5.0)
-X_train, y_train = X[:600], y[:600]
-X_valid, y_valid = X[600:800], y[600:800]
-X_train_valid, y_train_valid = X[:800], y[:800]
-X_test, y_test = X[800:], y[800:]
-
-# Train uncalibrated random forest classifier on whole train and validation
-# data and evaluate on test data
+
 clf = RandomForestClassifier(n_estimators=25)
 clf.fit(X_train_valid, y_train_valid)
-clf_probs = clf.predict_proba(X_test)
-score = log_loss(y_test, clf_probs)
-
-# Train random forest classifier, calibrate on validation data and evaluate
-# on test data
+
+# %%
+# To train the calibrated classifier, we start with the same
+# :class:`~sklearn.ensemble.RandomForestClassifier` but train it using only
+# the train data subset (600 samples) then calibrate, with `method='sigmoid'`,
+# using the valid data subset (400 samples) in a 2-stage process.
+
+from sklearn.calibration import CalibratedClassifierCV
+
 clf = RandomForestClassifier(n_estimators=25)
 clf.fit(X_train, y_train)
+cal_clf = CalibratedClassifierCV(clf, method="sigmoid", cv="prefit")
+cal_clf.fit(X_valid, y_valid)
+
+# %%
+# Compare probabilities
+# ---------------------
+# Below we plot a 2-simplex with arrows showing the change in predicted
+# probabilities of the test samples.
+
+import matplotlib.pyplot as plt
+
+plt.figure(figsize=(10, 10))
+colors = ["r", "g", "b"]
+
 clf_probs = clf.predict_proba(X_test)
-sig_clf = CalibratedClassifierCV(clf, method="sigmoid", cv="prefit")
-sig_clf.fit(X_valid, y_valid)
-sig_clf_probs = sig_clf.predict_proba(X_test)
-sig_score = log_loss(y_test, sig_clf_probs)
-
-# Plot changes in predicted probabilities via arrows
-plt.figure()
-colors = ["r", "g", "b"]
+cal_clf_probs = cal_clf.predict_proba(X_test)
+# Plot arrows
 for i in range(clf_probs.shape[0]):
-    plt.arrow(clf_probs[i, 0], clf_probs[i, 1],
-              sig_clf_probs[i, 0] - clf_probs[i, 0],
-              sig_clf_probs[i, 1] - clf_probs[i, 1],
-              color=colors[y_test[i]], head_width=1e-2)
-
-# Plot perfect predictions
-plt.plot([1.0], [0.0], 'ro', ms=20, label="Class 1")
-plt.plot([0.0], [1.0], 'go', ms=20, label="Class 2")
-plt.plot([0.0], [0.0], 'bo', ms=20, label="Class 3")
+    plt.arrow(
+        clf_probs[i, 0],
+        clf_probs[i, 1],
+        cal_clf_probs[i, 0] - clf_probs[i, 0],
+        cal_clf_probs[i, 1] - clf_probs[i, 1],
+        color=colors[y_test[i]],
+        head_width=1e-2,
+    )
+
+# Plot perfect predictions, at each vertex
+plt.plot([1.0], [0.0], "ro", ms=20, label="Class 1")
+plt.plot([0.0], [1.0], "go", ms=20, label="Class 2")
+plt.plot([0.0], [0.0], "bo", ms=20, label="Class 3")

 # Plot boundaries of unit simplex
-plt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], 'k', label="Simplex")
-
-# Annotate points on the simplex
-plt.annotate(r'($\frac{1}{3}$, $\frac{1}{3}$, $\frac{1}{3}$)',
-             xy=(1.0/3, 1.0/3), xytext=(1.0/3, .23), xycoords='data',
-             arrowprops=dict(facecolor='black', shrink=0.05),
-             horizontalalignment='center', verticalalignment='center')
-plt.plot([1.0/3], [1.0/3], 'ko', ms=5)
-plt.annotate(r'($\frac{1}{2}$, $0$, $\frac{1}{2}$)',
-             xy=(.5, .0), xytext=(.5, .1), xycoords='data',
-             arrowprops=dict(facecolor='black', shrink=0.05),
-             horizontalalignment='center', verticalalignment='center')
-plt.annotate(r'($0$, $\frac{1}{2}$, $\frac{1}{2}$)',
-             xy=(.0, .5), xytext=(.1, .5), xycoords='data',
-             arrowprops=dict(facecolor='black', shrink=0.05),
-             horizontalalignment='center', verticalalignment='center')
-plt.annotate(r'($\frac{1}{2}$, $\frac{1}{2}$, $0$)',
-             xy=(.5, .5), xytext=(.6, .6), xycoords='data',
-             arrowprops=dict(facecolor='black', shrink=0.05),
-             horizontalalignment='center', verticalalignment='center')
-plt.annotate(r'($0$, $0$, $1$)',
-             xy=(0, 0), xytext=(.1, .1), xycoords='data',
-             arrowprops=dict(facecolor='black', shrink=0.05),
-             horizontalalignment='center', verticalalignment='center')
-plt.annotate(r'($1$, $0$, $0$)',
-             xy=(1, 0), xytext=(1, .1), xycoords='data',
-             arrowprops=dict(facecolor='black', shrink=0.05),
-             horizontalalignment='center', verticalalignment='center')
-plt.annotate(r'($0$, $1$, $0$)',
-             xy=(0, 1), xytext=(.1, 1), xycoords='data',
-             arrowprops=dict(facecolor='black', shrink=0.05),
-             horizontalalignment='center', verticalalignment='center')
+plt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], "k", label="Simplex")
+
+# Annotate points 6 points around the simplex, and mid point inside simplex
+plt.annotate(
+    r"($\frac{1}{3}$, $\frac{1}{3}$, $\frac{1}{3}$)",
+    xy=(1.0 / 3, 1.0 / 3),
+    xytext=(1.0 / 3, 0.23),
+    xycoords="data",
+    arrowprops=dict(facecolor="black", shrink=0.05),
+    horizontalalignment="center",
+    verticalalignment="center",
+)
+plt.plot([1.0 / 3], [1.0 / 3], "ko", ms=5)
+plt.annotate(
+    r"($\frac{1}{2}$, $0$, $\frac{1}{2}$)",
+    xy=(0.5, 0.0),
+    xytext=(0.5, 0.1),
+    xycoords="data",
+    arrowprops=dict(facecolor="black", shrink=0.05),
+    horizontalalignment="center",
+    verticalalignment="center",
+)
+plt.annotate(
+    r"($0$, $\frac{1}{2}$, $\frac{1}{2}$)",
+    xy=(0.0, 0.5),
+    xytext=(0.1, 0.5),
+    xycoords="data",
+    arrowprops=dict(facecolor="black", shrink=0.05),
+    horizontalalignment="center",
+    verticalalignment="center",
+)
+plt.annotate(
+    r"($\frac{1}{2}$, $\frac{1}{2}$, $0$)",
+    xy=(0.5, 0.5),
+    xytext=(0.6, 0.6),
+    xycoords="data",
+    arrowprops=dict(facecolor="black", shrink=0.05),
+    horizontalalignment="center",
+    verticalalignment="center",
+)
+plt.annotate(
+    r"($0$, $0$, $1$)",
+    xy=(0, 0),
+    xytext=(0.1, 0.1),
+    xycoords="data",
+    arrowprops=dict(facecolor="black", shrink=0.05),
+    horizontalalignment="center",
+    verticalalignment="center",
+)
+plt.annotate(
+    r"($1$, $0$, $0$)",
+    xy=(1, 0),
+    xytext=(1, 0.1),
+    xycoords="data",
+    arrowprops=dict(facecolor="black", shrink=0.05),
+    horizontalalignment="center",
+    verticalalignment="center",
+)
+plt.annotate(
+    r"($0$, $1$, $0$)",
+    xy=(0, 1),
+    xytext=(0.1, 1),
+    xycoords="data",
+    arrowprops=dict(facecolor="black", shrink=0.05),
+    horizontalalignment="center",
+    verticalalignment="center",
+)
 # Add grid
 plt.grid(False)
 for x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:
-    plt.plot([0, x], [x, 0], 'k', alpha=0.2)
-    plt.plot([0, 0 + (1-x)/2], [x, x + (1-x)/2], 'k', alpha=0.2)
-    plt.plot([x, x + (1-x)/2], [0, 0 + (1-x)/2], 'k', alpha=0.2)
-
-plt.title("Change of predicted probabilities after sigmoid calibration")
+    plt.plot([0, x], [x, 0], "k", alpha=0.2)
+    plt.plot([0, 0 + (1 - x) / 2], [x, x + (1 - x) / 2], "k", alpha=0.2)
+    plt.plot([x, x + (1 - x) / 2], [0, 0 + (1 - x) / 2], "k", alpha=0.2)
+
+plt.title("Change of predicted probabilities on test samples after sigmoid calibration")
 plt.xlabel("Probability class 1")
 plt.ylabel("Probability class 2")
 plt.xlim(-0.05, 1.05)
 plt.ylim(-0.05, 1.05)
-plt.legend(loc="best")
+_ = plt.legend(loc="best")
+
+# %%
+# In the figure above, each vertex of the simplex represents
+# a perfectly predicted class (e.g., 1, 0, 0). The mid point
+# inside the simplex represents predicting the three classes with equal
+# probability (i.e., 1/3, 1/3, 1/3). Each arrow starts at the
+# uncalibrated probabilities and end with the arrow head at the calibrated
+# probability. The color of the arrow represents the true class of that test
+# sample.
+#
+# The uncalibrated classifier is overly confident in its predictions and
+# incurs a large :ref:`log loss <log_loss>`. The calibrated classifier incurs
+# a lower :ref:`log loss <log_loss>` due to two factors. First, notice in the
+# figure above that the arrows generally point away from the edges of the
+# simplex, where the probability of one class is 0. Second, a large proportion
+# of the arrows point towards the true class, e.g., green arrows (samples where
+# the true class is 'green') generally point towards the green vertex. This
+# results in fewer over-confident, 0 predicted probabilities and at the same
+# time an increase in the predicted probabilities of the correct class.
+# Thus, the calibrated classifier produces more accurate predicted probabilities
+# that incur a lower :ref:`log loss <log_loss>`
+#
+# We can show this objectively by comparing the :ref:`log loss <log_loss>` of
+# the uncalibrated and calibrated classifiers on the predictions of the 1000
+# test samples. Note that an alternative would have been to increase the number
+# of base estimators (trees) of the
+# :class:`~sklearn.ensemble.RandomForestClassifier` which would have resulted
+# in a similar decrease in :ref:`log loss <log_loss>`.
+
+from sklearn.metrics import log_loss
+
+score = log_loss(y_test, clf_probs)
+cal_score = log_loss(y_test, cal_clf_probs)

 print("Log-loss of")
-print(" * uncalibrated classifier trained on 800 datapoints: %.3f "
-      % score)
-print(" * classifier trained on 600 datapoints and calibrated on "
-      "200 datapoint: %.3f" % sig_score)
-
-# Illustrate calibrator
-plt.figure()
-# generate grid over 2-simplex
+print(f" * uncalibrated classifier: {score:.3f}")
+print(f" * calibrated classifier: {cal_score:.3f}")
+
+# %%
+# Finally we generate a grid of possible uncalibrated probabilities over
+# the 2-simplex, compute the corresponding calibrated probabilities and
+# plot arrows for each. The arrows are colored according the highest
+# uncalibrated probability. This illustrates the learned calibration map:
+
+plt.figure(figsize=(10, 10))
+# Generate grid of probability values
 p1d = np.linspace(0, 1, 20)
 p0, p1 = np.meshgrid(p1d, p1d)
 p2 = 1 - p0 - p1
 p = np.c_[p0.ravel(), p1.ravel(), p2.ravel()]
 p = p[p[:, 2] >= 0]

-calibrated_classifier = sig_clf.calibrated_classifiers_[0]
-prediction = np.vstack([calibrator.predict(this_p)
-                        for calibrator, this_p in
-                        zip(calibrated_classifier.calibrators_, p.T)]).T
+# Use the three class-wise calibrators to compute calibrated probabilities
+calibrated_classifier = cal_clf.calibrated_classifiers_[0]
+prediction = np.vstack(
+    [
+        calibrator.predict(this_p)
+        for calibrator, this_p in zip(calibrated_classifier.calibrators, p.T)
+    ]
+).T
+
+# Re-normalize the calibrated predictions to make sure they stay inside the
+# simplex. This same renormalization step is performed internally by the
+# predict method of CalibratedClassifierCV on multiclass problems.
 prediction /= prediction.sum(axis=1)[:, None]

-# Plot modifications of calibrator
+# Plot changes in predicted probabilities induced by the calibrators
 for i in range(prediction.shape[0]):
-    plt.arrow(p[i, 0], p[i, 1],
-              prediction[i, 0] - p[i, 0], prediction[i, 1] - p[i, 1],
-              head_width=1e-2, color=colors[np.argmax(p[i])])
-# Plot boundaries of unit simplex
-plt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], 'k', label="Simplex")
+    plt.arrow(
+        p[i, 0],
+        p[i, 1],
+        prediction[i, 0] - p[i, 0],
+        prediction[i, 1] - p[i, 1],
+        head_width=1e-2,
+        color=colors[np.argmax(p[i])],
+    )
+
+# Plot the boundaries of the unit simplex
+plt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], "k", label="Simplex")

 plt.grid(False)
 for x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:
-    plt.plot([0, x], [x, 0], 'k', alpha=0.2)
-    plt.plot([0, 0 + (1-x)/2], [x, x + (1-x)/2], 'k', alpha=0.2)
-    plt.plot([x, x + (1-x)/2], [0, 0 + (1-x)/2], 'k', alpha=0.2)
-
-plt.title("Illustration of sigmoid calibrator")
+    plt.plot([0, x], [x, 0], "k", alpha=0.2)
+    plt.plot([0, 0 + (1 - x) / 2], [x, x + (1 - x) / 2], "k", alpha=0.2)
+    plt.plot([x, x + (1 - x) / 2], [0, 0 + (1 - x) / 2], "k", alpha=0.2)
+
+plt.title("Learned sigmoid calibration map")
 plt.xlabel("Probability class 1")
 plt.ylabel("Probability class 2")
 plt.xlim(-0.05, 1.05)
('examples/gaussian_process', 'plot_compare_gpr_krr.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,119 +3,393 @@
 Comparison of kernel ridge and Gaussian process regression
 ==========================================================

-Both kernel ridge regression (KRR) and Gaussian process regression (GPR) learn
-a target function by employing internally the "kernel trick". KRR learns a
-linear function in the space induced by the respective kernel which corresponds
-to a non-linear function in the original space. The linear function in the
-kernel space is chosen based on the mean-squared error loss with
-ridge regularization. GPR uses the kernel to define the covariance of
-a prior distribution over the target functions and uses the observed training
-data to define a likelihood function. Based on Bayes theorem, a (Gaussian)
-posterior distribution over target functions is defined, whose mean is used
-for prediction.
-
-A major difference is that GPR can choose the kernel's hyperparameters based
-on gradient-ascent on the marginal likelihood function while KRR needs to
-perform a grid search on a cross-validated loss function (mean-squared error
-loss). A further difference is that GPR learns a generative, probabilistic
-model of the target function and can thus provide meaningful confidence
-intervals and posterior samples along with the predictions while KRR only
-provides predictions.
-
-This example illustrates both methods on an artificial dataset, which
-consists of a sinusoidal target function and strong noise. The figure compares
-the learned model of KRR and GPR based on a ExpSineSquared kernel, which is
-suited for learning periodic functions. The kernel's hyperparameters control
-the smoothness (l) and periodicity of the kernel (p). Moreover, the noise level
-of the data is learned explicitly by GPR by an additional WhiteKernel component
-in the kernel and by the regularization parameter alpha of KRR.
-
-The figure shows that both methods learn reasonable models of the target
-function. GPR correctly identifies the periodicity of the function to be
-roughly 2*pi (6.28), while KRR chooses the doubled periodicity 4*pi. Besides
-that, GPR provides reasonable confidence bounds on the prediction which are not
-available for KRR. A major difference between the two methods is the time
-required for fitting and predicting: while fitting KRR is fast in principle,
-the grid-search for hyperparameter optimization scales exponentially with the
-number of hyperparameters ("curse of dimensionality"). The gradient-based
-optimization of the parameters in GPR does not suffer from this exponential
-scaling and is thus considerable faster on this example with 3-dimensional
-hyperparameter space. The time for predicting is similar; however, generating
-the variance of the predictive distribution of GPR takes considerable longer
-than just predicting the mean.
+This example illustrates differences between a kernel ridge regression and a
+Gaussian process regression.
+
+Both kernel ridge regression and Gaussian process regression are using a
+so-called "kernel trick" to make their models expressive enough to fit
+the training data. However, the machine learning problems solved by the two
+methods are drastically different.
+
+Kernel ridge regression will find the target function that minimizes a loss
+function (the mean squared error).
+
+Instead of finding a single target function, the Gaussian process regression
+employs a probabilistic approach : a Gaussian posterior distribution over
+target functions is defined based on the Bayes' theorem, Thus prior
+probabilities on target functions are being combined with a likelihood function
+defined by the observed training data to provide estimates of the posterior
+distributions.
+
+We will illustrate these differences with an example and we will also focus on
+tuning the kernel hyperparameters.
 """
-print(__doc__)

 # Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
+#          Guillaume Lemaitre <g.lemaitre58@gmail.com>
 # License: BSD 3 clause

-
+# %%
+# Generating a dataset
+# --------------------
+#
+# We create a synthetic dataset. The true generative process will take a 1-D
+# vector and compute its sine. Note that the period of this sine is thus
+# :math:`2 \pi`. We will reuse this information later in this example.
+import numpy as np
+
+rng = np.random.RandomState(0)
+data = np.linspace(0, 30, num=1_000).reshape(-1, 1)
+target = np.sin(data).ravel()
+
+# %%
+# Now, we can imagine a scenario where we get observations from this true
+# process. However, we will add some challenges:
+#
+# - the measurements will be noisy;
+# - only samples from the beginning of the signal will be available.
+training_sample_indices = rng.choice(np.arange(0, 400), size=40, replace=False)
+training_data = data[training_sample_indices]
+training_noisy_target = target[training_sample_indices] + 0.5 * rng.randn(
+    len(training_sample_indices)
+)
+
+# %%
+# Let's plot the true signal and the noisy measurements available for training.
+import matplotlib.pyplot as plt
+
+plt.plot(data, target, label="True signal", linewidth=2)
+plt.scatter(
+    training_data,
+    training_noisy_target,
+    color="black",
+    label="Noisy measurements",
+)
+plt.legend()
+plt.xlabel("data")
+plt.ylabel("target")
+_ = plt.title(
+    "Illustration of the true generative process and \n"
+    "noisy measurements available during training"
+)
+
+# %%
+# Limitations of a simple linear model
+# ------------------------------------
+#
+# First, we would like to highlight the limitations of a linear model given
+# our dataset. We fit a :class:`~sklearn.linear_model.Ridge` and check the
+# predictions of this model on our dataset.
+from sklearn.linear_model import Ridge
+
+ridge = Ridge().fit(training_data, training_noisy_target)
+
+plt.plot(data, target, label="True signal", linewidth=2)
+plt.scatter(
+    training_data,
+    training_noisy_target,
+    color="black",
+    label="Noisy measurements",
+)
+plt.plot(data, ridge.predict(data), label="Ridge regression")
+plt.legend()
+plt.xlabel("data")
+plt.ylabel("target")
+_ = plt.title("Limitation of a linear model such as ridge")
+
+# %%
+# Such a ridge regressor underfits data since it is not expressive enough.
+#
+# Kernel methods: kernel ridge and Gaussian process
+# -------------------------------------------------
+#
+# Kernel ridge
+# ............
+#
+# We can make the previous linear model more expressive by using a so-called
+# kernel. A kernel is an embedding from the original feature space to another
+# one. Simply put, it is used to map our original data into a newer and more
+# complex feature space. This new space is explicitly defined by the choice of
+# kernel.
+#
+# In our case, we know that the true generative process is a periodic function.
+# We can use a :class:`~sklearn.gaussian_process.kernels.ExpSineSquared` kernel
+# which allows recovering the periodicity. The class
+# :class:`~sklearn.kernel_ridge.KernelRidge` will accept such a kernel.
+#
+# Using this model together with a kernel is equivalent to embed the data
+# using the mapping function of the kernel and then apply a ridge regression.
+# In practice, the data are not mapped explicitly; instead the dot product
+# between samples in the higher dimensional feature space is computed using the
+# "kernel trick".
+#
+# Thus, let's use such a :class:`~sklearn.kernel_ridge.KernelRidge`.
 import time
-
-import numpy as np
-
-import matplotlib.pyplot as plt
-
+from sklearn.gaussian_process.kernels import ExpSineSquared
 from sklearn.kernel_ridge import KernelRidge
-from sklearn.model_selection import GridSearchCV
+
+kernel_ridge = KernelRidge(kernel=ExpSineSquared())
+
+start_time = time.time()
+kernel_ridge.fit(training_data, training_noisy_target)
+print(
+    f"Fitting KernelRidge with default kernel: {time.time() - start_time:.3f} seconds"
+)
+
+# %%
+plt.plot(data, target, label="True signal", linewidth=2, linestyle="dashed")
+plt.scatter(
+    training_data,
+    training_noisy_target,
+    color="black",
+    label="Noisy measurements",
+)
+plt.plot(
+    data,
+    kernel_ridge.predict(data),
+    label="Kernel ridge",
+    linewidth=2,
+    linestyle="dashdot",
+)
+plt.legend(loc="lower right")
+plt.xlabel("data")
+plt.ylabel("target")
+_ = plt.title(
+    "Kernel ridge regression with an exponential sine squared\n "
+    "kernel using default hyperparameters"
+)
+
+# %%
+# This fitted model is not accurate. Indeed, we did not set the parameters of
+# the kernel and instead used the default ones. We can inspect them.
+kernel_ridge.kernel
+
+# %%
+# Our kernel has two parameters: the length-scale and the periodicity. For our
+# dataset, we use `sin` as the generative process, implying a
+# :math:`2 \pi`-periodicity for the signal. The default value of the parameter
+# being :math:`1`, it explains the high frequency observed in the predictions of
+# our model.
+# Similar conclusions could be drawn with the length-scale parameter. Thus, it
+# tell us that the kernel parameters need to be tuned. We will use a randomized
+# search to tune the different parameters the kernel ridge model: the `alpha`
+# parameter and the kernel parameters.
+
+# %%
+from sklearn.model_selection import RandomizedSearchCV
+from sklearn.utils.fixes import loguniform
+
+param_distributions = {
+    "alpha": loguniform(1e0, 1e3),
+    "kernel__length_scale": loguniform(1e-2, 1e2),
+    "kernel__periodicity": loguniform(1e0, 1e1),
+}
+kernel_ridge_tuned = RandomizedSearchCV(
+    kernel_ridge,
+    param_distributions=param_distributions,
+    n_iter=500,
+    random_state=0,
+)
+start_time = time.time()
+kernel_ridge_tuned.fit(training_data, training_noisy_target)
+print(f"Time for KernelRidge fitting: {time.time() - start_time:.3f} seconds")
+
+# %%
+# Fitting the model is now more computationally expensive since we have to try
+# several combinations of hyperparameters. We can have a look at the
+# hyperparameters found to get some intuitions.
+kernel_ridge_tuned.best_params_
+
+# %%
+# Looking at the best parameters, we see that they are different from the
+# defaults. We also see that the periodicity is closer to the expected value:
+# :math:`2 \pi`. We can now inspect the predictions of our tuned kernel ridge.
+start_time = time.time()
+predictions_kr = kernel_ridge_tuned.predict(data)
+print(f"Time for KernelRidge predict: {time.time() - start_time:.3f} seconds")
+
+# %%
+plt.plot(data, target, label="True signal", linewidth=2, linestyle="dashed")
+plt.scatter(
+    training_data,
+    training_noisy_target,
+    color="black",
+    label="Noisy measurements",
+)
+plt.plot(
+    data,
+    predictions_kr,
+    label="Kernel ridge",
+    linewidth=2,
+    linestyle="dashdot",
+)
+plt.legend(loc="lower right")
+plt.xlabel("data")
+plt.ylabel("target")
+_ = plt.title(
+    "Kernel ridge regression with an exponential sine squared\n "
+    "kernel using tuned hyperparameters"
+)
+
+# %%
+# We get a much more accurate model. We still observe some errors mainly due to
+# the noise added to the dataset.
+#
+# Gaussian process regression
+# ...........................
+#
+# Now, we will use a
+# :class:`~sklearn.gaussian_process.GaussianProcessRegressor` to fit the same
+# dataset. When training a Gaussian process, the hyperparameters of the kernel
+# are optimized during the fitting process. There is no need for an external
+# hyperparameter search. Here, we create a slightly more complex kernel than
+# for the kernel ridge regressor: we add a
+# :class:`~sklearn.gaussian_process.kernels.WhiteKernel` that is used to
+# estimate the noise in the dataset.
 from sklearn.gaussian_process import GaussianProcessRegressor
-from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared
-
-rng = np.random.RandomState(0)
-
-# Generate sample data
-X = 15 * rng.rand(100, 1)
-y = np.sin(X).ravel()
-y += 3 * (0.5 - rng.rand(X.shape[0]))  # add noise
-
-# Fit KernelRidge with parameter selection based on 5-fold cross validation
-param_grid = {"alpha": [1e0, 1e-1, 1e-2, 1e-3],
-              "kernel": [ExpSineSquared(l, p)
-                         for l in np.logspace(-2, 2, 10)
-                         for p in np.logspace(0, 2, 10)]}
-kr = GridSearchCV(KernelRidge(), cv=5, param_grid=param_grid)
-stime = time.time()
-kr.fit(X, y)
-print("Time for KRR fitting: %.3f" % (time.time() - stime))
-
-gp_kernel = ExpSineSquared(1.0, 5.0, periodicity_bounds=(1e-2, 1e1)) \
-    + WhiteKernel(1e-1)
-gpr = GaussianProcessRegressor(kernel=gp_kernel)
-stime = time.time()
-gpr.fit(X, y)
-print("Time for GPR fitting: %.3f" % (time.time() - stime))
-
-# Predict using kernel ridge
-X_plot = np.linspace(0, 20, 10000)[:, None]
-stime = time.time()
-y_kr = kr.predict(X_plot)
-print("Time for KRR prediction: %.3f" % (time.time() - stime))
-
-# Predict using gaussian process regressor
-stime = time.time()
-y_gpr = gpr.predict(X_plot, return_std=False)
-print("Time for GPR prediction: %.3f" % (time.time() - stime))
-
-stime = time.time()
-y_gpr, y_std = gpr.predict(X_plot, return_std=True)
-print("Time for GPR prediction with standard-deviation: %.3f"
-      % (time.time() - stime))
-
-# Plot results
-plt.figure(figsize=(10, 5))
-lw = 2
-plt.scatter(X, y, c='k', label='data')
-plt.plot(X_plot, np.sin(X_plot), color='navy', lw=lw, label='True')
-plt.plot(X_plot, y_kr, color='turquoise', lw=lw,
-         label='KRR (%s)' % kr.best_params_)
-plt.plot(X_plot, y_gpr, color='darkorange', lw=lw,
-         label='GPR (%s)' % gpr.kernel_)
-plt.fill_between(X_plot[:, 0], y_gpr - y_std, y_gpr + y_std, color='darkorange',
-                 alpha=0.2)
-plt.xlabel('data')
-plt.ylabel('target')
-plt.xlim(0, 20)
-plt.ylim(-4, 4)
-plt.title('GPR versus Kernel Ridge')
-plt.legend(loc="best",  scatterpoints=1, prop={'size': 8})
-plt.show()
+from sklearn.gaussian_process.kernels import WhiteKernel
+
+kernel = 1.0 * ExpSineSquared(1.0, 5.0, periodicity_bounds=(1e-2, 1e1)) + WhiteKernel(
+    1e-1
+)
+gaussian_process = GaussianProcessRegressor(kernel=kernel)
+start_time = time.time()
+gaussian_process.fit(training_data, training_noisy_target)
+print(
+    f"Time for GaussianProcessRegressor fitting: {time.time() - start_time:.3f} seconds"
+)
+
+# %%
+# The computation cost of training a Gaussian process is much less than the
+# kernel ridge that uses a randomized search. We can check the parameters of
+# the kernels that we computed.
+gaussian_process.kernel_
+
+# %%
+# Indeed, we see that the parameters have been optimized. Looking at the
+# `periodicity` parameter, we see that we found a period close to the
+# theoretical value :math:`2 \pi`. We can have a look now at the predictions of
+# our model.
+start_time = time.time()
+mean_predictions_gpr, std_predictions_gpr = gaussian_process.predict(
+    data,
+    return_std=True,
+)
+print(
+    f"Time for GaussianProcessRegressor predict: {time.time() - start_time:.3f} seconds"
+)
+
+# %%
+plt.plot(data, target, label="True signal", linewidth=2, linestyle="dashed")
+plt.scatter(
+    training_data,
+    training_noisy_target,
+    color="black",
+    label="Noisy measurements",
+)
+# Plot the predictions of the kernel ridge
+plt.plot(
+    data,
+    predictions_kr,
+    label="Kernel ridge",
+    linewidth=2,
+    linestyle="dashdot",
+)
+# Plot the predictions of the gaussian process regressor
+plt.plot(
+    data,
+    mean_predictions_gpr,
+    label="Gaussian process regressor",
+    linewidth=2,
+    linestyle="dotted",
+)
+plt.fill_between(
+    data.ravel(),
+    mean_predictions_gpr - std_predictions_gpr,
+    mean_predictions_gpr + std_predictions_gpr,
+    color="tab:green",
+    alpha=0.2,
+)
+plt.legend(loc="lower right")
+plt.xlabel("data")
+plt.ylabel("target")
+_ = plt.title("Comparison between kernel ridge and gaussian process regressor")
+
+# %%
+# We observe that the results of the kernel ridge and the Gaussian process
+# regressor are close. However, the Gaussian process regressor also provide
+# an uncertainty information that is not available with a kernel ridge.
+# Due to the probabilistic formulation of the target functions, the
+# Gaussian process can output the standard deviation (or the covariance)
+# together with the mean predictions of the target functions.
+#
+# However, it comes at a cost: the time to compute the predictions is higher
+# with a Gaussian process.
+#
+# Final conclusion
+# ----------------
+#
+# We can give a final word regarding the possibility of the two models to
+# extrapolate. Indeed, we only provided the beginning of the signal as a
+# training set. Using a periodic kernel forces our model to repeat the pattern
+# found on the training set. Using this kernel information together with the
+# capacity of the both models to extrapolate, we observe that the models will
+# continue to predict the sine pattern.
+#
+# Gaussian process allows to combine kernels together. Thus, we could associate
+# the exponential sine squared kernel together with a radial basis function
+# kernel.
+from sklearn.gaussian_process.kernels import RBF
+
+kernel = 1.0 * ExpSineSquared(1.0, 5.0, periodicity_bounds=(1e-2, 1e1)) * RBF(
+    length_scale=15, length_scale_bounds="fixed"
+) + WhiteKernel(1e-1)
+gaussian_process = GaussianProcessRegressor(kernel=kernel)
+gaussian_process.fit(training_data, training_noisy_target)
+mean_predictions_gpr, std_predictions_gpr = gaussian_process.predict(
+    data,
+    return_std=True,
+)
+
+# %%
+plt.plot(data, target, label="True signal", linewidth=2, linestyle="dashed")
+plt.scatter(
+    training_data,
+    training_noisy_target,
+    color="black",
+    label="Noisy measurements",
+)
+# Plot the predictions of the kernel ridge
+plt.plot(
+    data,
+    predictions_kr,
+    label="Kernel ridge",
+    linewidth=2,
+    linestyle="dashdot",
+)
+# Plot the predictions of the gaussian process regressor
+plt.plot(
+    data,
+    mean_predictions_gpr,
+    label="Gaussian process regressor",
+    linewidth=2,
+    linestyle="dotted",
+)
+plt.fill_between(
+    data.ravel(),
+    mean_predictions_gpr - std_predictions_gpr,
+    mean_predictions_gpr + std_predictions_gpr,
+    color="tab:green",
+    alpha=0.2,
+)
+plt.legend(loc="lower right")
+plt.xlabel("data")
+plt.ylabel("target")
+_ = plt.title("Effect of using a radial basis function kernel")
+
+# %%
+# The effect of using a radial basis function kernel will attenuate the
+# periodicity effect once that no sample are available in the training.
+# As testing samples get further away from the training ones, predictions
+# are converging towards their mean and their standard deviation
+# also increases.
('examples/gaussian_process', 'plot_gpr_prior_posterior.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,76 +3,256 @@
 Illustration of prior and posterior Gaussian process for different kernels
 ==========================================================================

-This example illustrates the prior and posterior of a GPR with different
-kernels. Mean, standard deviation, and 10 samples are shown for both prior
-and posterior.
+This example illustrates the prior and posterior of a
+:class:`~sklearn.gaussian_process.GaussianProcessRegressor` with different
+kernels. Mean, standard deviation, and 5 samples are shown for both prior
+and posterior distributions.
+
+Here, we only give some illustration. To know more about kernels' formulation,
+refer to the :ref:`User Guide <gp_kernels>`.
+
 """
-print(__doc__)

 # Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
-#
+#          Guillaume Lemaitre <g.lemaitre58@gmail.com>
 # License: BSD 3 clause

+# %%
+# Helper function
+# ---------------
+#
+# Before presenting each individual kernel available for Gaussian processes,
+# we will define an helper function allowing us plotting samples drawn from
+# the Gaussian process.
+#
+# This function will take a
+# :class:`~sklearn.gaussian_process.GaussianProcessRegressor` model and will
+# drawn sample from the Gaussian process. If the model was not fit, the samples
+# are drawn from the prior distribution while after model fitting, the samples are
+# drawn from the posterior distribution.
+import matplotlib.pyplot as plt
 import numpy as np

-from matplotlib import pyplot as plt
-
+
+def plot_gpr_samples(gpr_model, n_samples, ax):
+    """Plot samples drawn from the Gaussian process model.
+
+    If the Gaussian process model is not trained then the drawn samples are
+    drawn from the prior distribution. Otherwise, the samples are drawn from
+    the posterior distribution. Be aware that a sample here corresponds to a
+    function.
+
+    Parameters
+    ----------
+    gpr_model : `GaussianProcessRegressor`
+        A :class:`~sklearn.gaussian_process.GaussianProcessRegressor` model.
+    n_samples : int
+        The number of samples to draw from the Gaussian process distribution.
+    ax : matplotlib axis
+        The matplotlib axis where to plot the samples.
+    """
+    x = np.linspace(0, 5, 100)
+    X = x.reshape(-1, 1)
+
+    y_mean, y_std = gpr_model.predict(X, return_std=True)
+    y_samples = gpr_model.sample_y(X, n_samples)
+
+    for idx, single_prior in enumerate(y_samples.T):
+        ax.plot(
+            x,
+            single_prior,
+            linestyle="--",
+            alpha=0.7,
+            label=f"Sampled function #{idx + 1}",
+        )
+    ax.plot(x, y_mean, color="black", label="Mean")
+    ax.fill_between(
+        x,
+        y_mean - y_std,
+        y_mean + y_std,
+        alpha=0.1,
+        color="black",
+        label=r"$\pm$ 1 std. dev.",
+    )
+    ax.set_xlabel("x")
+    ax.set_ylabel("y")
+    ax.set_ylim([-3, 3])
+
+
+# %%
+# Dataset and Gaussian process generation
+# ---------------------------------------
+# We will create a training dataset that we will use in the different sections.
+rng = np.random.RandomState(4)
+X_train = rng.uniform(0, 5, 10).reshape(-1, 1)
+y_train = np.sin((X_train[:, 0] - 2.5) ** 2)
+n_samples = 5
+
+# %%
+# Kernel cookbook
+# ---------------
+#
+# In this section, we illustrate some samples drawn from the prior and posterior
+# distributions of the Gaussian process with different kernels.
+#
+# Radial Basis Function kernel
+# ............................
 from sklearn.gaussian_process import GaussianProcessRegressor
-from sklearn.gaussian_process.kernels import (RBF, Matern, RationalQuadratic,
-                                              ExpSineSquared, DotProduct,
-                                              ConstantKernel)
-
-
-kernels = [1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0)),
-           1.0 * RationalQuadratic(length_scale=1.0, alpha=0.1),
-           1.0 * ExpSineSquared(length_scale=1.0, periodicity=3.0,
-                                length_scale_bounds=(0.1, 10.0),
-                                periodicity_bounds=(1.0, 10.0)),
-           ConstantKernel(0.1, (0.01, 10.0))
-               * (DotProduct(sigma_0=1.0, sigma_0_bounds=(0.1, 10.0)) ** 2),
-           1.0 * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0),
-                        nu=1.5)]
-
-for kernel in kernels:
-    # Specify Gaussian Process
-    gp = GaussianProcessRegressor(kernel=kernel)
-
-    # Plot prior
-    plt.figure(figsize=(8, 8))
-    plt.subplot(2, 1, 1)
-    X_ = np.linspace(0, 5, 100)
-    y_mean, y_std = gp.predict(X_[:, np.newaxis], return_std=True)
-    plt.plot(X_, y_mean, 'k', lw=3, zorder=9)
-    plt.fill_between(X_, y_mean - y_std, y_mean + y_std,
-                     alpha=0.2, color='k')
-    y_samples = gp.sample_y(X_[:, np.newaxis], 10)
-    plt.plot(X_, y_samples, lw=1)
-    plt.xlim(0, 5)
-    plt.ylim(-3, 3)
-    plt.title("Prior (kernel:  %s)" % kernel, fontsize=12)
-
-    # Generate data and fit GP
-    rng = np.random.RandomState(4)
-    X = rng.uniform(0, 5, 10)[:, np.newaxis]
-    y = np.sin((X[:, 0] - 2.5) ** 2)
-    gp.fit(X, y)
-
-    # Plot posterior
-    plt.subplot(2, 1, 2)
-    X_ = np.linspace(0, 5, 100)
-    y_mean, y_std = gp.predict(X_[:, np.newaxis], return_std=True)
-    plt.plot(X_, y_mean, 'k', lw=3, zorder=9)
-    plt.fill_between(X_, y_mean - y_std, y_mean + y_std,
-                     alpha=0.2, color='k')
-
-    y_samples = gp.sample_y(X_[:, np.newaxis], 10)
-    plt.plot(X_, y_samples, lw=1)
-    plt.scatter(X[:, 0], y, c='r', s=50, zorder=10, edgecolors=(0, 0, 0))
-    plt.xlim(0, 5)
-    plt.ylim(-3, 3)
-    plt.title("Posterior (kernel: %s)\n Log-Likelihood: %.3f"
-              % (gp.kernel_, gp.log_marginal_likelihood(gp.kernel_.theta)),
-              fontsize=12)
-    plt.tight_layout()
-
-plt.show()
+from sklearn.gaussian_process.kernels import RBF
+
+kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0))
+gpr = GaussianProcessRegressor(kernel=kernel, random_state=0)
+
+fig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))
+
+# plot prior
+plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])
+axs[0].set_title("Samples from prior distribution")
+
+# plot posterior
+gpr.fit(X_train, y_train)
+plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])
+axs[1].scatter(X_train[:, 0], y_train, color="red", zorder=10, label="Observations")
+axs[1].legend(bbox_to_anchor=(1.05, 1.5), loc="upper left")
+axs[1].set_title("Samples from posterior distribution")
+
+fig.suptitle("Radial Basis Function kernel", fontsize=18)
+plt.tight_layout()
+
+# %%
+print(f"Kernel parameters before fit:\n{kernel})")
+print(
+    f"Kernel parameters after fit: \n{gpr.kernel_} \n"
+    f"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}"
+)
+
+# %%
+# Rational Quadradtic kernel
+# ..........................
+from sklearn.gaussian_process.kernels import RationalQuadratic
+
+kernel = 1.0 * RationalQuadratic(length_scale=1.0, alpha=0.1, alpha_bounds=(1e-5, 1e15))
+gpr = GaussianProcessRegressor(kernel=kernel, random_state=0)
+
+fig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))
+
+# plot prior
+plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])
+axs[0].set_title("Samples from prior distribution")
+
+# plot posterior
+gpr.fit(X_train, y_train)
+plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])
+axs[1].scatter(X_train[:, 0], y_train, color="red", zorder=10, label="Observations")
+axs[1].legend(bbox_to_anchor=(1.05, 1.5), loc="upper left")
+axs[1].set_title("Samples from posterior distribution")
+
+fig.suptitle("Rational Quadratic kernel", fontsize=18)
+plt.tight_layout()
+
+# %%
+print(f"Kernel parameters before fit:\n{kernel})")
+print(
+    f"Kernel parameters after fit: \n{gpr.kernel_} \n"
+    f"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}"
+)
+
+# %%
+# Periodic kernel
+# ...............
+from sklearn.gaussian_process.kernels import ExpSineSquared
+
+kernel = 1.0 * ExpSineSquared(
+    length_scale=1.0,
+    periodicity=3.0,
+    length_scale_bounds=(0.1, 10.0),
+    periodicity_bounds=(1.0, 10.0),
+)
+gpr = GaussianProcessRegressor(kernel=kernel, random_state=0)
+
+fig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))
+
+# plot prior
+plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])
+axs[0].set_title("Samples from prior distribution")
+
+# plot posterior
+gpr.fit(X_train, y_train)
+plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])
+axs[1].scatter(X_train[:, 0], y_train, color="red", zorder=10, label="Observations")
+axs[1].legend(bbox_to_anchor=(1.05, 1.5), loc="upper left")
+axs[1].set_title("Samples from posterior distribution")
+
+fig.suptitle("Periodic kernel", fontsize=18)
+plt.tight_layout()
+
+# %%
+print(f"Kernel parameters before fit:\n{kernel})")
+print(
+    f"Kernel parameters after fit: \n{gpr.kernel_} \n"
+    f"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}"
+)
+
+# %%
+# Dot product kernel
+# ..................
+from sklearn.gaussian_process.kernels import ConstantKernel, DotProduct
+
+kernel = ConstantKernel(0.1, (0.01, 10.0)) * (
+    DotProduct(sigma_0=1.0, sigma_0_bounds=(0.1, 10.0)) ** 2
+)
+gpr = GaussianProcessRegressor(kernel=kernel, random_state=0)
+
+fig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))
+
+# plot prior
+plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])
+axs[0].set_title("Samples from prior distribution")
+
+# plot posterior
+gpr.fit(X_train, y_train)
+plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])
+axs[1].scatter(X_train[:, 0], y_train, color="red", zorder=10, label="Observations")
+axs[1].legend(bbox_to_anchor=(1.05, 1.5), loc="upper left")
+axs[1].set_title("Samples from posterior distribution")
+
+fig.suptitle("Dot product kernel", fontsize=18)
+plt.tight_layout()
+
+# %%
+print(f"Kernel parameters before fit:\n{kernel})")
+print(
+    f"Kernel parameters after fit: \n{gpr.kernel_} \n"
+    f"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}"
+)
+
+# %%
+# Mattern kernel
+# ..............
+from sklearn.gaussian_process.kernels import Matern
+
+kernel = 1.0 * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0), nu=1.5)
+gpr = GaussianProcessRegressor(kernel=kernel, random_state=0)
+
+fig, axs = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))
+
+# plot prior
+plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[0])
+axs[0].set_title("Samples from prior distribution")
+
+# plot posterior
+gpr.fit(X_train, y_train)
+plot_gpr_samples(gpr, n_samples=n_samples, ax=axs[1])
+axs[1].scatter(X_train[:, 0], y_train, color="red", zorder=10, label="Observations")
+axs[1].legend(bbox_to_anchor=(1.05, 1.5), loc="upper left")
+axs[1].set_title("Samples from posterior distribution")
+
+fig.suptitle("Mattern kernel", fontsize=18)
+plt.tight_layout()
+
+# %%
+print(f"Kernel parameters before fit:\n{kernel})")
+print(
+    f"Kernel parameters after fit: \n{gpr.kernel_} \n"
+    f"Log-likelihood: {gpr.log_marginal_likelihood(gpr.kernel_.theta):.3f}"
+)
('examples/gaussian_process', 'plot_gpc_xor.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,8 +8,8 @@
 dataset, the DotProduct kernel obtains considerably better results because the
 class-boundaries are linear and coincide with the coordinate axes. In general,
 stationary kernels often obtain better results.
+
 """
-print(__doc__)

 # Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 #
@@ -22,15 +22,14 @@
 from sklearn.gaussian_process.kernels import RBF, DotProduct


-xx, yy = np.meshgrid(np.linspace(-3, 3, 50),
-                     np.linspace(-3, 3, 50))
+xx, yy = np.meshgrid(np.linspace(-3, 3, 50), np.linspace(-3, 3, 50))
 rng = np.random.RandomState(0)
 X = rng.randn(200, 2)
 Y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)

 # fit the model
 plt.figure(figsize=(10, 5))
-kernels = [1.0 * RBF(length_scale=1.0), 1.0 * DotProduct(sigma_0=1.0)**2]
+kernels = [1.0 * RBF(length_scale=1.0), 1.0 * DotProduct(sigma_0=1.0) ** 2]
 for i, kernel in enumerate(kernels):
     clf = GaussianProcessClassifier(kernel=kernel, warm_start=True).fit(X, Y)

@@ -39,20 +38,25 @@
     Z = Z.reshape(xx.shape)

     plt.subplot(1, 2, i + 1)
-    image = plt.imshow(Z, interpolation='nearest',
-                       extent=(xx.min(), xx.max(), yy.min(), yy.max()),
-                       aspect='auto', origin='lower', cmap=plt.cm.PuOr_r)
-    contours = plt.contour(xx, yy, Z, levels=[0.5], linewidths=2,
-                           colors=['k'])
-    plt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired,
-                edgecolors=(0, 0, 0))
+    image = plt.imshow(
+        Z,
+        interpolation="nearest",
+        extent=(xx.min(), xx.max(), yy.min(), yy.max()),
+        aspect="auto",
+        origin="lower",
+        cmap=plt.cm.PuOr_r,
+    )
+    contours = plt.contour(xx, yy, Z, levels=[0.5], linewidths=2, colors=["k"])
+    plt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired, edgecolors=(0, 0, 0))
     plt.xticks(())
     plt.yticks(())
     plt.axis([-3, 3, -3, 3])
     plt.colorbar(image)
-    plt.title("%s\n Log-Marginal-Likelihood:%.3f"
-              % (clf.kernel_, clf.log_marginal_likelihood(clf.kernel_.theta)),
-              fontsize=12)
+    plt.title(
+        "%s\n Log-Marginal-Likelihood:%.3f"
+        % (clf.kernel_, clf.log_marginal_likelihood(clf.kernel_.theta)),
+        fontsize=12,
+    )

 plt.tight_layout()
 plt.show()
('examples/gaussian_process', 'plot_gpc_isoprobability.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =================================================================
 Iso-probability lines for Gaussian Processes classification (GPC)
@@ -8,8 +6,8 @@

 A two-dimensional classification example showing iso-probability lines for
 the predicted probabilities.
+
 """
-print(__doc__)

 # Author: Vincent Dubourg <vincent.dubourg@gmail.com>
 # Adapted to GaussianProcessClassifier:
@@ -31,17 +29,22 @@
 def g(x):
     """The function to predict (classification will then consist in predicting
     whether g(x) <= 0 or not)"""
-    return 5. - x[:, 1] - .5 * x[:, 0] ** 2.
+    return 5.0 - x[:, 1] - 0.5 * x[:, 0] ** 2.0
+

 # Design of experiments
-X = np.array([[-4.61611719, -6.00099547],
-              [4.10469096, 5.32782448],
-              [0.00000000, -0.50000000],
-              [-6.17289014, -4.6984743],
-              [1.3109306, -6.93271427],
-              [-5.03823144, 3.10584743],
-              [-2.87600388, 6.74310541],
-              [5.21301203, 4.26386883]])
+X = np.array(
+    [
+        [-4.61611719, -6.00099547],
+        [4.10469096, 5.32782448],
+        [0.00000000, -0.50000000],
+        [-6.17289014, -4.6984743],
+        [1.3109306, -6.93271427],
+        [-5.03823144, 3.10584743],
+        [-2.87600388, 6.74310541],
+        [5.21301203, 4.26386883],
+    ]
+)

 # Observations
 y = np.array(g(X) > 0, dtype=int)
@@ -54,8 +57,7 @@

 # Evaluate real function and the predicted probability
 res = 50
-x1, x2 = np.meshgrid(np.linspace(- lim, lim, res),
-                     np.linspace(- lim, lim, res))
+x1, x2 = np.meshgrid(np.linspace(-lim, lim, res), np.linspace(-lim, lim, res))
 xx = np.vstack([x1.reshape(x1.size), x2.reshape(x2.size)]).T

 y_true = g(xx)
@@ -66,37 +68,33 @@
 # Plot the probabilistic classification iso-values
 fig = plt.figure(1)
 ax = fig.gca()
-ax.axes.set_aspect('equal')
+ax.axes.set_aspect("equal")
 plt.xticks([])
 plt.yticks([])
 ax.set_xticklabels([])
 ax.set_yticklabels([])
-plt.xlabel('$x_1$')
-plt.ylabel('$x_2$')
+plt.xlabel("$x_1$")
+plt.ylabel("$x_2$")

-cax = plt.imshow(y_prob, cmap=cm.gray_r, alpha=0.8,
-                 extent=(-lim, lim, -lim, lim))
-norm = plt.matplotlib.colors.Normalize(vmin=0., vmax=0.9)
-cb = plt.colorbar(cax, ticks=[0., 0.2, 0.4, 0.6, 0.8, 1.], norm=norm)
-cb.set_label(r'${\rm \mathbb{P}}\left[\widehat{G}(\mathbf{x}) \leq 0\right]$')
+cax = plt.imshow(y_prob, cmap=cm.gray_r, alpha=0.8, extent=(-lim, lim, -lim, lim))
+norm = plt.matplotlib.colors.Normalize(vmin=0.0, vmax=0.9)
+cb = plt.colorbar(cax, ticks=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0], norm=norm)
+cb.set_label(r"${\rm \mathbb{P}}\left[\widehat{G}(\mathbf{x}) \leq 0\right]$")
 plt.clim(0, 1)

-plt.plot(X[y <= 0, 0], X[y <= 0, 1], 'r.', markersize=12)
+plt.plot(X[y <= 0, 0], X[y <= 0, 1], "r.", markersize=12)

-plt.plot(X[y > 0, 0], X[y > 0, 1], 'b.', markersize=12)
+plt.plot(X[y > 0, 0], X[y > 0, 1], "b.", markersize=12)

-plt.contour(x1, x2, y_true, [0.], colors='k', linestyles='dashdot')
+plt.contour(x1, x2, y_true, [0.0], colors="k", linestyles="dashdot")

-cs = plt.contour(x1, x2, y_prob, [0.666], colors='b',
-                 linestyles='solid')
+cs = plt.contour(x1, x2, y_prob, [0.666], colors="b", linestyles="solid")
 plt.clabel(cs, fontsize=11)

-cs = plt.contour(x1, x2, y_prob, [0.5], colors='k',
-                 linestyles='dashed')
+cs = plt.contour(x1, x2, y_prob, [0.5], colors="k", linestyles="dashed")
 plt.clabel(cs, fontsize=11)

-cs = plt.contour(x1, x2, y_prob, [0.334], colors='r',
-                 linestyles='solid')
+cs = plt.contour(x1, x2, y_prob, [0.334], colors="r", linestyles="solid")
 plt.clabel(cs, fontsize=11)

 plt.show()
('examples/gaussian_process', 'plot_gpr_noisy.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,95 +3,186 @@
 Gaussian process regression (GPR) with noise-level estimation
 =============================================================

-This example illustrates that GPR with a sum-kernel including a WhiteKernel can
-estimate the noise level of data. An illustration of the
-log-marginal-likelihood (LML) landscape shows that there exist two local
-maxima of LML. The first corresponds to a model with a high noise level and a
-large length scale, which explains all variations in the data by noise. The
-second one has a smaller noise level and shorter length scale, which explains
-most of the variation by the noise-free functional relationship. The second
-model has a higher likelihood; however, depending on the initial value for the
-hyperparameters, the gradient-based optimization might also converge to the
-high-noise solution. It is thus important to repeat the optimization several
-times for different initializations.
+This example shows the ability of the
+:class:`~sklearn.gaussian_process.kernels.WhiteKernel` to estimate the noise
+level in the data. Moreover, we show the importance of kernel hyperparameters
+initialization.
 """
-print(__doc__)

 # Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
-#
+#          Guillaume Lemaitre <guillaume.lemaitre@inria.fr>
 # License: BSD 3 clause

+# %%
+# Data generation
+# ---------------
+#
+# We will work in a setting where `X` will contain a single feature. We create a
+# function that will generate the target to be predicted. We will add an
+# option to add some noise to the generated target.
 import numpy as np

-from matplotlib import pyplot as plt
-from matplotlib.colors import LogNorm

+def target_generator(X, add_noise=False):
+    target = 0.5 + np.sin(3 * X)
+    if add_noise:
+        rng = np.random.RandomState(1)
+        target += rng.normal(0, 0.3, size=target.shape)
+    return target.squeeze()
+
+
+# %%
+# Let's have a look to the target generator where we will not add any noise to
+# observe the signal that we would like to predict.
+X = np.linspace(0, 5, num=30).reshape(-1, 1)
+y = target_generator(X, add_noise=False)
+
+# %%
+import matplotlib.pyplot as plt
+
+plt.plot(X, y, label="Expected signal")
+plt.legend()
+plt.xlabel("X")
+_ = plt.ylabel("y")
+
+# %%
+# The target is transforming the input `X` using a sine function. Now, we will
+# generate few noisy training samples. To illustrate the noise level, we will
+# plot the true signal together with the noisy training samples.
+rng = np.random.RandomState(0)
+X_train = rng.uniform(0, 5, size=20).reshape(-1, 1)
+y_train = target_generator(X_train, add_noise=True)
+
+# %%
+plt.plot(X, y, label="Expected signal")
+plt.scatter(
+    x=X_train[:, 0],
+    y=y_train,
+    color="black",
+    alpha=0.4,
+    label="Observations",
+)
+plt.legend()
+plt.xlabel("X")
+_ = plt.ylabel("y")
+
+# %%
+# Optimisation of kernel hyperparameters in GPR
+# ---------------------------------------------
+#
+# Now, we will create a
+# :class:`~sklearn.gaussian_process.GaussianProcessRegressor`
+# using an additive kernel adding a
+# :class:`~sklearn.gaussian_process.kernels.RBF` and
+# :class:`~sklearn.gaussian_process.kernels.WhiteKernel` kernels.
+# The :class:`~sklearn.gaussian_process.kernels.WhiteKernel` is a kernel that
+# will able to estimate the amount of noise present in the data while the
+# :class:`~sklearn.gaussian_process.kernels.RBF` will serve at fitting the
+# non-linearity between the data and the target.
+#
+# However, we will show that the hyperparameter space contains several local
+# minima. It will highlights the importance of initial hyperparameter values.
+#
+# We will create a model using a kernel with a high noise level and a large
+# length scale, which will explain all variations in the data by noise.
 from sklearn.gaussian_process import GaussianProcessRegressor
 from sklearn.gaussian_process.kernels import RBF, WhiteKernel

+kernel = 1.0 * RBF(length_scale=1e1, length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(
+    noise_level=1, noise_level_bounds=(1e-5, 1e1)
+)
+gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.0)
+gpr.fit(X_train, y_train)
+y_mean, y_std = gpr.predict(X, return_std=True)

-rng = np.random.RandomState(0)
-X = rng.uniform(0, 5, 20)[:, np.newaxis]
-y = 0.5 * np.sin(3 * X[:, 0]) + rng.normal(0, 0.5, X.shape[0])
+# %%
+plt.plot(X, y, label="Expected signal")
+plt.scatter(x=X_train[:, 0], y=y_train, color="black", alpha=0.4, label="Observsations")
+plt.errorbar(X, y_mean, y_std)
+plt.legend()
+plt.xlabel("X")
+plt.ylabel("y")
+_ = plt.title(
+    f"Initial: {kernel}\nOptimum: {gpr.kernel_}\nLog-Marginal-Likelihood: "
+    f"{gpr.log_marginal_likelihood(gpr.kernel_.theta)}",
+    fontsize=8,
+)
+# %%
+# We see that the optimum kernel found still have a high noise level and
+# an even larger length scale. Furthermore, we observe that the
+# model does not provide faithful predictions.
+#
+# Now, we will initialize the
+# :class:`~sklearn.gaussian_process.kernels.RBF` with a
+# larger `length_scale` and the
+# :class:`~sklearn.gaussian_process.kernels.WhiteKernel`
+# with a smaller noise level lower bound.
+kernel = 1.0 * RBF(length_scale=1e-1, length_scale_bounds=(1e-2, 1e3)) + WhiteKernel(
+    noise_level=1e-2, noise_level_bounds=(1e-10, 1e1)
+)
+gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.0)
+gpr.fit(X_train, y_train)
+y_mean, y_std = gpr.predict(X, return_std=True)

-# First run
-plt.figure()
-kernel = 1.0 * RBF(length_scale=100.0, length_scale_bounds=(1e-2, 1e3)) \
-    + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))
-gp = GaussianProcessRegressor(kernel=kernel,
-                              alpha=0.0).fit(X, y)
-X_ = np.linspace(0, 5, 100)
-y_mean, y_cov = gp.predict(X_[:, np.newaxis], return_cov=True)
-plt.plot(X_, y_mean, 'k', lw=3, zorder=9)
-plt.fill_between(X_, y_mean - np.sqrt(np.diag(y_cov)),
-                 y_mean + np.sqrt(np.diag(y_cov)),
-                 alpha=0.5, color='k')
-plt.plot(X_, 0.5*np.sin(3*X_), 'r', lw=3, zorder=9)
-plt.scatter(X[:, 0], y, c='r', s=50, zorder=10, edgecolors=(0, 0, 0))
-plt.title("Initial: %s\nOptimum: %s\nLog-Marginal-Likelihood: %s"
-          % (kernel, gp.kernel_,
-             gp.log_marginal_likelihood(gp.kernel_.theta)))
-plt.tight_layout()
+# %%
+plt.plot(X, y, label="Expected signal")
+plt.scatter(x=X_train[:, 0], y=y_train, color="black", alpha=0.4, label="Observations")
+plt.errorbar(X, y_mean, y_std)
+plt.legend()
+plt.xlabel("X")
+plt.ylabel("y")
+_ = plt.title(
+    f"Initial: {kernel}\nOptimum: {gpr.kernel_}\nLog-Marginal-Likelihood: "
+    f"{gpr.log_marginal_likelihood(gpr.kernel_.theta)}",
+    fontsize=8,
+)

-# Second run
-plt.figure()
-kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \
-    + WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-10, 1e+1))
-gp = GaussianProcessRegressor(kernel=kernel,
-                              alpha=0.0).fit(X, y)
-X_ = np.linspace(0, 5, 100)
-y_mean, y_cov = gp.predict(X_[:, np.newaxis], return_cov=True)
-plt.plot(X_, y_mean, 'k', lw=3, zorder=9)
-plt.fill_between(X_, y_mean - np.sqrt(np.diag(y_cov)),
-                 y_mean + np.sqrt(np.diag(y_cov)),
-                 alpha=0.5, color='k')
-plt.plot(X_, 0.5*np.sin(3*X_), 'r', lw=3, zorder=9)
-plt.scatter(X[:, 0], y, c='r', s=50, zorder=10, edgecolors=(0, 0, 0))
-plt.title("Initial: %s\nOptimum: %s\nLog-Marginal-Likelihood: %s"
-          % (kernel, gp.kernel_,
-             gp.log_marginal_likelihood(gp.kernel_.theta)))
-plt.tight_layout()
+# %%
+# First, we see that the model's predictions are more precise than the
+# previous model's: this new model is able to estimate the noise-free
+# functional relationship.
+#
+# Looking at the kernel hyperparameters, we see that the best combination found
+# has a smaller noise level and shorter length scale than the first model.
+#
+# We can inspect the Log-Marginal-Likelihood (LML) of
+# :class:`~sklearn.gaussian_process.GaussianProcessRegressor`
+# for different hyperparameters to get a sense of the local minima.
+from matplotlib.colors import LogNorm

-# Plot LML landscape
-plt.figure()
-theta0 = np.logspace(-2, 3, 49)
-theta1 = np.logspace(-2, 0, 50)
-Theta0, Theta1 = np.meshgrid(theta0, theta1)
-LML = [[gp.log_marginal_likelihood(np.log([0.36, Theta0[i, j], Theta1[i, j]]))
-        for i in range(Theta0.shape[0])] for j in range(Theta0.shape[1])]
-LML = np.array(LML).T
+length_scale = np.logspace(-2, 4, num=50)
+noise_level = np.logspace(-2, 1, num=50)
+length_scale_grid, noise_level_grid = np.meshgrid(length_scale, noise_level)

-vmin, vmax = (-LML).min(), (-LML).max()
-vmax = 50
-level = np.around(np.logspace(np.log10(vmin), np.log10(vmax), 50), decimals=1)
-plt.contour(Theta0, Theta1, -LML,
-            levels=level, norm=LogNorm(vmin=vmin, vmax=vmax))
+log_marginal_likelihood = [
+    gpr.log_marginal_likelihood(theta=np.log([0.36, scale, noise]))
+    for scale, noise in zip(length_scale_grid.ravel(), noise_level_grid.ravel())
+]
+log_marginal_likelihood = np.reshape(
+    log_marginal_likelihood, newshape=noise_level_grid.shape
+)
+
+# %%
+vmin, vmax = (-log_marginal_likelihood).min(), 50
+level = np.around(np.logspace(np.log10(vmin), np.log10(vmax), num=50), decimals=1)
+plt.contour(
+    length_scale_grid,
+    noise_level_grid,
+    -log_marginal_likelihood,
+    levels=level,
+    norm=LogNorm(vmin=vmin, vmax=vmax),
+)
 plt.colorbar()
 plt.xscale("log")
 plt.yscale("log")
 plt.xlabel("Length-scale")
 plt.ylabel("Noise-level")
 plt.title("Log-marginal-likelihood")
-plt.tight_layout()
+plt.show()

-plt.show()
+# %%
+# We see that there are two local minima that correspond to the combination
+# of hyperparameters previously found. Depending on the initial values for the
+# hyperparameters, the gradient-based optimization might converge whether or
+# not to the best model. It is thus important to repeat the optimization
+# several times for different initializations.
('examples/gaussian_process', 'plot_gpc.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -19,8 +19,8 @@
 The second figure shows the log-marginal-likelihood for different choices of
 the kernel's hyperparameters, highlighting the two choices of the
 hyperparameters used in the first figure by black dots.
+
 """
-print(__doc__)

 # Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
 #
@@ -30,7 +30,7 @@

 from matplotlib import pyplot as plt

-from sklearn.metrics.classification import accuracy_score, log_loss
+from sklearn.metrics import accuracy_score, log_loss
 from sklearn.gaussian_process import GaussianProcessClassifier
 from sklearn.gaussian_process.kernels import RBF

@@ -42,37 +42,58 @@
 y = np.array(X[:, 0] > 2.5, dtype=int)

 # Specify Gaussian Processes with fixed and optimized hyperparameters
-gp_fix = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0),
-                                   optimizer=None)
+gp_fix = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0), optimizer=None)
 gp_fix.fit(X[:train_size], y[:train_size])

 gp_opt = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0))
 gp_opt.fit(X[:train_size], y[:train_size])

-print("Log Marginal Likelihood (initial): %.3f"
-      % gp_fix.log_marginal_likelihood(gp_fix.kernel_.theta))
-print("Log Marginal Likelihood (optimized): %.3f"
-      % gp_opt.log_marginal_likelihood(gp_opt.kernel_.theta))
+print(
+    "Log Marginal Likelihood (initial): %.3f"
+    % gp_fix.log_marginal_likelihood(gp_fix.kernel_.theta)
+)
+print(
+    "Log Marginal Likelihood (optimized): %.3f"
+    % gp_opt.log_marginal_likelihood(gp_opt.kernel_.theta)
+)

-print("Accuracy: %.3f (initial) %.3f (optimized)"
-      % (accuracy_score(y[:train_size], gp_fix.predict(X[:train_size])),
-         accuracy_score(y[:train_size], gp_opt.predict(X[:train_size]))))
-print("Log-loss: %.3f (initial) %.3f (optimized)"
-      % (log_loss(y[:train_size], gp_fix.predict_proba(X[:train_size])[:, 1]),
-         log_loss(y[:train_size], gp_opt.predict_proba(X[:train_size])[:, 1])))
+print(
+    "Accuracy: %.3f (initial) %.3f (optimized)"
+    % (
+        accuracy_score(y[:train_size], gp_fix.predict(X[:train_size])),
+        accuracy_score(y[:train_size], gp_opt.predict(X[:train_size])),
+    )
+)
+print(
+    "Log-loss: %.3f (initial) %.3f (optimized)"
+    % (
+        log_loss(y[:train_size], gp_fix.predict_proba(X[:train_size])[:, 1]),
+        log_loss(y[:train_size], gp_opt.predict_proba(X[:train_size])[:, 1]),
+    )
+)


 # Plot posteriors
 plt.figure()
-plt.scatter(X[:train_size, 0], y[:train_size], c='k', label="Train data",
-            edgecolors=(0, 0, 0))
-plt.scatter(X[train_size:, 0], y[train_size:], c='g', label="Test data",
-            edgecolors=(0, 0, 0))
+plt.scatter(
+    X[:train_size, 0], y[:train_size], c="k", label="Train data", edgecolors=(0, 0, 0)
+)
+plt.scatter(
+    X[train_size:, 0], y[train_size:], c="g", label="Test data", edgecolors=(0, 0, 0)
+)
 X_ = np.linspace(0, 5, 100)
-plt.plot(X_, gp_fix.predict_proba(X_[:, np.newaxis])[:, 1], 'r',
-         label="Initial kernel: %s" % gp_fix.kernel_)
-plt.plot(X_, gp_opt.predict_proba(X_[:, np.newaxis])[:, 1], 'b',
-         label="Optimized kernel: %s" % gp_opt.kernel_)
+plt.plot(
+    X_,
+    gp_fix.predict_proba(X_[:, np.newaxis])[:, 1],
+    "r",
+    label="Initial kernel: %s" % gp_fix.kernel_,
+)
+plt.plot(
+    X_,
+    gp_opt.predict_proba(X_[:, np.newaxis])[:, 1],
+    "b",
+    label="Optimized kernel: %s" % gp_opt.kernel_,
+)
 plt.xlabel("Feature")
 plt.ylabel("Class 1 probability")
 plt.xlim(0, 5)
@@ -84,13 +105,20 @@
 theta0 = np.logspace(0, 8, 30)
 theta1 = np.logspace(-1, 1, 29)
 Theta0, Theta1 = np.meshgrid(theta0, theta1)
-LML = [[gp_opt.log_marginal_likelihood(np.log([Theta0[i, j], Theta1[i, j]]))
-        for i in range(Theta0.shape[0])] for j in range(Theta0.shape[1])]
+LML = [
+    [
+        gp_opt.log_marginal_likelihood(np.log([Theta0[i, j], Theta1[i, j]]))
+        for i in range(Theta0.shape[0])
+    ]
+    for j in range(Theta0.shape[1])
+]
 LML = np.array(LML).T
-plt.plot(np.exp(gp_fix.kernel_.theta)[0], np.exp(gp_fix.kernel_.theta)[1],
-         'ko', zorder=10)
-plt.plot(np.exp(gp_opt.kernel_.theta)[0], np.exp(gp_opt.kernel_.theta)[1],
-         'ko', zorder=10)
+plt.plot(
+    np.exp(gp_fix.kernel_.theta)[0], np.exp(gp_fix.kernel_.theta)[1], "ko", zorder=10
+)
+plt.plot(
+    np.exp(gp_opt.kernel_.theta)[0], np.exp(gp_opt.kernel_.theta)[1], "ko", zorder=10
+)
 plt.pcolor(Theta0, Theta1, LML)
 plt.xscale("log")
 plt.yscale("log")
('examples/gaussian_process', 'plot_gpc_iris.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,8 +7,8 @@
 and anisotropic RBF kernel on a two-dimensional version for the iris-dataset.
 The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by
 assigning different length-scales to the two feature dimensions.
+
 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -21,7 +21,7 @@
 X = iris.data[:, :2]  # we only take the first two features.
 y = np.array(iris.target, dtype=int)

-h = .02  # step size in the mesh
+h = 0.02  # step size in the mesh

 kernel = 1.0 * RBF([1.0])
 gpc_rbf_isotropic = GaussianProcessClassifier(kernel=kernel).fit(X, y)
@@ -31,8 +31,7 @@
 # create a mesh to plot in
 x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
-xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
-                     np.arange(y_min, y_max, h))
+xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

 titles = ["Isotropic RBF", "Anisotropic RBF"]
 plt.figure(figsize=(10, 5))
@@ -48,16 +47,16 @@
     plt.imshow(Z, extent=(x_min, x_max, y_min, y_max), origin="lower")

     # Plot also the training points
-    plt.scatter(X[:, 0], X[:, 1], c=np.array(["r", "g", "b"])[y],
-                edgecolors=(0, 0, 0))
-    plt.xlabel('Sepal length')
-    plt.ylabel('Sepal width')
+    plt.scatter(X[:, 0], X[:, 1], c=np.array(["r", "g", "b"])[y], edgecolors=(0, 0, 0))
+    plt.xlabel("Sepal length")
+    plt.ylabel("Sepal width")
     plt.xlim(xx.min(), xx.max())
     plt.ylim(yy.min(), yy.max())
     plt.xticks(())
     plt.yticks(())
-    plt.title("%s, LML: %.3f" %
-              (titles[i], clf.log_marginal_likelihood(clf.kernel_.theta)))
+    plt.title(
+        "%s, LML: %.3f" % (titles[i], clf.log_marginal_likelihood(clf.kernel_.theta))
+    )

 plt.tight_layout()
 plt.show()
('examples/gaussian_process', 'plot_gpr_co2.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,156 +1,218 @@
 """
-========================================================
-Gaussian process regression (GPR) on Mauna Loa CO2 data.
-========================================================
+=======================================================
+Gaussian process regression (GPR) on Mauna Loa CO2 data
+=======================================================

 This example is based on Section 5.4.3 of "Gaussian Processes for Machine
-Learning" [RW2006]. It illustrates an example of complex kernel engineering and
-hyperparameter optimization using gradient ascent on the
+Learning" [RW2006]_. It illustrates an example of complex kernel engineering
+and hyperparameter optimization using gradient ascent on the
 log-marginal-likelihood. The data consists of the monthly average atmospheric
-CO2 concentrations (in parts per million by volume (ppmv)) collected at the
+CO2 concentrations (in parts per million by volume (ppm)) collected at the
 Mauna Loa Observatory in Hawaii, between 1958 and 2001. The objective is to
-model the CO2 concentration as a function of the time t.
-
-The kernel is composed of several terms that are responsible for explaining
-different properties of the signal:
-
-- a long term, smooth rising trend is to be explained by an RBF kernel. The
-  RBF kernel with a large length-scale enforces this component to be smooth;
-  it is not enforced that the trend is rising which leaves this choice to the
-  GP. The specific length-scale and the amplitude are free hyperparameters.
-
-- a seasonal component, which is to be explained by the periodic
-  ExpSineSquared kernel with a fixed periodicity of 1 year. The length-scale
-  of this periodic component, controlling its smoothness, is a free parameter.
-  In order to allow decaying away from exact periodicity, the product with an
-  RBF kernel is taken. The length-scale of this RBF component controls the
-  decay time and is a further free parameter.
-
-- smaller, medium term irregularities are to be explained by a
-  RationalQuadratic kernel component, whose length-scale and alpha parameter,
-  which determines the diffuseness of the length-scales, are to be determined.
-  According to [RW2006], these irregularities can better be explained by
-  a RationalQuadratic than an RBF kernel component, probably because it can
-  accommodate several length-scales.
-
-- a "noise" term, consisting of an RBF kernel contribution, which shall
-  explain the correlated noise components such as local weather phenomena,
-  and a WhiteKernel contribution for the white noise. The relative amplitudes
-  and the RBF's length scale are further free parameters.
-
-Maximizing the log-marginal-likelihood after subtracting the target's mean
-yields the following kernel with an LML of -83.214::
-
-   34.4**2 * RBF(length_scale=41.8)
-   + 3.27**2 * RBF(length_scale=180) * ExpSineSquared(length_scale=1.44,
-                                                      periodicity=1)
-   + 0.446**2 * RationalQuadratic(alpha=17.7, length_scale=0.957)
-   + 0.197**2 * RBF(length_scale=0.138) + WhiteKernel(noise_level=0.0336)
-
-Thus, most of the target signal (34.4ppm) is explained by a long-term rising
-trend (length-scale 41.8 years). The periodic component has an amplitude of
-3.27ppm, a decay time of 180 years and a length-scale of 1.44. The long decay
-time indicates that we have a locally very close to periodic seasonal
-component. The correlated noise has an amplitude of 0.197ppm with a length
-scale of 0.138 years and a white-noise contribution of 0.197ppm. Thus, the
-overall noise level is very small, indicating that the data can be very well
-explained by the model. The figure shows also that the model makes very
-confident predictions until around 2015.
+model the CO2 concentration as a function of the time :math:`t` and extrapolate
+for years after 2001.
+
+.. topic: References
+
+    .. [RW2006] `Rasmussen, Carl Edward.
+       "Gaussian processes in machine learning."
+       Summer school on machine learning. Springer, Berlin, Heidelberg, 2003
+       <http://www.gaussianprocess.org/gpml/chapters/RW.pdf>`_.
 """
+
+print(__doc__)
+
 # Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
-#
+#          Guillaume Lemaitre <g.lemaitre58@gmail.com>
 # License: BSD 3 clause

-
+# %%
+# Build the dataset
+# -----------------
+#
+# We will derive a dataset from the Mauna Loa Observatory that collected air
+# samples. We are interested in estimating the concentration of CO2 and
+# extrapolate it for further year. First, we load the original dataset available
+# in OpenML.
+from sklearn.datasets import fetch_openml
+
+co2 = fetch_openml(data_id=41187, as_frame=True)
+co2.frame.head()
+
+# %%
+# First, we process the original dataframe to create a date index and select
+# only the CO2 column.
+import pandas as pd
+
+co2_data = co2.frame
+co2_data["date"] = pd.to_datetime(co2_data[["year", "month", "day"]])
+co2_data = co2_data[["date", "co2"]].set_index("date")
+co2_data.head()
+
+# %%
+co2_data.index.min(), co2_data.index.max()
+
+# %%
+# We see that we get CO2 concentration for some days from March, 1958 to
+# December, 2001. We can plot these raw information to have a better
+# understanding.
+import matplotlib.pyplot as plt
+
+co2_data.plot()
+plt.ylabel("CO$_2$ concentration (ppm)")
+_ = plt.title("Raw air samples measurements from the Mauna Loa Observatory")
+
+# %%
+# We will preprocess the dataset by taking a monthly average and drop month
+# for which no measurements were collected. Such a processing will have an
+# smoothing effect on the data.
+co2_data = co2_data.resample("M").mean().dropna(axis="index", how="any")
+co2_data.plot()
+plt.ylabel("Monthly average of CO$_2$ concentration (ppm)")
+_ = plt.title(
+    "Monthly average of air samples measurements\nfrom the Mauna Loa Observatory"
+)
+
+# %%
+# The idea in this example will be to predict the CO2 concentration in function
+# of the date. We are as well interested in extrapolating for upcoming year
+# after 2001.
+#
+# As a first step, we will divide the data and the target to estimate. The data
+# being a date, we will convert it into a numeric.
+X = (co2_data.index.year + co2_data.index.month / 12).to_numpy().reshape(-1, 1)
+y = co2_data["co2"].to_numpy()
+
+# %%
+# Design the proper kernel
+# ------------------------
+#
+# To design the kernel to use with our Gaussian process, we can make some
+# assumption regarding the data at hand. We observe that they have several
+# characteristics: we see a long term rising trend, a pronounced seasonal
+# variation and some smaller irregularities. We can use different appropriate
+# kernel that would capture these features.
+#
+# First, the long term rising trend could be fitted using a radial basis
+# function (RBF) kernel with a large length-scale parameter. The RBF kernel
+# with a large length-scale enforces this component to be smooth. An trending
+# increase is not enforced as to give a degree of freedom to our model. The
+# specific length-scale and the amplitude are free hyperparameters.
+from sklearn.gaussian_process.kernels import RBF
+
+long_term_trend_kernel = 50.0**2 * RBF(length_scale=50.0)
+
+# %%
+# The seasonal variation is explained by the periodic exponential sine squared
+# kernel with a fixed periodicity of 1 year. The length-scale of this periodic
+# component, controlling its smoothness, is a free parameter. In order to allow
+# decaying away from exact periodicity, the product with an RBF kernel is
+# taken. The length-scale of this RBF component controls the decay time and is
+# a further free parameter. This type of kernel is also known as locally
+# periodic kernel.
+from sklearn.gaussian_process.kernels import ExpSineSquared
+
+seasonal_kernel = (
+    2.0**2
+    * RBF(length_scale=100.0)
+    * ExpSineSquared(length_scale=1.0, periodicity=1.0, periodicity_bounds="fixed")
+)
+
+# %%
+# The small irregularities are to be explained by a rational quadratic kernel
+# component, whose length-scale and alpha parameter, which quantifies the
+# diffuseness of the length-scales, are to be determined. A rational quadratic
+# kernel is equivalent to an RBF kernel with several length-scale and will
+# better accommodate the different irregularities.
+from sklearn.gaussian_process.kernels import RationalQuadratic
+
+irregularities_kernel = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)
+
+# %%
+# Finally, the noise in the dataset can be accounted with a kernel consisting
+# of an RBF kernel contribution, which shall explain the correlated noise
+# components such as local weather phenomena, and a white kernel contribution
+# for the white noise. The relative amplitudes and the RBF's length scale are
+# further free parameters.
+from sklearn.gaussian_process.kernels import WhiteKernel
+
+noise_kernel = 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(
+    noise_level=0.1**2, noise_level_bounds=(1e-5, 1e5)
+)
+
+# %%
+# Thus, our final kernel is an addition of all previous kernel.
+co2_kernel = (
+    long_term_trend_kernel + seasonal_kernel + irregularities_kernel + noise_kernel
+)
+co2_kernel
+
+# %%
+# Model fitting and extrapolation
+# -------------------------------
+#
+# Now, we are ready to use a Gaussian process regressor and fit the available
+# data. To follow the example from the literature, we will subtract the mean
+# from the target. We could have used `normalize_y=True`. However, doing so
+# would have also scaled the target (dividing `y` by its standard deviation).
+# Thus, the hyperparameters of the different kernel would have had different
+# meaning since they would not have been expressed in ppm.
+from sklearn.gaussian_process import GaussianProcessRegressor
+
+y_mean = y.mean()
+gaussian_process = GaussianProcessRegressor(kernel=co2_kernel, normalize_y=False)
+gaussian_process.fit(X, y - y_mean)
+
+# %%
+# Now, we will use the Gaussian process to predict on:
+#
+# - training data to inspect the goodness of fit;
+# - future data to see the extrapolation done by the model.
+#
+# Thus, we create synthetic data from 1958 to the current month. In addition,
+# we need to add the subtracted mean computed during training.
+import datetime
 import numpy as np

-from matplotlib import pyplot as plt
-from sklearn.datasets import fetch_openml
-from sklearn.gaussian_process import GaussianProcessRegressor
-from sklearn.gaussian_process.kernels \
-    import RBF, WhiteKernel, RationalQuadratic, ExpSineSquared
-
-print(__doc__)
-
-
-def load_mauna_loa_atmospheric_co2():
-    ml_data = fetch_openml(data_id=41187)
-    months = []
-    ppmv_sums = []
-    counts = []
-
-    y = ml_data.data[:, 0]
-    m = ml_data.data[:, 1]
-    month_float = y + (m - 1) / 12
-    ppmvs = ml_data.target
-
-    for month, ppmv in zip(month_float, ppmvs):
-        if not months or month != months[-1]:
-            months.append(month)
-            ppmv_sums.append(ppmv)
-            counts.append(1)
-        else:
-            # aggregate monthly sum to produce average
-            ppmv_sums[-1] += ppmv
-            counts[-1] += 1
-
-    months = np.asarray(months).reshape(-1, 1)
-    avg_ppmvs = np.asarray(ppmv_sums) / counts
-    return months, avg_ppmvs
-
-
-X, y = load_mauna_loa_atmospheric_co2()
-
-# Kernel with parameters given in GPML book
-k1 = 66.0**2 * RBF(length_scale=67.0)  # long term smooth rising trend
-k2 = 2.4**2 * RBF(length_scale=90.0) \
-    * ExpSineSquared(length_scale=1.3, periodicity=1.0)  # seasonal component
-# medium term irregularity
-k3 = 0.66**2 \
-    * RationalQuadratic(length_scale=1.2, alpha=0.78)
-k4 = 0.18**2 * RBF(length_scale=0.134) \
-    + WhiteKernel(noise_level=0.19**2)  # noise terms
-kernel_gpml = k1 + k2 + k3 + k4
-
-gp = GaussianProcessRegressor(kernel=kernel_gpml, alpha=0,
-                              optimizer=None, normalize_y=True)
-gp.fit(X, y)
-
-print("GPML kernel: %s" % gp.kernel_)
-print("Log-marginal-likelihood: %.3f"
-      % gp.log_marginal_likelihood(gp.kernel_.theta))
-
-# Kernel with optimized parameters
-k1 = 50.0**2 * RBF(length_scale=50.0)  # long term smooth rising trend
-k2 = 2.0**2 * RBF(length_scale=100.0) \
-    * ExpSineSquared(length_scale=1.0, periodicity=1.0,
-                     periodicity_bounds="fixed")  # seasonal component
-# medium term irregularities
-k3 = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)
-k4 = 0.1**2 * RBF(length_scale=0.1) \
-    + WhiteKernel(noise_level=0.1**2,
-                  noise_level_bounds=(1e-3, np.inf))  # noise terms
-kernel = k1 + k2 + k3 + k4
-
-gp = GaussianProcessRegressor(kernel=kernel, alpha=0,
-                              normalize_y=True)
-gp.fit(X, y)
-
-print("\nLearned kernel: %s" % gp.kernel_)
-print("Log-marginal-likelihood: %.3f"
-      % gp.log_marginal_likelihood(gp.kernel_.theta))
-
-X_ = np.linspace(X.min(), X.max() + 30, 1000)[:, np.newaxis]
-y_pred, y_std = gp.predict(X_, return_std=True)
-
-# Illustration
-plt.scatter(X, y, c='k')
-plt.plot(X_, y_pred)
-plt.fill_between(X_[:, 0], y_pred - y_std, y_pred + y_std,
-                 alpha=0.5, color='k')
-plt.xlim(X_.min(), X_.max())
+today = datetime.datetime.now()
+current_month = today.year + today.month / 12
+X_test = np.linspace(start=1958, stop=current_month, num=1_000).reshape(-1, 1)
+mean_y_pred, std_y_pred = gaussian_process.predict(X_test, return_std=True)
+mean_y_pred += y_mean
+
+# %%
+plt.plot(X, y, color="black", linestyle="dashed", label="Measurements")
+plt.plot(X_test, mean_y_pred, color="tab:blue", alpha=0.4, label="Gaussian process")
+plt.fill_between(
+    X_test.ravel(),
+    mean_y_pred - std_y_pred,
+    mean_y_pred + std_y_pred,
+    color="tab:blue",
+    alpha=0.2,
+)
+plt.legend()
 plt.xlabel("Year")
-plt.ylabel(r"CO$_2$ in ppm")
-plt.title(r"Atmospheric CO$_2$ concentration at Mauna Loa")
-plt.tight_layout()
-plt.show()
+plt.ylabel("Monthly average of CO$_2$ concentration (ppm)")
+_ = plt.title(
+    "Monthly average of air samples measurements\nfrom the Mauna Loa Observatory"
+)
+
+# %%
+# Our fitted model is capable to fit previous data properly and extrapolate to
+# future year with confidence.
+#
+# Interpretation of kernel hyperparameters
+# ----------------------------------------
+#
+# Now, we can have a look at the hyperparameters of the kernel.
+gaussian_process.kernel_
+
+# %%
+# Thus, most of the target signal, with the mean subtracted, is explained by a
+# long-term rising trend for ~45 ppm and a length-scale of ~52 years. The
+# periodic component has an amplitude of ~2.6ppm, a decay time of ~90 years and
+# a length-scale of ~1.5. The long decay time indicates that we have a
+# component very close to a seasonal periodicity. The correlated noise has an
+# amplitude of ~0.2 ppm with a length scale of ~0.12 years and a white-noise
+# contribution of ~0.04 ppm. Thus, the overall noise level is very small,
+# indicating that the data can be very well explained by the model.
('examples/gaussian_process', 'plot_gpr_noisy_targets.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -11,103 +11,143 @@
 In both cases, the kernel's parameters are estimated using the maximum
 likelihood principle.

-The figures illustrate the interpolating property of the Gaussian Process
-model as well as its probabilistic nature in the form of a pointwise 95%
-confidence interval.
+The figures illustrate the interpolating property of the Gaussian Process model
+as well as its probabilistic nature in the form of a pointwise 95% confidence
+interval.

-Note that the parameter ``alpha`` is applied as a Tikhonov
-regularization of the assumed covariance between the training points.
+Note that `alpha` is a parameter to control the strength of the Tikhonov
+regularization on the assumed training points' covariance matrix.
 """
-print(__doc__)

 # Author: Vincent Dubourg <vincent.dubourg@gmail.com>
 #         Jake Vanderplas <vanderplas@astro.washington.edu>
-#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>s
+#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>
+#         Guillaume Lemaitre <g.lemaitre58@gmail.com>
 # License: BSD 3 clause

+# %%
+# Dataset generation
+# ------------------
+#
+# We will start by generating a synthetic dataset. The true generative process
+# is defined as :math:`f(x) = x \sin(x)`.
 import numpy as np
-from matplotlib import pyplot as plt

+X = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)
+y = np.squeeze(X * np.sin(X))
+
+# %%
+import matplotlib.pyplot as plt
+
+plt.plot(X, y, label=r"$f(x) = x \sin(x)$", linestyle="dotted")
+plt.legend()
+plt.xlabel("$x$")
+plt.ylabel("$f(x)$")
+_ = plt.title("True generative process")
+
+# %%
+# We will use this dataset in the next experiment to illustrate how Gaussian
+# Process regression is working.
+#
+# Example with noise-free target
+# ------------------------------
+#
+# In this first example, we will use the true generative process without
+# adding any noise. For training the Gaussian Process regression, we will only
+# select few samples.
+rng = np.random.RandomState(1)
+training_indices = rng.choice(np.arange(y.size), size=6, replace=False)
+X_train, y_train = X[training_indices], y[training_indices]
+
+# %%
+# Now, we fit a Gaussian process on these few training data samples. We will
+# use a radial basis function (RBF) kernel and a constant parameter to fit the
+# amplitude.
 from sklearn.gaussian_process import GaussianProcessRegressor
-from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
+from sklearn.gaussian_process.kernels import RBF

-np.random.seed(1)
+kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))
+gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
+gaussian_process.fit(X_train, y_train)
+gaussian_process.kernel_

+# %%
+# After fitting our model, we see that the hyperparameters of the kernel have
+# been optimized. Now, we will use our kernel to compute the mean prediction
+# of the full dataset and plot the 95% confidence interval.
+mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)

-def f(x):
-    """The function to predict."""
-    return x * np.sin(x)
+plt.plot(X, y, label=r"$f(x) = x \sin(x)$", linestyle="dotted")
+plt.scatter(X_train, y_train, label="Observations")
+plt.plot(X, mean_prediction, label="Mean prediction")
+plt.fill_between(
+    X.ravel(),
+    mean_prediction - 1.96 * std_prediction,
+    mean_prediction + 1.96 * std_prediction,
+    alpha=0.5,
+    label=r"95% confidence interval",
+)
+plt.legend()
+plt.xlabel("$x$")
+plt.ylabel("$f(x)$")
+_ = plt.title("Gaussian process regression on noise-free dataset")

-# ----------------------------------------------------------------------
-#  First the noiseless case
-X = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T
+# %%
+# We see that for a prediction made on a data point close to the one from the
+# training set, the 95% confidence has a small amplitude. Whenever a sample
+# falls far from training data, our model's prediction is less accurate and the
+# model prediction is less precise (higher uncertainty).
+#
+# Example with noisy targets
+# --------------------------
+#
+# We can repeat a similar experiment adding an additional noise to the target
+# this time. It will allow seeing the effect of the noise on the fitted model.
+#
+# We add some random Gaussian noise to the target with an arbitrary
+# standard deviation.
+noise_std = 0.75
+y_train_noisy = y_train + rng.normal(loc=0.0, scale=noise_std, size=y_train.shape)

-# Observations
-y = f(X).ravel()
+# %%
+# We create a similar Gaussian process model. In addition to the kernel, this
+# time, we specify the parameter `alpha` which can be interpreted as the
+# variance of a Gaussian noise.
+gaussian_process = GaussianProcessRegressor(
+    kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=9
+)
+gaussian_process.fit(X_train, y_train_noisy)
+mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)

-# Mesh the input space for evaluations of the real function, the prediction and
-# its MSE
-x = np.atleast_2d(np.linspace(0, 10, 1000)).T
+# %%
+# Let's plot the mean prediction and the uncertainty region as before.
+plt.plot(X, y, label=r"$f(x) = x \sin(x)$", linestyle="dotted")
+plt.errorbar(
+    X_train,
+    y_train_noisy,
+    noise_std,
+    linestyle="None",
+    color="tab:blue",
+    marker=".",
+    markersize=10,
+    label="Observations",
+)
+plt.plot(X, mean_prediction, label="Mean prediction")
+plt.fill_between(
+    X.ravel(),
+    mean_prediction - 1.96 * std_prediction,
+    mean_prediction + 1.96 * std_prediction,
+    color="tab:orange",
+    alpha=0.5,
+    label=r"95% confidence interval",
+)
+plt.legend()
+plt.xlabel("$x$")
+plt.ylabel("$f(x)$")
+_ = plt.title("Gaussian process regression on a noisy dataset")

-# Instantiate a Gaussian Process model
-kernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))
-gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)
-
-# Fit to data using Maximum Likelihood Estimation of the parameters
-gp.fit(X, y)
-
-# Make the prediction on the meshed x-axis (ask for MSE as well)
-y_pred, sigma = gp.predict(x, return_std=True)
-
-# Plot the function, the prediction and the 95% confidence interval based on
-# the MSE
-plt.figure()
-plt.plot(x, f(x), 'r:', label=r'$f(x) = x\,\sin(x)$')
-plt.plot(X, y, 'r.', markersize=10, label='Observations')
-plt.plot(x, y_pred, 'b-', label='Prediction')
-plt.fill(np.concatenate([x, x[::-1]]),
-         np.concatenate([y_pred - 1.9600 * sigma,
-                        (y_pred + 1.9600 * sigma)[::-1]]),
-         alpha=.5, fc='b', ec='None', label='95% confidence interval')
-plt.xlabel('$x$')
-plt.ylabel('$f(x)$')
-plt.ylim(-10, 20)
-plt.legend(loc='upper left')
-
-# ----------------------------------------------------------------------
-# now the noisy case
-X = np.linspace(0.1, 9.9, 20)
-X = np.atleast_2d(X).T
-
-# Observations and noise
-y = f(X).ravel()
-dy = 0.5 + 1.0 * np.random.random(y.shape)
-noise = np.random.normal(0, dy)
-y += noise
-
-# Instantiate a Gaussian Process model
-gp = GaussianProcessRegressor(kernel=kernel, alpha=dy ** 2,
-                              n_restarts_optimizer=10)
-
-# Fit to data using Maximum Likelihood Estimation of the parameters
-gp.fit(X, y)
-
-# Make the prediction on the meshed x-axis (ask for MSE as well)
-y_pred, sigma = gp.predict(x, return_std=True)
-
-# Plot the function, the prediction and the 95% confidence interval based on
-# the MSE
-plt.figure()
-plt.plot(x, f(x), 'r:', label=r'$f(x) = x\,\sin(x)$')
-plt.errorbar(X.ravel(), y, dy, fmt='r.', markersize=10, label='Observations')
-plt.plot(x, y_pred, 'b-', label='Prediction')
-plt.fill(np.concatenate([x, x[::-1]]),
-         np.concatenate([y_pred - 1.9600 * sigma,
-                        (y_pred + 1.9600 * sigma)[::-1]]),
-         alpha=.5, fc='b', ec='None', label='95% confidence interval')
-plt.xlabel('$x$')
-plt.ylabel('$f(x)$')
-plt.ylim(-10, 20)
-plt.legend(loc='upper left')
-
-plt.show()
+# %%
+# The noise affects the predictions close to the training samples: the
+# predictive uncertainty near to the training samples is larger because we
+# explicitly model a given level target noise independent of the input
+# variable.
('examples/compose', 'plot_digits_pipe.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =========================================================
 Pipelining: chaining a PCA and a logistic regression
@@ -12,13 +10,10 @@
 We use a GridSearchCV to set the dimensionality of the PCA

 """
-print(__doc__)
-

 # Code source: Gaël Varoquaux
 # Modified for documentation by Jaques Grobler
 # License: BSD 3 clause
-

 import numpy as np
 import matplotlib.pyplot as plt
@@ -26,28 +21,28 @@

 from sklearn import datasets
 from sklearn.decomposition import PCA
-from sklearn.linear_model import SGDClassifier
+from sklearn.linear_model import LogisticRegression
 from sklearn.pipeline import Pipeline
 from sklearn.model_selection import GridSearchCV
-
+from sklearn.preprocessing import StandardScaler

 # Define a pipeline to search for the best combination of PCA truncation
 # and classifier regularization.
-logistic = SGDClassifier(loss='log', penalty='l2', early_stopping=True,
-                         max_iter=10000, tol=1e-5, random_state=0)
 pca = PCA()
-pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])
+# Define a Standard Scaler to normalize inputs
+scaler = StandardScaler()

-digits = datasets.load_digits()
-X_digits = digits.data
-y_digits = digits.target
+# set the tolerance to a large value to make the example faster
+logistic = LogisticRegression(max_iter=10000, tol=0.1)
+pipe = Pipeline(steps=[("scaler", scaler), ("pca", pca), ("logistic", logistic)])

+X_digits, y_digits = datasets.load_digits(return_X_y=True)
 # Parameters of pipelines can be set using ‘__’ separated parameter names:
 param_grid = {
-    'pca__n_components': [5, 20, 30, 40, 50, 64],
-    'logistic__alpha': np.logspace(-4, 4, 5),
+    "pca__n_components": [5, 15, 30, 45, 60],
+    "logistic__C": np.logspace(-4, 4, 4),
 }
-search = GridSearchCV(pipe, param_grid, iid=False, cv=5)
+search = GridSearchCV(pipe, param_grid, n_jobs=2)
 search.fit(X_digits, y_digits)
 print("Best parameter (CV score=%0.3f):" % search.best_score_)
 print(search.best_params_)
@@ -56,23 +51,32 @@
 pca.fit(X_digits)

 fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))
-ax0.plot(pca.explained_variance_ratio_, linewidth=2)
-ax0.set_ylabel('PCA explained variance')
+ax0.plot(
+    np.arange(1, pca.n_components_ + 1), pca.explained_variance_ratio_, "+", linewidth=2
+)
+ax0.set_ylabel("PCA explained variance ratio")

-ax0.axvline(search.best_estimator_.named_steps['pca'].n_components,
-            linestyle=':', label='n_components chosen')
+ax0.axvline(
+    search.best_estimator_.named_steps["pca"].n_components,
+    linestyle=":",
+    label="n_components chosen",
+)
 ax0.legend(prop=dict(size=12))

 # For each number of components, find the best classifier results
 results = pd.DataFrame(search.cv_results_)
-components_col = 'param_pca__n_components'
+components_col = "param_pca__n_components"
 best_clfs = results.groupby(components_col).apply(
-    lambda g: g.nlargest(1, 'mean_test_score'))
+    lambda g: g.nlargest(1, "mean_test_score")
+)

-best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',
-               legend=False, ax=ax1)
-ax1.set_ylabel('Classification accuracy (val)')
-ax1.set_xlabel('n_components')
+best_clfs.plot(
+    x=components_col, y="mean_test_score", yerr="std_test_score", legend=False, ax=ax1
+)
+ax1.set_ylabel("Classification accuracy (val)")
+ax1.set_xlabel("n_components")
+
+plt.xlim(-1, 70)

 plt.tight_layout()
 plt.show()
('examples/compose', 'plot_feature_union.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -13,6 +13,7 @@

 The combination used in this example is not particularly helpful on this
 dataset and is only used to illustrate the usage of FeatureUnion.
+
 """

 # Author: Andreas Mueller <amueller@ais.uni-bonn.de>
@@ -33,7 +34,7 @@
 # This dataset is way too high-dimensional. Better do PCA:
 pca = PCA(n_components=2)

-# Maybe some original features where good, too?
+# Maybe some original features were good, too?
 selection = SelectKBest(k=1)

 # Build estimator from PCA and Univariate selection:
@@ -50,10 +51,12 @@

 pipeline = Pipeline([("features", combined_features), ("svm", svm)])

-param_grid = dict(features__pca__n_components=[1, 2, 3],
-                  features__univ_select__k=[1, 2],
-                  svm__C=[0.1, 1, 10])
+param_grid = dict(
+    features__pca__n_components=[1, 2, 3],
+    features__univ_select__k=[1, 2],
+    svm__C=[0.1, 1, 10],
+)

-grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=10)
+grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)
 grid_search.fit(X, y)
 print(grid_search.best_estimator_)
('examples/compose', 'plot_column_transformer.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,24 +3,19 @@
 Column Transformer with Heterogeneous Data Sources
 ==================================================

-Datasets can often contain components of that require different feature
-extraction and processing pipelines.  This scenario might occur when:
+Datasets can often contain components that require different feature
+extraction and processing pipelines. This scenario might occur when:

-1. Your dataset consists of heterogeneous data types (e.g. raster images and
-   text captions)
-2. Your dataset is stored in a Pandas DataFrame and different columns
+1. your dataset consists of heterogeneous data types (e.g. raster images and
+   text captions),
+2. your dataset is stored in a :class:`pandas.DataFrame` and different columns
    require different processing pipelines.

 This example demonstrates how to use
-:class:`sklearn.compose.ColumnTransformer` on a dataset containing
-different types of features.  We use the 20-newsgroups dataset and compute
-standard bag-of-words features for the subject line and body in separate
-pipelines as well as ad hoc features on the body. We combine them (with
-weights) using a ColumnTransformer and finally train a classifier on the
-combined set of features.
+:class:`~sklearn.compose.ColumnTransformer` on a dataset containing
+different types of features. The choice of features is not particularly
+helpful, but serves to illustrate the technique.

-The choice of features is not particularly helpful, but serves to illustrate
-the technique.
 """

 # Author: Matt Terry <matt.terry@gmail.com>
@@ -29,10 +24,8 @@

 import numpy as np

-from sklearn.base import BaseEstimator, TransformerMixin
+from sklearn.preprocessing import FunctionTransformer
 from sklearn.datasets import fetch_20newsgroups
-from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer
-from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting
 from sklearn.decomposition import TruncatedSVD
 from sklearn.feature_extraction import DictVectorizer
 from sklearn.feature_extraction.text import TfidfVectorizer
@@ -41,95 +34,154 @@
 from sklearn.compose import ColumnTransformer
 from sklearn.svm import LinearSVC

+##############################################################################
+# 20 newsgroups dataset
+# ---------------------
+#
+# We will use the :ref:`20 newsgroups dataset <20newsgroups_dataset>`, which
+# comprises posts from newsgroups on 20 topics. This dataset is split
+# into train and test subsets based on messages posted before and after
+# a specific date. We will only use posts from 2 categories to speed up running
+# time.

-class TextStats(BaseEstimator, TransformerMixin):
-    """Extract features from each document for DictVectorizer"""
+categories = ["sci.med", "sci.space"]
+X_train, y_train = fetch_20newsgroups(
+    random_state=1,
+    subset="train",
+    categories=categories,
+    remove=("footers", "quotes"),
+    return_X_y=True,
+)
+X_test, y_test = fetch_20newsgroups(
+    random_state=1,
+    subset="test",
+    categories=categories,
+    remove=("footers", "quotes"),
+    return_X_y=True,
+)

-    def fit(self, x, y=None):
-        return self
+##############################################################################
+# Each feature comprises meta information about that post, such as the subject,
+# and the body of the news post.

-    def transform(self, posts):
-        return [{'length': len(text),
-                 'num_sentences': text.count('.')}
-                for text in posts]
+print(X_train[0])
+
+##############################################################################
+# Creating transformers
+# ---------------------
+#
+# First, we would like a transformer that extracts the subject and
+# body of each post. Since this is a stateless transformation (does not
+# require state information from training data), we can define a function that
+# performs the data transformation then use
+# :class:`~sklearn.preprocessing.FunctionTransformer` to create a scikit-learn
+# transformer.


-class SubjectBodyExtractor(BaseEstimator, TransformerMixin):
-    """Extract the subject & body from a usenet post in a single pass.
+def subject_body_extractor(posts):
+    # construct object dtype array with two columns
+    # first column = 'subject' and second column = 'body'
+    features = np.empty(shape=(len(posts), 2), dtype=object)
+    for i, text in enumerate(posts):
+        # temporary variable `_` stores '\n\n'
+        headers, _, body = text.partition("\n\n")
+        # store body text in second column
+        features[i, 1] = body

-    Takes a sequence of strings and produces a dict of sequences.  Keys are
-    `subject` and `body`.
-    """
-    def fit(self, x, y=None):
-        return self
+        prefix = "Subject:"
+        sub = ""
+        # save text after 'Subject:' in first column
+        for line in headers.split("\n"):
+            if line.startswith(prefix):
+                sub = line[len(prefix) :]
+                break
+        features[i, 0] = sub

-    def transform(self, posts):
-        # construct object dtype array with two columns
-        # first column = 'subject' and second column = 'body'
-        features = np.empty(shape=(len(posts), 2), dtype=object)
-        for i, text in enumerate(posts):
-            headers, _, bod = text.partition('\n\n')
-            bod = strip_newsgroup_footer(bod)
-            bod = strip_newsgroup_quoting(bod)
-            features[i, 1] = bod
-
-            prefix = 'Subject:'
-            sub = ''
-            for line in headers.split('\n'):
-                if line.startswith(prefix):
-                    sub = line[len(prefix):]
-                    break
-            features[i, 0] = sub
-
-        return features
+    return features


-pipeline = Pipeline([
-    # Extract the subject & body
-    ('subjectbody', SubjectBodyExtractor()),
+subject_body_transformer = FunctionTransformer(subject_body_extractor)

-    # Use ColumnTransformer to combine the features from subject and body
-    ('union', ColumnTransformer(
-        [
-            # Pulling features from the post's subject line (first column)
-            ('subject', TfidfVectorizer(min_df=50), 0),
+##############################################################################
+# We will also create a transformer that extracts the
+# length of the text and the number of sentences.

-            # Pipeline for standard bag-of-words model for body (second column)
-            ('body_bow', Pipeline([
-                ('tfidf', TfidfVectorizer()),
-                ('best', TruncatedSVD(n_components=50)),
-            ]), 1),

-            # Pipeline for pulling ad hoc features from post's body
-            ('body_stats', Pipeline([
-                ('stats', TextStats()),  # returns a list of dicts
-                ('vect', DictVectorizer()),  # list of dicts -> feature matrix
-            ]), 1),
-        ],
+def text_stats(posts):
+    return [{"length": len(text), "num_sentences": text.count(".")} for text in posts]

-        # weight components in ColumnTransformer
-        transformer_weights={
-            'subject': 0.8,
-            'body_bow': 0.5,
-            'body_stats': 1.0,
-        }
-    )),

-    # Use a SVC classifier on the combined features
-    ('svc', LinearSVC()),
-], verbose=True)
+text_stats_transformer = FunctionTransformer(text_stats)

-# limit the list of categories to make running this example faster.
-categories = ['alt.atheism', 'talk.religion.misc']
-train = fetch_20newsgroups(random_state=1,
-                           subset='train',
-                           categories=categories,
-                           )
-test = fetch_20newsgroups(random_state=1,
-                          subset='test',
-                          categories=categories,
-                          )
+##############################################################################
+# Classification pipeline
+# -----------------------
+#
+# The pipeline below extracts the subject and body from each post using
+# ``SubjectBodyExtractor``, producing a (n_samples, 2) array. This array is
+# then used to compute standard bag-of-words features for the subject and body
+# as well as text length and number of sentences on the body, using
+# ``ColumnTransformer``. We combine them, with weights, then train a
+# classifier on the combined set of features.

-pipeline.fit(train.data, train.target)
-y = pipeline.predict(test.data)
-print(classification_report(y, test.target))
+pipeline = Pipeline(
+    [
+        # Extract subject & body
+        ("subjectbody", subject_body_transformer),
+        # Use ColumnTransformer to combine the subject and body features
+        (
+            "union",
+            ColumnTransformer(
+                [
+                    # bag-of-words for subject (col 0)
+                    ("subject", TfidfVectorizer(min_df=50), 0),
+                    # bag-of-words with decomposition for body (col 1)
+                    (
+                        "body_bow",
+                        Pipeline(
+                            [
+                                ("tfidf", TfidfVectorizer()),
+                                ("best", TruncatedSVD(n_components=50)),
+                            ]
+                        ),
+                        1,
+                    ),
+                    # Pipeline for pulling text stats from post's body
+                    (
+                        "body_stats",
+                        Pipeline(
+                            [
+                                (
+                                    "stats",
+                                    text_stats_transformer,
+                                ),  # returns a list of dicts
+                                (
+                                    "vect",
+                                    DictVectorizer(),
+                                ),  # list of dicts -> feature matrix
+                            ]
+                        ),
+                        1,
+                    ),
+                ],
+                # weight above ColumnTransformer features
+                transformer_weights={
+                    "subject": 0.8,
+                    "body_bow": 0.5,
+                    "body_stats": 1.0,
+                },
+            ),
+        ),
+        # Use a SVC classifier on the combined features
+        ("svc", LinearSVC(dual=False)),
+    ],
+    verbose=True,
+)
+
+##############################################################################
+# Finally, we fit our pipeline on the training data and use it to predict
+# topics for ``X_test``. Performance metrics of our pipeline are then printed.
+
+pipeline.fit(X_train, y_train)
+y_pred = pipeline.predict(X_test)
+print("Classification report:\n\n{}".format(classification_report(y_test, y_pred)))
('examples/compose', 'plot_transformed_target.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,33 +1,22 @@
-#!/usr/bin/env python
 # -*- coding: utf-8 -*-
-
 """
 ======================================================
 Effect of transforming the targets in regression model
 ======================================================

-In this example, we give an overview of the
-:class:`sklearn.compose.TransformedTargetRegressor`. Two examples
-illustrate the benefit of transforming the targets before learning a linear
+In this example, we give an overview of
+:class:`~sklearn.compose.TransformedTargetRegressor`. We use two examples
+to illustrate the benefit of transforming the targets before learning a linear
 regression model. The first example uses synthetic data while the second
-example is based on the Boston housing data set.
+example is based on the Ames housing data set.

 """

 # Author: Guillaume Lemaitre <guillaume.lemaitre@inria.fr>
 # License: BSD 3 clause

-
 import numpy as np
-import matplotlib
 import matplotlib.pyplot as plt
-from distutils.version import LooseVersion
-
-print(__doc__)
-
-###############################################################################
-# Synthetic example
-###############################################################################

 from sklearn.datasets import make_regression
 from sklearn.model_selection import train_test_split
@@ -35,170 +24,216 @@
 from sklearn.compose import TransformedTargetRegressor
 from sklearn.metrics import median_absolute_error, r2_score

-
-# `normed` is being deprecated in favor of `density` in histograms
-if LooseVersion(matplotlib.__version__) >= '2.1':
-    density_param = {'density': True}
-else:
-    density_param = {'normed': True}
-
-###############################################################################
-# A synthetic random regression problem is generated. The targets ``y`` are
-# modified by: (i) translating all targets such that all entries are
-# non-negative and (ii) applying an exponential function to obtain non-linear
-# targets which cannot be fitted using a simple linear model.
+# %%
+# Synthetic example
+##############################################################################
+
+# %%
+# A synthetic random regression dataset is generated. The targets ``y`` are
+# modified by:
+#
+#   1. translating all targets such that all entries are
+#      non-negative (by adding the absolute value of the lowest ``y``) and
+#   2. applying an exponential function to obtain non-linear
+#      targets which cannot be fitted using a simple linear model.
 #
 # Therefore, a logarithmic (`np.log1p`) and an exponential function
 # (`np.expm1`) will be used to transform the targets before training a linear
 # regression model and using it for prediction.

 X, y = make_regression(n_samples=10000, noise=100, random_state=0)
-y = np.exp((y + abs(y.min())) / 200)
+y = np.expm1((y + abs(y.min())) / 200)
 y_trans = np.log1p(y)

-###############################################################################
-# The following illustrate the probability density functions of the target
+# %%
+# Below we plot the probability density functions of the target
 # before and after applying the logarithmic functions.

 f, (ax0, ax1) = plt.subplots(1, 2)

-ax0.hist(y, bins=100, **density_param)
+ax0.hist(y, bins=100, density=True)
 ax0.set_xlim([0, 2000])
-ax0.set_ylabel('Probability')
-ax0.set_xlabel('Target')
-ax0.set_title('Target distribution')
-
-ax1.hist(y_trans, bins=100, **density_param)
-ax1.set_ylabel('Probability')
-ax1.set_xlabel('Target')
-ax1.set_title('Transformed target distribution')
-
-f.suptitle("Synthetic data", y=0.035)
+ax0.set_ylabel("Probability")
+ax0.set_xlabel("Target")
+ax0.set_title("Target distribution")
+
+ax1.hist(y_trans, bins=100, density=True)
+ax1.set_ylabel("Probability")
+ax1.set_xlabel("Target")
+ax1.set_title("Transformed target distribution")
+
+f.suptitle("Synthetic data", y=0.06, x=0.53)
 f.tight_layout(rect=[0.05, 0.05, 0.95, 0.95])

 X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

-###############################################################################
+# %%
 # At first, a linear model will be applied on the original targets. Due to the
-# non-linearity, the model trained will not be precise during the
+# non-linearity, the model trained will not be precise during
 # prediction. Subsequently, a logarithmic function is used to linearize the
 # targets, allowing better prediction even with a similar linear model as
 # reported by the median absolute error (MAE).

 f, (ax0, ax1) = plt.subplots(1, 2, sharey=True)
-
+# Use linear model
 regr = RidgeCV()
 regr.fit(X_train, y_train)
 y_pred = regr.predict(X_test)
-
+# Plot results
 ax0.scatter(y_test, y_pred)
-ax0.plot([0, 2000], [0, 2000], '--k')
-ax0.set_ylabel('Target predicted')
-ax0.set_xlabel('True Target')
-ax0.set_title('Ridge regression \n without target transformation')
-ax0.text(100, 1750, r'$R^2$=%.2f, MAE=%.2f' % (
-    r2_score(y_test, y_pred), median_absolute_error(y_test, y_pred)))
+ax0.plot([0, 2000], [0, 2000], "--k")
+ax0.set_ylabel("Target predicted")
+ax0.set_xlabel("True Target")
+ax0.set_title("Ridge regression \n without target transformation")
+ax0.text(
+    100,
+    1750,
+    r"$R^2$=%.2f, MAE=%.2f"
+    % (r2_score(y_test, y_pred), median_absolute_error(y_test, y_pred)),
+)
 ax0.set_xlim([0, 2000])
 ax0.set_ylim([0, 2000])
-
-regr_trans = TransformedTargetRegressor(regressor=RidgeCV(),
-                                        func=np.log1p,
-                                        inverse_func=np.expm1)
+# Transform targets and use same linear model
+regr_trans = TransformedTargetRegressor(
+    regressor=RidgeCV(), func=np.log1p, inverse_func=np.expm1
+)
 regr_trans.fit(X_train, y_train)
 y_pred = regr_trans.predict(X_test)

 ax1.scatter(y_test, y_pred)
-ax1.plot([0, 2000], [0, 2000], '--k')
-ax1.set_ylabel('Target predicted')
-ax1.set_xlabel('True Target')
-ax1.set_title('Ridge regression \n with target transformation')
-ax1.text(100, 1750, r'$R^2$=%.2f, MAE=%.2f' % (
-    r2_score(y_test, y_pred), median_absolute_error(y_test, y_pred)))
+ax1.plot([0, 2000], [0, 2000], "--k")
+ax1.set_ylabel("Target predicted")
+ax1.set_xlabel("True Target")
+ax1.set_title("Ridge regression \n with target transformation")
+ax1.text(
+    100,
+    1750,
+    r"$R^2$=%.2f, MAE=%.2f"
+    % (r2_score(y_test, y_pred), median_absolute_error(y_test, y_pred)),
+)
 ax1.set_xlim([0, 2000])
 ax1.set_ylim([0, 2000])

 f.suptitle("Synthetic data", y=0.035)
 f.tight_layout(rect=[0.05, 0.05, 0.95, 0.95])

-###############################################################################
+# %%
 # Real-world data set
 ###############################################################################
-
-###############################################################################
-# In a similar manner, the boston housing data set is used to show the impact
+#
+# In a similar manner, the Ames housing data set is used to show the impact
 # of transforming the targets before learning a model. In this example, the
-# targets to be predicted corresponds to the weighted distances to the five
-# Boston employment centers.
-
-from sklearn.datasets import load_boston
+# target to be predicted is the selling price of each house.
+
+from sklearn.datasets import fetch_openml
 from sklearn.preprocessing import QuantileTransformer, quantile_transform

-dataset = load_boston()
-target = np.array(dataset.feature_names) == "DIS"
-X = dataset.data[:, np.logical_not(target)]
-y = dataset.data[:, target].squeeze()
-y_trans = quantile_transform(dataset.data[:, target],
-                             output_distribution='normal').squeeze()
-
-###############################################################################
-# A :class:`sklearn.preprocessing.QuantileTransformer` is used such that the
-# targets follows a normal distribution before applying a
-# :class:`sklearn.linear_model.RidgeCV` model.
+ames = fetch_openml(name="house_prices", as_frame=True)
+# Keep only numeric columns
+X = ames.data.select_dtypes(np.number)
+# Remove columns with NaN or Inf values
+X = X.drop(columns=["LotFrontage", "GarageYrBlt", "MasVnrArea"])
+y = ames.target
+y_trans = quantile_transform(
+    y.to_frame(), n_quantiles=900, output_distribution="normal", copy=True
+).squeeze()
+# %%
+# A :class:`~sklearn.preprocessing.QuantileTransformer` is used to normalize
+# the target distribution before applying a
+# :class:`~sklearn.linear_model.RidgeCV` model.

 f, (ax0, ax1) = plt.subplots(1, 2)

-ax0.hist(y, bins=100, **density_param)
-ax0.set_ylabel('Probability')
-ax0.set_xlabel('Target')
-ax0.set_title('Target distribution')
-
-ax1.hist(y_trans, bins=100, **density_param)
-ax1.set_ylabel('Probability')
-ax1.set_xlabel('Target')
-ax1.set_title('Transformed target distribution')
-
-f.suptitle("Boston housing data: distance to employment centers", y=0.035)
+ax0.hist(y, bins=100, density=True)
+ax0.set_ylabel("Probability")
+ax0.set_xlabel("Target")
+ax0.text(s="Target distribution", x=1.2e5, y=9.8e-6, fontsize=12)
+ax0.ticklabel_format(axis="both", style="sci", scilimits=(0, 0))
+
+ax1.hist(y_trans, bins=100, density=True)
+ax1.set_ylabel("Probability")
+ax1.set_xlabel("Target")
+ax1.text(s="Transformed target distribution", x=-6.8, y=0.479, fontsize=12)
+
+f.suptitle("Ames housing data: selling price", y=0.04)
 f.tight_layout(rect=[0.05, 0.05, 0.95, 0.95])

 X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

-###############################################################################
+# %%
 # The effect of the transformer is weaker than on the synthetic data. However,
-# the transform induces a decrease of the MAE.
-
-f, (ax0, ax1) = plt.subplots(1, 2, sharey=True)
+# the transformation results in an increase in :math:`R^2` and large decrease
+# of the MAE. The residual plot (predicted target - true target vs predicted
+# target) without target transformation takes on a curved, 'reverse smile'
+# shape due to residual values that vary depending on the value of predicted
+# target. With target transformation, the shape is more linear indicating
+# better model fit.
+
+f, (ax0, ax1) = plt.subplots(2, 2, sharey="row", figsize=(6.5, 8))

 regr = RidgeCV()
 regr.fit(X_train, y_train)
 y_pred = regr.predict(X_test)

-ax0.scatter(y_test, y_pred)
-ax0.plot([0, 10], [0, 10], '--k')
-ax0.set_ylabel('Target predicted')
-ax0.set_xlabel('True Target')
-ax0.set_title('Ridge regression \n without target transformation')
-ax0.text(1, 9, r'$R^2$=%.2f, MAE=%.2f' % (
-    r2_score(y_test, y_pred), median_absolute_error(y_test, y_pred)))
-ax0.set_xlim([0, 10])
-ax0.set_ylim([0, 10])
+ax0[0].scatter(y_pred, y_test, s=8)
+ax0[0].plot([0, 7e5], [0, 7e5], "--k")
+ax0[0].set_ylabel("True target")
+ax0[0].set_xlabel("Predicted target")
+ax0[0].text(
+    s="Ridge regression \n without target transformation",
+    x=-5e4,
+    y=8e5,
+    fontsize=12,
+    multialignment="center",
+)
+ax0[0].text(
+    3e4,
+    64e4,
+    r"$R^2$=%.2f, MAE=%.2f"
+    % (r2_score(y_test, y_pred), median_absolute_error(y_test, y_pred)),
+)
+ax0[0].set_xlim([0, 7e5])
+ax0[0].set_ylim([0, 7e5])
+ax0[0].ticklabel_format(axis="both", style="sci", scilimits=(0, 0))
+
+ax1[0].scatter(y_pred, (y_pred - y_test), s=8)
+ax1[0].set_ylabel("Residual")
+ax1[0].set_xlabel("Predicted target")
+ax1[0].ticklabel_format(axis="both", style="sci", scilimits=(0, 0))

 regr_trans = TransformedTargetRegressor(
     regressor=RidgeCV(),
-    transformer=QuantileTransformer(output_distribution='normal'))
+    transformer=QuantileTransformer(n_quantiles=900, output_distribution="normal"),
+)
 regr_trans.fit(X_train, y_train)
 y_pred = regr_trans.predict(X_test)

-ax1.scatter(y_test, y_pred)
-ax1.plot([0, 10], [0, 10], '--k')
-ax1.set_ylabel('Target predicted')
-ax1.set_xlabel('True Target')
-ax1.set_title('Ridge regression \n with target transformation')
-ax1.text(1, 9, r'$R^2$=%.2f, MAE=%.2f' % (
-    r2_score(y_test, y_pred), median_absolute_error(y_test, y_pred)))
-ax1.set_xlim([0, 10])
-ax1.set_ylim([0, 10])
-
-f.suptitle("Boston housing data: distance to employment centers", y=0.035)
-f.tight_layout(rect=[0.05, 0.05, 0.95, 0.95])
+ax0[1].scatter(y_pred, y_test, s=8)
+ax0[1].plot([0, 7e5], [0, 7e5], "--k")
+ax0[1].set_ylabel("True target")
+ax0[1].set_xlabel("Predicted target")
+ax0[1].text(
+    s="Ridge regression \n with target transformation",
+    x=-5e4,
+    y=8e5,
+    fontsize=12,
+    multialignment="center",
+)
+ax0[1].text(
+    3e4,
+    64e4,
+    r"$R^2$=%.2f, MAE=%.2f"
+    % (r2_score(y_test, y_pred), median_absolute_error(y_test, y_pred)),
+)
+ax0[1].set_xlim([0, 7e5])
+ax0[1].set_ylim([0, 7e5])
+ax0[1].ticklabel_format(axis="both", style="sci", scilimits=(0, 0))
+
+ax1[1].scatter(y_pred, (y_pred - y_test), s=8)
+ax1[1].set_ylabel("Residual")
+ax1[1].set_xlabel("Predicted target")
+ax1[1].ticklabel_format(axis="both", style="sci", scilimits=(0, 0))
+
+f.suptitle("Ames housing data: selling price", y=0.035)

 plt.show()
('examples/compose', 'plot_column_transformer_mixed_types.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,31 +3,36 @@
 Column Transformer with Mixed Types
 ===================================

-This example illustrates how to apply different preprocessing and
-feature extraction pipelines to different subsets of features,
-using :class:`sklearn.compose.ColumnTransformer`.
-This is particularly handy for the case of datasets that contain
-heterogeneous data types, since we may want to scale the
-numeric features and one-hot encode the categorical ones.
-
-In this example, the numeric data is standard-scaled after
-mean-imputation, while the categorical data is one-hot
-encoded after imputing missing values with a new category
-(``'missing'``).
-
-Finally, the preprocessing pipeline is integrated in a
-full prediction pipeline using :class:`sklearn.pipeline.Pipeline`,
-together with a simple classification model.
+.. currentmodule:: sklearn
+
+This example illustrates how to apply different preprocessing and feature
+extraction pipelines to different subsets of features, using
+:class:`~compose.ColumnTransformer`. This is particularly handy for the
+case of datasets that contain heterogeneous data types, since we may want to
+scale the numeric features and one-hot encode the categorical ones.
+
+In this example, the numeric data is standard-scaled after mean-imputation. The
+categorical data is one-hot encoded via ``OneHotEncoder``, which
+creates a new category for missing values.
+
+In addition, we show two different ways to dispatch the columns to the
+particular pre-processor: by column names and by column data types.
+
+Finally, the preprocessing pipeline is integrated in a full prediction pipeline
+using :class:`~pipeline.Pipeline`, together with a simple classification
+model.
+
 """

 # Author: Pedro Morales <part.morales@gmail.com>
 #
 # License: BSD 3 clause

-import pandas as pd
+# %%
 import numpy as np

 from sklearn.compose import ColumnTransformer
+from sklearn.datasets import fetch_openml
 from sklearn.pipeline import Pipeline
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import StandardScaler, OneHotEncoder
@@ -36,68 +41,182 @@

 np.random.seed(0)

-# Read data from Titanic dataset.
-titanic_url = ('https://raw.githubusercontent.com/amueller/'
-               'scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv')
-data = pd.read_csv(titanic_url)
-
+# %%
+# Load data from https://www.openml.org/d/40945
+X, y = fetch_openml("titanic", version=1, as_frame=True, return_X_y=True)
+
+# Alternatively X and y can be obtained directly from the frame attribute:
+# X = titanic.frame.drop('survived', axis=1)
+# y = titanic.frame['survived']
+
+# %%
+# Use ``ColumnTransformer`` by selecting column by names
+#
 # We will train our classifier with the following features:
+#
 # Numeric Features:
-# - age: float.
-# - fare: float.
+#
+# * ``age``: float;
+# * ``fare``: float.
+#
 # Categorical Features:
-# - embarked: categories encoded as strings {'C', 'S', 'Q'}.
-# - sex: categories encoded as strings {'female', 'male'}.
-# - pclass: ordinal integers {1, 2, 3}.
-
+#
+# * ``embarked``: categories encoded as strings ``{'C', 'S', 'Q'}``;
+# * ``sex``: categories encoded as strings ``{'female', 'male'}``;
+# * ``pclass``: ordinal integers ``{1, 2, 3}``.
+#
 # We create the preprocessing pipelines for both numeric and categorical data.
-numeric_features = ['age', 'fare']
-numeric_transformer = Pipeline(steps=[
-    ('imputer', SimpleImputer(strategy='median')),
-    ('scaler', StandardScaler())])
-
-categorical_features = ['embarked', 'sex', 'pclass']
-categorical_transformer = Pipeline(steps=[
-    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
-    ('onehot', OneHotEncoder(handle_unknown='ignore'))])
+# Note that ``pclass`` could either be treated as a categorical or numeric
+# feature.
+
+numeric_features = ["age", "fare"]
+numeric_transformer = Pipeline(
+    steps=[("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler())]
+)
+
+categorical_features = ["embarked", "sex", "pclass"]
+categorical_transformer = OneHotEncoder(handle_unknown="ignore")

 preprocessor = ColumnTransformer(
     transformers=[
-        ('num', numeric_transformer, numeric_features),
-        ('cat', categorical_transformer, categorical_features)])
-
+        ("num", numeric_transformer, numeric_features),
+        ("cat", categorical_transformer, categorical_features),
+    ]
+)
+
+# %%
 # Append classifier to preprocessing pipeline.
 # Now we have a full prediction pipeline.
-clf = Pipeline(steps=[('preprocessor', preprocessor),
-                      ('classifier', LogisticRegression(solver='lbfgs'))])
-
-X = data.drop('survived', axis=1)
-y = data['survived']
-
-X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
+clf = Pipeline(
+    steps=[("preprocessor", preprocessor), ("classifier", LogisticRegression())]
+)
+
+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

 clf.fit(X_train, y_train)
 print("model score: %.3f" % clf.score(X_test, y_test))

-
-###############################################################################
+# %%
+# HTML representation of ``Pipeline`` (display diagram)
+#
+# When the ``Pipeline`` is printed out in a jupyter notebook an HTML
+# representation of the estimator is displayed:
+clf
+
+# %%
+# Use ``ColumnTransformer`` by selecting column by data types
+#
+# When dealing with a cleaned dataset, the preprocessing can be automatic by
+# using the data types of the column to decide whether to treat a column as a
+# numerical or categorical feature.
+# :func:`sklearn.compose.make_column_selector` gives this possibility.
+# First, let's only select a subset of columns to simplify our
+# example.
+
+subset_feature = ["embarked", "sex", "pclass", "age", "fare"]
+X_train, X_test = X_train[subset_feature], X_test[subset_feature]
+
+# %%
+# Then, we introspect the information regarding each column data type.
+
+X_train.info()
+
+# %%
+# We can observe that the `embarked` and `sex` columns were tagged as
+# `category` columns when loading the data with ``fetch_openml``. Therefore, we
+# can use this information to dispatch the categorical columns to the
+# ``categorical_transformer`` and the remaining columns to the
+# ``numerical_transformer``.
+
+# %%
+# .. note:: In practice, you will have to handle yourself the column data type.
+#    If you want some columns to be considered as `category`, you will have to
+#    convert them into categorical columns. If you are using pandas, you can
+#    refer to their documentation regarding `Categorical data
+#    <https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html>`_.
+
+from sklearn.compose import make_column_selector as selector
+
+preprocessor = ColumnTransformer(
+    transformers=[
+        ("num", numeric_transformer, selector(dtype_exclude="category")),
+        ("cat", categorical_transformer, selector(dtype_include="category")),
+    ]
+)
+clf = Pipeline(
+    steps=[("preprocessor", preprocessor), ("classifier", LogisticRegression())]
+)
+
+
+clf.fit(X_train, y_train)
+print("model score: %.3f" % clf.score(X_test, y_test))
+clf
+
+# %%
+# The resulting score is not exactly the same as the one from the previous
+# pipeline because the dtype-based selector treats the ``pclass`` column as
+# a numeric feature instead of a categorical feature as previously:
+
+selector(dtype_exclude="category")(X_train)
+
+# %%
+
+selector(dtype_include="category")(X_train)
+
+# %%
 # Using the prediction pipeline in a grid search
-###############################################################################
+#
 # Grid search can also be performed on the different preprocessing steps
 # defined in the ``ColumnTransformer`` object, together with the classifier's
 # hyperparameters as part of the ``Pipeline``.
 # We will search for both the imputer strategy of the numeric preprocessing
 # and the regularization parameter of the logistic regression using
-# :class:`sklearn.model_selection.GridSearchCV`.
-
+# :class:`~sklearn.model_selection.GridSearchCV`.

 param_grid = {
-    'preprocessor__num__imputer__strategy': ['mean', 'median'],
-    'classifier__C': [0.1, 1.0, 10, 100],
+    "preprocessor__num__imputer__strategy": ["mean", "median"],
+    "classifier__C": [0.1, 1.0, 10, 100],
 }

-grid_search = GridSearchCV(clf, param_grid, cv=10, iid=False)
+grid_search = GridSearchCV(clf, param_grid, cv=10)
+grid_search
+
+# %%
+# Calling 'fit' triggers the cross-validated search for the best
+# hyper-parameters combination:
+#
 grid_search.fit(X_train, y_train)

-print(("best logistic regression from grid search: %.3f"
-       % grid_search.score(X_test, y_test)))
+print("Best params:")
+print(grid_search.best_params_)
+
+# %%
+# The internal cross-validation scores obtained by those parameters is:
+print(f"Internal CV score: {grid_search.best_score_:.3f}")
+
+# %%
+# We can also introspect the top grid search results as a pandas dataframe:
+import pandas as pd
+
+cv_results = pd.DataFrame(grid_search.cv_results_)
+cv_results = cv_results.sort_values("mean_test_score", ascending=False)
+cv_results[
+    [
+        "mean_test_score",
+        "std_test_score",
+        "param_preprocessor__num__imputer__strategy",
+        "param_classifier__C",
+    ]
+].head(5)
+
+# %%
+# The best hyper-parameters have be used to re-fit a final model on the full
+# training set. We can evaluate that final model on held out test data that was
+# not used for hyperparameter tuning.
+#
+print(
+    (
+        "best logistic regression from grid search: %.3f"
+        % grid_search.score(X_test, y_test)
+    )
+)
('examples/compose', 'plot_compare_reduction.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,4 +1,3 @@
-#!/usr/bin/env python
 # -*- coding: utf-8 -*-
 """
 =================================================================
@@ -19,16 +18,14 @@

 Note that the use of ``memory`` to enable caching becomes interesting when the
 fitting of a transformer is costly.
+
 """

-###############################################################################
+# %%
 # Illustration of ``Pipeline`` and ``GridSearchCV``
 ###############################################################################
-# This section illustrates the use of a ``Pipeline`` with
-# ``GridSearchCV``

 # Authors: Robert McGibbon, Joel Nothman, Guillaume Lemaitre
-

 import numpy as np
 import matplotlib.pyplot as plt
@@ -39,57 +36,56 @@
 from sklearn.decomposition import PCA, NMF
 from sklearn.feature_selection import SelectKBest, chi2

-print(__doc__)
-
-pipe = Pipeline([
-    # the reduce_dim stage is populated by the param_grid
-    ('reduce_dim', 'passthrough'),
-    ('classify', LinearSVC(dual=False, max_iter=10000))
-])
+pipe = Pipeline(
+    [
+        # the reduce_dim stage is populated by the param_grid
+        ("reduce_dim", "passthrough"),
+        ("classify", LinearSVC(dual=False, max_iter=10000)),
+    ]
+)

 N_FEATURES_OPTIONS = [2, 4, 8]
 C_OPTIONS = [1, 10, 100, 1000]
 param_grid = [
     {
-        'reduce_dim': [PCA(iterated_power=7), NMF()],
-        'reduce_dim__n_components': N_FEATURES_OPTIONS,
-        'classify__C': C_OPTIONS
+        "reduce_dim": [PCA(iterated_power=7), NMF()],
+        "reduce_dim__n_components": N_FEATURES_OPTIONS,
+        "classify__C": C_OPTIONS,
     },
     {
-        'reduce_dim': [SelectKBest(chi2)],
-        'reduce_dim__k': N_FEATURES_OPTIONS,
-        'classify__C': C_OPTIONS
+        "reduce_dim": [SelectKBest(chi2)],
+        "reduce_dim__k": N_FEATURES_OPTIONS,
+        "classify__C": C_OPTIONS,
     },
 ]
-reducer_labels = ['PCA', 'NMF', 'KBest(chi2)']
+reducer_labels = ["PCA", "NMF", "KBest(chi2)"]

-grid = GridSearchCV(pipe, cv=5, n_jobs=1, param_grid=param_grid, iid=False)
-digits = load_digits()
-grid.fit(digits.data, digits.target)
+grid = GridSearchCV(pipe, n_jobs=1, param_grid=param_grid)
+X, y = load_digits(return_X_y=True)
+grid.fit(X, y)

-mean_scores = np.array(grid.cv_results_['mean_test_score'])
+mean_scores = np.array(grid.cv_results_["mean_test_score"])
 # scores are in the order of param_grid iteration, which is alphabetical
 mean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))
 # select score for best C
 mean_scores = mean_scores.max(axis=0)
-bar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) *
-               (len(reducer_labels) + 1) + .5)
+bar_offsets = np.arange(len(N_FEATURES_OPTIONS)) * (len(reducer_labels) + 1) + 0.5

 plt.figure()
-COLORS = 'bgrcmyk'
+COLORS = "bgrcmyk"
 for i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):
     plt.bar(bar_offsets + i, reducer_scores, label=label, color=COLORS[i])

 plt.title("Comparing feature reduction techniques")
-plt.xlabel('Reduced number of features')
+plt.xlabel("Reduced number of features")
 plt.xticks(bar_offsets + len(reducer_labels) / 2, N_FEATURES_OPTIONS)
-plt.ylabel('Digit classification accuracy')
+plt.ylabel("Digit classification accuracy")
 plt.ylim((0, 1))
-plt.legend(loc='upper left')
+plt.legend(loc="upper left")

 plt.show()

-###############################################################################
+# %%
 # Caching transformers within a ``Pipeline``
 ###############################################################################
 # It is sometimes worthwhile storing the state of a specific transformer
@@ -102,27 +98,25 @@
 #     cache. Hence, use the ``memory`` constructor parameter when the fitting
 #     of a transformer is costly.

-from tempfile import mkdtemp
+from joblib import Memory
 from shutil import rmtree
-from joblib import Memory

 # Create a temporary folder to store the transformers of the pipeline
-cachedir = mkdtemp()
-memory = Memory(location=cachedir, verbose=10)
-cached_pipe = Pipeline([('reduce_dim', PCA()),
-                        ('classify', LinearSVC(dual=False, max_iter=10000))],
-                       memory=memory)
+location = "cachedir"
+memory = Memory(location=location, verbose=10)
+cached_pipe = Pipeline(
+    [("reduce_dim", PCA()), ("classify", LinearSVC(dual=False, max_iter=10000))],
+    memory=memory,
+)

 # This time, a cached pipeline will be used within the grid search
-grid = GridSearchCV(cached_pipe, cv=5, n_jobs=1, param_grid=param_grid,
-                    iid=False)
-digits = load_digits()
-grid.fit(digits.data, digits.target)
+

 # Delete the temporary cache before exiting
-rmtree(cachedir)
+memory.clear(warn=False)
+rmtree(location)

-###############################################################################
+# %%
 # The ``PCA`` fitting is only computed at the evaluation of the first
 # configuration of the ``C`` parameter of the ``LinearSVC`` classifier. The
 # other configurations of ``C`` will trigger the loading of the cached ``PCA``
('examples/datasets', 'plot_random_dataset.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,16 +3,16 @@
 Plot randomly generated classification dataset
 ==============================================

-Plot several randomly generated 2D classification datasets.
-This example illustrates the :func:`datasets.make_classification`
-:func:`datasets.make_blobs` and :func:`datasets.make_gaussian_quantiles`
-functions.
+This example plots several randomly generated classification datasets.
+For easy visualization, all datasets have 2 features, plotted on the x and y
+axis. The color of each point represents its class label.

-For ``make_classification``, three binary and two multi-class classification
-datasets are generated, with different numbers of informative features and
-clusters per class.  """
+The first 4 plots use the :func:`~sklearn.datasets.make_classification` with
+different numbers of informative features, clusters per class and classes.
+The final 2 plots use :func:`~sklearn.datasets.make_blobs` and
+:func:`~sklearn.datasets.make_gaussian_quantiles`.

-print(__doc__)
+"""

 import matplotlib.pyplot as plt

@@ -21,47 +21,42 @@
 from sklearn.datasets import make_gaussian_quantiles

 plt.figure(figsize=(8, 8))
-plt.subplots_adjust(bottom=.05, top=.9, left=.05, right=.95)
+plt.subplots_adjust(bottom=0.05, top=0.9, left=0.05, right=0.95)

 plt.subplot(321)
-plt.title("One informative feature, one cluster per class", fontsize='small')
-X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=1,
-                             n_clusters_per_class=1)
-plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,
-            s=25, edgecolor='k')
+plt.title("One informative feature, one cluster per class", fontsize="small")
+X1, Y1 = make_classification(
+    n_features=2, n_redundant=0, n_informative=1, n_clusters_per_class=1
+)
+plt.scatter(X1[:, 0], X1[:, 1], marker="o", c=Y1, s=25, edgecolor="k")

 plt.subplot(322)
-plt.title("Two informative features, one cluster per class", fontsize='small')
-X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,
-                             n_clusters_per_class=1)
-plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,
-            s=25, edgecolor='k')
+plt.title("Two informative features, one cluster per class", fontsize="small")
+X1, Y1 = make_classification(
+    n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1
+)
+plt.scatter(X1[:, 0], X1[:, 1], marker="o", c=Y1, s=25, edgecolor="k")

 plt.subplot(323)
-plt.title("Two informative features, two clusters per class",
-          fontsize='small')
+plt.title("Two informative features, two clusters per class", fontsize="small")
 X2, Y2 = make_classification(n_features=2, n_redundant=0, n_informative=2)
-plt.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y2,
-            s=25, edgecolor='k')
+plt.scatter(X2[:, 0], X2[:, 1], marker="o", c=Y2, s=25, edgecolor="k")

 plt.subplot(324)
-plt.title("Multi-class, two informative features, one cluster",
-          fontsize='small')
-X1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,
-                             n_clusters_per_class=1, n_classes=3)
-plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,
-            s=25, edgecolor='k')
+plt.title("Multi-class, two informative features, one cluster", fontsize="small")
+X1, Y1 = make_classification(
+    n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, n_classes=3
+)
+plt.scatter(X1[:, 0], X1[:, 1], marker="o", c=Y1, s=25, edgecolor="k")

 plt.subplot(325)
-plt.title("Three blobs", fontsize='small')
+plt.title("Three blobs", fontsize="small")
 X1, Y1 = make_blobs(n_features=2, centers=3)
-plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,
-            s=25, edgecolor='k')
+plt.scatter(X1[:, 0], X1[:, 1], marker="o", c=Y1, s=25, edgecolor="k")

 plt.subplot(326)
-plt.title("Gaussian divided into three quantiles", fontsize='small')
+plt.title("Gaussian divided into three quantiles", fontsize="small")
 X1, Y1 = make_gaussian_quantiles(n_features=2, n_classes=3)
-plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,
-            s=25, edgecolor='k')
+plt.scatter(X1[:, 0], X1[:, 1], marker="o", c=Y1, s=25, edgecolor="k")

 plt.show()
('examples/datasets', 'plot_random_multilabel_dataset.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,8 +3,8 @@
 Plot randomly generated multilabel dataset
 ==============================================

-This illustrates the `datasets.make_multilabel_classification` dataset
-generator. Each sample consists of counts of two features (up to 50 in
+This illustrates the :func:`~sklearn.datasets.make_multilabel_classification`
+dataset generator. Each sample consists of counts of two features (up to 50 in
 total), which are differently distributed in each of two classes.

 Points are labeled as follows, where Y means the class is present:
@@ -32,6 +32,7 @@
 "document length", while here we have much larger documents than vocabulary.
 Similarly, with ``n_classes > n_features``, it is much less likely that a
 feature distinguishes a particular class.
+
 """

 import numpy as np
@@ -39,56 +40,67 @@

 from sklearn.datasets import make_multilabel_classification as make_ml_clf

-print(__doc__)
-
-COLORS = np.array(['!',
-                   '#FF3333',  # red
-                   '#0198E1',  # blue
-                   '#BF5FFF',  # purple
-                   '#FCD116',  # yellow
-                   '#FF7216',  # orange
-                   '#4DBD33',  # green
-                   '#87421F'   # brown
-                   ])
+COLORS = np.array(
+    [
+        "!",
+        "#FF3333",  # red
+        "#0198E1",  # blue
+        "#BF5FFF",  # purple
+        "#FCD116",  # yellow
+        "#FF7216",  # orange
+        "#4DBD33",  # green
+        "#87421F",  # brown
+    ]
+)

 # Use same random seed for multiple calls to make_multilabel_classification to
 # ensure same distributions
-RANDOM_SEED = np.random.randint(2 ** 10)
+RANDOM_SEED = np.random.randint(2**10)


 def plot_2d(ax, n_labels=1, n_classes=3, length=50):
-    X, Y, p_c, p_w_c = make_ml_clf(n_samples=150, n_features=2,
-                                   n_classes=n_classes, n_labels=n_labels,
-                                   length=length, allow_unlabeled=False,
-                                   return_distributions=True,
-                                   random_state=RANDOM_SEED)
+    X, Y, p_c, p_w_c = make_ml_clf(
+        n_samples=150,
+        n_features=2,
+        n_classes=n_classes,
+        n_labels=n_labels,
+        length=length,
+        allow_unlabeled=False,
+        return_distributions=True,
+        random_state=RANDOM_SEED,
+    )

-    ax.scatter(X[:, 0], X[:, 1], color=COLORS.take((Y * [1, 2, 4]
-                                                    ).sum(axis=1)),
-               marker='.')
-    ax.scatter(p_w_c[0] * length, p_w_c[1] * length,
-               marker='*', linewidth=.5, edgecolor='black',
-               s=20 + 1500 * p_c ** 2,
-               color=COLORS.take([1, 2, 4]))
-    ax.set_xlabel('Feature 0 count')
+    ax.scatter(
+        X[:, 0], X[:, 1], color=COLORS.take((Y * [1, 2, 4]).sum(axis=1)), marker="."
+    )
+    ax.scatter(
+        p_w_c[0] * length,
+        p_w_c[1] * length,
+        marker="*",
+        linewidth=0.5,
+        edgecolor="black",
+        s=20 + 1500 * p_c**2,
+        color=COLORS.take([1, 2, 4]),
+    )
+    ax.set_xlabel("Feature 0 count")
     return p_c, p_w_c


-_, (ax1, ax2) = plt.subplots(1, 2, sharex='row', sharey='row', figsize=(8, 4))
-plt.subplots_adjust(bottom=.15)
+_, (ax1, ax2) = plt.subplots(1, 2, sharex="row", sharey="row", figsize=(8, 4))
+plt.subplots_adjust(bottom=0.15)

 p_c, p_w_c = plot_2d(ax1, n_labels=1)
-ax1.set_title('n_labels=1, length=50')
-ax1.set_ylabel('Feature 1 count')
+ax1.set_title("n_labels=1, length=50")
+ax1.set_ylabel("Feature 1 count")

 plot_2d(ax2, n_labels=3)
-ax2.set_title('n_labels=3, length=50')
+ax2.set_title("n_labels=3, length=50")
 ax2.set_xlim(left=0, auto=True)
 ax2.set_ylim(bottom=0, auto=True)

 plt.show()

-print('The data was generated from (random_state=%d):' % RANDOM_SEED)
-print('Class', 'P(C)', 'P(w0|C)', 'P(w1|C)', sep='\t')
-for k, p, p_w in zip(['red', 'blue', 'yellow'], p_c, p_w_c.T):
-    print('%s\t%0.2f\t%0.2f\t%0.2f' % (k, p, p_w[0], p_w[1]))
+print("The data was generated from (random_state=%d):" % RANDOM_SEED)
+print("Class", "P(C)", "P(w0|C)", "P(w1|C)", sep="\t")
+for k, p, p_w in zip(["red", "blue", "yellow"], p_c, p_w_c.T):
+    print("%s\t%0.2f\t%0.2f\t%0.2f" % (k, p, p_w[0], p_w[1]))
('examples/datasets', 'plot_iris_dataset.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =========================================================
 The Iris Dataset
@@ -15,16 +13,18 @@
 The below plot uses the first two features.
 See `here <https://en.wikipedia.org/wiki/Iris_flower_data_set>`_ for more
 information on this dataset.
+
 """
-print(__doc__)
-

 # Code source: Gaël Varoquaux
 # Modified for documentation by Jaques Grobler
 # License: BSD 3 clause

 import matplotlib.pyplot as plt
-from mpl_toolkits.mplot3d import Axes3D
+
+# unused but required import for doing 3d projections with matplotlib < 3.2
+import mpl_toolkits.mplot3d  # noqa: F401
+
 from sklearn import datasets
 from sklearn.decomposition import PCA

@@ -33,17 +33,16 @@
 X = iris.data[:, :2]  # we only take the first two features.
 y = iris.target

-x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
-y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
+x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
+y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5

 plt.figure(2, figsize=(8, 6))
 plt.clf()

 # Plot the training points
-plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,
-            edgecolor='k')
-plt.xlabel('Sepal length')
-plt.ylabel('Sepal width')
+plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor="k")
+plt.xlabel("Sepal length")
+plt.ylabel("Sepal width")

 plt.xlim(x_min, x_max)
 plt.ylim(y_min, y_max)
@@ -53,10 +52,19 @@
 # To getter a better understanding of interaction of the dimensions
 # plot the first three PCA dimensions
 fig = plt.figure(1, figsize=(8, 6))
-ax = Axes3D(fig, elev=-150, azim=110)
+ax = fig.add_subplot(111, projection="3d", elev=-150, azim=110)
+
 X_reduced = PCA(n_components=3).fit_transform(iris.data)
-ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y,
-           cmap=plt.cm.Set1, edgecolor='k', s=40)
+ax.scatter(
+    X_reduced[:, 0],
+    X_reduced[:, 1],
+    X_reduced[:, 2],
+    c=y,
+    cmap=plt.cm.Set1,
+    edgecolor="k",
+    s=40,
+)
+
 ax.set_title("First three PCA directions")
 ax.set_xlabel("1st eigenvector")
 ax.w_xaxis.set_ticklabels([])
('examples/datasets', 'plot_digits_last_image.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =========================================================
 The Digit Dataset
@@ -14,9 +12,8 @@
 See `here
 <https://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits>`_
 for more information about this dataset.
+
 """
-print(__doc__)
-

 # Code source: Gaël Varoquaux
 # Modified for documentation by Jaques Grobler
@@ -26,10 +23,10 @@

 import matplotlib.pyplot as plt

-#Load the digits dataset
+# Load the digits dataset
 digits = datasets.load_digits()

-#Display the first digit
+# Display the last digit
 plt.figure(1, figsize=(3, 3))
-plt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation='nearest')
+plt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation="nearest")
 plt.show()
('examples/linear_model', 'plot_ols.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,27 +1,22 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =========================================================
 Linear Regression Example
 =========================================================
-This example uses the only the first feature of the `diabetes` dataset, in
-order to illustrate a two-dimensional plot of this regression technique. The
-straight line can be seen in the plot, showing how linear regression attempts
-to draw a straight line that will best minimize the residual sum of squares
-between the observed responses in the dataset, and the responses predicted by
-the linear approximation.
+The example below uses only the first feature of the `diabetes` dataset,
+in order to illustrate the data points within the two-dimensional plot.
+The straight line can be seen in the plot, showing how linear regression
+attempts to draw a straight line that will best minimize the
+residual sum of squares between the observed responses in the dataset,
+and the responses predicted by the linear approximation.

-The coefficients, the residual sum of squares and the variance score are also
-calculated.
+The coefficients, residual sum of squares and the coefficient of
+determination are also calculated.

 """
-print(__doc__)
-

 # Code source: Jaques Grobler
 # License: BSD 3 clause
-

 import matplotlib.pyplot as plt
 import numpy as np
@@ -29,19 +24,18 @@
 from sklearn.metrics import mean_squared_error, r2_score

 # Load the diabetes dataset
-diabetes = datasets.load_diabetes()
-
+diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)

 # Use only one feature
-diabetes_X = diabetes.data[:, np.newaxis, 2]
+diabetes_X = diabetes_X[:, np.newaxis, 2]

 # Split the data into training/testing sets
 diabetes_X_train = diabetes_X[:-20]
 diabetes_X_test = diabetes_X[-20:]

 # Split the targets into training/testing sets
-diabetes_y_train = diabetes.target[:-20]
-diabetes_y_test = diabetes.target[-20:]
+diabetes_y_train = diabetes_y[:-20]
+diabetes_y_test = diabetes_y[-20:]

 # Create linear regression object
 regr = linear_model.LinearRegression()
@@ -53,16 +47,15 @@
 diabetes_y_pred = regr.predict(diabetes_X_test)

 # The coefficients
-print('Coefficients: \n', regr.coef_)
+print("Coefficients: \n", regr.coef_)
 # The mean squared error
-print("Mean squared error: %.2f"
-      % mean_squared_error(diabetes_y_test, diabetes_y_pred))
-# Explained variance score: 1 is perfect prediction
-print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))
+print("Mean squared error: %.2f" % mean_squared_error(diabetes_y_test, diabetes_y_pred))
+# The coefficient of determination: 1 is perfect prediction
+print("Coefficient of determination: %.2f" % r2_score(diabetes_y_test, diabetes_y_pred))

 # Plot outputs
-plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')
-plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)
+plt.scatter(diabetes_X_test, diabetes_y_test, color="black")
+plt.plot(diabetes_X_test, diabetes_y_pred, color="blue", linewidth=3)

 plt.xticks(())
 plt.yticks(())
('examples/linear_model', 'plot_ransac.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,6 +7,7 @@
 the RANSAC algorithm.

 """
+
 import numpy as np
 from matplotlib import pyplot as plt

@@ -17,9 +18,14 @@
 n_outliers = 50


-X, y, coef = datasets.make_regression(n_samples=n_samples, n_features=1,
-                                      n_informative=1, noise=10,
-                                      coef=True, random_state=0)
+X, y, coef = datasets.make_regression(
+    n_samples=n_samples,
+    n_features=1,
+    n_informative=1,
+    noise=10,
+    coef=True,
+    random_state=0,
+)

 # Add outlier data
 np.random.seed(0)
@@ -46,14 +52,21 @@
 print(coef, lr.coef_, ransac.estimator_.coef_)

 lw = 2
-plt.scatter(X[inlier_mask], y[inlier_mask], color='yellowgreen', marker='.',
-            label='Inliers')
-plt.scatter(X[outlier_mask], y[outlier_mask], color='gold', marker='.',
-            label='Outliers')
-plt.plot(line_X, line_y, color='navy', linewidth=lw, label='Linear regressor')
-plt.plot(line_X, line_y_ransac, color='cornflowerblue', linewidth=lw,
-         label='RANSAC regressor')
-plt.legend(loc='lower right')
+plt.scatter(
+    X[inlier_mask], y[inlier_mask], color="yellowgreen", marker=".", label="Inliers"
+)
+plt.scatter(
+    X[outlier_mask], y[outlier_mask], color="gold", marker=".", label="Outliers"
+)
+plt.plot(line_X, line_y, color="navy", linewidth=lw, label="Linear regressor")
+plt.plot(
+    line_X,
+    line_y_ransac,
+    color="cornflowerblue",
+    linewidth=lw,
+    label="RANSAC regressor",
+)
+plt.legend(loc="lower right")
 plt.xlabel("Input")
 plt.ylabel("Response")
 plt.show()
('examples/linear_model', 'plot_logistic_l1_l2_sparsity.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -12,9 +12,8 @@

 We classify 8x8 images of digits into two classes: 0-4 against 5-9.
 The visualization shows coefficients of the models for varying C.
+
 """
-
-print(__doc__)

 # Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>
 #          Mathieu Blondel <mathieu@mblondel.org>
@@ -28,13 +27,12 @@
 from sklearn import datasets
 from sklearn.preprocessing import StandardScaler

-digits = datasets.load_digits()
+X, y = datasets.load_digits(return_X_y=True)

-X, y = digits.data, digits.target
 X = StandardScaler().fit_transform(X)

 # classify small against large digits
-y = (y > 4).astype(np.int)
+y = (y > 4).astype(int)

 l1_ratio = 0.5  # L1 weight in the Elastic-Net regularization

@@ -43,10 +41,11 @@
 # Set regularization parameter
 for i, (C, axes_row) in enumerate(zip((1, 0.1, 0.01), axes)):
     # turn down tolerance for short training time
-    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01, solver='saga')
-    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01, solver='saga')
-    clf_en_LR = LogisticRegression(C=C, penalty='elasticnet', solver='saga',
-                                   l1_ratio=l1_ratio, tol=0.01)
+    clf_l1_LR = LogisticRegression(C=C, penalty="l1", tol=0.01, solver="saga")
+    clf_l2_LR = LogisticRegression(C=C, penalty="l2", tol=0.01, solver="saga")
+    clf_en_LR = LogisticRegression(
+        C=C, penalty="elasticnet", solver="saga", l1_ratio=l1_ratio, tol=0.01
+    )
     clf_l1_LR.fit(X, y)
     clf_l2_LR.fit(X, y)
     clf_en_LR.fit(X, y)
@@ -64,15 +63,13 @@

     print("C=%.2f" % C)
     print("{:<40} {:.2f}%".format("Sparsity with L1 penalty:", sparsity_l1_LR))
-    print("{:<40} {:.2f}%".format("Sparsity with Elastic-Net penalty:",
-                                  sparsity_en_LR))
+    print("{:<40} {:.2f}%".format("Sparsity with Elastic-Net penalty:", sparsity_en_LR))
     print("{:<40} {:.2f}%".format("Sparsity with L2 penalty:", sparsity_l2_LR))
-    print("{:<40} {:.2f}".format("Score with L1 penalty:",
-                                 clf_l1_LR.score(X, y)))
-    print("{:<40} {:.2f}".format("Score with Elastic-Net penalty:",
-                                 clf_en_LR.score(X, y)))
-    print("{:<40} {:.2f}".format("Score with L2 penalty:",
-                                 clf_l2_LR.score(X, y)))
+    print("{:<40} {:.2f}".format("Score with L1 penalty:", clf_l1_LR.score(X, y)))
+    print(
+        "{:<40} {:.2f}".format("Score with Elastic-Net penalty:", clf_en_LR.score(X, y))
+    )
+    print("{:<40} {:.2f}".format("Score with L2 penalty:", clf_l2_LR.score(X, y)))

     if i == 0:
         axes_row[0].set_title("L1 penalty")
@@ -80,11 +77,16 @@
         axes_row[2].set_title("L2 penalty")

     for ax, coefs in zip(axes_row, [coef_l1_LR, coef_en_LR, coef_l2_LR]):
-        ax.imshow(np.abs(coefs.reshape(8, 8)), interpolation='nearest',
-                  cmap='binary', vmax=1, vmin=0)
+        ax.imshow(
+            np.abs(coefs.reshape(8, 8)),
+            interpolation="nearest",
+            cmap="binary",
+            vmax=1,
+            vmin=0,
+        )
         ax.set_xticks(())
         ax.set_yticks(())

-    axes_row[0].set_ylabel('C = %s' % C)
+    axes_row[0].set_ylabel("C = %s" % C)

 plt.show()
('examples/linear_model', 'plot_ridge_coeffs.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -35,11 +35,10 @@

 Please note that in this example the data is non-noisy, hence
 it is possible to extract the exact coefficients.
+
 """

 # Author: Kornel Kielczewski -- <kornel.k@plusnet.pl>
-
-print(__doc__)

 import matplotlib.pyplot as plt
 import numpy as np
@@ -50,8 +49,9 @@

 clf = Ridge()

-X, y, w = make_regression(n_samples=10, n_features=10, coef=True,
-                          random_state=1, bias=3.5)
+X, y, w = make_regression(
+    n_samples=10, n_features=10, coef=True, random_state=1, bias=3.5
+)

 coefs = []
 errors = []
@@ -71,19 +71,19 @@
 plt.subplot(121)
 ax = plt.gca()
 ax.plot(alphas, coefs)
-ax.set_xscale('log')
-plt.xlabel('alpha')
-plt.ylabel('weights')
-plt.title('Ridge coefficients as a function of the regularization')
-plt.axis('tight')
+ax.set_xscale("log")
+plt.xlabel("alpha")
+plt.ylabel("weights")
+plt.title("Ridge coefficients as a function of the regularization")
+plt.axis("tight")

 plt.subplot(122)
 ax = plt.gca()
 ax.plot(alphas, errors)
-ax.set_xscale('log')
-plt.xlabel('alpha')
-plt.ylabel('error')
-plt.title('Coefficient error as a function of the regularization')
-plt.axis('tight')
+ax.set_xscale("log")
+plt.xlabel("alpha")
+plt.ylabel("error")
+plt.title("Coefficient error as a function of the regularization")
+plt.axis("tight")

 plt.show()
('examples/linear_model', 'plot_sgd_early_stopping.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -29,15 +29,18 @@
 at the attribute ``n_iter_``.

 This example illustrates how the early stopping can used in the
-:class:`sklearn.linear_model.SGDClassifier` model to achieve almost the same
+:class:`~sklearn.linear_model.SGDClassifier` model to achieve almost the same
 accuracy as compared to a model built without early stopping. This can
 significantly reduce training time. Note that scores differ between the
 stopping criteria even from early iterations because some of the training data
 is held out with the validation stopping criterion.
+
 """
+
 # Authors: Tom Dupre la Tour
 #
 # License: BSD 3 clause
+
 import time
 import sys

@@ -48,17 +51,15 @@
 from sklearn import linear_model
 from sklearn.datasets import fetch_openml
 from sklearn.model_selection import train_test_split
-from sklearn.utils.testing import ignore_warnings
+from sklearn.utils._testing import ignore_warnings
 from sklearn.exceptions import ConvergenceWarning
 from sklearn.utils import shuffle

-print(__doc__)

-
-def load_mnist(n_samples=None, class_0='0', class_1='8'):
+def load_mnist(n_samples=None, class_0="0", class_1="8"):
     """Load MNIST, select two classes, shuffle and return only n_samples."""
     # Load data from http://openml.org/d/554
-    mnist = fetch_openml('mnist_784', version=1)
+    mnist = fetch_openml("mnist_784", version=1, as_frame=False)

     # take only two classes for binary classification
     mask = np.logical_or(mnist.target == class_0, mnist.target == class_1)
@@ -88,55 +89,58 @@

 # Define the estimators to compare
 estimator_dict = {
-    'No stopping criterion':
-    linear_model.SGDClassifier(tol=1e-3, n_iter_no_change=3),
-    'Training loss':
-    linear_model.SGDClassifier(early_stopping=False, n_iter_no_change=3,
-                               tol=0.1),
-    'Validation score':
-    linear_model.SGDClassifier(early_stopping=True, n_iter_no_change=3,
-                               tol=0.0001, validation_fraction=0.2)
+    "No stopping criterion": linear_model.SGDClassifier(n_iter_no_change=3),
+    "Training loss": linear_model.SGDClassifier(
+        early_stopping=False, n_iter_no_change=3, tol=0.1
+    ),
+    "Validation score": linear_model.SGDClassifier(
+        early_stopping=True, n_iter_no_change=3, tol=0.0001, validation_fraction=0.2
+    ),
 }

 # Load the dataset
 X, y = load_mnist(n_samples=10000)
-X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,
-                                                    random_state=0)
+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)

 results = []
 for estimator_name, estimator in estimator_dict.items():
-    print(estimator_name + ': ', end='')
+    print(estimator_name + ": ", end="")
     for max_iter in range(1, 50):
-        print('.', end='')
+        print(".", end="")
         sys.stdout.flush()

         fit_time, n_iter, train_score, test_score = fit_and_score(
-            estimator, max_iter, X_train, X_test, y_train, y_test)
+            estimator, max_iter, X_train, X_test, y_train, y_test
+        )

-        results.append((estimator_name, max_iter, fit_time, n_iter,
-                        train_score, test_score))
-    print('')
+        results.append(
+            (estimator_name, max_iter, fit_time, n_iter, train_score, test_score)
+        )
+    print("")

 # Transform the results in a pandas dataframe for easy plotting
 columns = [
-    'Stopping criterion', 'max_iter', 'Fit time (sec)', 'n_iter_',
-    'Train score', 'Test score'
+    "Stopping criterion",
+    "max_iter",
+    "Fit time (sec)",
+    "n_iter_",
+    "Train score",
+    "Test score",
 ]
 results_df = pd.DataFrame(results, columns=columns)

 # Define what to plot (x_axis, y_axis)
-lines = 'Stopping criterion'
+lines = "Stopping criterion"
 plot_list = [
-    ('max_iter', 'Train score'),
-    ('max_iter', 'Test score'),
-    ('max_iter', 'n_iter_'),
-    ('max_iter', 'Fit time (sec)'),
+    ("max_iter", "Train score"),
+    ("max_iter", "Test score"),
+    ("max_iter", "n_iter_"),
+    ("max_iter", "Fit time (sec)"),
 ]

 nrows = 2
-ncols = int(np.ceil(len(plot_list) / 2.))
-fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(6 * ncols,
-                                                            4 * nrows))
+ncols = int(np.ceil(len(plot_list) / 2.0))
+fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(6 * ncols, 4 * nrows))
 axes[0, 0].get_shared_y_axes().join(axes[0, 0], axes[0, 1])

 for ax, (x_axis, y_axis) in zip(axes.ravel(), plot_list):
('examples/linear_model', 'plot_sgd_separating_hyperplane.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,20 +6,20 @@
 Plot the maximum margin separating hyperplane within a two-class
 separable dataset using a linear Support Vector Machines classifier
 trained using SGD.
+
 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
 from sklearn.linear_model import SGDClassifier
-from sklearn.datasets.samples_generator import make_blobs
+from sklearn.datasets import make_blobs

 # we create 50 separable points
 X, Y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)

 # fit the model
-clf = SGDClassifier(loss="hinge", alpha=0.01, max_iter=200,
-                    fit_intercept=True, tol=1e-3)
+clf = SGDClassifier(loss="hinge", alpha=0.01, max_iter=200)
+
 clf.fit(X, Y)

 # plot the line, the points, and the nearest vectors to the plane
@@ -34,11 +34,10 @@
     p = clf.decision_function([[x1, x2]])
     Z[i, j] = p[0]
 levels = [-1.0, 0.0, 1.0]
-linestyles = ['dashed', 'solid', 'dashed']
-colors = 'k'
+linestyles = ["dashed", "solid", "dashed"]
+colors = "k"
 plt.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)
-plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired,
-            edgecolor='black', s=20)
+plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired, edgecolor="black", s=20)

-plt.axis('tight')
+plt.axis("tight")
 plt.show()
('examples/linear_model', 'plot_polynomial_interpolation.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,72 +1,216 @@
-#!/usr/bin/env python
 """
-========================
-Polynomial interpolation
-========================
-
-This example demonstrates how to approximate a function with a polynomial of
-degree n_degree by using ridge regression. Concretely, from n_samples 1d
-points, it suffices to build the Vandermonde matrix, which is n_samples x
-n_degree+1 and has the following form:
-
-[[1, x_1, x_1 ** 2, x_1 ** 3, ...],
- [1, x_2, x_2 ** 2, x_2 ** 3, ...],
- ...]
-
-Intuitively, this matrix can be interpreted as a matrix of pseudo features (the
-points raised to some power). The matrix is akin to (but different from) the
-matrix induced by a polynomial kernel.
-
-This example shows that you can do non-linear regression with a linear model,
-using a pipeline to add non-linear features. Kernel methods extend this idea
-and can induce very high (even infinite) dimensional feature spaces.
+===================================
+Polynomial and Spline interpolation
+===================================
+
+This example demonstrates how to approximate a function with polynomials up to
+degree ``degree`` by using ridge regression. We show two different ways given
+``n_samples`` of 1d points ``x_i``:
+
+- :class:`~sklearn.preprocessing.PolynomialFeatures` generates all monomials
+  up to ``degree``. This gives us the so called Vandermonde matrix with
+  ``n_samples`` rows and ``degree + 1`` columns::
+
+    [[1, x_0, x_0 ** 2, x_0 ** 3, ..., x_0 ** degree],
+     [1, x_1, x_1 ** 2, x_1 ** 3, ..., x_1 ** degree],
+     ...]
+
+  Intuitively, this matrix can be interpreted as a matrix of pseudo features
+  (the points raised to some power). The matrix is akin to (but different from)
+  the matrix induced by a polynomial kernel.
+
+- :class:`~sklearn.preprocessing.SplineTransformer` generates B-spline basis
+  functions. A basis function of a B-spline is a piece-wise polynomial function
+  of degree ``degree`` that is non-zero only between ``degree+1`` consecutive
+  knots. Given ``n_knots`` number of knots, this results in matrix of
+  ``n_samples`` rows and ``n_knots + degree - 1`` columns::
+
+    [[basis_1(x_0), basis_2(x_0), ...],
+     [basis_1(x_1), basis_2(x_1), ...],
+     ...]
+
+This example shows that these two transformers are well suited to model
+non-linear effects with a linear model, using a pipeline to add non-linear
+features. Kernel methods extend this idea and can induce very high (even
+infinite) dimensional feature spaces.
+
 """
-print(__doc__)

 # Author: Mathieu Blondel
 #         Jake Vanderplas
+#         Christian Lorentzen
+#         Malte Londschien
 # License: BSD 3 clause

 import numpy as np
 import matplotlib.pyplot as plt

 from sklearn.linear_model import Ridge
-from sklearn.preprocessing import PolynomialFeatures
+from sklearn.preprocessing import PolynomialFeatures, SplineTransformer
 from sklearn.pipeline import make_pipeline


+# %%
+# We start by defining a function that we intend to approximate and prepare
+# plotting it.
+
+
 def f(x):
-    """ function to approximate by polynomial interpolation"""
+    """Function to be approximated by polynomial interpolation."""
     return x * np.sin(x)


-# generate points used to plot
-x_plot = np.linspace(0, 10, 100)
-
-# generate points and keep a subset of them
-x = np.linspace(0, 10, 100)
+# whole range we want to plot
+x_plot = np.linspace(-1, 11, 100)
+
+# %%
+# To make it interesting, we only give a small subset of points to train on.
+
+x_train = np.linspace(0, 10, 100)
 rng = np.random.RandomState(0)
-rng.shuffle(x)
-x = np.sort(x[:20])
-y = f(x)
-
-# create matrix versions of these arrays
-X = x[:, np.newaxis]
+x_train = np.sort(rng.choice(x_train, size=20, replace=False))
+y_train = f(x_train)
+
+# create 2D-array versions of these arrays to feed to transformers
+X_train = x_train[:, np.newaxis]
 X_plot = x_plot[:, np.newaxis]

-colors = ['teal', 'yellowgreen', 'gold']
+# %%
+# Now we are ready to create polynomial features and splines, fit on the
+# training points and show how well they interpolate.
+
+# plot function
 lw = 2
-plt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,
-         label="ground truth")
-plt.scatter(x, y, color='navy', s=30, marker='o', label="training points")
-
-for count, degree in enumerate([3, 4, 5]):
-    model = make_pipeline(PolynomialFeatures(degree), Ridge())
-    model.fit(X, y)
+fig, ax = plt.subplots()
+ax.set_prop_cycle(
+    color=["black", "teal", "yellowgreen", "gold", "darkorange", "tomato"]
+)
+ax.plot(x_plot, f(x_plot), linewidth=lw, label="ground truth")
+
+# plot training points
+ax.scatter(x_train, y_train, label="training points")
+
+# polynomial features
+for degree in [3, 4, 5]:
+    model = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=1e-3))
+    model.fit(X_train, y_train)
     y_plot = model.predict(X_plot)
-    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw,
-             label="degree %d" % degree)
-
-plt.legend(loc='lower left')
-
+    ax.plot(x_plot, y_plot, label=f"degree {degree}")
+
+# B-spline with 4 + 3 - 1 = 6 basis functions
+model = make_pipeline(SplineTransformer(n_knots=4, degree=3), Ridge(alpha=1e-3))
+model.fit(X_train, y_train)
+
+y_plot = model.predict(X_plot)
+ax.plot(x_plot, y_plot, label="B-spline")
+ax.legend(loc="lower center")
+ax.set_ylim(-20, 10)
 plt.show()
+
+# %%
+# This shows nicely that higher degree polynomials can fit the data better. But
+# at the same time, too high powers can show unwanted oscillatory behaviour
+# and are particularly dangerous for extrapolation beyond the range of fitted
+# data. This is an advantage of B-splines. They usually fit the data as well as
+# polynomials and show very nice and smooth behaviour. They have also good
+# options to control the extrapolation, which defaults to continue with a
+# constant. Note that most often, you would rather increase the number of knots
+# but keep ``degree=3``.
+#
+# In order to give more insights into the generated feature bases, we plot all
+# columns of both transformers separately.
+
+fig, axes = plt.subplots(ncols=2, figsize=(16, 5))
+pft = PolynomialFeatures(degree=3).fit(X_train)
+axes[0].plot(x_plot, pft.transform(X_plot))
+axes[0].legend(axes[0].lines, [f"degree {n}" for n in range(4)])
+axes[0].set_title("PolynomialFeatures")
+
+splt = SplineTransformer(n_knots=4, degree=3).fit(X_train)
+axes[1].plot(x_plot, splt.transform(X_plot))
+axes[1].legend(axes[1].lines, [f"spline {n}" for n in range(6)])
+axes[1].set_title("SplineTransformer")
+
+# plot knots of spline
+knots = splt.bsplines_[0].t
+axes[1].vlines(knots[3:-3], ymin=0, ymax=0.8, linestyles="dashed")
+plt.show()
+
+# %%
+# In the left plot, we recognize the lines corresponding to simple monomials
+# from ``x**0`` to ``x**3``. In the right figure, we see the six B-spline
+# basis functions of ``degree=3`` and also the four knot positions that were
+# chosen during ``fit``. Note that there are ``degree`` number of additional
+# knots each to the left and to the right of the fitted interval. These are
+# there for technical reasons, so we refrain from showing them. Every basis
+# function has local support and is continued as a constant beyond the fitted
+# range. This extrapolating behaviour could be changed by the argument
+# ``extrapolation``.
+
+# %%
+# Periodic Splines
+# ----------------
+# In the previous example we saw the limitations of polynomials and splines for
+# extrapolation beyond the range of the training observations. In some
+# settings, e.g. with seasonal effects, we expect a periodic continuation of
+# the underlying signal. Such effects can be modelled using periodic splines,
+# which have equal function value and equal derivatives at the first and last
+# knot. In the following case we show how periodic splines provide a better fit
+# both within and outside of the range of training data given the additional
+# information of periodicity. The splines period is the distance between
+# the first and last knot, which we specify manually.
+#
+# Periodic splines can also be useful for naturally periodic features (such as
+# day of the year), as the smoothness at the boundary knots prevents a jump in
+# the transformed values (e.g. from Dec 31st to Jan 1st). For such naturally
+# periodic features or more generally features where the period is known, it is
+# advised to explicitly pass this information to the `SplineTransformer` by
+# setting the knots manually.
+
+
+# %%
+def g(x):
+    """Function to be approximated by periodic spline interpolation."""
+    return np.sin(x) - 0.7 * np.cos(x * 3)
+
+
+y_train = g(x_train)
+
+# Extend the test data into the future:
+x_plot_ext = np.linspace(-1, 21, 200)
+X_plot_ext = x_plot_ext[:, np.newaxis]
+
+lw = 2
+fig, ax = plt.subplots()
+ax.set_prop_cycle(color=["black", "tomato", "teal"])
+ax.plot(x_plot_ext, g(x_plot_ext), linewidth=lw, label="ground truth")
+ax.scatter(x_train, y_train, label="training points")
+
+for transformer, label in [
+    (SplineTransformer(degree=3, n_knots=10), "spline"),
+    (
+        SplineTransformer(
+            degree=3,
+            knots=np.linspace(0, 2 * np.pi, 10)[:, None],
+            extrapolation="periodic",
+        ),
+        "periodic spline",
+    ),
+]:
+    model = make_pipeline(transformer, Ridge(alpha=1e-3))
+    model.fit(X_train, y_train)
+    y_plot_ext = model.predict(X_plot_ext)
+    ax.plot(x_plot_ext, y_plot_ext, label=label)
+
+ax.legend()
+fig.show()
+
+# %% We again plot the underlying splines.
+fig, ax = plt.subplots()
+knots = np.linspace(0, 2 * np.pi, 4)
+splt = SplineTransformer(knots=knots[:, None], degree=3, extrapolation="periodic").fit(
+    X_train
+)
+ax.plot(x_plot_ext, splt.transform(X_plot_ext))
+ax.legend(ax.lines, [f"spline {n}" for n in range(3)])
+plt.show()
('examples/linear_model', 'plot_lasso_coordinate_descent_path.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,8 +7,8 @@
 coordinate descent.

 The coefficients can be forced to be positive.
+
 """
-print(__doc__)

 # Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
 # License: BSD 3 clause
@@ -20,9 +20,9 @@
 from sklearn.linear_model import lasso_path, enet_path
 from sklearn import datasets

-diabetes = datasets.load_diabetes()
-X = diabetes.data
-y = diabetes.target
+
+X, y = datasets.load_diabetes(return_X_y=True)
+

 X /= X.std(axis=0)  # Standardize data (easier to set the l1_ratio parameter)

@@ -31,59 +31,59 @@
 eps = 5e-3  # the smaller it is the longer is the path

 print("Computing regularization path using the lasso...")
-alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps, fit_intercept=False)
+alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps=eps)

 print("Computing regularization path using the positive lasso...")
 alphas_positive_lasso, coefs_positive_lasso, _ = lasso_path(
-    X, y, eps, positive=True, fit_intercept=False)
+    X, y, eps=eps, positive=True
+)
 print("Computing regularization path using the elastic net...")
-alphas_enet, coefs_enet, _ = enet_path(
-    X, y, eps=eps, l1_ratio=0.8, fit_intercept=False)
+alphas_enet, coefs_enet, _ = enet_path(X, y, eps=eps, l1_ratio=0.8)

 print("Computing regularization path using the positive elastic net...")
 alphas_positive_enet, coefs_positive_enet, _ = enet_path(
-    X, y, eps=eps, l1_ratio=0.8, positive=True, fit_intercept=False)
+    X, y, eps=eps, l1_ratio=0.8, positive=True
+)

 # Display results

 plt.figure(1)
-colors = cycle(['b', 'r', 'g', 'c', 'k'])
+colors = cycle(["b", "r", "g", "c", "k"])
 neg_log_alphas_lasso = -np.log10(alphas_lasso)
 neg_log_alphas_enet = -np.log10(alphas_enet)
 for coef_l, coef_e, c in zip(coefs_lasso, coefs_enet, colors):
     l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c)
-    l2 = plt.plot(neg_log_alphas_enet, coef_e, linestyle='--', c=c)
+    l2 = plt.plot(neg_log_alphas_enet, coef_e, linestyle="--", c=c)

-plt.xlabel('-Log(alpha)')
-plt.ylabel('coefficients')
-plt.title('Lasso and Elastic-Net Paths')
-plt.legend((l1[-1], l2[-1]), ('Lasso', 'Elastic-Net'), loc='lower left')
-plt.axis('tight')
+plt.xlabel("-Log(alpha)")
+plt.ylabel("coefficients")
+plt.title("Lasso and Elastic-Net Paths")
+plt.legend((l1[-1], l2[-1]), ("Lasso", "Elastic-Net"), loc="lower left")
+plt.axis("tight")


 plt.figure(2)
 neg_log_alphas_positive_lasso = -np.log10(alphas_positive_lasso)
 for coef_l, coef_pl, c in zip(coefs_lasso, coefs_positive_lasso, colors):
     l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c)
-    l2 = plt.plot(neg_log_alphas_positive_lasso, coef_pl, linestyle='--', c=c)
+    l2 = plt.plot(neg_log_alphas_positive_lasso, coef_pl, linestyle="--", c=c)

-plt.xlabel('-Log(alpha)')
-plt.ylabel('coefficients')
-plt.title('Lasso and positive Lasso')
-plt.legend((l1[-1], l2[-1]), ('Lasso', 'positive Lasso'), loc='lower left')
-plt.axis('tight')
+plt.xlabel("-Log(alpha)")
+plt.ylabel("coefficients")
+plt.title("Lasso and positive Lasso")
+plt.legend((l1[-1], l2[-1]), ("Lasso", "positive Lasso"), loc="lower left")
+plt.axis("tight")


 plt.figure(3)
 neg_log_alphas_positive_enet = -np.log10(alphas_positive_enet)
-for (coef_e, coef_pe, c) in zip(coefs_enet, coefs_positive_enet, colors):
+for coef_e, coef_pe, c in zip(coefs_enet, coefs_positive_enet, colors):
     l1 = plt.plot(neg_log_alphas_enet, coef_e, c=c)
-    l2 = plt.plot(neg_log_alphas_positive_enet, coef_pe, linestyle='--', c=c)
+    l2 = plt.plot(neg_log_alphas_positive_enet, coef_pe, linestyle="--", c=c)

-plt.xlabel('-Log(alpha)')
-plt.ylabel('coefficients')
-plt.title('Elastic-Net and positive Elastic-Net')
-plt.legend((l1[-1], l2[-1]), ('Elastic-Net', 'positive Elastic-Net'),
-           loc='lower left')
-plt.axis('tight')
+plt.xlabel("-Log(alpha)")
+plt.ylabel("coefficients")
+plt.title("Elastic-Net and positive Elastic-Net")
+plt.legend((l1[-1], l2[-1]), ("Elastic-Net", "positive Elastic-Net"), loc="lower left")
+plt.axis("tight")
 plt.show()
('examples/linear_model', 'plot_huber_vs_ridge.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -10,12 +10,11 @@
 influenced by the outliers since the model uses the linear loss for these.
 As the parameter epsilon is increased for the Huber regressor, the decision
 function approaches that of the ridge.
+
 """

 # Authors: Manoj Kumar mks542@nyu.edu
 # License: BSD 3 clause
-
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -25,38 +24,38 @@

 # Generate toy data.
 rng = np.random.RandomState(0)
-X, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4.0,
-                       bias=100.0)
+X, y = make_regression(
+    n_samples=20, n_features=1, random_state=0, noise=4.0, bias=100.0
+)

 # Add four strong outliers to the dataset.
 X_outliers = rng.normal(0, 0.5, size=(4, 1))
 y_outliers = rng.normal(0, 2.0, size=4)
-X_outliers[:2, :] += X.max() + X.mean() / 4.
-X_outliers[2:, :] += X.min() - X.mean() / 4.
-y_outliers[:2] += y.min() - y.mean() / 4.
-y_outliers[2:] += y.max() + y.mean() / 4.
+X_outliers[:2, :] += X.max() + X.mean() / 4.0
+X_outliers[2:, :] += X.min() - X.mean() / 4.0
+y_outliers[:2] += y.min() - y.mean() / 4.0
+y_outliers[2:] += y.max() + y.mean() / 4.0
 X = np.vstack((X, X_outliers))
 y = np.concatenate((y, y_outliers))
-plt.plot(X, y, 'b.')
+plt.plot(X, y, "b.")

 # Fit the huber regressor over a series of epsilon values.
-colors = ['r-', 'b-', 'y-', 'm-']
+colors = ["r-", "b-", "y-", "m-"]

 x = np.linspace(X.min(), X.max(), 7)
-epsilon_values = [1.35, 1.5, 1.75, 1.9]
+epsilon_values = [1, 1.5, 1.75, 1.9]
 for k, epsilon in enumerate(epsilon_values):
-    huber = HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100,
-                           epsilon=epsilon)
+    huber = HuberRegressor(alpha=0.0, epsilon=epsilon)
     huber.fit(X, y)
     coef_ = huber.coef_ * x + huber.intercept_
     plt.plot(x, coef_, colors[k], label="huber loss, %s" % epsilon)

 # Fit a ridge regressor to compare it to huber regressor.
-ridge = Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True)
+ridge = Ridge(alpha=0.0, random_state=0)
 ridge.fit(X, y)
 coef_ridge = ridge.coef_
 coef_ = ridge.coef_ * x + ridge.intercept_
-plt.plot(x, coef_, 'g-', label="ridge regression")
+plt.plot(x, coef_, "g-", label="ridge regression")

 plt.title("Comparison of HuberRegressor vs Ridge")
 plt.xlabel("X")
('examples/linear_model', 'plot_logistic_path.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,4 +1,3 @@
-#!/usr/bin/env python
 """
 ==============================================
 Regularization path of L1- Logistic Regression
@@ -14,7 +13,7 @@
 coefficients are exactly 0. When regularization gets progressively looser,
 coefficients can get non-zero values one after the other.

-Here we choose the SAGA solver because it can efficiently optimize for the
+Here we choose the liblinear solver because it can efficiently optimize for the
 Logistic Regression loss with a non-smooth, sparsity inducing l1 penalty.

 Also note that we set a low value for the tolerance to make sure that the model
@@ -25,18 +24,15 @@
 full-path.

 """
-print(__doc__)

 # Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
 # License: BSD 3 clause

-from time import time
-import numpy as np
-import matplotlib.pyplot as plt
+# %%
+# Load data
+# ---------

-from sklearn import linear_model
 from sklearn import datasets
-from sklearn.svm import l1_min_c

 iris = datasets.load_iris()
 X = iris.data
@@ -47,29 +43,43 @@

 X /= X.max()  # Normalize X to speed-up convergence

-# #############################################################################
-# Demo path functions
+# %%
+# Compute regularization path
+# ---------------------------

-cs = l1_min_c(X, y, loss='log') * np.logspace(0, 7, 16)
+import numpy as np

+from sklearn import linear_model
+from sklearn.svm import l1_min_c

-print("Computing regularization path ...")
-start = time()
-clf = linear_model.LogisticRegression(penalty='l1', solver='saga',
-                                      tol=1e-6, max_iter=int(1e6),
-                                      warm_start=True)
+cs = l1_min_c(X, y, loss="log") * np.logspace(0, 7, 16)
+
+clf = linear_model.LogisticRegression(
+    penalty="l1",
+    solver="liblinear",
+    tol=1e-6,
+    max_iter=int(1e6),
+    warm_start=True,
+    intercept_scaling=10000.0,
+)
 coefs_ = []
 for c in cs:
     clf.set_params(C=c)
     clf.fit(X, y)
     coefs_.append(clf.coef_.ravel().copy())
-print("This took %0.3fs" % (time() - start))

 coefs_ = np.array(coefs_)
-plt.plot(np.log10(cs), coefs_, marker='o')
+
+# %%
+# Plot regularization path
+# ------------------------
+
+import matplotlib.pyplot as plt
+
+plt.plot(np.log10(cs), coefs_, marker="o")
 ymin, ymax = plt.ylim()
-plt.xlabel('log(C)')
-plt.ylabel('Coefficients')
-plt.title('Logistic Regression Path')
-plt.axis('tight')
+plt.xlabel("log(C)")
+plt.ylabel("Coefficients")
+plt.title("Logistic Regression Path")
+plt.axis("tight")
 plt.show()
('examples/linear_model', 'plot_lasso_lars.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,4 +1,3 @@
-#!/usr/bin/env python
 """
 =====================
 Lasso path using LARS
@@ -10,7 +9,6 @@
 of the regularization parameter.

 """
-print(__doc__)

 # Author: Fabian Pedregosa <fabian.pedregosa@inria.fr>
 #         Alexandre Gramfort <alexandre.gramfort@inria.fr>
@@ -22,21 +20,19 @@
 from sklearn import linear_model
 from sklearn import datasets

-diabetes = datasets.load_diabetes()
-X = diabetes.data
-y = diabetes.target
+X, y = datasets.load_diabetes(return_X_y=True)

 print("Computing regularization path using the LARS ...")
-_, _, coefs = linear_model.lars_path(X, y, method='lasso', verbose=True)
+_, _, coefs = linear_model.lars_path(X, y, method="lasso", verbose=True)

 xx = np.sum(np.abs(coefs.T), axis=1)
 xx /= xx[-1]

 plt.plot(xx, coefs.T)
 ymin, ymax = plt.ylim()
-plt.vlines(xx, ymin, ymax, linestyle='dashed')
-plt.xlabel('|coef| / max|coef|')
-plt.ylabel('Coefficients')
-plt.title('LASSO Path')
-plt.axis('tight')
+plt.vlines(xx, ymin, ymax, linestyle="dashed")
+plt.xlabel("|coef| / max|coef|")
+plt.ylabel("Coefficients")
+plt.title("LASSO Path")
+plt.axis("tight")
 plt.show()
('examples/linear_model', 'plot_sparse_logistic_regression_mnist.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,6 @@
 """
 =====================================================
-MNIST classfification using multinomial logistic + L1
+MNIST classification using multinomial logistic + L1
 =====================================================

 Here we fit a multinomial logistic regression with L1 penalty on a subset of
@@ -16,6 +16,10 @@
 multi-layer perceptron model on this dataset.

 """
+
+# Author: Arthur Mensch <arthur.mensch@m4x.org>
+# License: BSD 3 clause
+
 import time
 import matplotlib.pyplot as plt
 import numpy as np
@@ -26,17 +30,12 @@
 from sklearn.preprocessing import StandardScaler
 from sklearn.utils import check_random_state

-print(__doc__)
-
-# Author: Arthur Mensch <arthur.mensch@m4x.org>
-# License: BSD 3 clause
-
 # Turn down for faster convergence
 t0 = time.time()
 train_samples = 5000

 # Load data from https://www.openml.org/d/554
-X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
+X, y = fetch_openml("mnist_784", version=1, return_X_y=True, as_frame=False)

 random_state = check_random_state(0)
 permutation = random_state.permutation(X.shape[0])
@@ -45,16 +44,15 @@
 X = X.reshape((X.shape[0], -1))

 X_train, X_test, y_train, y_test = train_test_split(
-    X, y, train_size=train_samples, test_size=10000)
+    X, y, train_size=train_samples, test_size=10000
+)

 scaler = StandardScaler()
 X_train = scaler.fit_transform(X_train)
 X_test = scaler.transform(X_test)

 # Turn up tolerance for faster convergence
-clf = LogisticRegression(C=50. / train_samples,
-                         multi_class='multinomial',
-                         penalty='l1', solver='saga', tol=0.1)
+clf = LogisticRegression(C=50.0 / train_samples, penalty="l1", solver="saga", tol=0.1)
 clf.fit(X_train, y_train)
 sparsity = np.mean(clf.coef_ == 0) * 100
 score = clf.score(X_test, y_test)
@@ -67,13 +65,18 @@
 scale = np.abs(coef).max()
 for i in range(10):
     l1_plot = plt.subplot(2, 5, i + 1)
-    l1_plot.imshow(coef[i].reshape(28, 28), interpolation='nearest',
-                   cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)
+    l1_plot.imshow(
+        coef[i].reshape(28, 28),
+        interpolation="nearest",
+        cmap=plt.cm.RdBu,
+        vmin=-scale,
+        vmax=scale,
+    )
     l1_plot.set_xticks(())
     l1_plot.set_yticks(())
-    l1_plot.set_xlabel('Class %i' % i)
-plt.suptitle('Classification vector for...')
+    l1_plot.set_xlabel("Class %i" % i)
+plt.suptitle("Classification vector for...")

 run_time = time.time() - t0
-print('Example run in %.3f s' % run_time)
+print("Example run in %.3f s" % run_time)
 plt.show()
('examples/linear_model', 'plot_ard.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,116 +1,210 @@
 """
-==================================================
-Automatic Relevance Determination Regression (ARD)
-==================================================
-
-Fit regression model with Bayesian Ridge Regression.
-
-See :ref:`bayesian_ridge_regression` for more information on the regressor.
-
-Compared to the OLS (ordinary least squares) estimator, the coefficient
-weights are slightly shifted toward zeros, which stabilises them.
-
-The histogram of the estimated weights is very peaked, as a sparsity-inducing
-prior is implied on the weights.
-
-The estimation of the model is done by iteratively maximizing the
-marginal log-likelihood of the observations.
-
-We also plot predictions and uncertainties for ARD
-for one dimensional regression using polynomial feature expansion.
-Note the uncertainty starts going up on the right side of the plot.
-This is because these test samples are outside of the range of the training
-samples.
+====================================
+Comparing Linear Bayesian Regressors
+====================================
+
+This example compares two different bayesian regressors:
+
+ - a :ref:`automatic_relevance_determination`
+ - a :ref:`bayesian_ridge_regression`
+
+In the first part, we use an :ref:`ordinary_least_squares` (OLS) model as a
+baseline for comparing the models' coefficients with respect to the true
+coefficients. Thereafter, we show that the estimation of such models is done by
+iteratively maximizing the marginal log-likelihood of the observations.
+
+In the last section we plot predictions and uncertainties for the ARD and the
+Bayesian Ridge regressions using a polynomial feature expansion to fit a
+non-linear relationship between `X` and `y`.
+
 """
-print(__doc__)
-
+
+# Author: Arturo Amor <david-arturo.amor-quiroz@inria.fr>
+
+# %%
+# Models robustness to recover the ground truth weights
+# =====================================================
+#
+# Generate synthetic dataset
+# --------------------------
+#
+# We generate a dataset where `X` and `y` are linearly linked: 10 of the
+# features of `X` will be used to generate `y`. The other features are not
+# useful at predicting `y`. In addition, we generate a dataset where `n_samples
+# == n_features`. Such a setting is challenging for an OLS model and leads
+# potentially to arbitrary large weights. Having a prior on the weights and a
+# penalty alleviates the problem. Finally, gaussian noise is added.
+
+from sklearn.datasets import make_regression
+
+X, y, true_weights = make_regression(
+    n_samples=100,
+    n_features=100,
+    n_informative=10,
+    noise=8,
+    coef=True,
+    random_state=42,
+)
+
+# %%
+# Fit the regressors
+# ------------------
+#
+# We now fit both Bayesian models and the OLS to later compare the models'
+# coefficients.
+
+import pandas as pd
+from sklearn.linear_model import ARDRegression, LinearRegression, BayesianRidge
+
+olr = LinearRegression().fit(X, y)
+brr = BayesianRidge(compute_score=True, n_iter=30).fit(X, y)
+ard = ARDRegression(compute_score=True, n_iter=30).fit(X, y)
+df = pd.DataFrame(
+    {
+        "Weights of true generative process": true_weights,
+        "ARDRegression": ard.coef_,
+        "BayesianRidge": brr.coef_,
+        "LinearRegression": olr.coef_,
+    }
+)
+
+# %%
+# Plot the true and estimated coefficients
+# ----------------------------------------
+#
+# Now we compare the coefficients of each model with the weights of
+# the true generative model.
+import matplotlib.pyplot as plt
+import seaborn as sns
+from matplotlib.colors import SymLogNorm
+
+plt.figure(figsize=(10, 6))
+ax = sns.heatmap(
+    df.T,
+    norm=SymLogNorm(linthresh=10e-4, vmin=-80, vmax=80),
+    cbar_kws={"label": "coefficients' values"},
+    cmap="seismic_r",
+)
+plt.ylabel("linear model")
+plt.xlabel("coefficients")
+plt.tight_layout(rect=(0, 0, 1, 0.95))
+_ = plt.title("Models' coefficients")
+
+# %%
+# Due to the added noise, none of the models recover the true weights. Indeed,
+# all models always have more than 10 non-zero coefficients. Compared to the OLS
+# estimator, the coefficients using a Bayesian Ridge regression are slightly
+# shifted toward zero, which stabilises them. The ARD regression provides a
+# sparser solution: some of the non-informative coefficients are set exactly to
+# zero, while shifting others closer to zero. Some non-informative coefficients
+# are still present and retain large values.
+
+# %%
+# Plot the marginal log-likelihood
+# --------------------------------
 import numpy as np
-import matplotlib.pyplot as plt
-from scipy import stats
-
-from sklearn.linear_model import ARDRegression, LinearRegression
-
-# #############################################################################
-# Generating simulated data with Gaussian weights
-
-# Parameters of the example
-np.random.seed(0)
-n_samples, n_features = 100, 100
-# Create Gaussian data
-X = np.random.randn(n_samples, n_features)
-# Create weights with a precision lambda_ of 4.
-lambda_ = 4.
-w = np.zeros(n_features)
-# Only keep 10 weights of interest
-relevant_features = np.random.randint(0, n_features, 10)
-for i in relevant_features:
-    w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(lambda_))
-# Create noise with a precision alpha of 50.
-alpha_ = 50.
-noise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_), size=n_samples)
-# Create the target
-y = np.dot(X, w) + noise
-
-# #############################################################################
-# Fit the ARD Regression
-clf = ARDRegression(compute_score=True)
-clf.fit(X, y)
-
-ols = LinearRegression()
-ols.fit(X, y)
-
-# #############################################################################
-# Plot the true weights, the estimated weights, the histogram of the
-# weights, and predictions with standard deviations
-plt.figure(figsize=(6, 5))
-plt.title("Weights of the model")
-plt.plot(clf.coef_, color='darkblue', linestyle='-', linewidth=2,
-         label="ARD estimate")
-plt.plot(ols.coef_, color='yellowgreen', linestyle=':', linewidth=2,
-         label="OLS estimate")
-plt.plot(w, color='orange', linestyle='-', linewidth=2, label="Ground truth")
-plt.xlabel("Features")
-plt.ylabel("Values of the weights")
-plt.legend(loc=1)
-
-plt.figure(figsize=(6, 5))
-plt.title("Histogram of the weights")
-plt.hist(clf.coef_, bins=n_features, color='navy', log=True)
-plt.scatter(clf.coef_[relevant_features], np.full(len(relevant_features), 5.),
-            color='gold', marker='o', label="Relevant features")
-plt.ylabel("Features")
-plt.xlabel("Values of the weights")
-plt.legend(loc=1)
-
-plt.figure(figsize=(6, 5))
-plt.title("Marginal log-likelihood")
-plt.plot(clf.scores_, color='navy', linewidth=2)
-plt.ylabel("Score")
+
+ard_scores = -np.array(ard.scores_)
+brr_scores = -np.array(brr.scores_)
+plt.plot(ard_scores, color="navy", label="ARD")
+plt.plot(brr_scores, color="red", label="BayesianRidge")
+plt.ylabel("Log-likelihood")
 plt.xlabel("Iterations")
-
-
-# Plotting some predictions for polynomial regression
-def f(x, noise_amount):
-    y = np.sqrt(x) * np.sin(x)
-    noise = np.random.normal(0, 1, len(x))
-    return y + noise_amount * noise
-
-
-degree = 10
-X = np.linspace(0, 10, 100)
-y = f(X, noise_amount=1)
-clf_poly = ARDRegression(threshold_lambda=1e5)
-clf_poly.fit(np.vander(X, degree), y)
-
-X_plot = np.linspace(0, 11, 25)
-y_plot = f(X_plot, noise_amount=0)
-y_mean, y_std = clf_poly.predict(np.vander(X_plot, degree), return_std=True)
-plt.figure(figsize=(6, 5))
-plt.errorbar(X_plot, y_mean, y_std, color='navy',
-             label="Polynomial ARD", linewidth=2)
-plt.plot(X_plot, y_plot, color='gold', linewidth=2,
-         label="Ground Truth")
-plt.ylabel("Output y")
-plt.xlabel("Feature X")
-plt.legend(loc="lower left")
-plt.show()
+plt.xlim(1, 30)
+plt.legend()
+_ = plt.title("Models log-likelihood")
+
+# %%
+# Indeed, both models minimize the log-likelihood up to an arbitrary cutoff
+# defined by the `n_iter` parameter.
+#
+# Bayesian regressions with polynomial feature expansion
+# ======================================================
+# Generate synthetic dataset
+# --------------------------
+# We create a target that is a non-linear function of the input feature.
+# Noise following a standard uniform distribution is added.
+
+from sklearn.pipeline import make_pipeline
+from sklearn.preprocessing import PolynomialFeatures, StandardScaler
+
+rng = np.random.RandomState(0)
+n_samples = 110
+
+# sort the data to make plotting easier later
+X = np.sort(-10 * rng.rand(n_samples) + 10)
+noise = rng.normal(0, 1, n_samples) * 1.35
+y = np.sqrt(X) * np.sin(X) + noise
+full_data = pd.DataFrame({"input_feature": X, "target": y})
+X = X.reshape((-1, 1))
+
+# extrapolation
+X_plot = np.linspace(10, 10.4, 10)
+y_plot = np.sqrt(X_plot) * np.sin(X_plot)
+X_plot = np.concatenate((X, X_plot.reshape((-1, 1))))
+y_plot = np.concatenate((y - noise, y_plot))
+
+# %%
+# Fit the regressors
+# ------------------
+#
+# Here we try a degree 10 polynomial to potentially overfit, though the bayesian
+# linear models regularize the size of the polynomial coefficients. As
+# `fit_intercept=True` by default for
+# :class:`~sklearn.linear_model.ARDRegression` and
+# :class:`~sklearn.linear_model.BayesianRidge`, then
+# :class:`~sklearn.preprocessing.PolynomialFeatures` should not introduce an
+# additional bias feature. By setting `return_std=True`, the bayesian regressors
+# return the standard deviation of the posterior distribution for the model
+# parameters.
+
+ard_poly = make_pipeline(
+    PolynomialFeatures(degree=10, include_bias=False),
+    StandardScaler(),
+    ARDRegression(),
+).fit(X, y)
+brr_poly = make_pipeline(
+    PolynomialFeatures(degree=10, include_bias=False),
+    StandardScaler(),
+    BayesianRidge(),
+).fit(X, y)
+
+y_ard, y_ard_std = ard_poly.predict(X_plot, return_std=True)
+y_brr, y_brr_std = brr_poly.predict(X_plot, return_std=True)
+
+# %%
+# Plotting polynomial regressions with std errors of the scores
+# -------------------------------------------------------------
+
+ax = sns.scatterplot(
+    data=full_data, x="input_feature", y="target", color="black", alpha=0.75
+)
+ax.plot(X_plot, y_plot, color="black", label="Ground Truth")
+ax.plot(X_plot, y_brr, color="red", label="BayesianRidge with polynomial features")
+ax.plot(X_plot, y_ard, color="navy", label="ARD with polynomial features")
+ax.fill_between(
+    X_plot.ravel(),
+    y_ard - y_ard_std,
+    y_ard + y_ard_std,
+    color="navy",
+    alpha=0.3,
+)
+ax.fill_between(
+    X_plot.ravel(),
+    y_brr - y_brr_std,
+    y_brr + y_brr_std,
+    color="red",
+    alpha=0.3,
+)
+ax.legend()
+_ = ax.set_title("Polynomial fit of a non-linear feature")
+
+# %%
+# The error bars represent one standard deviation of the predicted gaussian
+# distribution of the query points. Notice that the ARD regression captures the
+# ground truth the best when using the default parameters in both models, but
+# further reducing the `lambda_init` hyperparameter of the Bayesian Ridge can
+# reduce its bias (see example
+# :ref:`sphx_glr_auto_examples_linear_model_plot_bayesian_ridge_curvefit.py`).
+# Finally, due to the intrinsic limitations of a polynomial regression, both
+# models fail when extrapolating.
('examples/linear_model', 'plot_lasso_model_selection.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,158 +1,253 @@
 """
-===================================================
-Lasso model selection: Cross-Validation / AIC / BIC
-===================================================
-
-Use the Akaike information criterion (AIC), the Bayes Information
-criterion (BIC) and cross-validation to select an optimal value
-of the regularization parameter alpha of the :ref:`lasso` estimator.
-
-Results obtained with LassoLarsIC are based on AIC/BIC criteria.
-
-Information-criterion based model selection is very fast, but it
-relies on a proper estimation of degrees of freedom, are
-derived for large samples (asymptotic results) and assume the model
-is correct, i.e. that the data are actually generated by this model.
-They also tend to break when the problem is badly conditioned
-(more features than samples).
-
-For cross-validation, we use 20-fold with 2 algorithms to compute the
-Lasso path: coordinate descent, as implemented by the LassoCV class, and
-Lars (least angle regression) as implemented by the LassoLarsCV class.
-Both algorithms give roughly the same results. They differ with regards
-to their execution speed and sources of numerical errors.
-
-Lars computes a path solution only for each kink in the path. As a
-result, it is very efficient when there are only of few kinks, which is
-the case if there are few features or samples. Also, it is able to
-compute the full path without setting any meta parameter. On the
-opposite, coordinate descent compute the path points on a pre-specified
-grid (here we use the default). Thus it is more efficient if the number
-of grid points is smaller than the number of kinks in the path. Such a
-strategy can be interesting if the number of features is really large
-and there are enough samples to select a large amount. In terms of
-numerical errors, for heavily correlated variables, Lars will accumulate
-more errors, while the coordinate descent algorithm will only sample the
-path on a grid.
-
-Note how the optimal value of alpha varies for each fold. This
-illustrates why nested-cross validation is necessary when trying to
-evaluate the performance of a method for which a parameter is chosen by
-cross-validation: this choice of parameter may not be optimal for unseen
-data.
+=================================================
+Lasso model selection: AIC-BIC / cross-validation
+=================================================
+
+This example focuses on model selection for Lasso models that are
+linear models with an L1 penalty for regression problems.
+
+Indeed, several strategies can be used to select the value of the
+regularization parameter: via cross-validation or using an information
+criterion, namely AIC or BIC.
+
+In what follows, we will discuss in details the different strategies.
 """
-print(__doc__)
-
-# Author: Olivier Grisel, Gael Varoquaux, Alexandre Gramfort
+
+# Author: Olivier Grisel
+#         Gael Varoquaux
+#         Alexandre Gramfort
+#         Guillaume Lemaitre
 # License: BSD 3 clause

+# %%
+# Dataset
+# -------
+# In this example, we will use the diabetes dataset.
+from sklearn.datasets import load_diabetes
+
+X, y = load_diabetes(return_X_y=True, as_frame=True)
+X.head()
+
+# %%
+# In addition, we add some random features to the original data to
+# better illustrate the feature selection performed by the Lasso model.
+import numpy as np
+import pandas as pd
+
+rng = np.random.RandomState(42)
+n_random_features = 14
+X_random = pd.DataFrame(
+    rng.randn(X.shape[0], n_random_features),
+    columns=[f"random_{i:02d}" for i in range(n_random_features)],
+)
+X = pd.concat([X, X_random], axis=1)
+# Show only a subset of the columns
+X[X.columns[::3]].head()
+
+# %%
+# Selecting Lasso via an information criterion
+# --------------------------------------------
+# :class:`~sklearn.linear_model.LassoLarsIC` provides a Lasso estimator that
+# uses the Akaike information criterion (AIC) or the Bayes information
+# criterion (BIC) to select the optimal value of the regularization
+# parameter alpha.
+#
+# Before fitting the model, we will standardize the data with a
+# :class:`~sklearn.preprocessing.StandardScaler`. In addition, we will
+# measure the time to fit and tune the hyperparameter alpha in order to
+# compare with the cross-validation strategy.
+#
+# We will first fit a Lasso model with the AIC criterion.
 import time
-
-import numpy as np
+from sklearn.preprocessing import StandardScaler
+from sklearn.linear_model import LassoLarsIC
+from sklearn.pipeline import make_pipeline
+
+start_time = time.time()
+lasso_lars_ic = make_pipeline(
+    StandardScaler(), LassoLarsIC(criterion="aic", normalize=False)
+).fit(X, y)
+fit_time = time.time() - start_time
+
+# %%
+# We store the AIC metric for each value of alpha used during `fit`.
+results = pd.DataFrame(
+    {
+        "alphas": lasso_lars_ic[-1].alphas_,
+        "AIC criterion": lasso_lars_ic[-1].criterion_,
+    }
+).set_index("alphas")
+alpha_aic = lasso_lars_ic[-1].alpha_
+
+# %%
+# Now, we perform the same analysis using the BIC criterion.
+lasso_lars_ic.set_params(lassolarsic__criterion="bic").fit(X, y)
+results["BIC criterion"] = lasso_lars_ic[-1].criterion_
+alpha_bic = lasso_lars_ic[-1].alpha_
+
+
+# %%
+# We can check which value of `alpha` leads to the minimum AIC and BIC.
+def highlight_min(x):
+    x_min = x.min()
+    return ["font-weight: bold" if v == x_min else "" for v in x]
+
+
+results.style.apply(highlight_min)
+
+# %%
+# Finally, we can plot the AIC and BIC values for the different alpha values.
+# The vertical lines in the plot correspond to the alpha chosen for each
+# criterion. The selected alpha corresponds to the minimum of the AIC or BIC
+# criterion.
+ax = results.plot()
+ax.vlines(
+    alpha_aic,
+    results["AIC criterion"].min(),
+    results["AIC criterion"].max(),
+    label="alpha: AIC estimate",
+    linestyles="--",
+    color="tab:blue",
+)
+ax.vlines(
+    alpha_bic,
+    results["BIC criterion"].min(),
+    results["BIC criterion"].max(),
+    label="alpha: BIC estimate",
+    linestyle="--",
+    color="tab:orange",
+)
+ax.set_xlabel(r"$\alpha$")
+ax.set_ylabel("criterion")
+ax.set_xscale("log")
+ax.legend()
+_ = ax.set_title(
+    f"Information-criterion for model selection (training time {fit_time:.2f}s)"
+)
+
+# %%
+# Model selection with an information-criterion is very fast. It relies on
+# computing the criterion on the in-sample set provided to `fit`. Both criteria
+# estimate the model generalization error based on the training set error and
+# penalize this overly optimistic error. However, this penalty relies on a
+# proper estimation of the degrees of freedom and the noise variance. Both are
+# derived for large samples (asymptotic results) and assume the model is
+# correct, i.e. that the data are actually generated by this model.
+#
+# These models also tend to break when the problem is badly conditioned (more
+# features than samples). It is then required to provide an estimate of the
+# noise variance.
+#
+# Selecting Lasso via cross-validation
+# ------------------------------------
+# The Lasso estimator can be implemented with different solvers: coordinate
+# descent and least angle regression. They differ with regards to their
+# execution speed and sources of numerical errors.
+#
+# In scikit-learn, two different estimators are available with integrated
+# cross-validation: :class:`~sklearn.linear_model.LassoCV` and
+# :class:`~sklearn.linear_model.LassoLarsCV` that respectively solve the
+# problem with coordinate descent and least angle regression.
+#
+# In the remainder of this section, we will present both approaches. For both
+# algorithms, we will use a 20-fold cross-validation strategy.
+#
+# Lasso via coordinate descent
+# ............................
+# Let's start by making the hyperparameter tuning using
+# :class:`~sklearn.linear_model.LassoCV`.
+from sklearn.linear_model import LassoCV
+
+start_time = time.time()
+model = make_pipeline(StandardScaler(), LassoCV(cv=20)).fit(X, y)
+fit_time = time.time() - start_time
+
+# %%
 import matplotlib.pyplot as plt

-from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC
-from sklearn import datasets
-
-# This is to avoid division by zero while doing np.log10
-EPSILON = 1e-4
-
-diabetes = datasets.load_diabetes()
-X = diabetes.data
-y = diabetes.target
-
-rng = np.random.RandomState(42)
-X = np.c_[X, rng.randn(X.shape[0], 14)]  # add some bad features
-
-# normalize data as done by Lars to allow for comparison
-X /= np.sqrt(np.sum(X ** 2, axis=0))
-
-# #############################################################################
-# LassoLarsIC: least angle regression with BIC/AIC criterion
-
-model_bic = LassoLarsIC(criterion='bic')
-t1 = time.time()
-model_bic.fit(X, y)
-t_bic = time.time() - t1
-alpha_bic_ = model_bic.alpha_
-
-model_aic = LassoLarsIC(criterion='aic')
-model_aic.fit(X, y)
-alpha_aic_ = model_aic.alpha_
-
-
-def plot_ic_criterion(model, name, color):
-    alpha_ = model.alpha_ + EPSILON
-    alphas_ = model.alphas_ + EPSILON
-    criterion_ = model.criterion_
-    plt.plot(-np.log10(alphas_), criterion_, '--', color=color,
-             linewidth=3, label='%s criterion' % name)
-    plt.axvline(-np.log10(alpha_), color=color, linewidth=3,
-                label='alpha: %s estimate' % name)
-    plt.xlabel('-log(alpha)')
-    plt.ylabel('criterion')
-
-plt.figure()
-plot_ic_criterion(model_aic, 'AIC', 'b')
-plot_ic_criterion(model_bic, 'BIC', 'r')
+ymin, ymax = 2300, 3800
+lasso = model[-1]
+plt.semilogx(lasso.alphas_, lasso.mse_path_, linestyle=":")
+plt.plot(
+    lasso.alphas_,
+    lasso.mse_path_.mean(axis=-1),
+    color="black",
+    label="Average across the folds",
+    linewidth=2,
+)
+plt.axvline(lasso.alpha_, linestyle="--", color="black", label="alpha: CV estimate")
+
+plt.ylim(ymin, ymax)
+plt.xlabel(r"$\alpha$")
+plt.ylabel("Mean square error")
 plt.legend()
-plt.title('Information-criterion for model selection (training time %.3fs)'
-          % t_bic)
-
-# #############################################################################
-# LassoCV: coordinate descent
-
-# Compute paths
-print("Computing regularization path using the coordinate descent lasso...")
-t1 = time.time()
-model = LassoCV(cv=20).fit(X, y)
-t_lasso_cv = time.time() - t1
-
-# Display results
-m_log_alphas = -np.log10(model.alphas_ + EPSILON)
-
-plt.figure()
-ymin, ymax = 2300, 3800
-plt.plot(m_log_alphas, model.mse_path_, ':')
-plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',
-         label='Average across the folds', linewidth=2)
-plt.axvline(-np.log10(model.alpha_ + EPSILON), linestyle='--', color='k',
-            label='alpha: CV estimate')
-
+_ = plt.title(
+    f"Mean square error on each fold: coordinate descent (train time: {fit_time:.2f}s)"
+)
+
+# %%
+# Lasso via least angle regression
+# ................................
+# Let's start by making the hyperparameter tuning using
+# :class:`~sklearn.linear_model.LassoLarsCV`.
+from sklearn.linear_model import LassoLarsCV
+
+start_time = time.time()
+model = make_pipeline(StandardScaler(), LassoLarsCV(cv=20, normalize=False)).fit(X, y)
+fit_time = time.time() - start_time
+
+# %%
+lasso = model[-1]
+plt.semilogx(lasso.cv_alphas_, lasso.mse_path_, ":")
+plt.semilogx(
+    lasso.cv_alphas_,
+    lasso.mse_path_.mean(axis=-1),
+    color="black",
+    label="Average across the folds",
+    linewidth=2,
+)
+plt.axvline(lasso.alpha_, linestyle="--", color="black", label="alpha CV")
+
+plt.ylim(ymin, ymax)
+plt.xlabel(r"$\alpha$")
+plt.ylabel("Mean square error")
 plt.legend()
-
-plt.xlabel('-log(alpha)')
-plt.ylabel('Mean square error')
-plt.title('Mean square error on each fold: coordinate descent '
-          '(train time: %.2fs)' % t_lasso_cv)
-plt.axis('tight')
-plt.ylim(ymin, ymax)
-
-# #############################################################################
-# LassoLarsCV: least angle regression
-
-# Compute paths
-print("Computing regularization path using the Lars lasso...")
-t1 = time.time()
-model = LassoLarsCV(cv=20).fit(X, y)
-t_lasso_lars_cv = time.time() - t1
-
-# Display results
-m_log_alphas = -np.log10(model.cv_alphas_ + EPSILON)
-
-plt.figure()
-plt.plot(m_log_alphas, model.mse_path_, ':')
-plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',
-         label='Average across the folds', linewidth=2)
-plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',
-            label='alpha CV')
-plt.legend()
-
-plt.xlabel('-log(alpha)')
-plt.ylabel('Mean square error')
-plt.title('Mean square error on each fold: Lars (train time: %.2fs)'
-          % t_lasso_lars_cv)
-plt.axis('tight')
-plt.ylim(ymin, ymax)
-
-plt.show()
+_ = plt.title(f"Mean square error on each fold: Lars (train time: {fit_time:.2f}s)")
+
+# %%
+# Summary of cross-validation approach
+# ....................................
+# Both algorithms give roughly the same results.
+#
+# Lars computes a solution path only for each kink in the path. As a result, it
+# is very efficient when there are only of few kinks, which is the case if
+# there are few features or samples. Also, it is able to compute the full path
+# without setting any hyperparameter. On the opposite, coordinate descent
+# computes the path points on a pre-specified grid (here we use the default).
+# Thus it is more efficient if the number of grid points is smaller than the
+# number of kinks in the path. Such a strategy can be interesting if the number
+# of features is really large and there are enough samples to be selected in
+# each of the cross-validation fold. In terms of numerical errors, for heavily
+# correlated variables, Lars will accumulate more errors, while the coordinate
+# descent algorithm will only sample the path on a grid.
+#
+# Note how the optimal value of alpha varies for each fold. This illustrates
+# why nested-cross validation is a good strategy when trying to evaluate the
+# performance of a method for which a parameter is chosen by cross-validation:
+# this choice of parameter may not be optimal for a final evaluation on
+# unseen test set only.
+#
+# Conclusion
+# ----------
+# In this tutorial, we presented two approaches for selecting the best
+# hyperparameter `alpha`: one strategy finds the optimal value of `alpha`
+# by only using the training set and some information criterion, and another
+# strategy is based on cross-validation.
+#
+# In this example, both approaches are working similarly. The in-sample
+# hyperparameter selection even shows its efficacy in terms of computational
+# performance. However, it can only be used when the number of samples is large
+# enough compared to the number of features.
+#
+# That's why hyperparameter optimization via cross-validation is a safe
+# strategy: it works in different settings.
('examples/linear_model', 'plot_sgd_iris.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,12 +8,12 @@
 are represented by the dashed lines.

 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
 from sklearn import datasets
 from sklearn.linear_model import SGDClassifier
+from sklearn.inspection import DecisionBoundaryDisplay

 # import some data to play with
 iris = datasets.load_iris()
@@ -36,31 +36,33 @@
 std = X.std(axis=0)
 X = (X - mean) / std

-h = .02  # step size in the mesh
-
-clf = SGDClassifier(alpha=0.001, max_iter=100, tol=1e-3).fit(X, y)
-
-# create a mesh to plot in
-x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
-y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
-xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
-                     np.arange(y_min, y_max, h))
-
-# Plot the decision boundary. For that, we will assign a color to each
-# point in the mesh [x_min, x_max]x[y_min, y_max].
-Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
-# Put the result into a color plot
-Z = Z.reshape(xx.shape)
-cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
-plt.axis('tight')
+clf = SGDClassifier(alpha=0.001, max_iter=100).fit(X, y)
+ax = plt.gca()
+DecisionBoundaryDisplay.from_estimator(
+    clf,
+    X,
+    cmap=plt.cm.Paired,
+    ax=ax,
+    response_method="predict",
+    xlabel=iris.feature_names[0],
+    ylabel=iris.feature_names[1],
+)
+plt.axis("tight")

 # Plot also the training points
 for i, color in zip(clf.classes_, colors):
     idx = np.where(y == i)
-    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],
-                cmap=plt.cm.Paired, edgecolor='black', s=20)
+    plt.scatter(
+        X[idx, 0],
+        X[idx, 1],
+        c=color,
+        label=iris.target_names[i],
+        cmap=plt.cm.Paired,
+        edgecolor="black",
+        s=20,
+    )
 plt.title("Decision surface of multi-class SGD")
-plt.axis('tight')
+plt.axis("tight")

 # Plot the three one-against-all classifiers
 xmin, xmax = plt.xlim()
@@ -73,8 +75,7 @@
     def line(x0):
         return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]

-    plt.plot([xmin, xmax], [line(xmin), line(xmax)],
-             ls="--", color=color)
+    plt.plot([xmin, xmax], [line(xmin), line(xmax)], ls="--", color=color)


 for i, color in zip(clf.classes_, colors):
('examples/linear_model', 'plot_ridge_path.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -24,19 +24,18 @@
 and the solution tends towards the ordinary least squares, coefficients
 exhibit big oscillations. In practise it is necessary to tune alpha
 in such a way that a balance is maintained between both.
+
 """

 # Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>
 # License: BSD 3 clause
-
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
 from sklearn import linear_model

 # X is the 10x10 Hilbert matrix
-X = 1. / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])
+X = 1.0 / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])
 y = np.ones(10)

 # #############################################################################
@@ -57,10 +56,10 @@
 ax = plt.gca()

 ax.plot(alphas, coefs)
-ax.set_xscale('log')
+ax.set_xscale("log")
 ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis
-plt.xlabel('alpha')
-plt.ylabel('weights')
-plt.title('Ridge coefficients as a function of the regularization')
-plt.axis('tight')
+plt.xlabel("alpha")
+plt.ylabel("weights")
+plt.title("Ridge coefficients as a function of the regularization")
+plt.axis("tight")
 plt.show()
('examples/linear_model', 'plot_sgd_comparison.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -2,11 +2,10 @@
 ==================================
 Comparing various online solvers
 ==================================
-
 An example showing how different online solvers perform
 on the hand-written digits dataset.
+"""

-"""
 # Author: Rob Zinkov <rob at zinkov dot com>
 # License: BSD 3 clause

@@ -20,23 +19,31 @@
 from sklearn.linear_model import LogisticRegression

 heldout = [0.95, 0.90, 0.75, 0.50, 0.01]
-rounds = 20
-digits = datasets.load_digits()
-X, y = digits.data, digits.target
+# Number of rounds to fit and evaluate an estimator.
+rounds = 10
+X, y = datasets.load_digits(return_X_y=True)

 classifiers = [
-    ("SGD", SGDClassifier(max_iter=100, tol=1e-3)),
-    ("ASGD", SGDClassifier(average=True, max_iter=1000, tol=1e-3)),
-    ("Perceptron", Perceptron(tol=1e-3)),
-    ("Passive-Aggressive I", PassiveAggressiveClassifier(loss='hinge',
-                                                         C=1.0, tol=1e-4)),
-    ("Passive-Aggressive II", PassiveAggressiveClassifier(loss='squared_hinge',
-                                                          C=1.0, tol=1e-4)),
-    ("SAG", LogisticRegression(solver='sag', tol=1e-1, C=1.e4 / X.shape[0],
-                               multi_class='auto'))
+    ("SGD", SGDClassifier(max_iter=110)),
+    ("ASGD", SGDClassifier(max_iter=110, average=True)),
+    ("Perceptron", Perceptron(max_iter=110)),
+    (
+        "Passive-Aggressive I",
+        PassiveAggressiveClassifier(max_iter=110, loss="hinge", C=1.0, tol=1e-4),
+    ),
+    (
+        "Passive-Aggressive II",
+        PassiveAggressiveClassifier(
+            max_iter=110, loss="squared_hinge", C=1.0, tol=1e-4
+        ),
+    ),
+    (
+        "SAG",
+        LogisticRegression(max_iter=110, solver="sag", tol=1e-1, C=1.0e4 / X.shape[0]),
+    ),
 ]

-xx = 1. - np.array(heldout)
+xx = 1.0 - np.array(heldout)

 for name, clf in classifiers:
     print("training %s" % name)
@@ -45,8 +52,9 @@
     for i in heldout:
         yy_ = []
         for r in range(rounds):
-            X_train, X_test, y_train, y_test = \
-                train_test_split(X, y, test_size=i, random_state=rng)
+            X_train, X_test, y_train, y_test = train_test_split(
+                X, y, test_size=i, random_state=rng
+            )
             clf.fit(X_train, y_train)
             y_pred = clf.predict(X_test)
             yy_.append(1 - np.mean(y_pred == y_test))
('examples/linear_model', 'plot_multi_task_lasso_support.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,4 +1,3 @@
-#!/usr/bin/env python
 """
 =============================================
 Joint feature selection with multi-task Lasso
@@ -13,7 +12,6 @@
 point. This makes feature selection by the Lasso more stable.

 """
-print(__doc__)

 # Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
 # License: BSD 3 clause
@@ -31,39 +29,43 @@
 coef = np.zeros((n_tasks, n_features))
 times = np.linspace(0, 2 * np.pi, n_tasks)
 for k in range(n_relevant_features):
-    coef[:, k] = np.sin((1. + rng.randn(1)) * times + 3 * rng.randn(1))
+    coef[:, k] = np.sin((1.0 + rng.randn(1)) * times + 3 * rng.randn(1))

 X = rng.randn(n_samples, n_features)
 Y = np.dot(X, coef.T) + rng.randn(n_samples, n_tasks)

 coef_lasso_ = np.array([Lasso(alpha=0.5).fit(X, y).coef_ for y in Y.T])
-coef_multi_task_lasso_ = MultiTaskLasso(alpha=1.).fit(X, Y).coef_
+coef_multi_task_lasso_ = MultiTaskLasso(alpha=1.0).fit(X, Y).coef_

 # #############################################################################
 # Plot support and time series
 fig = plt.figure(figsize=(8, 5))
 plt.subplot(1, 2, 1)
 plt.spy(coef_lasso_)
-plt.xlabel('Feature')
-plt.ylabel('Time (or Task)')
-plt.text(10, 5, 'Lasso')
+plt.xlabel("Feature")
+plt.ylabel("Time (or Task)")
+plt.text(10, 5, "Lasso")
 plt.subplot(1, 2, 2)
 plt.spy(coef_multi_task_lasso_)
-plt.xlabel('Feature')
-plt.ylabel('Time (or Task)')
-plt.text(10, 5, 'MultiTaskLasso')
-fig.suptitle('Coefficient non-zero location')
+plt.xlabel("Feature")
+plt.ylabel("Time (or Task)")
+plt.text(10, 5, "MultiTaskLasso")
+fig.suptitle("Coefficient non-zero location")

 feature_to_plot = 0
 plt.figure()
 lw = 2
-plt.plot(coef[:, feature_to_plot], color='seagreen', linewidth=lw,
-         label='Ground truth')
-plt.plot(coef_lasso_[:, feature_to_plot], color='cornflowerblue', linewidth=lw,
-         label='Lasso')
-plt.plot(coef_multi_task_lasso_[:, feature_to_plot], color='gold', linewidth=lw,
-         label='MultiTaskLasso')
-plt.legend(loc='upper center')
-plt.axis('tight')
+plt.plot(coef[:, feature_to_plot], color="seagreen", linewidth=lw, label="Ground truth")
+plt.plot(
+    coef_lasso_[:, feature_to_plot], color="cornflowerblue", linewidth=lw, label="Lasso"
+)
+plt.plot(
+    coef_multi_task_lasso_[:, feature_to_plot],
+    color="gold",
+    linewidth=lw,
+    label="MultiTaskLasso",
+)
+plt.legend(loc="upper center")
+plt.axis("tight")
 plt.ylim([-1.1, 1.1])
 plt.show()
('examples/linear_model', 'plot_sgd_penalties.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,11 +6,10 @@
 Contours of where the penalty is equal to 1
 for the three penalties L1, L2 and elastic-net.

-All of the above are supported by
-:class:`sklearn.linear_model.stochastic_gradient`.
+All of the above are supported by :class:`~sklearn.linear_model.SGDClassifier`
+and :class:`~sklearn.linear_model.SGDRegressor`.

 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -22,7 +21,7 @@
 line = np.linspace(-1.5, 1.5, 1001)
 xx, yy = np.meshgrid(line, line)

-l2 = xx ** 2 + yy ** 2
+l2 = xx**2 + yy**2
 l1 = np.abs(xx) + np.abs(yy)
 rho = 0.5
 elastic_net = rho * l1 + (1 - rho) * l2
@@ -30,22 +29,26 @@
 plt.figure(figsize=(10, 10), dpi=100)
 ax = plt.gca()

-elastic_net_contour = plt.contour(xx, yy, elastic_net, levels=[1],
-                                  colors=elastic_net_color)
+elastic_net_contour = plt.contour(
+    xx, yy, elastic_net, levels=[1], colors=elastic_net_color
+)
 l2_contour = plt.contour(xx, yy, l2, levels=[1], colors=l2_color)
 l1_contour = plt.contour(xx, yy, l1, levels=[1], colors=l1_color)
 ax.set_aspect("equal")
-ax.spines['left'].set_position('center')
-ax.spines['right'].set_color('none')
-ax.spines['bottom'].set_position('center')
-ax.spines['top'].set_color('none')
+ax.spines["left"].set_position("center")
+ax.spines["right"].set_color("none")
+ax.spines["bottom"].set_position("center")
+ax.spines["top"].set_color("none")

-plt.clabel(elastic_net_contour, inline=1, fontsize=18,
-           fmt={1.0: 'elastic-net'}, manual=[(-1, -1)])
-plt.clabel(l2_contour, inline=1, fontsize=18,
-           fmt={1.0: 'L2'}, manual=[(-1, -1)])
-plt.clabel(l1_contour, inline=1, fontsize=18,
-           fmt={1.0: 'L1'}, manual=[(-1, -1)])
+plt.clabel(
+    elastic_net_contour,
+    inline=1,
+    fontsize=18,
+    fmt={1.0: "elastic-net"},
+    manual=[(-1, -1)],
+)
+plt.clabel(l2_contour, inline=1, fontsize=18, fmt={1.0: "L2"}, manual=[(-1, -1)])
+plt.clabel(l1_contour, inline=1, fontsize=18, fmt={1.0: "L1"}, manual=[(-1, -1)])

 plt.tight_layout()
 plt.show()
('examples/linear_model', 'plot_ols_3d.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =========================================================
 Sparsity Example: Fitting only features 1  and 2
@@ -9,65 +7,74 @@
 Features 1 and 2 of the diabetes-dataset are fitted and
 plotted below. It illustrates that although feature 2
 has a strong coefficient on the full model, it does not
-give us much regarding `y` when compared to just feature 1
-
+give us much regarding `y` when compared to just feature 1.
 """
-print(__doc__)
-

 # Code source: Gaël Varoquaux
 # Modified for documentation by Jaques Grobler
 # License: BSD 3 clause

-import matplotlib.pyplot as plt
+# %%
+# First we load the diabetes dataset.
+
+from sklearn import datasets
 import numpy as np
-from mpl_toolkits.mplot3d import Axes3D

-from sklearn import datasets, linear_model
-
-diabetes = datasets.load_diabetes()
+X, y = datasets.load_diabetes(return_X_y=True)
 indices = (0, 1)

-X_train = diabetes.data[:-20, indices]
-X_test = diabetes.data[-20:, indices]
-y_train = diabetes.target[:-20]
-y_test = diabetes.target[-20:]
+X_train = X[:-20, indices]
+X_test = X[-20:, indices]
+y_train = y[:-20]
+y_test = y[-20:]
+
+# %%
+# Next we fit a linear regression model.
+
+from sklearn import linear_model

 ols = linear_model.LinearRegression()
-ols.fit(X_train, y_train)
+_ = ols.fit(X_train, y_train)


-# #############################################################################
-# Plot the figure
+# %%
+# Finally we plot the figure from three different views.
+
+import matplotlib.pyplot as plt
+
+
 def plot_figs(fig_num, elev, azim, X_train, clf):
     fig = plt.figure(fig_num, figsize=(4, 3))
     plt.clf()
-    ax = Axes3D(fig, elev=elev, azim=azim)
+    ax = fig.add_subplot(111, projection="3d", elev=elev, azim=azim)

-    ax.scatter(X_train[:, 0], X_train[:, 1], y_train, c='k', marker='+')
-    ax.plot_surface(np.array([[-.1, -.1], [.15, .15]]),
-                    np.array([[-.1, .15], [-.1, .15]]),
-                    clf.predict(np.array([[-.1, -.1, .15, .15],
-                                          [-.1, .15, -.1, .15]]).T
-                                ).reshape((2, 2)),
-                    alpha=.5)
-    ax.set_xlabel('X_1')
-    ax.set_ylabel('X_2')
-    ax.set_zlabel('Y')
+    ax.scatter(X_train[:, 0], X_train[:, 1], y_train, c="k", marker="+")
+    ax.plot_surface(
+        np.array([[-0.1, -0.1], [0.15, 0.15]]),
+        np.array([[-0.1, 0.15], [-0.1, 0.15]]),
+        clf.predict(
+            np.array([[-0.1, -0.1, 0.15, 0.15], [-0.1, 0.15, -0.1, 0.15]]).T
+        ).reshape((2, 2)),
+        alpha=0.5,
+    )
+    ax.set_xlabel("X_1")
+    ax.set_ylabel("X_2")
+    ax.set_zlabel("Y")
     ax.w_xaxis.set_ticklabels([])
     ax.w_yaxis.set_ticklabels([])
     ax.w_zaxis.set_ticklabels([])

-#Generate the three different figures from different views
+
+# Generate the three different figures from different views
 elev = 43.5
 azim = -110
 plot_figs(1, elev, azim, X_train, ols)

-elev = -.5
+elev = -0.5
 azim = 0
 plot_figs(2, elev, azim, X_train, ols)

-elev = -.5
+elev = -0.5
 azim = 90
 plot_figs(3, elev, azim, X_train, ols)

('examples/linear_model', 'plot_theilsen.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -32,6 +32,7 @@
 therefore also limits the runtime. Therefore, Theil-Sen is applicable to larger
 problems with the drawback of losing some of its mathematical properties since
 it then works on a random subset.
+
 """

 # Author: Florian Wilhelm -- <florian.wilhelm@gmail.com>
@@ -43,45 +44,52 @@
 from sklearn.linear_model import LinearRegression, TheilSenRegressor
 from sklearn.linear_model import RANSACRegressor

-print(__doc__)
-
-estimators = [('OLS', LinearRegression()),
-              ('Theil-Sen', TheilSenRegressor(random_state=42)),
-              ('RANSAC', RANSACRegressor(random_state=42)), ]
-colors = {'OLS': 'turquoise', 'Theil-Sen': 'gold', 'RANSAC': 'lightgreen'}
+estimators = [
+    ("OLS", LinearRegression()),
+    ("Theil-Sen", TheilSenRegressor(random_state=42)),
+    ("RANSAC", RANSACRegressor(random_state=42)),
+]
+colors = {"OLS": "turquoise", "Theil-Sen": "gold", "RANSAC": "lightgreen"}
 lw = 2

-# #############################################################################
+# %%
 # Outliers only in the y direction
+# --------------------------------

 np.random.seed(0)
 n_samples = 200
 # Linear model y = 3*x + N(2, 0.1**2)
 x = np.random.randn(n_samples)
-w = 3.
-c = 2.
+w = 3.0
+c = 2.0
 noise = 0.1 * np.random.randn(n_samples)
 y = w * x + c + noise
 # 10% outliers
 y[-20:] += -20 * x[-20:]
 X = x[:, np.newaxis]

-plt.scatter(x, y, color='indigo', marker='x', s=40)
+plt.scatter(x, y, color="indigo", marker="x", s=40)
 line_x = np.array([-3, 3])
 for name, estimator in estimators:
     t0 = time.time()
     estimator.fit(X, y)
     elapsed_time = time.time() - t0
     y_pred = estimator.predict(line_x.reshape(2, 1))
-    plt.plot(line_x, y_pred, color=colors[name], linewidth=lw,
-             label='%s (fit time: %.2fs)' % (name, elapsed_time))
+    plt.plot(
+        line_x,
+        y_pred,
+        color=colors[name],
+        linewidth=lw,
+        label="%s (fit time: %.2fs)" % (name, elapsed_time),
+    )

-plt.axis('tight')
-plt.legend(loc='upper left')
-plt.title("Corrupt y")
+plt.axis("tight")
+plt.legend(loc="upper left")
+_ = plt.title("Corrupt y")

-# #############################################################################
+# %%
 # Outliers in the X direction
+# ---------------------------

 np.random.seed(0)
 # Linear model y = 3*x + N(2, 0.1**2)
@@ -94,7 +102,7 @@
 X = x[:, np.newaxis]

 plt.figure()
-plt.scatter(x, y, color='indigo', marker='x', s=40)
+plt.scatter(x, y, color="indigo", marker="x", s=40)

 line_x = np.array([-3, 10])
 for name, estimator in estimators:
@@ -102,10 +110,15 @@
     estimator.fit(X, y)
     elapsed_time = time.time() - t0
     y_pred = estimator.predict(line_x.reshape(2, 1))
-    plt.plot(line_x, y_pred, color=colors[name], linewidth=lw,
-             label='%s (fit time: %.2fs)' % (name, elapsed_time))
+    plt.plot(
+        line_x,
+        y_pred,
+        color=colors[name],
+        linewidth=lw,
+        label="%s (fit time: %.2fs)" % (name, elapsed_time),
+    )

-plt.axis('tight')
-plt.legend(loc='upper left')
+plt.axis("tight")
+plt.legend(loc="upper left")
 plt.title("Corrupt x")
 plt.show()
('examples/linear_model', 'plot_lasso_and_elasticnet.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,15 +8,16 @@
 compared with the ground-truth.

 """
-print(__doc__)
+
+# %%
+# Data Generation
+# ---------------------------------------------------

 import numpy as np
 import matplotlib.pyplot as plt

 from sklearn.metrics import r2_score

-# #############################################################################
-# Generate some sparse data to play with
 np.random.seed(42)

 n_samples, n_features = 50, 100
@@ -33,11 +34,13 @@

 # Split data in train set and test set
 n_samples = X.shape[0]
-X_train, y_train = X[:n_samples // 2], y[:n_samples // 2]
-X_test, y_test = X[n_samples // 2:], y[n_samples // 2:]
+X_train, y_train = X[: n_samples // 2], y[: n_samples // 2]
+X_test, y_test = X[n_samples // 2 :], y[n_samples // 2 :]

-# #############################################################################
+# %%
 # Lasso
+# ---------------------------------------------------
+
 from sklearn.linear_model import Lasso

 alpha = 0.1
@@ -48,8 +51,10 @@
 print(lasso)
 print("r^2 on test data : %f" % r2_score_lasso)

-# #############################################################################
+# %%
 # ElasticNet
+# ---------------------------------------------------
+
 from sklearn.linear_model import ElasticNet

 enet = ElasticNet(alpha=alpha, l1_ratio=0.7)
@@ -59,16 +64,37 @@
 print(enet)
 print("r^2 on test data : %f" % r2_score_enet)

-m, s, _ = plt.stem(np.where(enet.coef_)[0], enet.coef_[enet.coef_ != 0],
-                   markerfmt='x', label='Elastic net coefficients')
+
+# %%
+# Plot
+# ---------------------------------------------------
+
+m, s, _ = plt.stem(
+    np.where(enet.coef_)[0],
+    enet.coef_[enet.coef_ != 0],
+    markerfmt="x",
+    label="Elastic net coefficients",
+    use_line_collection=True,
+)
 plt.setp([m, s], color="#2ca02c")
-m, s, _ = plt.stem(np.where(lasso.coef_)[0], lasso.coef_[lasso.coef_ != 0],
-                   markerfmt='x', label='Lasso coefficients')
-plt.setp([m, s], color='#ff7f0e')
-plt.stem(np.where(coef)[0], coef[coef != 0], label='true coefficients',
-         markerfmt='bx')
+m, s, _ = plt.stem(
+    np.where(lasso.coef_)[0],
+    lasso.coef_[lasso.coef_ != 0],
+    markerfmt="x",
+    label="Lasso coefficients",
+    use_line_collection=True,
+)
+plt.setp([m, s], color="#ff7f0e")
+plt.stem(
+    np.where(coef)[0],
+    coef[coef != 0],
+    label="true coefficients",
+    markerfmt="bx",
+    use_line_collection=True,
+)

-plt.legend(loc='best')
-plt.title("Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f"
-          % (r2_score_lasso, r2_score_enet))
+plt.legend(loc="best")
+plt.title(
+    "Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f" % (r2_score_lasso, r2_score_enet)
+)
 plt.show()
('examples/linear_model', 'plot_logistic.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,7 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
-
 """
 =========================================================
 Logistic function
@@ -12,8 +9,6 @@
 i.e. class one or two, using the logistic curve.

 """
-print(__doc__)
-

 # Code source: Gael Varoquaux
 # License: BSD 3 clause
@@ -21,45 +16,48 @@
 import numpy as np
 import matplotlib.pyplot as plt

-from sklearn import linear_model
+from sklearn.linear_model import LogisticRegression, LinearRegression
 from scipy.special import expit

-# General a toy dataset:s it's just a straight line with some Gaussian noise:
+# Generate a toy dataset, it's just a straight line with some Gaussian noise:
 xmin, xmax = -5, 5
 n_samples = 100
 np.random.seed(0)
 X = np.random.normal(size=n_samples)
-y = (X > 0).astype(np.float)
+y = (X > 0).astype(float)
 X[X > 0] *= 4
-X += .3 * np.random.normal(size=n_samples)
+X += 0.3 * np.random.normal(size=n_samples)

 X = X[:, np.newaxis]

 # Fit the classifier
-clf = linear_model.LogisticRegression(C=1e5, solver='lbfgs')
+clf = LogisticRegression(C=1e5)
 clf.fit(X, y)

 # and plot the result
 plt.figure(1, figsize=(4, 3))
 plt.clf()
-plt.scatter(X.ravel(), y, color='black', zorder=20)
+plt.scatter(X.ravel(), y, color="black", zorder=20)
 X_test = np.linspace(-5, 10, 300)

 loss = expit(X_test * clf.coef_ + clf.intercept_).ravel()
-plt.plot(X_test, loss, color='red', linewidth=3)
+plt.plot(X_test, loss, color="red", linewidth=3)

-ols = linear_model.LinearRegression()
+ols = LinearRegression()
 ols.fit(X, y)
 plt.plot(X_test, ols.coef_ * X_test + ols.intercept_, linewidth=1)
-plt.axhline(.5, color='.5')
+plt.axhline(0.5, color=".5")

-plt.ylabel('y')
-plt.xlabel('X')
+plt.ylabel("y")
+plt.xlabel("X")
 plt.xticks(range(-5, 10))
 plt.yticks([0, 0.5, 1])
-plt.ylim(-.25, 1.25)
+plt.ylim(-0.25, 1.25)
 plt.xlim(-4, 10)
-plt.legend(('Logistic Regression Model', 'Linear Regression Model'),
-           loc="lower right", fontsize='small')
+plt.legend(
+    ("Logistic Regression Model", "Linear Regression Model"),
+    loc="lower right",
+    fontsize="small",
+)
 plt.tight_layout()
 plt.show()
('examples/linear_model', 'plot_sgd_weighted_samples.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,8 +5,8 @@

 Plot decision function of a weighted dataset, where the size of points
 is proportional to its weight.
+
 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -22,27 +22,38 @@

 # plot the weighted data points
 xx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))
-plt.figure()
-plt.scatter(X[:, 0], X[:, 1], c=y, s=sample_weight, alpha=0.9,
-            cmap=plt.cm.bone, edgecolor='black')
+fig, ax = plt.subplots()
+ax.scatter(
+    X[:, 0],
+    X[:, 1],
+    c=y,
+    s=sample_weight,
+    alpha=0.9,
+    cmap=plt.cm.bone,
+    edgecolor="black",
+)

 # fit the unweighted model
-clf = linear_model.SGDClassifier(alpha=0.01, max_iter=100, tol=1e-3)
+clf = linear_model.SGDClassifier(alpha=0.01, max_iter=100)
 clf.fit(X, y)
 Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
 Z = Z.reshape(xx.shape)
-no_weights = plt.contour(xx, yy, Z, levels=[0], linestyles=['solid'])
+no_weights = ax.contour(xx, yy, Z, levels=[0], linestyles=["solid"])

 # fit the weighted model
-clf = linear_model.SGDClassifier(alpha=0.01, max_iter=100, tol=1e-3)
+clf = linear_model.SGDClassifier(alpha=0.01, max_iter=100)
 clf.fit(X, y, sample_weight=sample_weight)
 Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
 Z = Z.reshape(xx.shape)
-samples_weights = plt.contour(xx, yy, Z, levels=[0], linestyles=['dashed'])
+samples_weights = ax.contour(xx, yy, Z, levels=[0], linestyles=["dashed"])

-plt.legend([no_weights.collections[0], samples_weights.collections[0]],
-           ["no weights", "with weights"], loc="lower left")
+no_weights_handles, _ = no_weights.legend_elements()
+weights_handles, _ = samples_weights.legend_elements()
+ax.legend(
+    [no_weights_handles[0], weights_handles[0]],
+    ["no weights", "with weights"],
+    loc="lower left",
+)

-plt.xticks(())
-plt.yticks(())
+ax.set(xticks=(), yticks=())
 plt.show()
('examples/linear_model', 'plot_lasso_dense_vs_sparse_data.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,21 +7,29 @@
 data and that in the case of sparse data the speed is improved.

 """
-print(__doc__)

 from time import time
 from scipy import sparse
 from scipy import linalg

-from sklearn.datasets.samples_generator import make_regression
+from sklearn.datasets import make_regression
 from sklearn.linear_model import Lasso


-# #############################################################################
-# The two Lasso implementations on Dense data
-print("--- Dense matrices")
+# %%
+# Comparing the two Lasso implementations on Dense data
+# -----------------------------------------------------
+#
+# We create a linear regression problem that is suitable for the Lasso,
+# that is to say, with more features than samples. We then store the data
+# matrix in both dense (the usual) and sparse format, and train a Lasso on
+# each. We compute the runtime of both and check that they learned the
+# same model by computing the Euclidean norm of the difference between the
+# coefficients they learned. Because the data is dense, we expect better
+# runtime with a dense data format.

 X, y = make_regression(n_samples=200, n_features=5000, random_state=0)
+# create a copy of X in sparse format
 X_sp = sparse.coo_matrix(X)

 alpha = 1
@@ -30,37 +38,50 @@

 t0 = time()
 sparse_lasso.fit(X_sp, y)
-print("Sparse Lasso done in %fs" % (time() - t0))
+print(f"Sparse Lasso done in {(time() - t0):.3f}s")

 t0 = time()
 dense_lasso.fit(X, y)
-print("Dense Lasso done in %fs" % (time() - t0))
+print(f"Dense Lasso done in {(time() - t0):.3f}s")

-print("Distance between coefficients : %s"
-      % linalg.norm(sparse_lasso.coef_ - dense_lasso.coef_))
+# compare the regression coefficients
+coeff_diff = linalg.norm(sparse_lasso.coef_ - dense_lasso.coef_)
+print(f"Distance between coefficients : {coeff_diff:.2e}")

-# #############################################################################
-# The two Lasso implementations on Sparse data
-print("--- Sparse matrices")
+#
+# %%
+# Comparing the two Lasso implementations on Sparse data
+# ------------------------------------------------------
+#
+# We make the previous problem sparse by replacing all small values with 0
+# and run the same comparisons as above. Because the data is now sparse, we
+# expect the implementation that uses the sparse data format to be faster.

+# make a copy of the previous data
 Xs = X.copy()
+# make Xs sparse by replacing the values lower than 2.5 with 0s
 Xs[Xs < 2.5] = 0.0
-Xs = sparse.coo_matrix(Xs)
-Xs = Xs.tocsc()
+# create a copy of Xs in sparse format
+Xs_sp = sparse.coo_matrix(Xs)
+Xs_sp = Xs_sp.tocsc()

-print("Matrix density : %s %%" % (Xs.nnz / float(X.size) * 100))
+# compute the proportion of non-zero coefficient in the data matrix
+print(f"Matrix density : {(Xs_sp.nnz / float(X.size) * 100):.3f}%")

 alpha = 0.1
 sparse_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=10000)
 dense_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=10000)

 t0 = time()
-sparse_lasso.fit(Xs, y)
-print("Sparse Lasso done in %fs" % (time() - t0))
+sparse_lasso.fit(Xs_sp, y)
+print(f"Sparse Lasso done in {(time() - t0):.3f}s")

 t0 = time()
-dense_lasso.fit(Xs.toarray(), y)
-print("Dense Lasso done in %fs" % (time() - t0))
+dense_lasso.fit(Xs, y)
+print(f"Dense Lasso done in  {(time() - t0):.3f}s")

-print("Distance between coefficients : %s"
-      % linalg.norm(sparse_lasso.coef_ - dense_lasso.coef_))
+# compare the regression coefficients
+coeff_diff = linalg.norm(sparse_lasso.coef_ - dense_lasso.coef_)
+print(f"Distance between coefficients : {coeff_diff:.2e}")
+
+# %%
('examples/linear_model', 'plot_logistic_multinomial.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,8 +6,9 @@
 Plot decision surface of multinomial and One-vs-Rest Logistic Regression.
 The hyperplanes corresponding to the three One-vs-Rest (OVR) classifiers
 are represented by the dashed lines.
+
 """
-print(__doc__)
+
 # Authors: Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>
 # License: BSD 3 clause

@@ -15,6 +16,7 @@
 import matplotlib.pyplot as plt
 from sklearn.datasets import make_blobs
 from sklearn.linear_model import LogisticRegression
+from sklearn.inspection import DecisionBoundaryDisplay

 # make 3-class dataset for classification
 centers = [[-5, 0], [0, 1.5], [5, -1]]
@@ -22,36 +24,28 @@
 transformation = [[0.4, 0.2], [-0.4, 1.2]]
 X = np.dot(X, transformation)

-for multi_class in ('multinomial', 'ovr'):
-    clf = LogisticRegression(solver='sag', max_iter=100, random_state=42,
-                             multi_class=multi_class).fit(X, y)
+for multi_class in ("multinomial", "ovr"):
+    clf = LogisticRegression(
+        solver="sag", max_iter=100, random_state=42, multi_class=multi_class
+    ).fit(X, y)

     # print the training scores
     print("training score : %.3f (%s)" % (clf.score(X, y), multi_class))

-    # create a mesh to plot in
-    h = .02  # step size in the mesh
-    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
-    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
-    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
-                         np.arange(y_min, y_max, h))
-
-    # Plot the decision boundary. For that, we will assign a color to each
-    # point in the mesh [x_min, x_max]x[y_min, y_max].
-    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
-    # Put the result into a color plot
-    Z = Z.reshape(xx.shape)
-    plt.figure()
-    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
+    _, ax = plt.subplots()
+    DecisionBoundaryDisplay.from_estimator(
+        clf, X, response_method="predict", cmap=plt.cm.Paired, ax=ax
+    )
     plt.title("Decision surface of LogisticRegression (%s)" % multi_class)
-    plt.axis('tight')
+    plt.axis("tight")

     # Plot also the training points
     colors = "bry"
     for i, color in zip(clf.classes_, colors):
         idx = np.where(y == i)
-        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired,
-                    edgecolor='black', s=20)
+        plt.scatter(
+            X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired, edgecolor="black", s=20
+        )

     # Plot the three one-against-all classifiers
     xmin, xmax = plt.xlim()
@@ -62,8 +56,8 @@
     def plot_hyperplane(c, color):
         def line(x0):
             return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]
-        plt.plot([xmin, xmax], [line(xmin), line(xmax)],
-                 ls="--", color=color)
+
+        plt.plot([xmin, xmax], [line(xmin), line(xmax)], ls="--", color=color)

     for i, color in zip(clf.classes_, colors):
         plot_hyperplane(i, color)
('examples/linear_model', 'plot_sgd_loss_functions.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,9 +4,9 @@
 ==========================

 A plot that compares the various convex loss functions supported by
-:class:`sklearn.linear_model.SGDClassifier` .
+:class:`~sklearn.linear_model.SGDClassifier` .
+
 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -16,25 +16,32 @@
     z = y_pred * y_true
     loss = -4 * z
     loss[z >= -1] = (1 - z[z >= -1]) ** 2
-    loss[z >= 1.] = 0
+    loss[z >= 1.0] = 0
     return loss


 xmin, xmax = -4, 4
 xx = np.linspace(xmin, xmax, 100)
 lw = 2
-plt.plot([xmin, 0, 0, xmax], [1, 1, 0, 0], color='gold', lw=lw,
-         label="Zero-one loss")
-plt.plot(xx, np.where(xx < 1, 1 - xx, 0), color='teal', lw=lw,
-         label="Hinge loss")
-plt.plot(xx, -np.minimum(xx, 0), color='yellowgreen', lw=lw,
-         label="Perceptron loss")
-plt.plot(xx, np.log2(1 + np.exp(-xx)), color='cornflowerblue', lw=lw,
-         label="Log loss")
-plt.plot(xx, np.where(xx < 1, 1 - xx, 0) ** 2, color='orange', lw=lw,
-         label="Squared hinge loss")
-plt.plot(xx, modified_huber_loss(xx, 1), color='darkorchid', lw=lw,
-         linestyle='--', label="Modified Huber loss")
+plt.plot([xmin, 0, 0, xmax], [1, 1, 0, 0], color="gold", lw=lw, label="Zero-one loss")
+plt.plot(xx, np.where(xx < 1, 1 - xx, 0), color="teal", lw=lw, label="Hinge loss")
+plt.plot(xx, -np.minimum(xx, 0), color="yellowgreen", lw=lw, label="Perceptron loss")
+plt.plot(xx, np.log2(1 + np.exp(-xx)), color="cornflowerblue", lw=lw, label="Log loss")
+plt.plot(
+    xx,
+    np.where(xx < 1, 1 - xx, 0) ** 2,
+    color="orange",
+    lw=lw,
+    label="Squared hinge loss",
+)
+plt.plot(
+    xx,
+    modified_huber_loss(xx, 1),
+    color="darkorchid",
+    lw=lw,
+    linestyle="--",
+    label="Modified Huber loss",
+)
 plt.ylim((0, 8))
 plt.legend(loc="upper right")
 plt.xlabel(r"Decision function $f(x)$")
('examples/linear_model', 'plot_ols_ridge_variance.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =========================================================
 Ordinary Least Squares and Ridge Regression Variance
@@ -19,9 +17,8 @@
 of the prediction is much more stable and the variance
 in the line itself is greatly reduced, in comparison to that
 of the standard linear regression
+
 """
-print(__doc__)
-

 # Code source: Gaël Varoquaux
 # Modified for documentation by Jaques Grobler
@@ -33,34 +30,35 @@

 from sklearn import linear_model

-X_train = np.c_[.5, 1].T
-y_train = [.5, 1]
+X_train = np.c_[0.5, 1].T
+y_train = [0.5, 1]
 X_test = np.c_[0, 2].T

 np.random.seed(0)

-classifiers = dict(ols=linear_model.LinearRegression(),
-                   ridge=linear_model.Ridge(alpha=.1))
+classifiers = dict(
+    ols=linear_model.LinearRegression(), ridge=linear_model.Ridge(alpha=0.1)
+)

 for name, clf in classifiers.items():
     fig, ax = plt.subplots(figsize=(4, 3))

     for _ in range(6):
-        this_X = .1 * np.random.normal(size=(2, 1)) + X_train
+        this_X = 0.1 * np.random.normal(size=(2, 1)) + X_train
         clf.fit(this_X, y_train)

-        ax.plot(X_test, clf.predict(X_test), color='gray')
-        ax.scatter(this_X, y_train, s=3, c='gray', marker='o', zorder=10)
+        ax.plot(X_test, clf.predict(X_test), color="gray")
+        ax.scatter(this_X, y_train, s=3, c="gray", marker="o", zorder=10)

     clf.fit(X_train, y_train)
-    ax.plot(X_test, clf.predict(X_test), linewidth=2, color='blue')
-    ax.scatter(X_train, y_train, s=30, c='red', marker='+', zorder=10)
+    ax.plot(X_test, clf.predict(X_test), linewidth=2, color="blue")
+    ax.scatter(X_train, y_train, s=30, c="red", marker="+", zorder=10)

     ax.set_title(name)
     ax.set_xlim(0, 2)
     ax.set_ylim((0, 1.6))
-    ax.set_xlabel('X')
-    ax.set_ylabel('y')
+    ax.set_xlabel("X")
+    ax.set_ylabel("y")

     fig.tight_layout()

('examples/linear_model', 'plot_omp.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,8 +5,8 @@

 Using orthogonal matching pursuit for recovering a sparse signal from a noisy
 measurement encoded with a dictionary
+
 """
-print(__doc__)

 import matplotlib.pyplot as plt
 import numpy as np
@@ -18,65 +18,60 @@
 n_nonzero_coefs = 17

 # generate the data
-###################

 # y = Xw
 # |x|_0 = n_nonzero_coefs

-y, X, w = make_sparse_coded_signal(n_samples=1,
-                                   n_components=n_components,
-                                   n_features=n_features,
-                                   n_nonzero_coefs=n_nonzero_coefs,
-                                   random_state=0)
+y, X, w = make_sparse_coded_signal(
+    n_samples=1,
+    n_components=n_components,
+    n_features=n_features,
+    n_nonzero_coefs=n_nonzero_coefs,
+    random_state=0,
+    data_transposed=True,
+)

-idx, = w.nonzero()
+(idx,) = w.nonzero()

 # distort the clean signal
-##########################
 y_noisy = y + 0.05 * np.random.randn(len(y))

 # plot the sparse signal
-########################
 plt.figure(figsize=(7, 7))
 plt.subplot(4, 1, 1)
 plt.xlim(0, 512)
 plt.title("Sparse signal")
-plt.stem(idx, w[idx])
+plt.stem(idx, w[idx], use_line_collection=True)

 # plot the noise-free reconstruction
-####################################
-
-omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)
+omp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs, normalize=False)
 omp.fit(X, y)
 coef = omp.coef_
-idx_r, = coef.nonzero()
+(idx_r,) = coef.nonzero()
 plt.subplot(4, 1, 2)
 plt.xlim(0, 512)
 plt.title("Recovered signal from noise-free measurements")
-plt.stem(idx_r, coef[idx_r])
+plt.stem(idx_r, coef[idx_r], use_line_collection=True)

 # plot the noisy reconstruction
-###############################
 omp.fit(X, y_noisy)
 coef = omp.coef_
-idx_r, = coef.nonzero()
+(idx_r,) = coef.nonzero()
 plt.subplot(4, 1, 3)
 plt.xlim(0, 512)
 plt.title("Recovered signal from noisy measurements")
-plt.stem(idx_r, coef[idx_r])
+plt.stem(idx_r, coef[idx_r], use_line_collection=True)

 # plot the noisy reconstruction with number of non-zeros set by CV
-##################################################################
-omp_cv = OrthogonalMatchingPursuitCV(cv=5)
+omp_cv = OrthogonalMatchingPursuitCV(normalize=False)
 omp_cv.fit(X, y_noisy)
 coef = omp_cv.coef_
-idx_r, = coef.nonzero()
+(idx_r,) = coef.nonzero()
 plt.subplot(4, 1, 4)
 plt.xlim(0, 512)
 plt.title("Recovered signal from noisy measurements with CV")
-plt.stem(idx_r, coef[idx_r])
+plt.stem(idx_r, coef[idx_r], use_line_collection=True)

 plt.subplots_adjust(0.06, 0.04, 0.94, 0.90, 0.20, 0.38)
-plt.suptitle('Sparse signal recovery with Orthogonal Matching Pursuit',
-             fontsize=16)
+plt.suptitle("Sparse signal recovery with Orthogonal Matching Pursuit", fontsize=16)
 plt.show()
('examples/linear_model', 'plot_iris_logistic.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =========================================================
 Logistic Regression 3-class Classifier
@@ -12,47 +10,43 @@
 are colored according to their labels.

 """
-print(__doc__)

 # Code source: Gaël Varoquaux
 # Modified for documentation by Jaques Grobler
 # License: BSD 3 clause

-import numpy as np
 import matplotlib.pyplot as plt
 from sklearn.linear_model import LogisticRegression
 from sklearn import datasets
+from sklearn.inspection import DecisionBoundaryDisplay

 # import some data to play with
 iris = datasets.load_iris()
 X = iris.data[:, :2]  # we only take the first two features.
 Y = iris.target

-logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')
-
 # Create an instance of Logistic Regression Classifier and fit the data.
+logreg = LogisticRegression(C=1e5)
 logreg.fit(X, Y)

-# Plot the decision boundary. For that, we will assign a color to each
-# point in the mesh [x_min, x_max]x[y_min, y_max].
-x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
-y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
-h = .02  # step size in the mesh
-xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
-Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])
-
-# Put the result into a color plot
-Z = Z.reshape(xx.shape)
-plt.figure(1, figsize=(4, 3))
-plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)
+_, ax = plt.subplots(figsize=(4, 3))
+DecisionBoundaryDisplay.from_estimator(
+    logreg,
+    X,
+    cmap=plt.cm.Paired,
+    ax=ax,
+    response_method="predict",
+    plot_method="pcolormesh",
+    shading="auto",
+    xlabel="Sepal length",
+    ylabel="Sepal width",
+    eps=0.5,
+)

 # Plot also the training points
-plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)
-plt.xlabel('Sepal length')
-plt.ylabel('Sepal width')
+plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors="k", cmap=plt.cm.Paired)

-plt.xlim(xx.min(), xx.max())
-plt.ylim(yy.min(), yy.max())
+
 plt.xticks(())
 plt.yticks(())

('examples/linear_model', 'plot_sparse_logistic_regression_20newsgroups.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,7 +1,7 @@
 """
-=====================================================
-Multiclass sparse logisitic regression on newgroups20
-=====================================================
+====================================================
+Multiclass sparse logistic regression on 20newgroups
+====================================================

 Comparison of multinomial logistic L1 vs one-versus-rest L1 logistic regression
 to classify documents from the newgroups20 dataset. Multinomial logistic
@@ -17,7 +17,11 @@
 A more traditional (and possibly better) way to predict on a sparse subset of
 input features would be to use univariate feature selection followed by a
 traditional (l2-penalised) logistic regression model.
+
 """
+
+# Author: Arthur Mensch
+
 import timeit
 import warnings

@@ -29,38 +33,34 @@
 from sklearn.model_selection import train_test_split
 from sklearn.exceptions import ConvergenceWarning

-print(__doc__)
-# Author: Arthur Mensch
-
-warnings.filterwarnings("ignore", category=ConvergenceWarning,
-                        module="sklearn")
+warnings.filterwarnings("ignore", category=ConvergenceWarning, module="sklearn")
 t0 = timeit.default_timer()

 # We use SAGA solver
-solver = 'saga'
+solver = "saga"

 # Turn down for faster run time
-n_samples = 10000
+n_samples = 5000

-# Memorized fetch_rcv1 for faster access
-dataset = fetch_20newsgroups_vectorized('all')
-X = dataset.data
-y = dataset.target
+X, y = fetch_20newsgroups_vectorized(subset="all", return_X_y=True)
 X = X[:n_samples]
 y = y[:n_samples]

-X_train, X_test, y_train, y_test = train_test_split(X, y,
-                                                    random_state=42,
-                                                    stratify=y,
-                                                    test_size=0.1)
+X_train, X_test, y_train, y_test = train_test_split(
+    X, y, random_state=42, stratify=y, test_size=0.1
+)
 train_samples, n_features = X_train.shape
 n_classes = np.unique(y).shape[0]

-print('Dataset 20newsgroup, train_samples=%i, n_features=%i, n_classes=%i'
-      % (train_samples, n_features, n_classes))
+print(
+    "Dataset 20newsgroup, train_samples=%i, n_features=%i, n_classes=%i"
+    % (train_samples, n_features, n_classes)
+)

-models = {'ovr': {'name': 'One versus Rest', 'iters': [1, 2, 4]},
-          'multinomial': {'name': 'Multinomial', 'iters': [1, 3, 7]}}
+models = {
+    "ovr": {"name": "One versus Rest", "iters": [1, 2, 3]},
+    "multinomial": {"name": "Multinomial", "iters": [1, 2, 5]},
+}

 for model in models:
     # Add initial chance-level values for plotting purpose
@@ -71,17 +71,18 @@
     model_params = models[model]

     # Small number of epochs for fast runtime
-    for this_max_iter in model_params['iters']:
-        print('[model=%s, solver=%s] Number of epochs: %s' %
-              (model_params['name'], solver, this_max_iter))
-        lr = LogisticRegression(solver=solver,
-                                multi_class=model,
-                                C=1,
-                                penalty='l1',
-                                fit_intercept=True,
-                                max_iter=this_max_iter,
-                                random_state=42,
-                                )
+    for this_max_iter in model_params["iters"]:
+        print(
+            "[model=%s, solver=%s] Number of epochs: %s"
+            % (model_params["name"], solver, this_max_iter)
+        )
+        lr = LogisticRegression(
+            solver=solver,
+            multi_class=model,
+            penalty="l1",
+            max_iter=this_max_iter,
+            random_state=42,
+        )
         t1 = timeit.default_timer()
         lr.fit(X_train, y_train)
         train_time = timeit.default_timer() - t1
@@ -92,31 +93,33 @@
         accuracies.append(accuracy)
         densities.append(density)
         times.append(train_time)
-    models[model]['times'] = times
-    models[model]['densities'] = densities
-    models[model]['accuracies'] = accuracies
-    print('Test accuracy for model %s: %.4f' % (model, accuracies[-1]))
-    print('%% non-zero coefficients for model %s, '
-          'per class:\n %s' % (model, densities[-1]))
-    print('Run time (%i epochs) for model %s:'
-          '%.2f' % (model_params['iters'][-1], model, times[-1]))
+    models[model]["times"] = times
+    models[model]["densities"] = densities
+    models[model]["accuracies"] = accuracies
+    print("Test accuracy for model %s: %.4f" % (model, accuracies[-1]))
+    print(
+        "%% non-zero coefficients for model %s, per class:\n %s"
+        % (model, densities[-1])
+    )
+    print(
+        "Run time (%i epochs) for model %s:%.2f"
+        % (model_params["iters"][-1], model, times[-1])
+    )

 fig = plt.figure()
 ax = fig.add_subplot(111)

 for model in models:
-    name = models[model]['name']
-    times = models[model]['times']
-    accuracies = models[model]['accuracies']
-    ax.plot(times, accuracies, marker='o',
-            label='Model: %s' % name)
-    ax.set_xlabel('Train time (s)')
-    ax.set_ylabel('Test accuracy')
+    name = models[model]["name"]
+    times = models[model]["times"]
+    accuracies = models[model]["accuracies"]
+    ax.plot(times, accuracies, marker="o", label="Model: %s" % name)
+    ax.set_xlabel("Train time (s)")
+    ax.set_ylabel("Test accuracy")
 ax.legend()
-fig.suptitle('Multinomial vs One-vs-Rest Logistic L1\n'
-             'Dataset %s' % '20newsgroups')
+fig.suptitle("Multinomial vs One-vs-Rest Logistic L1\nDataset %s" % "20newsgroups")
 fig.tight_layout()
 fig.subplots_adjust(top=0.85)
 run_time = timeit.default_timer() - t0
-print('Example run in %.3f s' % run_time)
+print("Example run in %.3f s" % run_time)
 plt.show()
('examples/linear_model', 'plot_robust_fit.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -34,7 +34,11 @@
 import numpy as np

 from sklearn.linear_model import (
-    LinearRegression, TheilSenRegressor, RANSACRegressor, HuberRegressor)
+    LinearRegression,
+    TheilSenRegressor,
+    RANSACRegressor,
+    HuberRegressor,
+)
 from sklearn.metrics import mean_squared_error
 from sklearn.preprocessing import PolynomialFeatures
 from sklearn.pipeline import make_pipeline
@@ -62,35 +66,50 @@
 X_errors_large = X.copy()
 X_errors_large[::3] = 10

-estimators = [('OLS', LinearRegression()),
-              ('Theil-Sen', TheilSenRegressor(random_state=42)),
-              ('RANSAC', RANSACRegressor(random_state=42)),
-              ('HuberRegressor', HuberRegressor())]
-colors = {'OLS': 'turquoise', 'Theil-Sen': 'gold', 'RANSAC': 'lightgreen', 'HuberRegressor': 'black'}
-linestyle = {'OLS': '-', 'Theil-Sen': '-.', 'RANSAC': '--', 'HuberRegressor': '--'}
+estimators = [
+    ("OLS", LinearRegression()),
+    ("Theil-Sen", TheilSenRegressor(random_state=42)),
+    ("RANSAC", RANSACRegressor(random_state=42)),
+    ("HuberRegressor", HuberRegressor()),
+]
+colors = {
+    "OLS": "turquoise",
+    "Theil-Sen": "gold",
+    "RANSAC": "lightgreen",
+    "HuberRegressor": "black",
+}
+linestyle = {"OLS": "-", "Theil-Sen": "-.", "RANSAC": "--", "HuberRegressor": "--"}
 lw = 3

 x_plot = np.linspace(X.min(), X.max())
 for title, this_X, this_y in [
-        ('Modeling Errors Only', X, y),
-        ('Corrupt X, Small Deviants', X_errors, y),
-        ('Corrupt y, Small Deviants', X, y_errors),
-        ('Corrupt X, Large Deviants', X_errors_large, y),
-        ('Corrupt y, Large Deviants', X, y_errors_large)]:
+    ("Modeling Errors Only", X, y),
+    ("Corrupt X, Small Deviants", X_errors, y),
+    ("Corrupt y, Small Deviants", X, y_errors),
+    ("Corrupt X, Large Deviants", X_errors_large, y),
+    ("Corrupt y, Large Deviants", X, y_errors_large),
+]:
     plt.figure(figsize=(5, 4))
-    plt.plot(this_X[:, 0], this_y, 'b+')
+    plt.plot(this_X[:, 0], this_y, "b+")

     for name, estimator in estimators:
         model = make_pipeline(PolynomialFeatures(3), estimator)
         model.fit(this_X, this_y)
         mse = mean_squared_error(model.predict(X_test), y_test)
         y_plot = model.predict(x_plot[:, np.newaxis])
-        plt.plot(x_plot, y_plot, color=colors[name], linestyle=linestyle[name],
-                 linewidth=lw, label='%s: error = %.3f' % (name, mse))
+        plt.plot(
+            x_plot,
+            y_plot,
+            color=colors[name],
+            linestyle=linestyle[name],
+            linewidth=lw,
+            label="%s: error = %.3f" % (name, mse),
+        )

-    legend_title = 'Error of Mean\nAbsolute Deviation\nto Non-corrupt Data'
-    legend = plt.legend(loc='upper right', frameon=False, title=legend_title,
-                        prop=dict(size='x-small'))
+    legend_title = "Error of Mean\nAbsolute Deviation\nto Non-corrupt Data"
+    legend = plt.legend(
+        loc="upper right", frameon=False, title=legend_title, prop=dict(size="x-small")
+    )
     plt.xlim(-4, 10.2)
     plt.ylim(-2, 10.2)
     plt.title(title)
('examples/impute', 'plot_missing_values.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,139 +4,307 @@
 ====================================================

 Missing values can be replaced by the mean, the median or the most frequent
-value using the basic :class:`sklearn.impute.SimpleImputer`.
-The median is a more robust estimator for data with high magnitude variables
-which could dominate results (otherwise known as a 'long tail').
-
-Another option is the :class:`sklearn.impute.IterativeImputer`. This uses
-round-robin linear regression, treating every variable as an output in
-turn. The version implemented assumes Gaussian (output) variables. If your
-features are obviously non-Normal, consider transforming them to look more
-Normal so as to potentially improve performance.
-
-In addition of using an imputing method, we can also keep an indication of the
-missing information using :func:`sklearn.impute.MissingIndicator` which might
-carry some information.
+value using the basic :class:`~sklearn.impute.SimpleImputer`.
+
+In this example we will investigate different imputation techniques:
+
+- imputation by the constant value 0
+- imputation by the mean value of each feature combined with a missing-ness
+  indicator auxiliary variable
+- k nearest neighbor imputation
+- iterative imputation
+
+We will use two datasets: Diabetes dataset which consists of 10 feature
+variables collected from diabetes patients with an aim to predict disease
+progression and California Housing dataset for which the target is the median
+house value for California districts.
+
+As neither of these datasets have missing values, we will remove some
+values to create new versions with artificially missing data. The performance
+of
+:class:`~sklearn.ensemble.RandomForestRegressor` on the full original dataset
+is then compared the performance on the altered datasets with the artificially
+missing values imputed using different techniques.
+
 """
-print(__doc__)
+
+# Authors: Maria Telenczuk  <https://github.com/maikia>
+# License: BSD 3 clause
+
+# %%
+# Download the data and make missing values sets
+################################################
+#
+# First we download the two datasets. Diabetes dataset is shipped with
+# scikit-learn. It has 442 entries, each with 10 features. California Housing
+# dataset is much larger with 20640 entries and 8 features. It needs to be
+# downloaded. We will only use the first 400 entries for the sake of speeding
+# up the calculations but feel free to use the whole dataset.
+#

 import numpy as np
-import matplotlib.pyplot as plt
-
-# To use the experimental IterativeImputer, we need to explicitly ask for it:
-from sklearn.experimental import enable_iterative_imputer  # noqa
+
+from sklearn.datasets import fetch_california_housing
 from sklearn.datasets import load_diabetes
-from sklearn.datasets import load_boston
-from sklearn.ensemble import RandomForestRegressor
-from sklearn.pipeline import make_pipeline, make_union
-from sklearn.impute import SimpleImputer, IterativeImputer, MissingIndicator
-from sklearn.model_selection import cross_val_score
-
-rng = np.random.RandomState(0)
-
-N_SPLITS = 5
-REGRESSOR = RandomForestRegressor(random_state=0, n_estimators=100)
-
-
-def get_scores_for_imputer(imputer, X_missing, y_missing):
-    estimator = make_pipeline(
-        make_union(imputer, MissingIndicator(missing_values=0)),
-        REGRESSOR)
-    impute_scores = cross_val_score(estimator, X_missing, y_missing,
-                                    scoring='neg_mean_squared_error',
-                                    cv=N_SPLITS)
-    return impute_scores
-
-
-def get_results(dataset):
-    X_full, y_full = dataset.data, dataset.target
-    n_samples = X_full.shape[0]
-    n_features = X_full.shape[1]
-
-    # Estimate the score on the entire dataset, with no missing values
-    full_scores = cross_val_score(REGRESSOR, X_full, y_full,
-                                  scoring='neg_mean_squared_error',
-                                  cv=N_SPLITS)
+
+
+rng = np.random.RandomState(42)
+
+X_diabetes, y_diabetes = load_diabetes(return_X_y=True)
+X_california, y_california = fetch_california_housing(return_X_y=True)
+X_california = X_california[:400]
+y_california = y_california[:400]
+
+
+def add_missing_values(X_full, y_full):
+    n_samples, n_features = X_full.shape

     # Add missing values in 75% of the lines
     missing_rate = 0.75
-    n_missing_samples = int(np.floor(n_samples * missing_rate))
-    missing_samples = np.hstack((np.zeros(n_samples - n_missing_samples,
-                                          dtype=np.bool),
-                                 np.ones(n_missing_samples,
-                                         dtype=np.bool)))
+    n_missing_samples = int(n_samples * missing_rate)
+
+    missing_samples = np.zeros(n_samples, dtype=bool)
+    missing_samples[:n_missing_samples] = True
+
     rng.shuffle(missing_samples)
     missing_features = rng.randint(0, n_features, n_missing_samples)
     X_missing = X_full.copy()
-    X_missing[np.where(missing_samples)[0], missing_features] = 0
+    X_missing[missing_samples, missing_features] = np.nan
     y_missing = y_full.copy()

-    # Estimate the score after replacing missing values by 0
-    imputer = SimpleImputer(missing_values=0,
-                            strategy='constant',
-                            fill_value=0)
+    return X_missing, y_missing
+
+
+X_miss_california, y_miss_california = add_missing_values(X_california, y_california)
+
+X_miss_diabetes, y_miss_diabetes = add_missing_values(X_diabetes, y_diabetes)
+
+
+# %%
+# Impute the missing data and score
+# #################################
+# Now we will write a function which will score the results on the differently
+# imputed data. Let's look at each imputer separately:
+#
+
+rng = np.random.RandomState(0)
+
+from sklearn.ensemble import RandomForestRegressor
+
+# To use the experimental IterativeImputer, we need to explicitly ask for it:
+from sklearn.experimental import enable_iterative_imputer  # noqa
+from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
+from sklearn.model_selection import cross_val_score
+from sklearn.pipeline import make_pipeline
+
+
+N_SPLITS = 5
+regressor = RandomForestRegressor(random_state=0)
+
+# %%
+# Missing information
+# -------------------
+# In addition to imputing the missing values, the imputers have an
+# `add_indicator` parameter that marks the values that were missing, which
+# might carry some information.
+#
+
+
+def get_scores_for_imputer(imputer, X_missing, y_missing):
+    estimator = make_pipeline(imputer, regressor)
+    impute_scores = cross_val_score(
+        estimator, X_missing, y_missing, scoring="neg_mean_squared_error", cv=N_SPLITS
+    )
+    return impute_scores
+
+
+x_labels = []
+
+mses_california = np.zeros(5)
+stds_california = np.zeros(5)
+mses_diabetes = np.zeros(5)
+stds_diabetes = np.zeros(5)
+
+# %%
+# Estimate the score
+# ------------------
+# First, we want to estimate the score on the original data:
+#
+
+
+def get_full_score(X_full, y_full):
+    full_scores = cross_val_score(
+        regressor, X_full, y_full, scoring="neg_mean_squared_error", cv=N_SPLITS
+    )
+    return full_scores.mean(), full_scores.std()
+
+
+mses_california[0], stds_california[0] = get_full_score(X_california, y_california)
+mses_diabetes[0], stds_diabetes[0] = get_full_score(X_diabetes, y_diabetes)
+x_labels.append("Full data")
+
+
+# %%
+# Replace missing values by 0
+# ---------------------------
+#
+# Now we will estimate the score on the data where the missing values are
+# replaced by 0:
+#
+
+
+def get_impute_zero_score(X_missing, y_missing):
+
+    imputer = SimpleImputer(
+        missing_values=np.nan, add_indicator=True, strategy="constant", fill_value=0
+    )
     zero_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)
-
-    # Estimate the score after imputation (mean strategy) of the missing values
-    imputer = SimpleImputer(missing_values=0, strategy="mean")
+    return zero_impute_scores.mean(), zero_impute_scores.std()
+
+
+mses_california[1], stds_california[1] = get_impute_zero_score(
+    X_miss_california, y_miss_california
+)
+mses_diabetes[1], stds_diabetes[1] = get_impute_zero_score(
+    X_miss_diabetes, y_miss_diabetes
+)
+x_labels.append("Zero imputation")
+
+
+# %%
+# kNN-imputation of the missing values
+# ------------------------------------
+#
+# :class:`~sklearn.impute.KNNImputer` imputes missing values using the weighted
+# or unweighted mean of the desired number of nearest neighbors.
+
+
+def get_impute_knn_score(X_missing, y_missing):
+    imputer = KNNImputer(missing_values=np.nan, add_indicator=True)
+    knn_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)
+    return knn_impute_scores.mean(), knn_impute_scores.std()
+
+
+mses_california[2], stds_california[2] = get_impute_knn_score(
+    X_miss_california, y_miss_california
+)
+mses_diabetes[2], stds_diabetes[2] = get_impute_knn_score(
+    X_miss_diabetes, y_miss_diabetes
+)
+x_labels.append("KNN Imputation")
+
+
+# %%
+# Impute missing values with mean
+# -------------------------------
+#
+
+
+def get_impute_mean(X_missing, y_missing):
+    imputer = SimpleImputer(missing_values=np.nan, strategy="mean", add_indicator=True)
     mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)
-
-    # Estimate the score after iterative imputation of the missing values
-    imputer = IterativeImputer(missing_values=0,
-                               random_state=0,
-                               n_nearest_features=5)
-    iterative_impute_scores = get_scores_for_imputer(imputer,
-                                                     X_missing,
-                                                     y_missing)
-
-    return ((full_scores.mean(), full_scores.std()),
-            (zero_impute_scores.mean(), zero_impute_scores.std()),
-            (mean_impute_scores.mean(), mean_impute_scores.std()),
-            (iterative_impute_scores.mean(), iterative_impute_scores.std()))
-
-
-results_diabetes = np.array(get_results(load_diabetes()))
-mses_diabetes = results_diabetes[:, 0] * -1
-stds_diabetes = results_diabetes[:, 1]
-
-results_boston = np.array(get_results(load_boston()))
-mses_boston = results_boston[:, 0] * -1
-stds_boston = results_boston[:, 1]
+    return mean_impute_scores.mean(), mean_impute_scores.std()
+
+
+mses_california[3], stds_california[3] = get_impute_mean(
+    X_miss_california, y_miss_california
+)
+mses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes, y_miss_diabetes)
+x_labels.append("Mean Imputation")
+
+
+# %%
+# Iterative imputation of the missing values
+# ------------------------------------------
+#
+# Another option is the :class:`~sklearn.impute.IterativeImputer`. This uses
+# round-robin linear regression, modeling each feature with missing values as a
+# function of other features, in turn.
+# The version implemented assumes Gaussian (output) variables. If your features
+# are obviously non-normal, consider transforming them to look more normal
+# to potentially improve performance.
+#
+
+
+def get_impute_iterative(X_missing, y_missing):
+    imputer = IterativeImputer(
+        missing_values=np.nan,
+        add_indicator=True,
+        random_state=0,
+        n_nearest_features=5,
+        sample_posterior=True,
+    )
+    iterative_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)
+    return iterative_impute_scores.mean(), iterative_impute_scores.std()
+
+
+mses_california[4], stds_california[4] = get_impute_iterative(
+    X_miss_california, y_miss_california
+)
+mses_diabetes[4], stds_diabetes[4] = get_impute_iterative(
+    X_miss_diabetes, y_miss_diabetes
+)
+x_labels.append("Iterative Imputation")
+
+mses_diabetes = mses_diabetes * -1
+mses_california = mses_california * -1
+
+# %%
+# Plot the results
+# ################
+#
+# Finally we are going to visualize the score:
+#
+
+import matplotlib.pyplot as plt
+

 n_bars = len(mses_diabetes)
 xval = np.arange(n_bars)

-x_labels = ['Full data',
-            'Zero imputation',
-            'Mean Imputation',
-            'Multivariate Imputation']
-colors = ['r', 'g', 'b', 'orange']
+colors = ["r", "g", "b", "orange", "black"]

 # plot diabetes results
 plt.figure(figsize=(12, 6))
 ax1 = plt.subplot(121)
 for j in xval:
-    ax1.barh(j, mses_diabetes[j], xerr=stds_diabetes[j],
-             color=colors[j], alpha=0.6, align='center')
-
-ax1.set_title('Imputation Techniques with Diabetes Data')
-ax1.set_xlim(left=np.min(mses_diabetes) * 0.9,
-             right=np.max(mses_diabetes) * 1.1)
+    ax1.barh(
+        j,
+        mses_diabetes[j],
+        xerr=stds_diabetes[j],
+        color=colors[j],
+        alpha=0.6,
+        align="center",
+    )
+
+ax1.set_title("Imputation Techniques with Diabetes Data")
+ax1.set_xlim(left=np.min(mses_diabetes) * 0.9, right=np.max(mses_diabetes) * 1.1)
 ax1.set_yticks(xval)
-ax1.set_xlabel('MSE')
+ax1.set_xlabel("MSE")
 ax1.invert_yaxis()
 ax1.set_yticklabels(x_labels)

-# plot boston results
+# plot california dataset results
 ax2 = plt.subplot(122)
 for j in xval:
-    ax2.barh(j, mses_boston[j], xerr=stds_boston[j],
-             color=colors[j], alpha=0.6, align='center')
-
-ax2.set_title('Imputation Techniques with Boston Data')
+    ax2.barh(
+        j,
+        mses_california[j],
+        xerr=stds_california[j],
+        color=colors[j],
+        alpha=0.6,
+        align="center",
+    )
+
+ax2.set_title("Imputation Techniques with California Data")
 ax2.set_yticks(xval)
-ax2.set_xlabel('MSE')
+ax2.set_xlabel("MSE")
 ax2.invert_yaxis()
-ax2.set_yticklabels([''] * n_bars)
+ax2.set_yticklabels([""] * n_bars)

 plt.show()
+
+# %%
+# You can also try different techniques. For instance, the median is a more
+# robust estimator for data with high magnitude variables which could dominate
+# results (otherwise known as a 'long tail').
('examples/impute', 'plot_iterative_imputer_variants_comparison.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,40 +3,46 @@
 Imputing missing values with variants of IterativeImputer
 =========================================================

-The :class:`sklearn.impute.IterativeImputer` class is very flexible - it can be
+.. currentmodule:: sklearn
+
+The :class:`~impute.IterativeImputer` class is very flexible - it can be
 used with a variety of estimators to do round-robin regression, treating every
 variable as an output in turn.

 In this example we compare some estimators for the purpose of missing feature
-imputation with :class:`sklearn.impute.IterativeImputer`:
+imputation with :class:`~impute.IterativeImputer`:

-* :class:`~sklearn.linear_model.BayesianRidge`: regularized linear regression
-* :class:`~sklearn.tree.DecisionTreeRegressor`: non-linear regression
-* :class:`~sklearn.ensemble.ExtraTreesRegressor`: similar to missForest in R
-* :class:`~sklearn.neighbors.KNeighborsRegressor`: comparable to other KNN
+* :class:`~linear_model.BayesianRidge`: regularized linear regression
+* :class:`~tree.RandomForestRegressor`: Forests of randomized trees regression
+* :func:`~pipeline.make_pipeline`(:class:`~kernel_approximation.Nystroem`,
+  :class:`~linear_model.Ridge`): a pipeline with the expansion of a degree 2
+  polynomial kernel and regularized linear regression
+* :class:`~neighbors.KNeighborsRegressor`: comparable to other KNN
   imputation approaches

 Of particular interest is the ability of
-:class:`sklearn.impute.IterativeImputer` to mimic the behavior of missForest, a
-popular imputation package for R. In this example, we have chosen to use
-:class:`sklearn.ensemble.ExtraTreesRegressor` instead of
-:class:`sklearn.ensemble.RandomForestRegressor` (as in missForest) due to its
-increased speed.
+:class:`~impute.IterativeImputer` to mimic the behavior of missForest, a
+popular imputation package for R.

-Note that :class:`sklearn.neighbors.KNeighborsRegressor` is different from KNN
+Note that :class:`~neighbors.KNeighborsRegressor` is different from KNN
 imputation, which learns from samples with missing values by using a distance
 metric that accounts for missing values, rather than imputing them.

 The goal is to compare different estimators to see which one is best for the
-:class:`sklearn.impute.IterativeImputer` when using a
-:class:`sklearn.linear_model.BayesianRidge` estimator on the California housing
+:class:`~impute.IterativeImputer` when using a
+:class:`~linear_model.BayesianRidge` estimator on the California housing
 dataset with a single value randomly removed from each row.

 For this particular pattern of missing values we see that
-:class:`sklearn.ensemble.ExtraTreesRegressor` and
-:class:`sklearn.linear_model.BayesianRidge` give the best results.
+:class:`~linear_model.BayesianRidge` and
+:class:`~ensemble.RandomForestRegressor` give the best results.
+
+It should be noted that some estimators such as
+:class:`~ensemble.HistGradientBoostingRegressor` can natively deal with
+missing features and are often recommended over building pipelines with
+complex and costly missing values imputation strategies.
+
 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -47,9 +53,9 @@
 from sklearn.datasets import fetch_california_housing
 from sklearn.impute import SimpleImputer
 from sklearn.impute import IterativeImputer
-from sklearn.linear_model import BayesianRidge
-from sklearn.tree import DecisionTreeRegressor
-from sklearn.ensemble import ExtraTreesRegressor
+from sklearn.linear_model import BayesianRidge, Ridge
+from sklearn.kernel_approximation import Nystroem
+from sklearn.ensemble import RandomForestRegressor
 from sklearn.neighbors import KNeighborsRegressor
 from sklearn.pipeline import make_pipeline
 from sklearn.model_selection import cross_val_score
@@ -69,10 +75,9 @@
 br_estimator = BayesianRidge()
 score_full_data = pd.DataFrame(
     cross_val_score(
-        br_estimator, X_full, y_full, scoring='neg_mean_squared_error',
-        cv=N_SPLITS
+        br_estimator, X_full, y_full, scoring="neg_mean_squared_error", cv=N_SPLITS
     ),
-    columns=['Full Data']
+    columns=["Full Data"],
 )

 # Add a single missing value to each row
@@ -84,49 +89,65 @@

 # Estimate the score after imputation (mean and median strategies)
 score_simple_imputer = pd.DataFrame()
-for strategy in ('mean', 'median'):
+for strategy in ("mean", "median"):
     estimator = make_pipeline(
-        SimpleImputer(missing_values=np.nan, strategy=strategy),
-        br_estimator
+        SimpleImputer(missing_values=np.nan, strategy=strategy), br_estimator
     )
     score_simple_imputer[strategy] = cross_val_score(
-        estimator, X_missing, y_missing, scoring='neg_mean_squared_error',
-        cv=N_SPLITS
+        estimator, X_missing, y_missing, scoring="neg_mean_squared_error", cv=N_SPLITS
     )

 # Estimate the score after iterative imputation of the missing values
 # with different estimators
 estimators = [
     BayesianRidge(),
-    DecisionTreeRegressor(max_features='sqrt', random_state=0),
-    ExtraTreesRegressor(n_estimators=10, random_state=0),
-    KNeighborsRegressor(n_neighbors=15)
+    RandomForestRegressor(
+        # We tuned the hyperparameters of the RandomForestRegressor to get a good
+        # enough predictive performance for a restricted execution time.
+        n_estimators=4,
+        max_depth=10,
+        bootstrap=True,
+        max_samples=0.5,
+        n_jobs=2,
+        random_state=0,
+    ),
+    make_pipeline(
+        Nystroem(kernel="polynomial", degree=2, random_state=0), Ridge(alpha=1e3)
+    ),
+    KNeighborsRegressor(n_neighbors=15),
 ]
 score_iterative_imputer = pd.DataFrame()
-for impute_estimator in estimators:
+# iterative imputer is sensible to the tolerance and
+# dependent on the estimator used internally.
+# we tuned the tolerance to keep this example run with limited computational
+# resources while not changing the results too much compared to keeping the
+# stricter default value for the tolerance parameter.
+tolerances = (1e-3, 1e-1, 1e-1, 1e-2)
+for impute_estimator, tol in zip(estimators, tolerances):
     estimator = make_pipeline(
-        IterativeImputer(random_state=0, estimator=impute_estimator),
-        br_estimator
+        IterativeImputer(
+            random_state=0, estimator=impute_estimator, max_iter=25, tol=tol
+        ),
+        br_estimator,
     )
-    score_iterative_imputer[impute_estimator.__class__.__name__] = \
-        cross_val_score(
-            estimator, X_missing, y_missing, scoring='neg_mean_squared_error',
-            cv=N_SPLITS
-        )
+    score_iterative_imputer[impute_estimator.__class__.__name__] = cross_val_score(
+        estimator, X_missing, y_missing, scoring="neg_mean_squared_error", cv=N_SPLITS
+    )

 scores = pd.concat(
     [score_full_data, score_simple_imputer, score_iterative_imputer],
-    keys=['Original', 'SimpleImputer', 'IterativeImputer'], axis=1
+    keys=["Original", "SimpleImputer", "IterativeImputer"],
+    axis=1,
 )

-# plot boston results
+# plot california housing results
 fig, ax = plt.subplots(figsize=(13, 6))
 means = -scores.mean()
 errors = scores.std()
 means.plot.barh(xerr=errors, ax=ax)
-ax.set_title('California Housing Regression with Different Imputation Methods')
-ax.set_xlabel('MSE (smaller is better)')
+ax.set_title("California Housing Regression with Different Imputation Methods")
+ax.set_xlabel("MSE (smaller is better)")
 ax.set_yticks(np.arange(means.shape[0]))
-ax.set_yticklabels([" w/ ".join(label) for label in means.index.get_values()])
+ax.set_yticklabels([" w/ ".join(label) for label in means.index.tolist()])
 plt.tight_layout(pad=1)
 plt.show()
('examples/covariance', 'plot_mahalanobis_distances.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,142 +3,209 @@
 Robust covariance estimation and Mahalanobis distances relevance
 ================================================================

-An example to show covariance estimation with the Mahalanobis
+This example shows covariance estimation with Mahalanobis
 distances on Gaussian distributed data.

 For Gaussian distributed data, the distance of an observation
 :math:`x_i` to the mode of the distribution can be computed using its
-Mahalanobis distance: :math:`d_{(\mu,\Sigma)}(x_i)^2 = (x_i -
-\mu)'\Sigma^{-1}(x_i - \mu)` where :math:`\mu` and :math:`\Sigma` are
-the location and the covariance of the underlying Gaussian
-distribution.
+Mahalanobis distance:
+
+.. math::
+
+    d_{(\mu,\Sigma)}(x_i)^2 = (x_i - \mu)^T\Sigma^{-1}(x_i - \mu)
+
+where :math:`\mu` and :math:`\Sigma` are the location and the covariance of
+the underlying Gaussian distributions.

 In practice, :math:`\mu` and :math:`\Sigma` are replaced by some
-estimates.  The usual covariance maximum likelihood estimate is very
-sensitive to the presence of outliers in the data set and therefor,
-the corresponding Mahalanobis distances are. One would better have to
+estimates. The standard covariance maximum likelihood estimate (MLE) is very
+sensitive to the presence of outliers in the data set and therefore,
+the downstream Mahalanobis distances also are. It would be better to
 use a robust estimator of covariance to guarantee that the estimation is
-resistant to "erroneous" observations in the data set and that the
-associated Mahalanobis distances accurately reflect the true
-organisation of the observations.
-
-The Minimum Covariance Determinant estimator is a robust,
+resistant to "erroneous" observations in the dataset and that the
+calculated Mahalanobis distances accurately reflect the true
+organization of the observations.
+
+The Minimum Covariance Determinant estimator (MCD) is a robust,
 high-breakdown point (i.e. it can be used to estimate the covariance
 matrix of highly contaminated datasets, up to
 :math:`\frac{n_\text{samples}-n_\text{features}-1}{2}` outliers)
-estimator of covariance. The idea is to find
+estimator of covariance. The idea behind the MCD is to find
 :math:`\frac{n_\text{samples}+n_\text{features}+1}{2}`
 observations whose empirical covariance has the smallest determinant,
 yielding a "pure" subset of observations from which to compute
-standards estimates of location and covariance.
-
-The Minimum Covariance Determinant estimator (MCD) has been introduced
-by P.J.Rousseuw in [1].
+standards estimates of location and covariance. The MCD was introduced by
+P.J.Rousseuw in [1]_.

 This example illustrates how the Mahalanobis distances are affected by
-outlying data: observations drawn from a contaminating distribution
+outlying data. Observations drawn from a contaminating distribution
 are not distinguishable from the observations coming from the real,
-Gaussian distribution that one may want to work with. Using MCD-based
+Gaussian distribution when using standard covariance MLE based Mahalanobis
+distances. Using MCD-based
 Mahalanobis distances, the two populations become
-distinguishable. Associated applications are outliers detection,
-observations ranking, clustering, ...
-For visualization purpose, the cubic root of the Mahalanobis distances
-are represented in the boxplot, as Wilson and Hilferty suggest [2]
-
-[1] P. J. Rousseeuw. Least median of squares regression. J. Am
-    Stat Ass, 79:871, 1984.
-[2] Wilson, E. B., & Hilferty, M. M. (1931). The distribution of chi-square.
-    Proceedings of the National Academy of Sciences of the United States
-    of America, 17, 684-688.
-
-"""
-print(__doc__)
+distinguishable. Associated applications include outlier detection,
+observation ranking and clustering.
+
+.. note::
+
+    See also :ref:`sphx_glr_auto_examples_covariance_plot_robust_vs_empirical_covariance.py`
+
+.. topic:: References:
+
+    .. [1] P. J. Rousseeuw. `Least median of squares regression
+        <http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/LeastMedianOfSquares.pdf>`_. J. Am
+        Stat Ass, 79:871, 1984.
+    .. [2] Wilson, E. B., & Hilferty, M. M. (1931). `The distribution of chi-square.
+        <https://water.usgs.gov/osw/bulletin17b/Wilson_Hilferty_1931.pdf>`_
+        Proceedings of the National Academy of Sciences of the United States
+        of America, 17, 684-688.
+
+"""  # noqa: E501
+
+# %%
+# Generate data
+# --------------
+#
+# First, we generate a dataset of 125 samples and 2 features. Both features
+# are Gaussian distributed with mean of 0 but feature 1 has a standard
+# deviation equal to 2 and feature 2 has a standard deviation equal to 1. Next,
+# 25 samples are replaced with Gaussian outlier samples where feature 1 has
+# a standard deviation equal to 1 and feature 2 has a standard deviation equal
+# to 7.

 import numpy as np
-import matplotlib.pyplot as plt
-
-from sklearn.covariance import EmpiricalCovariance, MinCovDet
+
+# for consistent results
+np.random.seed(7)

 n_samples = 125
 n_outliers = 25
 n_features = 2

-# generate data
+# generate Gaussian data of shape (125, 2)
 gen_cov = np.eye(n_features)
-gen_cov[0, 0] = 2.
+gen_cov[0, 0] = 2.0
 X = np.dot(np.random.randn(n_samples, n_features), gen_cov)
 # add some outliers
 outliers_cov = np.eye(n_features)
-outliers_cov[np.arange(1, n_features), np.arange(1, n_features)] = 7.
+outliers_cov[np.arange(1, n_features), np.arange(1, n_features)] = 7.0
 X[-n_outliers:] = np.dot(np.random.randn(n_outliers, n_features), outliers_cov)

-# fit a Minimum Covariance Determinant (MCD) robust estimator to data
+# %%
+# Comparison of results
+# ---------------------
+#
+# Below, we fit MCD and MLE based covariance estimators to our data and print
+# the estimated covariance matrices. Note that the estimated variance of
+# feature 2 is much higher with the MLE based estimator (7.5) than
+# that of the MCD robust estimator (1.2). This shows that the MCD based
+# robust estimator is much more resistant to the outlier samples, which were
+# designed to have a much larger variance in feature 2.
+
+import matplotlib.pyplot as plt
+from sklearn.covariance import EmpiricalCovariance, MinCovDet
+
+# fit a MCD robust estimator to data
 robust_cov = MinCovDet().fit(X)
-
-# compare estimators learnt from the full data set with true parameters
+# fit a MLE estimator to data
 emp_cov = EmpiricalCovariance().fit(X)
-
-# #############################################################################
-# Display results
-fig = plt.figure()
-plt.subplots_adjust(hspace=-.1, wspace=.4, top=.95, bottom=.05)
-
-# Show data set
-subfig1 = plt.subplot(3, 1, 1)
-inlier_plot = subfig1.scatter(X[:, 0], X[:, 1],
-                              color='black', label='inliers')
-outlier_plot = subfig1.scatter(X[:, 0][-n_outliers:], X[:, 1][-n_outliers:],
-                               color='red', label='outliers')
-subfig1.set_xlim(subfig1.get_xlim()[0], 11.)
-subfig1.set_title("Mahalanobis distances of a contaminated data set:")
-
-# Show contours of the distance functions
-xx, yy = np.meshgrid(np.linspace(plt.xlim()[0], plt.xlim()[1], 100),
-                     np.linspace(plt.ylim()[0], plt.ylim()[1], 100))
+print(
+    "Estimated covariance matrix:\nMCD (Robust):\n{}\nMLE:\n{}".format(
+        robust_cov.covariance_, emp_cov.covariance_
+    )
+)
+
+# %%
+# To better visualize the difference, we plot contours of the
+# Mahalanobis distances calculated by both methods. Notice that the robust
+# MCD based Mahalanobis distances fit the inlier black points much better,
+# whereas the MLE based distances are more influenced by the outlier
+# red points.
+
+fig, ax = plt.subplots(figsize=(10, 5))
+# Plot data set
+inlier_plot = ax.scatter(X[:, 0], X[:, 1], color="black", label="inliers")
+outlier_plot = ax.scatter(
+    X[:, 0][-n_outliers:], X[:, 1][-n_outliers:], color="red", label="outliers"
+)
+ax.set_xlim(ax.get_xlim()[0], 10.0)
+ax.set_title("Mahalanobis distances of a contaminated data set")
+
+# Create meshgrid of feature 1 and feature 2 values
+xx, yy = np.meshgrid(
+    np.linspace(plt.xlim()[0], plt.xlim()[1], 100),
+    np.linspace(plt.ylim()[0], plt.ylim()[1], 100),
+)
 zz = np.c_[xx.ravel(), yy.ravel()]
-
+# Calculate the MLE based Mahalanobis distances of the meshgrid
 mahal_emp_cov = emp_cov.mahalanobis(zz)
 mahal_emp_cov = mahal_emp_cov.reshape(xx.shape)
-emp_cov_contour = subfig1.contour(xx, yy, np.sqrt(mahal_emp_cov),
-                                  cmap=plt.cm.PuBu_r,
-                                  linestyles='dashed')
-
+emp_cov_contour = plt.contour(
+    xx, yy, np.sqrt(mahal_emp_cov), cmap=plt.cm.PuBu_r, linestyles="dashed"
+)
+# Calculate the MCD based Mahalanobis distances
 mahal_robust_cov = robust_cov.mahalanobis(zz)
 mahal_robust_cov = mahal_robust_cov.reshape(xx.shape)
-robust_contour = subfig1.contour(xx, yy, np.sqrt(mahal_robust_cov),
-                                 cmap=plt.cm.YlOrBr_r, linestyles='dotted')
-
-subfig1.legend([emp_cov_contour.collections[1], robust_contour.collections[1],
-                inlier_plot, outlier_plot],
-               ['MLE dist', 'robust dist', 'inliers', 'outliers'],
-               loc="upper right", borderaxespad=0)
-plt.xticks(())
-plt.yticks(())
-
-# Plot the scores for each point
+robust_contour = ax.contour(
+    xx, yy, np.sqrt(mahal_robust_cov), cmap=plt.cm.YlOrBr_r, linestyles="dotted"
+)
+
+# Add legend
+ax.legend(
+    [
+        emp_cov_contour.collections[1],
+        robust_contour.collections[1],
+        inlier_plot,
+        outlier_plot,
+    ],
+    ["MLE dist", "MCD dist", "inliers", "outliers"],
+    loc="upper right",
+    borderaxespad=0,
+)
+
+plt.show()
+
+# %%
+# Finally, we highlight the ability of MCD based Mahalanobis distances to
+# distinguish outliers. We take the cubic root of the Mahalanobis distances,
+# yielding approximately normal distributions (as suggested by Wilson and
+# Hilferty [2]_), then plot the values of inlier and outlier samples with
+# boxplots. The distribution of outlier samples is more separated from the
+# distribution of inlier samples for robust MCD based Mahalanobis distances.
+
+fig, (ax1, ax2) = plt.subplots(1, 2)
+plt.subplots_adjust(wspace=0.6)
+
+# Calculate cubic root of MLE Mahalanobis distances for samples
 emp_mahal = emp_cov.mahalanobis(X - np.mean(X, 0)) ** (0.33)
-subfig2 = plt.subplot(2, 2, 3)
-subfig2.boxplot([emp_mahal[:-n_outliers], emp_mahal[-n_outliers:]], widths=.25)
-subfig2.plot(np.full(n_samples - n_outliers, 1.26),
-             emp_mahal[:-n_outliers], '+k', markeredgewidth=1)
-subfig2.plot(np.full(n_outliers, 2.26),
-             emp_mahal[-n_outliers:], '+k', markeredgewidth=1)
-subfig2.axes.set_xticklabels(('inliers', 'outliers'), size=15)
-subfig2.set_ylabel(r"$\sqrt[3]{\rm{(Mahal. dist.)}}$", size=16)
-subfig2.set_title("1. from non-robust estimates\n(Maximum Likelihood)")
-plt.yticks(())
-
+# Plot boxplots
+ax1.boxplot([emp_mahal[:-n_outliers], emp_mahal[-n_outliers:]], widths=0.25)
+# Plot individual samples
+ax1.plot(
+    np.full(n_samples - n_outliers, 1.26),
+    emp_mahal[:-n_outliers],
+    "+k",
+    markeredgewidth=1,
+)
+ax1.plot(np.full(n_outliers, 2.26), emp_mahal[-n_outliers:], "+k", markeredgewidth=1)
+ax1.axes.set_xticklabels(("inliers", "outliers"), size=15)
+ax1.set_ylabel(r"$\sqrt[3]{\rm{(Mahal. dist.)}}$", size=16)
+ax1.set_title("Using non-robust estimates\n(Maximum Likelihood)")
+
+# Calculate cubic root of MCD Mahalanobis distances for samples
 robust_mahal = robust_cov.mahalanobis(X - robust_cov.location_) ** (0.33)
-subfig3 = plt.subplot(2, 2, 4)
-subfig3.boxplot([robust_mahal[:-n_outliers], robust_mahal[-n_outliers:]],
-                widths=.25)
-subfig3.plot(np.full(n_samples - n_outliers, 1.26),
-             robust_mahal[:-n_outliers], '+k', markeredgewidth=1)
-subfig3.plot(np.full(n_outliers, 2.26),
-             robust_mahal[-n_outliers:], '+k', markeredgewidth=1)
-subfig3.axes.set_xticklabels(('inliers', 'outliers'), size=15)
-subfig3.set_ylabel(r"$\sqrt[3]{\rm{(Mahal. dist.)}}$", size=16)
-subfig3.set_title("2. from robust estimates\n(Minimum Covariance Determinant)")
-plt.yticks(())
+# Plot boxplots
+ax2.boxplot([robust_mahal[:-n_outliers], robust_mahal[-n_outliers:]], widths=0.25)
+# Plot individual samples
+ax2.plot(
+    np.full(n_samples - n_outliers, 1.26),
+    robust_mahal[:-n_outliers],
+    "+k",
+    markeredgewidth=1,
+)
+ax2.plot(np.full(n_outliers, 2.26), robust_mahal[-n_outliers:], "+k", markeredgewidth=1)
+ax2.axes.set_xticklabels(("inliers", "outliers"), size=15)
+ax2.set_ylabel(r"$\sqrt[3]{\rm{(Mahal. dist.)}}$", size=16)
+ax2.set_title("Using robust estimates\n(Minimum Covariance Determinant)")

 plt.show()
('examples/covariance', 'plot_robust_vs_empirical_covariance.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -52,7 +52,6 @@
     Statistical Ass., 79:871, 1984.

 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -66,8 +65,11 @@
 repeat = 10

 range_n_outliers = np.concatenate(
-    (np.linspace(0, n_samples / 8, 5),
-     np.linspace(n_samples / 8, n_samples / 2, 5)[1:-1])).astype(np.int)
+    (
+        np.linspace(0, n_samples / 8, 5),
+        np.linspace(n_samples / 8, n_samples / 2, 5)[1:-1],
+    )
+).astype(int)

 # definition of arrays to store results
 err_loc_mcd = np.zeros((range_n_outliers.size, repeat))
@@ -87,8 +89,9 @@
         X = rng.randn(n_samples, n_features)
         # add some outliers
         outliers_index = rng.permutation(n_samples)[:n_outliers]
-        outliers_offset = 10. * \
-            (np.random.randint(2, size=(n_outliers, n_features)) - 0.5)
+        outliers_offset = 10.0 * (
+            np.random.randint(2, size=(n_outliers, n_features)) - 0.5
+        )
         X[outliers_index] += outliers_offset
         inliers_mask = np.ones(n_samples).astype(bool)
         inliers_mask[outliers_index] = False
@@ -96,55 +99,85 @@
         # fit a Minimum Covariance Determinant (MCD) robust estimator to data
         mcd = MinCovDet().fit(X)
         # compare raw robust estimates with the true location and covariance
-        err_loc_mcd[i, j] = np.sum(mcd.location_ ** 2)
+        err_loc_mcd[i, j] = np.sum(mcd.location_**2)
         err_cov_mcd[i, j] = mcd.error_norm(np.eye(n_features))

         # compare estimators learned from the full data set with true
         # parameters
         err_loc_emp_full[i, j] = np.sum(X.mean(0) ** 2)
-        err_cov_emp_full[i, j] = EmpiricalCovariance().fit(X).error_norm(
-            np.eye(n_features))
+        err_cov_emp_full[i, j] = (
+            EmpiricalCovariance().fit(X).error_norm(np.eye(n_features))
+        )

         # compare with an empirical covariance learned from a pure data set
         # (i.e. "perfect" mcd)
         pure_X = X[inliers_mask]
         pure_location = pure_X.mean(0)
         pure_emp_cov = EmpiricalCovariance().fit(pure_X)
-        err_loc_emp_pure[i, j] = np.sum(pure_location ** 2)
+        err_loc_emp_pure[i, j] = np.sum(pure_location**2)
         err_cov_emp_pure[i, j] = pure_emp_cov.error_norm(np.eye(n_features))

 # Display results
 font_prop = matplotlib.font_manager.FontProperties(size=11)
 plt.subplot(2, 1, 1)
 lw = 2
-plt.errorbar(range_n_outliers, err_loc_mcd.mean(1),
-             yerr=err_loc_mcd.std(1) / np.sqrt(repeat),
-             label="Robust location", lw=lw, color='m')
-plt.errorbar(range_n_outliers, err_loc_emp_full.mean(1),
-             yerr=err_loc_emp_full.std(1) / np.sqrt(repeat),
-             label="Full data set mean", lw=lw, color='green')
-plt.errorbar(range_n_outliers, err_loc_emp_pure.mean(1),
-             yerr=err_loc_emp_pure.std(1) / np.sqrt(repeat),
-             label="Pure data set mean", lw=lw, color='black')
+plt.errorbar(
+    range_n_outliers,
+    err_loc_mcd.mean(1),
+    yerr=err_loc_mcd.std(1) / np.sqrt(repeat),
+    label="Robust location",
+    lw=lw,
+    color="m",
+)
+plt.errorbar(
+    range_n_outliers,
+    err_loc_emp_full.mean(1),
+    yerr=err_loc_emp_full.std(1) / np.sqrt(repeat),
+    label="Full data set mean",
+    lw=lw,
+    color="green",
+)
+plt.errorbar(
+    range_n_outliers,
+    err_loc_emp_pure.mean(1),
+    yerr=err_loc_emp_pure.std(1) / np.sqrt(repeat),
+    label="Pure data set mean",
+    lw=lw,
+    color="black",
+)
 plt.title("Influence of outliers on the location estimation")
 plt.ylabel(r"Error ($||\mu - \hat{\mu}||_2^2$)")
 plt.legend(loc="upper left", prop=font_prop)

 plt.subplot(2, 1, 2)
 x_size = range_n_outliers.size
-plt.errorbar(range_n_outliers, err_cov_mcd.mean(1),
-             yerr=err_cov_mcd.std(1),
-             label="Robust covariance (mcd)", color='m')
-plt.errorbar(range_n_outliers[:(x_size // 5 + 1)],
-             err_cov_emp_full.mean(1)[:(x_size // 5 + 1)],
-             yerr=err_cov_emp_full.std(1)[:(x_size // 5 + 1)],
-             label="Full data set empirical covariance", color='green')
-plt.plot(range_n_outliers[(x_size // 5):(x_size // 2 - 1)],
-         err_cov_emp_full.mean(1)[(x_size // 5):(x_size // 2 - 1)],
-         color='green', ls='--')
-plt.errorbar(range_n_outliers, err_cov_emp_pure.mean(1),
-             yerr=err_cov_emp_pure.std(1),
-             label="Pure data set empirical covariance", color='black')
+plt.errorbar(
+    range_n_outliers,
+    err_cov_mcd.mean(1),
+    yerr=err_cov_mcd.std(1),
+    label="Robust covariance (mcd)",
+    color="m",
+)
+plt.errorbar(
+    range_n_outliers[: (x_size // 5 + 1)],
+    err_cov_emp_full.mean(1)[: (x_size // 5 + 1)],
+    yerr=err_cov_emp_full.std(1)[: (x_size // 5 + 1)],
+    label="Full data set empirical covariance",
+    color="green",
+)
+plt.plot(
+    range_n_outliers[(x_size // 5) : (x_size // 2 - 1)],
+    err_cov_emp_full.mean(1)[(x_size // 5) : (x_size // 2 - 1)],
+    color="green",
+    ls="--",
+)
+plt.errorbar(
+    range_n_outliers,
+    err_cov_emp_pure.mean(1),
+    yerr=err_cov_emp_pure.std(1),
+    label="Pure data set empirical covariance",
+    color="black",
+)
 plt.title("Influence of outliers on the covariance estimation")
 plt.xlabel("Amount of contamination (%)")
 plt.ylabel("RMSE")
('examples/covariance', 'plot_covariance_estimation.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,7 +5,7 @@

 When working with covariance estimation, the usual approach is to use
 a maximum likelihood estimator, such as the
-:class:`sklearn.covariance.EmpiricalCovariance`. It is unbiased, i.e. it
+:class:`~sklearn.covariance.EmpiricalCovariance`. It is unbiased, i.e. it
 converges to the true (population) covariance when given many
 observations. However, it can also be beneficial to regularize it, in
 order to reduce its variance; this, in turn, introduces some bias. This
@@ -21,11 +21,11 @@

 * A close formula proposed by Ledoit and Wolf to compute
   the asymptotically optimal regularization parameter (minimizing a MSE
-  criterion), yielding the :class:`sklearn.covariance.LedoitWolf`
+  criterion), yielding the :class:`~sklearn.covariance.LedoitWolf`
   covariance estimate.

 * An improvement of the Ledoit-Wolf shrinkage, the
-  :class:`sklearn.covariance.OAS`, proposed by Chen et al. Its
+  :class:`~sklearn.covariance.OAS`, proposed by Chen et al. Its
   convergence is significantly better under the assumption that the data
   are Gaussian, in particular for small samples.

@@ -41,14 +41,18 @@
 computationally costly.

 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
 from scipy import linalg

-from sklearn.covariance import LedoitWolf, OAS, ShrunkCovariance, \
-    log_likelihood, empirical_covariance
+from sklearn.covariance import (
+    LedoitWolf,
+    OAS,
+    ShrunkCovariance,
+    log_likelihood,
+    empirical_covariance,
+)
 from sklearn.model_selection import GridSearchCV


@@ -69,8 +73,9 @@

 # spanning a range of possible shrinkage coefficient values
 shrinkages = np.logspace(-2, 0, 30)
-negative_logliks = [-ShrunkCovariance(shrinkage=s).fit(X_train).score(X_test)
-                    for s in shrinkages]
+negative_logliks = [
+    -ShrunkCovariance(shrinkage=s).fit(X_train).score(X_test) for s in shrinkages
+]

 # under the ground-truth model, which we would not have access to in real
 # settings
@@ -82,8 +87,8 @@
 # Compare different approaches to setting the parameter

 # GridSearch for an optimal shrinkage coefficient
-tuned_parameters = [{'shrinkage': shrinkages}]
-cv = GridSearchCV(ShrunkCovariance(), tuned_parameters, cv=5)
+tuned_parameters = [{"shrinkage": shrinkages}]
+cv = GridSearchCV(ShrunkCovariance(), tuned_parameters)
 cv.fit(X_train)

 # Ledoit-Wolf optimal shrinkage coefficient estimate
@@ -98,31 +103,42 @@
 # Plot results
 fig = plt.figure()
 plt.title("Regularized covariance: likelihood and shrinkage coefficient")
-plt.xlabel('Regularization parameter: shrinkage coefficient')
-plt.ylabel('Error: negative log-likelihood on test data')
+plt.xlabel("Regularization parameter: shrinkage coefficient")
+plt.ylabel("Error: negative log-likelihood on test data")
 # range shrinkage curve
 plt.loglog(shrinkages, negative_logliks, label="Negative log-likelihood")

-plt.plot(plt.xlim(), 2 * [loglik_real], '--r',
-         label="Real covariance likelihood")
+plt.plot(plt.xlim(), 2 * [loglik_real], "--r", label="Real covariance likelihood")

 # adjust view
 lik_max = np.amax(negative_logliks)
 lik_min = np.amin(negative_logliks)
-ymin = lik_min - 6. * np.log((plt.ylim()[1] - plt.ylim()[0]))
-ymax = lik_max + 10. * np.log(lik_max - lik_min)
+ymin = lik_min - 6.0 * np.log((plt.ylim()[1] - plt.ylim()[0]))
+ymax = lik_max + 10.0 * np.log(lik_max - lik_min)
 xmin = shrinkages[0]
 xmax = shrinkages[-1]
 # LW likelihood
-plt.vlines(lw.shrinkage_, ymin, -loglik_lw, color='magenta',
-           linewidth=3, label='Ledoit-Wolf estimate')
+plt.vlines(
+    lw.shrinkage_,
+    ymin,
+    -loglik_lw,
+    color="magenta",
+    linewidth=3,
+    label="Ledoit-Wolf estimate",
+)
 # OAS likelihood
-plt.vlines(oa.shrinkage_, ymin, -loglik_oa, color='purple',
-           linewidth=3, label='OAS estimate')
+plt.vlines(
+    oa.shrinkage_, ymin, -loglik_oa, color="purple", linewidth=3, label="OAS estimate"
+)
 # best CV estimator likelihood
-plt.vlines(cv.best_estimator_.shrinkage, ymin,
-           -cv.best_estimator_.score(X_test), color='cyan',
-           linewidth=3, label='Cross-validation best estimate')
+plt.vlines(
+    cv.best_estimator_.shrinkage,
+    ymin,
+    -cv.best_estimator_.score(X_test),
+    color="cyan",
+    linewidth=3,
+    label="Cross-validation best estimate",
+)

 plt.ylim(ymin, ymax)
 plt.xlim(xmin, xmax)
('examples/covariance', 'plot_lw_vs_oas.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -20,7 +20,6 @@
 Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.

 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -29,7 +28,7 @@
 from sklearn.covariance import LedoitWolf, OAS

 np.random.seed(0)
-###############################################################################
+# %%
 n_features = 100
 # simulation covariance matrix (AR(1) process)
 r = 0.1
@@ -44,8 +43,7 @@
 oa_shrinkage = np.zeros((n_samples_range.size, repeat))
 for i, n_samples in enumerate(n_samples_range):
     for j in range(repeat):
-        X = np.dot(
-            np.random.normal(size=(n_samples, n_features)), coloring_matrix.T)
+        X = np.dot(np.random.normal(size=(n_samples, n_features)), coloring_matrix.T)

         lw = LedoitWolf(store_precision=False, assume_centered=True)
         lw.fit(X)
@@ -59,10 +57,22 @@

 # plot MSE
 plt.subplot(2, 1, 1)
-plt.errorbar(n_samples_range, lw_mse.mean(1), yerr=lw_mse.std(1),
-             label='Ledoit-Wolf', color='navy', lw=2)
-plt.errorbar(n_samples_range, oa_mse.mean(1), yerr=oa_mse.std(1),
-             label='OAS', color='darkorange', lw=2)
+plt.errorbar(
+    n_samples_range,
+    lw_mse.mean(1),
+    yerr=lw_mse.std(1),
+    label="Ledoit-Wolf",
+    color="navy",
+    lw=2,
+)
+plt.errorbar(
+    n_samples_range,
+    oa_mse.mean(1),
+    yerr=oa_mse.std(1),
+    label="OAS",
+    color="darkorange",
+    lw=2,
+)
 plt.ylabel("Squared error")
 plt.legend(loc="upper right")
 plt.title("Comparison of covariance estimators")
@@ -70,14 +80,26 @@

 # plot shrinkage coefficient
 plt.subplot(2, 1, 2)
-plt.errorbar(n_samples_range, lw_shrinkage.mean(1), yerr=lw_shrinkage.std(1),
-             label='Ledoit-Wolf', color='navy', lw=2)
-plt.errorbar(n_samples_range, oa_shrinkage.mean(1), yerr=oa_shrinkage.std(1),
-             label='OAS', color='darkorange', lw=2)
+plt.errorbar(
+    n_samples_range,
+    lw_shrinkage.mean(1),
+    yerr=lw_shrinkage.std(1),
+    label="Ledoit-Wolf",
+    color="navy",
+    lw=2,
+)
+plt.errorbar(
+    n_samples_range,
+    oa_shrinkage.mean(1),
+    yerr=oa_shrinkage.std(1),
+    label="OAS",
+    color="darkorange",
+    lw=2,
+)
 plt.xlabel("n_samples")
 plt.ylabel("Shrinkage")
 plt.legend(loc="lower right")
-plt.ylim(plt.ylim()[0], 1. + (plt.ylim()[1] - plt.ylim()[0]) / 10.)
+plt.ylim(plt.ylim()[0], 1.0 + (plt.ylim()[1] - plt.ylim()[0]) / 10.0)
 plt.xlim(5, 31)

 plt.show()
('examples/covariance', 'plot_sparse_cov.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -47,28 +47,27 @@
 set by internal cross-validation in the GraphicalLassoCV. As can be
 seen on figure 2, the grid to compute the cross-validation score is
 iteratively refined in the neighborhood of the maximum.
+
 """
-print(__doc__)
+
 # author: Gael Varoquaux <gael.varoquaux@inria.fr>
 # License: BSD 3 clause
 # Copyright: INRIA

+# %%
+# Generate the data
+# -----------------
 import numpy as np
 from scipy import linalg
 from sklearn.datasets import make_sparse_spd_matrix
-from sklearn.covariance import GraphicalLassoCV, ledoit_wolf
-import matplotlib.pyplot as plt

-# #############################################################################
-# Generate the data
 n_samples = 60
 n_features = 20

 prng = np.random.RandomState(1)
-prec = make_sparse_spd_matrix(n_features, alpha=.98,
-                              smallest_coef=.4,
-                              largest_coef=.7,
-                              random_state=prng)
+prec = make_sparse_spd_matrix(
+    n_features, alpha=0.98, smallest_coef=0.4, largest_coef=0.7, random_state=prng
+)
 cov = linalg.inv(prec)
 d = np.sqrt(np.diag(cov))
 cov /= d
@@ -79,11 +78,14 @@
 X -= X.mean(axis=0)
 X /= X.std(axis=0)

-# #############################################################################
+# %%
 # Estimate the covariance
+# -----------------------
+from sklearn.covariance import GraphicalLassoCV, ledoit_wolf
+
 emp_cov = np.dot(X.T, X) / n_samples

-model = GraphicalLassoCV(cv=5)
+model = GraphicalLassoCV()
 model.fit(X)
 cov_ = model.covariance_
 prec_ = model.precision_
@@ -91,48 +93,66 @@
 lw_cov_, _ = ledoit_wolf(X)
 lw_prec_ = linalg.inv(lw_cov_)

-# #############################################################################
+# %%
 # Plot the results
+# ----------------
+import matplotlib.pyplot as plt
+
 plt.figure(figsize=(10, 6))
 plt.subplots_adjust(left=0.02, right=0.98)

 # plot the covariances
-covs = [('Empirical', emp_cov), ('Ledoit-Wolf', lw_cov_),
-        ('GraphicalLassoCV', cov_), ('True', cov)]
+covs = [
+    ("Empirical", emp_cov),
+    ("Ledoit-Wolf", lw_cov_),
+    ("GraphicalLassoCV", cov_),
+    ("True", cov),
+]
 vmax = cov_.max()
 for i, (name, this_cov) in enumerate(covs):
     plt.subplot(2, 4, i + 1)
-    plt.imshow(this_cov, interpolation='nearest', vmin=-vmax, vmax=vmax,
-               cmap=plt.cm.RdBu_r)
+    plt.imshow(
+        this_cov, interpolation="nearest", vmin=-vmax, vmax=vmax, cmap=plt.cm.RdBu_r
+    )
     plt.xticks(())
     plt.yticks(())
-    plt.title('%s covariance' % name)
+    plt.title("%s covariance" % name)


 # plot the precisions
-precs = [('Empirical', linalg.inv(emp_cov)), ('Ledoit-Wolf', lw_prec_),
-         ('GraphicalLasso', prec_), ('True', prec)]
-vmax = .9 * prec_.max()
+precs = [
+    ("Empirical", linalg.inv(emp_cov)),
+    ("Ledoit-Wolf", lw_prec_),
+    ("GraphicalLasso", prec_),
+    ("True", prec),
+]
+vmax = 0.9 * prec_.max()
 for i, (name, this_prec) in enumerate(precs):
     ax = plt.subplot(2, 4, i + 5)
-    plt.imshow(np.ma.masked_equal(this_prec, 0),
-               interpolation='nearest', vmin=-vmax, vmax=vmax,
-               cmap=plt.cm.RdBu_r)
+    plt.imshow(
+        np.ma.masked_equal(this_prec, 0),
+        interpolation="nearest",
+        vmin=-vmax,
+        vmax=vmax,
+        cmap=plt.cm.RdBu_r,
+    )
     plt.xticks(())
     plt.yticks(())
-    plt.title('%s precision' % name)
-    if hasattr(ax, 'set_facecolor'):
-        ax.set_facecolor('.7')
+    plt.title("%s precision" % name)
+    if hasattr(ax, "set_facecolor"):
+        ax.set_facecolor(".7")
     else:
-        ax.set_axis_bgcolor('.7')
+        ax.set_axis_bgcolor(".7")
+
+# %%

 # plot the model selection metric
 plt.figure(figsize=(4, 3))
-plt.axes([.2, .15, .75, .7])
-plt.plot(model.cv_alphas_, np.mean(model.grid_scores_, axis=1), 'o-')
-plt.axvline(model.alpha_, color='.5')
-plt.title('Model selection')
-plt.ylabel('Cross-validation score')
-plt.xlabel('alpha')
+plt.axes([0.2, 0.15, 0.75, 0.7])
+plt.plot(model.cv_results_["alphas"], model.cv_results_["mean_test_score"], "o-")
+plt.axvline(model.alpha_, color=".5")
+plt.title("Model selection")
+plt.ylabel("Cross-validation score")
+plt.xlabel("alpha")

 plt.show()
('examples/multioutput', 'plot_classifier_chain_yeast.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,12 +5,12 @@
 Example of using classifier chain on a multilabel dataset.

 For this example we will use the `yeast
-<http://mldata.org/repository/data/viewslug/yeast>`_ dataset which contains
+<https://www.openml.org/d/40597>`_ dataset which contains
 2417 datapoints each with 103 features and 14 possible labels. Each
 data point has at least one label. As a baseline we first train a logistic
 regression classifier for each of the 14 labels. To evaluate the performance of
 these classifiers we predict on a held-out test set and calculate the
-:ref:`jaccard score <jaccard_score>` for each sample.
+:ref:`jaccard score <jaccard_similarity_score>` for each sample.

 Next we create 10 classifier chains. Each classifier chain contains a
 logistic regression model for each of the 14 labels. The models in each
@@ -30,6 +30,7 @@
 ensemble is greater than that of the independent models and tends to exceed
 the score of each chain in the ensemble (although this is not guaranteed
 with randomly ordered chains).
+
 """

 # Author: Adam Kleczewski
@@ -44,55 +45,53 @@
 from sklearn.metrics import jaccard_score
 from sklearn.linear_model import LogisticRegression

-print(__doc__)
-
 # Load a multi-label dataset from https://www.openml.org/d/40597
-X, Y = fetch_openml('yeast', version=4, return_X_y=True)
-Y = Y == 'TRUE'
-X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2,
-                                                    random_state=0)
+X, Y = fetch_openml("yeast", version=4, return_X_y=True)
+Y = Y == "TRUE"
+X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

 # Fit an independent logistic regression model for each class using the
 # OneVsRestClassifier wrapper.
-base_lr = LogisticRegression(solver='lbfgs')
+base_lr = LogisticRegression()
 ovr = OneVsRestClassifier(base_lr)
 ovr.fit(X_train, Y_train)
 Y_pred_ovr = ovr.predict(X_test)
-ovr_jaccard_score = jaccard_score(Y_test, Y_pred_ovr, average='samples')
+ovr_jaccard_score = jaccard_score(Y_test, Y_pred_ovr, average="samples")

 # Fit an ensemble of logistic regression classifier chains and take the
 # take the average prediction of all the chains.
-chains = [ClassifierChain(base_lr, order='random', random_state=i)
-          for i in range(10)]
+chains = [ClassifierChain(base_lr, order="random", random_state=i) for i in range(10)]
 for chain in chains:
     chain.fit(X_train, Y_train)

-Y_pred_chains = np.array([chain.predict(X_test) for chain in
-                          chains])
-chain_jaccard_scores = [jaccard_score(Y_test, Y_pred_chain >= .5,
-                                      average='samples')
-                        for Y_pred_chain in Y_pred_chains]
+Y_pred_chains = np.array([chain.predict(X_test) for chain in chains])
+chain_jaccard_scores = [
+    jaccard_score(Y_test, Y_pred_chain >= 0.5, average="samples")
+    for Y_pred_chain in Y_pred_chains
+]

 Y_pred_ensemble = Y_pred_chains.mean(axis=0)
-ensemble_jaccard_score = jaccard_score(Y_test,
-                                       Y_pred_ensemble >= .5,
-                                       average='samples')
+ensemble_jaccard_score = jaccard_score(
+    Y_test, Y_pred_ensemble >= 0.5, average="samples"
+)

 model_scores = [ovr_jaccard_score] + chain_jaccard_scores
 model_scores.append(ensemble_jaccard_score)

-model_names = ('Independent',
-               'Chain 1',
-               'Chain 2',
-               'Chain 3',
-               'Chain 4',
-               'Chain 5',
-               'Chain 6',
-               'Chain 7',
-               'Chain 8',
-               'Chain 9',
-               'Chain 10',
-               'Ensemble')
+model_names = (
+    "Independent",
+    "Chain 1",
+    "Chain 2",
+    "Chain 3",
+    "Chain 4",
+    "Chain 5",
+    "Chain 6",
+    "Chain 7",
+    "Chain 8",
+    "Chain 9",
+    "Chain 10",
+    "Ensemble",
+)

 x_pos = np.arange(len(model_names))

@@ -102,12 +101,12 @@

 fig, ax = plt.subplots(figsize=(7, 4))
 ax.grid(True)
-ax.set_title('Classifier Chain Ensemble Performance Comparison')
+ax.set_title("Classifier Chain Ensemble Performance Comparison")
 ax.set_xticks(x_pos)
-ax.set_xticklabels(model_names, rotation='vertical')
-ax.set_ylabel('Jaccard Similarity Score')
-ax.set_ylim([min(model_scores) * .9, max(model_scores) * 1.1])
-colors = ['r'] + ['b'] * len(chain_jaccard_scores) + ['g']
+ax.set_xticklabels(model_names, rotation="vertical")
+ax.set_ylabel("Jaccard Similarity Score")
+ax.set_ylim([min(model_scores) * 0.9, max(model_scores) * 1.1])
+colors = ["r"] + ["b"] * len(chain_jaccard_scores) + ["g"]
 ax.bar(x_pos, model_scores, alpha=0.5, color=colors)
 plt.tight_layout()
 plt.show()
('examples/feature_selection', 'plot_rfe_with_cross_validation.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,8 +5,8 @@

 A recursive feature elimination example with automatic tuning of the
 number of features selected with cross-validation.
+
 """
-print(__doc__)

 import matplotlib.pyplot as plt
 from sklearn.svm import SVC
@@ -15,16 +15,29 @@
 from sklearn.datasets import make_classification

 # Build a classification task using 3 informative features
-X, y = make_classification(n_samples=1000, n_features=25, n_informative=3,
-                           n_redundant=2, n_repeated=0, n_classes=8,
-                           n_clusters_per_class=1, random_state=0)
+X, y = make_classification(
+    n_samples=1000,
+    n_features=25,
+    n_informative=3,
+    n_redundant=2,
+    n_repeated=0,
+    n_classes=8,
+    n_clusters_per_class=1,
+    random_state=0,
+)

 # Create the RFE object and compute a cross-validated score.
 svc = SVC(kernel="linear")
-# The "accuracy" scoring is proportional to the number of correct
-# classifications
-rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),
-              scoring='accuracy')
+# The "accuracy" scoring shows the proportion of correct classifications
+
+min_features_to_select = 1  # Minimum number of features to consider
+rfecv = RFECV(
+    estimator=svc,
+    step=1,
+    cv=StratifiedKFold(2),
+    scoring="accuracy",
+    min_features_to_select=min_features_to_select,
+)
 rfecv.fit(X, y)

 print("Optimal number of features : %d" % rfecv.n_features_)
@@ -32,6 +45,9 @@
 # Plot number of features VS. cross-validation scores
 plt.figure()
 plt.xlabel("Number of features selected")
-plt.ylabel("Cross validation score (nb of correct classifications)")
-plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
+plt.ylabel("Cross validation score (accuracy)")
+plt.plot(
+    range(min_features_to_select, len(rfecv.grid_scores_) + min_features_to_select),
+    rfecv.grid_scores_,
+)
 plt.show()
('examples/feature_selection', 'plot_rfe_digits.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -10,8 +10,7 @@

     See also :ref:`sphx_glr_auto_examples_feature_selection_plot_rfe_with_cross_validation.py`

-"""
-print(__doc__)
+"""  # noqa: E501

 from sklearn.svm import SVC
 from sklearn.datasets import load_digits
('examples/feature_selection', 'plot_feature_selection.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,86 +1,129 @@
 """
-===============================
+============================
 Univariate Feature Selection
-===============================
+============================

-An example showing univariate feature selection.
+This notebook is an example of using univariate feature selection
+to improve classification accuracy on a noisy dataset.

-Noisy (non informative) features are added to the iris data and
-univariate feature selection is applied. For each feature, we plot the
-p-values for the univariate feature selection and the corresponding
-weights of an SVM. We can see that univariate feature selection
-selects the informative features and that these have larger SVM weights.
+In this example, some noisy (non informative) features are added to
+the iris dataset. Support vector machine (SVM) is used to classify the
+dataset both before and after applying univariate feature selection.
+For each feature, we plot the p-values for the univariate feature selection
+and the corresponding weights of SVMs. With this, we will compare model
+accuracy and examine the impact of univariate feature selection on model
+weights.

-In the total set of features, only the 4 first ones are significant. We
-can see that they have the highest score with univariate feature
-selection. The SVM assigns a large weight to one of these features, but also
-Selects many of the non-informative features.
-Applying univariate feature selection before the SVM
-increases the SVM weight attributed to the significant features, and will
-thus improve classification.
 """
-print(__doc__)

+# %%
+# Generate sample data
+# --------------------
+#
 import numpy as np
+from sklearn.datasets import load_iris
+from sklearn.model_selection import train_test_split
+
+# The iris dataset
+X, y = load_iris(return_X_y=True)
+
+# Some noisy data not correlated
+E = np.random.RandomState(42).uniform(0, 0.1, size=(X.shape[0], 20))
+
+# Add the noisy data to the informative features
+X = np.hstack((X, E))
+
+# Split dataset to select feature and evaluate the classifier
+X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)
+
+# %%
+# Univariate feature selection
+# ----------------------------
+#
+# Univariate feature selection with F-test for feature scoring.
+# We use the default selection function to select
+# the four most significant features.
+from sklearn.feature_selection import SelectKBest, f_classif
+
+selector = SelectKBest(f_classif, k=4)
+selector.fit(X_train, y_train)
+scores = -np.log10(selector.pvalues_)
+scores /= scores.max()
+
+# %%
 import matplotlib.pyplot as plt

-from sklearn import datasets, svm
-from sklearn.feature_selection import SelectPercentile, f_classif
-
-# #############################################################################
-# Import some data to play with
-
-# The iris dataset
-iris = datasets.load_iris()
-
-# Some noisy data not correlated
-E = np.random.uniform(0, 0.1, size=(len(iris.data), 20))
-
-# Add the noisy data to the informative features
-X = np.hstack((iris.data, E))
-y = iris.target
-
+X_indices = np.arange(X.shape[-1])
 plt.figure(1)
 plt.clf()
+plt.bar(X_indices - 0.05, scores, width=0.2)
+plt.title("Feature univariate score")
+plt.xlabel("Feature number")
+plt.ylabel(r"Univariate score ($-Log(p_{value})$)")
+plt.show()

-X_indices = np.arange(X.shape[-1])
+# %%
+# In the total set of features, only the 4 of the original features are significant.
+# We can see that they have the highest score with univariate feature
+# selection.

-# #############################################################################
-# Univariate feature selection with F-test for feature scoring
-# We use the default selection function: the 10% most significant features
-selector = SelectPercentile(f_classif, percentile=10)
-selector.fit(X, y)
-scores = -np.log10(selector.pvalues_)
-scores /= scores.max()
-plt.bar(X_indices - .45, scores, width=.2,
-        label=r'Univariate score ($-Log(p_{value})$)', color='darkorange',
-        edgecolor='black')
+# %%
+# Compare with SVMs
+# -----------------
+#
+# Without univariate feature selection
+from sklearn.pipeline import make_pipeline
+from sklearn.preprocessing import MinMaxScaler
+from sklearn.svm import LinearSVC

-# #############################################################################
-# Compare to the weights of an SVM
-clf = svm.SVC(kernel='linear')
-clf.fit(X, y)
+clf = make_pipeline(MinMaxScaler(), LinearSVC())
+clf.fit(X_train, y_train)
+print(
+    "Classification accuracy without selecting features: {:.3f}".format(
+        clf.score(X_test, y_test)
+    )
+)

-svm_weights = (clf.coef_ ** 2).sum(axis=0)
-svm_weights /= svm_weights.max()
+svm_weights = np.abs(clf[-1].coef_).sum(axis=0)
+svm_weights /= svm_weights.sum()

-plt.bar(X_indices - .25, svm_weights, width=.2, label='SVM weight',
-        color='navy', edgecolor='black')
+# %%
+# After univariate feature selection
+clf_selected = make_pipeline(SelectKBest(f_classif, k=4), MinMaxScaler(), LinearSVC())
+clf_selected.fit(X_train, y_train)
+print(
+    "Classification accuracy after univariate feature selection: {:.3f}".format(
+        clf_selected.score(X_test, y_test)
+    )
+)

-clf_selected = svm.SVC(kernel='linear')
-clf_selected.fit(selector.transform(X), y)
+svm_weights_selected = np.abs(clf_selected[-1].coef_).sum(axis=0)
+svm_weights_selected /= svm_weights_selected.sum()

-svm_weights_selected = (clf_selected.coef_ ** 2).sum(axis=0)
-svm_weights_selected /= svm_weights_selected.max()
+# %%
+plt.bar(
+    X_indices - 0.45, scores, width=0.2, label=r"Univariate score ($-Log(p_{value})$)"
+)

-plt.bar(X_indices[selector.get_support()] - .05, svm_weights_selected,
-        width=.2, label='SVM weights after selection', color='c',
-        edgecolor='black')
+plt.bar(X_indices - 0.25, svm_weights, width=0.2, label="SVM weight")

+plt.bar(
+    X_indices[selector.get_support()] - 0.05,
+    svm_weights_selected,
+    width=0.2,
+    label="SVM weights after selection",
+)

 plt.title("Comparing feature selection")
-plt.xlabel('Feature number')
+plt.xlabel("Feature number")
 plt.yticks(())
-plt.axis('tight')
-plt.legend(loc='upper right')
+plt.axis("tight")
+plt.legend(loc="upper right")
 plt.show()
+
+# %%
+# Without univariate feature selection, the SVM assigns a large weight
+# to the first 4 original significant features, but also selects many of the
+# non-informative features. Applying univariate feature selection before
+# the SVM increases the SVM weight attributed to the significant features,
+# and will thus improve classification.
('examples/feature_selection', 'plot_f_test_vs_mi.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -20,8 +20,8 @@
 kind of dependency between variables and it rates x_2 as the most
 discriminative feature, which probably agrees better with our intuitive
 perception for this example. Both methods correctly marks x_3 as irrelevant.
+
 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -40,10 +40,9 @@
 plt.figure(figsize=(15, 5))
 for i in range(3):
     plt.subplot(1, 3, i + 1)
-    plt.scatter(X[:, i], y, edgecolor='black', s=20)
+    plt.scatter(X[:, i], y, edgecolor="black", s=20)
     plt.xlabel("$x_{}$".format(i + 1), fontsize=14)
     if i == 0:
         plt.ylabel("$y$", fontsize=14)
-    plt.title("F-test={:.2f}, MI={:.2f}".format(f_test[i], mi[i]),
-              fontsize=16)
+    plt.title("F-test={:.2f}, MI={:.2f}".format(f_test[i], mi[i]), fontsize=16)
 plt.show()
('examples/feature_selection', 'plot_feature_selection_pipeline.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,40 +1,83 @@
 """
 ==================
-Pipeline Anova SVM
+Pipeline ANOVA SVM
 ==================

-Simple usage of Pipeline that runs successively a univariate
-feature selection with anova and then a SVM of the selected features.
+This example shows how a feature selection can be easily integrated within
+a machine learning pipeline.

-Using a sub-pipeline, the fitted coefficients can be mapped back into
-the original feature space.
+We also show that you can easily introspect part of the pipeline.
+
 """
-from sklearn import svm
-from sklearn.datasets import samples_generator
-from sklearn.feature_selection import SelectKBest, f_regression
+
+# %%
+# We will start by generating a binary classification dataset. Subsequently, we
+# will divide the dataset into two subsets.
+
+from sklearn.datasets import make_classification
+from sklearn.model_selection import train_test_split
+
+X, y = make_classification(
+    n_features=20,
+    n_informative=3,
+    n_redundant=0,
+    n_classes=2,
+    n_clusters_per_class=2,
+    random_state=42,
+)
+X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
+
+# %%
+# A common mistake done with feature selection is to search a subset of
+# discriminative features on the full dataset instead of only using the
+# training set. The usage of scikit-learn :func:`~sklearn.pipeline.Pipeline`
+# prevents to make such mistake.
+#
+# Here, we will demonstrate how to build a pipeline where the first step will
+# be the feature selection.
+#
+# When calling `fit` on the training data, a subset of feature will be selected
+# and the index of these selected features will be stored. The feature selector
+# will subsequently reduce the number of feature and pass this subset to the
+# classifier which will be trained.
+
+from sklearn.feature_selection import SelectKBest, f_classif
 from sklearn.pipeline import make_pipeline
-from sklearn.model_selection import train_test_split
+from sklearn.svm import LinearSVC
+
+anova_filter = SelectKBest(f_classif, k=3)
+clf = LinearSVC()
+anova_svm = make_pipeline(anova_filter, clf)
+anova_svm.fit(X_train, y_train)
+
+# %%
+# Once the training accomplished, we can predict on new unseen samples. In this
+# case, the feature selector will only select the most discriminative features
+# based on the information stored during training. Then, the data will be
+# passed to the classifier which will make the prediction.
+#
+# Here, we report the final metrics via a classification report.
+
 from sklearn.metrics import classification_report

-print(__doc__)
-
-# import some data to play with
-X, y = samples_generator.make_classification(
-    n_features=20, n_informative=3, n_redundant=0, n_classes=4,
-    n_clusters_per_class=2)
-
-X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
-
-# ANOVA SVM-C
-# 1) anova filter, take 3 best ranked features
-anova_filter = SelectKBest(f_regression, k=3)
-# 2) svm
-clf = svm.LinearSVC()
-
-anova_svm = make_pipeline(anova_filter, clf)
-anova_svm.fit(X_train, y_train)
 y_pred = anova_svm.predict(X_test)
 print(classification_report(y_test, y_pred))

-coef = anova_svm[:-1].inverse_transform(anova_svm['linearsvc'].coef_)
-print(coef)
+# %%
+# Be aware that you can inspect a step in the pipeline. For instance, we might
+# be interested about the parameters of the classifier. Since we selected
+# three features, we expect to have three coefficients.
+
+anova_svm[-1].coef_
+
+# %%
+# However, we do not know which features where selected from the original
+# dataset. We could proceed by several manner. Here, we will inverse the
+# transformation of these coefficients to get information about the original
+# space.
+
+anova_svm[:-1].inverse_transform(anova_svm[-1].coef_)
+
+# %%
+# We can see that the first three features where the selected features by
+# the first step.
('examples/inspection', 'plot_partial_dependence.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,137 +1,293 @@
 """
-========================
-Partial Dependence Plots
-========================
+===============================================================
+Partial Dependence and Individual Conditional Expectation Plots
+===============================================================

 Partial dependence plots show the dependence between the target function [2]_
-and a set of 'target' features, marginalizing over the
-values of all other features (the complement features). Due to the limits
-of human perception the size of the target feature set must be small (usually,
-one or two) thus the target features are usually chosen among the most
-important features.
-
-This example shows how to obtain partial dependence plots from a
+and a set of features of interest, marginalizing over the values of all other
+features (the complement features). Due to the limits of human perception, the
+size of the set of features of interest must be small (usually, one or two)
+thus they are usually chosen among the most important features.
+
+Similarly, an individual conditional expectation (ICE) plot [3]_
+shows the dependence between the target function and a feature of interest.
+However, unlike partial dependence plots, which show the average effect of the
+features of interest, ICE plots visualize the dependence of the prediction on a
+feature for each :term:`sample` separately, with one line per sample.
+Only one feature of interest is supported for ICE plots.
+
+This example shows how to obtain partial dependence and ICE plots from a
 :class:`~sklearn.neural_network.MLPRegressor` and a
-:class:`~sklearn.ensemble.GradientBoostingRegressor` trained on the
+:class:`~sklearn.ensemble.HistGradientBoostingRegressor` trained on the
 California housing dataset. The example is taken from [1]_.

-The plots show four 1-way and two 1-way partial dependence plots (ommitted for
-:class:`~sklearn.neural_network.MLPRegressor` due to computation time).
-The target variables for the one-way PDP are: median income (`MedInc`),
-average occupants per household (`AvgOccup`), median house age (`HouseAge`),
-and average rooms per household (`AveRooms`).
-
-We can clearly see that the median house price shows a linear relationship
-with the median income (top left) and that the house price drops when the
-average occupants per household increases (top middle).
-The top right plot shows that the house age in a district does not have
-a strong influence on the (median) house price; so does the average rooms
-per household.
-The tick marks on the x-axis represent the deciles of the feature values
-in the training data.
-
-We also observe that :class:`~sklearn.neural_network.MLPRegressor` has much
-smoother predictions than
-:class:`~sklearn.ensemble.GradientBoostingRegressor`. For the plots to be
-comparable, it is necessary to subtract the average value of the target
-``y``: The 'recursion' method, used by default for
-:class:`~sklearn.ensemble.GradientBoostingRegressor`, does not account for
-the initial predictor (in our case the average target). Setting the target
-average to 0 avoids this bias.
-
-Partial dependence plots with two target features enable us to visualize
-interactions among them. The two-way partial dependence plot shows the
-dependence of median house price on joint values of house age and average
-occupants per household. We can clearly see an interaction between the
-two features: for an average occupancy greater than two, the house price is
-nearly independent of the house age, whereas for values less than two there
-is a strong dependence on age.
-
-On a third figure, we have plotted the same partial dependence plot, this time
-in 3 dimensions.
-
-.. [1] T. Hastie, R. Tibshirani and J. Friedman,
-    "Elements of Statistical Learning Ed. 2", Springer, 2009.
+.. [1] T. Hastie, R. Tibshirani and J. Friedman, "Elements of Statistical
+       Learning Ed. 2", Springer, 2009.

 .. [2] For classification you can think of it as the regression score before
        the link function.
+
+.. [3] :arxiv:`Goldstein, A., Kapelner, A., Bleich, J., and Pitkin, E. (2015).
+       "Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of
+       Individual Conditional Expectation". Journal of Computational and
+       Graphical Statistics, 24(1): 44-65 <1309.6392>`
+
 """
-print(__doc__)
-
+
+# %%
+# California Housing data preprocessing
+# -------------------------------------
+#
+# Center target to avoid gradient boosting init bias: gradient boosting
+# with the 'recursion' method does not account for the initial estimator
+# (here the average target, by default).
+
+import pandas as pd
+from sklearn.datasets import fetch_california_housing
+from sklearn.model_selection import train_test_split
+
+cal_housing = fetch_california_housing()
+X = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)
+y = cal_housing.target
+
+y -= y.mean()
+
+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)
+
+# %%
+# 1-way partial dependence with different models
+# ----------------------------------------------
+#
+# In this section, we will compute 1-way partial dependence with two different
+# machine-learning models: (i) a multi-layer perceptron and (ii) a
+# gradient-boosting. With these two models, we illustrate how to compute and
+# interpret both partial dependence plot (PDP) and individual conditional
+# expectation (ICE).
+#
+# Multi-layer perceptron
+# ......................
+#
+# Let's fit a :class:`~sklearn.neural_network.MLPRegressor` and compute
+# single-variable partial dependence plots.
+
+from time import time
+from sklearn.pipeline import make_pipeline
+from sklearn.preprocessing import QuantileTransformer
+from sklearn.neural_network import MLPRegressor
+
+print("Training MLPRegressor...")
+tic = time()
+est = make_pipeline(
+    QuantileTransformer(),
+    MLPRegressor(
+        hidden_layer_sizes=(30, 15),
+        learning_rate_init=0.01,
+        early_stopping=True,
+        random_state=0,
+    ),
+)
+est.fit(X_train, y_train)
+print(f"done in {time() - tic:.3f}s")
+print(f"Test R2 score: {est.score(X_test, y_test):.2f}")
+
+# %%
+# We configured a pipeline to scale the numerical input features and tuned the
+# neural network size and learning rate to get a reasonable compromise between
+# training time and predictive performance on a test set.
+#
+# Importantly, this tabular dataset has very different dynamic ranges for its
+# features. Neural networks tend to be very sensitive to features with varying
+# scales and forgetting to preprocess the numeric feature would lead to a very
+# poor model.
+#
+# It would be possible to get even higher predictive performance with a larger
+# neural network but the training would also be significantly more expensive.
+#
+# Note that it is important to check that the model is accurate enough on a
+# test set before plotting the partial dependence since there would be little
+# use in explaining the impact of a given feature on the prediction function of
+# a poor model.
+#
+# We will plot the partial dependence, both individual (ICE) and averaged one
+# (PDP). We limit to only 50 ICE curves to not overcrowd the plot.
+
+from sklearn.inspection import PartialDependenceDisplay
+
+common_params = {
+    "subsample": 50,
+    "n_jobs": 2,
+    "grid_resolution": 20,
+    "centered": True,
+    "random_state": 0,
+}
+
+print("Computing partial dependence plots...")
+tic = time()
+display = PartialDependenceDisplay.from_estimator(
+    est,
+    X_train,
+    features=["MedInc", "AveOccup", "HouseAge", "AveRooms"],
+    kind="both",
+    **common_params,
+)
+print(f"done in {time() - tic:.3f}s")
+display.figure_.suptitle(
+    "Partial dependence of house value on non-location features\n"
+    "for the California housing dataset, with MLPRegressor"
+)
+display.figure_.subplots_adjust(hspace=0.3)
+
+# %%
+# Gradient boosting
+# .................
+#
+# Let's now fit a :class:`~sklearn.ensemble.HistGradientBoostingRegressor` and
+# compute the partial dependence on the same features.
+
+from sklearn.ensemble import HistGradientBoostingRegressor
+
+print("Training HistGradientBoostingRegressor...")
+tic = time()
+est = HistGradientBoostingRegressor(random_state=0)
+est.fit(X_train, y_train)
+print(f"done in {time() - tic:.3f}s")
+print(f"Test R2 score: {est.score(X_test, y_test):.2f}")
+
+# %%
+# Here, we used the default hyperparameters for the gradient boosting model
+# without any preprocessing as tree-based models are naturally robust to
+# monotonic transformations of numerical features.
+#
+# Note that on this tabular dataset, Gradient Boosting Machines are both
+# significantly faster to train and more accurate than neural networks. It is
+# also significantly cheaper to tune their hyperparameters (the defaults tend
+# to work well while this is not often the case for neural networks).
+#
+# We will plot the partial dependence, both individual (ICE) and averaged one
+# (PDP). We limit to only 50 ICE curves to not overcrowd the plot.
+
+print("Computing partial dependence plots...")
+tic = time()
+display = PartialDependenceDisplay.from_estimator(
+    est,
+    X_train,
+    features=["MedInc", "AveOccup", "HouseAge", "AveRooms"],
+    kind="both",
+    **common_params,
+)
+print(f"done in {time() - tic:.3f}s")
+display.figure_.suptitle(
+    "Partial dependence of house value on non-location features\n"
+    "for the California housing dataset, with Gradient Boosting"
+)
+display.figure_.subplots_adjust(wspace=0.4, hspace=0.3)
+
+# %%
+# Analysis of the plots
+# .....................
+#
+# We can clearly see on the PDPs (dashed orange line) that the median house price
+# shows a linear relationship with the median income (top left) and that the
+# house price drops when the average occupants per household increases (top
+# middle). The top right plot shows that the house age in a district does not
+# have a strong influence on the (median) house price; so does the average
+# rooms per household.
+#
+# The ICE curves (light blue lines) complement the analysis: we can see that
+# there are some exceptions (which are better highlighted with the option
+# `centered=True`), where the house price remains constant with respect to
+# median income and average occupants variations.
+# On the other hand, while the house age (top right) does not have a strong
+# influence on the median house price on average, there seems to be a number
+# of exceptions where the house price increases when
+# between the ages 15-25. Similar exceptions can be observed for the average
+# number of rooms (bottom left). Therefore, ICE plots show some individual
+# effect which are attenuated by taking the averages.
+#
+# In all plots, the tick marks on the x-axis represent the deciles of the
+# feature values in the training data.
+#
+# We also observe that :class:`~sklearn.neural_network.MLPRegressor` has much
+# smoother predictions than
+# :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
+#
+# However, it is worth noting that we are creating potential meaningless
+# synthetic samples if features are correlated.
+
+# %%
+# 2D interaction plots
+# --------------------
+#
+# PDPs with two features of interest enable us to visualize interactions among
+# them. However, ICEs cannot be plotted in an easy manner and thus interpreted.
+# Another consideration is linked to the performance to compute the PDPs. With
+# the tree-based algorithm, when only PDPs are requested, they can be computed
+# on an efficient way using the `'recursion'` method.
+import matplotlib.pyplot as plt
+
+print("Computing partial dependence plots...")
+tic = time()
+_, ax = plt.subplots(ncols=3, figsize=(9, 4))
+
+# Note that we could have called the method `from_estimator` three times and
+# provide one feature, one kind of plot, and one axis for each call.
+display = PartialDependenceDisplay.from_estimator(
+    est,
+    X_train,
+    features=["AveOccup", "HouseAge", ("AveOccup", "HouseAge")],
+    kind=["both", "both", "average"],
+    ax=ax,
+    **common_params,
+)
+
+print(f"done in {time() - tic:.3f}s")
+display.figure_.suptitle(
+    "Partial dependence of house value on non-location features\n"
+    "for the California housing dataset, with Gradient Boosting"
+)
+display.figure_.subplots_adjust(wspace=0.4, hspace=0.3)
+
+# %%
+# The two-way partial dependence plot shows the dependence of median house
+# price on joint values of house age and average occupants per household. We
+# can clearly see an interaction between the two features: for an average
+# occupancy greater than two, the house price is nearly independent of the
+# house age, whereas for values less than two there is a strong dependence on
+# age.
+#
+# 3D interaction plots
+# --------------------
+#
+# Let's make the same partial dependence plot for the 2 features interaction,
+# this time in 3 dimensions.
 import numpy as np
-import matplotlib.pyplot as plt
-from mpl_toolkits.mplot3d import Axes3D
+
+# unused but required import for doing 3d projections with matplotlib < 3.2
+import mpl_toolkits.mplot3d  # noqa: F401

 from sklearn.inspection import partial_dependence
-from sklearn.inspection import plot_partial_dependence
-from sklearn.ensemble import GradientBoostingRegressor
-from sklearn.neural_network import MLPRegressor
-from sklearn.datasets.california_housing import fetch_california_housing
-
-
-def main():
-    cal_housing = fetch_california_housing()
-
-    X, y = cal_housing.data, cal_housing.target
-    names = cal_housing.feature_names
-
-    # Center target to avoid gradient boosting init bias: gradient boosting
-    # with the 'recursion' method does not account for the initial estimator
-    # (here the average target, by default)
-    y -= y.mean()
-
-    print("Training MLPRegressor...")
-    est = MLPRegressor(activation='logistic')
-    est.fit(X, y)
-    print('Computing partial dependence plots...')
-    # We don't compute the 2-way PDP (5, 1) here, because it is a lot slower
-    # with the brute method.
-    features = [0, 5, 1, 2]
-    plot_partial_dependence(est, X, features, feature_names=names,
-                            n_jobs=3, grid_resolution=50)
-    fig = plt.gcf()
-    fig.suptitle('Partial dependence of house value on non-location features\n'
-                 'for the California housing dataset, with MLPRegressor')
-    plt.subplots_adjust(top=0.9)  # tight_layout causes overlap with suptitle
-
-    print("Training GradientBoostingRegressor...")
-    est = GradientBoostingRegressor(n_estimators=100, max_depth=4,
-                                    learning_rate=0.1, loss='huber',
-                                    random_state=1)
-    est.fit(X, y)
-    print('Computing partial dependence plots...')
-    features = [0, 5, 1, 2, (5, 1)]
-    plot_partial_dependence(est, X, features, feature_names=names,
-                            n_jobs=3, grid_resolution=50)
-    fig = plt.gcf()
-    fig.suptitle('Partial dependence of house value on non-location features\n'
-                 'for the California housing dataset, with Gradient Boosting')
-    plt.subplots_adjust(top=0.9)
-
-    print('Custom 3d plot via ``partial_dependence``')
-    fig = plt.figure()
-
-    target_feature = (1, 5)
-    pdp, axes = partial_dependence(est, X, target_feature,
-                                   grid_resolution=50)
-    XX, YY = np.meshgrid(axes[0], axes[1])
-    Z = pdp[0].T
-    ax = Axes3D(fig)
-    surf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1,
-                           cmap=plt.cm.BuPu, edgecolor='k')
-    ax.set_xlabel(names[target_feature[0]])
-    ax.set_ylabel(names[target_feature[1]])
-    ax.set_zlabel('Partial dependence')
-    #  pretty init view
-    ax.view_init(elev=22, azim=122)
-    plt.colorbar(surf)
-    plt.suptitle('Partial dependence of house value on median\n'
-                 'age and average occupancy, with Gradient Boosting')
-    plt.subplots_adjust(top=0.9)
-
-    plt.show()
-
-
-# Needed on Windows because plot_partial_dependence uses multiprocessing
-if __name__ == '__main__':
-    main()
+
+fig = plt.figure()
+
+features = ("AveOccup", "HouseAge")
+pdp = partial_dependence(
+    est, X_train, features=features, kind="average", grid_resolution=10
+)
+XX, YY = np.meshgrid(pdp["values"][0], pdp["values"][1])
+Z = pdp.average[0].T
+ax = fig.add_subplot(projection="3d")
+fig.add_axes(ax)
+
+surf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1, cmap=plt.cm.BuPu, edgecolor="k")
+ax.set_xlabel(features[0])
+ax.set_ylabel(features[1])
+ax.set_zlabel("Partial dependence")
+# pretty init view
+ax.view_init(elev=22, azim=122)
+plt.colorbar(surf)
+plt.suptitle(
+    "Partial dependence of house value on median\n"
+    "age and average occupancy, with Gradient Boosting"
+)
+plt.subplots_adjust(top=0.9)
+plt.show()
('examples/svm', 'plot_svm_nonlinear.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,34 +8,36 @@
 inputs.

 The color map illustrates the decision function learned by the SVC.
+
 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
 from sklearn import svm

-xx, yy = np.meshgrid(np.linspace(-3, 3, 500),
-                     np.linspace(-3, 3, 500))
+xx, yy = np.meshgrid(np.linspace(-3, 3, 500), np.linspace(-3, 3, 500))
 np.random.seed(0)
 X = np.random.randn(300, 2)
 Y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)

 # fit the model
-clf = svm.NuSVC(gamma='auto')
+clf = svm.NuSVC(gamma="auto")
 clf.fit(X, Y)

 # plot the decision function for each datapoint on the grid
 Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
 Z = Z.reshape(xx.shape)

-plt.imshow(Z, interpolation='nearest',
-           extent=(xx.min(), xx.max(), yy.min(), yy.max()), aspect='auto',
-           origin='lower', cmap=plt.cm.PuOr_r)
-contours = plt.contour(xx, yy, Z, levels=[0], linewidths=2,
-                       linestyles='dashed')
-plt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired,
-            edgecolors='k')
+plt.imshow(
+    Z,
+    interpolation="nearest",
+    extent=(xx.min(), xx.max(), yy.min(), yy.max()),
+    aspect="auto",
+    origin="lower",
+    cmap=plt.cm.PuOr_r,
+)
+contours = plt.contour(xx, yy, Z, levels=[0], linewidths=2, linestyles="dashed")
+plt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired, edgecolors="k")
 plt.xticks(())
 plt.yticks(())
 plt.axis([-3, 3, -3, 3])
('examples/svm', 'plot_separating_hyperplane.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,40 +6,43 @@
 Plot the maximum margin separating hyperplane within a two-class
 separable dataset using a Support Vector Machine classifier with
 linear kernel.
+
 """
-print(__doc__)

-import numpy as np
 import matplotlib.pyplot as plt
 from sklearn import svm
 from sklearn.datasets import make_blobs
+from sklearn.inspection import DecisionBoundaryDisplay


 # we create 40 separable points
 X, y = make_blobs(n_samples=40, centers=2, random_state=6)

 # fit the model, don't regularize for illustration purposes
-clf = svm.SVC(kernel='linear', C=1000)
+clf = svm.SVC(kernel="linear", C=1000)
 clf.fit(X, y)

 plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)

 # plot the decision function
 ax = plt.gca()
-xlim = ax.get_xlim()
-ylim = ax.get_ylim()
-
-# create grid to evaluate model
-xx = np.linspace(xlim[0], xlim[1], 30)
-yy = np.linspace(ylim[0], ylim[1], 30)
-YY, XX = np.meshgrid(yy, xx)
-xy = np.vstack([XX.ravel(), YY.ravel()]).T
-Z = clf.decision_function(xy).reshape(XX.shape)
-
-# plot decision boundary and margins
-ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,
-           linestyles=['--', '-', '--'])
+DecisionBoundaryDisplay.from_estimator(
+    clf,
+    X,
+    plot_method="contour",
+    colors="k",
+    levels=[-1, 0, 1],
+    alpha=0.5,
+    linestyles=["--", "-", "--"],
+    ax=ax,
+)
 # plot support vectors
-ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,
-           linewidth=1, facecolors='none', edgecolors='k')
+ax.scatter(
+    clf.support_vectors_[:, 0],
+    clf.support_vectors_[:, 1],
+    s=100,
+    linewidth=1,
+    facecolors="none",
+    edgecolors="k",
+)
 plt.show()
('examples/svm', 'plot_custom_kernel.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,16 +7,16 @@
 plot the decision surface and the support vectors.

 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
 from sklearn import svm, datasets
+from sklearn.inspection import DecisionBoundaryDisplay

 # import some data to play with
 iris = datasets.load_iris()
 X = iris.data[:, :2]  # we only take the first two features. We could
-                      # avoid this ugly slicing by using a two-dim dataset
+# avoid this ugly slicing by using a two-dim dataset
 Y = iris.target


@@ -32,26 +32,25 @@
     return np.dot(np.dot(X, M), Y.T)


-h = .02  # step size in the mesh
+h = 0.02  # step size in the mesh

 # we create an instance of SVM and fit out data.
 clf = svm.SVC(kernel=my_kernel)
 clf.fit(X, Y)

-# Plot the decision boundary. For that, we will assign a color to each
-# point in the mesh [x_min, x_max]x[y_min, y_max].
-x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
-y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
-xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
-Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
-
-# Put the result into a color plot
-Z = Z.reshape(xx.shape)
-plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)
+ax = plt.gca()
+DecisionBoundaryDisplay.from_estimator(
+    clf,
+    X,
+    cmap=plt.cm.Paired,
+    ax=ax,
+    response_method="predict",
+    plot_method="pcolormesh",
+    shading="auto",
+)

 # Plot also the training points
-plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired, edgecolors='k')
-plt.title('3-Class classification using Support Vector Machine with custom'
-          ' kernel')
-plt.axis('tight')
+plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired, edgecolors="k")
+plt.title("3-Class classification using Support Vector Machine with custom kernel")
+plt.axis("tight")
 plt.show()
('examples/svm', 'plot_svm_margin.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =========================================================
 SVM Margins Example
@@ -15,8 +13,6 @@
 the margins to be calculated using all the data in the area.

 """
-print(__doc__)
-

 # Code source: Gaël Varoquaux
 # Modified for documentation by Jaques Grobler
@@ -24,6 +20,7 @@

 import numpy as np
 import matplotlib.pyplot as plt
+from matplotlib import cm
 from sklearn import svm

 # we create 40 separable points
@@ -35,9 +32,9 @@
 fignum = 1

 # fit the model
-for name, penalty in (('unreg', 1), ('reg', 0.05)):
+for name, penalty in (("unreg", 1), ("reg", 0.05)):

-    clf = svm.SVC(kernel='linear', C=penalty)
+    clf = svm.SVC(kernel="linear", C=penalty)
     clf.fit(X, Y)

     # get the separating hyperplane
@@ -50,35 +47,42 @@
     # support vectors (margin away from hyperplane in direction
     # perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in
     # 2-d.
-    margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))
-    yy_down = yy - np.sqrt(1 + a ** 2) * margin
-    yy_up = yy + np.sqrt(1 + a ** 2) * margin
+    margin = 1 / np.sqrt(np.sum(clf.coef_**2))
+    yy_down = yy - np.sqrt(1 + a**2) * margin
+    yy_up = yy + np.sqrt(1 + a**2) * margin

     # plot the line, the points, and the nearest vectors to the plane
     plt.figure(fignum, figsize=(4, 3))
     plt.clf()
-    plt.plot(xx, yy, 'k-')
-    plt.plot(xx, yy_down, 'k--')
-    plt.plot(xx, yy_up, 'k--')
+    plt.plot(xx, yy, "k-")
+    plt.plot(xx, yy_down, "k--")
+    plt.plot(xx, yy_up, "k--")

-    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,
-                facecolors='none', zorder=10, edgecolors='k')
-    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,
-                edgecolors='k')
+    plt.scatter(
+        clf.support_vectors_[:, 0],
+        clf.support_vectors_[:, 1],
+        s=80,
+        facecolors="none",
+        zorder=10,
+        edgecolors="k",
+        cmap=cm.get_cmap("RdBu"),
+    )
+    plt.scatter(
+        X[:, 0], X[:, 1], c=Y, zorder=10, cmap=cm.get_cmap("RdBu"), edgecolors="k"
+    )

-    plt.axis('tight')
+    plt.axis("tight")
     x_min = -4.8
     x_max = 4.2
     y_min = -6
     y_max = 6

-    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]
-    Z = clf.predict(np.c_[XX.ravel(), YY.ravel()])
+    YY, XX = np.meshgrid(yy, xx)
+    xy = np.vstack([XX.ravel(), YY.ravel()]).T
+    Z = clf.decision_function(xy).reshape(XX.shape)

-    # Put the result into a color plot
-    Z = Z.reshape(XX.shape)
-    plt.figure(fignum, figsize=(4, 3))
-    plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired)
+    # Put the result into a contour plot
+    plt.contourf(XX, YY, Z, cmap=cm.get_cmap("RdBu"), alpha=0.5, linestyles=["-"])

     plt.xlim(x_min, x_max)
     plt.ylim(y_min, y_max)
('examples/svm', 'plot_iris_svc.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -33,48 +33,10 @@
    more realistic high-dimensional problems.

 """
-print(__doc__)

-import numpy as np
 import matplotlib.pyplot as plt
 from sklearn import svm, datasets
-
-
-def make_meshgrid(x, y, h=.02):
-    """Create a mesh of points to plot in
-
-    Parameters
-    ----------
-    x: data to base x-axis meshgrid on
-    y: data to base y-axis meshgrid on
-    h: stepsize for meshgrid, optional
-
-    Returns
-    -------
-    xx, yy : ndarray
-    """
-    x_min, x_max = x.min() - 1, x.max() + 1
-    y_min, y_max = y.min() - 1, y.max() + 1
-    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
-                         np.arange(y_min, y_max, h))
-    return xx, yy
-
-
-def plot_contours(ax, clf, xx, yy, **params):
-    """Plot the decision boundaries for a classifier.
-
-    Parameters
-    ----------
-    ax: matplotlib axes object
-    clf: a classifier
-    xx: meshgrid ndarray
-    yy: meshgrid ndarray
-    params: dictionary of params to pass to contourf, optional
-    """
-    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
-    Z = Z.reshape(xx.shape)
-    out = ax.contourf(xx, yy, Z, **params)
-    return out
+from sklearn.inspection import DecisionBoundaryDisplay


 # import some data to play with
@@ -86,33 +48,40 @@
 # we create an instance of SVM and fit out data. We do not scale our
 # data since we want to plot the support vectors
 C = 1.0  # SVM regularization parameter
-models = (svm.SVC(kernel='linear', C=C),
-          svm.LinearSVC(C=C, max_iter=10000),
-          svm.SVC(kernel='rbf', gamma=0.7, C=C),
-          svm.SVC(kernel='poly', degree=3, gamma='auto', C=C))
+models = (
+    svm.SVC(kernel="linear", C=C),
+    svm.LinearSVC(C=C, max_iter=10000),
+    svm.SVC(kernel="rbf", gamma=0.7, C=C),
+    svm.SVC(kernel="poly", degree=3, gamma="auto", C=C),
+)
 models = (clf.fit(X, y) for clf in models)

 # title for the plots
-titles = ('SVC with linear kernel',
-          'LinearSVC (linear kernel)',
-          'SVC with RBF kernel',
-          'SVC with polynomial (degree 3) kernel')
+titles = (
+    "SVC with linear kernel",
+    "LinearSVC (linear kernel)",
+    "SVC with RBF kernel",
+    "SVC with polynomial (degree 3) kernel",
+)

 # Set-up 2x2 grid for plotting.
 fig, sub = plt.subplots(2, 2)
 plt.subplots_adjust(wspace=0.4, hspace=0.4)

 X0, X1 = X[:, 0], X[:, 1]
-xx, yy = make_meshgrid(X0, X1)

 for clf, title, ax in zip(models, titles, sub.flatten()):
-    plot_contours(ax, clf, xx, yy,
-                  cmap=plt.cm.coolwarm, alpha=0.8)
-    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
-    ax.set_xlim(xx.min(), xx.max())
-    ax.set_ylim(yy.min(), yy.max())
-    ax.set_xlabel('Sepal length')
-    ax.set_ylabel('Sepal width')
+    disp = DecisionBoundaryDisplay.from_estimator(
+        clf,
+        X,
+        response_method="predict",
+        cmap=plt.cm.coolwarm,
+        alpha=0.8,
+        ax=ax,
+        xlabel=iris.feature_names[0],
+        ylabel=iris.feature_names[1],
+    )
+    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors="k")
     ax.set_xticks(())
     ax.set_yticks(())
     ax.set_title(title)
('examples/svm', 'plot_svm_anova.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,50 +7,60 @@
 SVC (support vector classifier) to improve the classification scores. We use
 the iris dataset (4 features) and add 36 non-informative features. We can find
 that our model achieves best performance when we select around 10% of features.
+
 """
-print(__doc__)

+# %%
+# Load some data to play with
+# ---------------------------
 import numpy as np
-import matplotlib.pyplot as plt
 from sklearn.datasets import load_iris
+
+X, y = load_iris(return_X_y=True)
+
+# Add non-informative features
+rng = np.random.RandomState(0)
+X = np.hstack((X, 2 * rng.random((X.shape[0], 36))))
+
+# %%
+# Create the pipeline
+# -------------------
+from sklearn.pipeline import Pipeline
 from sklearn.feature_selection import SelectPercentile, chi2
-from sklearn.model_selection import cross_val_score
-from sklearn.pipeline import Pipeline
 from sklearn.preprocessing import StandardScaler
 from sklearn.svm import SVC

+# Create a feature-selection transform, a scaler and an instance of SVM that we
+# combine together to have a full-blown estimator

-# #############################################################################
-# Import some data to play with
-X, y = load_iris(return_X_y=True)
-# Add non-informative features
-np.random.seed(0)
-X = np.hstack((X, 2 * np.random.random((X.shape[0], 36))))
+clf = Pipeline(
+    [
+        ("anova", SelectPercentile(chi2)),
+        ("scaler", StandardScaler()),
+        ("svc", SVC(gamma="auto")),
+    ]
+)

-# #############################################################################
-# Create a feature-selection transform, a scaler and an instance of SVM that we
-# combine together to have an full-blown estimator
-clf = Pipeline([('anova', SelectPercentile(chi2)),
-                ('scaler', StandardScaler()),
-                ('svc', SVC(gamma="auto"))])
+# %%
+# Plot the cross-validation score as a function of percentile of features
+# -----------------------------------------------------------------------
+import matplotlib.pyplot as plt
+from sklearn.model_selection import cross_val_score

-# #############################################################################
-# Plot the cross-validation score as a function of percentile of features
 score_means = list()
 score_stds = list()
 percentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)

 for percentile in percentiles:
     clf.set_params(anova__percentile=percentile)
-    this_scores = cross_val_score(clf, X, y, cv=5)
+    this_scores = cross_val_score(clf, X, y)
     score_means.append(this_scores.mean())
     score_stds.append(this_scores.std())

 plt.errorbar(percentiles, score_means, np.array(score_stds))
-plt.title(
-    'Performance of the SVM-Anova varying the percentile of features selected')
+plt.title("Performance of the SVM-Anova varying the percentile of features selected")
 plt.xticks(np.linspace(0, 100, 11, endpoint=True))
-plt.xlabel('Percentile')
-plt.ylabel('Accuracy Score')
-plt.axis('tight')
+plt.xlabel("Percentile")
+plt.ylabel("Accuracy Score")
+plt.axis("tight")
 plt.show()
('examples/svm', 'plot_rbf_parameters.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,4 +1,4 @@
-'''
+"""
 ==================
 RBF SVM parameters
 ==================
@@ -16,7 +16,7 @@
 ``C``, a smaller margin will be accepted if the decision function is better at
 classifying all training points correctly. A lower ``C`` will encourage a
 larger margin, therefore a simpler decision function, at the cost of training
-accuracy. In other words``C`` behaves as a regularization parameter in the
+accuracy. In other words ``C`` behaves as a regularization parameter in the
 SVM.

 The first plot is a visualization of the decision function for a variety of
@@ -53,13 +53,18 @@
 each point correctly (larger ``C`` values) hence the diagonal of good
 performing models.

-Finally one can also observe that for some intermediate values of ``gamma`` we
-get equally performing models when ``C`` becomes very large: it is not
-necessary to regularize by enforcing a larger margin. The radius of the RBF
-kernel alone acts as a good structural regularizer. In practice though it
-might still be interesting to simplify the decision function with a lower
-value of ``C`` so as to favor models that use less memory and that are faster
-to predict.
+Finally, one can also observe that for some intermediate values of ``gamma`` we
+get equally performing models when ``C`` becomes very large. This suggests that
+the set of support vectors does not change anymore. The radius of the RBF
+kernel alone acts as a good structural regularizer. Increasing ``C`` further
+doesn't help, likely because there are no more training points in violation
+(inside the margin or wrongly classified), or at least no better solution can
+be found. Scores being equal, it may make sense to use the smaller ``C``
+values, since very high ``C`` values typically increase fitting time.
+
+On the other hand, lower ``C`` values generally lead to more support vectors,
+which may increase prediction time. Therefore, lowering the value of ``C``
+involves a trade-off between fitting time and prediction time.

 We should also note that small differences in scores results from the random
 splits of the cross-validation procedure. Those spurious variations can be
@@ -68,25 +73,17 @@
 ``gamma_range`` steps will increase the resolution of the hyper-parameter heat
 map.

-'''
-print(__doc__)
+"""
+
+# %%
+# Utility class to move the midpoint of a colormap to be around
+# the values of interest.

 import numpy as np
-import matplotlib.pyplot as plt
 from matplotlib.colors import Normalize

-from sklearn.svm import SVC
-from sklearn.preprocessing import StandardScaler
-from sklearn.datasets import load_iris
-from sklearn.model_selection import StratifiedShuffleSplit
-from sklearn.model_selection import GridSearchCV
-
-
-# Utility function to move the midpoint of a colormap to be around
-# the values of interest.

 class MidpointNormalize(Normalize):
-
     def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):
         self.midpoint = midpoint
         Normalize.__init__(self, vmin, vmax, clip)
@@ -95,15 +92,20 @@
         x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]
         return np.ma.masked_array(np.interp(value, x, y))

-# #############################################################################
+
+# %%
 # Load and prepare data set
+# -------------------------
 #
 # dataset for grid search
+
+from sklearn.datasets import load_iris

 iris = load_iris()
 X = iris.data
 y = iris.target

+# %%
 # Dataset for decision function visualization: we only keep the first two
 # features in X and sub-sample the dataset to keep only 2 classes and
 # make it a binary classification problem.
@@ -113,21 +115,29 @@
 y_2d = y[y > 0]
 y_2d -= 1

+# %%
 # It is usually a good idea to scale the data for SVM training.
 # We are cheating a bit in this example in scaling all of the data,
 # instead of fitting the transformation on the training set and
 # just applying it on the test set.

+from sklearn.preprocessing import StandardScaler
+
 scaler = StandardScaler()
 X = scaler.fit_transform(X)
 X_2d = scaler.fit_transform(X_2d)

-# #############################################################################
+# %%
 # Train classifiers
+# -----------------
 #
 # For an initial search, a logarithmic grid with basis
 # 10 is often helpful. Using a basis of 2, a finer
 # tuning can be achieved but at a much higher cost.
+
+from sklearn.svm import SVC
+from sklearn.model_selection import StratifiedShuffleSplit
+from sklearn.model_selection import GridSearchCV

 C_range = np.logspace(-2, 10, 13)
 gamma_range = np.logspace(-9, 3, 13)
@@ -136,9 +146,12 @@
 grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)
 grid.fit(X, y)

-print("The best parameters are %s with a score of %0.2f"
-      % (grid.best_params_, grid.best_score_))
-
+print(
+    "The best parameters are %s with a score of %0.2f"
+    % (grid.best_params_, grid.best_score_)
+)
+
+# %%
 # Now we need to fit a classifier for all parameters in the 2d version
 # (we use a smaller set of parameters here because it takes a while to train)

@@ -151,34 +164,35 @@
         clf.fit(X_2d, y_2d)
         classifiers.append((C, gamma, clf))

-# #############################################################################
+# %%
 # Visualization
+# -------------
 #
 # draw visualization of parameter effects
+
+import matplotlib.pyplot as plt

 plt.figure(figsize=(8, 6))
 xx, yy = np.meshgrid(np.linspace(-3, 3, 200), np.linspace(-3, 3, 200))
-for (k, (C, gamma, clf)) in enumerate(classifiers):
+for k, (C, gamma, clf) in enumerate(classifiers):
     # evaluate decision function in a grid
     Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
     Z = Z.reshape(xx.shape)

     # visualize decision function for these parameters
     plt.subplot(len(C_2d_range), len(gamma_2d_range), k + 1)
-    plt.title("gamma=10^%d, C=10^%d" % (np.log10(gamma), np.log10(C)),
-              size='medium')
+    plt.title("gamma=10^%d, C=10^%d" % (np.log10(gamma), np.log10(C)), size="medium")

     # visualize parameter's effect on decision function
     plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)
-    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu_r,
-                edgecolors='k')
+    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu_r, edgecolors="k")
     plt.xticks(())
     plt.yticks(())
-    plt.axis('tight')
-
-scores = grid.cv_results_['mean_test_score'].reshape(len(C_range),
-                                                     len(gamma_range))
-
+    plt.axis("tight")
+
+scores = grid.cv_results_["mean_test_score"].reshape(len(C_range), len(gamma_range))
+
+# %%
 # Draw heatmap of the validation accuracy as a function of gamma and C
 #
 # The score are encoded as colors with the hot colormap which varies from dark
@@ -189,13 +203,17 @@
 # the same color.

 plt.figure(figsize=(8, 6))
-plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)
-plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,
-           norm=MidpointNormalize(vmin=0.2, midpoint=0.92))
-plt.xlabel('gamma')
-plt.ylabel('C')
+plt.subplots_adjust(left=0.2, right=0.95, bottom=0.15, top=0.95)
+plt.imshow(
+    scores,
+    interpolation="nearest",
+    cmap=plt.cm.hot,
+    norm=MidpointNormalize(vmin=0.2, midpoint=0.92),
+)
+plt.xlabel("gamma")
+plt.ylabel("C")
 plt.colorbar()
 plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)
 plt.yticks(np.arange(len(C_range)), C_range)
-plt.title('Validation accuracy')
+plt.title("Validation accuracy")
 plt.show()
('examples/svm', 'plot_separating_hyperplane_unbalanced.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -24,57 +24,64 @@
         clf = SGDClassifier(n_iter=100, alpha=0.01)

 """
-print(__doc__)

-import numpy as np
 import matplotlib.pyplot as plt
 from sklearn import svm
 from sklearn.datasets import make_blobs
+from sklearn.inspection import DecisionBoundaryDisplay

 # we create two clusters of random points
 n_samples_1 = 1000
 n_samples_2 = 100
 centers = [[0.0, 0.0], [2.0, 2.0]]
 clusters_std = [1.5, 0.5]
-X, y = make_blobs(n_samples=[n_samples_1, n_samples_2],
-                  centers=centers,
-                  cluster_std=clusters_std,
-                  random_state=0, shuffle=False)
+X, y = make_blobs(
+    n_samples=[n_samples_1, n_samples_2],
+    centers=centers,
+    cluster_std=clusters_std,
+    random_state=0,
+    shuffle=False,
+)

 # fit the model and get the separating hyperplane
-clf = svm.SVC(kernel='linear', C=1.0)
+clf = svm.SVC(kernel="linear", C=1.0)
 clf.fit(X, y)

 # fit the model and get the separating hyperplane using weighted classes
-wclf = svm.SVC(kernel='linear', class_weight={1: 10})
+wclf = svm.SVC(kernel="linear", class_weight={1: 10})
 wclf.fit(X, y)

 # plot the samples
-plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')
+plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors="k")

 # plot the decision functions for both classifiers
 ax = plt.gca()
-xlim = ax.get_xlim()
-ylim = ax.get_ylim()
-
-# create grid to evaluate model
-xx = np.linspace(xlim[0], xlim[1], 30)
-yy = np.linspace(ylim[0], ylim[1], 30)
-YY, XX = np.meshgrid(yy, xx)
-xy = np.vstack([XX.ravel(), YY.ravel()]).T
-
-# get the separating hyperplane
-Z = clf.decision_function(xy).reshape(XX.shape)
-
-# plot decision boundary and margins
-a = ax.contour(XX, YY, Z, colors='k', levels=[0], alpha=0.5, linestyles=['-'])
-
-# get the separating hyperplane for weighted classes
-Z = wclf.decision_function(xy).reshape(XX.shape)
+disp = DecisionBoundaryDisplay.from_estimator(
+    clf,
+    X,
+    plot_method="contour",
+    colors="k",
+    levels=[0],
+    alpha=0.5,
+    linestyles=["-"],
+    ax=ax,
+)

 # plot decision boundary and margins for weighted classes
-b = ax.contour(XX, YY, Z, colors='r', levels=[0], alpha=0.5, linestyles=['-'])
+wdisp = DecisionBoundaryDisplay.from_estimator(
+    wclf,
+    X,
+    plot_method="contour",
+    colors="r",
+    levels=[0],
+    alpha=0.5,
+    linestyles=["-"],
+    ax=ax,
+)

-plt.legend([a.collections[0], b.collections[0]], ["non weighted", "weighted"],
-           loc="upper right")
+plt.legend(
+    [disp.surface_.collections[0], wdisp.surface_.collections[0]],
+    ["non weighted", "weighted"],
+    loc="upper right",
+)
 plt.show()
('examples/svm', 'plot_oneclass.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,8 +8,8 @@
 :ref:`One-class SVM <svm_outlier_detection>` is an unsupervised
 algorithm that learns a decision function for novelty detection:
 classifying new data as similar or different to the training set.
+
 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -42,25 +42,29 @@

 plt.title("Novelty Detection")
 plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)
-a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')
-plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')
+a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors="darkred")
+plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors="palevioletred")

 s = 40
-b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s, edgecolors='k')
-b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='blueviolet', s=s,
-                 edgecolors='k')
-c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='gold', s=s,
-                edgecolors='k')
-plt.axis('tight')
+b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c="white", s=s, edgecolors="k")
+b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c="blueviolet", s=s, edgecolors="k")
+c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c="gold", s=s, edgecolors="k")
+plt.axis("tight")
 plt.xlim((-5, 5))
 plt.ylim((-5, 5))
-plt.legend([a.collections[0], b1, b2, c],
-           ["learned frontier", "training observations",
-            "new regular observations", "new abnormal observations"],
-           loc="upper left",
-           prop=matplotlib.font_manager.FontProperties(size=11))
+plt.legend(
+    [a.collections[0], b1, b2, c],
+    [
+        "learned frontier",
+        "training observations",
+        "new regular observations",
+        "new abnormal observations",
+    ],
+    loc="upper left",
+    prop=matplotlib.font_manager.FontProperties(size=11),
+)
 plt.xlabel(
-    "error train: %d/200 ; errors novel regular: %d/40 ; "
-    "errors novel abnormal: %d/40"
-    % (n_error_train, n_error_test, n_error_outliers))
+    "error train: %d/200 ; errors novel regular: %d/40 ; errors novel abnormal: %d/40"
+    % (n_error_train, n_error_test, n_error_outliers)
+)
 plt.show()
('examples/svm', 'plot_svm_scale_c.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -75,14 +75,12 @@
     Two separate datasets are used for the two different plots. The reason
     behind this is the `l1` case works better on sparse data, while `l2`
     is better suited to the non-sparse case.
+
 """
-print(__doc__)
-

 # Author: Andreas Mueller <amueller@ais.uni-bonn.de>
 #         Jaques Grobler <jaques.grobler@inria.fr>
 # License: BSD 3 clause
-

 import numpy as np
 import matplotlib.pyplot as plt
@@ -100,23 +98,31 @@
 n_features = 300

 # l1 data (only 5 informative features)
-X_1, y_1 = datasets.make_classification(n_samples=n_samples,
-                                        n_features=n_features, n_informative=5,
-                                        random_state=1)
+X_1, y_1 = datasets.make_classification(
+    n_samples=n_samples, n_features=n_features, n_informative=5, random_state=1
+)

 # l2 data: non sparse, but less features
-y_2 = np.sign(.5 - rnd.rand(n_samples))
+y_2 = np.sign(0.5 - rnd.rand(n_samples))
 X_2 = rnd.randn(n_samples, n_features // 5) + y_2[:, np.newaxis]
 X_2 += 5 * rnd.randn(n_samples, n_features // 5)

-clf_sets = [(LinearSVC(penalty='l1', loss='squared_hinge', dual=False,
-                       tol=1e-3),
-             np.logspace(-2.3, -1.3, 10), X_1, y_1),
-            (LinearSVC(penalty='l2', loss='squared_hinge', dual=True,
-                       tol=1e-4),
-             np.logspace(-4.5, -2, 10), X_2, y_2)]
+clf_sets = [
+    (
+        LinearSVC(penalty="l1", loss="squared_hinge", dual=False, tol=1e-3),
+        np.logspace(-2.3, -1.3, 10),
+        X_1,
+        y_1,
+    ),
+    (
+        LinearSVC(penalty="l2", loss="squared_hinge", dual=True),
+        np.logspace(-4.5, -2, 10),
+        X_2,
+        y_2,
+    ),
+]

-colors = ['navy', 'cyan', 'darkorange']
+colors = ["navy", "cyan", "darkorange"]
 lw = 2

 for clf, cs, X, y in clf_sets:
@@ -127,25 +133,36 @@
         param_grid = dict(C=cs)
         # To get nice curve, we need a large number of iterations to
         # reduce the variance
-        grid = GridSearchCV(clf, refit=False, param_grid=param_grid,
-                            cv=ShuffleSplit(train_size=train_size,
-                                            test_size=.3,
-                                            n_splits=250, random_state=1))
+        grid = GridSearchCV(
+            clf,
+            refit=False,
+            param_grid=param_grid,
+            cv=ShuffleSplit(
+                train_size=train_size, test_size=0.3, n_splits=50, random_state=1
+            ),
+        )
         grid.fit(X, y)
-        scores = grid.cv_results_['mean_test_score']
+        scores = grid.cv_results_["mean_test_score"]

-        scales = [(1, 'No scaling'),
-                  ((n_samples * train_size), '1/n_samples'),
-                  ]
+        scales = [
+            (1, "No scaling"),
+            ((n_samples * train_size), "1/n_samples"),
+        ]

         for ax, (scaler, name) in zip(axes, scales):
-            ax.set_xlabel('C')
-            ax.set_ylabel('CV Score')
+            ax.set_xlabel("C")
+            ax.set_ylabel("CV Score")
             grid_cs = cs * float(scaler)  # scale the C's
-            ax.semilogx(grid_cs, scores, label="fraction %.2f" %
-                        train_size, color=colors[k], lw=lw)
-            ax.set_title('scaling=%s, penalty=%s, loss=%s' %
-                         (name, clf.penalty, clf.loss))
+            ax.semilogx(
+                grid_cs,
+                scores,
+                label="fraction %.2f" % train_size,
+                color=colors[k],
+                lw=lw,
+            )
+            ax.set_title(
+                "scaling=%s, penalty=%s, loss=%s" % (name, clf.penalty, clf.loss)
+            )

     plt.legend(loc="best")
 plt.show()
('examples/svm', 'plot_svm_kernels.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =========================================================
 SVM-Kernels
@@ -12,8 +10,6 @@


 """
-print(__doc__)
-

 # Code source: Gaël Varoquaux
 # License: BSD 3 clause
@@ -24,30 +20,32 @@


 # Our dataset and targets
-X = np.c_[(.4, -.7),
-          (-1.5, -1),
-          (-1.4, -.9),
-          (-1.3, -1.2),
-          (-1.1, -.2),
-          (-1.2, -.4),
-          (-.5, 1.2),
-          (-1.5, 2.1),
-          (1, 1),
-          # --
-          (1.3, .8),
-          (1.2, .5),
-          (.2, -2),
-          (.5, -2.4),
-          (.2, -2.3),
-          (0, -2.7),
-          (1.3, 2.1)].T
+X = np.c_[
+    (0.4, -0.7),
+    (-1.5, -1),
+    (-1.4, -0.9),
+    (-1.3, -1.2),
+    (-1.1, -0.2),
+    (-1.2, -0.4),
+    (-0.5, 1.2),
+    (-1.5, 2.1),
+    (1, 1),
+    # --
+    (1.3, 0.8),
+    (1.2, 0.5),
+    (0.2, -2),
+    (0.5, -2.4),
+    (0.2, -2.3),
+    (0, -2.7),
+    (1.3, 2.1),
+].T
 Y = [0] * 8 + [1] * 8

 # figure number
 fignum = 1

 # fit the model
-for kernel in ('linear', 'poly', 'rbf'):
+for kernel in ("linear", "poly", "rbf"):
     clf = svm.SVC(kernel=kernel, gamma=2)
     clf.fit(X, Y)

@@ -55,12 +53,17 @@
     plt.figure(fignum, figsize=(4, 3))
     plt.clf()

-    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,
-                facecolors='none', zorder=10, edgecolors='k')
-    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,
-                edgecolors='k')
+    plt.scatter(
+        clf.support_vectors_[:, 0],
+        clf.support_vectors_[:, 1],
+        s=80,
+        facecolors="none",
+        zorder=10,
+        edgecolors="k",
+    )
+    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired, edgecolors="k")

-    plt.axis('tight')
+    plt.axis("tight")
     x_min = -3
     x_max = 3
     y_min = -3
@@ -73,8 +76,14 @@
     Z = Z.reshape(XX.shape)
     plt.figure(fignum, figsize=(4, 3))
     plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)
-    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],
-                levels=[-.5, 0, .5])
+    plt.contour(
+        XX,
+        YY,
+        Z,
+        colors=["k", "k", "k"],
+        linestyles=["--", "-", "--"],
+        levels=[-0.5, 0, 0.5],
+    )

     plt.xlim(x_min, x_max)
     plt.ylim(y_min, y_max)
('examples/svm', 'plot_svm_regression.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,51 +6,70 @@
 Toy example of 1D regression using linear, polynomial and RBF kernels.

 """
-print(__doc__)

 import numpy as np
 from sklearn.svm import SVR
 import matplotlib.pyplot as plt

-# #############################################################################
+# %%
 # Generate sample data
+# --------------------
 X = np.sort(5 * np.random.rand(40, 1), axis=0)
 y = np.sin(X).ravel()

-# #############################################################################
-# Add noise to targets
+# add noise to targets
 y[::5] += 3 * (0.5 - np.random.rand(8))

-# #############################################################################
+# %%
 # Fit regression model
-svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)
-svr_lin = SVR(kernel='linear', C=100, gamma='auto')
-svr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1,
-               coef0=1)
+# --------------------
+svr_rbf = SVR(kernel="rbf", C=100, gamma=0.1, epsilon=0.1)
+svr_lin = SVR(kernel="linear", C=100, gamma="auto")
+svr_poly = SVR(kernel="poly", C=100, gamma="auto", degree=3, epsilon=0.1, coef0=1)

-# #############################################################################
+# %%
 # Look at the results
+# -------------------
 lw = 2

 svrs = [svr_rbf, svr_lin, svr_poly]
-kernel_label = ['RBF', 'Linear', 'Polynomial']
-model_color = ['m', 'c', 'g']
+kernel_label = ["RBF", "Linear", "Polynomial"]
+model_color = ["m", "c", "g"]

 fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)
 for ix, svr in enumerate(svrs):
-    axes[ix].plot(X, svr.fit(X, y).predict(X), color=model_color[ix], lw=lw,
-                  label='{} model'.format(kernel_label[ix]))
-    axes[ix].scatter(X[svr.support_], y[svr.support_], facecolor="none",
-                     edgecolor=model_color[ix], s=50,
-                     label='{} support vectors'.format(kernel_label[ix]))
-    axes[ix].scatter(X[np.setdiff1d(np.arange(len(X)), svr.support_)],
-                     y[np.setdiff1d(np.arange(len(X)), svr.support_)],
-                     facecolor="none", edgecolor="k", s=50,
-                     label='other training data')
-    axes[ix].legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),
-                    ncol=1, fancybox=True, shadow=True)
+    axes[ix].plot(
+        X,
+        svr.fit(X, y).predict(X),
+        color=model_color[ix],
+        lw=lw,
+        label="{} model".format(kernel_label[ix]),
+    )
+    axes[ix].scatter(
+        X[svr.support_],
+        y[svr.support_],
+        facecolor="none",
+        edgecolor=model_color[ix],
+        s=50,
+        label="{} support vectors".format(kernel_label[ix]),
+    )
+    axes[ix].scatter(
+        X[np.setdiff1d(np.arange(len(X)), svr.support_)],
+        y[np.setdiff1d(np.arange(len(X)), svr.support_)],
+        facecolor="none",
+        edgecolor="k",
+        s=50,
+        label="other training data",
+    )
+    axes[ix].legend(
+        loc="upper center",
+        bbox_to_anchor=(0.5, 1.1),
+        ncol=1,
+        fancybox=True,
+        shadow=True,
+    )

-fig.text(0.5, 0.04, 'data', ha='center', va='center')
-fig.text(0.06, 0.5, 'target', ha='center', va='center', rotation='vertical')
+fig.text(0.5, 0.04, "data", ha="center", va="center")
+fig.text(0.06, 0.5, "target", ha="center", va="center", rotation="vertical")
 fig.suptitle("Support Vector Regression", fontsize=14)
 plt.show()
('examples/svm', 'plot_weighted_samples.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -11,8 +11,8 @@
 subtle.
 To emphasize the effect here, we particularly weight outliers, making the
 deformation of the decision boundary very visible.
+
 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -28,10 +28,17 @@

     # plot the line, the points, and the nearest vectors to the plane
     axis.contourf(xx, yy, Z, alpha=0.75, cmap=plt.cm.bone)
-    axis.scatter(X[:, 0], X[:, 1], c=y, s=100 * sample_weight, alpha=0.9,
-                 cmap=plt.cm.bone, edgecolors='black')
+    axis.scatter(
+        X[:, 0],
+        X[:, 1],
+        c=y,
+        s=100 * sample_weight,
+        alpha=0.9,
+        cmap=plt.cm.bone,
+        edgecolors="black",
+    )

-    axis.axis('off')
+    axis.axis("off")
     axis.set_title(title)


@@ -45,19 +52,20 @@
 sample_weight_last_ten[15:] *= 5
 sample_weight_last_ten[9] *= 15

-# for reference, first fit without sample weights
+# Fit the models.

-# fit the model
+# This model does not take into account sample weights.
+clf_no_weights = svm.SVC(gamma=1)
+clf_no_weights.fit(X, y)
+
+# This other model takes into account some dedicated sample weights.
 clf_weights = svm.SVC(gamma=1)
 clf_weights.fit(X, y, sample_weight=sample_weight_last_ten)

-clf_no_weights = svm.SVC(gamma=1)
-clf_no_weights.fit(X, y)
-
 fig, axes = plt.subplots(1, 2, figsize=(14, 6))
-plot_decision_function(clf_no_weights, sample_weight_constant, axes[0],
-                       "Constant weights")
-plot_decision_function(clf_weights, sample_weight_last_ten, axes[1],
-                       "Modified weights")
+plot_decision_function(
+    clf_no_weights, sample_weight_constant, axes[0], "Constant weights"
+)
+plot_decision_function(clf_weights, sample_weight_last_ten, axes[1], "Modified weights")

 plt.show()
('examples/manifold', 'plot_mds.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,12 +7,12 @@

 The reconstructed points using the metric MDS and non metric MDS are slightly
 shifted to avoid overlapping.
+
 """

 # Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>
 # License: BSD

-print(__doc__)
 import numpy as np

 from matplotlib import pyplot as plt
@@ -22,9 +22,10 @@
 from sklearn.metrics import euclidean_distances
 from sklearn.decomposition import PCA

+EPSILON = np.finfo(np.float32).eps
 n_samples = 20
 seed = np.random.RandomState(seed=3)
-X_true = seed.randint(0, 20, 2 * n_samples).astype(np.float)
+X_true = seed.randint(0, 20, 2 * n_samples).astype(float)
 X_true = X_true.reshape((n_samples, 2))
 # Center the data
 X_true -= X_true.mean()
@@ -37,18 +38,31 @@
 noise[np.arange(noise.shape[0]), np.arange(noise.shape[0])] = 0
 similarities += noise

-mds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=seed,
-                   dissimilarity="precomputed", n_jobs=1)
+mds = manifold.MDS(
+    n_components=2,
+    max_iter=3000,
+    eps=1e-9,
+    random_state=seed,
+    dissimilarity="precomputed",
+    n_jobs=1,
+)
 pos = mds.fit(similarities).embedding_

-nmds = manifold.MDS(n_components=2, metric=False, max_iter=3000, eps=1e-12,
-                    dissimilarity="precomputed", random_state=seed, n_jobs=1,
-                    n_init=1)
+nmds = manifold.MDS(
+    n_components=2,
+    metric=False,
+    max_iter=3000,
+    eps=1e-12,
+    dissimilarity="precomputed",
+    random_state=seed,
+    n_jobs=1,
+    n_init=1,
+)
 npos = nmds.fit_transform(similarities, init=pos)

 # Rescale the data
-pos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((pos ** 2).sum())
-npos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((npos ** 2).sum())
+pos *= np.sqrt((X_true**2).sum()) / np.sqrt((pos**2).sum())
+npos *= np.sqrt((X_true**2).sum()) / np.sqrt((npos**2).sum())

 # Rotate the data
 clf = PCA(n_components=2)
@@ -59,28 +73,27 @@
 npos = clf.fit_transform(npos)

 fig = plt.figure(1)
-ax = plt.axes([0., 0., 1., 1.])
+ax = plt.axes([0.0, 0.0, 1.0, 1.0])

 s = 100
-plt.scatter(X_true[:, 0], X_true[:, 1], color='navy', s=s, lw=0,
-            label='True Position')
-plt.scatter(pos[:, 0], pos[:, 1], color='turquoise', s=s, lw=0, label='MDS')
-plt.scatter(npos[:, 0], npos[:, 1], color='darkorange', s=s, lw=0, label='NMDS')
-plt.legend(scatterpoints=1, loc='best', shadow=False)
+plt.scatter(X_true[:, 0], X_true[:, 1], color="navy", s=s, lw=0, label="True Position")
+plt.scatter(pos[:, 0], pos[:, 1], color="turquoise", s=s, lw=0, label="MDS")
+plt.scatter(npos[:, 0], npos[:, 1], color="darkorange", s=s, lw=0, label="NMDS")
+plt.legend(scatterpoints=1, loc="best", shadow=False)

-similarities = similarities.max() / similarities * 100
-similarities[np.isinf(similarities)] = 0
-
+similarities = similarities.max() / (similarities + EPSILON) * 100
+np.fill_diagonal(similarities, 0)
 # Plot the edges
 start_idx, end_idx = np.where(pos)
 # a sequence of (*line0*, *line1*, *line2*), where::
 #            linen = (x0, y0), (x1, y1), ... (xm, ym)
-segments = [[X_true[i, :], X_true[j, :]]
-            for i in range(len(pos)) for j in range(len(pos))]
+segments = [
+    [X_true[i, :], X_true[j, :]] for i in range(len(pos)) for j in range(len(pos))
+]
 values = np.abs(similarities)
-lc = LineCollection(segments,
-                    zorder=0, cmap=plt.cm.Blues,
-                    norm=plt.Normalize(0, values.max()))
+lc = LineCollection(
+    segments, zorder=0, cmap=plt.cm.Blues, norm=plt.Normalize(0, values.max())
+)
 lc.set_array(similarities.flatten())
 lc.set_linewidths(np.full(len(segments), 0.5))
 ax.add_collection(lc)
('examples/manifold', 'plot_compare_methods.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -17,103 +17,194 @@
 the distances in the original high-dimensional space, unlike other
 manifold-learning algorithms, it does not seeks an isotropic
 representation of the data in the low-dimensional space.
+
 """

 # Author: Jake Vanderplas -- <vanderplas@astro.washington.edu>

-print(__doc__)
-
-from time import time
-
+# %%
+# Dataset preparation
+# -------------------
+#
+# We start by generating the S-curve dataset.
+
+from numpy.random import RandomState
 import matplotlib.pyplot as plt
-from mpl_toolkits.mplot3d import Axes3D
-from matplotlib.ticker import NullFormatter
+from matplotlib import ticker
+
+# unused but required import for doing 3d projections with matplotlib < 3.2
+import mpl_toolkits.mplot3d  # noqa: F401

 from sklearn import manifold, datasets

-# Next line to silence pyflakes. This import is needed.
-Axes3D
-
-n_points = 1000
-X, color = datasets.samples_generator.make_s_curve(n_points, random_state=0)
-n_neighbors = 10
-n_components = 2
-
-fig = plt.figure(figsize=(15, 8))
-plt.suptitle("Manifold Learning with %i points, %i neighbors"
-             % (1000, n_neighbors), fontsize=14)
-
-
-ax = fig.add_subplot(251, projection='3d')
-ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
-ax.view_init(4, -72)
-
-methods = ['standard', 'ltsa', 'hessian', 'modified']
-labels = ['LLE', 'LTSA', 'Hessian LLE', 'Modified LLE']
-
-for i, method in enumerate(methods):
-    t0 = time()
-    Y = manifold.LocallyLinearEmbedding(n_neighbors, n_components,
-                                        eigen_solver='auto',
-                                        method=method).fit_transform(X)
-    t1 = time()
-    print("%s: %.2g sec" % (methods[i], t1 - t0))
-
-    ax = fig.add_subplot(252 + i)
-    plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
-    plt.title("%s (%.2g sec)" % (labels[i], t1 - t0))
-    ax.xaxis.set_major_formatter(NullFormatter())
-    ax.yaxis.set_major_formatter(NullFormatter())
-    plt.axis('tight')
-
-t0 = time()
-Y = manifold.Isomap(n_neighbors, n_components).fit_transform(X)
-t1 = time()
-print("Isomap: %.2g sec" % (t1 - t0))
-ax = fig.add_subplot(257)
-plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
-plt.title("Isomap (%.2g sec)" % (t1 - t0))
-ax.xaxis.set_major_formatter(NullFormatter())
-ax.yaxis.set_major_formatter(NullFormatter())
-plt.axis('tight')
-
-
-t0 = time()
-mds = manifold.MDS(n_components, max_iter=100, n_init=1)
-Y = mds.fit_transform(X)
-t1 = time()
-print("MDS: %.2g sec" % (t1 - t0))
-ax = fig.add_subplot(258)
-plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
-plt.title("MDS (%.2g sec)" % (t1 - t0))
-ax.xaxis.set_major_formatter(NullFormatter())
-ax.yaxis.set_major_formatter(NullFormatter())
-plt.axis('tight')
-
-
-t0 = time()
-se = manifold.SpectralEmbedding(n_components=n_components,
-                                n_neighbors=n_neighbors)
-Y = se.fit_transform(X)
-t1 = time()
-print("SpectralEmbedding: %.2g sec" % (t1 - t0))
-ax = fig.add_subplot(259)
-plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
-plt.title("SpectralEmbedding (%.2g sec)" % (t1 - t0))
-ax.xaxis.set_major_formatter(NullFormatter())
-ax.yaxis.set_major_formatter(NullFormatter())
-plt.axis('tight')
-
-t0 = time()
-tsne = manifold.TSNE(n_components=n_components, init='pca', random_state=0)
-Y = tsne.fit_transform(X)
-t1 = time()
-print("t-SNE: %.2g sec" % (t1 - t0))
-ax = fig.add_subplot(2, 5, 10)
-plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
-plt.title("t-SNE (%.2g sec)" % (t1 - t0))
-ax.xaxis.set_major_formatter(NullFormatter())
-ax.yaxis.set_major_formatter(NullFormatter())
-plt.axis('tight')
+rng = RandomState(0)
+
+n_samples = 1500
+S_points, S_color = datasets.make_s_curve(n_samples, random_state=rng)
+
+# %%
+# Let's look at the original data. Also define some helping
+# functions, which we will use further on.
+
+
+def plot_3d(points, points_color, title):
+    x, y, z = points.T
+
+    fig, ax = plt.subplots(
+        figsize=(6, 6),
+        facecolor="white",
+        tight_layout=True,
+        subplot_kw={"projection": "3d"},
+    )
+    fig.suptitle(title, size=16)
+    col = ax.scatter(x, y, z, c=points_color, s=50, alpha=0.8)
+    ax.view_init(azim=-60, elev=9)
+    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
+    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))
+    ax.zaxis.set_major_locator(ticker.MultipleLocator(1))
+
+    fig.colorbar(col, ax=ax, orientation="horizontal", shrink=0.6, aspect=60, pad=0.01)
+    plt.show()
+
+
+def plot_2d(points, points_color, title):
+    fig, ax = plt.subplots(figsize=(3, 3), facecolor="white", constrained_layout=True)
+    fig.suptitle(title, size=16)
+    add_2d_scatter(ax, points, points_color)
+    plt.show()
+
+
+def add_2d_scatter(ax, points, points_color, title=None):
+    x, y = points.T
+    ax.scatter(x, y, c=points_color, s=50, alpha=0.8)
+    ax.set_title(title)
+    ax.xaxis.set_major_formatter(ticker.NullFormatter())
+    ax.yaxis.set_major_formatter(ticker.NullFormatter())
+
+
+plot_3d(S_points, S_color, "Original S-curve samples")
+
+# %%
+# Define algorithms for the manifold learning
+# -------------------------------------------
+#
+# Manifold learning is an approach to non-linear dimensionality reduction.
+# Algorithms for this task are based on the idea that the dimensionality of
+# many data sets is only artificially high.
+#
+# Read more in the :ref:`User Guide <manifold>`.
+
+n_neighbors = 12  # neighborhood which is used to recover the locally linear structure
+n_components = 2  # number of coordinates for the manifold
+
+# %%
+# Locally Linear Embeddings
+# ^^^^^^^^^^^^^^^^^^^^^^^^^
+#
+# Locally linear embedding (LLE) can be thought of as a series of local
+# Principal Component Analyses which are globally compared to find the
+# best non-linear embedding.
+# Read more in the :ref:`User Guide <locally_linear_embedding>`.
+
+params = {
+    "n_neighbors": n_neighbors,
+    "n_components": n_components,
+    "eigen_solver": "auto",
+    "random_state": rng,
+}
+
+lle_standard = manifold.LocallyLinearEmbedding(method="standard", **params)
+S_standard = lle_standard.fit_transform(S_points)
+
+lle_ltsa = manifold.LocallyLinearEmbedding(method="ltsa", **params)
+S_ltsa = lle_ltsa.fit_transform(S_points)
+
+lle_hessian = manifold.LocallyLinearEmbedding(method="hessian", **params)
+S_hessian = lle_hessian.fit_transform(S_points)
+
+lle_mod = manifold.LocallyLinearEmbedding(method="modified", modified_tol=0.8, **params)
+S_mod = lle_mod.fit_transform(S_points)
+
+# %%
+fig, axs = plt.subplots(
+    nrows=2, ncols=2, figsize=(7, 7), facecolor="white", constrained_layout=True
+)
+fig.suptitle("Locally Linear Embeddings", size=16)
+
+lle_methods = [
+    ("Standard locally linear embedding", S_standard),
+    ("Local tangent space alignment", S_ltsa),
+    ("Hessian eigenmap", S_hessian),
+    ("Modified locally linear embedding", S_mod),
+]
+for ax, method in zip(axs.flat, lle_methods):
+    name, points = method
+    add_2d_scatter(ax, points, S_color, name)

 plt.show()
+
+# %%
+# Isomap Embedding
+# ^^^^^^^^^^^^^^^^
+#
+# Non-linear dimensionality reduction through Isometric Mapping.
+# Isomap seeks a lower-dimensional embedding which maintains geodesic
+# distances between all points. Read more in the :ref:`User Guide <isomap>`.
+
+isomap = manifold.Isomap(n_neighbors=n_neighbors, n_components=n_components, p=1)
+S_isomap = isomap.fit_transform(S_points)
+
+plot_2d(S_isomap, S_color, "Isomap Embedding")
+
+# %%
+# Multidimensional scaling
+# ^^^^^^^^^^^^^^^^^^^^^^^^
+#
+# Multidimensional scaling (MDS) seeks a low-dimensional representation
+# of the data in which the distances respect well the distances in the
+# original high-dimensional space.
+# Read more in the :ref:`User Guide <multidimensional_scaling>`.
+
+md_scaling = manifold.MDS(
+    n_components=n_components, max_iter=50, n_init=4, random_state=rng
+)
+S_scaling = md_scaling.fit_transform(S_points)
+
+plot_2d(S_scaling, S_color, "Multidimensional scaling")
+
+# %%
+# Spectral embedding for non-linear dimensionality reduction
+# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+#
+# This implementation uses Laplacian Eigenmaps, which finds a low dimensional
+# representation of the data using a spectral decomposition of the graph Laplacian.
+# Read more in the :ref:`User Guide <spectral_embedding>`.
+
+spectral = manifold.SpectralEmbedding(
+    n_components=n_components, n_neighbors=n_neighbors
+)
+S_spectral = spectral.fit_transform(S_points)
+
+plot_2d(S_spectral, S_color, "Spectral Embedding")
+
+# %%
+# T-distributed Stochastic Neighbor Embedding
+# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+#
+# It converts similarities between data points to joint probabilities and
+# tries to minimize the Kullback-Leibler divergence between the joint probabilities
+# of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost
+# function that is not convex, i.e. with different initializations we can get
+# different results. Read more in the :ref:`User Guide <t_sne>`.
+
+t_sne = manifold.TSNE(
+    n_components=n_components,
+    learning_rate="auto",
+    perplexity=30,
+    n_iter=250,
+    init="random",
+    random_state=rng,
+)
+S_t_sne = t_sne.fit_transform(S_points)
+
+plot_2d(S_t_sne, S_color, "T-distributed Stochastic  \n Neighbor Embedding")
('examples/manifold', 'plot_lle_digits.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,244 +3,182 @@
 Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...
 =============================================================================

-An illustration of various embeddings on the digits dataset.
+We illustrate various embedding techniques on the digits dataset.

-The RandomTreesEmbedding, from the :mod:`sklearn.ensemble` module, is not
-technically a manifold embedding method, as it learn a high-dimensional
-representation on which we apply a dimensionality reduction method.
-However, it is often useful to cast a dataset into a representation in
-which the classes are linearly-separable.
-
-t-SNE will be initialized with the embedding that is generated by PCA in
-this example, which is not the default setting. It ensures global stability
-of the embedding, i.e., the embedding does not depend on random
-initialization.
-
-Linear Discriminant Analysis, from the :mod:`sklearn.discriminant_analysis`
-module, and Neighborhood Components Analysis, from the :mod:`sklearn.neighbors`
-module, are supervised dimensionality reduction method, i.e. they make use of
-the provided labels, contrary to other methods.
 """

 # Authors: Fabian Pedregosa <fabian.pedregosa@inria.fr>
 #          Olivier Grisel <olivier.grisel@ensta.org>
 #          Mathieu Blondel <mathieu@mblondel.org>
 #          Gael Varoquaux
+#          Guillaume Lemaitre <g.lemaitre58@gmail.com>
 # License: BSD 3 clause (C) INRIA 2011

-print(__doc__)
-from time import time

-import numpy as np
-import matplotlib.pyplot as plt
-from matplotlib import offsetbox
-from sklearn import (manifold, datasets, decomposition, ensemble,
-                     discriminant_analysis, random_projection, neighbors)
+# %%
+# Load digits dataset
+# -------------------
+# We will load the digits dataset and only use six first of the ten available classes.
+from sklearn.datasets import load_digits

-digits = datasets.load_digits(n_class=6)
-X = digits.data
-y = digits.target
+digits = load_digits(n_class=6)
+X, y = digits.data, digits.target
 n_samples, n_features = X.shape
 n_neighbors = 30

+# %%
+# We can plot the first hundred digits from this data set.
+import matplotlib.pyplot as plt

-# ----------------------------------------------------------------------
-# Scale and visualize the embedding vectors
-def plot_embedding(X, title=None):
-    x_min, x_max = np.min(X, 0), np.max(X, 0)
-    X = (X - x_min) / (x_max - x_min)
+fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(6, 6))
+for idx, ax in enumerate(axs.ravel()):
+    ax.imshow(X[idx].reshape((8, 8)), cmap=plt.cm.binary)
+    ax.axis("off")
+_ = fig.suptitle("A selection from the 64-dimensional digits dataset", fontsize=16)

-    plt.figure()
-    ax = plt.subplot(111)
-    for i in range(X.shape[0]):
-        plt.text(X[i, 0], X[i, 1], str(y[i]),
-                 color=plt.cm.Set1(y[i] / 10.),
-                 fontdict={'weight': 'bold', 'size': 9})
-
-    if hasattr(offsetbox, 'AnnotationBbox'):
-        # only print thumbnails with matplotlib > 1.0
-        shown_images = np.array([[1., 1.]])  # just something big
-        for i in range(X.shape[0]):
-            dist = np.sum((X[i] - shown_images) ** 2, 1)
-            if np.min(dist) < 4e-3:
-                # don't show points that are too close
-                continue
-            shown_images = np.r_[shown_images, [X[i]]]
-            imagebox = offsetbox.AnnotationBbox(
-                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),
-                X[i])
-            ax.add_artist(imagebox)
-    plt.xticks([]), plt.yticks([])
-    if title is not None:
-        plt.title(title)
+# %%
+# Helper function to plot embedding
+# ---------------------------------
+# Below, we will use different techniques to embed the digits dataset. We will plot
+# the projection of the original data onto each embedding. It will allow us to
+# check whether or digits are grouped together in the embedding space, or
+# scattered across it.
+import numpy as np
+from matplotlib import offsetbox
+from sklearn.preprocessing import MinMaxScaler


-# ----------------------------------------------------------------------
-# Plot images of the digits
-n_img_per_row = 20
-img = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))
-for i in range(n_img_per_row):
-    ix = 10 * i + 1
-    for j in range(n_img_per_row):
-        iy = 10 * j + 1
-        img[ix:ix + 8, iy:iy + 8] = X[i * n_img_per_row + j].reshape((8, 8))
+def plot_embedding(X, title):
+    _, ax = plt.subplots()
+    X = MinMaxScaler().fit_transform(X)

-plt.imshow(img, cmap=plt.cm.binary)
-plt.xticks([])
-plt.yticks([])
-plt.title('A selection from the 64-dimensional digits dataset')
+    for digit in digits.target_names:
+        ax.scatter(
+            *X[y == digit].T,
+            marker=f"${digit}$",
+            s=60,
+            color=plt.cm.Dark2(digit),
+            alpha=0.425,
+            zorder=2,
+        )
+    shown_images = np.array([[1.0, 1.0]])  # just something big
+    for i in range(X.shape[0]):
+        # plot every digit on the embedding
+        # show an annotation box for a group of digits
+        dist = np.sum((X[i] - shown_images) ** 2, 1)
+        if np.min(dist) < 4e-3:
+            # don't show points that are too close
+            continue
+        shown_images = np.concatenate([shown_images, [X[i]]], axis=0)
+        imagebox = offsetbox.AnnotationBbox(
+            offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r), X[i]
+        )
+        imagebox.set(zorder=1)
+        ax.add_artist(imagebox)
+
+    ax.set_title(title)
+    ax.axis("off")


-# ----------------------------------------------------------------------
-# Random 2D projection using a random unitary matrix
-print("Computing random projection")
-rp = random_projection.SparseRandomProjection(n_components=2, random_state=42)
-X_projected = rp.fit_transform(X)
-plot_embedding(X_projected, "Random Projection of the digits")
+# %%
+# Embedding techniques comparison
+# -------------------------------
+#
+# Below, we compare different techniques. However, there are a couple of things
+# to note:
+#
+# * the :class:`~sklearn.ensemble.RandomTreesEmbedding` is not
+#   technically a manifold embedding method, as it learn a high-dimensional
+#   representation on which we apply a dimensionality reduction method.
+#   However, it is often useful to cast a dataset into a representation in
+#   which the classes are linearly-separable.
+# * the :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis` and
+#   the :class:`~sklearn.neighbors.NeighborhoodComponentsAnalysis`, are supervised
+#   dimensionality reduction method, i.e. they make use of the provided labels,
+#   contrary to other methods.
+# * the :class:`~sklearn.manifold.TSNE` is initialized with the embedding that is
+#   generated by PCA in this example. It ensures global stability  of the embedding,
+#   i.e., the embedding does not depend on random initialization.
+from sklearn.decomposition import TruncatedSVD
+from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
+from sklearn.ensemble import RandomTreesEmbedding
+from sklearn.manifold import (
+    Isomap,
+    LocallyLinearEmbedding,
+    MDS,
+    SpectralEmbedding,
+    TSNE,
+)
+from sklearn.neighbors import NeighborhoodComponentsAnalysis
+from sklearn.pipeline import make_pipeline
+from sklearn.random_projection import SparseRandomProjection

+embeddings = {
+    "Random projection embedding": SparseRandomProjection(
+        n_components=2, random_state=42
+    ),
+    "Truncated SVD embedding": TruncatedSVD(n_components=2),
+    "Linear Discriminant Analysis embedding": LinearDiscriminantAnalysis(
+        n_components=2
+    ),
+    "Isomap embedding": Isomap(n_neighbors=n_neighbors, n_components=2),
+    "Standard LLE embedding": LocallyLinearEmbedding(
+        n_neighbors=n_neighbors, n_components=2, method="standard"
+    ),
+    "Modified LLE embedding": LocallyLinearEmbedding(
+        n_neighbors=n_neighbors, n_components=2, method="modified"
+    ),
+    "Hessian LLE embedding": LocallyLinearEmbedding(
+        n_neighbors=n_neighbors, n_components=2, method="hessian"
+    ),
+    "LTSA LLE embedding": LocallyLinearEmbedding(
+        n_neighbors=n_neighbors, n_components=2, method="ltsa"
+    ),
+    "MDS embedding": MDS(n_components=2, n_init=1, max_iter=120, n_jobs=2),
+    "Random Trees embedding": make_pipeline(
+        RandomTreesEmbedding(n_estimators=200, max_depth=5, random_state=0),
+        TruncatedSVD(n_components=2),
+    ),
+    "Spectral embedding": SpectralEmbedding(
+        n_components=2, random_state=0, eigen_solver="arpack"
+    ),
+    "t-SNE embeedding": TSNE(
+        n_components=2,
+        init="pca",
+        learning_rate="auto",
+        n_iter=500,
+        n_iter_without_progress=150,
+        n_jobs=2,
+        random_state=0,
+    ),
+    "NCA embedding": NeighborhoodComponentsAnalysis(
+        n_components=2, init="pca", random_state=0
+    ),
+}

-#----------------------------------------------------------------------
-# Projection on to the first 2 principal components
+# %%
+# Once we declared all the methodes of interest, we can run and perform the projection
+# of the original data. We will store the projected data as well as the computational
+# time needed to perform each projection.
+from time import time

-print("Computing PCA projection")
-t0 = time()
-X_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(X)
-plot_embedding(X_pca,
-               "Principal Components projection of the digits (time %.2fs)" %
-               (time() - t0))
+projections, timing = {}, {}
+for name, transformer in embeddings.items():
+    if name.startswith("Linear Discriminant Analysis"):
+        data = X.copy()
+        data.flat[:: X.shape[1] + 1] += 0.01  # Make X invertible
+    else:
+        data = X

-# ----------------------------------------------------------------------
-# Projection on to the first 2 linear discriminant components
+    print(f"Computing {name}...")
+    start_time = time()
+    projections[name] = transformer.fit_transform(data, y)
+    timing[name] = time() - start_time

-print("Computing Linear Discriminant Analysis projection")
-X2 = X.copy()
-X2.flat[::X.shape[1] + 1] += 0.01  # Make X invertible
-t0 = time()
-X_lda = discriminant_analysis.LinearDiscriminantAnalysis(n_components=2).fit_transform(X2, y)
-plot_embedding(X_lda,
-               "Linear Discriminant projection of the digits (time %.2fs)" %
-               (time() - t0))
-
-
-# ----------------------------------------------------------------------
-# Isomap projection of the digits dataset
-print("Computing Isomap projection")
-t0 = time()
-X_iso = manifold.Isomap(n_neighbors, n_components=2).fit_transform(X)
-print("Done.")
-plot_embedding(X_iso,
-               "Isomap projection of the digits (time %.2fs)" %
-               (time() - t0))
-
-
-# ----------------------------------------------------------------------
-# Locally linear embedding of the digits dataset
-print("Computing LLE embedding")
-clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,
-                                      method='standard')
-t0 = time()
-X_lle = clf.fit_transform(X)
-print("Done. Reconstruction error: %g" % clf.reconstruction_error_)
-plot_embedding(X_lle,
-               "Locally Linear Embedding of the digits (time %.2fs)" %
-               (time() - t0))
-
-
-# ----------------------------------------------------------------------
-# Modified Locally linear embedding of the digits dataset
-print("Computing modified LLE embedding")
-clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,
-                                      method='modified')
-t0 = time()
-X_mlle = clf.fit_transform(X)
-print("Done. Reconstruction error: %g" % clf.reconstruction_error_)
-plot_embedding(X_mlle,
-               "Modified Locally Linear Embedding of the digits (time %.2fs)" %
-               (time() - t0))
-
-
-# ----------------------------------------------------------------------
-# HLLE embedding of the digits dataset
-print("Computing Hessian LLE embedding")
-clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,
-                                      method='hessian')
-t0 = time()
-X_hlle = clf.fit_transform(X)
-print("Done. Reconstruction error: %g" % clf.reconstruction_error_)
-plot_embedding(X_hlle,
-               "Hessian Locally Linear Embedding of the digits (time %.2fs)" %
-               (time() - t0))
-
-
-# ----------------------------------------------------------------------
-# LTSA embedding of the digits dataset
-print("Computing LTSA embedding")
-clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,
-                                      method='ltsa')
-t0 = time()
-X_ltsa = clf.fit_transform(X)
-print("Done. Reconstruction error: %g" % clf.reconstruction_error_)
-plot_embedding(X_ltsa,
-               "Local Tangent Space Alignment of the digits (time %.2fs)" %
-               (time() - t0))
-
-# ----------------------------------------------------------------------
-# MDS  embedding of the digits dataset
-print("Computing MDS embedding")
-clf = manifold.MDS(n_components=2, n_init=1, max_iter=100)
-t0 = time()
-X_mds = clf.fit_transform(X)
-print("Done. Stress: %f" % clf.stress_)
-plot_embedding(X_mds,
-               "MDS embedding of the digits (time %.2fs)" %
-               (time() - t0))
-
-# ----------------------------------------------------------------------
-# Random Trees embedding of the digits dataset
-print("Computing Totally Random Trees embedding")
-hasher = ensemble.RandomTreesEmbedding(n_estimators=200, random_state=0,
-                                       max_depth=5)
-t0 = time()
-X_transformed = hasher.fit_transform(X)
-pca = decomposition.TruncatedSVD(n_components=2)
-X_reduced = pca.fit_transform(X_transformed)
-
-plot_embedding(X_reduced,
-               "Random forest embedding of the digits (time %.2fs)" %
-               (time() - t0))
-
-# ----------------------------------------------------------------------
-# Spectral embedding of the digits dataset
-print("Computing Spectral embedding")
-embedder = manifold.SpectralEmbedding(n_components=2, random_state=0,
-                                      eigen_solver="arpack")
-t0 = time()
-X_se = embedder.fit_transform(X)
-
-plot_embedding(X_se,
-               "Spectral embedding of the digits (time %.2fs)" %
-               (time() - t0))
-
-# ----------------------------------------------------------------------
-# t-SNE embedding of the digits dataset
-print("Computing t-SNE embedding")
-tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)
-t0 = time()
-X_tsne = tsne.fit_transform(X)
-
-plot_embedding(X_tsne,
-               "t-SNE embedding of the digits (time %.2fs)" %
-               (time() - t0))
-
-# ----------------------------------------------------------------------
-# NCA projection of the digits dataset
-print("Computing NCA projection")
-nca = neighbors.NeighborhoodComponentsAnalysis(n_components=2, random_state=0)
-t0 = time()
-X_nca = nca.fit_transform(X, y)
-
-plot_embedding(X_nca,
-               "NCA embedding of the digits (time %.2fs)" %
-               (time() - t0))
+# %%
+# Finally, we can plot the resulting projection given by each method.
+for name in timing:
+    title = f"{name} (time {timing[name]:.3f}s)"
+    plot_embedding(projections[name], title)

 plt.show()
('examples/manifold', 'plot_manifold_sphere.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =============================================
 Manifold Learning methods on a severed sphere
@@ -25,25 +23,22 @@
 the low-dimensional space. Here the manifold problem matches fairly
 that of representing a flat map of the Earth, as with
 `map projection <https://en.wikipedia.org/wiki/Map_projection>`_
+
 """

 # Author: Jaques Grobler <jaques.grobler@inria.fr>
 # License: BSD 3 clause

-print(__doc__)
-
 from time import time
-
 import numpy as np
 import matplotlib.pyplot as plt
-from mpl_toolkits.mplot3d import Axes3D
 from matplotlib.ticker import NullFormatter
-
 from sklearn import manifold
 from sklearn.utils import check_random_state

-# Next line to silence pyflakes.
-Axes3D
+# Unused but required import for doing 3d projections with matplotlib < 3.2
+import mpl_toolkits.mplot3d  # noqa: F401
+import warnings

 # Variables for manifold learning.
 n_neighbors = 10
@@ -55,32 +50,39 @@
 t = random_state.rand(n_samples) * np.pi

 # Sever the poles from the sphere.
-indices = ((t < (np.pi - (np.pi / 8))) & (t > ((np.pi / 8))))
+indices = (t < (np.pi - (np.pi / 8))) & (t > ((np.pi / 8)))
 colors = p[indices]
-x, y, z = np.sin(t[indices]) * np.cos(p[indices]), \
-    np.sin(t[indices]) * np.sin(p[indices]), \
-    np.cos(t[indices])
+x, y, z = (
+    np.sin(t[indices]) * np.cos(p[indices]),
+    np.sin(t[indices]) * np.sin(p[indices]),
+    np.cos(t[indices]),
+)

 # Plot our dataset.
 fig = plt.figure(figsize=(15, 8))
-plt.suptitle("Manifold Learning with %i points, %i neighbors"
-             % (1000, n_neighbors), fontsize=14)
+plt.suptitle(
+    "Manifold Learning with %i points, %i neighbors" % (1000, n_neighbors), fontsize=14
+)

-ax = fig.add_subplot(251, projection='3d')
+ax = fig.add_subplot(251, projection="3d")
 ax.scatter(x, y, z, c=p[indices], cmap=plt.cm.rainbow)
 ax.view_init(40, -10)

 sphere_data = np.array([x, y, z]).T

 # Perform Locally Linear Embedding Manifold learning
-methods = ['standard', 'ltsa', 'hessian', 'modified']
-labels = ['LLE', 'LTSA', 'Hessian LLE', 'Modified LLE']
+methods = ["standard", "ltsa", "hessian", "modified"]
+labels = ["LLE", "LTSA", "Hessian LLE", "Modified LLE"]

 for i, method in enumerate(methods):
     t0 = time()
-    trans_data = manifold\
-        .LocallyLinearEmbedding(n_neighbors, 2,
-                                method=method).fit_transform(sphere_data).T
+    trans_data = (
+        manifold.LocallyLinearEmbedding(
+            n_neighbors=n_neighbors, n_components=2, method=method
+        )
+        .fit_transform(sphere_data)
+        .T
+    )
     t1 = time()
     print("%s: %.2g sec" % (methods[i], t1 - t0))

@@ -89,21 +91,24 @@
     plt.title("%s (%.2g sec)" % (labels[i], t1 - t0))
     ax.xaxis.set_major_formatter(NullFormatter())
     ax.yaxis.set_major_formatter(NullFormatter())
-    plt.axis('tight')
+    plt.axis("tight")

 # Perform Isomap Manifold learning.
 t0 = time()
-trans_data = manifold.Isomap(n_neighbors, n_components=2)\
-    .fit_transform(sphere_data).T
+trans_data = (
+    manifold.Isomap(n_neighbors=n_neighbors, n_components=2)
+    .fit_transform(sphere_data)
+    .T
+)
 t1 = time()
-print("%s: %.2g sec" % ('ISO', t1 - t0))
+print("%s: %.2g sec" % ("ISO", t1 - t0))

 ax = fig.add_subplot(257)
 plt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)
-plt.title("%s (%.2g sec)" % ('Isomap', t1 - t0))
+plt.title("%s (%.2g sec)" % ("Isomap", t1 - t0))
 ax.xaxis.set_major_formatter(NullFormatter())
 ax.yaxis.set_major_formatter(NullFormatter())
-plt.axis('tight')
+plt.axis("tight")

 # Perform Multi-dimensional scaling.
 t0 = time()
@@ -117,12 +122,11 @@
 plt.title("MDS (%.2g sec)" % (t1 - t0))
 ax.xaxis.set_major_formatter(NullFormatter())
 ax.yaxis.set_major_formatter(NullFormatter())
-plt.axis('tight')
+plt.axis("tight")

 # Perform Spectral Embedding.
 t0 = time()
-se = manifold.SpectralEmbedding(n_components=2,
-                                n_neighbors=n_neighbors)
+se = manifold.SpectralEmbedding(n_components=2, n_neighbors=n_neighbors)
 trans_data = se.fit_transform(sphere_data).T
 t1 = time()
 print("Spectral Embedding: %.2g sec" % (t1 - t0))
@@ -132,13 +136,20 @@
 plt.title("Spectral Embedding (%.2g sec)" % (t1 - t0))
 ax.xaxis.set_major_formatter(NullFormatter())
 ax.yaxis.set_major_formatter(NullFormatter())
-plt.axis('tight')
+plt.axis("tight")

 # Perform t-distributed stochastic neighbor embedding.
-t0 = time()
-tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)
-trans_data = tsne.fit_transform(sphere_data).T
-t1 = time()
+# TODO(1.2) Remove warning handling.
+with warnings.catch_warnings():
+    warnings.filterwarnings(
+        "ignore", message="The PCA initialization", category=FutureWarning
+    )
+    t0 = time()
+    tsne = manifold.TSNE(
+        n_components=2, init="pca", random_state=0, learning_rate="auto"
+    )
+    trans_data = tsne.fit_transform(sphere_data).T
+    t1 = time()
 print("t-SNE: %.2g sec" % (t1 - t0))

 ax = fig.add_subplot(2, 5, 10)
@@ -146,6 +157,6 @@
 plt.title("t-SNE (%.2g sec)" % (t1 - t0))
 ax.xaxis.set_major_formatter(NullFormatter())
 ax.yaxis.set_major_formatter(NullFormatter())
-plt.axis('tight')
+plt.axis("tight")

 plt.show()
('examples/manifold', 'plot_swissroll.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,46 +1,119 @@
 """
 ===================================
-Swiss Roll reduction with LLE
+Swiss Roll And Swiss-Hole Reduction
 ===================================
-
-An illustration of Swiss Roll reduction
-with locally linear embedding
+This notebook seeks to compare two popular non-linear dimensionality
+techniques, T-distributed Stochastic Neighbor Embedding (t-SNE) and
+Locally Linear Embedding (LLE), on the classic Swiss Roll dataset.
+Then, we will explore how they both deal with the addition of a hole
+in the data.
 """
-
-# Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>
-# License: BSD 3 clause (C) INRIA 2011
-
-print(__doc__)
+# %%
+# Swiss Roll
+# ---------------------------------------------------
+#
+# We start by generating the Swiss Roll dataset.

 import matplotlib.pyplot as plt
+from sklearn import manifold, datasets

-# This import is needed to modify the way figure behaves
-from mpl_toolkits.mplot3d import Axes3D
-Axes3D

-#----------------------------------------------------------------------
-# Locally linear embedding of the swiss roll
+sr_points, sr_color = datasets.make_swiss_roll(n_samples=1500, random_state=0)

-from sklearn import manifold, datasets
-X, color = datasets.samples_generator.make_swiss_roll(n_samples=1500)
+# %%
+# Now, let's take a look at our data:

-print("Computing LLE embedding")
-X_r, err = manifold.locally_linear_embedding(X, n_neighbors=12,
-                                             n_components=2)
-print("Done. Reconstruction error: %g" % err)
+fig = plt.figure(figsize=(8, 6))
+ax = fig.add_subplot(111, projection="3d")
+fig.add_axes(ax)
+ax.scatter(
+    sr_points[:, 0], sr_points[:, 1], sr_points[:, 2], c=sr_color, s=50, alpha=0.8
+)
+ax.set_title("Swiss Roll in Ambient Space")
+ax.view_init(azim=-66, elev=12)
+_ = ax.text2D(0.8, 0.05, s="n_samples=1500", transform=ax.transAxes)

-#----------------------------------------------------------------------
-# Plot result
+# %%
+# Computing the LLE and t-SNE embeddings, we find that LLE seems to unroll the
+# Swiss Roll pretty effectively. t-SNE on the other hand, is able
+# to preserve the general structure of the data, but, poorly represents the
+# continuous nature of our original data. Instead, it seems to unnecessarily
+# clump sections of points together.

-fig = plt.figure()
+sr_lle, sr_err = manifold.locally_linear_embedding(
+    sr_points, n_neighbors=12, n_components=2
+)

-ax = fig.add_subplot(211, projection='3d')
-ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
+sr_tsne = manifold.TSNE(
+    n_components=2, learning_rate="auto", perplexity=40, init="pca", random_state=0
+).fit_transform(sr_points)

-ax.set_title("Original data")
-ax = fig.add_subplot(212)
-ax.scatter(X_r[:, 0], X_r[:, 1], c=color, cmap=plt.cm.Spectral)
-plt.axis('tight')
-plt.xticks([]), plt.yticks([])
-plt.title('Projected data')
-plt.show()
+fig, axs = plt.subplots(figsize=(8, 8), nrows=2)
+axs[0].scatter(sr_lle[:, 0], sr_lle[:, 1], c=sr_color)
+axs[0].set_title("LLE Embedding of Swiss Roll")
+axs[1].scatter(sr_tsne[:, 0], sr_tsne[:, 1], c=sr_color)
+_ = axs[1].set_title("t-SNE Embedding of Swiss Roll")
+
+# %%
+# .. note::
+#
+#     LLE seems to be stretching the points from the center (purple)
+#     of the swiss roll. However, we observe that this is simply a byproduct
+#     of how the data was generated. There is a higher density of points near the
+#     center of the roll, which ultimately affects how LLE reconstructs the
+#     data in a lower dimension.
+
+# %%
+# Swiss-Hole
+# ---------------------------------------------------
+#
+# Now let's take a look at how both algorithms deal with us adding a hole to
+# the data. First, we generate the Swiss-Hole dataset and plot it:
+
+sh_points, sh_color = datasets.make_swiss_roll(
+    n_samples=1500, hole=True, random_state=0
+)
+
+fig = plt.figure(figsize=(8, 6))
+ax = fig.add_subplot(111, projection="3d")
+fig.add_axes(ax)
+ax.scatter(
+    sh_points[:, 0], sh_points[:, 1], sh_points[:, 2], c=sh_color, s=50, alpha=0.8
+)
+ax.set_title("Swiss-Hole in Ambient Space")
+ax.view_init(azim=-66, elev=12)
+_ = ax.text2D(0.8, 0.05, s="n_samples=1500", transform=ax.transAxes)
+
+# %%
+# Computing the LLE and t-SNE embeddings, we obtain similar results to the
+# Swiss Roll. LLE very capably unrolls the data and even preserves
+# the hole. t-SNE, again seems to clump sections of points together, but, we
+# note that it preserves the general topology of the original data.
+
+
+sh_lle, sh_err = manifold.locally_linear_embedding(
+    sh_points, n_neighbors=12, n_components=2
+)
+
+sh_tsne = manifold.TSNE(
+    n_components=2, learning_rate="auto", perplexity=40, init="random", random_state=0
+).fit_transform(sh_points)
+
+fig, axs = plt.subplots(figsize=(8, 8), nrows=2)
+axs[0].scatter(sh_lle[:, 0], sh_lle[:, 1], c=sh_color)
+axs[0].set_title("LLE Embedding of Swiss-Hole")
+axs[1].scatter(sh_tsne[:, 0], sh_tsne[:, 1], c=sh_color)
+_ = axs[1].set_title("t-SNE Embedding of Swiss-Hole")
+
+# %%
+#
+# Concluding remarks
+# ------------------
+#
+# We note that t-SNE benefits from testing more combinations of parameters.
+# Better results could probably have been obtained by better tuning these
+# parameters.
+#
+# We observe that, as seen in the "Manifold learning on
+# handwritten digits" example, t-SNE generally performs better than LLE
+# on real world data.
('examples/manifold', 'plot_t_sne_perplexity.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,7 +6,7 @@
 An illustration of t-SNE on the two concentric circles and the S-curve
 datasets for different perplexity values.

-We observe a tendency towards clearer shapes as the preplexity value increases.
+We observe a tendency towards clearer shapes as the perplexity value increases.

 The size, the distance and the shape of clusters may vary upon initialization,
 perplexity values and does not always convey a meaning.
@@ -21,12 +21,11 @@
 https://distill.pub/2016/misread-tsne/ provides a good discussion of the
 effects of various parameters, as well as interactive plots to explore
 those effects.
+
 """

 # Author: Narine Kokhlikyan <narine@slice.com>
 # License: BSD
-
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -35,12 +34,14 @@
 from sklearn import manifold, datasets
 from time import time

-n_samples = 300
+n_samples = 150
 n_components = 2
 (fig, subplots) = plt.subplots(3, 5, figsize=(15, 8))
 perplexities = [5, 30, 50, 100]

-X, y = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)
+X, y = datasets.make_circles(
+    n_samples=n_samples, factor=0.5, noise=0.05, random_state=0
+)

 red = y == 0
 green = y == 1
@@ -50,14 +51,20 @@
 ax.scatter(X[green, 0], X[green, 1], c="g")
 ax.xaxis.set_major_formatter(NullFormatter())
 ax.yaxis.set_major_formatter(NullFormatter())
-plt.axis('tight')
+plt.axis("tight")

 for i, perplexity in enumerate(perplexities):
     ax = subplots[0][i + 1]

     t0 = time()
-    tsne = manifold.TSNE(n_components=n_components, init='random',
-                         random_state=0, perplexity=perplexity)
+    tsne = manifold.TSNE(
+        n_components=n_components,
+        init="random",
+        random_state=0,
+        perplexity=perplexity,
+        learning_rate="auto",
+        n_iter=300,
+    )
     Y = tsne.fit_transform(X)
     t1 = time()
     print("circles, perplexity=%d in %.2g sec" % (perplexity, t1 - t0))
@@ -66,10 +73,10 @@
     ax.scatter(Y[green, 0], Y[green, 1], c="g")
     ax.xaxis.set_major_formatter(NullFormatter())
     ax.yaxis.set_major_formatter(NullFormatter())
-    ax.axis('tight')
+    ax.axis("tight")

 # Another example using s-curve
-X, color = datasets.samples_generator.make_s_curve(n_samples, random_state=0)
+X, color = datasets.make_s_curve(n_samples, random_state=0)

 ax = subplots[1][0]
 ax.scatter(X[:, 0], X[:, 2], c=color)
@@ -80,8 +87,14 @@
     ax = subplots[1][i + 1]

     t0 = time()
-    tsne = manifold.TSNE(n_components=n_components, init='random',
-                         random_state=0, perplexity=perplexity)
+    tsne = manifold.TSNE(
+        n_components=n_components,
+        init="random",
+        random_state=0,
+        perplexity=perplexity,
+        learning_rate="auto",
+        n_iter=300,
+    )
     Y = tsne.fit_transform(X)
     t1 = time()
     print("S-curve, perplexity=%d in %.2g sec" % (perplexity, t1 - t0))
@@ -90,16 +103,18 @@
     ax.scatter(Y[:, 0], Y[:, 1], c=color)
     ax.xaxis.set_major_formatter(NullFormatter())
     ax.yaxis.set_major_formatter(NullFormatter())
-    ax.axis('tight')
+    ax.axis("tight")


 # Another example using a 2D uniform grid
 x = np.linspace(0, 1, int(np.sqrt(n_samples)))
 xx, yy = np.meshgrid(x, x)
-X = np.hstack([
-    xx.ravel().reshape(-1, 1),
-    yy.ravel().reshape(-1, 1),
-])
+X = np.hstack(
+    [
+        xx.ravel().reshape(-1, 1),
+        yy.ravel().reshape(-1, 1),
+    ]
+)
 color = xx.ravel()
 ax = subplots[2][0]
 ax.scatter(X[:, 0], X[:, 1], c=color)
@@ -110,8 +125,14 @@
     ax = subplots[2][i + 1]

     t0 = time()
-    tsne = manifold.TSNE(n_components=n_components, init='random',
-                         random_state=0, perplexity=perplexity)
+    tsne = manifold.TSNE(
+        n_components=n_components,
+        init="random",
+        random_state=0,
+        perplexity=perplexity,
+        learning_rate="auto",
+        n_iter=400,
+    )
     Y = tsne.fit_transform(X)
     t1 = time()
     print("uniform grid, perplexity=%d in %.2g sec" % (perplexity, t1 - t0))
@@ -120,7 +141,7 @@
     ax.scatter(Y[:, 0], Y[:, 1], c=color)
     ax.xaxis.set_major_formatter(NullFormatter())
     ax.yaxis.set_major_formatter(NullFormatter())
-    ax.axis('tight')
+    ax.axis("tight")


 plt.show()
('examples/exercises', 'plot_cv_diabetes.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,70 +7,81 @@

 This exercise is used in the :ref:`cv_estimators_tut` part of the
 :ref:`model_selection_tut` section of the :ref:`stat_learn_tut_index`.
+
 """

-print(__doc__)
-
+# %%
+# Load dataset and apply GridSearchCV
+# -----------------------------------
+import matplotlib.pyplot as plt
 import numpy as np
-import matplotlib.pyplot as plt

 from sklearn import datasets
-from sklearn.linear_model import LassoCV
 from sklearn.linear_model import Lasso
-from sklearn.model_selection import KFold
 from sklearn.model_selection import GridSearchCV

-diabetes = datasets.load_diabetes()
-X = diabetes.data[:150]
-y = diabetes.target[:150]
+X, y = datasets.load_diabetes(return_X_y=True)
+X = X[:150]
+y = y[:150]

 lasso = Lasso(random_state=0, max_iter=10000)
 alphas = np.logspace(-4, -0.5, 30)

-tuned_parameters = [{'alpha': alphas}]
+tuned_parameters = [{"alpha": alphas}]
 n_folds = 5

 clf = GridSearchCV(lasso, tuned_parameters, cv=n_folds, refit=False)
 clf.fit(X, y)
-scores = clf.cv_results_['mean_test_score']
-scores_std = clf.cv_results_['std_test_score']
+scores = clf.cv_results_["mean_test_score"]
+scores_std = clf.cv_results_["std_test_score"]
+
+# %%
+# Plot error lines showing +/- std. errors of the scores
+# ------------------------------------------------------
+
 plt.figure().set_size_inches(8, 6)
 plt.semilogx(alphas, scores)

-# plot error lines showing +/- std. errors of the scores
 std_error = scores_std / np.sqrt(n_folds)

-plt.semilogx(alphas, scores + std_error, 'b--')
-plt.semilogx(alphas, scores - std_error, 'b--')
+plt.semilogx(alphas, scores + std_error, "b--")
+plt.semilogx(alphas, scores - std_error, "b--")

 # alpha=0.2 controls the translucency of the fill color
 plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)

-plt.ylabel('CV score +/- std error')
-plt.xlabel('alpha')
-plt.axhline(np.max(scores), linestyle='--', color='.5')
+plt.ylabel("CV score +/- std error")
+plt.xlabel("alpha")
+plt.axhline(np.max(scores), linestyle="--", color=".5")
 plt.xlim([alphas[0], alphas[-1]])

-# #############################################################################
+# %%
 # Bonus: how much can you trust the selection of alpha?
+# -----------------------------------------------------

 # To answer this question we use the LassoCV object that sets its alpha
 # parameter automatically from the data by internal cross-validation (i.e. it
 # performs cross-validation on the training data it receives).
 # We use external cross-validation to see how much the automatically obtained
 # alphas differ across different cross-validation folds.
-lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=0, max_iter=10000)
+
+from sklearn.linear_model import LassoCV
+from sklearn.model_selection import KFold
+
+lasso_cv = LassoCV(alphas=alphas, random_state=0, max_iter=10000)
 k_fold = KFold(3)

-print("Answer to the bonus question:",
-      "how much can you trust the selection of alpha?")
+print("Answer to the bonus question:", "how much can you trust the selection of alpha?")
 print()
 print("Alpha parameters maximising the generalization score on different")
 print("subsets of the data:")
 for k, (train, test) in enumerate(k_fold.split(X, y)):
     lasso_cv.fit(X[train], y[train])
-    print("[fold {0}] alpha: {1:.5f}, score: {2:.5f}".
-          format(k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])))
+    print(
+        "[fold {0}] alpha: {1:.5f}, score: {2:.5f}".format(
+            k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])
+        )
+    )
 print()
 print("Answer: Not very much since we obtained different alphas for different")
 print("subsets of the data and moreover, the scores for these alphas differ")
('examples/exercises', 'plot_digits_classification_exercise.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -9,26 +9,26 @@
 This exercise is used in the :ref:`clf_tut` part of the
 :ref:`supervised_learning_tut` section of the
 :ref:`stat_learn_tut_index`.
+
 """
-print(__doc__)

 from sklearn import datasets, neighbors, linear_model

-digits = datasets.load_digits()
-X_digits = digits.data / digits.data.max()
-y_digits = digits.target
+X_digits, y_digits = datasets.load_digits(return_X_y=True)
+X_digits = X_digits / X_digits.max()

 n_samples = len(X_digits)

-X_train = X_digits[:int(.9 * n_samples)]
-y_train = y_digits[:int(.9 * n_samples)]
-X_test = X_digits[int(.9 * n_samples):]
-y_test = y_digits[int(.9 * n_samples):]
+X_train = X_digits[: int(0.9 * n_samples)]
+y_train = y_digits[: int(0.9 * n_samples)]
+X_test = X_digits[int(0.9 * n_samples) :]
+y_test = y_digits[int(0.9 * n_samples) :]

 knn = neighbors.KNeighborsClassifier()
-logistic = linear_model.LogisticRegression(solver='lbfgs', max_iter=1000,
-                                           multi_class='multinomial')
+logistic = linear_model.LogisticRegression(max_iter=1000)

-print('KNN score: %f' % knn.fit(X_train, y_train).score(X_test, y_test))
-print('LogisticRegression score: %f'
-      % logistic.fit(X_train, y_train).score(X_test, y_test))
+print("KNN score: %f" % knn.fit(X_train, y_train).score(X_test, y_test))
+print(
+    "LogisticRegression score: %f"
+    % logistic.fit(X_train, y_train).score(X_test, y_test)
+)
('examples/exercises', 'plot_cv_digits.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,38 +7,36 @@

 This exercise is used in the :ref:`cv_generators_tut` part of the
 :ref:`model_selection_tut` section of the :ref:`stat_learn_tut_index`.
+
 """
-print(__doc__)
-

 import numpy as np
 from sklearn.model_selection import cross_val_score
 from sklearn import datasets, svm

-digits = datasets.load_digits()
-X = digits.data
-y = digits.target
+X, y = datasets.load_digits(return_X_y=True)

-svc = svm.SVC(kernel='linear')
+svc = svm.SVC(kernel="linear")
 C_s = np.logspace(-10, 0, 10)

 scores = list()
 scores_std = list()
 for C in C_s:
     svc.C = C
-    this_scores = cross_val_score(svc, X, y, cv=5, n_jobs=1)
+    this_scores = cross_val_score(svc, X, y, n_jobs=1)
     scores.append(np.mean(this_scores))
     scores_std.append(np.std(this_scores))

 # Do the plotting
 import matplotlib.pyplot as plt
+
 plt.figure()
 plt.semilogx(C_s, scores)
-plt.semilogx(C_s, np.array(scores) + np.array(scores_std), 'b--')
-plt.semilogx(C_s, np.array(scores) - np.array(scores_std), 'b--')
+plt.semilogx(C_s, np.array(scores) + np.array(scores_std), "b--")
+plt.semilogx(C_s, np.array(scores) - np.array(scores_std), "b--")
 locs, labels = plt.yticks()
 plt.yticks(locs, list(map(lambda x: "%g" % x, locs)))
-plt.ylabel('CV score')
-plt.xlabel('Parameter C')
+plt.ylabel("CV score")
+plt.xlabel("Parameter C")
 plt.ylim(0, 1.1)
 plt.show()
('examples/exercises', 'plot_iris_exercise.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,9 +7,8 @@

 This exercise is used in the :ref:`using_kernels_tut` part of the
 :ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.
+
 """
-print(__doc__)
-

 import numpy as np
 import matplotlib.pyplot as plt
@@ -27,28 +26,30 @@
 np.random.seed(0)
 order = np.random.permutation(n_sample)
 X = X[order]
-y = y[order].astype(np.float)
+y = y[order].astype(float)

-X_train = X[:int(.9 * n_sample)]
-y_train = y[:int(.9 * n_sample)]
-X_test = X[int(.9 * n_sample):]
-y_test = y[int(.9 * n_sample):]
+X_train = X[: int(0.9 * n_sample)]
+y_train = y[: int(0.9 * n_sample)]
+X_test = X[int(0.9 * n_sample) :]
+y_test = y[int(0.9 * n_sample) :]

 # fit the model
-for kernel in ('linear', 'rbf', 'poly'):
+for kernel in ("linear", "rbf", "poly"):
     clf = svm.SVC(kernel=kernel, gamma=10)
     clf.fit(X_train, y_train)

     plt.figure()
     plt.clf()
-    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,
-                edgecolor='k', s=20)
+    plt.scatter(
+        X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired, edgecolor="k", s=20
+    )

     # Circle out the test data
-    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none',
-                zorder=10, edgecolor='k')
+    plt.scatter(
+        X_test[:, 0], X_test[:, 1], s=80, facecolors="none", zorder=10, edgecolor="k"
+    )

-    plt.axis('tight')
+    plt.axis("tight")
     x_min = X[:, 0].min()
     x_max = X[:, 0].max()
     y_min = X[:, 1].min()
@@ -60,8 +61,14 @@
     # Put the result into a color plot
     Z = Z.reshape(XX.shape)
     plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)
-    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],
-                linestyles=['--', '-', '--'], levels=[-.5, 0, .5])
+    plt.contour(
+        XX,
+        YY,
+        Z,
+        colors=["k", "k", "k"],
+        linestyles=["--", "-", "--"],
+        levels=[-0.5, 0, 0.5],
+    )

     plt.title(kernel)
 plt.show()
('examples/applications', 'plot_face_recognition.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -10,43 +10,23 @@

 .. _LFW: http://vis-www.cs.umass.edu/lfw/

-Expected results for the top 5 most represented people in the dataset:
-
-================== ============ ======= ========== =======
-                   precision    recall  f1-score   support
-================== ============ ======= ========== =======
-     Ariel Sharon       0.67      0.92      0.77        13
-     Colin Powell       0.75      0.78      0.76        60
-  Donald Rumsfeld       0.78      0.67      0.72        27
-    George W Bush       0.86      0.86      0.86       146
-Gerhard Schroeder       0.76      0.76      0.76        25
-      Hugo Chavez       0.67      0.67      0.67        15
-       Tony Blair       0.81      0.69      0.75        36
-
-      avg / total       0.80      0.80      0.80       322
-================== ============ ======= ========== =======
-
 """
+# %%
 from time import time
-import logging
 import matplotlib.pyplot as plt

 from sklearn.model_selection import train_test_split
-from sklearn.model_selection import GridSearchCV
+from sklearn.model_selection import RandomizedSearchCV
 from sklearn.datasets import fetch_lfw_people
 from sklearn.metrics import classification_report
-from sklearn.metrics import confusion_matrix
+from sklearn.metrics import ConfusionMatrixDisplay
+from sklearn.preprocessing import StandardScaler
 from sklearn.decomposition import PCA
 from sklearn.svm import SVC
+from sklearn.utils.fixes import loguniform


-print(__doc__)
-
-# Display progress logs on stdout
-logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')
-
-
-# #############################################################################
+# %%
 # Download the data, if not already on disk and load it as numpy arrays

 lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
@@ -70,24 +50,28 @@
 print("n_classes: %d" % n_classes)


-# #############################################################################
-# Split into a training set and a test set using a stratified k fold
+# %%
+# Split into a training set and a test and keep 25% of the data for testing.

-# split into a training and testing set
 X_train, X_test, y_train, y_test = train_test_split(
-    X, y, test_size=0.25, random_state=42)
+    X, y, test_size=0.25, random_state=42
+)

+scaler = StandardScaler()
+X_train = scaler.fit_transform(X_train)
+X_test = scaler.transform(X_test)

-# #############################################################################
+# %%
 # Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled
 # dataset): unsupervised feature extraction / dimensionality reduction
+
 n_components = 150

-print("Extracting the top %d eigenfaces from %d faces"
-      % (n_components, X_train.shape[0]))
+print(
+    "Extracting the top %d eigenfaces from %d faces" % (n_components, X_train.shape[0])
+)
 t0 = time()
-pca = PCA(n_components=n_components, svd_solver='randomized',
-          whiten=True).fit(X_train)
+pca = PCA(n_components=n_components, svd_solver="randomized", whiten=True).fit(X_train)
 print("done in %0.3fs" % (time() - t0))

 eigenfaces = pca.components_.reshape((n_components, h, w))
@@ -99,22 +83,25 @@
 print("done in %0.3fs" % (time() - t0))


-# #############################################################################
+# %%
 # Train a SVM classification model

 print("Fitting the classifier to the training set")
 t0 = time()
-param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],
-              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }
-clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'),
-                   param_grid, cv=5, iid=False)
+param_grid = {
+    "C": loguniform(1e3, 1e5),
+    "gamma": loguniform(1e-4, 1e-1),
+}
+clf = RandomizedSearchCV(
+    SVC(kernel="rbf", class_weight="balanced"), param_grid, n_iter=10
+)
 clf = clf.fit(X_train_pca, y_train)
 print("done in %0.3fs" % (time() - t0))
 print("Best estimator found by grid search:")
 print(clf.best_estimator_)


-# #############################################################################
+# %%
 # Quantitative evaluation of the model quality on the test set

 print("Predicting people's names on the test set")
@@ -123,16 +110,21 @@
 print("done in %0.3fs" % (time() - t0))

 print(classification_report(y_test, y_pred, target_names=target_names))
-print(confusion_matrix(y_test, y_pred, labels=range(n_classes)))
+ConfusionMatrixDisplay.from_estimator(
+    clf, X_test_pca, y_test, display_labels=target_names, xticks_rotation="vertical"
+)
+plt.tight_layout()
+plt.show()


-# #############################################################################
+# %%
 # Qualitative evaluation of the predictions using matplotlib
+

 def plot_gallery(images, titles, h, w, n_row=3, n_col=4):
     """Helper function to plot a gallery of portraits"""
     plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))
-    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)
+    plt.subplots_adjust(bottom=0, left=0.01, right=0.99, top=0.90, hspace=0.35)
     for i in range(n_row * n_col):
         plt.subplot(n_row, n_col, i + 1)
         plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)
@@ -141,21 +133,31 @@
         plt.yticks(())


+# %%
 # plot the result of the prediction on a portion of the test set

+
 def title(y_pred, y_test, target_names, i):
-    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]
-    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]
-    return 'predicted: %s\ntrue:      %s' % (pred_name, true_name)
+    pred_name = target_names[y_pred[i]].rsplit(" ", 1)[-1]
+    true_name = target_names[y_test[i]].rsplit(" ", 1)[-1]
+    return "predicted: %s\ntrue:      %s" % (pred_name, true_name)

-prediction_titles = [title(y_pred, y_test, target_names, i)
-                     for i in range(y_pred.shape[0])]
+
+prediction_titles = [
+    title(y_pred, y_test, target_names, i) for i in range(y_pred.shape[0])
+]

 plot_gallery(X_test, prediction_titles, h, w)
-
+# %%
 # plot the gallery of the most significative eigenfaces

 eigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]
 plot_gallery(eigenfaces, eigenface_titles, h, w)

 plt.show()
+
+# %%
+# Face recognition problem would be much more effectively solved by training
+# convolutional neural networks but this family of models is outside of the scope of
+# the scikit-learn library. Interested readers should instead try to use pytorch or
+# tensorflow to implement such models.
('examples/applications', 'plot_stock_market.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,140 +7,83 @@
 the stock market structure from variations in historical quotes.

 The quantity that we use is the daily variation in quote price: quotes
-that are linked tend to cofluctuate during a day.
-
-.. _stock_market:
-
-Learning a graph structure
---------------------------
-
-We use sparse inverse covariance estimation to find which quotes are
-correlated conditionally on the others. Specifically, sparse inverse
-covariance gives us a graph, that is a list of connection. For each
-symbol, the symbols that it is connected too are those useful to explain
-its fluctuations.
-
-Clustering
-----------
-
-We use clustering to group together quotes that behave similarly. Here,
-amongst the :ref:`various clustering techniques <clustering>` available
-in the scikit-learn, we use :ref:`affinity_propagation` as it does
-not enforce equal-size clusters, and it can choose automatically the
-number of clusters from the data.
-
-Note that this gives us a different indication than the graph, as the
-graph reflects conditional relations between variables, while the
-clustering reflects marginal properties: variables clustered together can
-be considered as having a similar impact at the level of the full stock
-market.
-
-Embedding in 2D space
----------------------
-
-For visualization purposes, we need to lay out the different symbols on a
-2D canvas. For this we use :ref:`manifold` techniques to retrieve 2D
-embedding.
-
-
-Visualization
--------------
-
-The output of the 3 models are combined in a 2D graph where nodes
-represents the stocks and edges the:
-
-- cluster labels are used to define the color of the nodes
-- the sparse covariance model is used to display the strength of the edges
-- the 2D embedding is used to position the nodes in the plan
-
-This example has a fair amount of visualization-related code, as
-visualization is crucial here to display the graph. One of the challenge
-is to position the labels minimizing overlap. For this we use an
-heuristic based on the direction of the nearest neighbor along each
-axis.
+that are linked tend to fluctuate in relation to each other during a day.
 """

 # Author: Gael Varoquaux gael.varoquaux@normalesup.org
 # License: BSD 3 clause

-import sys
-
-import numpy as np
-import matplotlib.pyplot as plt
-from matplotlib.collections import LineCollection
-
-import pandas as pd
-
-from sklearn import cluster, covariance, manifold
-
-print(__doc__)
-
-
-# #############################################################################
+# %%
 # Retrieve the data from Internet
-
+# -------------------------------
+#
 # The data is from 2003 - 2008. This is reasonably calm: (not too long ago so
 # that we get high-tech firms, and before the 2008 crash). This kind of
-# historical data can be obtained for from APIs like the quandl.com and
-# alphavantage.co ones.
+# historical data can be obtained from APIs like the quandl.com and
+# alphavantage.co .
+
+import sys
+import numpy as np
+import pandas as pd

 symbol_dict = {
-    'TOT': 'Total',
-    'XOM': 'Exxon',
-    'CVX': 'Chevron',
-    'COP': 'ConocoPhillips',
-    'VLO': 'Valero Energy',
-    'MSFT': 'Microsoft',
-    'IBM': 'IBM',
-    'TWX': 'Time Warner',
-    'CMCSA': 'Comcast',
-    'CVC': 'Cablevision',
-    'YHOO': 'Yahoo',
-    'DELL': 'Dell',
-    'HPQ': 'HP',
-    'AMZN': 'Amazon',
-    'TM': 'Toyota',
-    'CAJ': 'Canon',
-    'SNE': 'Sony',
-    'F': 'Ford',
-    'HMC': 'Honda',
-    'NAV': 'Navistar',
-    'NOC': 'Northrop Grumman',
-    'BA': 'Boeing',
-    'KO': 'Coca Cola',
-    'MMM': '3M',
-    'MCD': 'McDonald\'s',
-    'PEP': 'Pepsi',
-    'K': 'Kellogg',
-    'UN': 'Unilever',
-    'MAR': 'Marriott',
-    'PG': 'Procter Gamble',
-    'CL': 'Colgate-Palmolive',
-    'GE': 'General Electrics',
-    'WFC': 'Wells Fargo',
-    'JPM': 'JPMorgan Chase',
-    'AIG': 'AIG',
-    'AXP': 'American express',
-    'BAC': 'Bank of America',
-    'GS': 'Goldman Sachs',
-    'AAPL': 'Apple',
-    'SAP': 'SAP',
-    'CSCO': 'Cisco',
-    'TXN': 'Texas Instruments',
-    'XRX': 'Xerox',
-    'WMT': 'Wal-Mart',
-    'HD': 'Home Depot',
-    'GSK': 'GlaxoSmithKline',
-    'PFE': 'Pfizer',
-    'SNY': 'Sanofi-Aventis',
-    'NVS': 'Novartis',
-    'KMB': 'Kimberly-Clark',
-    'R': 'Ryder',
-    'GD': 'General Dynamics',
-    'RTN': 'Raytheon',
-    'CVS': 'CVS',
-    'CAT': 'Caterpillar',
-    'DD': 'DuPont de Nemours'}
+    "TOT": "Total",
+    "XOM": "Exxon",
+    "CVX": "Chevron",
+    "COP": "ConocoPhillips",
+    "VLO": "Valero Energy",
+    "MSFT": "Microsoft",
+    "IBM": "IBM",
+    "TWX": "Time Warner",
+    "CMCSA": "Comcast",
+    "CVC": "Cablevision",
+    "YHOO": "Yahoo",
+    "DELL": "Dell",
+    "HPQ": "HP",
+    "AMZN": "Amazon",
+    "TM": "Toyota",
+    "CAJ": "Canon",
+    "SNE": "Sony",
+    "F": "Ford",
+    "HMC": "Honda",
+    "NAV": "Navistar",
+    "NOC": "Northrop Grumman",
+    "BA": "Boeing",
+    "KO": "Coca Cola",
+    "MMM": "3M",
+    "MCD": "McDonald's",
+    "PEP": "Pepsi",
+    "K": "Kellogg",
+    "UN": "Unilever",
+    "MAR": "Marriott",
+    "PG": "Procter Gamble",
+    "CL": "Colgate-Palmolive",
+    "GE": "General Electrics",
+    "WFC": "Wells Fargo",
+    "JPM": "JPMorgan Chase",
+    "AIG": "AIG",
+    "AXP": "American express",
+    "BAC": "Bank of America",
+    "GS": "Goldman Sachs",
+    "AAPL": "Apple",
+    "SAP": "SAP",
+    "CSCO": "Cisco",
+    "TXN": "Texas Instruments",
+    "XRX": "Xerox",
+    "WMT": "Wal-Mart",
+    "HD": "Home Depot",
+    "GSK": "GlaxoSmithKline",
+    "PFE": "Pfizer",
+    "SNY": "Sanofi-Aventis",
+    "NVS": "Novartis",
+    "KMB": "Kimberly-Clark",
+    "R": "Ryder",
+    "GD": "General Dynamics",
+    "RTN": "Raytheon",
+    "CVS": "CVS",
+    "CAT": "Caterpillar",
+    "DD": "DuPont de Nemours",
+}


 symbols, names = np.array(sorted(symbol_dict.items())).T
@@ -148,85 +91,143 @@
 quotes = []

 for symbol in symbols:
-    print('Fetching quote history for %r' % symbol, file=sys.stderr)
-    url = ('https://raw.githubusercontent.com/scikit-learn/examples-data/'
-           'master/financial-data/{}.csv')
+    print("Fetching quote history for %r" % symbol, file=sys.stderr)
+    url = (
+        "https://raw.githubusercontent.com/scikit-learn/examples-data/"
+        "master/financial-data/{}.csv"
+    )
     quotes.append(pd.read_csv(url.format(symbol)))

-close_prices = np.vstack([q['close'] for q in quotes])
-open_prices = np.vstack([q['open'] for q in quotes])
-
-# The daily variations of the quotes are what carry most information
+close_prices = np.vstack([q["close"] for q in quotes])
+open_prices = np.vstack([q["open"] for q in quotes])
+
+# The daily variations of the quotes are what carry the most information
 variation = close_prices - open_prices

-
-# #############################################################################
-# Learn a graphical structure from the correlations
-edge_model = covariance.GraphicalLassoCV(cv=5)
+# %%
+# .. _stock_market:
+#
+# Learning a graph structure
+# --------------------------
+#
+# We use sparse inverse covariance estimation to find which quotes are
+# correlated conditionally on the others. Specifically, sparse inverse
+# covariance gives us a graph, that is a list of connection. For each
+# symbol, the symbols that it is connected too are those useful to explain
+# its fluctuations.
+
+from sklearn import covariance
+
+alphas = np.logspace(-1.5, 1, num=10)
+edge_model = covariance.GraphicalLassoCV(alphas=alphas)

 # standardize the time series: using correlations rather than covariance
-# is more efficient for structure recovery
+# former is more efficient for structure recovery
 X = variation.copy().T
 X /= X.std(axis=0)
 edge_model.fit(X)

-# #############################################################################
-# Cluster using affinity propagation
-
-_, labels = cluster.affinity_propagation(edge_model.covariance_)
+# %%
+# Clustering using affinity propagation
+# -------------------------------------
+#
+# We use clustering to group together quotes that behave similarly. Here,
+# amongst the :ref:`various clustering techniques <clustering>` available
+# in the scikit-learn, we use :ref:`affinity_propagation` as it does
+# not enforce equal-size clusters, and it can choose automatically the
+# number of clusters from the data.
+#
+# Note that this gives us a different indication than the graph, as the
+# graph reflects conditional relations between variables, while the
+# clustering reflects marginal properties: variables clustered together can
+# be considered as having a similar impact at the level of the full stock
+# market.
+
+from sklearn import cluster
+
+_, labels = cluster.affinity_propagation(edge_model.covariance_, random_state=0)
 n_labels = labels.max()

 for i in range(n_labels + 1):
-    print('Cluster %i: %s' % ((i + 1), ', '.join(names[labels == i])))
-
-# #############################################################################
-# Find a low-dimension embedding for visualization: find the best position of
+    print(f"Cluster {i + 1}: {', '.join(names[labels == i])}")
+
+# %%
+# Embedding in 2D space
+# ---------------------
+#
+# For visualization purposes, we need to lay out the different symbols on a
+# 2D canvas. For this we use :ref:`manifold` techniques to retrieve 2D
+# embedding.
+# We use a dense eigen_solver to achieve reproducibility (arpack is initiated
+# with the random vectors that we don't control). In addition, we use a large
+# number of neighbors to capture the large-scale structure.
+
+# Finding a low-dimension embedding for visualization: find the best position of
 # the nodes (the stocks) on a 2D plane

-# We use a dense eigen_solver to achieve reproducibility (arpack is
-# initiated with random vectors that we don't control). In addition, we
-# use a large number of neighbors to capture the large-scale structure.
+from sklearn import manifold
+
 node_position_model = manifold.LocallyLinearEmbedding(
-    n_components=2, eigen_solver='dense', n_neighbors=6)
+    n_components=2, eigen_solver="dense", n_neighbors=6
+)

 embedding = node_position_model.fit_transform(X.T).T

-# #############################################################################
+# %%
 # Visualization
-plt.figure(1, facecolor='w', figsize=(10, 8))
+# -------------
+#
+# The output of the 3 models are combined in a 2D graph where nodes
+# represents the stocks and edges the:
+#
+# - cluster labels are used to define the color of the nodes
+# - the sparse covariance model is used to display the strength of the edges
+# - the 2D embedding is used to position the nodes in the plan
+#
+# This example has a fair amount of visualization-related code, as
+# visualization is crucial here to display the graph. One of the challenge
+# is to position the labels minimizing overlap. For this we use an
+# heuristic based on the direction of the nearest neighbor along each
+# axis.
+
+import matplotlib.pyplot as plt
+from matplotlib.collections import LineCollection
+
+plt.figure(1, facecolor="w", figsize=(10, 8))
 plt.clf()
-ax = plt.axes([0., 0., 1., 1.])
-plt.axis('off')
-
-# Display a graph of the partial correlations
+ax = plt.axes([0.0, 0.0, 1.0, 1.0])
+plt.axis("off")
+
+# Plot the graph of partial correlations
 partial_correlations = edge_model.precision_.copy()
 d = 1 / np.sqrt(np.diag(partial_correlations))
 partial_correlations *= d
 partial_correlations *= d[:, np.newaxis]
-non_zero = (np.abs(np.triu(partial_correlations, k=1)) > 0.02)
+non_zero = np.abs(np.triu(partial_correlations, k=1)) > 0.02

 # Plot the nodes using the coordinates of our embedding
-plt.scatter(embedding[0], embedding[1], s=100 * d ** 2, c=labels,
-            cmap=plt.cm.nipy_spectral)
+plt.scatter(
+    embedding[0], embedding[1], s=100 * d**2, c=labels, cmap=plt.cm.nipy_spectral
+)

 # Plot the edges
 start_idx, end_idx = np.where(non_zero)
 # a sequence of (*line0*, *line1*, *line2*), where::
 #            linen = (x0, y0), (x1, y1), ... (xm, ym)
-segments = [[embedding[:, start], embedding[:, stop]]
-            for start, stop in zip(start_idx, end_idx)]
+segments = [
+    [embedding[:, start], embedding[:, stop]] for start, stop in zip(start_idx, end_idx)
+]
 values = np.abs(partial_correlations[non_zero])
-lc = LineCollection(segments,
-                    zorder=0, cmap=plt.cm.hot_r,
-                    norm=plt.Normalize(0, .7 * values.max()))
+lc = LineCollection(
+    segments, zorder=0, cmap=plt.cm.hot_r, norm=plt.Normalize(0, 0.7 * values.max())
+)
 lc.set_array(values)
 lc.set_linewidths(15 * values)
 ax.add_collection(lc)

 # Add a label to each node. The challenge here is that we want to
 # position the labels to avoid overlap with other labels
-for index, (name, label, (x, y)) in enumerate(
-        zip(names, labels, embedding.T)):
+for index, (name, label, (x, y)) in enumerate(zip(names, labels, embedding.T)):

     dx = x - embedding[0]
     dx[index] = 1
@@ -235,27 +236,38 @@
     this_dx = dx[np.argmin(np.abs(dy))]
     this_dy = dy[np.argmin(np.abs(dx))]
     if this_dx > 0:
-        horizontalalignment = 'left'
-        x = x + .002
+        horizontalalignment = "left"
+        x = x + 0.002
     else:
-        horizontalalignment = 'right'
-        x = x - .002
+        horizontalalignment = "right"
+        x = x - 0.002
     if this_dy > 0:
-        verticalalignment = 'bottom'
-        y = y + .002
+        verticalalignment = "bottom"
+        y = y + 0.002
     else:
-        verticalalignment = 'top'
-        y = y - .002
-    plt.text(x, y, name, size=10,
-             horizontalalignment=horizontalalignment,
-             verticalalignment=verticalalignment,
-             bbox=dict(facecolor='w',
-                       edgecolor=plt.cm.nipy_spectral(label / float(n_labels)),
-                       alpha=.6))
-
-plt.xlim(embedding[0].min() - .15 * embedding[0].ptp(),
-         embedding[0].max() + .10 * embedding[0].ptp(),)
-plt.ylim(embedding[1].min() - .03 * embedding[1].ptp(),
-         embedding[1].max() + .03 * embedding[1].ptp())
+        verticalalignment = "top"
+        y = y - 0.002
+    plt.text(
+        x,
+        y,
+        name,
+        size=10,
+        horizontalalignment=horizontalalignment,
+        verticalalignment=verticalalignment,
+        bbox=dict(
+            facecolor="w",
+            edgecolor=plt.cm.nipy_spectral(label / float(n_labels)),
+            alpha=0.6,
+        ),
+    )
+
+plt.xlim(
+    embedding[0].min() - 0.15 * embedding[0].ptp(),
+    embedding[0].max() + 0.10 * embedding[0].ptp(),
+)
+plt.ylim(
+    embedding[1].min() - 0.03 * embedding[1].ptp(),
+    embedding[1].max() + 0.03 * embedding[1].ptp(),
+)

 plt.show()
('examples/applications', 'plot_out_of_core_classification.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -12,15 +12,6 @@
 This is especially useful in the case of text classification where new
 features (words) may appear in each batch.

-The dataset used in this example is Reuters-21578 as provided by the UCI ML
-repository. It will be automatically downloaded and uncompressed on first run.
-
-The plot represents the learning curve of the classifier: the evolution
-of classification accuracy over the course of the mini-batches. Accuracy is
-measured on the first 1000 samples, held out as a validation set.
-
-To limit the memory consumption, we queue examples up to a fixed amount before
-feeding them to the learner.
 """

 # Authors: Eustache Diemert <eustache@diemert.fr>
@@ -51,28 +42,32 @@

 def _not_in_sphinx():
     # Hack to detect whether we are running by the sphinx builder
-    return '__file__' in globals()
-
-###############################################################################
+    return "__file__" in globals()
+
+
+# %%
 # Reuters Dataset related routines
 # --------------------------------
 #
+# The dataset used in this example is Reuters-21578 as provided by the UCI ML
+# repository. It will be automatically downloaded and uncompressed on first
+# run.


 class ReutersParser(HTMLParser):
     """Utility class to parse a SGML file and yield documents one at a time."""

-    def __init__(self, encoding='latin-1'):
+    def __init__(self, encoding="latin-1"):
         HTMLParser.__init__(self)
         self._reset()
         self.encoding = encoding

     def handle_starttag(self, tag, attrs):
-        method = 'start_' + tag
+        method = "start_" + tag
         getattr(self, method, lambda x: None)(attrs)

     def handle_endtag(self, tag):
-        method = 'end_' + tag
+        method = "end_" + tag
         getattr(self, method, lambda: None)()

     def _reset(self):
@@ -106,10 +101,10 @@
         pass

     def end_reuters(self):
-        self.body = re.sub(r'\s+', r' ', self.body)
-        self.docs.append({'title': self.title,
-                          'body': self.body,
-                          'topics': self.topics})
+        self.body = re.sub(r"\s+", r" ", self.body)
+        self.docs.append(
+            {"title": self.title, "body": self.body, "topics": self.topics}
+        )
         self._reset()

     def start_title(self, attributes):
@@ -150,49 +145,49 @@

     """

-    DOWNLOAD_URL = ('http://archive.ics.uci.edu/ml/machine-learning-databases/'
-                    'reuters21578-mld/reuters21578.tar.gz')
-    ARCHIVE_FILENAME = 'reuters21578.tar.gz'
+    DOWNLOAD_URL = (
+        "http://archive.ics.uci.edu/ml/machine-learning-databases/"
+        "reuters21578-mld/reuters21578.tar.gz"
+    )
+    ARCHIVE_FILENAME = "reuters21578.tar.gz"

     if data_path is None:
         data_path = os.path.join(get_data_home(), "reuters")
     if not os.path.exists(data_path):
         """Download the dataset."""
-        print("downloading dataset (once and for all) into %s" %
-              data_path)
+        print("downloading dataset (once and for all) into %s" % data_path)
         os.mkdir(data_path)

         def progress(blocknum, bs, size):
-            total_sz_mb = '%.2f MB' % (size / 1e6)
-            current_sz_mb = '%.2f MB' % ((blocknum * bs) / 1e6)
+            total_sz_mb = "%.2f MB" % (size / 1e6)
+            current_sz_mb = "%.2f MB" % ((blocknum * bs) / 1e6)
             if _not_in_sphinx():
-                sys.stdout.write(
-                    '\rdownloaded %s / %s' % (current_sz_mb, total_sz_mb))
+                sys.stdout.write("\rdownloaded %s / %s" % (current_sz_mb, total_sz_mb))

         archive_path = os.path.join(data_path, ARCHIVE_FILENAME)
-        urlretrieve(DOWNLOAD_URL, filename=archive_path,
-                    reporthook=progress)
+        urlretrieve(DOWNLOAD_URL, filename=archive_path, reporthook=progress)
         if _not_in_sphinx():
-            sys.stdout.write('\r')
+            sys.stdout.write("\r")
         print("untarring Reuters dataset...")
-        tarfile.open(archive_path, 'r:gz').extractall(data_path)
+        tarfile.open(archive_path, "r:gz").extractall(data_path)
         print("done.")

     parser = ReutersParser()
     for filename in glob(os.path.join(data_path, "*.sgm")):
-        for doc in parser.parse(open(filename, 'rb')):
+        for doc in parser.parse(open(filename, "rb")):
             yield doc


-###############################################################################
+# %%
 # Main
 # ----
 #
 # Create the vectorizer and limit the number of features to a reasonable
 # maximum

-vectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18,
-                               alternate_sign=False)
+vectorizer = HashingVectorizer(
+    decode_error="ignore", n_features=2**18, alternate_sign=False
+)


 # Iterator over parsed Reuters SGML files.
@@ -203,14 +198,14 @@
 # files. For other datasets, one should take care of creating a test set with
 # a realistic portion of positive instances.
 all_classes = np.array([0, 1])
-positive_class = 'acq'
+positive_class = "acq"

 # Here are some classifiers that support the `partial_fit` method
 partial_fit_classifiers = {
-    'SGD': SGDClassifier(max_iter=5, tol=1e-3),
-    'Perceptron': Perceptron(tol=1e-3),
-    'NB Multinomial': MultinomialNB(alpha=0.01),
-    'Passive-Aggressive': PassiveAggressiveClassifier(tol=1e-3),
+    "SGD": SGDClassifier(max_iter=5),
+    "Perceptron": Perceptron(),
+    "NB Multinomial": MultinomialNB(alpha=0.01),
+    "Passive-Aggressive": PassiveAggressiveClassifier(),
 }


@@ -220,9 +215,11 @@
     Note: size is before excluding invalid docs with no topics assigned.

     """
-    data = [('{title}\n\n{body}'.format(**doc), pos_class in doc['topics'])
-            for doc in itertools.islice(doc_iter, size)
-            if doc['topics']]
+    data = [
+        ("{title}\n\n{body}".format(**doc), pos_class in doc["topics"])
+        for doc in itertools.islice(doc_iter, size)
+        if doc["topics"]
+    ]
     if not len(data):
         return np.asarray([], dtype=int), np.asarray([], dtype=int)
     X_text, y = zip(*data)
@@ -238,7 +235,7 @@


 # test data statistics
-test_stats = {'n_test': 0, 'n_test_pos': 0}
+test_stats = {"n_test": 0, "n_test_pos": 0}

 # First we hold out a number of examples to estimate accuracy
 n_test_documents = 1000
@@ -248,28 +245,34 @@
 tick = time.time()
 X_test = vectorizer.transform(X_test_text)
 vectorizing_time = time.time() - tick
-test_stats['n_test'] += len(y_test)
-test_stats['n_test_pos'] += sum(y_test)
+test_stats["n_test"] += len(y_test)
+test_stats["n_test_pos"] += sum(y_test)
 print("Test set is %d documents (%d positive)" % (len(y_test), sum(y_test)))


 def progress(cls_name, stats):
     """Report progress information, return a string."""
-    duration = time.time() - stats['t0']
+    duration = time.time() - stats["t0"]
     s = "%20s classifier : \t" % cls_name
     s += "%(n_train)6d train docs (%(n_train_pos)6d positive) " % stats
     s += "%(n_test)6d test docs (%(n_test_pos)6d positive) " % test_stats
     s += "accuracy: %(accuracy).3f " % stats
-    s += "in %.2fs (%5d docs/s)" % (duration, stats['n_train'] / duration)
+    s += "in %.2fs (%5d docs/s)" % (duration, stats["n_train"] / duration)
     return s


 cls_stats = {}

 for cls_name in partial_fit_classifiers:
-    stats = {'n_train': 0, 'n_train_pos': 0,
-             'accuracy': 0.0, 'accuracy_history': [(0, 0)], 't0': time.time(),
-             'runtime_history': [(0, 0)], 'total_fit_time': 0.0}
+    stats = {
+        "n_train": 0,
+        "n_train_pos": 0,
+        "accuracy": 0.0,
+        "accuracy_history": [(0, 0)],
+        "t0": time.time(),
+        "runtime_history": [(0, 0)],
+        "total_fit_time": 0.0,
+    }
     cls_stats[cls_name] = stats

 get_minibatch(data_stream, n_test_documents)
@@ -298,92 +301,102 @@
         cls.partial_fit(X_train, y_train, classes=all_classes)

         # accumulate test accuracy stats
-        cls_stats[cls_name]['total_fit_time'] += time.time() - tick
-        cls_stats[cls_name]['n_train'] += X_train.shape[0]
-        cls_stats[cls_name]['n_train_pos'] += sum(y_train)
+        cls_stats[cls_name]["total_fit_time"] += time.time() - tick
+        cls_stats[cls_name]["n_train"] += X_train.shape[0]
+        cls_stats[cls_name]["n_train_pos"] += sum(y_train)
         tick = time.time()
-        cls_stats[cls_name]['accuracy'] = cls.score(X_test, y_test)
-        cls_stats[cls_name]['prediction_time'] = time.time() - tick
-        acc_history = (cls_stats[cls_name]['accuracy'],
-                       cls_stats[cls_name]['n_train'])
-        cls_stats[cls_name]['accuracy_history'].append(acc_history)
-        run_history = (cls_stats[cls_name]['accuracy'],
-                       total_vect_time + cls_stats[cls_name]['total_fit_time'])
-        cls_stats[cls_name]['runtime_history'].append(run_history)
+        cls_stats[cls_name]["accuracy"] = cls.score(X_test, y_test)
+        cls_stats[cls_name]["prediction_time"] = time.time() - tick
+        acc_history = (cls_stats[cls_name]["accuracy"], cls_stats[cls_name]["n_train"])
+        cls_stats[cls_name]["accuracy_history"].append(acc_history)
+        run_history = (
+            cls_stats[cls_name]["accuracy"],
+            total_vect_time + cls_stats[cls_name]["total_fit_time"],
+        )
+        cls_stats[cls_name]["runtime_history"].append(run_history)

         if i % 3 == 0:
             print(progress(cls_name, cls_stats[cls_name]))
     if i % 3 == 0:
-        print('\n')
-
-
-###############################################################################
+        print("\n")
+
+
+# %%
 # Plot results
 # ------------
+#
+# The plot represents the learning curve of the classifier: the evolution
+# of classification accuracy over the course of the mini-batches. Accuracy is
+# measured on the first 1000 samples, held out as a validation set.
+#
+# To limit the memory consumption, we queue examples up to a fixed amount
+# before feeding them to the learner.


 def plot_accuracy(x, y, x_legend):
     """Plot accuracy as a function of x."""
     x = np.array(x)
     y = np.array(y)
-    plt.title('Classification accuracy as a function of %s' % x_legend)
-    plt.xlabel('%s' % x_legend)
-    plt.ylabel('Accuracy')
+    plt.title("Classification accuracy as a function of %s" % x_legend)
+    plt.xlabel("%s" % x_legend)
+    plt.ylabel("Accuracy")
     plt.grid(True)
     plt.plot(x, y)


-rcParams['legend.fontsize'] = 10
+rcParams["legend.fontsize"] = 10
 cls_names = list(sorted(cls_stats.keys()))

 # Plot accuracy evolution
 plt.figure()
 for _, stats in sorted(cls_stats.items()):
     # Plot accuracy evolution with #examples
-    accuracy, n_examples = zip(*stats['accuracy_history'])
+    accuracy, n_examples = zip(*stats["accuracy_history"])
     plot_accuracy(n_examples, accuracy, "training examples (#)")
     ax = plt.gca()
     ax.set_ylim((0.8, 1))
-plt.legend(cls_names, loc='best')
+plt.legend(cls_names, loc="best")

 plt.figure()
 for _, stats in sorted(cls_stats.items()):
     # Plot accuracy evolution with runtime
-    accuracy, runtime = zip(*stats['runtime_history'])
-    plot_accuracy(runtime, accuracy, 'runtime (s)')
+    accuracy, runtime = zip(*stats["runtime_history"])
+    plot_accuracy(runtime, accuracy, "runtime (s)")
     ax = plt.gca()
     ax.set_ylim((0.8, 1))
-plt.legend(cls_names, loc='best')
+plt.legend(cls_names, loc="best")

 # Plot fitting times
 plt.figure()
 fig = plt.gcf()
-cls_runtime = [stats['total_fit_time']
-               for cls_name, stats in sorted(cls_stats.items())]
+cls_runtime = [stats["total_fit_time"] for cls_name, stats in sorted(cls_stats.items())]

 cls_runtime.append(total_vect_time)
-cls_names.append('Vectorization')
-bar_colors = ['b', 'g', 'r', 'c', 'm', 'y']
+cls_names.append("Vectorization")
+bar_colors = ["b", "g", "r", "c", "m", "y"]

 ax = plt.subplot(111)
-rectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,
-                     color=bar_colors)
+rectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)

 ax.set_xticks(np.linspace(0, len(cls_names) - 1, len(cls_names)))
 ax.set_xticklabels(cls_names, fontsize=10)
 ymax = max(cls_runtime) * 1.2
 ax.set_ylim((0, ymax))
-ax.set_ylabel('runtime (s)')
-ax.set_title('Training Times')
+ax.set_ylabel("runtime (s)")
+ax.set_title("Training Times")


 def autolabel(rectangles):
     """attach some text vi autolabel on rectangles."""
     for rect in rectangles:
         height = rect.get_height()
-        ax.text(rect.get_x() + rect.get_width() / 2.,
-                1.05 * height, '%.4f' % height,
-                ha='center', va='bottom')
+        ax.text(
+            rect.get_x() + rect.get_width() / 2.0,
+            1.05 * height,
+            "%.4f" % height,
+            ha="center",
+            va="bottom",
+        )
         plt.setp(plt.xticks()[1], rotation=30)


@@ -396,23 +409,22 @@
 cls_runtime = []
 cls_names = list(sorted(cls_stats.keys()))
 for cls_name, stats in sorted(cls_stats.items()):
-    cls_runtime.append(stats['prediction_time'])
+    cls_runtime.append(stats["prediction_time"])
 cls_runtime.append(parsing_time)
-cls_names.append('Read/Parse\n+Feat.Extr.')
+cls_names.append("Read/Parse\n+Feat.Extr.")
 cls_runtime.append(vectorizing_time)
-cls_names.append('Hashing\n+Vect.')
+cls_names.append("Hashing\n+Vect.")

 ax = plt.subplot(111)
-rectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,
-                     color=bar_colors)
+rectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)

 ax.set_xticks(np.linspace(0, len(cls_names) - 1, len(cls_names)))
 ax.set_xticklabels(cls_names, fontsize=8)
 plt.setp(plt.xticks()[1], rotation=30)
 ymax = max(cls_runtime) * 1.2
 ax.set_ylim((0, ymax))
-ax.set_ylabel('runtime (s)')
-ax.set_title('Prediction Times (%d instances)' % n_test_documents)
+ax.set_ylabel("runtime (s)")
+ax.set_title("Prediction Times (%d instances)" % n_test_documents)
 autolabel(rectangles)
 plt.tight_layout()
 plt.show()
('examples/applications', 'plot_prediction_latency.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -25,46 +25,59 @@

 from sklearn.preprocessing import StandardScaler
 from sklearn.model_selection import train_test_split
-from sklearn.datasets.samples_generator import make_regression
-from sklearn.ensemble.forest import RandomForestRegressor
-from sklearn.linear_model.ridge import Ridge
-from sklearn.linear_model.stochastic_gradient import SGDRegressor
-from sklearn.svm.classes import SVR
+from sklearn.datasets import make_regression
+from sklearn.ensemble import RandomForestRegressor
+from sklearn.linear_model import Ridge
+from sklearn.linear_model import SGDRegressor
+from sklearn.svm import SVR
 from sklearn.utils import shuffle


 def _not_in_sphinx():
     # Hack to detect whether we are running by the sphinx builder
-    return '__file__' in globals()
+    return "__file__" in globals()
+
+
+# %%
+# Benchmark and plot helper functions
+# -----------------------------------


 def atomic_benchmark_estimator(estimator, X_test, verbose=False):
     """Measure runtime prediction of each instance."""
     n_instances = X_test.shape[0]
-    runtimes = np.zeros(n_instances, dtype=np.float)
+    runtimes = np.zeros(n_instances, dtype=float)
     for i in range(n_instances):
         instance = X_test[[i], :]
         start = time.time()
         estimator.predict(instance)
         runtimes[i] = time.time() - start
     if verbose:
-        print("atomic_benchmark runtimes:", min(runtimes), np.percentile(
-            runtimes, 50), max(runtimes))
+        print(
+            "atomic_benchmark runtimes:",
+            min(runtimes),
+            np.percentile(runtimes, 50),
+            max(runtimes),
+        )
     return runtimes


 def bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats, verbose):
     """Measure runtime prediction of the whole input."""
     n_instances = X_test.shape[0]
-    runtimes = np.zeros(n_bulk_repeats, dtype=np.float)
+    runtimes = np.zeros(n_bulk_repeats, dtype=float)
     for i in range(n_bulk_repeats):
         start = time.time()
         estimator.predict(X_test)
         runtimes[i] = time.time() - start
     runtimes = np.array(list(map(lambda x: x / float(n_instances), runtimes)))
     if verbose:
-        print("bulk_benchmark runtimes:", min(runtimes), np.percentile(
-            runtimes, 50), max(runtimes))
+        print(
+            "bulk_benchmark runtimes:",
+            min(runtimes),
+            np.percentile(runtimes, 50),
+            max(runtimes),
+        )
     return runtimes


@@ -85,8 +98,7 @@

     """
     atomic_runtimes = atomic_benchmark_estimator(estimator, X_test, verbose)
-    bulk_runtimes = bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats,
-                                             verbose)
+    bulk_runtimes = bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats, verbose)
     return atomic_runtimes, bulk_runtimes


@@ -95,12 +107,14 @@
     if verbose:
         print("generating dataset...")

-    X, y, coef = make_regression(n_samples=n_train + n_test,
-                                 n_features=n_features, noise=noise, coef=True)
+    X, y, coef = make_regression(
+        n_samples=n_train + n_test, n_features=n_features, noise=noise, coef=True
+    )

     random_seed = 13
     X_train, X_test, y_train, y_test = train_test_split(
-        X, y, train_size=n_train, test_size=n_test, random_state=random_seed)
+        X, y, train_size=n_train, test_size=n_test, random_state=random_seed
+    )
     X_train, y_train = shuffle(X_train, y_train, random_state=random_seed)

     X_scaler = StandardScaler()
@@ -130,26 +144,32 @@
     """

     fig, ax1 = plt.subplots(figsize=(10, 6))
-    bp = plt.boxplot(runtimes, )
-
-    cls_infos = ['%s\n(%d %s)' % (estimator_conf['name'],
-                                  estimator_conf['complexity_computer'](
-                                      estimator_conf['instance']),
-                                  estimator_conf['complexity_label']) for
-                 estimator_conf in configuration['estimators']]
+    bp = plt.boxplot(
+        runtimes,
+    )
+
+    cls_infos = [
+        "%s\n(%d %s)"
+        % (
+            estimator_conf["name"],
+            estimator_conf["complexity_computer"](estimator_conf["instance"]),
+            estimator_conf["complexity_label"],
+        )
+        for estimator_conf in configuration["estimators"]
+    ]
     plt.setp(ax1, xticklabels=cls_infos)
-    plt.setp(bp['boxes'], color='black')
-    plt.setp(bp['whiskers'], color='black')
-    plt.setp(bp['fliers'], color='red', marker='+')
-
-    ax1.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',
-                   alpha=0.5)
+    plt.setp(bp["boxes"], color="black")
+    plt.setp(bp["whiskers"], color="black")
+    plt.setp(bp["fliers"], color="red", marker="+")
+
+    ax1.yaxis.grid(True, linestyle="-", which="major", color="lightgrey", alpha=0.5)

     ax1.set_axisbelow(True)
-    ax1.set_title('Prediction Time per Instance - %s, %d feats.' % (
-        pred_type.capitalize(),
-        configuration['n_features']))
-    ax1.set_ylabel('Prediction Time (us)')
+    ax1.set_title(
+        "Prediction Time per Instance - %s, %d feats."
+        % (pred_type.capitalize(), configuration["n_features"])
+    )
+    ax1.set_ylabel("Prediction Time (us)")

     plt.show()

@@ -157,24 +177,24 @@
 def benchmark(configuration):
     """Run the whole benchmark."""
     X_train, y_train, X_test, y_test = generate_dataset(
-        configuration['n_train'], configuration['n_test'],
-        configuration['n_features'])
+        configuration["n_train"], configuration["n_test"], configuration["n_features"]
+    )

     stats = {}
-    for estimator_conf in configuration['estimators']:
-        print("Benchmarking", estimator_conf['instance'])
-        estimator_conf['instance'].fit(X_train, y_train)
+    for estimator_conf in configuration["estimators"]:
+        print("Benchmarking", estimator_conf["instance"])
+        estimator_conf["instance"].fit(X_train, y_train)
         gc.collect()
-        a, b = benchmark_estimator(estimator_conf['instance'], X_test)
-        stats[estimator_conf['name']] = {'atomic': a, 'bulk': b}
-
-    cls_names = [estimator_conf['name'] for estimator_conf in configuration[
-        'estimators']]
-    runtimes = [1e6 * stats[clf_name]['atomic'] for clf_name in cls_names]
-    boxplot_runtimes(runtimes, 'atomic', configuration)
-    runtimes = [1e6 * stats[clf_name]['bulk'] for clf_name in cls_names]
-    boxplot_runtimes(runtimes, 'bulk (%d)' % configuration['n_test'],
-                     configuration)
+        a, b = benchmark_estimator(estimator_conf["instance"], X_test)
+        stats[estimator_conf["name"]] = {"atomic": a, "bulk": b}
+
+    cls_names = [
+        estimator_conf["name"] for estimator_conf in configuration["estimators"]
+    ]
+    runtimes = [1e6 * stats[clf_name]["atomic"] for clf_name in cls_names]
+    boxplot_runtimes(runtimes, "atomic", configuration)
+    runtimes = [1e6 * stats[clf_name]["bulk"] for clf_name in cls_names]
+    boxplot_runtimes(runtimes, "bulk (%d)" % configuration["n_test"], configuration)


 def n_feature_influence(estimators, n_train, n_test, n_features, percentile):
@@ -205,106 +225,125 @@
             estimator.fit(X_train, y_train)
             gc.collect()
             runtimes = bulk_benchmark_estimator(estimator, X_test, 30, False)
-            percentiles[cls_name][n] = 1e6 * np.percentile(runtimes,
-                                                           percentile)
+            percentiles[cls_name][n] = 1e6 * np.percentile(runtimes, percentile)
     return percentiles


 def plot_n_features_influence(percentiles, percentile):
     fig, ax1 = plt.subplots(figsize=(10, 6))
-    colors = ['r', 'g', 'b']
+    colors = ["r", "g", "b"]
     for i, cls_name in enumerate(percentiles.keys()):
         x = np.array(sorted([n for n in percentiles[cls_name].keys()]))
         y = np.array([percentiles[cls_name][n] for n in x])
-        plt.plot(x, y, color=colors[i], )
-    ax1.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',
-                   alpha=0.5)
+        plt.plot(
+            x,
+            y,
+            color=colors[i],
+        )
+    ax1.yaxis.grid(True, linestyle="-", which="major", color="lightgrey", alpha=0.5)
     ax1.set_axisbelow(True)
-    ax1.set_title('Evolution of Prediction Time with #Features')
-    ax1.set_xlabel('#Features')
-    ax1.set_ylabel('Prediction Time at %d%%-ile (us)' % percentile)
+    ax1.set_title("Evolution of Prediction Time with #Features")
+    ax1.set_xlabel("#Features")
+    ax1.set_ylabel("Prediction Time at %d%%-ile (us)" % percentile)
     plt.show()


 def benchmark_throughputs(configuration, duration_secs=0.1):
     """benchmark throughput for different estimators."""
     X_train, y_train, X_test, y_test = generate_dataset(
-        configuration['n_train'], configuration['n_test'],
-        configuration['n_features'])
+        configuration["n_train"], configuration["n_test"], configuration["n_features"]
+    )
     throughputs = dict()
-    for estimator_config in configuration['estimators']:
-        estimator_config['instance'].fit(X_train, y_train)
+    for estimator_config in configuration["estimators"]:
+        estimator_config["instance"].fit(X_train, y_train)
         start_time = time.time()
         n_predictions = 0
         while (time.time() - start_time) < duration_secs:
-            estimator_config['instance'].predict(X_test[[0]])
+            estimator_config["instance"].predict(X_test[[0]])
             n_predictions += 1
-        throughputs[estimator_config['name']] = n_predictions / duration_secs
+        throughputs[estimator_config["name"]] = n_predictions / duration_secs
     return throughputs


 def plot_benchmark_throughput(throughputs, configuration):
     fig, ax = plt.subplots(figsize=(10, 6))
-    colors = ['r', 'g', 'b']
-    cls_infos = ['%s\n(%d %s)' % (estimator_conf['name'],
-                                  estimator_conf['complexity_computer'](
-                                      estimator_conf['instance']),
-                                  estimator_conf['complexity_label']) for
-                 estimator_conf in configuration['estimators']]
-    cls_values = [throughputs[estimator_conf['name']] for estimator_conf in
-                  configuration['estimators']]
+    colors = ["r", "g", "b"]
+    cls_infos = [
+        "%s\n(%d %s)"
+        % (
+            estimator_conf["name"],
+            estimator_conf["complexity_computer"](estimator_conf["instance"]),
+            estimator_conf["complexity_label"],
+        )
+        for estimator_conf in configuration["estimators"]
+    ]
+    cls_values = [
+        throughputs[estimator_conf["name"]]
+        for estimator_conf in configuration["estimators"]
+    ]
     plt.bar(range(len(throughputs)), cls_values, width=0.5, color=colors)
     ax.set_xticks(np.linspace(0.25, len(throughputs) - 0.75, len(throughputs)))
     ax.set_xticklabels(cls_infos, fontsize=10)
     ymax = max(cls_values) * 1.2
     ax.set_ylim((0, ymax))
-    ax.set_ylabel('Throughput (predictions/sec)')
-    ax.set_title('Prediction Throughput for different estimators (%d '
-                 'features)' % configuration['n_features'])
+    ax.set_ylabel("Throughput (predictions/sec)")
+    ax.set_title(
+        "Prediction Throughput for different estimators (%d features)"
+        % configuration["n_features"]
+    )
     plt.show()


-# #############################################################################
-# Main code
-
-start_time = time.time()
-
-# #############################################################################
+# %%
 # Benchmark bulk/atomic prediction speed for various regressors
+# -------------------------------------------------------------
+
 configuration = {
-    'n_train': int(1e3),
-    'n_test': int(1e2),
-    'n_features': int(1e2),
-    'estimators': [
-        {'name': 'Linear Model',
-         'instance': SGDRegressor(penalty='elasticnet', alpha=0.01,
-                                  l1_ratio=0.25, fit_intercept=True,
-                                  tol=1e-4),
-         'complexity_label': 'non-zero coefficients',
-         'complexity_computer': lambda clf: np.count_nonzero(clf.coef_)},
-        {'name': 'RandomForest',
-         'instance': RandomForestRegressor(n_estimators=100),
-         'complexity_label': 'estimators',
-         'complexity_computer': lambda clf: clf.n_estimators},
-        {'name': 'SVR',
-         'instance': SVR(kernel='rbf'),
-         'complexity_label': 'support vectors',
-         'complexity_computer': lambda clf: len(clf.support_vectors_)},
-    ]
+    "n_train": int(1e3),
+    "n_test": int(1e2),
+    "n_features": int(1e2),
+    "estimators": [
+        {
+            "name": "Linear Model",
+            "instance": SGDRegressor(
+                penalty="elasticnet", alpha=0.01, l1_ratio=0.25, tol=1e-4
+            ),
+            "complexity_label": "non-zero coefficients",
+            "complexity_computer": lambda clf: np.count_nonzero(clf.coef_),
+        },
+        {
+            "name": "RandomForest",
+            "instance": RandomForestRegressor(),
+            "complexity_label": "estimators",
+            "complexity_computer": lambda clf: clf.n_estimators,
+        },
+        {
+            "name": "SVR",
+            "instance": SVR(kernel="rbf"),
+            "complexity_label": "support vectors",
+            "complexity_computer": lambda clf: len(clf.support_vectors_),
+        },
+    ],
 }
 benchmark(configuration)

-# benchmark n_features influence on prediction speed
+# %%
+# Benchmark n_features influence on prediction speed
+# --------------------------------------------------
+
 percentile = 90
-percentiles = n_feature_influence({'ridge': Ridge()},
-                                  configuration['n_train'],
-                                  configuration['n_test'],
-                                  [100, 250, 500], percentile)
+percentiles = n_feature_influence(
+    {"ridge": Ridge()},
+    configuration["n_train"],
+    configuration["n_test"],
+    [100, 250, 500],
+    percentile,
+)
 plot_n_features_influence(percentiles, percentile)

-# benchmark throughput
+# %%
+# Benchmark throughput
+# --------------------
+
 throughputs = benchmark_throughputs(configuration)
 plot_benchmark_throughput(throughputs, configuration)
-
-stop_time = time.time()
-print("example run in %.2fs" % (stop_time - start_time))
('examples/applications', 'plot_species_distribution_modeling.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -9,9 +9,8 @@
 mammals given past observations and 14 environmental
 variables. Since we have only positive examples (there are
 no unsuccessful observations), we cast this problem as a
-density estimation problem and use the `OneClassSVM` provided
-by the package `sklearn.svm` as our modeling tool.
-The dataset is provided by Phillips et. al. (2006).
+density estimation problem and use the :class:`~sklearn.svm.OneClassSVM`
+as our modeling tool. The dataset is provided by Phillips et. al. (2006).
 If available, the example uses
 `basemap <https://matplotlib.org/basemap/>`_
 to plot the coast lines and national boundaries of South America.
@@ -34,6 +33,7 @@
    <http://rob.schapire.net/papers/ecolmod.pdf>`_
    S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling,
    190:231-259, 2006.
+
 """

 # Authors: Peter Prettenhofer <peter.prettenhofer@gmail.com>
@@ -46,20 +46,45 @@
 import numpy as np
 import matplotlib.pyplot as plt

-from sklearn.datasets.base import Bunch
+from sklearn.utils import Bunch
 from sklearn.datasets import fetch_species_distributions
-from sklearn.datasets.species_distributions import construct_grids
 from sklearn import svm, metrics

 # if basemap is available, we'll use it.
 # otherwise, we'll improvise later...
 try:
     from mpl_toolkits.basemap import Basemap
+
     basemap = True
 except ImportError:
     basemap = False

-print(__doc__)
+
+def construct_grids(batch):
+    """Construct the map grid from the batch object
+
+    Parameters
+    ----------
+    batch : Batch object
+        The object returned by :func:`fetch_species_distributions`
+
+    Returns
+    -------
+    (xgrid, ygrid) : 1-D arrays
+        The grid corresponding to the values in batch.coverages
+    """
+    # x,y coordinates for corner cells
+    xmin = batch.x_left_lower_corner + batch.grid_size
+    xmax = xmin + (batch.Nx * batch.grid_size)
+    ymin = batch.y_left_lower_corner + batch.grid_size
+    ymax = ymin + (batch.Ny * batch.grid_size)
+
+    # x coordinates of the grid cells
+    xgrid = np.arange(xmin, xmax, batch.grid_size)
+    # y coordinates of the grid cells
+    ygrid = np.arange(ymin, ymax, batch.grid_size)
+
+    return (xgrid, ygrid)


 def create_species_bunch(species_name, train, test, coverages, xgrid, ygrid):
@@ -68,31 +93,34 @@
     This will use the test/train record arrays to extract the
     data specific to the given species name.
     """
-    bunch = Bunch(name=' '.join(species_name.split("_")[:2]))
-    species_name = species_name.encode('ascii')
+    bunch = Bunch(name=" ".join(species_name.split("_")[:2]))
+    species_name = species_name.encode("ascii")
     points = dict(test=test, train=train)

     for label, pts in points.items():
         # choose points associated with the desired species
-        pts = pts[pts['species'] == species_name]
-        bunch['pts_%s' % label] = pts
+        pts = pts[pts["species"] == species_name]
+        bunch["pts_%s" % label] = pts

         # determine coverage values for each of the training & testing points
-        ix = np.searchsorted(xgrid, pts['dd long'])
-        iy = np.searchsorted(ygrid, pts['dd lat'])
-        bunch['cov_%s' % label] = coverages[:, -iy, ix].T
+        ix = np.searchsorted(xgrid, pts["dd long"])
+        iy = np.searchsorted(ygrid, pts["dd lat"])
+        bunch["cov_%s" % label] = coverages[:, -iy, ix].T

     return bunch


-def plot_species_distribution(species=("bradypus_variegatus_0",
-                                       "microryzomys_minutus_0")):
+def plot_species_distribution(
+    species=("bradypus_variegatus_0", "microryzomys_minutus_0")
+):
     """
     Plot the species distribution.
     """
     if len(species) > 2:
-        print("Note: when more than two species are provided,"
-              " only the first two will be used")
+        print(
+            "Note: when more than two species are provided,"
+            " only the first two will be used"
+        )

     t0 = time()

@@ -106,19 +134,19 @@
     X, Y = np.meshgrid(xgrid, ygrid[::-1])

     # create a bunch for each species
-    BV_bunch = create_species_bunch(species[0],
-                                    data.train, data.test,
-                                    data.coverages, xgrid, ygrid)
-    MM_bunch = create_species_bunch(species[1],
-                                    data.train, data.test,
-                                    data.coverages, xgrid, ygrid)
+    BV_bunch = create_species_bunch(
+        species[0], data.train, data.test, data.coverages, xgrid, ygrid
+    )
+    MM_bunch = create_species_bunch(
+        species[1], data.train, data.test, data.coverages, xgrid, ygrid
+    )

     # background points (grid coordinates) for evaluation
     np.random.seed(13)
-    background_points = np.c_[np.random.randint(low=0, high=data.Ny,
-                                                size=10000),
-                              np.random.randint(low=0, high=data.Nx,
-                                                size=10000)].T
+    background_points = np.c_[
+        np.random.randint(low=0, high=data.Ny, size=10000),
+        np.random.randint(low=0, high=data.Nx, size=10000),
+    ].T

     # We'll make use of the fact that coverages[6] has measurements at all
     # land points.  This will help us decide between land and water.
@@ -135,7 +163,7 @@
         train_cover_std = (species.cov_train - mean) / std

         # Fit OneClassSVM
-        print(" - fit OneClassSVM ... ", end='')
+        print(" - fit OneClassSVM ... ", end="")
         clf = svm.OneClassSVM(nu=0.1, kernel="rbf", gamma=0.5)
         clf.fit(train_cover_std)
         print("done.")
@@ -144,16 +172,21 @@
         plt.subplot(1, 2, i + 1)
         if basemap:
             print(" - plot coastlines using basemap")
-            m = Basemap(projection='cyl', llcrnrlat=Y.min(),
-                        urcrnrlat=Y.max(), llcrnrlon=X.min(),
-                        urcrnrlon=X.max(), resolution='c')
+            m = Basemap(
+                projection="cyl",
+                llcrnrlat=Y.min(),
+                urcrnrlat=Y.max(),
+                llcrnrlon=X.min(),
+                urcrnrlon=X.max(),
+                resolution="c",
+            )
             m.drawcoastlines()
             m.drawcountries()
         else:
             print(" - plot coastlines from coverage")
-            plt.contour(X, Y, land_reference,
-                        levels=[-9998], colors="k",
-                        linestyles="solid")
+            plt.contour(
+                X, Y, land_reference, levels=[-9998], colors="k", linestyles="solid"
+            )
             plt.xticks([])
             plt.yticks([])

@@ -175,18 +208,28 @@

         # plot contours of the prediction
         plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)
-        plt.colorbar(format='%.2f')
+        plt.colorbar(format="%.2f")

         # scatter training/testing points
-        plt.scatter(species.pts_train['dd long'], species.pts_train['dd lat'],
-                    s=2 ** 2, c='black',
-                    marker='^', label='train')
-        plt.scatter(species.pts_test['dd long'], species.pts_test['dd lat'],
-                    s=2 ** 2, c='black',
-                    marker='x', label='test')
+        plt.scatter(
+            species.pts_train["dd long"],
+            species.pts_train["dd lat"],
+            s=2**2,
+            c="black",
+            marker="^",
+            label="train",
+        )
+        plt.scatter(
+            species.pts_test["dd long"],
+            species.pts_test["dd lat"],
+            s=2**2,
+            c="black",
+            marker="x",
+            label="test",
+        )
         plt.legend()
         plt.title(species.name)
-        plt.axis('equal')
+        plt.axis("equal")

         # Compute AUC with regards to background points
         pred_background = Z[background_points[0], background_points[1]]
('examples/applications', 'wikipedia_principal_eigenvector.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -27,6 +27,7 @@

 The graph data is fetched from the DBpedia dumps. DBpedia is an extraction
 of the latent structured data of the Wikipedia content.
+
 """

 # Author: Olivier Grisel <olivier.grisel@ensta.org>
@@ -42,16 +43,13 @@

 from scipy import sparse

-from joblib import Memory
-
 from sklearn.decomposition import randomized_svd
 from urllib.request import urlopen


-print(__doc__)
-
-# #############################################################################
-# Where to download the data, if not already on disk
+# %%
+# Download data, if not already on disk
+# -------------------------------------
 redirects_url = "http://downloads.dbpedia.org/3.5.1/en/redirects_en.nt.bz2"
 redirects_filename = redirects_url.rsplit("/", 1)[1]

@@ -67,16 +65,13 @@
     if not os.path.exists(filename):
         print("Downloading data from '%s', please wait..." % url)
         opener = urlopen(url)
-        open(filename, 'wb').write(opener.read())
+        open(filename, "wb").write(opener.read())
         print()


-# #############################################################################
+# %%
 # Loading the redirect files
-
-memory = Memory(cachedir=".")
-
-
+# --------------------------
 def index(redirects, index_map, k):
     """Find the index of an article name after redirect resolution"""
     k = redirects.get(k, k)
@@ -124,8 +119,9 @@
     return redirects


-# disabling joblib as the pickling of large dicts seems much too slow
-#@memory.cache
+# %%
+# Computing the Adjacency matrix
+# ------------------------------
 def get_adjacency_matrix(redirects_filename, page_links_filename, limit=None):
     """Extract the adjacency graph as a scipy sparse matrix

@@ -169,9 +165,14 @@

 # stop after 5M links to make it possible to work in RAM
 X, redirects, index_map = get_adjacency_matrix(
-    redirects_filename, page_links_filename, limit=5000000)
+    redirects_filename, page_links_filename, limit=5000000
+)
 names = {i: name for name, i in index_map.items()}

+
+# %%
+# Computing Principal Singular Vector using Randomized SVD
+# --------------------------------------------------------
 print("Computing the principal singular vectors using randomized_svd")
 t0 = time()
 U, s, V = randomized_svd(X, 5, n_iter=3)
@@ -184,6 +185,9 @@
 pprint([names[i] for i in np.abs(V[0]).argsort()[-10:]])


+# %%
+# Computing Centrality scores
+# ---------------------------
 def centrality_scores(X, alpha=0.85, max_iter=100, tol=1e-10):
     """Power iteration computation of the principal eigenvector

@@ -201,16 +205,17 @@

     print("Normalizing the graph")
     for i in incoming_counts.nonzero()[0]:
-        X.data[X.indptr[i]:X.indptr[i + 1]] *= 1.0 / incoming_counts[i]
-    dangle = np.asarray(np.where(np.isclose(X.sum(axis=1), 0),
-                                 1.0 / n, 0)).ravel()
-
-    scores = np.full(n, 1. / n, dtype=np.float32)  # initial guess
+        X.data[X.indptr[i] : X.indptr[i + 1]] *= 1.0 / incoming_counts[i]
+    dangle = np.asarray(np.where(np.isclose(X.sum(axis=1), 0), 1.0 / n, 0)).ravel()
+
+    scores = np.full(n, 1.0 / n, dtype=np.float32)  # initial guess
     for i in range(max_iter):
         print("power iteration #%d" % i)
         prev_scores = scores
-        scores = (alpha * (scores * X + np.dot(dangle, prev_scores))
-                  + (1 - alpha) * prev_scores.sum() / n)
+        scores = (
+            alpha * (scores * X + np.dot(dangle, prev_scores))
+            + (1 - alpha) * prev_scores.sum() / n
+        )
         # check convergence: normalized l_inf norm
         scores_max = np.abs(scores).max()
         if scores_max == 0.0:
@@ -222,8 +227,9 @@

     return scores

+
 print("Computing principal eigenvector score using a power iteration method")
 t0 = time()
-scores = centrality_scores(X, max_iter=100, tol=1e-10)
+scores = centrality_scores(X, max_iter=100)
 print("done in %0.3fs" % (time() - t0))
 pprint([names[i] for i in np.abs(scores).argsort()[-10:]])
('examples/applications', 'plot_model_complexity_influence.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,164 +6,285 @@
 Demonstrate how model complexity influences both prediction accuracy and
 computational performance.

-The dataset is the Boston Housing dataset (resp. 20 Newsgroups) for
-regression (resp. classification).
-
-For each class of models we make the model complexity vary through the choice
-of relevant model parameters and measure the influence on both computational
-performance (latency) and predictive power (MSE or Hamming Loss).
+We will be using two datasets:
+    - :ref:`diabetes_dataset` for regression.
+      This dataset consists of 10 measurements taken from diabetes patients.
+      The task is to predict disease progression;
+    - :ref:`20newsgroups_dataset` for classification. This dataset consists of
+      newsgroup posts. The task is to predict on which topic (out of 20 topics)
+      the post is written about.
+
+We will model the complexity influence on three different estimators:
+    - :class:`~sklearn.linear_model.SGDClassifier` (for classification data)
+      which implements stochastic gradient descent learning;
+
+    - :class:`~sklearn.svm.NuSVR` (for regression data) which implements
+      Nu support vector regression;
+
+    - :class:`~sklearn.ensemble.GradientBoostingRegressor` (for regression
+      data) which builds an additive model in a forward stage-wise fashion.
+
+
+We make the model complexity vary through the choice of relevant model
+parameters in each of our selected models. Next, we will measure the influence
+on both computational performance (latency) and predictive power (MSE or
+Hamming Loss).
+
 """

-print(__doc__)
-
-# Author: Eustache Diemert <eustache@diemert.fr>
+# Authors: Eustache Diemert <eustache@diemert.fr>
+#          Maria Telenczuk <https://github.com/maikia>
+#          Guillaume Lemaitre <g.lemaitre58@gmail.com>
 # License: BSD 3 clause

 import time
 import numpy as np
 import matplotlib.pyplot as plt
-from mpl_toolkits.axes_grid1.parasite_axes import host_subplot
-from mpl_toolkits.axisartist.axislines import Axes
-from scipy.sparse.csr import csr_matrix

 from sklearn import datasets
-from sklearn.utils import shuffle
+from sklearn.model_selection import train_test_split
 from sklearn.metrics import mean_squared_error
-from sklearn.svm.classes import NuSVR
-from sklearn.ensemble.gradient_boosting import GradientBoostingRegressor
-from sklearn.linear_model.stochastic_gradient import SGDClassifier
+from sklearn.svm import NuSVR
+from sklearn.ensemble import GradientBoostingRegressor
+from sklearn.linear_model import SGDClassifier
 from sklearn.metrics import hamming_loss
-
-# #############################################################################
-# Routines
-

 # Initialize random generator
 np.random.seed(0)

-
-def generate_data(case, sparse=False):
+##############################################################################
+# Load the data
+# -------------
+#
+# First we load both datasets.
+#
+# .. note:: We are using
+#    :func:`~sklearn.datasets.fetch_20newsgroups_vectorized` to download 20
+#    newsgroups dataset. It returns ready-to-use features.
+#
+# .. note:: ``X`` of the 20 newsgroups dataset is a sparse matrix while ``X``
+#    of diabetes dataset is a numpy array.
+#
+
+
+def generate_data(case):
     """Generate regression/classification data."""
-    bunch = None
-    if case == 'regression':
-        bunch = datasets.load_boston()
-    elif case == 'classification':
-        bunch = datasets.fetch_20newsgroups_vectorized(subset='all')
-    X, y = shuffle(bunch.data, bunch.target)
-    offset = int(X.shape[0] * 0.8)
-    X_train, y_train = X[:offset], y[:offset]
-    X_test, y_test = X[offset:], y[offset:]
-    if sparse:
-        X_train = csr_matrix(X_train)
-        X_test = csr_matrix(X_test)
-    else:
-        X_train = np.array(X_train)
-        X_test = np.array(X_test)
-    y_test = np.array(y_test)
-    y_train = np.array(y_train)
-    data = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train,
-            'y_test': y_test}
+    if case == "regression":
+        X, y = datasets.load_diabetes(return_X_y=True)
+        train_size = 0.8
+    elif case == "classification":
+        X, y = datasets.fetch_20newsgroups_vectorized(subset="all", return_X_y=True)
+        train_size = 0.4  # to make the example run faster
+
+    X_train, X_test, y_train, y_test = train_test_split(
+        X, y, train_size=train_size, random_state=0
+    )
+
+    data = {"X_train": X_train, "X_test": X_test, "y_train": y_train, "y_test": y_test}
     return data


+regression_data = generate_data("regression")
+classification_data = generate_data("classification")
+
+
+##############################################################################
+# Benchmark influence
+# -------------------
+# Next, we can calculate the influence of the parameters on the given
+# estimator. In each round, we will set the estimator with the new value of
+# ``changing_param`` and we will be collecting the prediction times, prediction
+# performance and complexities to see how those changes affect the estimator.
+# We will calculate the complexity using ``complexity_computer`` passed as a
+# parameter.
+#
+
+
 def benchmark_influence(conf):
     """
-    Benchmark influence of :changing_param: on both MSE and latency.
+    Benchmark influence of `changing_param` on both MSE and latency.
     """
     prediction_times = []
     prediction_powers = []
     complexities = []
-    for param_value in conf['changing_param_values']:
-        conf['tuned_params'][conf['changing_param']] = param_value
-        estimator = conf['estimator'](**conf['tuned_params'])
+    for param_value in conf["changing_param_values"]:
+        conf["tuned_params"][conf["changing_param"]] = param_value
+        estimator = conf["estimator"](**conf["tuned_params"])
+
         print("Benchmarking %s" % estimator)
-        estimator.fit(conf['data']['X_train'], conf['data']['y_train'])
-        conf['postfit_hook'](estimator)
-        complexity = conf['complexity_computer'](estimator)
+        estimator.fit(conf["data"]["X_train"], conf["data"]["y_train"])
+        conf["postfit_hook"](estimator)
+        complexity = conf["complexity_computer"](estimator)
         complexities.append(complexity)
         start_time = time.time()
-        for _ in range(conf['n_samples']):
-            y_pred = estimator.predict(conf['data']['X_test'])
-        elapsed_time = (time.time() - start_time) / float(conf['n_samples'])
+        for _ in range(conf["n_samples"]):
+            y_pred = estimator.predict(conf["data"]["X_test"])
+        elapsed_time = (time.time() - start_time) / float(conf["n_samples"])
         prediction_times.append(elapsed_time)
-        pred_score = conf['prediction_performance_computer'](
-            conf['data']['y_test'], y_pred)
+        pred_score = conf["prediction_performance_computer"](
+            conf["data"]["y_test"], y_pred
+        )
         prediction_powers.append(pred_score)
-        print("Complexity: %d | %s: %.4f | Pred. Time: %fs\n" % (
-            complexity, conf['prediction_performance_label'], pred_score,
-            elapsed_time))
+        print(
+            "Complexity: %d | %s: %.4f | Pred. Time: %fs\n"
+            % (
+                complexity,
+                conf["prediction_performance_label"],
+                pred_score,
+                elapsed_time,
+            )
+        )
     return prediction_powers, prediction_times, complexities


-def plot_influence(conf, mse_values, prediction_times, complexities):
-    """
-    Plot influence of model complexity on both accuracy and latency.
-    """
-    plt.figure(figsize=(12, 6))
-    host = host_subplot(111, axes_class=Axes)
-    plt.subplots_adjust(right=0.75)
-    par1 = host.twinx()
-    host.set_xlabel('Model Complexity (%s)' % conf['complexity_label'])
-    y1_label = conf['prediction_performance_label']
-    y2_label = "Time (s)"
-    host.set_ylabel(y1_label)
-    par1.set_ylabel(y2_label)
-    p1, = host.plot(complexities, mse_values, 'b-', label="prediction error")
-    p2, = par1.plot(complexities, prediction_times, 'r-',
-                    label="latency")
-    host.legend(loc='upper right')
-    host.axis["left"].label.set_color(p1.get_color())
-    par1.axis["right"].label.set_color(p2.get_color())
-    plt.title('Influence of Model Complexity - %s' % conf['estimator'].__name__)
-    plt.show()
+##############################################################################
+# Choose parameters
+# -----------------
+#
+# We choose the parameters for each of our estimators by making
+# a dictionary with all the necessary values.
+# ``changing_param`` is the name of the parameter which will vary in each
+# estimator.
+# Complexity will be defined by the ``complexity_label`` and calculated using
+# `complexity_computer`.
+# Also note that depending on the estimator type we are passing
+# different data.
+#


 def _count_nonzero_coefficients(estimator):
     a = estimator.coef_.toarray()
     return np.count_nonzero(a)

-# #############################################################################
-# Main code
-regression_data = generate_data('regression')
-classification_data = generate_data('classification', sparse=True)
+
 configurations = [
-    {'estimator': SGDClassifier,
-     'tuned_params': {'penalty': 'elasticnet', 'alpha': 0.001, 'loss':
-                      'modified_huber', 'fit_intercept': True, 'tol': 1e-3},
-     'changing_param': 'l1_ratio',
-     'changing_param_values': [0.25, 0.5, 0.75, 0.9],
-     'complexity_label': 'non_zero coefficients',
-     'complexity_computer': _count_nonzero_coefficients,
-     'prediction_performance_computer': hamming_loss,
-     'prediction_performance_label': 'Hamming Loss (Misclassification Ratio)',
-     'postfit_hook': lambda x: x.sparsify(),
-     'data': classification_data,
-     'n_samples': 30},
-    {'estimator': NuSVR,
-     'tuned_params': {'C': 1e3, 'gamma': 2 ** -15},
-     'changing_param': 'nu',
-     'changing_param_values': [0.1, 0.25, 0.5, 0.75, 0.9],
-     'complexity_label': 'n_support_vectors',
-     'complexity_computer': lambda x: len(x.support_vectors_),
-     'data': regression_data,
-     'postfit_hook': lambda x: x,
-     'prediction_performance_computer': mean_squared_error,
-     'prediction_performance_label': 'MSE',
-     'n_samples': 30},
-    {'estimator': GradientBoostingRegressor,
-     'tuned_params': {'loss': 'ls'},
-     'changing_param': 'n_estimators',
-     'changing_param_values': [10, 50, 100, 200, 500],
-     'complexity_label': 'n_trees',
-     'complexity_computer': lambda x: x.n_estimators,
-     'data': regression_data,
-     'postfit_hook': lambda x: x,
-     'prediction_performance_computer': mean_squared_error,
-     'prediction_performance_label': 'MSE',
-     'n_samples': 30},
+    {
+        "estimator": SGDClassifier,
+        "tuned_params": {
+            "penalty": "elasticnet",
+            "alpha": 0.001,
+            "loss": "modified_huber",
+            "fit_intercept": True,
+            "tol": 1e-1,
+            "n_iter_no_change": 2,
+        },
+        "changing_param": "l1_ratio",
+        "changing_param_values": [0.25, 0.5, 0.75, 0.9],
+        "complexity_label": "non_zero coefficients",
+        "complexity_computer": _count_nonzero_coefficients,
+        "prediction_performance_computer": hamming_loss,
+        "prediction_performance_label": "Hamming Loss (Misclassification Ratio)",
+        "postfit_hook": lambda x: x.sparsify(),
+        "data": classification_data,
+        "n_samples": 5,
+    },
+    {
+        "estimator": NuSVR,
+        "tuned_params": {"C": 1e3, "gamma": 2**-15},
+        "changing_param": "nu",
+        "changing_param_values": [0.05, 0.1, 0.2, 0.35, 0.5],
+        "complexity_label": "n_support_vectors",
+        "complexity_computer": lambda x: len(x.support_vectors_),
+        "data": regression_data,
+        "postfit_hook": lambda x: x,
+        "prediction_performance_computer": mean_squared_error,
+        "prediction_performance_label": "MSE",
+        "n_samples": 15,
+    },
+    {
+        "estimator": GradientBoostingRegressor,
+        "tuned_params": {
+            "loss": "squared_error",
+            "learning_rate": 0.05,
+            "max_depth": 2,
+        },
+        "changing_param": "n_estimators",
+        "changing_param_values": [10, 25, 50, 75, 100],
+        "complexity_label": "n_trees",
+        "complexity_computer": lambda x: x.n_estimators,
+        "data": regression_data,
+        "postfit_hook": lambda x: x,
+        "prediction_performance_computer": mean_squared_error,
+        "prediction_performance_label": "MSE",
+        "n_samples": 15,
+    },
 ]
+
+
+##############################################################################
+# Run the code and plot the results
+# ---------------------------------
+#
+# We defined all the functions required to run our benchmark. Now, we will loop
+# over the different configurations that we defined previously. Subsequently,
+# we can analyze the plots obtained from the benchmark:
+# Relaxing the `L1` penalty in the SGD classifier reduces the prediction error
+# but leads to an increase in the training time.
+# We can draw a similar analysis regarding the training time which increases
+# with the number of support vectors with a Nu-SVR. However, we observed that
+# there is an optimal number of support vectors which reduces the prediction
+# error. Indeed, too few support vectors lead to an under-fitted model while
+# too many support vectors lead to an over-fitted model.
+# The exact same conclusion can be drawn for the gradient-boosting model. The
+# only the difference with the Nu-SVR is that having too many trees in the
+# ensemble is not as detrimental.
+#
+
+
+def plot_influence(conf, mse_values, prediction_times, complexities):
+    """
+    Plot influence of model complexity on both accuracy and latency.
+    """
+
+    fig = plt.figure()
+    fig.subplots_adjust(right=0.75)
+
+    # first axes (prediction error)
+    ax1 = fig.add_subplot(111)
+    line1 = ax1.plot(complexities, mse_values, c="tab:blue", ls="-")[0]
+    ax1.set_xlabel("Model Complexity (%s)" % conf["complexity_label"])
+    y1_label = conf["prediction_performance_label"]
+    ax1.set_ylabel(y1_label)
+
+    ax1.spines["left"].set_color(line1.get_color())
+    ax1.yaxis.label.set_color(line1.get_color())
+    ax1.tick_params(axis="y", colors=line1.get_color())
+
+    # second axes (latency)
+    ax2 = fig.add_subplot(111, sharex=ax1, frameon=False)
+    line2 = ax2.plot(complexities, prediction_times, c="tab:orange", ls="-")[0]
+    ax2.yaxis.tick_right()
+    ax2.yaxis.set_label_position("right")
+    y2_label = "Time (s)"
+    ax2.set_ylabel(y2_label)
+    ax1.spines["right"].set_color(line2.get_color())
+    ax2.yaxis.label.set_color(line2.get_color())
+    ax2.tick_params(axis="y", colors=line2.get_color())
+
+    plt.legend(
+        (line1, line2), ("prediction error", "prediction latency"), loc="upper right"
+    )
+
+    plt.title(
+        "Influence of varying '%s' on %s"
+        % (conf["changing_param"], conf["estimator"].__name__)
+    )
+
+
 for conf in configurations:
-    prediction_performances, prediction_times, complexities = \
-        benchmark_influence(conf)
-    plot_influence(conf, prediction_performances, prediction_times,
-                   complexities)
+    prediction_performances, prediction_times, complexities = benchmark_influence(conf)
+    plot_influence(conf, prediction_performances, prediction_times, complexities)
+plt.show()
+
+##############################################################################
+# Conclusion
+# ----------
+#
+# As a conclusion, we can deduce the following insights:
+#
+# * a model which is more complex (or expressive) will require a larger
+#   training time;
+# * a more complex model does not guarantee to reduce the prediction error.
+#
+# These aspects are related to model generalization and avoiding model
+# under-fitting or over-fitting.
('examples/applications', 'svm_gui.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -14,17 +14,22 @@

 """

-print(__doc__)
-
 # Author: Peter Prettenhoer <peter.prettenhofer@gmail.com>
 #
 # License: BSD 3 clause

 import matplotlib
-matplotlib.use('TkAgg')
-
+
+matplotlib.use("TkAgg")
 from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
-from matplotlib.backends.backend_tkagg import NavigationToolbar2TkAgg
+
+try:
+    from matplotlib.backends.backend_tkagg import NavigationToolbar2Tk
+except ImportError:
+    # NavigationToolbar2TkAgg was deprecated in matplotlib 2.2
+    from matplotlib.backends.backend_tkagg import (
+        NavigationToolbar2TkAgg as NavigationToolbar2Tk,
+    )
 from matplotlib.figure import Figure
 from matplotlib.contour import ContourSet

@@ -53,12 +58,12 @@
         self.surface_type = 0

     def changed(self, event):
-        """Notify the observers. """
+        """Notify the observers."""
         for observer in self.observers:
             observer.update(event, self)

     def add_observer(self, observer):
-        """Register an observer. """
+        """Register an observer."""
         self.observers.append(observer)

     def set_surface(self, surface):
@@ -91,14 +96,23 @@
         degree = int(self.degree.get())
         kernel_map = {0: "linear", 1: "rbf", 2: "poly"}
         if len(np.unique(y)) == 1:
-            clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],
-                                  gamma=gamma, coef0=coef0, degree=degree)
+            clf = svm.OneClassSVM(
+                kernel=kernel_map[self.kernel.get()],
+                gamma=gamma,
+                coef0=coef0,
+                degree=degree,
+            )
             clf.fit(X)
         else:
-            clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,
-                          gamma=gamma, coef0=coef0, degree=degree)
+            clf = svm.SVC(
+                kernel=kernel_map[self.kernel.get()],
+                C=C,
+                gamma=gamma,
+                coef0=coef0,
+                degree=degree,
+            )
             clf.fit(X, y)
-        if hasattr(clf, 'score'):
+        if hasattr(clf, "score"):
             print("Accuracy:", clf.score(X, y) * 100)
         X1, X2, Z = self.decision_surface(clf)
         self.model.clf = clf
@@ -129,13 +143,14 @@
         self.refit()

     def refit(self):
-        """Refit the model if already fitted. """
+        """Refit the model if already fitted."""
         if self.fitted:
             self.fit()


 class View:
-    """Test docstring. """
+    """Test docstring."""
+
     def __init__(self, root, controller):
         f = Figure()
         ax = f.add_subplot(111)
@@ -144,11 +159,15 @@
         ax.set_xlim((x_min, x_max))
         ax.set_ylim((y_min, y_max))
         canvas = FigureCanvasTkAgg(f, master=root)
-        canvas.show()
+        try:
+            canvas.draw()
+        except AttributeError:
+            # support for matplotlib (1.*)
+            canvas.show()
         canvas.get_tk_widget().pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)
         canvas._tkcanvas.pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)
-        canvas.mpl_connect('button_press_event', self.onclick)
-        toolbar = NavigationToolbar2TkAgg(canvas, root)
+        canvas.mpl_connect("button_press_event", self.onclick)
+        toolbar = NavigationToolbar2Tk(canvas, root)
         toolbar.update()
         self.controllbar = ControllBar(root, controller)
         self.f = f
@@ -174,9 +193,9 @@
     def update_example(self, model, idx):
         x, y, l = model.data[idx]
         if l == 1:
-            color = 'w'
+            color = "w"
         elif l == -1:
-            color = 'k'
+            color = "k"
         self.ax.plot([x], [y], "%so" % color, scalex=0.0, scaley=0.0)

     def update(self, event, model):
@@ -217,25 +236,33 @@
         """Plot the support vectors by placing circles over the
         corresponding data points and adds the circle collection
         to the contours list."""
-        cs = self.ax.scatter(support_vectors[:, 0], support_vectors[:, 1],
-                             s=80, edgecolors="k", facecolors="none")
+        cs = self.ax.scatter(
+            support_vectors[:, 0],
+            support_vectors[:, 1],
+            s=80,
+            edgecolors="k",
+            facecolors="none",
+        )
         self.contours.append(cs)

     def plot_decision_surface(self, surface, type):
         X1, X2, Z = surface
         if type == 0:
             levels = [-1.0, 0.0, 1.0]
-            linestyles = ['dashed', 'solid', 'dashed']
-            colors = 'k'
-            self.contours.append(self.ax.contour(X1, X2, Z, levels,
-                                                 colors=colors,
-                                                 linestyles=linestyles))
+            linestyles = ["dashed", "solid", "dashed"]
+            colors = "k"
+            self.contours.append(
+                self.ax.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)
+            )
         elif type == 1:
-            self.contours.append(self.ax.contourf(X1, X2, Z, 10,
-                                                  cmap=matplotlib.cm.bone,
-                                                  origin='lower', alpha=0.85))
-            self.contours.append(self.ax.contour(X1, X2, Z, [0.0], colors='k',
-                                                 linestyles=['solid']))
+            self.contours.append(
+                self.ax.contourf(
+                    X1, X2, Z, 10, cmap=matplotlib.cm.bone, origin="lower", alpha=0.85
+                )
+            )
+            self.contours.append(
+                self.ax.contour(X1, X2, Z, [0.0], colors="k", linestyles=["solid"])
+            )
         else:
             raise ValueError("surface type unknown")

@@ -244,12 +271,27 @@
     def __init__(self, root, controller):
         fm = Tk.Frame(root)
         kernel_group = Tk.Frame(fm)
-        Tk.Radiobutton(kernel_group, text="Linear", variable=controller.kernel,
-                       value=0, command=controller.refit).pack(anchor=Tk.W)
-        Tk.Radiobutton(kernel_group, text="RBF", variable=controller.kernel,
-                       value=1, command=controller.refit).pack(anchor=Tk.W)
-        Tk.Radiobutton(kernel_group, text="Poly", variable=controller.kernel,
-                       value=2, command=controller.refit).pack(anchor=Tk.W)
+        Tk.Radiobutton(
+            kernel_group,
+            text="Linear",
+            variable=controller.kernel,
+            value=0,
+            command=controller.refit,
+        ).pack(anchor=Tk.W)
+        Tk.Radiobutton(
+            kernel_group,
+            text="RBF",
+            variable=controller.kernel,
+            value=1,
+            command=controller.refit,
+        ).pack(anchor=Tk.W)
+        Tk.Radiobutton(
+            kernel_group,
+            text="Poly",
+            variable=controller.kernel,
+            value=2,
+            command=controller.refit,
+        ).pack(anchor=Tk.W)
         kernel_group.pack(side=Tk.LEFT)

         valbox = Tk.Frame(fm)
@@ -257,8 +299,7 @@
         controller.complexity.set("1.0")
         c = Tk.Frame(valbox)
         Tk.Label(c, text="C:", anchor="e", width=7).pack(side=Tk.LEFT)
-        Tk.Entry(c, width=6, textvariable=controller.complexity).pack(
-            side=Tk.LEFT)
+        Tk.Entry(c, width=6, textvariable=controller.complexity).pack(side=Tk.LEFT)
         c.pack()

         controller.gamma = Tk.StringVar()
@@ -284,29 +325,42 @@
         valbox.pack(side=Tk.LEFT)

         cmap_group = Tk.Frame(fm)
-        Tk.Radiobutton(cmap_group, text="Hyperplanes",
-                       variable=controller.surface_type, value=0,
-                       command=controller.refit).pack(anchor=Tk.W)
-        Tk.Radiobutton(cmap_group, text="Surface",
-                       variable=controller.surface_type, value=1,
-                       command=controller.refit).pack(anchor=Tk.W)
+        Tk.Radiobutton(
+            cmap_group,
+            text="Hyperplanes",
+            variable=controller.surface_type,
+            value=0,
+            command=controller.refit,
+        ).pack(anchor=Tk.W)
+        Tk.Radiobutton(
+            cmap_group,
+            text="Surface",
+            variable=controller.surface_type,
+            value=1,
+            command=controller.refit,
+        ).pack(anchor=Tk.W)

         cmap_group.pack(side=Tk.LEFT)

-        train_button = Tk.Button(fm, text='Fit', width=5,
-                                 command=controller.fit)
+        train_button = Tk.Button(fm, text="Fit", width=5, command=controller.fit)
         train_button.pack()
         fm.pack(side=Tk.LEFT)
-        Tk.Button(fm, text='Clear', width=5,
-                  command=controller.clear_data).pack(side=Tk.LEFT)
+        Tk.Button(fm, text="Clear", width=5, command=controller.clear_data).pack(
+            side=Tk.LEFT
+        )


 def get_parser():
     from optparse import OptionParser
+
     op = OptionParser()
-    op.add_option("--output",
-                  action="store", type="str", dest="output",
-                  help="Path where to dump data.")
+    op.add_option(
+        "--output",
+        action="store",
+        type="str",
+        dest="output",
+        help="Path where to dump data.",
+    )
     return op


@@ -324,5 +378,6 @@
     if opts.output:
         model.dump_svmlight_file(opts.output)

+
 if __name__ == "__main__":
     main(sys.argv)
('examples/applications', 'plot_tomography_l1_reconstruction.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -21,21 +21,20 @@
 addition to the data-fidelity term corresponding to a linear regression,
 we penalize the L1 norm of the image to account for its sparsity. The
 resulting optimization problem is called the :ref:`lasso`. We use the
-class :class:`sklearn.linear_model.Lasso`, that uses the coordinate descent
+class :class:`~sklearn.linear_model.Lasso`, that uses the coordinate descent
 algorithm. Importantly, this implementation is more computationally efficient
 on a sparse matrix, than the projection operator used here.

 The reconstruction with L1 penalization gives a result with zero error
 (all pixels are successfully labeled with 0 or 1), even if noise was
 added to the projections. In comparison, an L2 penalization
-(:class:`sklearn.linear_model.Ridge`) produces a large number of labeling
+(:class:`~sklearn.linear_model.Ridge`) produces a large number of labeling
 errors for the pixels. Important artifacts are observed on the
 reconstructed image, contrary to the L1 penalization. Note in particular
 the circular artifact separating the pixels in the corners, that have
 contributed to fewer projections than the central disk.
+
 """
-
-print(__doc__)

 # Author: Emmanuelle Gouillart <emmanuelle.gouillart@nsup.org>
 # License: BSD 3 clause
@@ -57,14 +56,14 @@

 def _generate_center_coordinates(l_x):
     X, Y = np.mgrid[:l_x, :l_x].astype(np.float64)
-    center = l_x / 2.
+    center = l_x / 2.0
     X += 0.5 - center
     Y += 0.5 - center
     return X, Y


 def build_projection_operator(l_x, n_dir):
-    """ Compute the tomography design matrix.
+    """Compute the tomography design matrix.

     Parameters
     ----------
@@ -82,9 +81,8 @@
     X, Y = _generate_center_coordinates(l_x)
     angles = np.linspace(0, np.pi, n_dir, endpoint=False)
     data_inds, weights, camera_inds = [], [], []
-    data_unravel_indices = np.arange(l_x ** 2)
-    data_unravel_indices = np.hstack((data_unravel_indices,
-                                      data_unravel_indices))
+    data_unravel_indices = np.arange(l_x**2)
+    data_unravel_indices = np.hstack((data_unravel_indices, data_unravel_indices))
     for i, angle in enumerate(angles):
         Xrot = np.cos(angle) * X - np.sin(angle) * Y
         inds, w = _weights(Xrot, dx=1, orig=X.min())
@@ -97,14 +95,14 @@


 def generate_synthetic_data():
-    """ Synthetic binary data """
+    """Synthetic binary data"""
     rs = np.random.RandomState(0)
     n_pts = 36
     x, y = np.ogrid[0:l, 0:l]
-    mask_outer = (x - l / 2.) ** 2 + (y - l / 2.) ** 2 < (l / 2.) ** 2
+    mask_outer = (x - l / 2.0) ** 2 + (y - l / 2.0) ** 2 < (l / 2.0) ** 2
     mask = np.zeros((l, l))
     points = l * rs.rand(2, n_pts)
-    mask[(points[0]).astype(np.int), (points[1]).astype(np.int)] = 1
+    mask[(points[0]).astype(int), (points[1]).astype(int)] = 1
     mask = ndimage.gaussian_filter(mask, sigma=l / n_pts)
     res = np.logical_and(mask > mask.mean(), mask_outer)
     return np.logical_xor(res, ndimage.binary_erosion(res))
@@ -114,7 +112,7 @@
 l = 128
 proj_operator = build_projection_operator(l, l // 7)
 data = generate_synthetic_data()
-proj = proj_operator * data.ravel()[:, np.newaxis]
+proj = proj_operator @ data.ravel()[:, np.newaxis]
 proj += 0.15 * np.random.randn(*proj.shape)

 # Reconstruction with L2 (Ridge) penalization
@@ -131,19 +129,18 @@

 plt.figure(figsize=(8, 3.3))
 plt.subplot(131)
-plt.imshow(data, cmap=plt.cm.gray, interpolation='nearest')
-plt.axis('off')
-plt.title('original image')
+plt.imshow(data, cmap=plt.cm.gray, interpolation="nearest")
+plt.axis("off")
+plt.title("original image")
 plt.subplot(132)
-plt.imshow(rec_l2, cmap=plt.cm.gray, interpolation='nearest')
-plt.title('L2 penalization')
-plt.axis('off')
+plt.imshow(rec_l2, cmap=plt.cm.gray, interpolation="nearest")
+plt.title("L2 penalization")
+plt.axis("off")
 plt.subplot(133)
-plt.imshow(rec_l1, cmap=plt.cm.gray, interpolation='nearest')
-plt.title('L1 penalization')
-plt.axis('off')
+plt.imshow(rec_l1, cmap=plt.cm.gray, interpolation="nearest")
+plt.title("L1 penalization")
+plt.axis("off")

-plt.subplots_adjust(hspace=0.01, wspace=0.01, top=1, bottom=0, left=0,
-                    right=1)
+plt.subplots_adjust(hspace=0.01, wspace=0.01, top=1, bottom=0, left=0, right=1)

 plt.show()
('examples/applications', 'plot_topics_extraction_with_nmf_lda.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,11 +3,11 @@
 Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation
 =======================================================================================

-This is an example of applying :class:`sklearn.decomposition.NMF` and
-:class:`sklearn.decomposition.LatentDirichletAllocation` on a corpus
+This is an example of applying :class:`~sklearn.decomposition.NMF` and
+:class:`~sklearn.decomposition.LatentDirichletAllocation` on a corpus
 of documents and extract additive models of the topic structure of the
-corpus.  The output is a list of topics, each represented as a list of
-terms (weights are not shown).
+corpus.  The output is a plot of topics, each represented as bar plot
+using top few words based on weights.

 Non-negative Matrix Factorization is applied with two different objective
 functions: the Frobenius norm, and the generalized Kullback-Leibler divergence.
@@ -27,24 +27,39 @@
 # License: BSD 3 clause

 from time import time
+import matplotlib.pyplot as plt

 from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
-from sklearn.decomposition import NMF, LatentDirichletAllocation
+from sklearn.decomposition import NMF, MiniBatchNMF, LatentDirichletAllocation
 from sklearn.datasets import fetch_20newsgroups

 n_samples = 2000
 n_features = 1000
 n_components = 10
 n_top_words = 20
-
-
-def print_top_words(model, feature_names, n_top_words):
+batch_size = 128
+init = "nndsvda"
+
+
+def plot_top_words(model, feature_names, n_top_words, title):
+    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)
+    axes = axes.flatten()
     for topic_idx, topic in enumerate(model.components_):
-        message = "Topic #%d: " % topic_idx
-        message += " ".join([feature_names[i]
-                             for i in topic.argsort()[:-n_top_words - 1:-1]])
-        print(message)
-    print()
+        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]
+        top_features = [feature_names[i] for i in top_features_ind]
+        weights = topic[top_features_ind]
+
+        ax = axes[topic_idx]
+        ax.barh(top_features, weights, height=0.7)
+        ax.set_title(f"Topic {topic_idx +1}", fontdict={"fontsize": 30})
+        ax.invert_yaxis()
+        ax.tick_params(axis="both", which="major", labelsize=20)
+        for i in "top right left".split():
+            ax.spines[i].set_visible(False)
+        fig.suptitle(title, fontsize=40)
+
+    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)
+    plt.show()


 # Load the 20 newsgroups dataset and vectorize it. We use a few heuristics
@@ -54,68 +69,158 @@

 print("Loading dataset...")
 t0 = time()
-dataset = fetch_20newsgroups(shuffle=True, random_state=1,
-                             remove=('headers', 'footers', 'quotes'))
-data_samples = dataset.data[:n_samples]
+data, _ = fetch_20newsgroups(
+    shuffle=True,
+    random_state=1,
+    remove=("headers", "footers", "quotes"),
+    return_X_y=True,
+)
+data_samples = data[:n_samples]
 print("done in %0.3fs." % (time() - t0))

 # Use tf-idf features for NMF.
 print("Extracting tf-idf features for NMF...")
-tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,
-                                   max_features=n_features,
-                                   stop_words='english')
+tfidf_vectorizer = TfidfVectorizer(
+    max_df=0.95, min_df=2, max_features=n_features, stop_words="english"
+)
 t0 = time()
 tfidf = tfidf_vectorizer.fit_transform(data_samples)
 print("done in %0.3fs." % (time() - t0))

 # Use tf (raw term count) features for LDA.
 print("Extracting tf features for LDA...")
-tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,
-                                max_features=n_features,
-                                stop_words='english')
+tf_vectorizer = CountVectorizer(
+    max_df=0.95, min_df=2, max_features=n_features, stop_words="english"
+)
 t0 = time()
 tf = tf_vectorizer.fit_transform(data_samples)
 print("done in %0.3fs." % (time() - t0))
 print()

 # Fit the NMF model
-print("Fitting the NMF model (Frobenius norm) with tf-idf features, "
-      "n_samples=%d and n_features=%d..."
-      % (n_samples, n_features))
-t0 = time()
-nmf = NMF(n_components=n_components, random_state=1,
-          alpha=.1, l1_ratio=.5).fit(tfidf)
-print("done in %0.3fs." % (time() - t0))
-
-print("\nTopics in NMF model (Frobenius norm):")
-tfidf_feature_names = tfidf_vectorizer.get_feature_names()
-print_top_words(nmf, tfidf_feature_names, n_top_words)
+print(
+    "Fitting the NMF model (Frobenius norm) with tf-idf features, "
+    "n_samples=%d and n_features=%d..." % (n_samples, n_features)
+)
+t0 = time()
+nmf = NMF(
+    n_components=n_components,
+    random_state=1,
+    init=init,
+    beta_loss="frobenius",
+    alpha_W=0.00005,
+    alpha_H=0.00005,
+    l1_ratio=1,
+).fit(tfidf)
+print("done in %0.3fs." % (time() - t0))
+
+
+tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()
+plot_top_words(
+    nmf, tfidf_feature_names, n_top_words, "Topics in NMF model (Frobenius norm)"
+)

 # Fit the NMF model
-print("Fitting the NMF model (generalized Kullback-Leibler divergence) with "
-      "tf-idf features, n_samples=%d and n_features=%d..."
-      % (n_samples, n_features))
-t0 = time()
-nmf = NMF(n_components=n_components, random_state=1,
-          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,
-          l1_ratio=.5).fit(tfidf)
-print("done in %0.3fs." % (time() - t0))
-
-print("\nTopics in NMF model (generalized Kullback-Leibler divergence):")
-tfidf_feature_names = tfidf_vectorizer.get_feature_names()
-print_top_words(nmf, tfidf_feature_names, n_top_words)
-
-print("Fitting LDA models with tf features, "
-      "n_samples=%d and n_features=%d..."
-      % (n_samples, n_features))
-lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,
-                                learning_method='online',
-                                learning_offset=50.,
-                                random_state=0)
+print(
+    "\n" * 2,
+    "Fitting the NMF model (generalized Kullback-Leibler "
+    "divergence) with tf-idf features, n_samples=%d and n_features=%d..."
+    % (n_samples, n_features),
+)
+t0 = time()
+nmf = NMF(
+    n_components=n_components,
+    random_state=1,
+    init=init,
+    beta_loss="kullback-leibler",
+    solver="mu",
+    max_iter=1000,
+    alpha_W=0.00005,
+    alpha_H=0.00005,
+    l1_ratio=0.5,
+).fit(tfidf)
+print("done in %0.3fs." % (time() - t0))
+
+tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()
+plot_top_words(
+    nmf,
+    tfidf_feature_names,
+    n_top_words,
+    "Topics in NMF model (generalized Kullback-Leibler divergence)",
+)
+
+# Fit the MiniBatchNMF model
+print(
+    "\n" * 2,
+    "Fitting the MiniBatchNMF model (Frobenius norm) with tf-idf "
+    "features, n_samples=%d and n_features=%d, batch_size=%d..."
+    % (n_samples, n_features, batch_size),
+)
+t0 = time()
+mbnmf = MiniBatchNMF(
+    n_components=n_components,
+    random_state=1,
+    batch_size=batch_size,
+    init=init,
+    beta_loss="frobenius",
+    alpha_W=0.00005,
+    alpha_H=0.00005,
+    l1_ratio=0.5,
+).fit(tfidf)
+print("done in %0.3fs." % (time() - t0))
+
+
+tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()
+plot_top_words(
+    mbnmf,
+    tfidf_feature_names,
+    n_top_words,
+    "Topics in MiniBatchNMF model (Frobenius norm)",
+)
+
+# Fit the MiniBatchNMF model
+print(
+    "\n" * 2,
+    "Fitting the MiniBatchNMF model (generalized Kullback-Leibler "
+    "divergence) with tf-idf features, n_samples=%d and n_features=%d, "
+    "batch_size=%d..." % (n_samples, n_features, batch_size),
+)
+t0 = time()
+mbnmf = MiniBatchNMF(
+    n_components=n_components,
+    random_state=1,
+    batch_size=batch_size,
+    init=init,
+    beta_loss="kullback-leibler",
+    alpha_W=0.00005,
+    alpha_H=0.00005,
+    l1_ratio=0.5,
+).fit(tfidf)
+print("done in %0.3fs." % (time() - t0))
+
+tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()
+plot_top_words(
+    mbnmf,
+    tfidf_feature_names,
+    n_top_words,
+    "Topics in MiniBatchNMF model (generalized Kullback-Leibler divergence)",
+)
+
+print(
+    "\n" * 2,
+    "Fitting LDA models with tf features, n_samples=%d and n_features=%d..."
+    % (n_samples, n_features),
+)
+lda = LatentDirichletAllocation(
+    n_components=n_components,
+    max_iter=5,
+    learning_method="online",
+    learning_offset=50.0,
+    random_state=0,
+)
 t0 = time()
 lda.fit(tf)
 print("done in %0.3fs." % (time() - t0))

-print("\nTopics in LDA model:")
-tf_feature_names = tf_vectorizer.get_feature_names()
-print_top_words(lda, tf_feature_names, n_top_words)
+tf_feature_names = tf_vectorizer.get_feature_names_out()
+plot_top_words(lda, tf_feature_names, n_top_words, "Topics in LDA model")
('examples/mixture', 'plot_concentration_prior.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -26,7 +26,9 @@
 On the contrary the classical finite mixture model with a Dirichlet
 distribution prior will favor more uniformly weighted components and therefore
 tends to divide natural clusters into unnecessary sub-components.
+
 """
+
 # Author: Thierry Guillemot <thierry.guillemot.work@gmail.com>
 # License: BSD 3 clause

@@ -36,8 +38,6 @@
 import matplotlib.gridspec as gridspec

 from sklearn.mixture import BayesianGaussianMixture
-
-print(__doc__)


 def plot_ellipses(ax, weights, means, covars):
@@ -49,88 +49,116 @@
         angle = 180 * angle / np.pi
         # eigenvector normalization
         eig_vals = 2 * np.sqrt(2) * np.sqrt(eig_vals)
-        ell = mpl.patches.Ellipse(means[n], eig_vals[0], eig_vals[1],
-                                  180 + angle, edgecolor='black')
+        ell = mpl.patches.Ellipse(
+            means[n], eig_vals[0], eig_vals[1], 180 + angle, edgecolor="black"
+        )
         ell.set_clip_box(ax.bbox)
         ell.set_alpha(weights[n])
-        ell.set_facecolor('#56B4E9')
+        ell.set_facecolor("#56B4E9")
         ax.add_artist(ell)


 def plot_results(ax1, ax2, estimator, X, y, title, plot_title=False):
     ax1.set_title(title)
-    ax1.scatter(X[:, 0], X[:, 1], s=5, marker='o', color=colors[y], alpha=0.8)
-    ax1.set_xlim(-2., 2.)
-    ax1.set_ylim(-3., 3.)
+    ax1.scatter(X[:, 0], X[:, 1], s=5, marker="o", color=colors[y], alpha=0.8)
+    ax1.set_xlim(-2.0, 2.0)
+    ax1.set_ylim(-3.0, 3.0)
     ax1.set_xticks(())
     ax1.set_yticks(())
-    plot_ellipses(ax1, estimator.weights_, estimator.means_,
-                  estimator.covariances_)
+    plot_ellipses(ax1, estimator.weights_, estimator.means_, estimator.covariances_)

-    ax2.get_xaxis().set_tick_params(direction='out')
+    ax2.get_xaxis().set_tick_params(direction="out")
     ax2.yaxis.grid(True, alpha=0.7)
     for k, w in enumerate(estimator.weights_):
-        ax2.bar(k, w, width=0.9, color='#56B4E9', zorder=3,
-                align='center', edgecolor='black')
-        ax2.text(k, w + 0.007, "%.1f%%" % (w * 100.),
-                 horizontalalignment='center')
-    ax2.set_xlim(-.6, 2 * n_components - .4)
-    ax2.set_ylim(0., 1.1)
-    ax2.tick_params(axis='y', which='both', left=False,
-                    right=False, labelleft=False)
-    ax2.tick_params(axis='x', which='both', top=False)
+        ax2.bar(
+            k,
+            w,
+            width=0.9,
+            color="#56B4E9",
+            zorder=3,
+            align="center",
+            edgecolor="black",
+        )
+        ax2.text(k, w + 0.007, "%.1f%%" % (w * 100.0), horizontalalignment="center")
+    ax2.set_xlim(-0.6, 2 * n_components - 0.4)
+    ax2.set_ylim(0.0, 1.1)
+    ax2.tick_params(axis="y", which="both", left=False, right=False, labelleft=False)
+    ax2.tick_params(axis="x", which="both", top=False)

     if plot_title:
-        ax1.set_ylabel('Estimated Mixtures')
-        ax2.set_ylabel('Weight of each component')
+        ax1.set_ylabel("Estimated Mixtures")
+        ax2.set_ylabel("Weight of each component")
+

 # Parameters of the dataset
 random_state, n_components, n_features = 2, 3, 2
-colors = np.array(['#0072B2', '#F0E442', '#D55E00'])
+colors = np.array(["#0072B2", "#F0E442", "#D55E00"])

-covars = np.array([[[.7, .0], [.0, .1]],
-                   [[.5, .0], [.0, .1]],
-                   [[.5, .0], [.0, .1]]])
+covars = np.array(
+    [[[0.7, 0.0], [0.0, 0.1]], [[0.5, 0.0], [0.0, 0.1]], [[0.5, 0.0], [0.0, 0.1]]]
+)
 samples = np.array([200, 500, 200])
-means = np.array([[.0, -.70],
-                  [.0, .0],
-                  [.0, .70]])
+means = np.array([[0.0, -0.70], [0.0, 0.0], [0.0, 0.70]])

 # mean_precision_prior= 0.8 to minimize the influence of the prior
 estimators = [
-    ("Finite mixture with a Dirichlet distribution\nprior and "
-     r"$\gamma_0=$", BayesianGaussianMixture(
-        weight_concentration_prior_type="dirichlet_distribution",
-        n_components=2 * n_components, reg_covar=0, init_params='random',
-        max_iter=1500, mean_precision_prior=.8,
-        random_state=random_state), [0.001, 1, 1000]),
-    ("Infinite mixture with a Dirichlet process\n prior and" r"$\gamma_0=$",
-     BayesianGaussianMixture(
-        weight_concentration_prior_type="dirichlet_process",
-        n_components=2 * n_components, reg_covar=0, init_params='random',
-        max_iter=1500, mean_precision_prior=.8,
-        random_state=random_state), [1, 1000, 100000])]
+    (
+        "Finite mixture with a Dirichlet distribution\nprior and " r"$\gamma_0=$",
+        BayesianGaussianMixture(
+            weight_concentration_prior_type="dirichlet_distribution",
+            n_components=2 * n_components,
+            reg_covar=0,
+            init_params="random",
+            max_iter=1500,
+            mean_precision_prior=0.8,
+            random_state=random_state,
+        ),
+        [0.001, 1, 1000],
+    ),
+    (
+        "Infinite mixture with a Dirichlet process\n prior and" r"$\gamma_0=$",
+        BayesianGaussianMixture(
+            weight_concentration_prior_type="dirichlet_process",
+            n_components=2 * n_components,
+            reg_covar=0,
+            init_params="random",
+            max_iter=1500,
+            mean_precision_prior=0.8,
+            random_state=random_state,
+        ),
+        [1, 1000, 100000],
+    ),
+]

 # Generate data
 rng = np.random.RandomState(random_state)
-X = np.vstack([
-    rng.multivariate_normal(means[j], covars[j], samples[j])
-    for j in range(n_components)])
-y = np.concatenate([np.full(samples[j], j, dtype=int)
-                    for j in range(n_components)])
+X = np.vstack(
+    [
+        rng.multivariate_normal(means[j], covars[j], samples[j])
+        for j in range(n_components)
+    ]
+)
+y = np.concatenate([np.full(samples[j], j, dtype=int) for j in range(n_components)])

 # Plot results in two different figures
-for (title, estimator, concentrations_prior) in estimators:
+for title, estimator, concentrations_prior in estimators:
     plt.figure(figsize=(4.7 * 3, 8))
-    plt.subplots_adjust(bottom=.04, top=0.90, hspace=.05, wspace=.05,
-                        left=.03, right=.99)
+    plt.subplots_adjust(
+        bottom=0.04, top=0.90, hspace=0.05, wspace=0.05, left=0.03, right=0.99
+    )

     gs = gridspec.GridSpec(3, len(concentrations_prior))
     for k, concentration in enumerate(concentrations_prior):
         estimator.weight_concentration_prior = concentration
         estimator.fit(X)
-        plot_results(plt.subplot(gs[0:2, k]), plt.subplot(gs[2, k]), estimator,
-                     X, y, r"%s$%.1e$" % (title, concentration),
-                     plot_title=k == 0)
+        plot_results(
+            plt.subplot(gs[0:2, k]),
+            plt.subplot(gs[2, k]),
+            estimator,
+            X,
+            y,
+            r"%s$%.1e$" % (title, concentration),
+            plot_title=k == 0,
+        )

 plt.show()
('examples/mixture', 'plot_gmm_covariances.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -24,6 +24,7 @@
 crosses. The iris dataset is four-dimensional. Only the first two
 dimensions are shown here, and thus some points are separated in other
 dimensions.
+
 """

 # Author: Ron Weiss <ronweiss@gmail.com>, Gael Varoquaux
@@ -39,32 +40,32 @@
 from sklearn.mixture import GaussianMixture
 from sklearn.model_selection import StratifiedKFold

-print(__doc__)
-
-colors = ['navy', 'turquoise', 'darkorange']
+colors = ["navy", "turquoise", "darkorange"]


 def make_ellipses(gmm, ax):
     for n, color in enumerate(colors):
-        if gmm.covariance_type == 'full':
+        if gmm.covariance_type == "full":
             covariances = gmm.covariances_[n][:2, :2]
-        elif gmm.covariance_type == 'tied':
+        elif gmm.covariance_type == "tied":
             covariances = gmm.covariances_[:2, :2]
-        elif gmm.covariance_type == 'diag':
+        elif gmm.covariance_type == "diag":
             covariances = np.diag(gmm.covariances_[n][:2])
-        elif gmm.covariance_type == 'spherical':
+        elif gmm.covariance_type == "spherical":
             covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]
         v, w = np.linalg.eigh(covariances)
         u = w[0] / np.linalg.norm(w[0])
         angle = np.arctan2(u[1], u[0])
         angle = 180 * angle / np.pi  # convert to degrees
-        v = 2. * np.sqrt(2.) * np.sqrt(v)
-        ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1],
-                                  180 + angle, color=color)
+        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)
+        ell = mpl.patches.Ellipse(
+            gmm.means_[n, :2], v[0], v[1], 180 + angle, color=color
+        )
         ell.set_clip_box(ax.bbox)
         ell.set_alpha(0.5)
         ax.add_artist(ell)
-        ax.set_aspect('equal', 'datalim')
+        ax.set_aspect("equal", "datalim")
+

 iris = datasets.load_iris()

@@ -83,22 +84,27 @@
 n_classes = len(np.unique(y_train))

 # Try GMMs using different types of covariances.
-estimators = {cov_type: GaussianMixture(n_components=n_classes,
-              covariance_type=cov_type, max_iter=20, random_state=0)
-              for cov_type in ['spherical', 'diag', 'tied', 'full']}
+estimators = {
+    cov_type: GaussianMixture(
+        n_components=n_classes, covariance_type=cov_type, max_iter=20, random_state=0
+    )
+    for cov_type in ["spherical", "diag", "tied", "full"]
+}

 n_estimators = len(estimators)

 plt.figure(figsize=(3 * n_estimators // 2, 6))
-plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,
-                    left=.01, right=.99)
+plt.subplots_adjust(
+    bottom=0.01, top=0.95, hspace=0.15, wspace=0.05, left=0.01, right=0.99
+)


 for index, (name, estimator) in enumerate(estimators.items()):
     # Since we have class labels for the training data, we can
     # initialize the GMM parameters in a supervised manner.
-    estimator.means_init = np.array([X_train[y_train == i].mean(axis=0)
-                                    for i in range(n_classes)])
+    estimator.means_init = np.array(
+        [X_train[y_train == i].mean(axis=0) for i in range(n_classes)]
+    )

     # Train the other parameters using the EM algorithm.
     estimator.fit(X_train)
@@ -108,28 +114,27 @@

     for n, color in enumerate(colors):
         data = iris.data[iris.target == n]
-        plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color,
-                    label=iris.target_names[n])
+        plt.scatter(
+            data[:, 0], data[:, 1], s=0.8, color=color, label=iris.target_names[n]
+        )
     # Plot the test data with crosses
     for n, color in enumerate(colors):
         data = X_test[y_test == n]
-        plt.scatter(data[:, 0], data[:, 1], marker='x', color=color)
+        plt.scatter(data[:, 0], data[:, 1], marker="x", color=color)

     y_train_pred = estimator.predict(X_train)
     train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100
-    plt.text(0.05, 0.9, 'Train accuracy: %.1f' % train_accuracy,
-             transform=h.transAxes)
+    plt.text(0.05, 0.9, "Train accuracy: %.1f" % train_accuracy, transform=h.transAxes)

     y_test_pred = estimator.predict(X_test)
     test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100
-    plt.text(0.05, 0.8, 'Test accuracy: %.1f' % test_accuracy,
-             transform=h.transAxes)
+    plt.text(0.05, 0.8, "Test accuracy: %.1f" % test_accuracy, transform=h.transAxes)

     plt.xticks(())
     plt.yticks(())
     plt.title(name)

-plt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))
+plt.legend(scatterpoints=1, loc="lower right", prop=dict(size=12))


 plt.show()
('examples/mixture', 'plot_gmm.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -21,6 +21,7 @@
 full covariance matrices effectively even when there are less examples
 per cluster than there are dimensions in the data, due to
 regularization properties of the inference algorithm.
+
 """

 import itertools
@@ -32,34 +33,32 @@

 from sklearn import mixture

-color_iter = itertools.cycle(['navy', 'c', 'cornflowerblue', 'gold',
-                              'darkorange'])
+color_iter = itertools.cycle(["navy", "c", "cornflowerblue", "gold", "darkorange"])


 def plot_results(X, Y_, means, covariances, index, title):
     splot = plt.subplot(2, 1, 1 + index)
-    for i, (mean, covar, color) in enumerate(zip(
-            means, covariances, color_iter)):
+    for i, (mean, covar, color) in enumerate(zip(means, covariances, color_iter)):
         v, w = linalg.eigh(covar)
-        v = 2. * np.sqrt(2.) * np.sqrt(v)
+        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)
         u = w[0] / linalg.norm(w[0])
         # as the DP will not use every component it has access to
         # unless it needs it, we shouldn't plot the redundant
         # components.
         if not np.any(Y_ == i):
             continue
-        plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)
+        plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], 0.8, color=color)

         # Plot an ellipse to show the Gaussian component
         angle = np.arctan(u[1] / u[0])
-        angle = 180. * angle / np.pi  # convert to degrees
-        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)
+        angle = 180.0 * angle / np.pi  # convert to degrees
+        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180.0 + angle, color=color)
         ell.set_clip_box(splot.bbox)
         ell.set_alpha(0.5)
         splot.add_artist(ell)

-    plt.xlim(-9., 5.)
-    plt.ylim(-3., 6.)
+    plt.xlim(-9.0, 5.0)
+    plt.ylim(-3.0, 6.0)
     plt.xticks(())
     plt.yticks(())
     plt.title(title)
@@ -70,19 +69,25 @@

 # Generate random sample, two components
 np.random.seed(0)
-C = np.array([[0., -0.1], [1.7, .4]])
-X = np.r_[np.dot(np.random.randn(n_samples, 2), C),
-          .7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]
+C = np.array([[0.0, -0.1], [1.7, 0.4]])
+X = np.r_[
+    np.dot(np.random.randn(n_samples, 2), C),
+    0.7 * np.random.randn(n_samples, 2) + np.array([-6, 3]),
+]

 # Fit a Gaussian mixture with EM using five components
-gmm = mixture.GaussianMixture(n_components=5, covariance_type='full').fit(X)
-plot_results(X, gmm.predict(X), gmm.means_, gmm.covariances_, 0,
-             'Gaussian Mixture')
+gmm = mixture.GaussianMixture(n_components=5, covariance_type="full").fit(X)
+plot_results(X, gmm.predict(X), gmm.means_, gmm.covariances_, 0, "Gaussian Mixture")

 # Fit a Dirichlet process Gaussian mixture using five components
-dpgmm = mixture.BayesianGaussianMixture(n_components=5,
-                                        covariance_type='full').fit(X)
-plot_results(X, dpgmm.predict(X), dpgmm.means_, dpgmm.covariances_, 1,
-             'Bayesian Gaussian Mixture with a Dirichlet process prior')
+dpgmm = mixture.BayesianGaussianMixture(n_components=5, covariance_type="full").fit(X)
+plot_results(
+    X,
+    dpgmm.predict(X),
+    dpgmm.means_,
+    dpgmm.covariances_,
+    1,
+    "Bayesian Gaussian Mixture with a Dirichlet process prior",
+)

 plt.show()
('examples/mixture', 'plot_gmm_selection.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,7 +4,7 @@
 ================================

 This example shows that model selection can be performed with
-Gaussian Mixture Models using information-theoretic criteria (BIC).
+Gaussian Mixture Models using :ref:`information-theoretic criteria (BIC) <aic_bic>`.
 Model selection concerns both the covariance type
 and the number of components in the model.
 In that case, AIC also provides the right result (not shown to save time),
@@ -13,6 +13,7 @@

 In that case, the model with 2 components and full covariance
 (which corresponds to the true generative model) is selected.
+
 """

 import numpy as np
@@ -24,26 +25,27 @@

 from sklearn import mixture

-print(__doc__)
-
 # Number of samples per component
 n_samples = 500

 # Generate random sample, two components
 np.random.seed(0)
-C = np.array([[0., -0.1], [1.7, .4]])
-X = np.r_[np.dot(np.random.randn(n_samples, 2), C),
-          .7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]
+C = np.array([[0.0, -0.1], [1.7, 0.4]])
+X = np.r_[
+    np.dot(np.random.randn(n_samples, 2), C),
+    0.7 * np.random.randn(n_samples, 2) + np.array([-6, 3]),
+]

 lowest_bic = np.infty
 bic = []
 n_components_range = range(1, 7)
-cv_types = ['spherical', 'tied', 'diag', 'full']
+cv_types = ["spherical", "tied", "diag", "full"]
 for cv_type in cv_types:
     for n_components in n_components_range:
         # Fit a Gaussian mixture with EM
-        gmm = mixture.GaussianMixture(n_components=n_components,
-                                      covariance_type=cv_type)
+        gmm = mixture.GaussianMixture(
+            n_components=n_components, covariance_type=cv_type
+        )
         gmm.fit(X)
         bic.append(gmm.bic(X))
         if bic[-1] < lowest_bic:
@@ -51,8 +53,7 @@
             best_gmm = gmm

 bic = np.array(bic)
-color_iter = itertools.cycle(['navy', 'turquoise', 'cornflowerblue',
-                              'darkorange'])
+color_iter = itertools.cycle(["navy", "turquoise", "cornflowerblue", "darkorange"])
 clf = best_gmm
 bars = []

@@ -60,40 +61,50 @@
 plt.figure(figsize=(8, 6))
 spl = plt.subplot(2, 1, 1)
 for i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):
-    xpos = np.array(n_components_range) + .2 * (i - 2)
-    bars.append(plt.bar(xpos, bic[i * len(n_components_range):
-                                  (i + 1) * len(n_components_range)],
-                        width=.2, color=color))
+    xpos = np.array(n_components_range) + 0.2 * (i - 2)
+    bars.append(
+        plt.bar(
+            xpos,
+            bic[i * len(n_components_range) : (i + 1) * len(n_components_range)],
+            width=0.2,
+            color=color,
+        )
+    )
 plt.xticks(n_components_range)
-plt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])
-plt.title('BIC score per model')
-xpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\
-    .2 * np.floor(bic.argmin() / len(n_components_range))
-plt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)
-spl.set_xlabel('Number of components')
+plt.ylim([bic.min() * 1.01 - 0.01 * bic.max(), bic.max()])
+plt.title("BIC score per model")
+xpos = (
+    np.mod(bic.argmin(), len(n_components_range))
+    + 0.65
+    + 0.2 * np.floor(bic.argmin() / len(n_components_range))
+)
+plt.text(xpos, bic.min() * 0.97 + 0.03 * bic.max(), "*", fontsize=14)
+spl.set_xlabel("Number of components")
 spl.legend([b[0] for b in bars], cv_types)

 # Plot the winner
 splot = plt.subplot(2, 1, 2)
 Y_ = clf.predict(X)
-for i, (mean, cov, color) in enumerate(zip(clf.means_, clf.covariances_,
-                                           color_iter)):
+for i, (mean, cov, color) in enumerate(zip(clf.means_, clf.covariances_, color_iter)):
     v, w = linalg.eigh(cov)
     if not np.any(Y_ == i):
         continue
-    plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)
+    plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], 0.8, color=color)

     # Plot an ellipse to show the Gaussian component
     angle = np.arctan2(w[0][1], w[0][0])
-    angle = 180. * angle / np.pi  # convert to degrees
-    v = 2. * np.sqrt(2.) * np.sqrt(v)
-    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)
+    angle = 180.0 * angle / np.pi  # convert to degrees
+    v = 2.0 * np.sqrt(2.0) * np.sqrt(v)
+    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180.0 + angle, color=color)
     ell.set_clip_box(splot.bbox)
-    ell.set_alpha(.5)
+    ell.set_alpha(0.5)
     splot.add_artist(ell)

 plt.xticks(())
 plt.yticks(())
-plt.title('Selected GMM: full model, 2 components')
-plt.subplots_adjust(hspace=.35, bottom=.02)
+plt.title(
+    f"Selected GMM: {best_gmm.covariance_type} model, "
+    f"{best_gmm.n_components} components"
+)
+plt.subplots_adjust(hspace=0.35, bottom=0.02)
 plt.show()
('examples/mixture', 'plot_gmm_pdf.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,6 +6,7 @@
 Plot the density estimation of a mixture of two Gaussians. Data is
 generated from two Gaussians with different centers and covariance
 matrices.
+
 """

 import numpy as np
@@ -22,29 +23,30 @@
 shifted_gaussian = np.random.randn(n_samples, 2) + np.array([20, 20])

 # generate zero centered stretched Gaussian data
-C = np.array([[0., -0.7], [3.5, .7]])
+C = np.array([[0.0, -0.7], [3.5, 0.7]])
 stretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)

 # concatenate the two datasets into the final training set
 X_train = np.vstack([shifted_gaussian, stretched_gaussian])

 # fit a Gaussian Mixture Model with two components
-clf = mixture.GaussianMixture(n_components=2, covariance_type='full')
+clf = mixture.GaussianMixture(n_components=2, covariance_type="full")
 clf.fit(X_train)

 # display predicted scores by the model as a contour plot
-x = np.linspace(-20., 30.)
-y = np.linspace(-20., 40.)
+x = np.linspace(-20.0, 30.0)
+y = np.linspace(-20.0, 40.0)
 X, Y = np.meshgrid(x, y)
 XX = np.array([X.ravel(), Y.ravel()]).T
 Z = -clf.score_samples(XX)
 Z = Z.reshape(X.shape)

-CS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0),
-                 levels=np.logspace(0, 3, 10))
-CB = plt.colorbar(CS, shrink=0.8, extend='both')
-plt.scatter(X_train[:, 0], X_train[:, 1], .8)
+CS = plt.contour(
+    X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0), levels=np.logspace(0, 3, 10)
+)
+CB = plt.colorbar(CS, shrink=0.8, extend="both")
+plt.scatter(X_train[:, 0], X_train[:, 1], 0.8)

-plt.title('Negative log-likelihood predicted by a GMM')
-plt.axis('tight')
+plt.title("Negative log-likelihood predicted by a GMM")
+plt.axis("tight")
 plt.show()
('examples/mixture', 'plot_gmm_sin.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -26,7 +26,7 @@
 similar to the first model where we arbitrarily decided to fix the number of
 components to 10.

-Which model is the best is a matter of subjective judgement: do we want to
+Which model is the best is a matter of subjective judgment: do we want to
 favor models that only capture the big picture to summarize and explain most of
 the structure of the data while ignoring the details or do we prefer models
 that closely follow the high density regions of the signal?
@@ -48,36 +48,32 @@

 from sklearn import mixture

-print(__doc__)
-
-color_iter = itertools.cycle(['navy', 'c', 'cornflowerblue', 'gold',
-                              'darkorange'])
+color_iter = itertools.cycle(["navy", "c", "cornflowerblue", "gold", "darkorange"])


 def plot_results(X, Y, means, covariances, index, title):
     splot = plt.subplot(5, 1, 1 + index)
-    for i, (mean, covar, color) in enumerate(zip(
-            means, covariances, color_iter)):
+    for i, (mean, covar, color) in enumerate(zip(means, covariances, color_iter)):
         v, w = linalg.eigh(covar)
-        v = 2. * np.sqrt(2.) * np.sqrt(v)
+        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)
         u = w[0] / linalg.norm(w[0])
         # as the DP will not use every component it has access to
         # unless it needs it, we shouldn't plot the redundant
         # components.
         if not np.any(Y == i):
             continue
-        plt.scatter(X[Y == i, 0], X[Y == i, 1], .8, color=color)
+        plt.scatter(X[Y == i, 0], X[Y == i, 1], 0.8, color=color)

         # Plot an ellipse to show the Gaussian component
         angle = np.arctan(u[1] / u[0])
-        angle = 180. * angle / np.pi  # convert to degrees
-        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)
+        angle = 180.0 * angle / np.pi  # convert to degrees
+        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180.0 + angle, color=color)
         ell.set_clip_box(splot.bbox)
         ell.set_alpha(0.5)
         splot.add_artist(ell)

-    plt.xlim(-6., 4. * np.pi - 6.)
-    plt.ylim(-5., 5.)
+    plt.xlim(-6.0, 4.0 * np.pi - 6.0)
+    plt.ylim(-5.0, 5.0)
     plt.title(title)
     plt.xticks(())
     plt.yticks(())
@@ -91,10 +87,10 @@
         # components.
         if not np.any(Y == i):
             continue
-        plt.scatter(X[Y == i, 0], X[Y == i, 1], .8, color=color)
+        plt.scatter(X[Y == i, 0], X[Y == i, 1], 0.8, color=color)

-    plt.xlim(-6., 4. * np.pi - 6.)
-    plt.ylim(-5., 5.)
+    plt.xlim(-6.0, 4.0 * np.pi - 6.0)
+    plt.ylim(-5.0, 5.0)
     plt.title(title)
     plt.xticks(())
     plt.yticks(())
@@ -106,49 +102,86 @@
 # Generate random sample following a sine curve
 np.random.seed(0)
 X = np.zeros((n_samples, 2))
-step = 4. * np.pi / n_samples
+step = 4.0 * np.pi / n_samples

 for i in range(X.shape[0]):
-    x = i * step - 6.
+    x = i * step - 6.0
     X[i, 0] = x + np.random.normal(0, 0.1)
-    X[i, 1] = 3. * (np.sin(x) + np.random.normal(0, .2))
+    X[i, 1] = 3.0 * (np.sin(x) + np.random.normal(0, 0.2))

 plt.figure(figsize=(10, 10))
-plt.subplots_adjust(bottom=.04, top=0.95, hspace=.2, wspace=.05,
-                    left=.03, right=.97)
+plt.subplots_adjust(
+    bottom=0.04, top=0.95, hspace=0.2, wspace=0.05, left=0.03, right=0.97
+)

 # Fit a Gaussian mixture with EM using ten components
-gmm = mixture.GaussianMixture(n_components=10, covariance_type='full',
-                              max_iter=100).fit(X)
-plot_results(X, gmm.predict(X), gmm.means_, gmm.covariances_, 0,
-             'Expectation-maximization')
+gmm = mixture.GaussianMixture(
+    n_components=10, covariance_type="full", max_iter=100
+).fit(X)
+plot_results(
+    X, gmm.predict(X), gmm.means_, gmm.covariances_, 0, "Expectation-maximization"
+)

 dpgmm = mixture.BayesianGaussianMixture(
-    n_components=10, covariance_type='full', weight_concentration_prior=1e-2,
-    weight_concentration_prior_type='dirichlet_process',
-    mean_precision_prior=1e-2, covariance_prior=1e0 * np.eye(2),
-    init_params="random", max_iter=100, random_state=2).fit(X)
-plot_results(X, dpgmm.predict(X), dpgmm.means_, dpgmm.covariances_, 1,
-             "Bayesian Gaussian mixture models with a Dirichlet process prior "
-             r"for $\gamma_0=0.01$.")
+    n_components=10,
+    covariance_type="full",
+    weight_concentration_prior=1e-2,
+    weight_concentration_prior_type="dirichlet_process",
+    mean_precision_prior=1e-2,
+    covariance_prior=1e0 * np.eye(2),
+    init_params="random",
+    max_iter=100,
+    random_state=2,
+).fit(X)
+plot_results(
+    X,
+    dpgmm.predict(X),
+    dpgmm.means_,
+    dpgmm.covariances_,
+    1,
+    "Bayesian Gaussian mixture models with a Dirichlet process prior "
+    r"for $\gamma_0=0.01$.",
+)

 X_s, y_s = dpgmm.sample(n_samples=2000)
-plot_samples(X_s, y_s, dpgmm.n_components, 0,
-             "Gaussian mixture with a Dirichlet process prior "
-             r"for $\gamma_0=0.01$ sampled with $2000$ samples.")
+plot_samples(
+    X_s,
+    y_s,
+    dpgmm.n_components,
+    0,
+    "Gaussian mixture with a Dirichlet process prior "
+    r"for $\gamma_0=0.01$ sampled with $2000$ samples.",
+)

 dpgmm = mixture.BayesianGaussianMixture(
-    n_components=10, covariance_type='full', weight_concentration_prior=1e+2,
-    weight_concentration_prior_type='dirichlet_process',
-    mean_precision_prior=1e-2, covariance_prior=1e0 * np.eye(2),
-    init_params="kmeans", max_iter=100, random_state=2).fit(X)
-plot_results(X, dpgmm.predict(X), dpgmm.means_, dpgmm.covariances_, 2,
-             "Bayesian Gaussian mixture models with a Dirichlet process prior "
-             r"for $\gamma_0=100$")
+    n_components=10,
+    covariance_type="full",
+    weight_concentration_prior=1e2,
+    weight_concentration_prior_type="dirichlet_process",
+    mean_precision_prior=1e-2,
+    covariance_prior=1e0 * np.eye(2),
+    init_params="kmeans",
+    max_iter=100,
+    random_state=2,
+).fit(X)
+plot_results(
+    X,
+    dpgmm.predict(X),
+    dpgmm.means_,
+    dpgmm.covariances_,
+    2,
+    "Bayesian Gaussian mixture models with a Dirichlet process prior "
+    r"for $\gamma_0=100$",
+)

 X_s, y_s = dpgmm.sample(n_samples=2000)
-plot_samples(X_s, y_s, dpgmm.n_components, 1,
-             "Gaussian mixture with a Dirichlet process prior "
-             r"for $\gamma_0=100$ sampled with $2000$ samples.")
+plot_samples(
+    X_s,
+    y_s,
+    dpgmm.n_components,
+    1,
+    "Gaussian mixture with a Dirichlet process prior "
+    r"for $\gamma_0=100$ sampled with $2000$ samples.",
+)

 plt.show()
('examples/neural_networks', 'plot_mlp_training_curves.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -11,47 +11,89 @@

 Note that those results can be highly dependent on the value of
 ``learning_rate_init``.
+
 """

-print(__doc__)
+import warnings
+
 import matplotlib.pyplot as plt
+
 from sklearn.neural_network import MLPClassifier
 from sklearn.preprocessing import MinMaxScaler
 from sklearn import datasets
+from sklearn.exceptions import ConvergenceWarning

 # different learning rate schedules and momentum parameters
-params = [{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0,
-           'learning_rate_init': 0.2},
-          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,
-           'nesterovs_momentum': False, 'learning_rate_init': 0.2},
-          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,
-           'nesterovs_momentum': True, 'learning_rate_init': 0.2},
-          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0,
-           'learning_rate_init': 0.2},
-          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,
-           'nesterovs_momentum': True, 'learning_rate_init': 0.2},
-          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,
-           'nesterovs_momentum': False, 'learning_rate_init': 0.2},
-          {'solver': 'adam', 'learning_rate_init': 0.01}]
+params = [
+    {
+        "solver": "sgd",
+        "learning_rate": "constant",
+        "momentum": 0,
+        "learning_rate_init": 0.2,
+    },
+    {
+        "solver": "sgd",
+        "learning_rate": "constant",
+        "momentum": 0.9,
+        "nesterovs_momentum": False,
+        "learning_rate_init": 0.2,
+    },
+    {
+        "solver": "sgd",
+        "learning_rate": "constant",
+        "momentum": 0.9,
+        "nesterovs_momentum": True,
+        "learning_rate_init": 0.2,
+    },
+    {
+        "solver": "sgd",
+        "learning_rate": "invscaling",
+        "momentum": 0,
+        "learning_rate_init": 0.2,
+    },
+    {
+        "solver": "sgd",
+        "learning_rate": "invscaling",
+        "momentum": 0.9,
+        "nesterovs_momentum": True,
+        "learning_rate_init": 0.2,
+    },
+    {
+        "solver": "sgd",
+        "learning_rate": "invscaling",
+        "momentum": 0.9,
+        "nesterovs_momentum": False,
+        "learning_rate_init": 0.2,
+    },
+    {"solver": "adam", "learning_rate_init": 0.01},
+]

-labels = ["constant learning-rate", "constant with momentum",
-          "constant with Nesterov's momentum",
-          "inv-scaling learning-rate", "inv-scaling with momentum",
-          "inv-scaling with Nesterov's momentum", "adam"]
+labels = [
+    "constant learning-rate",
+    "constant with momentum",
+    "constant with Nesterov's momentum",
+    "inv-scaling learning-rate",
+    "inv-scaling with momentum",
+    "inv-scaling with Nesterov's momentum",
+    "adam",
+]

-plot_args = [{'c': 'red', 'linestyle': '-'},
-             {'c': 'green', 'linestyle': '-'},
-             {'c': 'blue', 'linestyle': '-'},
-             {'c': 'red', 'linestyle': '--'},
-             {'c': 'green', 'linestyle': '--'},
-             {'c': 'blue', 'linestyle': '--'},
-             {'c': 'black', 'linestyle': '-'}]
+plot_args = [
+    {"c": "red", "linestyle": "-"},
+    {"c": "green", "linestyle": "-"},
+    {"c": "blue", "linestyle": "-"},
+    {"c": "red", "linestyle": "--"},
+    {"c": "green", "linestyle": "--"},
+    {"c": "blue", "linestyle": "--"},
+    {"c": "black", "linestyle": "-"},
+]


 def plot_on_dataset(X, y, ax, name):
     # for each dataset, plot learning for each learning strategy
     print("\nlearning on dataset %s" % name)
     ax.set_title(name)
+
     X = MinMaxScaler().fit_transform(X)
     mlps = []
     if name == "digits":
@@ -62,27 +104,37 @@

     for label, param in zip(labels, params):
         print("training: %s" % label)
-        mlp = MLPClassifier(verbose=0, random_state=0,
-                            max_iter=max_iter, **param)
-        mlp.fit(X, y)
+        mlp = MLPClassifier(random_state=0, max_iter=max_iter, **param)
+
+        # some parameter combinations will not converge as can be seen on the
+        # plots so they are ignored here
+        with warnings.catch_warnings():
+            warnings.filterwarnings(
+                "ignore", category=ConvergenceWarning, module="sklearn"
+            )
+            mlp.fit(X, y)
+
         mlps.append(mlp)
         print("Training set score: %f" % mlp.score(X, y))
         print("Training set loss: %f" % mlp.loss_)
     for mlp, label, args in zip(mlps, labels, plot_args):
-            ax.plot(mlp.loss_curve_, label=label, **args)
+        ax.plot(mlp.loss_curve_, label=label, **args)


 fig, axes = plt.subplots(2, 2, figsize=(15, 10))
 # load / generate some toy datasets
 iris = datasets.load_iris()
-digits = datasets.load_digits()
-data_sets = [(iris.data, iris.target),
-             (digits.data, digits.target),
-             datasets.make_circles(noise=0.2, factor=0.5, random_state=1),
-             datasets.make_moons(noise=0.3, random_state=0)]
+X_digits, y_digits = datasets.load_digits(return_X_y=True)
+data_sets = [
+    (iris.data, iris.target),
+    (X_digits, y_digits),
+    datasets.make_circles(noise=0.2, factor=0.5, random_state=1),
+    datasets.make_moons(noise=0.3, random_state=0),
+]

-for ax, data, name in zip(axes.ravel(), data_sets, ['iris', 'digits',
-                                                    'circles', 'moons']):
+for ax, data, name in zip(
+    axes.ravel(), data_sets, ["iris", "digits", "circles", "moons"]
+):
     plot_on_dataset(*data, ax=ax, name=name)

 fig.legend(ax.get_lines(), labels, ncol=3, loc="upper center")
('examples/neural_networks', 'plot_rbm_logistic_classification.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -9,39 +9,28 @@
 <sklearn.neural_network.BernoulliRBM>`) can perform effective non-linear
 feature extraction.

-In order to learn good latent representations from a small dataset, we
-artificially generate more labeled data by perturbing the training data with
-linear shifts of 1 pixel in each direction.
-
-This example shows how to build a classification pipeline with a BernoulliRBM
-feature extractor and a :class:`LogisticRegression
-<sklearn.linear_model.LogisticRegression>` classifier. The hyperparameters
-of the entire model (learning rate, hidden layer size, regularization)
-were optimized by grid search, but the search is not reproduced here because
-of runtime constraints.
-
-Logistic regression on raw pixel values is presented for comparison. The
-example shows that the features extracted by the BernoulliRBM help improve the
-classification accuracy.
 """
-print(__doc__)

 # Authors: Yann N. Dauphin, Vlad Niculae, Gabriel Synnaeve
 # License: BSD

+# %%
+# Generate data
+# -------------
+#
+# In order to learn good latent representations from a small dataset, we
+# artificially generate more labeled data by perturbing the training data with
+# linear shifts of 1 pixel in each direction.
+
 import numpy as np
-import matplotlib.pyplot as plt

 from scipy.ndimage import convolve
-from sklearn import linear_model, datasets, metrics
+
+from sklearn import datasets
+from sklearn.preprocessing import minmax_scale
+
 from sklearn.model_selection import train_test_split
-from sklearn.neural_network import BernoulliRBM
-from sklearn.pipeline import Pipeline
-from sklearn.base import clone

-
-# #############################################################################
-# Setting up

 def nudge_dataset(X, Y):
     """
@@ -49,57 +38,62 @@
     by moving the 8x8 images in X around by 1px to left, right, down, up
     """
     direction_vectors = [
-        [[0, 1, 0],
-         [0, 0, 0],
-         [0, 0, 0]],
-
-        [[0, 0, 0],
-         [1, 0, 0],
-         [0, 0, 0]],
-
-        [[0, 0, 0],
-         [0, 0, 1],
-         [0, 0, 0]],
-
-        [[0, 0, 0],
-         [0, 0, 0],
-         [0, 1, 0]]]
+        [[0, 1, 0], [0, 0, 0], [0, 0, 0]],
+        [[0, 0, 0], [1, 0, 0], [0, 0, 0]],
+        [[0, 0, 0], [0, 0, 1], [0, 0, 0]],
+        [[0, 0, 0], [0, 0, 0], [0, 1, 0]],
+    ]

     def shift(x, w):
-        return convolve(x.reshape((8, 8)), mode='constant', weights=w).ravel()
+        return convolve(x.reshape((8, 8)), mode="constant", weights=w).ravel()

-    X = np.concatenate([X] +
-                       [np.apply_along_axis(shift, 1, X, vector)
-                        for vector in direction_vectors])
+    X = np.concatenate(
+        [X] + [np.apply_along_axis(shift, 1, X, vector) for vector in direction_vectors]
+    )
     Y = np.concatenate([Y for _ in range(5)], axis=0)
     return X, Y


-# Load Data
-digits = datasets.load_digits()
-X = np.asarray(digits.data, 'float32')
-X, Y = nudge_dataset(X, digits.target)
-X = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  # 0-1 scaling
+X, y = datasets.load_digits(return_X_y=True)
+X = np.asarray(X, "float32")
+X, Y = nudge_dataset(X, y)
+X = minmax_scale(X, feature_range=(0, 1))  # 0-1 scaling

-X_train, X_test, Y_train, Y_test = train_test_split(
-    X, Y, test_size=0.2, random_state=0)
+X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

-# Models we will use
-logistic = linear_model.LogisticRegression(solver='newton-cg', tol=1,
-                                           multi_class='multinomial')
+# %%
+# Models definition
+# -----------------
+#
+# We build a classification pipeline with a BernoulliRBM feature extractor and
+# a :class:`LogisticRegression <sklearn.linear_model.LogisticRegression>`
+# classifier.
+
+from sklearn import linear_model
+from sklearn.neural_network import BernoulliRBM
+from sklearn.pipeline import Pipeline
+
+logistic = linear_model.LogisticRegression(solver="newton-cg", tol=1)
 rbm = BernoulliRBM(random_state=0, verbose=True)

-rbm_features_classifier = Pipeline(
-    steps=[('rbm', rbm), ('logistic', logistic)])
+rbm_features_classifier = Pipeline(steps=[("rbm", rbm), ("logistic", logistic)])

-# #############################################################################
+# %%
 # Training
+# --------
+#
+# The hyperparameters of the entire model (learning rate, hidden layer size,
+# regularization) were optimized by grid search, but the search is not
+# reproduced here because of runtime constraints.
+
+from sklearn.base import clone

 # Hyper-parameters. These were set by cross-validation,
 # using a GridSearchCV. Here we are not performing cross-validation to
 # save time.
 rbm.learning_rate = 0.06
-rbm.n_iter = 20
+rbm.n_iter = 10
+
 # More components tend to give better prediction performance, but larger
 # fitting time
 rbm.n_components = 100
@@ -110,31 +104,45 @@

 # Training the Logistic regression classifier directly on the pixel
 raw_pixel_classifier = clone(logistic)
-raw_pixel_classifier.C = 100.
+raw_pixel_classifier.C = 100.0
 raw_pixel_classifier.fit(X_train, Y_train)

-# #############################################################################
+# %%
 # Evaluation
+# ----------
+
+from sklearn import metrics

 Y_pred = rbm_features_classifier.predict(X_test)
-print("Logistic regression using RBM features:\n%s\n" % (
-    metrics.classification_report(Y_test, Y_pred)))
+print(
+    "Logistic regression using RBM features:\n%s\n"
+    % (metrics.classification_report(Y_test, Y_pred))
+)

+# %%
 Y_pred = raw_pixel_classifier.predict(X_test)
-print("Logistic regression using raw pixel features:\n%s\n" % (
-    metrics.classification_report(Y_test, Y_pred)))
+print(
+    "Logistic regression using raw pixel features:\n%s\n"
+    % (metrics.classification_report(Y_test, Y_pred))
+)

-# #############################################################################
+# %%
+# The features extracted by the BernoulliRBM help improve the classification
+# accuracy with respect to the logistic regression on raw pixels.
+
+# %%
 # Plotting
+# --------
+
+import matplotlib.pyplot as plt

 plt.figure(figsize=(4.2, 4))
 for i, comp in enumerate(rbm.components_):
     plt.subplot(10, 10, i + 1)
-    plt.imshow(comp.reshape((8, 8)), cmap=plt.cm.gray_r,
-               interpolation='nearest')
+    plt.imshow(comp.reshape((8, 8)), cmap=plt.cm.gray_r, interpolation="nearest")
     plt.xticks(())
     plt.yticks(())
-plt.suptitle('100 components extracted by RBM', fontsize=16)
+plt.suptitle("100 components extracted by RBM", fontsize=16)
 plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)

 plt.show()
('examples/neural_networks', 'plot_mnist_filters.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -12,35 +12,49 @@
 MLPClassifier trained on the MNIST dataset.

 The input data consists of 28x28 pixel handwritten digits, leading to 784
-features in the dataset. Therefore the first layer weight matrix have the shape
+features in the dataset. Therefore the first layer weight matrix has the shape
 (784, hidden_layer_sizes[0]).  We can therefore visualize a single column of
 the weight matrix as a 28x28 pixel image.

 To make the example run faster, we use very few hidden units, and train only
 for a very short time. Training longer would result in weights with a much
-smoother spatial appearance.
+smoother spatial appearance. The example will throw a warning because it
+doesn't converge, in this case this is what we want because of resource
+usage constraints on our Continuous Integration infrastructure that is used
+to build this documentation on a regular basis.
 """
+
+import warnings
 import matplotlib.pyplot as plt
 from sklearn.datasets import fetch_openml
+from sklearn.exceptions import ConvergenceWarning
 from sklearn.neural_network import MLPClassifier
-
-print(__doc__)
+from sklearn.model_selection import train_test_split

 # Load data from https://www.openml.org/d/554
-X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
-X = X / 255.
+X, y = fetch_openml("mnist_784", version=1, return_X_y=True, as_frame=False)
+X = X / 255.0

-# rescale the data, use the traditional train/test split
-X_train, X_test = X[:60000], X[60000:]
-y_train, y_test = y[:60000], y[60000:]
+# Split data into train partition and test partition
+X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.7)

-# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,
-#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)
-mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,
-                    solver='sgd', verbose=10, tol=1e-4, random_state=1,
-                    learning_rate_init=.1)
+mlp = MLPClassifier(
+    hidden_layer_sizes=(40,),
+    max_iter=8,
+    alpha=1e-4,
+    solver="sgd",
+    verbose=10,
+    random_state=1,
+    learning_rate_init=0.2,
+)

-mlp.fit(X_train, y_train)
+# this example won't converge because of resource usage constraints on
+# our Continuous Integration infrastructure, so we catch the warning and
+# ignore it here
+with warnings.catch_warnings():
+    warnings.filterwarnings("ignore", category=ConvergenceWarning, module="sklearn")
+    mlp.fit(X_train, y_train)
+
 print("Training set score: %f" % mlp.score(X_train, y_train))
 print("Test set score: %f" % mlp.score(X_test, y_test))

@@ -48,8 +62,7 @@
 # use global min / max to ensure all weights are shown on the same scale
 vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()
 for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):
-    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,
-               vmax=.5 * vmax)
+    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)
     ax.set_xticks(())
     ax.set_yticks(())

('examples/neural_networks', 'plot_mlp_alpha.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -14,9 +14,8 @@
 Similarly, decreasing alpha may fix high bias (a sign of underfitting) by
 encouraging larger weights, potentially resulting in a more complicated
 decision boundary.
+
 """
-print(__doc__)
-

 # Author: Issam H. Laradji
 # License: BSD 3 clause
@@ -28,42 +27,59 @@
 from sklearn.preprocessing import StandardScaler
 from sklearn.datasets import make_moons, make_circles, make_classification
 from sklearn.neural_network import MLPClassifier
+from sklearn.pipeline import make_pipeline

-h = .02  # step size in the mesh
+h = 0.02  # step size in the mesh

-alphas = np.logspace(-5, 3, 5)
-names = ['alpha ' + str(i) for i in alphas]
+alphas = np.logspace(-1, 1, 5)

 classifiers = []
-for i in alphas:
-    classifiers.append(MLPClassifier(alpha=i, random_state=1))
+names = []
+for alpha in alphas:
+    classifiers.append(
+        make_pipeline(
+            StandardScaler(),
+            MLPClassifier(
+                solver="lbfgs",
+                alpha=alpha,
+                random_state=1,
+                max_iter=2000,
+                early_stopping=True,
+                hidden_layer_sizes=[10, 10],
+            ),
+        )
+    )
+    names.append(f"alpha {alpha:.2f}")

-X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
-                           random_state=0, n_clusters_per_class=1)
+X, y = make_classification(
+    n_features=2, n_redundant=0, n_informative=2, random_state=0, n_clusters_per_class=1
+)
 rng = np.random.RandomState(2)
 X += 2 * rng.uniform(size=X.shape)
 linearly_separable = (X, y)

-datasets = [make_moons(noise=0.3, random_state=0),
-            make_circles(noise=0.2, factor=0.5, random_state=1),
-            linearly_separable]
+datasets = [
+    make_moons(noise=0.3, random_state=0),
+    make_circles(noise=0.2, factor=0.5, random_state=1),
+    linearly_separable,
+]

 figure = plt.figure(figsize=(17, 9))
 i = 1
 # iterate over datasets
 for X, y in datasets:
-    # preprocess dataset, split into training and test part
-    X = StandardScaler().fit_transform(X)
-    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)
+    # split into training and test part
+    X_train, X_test, y_train, y_test = train_test_split(
+        X, y, test_size=0.4, random_state=42
+    )

-    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
-    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
-    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
-                         np.arange(y_min, y_max, h))
+    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
+    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
+    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

     # just plot the dataset first
     cm = plt.cm.RdBu
-    cm_bright = ListedColormap(['#FF0000', '#0000FF'])
+    cm_bright = ListedColormap(["#FF0000", "#0000FF"])
     ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
     # Plot the training points
     ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)
@@ -82,31 +98,49 @@
         score = clf.score(X_test, y_test)

         # Plot the decision boundary. For that, we will assign a color to each
-        # point in the mesh [x_min, x_max]x[y_min, y_max].
+        # point in the mesh [x_min, x_max] x [y_min, y_max].
         if hasattr(clf, "decision_function"):
-            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
+            Z = clf.decision_function(np.column_stack([xx.ravel(), yy.ravel()]))
         else:
-            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
+            Z = clf.predict_proba(np.column_stack([xx.ravel(), yy.ravel()]))[:, 1]

         # Put the result into a color plot
         Z = Z.reshape(xx.shape)
-        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)
+        ax.contourf(xx, yy, Z, cmap=cm, alpha=0.8)

         # Plot also the training points
-        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
-                   edgecolors='black', s=25)
+        ax.scatter(
+            X_train[:, 0],
+            X_train[:, 1],
+            c=y_train,
+            cmap=cm_bright,
+            edgecolors="black",
+            s=25,
+        )
         # and testing points
-        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
-                   alpha=0.6, edgecolors='black', s=25)
+        ax.scatter(
+            X_test[:, 0],
+            X_test[:, 1],
+            c=y_test,
+            cmap=cm_bright,
+            alpha=0.6,
+            edgecolors="black",
+            s=25,
+        )

         ax.set_xlim(xx.min(), xx.max())
         ax.set_ylim(yy.min(), yy.max())
         ax.set_xticks(())
         ax.set_yticks(())
         ax.set_title(name)
-        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),
-                size=15, horizontalalignment='right')
+        ax.text(
+            xx.max() - 0.3,
+            yy.min() + 0.3,
+            f"{score:.3f}".lstrip("0"),
+            size=15,
+            horizontalalignment="right",
+        )
         i += 1

-figure.subplots_adjust(left=.02, right=.98)
+figure.subplots_adjust(left=0.02, right=0.98)
 plt.show()
('examples/preprocessing', 'plot_map_data_to_normal.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,8 +3,10 @@
 Map data to a normal distribution
 =================================

+.. currentmodule:: sklearn.preprocessing
+
 This example demonstrates the use of the Box-Cox and Yeo-Johnson transforms
-through :class:`preprocessing.PowerTransformer` to map data from various
+through :class:`~PowerTransformer` to map data from various
 distributions to a normal distribution.

 The power transform is useful as a transformation in modeling problems where
@@ -22,13 +24,14 @@
 support inputs with negative values.

 For comparison, we also add the output from
-:class:`preprocessing.QuantileTransformer`. It can force any arbitrary
+:class:`~QuantileTransformer`. It can force any arbitrary
 distribution into a gaussian, provided that there are enough training samples
 (thousands). Because it is a non-parametric method, it is harder to interpret
 than the parametric ones (Box-Cox and Yeo-Johnson).

 On "small" datasets (less than a few hundred points), the quantile transformer
 is prone to overfitting. The use of the power transform is then recommended.
+
 """

 # Author: Eric Chang <ericchang2017@u.northwestern.edu>
@@ -42,8 +45,6 @@
 from sklearn.preprocessing import QuantileTransformer
 from sklearn.model_selection import train_test_split

-print(__doc__)
-

 N_SAMPLES = 1000
 FONT_SIZE = 6
@@ -51,9 +52,13 @@


 rng = np.random.RandomState(304)
-bc = PowerTransformer(method='box-cox')
-yj = PowerTransformer(method='yeo-johnson')
-qt = QuantileTransformer(output_distribution='normal', random_state=rng)
+bc = PowerTransformer(method="box-cox")
+yj = PowerTransformer(method="yeo-johnson")
+# n_quantiles is set to the training set size rather than the default value
+# to avoid a warning being raised by this example
+qt = QuantileTransformer(
+    n_quantiles=500, output_distribution="normal", random_state=rng
+)
 size = (N_SAMPLES, 1)


@@ -83,28 +88,32 @@

 # create plots
 distributions = [
-    ('Lognormal', X_lognormal),
-    ('Chi-squared', X_chisq),
-    ('Weibull', X_weibull),
-    ('Gaussian', X_gaussian),
-    ('Uniform', X_uniform),
-    ('Bimodal', X_bimodal)
+    ("Lognormal", X_lognormal),
+    ("Chi-squared", X_chisq),
+    ("Weibull", X_weibull),
+    ("Gaussian", X_gaussian),
+    ("Uniform", X_uniform),
+    ("Bimodal", X_bimodal),
 ]

-colors = ['firebrick', 'darkorange', 'goldenrod',
-          'seagreen', 'royalblue', 'darkorchid']
+colors = ["#D81B60", "#0188FF", "#FFC107", "#B7A2FF", "#000000", "#2EC5AC"]

 fig, axes = plt.subplots(nrows=8, ncols=3, figsize=plt.figaspect(2))
 axes = axes.flatten()
-axes_idxs = [(0, 3, 6, 9), (1, 4, 7, 10), (2, 5, 8, 11), (12, 15, 18, 21),
-             (13, 16, 19, 22), (14, 17, 20, 23)]
-axes_list = [(axes[i], axes[j], axes[k], axes[l])
-             for (i, j, k, l) in axes_idxs]
+axes_idxs = [
+    (0, 3, 6, 9),
+    (1, 4, 7, 10),
+    (2, 5, 8, 11),
+    (12, 15, 18, 21),
+    (13, 16, 19, 22),
+    (14, 17, 20, 23),
+]
+axes_list = [(axes[i], axes[j], axes[k], axes[l]) for (i, j, k, l) in axes_idxs]


 for distribution, color, axes in zip(distributions, colors, axes_list):
     name, X = distribution
-    X_train, X_test = train_test_split(X, test_size=.5)
+    X_train, X_test = train_test_split(X, test_size=0.5)

     # perform power transforms and quantile transform
     X_trans_bc = bc.fit(X_train).transform(X_test)
@@ -117,19 +126,20 @@

     ax_original.hist(X_train, color=color, bins=BINS)
     ax_original.set_title(name, fontsize=FONT_SIZE)
-    ax_original.tick_params(axis='both', which='major', labelsize=FONT_SIZE)
+    ax_original.tick_params(axis="both", which="major", labelsize=FONT_SIZE)

     for ax, X_trans, meth_name, lmbda in zip(
-            (ax_bc, ax_yj, ax_qt),
-            (X_trans_bc, X_trans_yj, X_trans_qt),
-            ('Box-Cox', 'Yeo-Johnson', 'Quantile transform'),
-            (lmbda_bc, lmbda_yj, None)):
+        (ax_bc, ax_yj, ax_qt),
+        (X_trans_bc, X_trans_yj, X_trans_qt),
+        ("Box-Cox", "Yeo-Johnson", "Quantile transform"),
+        (lmbda_bc, lmbda_yj, None),
+    ):
         ax.hist(X_trans, color=color, bins=BINS)
-        title = 'After {}'.format(meth_name)
+        title = "After {}".format(meth_name)
         if lmbda is not None:
-            title += r'\n$\lambda$ = {}'.format(lmbda)
+            title += "\n$\\lambda$ = {}".format(lmbda)
         ax.set_title(title, fontsize=FONT_SIZE)
-        ax.tick_params(axis='both', which='major', labelsize=FONT_SIZE)
+        ax.tick_params(axis="both", which="major", labelsize=FONT_SIZE)
         ax.set_xlim([-3.5, 3.5])


('examples/preprocessing', 'plot_discretization_classification.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,4 +1,3 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
 """
 ======================
@@ -27,7 +26,9 @@
 The plots show training points in solid colors and testing points
 semi-transparent. The lower right shows the classification accuracy on the test
 set.
+
 """
+
 # Code source: Tom Dupré la Tour
 # Adapted from plot_classifier_comparison by Gaël Varoquaux and Andreas Müller
 #
@@ -45,135 +46,169 @@
 from sklearn.preprocessing import KBinsDiscretizer
 from sklearn.svm import SVC, LinearSVC
 from sklearn.ensemble import GradientBoostingClassifier
-from sklearn.utils.testing import ignore_warnings
+from sklearn.utils._testing import ignore_warnings
 from sklearn.exceptions import ConvergenceWarning

-print(__doc__)
-
-h = .02  # step size in the mesh
+h = 0.02  # step size in the mesh


 def get_name(estimator):
     name = estimator.__class__.__name__
-    if name == 'Pipeline':
+    if name == "Pipeline":
         name = [get_name(est[1]) for est in estimator.steps]
-        name = ' + '.join(name)
+        name = " + ".join(name)
     return name


 # list of (estimator, param_grid), where param_grid is used in GridSearchCV
+# The parameter spaces in this example are limited to a narrow band to reduce
+# its runtime. In a real use case, a broader search space for the algorithms
+# should be used.
 classifiers = [
-    (LogisticRegression(solver='lbfgs', random_state=0), {
-        'C': np.logspace(-2, 7, 10)
-    }),
-    (LinearSVC(random_state=0), {
-        'C': np.logspace(-2, 7, 10)
-    }),
-    (make_pipeline(
-        KBinsDiscretizer(encode='onehot'),
-        LogisticRegression(solver='lbfgs', random_state=0)), {
-            'kbinsdiscretizer__n_bins': np.arange(2, 10),
-            'logisticregression__C': np.logspace(-2, 7, 10),
-        }),
-    (make_pipeline(
-        KBinsDiscretizer(encode='onehot'), LinearSVC(random_state=0)), {
-            'kbinsdiscretizer__n_bins': np.arange(2, 10),
-            'linearsvc__C': np.logspace(-2, 7, 10),
-        }),
-    (GradientBoostingClassifier(n_estimators=50, random_state=0), {
-        'learning_rate': np.logspace(-4, 0, 10)
-    }),
-    (SVC(random_state=0, gamma='scale'), {
-        'C': np.logspace(-2, 7, 10)
-    }),
+    (
+        make_pipeline(StandardScaler(), LogisticRegression(random_state=0)),
+        {"logisticregression__C": np.logspace(-1, 1, 3)},
+    ),
+    (
+        make_pipeline(StandardScaler(), LinearSVC(random_state=0)),
+        {"linearsvc__C": np.logspace(-1, 1, 3)},
+    ),
+    (
+        make_pipeline(
+            StandardScaler(),
+            KBinsDiscretizer(encode="onehot"),
+            LogisticRegression(random_state=0),
+        ),
+        {
+            "kbinsdiscretizer__n_bins": np.arange(5, 8),
+            "logisticregression__C": np.logspace(-1, 1, 3),
+        },
+    ),
+    (
+        make_pipeline(
+            StandardScaler(),
+            KBinsDiscretizer(encode="onehot"),
+            LinearSVC(random_state=0),
+        ),
+        {
+            "kbinsdiscretizer__n_bins": np.arange(5, 8),
+            "linearsvc__C": np.logspace(-1, 1, 3),
+        },
+    ),
+    (
+        make_pipeline(
+            StandardScaler(), GradientBoostingClassifier(n_estimators=5, random_state=0)
+        ),
+        {"gradientboostingclassifier__learning_rate": np.logspace(-2, 0, 5)},
+    ),
+    (
+        make_pipeline(StandardScaler(), SVC(random_state=0)),
+        {"svc__C": np.logspace(-1, 1, 3)},
+    ),
 ]

-names = [get_name(e) for e, g in classifiers]
+names = [get_name(e).replace("StandardScaler + ", "") for e, _ in classifiers]

 n_samples = 100
 datasets = [
     make_moons(n_samples=n_samples, noise=0.2, random_state=0),
     make_circles(n_samples=n_samples, noise=0.2, factor=0.5, random_state=1),
-    make_classification(n_samples=n_samples, n_features=2, n_redundant=0,
-                        n_informative=2, random_state=2,
-                        n_clusters_per_class=1)
+    make_classification(
+        n_samples=n_samples,
+        n_features=2,
+        n_redundant=0,
+        n_informative=2,
+        random_state=2,
+        n_clusters_per_class=1,
+    ),
 ]

-fig, axes = plt.subplots(nrows=len(datasets), ncols=len(classifiers) + 1,
-                         figsize=(21, 9))
-
-cm = plt.cm.PiYG
-cm_bright = ListedColormap(['#b30065', '#178000'])
+fig, axes = plt.subplots(
+    nrows=len(datasets), ncols=len(classifiers) + 1, figsize=(21, 9)
+)
+
+cm_piyg = plt.cm.PiYG
+cm_bright = ListedColormap(["#b30065", "#178000"])

 # iterate over datasets
 for ds_cnt, (X, y) in enumerate(datasets):
-    print('\ndataset %d\n---------' % ds_cnt)
-
-    # preprocess dataset, split into training and test part
-    X = StandardScaler().fit_transform(X)
+    print(f"\ndataset {ds_cnt}\n---------")
+
+    # split into training and test part
     X_train, X_test, y_train, y_test = train_test_split(
-        X, y, test_size=.5, random_state=42)
+        X, y, test_size=0.5, random_state=42
+    )

     # create the grid for background colors
-    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
-    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
-    xx, yy = np.meshgrid(
-        np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
+    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
+    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
+    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

     # plot the dataset first
     ax = axes[ds_cnt, 0]
     if ds_cnt == 0:
         ax.set_title("Input data")
     # plot the training points
-    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
-               edgecolors='k')
+    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors="k")
     # and testing points
-    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,
-               edgecolors='k')
+    ax.scatter(
+        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors="k"
+    )
     ax.set_xlim(xx.min(), xx.max())
     ax.set_ylim(yy.min(), yy.max())
     ax.set_xticks(())
     ax.set_yticks(())

     # iterate over classifiers
-    for est_idx, (name, (estimator, param_grid)) in \
-            enumerate(zip(names, classifiers)):
+    for est_idx, (name, (estimator, param_grid)) in enumerate(zip(names, classifiers)):
         ax = axes[ds_cnt, est_idx + 1]

-        clf = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=5,
-                           iid=False)
+        clf = GridSearchCV(estimator=estimator, param_grid=param_grid)
         with ignore_warnings(category=ConvergenceWarning):
             clf.fit(X_train, y_train)
         score = clf.score(X_test, y_test)
-        print('%s: %.2f' % (name, score))
+        print(f"{name}: {score:.2f}")

         # plot the decision boundary. For that, we will assign a color to each
         # point in the mesh [x_min, x_max]*[y_min, y_max].
         if hasattr(clf, "decision_function"):
-            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
+            Z = clf.decision_function(np.column_stack([xx.ravel(), yy.ravel()]))
         else:
-            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]
+            Z = clf.predict_proba(np.column_stack([xx.ravel(), yy.ravel()]))[:, 1]

         # put the result into a color plot
         Z = Z.reshape(xx.shape)
-        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)
+        ax.contourf(xx, yy, Z, cmap=cm_piyg, alpha=0.8)

         # plot the training points
-        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
-                   edgecolors='k')
+        ax.scatter(
+            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors="k"
+        )
         # and testing points
-        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
-                   edgecolors='k', alpha=0.6)
+        ax.scatter(
+            X_test[:, 0],
+            X_test[:, 1],
+            c=y_test,
+            cmap=cm_bright,
+            edgecolors="k",
+            alpha=0.6,
+        )
         ax.set_xlim(xx.min(), xx.max())
         ax.set_ylim(yy.min(), yy.max())
         ax.set_xticks(())
         ax.set_yticks(())

         if ds_cnt == 0:
-            ax.set_title(name.replace(' + ', '\n'))
-        ax.text(0.95, 0.06, ('%.2f' % score).lstrip('0'), size=15,
-                bbox=dict(boxstyle='round', alpha=0.8, facecolor='white'),
-                transform=ax.transAxes, horizontalalignment='right')
+            ax.set_title(name.replace(" + ", "\n"))
+        ax.text(
+            0.95,
+            0.06,
+            (f"{score:.2f}").lstrip("0"),
+            size=15,
+            bbox=dict(boxstyle="round", alpha=0.8, facecolor="white"),
+            transform=ax.transAxes,
+            horizontalalignment="right",
+        )


 plt.tight_layout()
@@ -181,12 +216,18 @@
 # Add suptitles above the figure
 plt.subplots_adjust(top=0.90)
 suptitles = [
-    'Linear classifiers',
-    'Feature discretization and linear classifiers',
-    'Non-linear classifiers',
+    "Linear classifiers",
+    "Feature discretization and linear classifiers",
+    "Non-linear classifiers",
 ]
 for i, suptitle in zip([1, 3, 5], suptitles):
     ax = axes[0, i]
-    ax.text(1.05, 1.25, suptitle, transform=ax.transAxes,
-            horizontalalignment='center', size='x-large')
+    ax.text(
+        1.05,
+        1.25,
+        suptitle,
+        transform=ax.transAxes,
+        horizontalalignment="center",
+        size="x-large",
+    )
 plt.show()
('examples/preprocessing', 'plot_discretization_strategies.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -14,6 +14,7 @@
   procedure.

 The plot shows the regions where the discretized encoding is constant.
+
 """

 # Author: Tom Dupré la Tour
@@ -25,9 +26,7 @@
 from sklearn.preprocessing import KBinsDiscretizer
 from sklearn.datasets import make_blobs

-print(__doc__)
-
-strategies = ['uniform', 'quantile', 'kmeans']
+strategies = ["uniform", "quantile", "kmeans"]

 n_samples = 200
 centers_0 = np.array([[0, 0], [0, 5], [2, 4], [8, 8]])
@@ -37,13 +36,23 @@
 random_state = 42
 X_list = [
     np.random.RandomState(random_state).uniform(-3, 3, size=(n_samples, 2)),
-    make_blobs(n_samples=[n_samples // 10, n_samples * 4 // 10,
-                          n_samples // 10, n_samples * 4 // 10],
-               cluster_std=0.5, centers=centers_0,
-               random_state=random_state)[0],
-    make_blobs(n_samples=[n_samples // 5, n_samples * 4 // 5],
-               cluster_std=0.5, centers=centers_1,
-               random_state=random_state)[0],
+    make_blobs(
+        n_samples=[
+            n_samples // 10,
+            n_samples * 4 // 10,
+            n_samples // 10,
+            n_samples * 4 // 10,
+        ],
+        cluster_std=0.5,
+        centers=centers_0,
+        random_state=random_state,
+    )[0],
+    make_blobs(
+        n_samples=[n_samples // 5, n_samples * 4 // 5],
+        cluster_std=0.5,
+        centers=centers_1,
+        random_state=random_state,
+    )[0],
 ]

 figure = plt.figure(figsize=(14, 9))
@@ -51,13 +60,14 @@
 for ds_cnt, X in enumerate(X_list):

     ax = plt.subplot(len(X_list), len(strategies) + 1, i)
-    ax.scatter(X[:, 0], X[:, 1], edgecolors='k')
+    ax.scatter(X[:, 0], X[:, 1], edgecolors="k")
     if ds_cnt == 0:
         ax.set_title("Input data", size=14)

     xx, yy = np.meshgrid(
         np.linspace(X[:, 0].min(), X[:, 0].max(), 300),
-        np.linspace(X[:, 1].min(), X[:, 1].max(), 300))
+        np.linspace(X[:, 1].min(), X[:, 1].max(), 300),
+    )
     grid = np.c_[xx.ravel(), yy.ravel()]

     ax.set_xlim(xx.min(), xx.max())
@@ -68,7 +78,7 @@
     i += 1
     # transform the dataset with KBinsDiscretizer
     for strategy in strategies:
-        enc = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy=strategy)
+        enc = KBinsDiscretizer(n_bins=4, encode="ordinal", strategy=strategy)
         enc.fit(X)
         grid_encoded = enc.transform(grid)

@@ -76,18 +86,18 @@

         # horizontal stripes
         horizontal = grid_encoded[:, 0].reshape(xx.shape)
-        ax.contourf(xx, yy, horizontal, alpha=.5)
+        ax.contourf(xx, yy, horizontal, alpha=0.5)
         # vertical stripes
         vertical = grid_encoded[:, 1].reshape(xx.shape)
-        ax.contourf(xx, yy, vertical, alpha=.5)
+        ax.contourf(xx, yy, vertical, alpha=0.5)

-        ax.scatter(X[:, 0], X[:, 1], edgecolors='k')
+        ax.scatter(X[:, 0], X[:, 1], edgecolors="k")
         ax.set_xlim(xx.min(), xx.max())
         ax.set_ylim(yy.min(), yy.max())
         ax.set_xticks(())
         ax.set_yticks(())
         if ds_cnt == 0:
-            ax.set_title("strategy='%s'" % (strategy, ), size=14)
+            ax.set_title("strategy='%s'" % (strategy,), size=14)

         i += 1

('examples/preprocessing', 'plot_scaling_importance.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,4 +1,3 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
 """
 =========================================================
@@ -23,7 +22,8 @@
 height of one meter can be considered much more important than the
 change in weight of one kilogram, this is clearly incorrect.

-To illustrate this, PCA is performed comparing the use of data with
+To illustrate this, :class:`PCA <sklearn.decomposition.PCA>`
+is performed comparing the use of data with
 :class:`StandardScaler <sklearn.preprocessing.StandardScaler>` applied,
 to unscaled data. The results are visualized and a clear difference noted.
 The 1st principal component in the unscaled set can be seen. It can be seen
@@ -34,22 +34,22 @@

 The dataset used is the Wine Dataset available at UCI. This dataset
 has continuous features that are heterogeneous in scale due to differing
-properties that they measure (i.e alcohol content, and malic acid).
+properties that they measure (i.e. alcohol content and malic acid).

 The transformed data is then used to train a naive Bayes classifier, and a
 clear difference in prediction accuracies is observed wherein the dataset
 which is scaled before PCA vastly outperforms the unscaled version.

 """
+import matplotlib.pyplot as plt
+
 from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import StandardScaler
 from sklearn.decomposition import PCA
 from sklearn.naive_bayes import GaussianNB
-from sklearn import metrics
-import matplotlib.pyplot as plt
+from sklearn.metrics import accuracy_score
 from sklearn.datasets import load_wine
 from sklearn.pipeline import make_pipeline
-print(__doc__)

 # Code source: Tyler Lanigan <tylerlanigan@gmail.com>
 #              Sebastian Raschka <mail@sebastianraschka.com>
@@ -63,69 +63,75 @@
 features, target = load_wine(return_X_y=True)

 # Make a train/test split using 30% test size
-X_train, X_test, y_train, y_test = train_test_split(features, target,
-                                                    test_size=0.30,
-                                                    random_state=RANDOM_STATE)
+X_train, X_test, y_train, y_test = train_test_split(
+    features, target, test_size=0.30, random_state=RANDOM_STATE
+)

-# Fit to data and predict using pipelined GNB and PCA.
+# Fit to data and predict using pipelined GNB and PCA
 unscaled_clf = make_pipeline(PCA(n_components=2), GaussianNB())
 unscaled_clf.fit(X_train, y_train)
 pred_test = unscaled_clf.predict(X_test)

-# Fit to data and predict using pipelined scaling, GNB and PCA.
+# Fit to data and predict using pipelined scaling, GNB and PCA
 std_clf = make_pipeline(StandardScaler(), PCA(n_components=2), GaussianNB())
 std_clf.fit(X_train, y_train)
 pred_test_std = std_clf.predict(X_test)

 # Show prediction accuracies in scaled and unscaled data.
-print('\nPrediction accuracy for the normal test dataset with PCA')
-print('{:.2%}\n'.format(metrics.accuracy_score(y_test, pred_test)))
+print("\nPrediction accuracy for the normal test dataset with PCA")
+print(f"{accuracy_score(y_test, pred_test):.2%}\n")

-print('\nPrediction accuracy for the standardized test dataset with PCA')
-print('{:.2%}\n'.format(metrics.accuracy_score(y_test, pred_test_std)))
+print("\nPrediction accuracy for the standardized test dataset with PCA")
+print(f"{accuracy_score(y_test, pred_test_std):.2%}\n")

 # Extract PCA from pipeline
-pca = unscaled_clf.named_steps['pca']
-pca_std = std_clf.named_steps['pca']
+pca = unscaled_clf.named_steps["pca"]
+pca_std = std_clf.named_steps["pca"]

 # Show first principal components
-print('\nPC 1 without scaling:\n', pca.components_[0])
-print('\nPC 1 with scaling:\n', pca_std.components_[0])
+print(f"\nPC 1 without scaling:\n{pca.components_[0]}")
+print(f"\nPC 1 with scaling:\n{pca_std.components_[0]}")

 # Use PCA without and with scale on X_train data for visualization.
 X_train_transformed = pca.transform(X_train)
-scaler = std_clf.named_steps['standardscaler']
-X_train_std_transformed = pca_std.transform(scaler.transform(X_train))
+
+scaler = std_clf.named_steps["standardscaler"]
+scaled_X_train = scaler.transform(X_train)
+X_train_std_transformed = pca_std.transform(scaled_X_train)

 # visualize standardized vs. untouched dataset with PCA performed
 fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=FIG_SIZE)

+target_classes = range(0, 3)
+colors = ("blue", "red", "green")
+markers = ("^", "s", "o")

-for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):
-    ax1.scatter(X_train_transformed[y_train == l, 0],
-                X_train_transformed[y_train == l, 1],
-                color=c,
-                label='class %s' % l,
-                alpha=0.5,
-                marker=m
-                )
+for target_class, color, marker in zip(target_classes, colors, markers):
+    ax1.scatter(
+        x=X_train_transformed[y_train == target_class, 0],
+        y=X_train_transformed[y_train == target_class, 1],
+        color=color,
+        label=f"class {target_class}",
+        alpha=0.5,
+        marker=marker,
+    )

-for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):
-    ax2.scatter(X_train_std_transformed[y_train == l, 0],
-                X_train_std_transformed[y_train == l, 1],
-                color=c,
-                label='class %s' % l,
-                alpha=0.5,
-                marker=m
-                )
+    ax2.scatter(
+        x=X_train_std_transformed[y_train == target_class, 0],
+        y=X_train_std_transformed[y_train == target_class, 1],
+        color=color,
+        label=f"class {target_class}",
+        alpha=0.5,
+        marker=marker,
+    )

-ax1.set_title('Training dataset after PCA')
-ax2.set_title('Standardized training dataset after PCA')
+ax1.set_title("Training dataset after PCA")
+ax2.set_title("Standardized training dataset after PCA")

 for ax in (ax1, ax2):
-    ax.set_xlabel('1st principal component')
-    ax.set_ylabel('2nd principal component')
-    ax.legend(loc='upper right')
+    ax.set_xlabel("1st principal component")
+    ax.set_ylabel("2nd principal component")
+    ax.legend(loc="upper right")
     ax.grid()

 plt.tight_layout()
('examples/preprocessing', 'plot_discretization.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,5 +1,4 @@
 # -*- coding: utf-8 -*-
-
 """
 ================================================================
 Using KBinsDiscretizer to discretize continuous features
@@ -40,8 +39,6 @@
 from sklearn.preprocessing import KBinsDiscretizer
 from sklearn.tree import DecisionTreeRegressor

-print(__doc__)
-
 # construct the dataset
 rnd = np.random.RandomState(42)
 X = rnd.uniform(-3, 3, size=100)
@@ -49,19 +46,17 @@
 X = X.reshape(-1, 1)

 # transform the dataset with KBinsDiscretizer
-enc = KBinsDiscretizer(n_bins=10, encode='onehot')
+enc = KBinsDiscretizer(n_bins=10, encode="onehot")
 X_binned = enc.fit_transform(X)

 # predict with original dataset
 fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(10, 4))
 line = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)
 reg = LinearRegression().fit(X, y)
-ax1.plot(line, reg.predict(line), linewidth=2, color='green',
-         label="linear regression")
+ax1.plot(line, reg.predict(line), linewidth=2, color="green", label="linear regression")
 reg = DecisionTreeRegressor(min_samples_split=3, random_state=0).fit(X, y)
-ax1.plot(line, reg.predict(line), linewidth=2, color='red',
-         label="decision tree")
-ax1.plot(X[:, 0], y, 'o', c='k')
+ax1.plot(line, reg.predict(line), linewidth=2, color="red", label="decision tree")
+ax1.plot(X[:, 0], y, "o", c="k")
 ax1.legend(loc="best")
 ax1.set_ylabel("Regression output")
 ax1.set_xlabel("Input feature")
@@ -70,14 +65,25 @@
 # predict with transformed dataset
 line_binned = enc.transform(line)
 reg = LinearRegression().fit(X_binned, y)
-ax2.plot(line, reg.predict(line_binned), linewidth=2, color='green',
-         linestyle='-', label='linear regression')
-reg = DecisionTreeRegressor(min_samples_split=3,
-                            random_state=0).fit(X_binned, y)
-ax2.plot(line, reg.predict(line_binned), linewidth=2, color='red',
-         linestyle=':', label='decision tree')
-ax2.plot(X[:, 0], y, 'o', c='k')
-ax2.vlines(enc.bin_edges_[0], *plt.gca().get_ylim(), linewidth=1, alpha=.2)
+ax2.plot(
+    line,
+    reg.predict(line_binned),
+    linewidth=2,
+    color="green",
+    linestyle="-",
+    label="linear regression",
+)
+reg = DecisionTreeRegressor(min_samples_split=3, random_state=0).fit(X_binned, y)
+ax2.plot(
+    line,
+    reg.predict(line_binned),
+    linewidth=2,
+    color="red",
+    linestyle=":",
+    label="decision tree",
+)
+ax2.plot(X[:, 0], y, "o", c="k")
+ax2.vlines(enc.bin_edges_[0], *plt.gca().get_ylim(), linewidth=1, alpha=0.2)
 ax2.legend(loc="best")
 ax2.set_xlabel("Input feature")
 ax2.set_title("Result after discretization")
('examples/preprocessing', 'plot_all_scaling.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,14 +1,11 @@
-#!/usr/bin/env python
 # -*- coding: utf-8 -*-
-
 """
 =============================================================
 Compare the effect of different scalers on data with outliers
 =============================================================

-Feature 0 (median income in a block) and feature 5 (number of households) of
-the `California housing dataset
-<https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html>`_ have very
+Feature 0 (median income in a block) and feature 5 (average house occupancy) of
+the :ref:`california_housing_dataset` have very
 different scales and contain some very large outliers. These two
 characteristics lead to difficulties to visualize the data and, more
 importantly, they can degrade the predictive performance of many machine
@@ -26,11 +23,13 @@
 data within a pre-defined range.

 Scalers are linear (or more precisely affine) transformers and differ from each
-other in the way to estimate the parameters used to shift and scale each
+other in the way they estimate the parameters used to shift and scale each
 feature.

-``QuantileTransformer`` provides non-linear transformations in which distances
-between marginal outliers and inliers are shrunk. ``PowerTransformer`` provides
+:class:`~sklearn.preprocessing.QuantileTransformer` provides non-linear
+transformations in which distances
+between marginal outliers and inliers are shrunk.
+:class:`~sklearn.preprocessing.PowerTransformer` provides
 non-linear transformations in which data is mapped to a normal distribution to
 stabilize variance and minimize skewness.

@@ -64,46 +63,61 @@

 from sklearn.datasets import fetch_california_housing

-print(__doc__)
-
 dataset = fetch_california_housing()
 X_full, y_full = dataset.data, dataset.target
+feature_names = dataset.feature_names
+
+feature_mapping = {
+    "MedInc": "Median income in block",
+    "HousAge": "Median house age in block",
+    "AveRooms": "Average number of rooms",
+    "AveBedrms": "Average number of bedrooms",
+    "Population": "Block population",
+    "AveOccup": "Average house occupancy",
+    "Latitude": "House block latitude",
+    "Longitude": "House block longitude",
+}

 # Take only 2 features to make visualization easier
-# Feature of 0 has a long tail distribution.
-# Feature 5 has a few but very large outliers.
-
-X = X_full[:, [0, 5]]
-
+# Feature MedInc has a long tail distribution.
+# Feature AveOccup has a few but very large outliers.
+features = ["MedInc", "AveOccup"]
+features_idx = [feature_names.index(feature) for feature in features]
+X = X_full[:, features_idx]
 distributions = [
-    ('Unscaled data', X),
-    ('Data after standard scaling',
-        StandardScaler().fit_transform(X)),
-    ('Data after min-max scaling',
-        MinMaxScaler().fit_transform(X)),
-    ('Data after max-abs scaling',
-        MaxAbsScaler().fit_transform(X)),
-    ('Data after robust scaling',
-        RobustScaler(quantile_range=(25, 75)).fit_transform(X)),
-    ('Data after power transformation (Yeo-Johnson)',
-     PowerTransformer(method='yeo-johnson').fit_transform(X)),
-    ('Data after power transformation (Box-Cox)',
-     PowerTransformer(method='box-cox').fit_transform(X)),
-    ('Data after quantile transformation (gaussian pdf)',
-        QuantileTransformer(output_distribution='normal')
-        .fit_transform(X)),
-    ('Data after quantile transformation (uniform pdf)',
-        QuantileTransformer(output_distribution='uniform')
-        .fit_transform(X)),
-    ('Data after sample-wise L2 normalizing',
-        Normalizer().fit_transform(X)),
+    ("Unscaled data", X),
+    ("Data after standard scaling", StandardScaler().fit_transform(X)),
+    ("Data after min-max scaling", MinMaxScaler().fit_transform(X)),
+    ("Data after max-abs scaling", MaxAbsScaler().fit_transform(X)),
+    (
+        "Data after robust scaling",
+        RobustScaler(quantile_range=(25, 75)).fit_transform(X),
+    ),
+    (
+        "Data after power transformation (Yeo-Johnson)",
+        PowerTransformer(method="yeo-johnson").fit_transform(X),
+    ),
+    (
+        "Data after power transformation (Box-Cox)",
+        PowerTransformer(method="box-cox").fit_transform(X),
+    ),
+    (
+        "Data after quantile transformation (uniform pdf)",
+        QuantileTransformer(output_distribution="uniform").fit_transform(X),
+    ),
+    (
+        "Data after quantile transformation (gaussian pdf)",
+        QuantileTransformer(output_distribution="normal").fit_transform(X),
+    ),
+    ("Data after sample-wise L2 normalizing", Normalizer().fit_transform(X)),
 ]

 # scale the output between 0 and 1 for the colorbar
 y = minmax_scale(y_full)

 # plasma does not exist in matplotlib < 1.5
-cmap = getattr(cm, 'plasma_r', cm.hot_r)
+cmap = getattr(cm, "plasma_r", cm.hot_r)
+

 def create_axes(title, figsize=(16, 6)):
     fig = plt.figure(figsize=figsize)
@@ -141,13 +155,14 @@
     rect_colorbar = [left, bottom, width, height]
     ax_colorbar = plt.axes(rect_colorbar)

-    return ((ax_scatter, ax_histy, ax_histx),
-            (ax_scatter_zoom, ax_histy_zoom, ax_histx_zoom),
-            ax_colorbar)
-
-
-def plot_distribution(axes, X, y, hist_nbins=50, title="",
-                      x0_label="", x1_label=""):
+    return (
+        (ax_scatter, ax_histy, ax_histx),
+        (ax_scatter_zoom, ax_histy_zoom, ax_histx_zoom),
+        ax_colorbar,
+    )
+
+
+def plot_distribution(axes, X, y, hist_nbins=50, title="", x0_label="", x1_label=""):
     ax, hist_X1, hist_X0 = axes

     ax.set_title(title)
@@ -156,67 +171,83 @@

     # The scatter plot
     colors = cmap(y)
-    ax.scatter(X[:, 0], X[:, 1], alpha=0.5, marker='o', s=5, lw=0, c=colors)
+    ax.scatter(X[:, 0], X[:, 1], alpha=0.5, marker="o", s=5, lw=0, c=colors)

     # Removing the top and the right spine for aesthetics
     # make nice axis layout
-    ax.spines['top'].set_visible(False)
-    ax.spines['right'].set_visible(False)
+    ax.spines["top"].set_visible(False)
+    ax.spines["right"].set_visible(False)
     ax.get_xaxis().tick_bottom()
     ax.get_yaxis().tick_left()
-    ax.spines['left'].set_position(('outward', 10))
-    ax.spines['bottom'].set_position(('outward', 10))
+    ax.spines["left"].set_position(("outward", 10))
+    ax.spines["bottom"].set_position(("outward", 10))

     # Histogram for axis X1 (feature 5)
     hist_X1.set_ylim(ax.get_ylim())
-    hist_X1.hist(X[:, 1], bins=hist_nbins, orientation='horizontal',
-                 color='grey', ec='grey')
-    hist_X1.axis('off')
+    hist_X1.hist(
+        X[:, 1], bins=hist_nbins, orientation="horizontal", color="grey", ec="grey"
+    )
+    hist_X1.axis("off")

     # Histogram for axis X0 (feature 0)
     hist_X0.set_xlim(ax.get_xlim())
-    hist_X0.hist(X[:, 0], bins=hist_nbins, orientation='vertical',
-                 color='grey', ec='grey')
-    hist_X0.axis('off')
-
-###############################################################################
+    hist_X0.hist(
+        X[:, 0], bins=hist_nbins, orientation="vertical", color="grey", ec="grey"
+    )
+    hist_X0.axis("off")
+
+
+# %%
 # Two plots will be shown for each scaler/normalizer/transformer. The left
 # figure will show a scatter plot of the full data set while the right figure
 # will exclude the extreme values considering only 99 % of the data set,
 # excluding marginal outliers. In addition, the marginal distributions for each
-# feature will be shown on the side of the scatter plot.
+# feature will be shown on the sides of the scatter plot.


 def make_plot(item_idx):
     title, X = distributions[item_idx]
     ax_zoom_out, ax_zoom_in, ax_colorbar = create_axes(title)
     axarr = (ax_zoom_out, ax_zoom_in)
-    plot_distribution(axarr[0], X, y, hist_nbins=200,
-                      x0_label="Median Income",
-                      x1_label="Number of households",
-                      title="Full data")
+    plot_distribution(
+        axarr[0],
+        X,
+        y,
+        hist_nbins=200,
+        x0_label=feature_mapping[features[0]],
+        x1_label=feature_mapping[features[1]],
+        title="Full data",
+    )

     # zoom-in
     zoom_in_percentile_range = (0, 99)
     cutoffs_X0 = np.percentile(X[:, 0], zoom_in_percentile_range)
     cutoffs_X1 = np.percentile(X[:, 1], zoom_in_percentile_range)

-    non_outliers_mask = (
-        np.all(X > [cutoffs_X0[0], cutoffs_X1[0]], axis=1) &
-        np.all(X < [cutoffs_X0[1], cutoffs_X1[1]], axis=1))
-    plot_distribution(axarr[1], X[non_outliers_mask], y[non_outliers_mask],
-                      hist_nbins=50,
-                      x0_label="Median Income",
-                      x1_label="Number of households",
-                      title="Zoom-in")
+    non_outliers_mask = np.all(X > [cutoffs_X0[0], cutoffs_X1[0]], axis=1) & np.all(
+        X < [cutoffs_X0[1], cutoffs_X1[1]], axis=1
+    )
+    plot_distribution(
+        axarr[1],
+        X[non_outliers_mask],
+        y[non_outliers_mask],
+        hist_nbins=50,
+        x0_label=feature_mapping[features[0]],
+        x1_label=feature_mapping[features[1]],
+        title="Zoom-in",
+    )

     norm = mpl.colors.Normalize(y_full.min(), y_full.max())
-    mpl.colorbar.ColorbarBase(ax_colorbar, cmap=cmap,
-                              norm=norm, orientation='vertical',
-                              label='Color mapping for values of y')
-
-
-########################################################################
+    mpl.colorbar.ColorbarBase(
+        ax_colorbar,
+        cmap=cmap,
+        norm=norm,
+        orientation="vertical",
+        label="Color mapping for values of y",
+    )
+
+
+# %%
 # .. _results:
 #
 # Original data
@@ -226,63 +257,74 @@
 # left plot showing the entire dataset, and the right zoomed-in to show the
 # dataset without the marginal outliers. A large majority of the samples are
 # compacted to a specific range, [0, 10] for the median income and [0, 6] for
-# the number of households. Note that there are some marginal outliers (some
-# blocks have more than 1200 households). Therefore, a specific pre-processing
-# can be very beneficial depending of the application. In the following, we
-# present some insights and behaviors of those pre-processing methods in the
-# presence of marginal outliers.
+# the average house occupancy. Note that there are some marginal outliers (some
+# blocks have average occupancy of more than 1200). Therefore, a specific
+# pre-processing can be very beneficial depending of the application. In the
+# following, we present some insights and behaviors of those pre-processing
+# methods in the presence of marginal outliers.

 make_plot(0)

-#######################################################################
+# %%
 # StandardScaler
 # --------------
 #
-# ``StandardScaler`` removes the mean and scales the data to unit variance.
+# :class:`~sklearn.preprocessing.StandardScaler` removes the mean and scales
+# the data to unit variance. The scaling shrinks the range of the feature
+# values as shown in the left figure below.
 # However, the outliers have an influence when computing the empirical mean and
-# standard deviation which shrink the range of the feature values as shown in
-# the left figure below. Note in particular that because the outliers on each
+# standard deviation. Note in particular that because the outliers on each
 # feature have different magnitudes, the spread of the transformed data on
 # each feature is very different: most of the data lie in the [-2, 4] range for
 # the transformed median income feature while the same data is squeezed in the
-# smaller [-0.2, 0.2] range for the transformed number of households.
-#
-# ``StandardScaler`` therefore cannot guarantee balanced feature scales in the
+# smaller [-0.2, 0.2] range for the transformed average house occupancy.
+#
+# :class:`~sklearn.preprocessing.StandardScaler` therefore cannot guarantee
+# balanced feature scales in the
 # presence of outliers.

 make_plot(1)

-##########################################################################
+# %%
 # MinMaxScaler
 # ------------
 #
-# ``MinMaxScaler`` rescales the data set such that all feature values are in
+# :class:`~sklearn.preprocessing.MinMaxScaler` rescales the data set such that
+# all feature values are in
 # the range [0, 1] as shown in the right panel below. However, this scaling
-# compress all inliers in the narrow range [0, 0.005] for the transformed
-# number of households.
-#
-# As ``StandardScaler``, ``MinMaxScaler`` is very sensitive to the presence of
-# outliers.
+# compresses all inliers into the narrow range [0, 0.005] for the transformed
+# average house occupancy.
+#
+# Both :class:`~sklearn.preprocessing.StandardScaler` and
+# :class:`~sklearn.preprocessing.MinMaxScaler` are very sensitive to the
+# presence of outliers.

 make_plot(2)

-#############################################################################
+# %%
 # MaxAbsScaler
 # ------------
 #
-# ``MaxAbsScaler`` differs from the previous scaler such that the absolute
-# values are mapped in the range [0, 1]. On positive only data, this scaler
-# behaves similarly to ``MinMaxScaler`` and therefore also suffers from the
-# presence of large outliers.
+# :class:`~sklearn.preprocessing.MaxAbsScaler` is similar to
+# :class:`~sklearn.preprocessing.MinMaxScaler` except that the
+# values are mapped across several ranges depending on whether negative
+# OR positive values are present. If only positive values are present, the
+# range is [0, 1]. If only negative values are present, the range is [-1, 0].
+# If both negative and positive values are present, the range is [-1, 1].
+# On positive only data, both :class:`~sklearn.preprocessing.MinMaxScaler`
+# and :class:`~sklearn.preprocessing.MaxAbsScaler` behave similarly.
+# :class:`~sklearn.preprocessing.MaxAbsScaler` therefore also suffers from
+# the presence of large outliers.

 make_plot(3)

-##############################################################################
+# %%
 # RobustScaler
 # ------------
 #
-# Unlike the previous scalers, the centering and scaling statistics of this
-# scaler are based on percentiles and are therefore not influenced by a few
+# Unlike the previous scalers, the centering and scaling statistics of
+# :class:`~sklearn.preprocessing.RobustScaler`
+# is based on percentiles and are therefore not influenced by a few
 # number of very large marginal outliers. Consequently, the resulting range of
 # the transformed feature values is larger than for the previous scalers and,
 # more importantly, are approximately similar: for both features most of the
@@ -293,57 +335,61 @@

 make_plot(4)

-##############################################################################
+# %%
 # PowerTransformer
 # ----------------
 #
-# ``PowerTransformer`` applies a power transformation to each feature to make
-# the data more Gaussian-like. Currently, ``PowerTransformer`` implements the
-# Yeo-Johnson and Box-Cox transforms. The power transform finds the optimal
-# scaling factor to stabilize variance and mimimize skewness through maximum
-# likelihood estimation. By default, ``PowerTransformer`` also applies
-# zero-mean, unit variance normalization to the transformed output. Note that
-# Box-Cox can only be applied to strictly positive data. Income and number of
-# households happen to be strictly positive, but if negative values are present
-# the Yeo-Johnson transformed is to be preferred.
+# :class:`~sklearn.preprocessing.PowerTransformer` applies a power
+# transformation to each feature to make the data more Gaussian-like in order
+# to stabilize variance and minimize skewness. Currently the Yeo-Johnson
+# and Box-Cox transforms are supported and the optimal
+# scaling factor is determined via maximum likelihood estimation in both
+# methods. By default, :class:`~sklearn.preprocessing.PowerTransformer` applies
+# zero-mean, unit variance normalization. Note that
+# Box-Cox can only be applied to strictly positive data. Income and average
+# house occupancy happen to be strictly positive, but if negative values are
+# present the Yeo-Johnson transformed is preferred.

 make_plot(5)
 make_plot(6)
+
+# %%
+# QuantileTransformer (uniform output)
+# ------------------------------------
+#
+# :class:`~sklearn.preprocessing.QuantileTransformer` applies a non-linear
+# transformation such that the
+# probability density function of each feature will be mapped to a uniform
+# or Gaussian distribution. In this case, all the data, including outliers,
+# will be mapped to a uniform distribution with the range [0, 1], making
+# outliers indistinguishable from inliers.
+#
+# :class:`~sklearn.preprocessing.RobustScaler` and
+# :class:`~sklearn.preprocessing.QuantileTransformer` are robust to outliers in
+# the sense that adding or removing outliers in the training set will yield
+# approximately the same transformation. But contrary to
+# :class:`~sklearn.preprocessing.RobustScaler`,
+# :class:`~sklearn.preprocessing.QuantileTransformer` will also automatically
+# collapse any outlier by setting them to the a priori defined range boundaries
+# (0 and 1). This can result in saturation artifacts for extreme values.
+
+make_plot(7)

 ##############################################################################
 # QuantileTransformer (Gaussian output)
 # -------------------------------------
 #
-# ``QuantileTransformer`` has an additional ``output_distribution`` parameter
-# allowing to match a Gaussian distribution instead of a uniform distribution.
-# Note that this non-parametetric transformer introduces saturation artifacts
-# for extreme values.
-
-make_plot(7)
-
-###################################################################
-# QuantileTransformer (uniform output)
-# ------------------------------------
-#
-# ``QuantileTransformer`` applies a non-linear transformation such that the
-# probability density function of each feature will be mapped to a uniform
-# distribution. In this case, all the data will be mapped in the range [0, 1],
-# even the outliers which cannot be distinguished anymore from the inliers.
-#
-# As ``RobustScaler``, ``QuantileTransformer`` is robust to outliers in the
-# sense that adding or removing outliers in the training set will yield
-# approximately the same transformation on held out data. But contrary to
-# ``RobustScaler``, ``QuantileTransformer`` will also automatically collapse
-# any outlier by setting them to the a priori defined range boundaries (0 and
-# 1).
+# To map to a Gaussian distribution, set the parameter
+# ``output_distribution='normal'``.

 make_plot(8)

-##############################################################################
+# %%
 # Normalizer
 # ----------
 #
-# The ``Normalizer`` rescales the vector for each sample to have unit norm,
+# The :class:`~sklearn.preprocessing.Normalizer` rescales the vector for each
+# sample to have unit norm,
 # independently of the distribution of the samples. It can be seen on both
 # figures below where all samples are mapped onto the unit circle. In our
 # example the two selected features have only positive values; therefore the
('examples/text', 'plot_hashing_vs_dict_vectorizer.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -13,10 +13,12 @@

 A discrepancy between the number of terms reported for DictVectorizer and
 for FeatureHasher is to be expected due to hash collisions.
+
 """

 # Author: Lars Buitinck
 # License: BSD 3 clause
+
 from collections import defaultdict
 import re
 import sys
@@ -51,13 +53,13 @@


 categories = [
-    'alt.atheism',
-    'comp.graphics',
-    'comp.sys.ibm.pc.hardware',
-    'misc.forsale',
-    'rec.autos',
-    'sci.space',
-    'talk.religion.misc',
+    "alt.atheism",
+    "comp.graphics",
+    "comp.sys.ibm.pc.hardware",
+    "misc.forsale",
+    "rec.autos",
+    "sci.space",
+    "talk.religion.misc",
 ]
 # Uncomment the following line to use a larger set (11k+ documents)
 # categories = None
@@ -70,15 +72,15 @@
 try:
     n_features = int(sys.argv[1])
 except IndexError:
-    n_features = 2 ** 18
+    n_features = 2**18
 except ValueError:
     print("not a valid number of features: %r" % sys.argv[1])
     sys.exit(1)


 print("Loading 20 newsgroups training data")
-raw_data = fetch_20newsgroups(subset='train', categories=categories).data
-data_size_mb = sum(len(s.encode('utf-8')) for s in raw_data) / 1e6
+raw_data, _ = fetch_20newsgroups(subset="train", categories=categories, return_X_y=True)
+data_size_mb = sum(len(s.encode("utf-8")) for s in raw_data) / 1e6
 print("%d documents - %0.3fMB" % (len(raw_data), data_size_mb))
 print()

@@ -88,7 +90,7 @@
 vectorizer.fit_transform(token_freqs(d) for d in raw_data)
 duration = time() - t0
 print("done in %fs at %0.3fMB/s" % (duration, data_size_mb / duration))
-print("Found %d unique terms" % len(vectorizer.get_feature_names()))
+print("Found %d unique terms" % len(vectorizer.get_feature_names_out()))
 print()

 print("FeatureHasher on frequency dicts")
('examples/text', 'plot_document_clustering.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -52,6 +52,7 @@
 # Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>
 #         Lars Buitinck
 # License: BSD 3 clause
+
 from sklearn.datasets import fetch_20newsgroups
 from sklearn.decomposition import TruncatedSVD
 from sklearn.feature_extraction.text import TfidfVectorizer
@@ -72,37 +73,60 @@


 # Display progress logs on stdout
-logging.basicConfig(level=logging.INFO,
-                    format='%(asctime)s %(levelname)s %(message)s')
+logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

 # parse commandline arguments
 op = OptionParser()
-op.add_option("--lsa",
-              dest="n_components", type="int",
-              help="Preprocess documents with latent semantic analysis.")
-op.add_option("--no-minibatch",
-              action="store_false", dest="minibatch", default=True,
-              help="Use ordinary k-means algorithm (in batch mode).")
-op.add_option("--no-idf",
-              action="store_false", dest="use_idf", default=True,
-              help="Disable Inverse Document Frequency feature weighting.")
-op.add_option("--use-hashing",
-              action="store_true", default=False,
-              help="Use a hashing feature vectorizer")
-op.add_option("--n-features", type=int, default=10000,
-              help="Maximum number of features (dimensions)"
-                   " to extract from text.")
-op.add_option("--verbose",
-              action="store_true", dest="verbose", default=False,
-              help="Print progress reports inside k-means algorithm.")
+op.add_option(
+    "--lsa",
+    dest="n_components",
+    type="int",
+    help="Preprocess documents with latent semantic analysis.",
+)
+op.add_option(
+    "--no-minibatch",
+    action="store_false",
+    dest="minibatch",
+    default=True,
+    help="Use ordinary k-means algorithm (in batch mode).",
+)
+op.add_option(
+    "--no-idf",
+    action="store_false",
+    dest="use_idf",
+    default=True,
+    help="Disable Inverse Document Frequency feature weighting.",
+)
+op.add_option(
+    "--use-hashing",
+    action="store_true",
+    default=False,
+    help="Use a hashing feature vectorizer",
+)
+op.add_option(
+    "--n-features",
+    type=int,
+    default=10000,
+    help="Maximum number of features (dimensions) to extract from text.",
+)
+op.add_option(
+    "--verbose",
+    action="store_true",
+    dest="verbose",
+    default=False,
+    help="Print progress reports inside k-means algorithm.",
+)

 print(__doc__)
-op.print_help()


 def is_interactive():
-    return not hasattr(sys.modules['__main__'], '__file__')
-
+    return not hasattr(sys.modules["__main__"], "__file__")
+
+
+if not is_interactive():
+    op.print_help()
+    print()

 # work-around for Jupyter notebook and IPython console
 argv = [] if is_interactive() else sys.argv[1:]
@@ -112,13 +136,15 @@
     sys.exit(1)


-# #############################################################################
+# %%
 # Load some categories from the training set
+# ------------------------------------------
+
 categories = [
-    'alt.atheism',
-    'talk.religion.misc',
-    'comp.graphics',
-    'sci.space',
+    "alt.atheism",
+    "talk.religion.misc",
+    "comp.graphics",
+    "sci.space",
 ]
 # Uncomment the following to do the analysis on all the categories
 # categories = None
@@ -126,35 +152,49 @@
 print("Loading 20 newsgroups dataset for categories:")
 print(categories)

-dataset = fetch_20newsgroups(subset='all', categories=categories,
-                             shuffle=True, random_state=42)
+dataset = fetch_20newsgroups(
+    subset="all", categories=categories, shuffle=True, random_state=42
+)

 print("%d documents" % len(dataset.data))
 print("%d categories" % len(dataset.target_names))
 print()

+
+# %%
+# Feature Extraction
+# ------------------
+
 labels = dataset.target
 true_k = np.unique(labels).shape[0]

-print("Extracting features from the training dataset "
-      "using a sparse vectorizer")
+print("Extracting features from the training dataset using a sparse vectorizer")
 t0 = time()
 if opts.use_hashing:
     if opts.use_idf:
         # Perform an IDF normalization on the output of HashingVectorizer
-        hasher = HashingVectorizer(n_features=opts.n_features,
-                                   stop_words='english', alternate_sign=False,
-                                   norm=None, binary=False)
+        hasher = HashingVectorizer(
+            n_features=opts.n_features,
+            stop_words="english",
+            alternate_sign=False,
+            norm=None,
+        )
         vectorizer = make_pipeline(hasher, TfidfTransformer())
     else:
-        vectorizer = HashingVectorizer(n_features=opts.n_features,
-                                       stop_words='english',
-                                       alternate_sign=False, norm='l2',
-                                       binary=False)
+        vectorizer = HashingVectorizer(
+            n_features=opts.n_features,
+            stop_words="english",
+            alternate_sign=False,
+            norm="l2",
+        )
 else:
-    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,
-                                 min_df=2, stop_words='english',
-                                 use_idf=opts.use_idf)
+    vectorizer = TfidfVectorizer(
+        max_df=0.5,
+        max_features=opts.n_features,
+        min_df=2,
+        stop_words="english",
+        use_idf=opts.use_idf,
+    )
 X = vectorizer.fit_transform(dataset.data)

 print("done in %fs" % (time() - t0))
@@ -176,21 +216,34 @@
     print("done in %fs" % (time() - t0))

     explained_variance = svd.explained_variance_ratio_.sum()
-    print("Explained variance of the SVD step: {}%".format(
-        int(explained_variance * 100)))
+    print(
+        "Explained variance of the SVD step: {}%".format(int(explained_variance * 100))
+    )

     print()


-# #############################################################################
-# Do the actual clustering
+# %%
+# Clustering
+# ----------

 if opts.minibatch:
-    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,
-                         init_size=1000, batch_size=1000, verbose=opts.verbose)
+    km = MiniBatchKMeans(
+        n_clusters=true_k,
+        init="k-means++",
+        n_init=1,
+        init_size=1000,
+        batch_size=1000,
+        verbose=opts.verbose,
+    )
 else:
-    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,
-                verbose=opts.verbose)
+    km = KMeans(
+        n_clusters=true_k,
+        init="k-means++",
+        max_iter=100,
+        n_init=1,
+        verbose=opts.verbose,
+    )

 print("Clustering sparse data with %s" % km)
 t0 = time()
@@ -198,16 +251,24 @@
 print("done in %0.3fs" % (time() - t0))
 print()

+
+# %%
+# Performance metrics
+# -------------------
+
 print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels, km.labels_))
 print("Completeness: %0.3f" % metrics.completeness_score(labels, km.labels_))
 print("V-measure: %0.3f" % metrics.v_measure_score(labels, km.labels_))
-print("Adjusted Rand-Index: %.3f"
-      % metrics.adjusted_rand_score(labels, km.labels_))
-print("Silhouette Coefficient: %0.3f"
-      % metrics.silhouette_score(X, km.labels_, sample_size=1000))
-
-print()
-
+print("Adjusted Rand-Index: %.3f" % metrics.adjusted_rand_score(labels, km.labels_))
+print(
+    "Silhouette Coefficient: %0.3f"
+    % metrics.silhouette_score(X, km.labels_, sample_size=1000)
+)
+
+print()
+
+
+# %%

 if not opts.use_hashing:
     print("Top terms per cluster:")
@@ -218,9 +279,9 @@
     else:
         order_centroids = km.cluster_centers_.argsort()[:, ::-1]

-    terms = vectorizer.get_feature_names()
+    terms = vectorizer.get_feature_names_out()
     for i in range(true_k):
-        print("Cluster %d:" % i, end='')
+        print("Cluster %d:" % i, end="")
         for ind in order_centroids[i, :10]:
-            print(' %s' % terms[ind], end='')
+            print(" %s" % terms[ind], end="")
         print()
('examples/text', 'plot_document_classification_20newsgroups.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -10,9 +10,6 @@

 The dataset used in this example is the 20 newsgroups dataset. It will be
 automatically downloaded, then cached.
-
-The bar plot indicates the accuracy, training time (normalized) and test time
-(normalized) of each classifier.

 """

@@ -21,18 +18,182 @@
 #         Mathieu Blondel <mathieu@mblondel.org>
 #         Lars Buitinck
 # License: BSD 3 clause
-import logging
-import numpy as np
-from optparse import OptionParser
-import sys
+
+
+# %%
+# Configuration options for the analysis
+# --------------------------------------
+
+# If True, we use `HashingVectorizer`, otherwise we use a `TfidfVectorizer`
+USE_HASHING = False
+
+# Number of features used by `HashingVectorizer`
+N_FEATURES = 2**16
+
+# Optional feature selection: either False, or an integer: the number of
+# features to select
+SELECT_CHI2 = False
+
+
+# %%
+# Load data from the training set
+# ------------------------------------
+# Let's load data from the newsgroups dataset which comprises around 18000
+# newsgroups posts on 20 topics split in two subsets: one for training (or
+# development) and the other one for testing (or for performance evaluation).
+from sklearn.datasets import fetch_20newsgroups
+
+categories = [
+    "alt.atheism",
+    "talk.religion.misc",
+    "comp.graphics",
+    "sci.space",
+]
+
+data_train = fetch_20newsgroups(
+    subset="train", categories=categories, shuffle=True, random_state=42
+)
+
+data_test = fetch_20newsgroups(
+    subset="test", categories=categories, shuffle=True, random_state=42
+)
+print("data loaded")
+
+# order of labels in `target_names` can be different from `categories`
+target_names = data_train.target_names
+
+
+def size_mb(docs):
+    return sum(len(s.encode("utf-8")) for s in docs) / 1e6
+
+
+data_train_size_mb = size_mb(data_train.data)
+data_test_size_mb = size_mb(data_test.data)
+
+print(
+    "%d documents - %0.3fMB (training set)" % (len(data_train.data), data_train_size_mb)
+)
+print("%d documents - %0.3fMB (test set)" % (len(data_test.data), data_test_size_mb))
+print("%d categories" % len(target_names))
+
+# %%
+# Vectorize the training and test data
+# -------------------------------------
+#
+# split a training set and a test set
+y_train, y_test = data_train.target, data_test.target
+
+# %%
+# Extracting features from the training data using a sparse vectorizer
 from time import time
-import matplotlib.pyplot as plt
-
-from sklearn.datasets import fetch_20newsgroups
+
 from sklearn.feature_extraction.text import TfidfVectorizer
 from sklearn.feature_extraction.text import HashingVectorizer
+
+t0 = time()
+
+if USE_HASHING:
+    vectorizer = HashingVectorizer(
+        stop_words="english", alternate_sign=False, n_features=N_FEATURES
+    )
+    X_train = vectorizer.transform(data_train.data)
+else:
+    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words="english")
+    X_train = vectorizer.fit_transform(data_train.data)
+duration = time() - t0
+print("done in %fs at %0.3fMB/s" % (duration, data_train_size_mb / duration))
+print("n_samples: %d, n_features: %d" % X_train.shape)
+
+# %%
+# Extracting features from the test data using the same vectorizer
+t0 = time()
+X_test = vectorizer.transform(data_test.data)
+duration = time() - t0
+print("done in %fs at %0.3fMB/s" % (duration, data_test_size_mb / duration))
+print("n_samples: %d, n_features: %d" % X_test.shape)
+
+# %%
+# mapping from integer feature name to original token string
+if USE_HASHING:
+    feature_names = None
+else:
+    feature_names = vectorizer.get_feature_names_out()
+
+# %%
+# Keeping only the best features
+from sklearn.feature_selection import SelectKBest, chi2
+
+if SELECT_CHI2:
+    print("Extracting %d best features by a chi-squared test" % SELECT_CHI2)
+    t0 = time()
+    ch2 = SelectKBest(chi2, k=SELECT_CHI2)
+    X_train = ch2.fit_transform(X_train, y_train)
+    X_test = ch2.transform(X_test)
+    if feature_names is not None:
+        # keep selected feature names
+        feature_names = feature_names[ch2.get_support()]
+    print("done in %fs" % (time() - t0))
+    print()
+
+
+# %%
+# Benchmark classifiers
+# ------------------------------------
+#
+# First we define small benchmarking utilities
+import numpy as np
+from sklearn import metrics
+from sklearn.utils.extmath import density
+
+
+def trim(s):
+    """Trim string to fit on terminal (assuming 80-column display)"""
+    return s if len(s) <= 80 else s[:77] + "..."
+
+
+def benchmark(clf):
+    print("_" * 80)
+    print("Training: ")
+    print(clf)
+    t0 = time()
+    clf.fit(X_train, y_train)
+    train_time = time() - t0
+    print("train time: %0.3fs" % train_time)
+
+    t0 = time()
+    pred = clf.predict(X_test)
+    test_time = time() - t0
+    print("test time:  %0.3fs" % test_time)
+
+    score = metrics.accuracy_score(y_test, pred)
+    print("accuracy:   %0.3f" % score)
+
+    if hasattr(clf, "coef_"):
+        print("dimensionality: %d" % clf.coef_.shape[1])
+        print("density: %f" % density(clf.coef_))
+
+        if feature_names is not None:
+            print("top 10 keywords per class:")
+            for i, label in enumerate(target_names):
+                top10 = np.argsort(clf.coef_[i])[-10:]
+                print(trim("%s: %s" % (label, " ".join(feature_names[top10]))))
+        print()
+
+    print("classification report:")
+    print(metrics.classification_report(y_test, pred, target_names=target_names))
+
+    print("confusion matrix:")
+    print(metrics.confusion_matrix(y_test, pred))
+
+    print()
+    clf_descr = str(clf).split("(")[0]
+    return clf_descr, score, train_time, test_time
+
+
+# %%
+# We now train and test the datasets with 15 different classification
+# models and get performance results for each model.
 from sklearn.feature_selection import SelectFromModel
-from sklearn.feature_selection import SelectKBest, chi2
 from sklearn.linear_model import RidgeClassifier
 from sklearn.pipeline import Pipeline
 from sklearn.svm import LinearSVC
@@ -43,258 +204,73 @@
 from sklearn.neighbors import KNeighborsClassifier
 from sklearn.neighbors import NearestCentroid
 from sklearn.ensemble import RandomForestClassifier
-from sklearn.utils.extmath import density
-from sklearn import metrics
-
-
-# Display progress logs on stdout
-logging.basicConfig(level=logging.INFO,
-                    format='%(asctime)s %(levelname)s %(message)s')
-
-
-# parse commandline arguments
-op = OptionParser()
-op.add_option("--report",
-              action="store_true", dest="print_report",
-              help="Print a detailed classification report.")
-op.add_option("--chi2_select",
-              action="store", type="int", dest="select_chi2",
-              help="Select some number of features using a chi-squared test")
-op.add_option("--confusion_matrix",
-              action="store_true", dest="print_cm",
-              help="Print the confusion matrix.")
-op.add_option("--top10",
-              action="store_true", dest="print_top10",
-              help="Print ten most discriminative terms per class"
-                   " for every classifier.")
-op.add_option("--all_categories",
-              action="store_true", dest="all_categories",
-              help="Whether to use all categories or not.")
-op.add_option("--use_hashing",
-              action="store_true",
-              help="Use a hashing vectorizer.")
-op.add_option("--n_features",
-              action="store", type=int, default=2 ** 16,
-              help="n_features when using the hashing vectorizer.")
-op.add_option("--filtered",
-              action="store_true",
-              help="Remove newsgroup information that is easily overfit: "
-                   "headers, signatures, and quoting.")
-
-
-def is_interactive():
-    return not hasattr(sys.modules['__main__'], '__file__')
-
-
-# work-around for Jupyter notebook and IPython console
-argv = [] if is_interactive() else sys.argv[1:]
-(opts, args) = op.parse_args(argv)
-if len(args) > 0:
-    op.error("this script takes no arguments.")
-    sys.exit(1)
-
-print(__doc__)
-op.print_help()
-print()
-
-
-# #############################################################################
-# Load some categories from the training set
-if opts.all_categories:
-    categories = None
-else:
-    categories = [
-        'alt.atheism',
-        'talk.religion.misc',
-        'comp.graphics',
-        'sci.space',
-    ]
-
-if opts.filtered:
-    remove = ('headers', 'footers', 'quotes')
-else:
-    remove = ()
-
-print("Loading 20 newsgroups dataset for categories:")
-print(categories if categories else "all")
-
-data_train = fetch_20newsgroups(subset='train', categories=categories,
-                                shuffle=True, random_state=42,
-                                remove=remove)
-
-data_test = fetch_20newsgroups(subset='test', categories=categories,
-                               shuffle=True, random_state=42,
-                               remove=remove)
-print('data loaded')
-
-# order of labels in `target_names` can be different from `categories`
-target_names = data_train.target_names
-
-
-def size_mb(docs):
-    return sum(len(s.encode('utf-8')) for s in docs) / 1e6
-
-
-data_train_size_mb = size_mb(data_train.data)
-data_test_size_mb = size_mb(data_test.data)
-
-print("%d documents - %0.3fMB (training set)" % (
-    len(data_train.data), data_train_size_mb))
-print("%d documents - %0.3fMB (test set)" % (
-    len(data_test.data), data_test_size_mb))
-print("%d categories" % len(target_names))
-print()
-
-# split a training set and a test set
-y_train, y_test = data_train.target, data_test.target
-
-print("Extracting features from the training data using a sparse vectorizer")
-t0 = time()
-if opts.use_hashing:
-    vectorizer = HashingVectorizer(stop_words='english', alternate_sign=False,
-                                   n_features=opts.n_features)
-    X_train = vectorizer.transform(data_train.data)
-else:
-    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,
-                                 stop_words='english')
-    X_train = vectorizer.fit_transform(data_train.data)
-duration = time() - t0
-print("done in %fs at %0.3fMB/s" % (duration, data_train_size_mb / duration))
-print("n_samples: %d, n_features: %d" % X_train.shape)
-print()
-
-print("Extracting features from the test data using the same vectorizer")
-t0 = time()
-X_test = vectorizer.transform(data_test.data)
-duration = time() - t0
-print("done in %fs at %0.3fMB/s" % (duration, data_test_size_mb / duration))
-print("n_samples: %d, n_features: %d" % X_test.shape)
-print()
-
-# mapping from integer feature name to original token string
-if opts.use_hashing:
-    feature_names = None
-else:
-    feature_names = vectorizer.get_feature_names()
-
-if opts.select_chi2:
-    print("Extracting %d best features by a chi-squared test" %
-          opts.select_chi2)
-    t0 = time()
-    ch2 = SelectKBest(chi2, k=opts.select_chi2)
-    X_train = ch2.fit_transform(X_train, y_train)
-    X_test = ch2.transform(X_test)
-    if feature_names:
-        # keep selected feature names
-        feature_names = [feature_names[i] for i
-                         in ch2.get_support(indices=True)]
-    print("done in %fs" % (time() - t0))
-    print()
-
-if feature_names:
-    feature_names = np.asarray(feature_names)
-
-
-def trim(s):
-    """Trim string to fit on terminal (assuming 80-column display)"""
-    return s if len(s) <= 80 else s[:77] + "..."
-
-
-# #############################################################################
-# Benchmark classifiers
-def benchmark(clf):
-    print('_' * 80)
-    print("Training: ")
-    print(clf)
-    t0 = time()
-    clf.fit(X_train, y_train)
-    train_time = time() - t0
-    print("train time: %0.3fs" % train_time)
-
-    t0 = time()
-    pred = clf.predict(X_test)
-    test_time = time() - t0
-    print("test time:  %0.3fs" % test_time)
-
-    score = metrics.accuracy_score(y_test, pred)
-    print("accuracy:   %0.3f" % score)
-
-    if hasattr(clf, 'coef_'):
-        print("dimensionality: %d" % clf.coef_.shape[1])
-        print("density: %f" % density(clf.coef_))
-
-        if opts.print_top10 and feature_names is not None:
-            print("top 10 keywords per class:")
-            for i, label in enumerate(target_names):
-                top10 = np.argsort(clf.coef_[i])[-10:]
-                print(trim("%s: %s" % (label, " ".join(feature_names[top10]))))
-        print()
-
-    if opts.print_report:
-        print("classification report:")
-        print(metrics.classification_report(y_test, pred,
-                                            target_names=target_names))
-
-    if opts.print_cm:
-        print("confusion matrix:")
-        print(metrics.confusion_matrix(y_test, pred))
-
-    print()
-    clf_descr = str(clf).split('(')[0]
-    return clf_descr, score, train_time, test_time


 results = []
 for clf, name in (
-        (RidgeClassifier(tol=1e-2, solver="sag"), "Ridge Classifier"),
-        (Perceptron(max_iter=50, tol=1e-3), "Perceptron"),
-        (PassiveAggressiveClassifier(max_iter=50, tol=1e-3),
-         "Passive-Aggressive"),
-        (KNeighborsClassifier(n_neighbors=10), "kNN"),
-        (RandomForestClassifier(n_estimators=100), "Random forest")):
-    print('=' * 80)
+    (RidgeClassifier(tol=1e-2, solver="sag"), "Ridge Classifier"),
+    (Perceptron(max_iter=50), "Perceptron"),
+    (PassiveAggressiveClassifier(max_iter=50), "Passive-Aggressive"),
+    (KNeighborsClassifier(n_neighbors=10), "kNN"),
+    (RandomForestClassifier(), "Random forest"),
+):
+    print("=" * 80)
     print(name)
     results.append(benchmark(clf))

 for penalty in ["l2", "l1"]:
-    print('=' * 80)
+    print("=" * 80)
     print("%s penalty" % penalty.upper())
     # Train Liblinear model
-    results.append(benchmark(LinearSVC(penalty=penalty, dual=False,
-                                       tol=1e-3)))
+    results.append(benchmark(LinearSVC(penalty=penalty, dual=False, tol=1e-3)))

     # Train SGD model
-    results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,
-                                           penalty=penalty)))
+    results.append(benchmark(SGDClassifier(alpha=0.0001, max_iter=50, penalty=penalty)))

 # Train SGD with Elastic Net penalty
-print('=' * 80)
+print("=" * 80)
 print("Elastic-Net penalty")
-results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,
-                                       penalty="elasticnet")))
+results.append(
+    benchmark(SGDClassifier(alpha=0.0001, max_iter=50, penalty="elasticnet"))
+)

 # Train NearestCentroid without threshold
-print('=' * 80)
+print("=" * 80)
 print("NearestCentroid (aka Rocchio classifier)")
 results.append(benchmark(NearestCentroid()))

 # Train sparse Naive Bayes classifiers
-print('=' * 80)
+print("=" * 80)
 print("Naive Bayes")
-results.append(benchmark(MultinomialNB(alpha=.01)))
-results.append(benchmark(BernoulliNB(alpha=.01)))
-results.append(benchmark(ComplementNB(alpha=.1)))
-
-print('=' * 80)
+results.append(benchmark(MultinomialNB(alpha=0.01)))
+results.append(benchmark(BernoulliNB(alpha=0.01)))
+results.append(benchmark(ComplementNB(alpha=0.1)))
+
+print("=" * 80)
 print("LinearSVC with L1-based feature selection")
 # The smaller C, the stronger the regularization.
 # The more regularization, the more sparsity.
-results.append(benchmark(Pipeline([
-  ('feature_selection', SelectFromModel(LinearSVC(penalty="l1", dual=False,
-                                                  tol=1e-3))),
-  ('classification', LinearSVC(penalty="l2"))])))
-
-# make some plots
+results.append(
+    benchmark(
+        Pipeline(
+            [
+                (
+                    "feature_selection",
+                    SelectFromModel(LinearSVC(penalty="l1", dual=False, tol=1e-3)),
+                ),
+                ("classification", LinearSVC(penalty="l2")),
+            ]
+        )
+    )
+)
+
+
+# %%
+# Add plots
+# ------------------------------------
+# The bar plot indicates the accuracy, training time (normalized) and test time
+# (normalized) of each classifier.
+import matplotlib.pyplot as plt

 indices = np.arange(len(results))

@@ -306,17 +282,16 @@

 plt.figure(figsize=(12, 8))
 plt.title("Score")
-plt.barh(indices, score, .2, label="score", color='navy')
-plt.barh(indices + .3, training_time, .2, label="training time",
-         color='c')
-plt.barh(indices + .6, test_time, .2, label="test time", color='darkorange')
+plt.barh(indices, score, 0.2, label="score", color="navy")
+plt.barh(indices + 0.3, training_time, 0.2, label="training time", color="c")
+plt.barh(indices + 0.6, test_time, 0.2, label="test time", color="darkorange")
 plt.yticks(())
-plt.legend(loc='best')
-plt.subplots_adjust(left=.25)
-plt.subplots_adjust(top=.95)
-plt.subplots_adjust(bottom=.05)
+plt.legend(loc="best")
+plt.subplots_adjust(left=0.25)
+plt.subplots_adjust(top=0.95)
+plt.subplots_adjust(bottom=0.05)

 for i, c in zip(indices, clf_names):
-    plt.text(-.3, i, c)
+    plt.text(-0.3, i, c)

 plt.show()
('examples/model_selection', 'plot_roc.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -15,27 +15,23 @@
 The "steepness" of ROC curves is also important, since it is ideal to maximize
 the true positive rate while minimizing the false positive rate.

-Multiclass settings
--------------------
-
 ROC curves are typically used in binary classification to study the output of
-a classifier. In order to extend ROC curve and ROC area to multi-class
-or multi-label classification, it is necessary to binarize the output. One ROC
+a classifier. In order to extend ROC curve and ROC area to multi-label
+classification, it is necessary to binarize the output. One ROC
 curve can be drawn per label, but one can also draw a ROC curve by considering
 each element of the label indicator matrix as a binary prediction
 (micro-averaging).

-Another evaluation measure for multi-class classification is
+Another evaluation measure for multi-label classification is
 macro-averaging, which gives equal weight to the classification of each
 label.

 .. note::

     See also :func:`sklearn.metrics.roc_auc_score`,
-             :ref:`sphx_glr_auto_examples_model_selection_plot_roc_crossval.py`.
+             :ref:`sphx_glr_auto_examples_model_selection_plot_roc_crossval.py`

 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -46,7 +42,7 @@
 from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import label_binarize
 from sklearn.multiclass import OneVsRestClassifier
-from scipy import interp
+from sklearn.metrics import roc_auc_score

 # Import some data to play with
 iris = datasets.load_iris()
@@ -63,12 +59,12 @@
 X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]

 # shuffle and split training and test sets
-X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,
-                                                    random_state=0)
+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)

 # Learn to predict each class against the other
-classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,
-                                 random_state=random_state))
+classifier = OneVsRestClassifier(
+    svm.SVC(kernel="linear", probability=True, random_state=random_state)
+)
 y_score = classifier.fit(X_train, y_train).decision_function(X_test)

 # Compute ROC curve and ROC area for each class
@@ -84,25 +80,30 @@
 roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])


-##############################################################################
+# %%
 # Plot of a ROC curve for a specific class
 plt.figure()
 lw = 2
-plt.plot(fpr[2], tpr[2], color='darkorange',
-         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])
-plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
+plt.plot(
+    fpr[2],
+    tpr[2],
+    color="darkorange",
+    lw=lw,
+    label="ROC curve (area = %0.2f)" % roc_auc[2],
+)
+plt.plot([0, 1], [0, 1], color="navy", lw=lw, linestyle="--")
 plt.xlim([0.0, 1.0])
 plt.ylim([0.0, 1.05])
-plt.xlabel('False Positive Rate')
-plt.ylabel('True Positive Rate')
-plt.title('Receiver operating characteristic example')
+plt.xlabel("False Positive Rate")
+plt.ylabel("True Positive Rate")
+plt.title("Receiver operating characteristic example")
 plt.legend(loc="lower right")
 plt.show()


-##############################################################################
+# %%
 # Plot ROC curves for the multiclass problem
-
+# ..........................................
 # Compute macro-average ROC curve and ROC area

 # First aggregate all false positive rates
@@ -111,7 +112,7 @@
 # Then interpolate all ROC curves at this points
 mean_tpr = np.zeros_like(all_fpr)
 for i in range(n_classes):
-    mean_tpr += interp(all_fpr, fpr[i], tpr[i])
+    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])

 # Finally average it and compute AUC
 mean_tpr /= n_classes
@@ -122,27 +123,67 @@

 # Plot all ROC curves
 plt.figure()
-plt.plot(fpr["micro"], tpr["micro"],
-         label='micro-average ROC curve (area = {0:0.2f})'
-               ''.format(roc_auc["micro"]),
-         color='deeppink', linestyle=':', linewidth=4)
+plt.plot(
+    fpr["micro"],
+    tpr["micro"],
+    label="micro-average ROC curve (area = {0:0.2f})".format(roc_auc["micro"]),
+    color="deeppink",
+    linestyle=":",
+    linewidth=4,
+)

-plt.plot(fpr["macro"], tpr["macro"],
-         label='macro-average ROC curve (area = {0:0.2f})'
-               ''.format(roc_auc["macro"]),
-         color='navy', linestyle=':', linewidth=4)
+plt.plot(
+    fpr["macro"],
+    tpr["macro"],
+    label="macro-average ROC curve (area = {0:0.2f})".format(roc_auc["macro"]),
+    color="navy",
+    linestyle=":",
+    linewidth=4,
+)

-colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])
+colors = cycle(["aqua", "darkorange", "cornflowerblue"])
 for i, color in zip(range(n_classes), colors):
-    plt.plot(fpr[i], tpr[i], color=color, lw=lw,
-             label='ROC curve of class {0} (area = {1:0.2f})'
-             ''.format(i, roc_auc[i]))
+    plt.plot(
+        fpr[i],
+        tpr[i],
+        color=color,
+        lw=lw,
+        label="ROC curve of class {0} (area = {1:0.2f})".format(i, roc_auc[i]),
+    )

-plt.plot([0, 1], [0, 1], 'k--', lw=lw)
+plt.plot([0, 1], [0, 1], "k--", lw=lw)
 plt.xlim([0.0, 1.0])
 plt.ylim([0.0, 1.05])
-plt.xlabel('False Positive Rate')
-plt.ylabel('True Positive Rate')
-plt.title('Some extension of Receiver operating characteristic to multi-class')
+plt.xlabel("False Positive Rate")
+plt.ylabel("True Positive Rate")
+plt.title("Some extension of Receiver operating characteristic to multiclass")
 plt.legend(loc="lower right")
 plt.show()
+
+
+# %%
+# Area under ROC for the multiclass problem
+# .........................................
+# The :func:`sklearn.metrics.roc_auc_score` function can be used for
+# multi-class classification. The multi-class One-vs-One scheme compares every
+# unique pairwise combination of classes. In this section, we calculate the AUC
+# using the OvR and OvO schemes. We report a macro average, and a
+# prevalence-weighted average.
+y_prob = classifier.predict_proba(X_test)
+
+macro_roc_auc_ovo = roc_auc_score(y_test, y_prob, multi_class="ovo", average="macro")
+weighted_roc_auc_ovo = roc_auc_score(
+    y_test, y_prob, multi_class="ovo", average="weighted"
+)
+macro_roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class="ovr", average="macro")
+weighted_roc_auc_ovr = roc_auc_score(
+    y_test, y_prob, multi_class="ovr", average="weighted"
+)
+print(
+    "One-vs-One ROC AUC scores:\n{:.6f} (macro),\n{:.6f} "
+    "(weighted by prevalence)".format(macro_roc_auc_ovo, weighted_roc_auc_ovo)
+)
+print(
+    "One-vs-Rest ROC AUC scores:\n{:.6f} (macro),\n{:.6f} "
+    "(weighted by prevalence)".format(macro_roc_auc_ovr, weighted_roc_auc_ovr)
+)
('examples/model_selection', 'plot_confusion_matrix.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -24,15 +24,12 @@

 """

-print(__doc__)
-
 import numpy as np
 import matplotlib.pyplot as plt

 from sklearn import svm, datasets
 from sklearn.model_selection import train_test_split
-from sklearn.metrics import confusion_matrix
-from sklearn.utils.multiclass import unique_labels
+from sklearn.metrics import ConfusionMatrixDisplay

 # import some data to play with
 iris = datasets.load_iris()
@@ -45,72 +42,27 @@

 # Run classifier, using a model that is too regularized (C too low) to see
 # the impact on the results
-classifier = svm.SVC(kernel='linear', C=0.01)
-y_pred = classifier.fit(X_train, y_train).predict(X_test)
-
-
-def plot_confusion_matrix(y_true, y_pred, classes,
-                          normalize=False,
-                          title=None,
-                          cmap=plt.cm.Blues):
-    """
-    This function prints and plots the confusion matrix.
-    Normalization can be applied by setting `normalize=True`.
-    """
-    if not title:
-        if normalize:
-            title = 'Normalized confusion matrix'
-        else:
-            title = 'Confusion matrix, without normalization'
-
-    # Compute confusion matrix
-    cm = confusion_matrix(y_true, y_pred)
-    # Only use the labels that appear in the data
-    classes = classes[unique_labels(y_true, y_pred)]
-    if normalize:
-        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
-        print("Normalized confusion matrix")
-    else:
-        print('Confusion matrix, without normalization')
-
-    print(cm)
-
-    fig, ax = plt.subplots()
-    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
-    ax.figure.colorbar(im, ax=ax)
-    # We want to show all ticks...
-    ax.set(xticks=np.arange(cm.shape[1]),
-           yticks=np.arange(cm.shape[0]),
-           # ... and label them with the respective list entries
-           xticklabels=classes, yticklabels=classes,
-           title=title,
-           ylabel='True label',
-           xlabel='Predicted label')
-
-    # Rotate the tick labels and set their alignment.
-    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
-             rotation_mode="anchor")
-
-    # Loop over data dimensions and create text annotations.
-    fmt = '.2f' if normalize else 'd'
-    thresh = cm.max() / 2.
-    for i in range(cm.shape[0]):
-        for j in range(cm.shape[1]):
-            ax.text(j, i, format(cm[i, j], fmt),
-                    ha="center", va="center",
-                    color="white" if cm[i, j] > thresh else "black")
-    fig.tight_layout()
-    return ax
-
+classifier = svm.SVC(kernel="linear", C=0.01).fit(X_train, y_train)

 np.set_printoptions(precision=2)

 # Plot non-normalized confusion matrix
-plot_confusion_matrix(y_test, y_pred, classes=class_names,
-                      title='Confusion matrix, without normalization')
+titles_options = [
+    ("Confusion matrix, without normalization", None),
+    ("Normalized confusion matrix", "true"),
+]
+for title, normalize in titles_options:
+    disp = ConfusionMatrixDisplay.from_estimator(
+        classifier,
+        X_test,
+        y_test,
+        display_labels=class_names,
+        cmap=plt.cm.Blues,
+        normalize=normalize,
+    )
+    disp.ax_.set_title(title)

-# Plot normalized confusion matrix
-plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,
-                      title='Normalized confusion matrix')
+    print(title)
+    print(disp.confusion_matrix)

 plt.show()
('examples/model_selection', 'plot_train_error_vs_test_error.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -11,29 +11,33 @@
 measured using the explained variance a.k.a. R^2.

 """
-print(__doc__)

 # Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
 # License: BSD 3 clause

+# %%
+# Generate sample data
+# --------------------
 import numpy as np
 from sklearn import linear_model
+from sklearn.datasets import make_regression
+from sklearn.model_selection import train_test_split

-# #############################################################################
-# Generate sample data
 n_samples_train, n_samples_test, n_features = 75, 150, 500
-np.random.seed(0)
-coef = np.random.randn(n_features)
-coef[50:] = 0.0  # only the top 10 features are impacting the model
-X = np.random.randn(n_samples_train + n_samples_test, n_features)
-y = np.dot(X, coef)
-
-# Split train and test data
-X_train, X_test = X[:n_samples_train], X[n_samples_train:]
-y_train, y_test = y[:n_samples_train], y[n_samples_train:]
-
-# #############################################################################
+X, y, coef = make_regression(
+    n_samples=n_samples_train + n_samples_test,
+    n_features=n_features,
+    n_informative=50,
+    shuffle=False,
+    noise=1.0,
+    coef=True,
+)
+X_train, X_test, y_train, y_test = train_test_split(
+    X, y, train_size=n_samples_train, test_size=n_samples_test, shuffle=False
+)
+# %%
 # Compute train and test errors
+# -----------------------------
 alphas = np.logspace(-5, 1, 60)
 enet = linear_model.ElasticNet(l1_ratio=0.7, max_iter=10000)
 train_errors = list()
@@ -52,24 +56,32 @@
 enet.set_params(alpha=alpha_optim)
 coef_ = enet.fit(X, y).coef_

-# #############################################################################
+# %%
 # Plot results functions
+# ----------------------

 import matplotlib.pyplot as plt
+
 plt.subplot(2, 1, 1)
-plt.semilogx(alphas, train_errors, label='Train')
-plt.semilogx(alphas, test_errors, label='Test')
-plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',
-           linewidth=3, label='Optimum on test')
-plt.legend(loc='lower left')
+plt.semilogx(alphas, train_errors, label="Train")
+plt.semilogx(alphas, test_errors, label="Test")
+plt.vlines(
+    alpha_optim,
+    plt.ylim()[0],
+    np.max(test_errors),
+    color="k",
+    linewidth=3,
+    label="Optimum on test",
+)
+plt.legend(loc="lower left")
 plt.ylim([0, 1.2])
-plt.xlabel('Regularization parameter')
-plt.ylabel('Performance')
+plt.xlabel("Regularization parameter")
+plt.ylabel("Performance")

 # Show estimated coef_ vs true coef
 plt.subplot(2, 1, 2)
-plt.plot(coef, label='True coef')
-plt.plot(coef_, label='Estimated coef')
+plt.plot(coef, label="True coef")
+plt.plot(coef_, label="Estimated coef")
 plt.legend()
 plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.26)
 plt.show()
('examples/model_selection', 'plot_roc_crossval.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -29,18 +29,13 @@
              :ref:`sphx_glr_auto_examples_model_selection_plot_roc.py`,

 """
-print(__doc__)

+# %%
+# Data IO and generation
+# ----------------------
 import numpy as np
-from scipy import interp
-import matplotlib.pyplot as plt

-from sklearn import svm, datasets
-from sklearn.metrics import roc_curve, auc
-from sklearn.model_selection import StratifiedKFold
-
-# #############################################################################
-# Data IO and generation
+from sklearn import datasets

 # Import some data to play with
 iris = datasets.load_iris()
@@ -53,52 +48,72 @@
 random_state = np.random.RandomState(0)
 X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]

-# #############################################################################
+# %%
 # Classification and ROC analysis
+# -------------------------------
+import matplotlib.pyplot as plt
+
+from sklearn import svm
+from sklearn.metrics import auc
+from sklearn.metrics import RocCurveDisplay
+from sklearn.model_selection import StratifiedKFold

 # Run classifier with cross-validation and plot ROC curves
 cv = StratifiedKFold(n_splits=6)
-classifier = svm.SVC(kernel='linear', probability=True,
-                     random_state=random_state)
+classifier = svm.SVC(kernel="linear", probability=True, random_state=random_state)

 tprs = []
 aucs = []
 mean_fpr = np.linspace(0, 1, 100)

-i = 0
-for train, test in cv.split(X, y):
-    probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])
-    # Compute ROC curve and area the curve
-    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])
-    tprs.append(interp(mean_fpr, fpr, tpr))
-    tprs[-1][0] = 0.0
-    roc_auc = auc(fpr, tpr)
-    aucs.append(roc_auc)
-    plt.plot(fpr, tpr, lw=1, alpha=0.3,
-             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))
+fig, ax = plt.subplots()
+for i, (train, test) in enumerate(cv.split(X, y)):
+    classifier.fit(X[train], y[train])
+    viz = RocCurveDisplay.from_estimator(
+        classifier,
+        X[test],
+        y[test],
+        name="ROC fold {}".format(i),
+        alpha=0.3,
+        lw=1,
+        ax=ax,
+    )
+    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
+    interp_tpr[0] = 0.0
+    tprs.append(interp_tpr)
+    aucs.append(viz.roc_auc)

-    i += 1
-plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',
-         label='Chance', alpha=.8)
+ax.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)

 mean_tpr = np.mean(tprs, axis=0)
 mean_tpr[-1] = 1.0
 mean_auc = auc(mean_fpr, mean_tpr)
 std_auc = np.std(aucs)
-plt.plot(mean_fpr, mean_tpr, color='b',
-         label=r'Mean ROC (AUC = %0.2f $\pm$ %0.2f)' % (mean_auc, std_auc),
-         lw=2, alpha=.8)
+ax.plot(
+    mean_fpr,
+    mean_tpr,
+    color="b",
+    label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, std_auc),
+    lw=2,
+    alpha=0.8,
+)

 std_tpr = np.std(tprs, axis=0)
 tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
 tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
-plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,
-                 label=r'$\pm$ 1 std. dev.')
+ax.fill_between(
+    mean_fpr,
+    tprs_lower,
+    tprs_upper,
+    color="grey",
+    alpha=0.2,
+    label=r"$\pm$ 1 std. dev.",
+)

-plt.xlim([-0.05, 1.05])
-plt.ylim([-0.05, 1.05])
-plt.xlabel('False Positive Rate')
-plt.ylabel('True Positive Rate')
-plt.title('Receiver operating characteristic example')
-plt.legend(loc="lower right")
+ax.set(
+    xlim=[-0.05, 1.05],
+    ylim=[-0.05, 1.05],
+    title="Receiver operating characteristic example",
+)
+ax.legend(loc="lower right")
 plt.show()
('examples/model_selection', 'plot_underfitting_overfitting.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -18,9 +18,8 @@
 cross-validation. We calculate the mean squared error (MSE) on the validation
 set, the higher, the less likely the model generalizes correctly from the
 training data.
+
 """
-
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -32,6 +31,7 @@

 def true_fun(X):
     return np.cos(1.5 * np.pi * X)
+

 np.random.seed(0)

@@ -46,26 +46,33 @@
     ax = plt.subplot(1, len(degrees), i + 1)
     plt.setp(ax, xticks=(), yticks=())

-    polynomial_features = PolynomialFeatures(degree=degrees[i],
-                                             include_bias=False)
+    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)
     linear_regression = LinearRegression()
-    pipeline = Pipeline([("polynomial_features", polynomial_features),
-                         ("linear_regression", linear_regression)])
+    pipeline = Pipeline(
+        [
+            ("polynomial_features", polynomial_features),
+            ("linear_regression", linear_regression),
+        ]
+    )
     pipeline.fit(X[:, np.newaxis], y)

     # Evaluate the models using crossvalidation
-    scores = cross_val_score(pipeline, X[:, np.newaxis], y,
-                             scoring="neg_mean_squared_error", cv=10)
+    scores = cross_val_score(
+        pipeline, X[:, np.newaxis], y, scoring="neg_mean_squared_error", cv=10
+    )

     X_test = np.linspace(0, 1, 100)
     plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label="Model")
     plt.plot(X_test, true_fun(X_test), label="True function")
-    plt.scatter(X, y, edgecolor='b', s=20, label="Samples")
+    plt.scatter(X, y, edgecolor="b", s=20, label="Samples")
     plt.xlabel("x")
     plt.ylabel("y")
     plt.xlim((0, 1))
     plt.ylim((-2, 2))
     plt.legend(loc="best")
-    plt.title("Degree {}\nMSE = {:.2e}(+/- {:.2e})".format(
-        degrees[i], -scores.mean(), scores.std()))
+    plt.title(
+        "Degree {}\nMSE = {:.2e}(+/- {:.2e})".format(
+            degrees[i], -scores.mean(), scores.std()
+        )
+    )
 plt.show()
('examples/model_selection', 'plot_validation_curve.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -10,8 +10,8 @@
 values for both scores, i.e. the classifier is performing fairly well. If gamma
 is too high, the classifier will overfit, which means that the training score
 is good but the validation score is poor.
+
 """
-print(__doc__)

 import matplotlib.pyplot as plt
 import numpy as np
@@ -20,13 +20,20 @@
 from sklearn.svm import SVC
 from sklearn.model_selection import validation_curve

-digits = load_digits()
-X, y = digits.data, digits.target
+X, y = load_digits(return_X_y=True)
+subset_mask = np.isin(y, [1, 2])  # binary classification: 1 vs 2
+X, y = X[subset_mask], y[subset_mask]

 param_range = np.logspace(-6, -1, 5)
 train_scores, test_scores = validation_curve(
-    SVC(), X, y, param_name="gamma", param_range=param_range,
-    cv=5, scoring="accuracy", n_jobs=1)
+    SVC(),
+    X,
+    y,
+    param_name="gamma",
+    param_range=param_range,
+    scoring="accuracy",
+    n_jobs=2,
+)
 train_scores_mean = np.mean(train_scores, axis=1)
 train_scores_std = np.std(train_scores, axis=1)
 test_scores_mean = np.mean(test_scores, axis=1)
@@ -37,15 +44,27 @@
 plt.ylabel("Score")
 plt.ylim(0.0, 1.1)
 lw = 2
-plt.semilogx(param_range, train_scores_mean, label="Training score",
-             color="darkorange", lw=lw)
-plt.fill_between(param_range, train_scores_mean - train_scores_std,
-                 train_scores_mean + train_scores_std, alpha=0.2,
-                 color="darkorange", lw=lw)
-plt.semilogx(param_range, test_scores_mean, label="Cross-validation score",
-             color="navy", lw=lw)
-plt.fill_between(param_range, test_scores_mean - test_scores_std,
-                 test_scores_mean + test_scores_std, alpha=0.2,
-                 color="navy", lw=lw)
+plt.semilogx(
+    param_range, train_scores_mean, label="Training score", color="darkorange", lw=lw
+)
+plt.fill_between(
+    param_range,
+    train_scores_mean - train_scores_std,
+    train_scores_mean + train_scores_std,
+    alpha=0.2,
+    color="darkorange",
+    lw=lw,
+)
+plt.semilogx(
+    param_range, test_scores_mean, label="Cross-validation score", color="navy", lw=lw
+)
+plt.fill_between(
+    param_range,
+    test_scores_mean - test_scores_std,
+    test_scores_mean + test_scores_std,
+    alpha=0.2,
+    color="navy",
+    lw=lw,
+)
 plt.legend(loc="best")
 plt.show()
('examples/model_selection', 'plot_cv_predict.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,26 +3,27 @@
 Plotting Cross-Validated Predictions
 ====================================

-This example shows how to use `cross_val_predict` to visualize prediction
+This example shows how to use
+:func:`~sklearn.model_selection.cross_val_predict` to visualize prediction
 errors.

 """
+
 from sklearn import datasets
 from sklearn.model_selection import cross_val_predict
 from sklearn import linear_model
 import matplotlib.pyplot as plt

 lr = linear_model.LinearRegression()
-boston = datasets.load_boston()
-y = boston.target
+X, y = datasets.load_diabetes(return_X_y=True)

 # cross_val_predict returns an array of the same size as `y` where each entry
 # is a prediction obtained by cross validation:
-predicted = cross_val_predict(lr, boston.data, y, cv=10)
+predicted = cross_val_predict(lr, X, y, cv=10)

 fig, ax = plt.subplots()
 ax.scatter(y, predicted, edgecolors=(0, 0, 0))
-ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
-ax.set_xlabel('Measured')
-ax.set_ylabel('Predicted')
+ax.plot([y.min(), y.max()], [y.min(), y.max()], "k--", lw=4)
+ax.set_xlabel("Measured")
+ax.set_ylabel("Predicted")
 plt.show()
('examples/model_selection', 'grid_search_text_feature_extraction.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,4 +1,3 @@
-
 """
 ==========================================================
 Sample pipeline for text feature extraction and evaluation
@@ -46,6 +45,11 @@
 #         Peter Prettenhofer <peter.prettenhofer@gmail.com>
 #         Mathieu Blondel <mathieu@mblondel.org>
 # License: BSD 3 clause
+
+# %%
+# Data loading
+# ------------
+
 from pprint import pprint
 from time import time
 import logging
@@ -57,73 +61,69 @@
 from sklearn.model_selection import GridSearchCV
 from sklearn.pipeline import Pipeline

-print(__doc__)
+# Display progress logs on stdout
+logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

-# Display progress logs on stdout
-logging.basicConfig(level=logging.INFO,
-                    format='%(asctime)s %(levelname)s %(message)s')
-
-
-# #############################################################################
 # Load some categories from the training set
 categories = [
-    'alt.atheism',
-    'talk.religion.misc',
+    "alt.atheism",
+    "talk.religion.misc",
 ]
+
 # Uncomment the following to do the analysis on all the categories
-#categories = None
+# categories = None

 print("Loading 20 newsgroups dataset for categories:")
 print(categories)

-data = fetch_20newsgroups(subset='train', categories=categories)
+data = fetch_20newsgroups(subset="train", categories=categories)
 print("%d documents" % len(data.filenames))
 print("%d categories" % len(data.target_names))
 print()

-# #############################################################################
-# Define a pipeline combining a text feature extractor with a simple
-# classifier
-pipeline = Pipeline([
-    ('vect', CountVectorizer()),
-    ('tfidf', TfidfTransformer()),
-    ('clf', SGDClassifier(tol=1e-3)),
-])
+# %%
+# Pipeline with hyperparameter tuning
+# -----------------------------------

-# uncommenting more parameters will give better exploring power but will
-# increase processing time in a combinatorial way
+# Define a pipeline combining a text feature extractor with a simple classifier
+pipeline = Pipeline(
+    [
+        ("vect", CountVectorizer()),
+        ("tfidf", TfidfTransformer()),
+        ("clf", SGDClassifier()),
+    ]
+)
+
+# Parameters to use for grid search. Uncommenting more parameters will give
+# better exploring power but will increase processing time in a combinatorial
+# way
 parameters = {
-    'vect__max_df': (0.5, 0.75, 1.0),
+    "vect__max_df": (0.5, 0.75, 1.0),
     # 'vect__max_features': (None, 5000, 10000, 50000),
-    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams
+    "vect__ngram_range": ((1, 1), (1, 2)),  # unigrams or bigrams
     # 'tfidf__use_idf': (True, False),
     # 'tfidf__norm': ('l1', 'l2'),
-    'clf__max_iter': (20,),
-    'clf__alpha': (0.00001, 0.000001),
-    'clf__penalty': ('l2', 'elasticnet'),
+    "clf__max_iter": (20,),
+    "clf__alpha": (0.00001, 0.000001),
+    "clf__penalty": ("l2", "elasticnet"),
     # 'clf__max_iter': (10, 50, 80),
 }

-if __name__ == "__main__":
-    # multiprocessing requires the fork to happen in a __main__ protected
-    # block
+# Find the best parameters for both the feature extraction and the
+# classifier
+grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)

-    # find the best parameters for both the feature extraction and the
-    # classifier
-    grid_search = GridSearchCV(pipeline, parameters, cv=5,
-                               n_jobs=-1, verbose=1)
+print("Performing grid search...")
+print("pipeline:", [name for name, _ in pipeline.steps])
+print("parameters:")
+pprint(parameters)
+t0 = time()
+grid_search.fit(data.data, data.target)
+print("done in %0.3fs" % (time() - t0))
+print()

-    print("Performing grid search...")
-    print("pipeline:", [name for name, _ in pipeline.steps])
-    print("parameters:")
-    pprint(parameters)
-    t0 = time()
-    grid_search.fit(data.data, data.target)
-    print("done in %0.3fs" % (time() - t0))
-    print()
-
-    print("Best score: %0.3f" % grid_search.best_score_)
-    print("Best parameters set:")
-    best_parameters = grid_search.best_estimator_.get_params()
-    for param_name in sorted(parameters.keys()):
-        print("\t%s: %r" % (param_name, best_parameters[param_name]))
+print("Best score: %0.3f" % grid_search.best_score_)
+print("Best parameters set:")
+best_parameters = grid_search.best_estimator_.get_params()
+for param_name in sorted(parameters.keys()):
+    print("\t%s: %r" % (param_name, best_parameters[param_name]))
('examples/model_selection', 'plot_cv_indices.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -9,20 +9,29 @@

 This example visualizes the behavior of several common scikit-learn objects
 for comparison.
+
 """

-from sklearn.model_selection import (TimeSeriesSplit, KFold, ShuffleSplit,
-                                     StratifiedKFold, GroupShuffleSplit,
-                                     GroupKFold, StratifiedShuffleSplit)
+from sklearn.model_selection import (
+    TimeSeriesSplit,
+    KFold,
+    ShuffleSplit,
+    StratifiedKFold,
+    GroupShuffleSplit,
+    GroupKFold,
+    StratifiedShuffleSplit,
+    StratifiedGroupKFold,
+)
 import numpy as np
 import matplotlib.pyplot as plt
 from matplotlib.patches import Patch
-np.random.seed(1338)
+
+rng = np.random.RandomState(1338)
 cmap_data = plt.cm.Paired
 cmap_cv = plt.cm.coolwarm
 n_splits = 4

-###############################################################################
+# %%
 # Visualize our data
 # ------------------
 #
@@ -38,30 +47,46 @@

 # Generate the class/group data
 n_points = 100
-X = np.random.randn(100, 10)
-
-percentiles_classes = [.1, .3, .6]
-y = np.hstack([[ii] * int(100 * perc)
-               for ii, perc in enumerate(percentiles_classes)])
-
-# Evenly spaced groups repeated once
-groups = np.hstack([[ii] * 10 for ii in range(10)])
+X = rng.randn(100, 10)
+
+percentiles_classes = [0.1, 0.3, 0.6]
+y = np.hstack([[ii] * int(100 * perc) for ii, perc in enumerate(percentiles_classes)])
+
+# Generate uneven groups
+group_prior = rng.dirichlet([2] * 10)
+groups = np.repeat(np.arange(10), rng.multinomial(100, group_prior))


 def visualize_groups(classes, groups, name):
     # Visualize dataset groups
     fig, ax = plt.subplots()
-    ax.scatter(range(len(groups)),  [.5] * len(groups), c=groups, marker='_',
-               lw=50, cmap=cmap_data)
-    ax.scatter(range(len(groups)),  [3.5] * len(groups), c=classes, marker='_',
-               lw=50, cmap=cmap_data)
-    ax.set(ylim=[-1, 5], yticks=[.5, 3.5],
-           yticklabels=['Data\ngroup', 'Data\nclass'], xlabel="Sample index")
-
-
-visualize_groups(y, groups, 'no groups')
-
-###############################################################################
+    ax.scatter(
+        range(len(groups)),
+        [0.5] * len(groups),
+        c=groups,
+        marker="_",
+        lw=50,
+        cmap=cmap_data,
+    )
+    ax.scatter(
+        range(len(groups)),
+        [3.5] * len(groups),
+        c=classes,
+        marker="_",
+        lw=50,
+        cmap=cmap_data,
+    )
+    ax.set(
+        ylim=[-1, 5],
+        yticks=[0.5, 3.5],
+        yticklabels=["Data\ngroup", "Data\nclass"],
+        xlabel="Sample index",
+    )
+
+
+visualize_groups(y, groups, "no groups")
+
+# %%
 # Define a function to visualize cross-validation behavior
 # --------------------------------------------------------
 #
@@ -82,46 +107,74 @@
         indices[tr] = 0

         # Visualize the results
-        ax.scatter(range(len(indices)), [ii + .5] * len(indices),
-                   c=indices, marker='_', lw=lw, cmap=cmap_cv,
-                   vmin=-.2, vmax=1.2)
+        ax.scatter(
+            range(len(indices)),
+            [ii + 0.5] * len(indices),
+            c=indices,
+            marker="_",
+            lw=lw,
+            cmap=cmap_cv,
+            vmin=-0.2,
+            vmax=1.2,
+        )

     # Plot the data classes and groups at the end
-    ax.scatter(range(len(X)), [ii + 1.5] * len(X),
-               c=y, marker='_', lw=lw, cmap=cmap_data)
-
-    ax.scatter(range(len(X)), [ii + 2.5] * len(X),
-               c=group, marker='_', lw=lw, cmap=cmap_data)
+    ax.scatter(
+        range(len(X)), [ii + 1.5] * len(X), c=y, marker="_", lw=lw, cmap=cmap_data
+    )
+
+    ax.scatter(
+        range(len(X)), [ii + 2.5] * len(X), c=group, marker="_", lw=lw, cmap=cmap_data
+    )

     # Formatting
-    yticklabels = list(range(n_splits)) + ['class', 'group']
-    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,
-           xlabel='Sample index', ylabel="CV iteration",
-           ylim=[n_splits+2.2, -.2], xlim=[0, 100])
-    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)
+    yticklabels = list(range(n_splits)) + ["class", "group"]
+    ax.set(
+        yticks=np.arange(n_splits + 2) + 0.5,
+        yticklabels=yticklabels,
+        xlabel="Sample index",
+        ylabel="CV iteration",
+        ylim=[n_splits + 2.2, -0.2],
+        xlim=[0, 100],
+    )
+    ax.set_title("{}".format(type(cv).__name__), fontsize=15)
     return ax


-###############################################################################
-# Let's see how it looks for the `KFold` cross-validation object:
+# %%
+# Let's see how it looks for the :class:`~sklearn.model_selection.KFold`
+# cross-validation object:

 fig, ax = plt.subplots()
 cv = KFold(n_splits)
 plot_cv_indices(cv, X, y, groups, ax, n_splits)

-###############################################################################
+# %%
 # As you can see, by default the KFold cross-validation iterator does not
 # take either datapoint class or group into consideration. We can change this
-# by using the ``StratifiedKFold`` like so.
-
-fig, ax = plt.subplots()
-cv = StratifiedKFold(n_splits)
-plot_cv_indices(cv, X, y, groups, ax, n_splits)
-
-###############################################################################
-# In this case, the cross-validation retained the same ratio of classes across
-# each CV split. Next we'll visualize this behavior for a number of CV
-# iterators.
+# by using either:
+#
+# - ``StratifiedKFold`` to preserve the percentage of samples for each class.
+# - ``GroupKFold`` to ensure that the same group will not appear in two
+#   different folds.
+# - ``StratifiedGroupKFold`` to keep the constraint of ``GroupKFold`` while
+#   attempting to return stratified folds.
+cvs = [StratifiedKFold, GroupKFold, StratifiedGroupKFold]
+
+for cv in cvs:
+    fig, ax = plt.subplots(figsize=(6, 3))
+    plot_cv_indices(cv(n_splits), X, y, groups, ax, n_splits)
+    ax.legend(
+        [Patch(color=cmap_cv(0.8)), Patch(color=cmap_cv(0.02))],
+        ["Testing set", "Training set"],
+        loc=(1.02, 0.8),
+    )
+    # Make the legend fit
+    plt.tight_layout()
+    fig.subplots_adjust(right=0.7)
+
+# %%
+# Next we'll visualize this behavior for a number of CV iterators.
 #
 # Visualize cross-validation indices for many CV objects
 # ------------------------------------------------------
@@ -132,8 +185,16 @@
 #
 # Note how some use the group/class information while others do not.

-cvs = [KFold, GroupKFold, ShuffleSplit, StratifiedKFold,
-       GroupShuffleSplit, StratifiedShuffleSplit, TimeSeriesSplit]
+cvs = [
+    KFold,
+    GroupKFold,
+    ShuffleSplit,
+    StratifiedKFold,
+    StratifiedGroupKFold,
+    GroupShuffleSplit,
+    StratifiedShuffleSplit,
+    TimeSeriesSplit,
+]


 for cv in cvs:
@@ -141,9 +202,12 @@
     fig, ax = plt.subplots(figsize=(6, 3))
     plot_cv_indices(this_cv, X, y, groups, ax, n_splits)

-    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],
-              ['Testing set', 'Training set'], loc=(1.02, .8))
+    ax.legend(
+        [Patch(color=cmap_cv(0.8)), Patch(color=cmap_cv(0.02))],
+        ["Testing set", "Training set"],
+        loc=(1.02, 0.8),
+    )
     # Make the legend fit
     plt.tight_layout()
-    fig.subplots_adjust(right=.7)
+    fig.subplots_adjust(right=0.7)
 plt.show()
('examples/model_selection', 'plot_randomized_search.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,7 +4,7 @@
 =========================================================================

 Compare randomized search and grid search for optimizing hyperparameters of a
-random forest.
+linear SVM with SGD training.
 All parameters that influence the learning are searched simultaneously
 (except for the number of estimators, which poses a time / quality tradeoff).

@@ -12,75 +12,82 @@
 parameters. The result in parameter settings is quite similar, while the run
 time for randomized search is drastically lower.

-The performance is slightly worse for the randomized search, though this
-is most likely a noise effect and would not carry over to a held-out test set.
+The performance is may slightly worse for the randomized search, and is likely
+due to a noise effect and would not carry over to a held-out test set.

 Note that in practice, one would not search over this many different parameters
 simultaneously using grid search, but pick only the ones deemed most important.
+
 """
-print(__doc__)

 import numpy as np

 from time import time
-from scipy.stats import randint as sp_randint
+import scipy.stats as stats
+from sklearn.utils.fixes import loguniform

-from sklearn.model_selection import GridSearchCV
-from sklearn.model_selection import RandomizedSearchCV
+from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
 from sklearn.datasets import load_digits
-from sklearn.ensemble import RandomForestClassifier
+from sklearn.linear_model import SGDClassifier

 # get some data
-digits = load_digits()
-X, y = digits.data, digits.target
+X, y = load_digits(return_X_y=True, n_class=3)

 # build a classifier
-clf = RandomForestClassifier(n_estimators=20)
+clf = SGDClassifier(loss="hinge", penalty="elasticnet", fit_intercept=True)


 # Utility function to report best scores
 def report(results, n_top=3):
     for i in range(1, n_top + 1):
-        candidates = np.flatnonzero(results['rank_test_score'] == i)
+        candidates = np.flatnonzero(results["rank_test_score"] == i)
         for candidate in candidates:
             print("Model with rank: {0}".format(i))
-            print("Mean validation score: {0:.3f} (std: {1:.3f})".format(
-                  results['mean_test_score'][candidate],
-                  results['std_test_score'][candidate]))
-            print("Parameters: {0}".format(results['params'][candidate]))
+            print(
+                "Mean validation score: {0:.3f} (std: {1:.3f})".format(
+                    results["mean_test_score"][candidate],
+                    results["std_test_score"][candidate],
+                )
+            )
+            print("Parameters: {0}".format(results["params"][candidate]))
             print("")


 # specify parameters and distributions to sample from
-param_dist = {"max_depth": [3, None],
-              "max_features": sp_randint(1, 11),
-              "min_samples_split": sp_randint(2, 11),
-              "bootstrap": [True, False],
-              "criterion": ["gini", "entropy"]}
+param_dist = {
+    "average": [True, False],
+    "l1_ratio": stats.uniform(0, 1),
+    "alpha": loguniform(1e-2, 1e0),
+}

 # run randomized search
-n_iter_search = 20
-random_search = RandomizedSearchCV(clf, param_distributions=param_dist,
-                                   n_iter=n_iter_search, cv=5, iid=False)
+n_iter_search = 15
+random_search = RandomizedSearchCV(
+    clf, param_distributions=param_dist, n_iter=n_iter_search
+)

 start = time()
 random_search.fit(X, y)
-print("RandomizedSearchCV took %.2f seconds for %d candidates"
-      " parameter settings." % ((time() - start), n_iter_search))
+print(
+    "RandomizedSearchCV took %.2f seconds for %d candidates parameter settings."
+    % ((time() - start), n_iter_search)
+)
 report(random_search.cv_results_)

 # use a full grid over all parameters
-param_grid = {"max_depth": [3, None],
-              "max_features": [1, 3, 10],
-              "min_samples_split": [2, 3, 10],
-              "bootstrap": [True, False],
-              "criterion": ["gini", "entropy"]}
+param_grid = {
+    "average": [True, False],
+    "l1_ratio": np.linspace(0, 1, num=10),
+    "alpha": np.power(10, np.arange(-2, 1, dtype=float)),
+}

 # run grid search
-grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, iid=False)
+grid_search = GridSearchCV(clf, param_grid=param_grid)
 start = time()
 grid_search.fit(X, y)

-print("GridSearchCV took %.2f seconds for %d candidate parameter settings."
-      % (time() - start, len(grid_search.cv_results_['params'])))
+print(
+    "GridSearchCV took %.2f seconds for %d candidate parameter settings."
+    % (time() - start, len(grid_search.cv_results_["params"]))
+)
 report(grid_search.cv_results_)
('examples/model_selection', 'plot_learning_curve.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -2,18 +2,20 @@
 ========================
 Plotting Learning Curves
 ========================
-
-On the left side the learning curve of a naive Bayes classifier is shown for
-the digits dataset. Note that the training score and the cross-validation score
-are both not very good at the end. However, the shape of the curve can be found
-in more complex datasets very often: the training score is very high at the
-beginning and decreases and the cross-validation score is very low at the
-beginning and increases. On the right side we see the learning curve of an SVM
-with RBF kernel. We can see clearly that the training score is still around
-the maximum and the validation score could be increased with more training
-samples.
+In the first column, first row the learning curve of a naive Bayes classifier
+is shown for the digits dataset. Note that the training score and the
+cross-validation score are both not very good at the end. However, the shape
+of the curve can be found in more complex datasets very often: the training
+score is very high at the beginning and decreases and the cross-validation
+score is very low at the beginning and increases. In the second column, first
+row we see the learning curve of an SVM with RBF kernel. We can see clearly
+that the training score is still around the maximum and the validation score
+could be increased with more training samples. The plots in the second row
+show the times required by the models to train with various sizes of training
+dataset. The plots in the third row show how much time was required to train
+the models for each training sizes.
+
 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -24,34 +26,49 @@
 from sklearn.model_selection import ShuffleSplit


-def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,
-                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):
+def plot_learning_curve(
+    estimator,
+    title,
+    X,
+    y,
+    axes=None,
+    ylim=None,
+    cv=None,
+    n_jobs=None,
+    train_sizes=np.linspace(0.1, 1.0, 5),
+):
     """
-    Generate a simple plot of the test and training learning curve.
+    Generate 3 plots: the test and training learning curve, the training
+    samples vs fit times curve, the fit times vs score curve.

     Parameters
     ----------
-    estimator : object type that implements the "fit" and "predict" methods
-        An object of that type which is cloned for each validation.
-
-    title : string
+    estimator : estimator instance
+        An estimator instance implementing `fit` and `predict` methods which
+        will be cloned for each validation.
+
+    title : str
         Title for the chart.

-    X : array-like, shape (n_samples, n_features)
-        Training vector, where n_samples is the number of samples and
-        n_features is the number of features.
-
-    y : array-like, shape (n_samples) or (n_samples, n_features), optional
-        Target relative to X for classification or regression;
+    X : array-like of shape (n_samples, n_features)
+        Training vector, where ``n_samples`` is the number of samples and
+        ``n_features`` is the number of features.
+
+    y : array-like of shape (n_samples) or (n_samples, n_features)
+        Target relative to ``X`` for classification or regression;
         None for unsupervised learning.

-    ylim : tuple, shape (ymin, ymax), optional
-        Defines minimum and maximum yvalues plotted.
-
-    cv : int, cross-validation generator or an iterable, optional
+    axes : array-like of shape (3,), default=None
+        Axes to use for plotting the curves.
+
+    ylim : tuple of shape (2,), default=None
+        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).
+
+    cv : int, cross-validation generator or an iterable, default=None
         Determines the cross-validation splitting strategy.
         Possible inputs for cv are:
-          - None, to use the default 3-fold cross-validation,
+
+          - None, to use the default 5-fold cross-validation,
           - integer, to specify the number of folds.
           - :term:`CV splitter`,
           - An iterable yielding (train, test) splits as arrays of indices.
@@ -63,66 +80,124 @@
         Refer :ref:`User Guide <cross_validation>` for the various
         cross-validators that can be used here.

-    n_jobs : int or None, optional (default=None)
+    n_jobs : int or None, default=None
         Number of jobs to run in parallel.
         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
         for more details.

-    train_sizes : array-like, shape (n_ticks,), dtype float or int
+    train_sizes : array-like of shape (n_ticks,)
         Relative or absolute numbers of training examples that will be used to
-        generate the learning curve. If the dtype is float, it is regarded as a
-        fraction of the maximum size of the training set (that is determined
-        by the selected validation method), i.e. it has to be within (0, 1].
-        Otherwise it is interpreted as absolute sizes of the training sets.
-        Note that for classification the number of samples usually have to
-        be big enough to contain at least one sample from each class.
+        generate the learning curve. If the ``dtype`` is float, it is regarded
+        as a fraction of the maximum size of the training set (that is
+        determined by the selected validation method), i.e. it has to be within
+        (0, 1]. Otherwise it is interpreted as absolute sizes of the training
+        sets. Note that for classification the number of samples usually have
+        to be big enough to contain at least one sample from each class.
         (default: np.linspace(0.1, 1.0, 5))
     """
-    plt.figure()
-    plt.title(title)
+    if axes is None:
+        _, axes = plt.subplots(1, 3, figsize=(20, 5))
+
+    axes[0].set_title(title)
     if ylim is not None:
-        plt.ylim(*ylim)
-    plt.xlabel("Training examples")
-    plt.ylabel("Score")
-    train_sizes, train_scores, test_scores = learning_curve(
-        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
+        axes[0].set_ylim(*ylim)
+    axes[0].set_xlabel("Training examples")
+    axes[0].set_ylabel("Score")
+
+    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(
+        estimator,
+        X,
+        y,
+        cv=cv,
+        n_jobs=n_jobs,
+        train_sizes=train_sizes,
+        return_times=True,
+    )
     train_scores_mean = np.mean(train_scores, axis=1)
     train_scores_std = np.std(train_scores, axis=1)
     test_scores_mean = np.mean(test_scores, axis=1)
     test_scores_std = np.std(test_scores, axis=1)
-    plt.grid()
-
-    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
-                     train_scores_mean + train_scores_std, alpha=0.1,
-                     color="r")
-    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
-                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
-    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
-             label="Training score")
-    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
-             label="Cross-validation score")
-
-    plt.legend(loc="best")
+    fit_times_mean = np.mean(fit_times, axis=1)
+    fit_times_std = np.std(fit_times, axis=1)
+
+    # Plot learning curve
+    axes[0].grid()
+    axes[0].fill_between(
+        train_sizes,
+        train_scores_mean - train_scores_std,
+        train_scores_mean + train_scores_std,
+        alpha=0.1,
+        color="r",
+    )
+    axes[0].fill_between(
+        train_sizes,
+        test_scores_mean - test_scores_std,
+        test_scores_mean + test_scores_std,
+        alpha=0.1,
+        color="g",
+    )
+    axes[0].plot(
+        train_sizes, train_scores_mean, "o-", color="r", label="Training score"
+    )
+    axes[0].plot(
+        train_sizes, test_scores_mean, "o-", color="g", label="Cross-validation score"
+    )
+    axes[0].legend(loc="best")
+
+    # Plot n_samples vs fit_times
+    axes[1].grid()
+    axes[1].plot(train_sizes, fit_times_mean, "o-")
+    axes[1].fill_between(
+        train_sizes,
+        fit_times_mean - fit_times_std,
+        fit_times_mean + fit_times_std,
+        alpha=0.1,
+    )
+    axes[1].set_xlabel("Training examples")
+    axes[1].set_ylabel("fit_times")
+    axes[1].set_title("Scalability of the model")
+
+    # Plot fit_time vs score
+    fit_time_argsort = fit_times_mean.argsort()
+    fit_time_sorted = fit_times_mean[fit_time_argsort]
+    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]
+    test_scores_std_sorted = test_scores_std[fit_time_argsort]
+    axes[2].grid()
+    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, "o-")
+    axes[2].fill_between(
+        fit_time_sorted,
+        test_scores_mean_sorted - test_scores_std_sorted,
+        test_scores_mean_sorted + test_scores_std_sorted,
+        alpha=0.1,
+    )
+    axes[2].set_xlabel("fit_times")
+    axes[2].set_ylabel("Score")
+    axes[2].set_title("Performance of the model")
+
     return plt


-digits = load_digits()
-X, y = digits.data, digits.target
-
+fig, axes = plt.subplots(3, 2, figsize=(10, 15))
+
+X, y = load_digits(return_X_y=True)

 title = "Learning Curves (Naive Bayes)"
-# Cross validation with 100 iterations to get smoother mean test and train
+# Cross validation with 50 iterations to get smoother mean test and train
 # score curves, each time with 20% data randomly selected as a validation set.
-cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)
+cv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=0)

 estimator = GaussianNB()
-plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)
+plot_learning_curve(
+    estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01), cv=cv, n_jobs=4
+)

 title = r"Learning Curves (SVM, RBF kernel, $\gamma=0.001$)"
 # SVC is more expensive so we do a lower number of CV iterations:
-cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)
+cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)
 estimator = SVC(gamma=0.001)
-plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)
+plot_learning_curve(
+    estimator, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01), cv=cv, n_jobs=4
+)

 plt.show()
('examples/model_selection', 'plot_nested_cross_validation_iris.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -43,13 +43,12 @@
      <http://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf>`_

 """
+
 from sklearn.datasets import load_iris
 from matplotlib import pyplot as plt
 from sklearn.svm import SVC
 from sklearn.model_selection import GridSearchCV, cross_val_score, KFold
 import numpy as np
-
-print(__doc__)

 # Number of random trials
 NUM_TRIALS = 30
@@ -60,8 +59,7 @@
 y_iris = iris.target

 # Set up possible values of parameters to optimize over
-p_grid = {"C": [1, 10, 100],
-          "gamma": [.01, .1]}
+p_grid = {"C": [1, 10, 100], "gamma": [0.01, 0.1]}

 # We will use a Support Vector Classifier with "rbf" kernel
 svm = SVC(kernel="rbf")
@@ -80,39 +78,50 @@
     outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)

     # Non_nested parameter search and scoring
-    clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=inner_cv,
-                       iid=False)
+    clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=outer_cv)
     clf.fit(X_iris, y_iris)
     non_nested_scores[i] = clf.best_score_

     # Nested CV with parameter optimization
+    clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=inner_cv)
     nested_score = cross_val_score(clf, X=X_iris, y=y_iris, cv=outer_cv)
     nested_scores[i] = nested_score.mean()

 score_difference = non_nested_scores - nested_scores

-print("Average difference of {:6f} with std. dev. of {:6f}."
-      .format(score_difference.mean(), score_difference.std()))
+print(
+    "Average difference of {:6f} with std. dev. of {:6f}.".format(
+        score_difference.mean(), score_difference.std()
+    )
+)

 # Plot scores on each trial for nested and non-nested CV
 plt.figure()
 plt.subplot(211)
-non_nested_scores_line, = plt.plot(non_nested_scores, color='r')
-nested_line, = plt.plot(nested_scores, color='b')
+(non_nested_scores_line,) = plt.plot(non_nested_scores, color="r")
+(nested_line,) = plt.plot(nested_scores, color="b")
 plt.ylabel("score", fontsize="14")
-plt.legend([non_nested_scores_line, nested_line],
-           ["Non-Nested CV", "Nested CV"],
-           bbox_to_anchor=(0, .4, .5, 0))
-plt.title("Non-Nested and Nested Cross Validation on Iris Dataset",
-          x=.5, y=1.1, fontsize="15")
+plt.legend(
+    [non_nested_scores_line, nested_line],
+    ["Non-Nested CV", "Nested CV"],
+    bbox_to_anchor=(0, 0.4, 0.5, 0),
+)
+plt.title(
+    "Non-Nested and Nested Cross Validation on Iris Dataset",
+    x=0.5,
+    y=1.1,
+    fontsize="15",
+)

 # Plot bar chart of the difference.
 plt.subplot(212)
 difference_plot = plt.bar(range(NUM_TRIALS), score_difference)
 plt.xlabel("Individual Trial #")
-plt.legend([difference_plot],
-           ["Non-Nested CV - Nested CV Score"],
-           bbox_to_anchor=(0, 1, .8, 0))
+plt.legend(
+    [difference_plot],
+    ["Non-Nested CV - Nested CV Score"],
+    bbox_to_anchor=(0, 1, 0.8, 0),
+)
 plt.ylabel("score difference", fontsize="14")

 plt.show()
('examples/model_selection', 'plot_precision_recall.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -90,79 +90,83 @@
              :func:`sklearn.metrics.precision_score`,
              :func:`sklearn.metrics.f1_score`
 """
-###############################################################################
+
+# %%
 # In binary classification settings
-# --------------------------------------------------------
-#
-# Create simple data
-# ..................
-#
-# Try to differentiate the two first classes of the iris data
-from sklearn import svm, datasets
+# ---------------------------------
+#
+# Dataset and model
+# .................
+#
+# We will use a Linear SVC classifier to differentiate two types of irises.
+import numpy as np
+from sklearn.datasets import load_iris
 from sklearn.model_selection import train_test_split
-import numpy as np
-
-iris = datasets.load_iris()
-X = iris.data
-y = iris.target
+
+X, y = load_iris(return_X_y=True)

 # Add noisy features
 random_state = np.random.RandomState(0)
 n_samples, n_features = X.shape
-X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]
+X = np.concatenate([X, random_state.randn(n_samples, 200 * n_features)], axis=1)

 # Limit to the two first classes, and split into training and test
-X_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],
-                                                    test_size=.5,
-                                                    random_state=random_state)
-
-# Create a simple classifier
-classifier = svm.LinearSVC(random_state=random_state)
+X_train, X_test, y_train, y_test = train_test_split(
+    X[y < 2], y[y < 2], test_size=0.5, random_state=random_state
+)
+
+# %%
+# Linear SVC will expect each feature to have a similar range of values. Thus,
+# we will first scale the data using a
+# :class:`~sklearn.preprocessing.StandardScaler`.
+from sklearn.pipeline import make_pipeline
+from sklearn.preprocessing import StandardScaler
+from sklearn.svm import LinearSVC
+
+classifier = make_pipeline(StandardScaler(), LinearSVC(random_state=random_state))
 classifier.fit(X_train, y_train)
+
+# %%
+# Plot the Precision-Recall curve
+# ...............................
+#
+# To plot the precision-recall curve, you should use
+# :class:`~sklearn.metrics.PrecisionRecallDisplay`. Indeed, there is two
+# methods available depending if you already computed the predictions of the
+# classifier or not.
+#
+# Let's first plot the precision-recall curve without the classifier
+# predictions. We use
+# :func:`~sklearn.metrics.PrecisionRecallDisplay.from_estimator` that
+# computes the predictions for us before plotting the curve.
+from sklearn.metrics import PrecisionRecallDisplay
+
+display = PrecisionRecallDisplay.from_estimator(
+    classifier, X_test, y_test, name="LinearSVC"
+)
+_ = display.ax_.set_title("2-class Precision-Recall curve")
+
+# %%
+# If we already got the estimated probabilities or scores for
+# our model, then we can use
+# :func:`~sklearn.metrics.PrecisionRecallDisplay.from_predictions`.
 y_score = classifier.decision_function(X_test)

-###############################################################################
-# Compute the average precision score
-# ...................................
-from sklearn.metrics import average_precision_score
-average_precision = average_precision_score(y_test, y_score)
-
-print('Average precision-recall score: {0:0.2f}'.format(
-      average_precision))
-
-###############################################################################
-# Plot the Precision-Recall curve
-# ................................
-from sklearn.metrics import precision_recall_curve
-import matplotlib.pyplot as plt
-from inspect import signature
-
-precision, recall, _ = precision_recall_curve(y_test, y_score)
-
-# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument
-step_kwargs = ({'step': 'post'}
-               if 'step' in signature(plt.fill_between).parameters
-               else {})
-plt.step(recall, precision, color='b', alpha=0.2,
-         where='post')
-plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)
-
-plt.xlabel('Recall')
-plt.ylabel('Precision')
-plt.ylim([0.0, 1.05])
-plt.xlim([0.0, 1.0])
-plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(
-          average_precision))
-
-###############################################################################
+display = PrecisionRecallDisplay.from_predictions(y_test, y_score, name="LinearSVC")
+_ = display.ax_.set_title("2-class Precision-Recall curve")
+
+# %%
 # In multi-label settings
-# ------------------------
+# -----------------------
+#
+# The precision-recall curve does not support the multilabel setting. However,
+# one can decide how to handle this case. We show such an example below.
 #
 # Create multi-label data, fit, and predict
-# ...........................................
+# .........................................
 #
 # We create a multi-label dataset, to illustrate the precision-recall in
-# multi-label settings
+# multi-label settings.

 from sklearn.preprocessing import label_binarize

@@ -171,21 +175,25 @@
 n_classes = Y.shape[1]

 # Split into training and test
-X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5,
-                                                    random_state=random_state)
-
-# We use OneVsRestClassifier for multi-label prediction
+X_train, X_test, Y_train, Y_test = train_test_split(
+    X, Y, test_size=0.5, random_state=random_state
+)
+
+# %%
+# We use :class:`~sklearn.multiclass.OneVsRestClassifier` for multi-label
+# prediction.
 from sklearn.multiclass import OneVsRestClassifier

-# Run classifier
-classifier = OneVsRestClassifier(svm.LinearSVC(random_state=random_state))
+classifier = OneVsRestClassifier(
+    make_pipeline(StandardScaler(), LinearSVC(random_state=random_state))
+)
 classifier.fit(X_train, Y_train)
 y_score = classifier.decision_function(X_test)


-###############################################################################
+# %%
 # The average precision score in multi-label settings
-# ....................................................
+# ...................................................
 from sklearn.metrics import precision_recall_curve
 from sklearn.metrics import average_precision_score

@@ -194,76 +202,68 @@
 recall = dict()
 average_precision = dict()
 for i in range(n_classes):
-    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i],
-                                                        y_score[:, i])
+    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i], y_score[:, i])
     average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])

 # A "micro-average": quantifying score on all classes jointly
-precision["micro"], recall["micro"], _ = precision_recall_curve(Y_test.ravel(),
-    y_score.ravel())
-average_precision["micro"] = average_precision_score(Y_test, y_score,
-                                                     average="micro")
-print('Average precision score, micro-averaged over all classes: {0:0.2f}'
-      .format(average_precision["micro"]))
-
-###############################################################################
+precision["micro"], recall["micro"], _ = precision_recall_curve(
+    Y_test.ravel(), y_score.ravel()
+)
+average_precision["micro"] = average_precision_score(Y_test, y_score, average="micro")
+
+# %%
 # Plot the micro-averaged Precision-Recall curve
-# ...............................................
-#
-
-plt.figure()
-plt.step(recall['micro'], precision['micro'], color='b', alpha=0.2,
-         where='post')
-plt.fill_between(recall["micro"], precision["micro"], alpha=0.2, color='b',
-                 **step_kwargs)
-
-plt.xlabel('Recall')
-plt.ylabel('Precision')
-plt.ylim([0.0, 1.05])
-plt.xlim([0.0, 1.0])
-plt.title(
-    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'
-    .format(average_precision["micro"]))
-
-###############################################################################
+# ..............................................
+display = PrecisionRecallDisplay(
+    recall=recall["micro"],
+    precision=precision["micro"],
+    average_precision=average_precision["micro"],
+)
+display.plot()
+_ = display.ax_.set_title("Micro-averaged over all classes")
+
+# %%
 # Plot Precision-Recall curve for each class and iso-f1 curves
-# .............................................................
-#
+# ............................................................
+import matplotlib.pyplot as plt
 from itertools import cycle
+
 # setup plot details
-colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])
-
-plt.figure(figsize=(7, 8))
+colors = cycle(["navy", "turquoise", "darkorange", "cornflowerblue", "teal"])
+
+_, ax = plt.subplots(figsize=(7, 8))
+
 f_scores = np.linspace(0.2, 0.8, num=4)
-lines = []
-labels = []
+lines, labels = [], []
 for f_score in f_scores:
     x = np.linspace(0.01, 1)
     y = f_score * x / (2 * x - f_score)
-    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)
-    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))
-
-lines.append(l)
-labels.append('iso-f1 curves')
-l, = plt.plot(recall["micro"], precision["micro"], color='gold', lw=2)
-lines.append(l)
-labels.append('micro-average Precision-recall (area = {0:0.2f})'
-              ''.format(average_precision["micro"]))
+    (l,) = plt.plot(x[y >= 0], y[y >= 0], color="gray", alpha=0.2)
+    plt.annotate("f1={0:0.1f}".format(f_score), xy=(0.9, y[45] + 0.02))
+
+display = PrecisionRecallDisplay(
+    recall=recall["micro"],
+    precision=precision["micro"],
+    average_precision=average_precision["micro"],
+)
+display.plot(ax=ax, name="Micro-average precision-recall", color="gold")

 for i, color in zip(range(n_classes), colors):
-    l, = plt.plot(recall[i], precision[i], color=color, lw=2)
-    lines.append(l)
-    labels.append('Precision-recall for class {0} (area = {1:0.2f})'
-                  ''.format(i, average_precision[i]))
-
-fig = plt.gcf()
-fig.subplots_adjust(bottom=0.25)
-plt.xlim([0.0, 1.0])
-plt.ylim([0.0, 1.05])
-plt.xlabel('Recall')
-plt.ylabel('Precision')
-plt.title('Extension of Precision-Recall curve to multi-class')
-plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))
-
+    display = PrecisionRecallDisplay(
+        recall=recall[i],
+        precision=precision[i],
+        average_precision=average_precision[i],
+    )
+    display.plot(ax=ax, name=f"Precision-recall for class {i}", color=color)
+
+# add the legend for the iso-f1 curves
+handles, labels = display.ax_.get_legend_handles_labels()
+handles.extend([l])
+labels.extend(["iso-f1 curves"])
+# set the legend and the axes
+ax.set_xlim([0.0, 1.0])
+ax.set_ylim([0.0, 1.05])
+ax.legend(handles=handles, labels=labels, loc="best")
+ax.set_title("Extension of Precision-Recall curve to multi-class")

 plt.show()
('examples/model_selection', 'plot_grid_search_refit_callable.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,17 +8,17 @@
 score while minimising the number of PCA components [1].

 The figure shows the trade-off between cross-validated score and the number
-of PCA components. The balanced case is when n_components=6 and accuracy=0.80,
+of PCA components. The balanced case is when n_components=10 and accuracy=0.88,
 which falls into the range within 1 standard deviation of the best accuracy
 score.

 [1] Hastie, T., Tibshirani, R.,, Friedman, J. (2001). Model Assessment and
 Selection. The Elements of Statistical Learning (pp. 219-260). New York,
 NY, USA: Springer New York Inc..
+
 """
+
 # Author: Wenhao Zhang <wenhaoz@ucla.edu>
-
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -46,10 +46,12 @@
         Lower bound within 1 standard deviation of the
         best `mean_test_score`.
     """
-    best_score_idx = np.argmax(cv_results['mean_test_score'])
+    best_score_idx = np.argmax(cv_results["mean_test_score"])

-    return (cv_results['mean_test_score'][best_score_idx]
-            - cv_results['std_test_score'][best_score_idx])
+    return (
+        cv_results["mean_test_score"][best_score_idx]
+        - cv_results["std_test_score"][best_score_idx]
+    )


 def best_low_complexity(cv_results):
@@ -69,48 +71,56 @@
         `mean_test_score`.
     """
     threshold = lower_bound(cv_results)
-    candidate_idx = np.flatnonzero(cv_results['mean_test_score'] >= threshold)
-    best_idx = candidate_idx[cv_results['param_reduce_dim__n_components']
-                             [candidate_idx].argmin()]
+    candidate_idx = np.flatnonzero(cv_results["mean_test_score"] >= threshold)
+    best_idx = candidate_idx[
+        cv_results["param_reduce_dim__n_components"][candidate_idx].argmin()
+    ]
     return best_idx


-pipe = Pipeline([
-        ('reduce_dim', PCA(random_state=42)),
-        ('classify', LinearSVC(random_state=42)),
-])
+pipe = Pipeline(
+    [
+        ("reduce_dim", PCA(random_state=42)),
+        ("classify", LinearSVC(random_state=42, C=0.01)),
+    ]
+)

-param_grid = {
-    'reduce_dim__n_components': [2, 4, 6, 8]
-}
+param_grid = {"reduce_dim__n_components": [6, 8, 10, 12, 14]}

-grid = GridSearchCV(pipe, cv=10, n_jobs=1, param_grid=param_grid,
-                    scoring='accuracy', refit=best_low_complexity)
-digits = load_digits()
-grid.fit(digits.data, digits.target)
+grid = GridSearchCV(
+    pipe,
+    cv=10,
+    n_jobs=1,
+    param_grid=param_grid,
+    scoring="accuracy",
+    refit=best_low_complexity,
+)
+X, y = load_digits(return_X_y=True)
+grid.fit(X, y)

-n_components = grid.cv_results_['param_reduce_dim__n_components']
-test_scores = grid.cv_results_['mean_test_score']
+n_components = grid.cv_results_["param_reduce_dim__n_components"]
+test_scores = grid.cv_results_["mean_test_score"]

 plt.figure()
-plt.bar(n_components, test_scores, width=1.3, color='b')
+plt.bar(n_components, test_scores, width=1.3, color="b")

 lower = lower_bound(grid.cv_results_)
-plt.axhline(np.max(test_scores), linestyle='--', color='y',
-            label='Best score')
-plt.axhline(lower, linestyle='--', color='.5', label='Best score - 1 std')
+plt.axhline(np.max(test_scores), linestyle="--", color="y", label="Best score")
+plt.axhline(lower, linestyle="--", color=".5", label="Best score - 1 std")

 plt.title("Balance model complexity and cross-validated score")
-plt.xlabel('Number of PCA components used')
-plt.ylabel('Digit classification accuracy')
+plt.xlabel("Number of PCA components used")
+plt.ylabel("Digit classification accuracy")
 plt.xticks(n_components.tolist())
 plt.ylim((0, 1.0))
-plt.legend(loc='upper left')
+plt.legend(loc="upper left")

 best_index_ = grid.best_index_

 print("The best_index_ is %d" % best_index_)
 print("The n_components selected is %d" % n_components[best_index_])
-print("The corresponding accuracy score is %.2f"
-      % grid.cv_results_['mean_test_score'][best_index_])
+print(
+    "The corresponding accuracy score is %.2f"
+    % grid.cv_results_["mean_test_score"][best_index_]
+)
 plt.show()
('examples/model_selection', 'plot_multi_metric_evaluation.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -13,6 +13,7 @@

 The ``best_estimator_``, ``best_index_``, ``best_score_`` and ``best_params_``
 correspond to the scorer (key) that is set to the ``refit`` attribute.
+
 """

 # Author: Raghav RV <rvraghav93@gmail.com>
@@ -27,37 +28,39 @@
 from sklearn.metrics import accuracy_score
 from sklearn.tree import DecisionTreeClassifier

-print(__doc__)
-
-###############################################################################
+# %%
 # Running ``GridSearchCV`` using multiple evaluation metrics
 # ----------------------------------------------------------
 #

 X, y = make_hastie_10_2(n_samples=8000, random_state=42)

-# The scorers can be either be one of the predefined metric strings or a scorer
+# The scorers can be either one of the predefined metric strings or a scorer
 # callable, like the one returned by make_scorer
-scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}
+scoring = {"AUC": "roc_auc", "Accuracy": make_scorer(accuracy_score)}

 # Setting refit='AUC', refits an estimator on the whole dataset with the
 # parameter setting that has the best cross-validated AUC score.
 # That estimator is made available at ``gs.best_estimator_`` along with
 # parameters like ``gs.best_score_``, ``gs.best_params_`` and
 # ``gs.best_index_``
-gs = GridSearchCV(DecisionTreeClassifier(random_state=42),
-                  param_grid={'min_samples_split': range(2, 403, 10)},
-                  scoring=scoring, cv=5, refit='AUC', return_train_score=True)
+gs = GridSearchCV(
+    DecisionTreeClassifier(random_state=42),
+    param_grid={"min_samples_split": range(2, 403, 20)},
+    scoring=scoring,
+    refit="AUC",
+    n_jobs=2,
+    return_train_score=True,
+)
 gs.fit(X, y)
 results = gs.cv_results_

-###############################################################################
+# %%
 # Plotting the result
 # -------------------

 plt.figure(figsize=(13, 13))
-plt.title("GridSearchCV evaluating using multiple scorers simultaneously",
-          fontsize=16)
+plt.title("GridSearchCV evaluating using multiple scorers simultaneously", fontsize=16)

 plt.xlabel("min_samples_split")
 plt.ylabel("Score")
@@ -67,29 +70,47 @@
 ax.set_ylim(0.73, 1)

 # Get the regular numpy array from the MaskedArray
-X_axis = np.array(results['param_min_samples_split'].data, dtype=float)
+X_axis = np.array(results["param_min_samples_split"].data, dtype=float)

-for scorer, color in zip(sorted(scoring), ['g', 'k']):
-    for sample, style in (('train', '--'), ('test', '-')):
-        sample_score_mean = results['mean_%s_%s' % (sample, scorer)]
-        sample_score_std = results['std_%s_%s' % (sample, scorer)]
-        ax.fill_between(X_axis, sample_score_mean - sample_score_std,
-                        sample_score_mean + sample_score_std,
-                        alpha=0.1 if sample == 'test' else 0, color=color)
-        ax.plot(X_axis, sample_score_mean, style, color=color,
-                alpha=1 if sample == 'test' else 0.7,
-                label="%s (%s)" % (scorer, sample))
+for scorer, color in zip(sorted(scoring), ["g", "k"]):
+    for sample, style in (("train", "--"), ("test", "-")):
+        sample_score_mean = results["mean_%s_%s" % (sample, scorer)]
+        sample_score_std = results["std_%s_%s" % (sample, scorer)]
+        ax.fill_between(
+            X_axis,
+            sample_score_mean - sample_score_std,
+            sample_score_mean + sample_score_std,
+            alpha=0.1 if sample == "test" else 0,
+            color=color,
+        )
+        ax.plot(
+            X_axis,
+            sample_score_mean,
+            style,
+            color=color,
+            alpha=1 if sample == "test" else 0.7,
+            label="%s (%s)" % (scorer, sample),
+        )

-    best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]
-    best_score = results['mean_test_%s' % scorer][best_index]
+    best_index = np.nonzero(results["rank_test_%s" % scorer] == 1)[0][0]
+    best_score = results["mean_test_%s" % scorer][best_index]

     # Plot a dotted vertical line at the best score for that scorer marked by x
-    ax.plot([X_axis[best_index], ] * 2, [0, best_score],
-            linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)
+    ax.plot(
+        [
+            X_axis[best_index],
+        ]
+        * 2,
+        [0, best_score],
+        linestyle="-.",
+        color=color,
+        marker="x",
+        markeredgewidth=3,
+        ms=8,
+    )

     # Annotate the best score for that scorer
-    ax.annotate("%0.2f" % best_score,
-                (X_axis[best_index], best_score + 0.005))
+    ax.annotate("%0.2f" % best_score, (X_axis[best_index], best_score + 0.005))

 plt.legend(loc="best")
 plt.grid(False)
('examples/model_selection', 'plot_grid_search_digits.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,7 +4,7 @@
 ============================================================

 This examples shows how a classifier is optimized by cross-validation,
-which is done using the :class:`sklearn.model_selection.GridSearchCV` object
+which is done using the :class:`~sklearn.model_selection.GridSearchCV` object
 on a development set that comprises only half of the available labeled data.

 The performance of the selected hyper-parameters and trained model is
@@ -15,40 +15,32 @@
 sections on :ref:`cross_validation` and :ref:`grid_search`.

 """
+
 from sklearn import datasets
 from sklearn.model_selection import train_test_split
 from sklearn.model_selection import GridSearchCV
 from sklearn.metrics import classification_report
 from sklearn.svm import SVC

-print(__doc__)
-
 # Loading the Digits dataset
-digits = datasets.load_digits()
-
-# To apply an classifier on this data, we need to flatten the image, to
-# turn the data in a (samples, feature) matrix:
-n_samples = len(digits.images)
-X = digits.images.reshape((n_samples, -1))
-y = digits.target
+X, y = datasets.load_digits(return_X_y=True)

 # Split the dataset in two equal parts
-X_train, X_test, y_train, y_test = train_test_split(
-    X, y, test_size=0.5, random_state=0)
+X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)

 # Set the parameters by cross-validation
-tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
-                     'C': [1, 10, 100, 1000]},
-                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]
+tuned_parameters = [
+    {"kernel": ["rbf"], "gamma": [1e-3, 1e-4], "C": [1, 10, 100, 1000]},
+    {"kernel": ["linear"], "C": [1, 10, 100, 1000]},
+]

-scores = ['precision', 'recall']
+scores = ["precision", "recall"]

 for score in scores:
     print("# Tuning hyper-parameters for %s" % score)
     print()

-    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,
-                       scoring='%s_macro' % score)
+    clf = GridSearchCV(SVC(), tuned_parameters, scoring="%s_macro" % score)
     clf.fit(X_train, y_train)

     print("Best parameters set found on development set:")
@@ -57,11 +49,10 @@
     print()
     print("Grid scores on development set:")
     print()
-    means = clf.cv_results_['mean_test_score']
-    stds = clf.cv_results_['std_test_score']
-    for mean, std, params in zip(means, stds, clf.cv_results_['params']):
-        print("%0.3f (+/-%0.03f) for %r"
-              % (mean, std * 2, params))
+    means = clf.cv_results_["mean_test_score"]
+    stds = clf.cv_results_["std_test_score"]
+    for mean, std, params in zip(means, stds, clf.cv_results_["params"]):
+        print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))
     print()

     print("Detailed classification report:")
('examples/decomposition', 'plot_incremental_pca.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -18,7 +18,6 @@
 incremental approaches.

 """
-print(__doc__)

 # Authors: Kyle Kastner
 # License: BSD 3 clause
@@ -40,18 +39,22 @@
 pca = PCA(n_components=n_components)
 X_pca = pca.fit_transform(X)

-colors = ['navy', 'turquoise', 'darkorange']
+colors = ["navy", "turquoise", "darkorange"]

 for X_transformed, title in [(X_ipca, "Incremental PCA"), (X_pca, "PCA")]:
     plt.figure(figsize=(8, 8))
     for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):
-        plt.scatter(X_transformed[y == i, 0], X_transformed[y == i, 1],
-                    color=color, lw=2, label=target_name)
+        plt.scatter(
+            X_transformed[y == i, 0],
+            X_transformed[y == i, 1],
+            color=color,
+            lw=2,
+            label=target_name,
+        )

     if "Incremental" in title:
         err = np.abs(np.abs(X_pca) - np.abs(X_ipca)).mean()
-        plt.title(title + " of iris dataset\nMean absolute unsigned error "
-                  "%.6f" % err)
+        plt.title(title + " of iris dataset\nMean absolute unsigned error %.6f" % err)
     else:
         plt.title(title + " of iris dataset")
     plt.legend(loc="best", shadow=False, scatterpoints=1)
('examples/decomposition', 'plot_kernel_pca.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,79 +3,163 @@
 Kernel PCA
 ==========

-This example shows that Kernel PCA is able to find a projection of the data
-that makes data linearly separable.
+This example shows the difference between the Principal Components Analysis
+(:class:`~sklearn.decomposition.PCA`) and its kernalized version
+(:class:`~sklearn.decomposition.KernelPCA`).
+
+On the one hand, we show that :class:`~sklearn.decomposition.KernelPCA` is able
+to find a projection of the data which linearly separates them while it is not the case
+with :class:`~sklearn.decomposition.PCA`.
+
+Finally, we show that inverting this projection is an approximation with
+:class:`~sklearn.decomposition.KernelPCA`, while it is exact with
+:class:`~sklearn.decomposition.PCA`.
 """
-print(__doc__)

 # Authors: Mathieu Blondel
 #          Andreas Mueller
+#          Guillaume Lemaitre
 # License: BSD 3 clause

-import numpy as np
+# %%
+# Projecting data: `PCA` vs. `KernelPCA`
+# --------------------------------------
+#
+# In this section, we show the advantages of using a kernel when
+# projecting data using a Principal Component Analysis (PCA). We create a
+# dataset made of two nested circles.
+from sklearn.datasets import make_circles
+from sklearn.model_selection import train_test_split
+
+X, y = make_circles(n_samples=1_000, factor=0.3, noise=0.05, random_state=0)
+X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)
+
+# %%
+# Let's have a quick first look at the generated dataset.
 import matplotlib.pyplot as plt

+_, (train_ax, test_ax) = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(8, 4))
+
+train_ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train)
+train_ax.set_ylabel("Feature #1")
+train_ax.set_xlabel("Feature #0")
+train_ax.set_title("Training data")
+
+test_ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test)
+test_ax.set_xlabel("Feature #0")
+_ = test_ax.set_title("Testing data")
+
+# %%
+# The samples from each class cannot be linearly separated: there is no
+# straight line that can split the samples of the inner set from the outer
+# set.
+#
+# Now, we will use PCA with and without a kernel to see what is the effect of
+# using such a kernel. The kernel used here is a radial basis function (RBF)
+# kernel.
 from sklearn.decomposition import PCA, KernelPCA
-from sklearn.datasets import make_circles

-np.random.seed(0)
+pca = PCA(n_components=2)
+kernel_pca = KernelPCA(
+    n_components=None, kernel="rbf", gamma=10, fit_inverse_transform=True, alpha=0.1
+)

-X, y = make_circles(n_samples=400, factor=.3, noise=.05)
+X_test_pca = pca.fit(X_train).transform(X_test)
+X_test_kernel_pca = kernel_pca.fit(X_train).transform(X_test)

-kpca = KernelPCA(kernel="rbf", fit_inverse_transform=True, gamma=10)
-X_kpca = kpca.fit_transform(X)
-X_back = kpca.inverse_transform(X_kpca)
-pca = PCA()
-X_pca = pca.fit_transform(X)
+# %%
+fig, (orig_data_ax, pca_proj_ax, kernel_pca_proj_ax) = plt.subplots(
+    ncols=3, figsize=(14, 4)
+)

-# Plot results
+orig_data_ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test)
+orig_data_ax.set_ylabel("Feature #1")
+orig_data_ax.set_xlabel("Feature #0")
+orig_data_ax.set_title("Testing data")

-plt.figure()
-plt.subplot(2, 2, 1, aspect='equal')
-plt.title("Original space")
-reds = y == 0
-blues = y == 1
+pca_proj_ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test)
+pca_proj_ax.set_ylabel("Principal component #1")
+pca_proj_ax.set_xlabel("Principal component #0")
+pca_proj_ax.set_title("Projection of testing data\n using PCA")

-plt.scatter(X[reds, 0], X[reds, 1], c="red",
-            s=20, edgecolor='k')
-plt.scatter(X[blues, 0], X[blues, 1], c="blue",
-            s=20, edgecolor='k')
-plt.xlabel("$x_1$")
-plt.ylabel("$x_2$")
+kernel_pca_proj_ax.scatter(X_test_kernel_pca[:, 0], X_test_kernel_pca[:, 1], c=y_test)
+kernel_pca_proj_ax.set_ylabel("Principal component #1")
+kernel_pca_proj_ax.set_xlabel("Principal component #0")
+_ = kernel_pca_proj_ax.set_title("Projection of testing data\n using KernelPCA")

-X1, X2 = np.meshgrid(np.linspace(-1.5, 1.5, 50), np.linspace(-1.5, 1.5, 50))
-X_grid = np.array([np.ravel(X1), np.ravel(X2)]).T
-# projection on the first principal component (in the phi space)
-Z_grid = kpca.transform(X_grid)[:, 0].reshape(X1.shape)
-plt.contour(X1, X2, Z_grid, colors='grey', linewidths=1, origin='lower')
+# %%
+# We recall that PCA transforms the data linearly. Intuitively, it means that
+# the coordinate system will be centered, rescaled on each component
+# with respected to its variance and finally be rotated.
+# The obtained data from this transformation is isotropic and can now be
+# projected on its *principal components*.
+#
+# Thus, looking at the projection made using PCA (i.e. the middle figure), we
+# see that there is no change regarding the scaling; indeed the data being two
+# concentric circles centered in zero, the original data is already isotropic.
+# However, we can see that the data have been rotated. As a
+# conclusion, we see that such a projection would not help if define a linear
+# classifier to distinguish samples from both classes.
+#
+# Using a kernel allows to make a non-linear projection. Here, by using an RBF
+# kernel, we expect that the projection will unfold the dataset while keeping
+# approximately preserving the relative distances of pairs of data points that
+# are close to one another in the original space.
+#
+# We observe such behaviour in the figure on the right: the samples of a given
+# class are closer to each other than the samples from the opposite class,
+# untangling both sample sets. Now, we can use a linear classifier to separate
+# the samples from the two classes.
+#
+# Projecting into the original feature space
+# ------------------------------------------
+#
+# One particularity to have in mind when using
+# :class:`~sklearn.decomposition.KernelPCA` is related to the reconstruction
+# (i.e. the back projection in the original feature space). With
+# :class:`~sklearn.decomposition.PCA`, the reconstruction will be exact if
+# `n_components` is the same than the number of original features.
+# This is the case in this example.
+#
+# We can investigate if we get the original dataset when back projecting with
+# :class:`~sklearn.decomposition.KernelPCA`.
+X_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))
+X_reconstructed_kernel_pca = kernel_pca.inverse_transform(kernel_pca.transform(X_test))

-plt.subplot(2, 2, 2, aspect='equal')
-plt.scatter(X_pca[reds, 0], X_pca[reds, 1], c="red",
-            s=20, edgecolor='k')
-plt.scatter(X_pca[blues, 0], X_pca[blues, 1], c="blue",
-            s=20, edgecolor='k')
-plt.title("Projection by PCA")
-plt.xlabel("1st principal component")
-plt.ylabel("2nd component")
+# %%
+fig, (orig_data_ax, pca_back_proj_ax, kernel_pca_back_proj_ax) = plt.subplots(
+    ncols=3, sharex=True, sharey=True, figsize=(13, 4)
+)

-plt.subplot(2, 2, 3, aspect='equal')
-plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c="red",
-            s=20, edgecolor='k')
-plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c="blue",
-            s=20, edgecolor='k')
-plt.title("Projection by KPCA")
-plt.xlabel(r"1st principal component in space induced by $\phi$")
-plt.ylabel("2nd component")
+orig_data_ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test)
+orig_data_ax.set_ylabel("Feature #1")
+orig_data_ax.set_xlabel("Feature #0")
+orig_data_ax.set_title("Original test data")

-plt.subplot(2, 2, 4, aspect='equal')
-plt.scatter(X_back[reds, 0], X_back[reds, 1], c="red",
-            s=20, edgecolor='k')
-plt.scatter(X_back[blues, 0], X_back[blues, 1], c="blue",
-            s=20, edgecolor='k')
-plt.title("Original space after inverse transform")
-plt.xlabel("$x_1$")
-plt.ylabel("$x_2$")
+pca_back_proj_ax.scatter(X_reconstructed_pca[:, 0], X_reconstructed_pca[:, 1], c=y_test)
+pca_back_proj_ax.set_xlabel("Feature #0")
+pca_back_proj_ax.set_title("Reconstruction via PCA")

-plt.subplots_adjust(0.02, 0.10, 0.98, 0.94, 0.04, 0.35)
+kernel_pca_back_proj_ax.scatter(
+    X_reconstructed_kernel_pca[:, 0], X_reconstructed_kernel_pca[:, 1], c=y_test
+)
+kernel_pca_back_proj_ax.set_xlabel("Feature #0")
+_ = kernel_pca_back_proj_ax.set_title("Reconstruction via KernelPCA")

-plt.show()
+# %%
+# While we see a perfect reconstruction with
+# :class:`~sklearn.decomposition.PCA` we observe a different result for
+# :class:`~sklearn.decomposition.KernelPCA`.
+#
+# Indeed, :meth:`~sklearn.decomposition.KernelPCA.inverse_transform` cannot
+# rely on an analytical back-projection and thus an exact reconstruction.
+# Instead, a :class:`~sklearn.kernel_ridge.KernelRidge` is internally trained
+# to learn a mapping from the kernalized PCA basis to the original feature
+# space. This method therefore comes with an approximation introducing small
+# differences when back projecting in the original feature space.
+#
+# To improve the reconstruction using
+# :meth:`~sklearn.decomposition.KernelPCA.inverse_transform`, one can tune
+# `alpha` in :class:`~sklearn.decomposition.KernelPCA`, the regularization term
+# which controls the reliance on the training data during the training of
+# the mapping.
('examples/decomposition', 'plot_faces_decomposition.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,191 +3,331 @@
 Faces dataset decompositions
 ============================

-This example applies to :ref:`olivetti_faces` different unsupervised
+This example applies to :ref:`olivetti_faces_dataset` different unsupervised
 matrix decomposition (dimension reduction) methods from the module
 :py:mod:`sklearn.decomposition` (see the documentation chapter
-:ref:`decompositions`) .
-
+:ref:`decompositions`).
+
+
+- Authors: Vlad Niculae, Alexandre Gramfort
+- License: BSD 3 clause
 """
-print(__doc__)
-
-# Authors: Vlad Niculae, Alexandre Gramfort
-# License: BSD 3 clause
+
+# %%
+# Dataset preparation
+# -------------------
+#
+# Loading and preprocessing the Olivetti faces dataset.

 import logging
-from time import time

 from numpy.random import RandomState
 import matplotlib.pyplot as plt

 from sklearn.datasets import fetch_olivetti_faces
-from sklearn.cluster import MiniBatchKMeans
+from sklearn import cluster
 from sklearn import decomposition

+rng = RandomState(0)
+
 # Display progress logs on stdout
-logging.basicConfig(level=logging.INFO,
-                    format='%(asctime)s %(levelname)s %(message)s')
+logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
+
+faces, _ = fetch_olivetti_faces(return_X_y=True, shuffle=True, random_state=rng)
+n_samples, n_features = faces.shape
+
+# Global centering (focus on one feature, centering all samples)
+faces_centered = faces - faces.mean(axis=0)
+
+# Local centering (focus on one sample, centering all features)
+faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)
+
+print("Dataset consists of %d faces" % n_samples)
+
+# %%
+# Define a base function to plot the gallery of faces.
+
 n_row, n_col = 2, 3
 n_components = n_row * n_col
 image_shape = (64, 64)
-rng = RandomState(0)
-
-# #############################################################################
-# Load faces data
-dataset = fetch_olivetti_faces(shuffle=True, random_state=rng)
-faces = dataset.data
-
-n_samples, n_features = faces.shape
-
-# global centering
-faces_centered = faces - faces.mean(axis=0)
-
-# local centering
-faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)
-
-print("Dataset consists of %d faces" % n_samples)


 def plot_gallery(title, images, n_col=n_col, n_row=n_row, cmap=plt.cm.gray):
-    plt.figure(figsize=(2. * n_col, 2.26 * n_row))
-    plt.suptitle(title, size=16)
-    for i, comp in enumerate(images):
-        plt.subplot(n_row, n_col, i + 1)
-        vmax = max(comp.max(), -comp.min())
-        plt.imshow(comp.reshape(image_shape), cmap=cmap,
-                   interpolation='nearest',
-                   vmin=-vmax, vmax=vmax)
-        plt.xticks(())
-        plt.yticks(())
-    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)
-
-# #############################################################################
-# List of the different estimators, whether to center and transpose the
-# problem, and whether the transformer uses the clustering API.
-estimators = [
-    ('Eigenfaces - PCA using randomized SVD',
-     decomposition.PCA(n_components=n_components, svd_solver='randomized',
-                       whiten=True),
-     True),
-
-    ('Non-negative components - NMF',
-     decomposition.NMF(n_components=n_components, init='nndsvda', tol=5e-3),
-     False),
-
-    ('Independent components - FastICA',
-     decomposition.FastICA(n_components=n_components, whiten=True),
-     True),
-
-    ('Sparse comp. - MiniBatchSparsePCA',
-     decomposition.MiniBatchSparsePCA(n_components=n_components, alpha=0.8,
-                                      n_iter=100, batch_size=3,
-                                      random_state=rng,
-                                      normalize_components=True),
-     True),
-
-    ('MiniBatchDictionaryLearning',
-        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,
-                                                  n_iter=50, batch_size=3,
-                                                  random_state=rng),
-     True),
-
-    ('Cluster centers - MiniBatchKMeans',
-        MiniBatchKMeans(n_clusters=n_components, tol=1e-3, batch_size=20,
-                        max_iter=50, random_state=rng),
-     True),
-
-    ('Factor Analysis components - FA',
-     decomposition.FactorAnalysis(n_components=n_components, max_iter=20),
-     True),
-]
-
-
-# #############################################################################
-# Plot a sample of the input data
-
-plot_gallery("First centered Olivetti faces", faces_centered[:n_components])
-
-# #############################################################################
-# Do the estimation and plot it
-
-for name, estimator, center in estimators:
-    print("Extracting the top %d %s..." % (n_components, name))
-    t0 = time()
-    data = faces
-    if center:
-        data = faces_centered
-    estimator.fit(data)
-    train_time = (time() - t0)
-    print("done in %0.3fs" % train_time)
-    if hasattr(estimator, 'cluster_centers_'):
-        components_ = estimator.cluster_centers_
-    else:
-        components_ = estimator.components_
-
-    # Plot an image representing the pixelwise variance provided by the
-    # estimator e.g its noise_variance_ attribute. The Eigenfaces estimator,
-    # via the PCA decomposition, also provides a scalar noise_variance_
-    # (the mean of pixelwise variance) that cannot be displayed as an image
-    # so we skip it.
-    if (hasattr(estimator, 'noise_variance_') and
-            estimator.noise_variance_.ndim > 0):  # Skip the Eigenfaces case
-        plot_gallery("Pixelwise variance",
-                     estimator.noise_variance_.reshape(1, -1), n_col=1,
-                     n_row=1)
-    plot_gallery('%s - Train time %.1fs' % (name, train_time),
-                 components_[:n_components])
-
+    fig, axs = plt.subplots(
+        nrows=n_row,
+        ncols=n_col,
+        figsize=(2.0 * n_col, 2.3 * n_row),
+        facecolor="white",
+        constrained_layout=True,
+    )
+    fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, hspace=0, wspace=0)
+    fig.set_edgecolor("black")
+    fig.suptitle(title, size=16)
+    for ax, vec in zip(axs.flat, images):
+        vmax = max(vec.max(), -vec.min())
+        im = ax.imshow(
+            vec.reshape(image_shape),
+            cmap=cmap,
+            interpolation="nearest",
+            vmin=-vmax,
+            vmax=vmax,
+        )
+        ax.axis("off")
+
+    fig.colorbar(im, ax=axs, orientation="horizontal", shrink=0.99, aspect=40, pad=0.01)
+    plt.show()
+
+
+# %%
+# Let’s take a look at our data. Gray color indicates negative values,
+# white indicates positive values.
+
+plot_gallery("Faces from dataset", faces_centered[:n_components])
+
+# %%
+# Decomposition
+# -------------
+#
+# Initialise different estimators for decomposition and fit each
+# of them on all images and plot some results. Each estimator extracts
+# 6 components as vectors :math:`h \in \mathbb{R}^{4096}`.
+# We just displayed these vectors in human-friendly visualisation as 64x64 pixel images.
+#
+# Read more in the :ref:`User Guide <decompositions>`.
+
+# %%
+# Eigenfaces - PCA using randomized SVD
+# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+# Linear dimensionality reduction using Singular Value Decomposition (SVD) of the data
+# to project it to a lower dimensional space.
+#
+#
+# .. note::
+#
+#     The Eigenfaces estimator, via the :py:mod:`sklearn.decomposition.PCA`,
+#     also provides a scalar `noise_variance_` (the mean of pixelwise variance)
+#     that cannot be displayed as an image.
+
+# %%
+pca_estimator = decomposition.PCA(
+    n_components=n_components, svd_solver="randomized", whiten=True
+)
+pca_estimator.fit(faces_centered)
+plot_gallery(
+    "Eigenfaces - PCA using randomized SVD", pca_estimator.components_[:n_components]
+)
+
+# %%
+# Non-negative components - NMF
+# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+#
+# Estimate non-negative original data as production of two non-negative matrices.
+
+# %%
+nmf_estimator = decomposition.NMF(n_components=n_components, tol=5e-3)
+nmf_estimator.fit(faces)  # original non- negative dataset
+plot_gallery("Non-negative components - NMF", nmf_estimator.components_[:n_components])
+
+# %%
+# Independent components - FastICA
+# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+# Independent component analysis separates a multivariate vectors into additive
+# subcomponents that are maximally independent.
+
+# %%
+ica_estimator = decomposition.FastICA(
+    n_components=n_components, max_iter=400, whiten="arbitrary-variance", tol=15e-5
+)
+ica_estimator.fit(faces_centered)
+plot_gallery(
+    "Independent components - FastICA", ica_estimator.components_[:n_components]
+)
+
+# %%
+# Sparse components - MiniBatchSparsePCA
+# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+#
+# Mini-batch sparse PCA (`MiniBatchSparsePCA`) extracts the set of sparse
+# components that best reconstruct the data. This variant is faster but
+# less accurate than the similar :py:mod:`sklearn.decomposition.SparsePCA`.
+
+# %%
+batch_pca_estimator = decomposition.MiniBatchSparsePCA(
+    n_components=n_components, alpha=0.1, n_iter=100, batch_size=3, random_state=rng
+)
+batch_pca_estimator.fit(faces_centered)
+plot_gallery(
+    "Sparse components - MiniBatchSparsePCA",
+    batch_pca_estimator.components_[:n_components],
+)
+
+# %%
+# Dictionary learning
+# ^^^^^^^^^^^^^^^^^^^
+#
+# By default, :class:`MiniBatchDictionaryLearning` divides the data into
+# mini-batches and optimizes in an online manner by cycling over the
+# mini-batches for the specified number of iterations.
+
+# %%
+batch_dict_estimator = decomposition.MiniBatchDictionaryLearning(
+    n_components=n_components, alpha=0.1, n_iter=50, batch_size=3, random_state=rng
+)
+batch_dict_estimator.fit(faces_centered)
+plot_gallery("Dictionary learning", batch_dict_estimator.components_[:n_components])
+
+# %%
+# Cluster centers - MiniBatchKMeans
+# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+#
+# `MiniBatchKMeans` is computationally efficient and implements on-line
+# learning with a `partial_fit` method. That is why it could be beneficial
+# to enhance some time-consuming algorithms with  `MiniBatchKMeans`.
+
+# %%
+kmeans_estimator = cluster.MiniBatchKMeans(
+    n_clusters=n_components,
+    tol=1e-3,
+    batch_size=20,
+    max_iter=50,
+    random_state=rng,
+)
+kmeans_estimator.fit(faces_centered)
+plot_gallery(
+    "Cluster centers - MiniBatchKMeans",
+    kmeans_estimator.cluster_centers_[:n_components],
+)
+
+
+# %%
+# Factor Analysis components - FA
+# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+#
+# `Factor Analysis` is similar to `PCA` but has the advantage of modelling the
+# variance in every direction of the input space independently
+# (heteroscedastic noise).
+# Read more in the :ref:`User Guide <FA>`.
+
+# %%
+fa_estimator = decomposition.FactorAnalysis(n_components=n_components, max_iter=20)
+fa_estimator.fit(faces_centered)
+plot_gallery("Factor Analysis (FA)", fa_estimator.components_[:n_components])
+
+# --- Pixelwise variance
+plt.figure(figsize=(3.2, 3.6), facecolor="white", tight_layout=True)
+vec = fa_estimator.noise_variance_
+vmax = max(vec.max(), -vec.min())
+plt.imshow(
+    vec.reshape(image_shape),
+    cmap=plt.cm.gray,
+    interpolation="nearest",
+    vmin=-vmax,
+    vmax=vmax,
+)
+plt.axis("off")
+plt.title("Pixelwise variance from \n Factor Analysis (FA)", size=16, wrap=True)
+plt.colorbar(orientation="horizontal", shrink=0.8, pad=0.03)
 plt.show()

-# #############################################################################
-# Various positivity constraints applied to dictionary learning.
-estimators = [
-    ('Dictionary learning',
-        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,
-                                                  n_iter=50, batch_size=3,
-                                                  random_state=rng),
-     True),
-    ('Dictionary learning - positive dictionary',
-        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,
-                                                  n_iter=50, batch_size=3,
-                                                  random_state=rng,
-                                                  positive_dict=True),
-     True),
-    ('Dictionary learning - positive code',
-        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,
-                                                  n_iter=50, batch_size=3,
-                                                  random_state=rng,
-                                                  positive_code=True),
-     True),
-    ('Dictionary learning - positive dictionary & code',
-        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,
-                                                  n_iter=50, batch_size=3,
-                                                  random_state=rng,
-                                                  positive_dict=True,
-                                                  positive_code=True),
-     True),
-]
-
-
-# #############################################################################
-# Plot a sample of the input data
-
-plot_gallery("First centered Olivetti faces", faces_centered[:n_components],
-             cmap=plt.cm.RdBu)
-
-# #############################################################################
-# Do the estimation and plot it
-
-for name, estimator, center in estimators:
-    print("Extracting the top %d %s..." % (n_components, name))
-    t0 = time()
-    data = faces
-    if center:
-        data = faces_centered
-    estimator.fit(data)
-    train_time = (time() - t0)
-    print("done in %0.3fs" % train_time)
-    components_ = estimator.components_
-    plot_gallery(name, components_[:n_components], cmap=plt.cm.RdBu)
-
-plt.show()
+# %%
+# Decomposition: Dictionary learning
+# ----------------------------------
+#
+# In the further section, let's consider :ref:`DictionaryLearning` more precisely.
+# Dictionary learning is a problem that amounts to finding a sparse representation
+# of the input data as a combination of simple elements. These simple elements form
+# a dictionary. It is possible to constrain the dictionary and/or coding coefficients
+# to be positive to match constraints that may be present in the data.
+#
+# :class:`MiniBatchDictionaryLearning` implements a faster, but less accurate
+# version of the dictionary learning algorithm that is better suited for large
+# datasets. Read more in the :ref:`User Guide <MiniBatchDictionaryLearning>`.
+
+# %%
+# Plot the same samples from our dataset but with another colormap.
+# Red indicates negative values, blue indicates positive values,
+# and white represents zeros.
+
+plot_gallery("Faces from dataset", faces_centered[:n_components], cmap=plt.cm.RdBu)
+
+# %%
+# Similar to the previous examples, we change parameters and train
+# `MiniBatchDictionaryLearning` estimator on all images. Generally,
+# the dictionary learning and sparse encoding decompose input data
+# into the dictionary and the coding coefficients matrices.
+# :math:`X \approx UV`, where :math:`X = [x_1, . . . , x_n]`,
+# :math:`X \in \mathbb{R}^{m×n}`, dictionary :math:`U \in \mathbb{R}^{m×k}`, coding
+# coefficients :math:`V \in \mathbb{R}^{k×n}`.
+#
+# Also below are the results when the dictionary and coding
+# coefficients are positively constrained.
+
+# %%
+# Dictionary learning - positive dictionary
+# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+#
+# In the following section we enforce positivity when finding the dictionary.
+
+# %%
+dict_pos_dict_estimator = decomposition.MiniBatchDictionaryLearning(
+    n_components=n_components,
+    alpha=0.1,
+    n_iter=50,
+    batch_size=3,
+    random_state=rng,
+    positive_dict=True,
+)
+dict_pos_dict_estimator.fit(faces_centered)
+plot_gallery(
+    "Dictionary learning - positive dictionary",
+    dict_pos_dict_estimator.components_[:n_components],
+    cmap=plt.cm.RdBu,
+)
+
+# %%
+# Dictionary learning - positive code
+# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+#
+# Below we constrain the coding coefficients as a positive matrix.
+
+# %%
+dict_pos_code_estimator = decomposition.MiniBatchDictionaryLearning(
+    n_components=n_components,
+    alpha=0.1,
+    n_iter=50,
+    batch_size=3,
+    fit_algorithm="cd",
+    random_state=rng,
+    positive_code=True,
+)
+dict_pos_code_estimator.fit(faces_centered)
+plot_gallery(
+    "Dictionary learning - positive code",
+    dict_pos_code_estimator.components_[:n_components],
+    cmap=plt.cm.RdBu,
+)
+
+# %%
+# Dictionary learning - positive dictionary & code
+# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+#
+# Also below are the results if the dictionary values and coding
+# coefficients are positively constrained.
+
+# %%
+dict_pos_estimator = decomposition.MiniBatchDictionaryLearning(
+    n_components=n_components,
+    alpha=0.1,
+    n_iter=50,
+    batch_size=3,
+    fit_algorithm="cd",
+    random_state=rng,
+    positive_dict=True,
+    positive_code=True,
+)
+dict_pos_estimator.fit(faces_centered)
+plot_gallery(
+    "Dictionary learning - positive dictionary & code",
+    dict_pos_estimator.components_[:n_components],
+    cmap=plt.cm.RdBu,
+)
('examples/decomposition', 'plot_pca_vs_lda.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -15,8 +15,8 @@
 Linear Discriminant Analysis (LDA) tries to identify attributes that
 account for the most variance *between classes*. In particular,
 LDA, in contrast to PCA, is a supervised method, using known class labels.
+
 """
-print(__doc__)

 import matplotlib.pyplot as plt

@@ -37,24 +37,28 @@
 X_r2 = lda.fit(X, y).transform(X)

 # Percentage of variance explained for each components
-print('explained variance ratio (first two components): %s'
-      % str(pca.explained_variance_ratio_))
+print(
+    "explained variance ratio (first two components): %s"
+    % str(pca.explained_variance_ratio_)
+)

 plt.figure()
-colors = ['navy', 'turquoise', 'darkorange']
+colors = ["navy", "turquoise", "darkorange"]
 lw = 2

 for color, i, target_name in zip(colors, [0, 1, 2], target_names):
-    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,
-                label=target_name)
-plt.legend(loc='best', shadow=False, scatterpoints=1)
-plt.title('PCA of IRIS dataset')
+    plt.scatter(
+        X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=0.8, lw=lw, label=target_name
+    )
+plt.legend(loc="best", shadow=False, scatterpoints=1)
+plt.title("PCA of IRIS dataset")

 plt.figure()
 for color, i, target_name in zip(colors, [0, 1, 2], target_names):
-    plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color,
-                label=target_name)
-plt.legend(loc='best', shadow=False, scatterpoints=1)
-plt.title('LDA of IRIS dataset')
+    plt.scatter(
+        X_r2[y == i, 0], X_r2[y == i, 1], alpha=0.8, color=color, label=target_name
+    )
+plt.legend(loc="best", shadow=False, scatterpoints=1)
+plt.title("LDA of IRIS dataset")

 plt.show()
('examples/decomposition', 'plot_image_denoising.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -31,28 +31,24 @@
 necessarily related to visualisation.

 """
-print(__doc__)

-from time import time
-
-import matplotlib.pyplot as plt
+# %%
+# Generate distorted image
+# ------------------------
 import numpy as np
 import scipy as sp
-
-from sklearn.decomposition import MiniBatchDictionaryLearning
-from sklearn.feature_extraction.image import extract_patches_2d
-from sklearn.feature_extraction.image import reconstruct_from_patches_2d


 try:  # SciPy >= 0.16 have face in misc
     from scipy.misc import face
+
     face = face(gray=True)
 except ImportError:
     face = sp.face(gray=True)

 # Convert from uint8 representation with values between 0 and 255 to
 # a floating point representation with values between 0 and 1.
-face = face / 255.
+face = face / 255.0

 # downsample for higher speed
 face = face[::4, ::4] + face[1::4, ::4] + face[::4, 1::4] + face[1::4, 1::4]
@@ -60,91 +56,115 @@
 height, width = face.shape

 # Distort the right half of the image
-print('Distorting image...')
+print("Distorting image...")
 distorted = face.copy()
-distorted[:, width // 2:] += 0.075 * np.random.randn(height, width // 2)
-
-# Extract all reference patches from the left half of the image
-print('Extracting reference patches...')
-t0 = time()
-patch_size = (7, 7)
-data = extract_patches_2d(distorted[:, :width // 2], patch_size)
-data = data.reshape(data.shape[0], -1)
-data -= np.mean(data, axis=0)
-data /= np.std(data, axis=0)
-print('done in %.2fs.' % (time() - t0))
-
-# #############################################################################
-# Learn the dictionary from reference patches
-
-print('Learning the dictionary...')
-t0 = time()
-dico = MiniBatchDictionaryLearning(n_components=100, alpha=1, n_iter=500)
-V = dico.fit(data).components_
-dt = time() - t0
-print('done in %.2fs.' % dt)
-
-plt.figure(figsize=(4.2, 4))
-for i, comp in enumerate(V[:100]):
-    plt.subplot(10, 10, i + 1)
-    plt.imshow(comp.reshape(patch_size), cmap=plt.cm.gray_r,
-               interpolation='nearest')
-    plt.xticks(())
-    plt.yticks(())
-plt.suptitle('Dictionary learned from face patches\n' +
-             'Train time %.1fs on %d patches' % (dt, len(data)),
-             fontsize=16)
-plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)
+distorted[:, width // 2 :] += 0.075 * np.random.randn(height, width // 2)


-# #############################################################################
+# %%
 # Display the distorted image
+# ---------------------------
+import matplotlib.pyplot as plt
+

 def show_with_diff(image, reference, title):
     """Helper function to display denoising"""
     plt.figure(figsize=(5, 3.3))
     plt.subplot(1, 2, 1)
-    plt.title('Image')
-    plt.imshow(image, vmin=0, vmax=1, cmap=plt.cm.gray,
-               interpolation='nearest')
+    plt.title("Image")
+    plt.imshow(image, vmin=0, vmax=1, cmap=plt.cm.gray, interpolation="nearest")
     plt.xticks(())
     plt.yticks(())
     plt.subplot(1, 2, 2)
     difference = image - reference

-    plt.title('Difference (norm: %.2f)' % np.sqrt(np.sum(difference ** 2)))
-    plt.imshow(difference, vmin=-0.5, vmax=0.5, cmap=plt.cm.PuOr,
-               interpolation='nearest')
+    plt.title("Difference (norm: %.2f)" % np.sqrt(np.sum(difference**2)))
+    plt.imshow(
+        difference, vmin=-0.5, vmax=0.5, cmap=plt.cm.PuOr, interpolation="nearest"
+    )
     plt.xticks(())
     plt.yticks(())
     plt.suptitle(title, size=16)
     plt.subplots_adjust(0.02, 0.02, 0.98, 0.79, 0.02, 0.2)

-show_with_diff(distorted, face, 'Distorted image')

-# #############################################################################
+show_with_diff(distorted, face, "Distorted image")
+
+
+# %%
+# Extract reference patches
+# ----------------------------
+from time import time
+
+from sklearn.feature_extraction.image import extract_patches_2d
+
+# Extract all reference patches from the left half of the image
+print("Extracting reference patches...")
+t0 = time()
+patch_size = (7, 7)
+data = extract_patches_2d(distorted[:, : width // 2], patch_size)
+data = data.reshape(data.shape[0], -1)
+data -= np.mean(data, axis=0)
+data /= np.std(data, axis=0)
+print(f"{data.shape[0]} patches extracted in %.2fs." % (time() - t0))
+
+
+# %%
+# Learn the dictionary from reference patches
+# -------------------------------------------
+from sklearn.decomposition import MiniBatchDictionaryLearning
+
+print("Learning the dictionary...")
+t0 = time()
+dico = MiniBatchDictionaryLearning(
+    # increase to 300 for higher quality results at the cost of slower
+    # training times.
+    n_components=50,
+    batch_size=200,
+    alpha=1.0,
+    max_iter=10,
+)
+V = dico.fit(data).components_
+dt = time() - t0
+print(f"{dico.n_iter_} iterations / {dico.n_steps_} steps in {dt:.2f}.")
+
+plt.figure(figsize=(4.2, 4))
+for i, comp in enumerate(V[:100]):
+    plt.subplot(10, 10, i + 1)
+    plt.imshow(comp.reshape(patch_size), cmap=plt.cm.gray_r, interpolation="nearest")
+    plt.xticks(())
+    plt.yticks(())
+plt.suptitle(
+    "Dictionary learned from face patches\n"
+    + "Train time %.1fs on %d patches" % (dt, len(data)),
+    fontsize=16,
+)
+plt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)
+
+
+# %%
 # Extract noisy patches and reconstruct them using the dictionary
+# ---------------------------------------------------------------
+from sklearn.feature_extraction.image import reconstruct_from_patches_2d

-print('Extracting noisy patches... ')
+print("Extracting noisy patches... ")
 t0 = time()
-data = extract_patches_2d(distorted[:, width // 2:], patch_size)
+data = extract_patches_2d(distorted[:, width // 2 :], patch_size)
 data = data.reshape(data.shape[0], -1)
 intercept = np.mean(data, axis=0)
 data -= intercept
-print('done in %.2fs.' % (time() - t0))
+print("done in %.2fs." % (time() - t0))

 transform_algorithms = [
-    ('Orthogonal Matching Pursuit\n1 atom', 'omp',
-     {'transform_n_nonzero_coefs': 1}),
-    ('Orthogonal Matching Pursuit\n2 atoms', 'omp',
-     {'transform_n_nonzero_coefs': 2}),
-    ('Least-angle regression\n5 atoms', 'lars',
-     {'transform_n_nonzero_coefs': 5}),
-    ('Thresholding\n alpha=0.1', 'threshold', {'transform_alpha': .1})]
+    ("Orthogonal Matching Pursuit\n1 atom", "omp", {"transform_n_nonzero_coefs": 1}),
+    ("Orthogonal Matching Pursuit\n2 atoms", "omp", {"transform_n_nonzero_coefs": 2}),
+    ("Least-angle regression\n4 atoms", "lars", {"transform_n_nonzero_coefs": 4}),
+    ("Thresholding\n alpha=0.1", "threshold", {"transform_alpha": 0.1}),
+]

 reconstructions = {}
 for title, transform_algorithm, kwargs in transform_algorithms:
-    print(title + '...')
+    print(title + "...")
     reconstructions[title] = face.copy()
     t0 = time()
     dico.set_params(transform_algorithm=transform_algorithm, **kwargs)
@@ -153,14 +173,14 @@

     patches += intercept
     patches = patches.reshape(len(data), *patch_size)
-    if transform_algorithm == 'threshold':
+    if transform_algorithm == "threshold":
         patches -= patches.min()
         patches /= patches.max()
-    reconstructions[title][:, width // 2:] = reconstruct_from_patches_2d(
-        patches, (height, width // 2))
+    reconstructions[title][:, width // 2 :] = reconstruct_from_patches_2d(
+        patches, (height, width // 2)
+    )
     dt = time() - t0
-    print('done in %.2fs.' % dt)
-    show_with_diff(reconstructions[title], face,
-                   title + ' (time: %.1fs)' % dt)
+    print("done in %.2fs." % dt)
+    show_with_diff(reconstructions[title], face, title + " (time: %.1fs)" % dt)

 plt.show()
('examples/decomposition', 'plot_pca_vs_fa_model_selection.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -16,7 +16,8 @@
 in recovering the size of the low rank subspace. The likelihood with PCA
 is higher than FA in this case. However PCA fails and overestimates
 the rank when heteroscedastic noise is present. Under appropriate
-circumstances the low rank models are more likely than shrinkage models.
+circumstances (choice of the number of components), the held-out
+data is more likely for low rank models than for shrinkage models.

 The automatic estimation from
 Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604
@@ -37,13 +38,11 @@
 from sklearn.model_selection import cross_val_score
 from sklearn.model_selection import GridSearchCV

-print(__doc__)
-
 # #############################################################################
 # Create the data

-n_samples, n_features, rank = 1000, 50, 10
-sigma = 1.
+n_samples, n_features, rank = 500, 25, 5
+sigma = 1.0
 rng = np.random.RandomState(42)
 U, _, _ = linalg.svd(rng.randn(n_features, n_features))
 X = np.dot(rng.randn(n_samples, rank), U[:, :rank].T)
@@ -52,7 +51,7 @@
 X_homo = X + sigma * rng.randn(n_samples, n_features)

 # Adding heteroscedastic noise
-sigmas = sigma * rng.rand(n_features) + sigma / 2.
+sigmas = sigma * rng.rand(n_features) + sigma / 2.0
 X_hetero = X + rng.randn(n_samples, n_features) * sigmas

 # #############################################################################
@@ -62,36 +61,35 @@


 def compute_scores(X):
-    pca = PCA(svd_solver='full')
+    pca = PCA(svd_solver="full")
     fa = FactorAnalysis()

     pca_scores, fa_scores = [], []
     for n in n_components:
         pca.n_components = n
         fa.n_components = n
-        pca_scores.append(np.mean(cross_val_score(pca, X, cv=5)))
-        fa_scores.append(np.mean(cross_val_score(fa, X, cv=5)))
+        pca_scores.append(np.mean(cross_val_score(pca, X)))
+        fa_scores.append(np.mean(cross_val_score(fa, X)))

     return pca_scores, fa_scores


 def shrunk_cov_score(X):
     shrinkages = np.logspace(-2, 0, 30)
-    cv = GridSearchCV(ShrunkCovariance(), {'shrinkage': shrinkages}, cv=5)
-    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X, cv=5))
+    cv = GridSearchCV(ShrunkCovariance(), {"shrinkage": shrinkages})
+    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X))


 def lw_score(X):
-    return np.mean(cross_val_score(LedoitWolf(), X, cv=5))
+    return np.mean(cross_val_score(LedoitWolf(), X))


-for X, title in [(X_homo, 'Homoscedastic Noise'),
-                 (X_hetero, 'Heteroscedastic Noise')]:
+for X, title in [(X_homo, "Homoscedastic Noise"), (X_hetero, "Heteroscedastic Noise")]:
     pca_scores, fa_scores = compute_scores(X)
     n_components_pca = n_components[np.argmax(pca_scores)]
     n_components_fa = n_components[np.argmax(fa_scores)]

-    pca = PCA(svd_solver='full', n_components='mle')
+    pca = PCA(svd_solver="full", n_components="mle")
     pca.fit(X)
     n_components_pca_mle = pca.n_components_

@@ -100,26 +98,45 @@
     print("best n_components by PCA MLE = %d" % n_components_pca_mle)

     plt.figure()
-    plt.plot(n_components, pca_scores, 'b', label='PCA scores')
-    plt.plot(n_components, fa_scores, 'r', label='FA scores')
-    plt.axvline(rank, color='g', label='TRUTH: %d' % rank, linestyle='-')
-    plt.axvline(n_components_pca, color='b',
-                label='PCA CV: %d' % n_components_pca, linestyle='--')
-    plt.axvline(n_components_fa, color='r',
-                label='FactorAnalysis CV: %d' % n_components_fa,
-                linestyle='--')
-    plt.axvline(n_components_pca_mle, color='k',
-                label='PCA MLE: %d' % n_components_pca_mle, linestyle='--')
+    plt.plot(n_components, pca_scores, "b", label="PCA scores")
+    plt.plot(n_components, fa_scores, "r", label="FA scores")
+    plt.axvline(rank, color="g", label="TRUTH: %d" % rank, linestyle="-")
+    plt.axvline(
+        n_components_pca,
+        color="b",
+        label="PCA CV: %d" % n_components_pca,
+        linestyle="--",
+    )
+    plt.axvline(
+        n_components_fa,
+        color="r",
+        label="FactorAnalysis CV: %d" % n_components_fa,
+        linestyle="--",
+    )
+    plt.axvline(
+        n_components_pca_mle,
+        color="k",
+        label="PCA MLE: %d" % n_components_pca_mle,
+        linestyle="--",
+    )

     # compare with other covariance estimators
-    plt.axhline(shrunk_cov_score(X), color='violet',
-                label='Shrunk Covariance MLE', linestyle='-.')
-    plt.axhline(lw_score(X), color='orange',
-                label='LedoitWolf MLE' % n_components_pca_mle, linestyle='-.')
+    plt.axhline(
+        shrunk_cov_score(X),
+        color="violet",
+        label="Shrunk Covariance MLE",
+        linestyle="-.",
+    )
+    plt.axhline(
+        lw_score(X),
+        color="orange",
+        label="LedoitWolf MLE" % n_components_pca_mle,
+        linestyle="-.",
+    )

-    plt.xlabel('nb of components')
-    plt.ylabel('CV scores')
-    plt.legend(loc='lower right')
+    plt.xlabel("nb of components")
+    plt.ylabel("CV scores")
+    plt.legend(loc="lower right")
     plt.title(title)

 plt.show()
('examples/decomposition', 'plot_pca_iris.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =========================================================
 PCA example with Iris Data-set
@@ -12,15 +10,12 @@
 information on this dataset.

 """
-print(__doc__)
-

 # Code source: Gaël Varoquaux
 # License: BSD 3 clause

 import numpy as np
 import matplotlib.pyplot as plt
-from mpl_toolkits.mplot3d import Axes3D


 from sklearn import decomposition
@@ -28,30 +23,34 @@

 np.random.seed(5)

-centers = [[1, 1], [-1, -1], [1, -1]]
 iris = datasets.load_iris()
 X = iris.data
 y = iris.target

 fig = plt.figure(1, figsize=(4, 3))
 plt.clf()
-ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)
+
+ax = fig.add_subplot(111, projection="3d", elev=48, azim=134)
+ax.set_position([0, 0, 0.95, 1])
+

 plt.cla()
 pca = decomposition.PCA(n_components=3)
 pca.fit(X)
 X = pca.transform(X)

-for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:
-    ax.text3D(X[y == label, 0].mean(),
-              X[y == label, 1].mean() + 1.5,
-              X[y == label, 2].mean(), name,
-              horizontalalignment='center',
-              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))
+for name, label in [("Setosa", 0), ("Versicolour", 1), ("Virginica", 2)]:
+    ax.text3D(
+        X[y == label, 0].mean(),
+        X[y == label, 1].mean() + 1.5,
+        X[y == label, 2].mean(),
+        name,
+        horizontalalignment="center",
+        bbox=dict(alpha=0.5, edgecolor="w", facecolor="w"),
+    )
 # Reorder the labels to have colors matching the cluster results
-y = np.choose(y, [1, 2, 0]).astype(np.float)
-ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral,
-           edgecolor='k')
+y = np.choose(y, [1, 2, 0]).astype(float)
+ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor="k")

 ax.w_xaxis.set_ticklabels([])
 ax.w_yaxis.set_ticklabels([])
('examples/decomposition', 'plot_ica_blind_source_separation.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -13,7 +13,6 @@
 non-Gaussian processes.

 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -57,11 +56,13 @@
 plt.figure()

 models = [X, S, S_, H]
-names = ['Observations (mixed signal)',
-         'True Sources',
-         'ICA recovered signals',
-         'PCA recovered signals']
-colors = ['red', 'steelblue', 'orange']
+names = [
+    "Observations (mixed signal)",
+    "True Sources",
+    "ICA recovered signals",
+    "PCA recovered signals",
+]
+colors = ["red", "steelblue", "orange"]

 for ii, (model, name) in enumerate(zip(models, names), 1):
     plt.subplot(4, 1, ii)
@@ -69,5 +70,5 @@
     for sig, color in zip(model.T, colors):
         plt.plot(sig, color=color)

-plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.46)
+plt.tight_layout()
 plt.show()
('examples/decomposition', 'plot_beta_divergence.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,19 +4,19 @@
 ==============================

 A plot that compares the various Beta-divergence loss functions supported by
-the Multiplicative-Update ('mu') solver in :class:`sklearn.decomposition.NMF`.
+the Multiplicative-Update ('mu') solver in :class:`~sklearn.decomposition.NMF`.
+
 """
+
 import numpy as np
 import matplotlib.pyplot as plt
-from sklearn.decomposition.nmf import _beta_divergence
-
-print(__doc__)
+from sklearn.decomposition._nmf import _beta_divergence

 x = np.linspace(0.001, 4, 1000)
 y = np.zeros(x.shape)

-colors = 'mbgyr'
-for j, beta in enumerate((0., 0.5, 1., 1.5, 2.)):
+colors = "mbgyr"
+for j, beta in enumerate((0.0, 0.5, 1.0, 1.5, 2.0)):
     for i, xi in enumerate(x):
         y[i] = _beta_divergence(1, xi, 1, beta)
     name = "beta = %1.1f" % beta
('examples/decomposition', 'plot_pca_3d.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,4 @@
-#!/usr/bin/python
 # -*- coding: utf-8 -*-
-
 """
 =========================================================
 Principal components analysis (PCA)
@@ -11,31 +9,27 @@
 comes in to choose a direction that is not flat.

 """
-print(__doc__)

 # Authors: Gael Varoquaux
 #          Jaques Grobler
 #          Kevin Hughes
 # License: BSD 3 clause

-from sklearn.decomposition import PCA
+# %%
+# Create the data
+# ---------------

-from mpl_toolkits.mplot3d import Axes3D
 import numpy as np
-import matplotlib.pyplot as plt
+
 from scipy import stats
-
-
-# #############################################################################
-# Create the data

 e = np.exp(1)
 np.random.seed(4)


 def pdf(x):
-    return 0.5 * (stats.norm(scale=0.25 / e).pdf(x)
-                  + stats.norm(scale=4 / e).pdf(x))
+    return 0.5 * (stats.norm(scale=0.25 / e).pdf(x) + stats.norm(scale=4 / e).pdf(x))
+

 y = np.random.normal(scale=0.5, size=(30000))
 x = np.random.normal(scale=0.5, size=(30000))
@@ -55,28 +49,38 @@
 b /= norm


-# #############################################################################
+# %%
 # Plot the figures
+# ----------------
+
+from sklearn.decomposition import PCA
+
+import matplotlib.pyplot as plt
+
+# unused but required import for doing 3d projections with matplotlib < 3.2
+import mpl_toolkits.mplot3d  # noqa: F401
+
+
 def plot_figs(fig_num, elev, azim):
     fig = plt.figure(fig_num, figsize=(4, 3))
     plt.clf()
-    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=elev, azim=azim)
+    ax = fig.add_subplot(111, projection="3d", elev=elev, azim=azim)
+    ax.set_position([0, 0, 0.95, 1])

-    ax.scatter(a[::10], b[::10], c[::10], c=density[::10], marker='+', alpha=.4)
+    ax.scatter(a[::10], b[::10], c[::10], c=density[::10], marker="+", alpha=0.4)
     Y = np.c_[a, b, c]

     # Using SciPy's SVD, this would be:
-    # _, pca_score, V = scipy.linalg.svd(Y, full_matrices=False)
+    # _, pca_score, Vt = scipy.linalg.svd(Y, full_matrices=False)

     pca = PCA(n_components=3)
     pca.fit(Y)
-    pca_score = pca.explained_variance_ratio_
-    V = pca.components_
+    V = pca.components_.T

-    x_pca_axis, y_pca_axis, z_pca_axis = 3 * V.T
-    x_pca_plane = np.r_[x_pca_axis[:2], - x_pca_axis[1::-1]]
-    y_pca_plane = np.r_[y_pca_axis[:2], - y_pca_axis[1::-1]]
-    z_pca_plane = np.r_[z_pca_axis[:2], - z_pca_axis[1::-1]]
+    x_pca_axis, y_pca_axis, z_pca_axis = 3 * V
+    x_pca_plane = np.r_[x_pca_axis[:2], -x_pca_axis[1::-1]]
+    y_pca_plane = np.r_[y_pca_axis[:2], -y_pca_axis[1::-1]]
+    z_pca_plane = np.r_[z_pca_axis[:2], -z_pca_axis[1::-1]]
     x_pca_plane.shape = (2, 2)
     y_pca_plane.shape = (2, 2)
     z_pca_plane.shape = (2, 2)
('examples/decomposition', 'plot_sparse_coding.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,7 +5,7 @@

 Transform a signal as a sparse combination of Ricker wavelets. This example
 visually compares different sparse coding methods using the
-:class:`sklearn.decomposition.SparseCoder` estimator. The Ricker (also known
+:class:`~sklearn.decomposition.SparseCoder` estimator. The Ricker (also known
 as Mexican hat or the second derivative of a Gaussian) is not a particularly
 good kernel to represent piecewise constant signals like this one. It can
 therefore be seen how much adding different widths of atoms matters and it
@@ -13,10 +13,8 @@

 The richer dictionary on the right is not larger in size, heavier subsampling
 is performed in order to stay on the same order of magnitude.
+
 """
-print(__doc__)
-
-from distutils.version import LooseVersion

 import numpy as np
 import matplotlib.pyplot as plt
@@ -27,9 +25,11 @@
 def ricker_function(resolution, center, width):
     """Discrete sub-sampled Ricker (Mexican hat) wavelet"""
     x = np.linspace(0, resolution - 1, resolution)
-    x = ((2 / ((np.sqrt(3 * width) * np.pi ** 1 / 4)))
-         * (1 - ((x - center) ** 2 / width ** 2))
-         * np.exp((-(x - center) ** 2) / (2 * width ** 2)))
+    x = (
+        (2 / (np.sqrt(3 * width) * np.pi**0.25))
+        * (1 - (x - center) ** 2 / width**2)
+        * np.exp(-((x - center) ** 2) / (2 * width**2))
+    )
     return x


@@ -39,7 +39,7 @@
     D = np.empty((n_components, resolution))
     for i, center in enumerate(centers):
         D[i] = ricker_function(resolution, center, width)
-    D /= np.sqrt(np.sum(D ** 2, axis=1))[:, np.newaxis]
+    D /= np.sqrt(np.sum(D**2, axis=1))[:, np.newaxis]
     return D


@@ -49,57 +49,72 @@
 n_components = resolution // subsampling

 # Compute a wavelet dictionary
-D_fixed = ricker_matrix(width=width, resolution=resolution,
-                        n_components=n_components)
-D_multi = np.r_[tuple(ricker_matrix(width=w, resolution=resolution,
-                      n_components=n_components // 5)
-                for w in (10, 50, 100, 500, 1000))]
+D_fixed = ricker_matrix(width=width, resolution=resolution, n_components=n_components)
+D_multi = np.r_[
+    tuple(
+        ricker_matrix(width=w, resolution=resolution, n_components=n_components // 5)
+        for w in (10, 50, 100, 500, 1000)
+    )
+]

 # Generate a signal
 y = np.linspace(0, resolution - 1, resolution)
 first_quarter = y < resolution / 4
-y[first_quarter] = 3.
-y[np.logical_not(first_quarter)] = -1.
+y[first_quarter] = 3.0
+y[np.logical_not(first_quarter)] = -1.0

 # List the different sparse coding methods in the following format:
 # (title, transform_algorithm, transform_alpha,
 #  transform_n_nozero_coefs, color)
-estimators = [('OMP', 'omp', None, 15, 'navy'),
-              ('Lasso', 'lasso_lars', 2, None, 'turquoise'), ]
+estimators = [
+    ("OMP", "omp", None, 15, "navy"),
+    ("Lasso", "lasso_lars", 2, None, "turquoise"),
+]
 lw = 2
-# Avoid FutureWarning about default value change when numpy >= 1.14
-lstsq_rcond = None if LooseVersion(np.__version__) >= '1.14' else -1

 plt.figure(figsize=(13, 6))
-for subplot, (D, title) in enumerate(zip((D_fixed, D_multi),
-                                         ('fixed width', 'multiple widths'))):
+for subplot, (D, title) in enumerate(
+    zip((D_fixed, D_multi), ("fixed width", "multiple widths"))
+):
     plt.subplot(1, 2, subplot + 1)
-    plt.title('Sparse coding against %s dictionary' % title)
-    plt.plot(y, lw=lw, linestyle='--', label='Original signal')
+    plt.title("Sparse coding against %s dictionary" % title)
+    plt.plot(y, lw=lw, linestyle="--", label="Original signal")
     # Do a wavelet approximation
     for title, algo, alpha, n_nonzero, color in estimators:
-        coder = SparseCoder(dictionary=D, transform_n_nonzero_coefs=n_nonzero,
-                            transform_alpha=alpha, transform_algorithm=algo)
+        coder = SparseCoder(
+            dictionary=D,
+            transform_n_nonzero_coefs=n_nonzero,
+            transform_alpha=alpha,
+            transform_algorithm=algo,
+        )
         x = coder.transform(y.reshape(1, -1))
         density = len(np.flatnonzero(x))
         x = np.ravel(np.dot(x, D))
         squared_error = np.sum((y - x) ** 2)
-        plt.plot(x, color=color, lw=lw,
-                 label='%s: %s nonzero coefs,\n%.2f error'
-                 % (title, density, squared_error))
+        plt.plot(
+            x,
+            color=color,
+            lw=lw,
+            label="%s: %s nonzero coefs,\n%.2f error" % (title, density, squared_error),
+        )

     # Soft thresholding debiasing
-    coder = SparseCoder(dictionary=D, transform_algorithm='threshold',
-                        transform_alpha=20)
+    coder = SparseCoder(
+        dictionary=D, transform_algorithm="threshold", transform_alpha=20
+    )
     x = coder.transform(y.reshape(1, -1))
     _, idx = np.where(x != 0)
-    x[0, idx], _, _, _ = np.linalg.lstsq(D[idx, :].T, y, rcond=lstsq_rcond)
+    x[0, idx], _, _, _ = np.linalg.lstsq(D[idx, :].T, y, rcond=None)
     x = np.ravel(np.dot(x, D))
     squared_error = np.sum((y - x) ** 2)
-    plt.plot(x, color='darkorange', lw=lw,
-             label='Thresholding w/ debiasing:\n%d nonzero coefs, %.2f error'
-             % (len(idx), squared_error))
-    plt.axis('tight')
-    plt.legend(shadow=False, loc='best')
-plt.subplots_adjust(.04, .07, .97, .90, .09, .2)
+    plt.plot(
+        x,
+        color="darkorange",
+        lw=lw,
+        label="Thresholding w/ debiasing:\n%d nonzero coefs, %.2f error"
+        % (len(idx), squared_error),
+    )
+    plt.axis("tight")
+    plt.legend(shadow=False, loc="best")
+plt.subplots_adjust(0.04, 0.07, 0.97, 0.90, 0.09, 0.2)
 plt.show()
('examples/decomposition', 'plot_ica_vs_pca.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -26,22 +26,22 @@
 after whitening by the variance corresponding to the PCA vectors (lower
 left). Running ICA corresponds to finding a rotation in this space to
 identify the directions of largest non-Gaussianity (lower right).
+
 """
-print(__doc__)

 # Authors: Alexandre Gramfort, Gael Varoquaux
 # License: BSD 3 clause

+# %%
+# Generate sample data
+# --------------------
 import numpy as np
-import matplotlib.pyplot as plt

 from sklearn.decomposition import PCA, FastICA

-# #############################################################################
-# Generate sample data
 rng = np.random.RandomState(42)
 S = rng.standard_t(1.5, size=(20000, 2))
-S[:, 0] *= 2.
+S[:, 0] *= 2.0

 # Mix data
 A = np.array([[1, 1], [0, 2]])  # Mixing matrix
@@ -51,55 +51,68 @@
 pca = PCA()
 S_pca_ = pca.fit(X).transform(X)

-ica = FastICA(random_state=rng)
+ica = FastICA(random_state=rng, whiten="arbitrary-variance")
 S_ica_ = ica.fit(X).transform(X)  # Estimate the sources

 S_ica_ /= S_ica_.std(axis=0)


-# #############################################################################
+# %%
 # Plot results
+# ------------
+import matplotlib.pyplot as plt
+

 def plot_samples(S, axis_list=None):
-    plt.scatter(S[:, 0], S[:, 1], s=2, marker='o', zorder=10,
-                color='steelblue', alpha=0.5)
+    plt.scatter(
+        S[:, 0], S[:, 1], s=2, marker="o", zorder=10, color="steelblue", alpha=0.5
+    )
     if axis_list is not None:
-        colors = ['orange', 'red']
+        colors = ["orange", "red"]
         for color, axis in zip(colors, axis_list):
             axis /= axis.std()
             x_axis, y_axis = axis
             # Trick to get legend to work
             plt.plot(0.1 * x_axis, 0.1 * y_axis, linewidth=2, color=color)
-            plt.quiver(0, 0, x_axis, y_axis, zorder=11, width=0.01, scale=6,
-                       color=color)
+            plt.quiver(
+                (0, 0),
+                (0, 0),
+                x_axis,
+                y_axis,
+                zorder=11,
+                width=0.01,
+                scale=6,
+                color=color,
+            )

     plt.hlines(0, -3, 3)
     plt.vlines(0, -3, 3)
     plt.xlim(-3, 3)
     plt.ylim(-3, 3)
-    plt.xlabel('x')
-    plt.ylabel('y')
+    plt.xlabel("x")
+    plt.ylabel("y")
+

 plt.figure()
 plt.subplot(2, 2, 1)
 plot_samples(S / S.std())
-plt.title('True Independent Sources')
+plt.title("True Independent Sources")

 axis_list = [pca.components_.T, ica.mixing_]
 plt.subplot(2, 2, 2)
 plot_samples(X / np.std(X), axis_list=axis_list)
-legend = plt.legend(['PCA', 'ICA'], loc='upper right')
+legend = plt.legend(["PCA", "ICA"], loc="upper right")
 legend.set_zorder(100)

-plt.title('Observations')
+plt.title("Observations")

 plt.subplot(2, 2, 3)
 plot_samples(S_pca_ / np.std(S_pca_, axis=0))
-plt.title('PCA recovered signals')
+plt.title("PCA recovered signals")

 plt.subplot(2, 2, 4)
 plot_samples(S_ica_ / np.std(S_ica_))
-plt.title('ICA recovered signals')
+plt.title("ICA recovered signals")

 plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.36)
 plt.show()
('examples/cross_decomposition', 'plot_compare_cross_decomposition.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -17,8 +17,8 @@
 first diagonal). This is also true for components 2 in both dataset,
 however, the correlation across datasets for different components is
 weak: the point cloud is very spherical.
+
 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
@@ -36,10 +36,10 @@
 X = latents + np.random.normal(size=4 * n).reshape((n, 4))
 Y = latents + np.random.normal(size=4 * n).reshape((n, 4))

-X_train = X[:n // 2]
-Y_train = Y[:n // 2]
-X_test = X[n // 2:]
-Y_test = Y[n // 2:]
+X_train = X[: n // 2]
+Y_train = Y[: n // 2]
+X_test = X[n // 2 :]
+Y_test = Y[n // 2 :]

 print("Corr(X)")
 print(np.round(np.corrcoef(X.T), 2))
@@ -61,54 +61,54 @@
 # 1) On diagonal plot X vs Y scores on each components
 plt.figure(figsize=(12, 8))
 plt.subplot(221)
-plt.scatter(X_train_r[:, 0], Y_train_r[:, 0], label="train",
-            marker="o", c="b", s=25)
-plt.scatter(X_test_r[:, 0], Y_test_r[:, 0], label="test",
-            marker="o", c="r", s=25)
+plt.scatter(X_train_r[:, 0], Y_train_r[:, 0], label="train", marker="o", s=25)
+plt.scatter(X_test_r[:, 0], Y_test_r[:, 0], label="test", marker="o", s=25)
 plt.xlabel("x scores")
 plt.ylabel("y scores")
-plt.title('Comp. 1: X vs Y (test corr = %.2f)' %
-          np.corrcoef(X_test_r[:, 0], Y_test_r[:, 0])[0, 1])
+plt.title(
+    "Comp. 1: X vs Y (test corr = %.2f)"
+    % np.corrcoef(X_test_r[:, 0], Y_test_r[:, 0])[0, 1]
+)
 plt.xticks(())
 plt.yticks(())
 plt.legend(loc="best")

 plt.subplot(224)
-plt.scatter(X_train_r[:, 1], Y_train_r[:, 1], label="train",
-            marker="o", c="b", s=25)
-plt.scatter(X_test_r[:, 1], Y_test_r[:, 1], label="test",
-            marker="o", c="r", s=25)
+plt.scatter(X_train_r[:, 1], Y_train_r[:, 1], label="train", marker="o", s=25)
+plt.scatter(X_test_r[:, 1], Y_test_r[:, 1], label="test", marker="o", s=25)
 plt.xlabel("x scores")
 plt.ylabel("y scores")
-plt.title('Comp. 2: X vs Y (test corr = %.2f)' %
-          np.corrcoef(X_test_r[:, 1], Y_test_r[:, 1])[0, 1])
+plt.title(
+    "Comp. 2: X vs Y (test corr = %.2f)"
+    % np.corrcoef(X_test_r[:, 1], Y_test_r[:, 1])[0, 1]
+)
 plt.xticks(())
 plt.yticks(())
 plt.legend(loc="best")

 # 2) Off diagonal plot components 1 vs 2 for X and Y
 plt.subplot(222)
-plt.scatter(X_train_r[:, 0], X_train_r[:, 1], label="train",
-            marker="*", c="b", s=50)
-plt.scatter(X_test_r[:, 0], X_test_r[:, 1], label="test",
-            marker="*", c="r", s=50)
+plt.scatter(X_train_r[:, 0], X_train_r[:, 1], label="train", marker="*", s=50)
+plt.scatter(X_test_r[:, 0], X_test_r[:, 1], label="test", marker="*", s=50)
 plt.xlabel("X comp. 1")
 plt.ylabel("X comp. 2")
-plt.title('X comp. 1 vs X comp. 2 (test corr = %.2f)'
-          % np.corrcoef(X_test_r[:, 0], X_test_r[:, 1])[0, 1])
+plt.title(
+    "X comp. 1 vs X comp. 2 (test corr = %.2f)"
+    % np.corrcoef(X_test_r[:, 0], X_test_r[:, 1])[0, 1]
+)
 plt.legend(loc="best")
 plt.xticks(())
 plt.yticks(())

 plt.subplot(223)
-plt.scatter(Y_train_r[:, 0], Y_train_r[:, 1], label="train",
-            marker="*", c="b", s=50)
-plt.scatter(Y_test_r[:, 0], Y_test_r[:, 1], label="test",
-            marker="*", c="r", s=50)
+plt.scatter(Y_train_r[:, 0], Y_train_r[:, 1], label="train", marker="*", s=50)
+plt.scatter(Y_test_r[:, 0], Y_test_r[:, 1], label="test", marker="*", s=50)
 plt.xlabel("Y comp. 1")
 plt.ylabel("Y comp. 2")
-plt.title('Y comp. 1 vs Y comp. 2 , (test corr = %.2f)'
-          % np.corrcoef(Y_test_r[:, 0], Y_test_r[:, 1])[0, 1])
+plt.title(
+    "Y comp. 1 vs Y comp. 2 , (test corr = %.2f)"
+    % np.corrcoef(Y_test_r[:, 0], Y_test_r[:, 1])[0, 1]
+)
 plt.legend(loc="best")
 plt.xticks(())
 plt.yticks(())
('examples/neighbors', 'plot_species_kde.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -34,7 +34,8 @@
    <http://rob.schapire.net/papers/ecolmod.pdf>`_
    S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling,
    190:231-259, 2006.
-"""
+"""  # noqa: E501
+
 # Author: Jake Vanderplas <jakevdp@cs.washington.edu>
 #
 # License: BSD 3 clause
@@ -42,26 +43,55 @@
 import numpy as np
 import matplotlib.pyplot as plt
 from sklearn.datasets import fetch_species_distributions
-from sklearn.datasets.species_distributions import construct_grids
 from sklearn.neighbors import KernelDensity

 # if basemap is available, we'll use it.
 # otherwise, we'll improvise later...
 try:
     from mpl_toolkits.basemap import Basemap
+
     basemap = True
 except ImportError:
     basemap = False

+
+def construct_grids(batch):
+    """Construct the map grid from the batch object
+
+    Parameters
+    ----------
+    batch : Batch object
+        The object returned by :func:`fetch_species_distributions`
+
+    Returns
+    -------
+    (xgrid, ygrid) : 1-D arrays
+        The grid corresponding to the values in batch.coverages
+    """
+    # x,y coordinates for corner cells
+    xmin = batch.x_left_lower_corner + batch.grid_size
+    xmax = xmin + (batch.Nx * batch.grid_size)
+    ymin = batch.y_left_lower_corner + batch.grid_size
+    ymax = ymin + (batch.Ny * batch.grid_size)
+
+    # x coordinates of the grid cells
+    xgrid = np.arange(xmin, xmax, batch.grid_size)
+    # y coordinates of the grid cells
+    ygrid = np.arange(ymin, ymax, batch.grid_size)
+
+    return (xgrid, ygrid)
+
+
 # Get matrices/arrays of species IDs and locations
 data = fetch_species_distributions()
-species_names = ['Bradypus Variegatus', 'Microryzomys Minutus']
+species_names = ["Bradypus Variegatus", "Microryzomys Minutus"]

-Xtrain = np.vstack([data['train']['dd lat'],
-                    data['train']['dd long']]).T
-ytrain = np.array([d.decode('ascii').startswith('micro')
-                  for d in data['train']['species']], dtype='int')
-Xtrain *= np.pi / 180.  # Convert lat/long to radians
+Xtrain = np.vstack([data["train"]["dd lat"], data["train"]["dd long"]]).T
+ytrain = np.array(
+    [d.decode("ascii").startswith("micro") for d in data["train"]["species"]],
+    dtype="int",
+)
+Xtrain *= np.pi / 180.0  # Convert lat/long to radians

 # Set up the data grid for the contour plot
 xgrid, ygrid = construct_grids(data)
@@ -71,7 +101,7 @@

 xy = np.vstack([Y.ravel(), X.ravel()]).T
 xy = xy[land_mask]
-xy *= np.pi / 180.
+xy *= np.pi / 180.0

 # Plot map of South America with distributions of each species
 fig = plt.figure()
@@ -82,12 +112,13 @@

     # construct a kernel density estimate of the distribution
     print(" - computing KDE in spherical coordinates")
-    kde = KernelDensity(bandwidth=0.04, metric='haversine',
-                        kernel='gaussian', algorithm='ball_tree')
+    kde = KernelDensity(
+        bandwidth=0.04, metric="haversine", kernel="gaussian", algorithm="ball_tree"
+    )
     kde.fit(Xtrain[ytrain == i])

     # evaluate only on the land: -9999 indicates ocean
-    Z = np.full(land_mask.shape[0], -9999, dtype='int')
+    Z = np.full(land_mask.shape[0], -9999, dtype="int")
     Z[land_mask] = np.exp(kde.score_samples(xy))
     Z = Z.reshape(X.shape)

@@ -97,16 +128,21 @@

     if basemap:
         print(" - plot coastlines using basemap")
-        m = Basemap(projection='cyl', llcrnrlat=Y.min(),
-                    urcrnrlat=Y.max(), llcrnrlon=X.min(),
-                    urcrnrlon=X.max(), resolution='c')
+        m = Basemap(
+            projection="cyl",
+            llcrnrlat=Y.min(),
+            urcrnrlat=Y.max(),
+            llcrnrlon=X.min(),
+            urcrnrlon=X.max(),
+            resolution="c",
+        )
         m.drawcoastlines()
         m.drawcountries()
     else:
         print(" - plot coastlines from coverage")
-        plt.contour(X, Y, land_reference,
-                    levels=[-9998], colors="k",
-                    linestyles="solid")
+        plt.contour(
+            X, Y, land_reference, levels=[-9998], colors="k", linestyles="solid"
+        )
         plt.xticks([])
         plt.yticks([])

('examples/neighbors', 'plot_nca_classification.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -12,22 +12,20 @@
 Components Analysis. The latter aims to find a linear transformation that
 maximises the (stochastic) nearest neighbor classification accuracy on the
 training set.
+
 """

 # License: BSD 3 clause

-import numpy as np
 import matplotlib.pyplot as plt
 from matplotlib.colors import ListedColormap
 from sklearn import datasets
 from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import StandardScaler
-from sklearn.neighbors import (KNeighborsClassifier,
-                               NeighborhoodComponentsAnalysis)
+from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis
 from sklearn.pipeline import Pipeline
+from sklearn.inspection import DecisionBoundaryDisplay

-
-print(__doc__)

 n_neighbors = 1

@@ -38,51 +36,62 @@
 # slicing by using a two-dim dataset
 X = X[:, [0, 2]]

-X_train, X_test, y_train, y_test = \
-    train_test_split(X, y, stratify=y, test_size=0.7, random_state=42)
+X_train, X_test, y_train, y_test = train_test_split(
+    X, y, stratify=y, test_size=0.7, random_state=42
+)

-h = .01  # step size in the mesh
+h = 0.05  # step size in the mesh

 # Create color maps
-cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
-cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
+cmap_light = ListedColormap(["#FFAAAA", "#AAFFAA", "#AAAAFF"])
+cmap_bold = ListedColormap(["#FF0000", "#00FF00", "#0000FF"])

-names = ['KNN', 'NCA, KNN']
+names = ["KNN", "NCA, KNN"]

-classifiers = [Pipeline([('scaler', StandardScaler()),
-                         ('knn', KNeighborsClassifier(n_neighbors=n_neighbors))
-                         ]),
-               Pipeline([('scaler', StandardScaler()),
-                         ('nca', NeighborhoodComponentsAnalysis()),
-                         ('knn', KNeighborsClassifier(n_neighbors=n_neighbors))
-                         ])
-               ]
-
-x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
-y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
-xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
-                     np.arange(y_min, y_max, h))
+classifiers = [
+    Pipeline(
+        [
+            ("scaler", StandardScaler()),
+            ("knn", KNeighborsClassifier(n_neighbors=n_neighbors)),
+        ]
+    ),
+    Pipeline(
+        [
+            ("scaler", StandardScaler()),
+            ("nca", NeighborhoodComponentsAnalysis()),
+            ("knn", KNeighborsClassifier(n_neighbors=n_neighbors)),
+        ]
+    ),
+]

 for name, clf in zip(names, classifiers):

     clf.fit(X_train, y_train)
     score = clf.score(X_test, y_test)

-    # Plot the decision boundary. For that, we will assign a color to each
-    # point in the mesh [x_min, x_max]x[y_min, y_max].
-    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
-
-    # Put the result into a color plot
-    Z = Z.reshape(xx.shape)
-    plt.figure()
-    plt.pcolormesh(xx, yy, Z, cmap=cmap_light, alpha=.8)
+    _, ax = plt.subplots()
+    DecisionBoundaryDisplay.from_estimator(
+        clf,
+        X,
+        cmap=cmap_light,
+        alpha=0.8,
+        ax=ax,
+        response_method="predict",
+        plot_method="pcolormesh",
+        shading="auto",
+    )

     # Plot also the training and testing points
-    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)
-    plt.xlim(xx.min(), xx.max())
-    plt.ylim(yy.min(), yy.max())
+    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor="k", s=20)
     plt.title("{} (k = {})".format(name, n_neighbors))
-    plt.text(0.9, 0.1, '{:.2f}'.format(score), size=15,
-             ha='center', va='center', transform=plt.gca().transAxes)
+    plt.text(
+        0.9,
+        0.1,
+        "{:.2f}".format(score),
+        size=15,
+        ha="center",
+        va="center",
+        transform=plt.gca().transAxes,
+    )

 plt.show()
('examples/neighbors', 'plot_nearest_centroid.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,14 +5,15 @@

 Sample usage of Nearest Centroid classification.
 It will plot the decision boundaries for each class.
+
 """
-print(__doc__)

 import numpy as np
 import matplotlib.pyplot as plt
 from matplotlib.colors import ListedColormap
 from sklearn import datasets
 from sklearn.neighbors import NearestCentroid
+from sklearn.inspection import DecisionBoundaryDisplay

 n_neighbors = 15

@@ -23,36 +24,25 @@
 X = iris.data[:, :2]
 y = iris.target

-h = .02  # step size in the mesh
+# Create color maps
+cmap_light = ListedColormap(["orange", "cyan", "cornflowerblue"])
+cmap_bold = ListedColormap(["darkorange", "c", "darkblue"])

-# Create color maps
-cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
-cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
-
-for shrinkage in [None, .2]:
+for shrinkage in [None, 0.2]:
     # we create an instance of Neighbours Classifier and fit the data.
     clf = NearestCentroid(shrink_threshold=shrinkage)
     clf.fit(X, y)
     y_pred = clf.predict(X)
     print(shrinkage, np.mean(y == y_pred))
-    # Plot the decision boundary. For that, we will assign a color to each
-    # point in the mesh [x_min, x_max]x[y_min, y_max].
-    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
-    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
-    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
-                         np.arange(y_min, y_max, h))
-    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

-    # Put the result into a color plot
-    Z = Z.reshape(xx.shape)
-    plt.figure()
-    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)
+    _, ax = plt.subplots()
+    DecisionBoundaryDisplay.from_estimator(
+        clf, X, cmap=cmap_light, ax=ax, response_method="predict"
+    )

     # Plot also the training points
-    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,
-                edgecolor='k', s=20)
-    plt.title("3-Class classification (shrink_threshold=%r)"
-              % shrinkage)
-    plt.axis('tight')
+    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor="k", s=20)
+    plt.title("3-Class classification (shrink_threshold=%r)" % shrinkage)
+    plt.axis("tight")

 plt.show()
('examples/neighbors', 'plot_nca_dim_reduction.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -25,6 +25,7 @@

 One can see that NCA enforces a clustering of the data that is visually
 meaningful despite the large reduction in dimension.
+
 """

 # License: BSD 3 clause
@@ -35,46 +36,41 @@
 from sklearn.model_selection import train_test_split
 from sklearn.decomposition import PCA
 from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
-from sklearn.neighbors import (KNeighborsClassifier,
-                               NeighborhoodComponentsAnalysis)
+from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis
 from sklearn.pipeline import make_pipeline
 from sklearn.preprocessing import StandardScaler
-
-print(__doc__)

 n_neighbors = 3
 random_state = 0

 # Load Digits dataset
-digits = datasets.load_digits()
-X, y = digits.data, digits.target
+X, y = datasets.load_digits(return_X_y=True)

 # Split into train/test
-X_train, X_test, y_train, y_test = \
-    train_test_split(X, y, test_size=0.5, stratify=y,
-                     random_state=random_state)
+X_train, X_test, y_train, y_test = train_test_split(
+    X, y, test_size=0.5, stratify=y, random_state=random_state
+)

 dim = len(X[0])
 n_classes = len(np.unique(y))

 # Reduce dimension to 2 with PCA
-pca = make_pipeline(StandardScaler(),
-                    PCA(n_components=2, random_state=random_state))
+pca = make_pipeline(StandardScaler(), PCA(n_components=2, random_state=random_state))

 # Reduce dimension to 2 with LinearDiscriminantAnalysis
-lda = make_pipeline(StandardScaler(),
-                    LinearDiscriminantAnalysis(n_components=2))
+lda = make_pipeline(StandardScaler(), LinearDiscriminantAnalysis(n_components=2))

 # Reduce dimension to 2 with NeighborhoodComponentAnalysis
-nca = make_pipeline(StandardScaler(),
-                    NeighborhoodComponentsAnalysis(n_components=2,
-                                                   random_state=random_state))
+nca = make_pipeline(
+    StandardScaler(),
+    NeighborhoodComponentsAnalysis(n_components=2, random_state=random_state),
+)

 # Use a nearest neighbor classifier to evaluate the methods
 knn = KNeighborsClassifier(n_neighbors=n_neighbors)

 # Make a list of the methods to be compared
-dim_reduction_methods = [('PCA', pca), ('LDA', lda), ('NCA', nca)]
+dim_reduction_methods = [("PCA", pca), ("LDA", lda), ("NCA", nca)]

 # plt.figure()
 for i, (name, model) in enumerate(dim_reduction_methods):
@@ -94,8 +90,8 @@
     X_embedded = model.transform(X)

     # Plot the projected points and show the evaluation score
-    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, s=30, cmap='Set1')
-    plt.title("{}, KNN (k={})\nTest accuracy = {:.2f}".format(name,
-                                                              n_neighbors,
-                                                              acc_knn))
+    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, s=30, cmap="Set1")
+    plt.title(
+        "{}, KNN (k={})\nTest accuracy = {:.2f}".format(name, n_neighbors, acc_knn)
+    )
 plt.show()
('examples/neighbors', 'plot_lof_outlier_detection.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -19,15 +19,14 @@
 so that other samples can be local outliers relative to this cluster, and 2)
 smaller than the maximum number of close by samples that can potentially be
 local outliers.
-In practice, such informations are generally not available, and taking
+In practice, such information is generally not available, and taking
 n_neighbors=20 appears to work well in general.
+
 """

 import numpy as np
 import matplotlib.pyplot as plt
 from sklearn.neighbors import LocalOutlierFactor
-
-print(__doc__)

 np.random.seed(42)

@@ -53,16 +52,22 @@
 X_scores = clf.negative_outlier_factor_

 plt.title("Local Outlier Factor (LOF)")
-plt.scatter(X[:, 0], X[:, 1], color='k', s=3., label='Data points')
+plt.scatter(X[:, 0], X[:, 1], color="k", s=3.0, label="Data points")
 # plot circles with radius proportional to the outlier scores
 radius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())
-plt.scatter(X[:, 0], X[:, 1], s=1000 * radius, edgecolors='r',
-            facecolors='none', label='Outlier scores')
-plt.axis('tight')
+plt.scatter(
+    X[:, 0],
+    X[:, 1],
+    s=1000 * radius,
+    edgecolors="r",
+    facecolors="none",
+    label="Outlier scores",
+)
+plt.axis("tight")
 plt.xlim((-5, 5))
 plt.ylim((-5, 5))
 plt.xlabel("prediction errors: %d" % (n_errors))
-legend = plt.legend(loc='upper left')
+legend = plt.legend(loc="upper left")
 legend.legendHandles[0]._sizes = [10]
 legend.legendHandles[1]._sizes = [20]
 plt.show()
('examples/neighbors', 'plot_nca_illustration.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,10 +3,11 @@
 Neighborhood Components Analysis Illustration
 =============================================

-An example illustrating the goal of learning a distance metric that maximizes
-the nearest neighbors classification accuracy. The example is solely for
-illustration purposes. Please refer to the :ref:`User Guide <nca>` for
-more information.
+This example illustrates a learned distance metric that maximizes
+the nearest neighbors classification accuracy. It provides a visual
+representation of this metric compared to the original point
+space. Please refer to the :ref:`User Guide <nca>` for more information.
+
 """

 # License: BSD 3 clause
@@ -16,83 +17,84 @@
 from sklearn.datasets import make_classification
 from sklearn.neighbors import NeighborhoodComponentsAnalysis
 from matplotlib import cm
-from sklearn.utils.fixes import logsumexp
+from scipy.special import logsumexp

-print(__doc__)
+# %%
+# Original points
+# ---------------
+# First we create a data set of 9 samples from 3 classes, and plot the points
+# in the original space. For this example, we focus on the classification of
+# point no. 3. The thickness of a link between point no. 3 and another point
+# is proportional to their distance.

-n_neighbors = 1
-random_state = 0
+X, y = make_classification(
+    n_samples=9,
+    n_features=2,
+    n_informative=2,
+    n_redundant=0,
+    n_classes=3,
+    n_clusters_per_class=1,
+    class_sep=1.0,
+    random_state=0,
+)

-# Create a tiny data set of 9 samples from 3 classes
-X, y = make_classification(n_samples=9, n_features=2, n_informative=2,
-                           n_redundant=0, n_classes=3, n_clusters_per_class=1,
-                           class_sep=1.0, random_state=random_state)
+plt.figure(1)
+ax = plt.gca()
+for i in range(X.shape[0]):
+    ax.text(X[i, 0], X[i, 1], str(i), va="center", ha="center")
+    ax.scatter(X[i, 0], X[i, 1], s=300, c=cm.Set1(y[[i]]), alpha=0.4)

-# Plot the points in the original space
-plt.figure()
-ax = plt.gca()
-
-# Draw the graph nodes
-for i in range(X.shape[0]):
-    ax.text(X[i, 0], X[i, 1], str(i), va='center', ha='center')
-    ax.scatter(X[i, 0], X[i, 1], s=300, c=cm.Set1(y[i]), alpha=0.4)
+ax.set_title("Original points")
+ax.axes.get_xaxis().set_visible(False)
+ax.axes.get_yaxis().set_visible(False)
+ax.axis("equal")  # so that boundaries are displayed correctly as circles


-def p_i(X, i):
+def link_thickness_i(X, i):
     diff_embedded = X[i] - X
-    dist_embedded = np.einsum('ij,ij->i', diff_embedded,
-                              diff_embedded)
+    dist_embedded = np.einsum("ij,ij->i", diff_embedded, diff_embedded)
     dist_embedded[i] = np.inf

     # compute exponentiated distances (use the log-sum-exp trick to
     # avoid numerical instabilities
-    exp_dist_embedded = np.exp(-dist_embedded -
-                               logsumexp(-dist_embedded))
+    exp_dist_embedded = np.exp(-dist_embedded - logsumexp(-dist_embedded))
     return exp_dist_embedded


 def relate_point(X, i, ax):
     pt_i = X[i]
     for j, pt_j in enumerate(X):
-        thickness = p_i(X, i)
+        thickness = link_thickness_i(X, i)
         if i != j:
             line = ([pt_i[0], pt_j[0]], [pt_i[1], pt_j[1]])
-            ax.plot(*line, c=cm.Set1(y[j]),
-                    linewidth=5*thickness[j])
+            ax.plot(*line, c=cm.Set1(y[j]), linewidth=5 * thickness[j])


-# we consider only point 3
 i = 3
+relate_point(X, i, ax)
+plt.show()

-# Plot bonds linked to sample i in the original space
-relate_point(X, i, ax)
-ax.set_title("Original points")
-ax.axes.get_xaxis().set_visible(False)
-ax.axes.get_yaxis().set_visible(False)
-ax.axis('equal')
+# %%
+# Learning an embedding
+# ---------------------
+# We use :class:`~sklearn.neighbors.NeighborhoodComponentsAnalysis` to learn an
+# embedding and plot the points after the transformation. We then take the
+# embedding and find the nearest neighbors.

-# Learn an embedding with NeighborhoodComponentsAnalysis
-nca = NeighborhoodComponentsAnalysis(max_iter=30, random_state=random_state)
+nca = NeighborhoodComponentsAnalysis(max_iter=30, random_state=0)
 nca = nca.fit(X, y)

-# Plot the points after transformation with NeighborhoodComponentsAnalysis
-plt.figure()
+plt.figure(2)
 ax2 = plt.gca()
-
-# Get the embedding and find the new nearest neighbors
 X_embedded = nca.transform(X)
-
 relate_point(X_embedded, i, ax2)

 for i in range(len(X)):
-    ax2.text(X_embedded[i, 0], X_embedded[i, 1], str(i),
-             va='center', ha='center')
-    ax2.scatter(X_embedded[i, 0], X_embedded[i, 1], s=300, c=cm.Set1(y[i]),
-                alpha=0.4)
+    ax2.text(X_embedded[i, 0], X_embedded[i, 1], str(i), va="center", ha="center")
+    ax2.scatter(X_embedded[i, 0], X_embedded[i, 1], s=300, c=cm.Set1(y[[i]]), alpha=0.4)

-# Make axes equal so that boundaries are displayed correctly as circles
 ax2.set_title("NCA embedding")
 ax2.axes.get_xaxis().set_visible(False)
 ax2.axes.get_yaxis().set_visible(False)
-ax2.axis('equal')
+ax2.axis("equal")
 plt.show()
('examples/neighbors', 'plot_lof_novelty_detection.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -20,16 +20,15 @@
 so that other samples can be local outliers relative to this cluster, and 2)
 smaller than the maximum number of close by samples that can potentially be
 local outliers.
-In practice, such informations are generally not available, and taking
+In practice, such information is generally not available, and taking
 n_neighbors=20 appears to work well in general.
+
 """

 import numpy as np
 import matplotlib
 import matplotlib.pyplot as plt
 from sklearn.neighbors import LocalOutlierFactor
-
-print(__doc__)

 np.random.seed(42)

@@ -60,24 +59,29 @@

 plt.title("Novelty Detection with LOF")
 plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)
-a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')
-plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')
+a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors="darkred")
+plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors="palevioletred")

 s = 40
-b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s, edgecolors='k')
-b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='blueviolet', s=s,
-                 edgecolors='k')
-c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='gold', s=s,
-                edgecolors='k')
-plt.axis('tight')
+b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c="white", s=s, edgecolors="k")
+b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c="blueviolet", s=s, edgecolors="k")
+c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c="gold", s=s, edgecolors="k")
+plt.axis("tight")
 plt.xlim((-5, 5))
 plt.ylim((-5, 5))
-plt.legend([a.collections[0], b1, b2, c],
-           ["learned frontier", "training observations",
-            "new regular observations", "new abnormal observations"],
-           loc="upper left",
-           prop=matplotlib.font_manager.FontProperties(size=11))
+plt.legend(
+    [a.collections[0], b1, b2, c],
+    [
+        "learned frontier",
+        "training observations",
+        "new regular observations",
+        "new abnormal observations",
+    ],
+    loc="upper left",
+    prop=matplotlib.font_manager.FontProperties(size=11),
+)
 plt.xlabel(
     "errors novel regular: %d/40 ; errors novel abnormal: %d/40"
-    % (n_error_test, n_error_outliers))
+    % (n_error_test, n_error_outliers)
+)
 plt.show()
('examples/neighbors', 'plot_regression.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,7 +8,6 @@
 target using both barycenter and constant weights.

 """
-print(__doc__)

 # Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>
 #         Fabian Pedregosa <fabian.pedregosa@inria.fr>
@@ -16,8 +15,9 @@
 # License: BSD 3 clause (C) INRIA


-# #############################################################################
+# %%
 # Generate sample data
+# --------------------
 import numpy as np
 import matplotlib.pyplot as plt
 from sklearn import neighbors
@@ -30,21 +30,21 @@
 # Add noise to targets
 y[::5] += 1 * (0.5 - np.random.rand(8))

-# #############################################################################
+# %%
 # Fit regression model
+# --------------------
 n_neighbors = 5

-for i, weights in enumerate(['uniform', 'distance']):
+for i, weights in enumerate(["uniform", "distance"]):
     knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)
     y_ = knn.fit(X, y).predict(T)

     plt.subplot(2, 1, i + 1)
-    plt.scatter(X, y, c='k', label='data')
-    plt.plot(T, y_, c='g', label='prediction')
-    plt.axis('tight')
+    plt.scatter(X, y, color="darkorange", label="data")
+    plt.plot(T, y_, color="navy", label="prediction")
+    plt.axis("tight")
     plt.legend()
-    plt.title("KNeighborsRegressor (k = %i, weights = '%s')" % (n_neighbors,
-                                                                weights))
+    plt.title("KNeighborsRegressor (k = %i, weights = '%s')" % (n_neighbors, weights))

 plt.tight_layout()
 plt.show()
('examples/neighbors', 'plot_classification.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -5,13 +5,14 @@

 Sample usage of Nearest Neighbors classification.
 It will plot the decision boundaries for each class.
+
 """
-print(__doc__)

-import numpy as np
 import matplotlib.pyplot as plt
+import seaborn as sns
 from matplotlib.colors import ListedColormap
 from sklearn import neighbors, datasets
+from sklearn.inspection import DecisionBoundaryDisplay

 n_neighbors = 15

@@ -23,36 +24,39 @@
 X = iris.data[:, :2]
 y = iris.target

-h = .02  # step size in the mesh
+# Create color maps
+cmap_light = ListedColormap(["orange", "cyan", "cornflowerblue"])
+cmap_bold = ["darkorange", "c", "darkblue"]

-# Create color maps
-cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
-cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])
-
-for weights in ['uniform', 'distance']:
+for weights in ["uniform", "distance"]:
     # we create an instance of Neighbours Classifier and fit the data.
     clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)
     clf.fit(X, y)

-    # Plot the decision boundary. For that, we will assign a color to each
-    # point in the mesh [x_min, x_max]x[y_min, y_max].
-    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
-    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
-    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
-                         np.arange(y_min, y_max, h))
-    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
-
-    # Put the result into a color plot
-    Z = Z.reshape(xx.shape)
-    plt.figure()
-    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)
+    _, ax = plt.subplots()
+    DecisionBoundaryDisplay.from_estimator(
+        clf,
+        X,
+        cmap=cmap_light,
+        ax=ax,
+        response_method="predict",
+        plot_method="pcolormesh",
+        xlabel=iris.feature_names[0],
+        ylabel=iris.feature_names[1],
+        shading="auto",
+    )

     # Plot also the training points
-    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,
-                edgecolor='k', s=20)
-    plt.xlim(xx.min(), xx.max())
-    plt.ylim(yy.min(), yy.max())
-    plt.title("3-Class classification (k = %i, weights = '%s')"
-              % (n_neighbors, weights))
+    sns.scatterplot(
+        x=X[:, 0],
+        y=X[:, 1],
+        hue=iris.target_names[y],
+        palette=cmap_bold,
+        alpha=1.0,
+        edgecolor="black",
+    )
+    plt.title(
+        "3-Class classification (k = %i, weights = '%s')" % (n_neighbors, weights)
+    )

 plt.show()
('examples/neighbors', 'plot_kde_1d.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -2,7 +2,7 @@
 ===================================
 Simple 1D Kernel Density Estimation
 ===================================
-This example uses the :class:`sklearn.neighbors.KernelDensity` class to
+This example uses the :class:`~sklearn.neighbors.KernelDensity` class to
 demonstrate the principles of Kernel Density Estimation in one dimension.

 The first plot shows one of the problems with using histograms to visualize
@@ -18,35 +18,30 @@

 Scikit-learn implements efficient kernel density estimation using either
 a Ball Tree or KD Tree structure, through the
-:class:`sklearn.neighbors.KernelDensity` estimator.  The available kernels
+:class:`~sklearn.neighbors.KernelDensity` estimator.  The available kernels
 are shown in the second figure of this example.

 The third figure compares kernel density estimates for a distribution of 100
 samples in 1 dimension.  Though this example uses 1D distributions, kernel
 density estimation is easily and efficiently extensible to higher dimensions
 as well.
+
 """
+
 # Author: Jake Vanderplas <jakevdp@cs.washington.edu>
 #
 import numpy as np
-import matplotlib
 import matplotlib.pyplot as plt
-from distutils.version import LooseVersion
 from scipy.stats import norm
 from sklearn.neighbors import KernelDensity

-# `normed` is being deprecated in favor of `density` in histograms
-if LooseVersion(matplotlib.__version__) >= '2.1':
-    density_param = {'density': True}
-else:
-    density_param = {'normed': True}
-
-#----------------------------------------------------------------------
+# ----------------------------------------------------------------------
 # Plot the progression of histograms to kernels
 np.random.seed(1)
 N = 20
-X = np.concatenate((np.random.normal(0, 1, int(0.3 * N)),
-                    np.random.normal(5, 1, int(0.7 * N))))[:, np.newaxis]
+X = np.concatenate(
+    (np.random.normal(0, 1, int(0.3 * N)), np.random.normal(5, 1, int(0.7 * N)))
+)[:, np.newaxis]
 X_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]
 bins = np.linspace(-5, 10, 10)

@@ -54,37 +49,37 @@
 fig.subplots_adjust(hspace=0.05, wspace=0.05)

 # histogram 1
-ax[0, 0].hist(X[:, 0], bins=bins, fc='#AAAAFF', **density_param)
+ax[0, 0].hist(X[:, 0], bins=bins, fc="#AAAAFF", density=True)
 ax[0, 0].text(-3.5, 0.31, "Histogram")

 # histogram 2
-ax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc='#AAAAFF', **density_param)
+ax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc="#AAAAFF", density=True)
 ax[0, 1].text(-3.5, 0.31, "Histogram, bins shifted")

 # tophat KDE
-kde = KernelDensity(kernel='tophat', bandwidth=0.75).fit(X)
+kde = KernelDensity(kernel="tophat", bandwidth=0.75).fit(X)
 log_dens = kde.score_samples(X_plot)
-ax[1, 0].fill(X_plot[:, 0], np.exp(log_dens), fc='#AAAAFF')
+ax[1, 0].fill(X_plot[:, 0], np.exp(log_dens), fc="#AAAAFF")
 ax[1, 0].text(-3.5, 0.31, "Tophat Kernel Density")

 # Gaussian KDE
-kde = KernelDensity(kernel='gaussian', bandwidth=0.75).fit(X)
+kde = KernelDensity(kernel="gaussian", bandwidth=0.75).fit(X)
 log_dens = kde.score_samples(X_plot)
-ax[1, 1].fill(X_plot[:, 0], np.exp(log_dens), fc='#AAAAFF')
+ax[1, 1].fill(X_plot[:, 0], np.exp(log_dens), fc="#AAAAFF")
 ax[1, 1].text(-3.5, 0.31, "Gaussian Kernel Density")

 for axi in ax.ravel():
-    axi.plot(X[:, 0], np.full(X.shape[0], -0.01), '+k')
+    axi.plot(X[:, 0], np.full(X.shape[0], -0.01), "+k")
     axi.set_xlim(-4, 9)
     axi.set_ylim(-0.02, 0.34)

 for axi in ax[:, 0]:
-    axi.set_ylabel('Normalized Density')
+    axi.set_ylabel("Normalized Density")

 for axi in ax[1, :]:
-    axi.set_xlabel('x')
+    axi.set_xlabel("x")

-#----------------------------------------------------------------------
+# ----------------------------------------------------------------------
 # Plot all available kernels
 X_plot = np.linspace(-6, 6, 1000)[:, None]
 X_src = np.zeros((1, 1))
@@ -95,19 +90,21 @@

 def format_func(x, loc):
     if x == 0:
-        return '0'
+        return "0"
     elif x == 1:
-        return 'h'
+        return "h"
     elif x == -1:
-        return '-h'
+        return "-h"
     else:
-        return '%ih' % x
+        return "%ih" % x

-for i, kernel in enumerate(['gaussian', 'tophat', 'epanechnikov',
-                            'exponential', 'linear', 'cosine']):
+
+for i, kernel in enumerate(
+    ["gaussian", "tophat", "epanechnikov", "exponential", "linear", "cosine"]
+):
     axi = ax.ravel()[i]
     log_dens = KernelDensity(kernel=kernel).fit(X_src).score_samples(X_plot)
-    axi.fill(X_plot[:, 0], np.exp(log_dens), '-k', fc='#AAAAFF')
+    axi.fill(X_plot[:, 0], np.exp(log_dens), "-k", fc="#AAAAFF")
     axi.text(-2.6, 0.95, kernel)

     axi.xaxis.set_major_formatter(plt.FuncFormatter(format_func))
@@ -117,34 +114,42 @@
     axi.set_ylim(0, 1.05)
     axi.set_xlim(-2.9, 2.9)

-ax[0, 1].set_title('Available Kernels')
+ax[0, 1].set_title("Available Kernels")

-#----------------------------------------------------------------------
+# ----------------------------------------------------------------------
 # Plot a 1D density example
 N = 100
 np.random.seed(1)
-X = np.concatenate((np.random.normal(0, 1, int(0.3 * N)),
-                    np.random.normal(5, 1, int(0.7 * N))))[:, np.newaxis]
+X = np.concatenate(
+    (np.random.normal(0, 1, int(0.3 * N)), np.random.normal(5, 1, int(0.7 * N)))
+)[:, np.newaxis]

 X_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]

-true_dens = (0.3 * norm(0, 1).pdf(X_plot[:, 0])
-             + 0.7 * norm(5, 1).pdf(X_plot[:, 0]))
+true_dens = 0.3 * norm(0, 1).pdf(X_plot[:, 0]) + 0.7 * norm(5, 1).pdf(X_plot[:, 0])

 fig, ax = plt.subplots()
-ax.fill(X_plot[:, 0], true_dens, fc='black', alpha=0.2,
-        label='input distribution')
+ax.fill(X_plot[:, 0], true_dens, fc="black", alpha=0.2, label="input distribution")
+colors = ["navy", "cornflowerblue", "darkorange"]
+kernels = ["gaussian", "tophat", "epanechnikov"]
+lw = 2

-for kernel in ['gaussian', 'tophat', 'epanechnikov']:
+for color, kernel in zip(colors, kernels):
     kde = KernelDensity(kernel=kernel, bandwidth=0.5).fit(X)
     log_dens = kde.score_samples(X_plot)
-    ax.plot(X_plot[:, 0], np.exp(log_dens), '-',
-            label="kernel = '{0}'".format(kernel))
+    ax.plot(
+        X_plot[:, 0],
+        np.exp(log_dens),
+        color=color,
+        lw=lw,
+        linestyle="-",
+        label="kernel = '{0}'".format(kernel),
+    )

 ax.text(6, 0.38, "N={0} points".format(N))

-ax.legend(loc='upper left')
-ax.plot(X[:, 0], -0.005 - 0.01 * np.random.random(X.shape[0]), '+k')
+ax.legend(loc="upper left")
+ax.plot(X[:, 0], -0.005 - 0.01 * np.random.random(X.shape[0]), "+k")

 ax.set_xlim(-4, 9)
 ax.set_ylim(-0.02, 0.4)
('examples/neighbors', 'plot_digits_kde_sampling.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,6 +8,7 @@
 a generative model for a dataset.  With this generative model in place,
 new samples can be drawn.  These new samples reflect the underlying model
 of the data.
+
 """

 import numpy as np
@@ -26,8 +27,8 @@
 data = pca.fit_transform(digits.data)

 # use grid search cross-validation to optimize the bandwidth
-params = {'bandwidth': np.logspace(-1, 1, 20)}
-grid = GridSearchCV(KernelDensity(), params, cv=5, iid=False)
+params = {"bandwidth": np.logspace(-1, 1, 20)}
+grid = GridSearchCV(KernelDensity(), params)
 grid.fit(data)

 print("best bandwidth: {0}".format(grid.best_estimator_.bandwidth))
@@ -48,14 +49,16 @@
 for j in range(11):
     ax[4, j].set_visible(False)
     for i in range(4):
-        im = ax[i, j].imshow(real_data[i, j].reshape((8, 8)),
-                             cmap=plt.cm.binary, interpolation='nearest')
+        im = ax[i, j].imshow(
+            real_data[i, j].reshape((8, 8)), cmap=plt.cm.binary, interpolation="nearest"
+        )
         im.set_clim(0, 16)
-        im = ax[i + 5, j].imshow(new_data[i, j].reshape((8, 8)),
-                                 cmap=plt.cm.binary, interpolation='nearest')
+        im = ax[i + 5, j].imshow(
+            new_data[i, j].reshape((8, 8)), cmap=plt.cm.binary, interpolation="nearest"
+        )
         im.set_clim(0, 16)

-ax[0, 5].set_title('Selection from the input data')
+ax[0, 5].set_title("Selection from the input data")
 ax[5, 5].set_title('"New" digits drawn from the kernel density model')

 plt.show()
('benchmarks', 'bench_mnist.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -6,7 +6,7 @@
 Benchmark on the MNIST dataset.  The dataset comprises 70,000 samples
 and 784 features. Here, we consider the task of predicting
 10 classes -  digits from 0 to 9 from their raw images. By contrast to the
-covertype dataset, the feature space is homogenous.
+covertype dataset, the feature space is homogeneous.

 Example of output :
     [..]
@@ -36,7 +36,7 @@
 import numpy as np
 from joblib import Memory

-from sklearn.datasets import fetch_mldata
+from sklearn.datasets import fetch_openml
 from sklearn.datasets import get_data_home
 from sklearn.ensemble import ExtraTreesClassifier
 from sklearn.ensemble import RandomForestClassifier
@@ -53,18 +53,17 @@

 # Memoize the data extraction and memory map the resulting
 # train / test splits in readonly mode
-memory = Memory(os.path.join(get_data_home(), 'mnist_benchmark_data'),
-                mmap_mode='r')
+memory = Memory(os.path.join(get_data_home(), "mnist_benchmark_data"), mmap_mode="r")


 @memory.cache
-def load_data(dtype=np.float32, order='F'):
+def load_data(dtype=np.float32, order="F"):
     """Load the data, then cache and memmap the train/test split"""
     ######################################################################
     # Load dataset
     print("Loading dataset...")
-    data = fetch_mldata('MNIST original')
-    X = check_array(data['data'], dtype=dtype, order=order)
+    data = fetch_openml("mnist_784")
+    X = check_array(data["data"], dtype=dtype, order=order)
     y = data["target"]

     # Normalize features
@@ -83,43 +82,76 @@

 ESTIMATORS = {
     "dummy": DummyClassifier(),
-    'CART': DecisionTreeClassifier(),
-    'ExtraTrees': ExtraTreesClassifier(n_estimators=100),
-    'RandomForest': RandomForestClassifier(n_estimators=100),
-    'Nystroem-SVM': make_pipeline(
-        Nystroem(gamma=0.015, n_components=1000), LinearSVC(C=100)),
-    'SampledRBF-SVM': make_pipeline(
-        RBFSampler(gamma=0.015, n_components=1000), LinearSVC(C=100)),
-    'LogisticRegression-SAG': LogisticRegression(solver='sag', tol=1e-1,
-                                                 C=1e4),
-    'LogisticRegression-SAGA': LogisticRegression(solver='saga', tol=1e-1,
-                                                  C=1e4),
-    'MultilayerPerceptron': MLPClassifier(
-        hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,
-        solver='sgd', learning_rate_init=0.2, momentum=0.9, verbose=1,
-        tol=1e-4, random_state=1),
-    'MLP-adam': MLPClassifier(
-        hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,
-        solver='adam', learning_rate_init=0.001, verbose=1,
-        tol=1e-4, random_state=1)
+    "CART": DecisionTreeClassifier(),
+    "ExtraTrees": ExtraTreesClassifier(),
+    "RandomForest": RandomForestClassifier(),
+    "Nystroem-SVM": make_pipeline(
+        Nystroem(gamma=0.015, n_components=1000), LinearSVC(C=100)
+    ),
+    "SampledRBF-SVM": make_pipeline(
+        RBFSampler(gamma=0.015, n_components=1000), LinearSVC(C=100)
+    ),
+    "LogisticRegression-SAG": LogisticRegression(solver="sag", tol=1e-1, C=1e4),
+    "LogisticRegression-SAGA": LogisticRegression(solver="saga", tol=1e-1, C=1e4),
+    "MultilayerPerceptron": MLPClassifier(
+        hidden_layer_sizes=(100, 100),
+        max_iter=400,
+        alpha=1e-4,
+        solver="sgd",
+        learning_rate_init=0.2,
+        momentum=0.9,
+        verbose=1,
+        tol=1e-4,
+        random_state=1,
+    ),
+    "MLP-adam": MLPClassifier(
+        hidden_layer_sizes=(100, 100),
+        max_iter=400,
+        alpha=1e-4,
+        solver="adam",
+        learning_rate_init=0.001,
+        verbose=1,
+        tol=1e-4,
+        random_state=1,
+    ),
 }


 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
-    parser.add_argument('--classifiers', nargs="+",
-                        choices=ESTIMATORS, type=str,
-                        default=['ExtraTrees', 'Nystroem-SVM'],
-                        help="list of classifiers to benchmark.")
-    parser.add_argument('--n-jobs', nargs="?", default=1, type=int,
-                        help="Number of concurrently running workers for "
-                             "models that support parallelism.")
-    parser.add_argument('--order', nargs="?", default="C", type=str,
-                        choices=["F", "C"],
-                        help="Allow to choose between fortran and C ordered "
-                             "data")
-    parser.add_argument('--random-seed', nargs="?", default=0, type=int,
-                        help="Common seed used by random number generator.")
+    parser.add_argument(
+        "--classifiers",
+        nargs="+",
+        choices=ESTIMATORS,
+        type=str,
+        default=["ExtraTrees", "Nystroem-SVM"],
+        help="list of classifiers to benchmark.",
+    )
+    parser.add_argument(
+        "--n-jobs",
+        nargs="?",
+        default=1,
+        type=int,
+        help=(
+            "Number of concurrently running workers for "
+            "models that support parallelism."
+        ),
+    )
+    parser.add_argument(
+        "--order",
+        nargs="?",
+        default="C",
+        type=str,
+        choices=["F", "C"],
+        help="Allow to choose between fortran and C ordered data",
+    )
+    parser.add_argument(
+        "--random-seed",
+        nargs="?",
+        default=0,
+        type=int,
+        help="Common seed used by random number generator.",
+    )
     args = vars(parser.parse_args())

     print(__doc__)
@@ -132,10 +164,22 @@
     print("%s %d" % ("number of features:".ljust(25), X_train.shape[1]))
     print("%s %d" % ("number of classes:".ljust(25), np.unique(y_train).size))
     print("%s %s" % ("data type:".ljust(25), X_train.dtype))
-    print("%s %d (size=%dMB)" % ("number of train samples:".ljust(25),
-                                 X_train.shape[0], int(X_train.nbytes / 1e6)))
-    print("%s %d (size=%dMB)" % ("number of test samples:".ljust(25),
-                                 X_test.shape[0], int(X_test.nbytes / 1e6)))
+    print(
+        "%s %d (size=%dMB)"
+        % (
+            "number of train samples:".ljust(25),
+            X_train.shape[0],
+            int(X_train.nbytes / 1e6),
+        )
+    )
+    print(
+        "%s %d (size=%dMB)"
+        % (
+            "number of test samples:".ljust(25),
+            X_test.shape[0],
+            int(X_test.nbytes / 1e6),
+        )
+    )

     print()
     print("Training Classifiers")
@@ -146,9 +190,13 @@
         estimator = ESTIMATORS[name]
         estimator_params = estimator.get_params()

-        estimator.set_params(**{p: args["random_seed"]
-                                for p in estimator_params
-                                if p.endswith("random_state")})
+        estimator.set_params(
+            **{
+                p: args["random_seed"]
+                for p in estimator_params
+                if p.endswith("random_state")
+            }
+        )

         if "n_jobs" in estimator_params:
             estimator.set_params(n_jobs=args["n_jobs"])
@@ -168,12 +216,18 @@
     print()
     print("Classification performance:")
     print("===========================")
-    print("{0: <24} {1: >10} {2: >11} {3: >12}"
-          "".format("Classifier  ", "train-time", "test-time", "error-rate"))
+    print(
+        "{0: <24} {1: >10} {2: >11} {3: >12}".format(
+            "Classifier  ", "train-time", "test-time", "error-rate"
+        )
+    )
     print("-" * 60)
     for name in sorted(args["classifiers"], key=error.get):

-        print("{0: <23} {1: >10.2f}s {2: >10.2f}s {3: >12.4f}"
-              "".format(name, train_time[name], test_time[name], error[name]))
+        print(
+            "{0: <23} {1: >10.2f}s {2: >10.2f}s {3: >12.4f}".format(
+                name, train_time[name], test_time[name], error[name]
+            )
+        )

     print()
('benchmarks', 'plot_tsne_mnist.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -9,15 +9,19 @@


 if __name__ == "__main__":
-    parser = argparse.ArgumentParser('Plot benchmark results for t-SNE')
+    parser = argparse.ArgumentParser("Plot benchmark results for t-SNE")
     parser.add_argument(
-        '--labels', type=str,
-        default=op.join(LOG_DIR, 'mnist_original_labels_10000.npy'),
-        help='1D integer numpy array for labels')
+        "--labels",
+        type=str,
+        default=op.join(LOG_DIR, "mnist_original_labels_10000.npy"),
+        help="1D integer numpy array for labels",
+    )
     parser.add_argument(
-        '--embedding', type=str,
-        default=op.join(LOG_DIR, 'mnist_sklearn_TSNE_10000.npy'),
-        help='2D float numpy array for embedded data')
+        "--embedding",
+        type=str,
+        default=op.join(LOG_DIR, "mnist_sklearn_TSNE_10000.npy"),
+        help="2D float numpy array for embedded data",
+    )
     args = parser.parse_args()

     X = np.load(args.embedding)
@@ -26,5 +30,5 @@
     for i in np.unique(y):
         mask = y == i
         plt.scatter(X[mask, 0], X[mask, 1], alpha=0.2, label=int(i))
-    plt.legend(loc='best')
+    plt.legend(loc="best")
     plt.show()
('benchmarks', 'bench_rcv1_logreg_convergence.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -9,25 +9,25 @@
 import gc
 import time

-from sklearn.linear_model import (LogisticRegression, SGDClassifier)
+from sklearn.linear_model import LogisticRegression, SGDClassifier
 from sklearn.datasets import fetch_rcv1
-from sklearn.linear_model.sag import get_auto_step_size
+from sklearn.linear_model._sag import get_auto_step_size

 try:
     import lightning.classification as lightning_clf
 except ImportError:
     lightning_clf = None

-m = Memory(cachedir='.', verbose=0)
+m = Memory(cachedir=".", verbose=0)


 # compute logistic loss
 def get_loss(w, intercept, myX, myy, C):
     n_samples = myX.shape[0]
     w = w.ravel()
-    p = np.mean(np.log(1. + np.exp(-myy * (myX.dot(w) + intercept))))
-    print("%f + %f" % (p, w.dot(w) / 2. / C / n_samples))
-    p += w.dot(w) / 2. / C / n_samples
+    p = np.mean(np.log(1.0 + np.exp(-myy * (myX.dot(w) + intercept))))
+    print("%f + %f" % (p, w.dot(w) / 2.0 / C / n_samples))
+    p += w.dot(w) / 2.0 / C / n_samples
     return p


@@ -39,7 +39,7 @@
     clf = clf_type(**clf_params)
     try:
         clf.set_params(max_iter=n_iter, random_state=42)
-    except:
+    except Exception:
         clf.set_params(n_iter=n_iter, random_state=42)

     st = time.time()
@@ -48,13 +48,13 @@

     try:
         C = 1.0 / clf.alpha / n_samples
-    except:
+    except Exception:
         C = clf.C

     try:
         intercept = clf.intercept_
-    except:
-        intercept = 0.
+    except Exception:
+        intercept = 0.0

     train_loss = get_loss(clf.coef_, intercept, X, y, C)
     train_score = clf.score(X, y)
@@ -65,8 +65,15 @@


 def bench(clfs):
-    for (name, clf, iter_range, train_losses, train_scores,
-         test_scores, durations) in clfs:
+    for (
+        name,
+        clf,
+        iter_range,
+        train_losses,
+        train_scores,
+        test_scores,
+        durations,
+    ) in clfs:
         print("training %s" % name)
         clf_type = type(clf)
         clf_params = clf.get_params()
@@ -75,7 +82,8 @@
             gc.collect()

             train_loss, train_score, test_score, duration = bench_one(
-                name, clf_type, clf_params, n_iter)
+                name, clf_type, clf_params, n_iter
+            )

             train_losses.append(train_loss)
             train_scores.append(train_score)
@@ -94,8 +102,8 @@

 def plot_train_losses(clfs):
     plt.figure()
-    for (name, _, _, train_losses, _, _, durations) in clfs:
-        plt.plot(durations, train_losses, '-o', label=name)
+    for name, _, _, train_losses, _, _, durations in clfs:
+        plt.plot(durations, train_losses, "-o", label=name)
         plt.legend(loc=0)
         plt.xlabel("seconds")
         plt.ylabel("train loss")
@@ -103,8 +111,8 @@

 def plot_train_scores(clfs):
     plt.figure()
-    for (name, _, _, _, train_scores, _, durations) in clfs:
-        plt.plot(durations, train_scores, '-o', label=name)
+    for name, _, _, _, train_scores, _, durations in clfs:
+        plt.plot(durations, train_scores, "-o", label=name)
         plt.legend(loc=0)
         plt.xlabel("seconds")
         plt.ylabel("train score")
@@ -113,8 +121,8 @@

 def plot_test_scores(clfs):
     plt.figure()
-    for (name, _, _, _, _, test_scores, durations) in clfs:
-        plt.plot(durations, test_scores, '-o', label=name)
+    for name, _, _, _, _, test_scores, durations in clfs:
+        plt.plot(durations, test_scores, "-o", label=name)
         plt.legend(loc=0)
         plt.xlabel("seconds")
         plt.ylabel("test score")
@@ -124,16 +132,16 @@
 def plot_dloss(clfs):
     plt.figure()
     pobj_final = []
-    for (name, _, _, train_losses, _, _, durations) in clfs:
+    for name, _, _, train_losses, _, _, durations in clfs:
         pobj_final.append(train_losses[-1])

     indices = np.argsort(pobj_final)
     pobj_best = pobj_final[indices[0]]

-    for (name, _, _, train_losses, _, _, durations) in clfs:
+    for name, _, _, train_losses, _, _, durations in clfs:
         log_pobj = np.log(abs(np.array(train_losses) - pobj_best)) / np.log(10)

-        plt.plot(durations, log_pobj, '-o', label=name)
+        plt.plot(durations, log_pobj, "-o", label=name)
         plt.legend(loc=0)
         plt.xlabel("seconds")
         plt.ylabel("log(best - train_loss)")
@@ -141,19 +149,20 @@

 def get_max_squared_sum(X):
     """Get the maximum row-wise sum of squares"""
-    return np.sum(X ** 2, axis=1).max()
+    return np.sum(X**2, axis=1).max()
+

 rcv1 = fetch_rcv1()
 X = rcv1.data
 n_samples, n_features = X.shape

 # consider the binary classification problem 'CCAT' vs the rest
-ccat_idx = rcv1.target_names.tolist().index('CCAT')
+ccat_idx = rcv1.target_names.tolist().index("CCAT")
 y = rcv1.target.tocsc()[:, ccat_idx].toarray().ravel().astype(np.float64)
 y[y == 0] = -1

 # parameters
-C = 1.
+C = 1.0
 fit_intercept = True
 tol = 1.0e-14

@@ -166,51 +175,116 @@
 sag_iter_range = list(range(1, 37, 3))

 clfs = [
-    ("LR-liblinear",
-     LogisticRegression(C=C, tol=tol,
-                        solver="liblinear", fit_intercept=fit_intercept,
-                        intercept_scaling=1),
-     liblinear_iter_range, [], [], [], []),
-    ("LR-liblinear-dual",
-     LogisticRegression(C=C, tol=tol, dual=True,
-                        solver="liblinear", fit_intercept=fit_intercept,
-                        intercept_scaling=1),
-     liblinear_dual_iter_range, [], [], [], []),
-    ("LR-SAG",
-     LogisticRegression(C=C, tol=tol,
-                        solver="sag", fit_intercept=fit_intercept),
-     sag_iter_range, [], [], [], []),
-    ("LR-newton-cg",
-     LogisticRegression(C=C, tol=tol, solver="newton-cg",
-                        fit_intercept=fit_intercept),
-     newton_iter_range, [], [], [], []),
-    ("LR-lbfgs",
-     LogisticRegression(C=C, tol=tol,
-                        solver="lbfgs", fit_intercept=fit_intercept),
-     lbfgs_iter_range, [], [], [], []),
-    ("SGD",
-     SGDClassifier(alpha=1.0 / C / n_samples, penalty='l2', loss='log',
-                   fit_intercept=fit_intercept, verbose=0),
-     sgd_iter_range, [], [], [], [])]
+    (
+        "LR-liblinear",
+        LogisticRegression(
+            C=C,
+            tol=tol,
+            solver="liblinear",
+            fit_intercept=fit_intercept,
+            intercept_scaling=1,
+        ),
+        liblinear_iter_range,
+        [],
+        [],
+        [],
+        [],
+    ),
+    (
+        "LR-liblinear-dual",
+        LogisticRegression(
+            C=C,
+            tol=tol,
+            dual=True,
+            solver="liblinear",
+            fit_intercept=fit_intercept,
+            intercept_scaling=1,
+        ),
+        liblinear_dual_iter_range,
+        [],
+        [],
+        [],
+        [],
+    ),
+    (
+        "LR-SAG",
+        LogisticRegression(C=C, tol=tol, solver="sag", fit_intercept=fit_intercept),
+        sag_iter_range,
+        [],
+        [],
+        [],
+        [],
+    ),
+    (
+        "LR-newton-cg",
+        LogisticRegression(
+            C=C, tol=tol, solver="newton-cg", fit_intercept=fit_intercept
+        ),
+        newton_iter_range,
+        [],
+        [],
+        [],
+        [],
+    ),
+    (
+        "LR-lbfgs",
+        LogisticRegression(C=C, tol=tol, solver="lbfgs", fit_intercept=fit_intercept),
+        lbfgs_iter_range,
+        [],
+        [],
+        [],
+        [],
+    ),
+    (
+        "SGD",
+        SGDClassifier(
+            alpha=1.0 / C / n_samples,
+            penalty="l2",
+            loss="log",
+            fit_intercept=fit_intercept,
+            verbose=0,
+        ),
+        sgd_iter_range,
+        [],
+        [],
+        [],
+        [],
+    ),
+]


 if lightning_clf is not None and not fit_intercept:
-    alpha = 1. / C / n_samples
+    alpha = 1.0 / C / n_samples
     # compute the same step_size than in LR-sag
     max_squared_sum = get_max_squared_sum(X)
-    step_size = get_auto_step_size(max_squared_sum, alpha, "log",
-                                   fit_intercept)
+    step_size = get_auto_step_size(max_squared_sum, alpha, "log", fit_intercept)

     clfs.append(
-        ("Lightning-SVRG",
-         lightning_clf.SVRGClassifier(alpha=alpha, eta=step_size,
-                                      tol=tol, loss="log"),
-         sag_iter_range, [], [], [], []))
+        (
+            "Lightning-SVRG",
+            lightning_clf.SVRGClassifier(
+                alpha=alpha, eta=step_size, tol=tol, loss="log"
+            ),
+            sag_iter_range,
+            [],
+            [],
+            [],
+            [],
+        )
+    )
     clfs.append(
-        ("Lightning-SAG",
-         lightning_clf.SAGClassifier(alpha=alpha, eta=step_size,
-                                     tol=tol, loss="log"),
-         sag_iter_range, [], [], [], []))
+        (
+            "Lightning-SAG",
+            lightning_clf.SAGClassifier(
+                alpha=alpha, eta=step_size, tol=tol, loss="log"
+            ),
+            sag_iter_range,
+            [],
+            [],
+            [],
+            [],
+        )
+    )

     # We keep only 200 features, to have a dense dataset,
     # and compare to lightning SAG, which seems incorrect in the sparse case.
('benchmarks', 'bench_plot_neighbors.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -10,12 +10,12 @@
 from sklearn import neighbors, datasets


-def get_data(N, D, dataset='dense'):
-    if dataset == 'dense':
+def get_data(N, D, dataset="dense"):
+    if dataset == "dense":
         np.random.seed(0)
         return np.random.random((N, D))
-    elif dataset == 'digits':
-        X = datasets.load_digits().data
+    elif dataset == "digits":
+        X, _ = datasets.load_digits(return_X_y=True)
         i = np.argsort(X[0])[::-1]
         X = X[:, i]
         return X[:N, :D]
@@ -23,129 +23,121 @@
         raise ValueError("invalid dataset: %s" % dataset)


-def barplot_neighbors(Nrange=2 ** np.arange(1, 11),
-                      Drange=2 ** np.arange(7),
-                      krange=2 ** np.arange(10),
-                      N=1000,
-                      D=64,
-                      k=5,
-                      leaf_size=30,
-                      dataset='digits'):
-    algorithms = ('kd_tree', 'brute', 'ball_tree')
-    fiducial_values = {'N': N,
-                       'D': D,
-                       'k': k}
+def barplot_neighbors(
+    Nrange=2 ** np.arange(1, 11),
+    Drange=2 ** np.arange(7),
+    krange=2 ** np.arange(10),
+    N=1000,
+    D=64,
+    k=5,
+    leaf_size=30,
+    dataset="digits",
+):
+    algorithms = ("kd_tree", "brute", "ball_tree")
+    fiducial_values = {"N": N, "D": D, "k": k}

-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     # varying N
-    N_results_build = {alg: np.zeros(len(Nrange))
-                       for alg in algorithms}
-    N_results_query = {alg: np.zeros(len(Nrange))
-                       for alg in algorithms}
+    N_results_build = {alg: np.zeros(len(Nrange)) for alg in algorithms}
+    N_results_query = {alg: np.zeros(len(Nrange)) for alg in algorithms}

     for i, NN in enumerate(Nrange):
         print("N = %i (%i out of %i)" % (NN, i + 1, len(Nrange)))
         X = get_data(NN, D, dataset)
         for algorithm in algorithms:
-            nbrs = neighbors.NearestNeighbors(n_neighbors=min(NN, k),
-                                              algorithm=algorithm,
-                                              leaf_size=leaf_size)
+            nbrs = neighbors.NearestNeighbors(
+                n_neighbors=min(NN, k), algorithm=algorithm, leaf_size=leaf_size
+            )
             t0 = time()
             nbrs.fit(X)
             t1 = time()
             nbrs.kneighbors(X)
             t2 = time()

-            N_results_build[algorithm][i] = (t1 - t0)
-            N_results_query[algorithm][i] = (t2 - t1)
+            N_results_build[algorithm][i] = t1 - t0
+            N_results_query[algorithm][i] = t2 - t1

-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     # varying D
-    D_results_build = {alg: np.zeros(len(Drange))
-                       for alg in algorithms}
-    D_results_query = {alg: np.zeros(len(Drange))
-                       for alg in algorithms}
+    D_results_build = {alg: np.zeros(len(Drange)) for alg in algorithms}
+    D_results_query = {alg: np.zeros(len(Drange)) for alg in algorithms}

     for i, DD in enumerate(Drange):
         print("D = %i (%i out of %i)" % (DD, i + 1, len(Drange)))
         X = get_data(N, DD, dataset)
         for algorithm in algorithms:
-            nbrs = neighbors.NearestNeighbors(n_neighbors=k,
-                                              algorithm=algorithm,
-                                              leaf_size=leaf_size)
+            nbrs = neighbors.NearestNeighbors(
+                n_neighbors=k, algorithm=algorithm, leaf_size=leaf_size
+            )
             t0 = time()
             nbrs.fit(X)
             t1 = time()
             nbrs.kneighbors(X)
             t2 = time()

-            D_results_build[algorithm][i] = (t1 - t0)
-            D_results_query[algorithm][i] = (t2 - t1)
+            D_results_build[algorithm][i] = t1 - t0
+            D_results_query[algorithm][i] = t2 - t1

-    #------------------------------------------------------------
+    # ------------------------------------------------------------
     # varying k
-    k_results_build = {alg: np.zeros(len(krange))
-                       for alg in algorithms}
-    k_results_query = {alg: np.zeros(len(krange))
-                       for alg in algorithms}
+    k_results_build = {alg: np.zeros(len(krange)) for alg in algorithms}
+    k_results_query = {alg: np.zeros(len(krange)) for alg in algorithms}

     X = get_data(N, DD, dataset)

     for i, kk in enumerate(krange):
         print("k = %i (%i out of %i)" % (kk, i + 1, len(krange)))
         for algorithm in algorithms:
-            nbrs = neighbors.NearestNeighbors(n_neighbors=kk,
-                                              algorithm=algorithm,
-                                              leaf_size=leaf_size)
+            nbrs = neighbors.NearestNeighbors(
+                n_neighbors=kk, algorithm=algorithm, leaf_size=leaf_size
+            )
             t0 = time()
             nbrs.fit(X)
             t1 = time()
             nbrs.kneighbors(X)
             t2 = time()

-            k_results_build[algorithm][i] = (t1 - t0)
-            k_results_query[algorithm][i] = (t2 - t1)
+            k_results_build[algorithm][i] = t1 - t0
+            k_results_query[algorithm][i] = t2 - t1

     plt.figure(figsize=(8, 11))

-    for (sbplt, vals, quantity,
-         build_time, query_time) in [(311, Nrange, 'N',
-                                      N_results_build,
-                                      N_results_query),
-                                     (312, Drange, 'D',
-                                      D_results_build,
-                                      D_results_query),
-                                     (313, krange, 'k',
-                                      k_results_build,
-                                      k_results_query)]:
-        ax = plt.subplot(sbplt, yscale='log')
+    for sbplt, vals, quantity, build_time, query_time in [
+        (311, Nrange, "N", N_results_build, N_results_query),
+        (312, Drange, "D", D_results_build, D_results_query),
+        (313, krange, "k", k_results_build, k_results_query),
+    ]:
+        ax = plt.subplot(sbplt, yscale="log")
         plt.grid(True)

         tick_vals = []
         tick_labels = []

-        bottom = 10 ** np.min([min(np.floor(np.log10(build_time[alg])))
-                               for alg in algorithms])
+        bottom = 10 ** np.min(
+            [min(np.floor(np.log10(build_time[alg]))) for alg in algorithms]
+        )

         for i, alg in enumerate(algorithms):
             xvals = 0.1 + i * (1 + len(vals)) + np.arange(len(vals))
             width = 0.8

-            c_bar = plt.bar(xvals, build_time[alg] - bottom,
-                            width, bottom, color='r')
-            q_bar = plt.bar(xvals, query_time[alg],
-                            width, build_time[alg], color='b')
+            c_bar = plt.bar(xvals, build_time[alg] - bottom, width, bottom, color="r")
+            q_bar = plt.bar(xvals, query_time[alg], width, build_time[alg], color="b")

             tick_vals += list(xvals + 0.5 * width)
-            tick_labels += ['%i' % val for val in vals]
+            tick_labels += ["%i" % val for val in vals]

-            plt.text((i + 0.02) / len(algorithms), 0.98, alg,
-                     transform=ax.transAxes,
-                     ha='left',
-                     va='top',
-                     bbox=dict(facecolor='w', edgecolor='w', alpha=0.5))
+            plt.text(
+                (i + 0.02) / len(algorithms),
+                0.98,
+                alg,
+                transform=ax.transAxes,
+                ha="left",
+                va="top",
+                bbox=dict(facecolor="w", edgecolor="w", alpha=0.5),
+            )

-            plt.ylabel('Time (s)')
+            plt.ylabel("Time (s)")

         ax.xaxis.set_major_locator(ticker.FixedLocator(tick_vals))
         ax.xaxis.set_major_formatter(ticker.FixedFormatter(tick_labels))
@@ -154,32 +146,45 @@
             label.set_rotation(-90)
             label.set_fontsize(10)

-        title_string = 'Varying %s' % quantity
+        title_string = "Varying %s" % quantity

-        descr_string = ''
+        descr_string = ""

-        for s in 'NDk':
+        for s in "NDk":
             if s == quantity:
                 pass
             else:
-                descr_string += '%s = %i, ' % (s, fiducial_values[s])
+                descr_string += "%s = %i, " % (s, fiducial_values[s])

         descr_string = descr_string[:-2]

-        plt.text(1.01, 0.5, title_string,
-                 transform=ax.transAxes, rotation=-90,
-                 ha='left', va='center', fontsize=20)
+        plt.text(
+            1.01,
+            0.5,
+            title_string,
+            transform=ax.transAxes,
+            rotation=-90,
+            ha="left",
+            va="center",
+            fontsize=20,
+        )

-        plt.text(0.99, 0.5, descr_string,
-                 transform=ax.transAxes, rotation=-90,
-                 ha='right', va='center')
+        plt.text(
+            0.99,
+            0.5,
+            descr_string,
+            transform=ax.transAxes,
+            rotation=-90,
+            ha="right",
+            va="center",
+        )

         plt.gcf().suptitle("%s data set" % dataset.capitalize(), fontsize=16)

-    plt.figlegend((c_bar, q_bar), ('construction', 'N-point query'),
-                  'upper right')
+    plt.figlegend((c_bar, q_bar), ("construction", "N-point query"), "upper right")

-if __name__ == '__main__':
-    barplot_neighbors(dataset='digits')
-    barplot_neighbors(dataset='dense')
+
+if __name__ == "__main__":
+    barplot_neighbors(dataset="digits")
+    barplot_neighbors(dataset="dense")
     plt.show()
('benchmarks', 'bench_tsne_mnist.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -15,30 +15,29 @@
 import argparse
 from joblib import Memory

-from sklearn.datasets import fetch_mldata
+from sklearn.datasets import fetch_openml
 from sklearn.manifold import TSNE
 from sklearn.neighbors import NearestNeighbors
 from sklearn.decomposition import PCA
 from sklearn.utils import check_array
 from sklearn.utils import shuffle as _shuffle
-
+from sklearn.utils._openmp_helpers import _openmp_effective_n_threads

 LOG_DIR = "mnist_tsne_output"
 if not os.path.exists(LOG_DIR):
     os.mkdir(LOG_DIR)


-memory = Memory(os.path.join(LOG_DIR, 'mnist_tsne_benchmark_data'),
-                mmap_mode='r')
+memory = Memory(os.path.join(LOG_DIR, "mnist_tsne_benchmark_data"), mmap_mode="r")


 @memory.cache
-def load_data(dtype=np.float32, order='C', shuffle=True, seed=0):
+def load_data(dtype=np.float32, order="C", shuffle=True, seed=0):
     """Load the data, then cache and memmap the train/test split"""
     print("Loading dataset...")
-    data = fetch_mldata('MNIST original')
-
-    X = check_array(data['data'], dtype=dtype, order=order)
+    data = fetch_openml("mnist_784")
+
+    X = check_array(data["data"], dtype=dtype, order=order)
     y = data["target"]

     if shuffle:
@@ -63,50 +62,75 @@


 def sanitize(filename):
-    return filename.replace("/", '-').replace(" ", "_")
+    return filename.replace("/", "-").replace(" ", "_")


 if __name__ == "__main__":
-    parser = argparse.ArgumentParser('Benchmark for t-SNE')
-    parser.add_argument('--order', type=str, default='C',
-                        help='Order of the input data')
-    parser.add_argument('--perplexity', type=float, default=30)
-    parser.add_argument('--bhtsne', action='store_true',
-                        help="if set and the reference bhtsne code is "
-                        "correctly installed, run it in the benchmark.")
-    parser.add_argument('--all', action='store_true',
-                        help="if set, run the benchmark with the whole MNIST."
-                             "dataset. Note that it will take up to 1 hour.")
-    parser.add_argument('--profile', action='store_true',
-                        help="if set, run the benchmark with a memory "
-                             "profiler.")
-    parser.add_argument('--verbose', type=int, default=0)
-    parser.add_argument('--pca-components', type=int, default=50,
-                        help="Number of principal components for "
-                             "preprocessing.")
+    parser = argparse.ArgumentParser("Benchmark for t-SNE")
+    parser.add_argument(
+        "--order", type=str, default="C", help="Order of the input data"
+    )
+    parser.add_argument("--perplexity", type=float, default=30)
+    parser.add_argument(
+        "--bhtsne",
+        action="store_true",
+        help=(
+            "if set and the reference bhtsne code is "
+            "correctly installed, run it in the benchmark."
+        ),
+    )
+    parser.add_argument(
+        "--all",
+        action="store_true",
+        help=(
+            "if set, run the benchmark with the whole MNIST."
+            "dataset. Note that it will take up to 1 hour."
+        ),
+    )
+    parser.add_argument(
+        "--profile",
+        action="store_true",
+        help="if set, run the benchmark with a memory profiler.",
+    )
+    parser.add_argument("--verbose", type=int, default=0)
+    parser.add_argument(
+        "--pca-components",
+        type=int,
+        default=50,
+        help="Number of principal components for preprocessing.",
+    )
     args = parser.parse_args()

+    print("Used number of threads: {}".format(_openmp_effective_n_threads()))
     X, y = load_data(order=args.order)

     if args.pca_components > 0:
         t0 = time()
         X = PCA(n_components=args.pca_components).fit_transform(X)
-        print("PCA preprocessing down to {} dimensions took {:0.3f}s"
-              .format(args.pca_components, time() - t0))
+        print(
+            "PCA preprocessing down to {} dimensions took {:0.3f}s".format(
+                args.pca_components, time() - t0
+            )
+        )

     methods = []

     # Put TSNE in methods
-    tsne = TSNE(n_components=2, init='pca', perplexity=args.perplexity,
-                verbose=args.verbose, n_iter=1000)
-    methods.append(("sklearn TSNE",
-                    lambda data: tsne_fit_transform(tsne, data)))
+    tsne = TSNE(
+        n_components=2,
+        init="pca",
+        perplexity=args.perplexity,
+        verbose=args.verbose,
+        n_iter=1000,
+    )
+    methods.append(("sklearn TSNE", lambda data: tsne_fit_transform(tsne, data)))

     if args.bhtsne:
         try:
             from bhtsne.bhtsne import run_bh_tsne
-        except ImportError:
-            raise ImportError("""\
+        except ImportError as e:
+            raise ImportError(
+                """\
 If you want comparison with the reference implementation, build the
 binary from source (https://github.com/lvdmaaten/bhtsne) in the folder
 benchmarks/bhtsne and add an empty `__init__.py` file in the folder:
@@ -116,24 +140,35 @@
 $ g++ sptree.cpp tsne.cpp tsne_main.cpp -o bh_tsne -O2
 $ touch __init__.py
 $ cd ..
-""")
+"""
+            ) from e

         def bhtsne(X):
             """Wrapper for the reference lvdmaaten/bhtsne implementation."""
             # PCA preprocessing is done elsewhere in the benchmark script
             n_iter = -1  # TODO find a way to report the number of iterations
-            return run_bh_tsne(X, use_pca=False, perplexity=args.perplexity,
-                               verbose=args.verbose > 0), n_iter
+            return (
+                run_bh_tsne(
+                    X,
+                    use_pca=False,
+                    perplexity=args.perplexity,
+                    verbose=args.verbose > 0,
+                ),
+                n_iter,
+            )
+
         methods.append(("lvdmaaten/bhtsne", bhtsne))

     if args.profile:

         try:
             from memory_profiler import profile
-        except ImportError:
-            raise ImportError("To run the benchmark with `--profile`, you "
-                              "need to install `memory_profiler`. Please "
-                              "run `pip install memory_profiler`.")
+        except ImportError as e:
+            raise ImportError(
+                "To run the benchmark with `--profile`, you "
+                "need to install `memory_profiler`. Please "
+                "run `pip install memory_profiler`."
+            ) from e
         methods = [(n, profile(m)) for n, m in methods]

     data_size = [100, 500, 1000, 5000, 10000]
@@ -141,8 +176,8 @@
         data_size.append(70000)

     results = []
-    basename, _ = os.path.splitext(__file__)
-    log_filename = os.path.join(LOG_DIR, basename + '.json')
+    basename = os.path.basename(os.path.splitext(__file__)[0])
+    log_filename = os.path.join(LOG_DIR, basename + ".json")
     for n in data_size:
         X_train = X[:n]
         y_train = y[:n]
@@ -150,19 +185,24 @@
         for name, method in methods:
             print("Fitting {} on {} samples...".format(name, n))
             t0 = time()
-            np.save(os.path.join(LOG_DIR, 'mnist_{}_{}.npy'
-                                 .format('original', n)), X_train)
-            np.save(os.path.join(LOG_DIR, 'mnist_{}_{}.npy'
-                                 .format('original_labels', n)), y_train)
+            np.save(
+                os.path.join(LOG_DIR, "mnist_{}_{}.npy".format("original", n)), X_train
+            )
+            np.save(
+                os.path.join(LOG_DIR, "mnist_{}_{}.npy".format("original_labels", n)),
+                y_train,
+            )
             X_embedded, n_iter = method(X_train)
             duration = time() - t0
             precision_5 = nn_accuracy(X_train, X_embedded)
-            print("Fitting {} on {} samples took {:.3f}s in {:d} iterations, "
-                  "nn accuracy: {:0.3f}".format(
-                      name, n, duration, n_iter, precision_5))
+            print(
+                "Fitting {} on {} samples took {:.3f}s in {:d} iterations, "
+                "nn accuracy: {:0.3f}".format(name, n, duration, n_iter, precision_5)
+            )
             results.append(dict(method=name, duration=duration, n_samples=n))
-            with open(log_filename, 'w', encoding='utf-8') as f:
+            with open(log_filename, "w", encoding="utf-8") as f:
                 json.dump(results, f)
             method_name = sanitize(name)
-            np.save(op.join(LOG_DIR, 'mnist_{}_{}.npy'.format(method_name, n)),
-                    X_embedded)
+            np.save(
+                op.join(LOG_DIR, "mnist_{}_{}.npy".format(method_name, n)), X_embedded
+            )
('benchmarks', 'bench_plot_randomized_svd.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -50,9 +50,10 @@

 References
 ----------
-(1) Finding structure with randomness: Stochastic algorithms for constructing
-    approximate matrix decompositions
-    Halko, et al., 2009 https://arxiv.org/abs/0909.4061
+(1) :arxiv:`"Finding structure with randomness:
+    Stochastic algorithms for constructing approximate matrix decompositions."
+    <0909.4061>`
+    Halko, et al., (2009)

 (2) A randomized algorithm for the decomposition of matrices
     Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert
@@ -74,19 +75,22 @@
 from collections import defaultdict
 import os.path

+from sklearn.utils._arpack import _init_arpack_v0
 from sklearn.utils import gen_batches
 from sklearn.utils.validation import check_random_state
 from sklearn.utils.extmath import randomized_svd
-from sklearn.datasets.samples_generator import (make_low_rank_matrix,
-                                                make_sparse_uncorrelated)
-from sklearn.datasets import (fetch_lfw_people,
-                              fetch_mldata,
-                              fetch_20newsgroups_vectorized,
-                              fetch_olivetti_faces,
-                              fetch_rcv1)
+from sklearn.datasets import make_low_rank_matrix, make_sparse_uncorrelated
+from sklearn.datasets import (
+    fetch_lfw_people,
+    fetch_openml,
+    fetch_20newsgroups_vectorized,
+    fetch_olivetti_faces,
+    fetch_rcv1,
+)

 try:
     import fbpca
+
     fbpca_available = True
 except ImportError:
     fbpca_available = False
@@ -103,23 +107,32 @@

 # Determine when to switch to batch computation for matrix norms,
 # in case the reconstructed (dense) matrix is too large
-MAX_MEMORY = np.int(2e9)
-
-# The following datasets can be dowloaded manually from:
+MAX_MEMORY = int(2e9)
+
+# The following datasets can be downloaded manually from:
 # CIFAR 10: https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz
 # SVHN: http://ufldl.stanford.edu/housenumbers/train_32x32.mat
 CIFAR_FOLDER = "./cifar-10-batches-py/"
 SVHN_FOLDER = "./SVHN/"

-datasets = ['low rank matrix', 'lfw_people', 'olivetti_faces', '20newsgroups',
-            'MNIST original', 'CIFAR', 'a1a', 'SVHN', 'uncorrelated matrix']
-
-big_sparse_datasets = ['big sparse matrix', 'rcv1']
+datasets = [
+    "low rank matrix",
+    "lfw_people",
+    "olivetti_faces",
+    "20newsgroups",
+    "mnist_784",
+    "CIFAR",
+    "a3a",
+    "SVHN",
+    "uncorrelated matrix",
+]
+
+big_sparse_datasets = ["big sparse matrix", "rcv1"]


 def unpickle(file_name):
-    with open(file_name, 'rb') as fo:
-        return pickle.load(fo, encoding='latin1')["data"]
+    with open(file_name, "rb") as fo:
+        return pickle.load(fo, encoding="latin1")["data"]


 def handle_missing_dataset(file_folder):
@@ -131,41 +144,45 @@
 def get_data(dataset_name):
     print("Getting dataset: %s" % dataset_name)

-    if dataset_name == 'lfw_people':
+    if dataset_name == "lfw_people":
         X = fetch_lfw_people().data
-    elif dataset_name == '20newsgroups':
+    elif dataset_name == "20newsgroups":
         X = fetch_20newsgroups_vectorized().data[:, :100000]
-    elif dataset_name == 'olivetti_faces':
+    elif dataset_name == "olivetti_faces":
         X = fetch_olivetti_faces().data
-    elif dataset_name == 'rcv1':
+    elif dataset_name == "rcv1":
         X = fetch_rcv1().data
-    elif dataset_name == 'CIFAR':
+    elif dataset_name == "CIFAR":
         if handle_missing_dataset(CIFAR_FOLDER) == "skip":
             return
-        X1 = [unpickle("%sdata_batch_%d" % (CIFAR_FOLDER, i + 1))
-              for i in range(5)]
+        X1 = [unpickle("%sdata_batch_%d" % (CIFAR_FOLDER, i + 1)) for i in range(5)]
         X = np.vstack(X1)
         del X1
-    elif dataset_name == 'SVHN':
+    elif dataset_name == "SVHN":
         if handle_missing_dataset(SVHN_FOLDER) == 0:
             return
-        X1 = sp.io.loadmat("%strain_32x32.mat" % SVHN_FOLDER)['X']
+        X1 = sp.io.loadmat("%strain_32x32.mat" % SVHN_FOLDER)["X"]
         X2 = [X1[:, :, :, i].reshape(32 * 32 * 3) for i in range(X1.shape[3])]
         X = np.vstack(X2)
         del X1
         del X2
-    elif dataset_name == 'low rank matrix':
-        X = make_low_rank_matrix(n_samples=500, n_features=np.int(1e4),
-                                 effective_rank=100, tail_strength=.5,
-                                 random_state=random_state)
-    elif dataset_name == 'uncorrelated matrix':
-        X, _ = make_sparse_uncorrelated(n_samples=500, n_features=10000,
-                                        random_state=random_state)
-    elif dataset_name == 'big sparse matrix':
-        sparsity = np.int(1e6)
-        size = np.int(1e6)
-        small_size = np.int(1e4)
-        data = np.random.normal(0, 1, np.int(sparsity/10))
+    elif dataset_name == "low rank matrix":
+        X = make_low_rank_matrix(
+            n_samples=500,
+            n_features=int(1e4),
+            effective_rank=100,
+            tail_strength=0.5,
+            random_state=random_state,
+        )
+    elif dataset_name == "uncorrelated matrix":
+        X, _ = make_sparse_uncorrelated(
+            n_samples=500, n_features=10000, random_state=random_state
+        )
+    elif dataset_name == "big sparse matrix":
+        sparsity = int(1e6)
+        size = int(1e6)
+        small_size = int(1e4)
+        data = np.random.normal(0, 1, int(sparsity / 10))
         data = np.repeat(data, 10)
         row = np.random.uniform(0, small_size, sparsity)
         col = np.random.uniform(0, small_size, sparsity)
@@ -174,22 +191,28 @@
         del row
         del col
     else:
-        X = fetch_mldata(dataset_name).data
+        X = fetch_openml(dataset_name).data
     return X


 def plot_time_vs_s(time, norm, point_labels, title):
     plt.figure()
-    colors = ['g', 'b', 'y']
+    colors = ["g", "b", "y"]
     for i, l in enumerate(sorted(norm.keys())):
         if l != "fbpca":
-            plt.plot(time[l], norm[l], label=l, marker='o', c=colors.pop())
+            plt.plot(time[l], norm[l], label=l, marker="o", c=colors.pop())
         else:
-            plt.plot(time[l], norm[l], label=l, marker='^', c='red')
+            plt.plot(time[l], norm[l], label=l, marker="^", c="red")

         for label, x, y in zip(point_labels, list(time[l]), list(norm[l])):
-            plt.annotate(label, xy=(x, y), xytext=(0, -20),
-                         textcoords='offset points', ha='right', va='bottom')
+            plt.annotate(
+                label,
+                xy=(x, y),
+                xytext=(0, -20),
+                textcoords="offset points",
+                ha="right",
+                va="bottom",
+            )
     plt.legend(loc="upper right")
     plt.suptitle(title)
     plt.ylabel("norm discrepancy")
@@ -201,21 +224,33 @@
     size = 100
     for i, l in enumerate(sorted(norm.keys())):
         if l != "fbpca":
-            plt.scatter(time[l], norm[l], label=l, marker='o', c='b', s=size)
+            plt.scatter(time[l], norm[l], label=l, marker="o", c="b", s=size)
             for label, x, y in zip(point_labels, list(time[l]), list(norm[l])):
-                plt.annotate(label, xy=(x, y), xytext=(0, -80),
-                             textcoords='offset points', ha='right',
-                             arrowprops=dict(arrowstyle="->",
-                                             connectionstyle="arc3"),
-                             va='bottom', size=11, rotation=90)
+                plt.annotate(
+                    label,
+                    xy=(x, y),
+                    xytext=(0, -80),
+                    textcoords="offset points",
+                    ha="right",
+                    arrowprops=dict(arrowstyle="->", connectionstyle="arc3"),
+                    va="bottom",
+                    size=11,
+                    rotation=90,
+                )
         else:
-            plt.scatter(time[l], norm[l], label=l, marker='^', c='red', s=size)
+            plt.scatter(time[l], norm[l], label=l, marker="^", c="red", s=size)
             for label, x, y in zip(point_labels, list(time[l]), list(norm[l])):
-                plt.annotate(label, xy=(x, y), xytext=(0, 30),
-                             textcoords='offset points', ha='right',
-                             arrowprops=dict(arrowstyle="->",
-                                             connectionstyle="arc3"),
-                             va='bottom', size=11, rotation=90)
+                plt.annotate(
+                    label,
+                    xy=(x, y),
+                    xytext=(0, 30),
+                    textcoords="offset points",
+                    ha="right",
+                    arrowprops=dict(arrowstyle="->", connectionstyle="arc3"),
+                    va="bottom",
+                    size=11,
+                    rotation=90,
+                )

     plt.legend(loc="best")
     plt.suptitle(title)
@@ -226,38 +261,46 @@
 def plot_power_iter_vs_s(power_iter, s, title):
     plt.figure()
     for l in sorted(s.keys()):
-        plt.plot(power_iter, s[l], label=l, marker='o')
-    plt.legend(loc="lower right", prop={'size': 10})
+        plt.plot(power_iter, s[l], label=l, marker="o")
+    plt.legend(loc="lower right", prop={"size": 10})
     plt.suptitle(title)
     plt.ylabel("norm discrepancy")
     plt.xlabel("n_iter")


-def svd_timing(X, n_comps, n_iter, n_oversamples,
-               power_iteration_normalizer='auto', method=None):
+def svd_timing(
+    X, n_comps, n_iter, n_oversamples, power_iteration_normalizer="auto", method=None
+):
     """
     Measure time for decomposition
     """
     print("... running SVD ...")
-    if method is not 'fbpca':
+    if method != "fbpca":
         gc.collect()
         t0 = time()
-        U, mu, V = randomized_svd(X, n_comps, n_oversamples, n_iter,
-                                  power_iteration_normalizer,
-                                  random_state=random_state, transpose=False)
+        U, mu, V = randomized_svd(
+            X,
+            n_comps,
+            n_oversamples,
+            n_iter,
+            power_iteration_normalizer,
+            random_state=random_state,
+            transpose=False,
+        )
         call_time = time() - t0
     else:
         gc.collect()
         t0 = time()
         # There is a different convention for l here
-        U, mu, V = fbpca.pca(X, n_comps, raw=True, n_iter=n_iter,
-                             l=n_oversamples+n_comps)
+        U, mu, V = fbpca.pca(
+            X, n_comps, raw=True, n_iter=n_iter, l=n_oversamples + n_comps
+        )
         call_time = time() - t0

     return U, mu, V, call_time


-def norm_diff(A, norm=2, msg=True):
+def norm_diff(A, norm=2, msg=True, random_state=None):
     """
     Compute the norm diff with the original matrix, when randomized
     SVD is called with *params.
@@ -269,7 +312,8 @@
         print("... computing %s norm ..." % norm)
     if norm == 2:
         # s = sp.linalg.norm(A, ord=2)  # slow
-        value = sp.sparse.linalg.svds(A, k=1, return_singular_vectors=False)
+        v0 = _init_arpack_v0(min(A.shape), random_state)
+        value = sp.sparse.linalg.svds(A, k=1, return_singular_vectors=False, v0=v0)
     else:
         if sp.sparse.issparse(A):
             value = sp.sparse.linalg.norm(A, ord=norm)
@@ -282,15 +326,15 @@
     # if the input is not too big, just call scipy
     if X.shape[0] * X.shape[1] < MAX_MEMORY:
         A = X - U.dot(np.diag(s).dot(V))
-        return norm_diff(A, norm='fro')
+        return norm_diff(A, norm="fro")

     print("... computing fro norm by batches...")
     batch_size = 1000
     Vhat = np.diag(s).dot(V)
-    cum_norm = .0
+    cum_norm = 0.0
     for batch in gen_batches(X.shape[0], batch_size):
         M = X[batch, :] - U[batch, :].dot(Vhat)
-        cum_norm += norm_diff(M, norm='fro', msg=False)
+        cum_norm += norm_diff(M, norm="fro", msg=False)
     return np.sqrt(cum_norm)


@@ -299,37 +343,47 @@
     all_time = defaultdict(list)
     if enable_spectral_norm:
         all_spectral = defaultdict(list)
-        X_spectral_norm = norm_diff(X, norm=2, msg=False)
+        X_spectral_norm = norm_diff(X, norm=2, msg=False, random_state=0)
     all_frobenius = defaultdict(list)
-    X_fro_norm = norm_diff(X, norm='fro', msg=False)
+    X_fro_norm = norm_diff(X, norm="fro", msg=False)

     for pi in power_iter:
-        for pm in ['none', 'LU', 'QR']:
+        for pm in ["none", "LU", "QR"]:
             print("n_iter = %d on sklearn - %s" % (pi, pm))
-            U, s, V, time = svd_timing(X, n_comps, n_iter=pi,
-                                       power_iteration_normalizer=pm,
-                                       n_oversamples=n_oversamples)
+            U, s, V, time = svd_timing(
+                X,
+                n_comps,
+                n_iter=pi,
+                power_iteration_normalizer=pm,
+                n_oversamples=n_oversamples,
+            )
             label = "sklearn - %s" % pm
             all_time[label].append(time)
             if enable_spectral_norm:
                 A = U.dot(np.diag(s).dot(V))
-                all_spectral[label].append(norm_diff(X - A, norm=2) /
-                                           X_spectral_norm)
+                all_spectral[label].append(
+                    norm_diff(X - A, norm=2, random_state=0) / X_spectral_norm
+                )
             f = scalable_frobenius_norm_discrepancy(X, U, s, V)
             all_frobenius[label].append(f / X_fro_norm)

         if fbpca_available:
             print("n_iter = %d on fbca" % (pi))
-            U, s, V, time = svd_timing(X, n_comps, n_iter=pi,
-                                       power_iteration_normalizer=pm,
-                                       n_oversamples=n_oversamples,
-                                       method='fbpca')
+            U, s, V, time = svd_timing(
+                X,
+                n_comps,
+                n_iter=pi,
+                power_iteration_normalizer=pm,
+                n_oversamples=n_oversamples,
+                method="fbpca",
+            )
             label = "fbpca"
             all_time[label].append(time)
             if enable_spectral_norm:
                 A = U.dot(np.diag(s).dot(V))
-                all_spectral[label].append(norm_diff(X - A, norm=2) /
-                                           X_spectral_norm)
+                all_spectral[label].append(
+                    norm_diff(X - A, norm=2, random_state=0) / X_spectral_norm
+                )
             f = scalable_frobenius_norm_discrepancy(X, U, s, V)
             all_frobenius[label].append(f / X_fro_norm)

@@ -343,8 +397,12 @@
 def bench_b(power_list):

     n_samples, n_features = 1000, 10000
-    data_params = {'n_samples': n_samples, 'n_features': n_features,
-                   'tail_strength': .7, 'random_state': random_state}
+    data_params = {
+        "n_samples": n_samples,
+        "n_features": n_features,
+        "tail_strength": 0.7,
+        "random_state": random_state,
+    }
     dataset_name = "low rank matrix %d x %d" % (n_samples, n_features)
     ranks = [10, 50, 100]

@@ -354,19 +412,25 @@
     for rank in ranks:
         X = make_low_rank_matrix(effective_rank=rank, **data_params)
         if enable_spectral_norm:
-            X_spectral_norm = norm_diff(X, norm=2, msg=False)
-        X_fro_norm = norm_diff(X, norm='fro', msg=False)
-
-        for n_comp in [np.int(rank/2), rank, rank*2]:
+            X_spectral_norm = norm_diff(X, norm=2, msg=False, random_state=0)
+        X_fro_norm = norm_diff(X, norm="fro", msg=False)
+
+        for n_comp in [int(rank / 2), rank, rank * 2]:
             label = "rank=%d, n_comp=%d" % (rank, n_comp)
             print(label)
             for pi in power_list:
-                U, s, V, _ = svd_timing(X, n_comp, n_iter=pi, n_oversamples=2,
-                                        power_iteration_normalizer='LU')
+                U, s, V, _ = svd_timing(
+                    X,
+                    n_comp,
+                    n_iter=pi,
+                    n_oversamples=2,
+                    power_iteration_normalizer="LU",
+                )
                 if enable_spectral_norm:
                     A = U.dot(np.diag(s).dot(V))
-                    all_spectral[label].append(norm_diff(X - A, norm=2) /
-                                               X_spectral_norm)
+                    all_spectral[label].append(
+                        norm_diff(X - A, norm=2, random_state=0) / X_spectral_norm
+                    )
                 f = scalable_frobenius_norm_discrepancy(X, U, s, V)
                 all_frobenius[label].append(f / X_fro_norm)

@@ -389,35 +453,35 @@
             continue

         if enable_spectral_norm:
-            X_spectral_norm = norm_diff(X, norm=2, msg=False)
-        X_fro_norm = norm_diff(X, norm='fro', msg=False)
+            X_spectral_norm = norm_diff(X, norm=2, msg=False, random_state=0)
+        X_fro_norm = norm_diff(X, norm="fro", msg=False)
         n_comps = np.minimum(n_comps, np.min(X.shape))

         label = "sklearn"
-        print("%s %d x %d - %s" %
-              (dataset_name, X.shape[0], X.shape[1], label))
-        U, s, V, time = svd_timing(X, n_comps, n_iter=2, n_oversamples=10,
-                                   method=label)
+        print("%s %d x %d - %s" % (dataset_name, X.shape[0], X.shape[1], label))
+        U, s, V, time = svd_timing(X, n_comps, n_iter=2, n_oversamples=10, method=label)

         all_time[label].append(time)
         if enable_spectral_norm:
             A = U.dot(np.diag(s).dot(V))
-            all_spectral[label].append(norm_diff(X - A, norm=2) /
-                                       X_spectral_norm)
+            all_spectral[label].append(
+                norm_diff(X - A, norm=2, random_state=0) / X_spectral_norm
+            )
         f = scalable_frobenius_norm_discrepancy(X, U, s, V)
         all_frobenius[label].append(f / X_fro_norm)

         if fbpca_available:
             label = "fbpca"
-            print("%s %d x %d - %s" %
-                  (dataset_name, X.shape[0], X.shape[1], label))
-            U, s, V, time = svd_timing(X, n_comps, n_iter=2, n_oversamples=2,
-                                       method=label)
+            print("%s %d x %d - %s" % (dataset_name, X.shape[0], X.shape[1], label))
+            U, s, V, time = svd_timing(
+                X, n_comps, n_iter=2, n_oversamples=2, method=label
+            )
             all_time[label].append(time)
             if enable_spectral_norm:
                 A = U.dot(np.diag(s).dot(V))
-                all_spectral[label].append(norm_diff(X - A, norm=2) /
-                                           X_spectral_norm)
+                all_spectral[label].append(
+                    norm_diff(X - A, norm=2, random_state=0) / X_spectral_norm
+                )
             f = scalable_frobenius_norm_discrepancy(X, U, s, V)
             all_frobenius[label].append(f / X_fro_norm)

@@ -431,7 +495,7 @@
     scatter_time_vs_s(all_time, all_frobenius, datasets, title)


-if __name__ == '__main__':
+if __name__ == "__main__":
     random_state = check_random_state(1234)

     power_iter = np.linspace(0, 6, 7, dtype=int)
@@ -441,10 +505,17 @@
         X = get_data(dataset_name)
         if X is None:
             continue
-        print(" >>>>>> Benching sklearn and fbpca on %s %d x %d" %
-              (dataset_name, X.shape[0], X.shape[1]))
-        bench_a(X, dataset_name, power_iter, n_oversamples=2,
-                n_comps=np.minimum(n_comps, np.min(X.shape)))
+        print(
+            " >>>>>> Benching sklearn and fbpca on %s %d x %d"
+            % (dataset_name, X.shape[0], X.shape[1])
+        )
+        bench_a(
+            X,
+            dataset_name,
+            power_iter,
+            n_oversamples=2,
+            n_comps=np.minimum(n_comps, np.min(X.shape)),
+        )

     print(" >>>>>> Benching on simulated low rank matrix with variable rank")
     bench_b(power_iter)
('benchmarks', 'bench_hist_gradient_boosting_higgsboson.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -9,28 +9,26 @@
 from joblib import Memory
 from sklearn.model_selection import train_test_split
 from sklearn.metrics import accuracy_score, roc_auc_score
-# To use this experimental feature, we need to explicitly ask for it:
-from sklearn.experimental import enable_hist_gradient_boosting  # noqa
 from sklearn.ensemble import HistGradientBoostingClassifier
-from sklearn.ensemble._hist_gradient_boosting.utils import (
-    get_equivalent_estimator)
+from sklearn.ensemble._hist_gradient_boosting.utils import get_equivalent_estimator


 parser = argparse.ArgumentParser()
-parser.add_argument('--n-leaf-nodes', type=int, default=31)
-parser.add_argument('--n-trees', type=int, default=10)
-parser.add_argument('--lightgbm', action="store_true", default=False)
-parser.add_argument('--xgboost', action="store_true", default=False)
-parser.add_argument('--catboost', action="store_true", default=False)
-parser.add_argument('--learning-rate', type=float, default=1.)
-parser.add_argument('--subsample', type=int, default=None)
-parser.add_argument('--max-bins', type=int, default=255)
+parser.add_argument("--n-leaf-nodes", type=int, default=31)
+parser.add_argument("--n-trees", type=int, default=10)
+parser.add_argument("--lightgbm", action="store_true", default=False)
+parser.add_argument("--xgboost", action="store_true", default=False)
+parser.add_argument("--catboost", action="store_true", default=False)
+parser.add_argument("--learning-rate", type=float, default=1.0)
+parser.add_argument("--subsample", type=int, default=None)
+parser.add_argument("--max-bins", type=int, default=255)
+parser.add_argument("--no-predict", action="store_true", default=False)
+parser.add_argument("--cache-loc", type=str, default="/tmp")
 args = parser.parse_args()

 HERE = os.path.dirname(__file__)
-URL = ("https://archive.ics.uci.edu/ml/machine-learning-databases/00280/"
-       "HIGGS.csv.gz")
-m = Memory(location='/tmp', mmap_mode='r')
+URL = "https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz"
+m = Memory(location=args.cache_loc, mmap_mode="r")

 n_leaf_nodes = args.n_leaf_nodes
 n_trees = args.n_trees
@@ -41,7 +39,7 @@

 @m.cache
 def load_data():
-    filename = os.path.join(HERE, URL.rsplit('/', 1)[-1])
+    filename = os.path.join(HERE, URL.rsplit("/", 1)[-1])
     if not os.path.exists(filename):
         print(f"Downloading {URL} to {filename} (2.6 GB)...")
         urlretrieve(URL, filename)
@@ -56,11 +54,33 @@
     return df


+def fit(est, data_train, target_train, libname):
+    print(f"Fitting a {libname} model...")
+    tic = time()
+    est.fit(data_train, target_train)
+    toc = time()
+    print(f"fitted in {toc - tic:.3f}s")
+
+
+def predict(est, data_test, target_test):
+    if args.no_predict:
+        return
+    tic = time()
+    predicted_test = est.predict(data_test)
+    predicted_proba_test = est.predict_proba(data_test)
+    toc = time()
+    roc_auc = roc_auc_score(target_test, predicted_proba_test[:, 1])
+    acc = accuracy_score(target_test, predicted_test)
+    print(f"predicted in {toc - tic:.3f}s, ROC AUC: {roc_auc:.4f}, ACC: {acc :.4f}")
+
+
 df = load_data()
 target = df.values[:, 0]
 data = np.ascontiguousarray(df.values[:, 1:])
 data_train, data_test, target_train, target_test = train_test_split(
-    data, target, test_size=.2, random_state=0)
+    data, target, test_size=0.2, random_state=0
+)
+n_classes = len(np.unique(target))

 if subsample is not None:
     data_train, target_train = data_train[:subsample], target_train[:subsample]
@@ -68,56 +88,30 @@
 n_samples, n_features = data_train.shape
 print(f"Training set with {n_samples} records with {n_features} features.")

-print("Fitting a sklearn model...")
-tic = time()
-est = HistGradientBoostingClassifier(loss='binary_crossentropy',
-                                     learning_rate=lr,
-                                     max_iter=n_trees,
-                                     max_bins=max_bins,
-                                     max_leaf_nodes=n_leaf_nodes,
-                                     n_iter_no_change=None,
-                                     random_state=0,
-                                     verbose=1)
-est.fit(data_train, target_train)
-toc = time()
-predicted_test = est.predict(data_test)
-predicted_proba_test = est.predict_proba(data_test)
-roc_auc = roc_auc_score(target_test, predicted_proba_test[:, 1])
-acc = accuracy_score(target_test, predicted_test)
-print(f"done in {toc - tic:.3f}s, ROC AUC: {roc_auc:.4f}, ACC: {acc :.4f}")
+est = HistGradientBoostingClassifier(
+    loss="log_loss",
+    learning_rate=lr,
+    max_iter=n_trees,
+    max_bins=max_bins,
+    max_leaf_nodes=n_leaf_nodes,
+    early_stopping=False,
+    random_state=0,
+    verbose=1,
+)
+fit(est, data_train, target_train, "sklearn")
+predict(est, data_test, target_test)

 if args.lightgbm:
-    print("Fitting a LightGBM model...")
-    tic = time()
-    lightgbm_est = get_equivalent_estimator(est, lib='lightgbm')
-    lightgbm_est.fit(data_train, target_train)
-    toc = time()
-    predicted_test = lightgbm_est.predict(data_test)
-    predicted_proba_test = lightgbm_est.predict_proba(data_test)
-    roc_auc = roc_auc_score(target_test, predicted_proba_test[:, 1])
-    acc = accuracy_score(target_test, predicted_test)
-    print(f"done in {toc - tic:.3f}s, ROC AUC: {roc_auc:.4f}, ACC: {acc :.4f}")
+    est = get_equivalent_estimator(est, lib="lightgbm", n_classes=n_classes)
+    fit(est, data_train, target_train, "lightgbm")
+    predict(est, data_test, target_test)

 if args.xgboost:
-    print("Fitting an XGBoost model...")
-    tic = time()
-    xgboost_est = get_equivalent_estimator(est, lib='xgboost')
-    xgboost_est.fit(data_train, target_train)
-    toc = time()
-    predicted_test = xgboost_est.predict(data_test)
-    predicted_proba_test = xgboost_est.predict_proba(data_test)
-    roc_auc = roc_auc_score(target_test, predicted_proba_test[:, 1])
-    acc = accuracy_score(target_test, predicted_test)
-    print(f"done in {toc - tic:.3f}s, ROC AUC: {roc_auc:.4f}, ACC: {acc :.4f}")
+    est = get_equivalent_estimator(est, lib="xgboost", n_classes=n_classes)
+    fit(est, data_train, target_train, "xgboost")
+    predict(est, data_test, target_test)

 if args.catboost:
-    print("Fitting a Catboost model...")
-    tic = time()
-    catboost_est = get_equivalent_estimator(est, lib='catboost')
-    catboost_est.fit(data_train, target_train)
-    toc = time()
-    predicted_test = catboost_est.predict(data_test)
-    predicted_proba_test = catboost_est.predict_proba(data_test)
-    roc_auc = roc_auc_score(target_test, predicted_proba_test[:, 1])
-    acc = accuracy_score(target_test, predicted_test)
-    print(f"done in {toc - tic:.3f}s, ROC AUC: {roc_auc:.4f}, ACC: {acc :.4f}")
+    est = get_equivalent_estimator(est, lib="catboost", n_classes=n_classes)
+    fit(est, data_train, target_train, "catboost")
+    predict(est, data_test, target_test)
('benchmarks', 'bench_isolation_forest.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -22,7 +22,7 @@

 from sklearn.ensemble import IsolationForest
 from sklearn.metrics import roc_curve, auc
-from sklearn.datasets import fetch_kddcup99, fetch_covtype, fetch_mldata
+from sklearn.datasets import fetch_kddcup99, fetch_covtype, fetch_openml
 from sklearn.preprocessing import LabelBinarizer
 from sklearn.utils import shuffle as sh

@@ -48,34 +48,35 @@
 with_decision_function_histograms = False

 # datasets available = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']
-datasets = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']
+datasets = ["http", "smtp", "SA", "SF", "shuttle", "forestcover"]

 # Loop over all datasets for fitting and scoring the estimator:
 for dat in datasets:

     # Loading and vectorizing the data:
-    print('====== %s ======' % dat)
-    print('--- Fetching data...')
-    if dat in ['http', 'smtp', 'SF', 'SA']:
-        dataset = fetch_kddcup99(subset=dat, shuffle=True,
-                                 percent10=True, random_state=random_state)
+    print("====== %s ======" % dat)
+    print("--- Fetching data...")
+    if dat in ["http", "smtp", "SF", "SA"]:
+        dataset = fetch_kddcup99(
+            subset=dat, shuffle=True, percent10=True, random_state=random_state
+        )
         X = dataset.data
         y = dataset.target

-    if dat == 'shuttle':
-        dataset = fetch_mldata('shuttle')
+    if dat == "shuttle":
+        dataset = fetch_openml("shuttle")
         X = dataset.data
         y = dataset.target
         X, y = sh(X, y, random_state=random_state)
         # we remove data with label 4
         # normal data are then those of class 1
-        s = (y != 4)
+        s = y != 4
         X = X[s, :]
         y = y[s]
         y = (y != 1).astype(int)
-        print('----- ')
+        print("----- ")

-    if dat == 'forestcover':
+    if dat == "forestcover":
         dataset = fetch_covtype(shuffle=True, random_state=random_state)
         X = dataset.data
         y = dataset.target
@@ -87,26 +88,26 @@
         y = (y != 2).astype(int)
         print_outlier_ratio(y)

-    print('--- Vectorizing data...')
+    print("--- Vectorizing data...")

-    if dat == 'SF':
+    if dat == "SF":
         lb = LabelBinarizer()
         x1 = lb.fit_transform(X[:, 1].astype(str))
         X = np.c_[X[:, :1], x1, X[:, 2:]]
-        y = (y != b'normal.').astype(int)
+        y = (y != b"normal.").astype(int)
         print_outlier_ratio(y)

-    if dat == 'SA':
+    if dat == "SA":
         lb = LabelBinarizer()
         x1 = lb.fit_transform(X[:, 1].astype(str))
         x2 = lb.fit_transform(X[:, 2].astype(str))
         x3 = lb.fit_transform(X[:, 3].astype(str))
         X = np.c_[X[:, :1], x1, x2, x3, X[:, 4:]]
-        y = (y != b'normal.').astype(int)
+        y = (y != b"normal.").astype(int)
         print_outlier_ratio(y)

-    if dat in ('http', 'smtp'):
-        y = (y != b'normal.').astype(int)
+    if dat in ("http", "smtp"):
+        y = (y != b"normal.").astype(int)
         print_outlier_ratio(y)

     n_samples, n_features = X.shape
@@ -118,33 +119,36 @@
     y_train = y[:n_samples_train]
     y_test = y[n_samples_train:]

-    print('--- Fitting the IsolationForest estimator...')
-    model = IsolationForest(behaviour='new', n_jobs=-1,
-                            random_state=random_state)
+    print("--- Fitting the IsolationForest estimator...")
+    model = IsolationForest(n_jobs=-1, random_state=random_state)
     tstart = time()
     model.fit(X_train)
     fit_time = time() - tstart
     tstart = time()

-    scoring = - model.decision_function(X_test)  # the lower, the more abnormal
+    scoring = -model.decision_function(X_test)  # the lower, the more abnormal

     print("--- Preparing the plot elements...")
     if with_decision_function_histograms:
         fig, ax = plt.subplots(3, sharex=True, sharey=True)
         bins = np.linspace(-0.5, 0.5, 200)
-        ax[0].hist(scoring, bins, color='black')
-        ax[0].set_title('Decision function for %s dataset' % dat)
-        ax[1].hist(scoring[y_test == 0], bins, color='b', label='normal data')
+        ax[0].hist(scoring, bins, color="black")
+        ax[0].set_title("Decision function for %s dataset" % dat)
+        ax[1].hist(scoring[y_test == 0], bins, color="b", label="normal data")
         ax[1].legend(loc="lower right")
-        ax[2].hist(scoring[y_test == 1], bins, color='r', label='outliers')
+        ax[2].hist(scoring[y_test == 1], bins, color="r", label="outliers")
         ax[2].legend(loc="lower right")

     # Show ROC Curves
     predict_time = time() - tstart
     fpr, tpr, thresholds = roc_curve(y_test, scoring)
     auc_score = auc(fpr, tpr)
-    label = ('%s (AUC: %0.3f, train_time= %0.2fs, '
-             'test_time= %0.2fs)' % (dat, auc_score, fit_time, predict_time))
+    label = "%s (AUC: %0.3f, train_time= %0.2fs, test_time= %0.2fs)" % (
+        dat,
+        auc_score,
+        fit_time,
+        predict_time,
+    )
     # Print AUC score and train/test time:
     print(label)
     ax_roc.plot(fpr, tpr, lw=1, label=label)
@@ -152,9 +156,9 @@

 ax_roc.set_xlim([-0.05, 1.05])
 ax_roc.set_ylim([-0.05, 1.05])
-ax_roc.set_xlabel('False Positive Rate')
-ax_roc.set_ylabel('True Positive Rate')
-ax_roc.set_title('Receiver operating characteristic (ROC) curves')
+ax_roc.set_xlabel("False Positive Rate")
+ax_roc.set_ylabel("True Positive Rate")
+ax_roc.set_title("Receiver operating characteristic (ROC) curves")
 ax_roc.legend(loc="lower right")
 fig_roc.tight_layout()
 plt.show()
('benchmarks', 'bench_plot_omp_lars.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -10,7 +10,7 @@
 import numpy as np

 from sklearn.linear_model import lars_path, lars_path_gram, orthogonal_mp
-from sklearn.datasets.samples_generator import make_sparse_coded_signal
+from sklearn.datasets import make_sparse_coded_signal


 def compute_bench(samples_range, features_range):
@@ -28,9 +28,9 @@
         for i_f, n_features in enumerate(features_range):
             it += 1
             n_informative = n_features / 10
-            print('====================')
-            print('Iteration %03d of %03d' % (it, max_it))
-            print('====================')
+            print("====================")
+            print("Iteration %03d of %03d" % (it, max_it))
+            print("====================")
             # dataset_kwargs = {
             #     'n_train_samples': n_samples,
             #     'n_test_samples': 2,
@@ -41,11 +41,12 @@
             #     'bias': 0.0,
             # }
             dataset_kwargs = {
-                'n_samples': 1,
-                'n_components': n_features,
-                'n_features': n_samples,
-                'n_nonzero_coefs': n_informative,
-                'random_state': 0
+                "n_samples": 1,
+                "n_components": n_features,
+                "n_features": n_samples,
+                "n_nonzero_coefs": n_informative,
+                "random_state": 0,
+                "data_transposed": True,
             }
             print("n_samples: %d" % n_samples)
             print("n_features: %d" % n_features)
@@ -53,19 +54,18 @@
             X = np.asfortranarray(X)

             gc.collect()
-            print("benchmarking lars_path (with Gram):", end='')
+            print("benchmarking lars_path (with Gram):", end="")
             sys.stdout.flush()
             tstart = time()
             G = np.dot(X.T, X)  # precomputed Gram matrix
             Xy = np.dot(X.T, y)
-            lars_path_gram(Xy=Xy, Gram=G, n_samples=y.size,
-                           max_iter=n_informative)
+            lars_path_gram(Xy=Xy, Gram=G, n_samples=y.size, max_iter=n_informative)
             delta = time() - tstart
             print("%0.3fs" % delta)
             lars_gram[i_f, i_s] = delta

             gc.collect()
-            print("benchmarking lars_path (without Gram):", end='')
+            print("benchmarking lars_path (without Gram):", end="")
             sys.stdout.flush()
             tstart = time()
             lars_path(X, y, Gram=None, max_iter=n_informative)
@@ -74,49 +74,48 @@
             lars[i_f, i_s] = delta

             gc.collect()
-            print("benchmarking orthogonal_mp (with Gram):", end='')
+            print("benchmarking orthogonal_mp (with Gram):", end="")
             sys.stdout.flush()
             tstart = time()
-            orthogonal_mp(X, y, precompute=True,
-                          n_nonzero_coefs=n_informative)
+            orthogonal_mp(X, y, precompute=True, n_nonzero_coefs=n_informative)
             delta = time() - tstart
             print("%0.3fs" % delta)
             omp_gram[i_f, i_s] = delta

             gc.collect()
-            print("benchmarking orthogonal_mp (without Gram):", end='')
+            print("benchmarking orthogonal_mp (without Gram):", end="")
             sys.stdout.flush()
             tstart = time()
-            orthogonal_mp(X, y, precompute=False,
-                          n_nonzero_coefs=n_informative)
+            orthogonal_mp(X, y, precompute=False, n_nonzero_coefs=n_informative)
             delta = time() - tstart
             print("%0.3fs" % delta)
             omp[i_f, i_s] = delta

-    results['time(LARS) / time(OMP)\n (w/ Gram)'] = (lars_gram / omp_gram)
-    results['time(LARS) / time(OMP)\n (w/o Gram)'] = (lars / omp)
+    results["time(LARS) / time(OMP)\n (w/ Gram)"] = lars_gram / omp_gram
+    results["time(LARS) / time(OMP)\n (w/o Gram)"] = lars / omp
     return results


-if __name__ == '__main__':
-    samples_range = np.linspace(1000, 5000, 5).astype(np.int)
-    features_range = np.linspace(1000, 5000, 5).astype(np.int)
+if __name__ == "__main__":
+    samples_range = np.linspace(1000, 5000, 5).astype(int)
+    features_range = np.linspace(1000, 5000, 5).astype(int)
     results = compute_bench(samples_range, features_range)
     max_time = max(np.max(t) for t in results.values())

     import matplotlib.pyplot as plt
-    fig = plt.figure('scikit-learn OMP vs. LARS benchmark results')
+
+    fig = plt.figure("scikit-learn OMP vs. LARS benchmark results")
     for i, (label, timings) in enumerate(sorted(results.items())):
-        ax = fig.add_subplot(1, 2, i+1)
+        ax = fig.add_subplot(1, 2, i + 1)
         vmax = max(1 - timings.min(), -1 + timings.max())
         plt.matshow(timings, fignum=False, vmin=1 - vmax, vmax=1 + vmax)
-        ax.set_xticklabels([''] + [str(each) for each in samples_range])
-        ax.set_yticklabels([''] + [str(each) for each in features_range])
-        plt.xlabel('n_samples')
-        plt.ylabel('n_features')
+        ax.set_xticklabels([""] + [str(each) for each in samples_range])
+        ax.set_yticklabels([""] + [str(each) for each in features_range])
+        plt.xlabel("n_samples")
+        plt.ylabel("n_features")
         plt.title(label)

     plt.subplots_adjust(0.1, 0.08, 0.96, 0.98, 0.4, 0.63)
     ax = plt.axes([0.1, 0.08, 0.8, 0.06])
-    plt.colorbar(cax=ax, orientation='horizontal')
+    plt.colorbar(cax=ax, orientation="horizontal")
     plt.show()
('benchmarks', 'bench_lasso.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -15,7 +15,7 @@
 from time import time
 import numpy as np

-from sklearn.datasets.samples_generator import make_regression
+from sklearn.datasets import make_regression


 def compute_bench(alpha, n_samples, n_features, precompute):
@@ -27,29 +27,32 @@
     for ns in n_samples:
         for nf in n_features:
             it += 1
-            print('==================')
-            print('Iteration %s of %s' % (it, max(len(n_samples),
-                                          len(n_features))))
-            print('==================')
+            print("==================")
+            print("Iteration %s of %s" % (it, max(len(n_samples), len(n_features))))
+            print("==================")
             n_informative = nf // 10
-            X, Y, coef_ = make_regression(n_samples=ns, n_features=nf,
-                                          n_informative=n_informative,
-                                          noise=0.1, coef=True)
+            X, Y, coef_ = make_regression(
+                n_samples=ns,
+                n_features=nf,
+                n_informative=n_informative,
+                noise=0.1,
+                coef=True,
+            )

-            X /= np.sqrt(np.sum(X ** 2, axis=0))  # Normalize data
+            X /= np.sqrt(np.sum(X**2, axis=0))  # Normalize data

             gc.collect()
             print("- benchmarking Lasso")
-            clf = Lasso(alpha=alpha, fit_intercept=False,
-                        precompute=precompute)
+            clf = Lasso(alpha=alpha, fit_intercept=False, precompute=precompute)
             tstart = time()
             clf.fit(X, Y)
             lasso_results.append(time() - tstart)

             gc.collect()
             print("- benchmarking LassoLars")
-            clf = LassoLars(alpha=alpha, fit_intercept=False,
-                            normalize=False, precompute=precompute)
+            clf = LassoLars(
+                alpha=alpha, fit_intercept=False, normalize=False, precompute=precompute
+            )
             tstart = time()
             clf.fit(X, Y)
             lars_lasso_results.append(time() - tstart)
@@ -57,40 +60,39 @@
     return lasso_results, lars_lasso_results


-if __name__ == '__main__':
+if __name__ == "__main__":
     from sklearn.linear_model import Lasso, LassoLars
     import matplotlib.pyplot as plt

     alpha = 0.01  # regularization parameter

     n_features = 10
-    list_n_samples = np.linspace(100, 1000000, 5).astype(np.int)
-    lasso_results, lars_lasso_results = compute_bench(alpha, list_n_samples,
-                                            [n_features], precompute=True)
+    list_n_samples = np.linspace(100, 1000000, 5).astype(int)
+    lasso_results, lars_lasso_results = compute_bench(
+        alpha, list_n_samples, [n_features], precompute=True
+    )

-    plt.figure('scikit-learn LASSO benchmark results')
+    plt.figure("scikit-learn LASSO benchmark results")
     plt.subplot(211)
-    plt.plot(list_n_samples, lasso_results, 'b-',
-                            label='Lasso')
-    plt.plot(list_n_samples, lars_lasso_results, 'r-',
-                            label='LassoLars')
-    plt.title('precomputed Gram matrix, %d features, alpha=%s' % (n_features,
-                            alpha))
-    plt.legend(loc='upper left')
-    plt.xlabel('number of samples')
-    plt.ylabel('Time (s)')
-    plt.axis('tight')
+    plt.plot(list_n_samples, lasso_results, "b-", label="Lasso")
+    plt.plot(list_n_samples, lars_lasso_results, "r-", label="LassoLars")
+    plt.title("precomputed Gram matrix, %d features, alpha=%s" % (n_features, alpha))
+    plt.legend(loc="upper left")
+    plt.xlabel("number of samples")
+    plt.ylabel("Time (s)")
+    plt.axis("tight")

     n_samples = 2000
-    list_n_features = np.linspace(500, 3000, 5).astype(np.int)
-    lasso_results, lars_lasso_results = compute_bench(alpha, [n_samples],
-                                           list_n_features, precompute=False)
+    list_n_features = np.linspace(500, 3000, 5).astype(int)
+    lasso_results, lars_lasso_results = compute_bench(
+        alpha, [n_samples], list_n_features, precompute=False
+    )
     plt.subplot(212)
-    plt.plot(list_n_features, lasso_results, 'b-', label='Lasso')
-    plt.plot(list_n_features, lars_lasso_results, 'r-', label='LassoLars')
-    plt.title('%d samples, alpha=%s' % (n_samples, alpha))
-    plt.legend(loc='upper left')
-    plt.xlabel('number of features')
-    plt.ylabel('Time (s)')
-    plt.axis('tight')
+    plt.plot(list_n_features, lasso_results, "b-", label="Lasso")
+    plt.plot(list_n_features, lars_lasso_results, "r-", label="LassoLars")
+    plt.title("%d samples, alpha=%s" % (n_samples, alpha))
+    plt.legend(loc="upper left")
+    plt.xlabel("number of features")
+    plt.ylabel("Time (s)")
+    plt.axis("tight")
     plt.show()
('benchmarks', 'bench_plot_svd.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -9,7 +9,7 @@

 from scipy.linalg import svd
 from sklearn.utils.extmath import randomized_svd
-from sklearn.datasets.samples_generator import make_low_rank_matrix
+from sklearn.datasets import make_low_rank_matrix


 def compute_bench(samples_range, features_range, n_iter=3, rank=50):
@@ -22,61 +22,58 @@
     for n_samples in samples_range:
         for n_features in features_range:
             it += 1
-            print('====================')
-            print('Iteration %03d of %03d' % (it, max_it))
-            print('====================')
-            X = make_low_rank_matrix(n_samples, n_features,
-                                  effective_rank=rank,
-                                  tail_strength=0.2)
+            print("====================")
+            print("Iteration %03d of %03d" % (it, max_it))
+            print("====================")
+            X = make_low_rank_matrix(
+                n_samples, n_features, effective_rank=rank, tail_strength=0.2
+            )

             gc.collect()
             print("benchmarking scipy svd: ")
             tstart = time()
             svd(X, full_matrices=False)
-            results['scipy svd'].append(time() - tstart)
+            results["scipy svd"].append(time() - tstart)

             gc.collect()
             print("benchmarking scikit-learn randomized_svd: n_iter=0")
             tstart = time()
             randomized_svd(X, rank, n_iter=0)
-            results['scikit-learn randomized_svd (n_iter=0)'].append(
-                time() - tstart)
+            results["scikit-learn randomized_svd (n_iter=0)"].append(time() - tstart)

             gc.collect()
-            print("benchmarking scikit-learn randomized_svd: n_iter=%d "
-                  % n_iter)
+            print("benchmarking scikit-learn randomized_svd: n_iter=%d " % n_iter)
             tstart = time()
             randomized_svd(X, rank, n_iter=n_iter)
-            results['scikit-learn randomized_svd (n_iter=%d)'
-                    % n_iter].append(time() - tstart)
+            results["scikit-learn randomized_svd (n_iter=%d)" % n_iter].append(
+                time() - tstart
+            )

     return results


-if __name__ == '__main__':
-    from mpl_toolkits.mplot3d import axes3d  # register the 3d projection
+if __name__ == "__main__":
+    from mpl_toolkits.mplot3d import axes3d  # noqa register the 3d projection
     import matplotlib.pyplot as plt

-    samples_range = np.linspace(2, 1000, 4).astype(np.int)
-    features_range = np.linspace(2, 1000, 4).astype(np.int)
+    samples_range = np.linspace(2, 1000, 4).astype(int)
+    features_range = np.linspace(2, 1000, 4).astype(int)
     results = compute_bench(samples_range, features_range)

-    label = 'scikit-learn singular value decomposition benchmark results'
+    label = "scikit-learn singular value decomposition benchmark results"
     fig = plt.figure(label)
-    ax = fig.gca(projection='3d')
-    for c, (label, timings) in zip('rbg', sorted(results.items())):
+    ax = fig.gca(projection="3d")
+    for c, (label, timings) in zip("rbg", sorted(results.items())):
         X, Y = np.meshgrid(samples_range, features_range)
-        Z = np.asarray(timings).reshape(samples_range.shape[0],
-                                        features_range.shape[0])
+        Z = np.asarray(timings).reshape(samples_range.shape[0], features_range.shape[0])
         # plot the actual surface
-        ax.plot_surface(X, Y, Z, rstride=8, cstride=8, alpha=0.3,
-                        color=c)
+        ax.plot_surface(X, Y, Z, rstride=8, cstride=8, alpha=0.3, color=c)
         # dummy point plot to stick the legend to since surface plot do not
         # support legends (yet?)
         ax.plot([1], [1], [1], color=c, label=label)

-    ax.set_xlabel('n_samples')
-    ax.set_ylabel('n_features')
-    ax.set_zlabel('Time (s)')
+    ax.set_xlabel("n_samples")
+    ax.set_ylabel("n_features")
+    ax.set_zlabel("Time (s)")
     ax.legend()
     plt.show()
('benchmarks', 'bench_feature_expansions.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -11,8 +11,9 @@
 densities = np.array([0.01, 0.1, 1.0])
 csr_times = {d: np.zeros(len(dimensionalities)) for d in densities}
 dense_times = {d: np.zeros(len(dimensionalities)) for d in densities}
-transform = PolynomialFeatures(degree=degree, include_bias=False,
-                               interaction_only=False)
+transform = PolynomialFeatures(
+    degree=degree, include_bias=False, interaction_only=False
+)

 for trial in range(trials):
     for density in densities:
@@ -35,15 +36,22 @@
 fig, axes = plt.subplots(nrows=len(densities), ncols=1, figsize=(8, 10))
 for density, ax in zip(densities, axes):

-    ax.plot(dimensionalities, csr_times[density] / trials,
-            label='csr', linestyle=csr_linestyle)
-    ax.plot(dimensionalities, dense_times[density] / trials,
-            label='dense', linestyle=dense_linestyle)
-    ax.set_title("density %0.2f, degree=%d, n_samples=%d" %
-                 (density, degree, num_rows))
+    ax.plot(
+        dimensionalities,
+        csr_times[density] / trials,
+        label="csr",
+        linestyle=csr_linestyle,
+    )
+    ax.plot(
+        dimensionalities,
+        dense_times[density] / trials,
+        label="dense",
+        linestyle=dense_linestyle,
+    )
+    ax.set_title("density %0.2f, degree=%d, n_samples=%d" % (density, degree, num_rows))
     ax.legend()
-    ax.set_xlabel('Dimensionality')
-    ax.set_ylabel('Time (seconds)')
+    ax.set_xlabel("Dimensionality")
+    ax.set_ylabel("Time (seconds)")

 plt.tight_layout()
 plt.show()
('benchmarks', 'bench_sample_without_replacement.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -16,7 +16,7 @@


 def compute_time(t_start, delta):
-    mu_second = 0.0 + 10 ** 6  # number of microseconds in a second
+    mu_second = 0.0 + 10**6  # number of microseconds in a second

     return delta.seconds + delta.microseconds / mu_second

@@ -26,38 +26,57 @@
     # start time
     t_start = datetime.now()
     sampling(n_population, n_samples)
-    delta = (datetime.now() - t_start)
+    delta = datetime.now() - t_start
     # stop time
     time = compute_time(t_start, delta)
     return time

+
 if __name__ == "__main__":
     ###########################################################################
     # Option parser
     ###########################################################################
     op = optparse.OptionParser()
-    op.add_option("--n-times",
-                  dest="n_times", default=5, type=int,
-                  help="Benchmark results are average over n_times experiments")
-
-    op.add_option("--n-population",
-                  dest="n_population", default=100000, type=int,
-                  help="Size of the population to sample from.")
-
-    op.add_option("--n-step",
-                  dest="n_steps", default=5, type=int,
-                  help="Number of step interval between 0 and n_population.")
-
-    default_algorithms = "custom-tracking-selection,custom-auto," \
-                         "custom-reservoir-sampling,custom-pool,"\
-                         "python-core-sample,numpy-permutation"
-
-    op.add_option("--algorithm",
-                  dest="selected_algorithm",
-                  default=default_algorithms,
-                  type=str,
-                  help="Comma-separated list of transformer to benchmark. "
-                       "Default: %default. \nAvailable: %default")
+    op.add_option(
+        "--n-times",
+        dest="n_times",
+        default=5,
+        type=int,
+        help="Benchmark results are average over n_times experiments",
+    )
+
+    op.add_option(
+        "--n-population",
+        dest="n_population",
+        default=100000,
+        type=int,
+        help="Size of the population to sample from.",
+    )
+
+    op.add_option(
+        "--n-step",
+        dest="n_steps",
+        default=5,
+        type=int,
+        help="Number of step interval between 0 and n_population.",
+    )
+
+    default_algorithms = (
+        "custom-tracking-selection,custom-auto,"
+        "custom-reservoir-sampling,custom-pool,"
+        "python-core-sample,numpy-permutation"
+    )
+
+    op.add_option(
+        "--algorithm",
+        dest="selected_algorithm",
+        default=default_algorithms,
+        type=str,
+        help=(
+            "Comma-separated list of transformer to benchmark. "
+            "Default: %default. \nAvailable: %default"
+        ),
+    )

     # op.add_option("--random-seed",
     #               dest="random_seed", default=13, type=int,
@@ -68,11 +87,13 @@
         op.error("this script takes no arguments.")
         sys.exit(1)

-    selected_algorithm = opts.selected_algorithm.split(',')
+    selected_algorithm = opts.selected_algorithm.split(",")
     for key in selected_algorithm:
-        if key not in default_algorithms.split(','):
-            raise ValueError("Unknown sampling algorithm \"%s\" not in (%s)."
-                             % (key, default_algorithms))
+        if key not in default_algorithms.split(","):
+            raise ValueError(
+                'Unknown sampling algorithm "%s" not in (%s).'
+                % (key, default_algorithms)
+            )

     ###########################################################################
     # List sampling algorithm
@@ -84,66 +105,67 @@

     ###########################################################################
     # Set Python core input
-    sampling_algorithm["python-core-sample"] = \
-        lambda n_population, n_sample: \
-        random.sample(range(n_population), n_sample)
+    sampling_algorithm[
+        "python-core-sample"
+    ] = lambda n_population, n_sample: random.sample(range(n_population), n_sample)

     ###########################################################################
     # Set custom automatic method selection
-    sampling_algorithm["custom-auto"] = \
-        lambda n_population, n_samples, random_state=None: \
-        sample_without_replacement(n_population, n_samples, method="auto",
-                                   random_state=random_state)
+    sampling_algorithm[
+        "custom-auto"
+    ] = lambda n_population, n_samples, random_state=None: sample_without_replacement(
+        n_population, n_samples, method="auto", random_state=random_state
+    )

     ###########################################################################
     # Set custom tracking based method
-    sampling_algorithm["custom-tracking-selection"] = \
-        lambda n_population, n_samples, random_state=None: \
-        sample_without_replacement(n_population,
-                                   n_samples,
-                                   method="tracking_selection",
-                                   random_state=random_state)
+    sampling_algorithm[
+        "custom-tracking-selection"
+    ] = lambda n_population, n_samples, random_state=None: sample_without_replacement(
+        n_population, n_samples, method="tracking_selection", random_state=random_state
+    )

     ###########################################################################
     # Set custom reservoir based method
-    sampling_algorithm["custom-reservoir-sampling"] = \
-        lambda n_population, n_samples, random_state=None: \
-        sample_without_replacement(n_population,
-                                   n_samples,
-                                   method="reservoir_sampling",
-                                   random_state=random_state)
+    sampling_algorithm[
+        "custom-reservoir-sampling"
+    ] = lambda n_population, n_samples, random_state=None: sample_without_replacement(
+        n_population, n_samples, method="reservoir_sampling", random_state=random_state
+    )

     ###########################################################################
     # Set custom reservoir based method
-    sampling_algorithm["custom-pool"] = \
-        lambda n_population, n_samples, random_state=None: \
-        sample_without_replacement(n_population,
-                                   n_samples,
-                                   method="pool",
-                                   random_state=random_state)
+    sampling_algorithm[
+        "custom-pool"
+    ] = lambda n_population, n_samples, random_state=None: sample_without_replacement(
+        n_population, n_samples, method="pool", random_state=random_state
+    )

     ###########################################################################
     # Numpy permutation based
-    sampling_algorithm["numpy-permutation"] = \
-        lambda n_population, n_sample: \
-        np.random.permutation(n_population)[:n_sample]
+    sampling_algorithm[
+        "numpy-permutation"
+    ] = lambda n_population, n_sample: np.random.permutation(n_population)[:n_sample]

     ###########################################################################
     # Remove unspecified algorithm
-    sampling_algorithm = {key: value
-                          for key, value in sampling_algorithm.items()
-                          if key in selected_algorithm}
+    sampling_algorithm = {
+        key: value
+        for key, value in sampling_algorithm.items()
+        if key in selected_algorithm
+    }

     ###########################################################################
     # Perform benchmark
     ###########################################################################
     time = {}
-    n_samples = np.linspace(start=0, stop=opts.n_population,
-        num=opts.n_steps).astype(np.int)
+    n_samples = np.linspace(start=0, stop=opts.n_population, num=opts.n_steps).astype(
+        int
+    )

     ratio = n_samples / opts.n_population

-    print('Benchmarks')
+    print("Benchmarks")
     print("===========================")

     for name in sorted(sampling_algorithm):
@@ -152,9 +174,9 @@

         for step in range(opts.n_steps):
             for it in range(opts.n_times):
-                time[name][step, it] = bench_sample(sampling_algorithm[name],
-                                                    opts.n_population,
-                                                    n_samples[step])
+                time[name][step, it] = bench_sample(
+                    sampling_algorithm[name], opts.n_population, n_samples[step]
+                )

         print("done")

@@ -168,12 +190,16 @@
     print("Script arguments")
     print("===========================")
     arguments = vars(opts)
-    print("%s \t | %s " % ("Arguments".ljust(16),
-                           "Value".center(12),))
+    print(
+        "%s \t | %s "
+        % (
+            "Arguments".ljust(16),
+            "Value".center(12),
+        )
+    )
     print(25 * "-" + ("|" + "-" * 14) * 1)
     for key, value in arguments.items():
-        print("%s \t | %s " % (str(key).ljust(16),
-                               str(value).strip().center(12)))
+        print("%s \t | %s " % (str(key).ljust(16), str(value).strip().center(12)))
     print("")

     print("Sampling algorithm performance:")
@@ -181,15 +207,14 @@
     print("Results are averaged over %s repetition(s)." % opts.n_times)
     print("")

-    fig = plt.figure('scikit-learn sample w/o replacement benchmark results')
-    plt.title("n_population = %s, n_times = %s" %
-              (opts.n_population, opts.n_times))
+    fig = plt.figure("scikit-learn sample w/o replacement benchmark results")
+    plt.title("n_population = %s, n_times = %s" % (opts.n_population, opts.n_times))
     ax = fig.add_subplot(111)
     for name in sampling_algorithm:
         ax.plot(ratio, time[name], label=name)

-    ax.set_xlabel('ratio of n_sample / n_population')
-    ax.set_ylabel('Time (s)')
+    ax.set_xlabel("ratio of n_sample / n_population")
+    ax.set_ylabel("Time (s)")
     ax.legend()

     # Sort legend labels
('benchmarks', 'bench_covertype.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -25,13 +25,13 @@

 The same task has been used in a number of papers including:

- * `"SVM Optimization: Inverse Dependence on Training Set Size"
-   <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.139.2112>`_
+ * :doi:`"SVM Optimization: Inverse Dependence on Training Set Size"
    S. Shalev-Shwartz, N. Srebro - In Proceedings of ICML '08.
-
- * `"Pegasos: Primal estimated sub-gradient solver for svm"
-   <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.74.8513>`_
+   <10.1145/1390156.1390273>`
+
+ * :doi:`"Pegasos: Primal estimated sub-gradient solver for svm"
    S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML '07.
+   <10.1145/1273496.1273598>`

  * `"Training Linear SVMs in Linear Time"
    <https://www.cs.cornell.edu/people/tj/publications/joachims_06a.pdf>`_
@@ -63,20 +63,22 @@

 # Memoize the data extraction and memory map the resulting
 # train / test splits in readonly mode
-memory = Memory(os.path.join(get_data_home(), 'covertype_benchmark_data'),
-                mmap_mode='r')
+memory = Memory(
+    os.path.join(get_data_home(), "covertype_benchmark_data"), mmap_mode="r"
+)


 @memory.cache
-def load_data(dtype=np.float32, order='C', random_state=13):
+def load_data(dtype=np.float32, order="C", random_state=13):
     """Load the data, then cache and memmap the train/test split"""
     ######################################################################
     # Load dataset
     print("Loading dataset...")
-    data = fetch_covtype(download_if_missing=True, shuffle=True,
-                         random_state=random_state)
-    X = check_array(data['data'], dtype=dtype, order=order)
-    y = (data['target'] != 1).astype(np.int)
+    data = fetch_covtype(
+        download_if_missing=True, shuffle=True, random_state=random_state
+    )
+    X = check_array(data["data"], dtype=dtype, order=order)
+    y = (data["target"] != 1).astype(int)

     # Create train-test split (as [Joachims, 2006])
     print("Creating train-test split...")
@@ -97,39 +99,59 @@


 ESTIMATORS = {
-    'GBRT': GradientBoostingClassifier(n_estimators=250),
-    'ExtraTrees': ExtraTreesClassifier(n_estimators=20),
-    'RandomForest': RandomForestClassifier(n_estimators=20),
-    'CART': DecisionTreeClassifier(min_samples_split=5),
-    'SGD': SGDClassifier(alpha=0.001, max_iter=1000, tol=1e-3),
-    'GaussianNB': GaussianNB(),
-    'liblinear': LinearSVC(loss="l2", penalty="l2", C=1000, dual=False,
-                           tol=1e-3),
-    'SAG': LogisticRegression(solver='sag', max_iter=2, C=1000)
+    "GBRT": GradientBoostingClassifier(n_estimators=250),
+    "ExtraTrees": ExtraTreesClassifier(n_estimators=20),
+    "RandomForest": RandomForestClassifier(n_estimators=20),
+    "CART": DecisionTreeClassifier(min_samples_split=5),
+    "SGD": SGDClassifier(alpha=0.001),
+    "GaussianNB": GaussianNB(),
+    "liblinear": LinearSVC(loss="l2", penalty="l2", C=1000, dual=False, tol=1e-3),
+    "SAG": LogisticRegression(solver="sag", max_iter=2, C=1000),
 }


 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
-    parser.add_argument('--classifiers', nargs="+",
-                        choices=ESTIMATORS, type=str,
-                        default=['liblinear', 'GaussianNB', 'SGD', 'CART'],
-                        help="list of classifiers to benchmark.")
-    parser.add_argument('--n-jobs', nargs="?", default=1, type=int,
-                        help="Number of concurrently running workers for "
-                             "models that support parallelism.")
-    parser.add_argument('--order', nargs="?", default="C", type=str,
-                        choices=["F", "C"],
-                        help="Allow to choose between fortran and C ordered "
-                             "data")
-    parser.add_argument('--random-seed', nargs="?", default=13, type=int,
-                        help="Common seed used by random number generator.")
+    parser.add_argument(
+        "--classifiers",
+        nargs="+",
+        choices=ESTIMATORS,
+        type=str,
+        default=["liblinear", "GaussianNB", "SGD", "CART"],
+        help="list of classifiers to benchmark.",
+    )
+    parser.add_argument(
+        "--n-jobs",
+        nargs="?",
+        default=1,
+        type=int,
+        help=(
+            "Number of concurrently running workers for "
+            "models that support parallelism."
+        ),
+    )
+    parser.add_argument(
+        "--order",
+        nargs="?",
+        default="C",
+        type=str,
+        choices=["F", "C"],
+        help="Allow to choose between fortran and C ordered data",
+    )
+    parser.add_argument(
+        "--random-seed",
+        nargs="?",
+        default=13,
+        type=int,
+        help="Common seed used by random number generator.",
+    )
     args = vars(parser.parse_args())

     print(__doc__)

     X_train, X_test, y_train, y_test = load_data(
-        order=args["order"], random_state=args["random_seed"])
+        order=args["order"], random_state=args["random_seed"]
+    )

     print("")
     print("Dataset statistics:")
@@ -137,14 +159,26 @@
     print("%s %d" % ("number of features:".ljust(25), X_train.shape[1]))
     print("%s %d" % ("number of classes:".ljust(25), np.unique(y_train).size))
     print("%s %s" % ("data type:".ljust(25), X_train.dtype))
-    print("%s %d (pos=%d, neg=%d, size=%dMB)"
-          % ("number of train samples:".ljust(25),
-             X_train.shape[0], np.sum(y_train == 1),
-             np.sum(y_train == 0), int(X_train.nbytes / 1e6)))
-    print("%s %d (pos=%d, neg=%d, size=%dMB)"
-          % ("number of test samples:".ljust(25),
-             X_test.shape[0], np.sum(y_test == 1),
-             np.sum(y_test == 0), int(X_test.nbytes / 1e6)))
+    print(
+        "%s %d (pos=%d, neg=%d, size=%dMB)"
+        % (
+            "number of train samples:".ljust(25),
+            X_train.shape[0],
+            np.sum(y_train == 1),
+            np.sum(y_train == 0),
+            int(X_train.nbytes / 1e6),
+        )
+    )
+    print(
+        "%s %d (pos=%d, neg=%d, size=%dMB)"
+        % (
+            "number of test samples:".ljust(25),
+            X_test.shape[0],
+            np.sum(y_test == 1),
+            np.sum(y_test == 0),
+            int(X_test.nbytes / 1e6),
+        )
+    )

     print()
     print("Training Classifiers")
@@ -155,9 +189,13 @@
         estimator = ESTIMATORS[name]
         estimator_params = estimator.get_params()

-        estimator.set_params(**{p: args["random_seed"]
-                                for p in estimator_params
-                                if p.endswith("random_state")})
+        estimator.set_params(
+            **{
+                p: args["random_seed"]
+                for p in estimator_params
+                if p.endswith("random_state")
+            }
+        )

         if "n_jobs" in estimator_params:
             estimator.set_params(n_jobs=args["n_jobs"])
@@ -177,13 +215,17 @@
     print()
     print("Classification performance:")
     print("===========================")
-    print("%s %s %s %s"
-          % ("Classifier  ", "train-time", "test-time", "error-rate"))
+    print("%s %s %s %s" % ("Classifier  ", "train-time", "test-time", "error-rate"))
     print("-" * 44)
     for name in sorted(args["classifiers"], key=error.get):
-        print("%s %s %s %s" % (name.ljust(12),
-                               ("%.4fs" % train_time[name]).center(10),
-                               ("%.4fs" % test_time[name]).center(10),
-                               ("%.4f" % error[name]).center(10)))
+        print(
+            "%s %s %s %s"
+            % (
+                name.ljust(12),
+                ("%.4fs" % train_time[name]).center(10),
+                ("%.4fs" % test_time[name]).center(10),
+                ("%.4f" % error[name]).center(10),
+            )
+        )

     print()
('benchmarks', 'bench_isotonic.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -20,8 +20,7 @@


 def generate_perturbed_logarithm_dataset(size):
-    return (np.random.randint(-50, 50, size=size) +
-            50. * np.log(1 + np.arange(size)))
+    return np.random.randint(-50, 50, size=size) + 50.0 * np.log(1 + np.arange(size))


 def generate_logistic_dataset(size):
@@ -31,15 +30,15 @@

 def generate_pathological_dataset(size):
     # Triggers O(n^2) complexity on the original implementation.
-    return np.r_[np.arange(size),
-                 np.arange(-(size - 1), size),
-                 np.arange(-(size - 1), 1)]
+    return np.r_[
+        np.arange(size), np.arange(-(size - 1), size), np.arange(-(size - 1), 1)
+    ]


 DATASET_GENERATORS = {
-    'perturbed_logarithm': generate_perturbed_logarithm_dataset,
-    'logistic': generate_logistic_dataset,
-    'pathological': generate_pathological_dataset,
+    "perturbed_logarithm": generate_perturbed_logarithm_dataset,
+    "logistic": generate_logistic_dataset,
+    "pathological": generate_pathological_dataset,
 }


@@ -55,34 +54,43 @@
     return (datetime.now() - tstart).total_seconds()


-if __name__ == '__main__':
-    parser = argparse.ArgumentParser(
-        description="Isotonic Regression benchmark tool")
-    parser.add_argument('--seed', type=int,
-                        help="RNG seed")
-    parser.add_argument('--iterations', type=int, required=True,
-                        help="Number of iterations to average timings over "
-                        "for each problem size")
-    parser.add_argument('--log_min_problem_size', type=int, required=True,
-                        help="Base 10 logarithm of the minimum problem size")
-    parser.add_argument('--log_max_problem_size', type=int, required=True,
-                        help="Base 10 logarithm of the maximum problem size")
-    parser.add_argument('--show_plot', action='store_true',
-                        help="Plot timing output with matplotlib")
-    parser.add_argument('--dataset', choices=DATASET_GENERATORS.keys(),
-                        required=True)
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser(description="Isotonic Regression benchmark tool")
+    parser.add_argument("--seed", type=int, help="RNG seed")
+    parser.add_argument(
+        "--iterations",
+        type=int,
+        required=True,
+        help="Number of iterations to average timings over for each problem size",
+    )
+    parser.add_argument(
+        "--log_min_problem_size",
+        type=int,
+        required=True,
+        help="Base 10 logarithm of the minimum problem size",
+    )
+    parser.add_argument(
+        "--log_max_problem_size",
+        type=int,
+        required=True,
+        help="Base 10 logarithm of the maximum problem size",
+    )
+    parser.add_argument(
+        "--show_plot", action="store_true", help="Plot timing output with matplotlib"
+    )
+    parser.add_argument("--dataset", choices=DATASET_GENERATORS.keys(), required=True)

     args = parser.parse_args()

     np.random.seed(args.seed)

     timings = []
-    for exponent in range(args.log_min_problem_size,
-                          args.log_max_problem_size):
-        n = 10 ** exponent
+    for exponent in range(args.log_min_problem_size, args.log_max_problem_size):
+        n = 10**exponent
         Y = DATASET_GENERATORS[args.dataset](n)
-        time_per_iteration = \
-            [bench_isotonic_regression(Y) for i in range(args.iterations)]
+        time_per_iteration = [
+            bench_isotonic_regression(Y) for i in range(args.iterations)
+        ]
         timing = (n, np.mean(time_per_iteration))
         timings.append(timing)

@@ -93,8 +101,8 @@
     if args.show_plot:
         plt.plot(*zip(*timings))
         plt.title("Average time taken running isotonic regression")
-        plt.xlabel('Number of observations')
-        plt.ylabel('Time (s)')
-        plt.axis('tight')
+        plt.xlabel("Number of observations")
+        plt.ylabel("Time (s)")
+        plt.axis("tight")
         plt.loglog()
         plt.show()
('benchmarks', 'bench_hist_gradient_boosting.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -2,33 +2,46 @@
 import argparse

 import matplotlib.pyplot as plt
+import numpy as np
 from sklearn.model_selection import train_test_split
-# To use this experimental feature, we need to explicitly ask for it:
-from sklearn.experimental import enable_hist_gradient_boosting  # noqa
 from sklearn.ensemble import HistGradientBoostingRegressor
 from sklearn.ensemble import HistGradientBoostingClassifier
 from sklearn.datasets import make_classification
 from sklearn.datasets import make_regression
-from sklearn.ensemble._hist_gradient_boosting.utils import (
-    get_equivalent_estimator)
+from sklearn.ensemble._hist_gradient_boosting.utils import get_equivalent_estimator


 parser = argparse.ArgumentParser()
-parser.add_argument('--n-leaf-nodes', type=int, default=31)
-parser.add_argument('--n-trees', type=int, default=10)
-parser.add_argument('--lightgbm', action="store_true", default=False,
-                    help='also plot lightgbm')
-parser.add_argument('--xgboost', action="store_true", default=False,
-                    help='also plot xgboost')
-parser.add_argument('--catboost', action="store_true", default=False,
-                    help='also plot catboost')
-parser.add_argument('--learning-rate', type=float, default=.1)
-parser.add_argument('--problem', type=str, default='classification',
-                    choices=['classification', 'regression'])
-parser.add_argument('--n-classes', type=int, default=2)
-parser.add_argument('--n-samples-max', type=int, default=int(1e6))
-parser.add_argument('--n-features', type=int, default=20)
-parser.add_argument('--max-bins', type=int, default=255)
+parser.add_argument("--n-leaf-nodes", type=int, default=31)
+parser.add_argument("--n-trees", type=int, default=10)
+parser.add_argument(
+    "--lightgbm", action="store_true", default=False, help="also plot lightgbm"
+)
+parser.add_argument(
+    "--xgboost", action="store_true", default=False, help="also plot xgboost"
+)
+parser.add_argument(
+    "--catboost", action="store_true", default=False, help="also plot catboost"
+)
+parser.add_argument("--learning-rate", type=float, default=0.1)
+parser.add_argument(
+    "--problem",
+    type=str,
+    default="classification",
+    choices=["classification", "regression"],
+)
+parser.add_argument("--loss", type=str, default="default")
+parser.add_argument("--missing-fraction", type=float, default=0)
+parser.add_argument("--n-classes", type=int, default=2)
+parser.add_argument("--n-samples-max", type=int, default=int(1e6))
+parser.add_argument("--n-features", type=int, default=20)
+parser.add_argument("--max-bins", type=int, default=255)
+parser.add_argument(
+    "--random-sample-weights",
+    action="store_true",
+    default=False,
+    help="generate and use random sample weights",
+)
 args = parser.parse_args()

 n_leaf_nodes = args.n_leaf_nodes
@@ -38,22 +51,42 @@


 def get_estimator_and_data():
-    if args.problem == 'classification':
-        X, y = make_classification(args.n_samples_max * 2,
-                                   n_features=args.n_features,
-                                   n_classes=args.n_classes,
-                                   n_clusters_per_class=1,
-                                   random_state=0)
+    if args.problem == "classification":
+        X, y = make_classification(
+            args.n_samples_max * 2,
+            n_features=args.n_features,
+            n_classes=args.n_classes,
+            n_clusters_per_class=1,
+            n_informative=args.n_classes,
+            random_state=0,
+        )
         return X, y, HistGradientBoostingClassifier
-    elif args.problem == 'regression':
-        X, y = make_regression(args.n_samples_max * 2,
-                               n_features=args.n_features, random_state=0)
+    elif args.problem == "regression":
+        X, y = make_regression(
+            args.n_samples_max * 2, n_features=args.n_features, random_state=0
+        )
         return X, y, HistGradientBoostingRegressor


 X, y, Estimator = get_estimator_and_data()
-X_train_, X_test_, y_train_, y_test_ = train_test_split(
-    X, y, test_size=0.5, random_state=0)
+if args.missing_fraction:
+    mask = np.random.binomial(1, args.missing_fraction, size=X.shape).astype(bool)
+    X[mask] = np.nan
+
+if args.random_sample_weights:
+    sample_weight = np.random.rand(len(X)) * 10
+else:
+    sample_weight = None
+
+if sample_weight is not None:
+    (X_train_, X_test_, y_train_, y_test_, sample_weight_train_, _) = train_test_split(
+        X, y, sample_weight, test_size=0.5, random_state=0
+    )
+else:
+    X_train_, X_test_, y_train_, y_test_ = train_test_split(
+        X, y, test_size=0.5, random_state=0
+    )
+    sample_weight_train_ = None


 def one_run(n_samples):
@@ -61,20 +94,34 @@
     X_test = X_test_[:n_samples]
     y_train = y_train_[:n_samples]
     y_test = y_test_[:n_samples]
+    if sample_weight is not None:
+        sample_weight_train = sample_weight_train_[:n_samples]
+    else:
+        sample_weight_train = None
     assert X_train.shape[0] == n_samples
     assert X_test.shape[0] == n_samples
-    print("Data size: %d samples train, %d samples test."
-          % (n_samples, n_samples))
+    print("Data size: %d samples train, %d samples test." % (n_samples, n_samples))
     print("Fitting a sklearn model...")
     tic = time()
-    est = Estimator(learning_rate=lr,
-                    max_iter=n_trees,
-                    max_bins=max_bins,
-                    max_leaf_nodes=n_leaf_nodes,
-                    n_iter_no_change=None,
-                    random_state=0,
-                    verbose=0)
-    est.fit(X_train, y_train)
+    est = Estimator(
+        learning_rate=lr,
+        max_iter=n_trees,
+        max_bins=max_bins,
+        max_leaf_nodes=n_leaf_nodes,
+        early_stopping=False,
+        random_state=0,
+        verbose=0,
+    )
+    loss = args.loss
+    if args.problem == "classification":
+        if loss == "default":
+            loss = "log_loss"
+    else:
+        # regression
+        if loss == "default":
+            loss = "squared_error"
+    est.set_params(loss=loss)
+    est.fit(X_train, y_train, sample_weight=sample_weight_train)
     sklearn_fit_duration = time() - tic
     tic = time()
     sklearn_score = est.score(X_test, y_test)
@@ -88,15 +135,12 @@
     lightgbm_score_duration = None
     if args.lightgbm:
         print("Fitting a LightGBM model...")
-        # get_lightgbm does not accept loss='auto'
-        if args.problem == 'classification':
-            loss = 'binary_crossentropy' if args.n_classes == 2 else \
-                'categorical_crossentropy'
-            est.set_params(loss=loss)
-        lightgbm_est = get_equivalent_estimator(est, lib='lightgbm')
-
-        tic = time()
-        lightgbm_est.fit(X_train, y_train)
+        lightgbm_est = get_equivalent_estimator(
+            est, lib="lightgbm", n_classes=args.n_classes
+        )
+
+        tic = time()
+        lightgbm_est.fit(X_train, y_train, sample_weight=sample_weight_train)
         lightgbm_fit_duration = time() - tic
         tic = time()
         lightgbm_score = lightgbm_est.score(X_test, y_test)
@@ -110,15 +154,10 @@
     xgb_score_duration = None
     if args.xgboost:
         print("Fitting an XGBoost model...")
-        # get_xgb does not accept loss='auto'
-        if args.problem == 'classification':
-            loss = 'binary_crossentropy' if args.n_classes == 2 else \
-                'categorical_crossentropy'
-            est.set_params(loss=loss)
-        xgb_est = get_equivalent_estimator(est, lib='xgboost')
-
-        tic = time()
-        xgb_est.fit(X_train, y_train)
+        xgb_est = get_equivalent_estimator(est, lib="xgboost", n_classes=args.n_classes)
+
+        tic = time()
+        xgb_est.fit(X_train, y_train, sample_weight=sample_weight_train)
         xgb_fit_duration = time() - tic
         tic = time()
         xgb_score = xgb_est.score(X_test, y_test)
@@ -132,15 +171,12 @@
     cat_score_duration = None
     if args.catboost:
         print("Fitting a CatBoost model...")
-        # get_cat does not accept loss='auto'
-        if args.problem == 'classification':
-            loss = 'binary_crossentropy' if args.n_classes == 2 else \
-                'categorical_crossentropy'
-            est.set_params(loss=loss)
-        cat_est = get_equivalent_estimator(est, lib='catboost')
-
-        tic = time()
-        cat_est.fit(X_train, y_train)
+        cat_est = get_equivalent_estimator(
+            est, lib="catboost", n_classes=args.n_classes
+        )
+
+        tic = time()
+        cat_est.fit(X_train, y_train, sample_weight=sample_weight_train)
         cat_fit_duration = time() - tic
         tic = time()
         cat_score = cat_est.score(X_test, y_test)
@@ -149,15 +185,26 @@
         print("fit duration: {:.3f}s,".format(cat_fit_duration))
         print("score duration: {:.3f}s,".format(cat_score_duration))

-    return (sklearn_score, sklearn_fit_duration, sklearn_score_duration,
-            lightgbm_score, lightgbm_fit_duration, lightgbm_score_duration,
-            xgb_score, xgb_fit_duration, xgb_score_duration,
-            cat_score, cat_fit_duration, cat_score_duration)
+    return (
+        sklearn_score,
+        sklearn_fit_duration,
+        sklearn_score_duration,
+        lightgbm_score,
+        lightgbm_fit_duration,
+        lightgbm_score_duration,
+        xgb_score,
+        xgb_fit_duration,
+        xgb_score_duration,
+        cat_score,
+        cat_fit_duration,
+        cat_score_duration,
+    )


 n_samples_list = [1000, 10000, 100000, 500000, 1000000, 5000000, 10000000]
-n_samples_list = [n_samples for n_samples in n_samples_list
-                  if n_samples <= args.n_samples_max]
+n_samples_list = [
+    n_samples for n_samples in n_samples_list if n_samples <= args.n_samples_max
+]

 sklearn_scores = []
 sklearn_fit_durations = []
@@ -173,67 +220,70 @@
 cat_score_durations = []

 for n_samples in n_samples_list:
-    (sklearn_score,
-     sklearn_fit_duration,
-     sklearn_score_duration,
-     lightgbm_score,
-     lightgbm_fit_duration,
-     lightgbm_score_duration,
-     xgb_score,
-     xgb_fit_duration,
-     xgb_score_duration,
-     cat_score,
-     cat_fit_duration,
-     cat_score_duration) = one_run(n_samples)
+    (
+        sklearn_score,
+        sklearn_fit_duration,
+        sklearn_score_duration,
+        lightgbm_score,
+        lightgbm_fit_duration,
+        lightgbm_score_duration,
+        xgb_score,
+        xgb_fit_duration,
+        xgb_score_duration,
+        cat_score,
+        cat_fit_duration,
+        cat_score_duration,
+    ) = one_run(n_samples)

     for scores, score in (
-            (sklearn_scores, sklearn_score),
-            (sklearn_fit_durations, sklearn_fit_duration),
-            (sklearn_score_durations, sklearn_score_duration),
-            (lightgbm_scores, lightgbm_score),
-            (lightgbm_fit_durations, lightgbm_fit_duration),
-            (lightgbm_score_durations, lightgbm_score_duration),
-            (xgb_scores, xgb_score),
-            (xgb_fit_durations, xgb_fit_duration),
-            (xgb_score_durations, xgb_score_duration),
-            (cat_scores, cat_score),
-            (cat_fit_durations, cat_fit_duration),
-            (cat_score_durations, cat_score_duration)):
+        (sklearn_scores, sklearn_score),
+        (sklearn_fit_durations, sklearn_fit_duration),
+        (sklearn_score_durations, sklearn_score_duration),
+        (lightgbm_scores, lightgbm_score),
+        (lightgbm_fit_durations, lightgbm_fit_duration),
+        (lightgbm_score_durations, lightgbm_score_duration),
+        (xgb_scores, xgb_score),
+        (xgb_fit_durations, xgb_fit_duration),
+        (xgb_score_durations, xgb_score_duration),
+        (cat_scores, cat_score),
+        (cat_fit_durations, cat_fit_duration),
+        (cat_score_durations, cat_score_duration),
+    ):
         scores.append(score)

 fig, axs = plt.subplots(3, sharex=True)

-axs[0].plot(n_samples_list, sklearn_scores, label='sklearn')
-axs[1].plot(n_samples_list, sklearn_fit_durations, label='sklearn')
-axs[2].plot(n_samples_list, sklearn_score_durations, label='sklearn')
+axs[0].plot(n_samples_list, sklearn_scores, label="sklearn")
+axs[1].plot(n_samples_list, sklearn_fit_durations, label="sklearn")
+axs[2].plot(n_samples_list, sklearn_score_durations, label="sklearn")

 if args.lightgbm:
-    axs[0].plot(n_samples_list, lightgbm_scores, label='lightgbm')
-    axs[1].plot(n_samples_list, lightgbm_fit_durations, label='lightgbm')
-    axs[2].plot(n_samples_list, lightgbm_score_durations, label='lightgbm')
+    axs[0].plot(n_samples_list, lightgbm_scores, label="lightgbm")
+    axs[1].plot(n_samples_list, lightgbm_fit_durations, label="lightgbm")
+    axs[2].plot(n_samples_list, lightgbm_score_durations, label="lightgbm")

 if args.xgboost:
-    axs[0].plot(n_samples_list, xgb_scores, label='XGBoost')
-    axs[1].plot(n_samples_list, xgb_fit_durations, label='XGBoost')
-    axs[2].plot(n_samples_list, xgb_score_durations, label='XGBoost')
+    axs[0].plot(n_samples_list, xgb_scores, label="XGBoost")
+    axs[1].plot(n_samples_list, xgb_fit_durations, label="XGBoost")
+    axs[2].plot(n_samples_list, xgb_score_durations, label="XGBoost")

 if args.catboost:
-    axs[0].plot(n_samples_list, cat_scores, label='CatBoost')
-    axs[1].plot(n_samples_list, cat_fit_durations, label='CatBoost')
-    axs[2].plot(n_samples_list, cat_score_durations, label='CatBoost')
+    axs[0].plot(n_samples_list, cat_scores, label="CatBoost")
+    axs[1].plot(n_samples_list, cat_fit_durations, label="CatBoost")
+    axs[2].plot(n_samples_list, cat_score_durations, label="CatBoost")

 for ax in axs:
-    ax.set_xscale('log')
-    ax.legend(loc='best')
-    ax.set_xlabel('n_samples')
-
-axs[0].set_title('scores')
-axs[1].set_title('fit duration (s)')
-axs[2].set_title('score duration (s)')
+    ax.set_xscale("log")
+    ax.legend(loc="best")
+    ax.set_xlabel("n_samples")
+
+axs[0].set_title("scores")
+axs[1].set_title("fit duration (s)")
+axs[2].set_title("score duration (s)")

 title = args.problem
-if args.problem == 'classification':
-    title += ' n_classes = {}'.format(args.n_classes)
+if args.problem == "classification":
+    title += " n_classes = {}".format(args.n_classes)
 fig.suptitle(title)


('benchmarks', 'bench_plot_parallel_pairwise.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,6 +7,7 @@
 from sklearn.utils import check_random_state
 from sklearn.metrics.pairwise import pairwise_distances
 from sklearn.metrics.pairwise import pairwise_kernels
+

 def plot(func):
     random_state = check_random_state(0)
@@ -25,12 +26,12 @@
         func(X, n_jobs=-1)
         multi_core.append(time.time() - start)

-    plt.figure('scikit-learn parallel %s benchmark results' % func.__name__)
+    plt.figure("scikit-learn parallel %s benchmark results" % func.__name__)
     plt.plot(sample_sizes, one_core, label="one core")
     plt.plot(sample_sizes, multi_core, label="multi core")
-    plt.xlabel('n_samples')
-    plt.ylabel('Time (s)')
-    plt.title('Parallel %s' % func.__name__)
+    plt.xlabel("n_samples")
+    plt.ylabel("Time (s)")
+    plt.title("Parallel %s" % func.__name__)
     plt.legend()


@@ -41,6 +42,7 @@
 def rbf_kernels(X, n_jobs):
     return pairwise_kernels(X, metric="rbf", n_jobs=n_jobs, gamma=0.1)

+
 plot(euclidean_distances)
 plot(rbf_kernels)
 plt.show()
('benchmarks', 'bench_plot_fastkmeans.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -4,7 +4,7 @@
 import numpy as np
 from numpy import random as nr

-from sklearn.cluster.k_means_ import KMeans, MiniBatchKMeans
+from sklearn.cluster import KMeans, MiniBatchKMeans


 def compute_bench(samples_range, features_range):
@@ -17,29 +17,29 @@
     for n_samples in samples_range:
         for n_features in features_range:
             it += 1
-            print('==============================')
-            print('Iteration %03d of %03d' % (it, max_it))
-            print('==============================')
+            print("==============================")
+            print("Iteration %03d of %03d" % (it, max_it))
+            print("==============================")
             print()
             data = nr.randint(-50, 51, (n_samples, n_features))

-            print('K-Means')
+            print("K-Means")
             tstart = time()
-            kmeans = KMeans(init='k-means++', n_clusters=10).fit(data)
+            kmeans = KMeans(init="k-means++", n_clusters=10).fit(data)

             delta = time() - tstart
             print("Speed: %0.3fs" % delta)
             print("Inertia: %0.5f" % kmeans.inertia_)
             print()

-            results['kmeans_speed'].append(delta)
-            results['kmeans_quality'].append(kmeans.inertia_)
+            results["kmeans_speed"].append(delta)
+            results["kmeans_quality"].append(kmeans.inertia_)

-            print('Fast K-Means')
+            print("Fast K-Means")
             # let's prepare the data in small chunks
-            mbkmeans = MiniBatchKMeans(init='k-means++',
-                                       n_clusters=10,
-                                       batch_size=chunk)
+            mbkmeans = MiniBatchKMeans(
+                init="k-means++", n_clusters=10, batch_size=chunk
+            )
             tstart = time()
             mbkmeans.fit(data)
             delta = time() - tstart
@@ -48,8 +48,8 @@
             print()
             print()

-            results['MiniBatchKMeans Speed'].append(delta)
-            results['MiniBatchKMeans Quality'].append(mbkmeans.inertia_)
+            results["MiniBatchKMeans Speed"].append(delta)
+            results["MiniBatchKMeans Quality"].append(mbkmeans.inertia_)

     return results

@@ -57,8 +57,18 @@
 def compute_bench_2(chunks):
     results = defaultdict(lambda: [])
     n_features = 50000
-    means = np.array([[1, 1], [-1, -1], [1, -1], [-1, 1],
-                      [0.5, 0.5], [0.75, -0.5], [-1, 0.75], [1, 0]])
+    means = np.array(
+        [
+            [1, 1],
+            [-1, -1],
+            [1, -1],
+            [-1, 1],
+            [0.5, 0.5],
+            [0.75, -0.5],
+            [-1, 0.75],
+            [1, 0],
+        ]
+    )
     X = np.empty((0, 2))
     for i in range(8):
         X = np.r_[X, means[i] + 0.8 * np.random.randn(n_features, 2)]
@@ -66,16 +76,14 @@
     it = 0
     for chunk in chunks:
         it += 1
-        print('==============================')
-        print('Iteration %03d of %03d' % (it, max_it))
-        print('==============================')
+        print("==============================")
+        print("Iteration %03d of %03d" % (it, max_it))
+        print("==============================")
         print()

-        print('Fast K-Means')
+        print("Fast K-Means")
         tstart = time()
-        mbkmeans = MiniBatchKMeans(init='k-means++',
-                                   n_clusters=8,
-                                   batch_size=chunk)
+        mbkmeans = MiniBatchKMeans(init="k-means++", n_clusters=8, batch_size=chunk)

         mbkmeans.fit(X)
         delta = time() - tstart
@@ -83,54 +91,52 @@
         print("Inertia: %0.3fs" % mbkmeans.inertia_)
         print()

-        results['MiniBatchKMeans Speed'].append(delta)
-        results['MiniBatchKMeans Quality'].append(mbkmeans.inertia_)
+        results["MiniBatchKMeans Speed"].append(delta)
+        results["MiniBatchKMeans Quality"].append(mbkmeans.inertia_)

     return results


-if __name__ == '__main__':
-    from mpl_toolkits.mplot3d import axes3d  # register the 3d projection
+if __name__ == "__main__":
+    from mpl_toolkits.mplot3d import axes3d  # noqa register the 3d projection
     import matplotlib.pyplot as plt

-    samples_range = np.linspace(50, 150, 5).astype(np.int)
-    features_range = np.linspace(150, 50000, 5).astype(np.int)
-    chunks = np.linspace(500, 10000, 15).astype(np.int)
+    samples_range = np.linspace(50, 150, 5).astype(int)
+    features_range = np.linspace(150, 50000, 5).astype(int)
+    chunks = np.linspace(500, 10000, 15).astype(int)

     results = compute_bench(samples_range, features_range)
     results_2 = compute_bench_2(chunks)

-    max_time = max([max(i) for i in [t for (label, t) in results.items()
-                                     if "speed" in label]])
-    max_inertia = max([max(i) for i in [
-        t for (label, t) in results.items()
-        if "speed" not in label]])
+    max_time = max(
+        [max(i) for i in [t for (label, t) in results.items() if "speed" in label]]
+    )
+    max_inertia = max(
+        [max(i) for i in [t for (label, t) in results.items() if "speed" not in label]]
+    )

-    fig = plt.figure('scikit-learn K-Means benchmark results')
-    for c, (label, timings) in zip('brcy',
-                                   sorted(results.items())):
-        if 'speed' in label:
-            ax = fig.add_subplot(2, 2, 1, projection='3d')
+    fig = plt.figure("scikit-learn K-Means benchmark results")
+    for c, (label, timings) in zip("brcy", sorted(results.items())):
+        if "speed" in label:
+            ax = fig.add_subplot(2, 2, 1, projection="3d")
             ax.set_zlim3d(0.0, max_time * 1.1)
         else:
-            ax = fig.add_subplot(2, 2, 2, projection='3d')
+            ax = fig.add_subplot(2, 2, 2, projection="3d")
             ax.set_zlim3d(0.0, max_inertia * 1.1)

         X, Y = np.meshgrid(samples_range, features_range)
-        Z = np.asarray(timings).reshape(samples_range.shape[0],
-                                        features_range.shape[0])
+        Z = np.asarray(timings).reshape(samples_range.shape[0], features_range.shape[0])
         ax.plot_surface(X, Y, Z.T, cstride=1, rstride=1, color=c, alpha=0.5)
-        ax.set_xlabel('n_samples')
-        ax.set_ylabel('n_features')
+        ax.set_xlabel("n_samples")
+        ax.set_ylabel("n_features")

     i = 0
-    for c, (label, timings) in zip('br',
-                                   sorted(results_2.items())):
+    for c, (label, timings) in zip("br", sorted(results_2.items())):
         i += 1
         ax = fig.add_subplot(2, 2, i + 2)
         y = np.asarray(timings)
         ax.plot(chunks, y, color=c, alpha=0.8)
-        ax.set_xlabel('Chunks')
+        ax.set_xlabel("Chunks")
         ax.set_ylabel(label)

     plt.show()
('benchmarks', 'bench_plot_nmf.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -16,19 +16,19 @@
 from joblib import Memory
 import pandas

-from sklearn.utils.testing import ignore_warnings
+from sklearn.utils._testing import ignore_warnings
 from sklearn.feature_extraction.text import TfidfVectorizer
-from sklearn.decomposition.nmf import NMF
-from sklearn.decomposition.nmf import _initialize_nmf
-from sklearn.decomposition.nmf import _beta_divergence
-from sklearn.decomposition.nmf import INTEGER_TYPES, _check_init
+from sklearn.decomposition import NMF
+from sklearn.decomposition._nmf import _initialize_nmf
+from sklearn.decomposition._nmf import _beta_divergence
+from sklearn.decomposition._nmf import _check_init
 from sklearn.exceptions import ConvergenceWarning
 from sklearn.utils.extmath import safe_sparse_dot, squared_norm
 from sklearn.utils import check_array
 from sklearn.utils.validation import check_is_fitted, check_non_negative


-mem = Memory(cachedir='.', verbose=0)
+mem = Memory(cachedir=".", verbose=0)

 ###################
 # Start of _PGNMF #
@@ -46,8 +46,9 @@
     return np.sqrt(squared_norm(x))


-def _nls_subproblem(X, W, H, tol, max_iter, alpha=0., l1_ratio=0.,
-                    sigma=0.01, beta=0.1):
+def _nls_subproblem(
+    X, W, H, tol, max_iter, alpha=0.0, l1_ratio=0.0, sigma=0.01, beta=0.1
+):
     """Non-negative least square solver
     Solves a non-negative least squares subproblem using the projected
     gradient descent algorithm.
@@ -104,7 +105,7 @@
     gamma = 1
     for n_iter in range(1, max_iter + 1):
         grad = np.dot(WtW, H) - WtX
-        if alpha > 0 and l1_ratio == 1.:
+        if alpha > 0 and l1_ratio == 1.0:
             grad += alpha
         elif alpha > 0:
             grad += alpha * (l1_ratio + (1 - l1_ratio) * H)
@@ -142,18 +143,14 @@
                 Hp = Hn

     if n_iter == max_iter:
-        warnings.warn("Iteration limit reached in nls subproblem.",
-                      ConvergenceWarning)
+        warnings.warn("Iteration limit reached in nls subproblem.", ConvergenceWarning)

     return H, grad, n_iter


-def _fit_projected_gradient(X, W, H, tol, max_iter, nls_max_iter, alpha,
-                            l1_ratio):
-    gradW = (np.dot(W, np.dot(H, H.T)) -
-             safe_sparse_dot(X, H.T, dense_output=True))
-    gradH = (np.dot(np.dot(W.T, W), H) -
-             safe_sparse_dot(W.T, X, dense_output=True))
+def _fit_projected_gradient(X, W, H, tol, max_iter, nls_max_iter, alpha, l1_ratio):
+    gradW = np.dot(W, np.dot(H, H.T)) - safe_sparse_dot(X, H.T, dense_output=True)
+    gradH = np.dot(np.dot(W.T, W), H) - safe_sparse_dot(W.T, X, dense_output=True)

     init_grad = squared_norm(gradW) + squared_norm(gradH.T)
     # max(0.001, tol) to force alternating minimizations of W and H
@@ -165,28 +162,31 @@
         proj_grad_W = squared_norm(gradW * np.logical_or(gradW < 0, W > 0))
         proj_grad_H = squared_norm(gradH * np.logical_or(gradH < 0, H > 0))

-        if (proj_grad_W + proj_grad_H) / init_grad < tol ** 2:
+        if (proj_grad_W + proj_grad_H) / init_grad < tol**2:
             break

         # update W
-        Wt, gradWt, iterW = _nls_subproblem(X.T, H.T, W.T, tolW, nls_max_iter,
-                                            alpha=alpha, l1_ratio=l1_ratio)
+        Wt, gradWt, iterW = _nls_subproblem(
+            X.T, H.T, W.T, tolW, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio
+        )
         W, gradW = Wt.T, gradWt.T

         if iterW == 1:
             tolW = 0.1 * tolW

         # update H
-        H, gradH, iterH = _nls_subproblem(X, W, H, tolH, nls_max_iter,
-                                          alpha=alpha, l1_ratio=l1_ratio)
+        H, gradH, iterH = _nls_subproblem(
+            X, W, H, tolH, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio
+        )
         if iterH == 1:
             tolH = 0.1 * tolH

-    H[H == 0] = 0   # fix up negative zeros
+    H[H == 0] = 0  # fix up negative zeros

     if n_iter == max_iter:
-        Wt, _, _ = _nls_subproblem(X.T, H.T, W.T, tolW, nls_max_iter,
-                                   alpha=alpha, l1_ratio=l1_ratio)
+        Wt, _, _ = _nls_subproblem(
+            X.T, H.T, W.T, tolW, nls_max_iter, alpha=alpha, l1_ratio=l1_ratio
+        )
         W = Wt.T

     return W, H, n_iter
@@ -199,13 +199,29 @@
     It may change or disappear without notice.

     """
-    def __init__(self, n_components=None, solver='pg', init=None,
-                 tol=1e-4, max_iter=200, random_state=None,
-                 alpha=0., l1_ratio=0., nls_max_iter=10):
+
+    def __init__(
+        self,
+        n_components=None,
+        solver="pg",
+        init=None,
+        tol=1e-4,
+        max_iter=200,
+        random_state=None,
+        alpha=0.0,
+        l1_ratio=0.0,
+        nls_max_iter=10,
+    ):
         super().__init__(
-            n_components=n_components, init=init, solver=solver, tol=tol,
-            max_iter=max_iter, random_state=random_state, alpha=alpha,
-            l1_ratio=l1_ratio)
+            n_components=n_components,
+            init=init,
+            solver=solver,
+            tol=tol,
+            max_iter=max_iter,
+            random_state=random_state,
+            alpha=alpha,
+            l1_ratio=l1_ratio,
+        )
         self.nls_max_iter = nls_max_iter

     def fit(self, X, y=None, **params):
@@ -213,13 +229,13 @@
         return self

     def transform(self, X):
-        check_is_fitted(self, 'components_')
+        check_is_fitted(self)
         H = self.components_
         W, _, self.n_iter_ = self._fit_transform(X, H=H, update_H=False)
         return W

     def inverse_transform(self, W):
-        check_is_fitted(self, 'components_')
+        check_is_fitted(self)
         return np.dot(W, self.components_)

     def fit_transform(self, X, y=None, W=None, H=None):
@@ -228,7 +244,7 @@
         return W

     def _fit_transform(self, X, y=None, W=None, H=None, update_H=True):
-        X = check_array(X, accept_sparse=('csr', 'csc'))
+        X = check_array(X, accept_sparse=("csr", "csc"))
         check_non_negative(X, "NMF (input X)")

         n_samples, n_features = X.shape
@@ -236,45 +252,68 @@
         if n_components is None:
             n_components = n_features

-        if (not isinstance(n_components, INTEGER_TYPES) or
-                n_components <= 0):
-            raise ValueError("Number of components must be a positive integer;"
-                             " got (n_components=%r)" % n_components)
-        if not isinstance(self.max_iter, INTEGER_TYPES) or self.max_iter < 0:
-            raise ValueError("Maximum number of iterations must be a positive "
-                             "integer; got (max_iter=%r)" % self.max_iter)
+        if not isinstance(n_components, numbers.Integral) or n_components <= 0:
+            raise ValueError(
+                "Number of components must be a positive integer; got (n_components=%r)"
+                % n_components
+            )
+        if not isinstance(self.max_iter, numbers.Integral) or self.max_iter < 0:
+            raise ValueError(
+                "Maximum number of iterations must be a positive "
+                "integer; got (max_iter=%r)"
+                % self.max_iter
+            )
         if not isinstance(self.tol, numbers.Number) or self.tol < 0:
-            raise ValueError("Tolerance for stopping criteria must be "
-                             "positive; got (tol=%r)" % self.tol)
+            raise ValueError(
+                "Tolerance for stopping criteria must be positive; got (tol=%r)"
+                % self.tol
+            )

         # check W and H, or initialize them
-        if self.init == 'custom' and update_H:
+        if self.init == "custom" and update_H:
             _check_init(H, (n_components, n_features), "NMF (input H)")
             _check_init(W, (n_samples, n_components), "NMF (input W)")
         elif not update_H:
             _check_init(H, (n_components, n_features), "NMF (input H)")
             W = np.zeros((n_samples, n_components))
         else:
-            W, H = _initialize_nmf(X, n_components, init=self.init,
-                                   random_state=self.random_state)
+            W, H = _initialize_nmf(
+                X, n_components, init=self.init, random_state=self.random_state
+            )

         if update_H:  # fit_transform
             W, H, n_iter = _fit_projected_gradient(
-                X, W, H, self.tol, self.max_iter, self.nls_max_iter,
-                self.alpha, self.l1_ratio)
+                X,
+                W,
+                H,
+                self.tol,
+                self.max_iter,
+                self.nls_max_iter,
+                self.alpha,
+                self.l1_ratio,
+            )
         else:  # transform
-            Wt, _, n_iter = _nls_subproblem(X.T, H.T, W.T, self.tol,
-                                            self.nls_max_iter,
-                                            alpha=self.alpha,
-                                            l1_ratio=self.l1_ratio)
+            Wt, _, n_iter = _nls_subproblem(
+                X.T,
+                H.T,
+                W.T,
+                self.tol,
+                self.nls_max_iter,
+                alpha=self.alpha,
+                l1_ratio=self.l1_ratio,
+            )
             W = Wt.T

         if n_iter == self.max_iter and self.tol > 0:
-            warnings.warn("Maximum number of iteration %d reached. Increase it"
-                          " to improve convergence." % self.max_iter,
-                          ConvergenceWarning)
+            warnings.warn(
+                "Maximum number of iteration %d reached. Increase it"
+                " to improve convergence."
+                % self.max_iter,
+                ConvergenceWarning,
+            )

         return W, H, n_iter
+

 #################
 # End of _PGNMF #
@@ -286,22 +325,27 @@
         return None

     plt.figure(figsize=(16, 6))
-    colors = 'bgr'
-    markers = 'ovs'
+    colors = "bgr"
+    markers = "ovs"
     ax = plt.subplot(1, 3, 1)
-    for i, init in enumerate(np.unique(results_df['init'])):
+    for i, init in enumerate(np.unique(results_df["init"])):
         plt.subplot(1, 3, i + 1, sharex=ax, sharey=ax)
-        for j, method in enumerate(np.unique(results_df['method'])):
-            mask = np.logical_and(results_df['init'] == init,
-                                  results_df['method'] == method)
+        for j, method in enumerate(np.unique(results_df["method"])):
+            mask = np.logical_and(
+                results_df["init"] == init, results_df["method"] == method
+            )
             selected_items = results_df[mask]

-            plt.plot(selected_items['time'], selected_items['loss'],
-                     color=colors[j % len(colors)], ls='-',
-                     marker=markers[j % len(markers)],
-                     label=method)
-
-        plt.legend(loc=0, fontsize='x-small')
+            plt.plot(
+                selected_items["time"],
+                selected_items["loss"],
+                color=colors[j % len(colors)],
+                ls="-",
+                marker=markers[j % len(markers)],
+                label=method,
+            )
+
+        plt.legend(loc=0, fontsize="x-small")
         plt.xlabel("Time (s)")
         plt.ylabel("loss")
         plt.title("%s" % init)
@@ -311,9 +355,10 @@
 @ignore_warnings(category=ConvergenceWarning)
 # use joblib to cache the results.
 # X_shape is specified in arguments for avoiding hashing X
-@mem.cache(ignore=['X', 'W0', 'H0'])
-def bench_one(name, X, W0, H0, X_shape, clf_type, clf_params, init,
-              n_components, random_state):
+@mem.cache(ignore=["X", "W0", "H0"])
+def bench_one(
+    name, X, W0, H0, X_shape, clf_type, clf_params, init, n_components, random_state
+):
     W = W0.copy()
     H = H0.copy()

@@ -333,22 +378,22 @@
     results = []
     for name, clf_type, iter_range, clf_params in clfs:
         print("Training %s:" % name)
-        for rs, init in enumerate(('nndsvd', 'nndsvdar', 'random')):
+        for rs, init in enumerate(("nndsvd", "nndsvdar", "random")):
             print("    %s %s: " % (init, " " * (8 - len(init))), end="")
             W, H = _initialize_nmf(X, n_components, init, 1e-6, rs)

             for max_iter in iter_range:
-                clf_params['alpha'] = alpha
-                clf_params['l1_ratio'] = l1_ratio
-                clf_params['max_iter'] = max_iter
-                clf_params['tol'] = tol
-                clf_params['random_state'] = rs
-                clf_params['init'] = 'custom'
-                clf_params['n_components'] = n_components
-
-                this_loss, duration = bench_one(name, X, W, H, X.shape,
-                                                clf_type, clf_params,
-                                                init, n_components, rs)
+                clf_params["alpha"] = alpha
+                clf_params["l1_ratio"] = l1_ratio
+                clf_params["max_iter"] = max_iter
+                clf_params["tol"] = tol
+                clf_params["random_state"] = rs
+                clf_params["init"] = "custom"
+                clf_params["n_components"] = n_components
+
+                this_loss, duration = bench_one(
+                    name, X, W, H, X.shape, clf_type, clf_params, init, n_components, rs
+                )

                 init_name = "init='%s'" % init
                 results.append((name, this_loss, duration, init_name))
@@ -358,8 +403,7 @@
             print(" ")

     # Use a panda dataframe to organize the results
-    results_df = pandas.DataFrame(results,
-                                  columns="method loss time init".split())
+    results_df = pandas.DataFrame(results, columns="method loss time init".split())
     print("Total time = %0.3f sec\n" % (time() - start))

     # plot the results
@@ -371,9 +415,11 @@
     print("Loading 20 newsgroups dataset")
     print("-----------------------------")
     from sklearn.datasets import fetch_20newsgroups
-    dataset = fetch_20newsgroups(shuffle=True, random_state=1,
-                                 remove=('headers', 'footers', 'quotes'))
-    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')
+
+    dataset = fetch_20newsgroups(
+        shuffle=True, random_state=1, remove=("headers", "footers", "quotes")
+    )
+    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words="english")
     tfidf = vectorizer.fit_transform(dataset.data)
     return tfidf

@@ -382,20 +428,22 @@
     print("Loading Olivetti face dataset")
     print("-----------------------------")
     from sklearn.datasets import fetch_olivetti_faces
+
     faces = fetch_olivetti_faces(shuffle=True)
     return faces.data


 def build_clfs(cd_iters, pg_iters, mu_iters):
-    clfs = [("Coordinate Descent", NMF, cd_iters, {'solver': 'cd'}),
-            ("Projected Gradient", _PGNMF, pg_iters, {'solver': 'pg'}),
-            ("Multiplicative Update", NMF, mu_iters, {'solver': 'mu'}),
-            ]
+    clfs = [
+        ("Coordinate Descent", NMF, cd_iters, {"solver": "cd"}),
+        ("Projected Gradient", _PGNMF, pg_iters, {"solver": "pg"}),
+        ("Multiplicative Update", NMF, mu_iters, {"solver": "mu"}),
+    ]
     return clfs


-if __name__ == '__main__':
-    alpha = 0.
+if __name__ == "__main__":
+    alpha = 0.0
     l1_ratio = 0.5
     n_components = 10
     tol = 1e-15
@@ -416,6 +464,14 @@
     mu_iters = np.arange(1, 30)
     clfs = build_clfs(cd_iters, pg_iters, mu_iters)
     X_faces = load_faces()
-    run_bench(X_faces, clfs, plot_name, n_components, tol, alpha, l1_ratio,)
+    run_bench(
+        X_faces,
+        clfs,
+        plot_name,
+        n_components,
+        tol,
+        alpha,
+        l1_ratio,
+    )

     plt.show()
('benchmarks', 'bench_plot_ward.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -10,12 +10,11 @@

 from sklearn.cluster import AgglomerativeClustering

-ward = AgglomerativeClustering(n_clusters=3, linkage='ward')
+ward = AgglomerativeClustering(n_clusters=3, linkage="ward")

-n_samples = np.logspace(.5, 3, 9)
+n_samples = np.logspace(0.5, 3, 9)
 n_features = np.logspace(1, 3.5, 7)
-N_samples, N_features = np.meshgrid(n_samples,
-                                    n_features)
+N_samples, N_features = np.meshgrid(n_samples, n_features)
 scikits_time = np.zeros(N_samples.shape)
 scipy_time = np.zeros(N_samples.shape)

@@ -32,12 +31,18 @@
 ratio = scikits_time / scipy_time

 plt.figure("scikit-learn Ward's method benchmark results")
-plt.imshow(np.log(ratio), aspect='auto', origin="lower")
+plt.imshow(np.log(ratio), aspect="auto", origin="lower")
 plt.colorbar()
-plt.contour(ratio, levels=[1, ], colors='k')
-plt.yticks(range(len(n_features)), n_features.astype(np.int))
-plt.ylabel('N features')
-plt.xticks(range(len(n_samples)), n_samples.astype(np.int))
-plt.xlabel('N samples')
+plt.contour(
+    ratio,
+    levels=[
+        1,
+    ],
+    colors="k",
+)
+plt.yticks(range(len(n_features)), n_features.astype(int))
+plt.ylabel("N features")
+plt.xticks(range(len(n_samples)), n_samples.astype(int))
+plt.xlabel("N samples")
 plt.title("Scikit's time, in units of scipy time (log)")
 plt.show()
('benchmarks', 'bench_plot_lasso_path.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -11,7 +11,7 @@

 from sklearn.linear_model import lars_path, lars_path_gram
 from sklearn.linear_model import lasso_path
-from sklearn.datasets.samples_generator import make_regression
+from sklearn.datasets import make_regression


 def compute_bench(samples_range, features_range):
@@ -24,79 +24,78 @@
     for n_samples in samples_range:
         for n_features in features_range:
             it += 1
-            print('====================')
-            print('Iteration %03d of %03d' % (it, max_it))
-            print('====================')
+            print("====================")
+            print("Iteration %03d of %03d" % (it, max_it))
+            print("====================")
             dataset_kwargs = {
-                'n_samples': n_samples,
-                'n_features': n_features,
-                'n_informative': n_features // 10,
-                'effective_rank': min(n_samples, n_features) / 10,
-                #'effective_rank': None,
-                'bias': 0.0,
+                "n_samples": n_samples,
+                "n_features": n_features,
+                "n_informative": n_features // 10,
+                "effective_rank": min(n_samples, n_features) / 10,
+                # 'effective_rank': None,
+                "bias": 0.0,
             }
             print("n_samples: %d" % n_samples)
             print("n_features: %d" % n_features)
             X, y = make_regression(**dataset_kwargs)

             gc.collect()
-            print("benchmarking lars_path (with Gram):", end='')
+            print("benchmarking lars_path (with Gram):", end="")
             sys.stdout.flush()
             tstart = time()
             G = np.dot(X.T, X)  # precomputed Gram matrix
             Xy = np.dot(X.T, y)
-            lars_path_gram(Xy=Xy, Gram=G, n_samples=y.size, method='lasso')
+            lars_path_gram(Xy=Xy, Gram=G, n_samples=y.size, method="lasso")
             delta = time() - tstart
             print("%0.3fs" % delta)
-            results['lars_path (with Gram)'].append(delta)
+            results["lars_path (with Gram)"].append(delta)

             gc.collect()
-            print("benchmarking lars_path (without Gram):", end='')
+            print("benchmarking lars_path (without Gram):", end="")
             sys.stdout.flush()
             tstart = time()
-            lars_path(X, y, method='lasso')
+            lars_path(X, y, method="lasso")
             delta = time() - tstart
             print("%0.3fs" % delta)
-            results['lars_path (without Gram)'].append(delta)
+            results["lars_path (without Gram)"].append(delta)

             gc.collect()
-            print("benchmarking lasso_path (with Gram):", end='')
+            print("benchmarking lasso_path (with Gram):", end="")
             sys.stdout.flush()
             tstart = time()
             lasso_path(X, y, precompute=True)
             delta = time() - tstart
             print("%0.3fs" % delta)
-            results['lasso_path (with Gram)'].append(delta)
+            results["lasso_path (with Gram)"].append(delta)

             gc.collect()
-            print("benchmarking lasso_path (without Gram):", end='')
+            print("benchmarking lasso_path (without Gram):", end="")
             sys.stdout.flush()
             tstart = time()
             lasso_path(X, y, precompute=False)
             delta = time() - tstart
             print("%0.3fs" % delta)
-            results['lasso_path (without Gram)'].append(delta)
+            results["lasso_path (without Gram)"].append(delta)

     return results


-if __name__ == '__main__':
-    from mpl_toolkits.mplot3d import axes3d  # register the 3d projection
+if __name__ == "__main__":
+    from mpl_toolkits.mplot3d import axes3d  # noqa register the 3d projection
     import matplotlib.pyplot as plt

-    samples_range = np.linspace(10, 2000, 5).astype(np.int)
-    features_range = np.linspace(10, 2000, 5).astype(np.int)
+    samples_range = np.linspace(10, 2000, 5).astype(int)
+    features_range = np.linspace(10, 2000, 5).astype(int)
     results = compute_bench(samples_range, features_range)

     max_time = max(max(t) for t in results.values())

-    fig = plt.figure('scikit-learn Lasso path benchmark results')
+    fig = plt.figure("scikit-learn Lasso path benchmark results")
     i = 1
-    for c, (label, timings) in zip('bcry', sorted(results.items())):
-        ax = fig.add_subplot(2, 2, i, projection='3d')
+    for c, (label, timings) in zip("bcry", sorted(results.items())):
+        ax = fig.add_subplot(2, 2, i, projection="3d")
         X, Y = np.meshgrid(samples_range, features_range)
-        Z = np.asarray(timings).reshape(samples_range.shape[0],
-                                        features_range.shape[0])
+        Z = np.asarray(timings).reshape(samples_range.shape[0], features_range.shape[0])

         # plot the actual surface
         ax.plot_surface(X, Y, Z.T, cstride=1, rstride=1, color=c, alpha=0.8)
@@ -105,9 +104,9 @@
         # support legends (yet?)
         # ax.plot([1], [1], [1], color=c, label=label)

-        ax.set_xlabel('n_samples')
-        ax.set_ylabel('n_features')
-        ax.set_zlabel('Time (s)')
+        ax.set_xlabel("n_samples")
+        ax.set_ylabel("n_features")
+        ax.set_zlabel("Time (s)")
         ax.set_zlim3d(0.0, max_time * 1.1)
         ax.set_title(label)
         # ax.legend()
('benchmarks', 'bench_tree.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -22,7 +22,7 @@
 scikit_classifier_results = []
 scikit_regressor_results = []

-mu_second = 0.0 + 10 ** 6  # number of microseconds in a second
+mu_second = 0.0 + 10**6  # number of microseconds in a second


 def bench_scikit_tree_classifier(X, Y):
@@ -36,11 +36,10 @@
     tstart = datetime.now()
     clf = DecisionTreeClassifier()
     clf.fit(X, Y).predict(X)
-    delta = (datetime.now() - tstart)
+    delta = datetime.now() - tstart
     # stop time

-    scikit_classifier_results.append(
-        delta.seconds + delta.microseconds / mu_second)
+    scikit_classifier_results.append(delta.seconds + delta.microseconds / mu_second)


 def bench_scikit_tree_regressor(X, Y):
@@ -54,18 +53,17 @@
     tstart = datetime.now()
     clf = DecisionTreeRegressor()
     clf.fit(X, Y).predict(X)
-    delta = (datetime.now() - tstart)
+    delta = datetime.now() - tstart
     # stop time

-    scikit_regressor_results.append(
-        delta.seconds + delta.microseconds / mu_second)
+    scikit_regressor_results.append(delta.seconds + delta.microseconds / mu_second)


-if __name__ == '__main__':
+if __name__ == "__main__":

-    print('============================================')
-    print('Warning: this is going to take a looong time')
-    print('============================================')
+    print("============================================")
+    print("Warning: this is going to take a looong time")
+    print("============================================")

     n = 10
     step = 10000
@@ -73,9 +71,9 @@
     dim = 10
     n_classes = 10
     for i in range(n):
-        print('============================================')
-        print('Entering iteration %s of %s' % (i, n))
-        print('============================================')
+        print("============================================")
+        print("Entering iteration %s of %s" % (i, n))
+        print("============================================")
         n_samples += step
         X = np.random.randn(n_samples, dim)
         Y = np.random.randint(0, n_classes, (n_samples,))
@@ -84,14 +82,14 @@
         bench_scikit_tree_regressor(X, Y)

     xx = range(0, n * step, step)
-    plt.figure('scikit-learn tree benchmark results')
+    plt.figure("scikit-learn tree benchmark results")
     plt.subplot(211)
-    plt.title('Learning with varying number of samples')
-    plt.plot(xx, scikit_classifier_results, 'g-', label='classification')
-    plt.plot(xx, scikit_regressor_results, 'r-', label='regression')
-    plt.legend(loc='upper left')
-    plt.xlabel('number of samples')
-    plt.ylabel('Time (s)')
+    plt.title("Learning with varying number of samples")
+    plt.plot(xx, scikit_classifier_results, "g-", label="classification")
+    plt.plot(xx, scikit_regressor_results, "r-", label="regression")
+    plt.legend(loc="upper left")
+    plt.xlabel("number of samples")
+    plt.ylabel("Time (s)")

     scikit_classifier_results = []
     scikit_regressor_results = []
@@ -102,9 +100,9 @@

     dim = start_dim
     for i in range(0, n):
-        print('============================================')
-        print('Entering iteration %s of %s' % (i, n))
-        print('============================================')
+        print("============================================")
+        print("Entering iteration %s of %s" % (i, n))
+        print("============================================")
         dim += step
         X = np.random.randn(100, dim)
         Y = np.random.randint(0, n_classes, (100,))
@@ -114,11 +112,11 @@

     xx = np.arange(start_dim, start_dim + n * step, step)
     plt.subplot(212)
-    plt.title('Learning in high dimensional spaces')
-    plt.plot(xx, scikit_classifier_results, 'g-', label='classification')
-    plt.plot(xx, scikit_regressor_results, 'r-', label='regression')
-    plt.legend(loc='upper left')
-    plt.xlabel('number of dimensions')
-    plt.ylabel('Time (s)')
-    plt.axis('tight')
+    plt.title("Learning in high dimensional spaces")
+    plt.plot(xx, scikit_classifier_results, "g-", label="classification")
+    plt.plot(xx, scikit_regressor_results, "r-", label="regression")
+    plt.legend(loc="upper left")
+    plt.xlabel("number of dimensions")
+    plt.ylabel("Time (s)")
+    plt.axis("tight")
     plt.show()
('benchmarks', 'bench_plot_incremental_pca.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -17,7 +17,7 @@


 def plot_results(X, y, label):
-    plt.plot(X, y, label=label, marker='o')
+    plt.plot(X, y, label=label, marker="o")


 def benchmark(estimator, data):
@@ -29,60 +29,68 @@
     data_t = estimator.transform(data)
     data_r = estimator.inverse_transform(data_t)
     reconstruction_error = np.mean(np.abs(data - data_r))
-    return {'time': training_time, 'error': reconstruction_error}
+    return {"time": training_time, "error": reconstruction_error}


 def plot_feature_times(all_times, batch_size, all_components, data):
     plt.figure()
-    plot_results(all_components, all_times['pca'], label="PCA")
-    plot_results(all_components, all_times['ipca'],
-                 label="IncrementalPCA, bsize=%i" % batch_size)
+    plot_results(all_components, all_times["pca"], label="PCA")
+    plot_results(
+        all_components, all_times["ipca"], label="IncrementalPCA, bsize=%i" % batch_size
+    )
     plt.legend(loc="upper left")
-    plt.suptitle("Algorithm runtime vs. n_components\n \
-                 LFW, size %i x %i" % data.shape)
+    plt.suptitle(
+        "Algorithm runtime vs. n_components\n                  LFW, size %i x %i"
+        % data.shape
+    )
     plt.xlabel("Number of components (out of max %i)" % data.shape[1])
     plt.ylabel("Time (seconds)")


 def plot_feature_errors(all_errors, batch_size, all_components, data):
     plt.figure()
-    plot_results(all_components, all_errors['pca'], label="PCA")
-    plot_results(all_components, all_errors['ipca'],
-                 label="IncrementalPCA, bsize=%i" % batch_size)
+    plot_results(all_components, all_errors["pca"], label="PCA")
+    plot_results(
+        all_components,
+        all_errors["ipca"],
+        label="IncrementalPCA, bsize=%i" % batch_size,
+    )
     plt.legend(loc="lower left")
-    plt.suptitle("Algorithm error vs. n_components\n"
-                 "LFW, size %i x %i" % data.shape)
+    plt.suptitle("Algorithm error vs. n_components\nLFW, size %i x %i" % data.shape)
     plt.xlabel("Number of components (out of max %i)" % data.shape[1])
     plt.ylabel("Mean absolute error")


 def plot_batch_times(all_times, n_features, all_batch_sizes, data):
     plt.figure()
-    plot_results(all_batch_sizes, all_times['pca'], label="PCA")
-    plot_results(all_batch_sizes, all_times['ipca'], label="IncrementalPCA")
+    plot_results(all_batch_sizes, all_times["pca"], label="PCA")
+    plot_results(all_batch_sizes, all_times["ipca"], label="IncrementalPCA")
     plt.legend(loc="lower left")
-    plt.suptitle("Algorithm runtime vs. batch_size for n_components %i\n \
-                 LFW, size %i x %i" % (
-                 n_features, data.shape[0], data.shape[1]))
+    plt.suptitle(
+        "Algorithm runtime vs. batch_size for n_components %i\n                  LFW,"
+        " size %i x %i" % (n_features, data.shape[0], data.shape[1])
+    )
     plt.xlabel("Batch size")
     plt.ylabel("Time (seconds)")


 def plot_batch_errors(all_errors, n_features, all_batch_sizes, data):
     plt.figure()
-    plot_results(all_batch_sizes, all_errors['pca'], label="PCA")
-    plot_results(all_batch_sizes, all_errors['ipca'], label="IncrementalPCA")
+    plot_results(all_batch_sizes, all_errors["pca"], label="PCA")
+    plot_results(all_batch_sizes, all_errors["ipca"], label="IncrementalPCA")
     plt.legend(loc="lower left")
-    plt.suptitle("Algorithm error vs. batch_size for n_components %i\n \
-                 LFW, size %i x %i" % (
-                 n_features, data.shape[0], data.shape[1]))
+    plt.suptitle(
+        "Algorithm error vs. batch_size for n_components %i\n                  LFW,"
+        " size %i x %i" % (n_features, data.shape[0], data.shape[1])
+    )
     plt.xlabel("Batch size")
     plt.ylabel("Mean absolute error")


 def fixed_batch_size_comparison(data):
-    all_features = [i.astype(int) for i in np.linspace(data.shape[1] // 10,
-                                                       data.shape[1], num=5)]
+    all_features = [
+        i.astype(int) for i in np.linspace(data.shape[1] // 10, data.shape[1], num=5)
+    ]
     batch_size = 1000
     # Compare runtimes and error for fixed batch size
     all_times = defaultdict(list)
@@ -90,53 +98,52 @@
     for n_components in all_features:
         pca = PCA(n_components=n_components)
         ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)
-        results_dict = {k: benchmark(est, data) for k, est in [('pca', pca),
-                                                               ('ipca', ipca)]}
+        results_dict = {
+            k: benchmark(est, data) for k, est in [("pca", pca), ("ipca", ipca)]
+        }

         for k in sorted(results_dict.keys()):
-            all_times[k].append(results_dict[k]['time'])
-            all_errors[k].append(results_dict[k]['error'])
+            all_times[k].append(results_dict[k]["time"])
+            all_errors[k].append(results_dict[k]["error"])

     plot_feature_times(all_times, batch_size, all_features, data)
     plot_feature_errors(all_errors, batch_size, all_features, data)


 def variable_batch_size_comparison(data):
-    batch_sizes = [i.astype(int) for i in np.linspace(data.shape[0] // 10,
-                                                      data.shape[0], num=10)]
+    batch_sizes = [
+        i.astype(int) for i in np.linspace(data.shape[0] // 10, data.shape[0], num=10)
+    ]

-    for n_components in [i.astype(int) for i in
-                         np.linspace(data.shape[1] // 10,
-                                     data.shape[1], num=4)]:
+    for n_components in [
+        i.astype(int) for i in np.linspace(data.shape[1] // 10, data.shape[1], num=4)
+    ]:
         all_times = defaultdict(list)
         all_errors = defaultdict(list)
         pca = PCA(n_components=n_components)
-        rpca = PCA(n_components=n_components, svd_solver='randomized',
-                   random_state=1999)
-        results_dict = {k: benchmark(est, data) for k, est in [('pca', pca),
-                                                               ('rpca', rpca)]}
+        rpca = PCA(
+            n_components=n_components, svd_solver="randomized", random_state=1999
+        )
+        results_dict = {
+            k: benchmark(est, data) for k, est in [("pca", pca), ("rpca", rpca)]
+        }

         # Create flat baselines to compare the variation over batch size
-        all_times['pca'].extend([results_dict['pca']['time']] *
-                                len(batch_sizes))
-        all_errors['pca'].extend([results_dict['pca']['error']] *
-                                 len(batch_sizes))
-        all_times['rpca'].extend([results_dict['rpca']['time']] *
-                                 len(batch_sizes))
-        all_errors['rpca'].extend([results_dict['rpca']['error']] *
-                                  len(batch_sizes))
+        all_times["pca"].extend([results_dict["pca"]["time"]] * len(batch_sizes))
+        all_errors["pca"].extend([results_dict["pca"]["error"]] * len(batch_sizes))
+        all_times["rpca"].extend([results_dict["rpca"]["time"]] * len(batch_sizes))
+        all_errors["rpca"].extend([results_dict["rpca"]["error"]] * len(batch_sizes))
         for batch_size in batch_sizes:
-            ipca = IncrementalPCA(n_components=n_components,
-                                  batch_size=batch_size)
-            results_dict = {k: benchmark(est, data) for k, est in [('ipca',
-                                                                   ipca)]}
-            all_times['ipca'].append(results_dict['ipca']['time'])
-            all_errors['ipca'].append(results_dict['ipca']['error'])
+            ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)
+            results_dict = {k: benchmark(est, data) for k, est in [("ipca", ipca)]}
+            all_times["ipca"].append(results_dict["ipca"]["time"])
+            all_errors["ipca"].append(results_dict["ipca"]["error"])

         plot_batch_times(all_times, n_components, batch_sizes, data)
         plot_batch_errors(all_errors, n_components, batch_sizes, data)

-faces = fetch_lfw_people(resize=.2, min_faces_per_person=5)
+
+faces = fetch_lfw_people(resize=0.2, min_faces_per_person=5)
 # limit dataset to 5000 people (don't care who they are!)
 X = faces.data[:5000]
 n_samples, h, w = faces.images.shape
('benchmarks', 'bench_multilabel_metrics.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -14,32 +14,40 @@
 import numpy as np

 from sklearn.datasets import make_multilabel_classification
-from sklearn.metrics import (f1_score, accuracy_score, hamming_loss,
-                             jaccard_similarity_score)
-from sklearn.utils.testing import ignore_warnings
+from sklearn.metrics import (
+    f1_score,
+    accuracy_score,
+    hamming_loss,
+    jaccard_similarity_score,
+)
+from sklearn.utils._testing import ignore_warnings


 METRICS = {
-    'f1': partial(f1_score, average='micro'),
-    'f1-by-sample': partial(f1_score, average='samples'),
-    'accuracy': accuracy_score,
-    'hamming': hamming_loss,
-    'jaccard': jaccard_similarity_score,
+    "f1": partial(f1_score, average="micro"),
+    "f1-by-sample": partial(f1_score, average="samples"),
+    "accuracy": accuracy_score,
+    "hamming": hamming_loss,
+    "jaccard": jaccard_similarity_score,
 }

 FORMATS = {
-    'sequences': lambda y: [list(np.flatnonzero(s)) for s in y],
-    'dense': lambda y: y,
-    'csr': lambda y: sp.csr_matrix(y),
-    'csc': lambda y: sp.csc_matrix(y),
+    "sequences": lambda y: [list(np.flatnonzero(s)) for s in y],
+    "dense": lambda y: y,
+    "csr": lambda y: sp.csr_matrix(y),
+    "csc": lambda y: sp.csc_matrix(y),
 }


 @ignore_warnings
-def benchmark(metrics=tuple(v for k, v in sorted(METRICS.items())),
-              formats=tuple(v for k, v in sorted(FORMATS.items())),
-              samples=1000, classes=4, density=.2,
-              n_times=5):
+def benchmark(
+    metrics=tuple(v for k, v in sorted(METRICS.items())),
+    formats=tuple(v for k, v in sorted(FORMATS.items())),
+    samples=1000,
+    classes=4,
+    density=0.2,
+    n_times=5,
+):
     """Times metric calculations for a number of inputs

     Parameters
@@ -73,16 +81,18 @@
     classes = np.atleast_1d(classes)
     density = np.atleast_1d(density)
     formats = np.atleast_1d(formats)
-    out = np.zeros((len(metrics), len(formats), len(samples), len(classes),
-                    len(density)), dtype=float)
+    out = np.zeros(
+        (len(metrics), len(formats), len(samples), len(classes), len(density)),
+        dtype=float,
+    )
     it = itertools.product(samples, classes, density)
     for i, (s, c, d) in enumerate(it):
-        _, y_true = make_multilabel_classification(n_samples=s, n_features=1,
-                                                   n_classes=c, n_labels=d * c,
-                                                   random_state=42)
-        _, y_pred = make_multilabel_classification(n_samples=s, n_features=1,
-                                                   n_classes=c, n_labels=d * c,
-                                                   random_state=84)
+        _, y_true = make_multilabel_classification(
+            n_samples=s, n_features=1, n_classes=c, n_labels=d * c, random_state=42
+        )
+        _, y_pred = make_multilabel_classification(
+            n_samples=s, n_features=1, n_classes=c, n_labels=d * c, random_state=84
+        )
         for j, f in enumerate(formats):
             f_true = f(y_true)
             f_pred = f(y_pred)
@@ -100,70 +110,95 @@
     """
     column_width = max(max(len(k) for k in formats) + 1, 8)
     first_width = max(len(k) for k in metrics)
-    head_fmt = ('{:<{fw}s}' + '{:>{cw}s}' * len(formats))
-    row_fmt = ('{:<{fw}s}' + '{:>{cw}.3f}' * len(formats))
-    print(head_fmt.format('Metric', *formats,
-                          cw=column_width, fw=first_width))
+    head_fmt = "{:<{fw}s}" + "{:>{cw}s}" * len(formats)
+    row_fmt = "{:<{fw}s}" + "{:>{cw}.3f}" * len(formats)
+    print(head_fmt.format("Metric", *formats, cw=column_width, fw=first_width))
     for metric, row in zip(metrics, results[:, :, -1, -1, -1]):
-        print(row_fmt.format(metric, *row,
-                             cw=column_width, fw=first_width))
-
-
-def _plot(results, metrics, formats, title, x_ticks, x_label,
-          format_markers=('x', '|', 'o', '+'),
-          metric_colors=('c', 'm', 'y', 'k', 'g', 'r', 'b')):
+        print(row_fmt.format(metric, *row, cw=column_width, fw=first_width))
+
+
+def _plot(
+    results,
+    metrics,
+    formats,
+    title,
+    x_ticks,
+    x_label,
+    format_markers=("x", "|", "o", "+"),
+    metric_colors=("c", "m", "y", "k", "g", "r", "b"),
+):
     """
     Plot the results by metric, format and some other variable given by
     x_label
     """
-    fig = plt.figure('scikit-learn multilabel metrics benchmarks')
+    fig = plt.figure("scikit-learn multilabel metrics benchmarks")
     plt.title(title)
     ax = fig.add_subplot(111)
     for i, metric in enumerate(metrics):
         for j, format in enumerate(formats):
-            ax.plot(x_ticks, results[i, j].flat,
-                    label='{}, {}'.format(metric, format),
-                    marker=format_markers[j],
-                    color=metric_colors[i % len(metric_colors)])
+            ax.plot(
+                x_ticks,
+                results[i, j].flat,
+                label="{}, {}".format(metric, format),
+                marker=format_markers[j],
+                color=metric_colors[i % len(metric_colors)],
+            )
     ax.set_xlabel(x_label)
-    ax.set_ylabel('Time (s)')
+    ax.set_ylabel("Time (s)")
     ax.legend()
     plt.show()


 if __name__ == "__main__":
     ap = argparse.ArgumentParser()
-    ap.add_argument('metrics', nargs='*', default=sorted(METRICS),
-                    help='Specifies metrics to benchmark, defaults to all. '
-                         'Choices are: {}'.format(sorted(METRICS)))
-    ap.add_argument('--formats', nargs='+', choices=sorted(FORMATS),
-                    help='Specifies multilabel formats to benchmark '
-                         '(defaults to all).')
-    ap.add_argument('--samples', type=int, default=1000,
-                    help='The number of samples to generate')
-    ap.add_argument('--classes', type=int, default=10,
-                    help='The number of classes')
-    ap.add_argument('--density', type=float, default=.2,
-                    help='The average density of labels per sample')
-    ap.add_argument('--plot', choices=['classes', 'density', 'samples'],
-                    default=None,
-                    help='Plot time with respect to this parameter varying '
-                         'up to the specified value')
-    ap.add_argument('--n-steps', default=10, type=int,
-                    help='Plot this many points for each metric')
-    ap.add_argument('--n-times',
-                    default=5, type=int,
-                    help="Time performance over n_times trials")
+    ap.add_argument(
+        "metrics",
+        nargs="*",
+        default=sorted(METRICS),
+        help="Specifies metrics to benchmark, defaults to all. Choices are: {}".format(
+            sorted(METRICS)
+        ),
+    )
+    ap.add_argument(
+        "--formats",
+        nargs="+",
+        choices=sorted(FORMATS),
+        help="Specifies multilabel formats to benchmark (defaults to all).",
+    )
+    ap.add_argument(
+        "--samples", type=int, default=1000, help="The number of samples to generate"
+    )
+    ap.add_argument("--classes", type=int, default=10, help="The number of classes")
+    ap.add_argument(
+        "--density",
+        type=float,
+        default=0.2,
+        help="The average density of labels per sample",
+    )
+    ap.add_argument(
+        "--plot",
+        choices=["classes", "density", "samples"],
+        default=None,
+        help=(
+            "Plot time with respect to this parameter varying up to the specified value"
+        ),
+    )
+    ap.add_argument(
+        "--n-steps", default=10, type=int, help="Plot this many points for each metric"
+    )
+    ap.add_argument(
+        "--n-times", default=5, type=int, help="Time performance over n_times trials"
+    )
     args = ap.parse_args()

     if args.plot is not None:
         max_val = getattr(args, args.plot)
-        if args.plot in ('classes', 'samples'):
+        if args.plot in ("classes", "samples"):
             min_val = 2
         else:
             min_val = 0
         steps = np.linspace(min_val, max_val, num=args.n_steps + 1)[1:]
-        if args.plot in ('classes', 'samples'):
+        if args.plot in ("classes", "samples"):
             steps = np.unique(np.round(steps).astype(int))
         setattr(args, args.plot, steps)

@@ -172,17 +207,22 @@
     if args.formats is None:
         args.formats = sorted(FORMATS)

-    results = benchmark([METRICS[k] for k in args.metrics],
-                        [FORMATS[k] for k in args.formats],
-                        args.samples, args.classes, args.density,
-                        args.n_times)
+    results = benchmark(
+        [METRICS[k] for k in args.metrics],
+        [FORMATS[k] for k in args.formats],
+        args.samples,
+        args.classes,
+        args.density,
+        args.n_times,
+    )

     _tabulate(results, args.metrics, args.formats)

     if args.plot is not None:
-        print('Displaying plot', file=sys.stderr)
-        title = ('Multilabel metrics with %s' %
-                 ', '.join('{0}={1}'.format(field, getattr(args, field))
-                           for field in ['samples', 'classes', 'density']
-                           if args.plot != field))
+        print("Displaying plot", file=sys.stderr)
+        title = "Multilabel metrics with %s" % ", ".join(
+            "{0}={1}".format(field, getattr(args, field))
+            for field in ["samples", "classes", "density"]
+            if args.plot != field
+        )
         _plot(results, args.metrics, args.formats, title, steps, args.plot)
('benchmarks', 'bench_sparsify.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -43,9 +43,9 @@
     60       300       381409   1271.4     97.1          clf.predict(X_test_sparse)
 """

-from scipy.sparse.csr import csr_matrix
+from scipy.sparse import csr_matrix
 import numpy as np
-from sklearn.linear_model.stochastic_gradient import SGDRegressor
+from sklearn.linear_model import SGDRegressor
 from sklearn.metrics import r2_score

 np.random.seed(42)
@@ -54,16 +54,17 @@
 def sparsity_ratio(X):
     return np.count_nonzero(X) / float(n_samples * n_features)

+
 n_samples, n_features = 5000, 300
 X = np.random.randn(n_samples, n_features)
 inds = np.arange(n_samples)
 np.random.shuffle(inds)
-X[inds[int(n_features / 1.2):]] = 0  # sparsify input
+X[inds[int(n_features / 1.2) :]] = 0  # sparsify input
 print("input data sparsity: %f" % sparsity_ratio(X))
 coef = 3 * np.random.randn(n_features)
 inds = np.arange(n_features)
 np.random.shuffle(inds)
-coef[inds[n_features // 2:]] = 0  # sparsify coef
+coef[inds[n_features // 2 :]] = 0  # sparsify coef
 print("true coef sparsity: %f" % sparsity_ratio(coef))
 y = np.dot(X, coef)

@@ -72,13 +73,12 @@

 # Split data in train set and test set
 n_samples = X.shape[0]
-X_train, y_train = X[:n_samples // 2], y[:n_samples // 2]
-X_test, y_test = X[n_samples // 2:], y[n_samples // 2:]
+X_train, y_train = X[: n_samples // 2], y[: n_samples // 2]
+X_test, y_test = X[n_samples // 2 :], y[n_samples // 2 :]
 print("test data sparsity: %f" % sparsity_ratio(X_test))

 ###############################################################################
-clf = SGDRegressor(penalty='l1', alpha=.2, fit_intercept=True, max_iter=2000,
-                   tol=None)
+clf = SGDRegressor(penalty="l1", alpha=0.2, max_iter=2000, tol=None)
 clf.fit(X_train, y_train)
 print("model sparsity: %f" % sparsity_ratio(clf.coef_))

@@ -98,8 +98,9 @@
     r2 = r2_score(y_test, y_pred)
     print("r^2 on test data (%s) : %f" % (case, r2))

-score(y_test, clf.predict(X_test), 'dense model')
+
+score(y_test, clf.predict(X_test), "dense model")
 benchmark_dense_predict()
 clf.sparsify()
-score(y_test, clf.predict(X_test), 'sparse model')
+score(y_test, clf.predict(X_test), "sparse model")
 benchmark_sparse_predict()
('benchmarks', 'bench_20newsgroups.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -16,12 +16,8 @@

 ESTIMATORS = {
     "dummy": DummyClassifier(),
-    "random_forest": RandomForestClassifier(n_estimators=100,
-                                            max_features="sqrt",
-                                            min_samples_split=10),
-    "extra_trees": ExtraTreesClassifier(n_estimators=100,
-                                        max_features="sqrt",
-                                        min_samples_split=10),
+    "random_forest": RandomForestClassifier(max_features="sqrt", min_samples_split=10),
+    "extra_trees": ExtraTreesClassifier(max_features="sqrt", min_samples_split=10),
     "logistic_regression": LogisticRegression(),
     "naive_bayes": MultinomialNB(),
     "adaboost": AdaBoostClassifier(n_estimators=10),
@@ -34,32 +30,30 @@
 if __name__ == "__main__":

     parser = argparse.ArgumentParser()
-    parser.add_argument('-e', '--estimators', nargs="+", required=True,
-                        choices=ESTIMATORS)
+    parser.add_argument(
+        "-e", "--estimators", nargs="+", required=True, choices=ESTIMATORS
+    )
     args = vars(parser.parse_args())

     data_train = fetch_20newsgroups_vectorized(subset="train")
     data_test = fetch_20newsgroups_vectorized(subset="test")
-    X_train = check_array(data_train.data, dtype=np.float32,
-                          accept_sparse="csc")
+    X_train = check_array(data_train.data, dtype=np.float32, accept_sparse="csc")
     X_test = check_array(data_test.data, dtype=np.float32, accept_sparse="csr")
     y_train = data_train.target
     y_test = data_test.target

     print("20 newsgroups")
     print("=============")
-    print("X_train.shape = {0}".format(X_train.shape))
-    print("X_train.format = {0}".format(X_train.format))
-    print("X_train.dtype = {0}".format(X_train.dtype))
-    print("X_train density = {0}"
-          "".format(X_train.nnz / np.product(X_train.shape)))
-    print("y_train {0}".format(y_train.shape))
-    print("X_test {0}".format(X_test.shape))
-    print("X_test.format = {0}".format(X_test.format))
-    print("X_test.dtype = {0}".format(X_test.dtype))
-    print("y_test {0}".format(y_test.shape))
+    print(f"X_train.shape = {X_train.shape}")
+    print(f"X_train.format = {X_train.format}")
+    print(f"X_train.dtype = {X_train.dtype}")
+    print(f"X_train density = {X_train.nnz / np.product(X_train.shape)}")
+    print(f"y_train {y_train.shape}")
+    print(f"X_test {X_test.shape}")
+    print(f"X_test.format = {X_test.format}")
+    print(f"X_test.dtype = {X_test.dtype}")
+    print(f"y_test {y_test.shape}")
     print()
-
     print("Classifier Training")
     print("===================")
     accuracy, train_time, test_time = {}, {}, {}
@@ -84,13 +78,17 @@
     print("Classification performance:")
     print("===========================")
     print()
-    print("%s %s %s %s" % ("Classifier  ", "train-time", "test-time",
-                           "Accuracy"))
+    print("%s %s %s %s" % ("Classifier  ", "train-time", "test-time", "Accuracy"))
     print("-" * 44)
     for name in sorted(accuracy, key=accuracy.get):
-        print("%s %s %s %s" % (name.ljust(16),
-                               ("%.4fs" % train_time[name]).center(10),
-                               ("%.4fs" % test_time[name]).center(10),
-                               ("%.4f" % accuracy[name]).center(10)))
+        print(
+            "%s %s %s %s"
+            % (
+                name.ljust(16),
+                ("%.4fs" % train_time[name]).center(10),
+                ("%.4fs" % test_time[name]).center(10),
+                ("%.4f" % accuracy[name]).center(10),
+            )
+        )

     print()
('benchmarks', 'bench_sgd_regression.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -10,7 +10,7 @@

 from sklearn.linear_model import Ridge, SGDRegressor, ElasticNet
 from sklearn.metrics import mean_squared_error
-from sklearn.datasets.samples_generator import make_regression
+from sklearn.datasets import make_regression

 """
 Benchmark for SGD regression
@@ -22,7 +22,7 @@
 print(__doc__)

 if __name__ == "__main__":
-    list_n_samples = np.linspace(100, 10000, 5).astype(np.int)
+    list_n_samples = np.linspace(100, 10000, 5).astype(int)
     list_n_features = [10, 100, 1000]
     n_test = 1000
     max_iter = 1000
@@ -35,8 +35,11 @@
     for i, n_train in enumerate(list_n_samples):
         for j, n_features in enumerate(list_n_features):
             X, y, coef = make_regression(
-                n_samples=n_train + n_test, n_features=n_features,
-                noise=noise, coef=True)
+                n_samples=n_train + n_test,
+                n_features=n_features,
+                noise=noise,
+                coef=True,
+            )

             X_train = X[:n_train]
             y_train = y[:n_train]
@@ -70,34 +73,43 @@
             clf = ElasticNet(alpha=alpha, l1_ratio=0.5, fit_intercept=False)
             tstart = time()
             clf.fit(X_train, y_train)
-            elnet_results[i, j, 0] = mean_squared_error(clf.predict(X_test),
-                                                        y_test)
+            elnet_results[i, j, 0] = mean_squared_error(clf.predict(X_test), y_test)
             elnet_results[i, j, 1] = time() - tstart

             gc.collect()
             print("- benchmarking SGD")
-            clf = SGDRegressor(alpha=alpha / n_train, fit_intercept=False,
-                               max_iter=max_iter, learning_rate="invscaling",
-                               eta0=.01, power_t=0.25, tol=1e-3)
+            clf = SGDRegressor(
+                alpha=alpha / n_train,
+                fit_intercept=False,
+                max_iter=max_iter,
+                learning_rate="invscaling",
+                eta0=0.01,
+                power_t=0.25,
+                tol=1e-3,
+            )

             tstart = time()
             clf.fit(X_train, y_train)
-            sgd_results[i, j, 0] = mean_squared_error(clf.predict(X_test),
-                                                      y_test)
+            sgd_results[i, j, 0] = mean_squared_error(clf.predict(X_test), y_test)
             sgd_results[i, j, 1] = time() - tstart

             gc.collect()
             print("max_iter", max_iter)
             print("- benchmarking A-SGD")
-            clf = SGDRegressor(alpha=alpha / n_train, fit_intercept=False,
-                               max_iter=max_iter, learning_rate="invscaling",
-                               eta0=.002, power_t=0.05, tol=1e-3,
-                               average=(max_iter * n_train // 2))
+            clf = SGDRegressor(
+                alpha=alpha / n_train,
+                fit_intercept=False,
+                max_iter=max_iter,
+                learning_rate="invscaling",
+                eta0=0.002,
+                power_t=0.05,
+                tol=1e-3,
+                average=(max_iter * n_train // 2),
+            )

             tstart = time()
             clf.fit(X_train, y_train)
-            asgd_results[i, j, 0] = mean_squared_error(clf.predict(X_test),
-                                                       y_test)
+            asgd_results[i, j, 0] = mean_squared_error(clf.predict(X_test), y_test)
             asgd_results[i, j, 1] = time() - tstart

             gc.collect()
@@ -105,25 +117,19 @@
             clf = Ridge(alpha=alpha, fit_intercept=False)
             tstart = time()
             clf.fit(X_train, y_train)
-            ridge_results[i, j, 0] = mean_squared_error(clf.predict(X_test),
-                                                        y_test)
+            ridge_results[i, j, 0] = mean_squared_error(clf.predict(X_test), y_test)
             ridge_results[i, j, 1] = time() - tstart

     # Plot results
     i = 0
     m = len(list_n_features)
-    plt.figure('scikit-learn SGD regression benchmark results',
-               figsize=(5 * 2, 4 * m))
+    plt.figure("scikit-learn SGD regression benchmark results", figsize=(5 * 2, 4 * m))
     for j in range(m):
         plt.subplot(m, 2, i + 1)
-        plt.plot(list_n_samples, np.sqrt(elnet_results[:, j, 0]),
-                 label="ElasticNet")
-        plt.plot(list_n_samples, np.sqrt(sgd_results[:, j, 0]),
-                 label="SGDRegressor")
-        plt.plot(list_n_samples, np.sqrt(asgd_results[:, j, 0]),
-                 label="A-SGDRegressor")
-        plt.plot(list_n_samples, np.sqrt(ridge_results[:, j, 0]),
-                 label="Ridge")
+        plt.plot(list_n_samples, np.sqrt(elnet_results[:, j, 0]), label="ElasticNet")
+        plt.plot(list_n_samples, np.sqrt(sgd_results[:, j, 0]), label="SGDRegressor")
+        plt.plot(list_n_samples, np.sqrt(asgd_results[:, j, 0]), label="A-SGDRegressor")
+        plt.plot(list_n_samples, np.sqrt(ridge_results[:, j, 0]), label="Ridge")
         plt.legend(prop={"size": 10})
         plt.xlabel("n_train")
         plt.ylabel("RMSE")
@@ -131,20 +137,16 @@
         i += 1

         plt.subplot(m, 2, i + 1)
-        plt.plot(list_n_samples, np.sqrt(elnet_results[:, j, 1]),
-                 label="ElasticNet")
-        plt.plot(list_n_samples, np.sqrt(sgd_results[:, j, 1]),
-                 label="SGDRegressor")
-        plt.plot(list_n_samples, np.sqrt(asgd_results[:, j, 1]),
-                 label="A-SGDRegressor")
-        plt.plot(list_n_samples, np.sqrt(ridge_results[:, j, 1]),
-                 label="Ridge")
+        plt.plot(list_n_samples, np.sqrt(elnet_results[:, j, 1]), label="ElasticNet")
+        plt.plot(list_n_samples, np.sqrt(sgd_results[:, j, 1]), label="SGDRegressor")
+        plt.plot(list_n_samples, np.sqrt(asgd_results[:, j, 1]), label="A-SGDRegressor")
+        plt.plot(list_n_samples, np.sqrt(ridge_results[:, j, 1]), label="Ridge")
         plt.legend(prop={"size": 10})
         plt.xlabel("n_train")
         plt.ylabel("Time [sec]")
         plt.title("Training time - %d features" % list_n_features[j])
         i += 1

-    plt.subplots_adjust(hspace=.30)
+    plt.subplots_adjust(hspace=0.30)

     plt.show()
('benchmarks', 'bench_saga.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,12 +7,17 @@
 import time
 import os

-from joblib import delayed, Parallel, Memory
+from joblib import Parallel
+from sklearn.utils.fixes import delayed
 import matplotlib.pyplot as plt
 import numpy as np

-from sklearn.datasets import fetch_rcv1, load_iris, load_digits, \
-    fetch_20newsgroups_vectorized
+from sklearn.datasets import (
+    fetch_rcv1,
+    load_iris,
+    load_digits,
+    fetch_20newsgroups_vectorized,
+)
 from sklearn.linear_model import LogisticRegression
 from sklearn.metrics import log_loss
 from sklearn.model_selection import train_test_split
@@ -20,27 +25,38 @@
 from sklearn.utils.extmath import safe_sparse_dot, softmax


-def fit_single(solver, X, y, penalty='l2', single_target=True, C=1,
-               max_iter=10, skip_slow=False, dtype=np.float64):
-    if skip_slow and solver == 'lightning' and penalty == 'l1':
-        print('skip_slowping l1 logistic regression with solver lightning.')
+def fit_single(
+    solver,
+    X,
+    y,
+    penalty="l2",
+    single_target=True,
+    C=1,
+    max_iter=10,
+    skip_slow=False,
+    dtype=np.float64,
+):
+    if skip_slow and solver == "lightning" and penalty == "l1":
+        print("skip_slowping l1 logistic regression with solver lightning.")
         return

-    print('Solving %s logistic regression with penalty %s, solver %s.'
-          % ('binary' if single_target else 'multinomial',
-             penalty, solver))
-
-    if solver == 'lightning':
+    print(
+        "Solving %s logistic regression with penalty %s, solver %s."
+        % ("binary" if single_target else "multinomial", penalty, solver)
+    )
+
+    if solver == "lightning":
         from lightning.classification import SAGAClassifier

-    if single_target or solver not in ['sag', 'saga']:
-        multi_class = 'ovr'
+    if single_target or solver not in ["sag", "saga"]:
+        multi_class = "ovr"
     else:
-        multi_class = 'multinomial'
+        multi_class = "multinomial"
     X = X.astype(dtype)
     y = y.astype(dtype)
-    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,
-                                                        stratify=y)
+    X_train, X_test, y_train, y_test = train_test_split(
+        X, y, random_state=42, stratify=y
+    )
     n_samples = X_train.shape[0]
     n_classes = np.unique(y_train).shape[0]
     test_scores = [1]
@@ -48,32 +64,45 @@
     accuracies = [1 / n_classes]
     times = [0]

-    if penalty == 'l2':
-        alpha = 1. / (C * n_samples)
+    if penalty == "l2":
+        alpha = 1.0 / (C * n_samples)
         beta = 0
         lightning_penalty = None
     else:
-        alpha = 0.
-        beta = 1. / (C * n_samples)
-        lightning_penalty = 'l1'
+        alpha = 0.0
+        beta = 1.0 / (C * n_samples)
+        lightning_penalty = "l1"

     for this_max_iter in range(1, max_iter + 1, 2):
-        print('[%s, %s, %s] Max iter: %s' %
-              ('binary' if single_target else 'multinomial',
-               penalty, solver, this_max_iter))
-        if solver == 'lightning':
-            lr = SAGAClassifier(loss='log', alpha=alpha, beta=beta,
-                                penalty=lightning_penalty,
-                                tol=-1, max_iter=this_max_iter)
+        print(
+            "[%s, %s, %s] Max iter: %s"
+            % (
+                "binary" if single_target else "multinomial",
+                penalty,
+                solver,
+                this_max_iter,
+            )
+        )
+        if solver == "lightning":
+            lr = SAGAClassifier(
+                loss="log",
+                alpha=alpha,
+                beta=beta,
+                penalty=lightning_penalty,
+                tol=-1,
+                max_iter=this_max_iter,
+            )
         else:
-            lr = LogisticRegression(solver=solver,
-                                    multi_class=multi_class,
-                                    C=C,
-                                    penalty=penalty,
-                                    fit_intercept=False, tol=0,
-                                    max_iter=this_max_iter,
-                                    random_state=42,
-                                    )
+            lr = LogisticRegression(
+                solver=solver,
+                multi_class=multi_class,
+                C=C,
+                penalty=penalty,
+                fit_intercept=False,
+                tol=0,
+                max_iter=this_max_iter,
+                random_state=42,
+            )

         # Makes cpu cache even for all fit calls
         X_train.max()
@@ -83,15 +112,16 @@
         train_time = time.clock() - t0

         scores = []
-        for (X, y) in [(X_train, y_train), (X_test, y_test)]:
+        for X, y in [(X_train, y_train), (X_test, y_test)]:
             try:
                 y_pred = lr.predict_proba(X)
             except NotImplementedError:
                 # Lightning predict_proba is not implemented for n_classes > 2
                 y_pred = _predict_proba(lr, X)
             score = log_loss(y, y_pred, normalize=False) / n_samples
-            score += (0.5 * alpha * np.sum(lr.coef_ ** 2) +
-                      beta * np.sum(np.abs(lr.coef_)))
+            score += 0.5 * alpha * np.sum(lr.coef_**2) + beta * np.sum(
+                np.abs(lr.coef_)
+            )
             scores.append(score)
         train_score, test_score = tuple(scores)

@@ -111,15 +141,22 @@
     return softmax(pred)


-def exp(solvers, penalty, single_target,
-        n_samples=30000, max_iter=20,
-        dataset='rcv1', n_jobs=1, skip_slow=False):
+def exp(
+    solvers,
+    penalty,
+    single_target,
+    n_samples=30000,
+    max_iter=20,
+    dataset="rcv1",
+    n_jobs=1,
+    skip_slow=False,
+):
     dtypes_mapping = {
         "float64": np.float64,
         "float32": np.float32,
     }

-    if dataset == 'rcv1':
+    if dataset == "rcv1":
         rcv1 = fetch_rcv1()

         lbin = LabelBinarizer()
@@ -136,18 +173,17 @@
             y_n[y <= 16] = 0
             y = y_n

-    elif dataset == 'digits':
-        digits = load_digits()
-        X, y = digits.data, digits.target
+    elif dataset == "digits":
+        X, y = load_digits(return_X_y=True)
         if single_target:
             y_n = y.copy()
             y_n[y < 5] = 1
             y_n[y >= 5] = 0
             y = y_n
-    elif dataset == 'iris':
+    elif dataset == "iris":
         iris = load_iris()
         X, y = iris.data, iris.target
-    elif dataset == '20newspaper':
+    elif dataset == "20newspaper":
         ng = fetch_20newsgroups_vectorized()
         X = ng.data
         y = ng.target
@@ -161,44 +197,55 @@
     y = y[:n_samples]

     out = Parallel(n_jobs=n_jobs, mmap_mode=None)(
-        delayed(fit_single)(solver, X, y,
-                            penalty=penalty, single_target=single_target,
-                            dtype=dtype,
-                            C=1, max_iter=max_iter, skip_slow=skip_slow)
+        delayed(fit_single)(
+            solver,
+            X,
+            y,
+            penalty=penalty,
+            single_target=single_target,
+            dtype=dtype,
+            C=1,
+            max_iter=max_iter,
+            skip_slow=skip_slow,
+        )
         for solver in solvers
-        for dtype in dtypes_mapping.values())
+        for dtype in dtypes_mapping.values()
+    )

     res = []
     idx = 0
     for dtype_name in dtypes_mapping.keys():
         for solver in solvers:
-            if not (skip_slow and
-                    solver == 'lightning' and
-                    penalty == 'l1'):
+            if not (skip_slow and solver == "lightning" and penalty == "l1"):
                 lr, times, train_scores, test_scores, accuracies = out[idx]
-                this_res = dict(solver=solver, penalty=penalty,
-                                dtype=dtype_name,
-                                single_target=single_target,
-                                times=times, train_scores=train_scores,
-                                test_scores=test_scores,
-                                accuracies=accuracies)
+                this_res = dict(
+                    solver=solver,
+                    penalty=penalty,
+                    dtype=dtype_name,
+                    single_target=single_target,
+                    times=times,
+                    train_scores=train_scores,
+                    test_scores=test_scores,
+                    accuracies=accuracies,
+                )
                 res.append(this_res)
             idx += 1

-    with open('bench_saga.json', 'w+') as f:
+    with open("bench_saga.json", "w+") as f:
         json.dump(res, f)


 def plot(outname=None):
     import pandas as pd
-    with open('bench_saga.json', 'r') as f:
+
+    with open("bench_saga.json", "r") as f:
         f = json.load(f)
     res = pd.DataFrame(f)
-    res.set_index(['single_target'], inplace=True)
-
-    grouped = res.groupby(level=['single_target'])
-
-    colors = {'saga': 'C0', 'liblinear': 'C1', 'lightning': 'C2'}
+    res.set_index(["single_target"], inplace=True)
+
+    grouped = res.groupby(level=["single_target"])
+
+    colors = {"saga": "C0", "liblinear": "C1", "lightning": "C2"}
     linestyles = {"float32": "--", "float64": "-"}
     alpha = {"float64": 0.5, "float32": 1}

@@ -207,93 +254,122 @@
         fig, axes = plt.subplots(figsize=(12, 4), ncols=4)
         ax = axes[0]

-        for scores, times, solver, dtype in zip(group['train_scores'],
-                                                group['times'],
-                                                group['solver'],
-                                                group["dtype"]):
-            ax.plot(times, scores, label="%s - %s" % (solver, dtype),
-                    color=colors[solver],
-                    alpha=alpha[dtype],
-                    marker=".",
-                    linestyle=linestyles[dtype])
-            ax.axvline(times[-1], color=colors[solver],
-                       alpha=alpha[dtype],
-                       linestyle=linestyles[dtype])
-        ax.set_xlabel('Time (s)')
-        ax.set_ylabel('Training objective (relative to min)')
-        ax.set_yscale('log')
+        for scores, times, solver, dtype in zip(
+            group["train_scores"], group["times"], group["solver"], group["dtype"]
+        ):
+            ax.plot(
+                times,
+                scores,
+                label="%s - %s" % (solver, dtype),
+                color=colors[solver],
+                alpha=alpha[dtype],
+                marker=".",
+                linestyle=linestyles[dtype],
+            )
+            ax.axvline(
+                times[-1],
+                color=colors[solver],
+                alpha=alpha[dtype],
+                linestyle=linestyles[dtype],
+            )
+        ax.set_xlabel("Time (s)")
+        ax.set_ylabel("Training objective (relative to min)")
+        ax.set_yscale("log")

         ax = axes[1]

-        for scores, times, solver, dtype in zip(group['test_scores'],
-                                                group['times'],
-                                                group['solver'],
-                                                group["dtype"]):
-            ax.plot(times, scores, label=solver, color=colors[solver],
-                    linestyle=linestyles[dtype],
-                    marker=".",
-                    alpha=alpha[dtype])
-            ax.axvline(times[-1], color=colors[solver],
-                       alpha=alpha[dtype],
-                       linestyle=linestyles[dtype])
-
-        ax.set_xlabel('Time (s)')
-        ax.set_ylabel('Test objective (relative to min)')
-        ax.set_yscale('log')
+        for scores, times, solver, dtype in zip(
+            group["test_scores"], group["times"], group["solver"], group["dtype"]
+        ):
+            ax.plot(
+                times,
+                scores,
+                label=solver,
+                color=colors[solver],
+                linestyle=linestyles[dtype],
+                marker=".",
+                alpha=alpha[dtype],
+            )
+            ax.axvline(
+                times[-1],
+                color=colors[solver],
+                alpha=alpha[dtype],
+                linestyle=linestyles[dtype],
+            )
+
+        ax.set_xlabel("Time (s)")
+        ax.set_ylabel("Test objective (relative to min)")
+        ax.set_yscale("log")

         ax = axes[2]
-        for accuracy, times, solver, dtype in zip(group['accuracies'],
-                                                  group['times'],
-                                                  group['solver'],
-                                                  group["dtype"]):
-            ax.plot(times, accuracy, label="%s - %s" % (solver, dtype),
-                    alpha=alpha[dtype],
-                    marker=".",
-                    color=colors[solver], linestyle=linestyles[dtype])
-            ax.axvline(times[-1], color=colors[solver],
-                       alpha=alpha[dtype],
-                       linestyle=linestyles[dtype])
-
-        ax.set_xlabel('Time (s)')
-        ax.set_ylabel('Test accuracy')
+        for accuracy, times, solver, dtype in zip(
+            group["accuracies"], group["times"], group["solver"], group["dtype"]
+        ):
+            ax.plot(
+                times,
+                accuracy,
+                label="%s - %s" % (solver, dtype),
+                alpha=alpha[dtype],
+                marker=".",
+                color=colors[solver],
+                linestyle=linestyles[dtype],
+            )
+            ax.axvline(
+                times[-1],
+                color=colors[solver],
+                alpha=alpha[dtype],
+                linestyle=linestyles[dtype],
+            )
+
+        ax.set_xlabel("Time (s)")
+        ax.set_ylabel("Test accuracy")
         ax.legend()
-        name = 'single_target' if single_target else 'multi_target'
-        name += '_%s' % penalty
+        name = "single_target" if single_target else "multi_target"
+        name += "_%s" % penalty
         plt.suptitle(name)
         if outname is None:
-            outname = name + '.png'
+            outname = name + ".png"
         fig.tight_layout()
         fig.subplots_adjust(top=0.9)

         ax = axes[3]
-        for scores, times, solver, dtype in zip(group['train_scores'],
-                                                group['times'],
-                                                group['solver'],
-                                                group["dtype"]):
-            ax.plot(np.arange(len(scores)),
-                    scores, label="%s - %s" % (solver, dtype),
-                    marker=".",
-                    alpha=alpha[dtype],
-                    color=colors[solver], linestyle=linestyles[dtype])
+        for scores, times, solver, dtype in zip(
+            group["train_scores"], group["times"], group["solver"], group["dtype"]
+        ):
+            ax.plot(
+                np.arange(len(scores)),
+                scores,
+                label="%s - %s" % (solver, dtype),
+                marker=".",
+                alpha=alpha[dtype],
+                color=colors[solver],
+                linestyle=linestyles[dtype],
+            )

         ax.set_yscale("log")
-        ax.set_xlabel('# iterations')
-        ax.set_ylabel('Objective function')
+        ax.set_xlabel("# iterations")
+        ax.set_ylabel("Objective function")
         ax.legend()

         plt.savefig(outname)


-if __name__ == '__main__':
-    solvers = ['saga', 'liblinear', 'lightning']
-    penalties = ['l1', 'l2']
+if __name__ == "__main__":
+    solvers = ["saga", "liblinear", "lightning"]
+    penalties = ["l1", "l2"]
     n_samples = [100000, 300000, 500000, 800000, None]
     single_target = True
     for penalty in penalties:
         for n_sample in n_samples:
-            exp(solvers, penalty, single_target,
-                n_samples=n_sample, n_jobs=1,
-                dataset='rcv1', max_iter=10)
+            exp(
+                solvers,
+                penalty,
+                single_target,
+                n_samples=n_sample,
+                n_jobs=1,
+                dataset="rcv1",
+                max_iter=10,
+            )
             if n_sample is not None:
                 outname = "figures/saga_%s_%d.png" % (penalty, n_sample)
             else:
('benchmarks', 'bench_random_projections.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -16,9 +16,11 @@
 import scipy.sparse as sp

 from sklearn import clone
-from sklearn.random_projection import (SparseRandomProjection,
-                                       GaussianRandomProjection,
-                                       johnson_lindenstrauss_min_dim)
+from sklearn.random_projection import (
+    SparseRandomProjection,
+    GaussianRandomProjection,
+    johnson_lindenstrauss_min_dim,
+)


 def type_auto_or_float(val):
@@ -36,27 +38,27 @@


 def compute_time(t_start, delta):
-    mu_second = 0.0 + 10 ** 6  # number of microseconds in a second
+    mu_second = 0.0 + 10**6  # number of microseconds in a second

     return delta.seconds + delta.microseconds / mu_second


-def bench_scikit_transformer(X, transfomer):
+def bench_scikit_transformer(X, transformer):
     gc.collect()

-    clf = clone(transfomer)
+    clf = clone(transformer)

     # start time
     t_start = datetime.now()
     clf.fit(X)
-    delta = (datetime.now() - t_start)
+    delta = datetime.now() - t_start
     # stop time
     time_to_fit = compute_time(t_start, delta)

     # start time
     t_start = datetime.now()
     clf.transform(X)
-    delta = (datetime.now() - t_start)
+    delta = datetime.now() - t_start
     # stop time
     time_to_transform = compute_time(t_start, delta)

@@ -65,21 +67,30 @@

 # Make some random data with uniformly located non zero entries with
 # Gaussian distributed values
-def make_sparse_random_data(n_samples, n_features, n_nonzeros,
-                            random_state=None):
+def make_sparse_random_data(n_samples, n_features, n_nonzeros, random_state=None):
     rng = np.random.RandomState(random_state)
     data_coo = sp.coo_matrix(
-        (rng.randn(n_nonzeros),
-        (rng.randint(n_samples, size=n_nonzeros),
-         rng.randint(n_features, size=n_nonzeros))),
-        shape=(n_samples, n_features))
+        (
+            rng.randn(n_nonzeros),
+            (
+                rng.randint(n_samples, size=n_nonzeros),
+                rng.randint(n_features, size=n_nonzeros),
+            ),
+        ),
+        shape=(n_samples, n_features),
+    )
     return data_coo.toarray(), data_coo.tocsr()


 def print_row(clf_type, time_fit, time_transform):
-    print("%s | %s | %s" % (clf_type.ljust(30),
-                           ("%.4fs" % time_fit).center(12),
-                           ("%.4fs" % time_transform).center(12)))
+    print(
+        "%s | %s | %s"
+        % (
+            clf_type.ljust(30),
+            ("%.4fs" % time_fit).center(12),
+            ("%.4fs" % time_transform).center(12),
+        )
+    )


 if __name__ == "__main__":
@@ -87,53 +98,89 @@
     # Option parser
     ###########################################################################
     op = optparse.OptionParser()
-    op.add_option("--n-times",
-                  dest="n_times", default=5, type=int,
-                  help="Benchmark results are average over n_times experiments")
-
-    op.add_option("--n-features",
-                  dest="n_features", default=10 ** 4, type=int,
-                  help="Number of features in the benchmarks")
-
-    op.add_option("--n-components",
-                  dest="n_components", default="auto",
-                  help="Size of the random subspace."
-                       " ('auto' or int > 0)")
-
-    op.add_option("--ratio-nonzeros",
-                  dest="ratio_nonzeros", default=10 ** -3, type=float,
-                  help="Number of features in the benchmarks")
-
-    op.add_option("--n-samples",
-                  dest="n_samples", default=500, type=int,
-                  help="Number of samples in the benchmarks")
-
-    op.add_option("--random-seed",
-                  dest="random_seed", default=13, type=int,
-                  help="Seed used by the random number generators.")
-
-    op.add_option("--density",
-                  dest="density", default=1 / 3,
-                  help="Density used by the sparse random projection."
-                       " ('auto' or float (0.0, 1.0]")
-
-    op.add_option("--eps",
-                  dest="eps", default=0.5, type=float,
-                  help="See the documentation of the underlying transformers.")
-
-    op.add_option("--transformers",
-                  dest="selected_transformers",
-                  default='GaussianRandomProjection,SparseRandomProjection',
-                  type=str,
-                  help="Comma-separated list of transformer to benchmark. "
-                       "Default: %default. Available: "
-                       "GaussianRandomProjection,SparseRandomProjection")
-
-    op.add_option("--dense",
-                  dest="dense",
-                  default=False,
-                  action="store_true",
-                  help="Set input space as a dense matrix.")
+    op.add_option(
+        "--n-times",
+        dest="n_times",
+        default=5,
+        type=int,
+        help="Benchmark results are average over n_times experiments",
+    )
+
+    op.add_option(
+        "--n-features",
+        dest="n_features",
+        default=10**4,
+        type=int,
+        help="Number of features in the benchmarks",
+    )
+
+    op.add_option(
+        "--n-components",
+        dest="n_components",
+        default="auto",
+        help="Size of the random subspace. ('auto' or int > 0)",
+    )
+
+    op.add_option(
+        "--ratio-nonzeros",
+        dest="ratio_nonzeros",
+        default=10**-3,
+        type=float,
+        help="Number of features in the benchmarks",
+    )
+
+    op.add_option(
+        "--n-samples",
+        dest="n_samples",
+        default=500,
+        type=int,
+        help="Number of samples in the benchmarks",
+    )
+
+    op.add_option(
+        "--random-seed",
+        dest="random_seed",
+        default=13,
+        type=int,
+        help="Seed used by the random number generators.",
+    )
+
+    op.add_option(
+        "--density",
+        dest="density",
+        default=1 / 3,
+        help=(
+            "Density used by the sparse random projection. ('auto' or float (0.0, 1.0]"
+        ),
+    )
+
+    op.add_option(
+        "--eps",
+        dest="eps",
+        default=0.5,
+        type=float,
+        help="See the documentation of the underlying transformers.",
+    )
+
+    op.add_option(
+        "--transformers",
+        dest="selected_transformers",
+        default="GaussianRandomProjection,SparseRandomProjection",
+        type=str,
+        help=(
+            "Comma-separated list of transformer to benchmark. "
+            "Default: %default. Available: "
+            "GaussianRandomProjection,SparseRandomProjection"
+        ),
+    )
+
+    op.add_option(
+        "--dense",
+        dest="dense",
+        default=False,
+        action="store_true",
+        help="Set input space as a dense matrix.",
+    )

     (opts, args) = op.parse_args()
     if len(args) > 0:
@@ -141,27 +188,28 @@
         sys.exit(1)
     opts.n_components = type_auto_or_int(opts.n_components)
     opts.density = type_auto_or_float(opts.density)
-    selected_transformers = opts.selected_transformers.split(',')
+    selected_transformers = opts.selected_transformers.split(",")

     ###########################################################################
     # Generate dataset
     ###########################################################################
     n_nonzeros = int(opts.ratio_nonzeros * opts.n_features)

-    print('Dataset statics')
+    print("Dataset statistics")
     print("===========================")
-    print('n_samples \t= %s' % opts.n_samples)
-    print('n_features \t= %s' % opts.n_features)
+    print("n_samples \t= %s" % opts.n_samples)
+    print("n_features \t= %s" % opts.n_features)
     if opts.n_components == "auto":
-        print('n_components \t= %s (auto)' %
-              johnson_lindenstrauss_min_dim(n_samples=opts.n_samples,
-                                            eps=opts.eps))
+        print(
+            "n_components \t= %s (auto)"
+            % johnson_lindenstrauss_min_dim(n_samples=opts.n_samples, eps=opts.eps)
+        )
     else:
-        print('n_components \t= %s' % opts.n_components)
-    print('n_elements \t= %s' % (opts.n_features * opts.n_samples))
-    print('n_nonzeros \t= %s per feature' % n_nonzeros)
-    print('ratio_nonzeros \t= %s' % opts.ratio_nonzeros)
-    print('')
+        print("n_components \t= %s" % opts.n_components)
+    print("n_elements \t= %s" % (opts.n_features * opts.n_samples))
+    print("n_nonzeros \t= %s per feature" % n_nonzeros)
+    print("ratio_nonzeros \t= %s" % opts.ratio_nonzeros)
+    print("")

     ###########################################################################
     # Set transformer input
@@ -172,10 +220,11 @@
     # Set GaussianRandomProjection input
     gaussian_matrix_params = {
         "n_components": opts.n_components,
-        "random_state": opts.random_seed
+        "random_state": opts.random_seed,
     }
-    transformers["GaussianRandomProjection"] = \
-        GaussianRandomProjection(**gaussian_matrix_params)
+    transformers["GaussianRandomProjection"] = GaussianRandomProjection(
+        **gaussian_matrix_params
+    )

     ###########################################################################
     # Set SparseRandomProjection input
@@ -186,8 +235,9 @@
         "eps": opts.eps,
     }

-    transformers["SparseRandomProjection"] = \
-        SparseRandomProjection(**sparse_matrix_params)
+    transformers["SparseRandomProjection"] = SparseRandomProjection(
+        **sparse_matrix_params
+    )

     ###########################################################################
     # Perform benchmark
@@ -195,13 +245,12 @@
     time_fit = collections.defaultdict(list)
     time_transform = collections.defaultdict(list)

-    print('Benchmarks')
+    print("Benchmarks")
     print("===========================")
     print("Generate dataset benchmarks... ", end="")
-    X_dense, X_sparse = make_sparse_random_data(opts.n_samples,
-                                                opts.n_features,
-                                                n_nonzeros,
-                                                random_state=opts.random_seed)
+    X_dense, X_sparse = make_sparse_random_data(
+        opts.n_samples, opts.n_features, n_nonzeros, random_state=opts.random_seed
+    )
     X = X_dense if opts.dense else X_sparse
     print("done")

@@ -210,8 +259,9 @@

         for iteration in range(opts.n_times):
             print("\titer %s..." % iteration, end="")
-            time_to_fit, time_to_transform = bench_scikit_transformer(X_dense,
-              transformers[name])
+            time_to_fit, time_to_transform = bench_scikit_transformer(
+                X_dense, transformers[name]
+            )
             time_fit[name].append(time_to_fit)
             time_transform[name].append(time_to_transform)
             print("done")
@@ -224,27 +274,30 @@
     print("Script arguments")
     print("===========================")
     arguments = vars(opts)
-    print("%s \t | %s " % ("Arguments".ljust(16),
-                           "Value".center(12),))
+    print(
+        "%s \t | %s "
+        % (
+            "Arguments".ljust(16),
+            "Value".center(12),
+        )
+    )
     print(25 * "-" + ("|" + "-" * 14) * 1)
     for key, value in arguments.items():
-        print("%s \t | %s " % (str(key).ljust(16),
-                               str(value).strip().center(12)))
+        print("%s \t | %s " % (str(key).ljust(16), str(value).strip().center(12)))
     print("")

     print("Transformer performance:")
     print("===========================")
     print("Results are averaged over %s repetition(s)." % opts.n_times)
     print("")
-    print("%s | %s | %s" % ("Transformer".ljust(30),
-                            "fit".center(12),
-                            "transform".center(12)))
+    print(
+        "%s | %s | %s"
+        % ("Transformer".ljust(30), "fit".center(12), "transform".center(12))
+    )
     print(31 * "-" + ("|" + "-" * 14) * 2)

     for name in sorted(selected_transformers):
-        print_row(name,
-                  np.mean(time_fit[name]),
-                  np.mean(time_transform[name]))
-
-    print("")
-    print("")
+        print_row(name, np.mean(time_fit[name]), np.mean(time_transform[name]))
+
+    print("")
+    print("")
('benchmarks', 'bench_lof.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -22,7 +22,7 @@
 import matplotlib.pyplot as plt
 from sklearn.neighbors import LocalOutlierFactor
 from sklearn.metrics import roc_curve, auc
-from sklearn.datasets import fetch_kddcup99, fetch_covtype, fetch_mldata
+from sklearn.datasets import fetch_kddcup99, fetch_covtype, fetch_openml
 from sklearn.preprocessing import LabelBinarizer

 print(__doc__)
@@ -30,30 +30,31 @@
 random_state = 2  # to control the random selection of anomalies in SA

 # datasets available: ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']
-datasets = ['http', 'smtp', 'SA', 'SF', 'shuttle', 'forestcover']
+datasets = ["http", "smtp", "SA", "SF", "shuttle", "forestcover"]

 plt.figure()
 for dataset_name in datasets:
     # loading and vectorization
-    print('loading data')
-    if dataset_name in ['http', 'smtp', 'SA', 'SF']:
-        dataset = fetch_kddcup99(subset=dataset_name, percent10=True,
-                                 random_state=random_state)
+    print("loading data")
+    if dataset_name in ["http", "smtp", "SA", "SF"]:
+        dataset = fetch_kddcup99(
+            subset=dataset_name, percent10=True, random_state=random_state
+        )
         X = dataset.data
         y = dataset.target

-    if dataset_name == 'shuttle':
-        dataset = fetch_mldata('shuttle')
+    if dataset_name == "shuttle":
+        dataset = fetch_openml("shuttle")
         X = dataset.data
         y = dataset.target
         # we remove data with label 4
         # normal data are then those of class 1
-        s = (y != 4)
+        s = y != 4
         X = X[s, :]
         y = y[s]
         y = (y != 1).astype(int)

-    if dataset_name == 'forestcover':
+    if dataset_name == "forestcover":
         dataset = fetch_covtype()
         X = dataset.data
         y = dataset.target
@@ -64,28 +65,28 @@
         y = y[s]
         y = (y != 2).astype(int)

-    print('vectorizing data')
+    print("vectorizing data")

-    if dataset_name == 'SF':
+    if dataset_name == "SF":
         lb = LabelBinarizer()
         x1 = lb.fit_transform(X[:, 1].astype(str))
         X = np.c_[X[:, :1], x1, X[:, 2:]]
-        y = (y != b'normal.').astype(int)
+        y = (y != b"normal.").astype(int)

-    if dataset_name == 'SA':
+    if dataset_name == "SA":
         lb = LabelBinarizer()
         x1 = lb.fit_transform(X[:, 1].astype(str))
         x2 = lb.fit_transform(X[:, 2].astype(str))
         x3 = lb.fit_transform(X[:, 3].astype(str))
         X = np.c_[X[:, :1], x1, x2, x3, X[:, 4:]]
-        y = (y != b'normal.').astype(int)
+        y = (y != b"normal.").astype(int)

-    if dataset_name == 'http' or dataset_name == 'smtp':
-        y = (y != b'normal.').astype(int)
+    if dataset_name == "http" or dataset_name == "smtp":
+        y = (y != b"normal.").astype(int)

     X = X.astype(float)

-    print('LocalOutlierFactor processing...')
+    print("LocalOutlierFactor processing...")
     model = LocalOutlierFactor(n_neighbors=20)
     tstart = time()
     model.fit(X)
@@ -93,14 +94,18 @@
     scoring = -model.negative_outlier_factor_  # the lower, the more normal
     fpr, tpr, thresholds = roc_curve(y, scoring)
     AUC = auc(fpr, tpr)
-    plt.plot(fpr, tpr, lw=1,
-             label=('ROC for %s (area = %0.3f, train-time: %0.2fs)'
-                    % (dataset_name, AUC, fit_time)))
+    plt.plot(
+        fpr,
+        tpr,
+        lw=1,
+        label="ROC for %s (area = %0.3f, train-time: %0.2fs)"
+        % (dataset_name, AUC, fit_time),
+    )

 plt.xlim([-0.05, 1.05])
 plt.ylim([-0.05, 1.05])
-plt.xlabel('False Positive Rate')
-plt.ylabel('True Positive Rate')
-plt.title('Receiver operating characteristic')
+plt.xlabel("False Positive Rate")
+plt.ylabel("True Positive Rate")
+plt.title("Receiver operating characteristic")
 plt.legend(loc="lower right")
 plt.show()
('benchmarks', 'bench_glmnet.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -19,7 +19,7 @@
 import numpy as np
 import gc
 from time import time
-from sklearn.datasets.samples_generator import make_regression
+from sklearn.datasets import make_regression

 alpha = 0.1
 # alpha = 0.01
@@ -35,7 +35,7 @@
     # start time
     tstart = time()
     clf = factory(alpha=alpha).fit(X, Y)
-    delta = (time() - tstart)
+    delta = time() - tstart
     # stop time

     print("duration: %0.3fs" % delta)
@@ -44,9 +44,10 @@
     return delta


-if __name__ == '__main__':
+if __name__ == "__main__":
     from glmnet.elastic_net import Lasso as GlmnetLasso
     from sklearn.linear_model import Lasso as ScikitLasso
+
     # Delayed import of matplotlib.pyplot
     import matplotlib.pyplot as plt

@@ -58,18 +59,22 @@
     n_informative = n_features / 10
     n_test_samples = 1000
     for i in range(1, n + 1):
-        print('==================')
-        print('Iteration %s of %s' % (i, n))
-        print('==================')
+        print("==================")
+        print("Iteration %s of %s" % (i, n))
+        print("==================")

         X, Y, coef_ = make_regression(
-            n_samples=(i * step) + n_test_samples, n_features=n_features,
-            noise=0.1, n_informative=n_informative, coef=True)
+            n_samples=(i * step) + n_test_samples,
+            n_features=n_features,
+            noise=0.1,
+            n_informative=n_informative,
+            coef=True,
+        )

         X_test = X[-n_test_samples:]
         Y_test = Y[-n_test_samples:]
-        X = X[:(i * step)]
-        Y = Y[:(i * step)]
+        X = X[: (i * step)]
+        Y = Y[: (i * step)]

         print("benchmarking scikit-learn: ")
         scikit_results.append(bench(ScikitLasso, X, Y, X_test, Y_test, coef_))
@@ -78,12 +83,12 @@

     plt.clf()
     xx = range(0, n * step, step)
-    plt.title('Lasso regression on sample dataset (%d features)' % n_features)
-    plt.plot(xx, scikit_results, 'b-', label='scikit-learn')
-    plt.plot(xx, glmnet_results, 'r-', label='glmnet')
+    plt.title("Lasso regression on sample dataset (%d features)" % n_features)
+    plt.plot(xx, scikit_results, "b-", label="scikit-learn")
+    plt.plot(xx, glmnet_results, "r-", label="glmnet")
     plt.legend()
-    plt.xlabel('number of samples to classify')
-    plt.ylabel('Time (s)')
+    plt.xlabel("number of samples to classify")
+    plt.ylabel("Time (s)")
     plt.show()

     # now do a benchmark where the number of points is fixed
@@ -96,15 +101,19 @@
     n_samples = 500

     for i in range(1, n + 1):
-        print('==================')
-        print('Iteration %02d of %02d' % (i, n))
-        print('==================')
+        print("==================")
+        print("Iteration %02d of %02d" % (i, n))
+        print("==================")
         n_features = i * step
         n_informative = n_features / 10

         X, Y, coef_ = make_regression(
-            n_samples=(i * step) + n_test_samples, n_features=n_features,
-            noise=0.1, n_informative=n_informative, coef=True)
+            n_samples=(i * step) + n_test_samples,
+            n_features=n_features,
+            noise=0.1,
+            n_informative=n_informative,
+            coef=True,
+        )

         X_test = X[-n_test_samples:]
         Y_test = Y[-n_test_samples:]
@@ -117,12 +126,12 @@
         glmnet_results.append(bench(GlmnetLasso, X, Y, X_test, Y_test, coef_))

     xx = np.arange(100, 100 + n * step, step)
-    plt.figure('scikit-learn vs. glmnet benchmark results')
-    plt.title('Regression in high dimensional spaces (%d samples)' % n_samples)
-    plt.plot(xx, scikit_results, 'b-', label='scikit-learn')
-    plt.plot(xx, glmnet_results, 'r-', label='glmnet')
+    plt.figure("scikit-learn vs. glmnet benchmark results")
+    plt.title("Regression in high dimensional spaces (%d samples)" % n_samples)
+    plt.plot(xx, scikit_results, "b-", label="scikit-learn")
+    plt.plot(xx, glmnet_results, "r-", label="glmnet")
     plt.legend()
-    plt.xlabel('number of features')
-    plt.ylabel('Time (s)')
-    plt.axis('tight')
+    plt.xlabel("number of features")
+    plt.ylabel("Time (s)")
+    plt.axis("tight")
     plt.show()
('benchmarks', 'bench_text_vectorizers.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -16,8 +16,11 @@
 from memory_profiler import memory_usage

 from sklearn.datasets import fetch_20newsgroups
-from sklearn.feature_extraction.text import (CountVectorizer, TfidfVectorizer,
-                                             HashingVectorizer)
+from sklearn.feature_extraction.text import (
+    CountVectorizer,
+    TfidfVectorizer,
+    HashingVectorizer,
+)

 n_repeat = 3

@@ -26,47 +29,46 @@
     def f():
         vect = Vectorizer(**params)
         vect.fit_transform(X)
+
     return f


-text = fetch_20newsgroups(subset='train').data[:1000]
+text = fetch_20newsgroups(subset="train").data[:1000]

-print("="*80 + '\n#' + "    Text vectorizers benchmark" + '\n' + '='*80 + '\n')
-print("Using a subset of the 20 newsrgoups dataset ({} documents)."
-      .format(len(text)))
+print("=" * 80 + "\n#" + "    Text vectorizers benchmark" + "\n" + "=" * 80 + "\n")
+print("Using a subset of the 20 newsgroups dataset ({} documents).".format(len(text)))
 print("This benchmarks runs in ~1 min ...")

 res = []

 for Vectorizer, (analyzer, ngram_range) in itertools.product(
-            [CountVectorizer, TfidfVectorizer, HashingVectorizer],
-            [('word', (1, 1)),
-             ('word', (1, 2)),
-             ('char', (4, 4)),
-             ('char_wb', (4, 4))
-             ]):
+    [CountVectorizer, TfidfVectorizer, HashingVectorizer],
+    [("word", (1, 1)), ("word", (1, 2)), ("char", (4, 4)), ("char_wb", (4, 4))],
+):

-    bench = {'vectorizer': Vectorizer.__name__}
-    params = {'analyzer': analyzer, 'ngram_range': ngram_range}
+    bench = {"vectorizer": Vectorizer.__name__}
+    params = {"analyzer": analyzer, "ngram_range": ngram_range}
     bench.update(params)
-    dt = timeit.repeat(run_vectorizer(Vectorizer, text, **params),
-                       number=1,
-                       repeat=n_repeat)
-    bench['time'] = "{:.3f} (+-{:.3f})".format(np.mean(dt), np.std(dt))
+    dt = timeit.repeat(
+        run_vectorizer(Vectorizer, text, **params), number=1, repeat=n_repeat
+    )
+    bench["time"] = "{:.3f} (+-{:.3f})".format(np.mean(dt), np.std(dt))

     mem_usage = memory_usage(run_vectorizer(Vectorizer, text, **params))

-    bench['memory'] = "{:.1f}".format(np.max(mem_usage))
+    bench["memory"] = "{:.1f}".format(np.max(mem_usage))

     res.append(bench)


-df = pd.DataFrame(res).set_index(['analyzer', 'ngram_range', 'vectorizer'])
+df = pd.DataFrame(res).set_index(["analyzer", "ngram_range", "vectorizer"])

-print('\n========== Run time performance (sec) ===========\n')
-print('Computing the mean and the standard deviation '
-      'of the run time over {} runs...\n'.format(n_repeat))
-print(df['time'].unstack(level=-1))
+print("\n========== Run time performance (sec) ===========\n")
+print(
+    "Computing the mean and the standard deviation "
+    "of the run time over {} runs...\n".format(n_repeat)
+)
+print(df["time"].unstack(level=-1))

-print('\n=============== Memory usage (MB) ===============\n')
-print(df['memory'].unstack(level=-1))
+print("\n=============== Memory usage (MB) ===============\n")
+print(df["memory"].unstack(level=-1))
('benchmarks', 'bench_glm.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -9,7 +9,7 @@
 from sklearn import linear_model


-if __name__ == '__main__':
+if __name__ == "__main__":

     import matplotlib.pyplot as plt

@@ -23,7 +23,7 @@

     for i in range(n_iter):

-        print('Iteration %s of %s' % (i, n_iter))
+        print("Iteration %s of %s" % (i, n_iter))

         n_samples, n_features = 10 * i + 3, 10 * i + 3

@@ -31,7 +31,7 @@
         Y = np.random.randn(n_samples)

         start = datetime.now()
-        ridge = linear_model.Ridge(alpha=1.)
+        ridge = linear_model.Ridge(alpha=1.0)
         ridge.fit(X, Y)
         time_ridge[i] = (datetime.now() - start).total_seconds()

@@ -45,13 +45,13 @@
         lasso.fit(X, Y)
         time_lasso[i] = (datetime.now() - start).total_seconds()

-    plt.figure('scikit-learn GLM benchmark results')
-    plt.xlabel('Dimensions')
-    plt.ylabel('Time (s)')
-    plt.plot(dimensions, time_ridge, color='r')
-    plt.plot(dimensions, time_ols, color='g')
-    plt.plot(dimensions, time_lasso, color='b')
+    plt.figure("scikit-learn GLM benchmark results")
+    plt.xlabel("Dimensions")
+    plt.ylabel("Time (s)")
+    plt.plot(dimensions, time_ridge, color="r")
+    plt.plot(dimensions, time_ols, color="g")
+    plt.plot(dimensions, time_lasso, color="b")

-    plt.legend(['Ridge', 'OLS', 'LassoLars'], loc='upper left')
-    plt.axis('tight')
+    plt.legend(["Ridge", "OLS", "LassoLars"], loc="upper left")
+    plt.axis("tight")
     plt.show()
('build_tools', 'Makefile')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,4 +1,4 @@
 # Makefile for maintenance tools

 authors:
-	python generate_authors_table.py > ../doc/authors.rst
+	python generate_authors_table.py
('build_tools', 'generate_authors_table.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -9,98 +9,197 @@
 import sys
 import requests
 import getpass
-
-# With authentication: up to 5000 requests per hour.
+import time
+from pathlib import Path
+from os import path
+
 print("user:", file=sys.stderr)
 user = input()
-passwd = getpass.getpass("Password or access token:\n")
-auth = (user, passwd)
-
-ROW_SIZE = 7
-LOGO_URL = 'https://avatars2.githubusercontent.com/u/365630?v=4'
-
-
-def group_iterable(iterable, size):
-    """Group iterable into lines"""
-    group = []
-    for element in iterable:
-        group.append(element)
-        if len(group) == size:
-            yield group
-            group = []
-    if len(group) != 0:
-        yield group
+token = getpass.getpass("access token:\n")
+auth = (user, token)
+
+LOGO_URL = "https://avatars2.githubusercontent.com/u/365630?v=4"
+REPO_FOLDER = Path(path.abspath(__file__)).parent.parent
+
+
+def get(url):
+    for sleep_time in [10, 30, 0]:
+        reply = requests.get(url, auth=auth)
+        api_limit = (
+            "message" in reply.json()
+            and "API rate limit exceeded" in reply.json()["message"]
+        )
+        if not api_limit:
+            break
+        print("API rate limit exceeded, waiting..")
+        time.sleep(sleep_time)
+
+    reply.raise_for_status()
+    return reply


 def get_contributors():
     """Get the list of contributor profiles. Require admin rights."""
-    # get members of scikit-learn teams on GitHub
+    # get core devs and contributor experience team
+    core_devs = []
+    contributor_experience_team = []
+    comm_team = []
+    core_devs_slug = "core-devs"
+    contributor_experience_team_slug = "contributor-experience-team"
+    comm_team_slug = "communication-team"
+
+    entry_point = "https://api.github.com/orgs/scikit-learn/"
+
+    for team_slug, lst in zip(
+        (core_devs_slug, contributor_experience_team_slug, comm_team_slug),
+        (core_devs, contributor_experience_team, comm_team),
+    ):
+        for page in [1, 2]:  # 30 per page
+            reply = get(f"{entry_point}teams/{team_slug}/members?page={page}")
+            lst.extend(reply.json())
+
+    # get members of scikit-learn on GitHub
     members = []
-    team = 11523
-    for page in [1, 2]:  # 30 per page
-        reply = requests.get(
-            "https://api.github.com/teams/%d/members?page=%d"
-            % (team, page), auth=auth)
-        reply.raise_for_status()
+    for page in [1, 2, 3]:  # 30 per page
+        reply = get(f"{entry_point}members?page={page}")
         members.extend(reply.json())

     # keep only the logins
-    logins = [c['login'] for c in members]
-    # remove duplicate
-    logins = set(logins)
+    core_devs = set(c["login"] for c in core_devs)
+    contributor_experience_team = set(c["login"] for c in contributor_experience_team)
+    comm_team = set(c["login"] for c in comm_team)
+    members = set(c["login"] for c in members)
+
+    # add missing contributors with GitHub accounts
+    members |= {"dubourg", "mbrucher", "thouis", "jarrodmillman"}
+    # add missing contributors without GitHub accounts
+    members |= {"Angel Soler Gollonet"}
+    # remove CI bots
+    members -= {"sklearn-ci", "sklearn-lgtm", "sklearn-wheels"}
+    contributor_experience_team -= (
+        core_devs  # remove ogrisel from contributor_experience_team
+    )
+
+    emeritus = members - core_devs - contributor_experience_team - comm_team
+
+    # hard coded
+    emeritus_comm_team = {"reshamas"}
+
+    comm_team -= {"reshamas"}  # in the comm team but not on the web page

     # get profiles from GitHub
-    profiles = [get_profile(login) for login in logins]
+    core_devs = [get_profile(login) for login in core_devs]
+    emeritus = [get_profile(login) for login in emeritus]
+    contributor_experience_team = [
+        get_profile(login) for login in contributor_experience_team
+    ]
+    comm_team = [get_profile(login) for login in comm_team]
+    emeritus_comm_team = [get_profile(login) for login in emeritus_comm_team]
+
     # sort by last name
-    profiles = sorted(profiles, key=key)
-
-    return profiles
+    core_devs = sorted(core_devs, key=key)
+    emeritus = sorted(emeritus, key=key)
+    contributor_experience_team = sorted(contributor_experience_team, key=key)
+    comm_team = sorted(comm_team, key=key)
+    emeritus_comm_team = sorted(emeritus_comm_team, key=key)
+
+    return (
+        core_devs,
+        emeritus,
+        contributor_experience_team,
+        comm_team,
+        emeritus_comm_team,
+    )


 def get_profile(login):
     """Get the GitHub profile from login"""
-    profile = requests.get("https://api.github.com/users/%s" % login,
-                           auth=auth).json()
-    if 'name' not in profile:
-        # default profile if the login does not exist
+    print("get profile for %s" % (login,))
+    try:
+        profile = get("https://api.github.com/users/%s" % login).json()
+    except requests.exceptions.HTTPError:
         return dict(name=login, avatar_url=LOGO_URL, html_url="")
-    else:
-        if profile["name"] is None:
-            profile["name"] = profile["login"]
-
-        # fix missing names
-        missing_names = {'bthirion': 'Bertrand Thirion',
-                         'Duchesnay': 'Edouard Duchesnay',
-                         'Lars': 'Lars Buitinck',
-                         'MechCoder': 'Manoj Kumar'}
-        if profile["name"] in missing_names:
-            profile["name"] = missing_names[profile["name"]]
-        return profile
+
+    if profile["name"] is None:
+        profile["name"] = profile["login"]
+
+    # fix missing names
+    missing_names = {
+        "bthirion": "Bertrand Thirion",
+        "dubourg": "Vincent Dubourg",
+        "Duchesnay": "Edouard Duchesnay",
+        "Lars": "Lars Buitinck",
+        "MechCoder": "Manoj Kumar",
+    }
+    if profile["name"] in missing_names:
+        profile["name"] = missing_names[profile["name"]]
+
+    return profile


 def key(profile):
-    """Get the last name in lower case"""
-    return profile["name"].split(' ')[-1].lower()
-
-
-contributors = get_contributors()
-
-print(".. raw :: html\n")
-print("    <!-- Generated by generate_authors_table.py -->")
-print("    <table>")
-print("    <col style='width:%d%%' span='%d'>"
-      % (int(100 / ROW_SIZE), ROW_SIZE))
-print("    <style>")
-print("      img.avatar {border-radius: 10px;}")
-print("      td {vertical-align: top;}")
-print("    </style>")
-for row in group_iterable(contributors, size=ROW_SIZE):
-    print("    <tr>")
-    for contributor in row:
-        print("    <td>")
-        print("    <a href='%s'><img src='%s' class='avatar' /></a> <br />"
-              % (contributor["html_url"], contributor["avatar_url"]))
-        print("    <p>%s</p>" % contributor["name"])
-        print("    </td>")
-    print("    </tr>")
-print("    </table>")
+    """Get a sorting key based on the lower case last name, then firstname"""
+    components = profile["name"].lower().split(" ")
+    return " ".join([components[-1]] + components[:-1])
+
+
+def generate_table(contributors):
+    lines = [
+        ".. raw :: html\n",
+        "    <!-- Generated by generate_authors_table.py -->",
+        '    <div class="sk-authors-container">',
+        "    <style>",
+        "      img.avatar {border-radius: 10px;}",
+        "    </style>",
+    ]
+    for contributor in contributors:
+        lines.append("    <div>")
+        lines.append(
+            "    <a href='%s'><img src='%s' class='avatar' /></a> <br />"
+            % (contributor["html_url"], contributor["avatar_url"])
+        )
+        lines.append("    <p>%s</p>" % (contributor["name"],))
+        lines.append("    </div>")
+    lines.append("    </div>")
+    return "\n".join(lines)
+
+
+def generate_list(contributors):
+    lines = []
+    for contributor in contributors:
+        lines.append("- %s" % (contributor["name"],))
+    return "\n".join(lines)
+
+
+if __name__ == "__main__":
+
+    (
+        core_devs,
+        emeritus,
+        contributor_experience_team,
+        comm_team,
+        emeritus_comm_team,
+    ) = get_contributors()
+
+    with open(REPO_FOLDER / "doc" / "authors.rst", "w+", encoding="utf-8") as rst_file:
+        rst_file.write(generate_table(core_devs))
+
+    with open(
+        REPO_FOLDER / "doc" / "authors_emeritus.rst", "w+", encoding="utf-8"
+    ) as rst_file:
+        rst_file.write(generate_list(emeritus))
+
+    with open(
+        REPO_FOLDER / "doc" / "contributor_experience_team.rst", "w+", encoding="utf-8"
+    ) as rst_file:
+        rst_file.write(generate_table(contributor_experience_team))
+
+    with open(
+        REPO_FOLDER / "doc" / "communication_team.rst", "w+", encoding="utf-8"
+    ) as rst_file:
+        rst_file.write(generate_table(comm_team))
+
+    with open(
+        REPO_FOLDER / "doc" / "communication_team_emeritus.rst", "w+", encoding="utf-8"
+    ) as rst_file:
+        rst_file.write(generate_list(emeritus_comm_team))
('build_tools/azure', 'windows.yml')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -3,22 +3,33 @@
   name: ''
   vmImage: ''
   matrix: []
+  dependsOn: []
+  condition: ne(variables['Build.Reason'], 'Schedule')

 jobs:
 - job: ${{ parameters.name }}
+  dependsOn: ${{ parameters.dependsOn }}
+  condition: ${{ parameters.condition }}
   pool:
     vmImage: ${{ parameters.vmImage }}
   variables:
     VIRTUALENV: 'testvenv'
     JUNITXML: 'test-data.xml'
     SKLEARN_SKIP_NETWORK_TESTS: '1'
-    TMP_FOLDER: '$(Agent.WorkFolder)\tmp_folder'
+    PYTEST_VERSION: '5.2.1'
+    PYTEST_XDIST_VERSION: 'latest'
+    TEST_DIR: '$(Agent.WorkFolder)/tmp_folder'
+    SHOW_SHORT_SUMMARY: 'false'
+    CPU_COUNT: '2'
   strategy:
     matrix:
       ${{ insert }}: ${{ parameters.matrix }}

   steps:
-    - powershell: Write-Host "##vso[task.prependpath]$env:CONDA\Scripts"
+    - bash: python build_tools/azure/get_selected_tests.py
+      displayName: Check selected tests for all random seeds
+      condition: eq(variables['Build.Reason'], 'PullRequest')
+    - bash: echo "##vso[task.prependpath]$CONDA/Scripts"
       displayName: Add conda to PATH for 64 bit Python
       condition: eq(variables['PYTHON_ARCH'], '64')
     - task: UsePythonVersion@0
@@ -28,21 +39,19 @@
         architecture: 'x86'
       displayName: Use 32 bit System Python
       condition: eq(variables['PYTHON_ARCH'], '32')
-    - script: |
-        build_tools\\azure\\install.cmd
+    - bash: ./build_tools/azure/install_win.sh
       displayName: 'Install'
-    - script: |
-        build_tools\\azure\\test_script.cmd
+    - bash: ./build_tools/azure/test_script.sh
       displayName: 'Test Library'
-    - script: |
-        build_tools\\azure\\upload_codecov.cmd
-      condition: and(succeeded(), eq(variables['COVERAGE'], 'true'))
+    - bash: ./build_tools/azure/upload_codecov.sh
+      condition: and(succeeded(), eq(variables['COVERAGE'], 'true'),
+                     eq(variables['SELECTED_TESTS'], ''))
       displayName: 'Upload To Codecov'
       env:
         CODECOV_TOKEN: $(CODECOV_TOKEN)
     - task: PublishTestResults@2
       inputs:
-        testResultsFiles: '$(TMP_FOLDER)\$(JUNITXML)'
+        testResultsFiles: '$(TEST_DIR)/$(JUNITXML)'
         testRunTitle: ${{ format('{0}-$(Agent.JobName)', parameters.name) }}
       displayName: 'Publish Test Results'
       condition: succeededOrFailed()
('build_tools/azure', 'install.sh')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,82 +1,190 @@
 #!/bin/bash

 set -e
+set -x
+
+# defines the get_dep and show_installed_libraries functions
+source build_tools/shared.sh

 UNAMESTR=`uname`

-if [[ "$UNAMESTR" == "Darwin" ]]; then
-    # install OpenMP not present by default on osx
-    HOMEBREW_NO_AUTO_UPDATE=1 brew install libomp
-
-    # enable OpenMP support for Apple-clang
-    export CC=/usr/bin/clang
-    export CXX=/usr/bin/clang++
-    export CPPFLAGS="$CPPFLAGS -Xpreprocessor -fopenmp"
-    export CFLAGS="$CFLAGS -I/usr/local/opt/libomp/include"
-    export CXXFLAGS="$CXXFLAGS -I/usr/local/opt/libomp/include"
-    export LDFLAGS="$LDFLAGS -L/usr/local/opt/libomp/lib -lomp"
-    export DYLD_LIBRARY_PATH=/usr/local/opt/libomp/lib
-fi
-
 make_conda() {
     TO_INSTALL="$@"
-    conda create -n $VIRTUALENV --yes $TO_INSTALL
+    if [[ "$DISTRIB" == *"mamba"* ]]; then
+        mamba create -n $VIRTUALENV --yes $TO_INSTALL
+    else
+        conda config --show
+        conda create -n $VIRTUALENV --yes $TO_INSTALL
+    fi
     source activate $VIRTUALENV
 }

-if [[ "$DISTRIB" == "conda" ]]; then
-    TO_INSTALL="python=$PYTHON_VERSION pip pytest pytest-cov \
-                numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION \
-                cython=$CYTHON_VERSION joblib=$JOBLIB_VERSION"
+setup_ccache() {
+    echo "Setting up ccache with CCACHE_DIR=${CCACHE_DIR}"
+    mkdir /tmp/ccache/
+    which ccache
+    for name in gcc g++ cc c++ clang clang++ i686-linux-gnu-gcc i686-linux-gnu-c++ x86_64-linux-gnu-gcc x86_64-linux-gnu-c++ x86_64-apple-darwin13.4.0-clang x86_64-apple-darwin13.4.0-clang++; do
+      ln -s $(which ccache) "/tmp/ccache/${name}"
+    done
+    export PATH="/tmp/ccache/:${PATH}"
+    ccache -M 256M
+}

-    if [[ "$INSTALL_MKL" == "true" ]]; then
-        TO_INSTALL="$TO_INSTALL mkl"
-    else
-        TO_INSTALL="$TO_INSTALL nomkl"
+pre_python_environment_install() {
+    if [[ "$DISTRIB" == "ubuntu" ]]; then
+        sudo add-apt-repository --remove ppa:ubuntu-toolchain-r/test
+        sudo apt-get update
+        sudo apt-get install python3-scipy python3-matplotlib \
+             libatlas3-base libatlas-base-dev python3-virtualenv ccache
+
+    elif [[ "$DISTRIB" == "debian-32" ]]; then
+        apt-get update
+        apt-get install -y python3-dev python3-numpy python3-scipy \
+                python3-matplotlib libatlas3-base libatlas-base-dev \
+                python3-virtualenv python3-pandas ccache
+
+    elif [[ "$DISTRIB" == "conda-mamba-pypy3" ]]; then
+        # condaforge/mambaforge-pypy3 needs compilers
+        apt-get -yq update
+        apt-get -yq install build-essential
+
+    elif [[ "$BUILD_WITH_ICC" == "true" ]]; then
+        wget https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB
+        sudo apt-key add GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB
+        rm GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB
+        sudo add-apt-repository "deb https://apt.repos.intel.com/oneapi all main"
+        sudo apt-get update
+        sudo apt-get install intel-oneapi-compiler-dpcpp-cpp-and-cpp-classic
+        source /opt/intel/oneapi/setvars.sh
+    fi
+}
+
+python_environment_install() {
+    if [[ "$DISTRIB" == "conda" || "$DISTRIB" == *"mamba"* ]]; then
+
+        if [[ "$CONDA_CHANNEL" != "" ]]; then
+            TO_INSTALL="--override-channels -c $CONDA_CHANNEL"
+        else
+            TO_INSTALL=""
+        fi
+
+        if [[ "$DISTRIB" == *"pypy"* ]]; then
+            TO_INSTALL="$TO_INSTALL pypy"
+        else
+            TO_INSTALL="$TO_INSTALL python=$PYTHON_VERSION"
+        fi
+
+        TO_INSTALL="$TO_INSTALL ccache pip blas[build=$BLAS]"
+
+        TO_INSTALL="$TO_INSTALL $(get_dep numpy $NUMPY_VERSION)"
+        TO_INSTALL="$TO_INSTALL $(get_dep scipy $SCIPY_VERSION)"
+        TO_INSTALL="$TO_INSTALL $(get_dep cython $CYTHON_VERSION)"
+        TO_INSTALL="$TO_INSTALL $(get_dep joblib $JOBLIB_VERSION)"
+        TO_INSTALL="$TO_INSTALL $(get_dep pandas $PANDAS_VERSION)"
+        TO_INSTALL="$TO_INSTALL $(get_dep pyamg $PYAMG_VERSION)"
+        TO_INSTALL="$TO_INSTALL $(get_dep Pillow $PILLOW_VERSION)"
+        TO_INSTALL="$TO_INSTALL $(get_dep matplotlib $MATPLOTLIB_VERSION)"
+
+        if [[ "$UNAMESTR" == "Darwin" ]] && [[ "$SKLEARN_TEST_NO_OPENMP" != "true" ]]; then
+                TO_INSTALL="$TO_INSTALL compilers llvm-openmp"
+        fi
+
+        make_conda $TO_INSTALL
+
+    elif [[ "$DISTRIB" == "ubuntu" ]] || [[ "$DISTRIB" == "debian-32" ]]; then
+        python3 -m virtualenv --system-site-packages --python=python3 $VIRTUALENV
+        source $VIRTUALENV/bin/activate
+
+        python -m pip install $(get_dep cython $CYTHON_VERSION) \
+                $(get_dep joblib $JOBLIB_VERSION)
+
+    elif [[ "$DISTRIB" == "conda-pip-latest" ]]; then
+        # Since conda main channel usually lacks behind on the latest releases,
+        # we use pypi to test against the latest releases of the dependencies.
+        # conda is still used as a convenient way to install Python and pip.
+        make_conda "ccache python=$PYTHON_VERSION"
+        python -m pip install -U pip
+
+        python -m pip install pandas matplotlib scikit-image pyamg
+        # do not install dependencies for lightgbm since it requires scikit-learn.
+        python -m pip install "lightgbm>=3.0.0" --no-deps
+
+    elif [[ "$DISTRIB" == "conda-pip-scipy-dev" ]]; then
+        make_conda "ccache python=$PYTHON_VERSION"
+        python -m pip install -U pip
+        echo "Installing numpy and scipy master wheels"
+        dev_anaconda_url=https://pypi.anaconda.org/scipy-wheels-nightly/simple
+        pip install --pre --upgrade --timeout=60 --extra-index $dev_anaconda_url numpy pandas scipy
+        pip install --pre cython
+        echo "Installing joblib master"
+        pip install https://github.com/joblib/joblib/archive/master.zip
+        echo "Installing pillow master"
+        pip install https://github.com/python-pillow/Pillow/archive/main.zip
     fi

-    if [[ -n "$PANDAS_VERSION" ]]; then
-        TO_INSTALL="$TO_INSTALL pandas=$PANDAS_VERSION"
+    python -m pip install $(get_dep threadpoolctl $THREADPOOLCTL_VERSION) \
+            $(get_dep pytest $PYTEST_VERSION) \
+            $(get_dep pytest-xdist $PYTEST_XDIST_VERSION)
+
+    if [[ "$COVERAGE" == "true" ]]; then
+        # XXX: coverage is temporary pinned to 6.2 because 6.3 is not fork-safe
+        # cf. https://github.com/nedbat/coveragepy/issues/1310
+        python -m pip install codecov pytest-cov coverage==6.2
     fi

-    if [[ -n "$PYAMG_VERSION" ]]; then
-        TO_INSTALL="$TO_INSTALL pyamg=$PYAMG_VERSION"
+    if [[ "$TEST_DOCSTRINGS" == "true" ]]; then
+        # numpydoc requires sphinx
+        python -m pip install sphinx
+        python -m pip install numpydoc
+    fi
+}
+
+scikit_learn_install() {
+    setup_ccache
+    show_installed_libraries
+
+    # Set parallelism to 3 to overlap IO bound tasks with CPU bound tasks on CI
+    # workers with 2 cores when building the compiled extensions of scikit-learn.
+    export SKLEARN_BUILD_PARALLEL=3
+
+    if [[ "$UNAMESTR" == "Darwin" ]] && [[ "$SKLEARN_TEST_NO_OPENMP" == "true" ]]; then
+        # Without openmp, we use the system clang. Here we use /usr/bin/ar
+        # instead because llvm-ar errors
+        export AR=/usr/bin/ar
     fi

-    if [[ -n "$PILLOW_VERSION" ]]; then
-        TO_INSTALL="$TO_INSTALL pillow=$PILLOW_VERSION"
+    if [[ "$UNAMESTR" == "Linux" ]]; then
+        # FIXME: temporary fix to link against system libraries on linux
+        # https://github.com/scikit-learn/scikit-learn/issues/20640
+        export LDFLAGS="$LDFLAGS -Wl,--sysroot=/"
     fi

-    if [[ -n "$MATPLOTLIB_VERSION" ]]; then
-        TO_INSTALL="$TO_INSTALL matplotlib=$MATPLOTLIB_VERSION"
+    if [[ "$BUILD_WITH_ICC" == "true" ]]; then
+        # The "build_clib" command is implicitly used to build "libsvm-skl".
+        # To compile with a different compiler, we also need to specify the
+        # compiler for this command
+        python setup.py build_ext --compiler=intelem -i build_clib --compiler=intelem
     fi

-	make_conda $TO_INSTALL
+    # TODO use a specific variable for this rather than using a particular build ...
+    if [[ "$DISTRIB" == "conda-pip-latest" ]]; then
+        # Check that pip can automatically build scikit-learn with the build
+        # dependencies specified in pyproject.toml using an isolated build
+        # environment:
+        pip install --verbose --editable .
+    else
+        # Use the pre-installed build dependencies and build directly in the
+        # current environment.
+        python setup.py develop
+    fi

-elif [[ "$DISTRIB" == "ubuntu" ]]; then
-    sudo apt-get install python3-scipy python3-matplotlib libatlas3-base libatlas-base-dev libatlas-dev python3-virtualenv
-    python3 -m virtualenv --system-site-packages --python=python3 $VIRTUALENV
-    source $VIRTUALENV/bin/activate
-    python -m pip install pytest pytest-cov cython joblib==$JOBLIB_VERSION
-fi
+    ccache -s
+}

-if [[ "$COVERAGE" == "true" ]]; then
-    python -m pip install coverage codecov
-fi
+main() {
+    pre_python_environment_install
+    python_environment_install
+    scikit_learn_install
+}

-if [[ "$TEST_DOCSTRINGS" == "true" ]]; then
-    python -m pip install sphinx numpydoc  # numpydoc requires sphinx
-fi
-
-python --version
-python -c "import numpy; print('numpy %s' % numpy.__version__)"
-python -c "import scipy; print('scipy %s' % scipy.__version__)"
-python -c "\
-try:
-    import pandas
-    print('pandas %s' % pandas.__version__)
-except ImportError:
-    print('pandas not installed')
-"
-pip list
-python setup.py develop
+main
('build_tools/azure', 'posix.yml')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -2,26 +2,69 @@
   name: ''
   vmImage: ''
   matrix: []
+  dependsOn: []
+  condition: ''

 jobs:
 - job: ${{ parameters.name }}
+  dependsOn: ${{ parameters.dependsOn }}
+  condition: ${{ parameters.condition }}
+  timeoutInMinutes: 120
   pool:
     vmImage: ${{ parameters.vmImage }}
   variables:
     TEST_DIR: '$(Agent.WorkFolder)/tmp_folder'
     VIRTUALENV: 'testvenv'
     JUNITXML: 'test-data.xml'
-    OMP_NUM_THREADS: '4'
-    OPENBLAS_NUM_THREADS: '4'
+    OMP_NUM_THREADS: '2'
+    OPENBLAS_NUM_THREADS: '2'
+    CPU_COUNT: '2'
     SKLEARN_SKIP_NETWORK_TESTS: '1'
+    CCACHE_DIR: $(Pipeline.Workspace)/ccache
+    CCACHE_COMPRESS: '1'
+    NUMPY_VERSION: 'latest'
+    SCIPY_VERSION: 'latest'
+    CYTHON_VERSION: 'latest'
+    JOBLIB_VERSION: 'latest'
+    PANDAS_VERSION: 'latest'
+    PYAMG_VERSION: 'latest'
+    PILLOW_VERSION: 'latest'
+    MATPLOTLIB_VERSION: 'latest'
+    PYTEST_VERSION: '6.2.5'
+    PYTEST_XDIST_VERSION: 'latest'
+    THREADPOOLCTL_VERSION: 'latest'
+    COVERAGE: 'true'
+    TEST_DOCSTRINGS: 'false'
+    CREATE_ISSUE_ON_TRACKER: 'true'
+    SHOW_SHORT_SUMMARY: 'false'
   strategy:
     matrix:
       ${{ insert }}: ${{ parameters.matrix }}

   steps:
+    - task: UsePythonVersion@0
+      inputs:
+        versionSpec: '3.9'
+        addToPath: false
+      name: pyTools
+      displayName: Select python version to run CI python scripts
+    - bash: $(pyTools.pythonLocation)/bin/python build_tools/azure/get_selected_tests.py
+      displayName: Check selected tests for all random seeds
+      condition: eq(variables['Build.Reason'], 'PullRequest')
     - bash: echo "##vso[task.prependpath]$CONDA/bin"
       displayName: Add conda to PATH
-      condition: eq(variables['DISTRIB'], 'conda')
+      condition: startsWith(variables['DISTRIB'], 'conda')
+    - bash: sudo chown -R $USER $CONDA
+      displayName: Take ownership of conda installation
+      condition: startsWith(variables['DISTRIB'], 'conda')
+    - task: Cache@2
+      inputs:
+        key: '"ccache-v1" | "$(Agent.JobName)" | "$(Build.BuildNumber)"'
+        restoreKeys: |
+          "ccache-v1" | "$(Agent.JobName)"
+        path: $(CCACHE_DIR)
+      displayName: ccache
+      continueOnError: true
     - script: |
         build_tools/azure/install.sh
       displayName: 'Install'
@@ -31,19 +74,46 @@
     - script: |
         build_tools/azure/test_docs.sh
       displayName: 'Test Docs'
+      condition: eq(variables['SELECTED_TESTS'], '')
     - script: |
         build_tools/azure/test_pytest_soft_dependency.sh
       displayName: 'Test Soft Dependency'
-      condition: and(eq(variables['CHECK_PYTEST_SOFT_DEPENDENCY'], 'true'), eq(variables['DISTRIB'], 'conda'))
+      condition: and(eq(variables['CHECK_PYTEST_SOFT_DEPENDENCY'], 'true'),
+                     eq(variables['SELECTED_TESTS'], ''))
     - task: PublishTestResults@2
       inputs:
         testResultsFiles: '$(TEST_DIR)/$(JUNITXML)'
         testRunTitle: ${{ format('{0}-$(Agent.JobName)', parameters.name) }}
       displayName: 'Publish Test Results'
       condition: succeededOrFailed()
+    - bash: |
+        set -ex
+        if [[ $(BOT_GITHUB_TOKEN) == "" ]]; then
+          echo "GitHub Token is not set. Issue tracker will not be updated."
+          exit
+        fi
+
+        LINK_TO_RUN="https://dev.azure.com/$BUILD_REPOSITORY_NAME/_build/results?buildId=$BUILD_BUILDID&view=logs&j=$SYSTEM_JOBID"
+        CI_NAME="$SYSTEM_JOBIDENTIFIER"
+        ISSUE_REPO="$BUILD_REPOSITORY_NAME"
+
+        $(pyTools.pythonLocation)/bin/pip install defusedxml PyGithub
+        $(pyTools.pythonLocation)/bin/python maint_tools/update_tracking_issue.py \
+          $(BOT_GITHUB_TOKEN) \
+          $CI_NAME \
+          $ISSUE_REPO \
+          $LINK_TO_RUN \
+          --junit-file $JUNIT_FILE \
+          --auto-close false
+      displayName: 'Update issue tracker'
+      env:
+        JUNIT_FILE: $(TEST_DIR)/$(JUNITXML)
+      condition: and(succeededOrFailed(), eq(variables['CREATE_ISSUE_ON_TRACKER'], 'true'),
+                     eq(variables['Build.Reason'], 'Schedule'))
     - script: |
         build_tools/azure/upload_codecov.sh
-      condition: and(succeeded(), eq(variables['COVERAGE'], 'true'), eq(variables['DISTRIB'], 'conda'))
+      condition: and(succeeded(), eq(variables['COVERAGE'], 'true'),
+                     eq(variables['SELECTED_TESTS'], ''))
       displayName: 'Upload To Codecov'
       env:
         CODECOV_TOKEN: $(CODECOV_TOKEN)
('build_tools/azure', 'test_pytest_soft_dependency.sh')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -7,6 +7,8 @@
 conda remove -y py pytest || pip uninstall -y py pytest

 if [[ "$COVERAGE" == "true" ]]; then
+    # conda may remove coverage when uninstall pytest and py
+    pip install coverage
     # Need to append the coverage to the existing .coverage generated by
     # running the tests. Make sure to reuse the same coverage
     # configuration as the one used by the main pytest run to be
('build_tools/azure', 'test_docs.sh')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -2,10 +2,14 @@

 set -e

-if [[ "$DISTRIB" == "conda" ]]; then
+if [[ "$DISTRIB" =~ ^conda.* ]]; then
     source activate $VIRTUALENV
 elif [[ "$DISTRIB" == "ubuntu" ]]; then
     source $VIRTUALENV/bin/activate
 fi

+if [[ "$BUILD_WITH_ICC" == "true" ]]; then
+    source /opt/intel/oneapi/setvars.sh
+fi
+
 make test-doc
('build_tools/azure', 'test_script.sh')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -2,40 +2,81 @@

 set -e

-if [[ "$DISTRIB" == "conda" ]]; then
+# defines the show_installed_libraries function
+source build_tools/shared.sh
+
+if [[ "$DISTRIB" =~ ^conda.* ]]; then
     source activate $VIRTUALENV
-elif [[ "$DISTRIB" == "ubuntu" ]]; then
+elif [[ "$DISTRIB" == "ubuntu" ]] || [[ "$DISTRIB" == "debian-32" ]]; then
     source $VIRTUALENV/bin/activate
 fi

-python --version
-python -c "import numpy; print('numpy %s' % numpy.__version__)"
-python -c "import scipy; print('scipy %s' % scipy.__version__)"
-python -c "\
-try:
-    import pandas
-    print('pandas %s' % pandas.__version__)
-except ImportError:
-    print('pandas not installed')
-"
-python -c "import multiprocessing as mp; print('%d CPUs' % mp.cpu_count())"
-pip list
-
-TEST_CMD="python -m pytest --showlocals --durations=20 --junitxml=$JUNITXML"
-
-if [[ "$COVERAGE" == "true" ]]; then
-    export COVERAGE_PROCESS_START="$BUILD_SOURCESDIRECTORY/.coveragerc"
-    TEST_CMD="$TEST_CMD --cov-config=$COVERAGE_PROCESS_START --cov sklearn"
+if [[ "$BUILD_WITH_ICC" == "true" ]]; then
+    source /opt/intel/oneapi/setvars.sh
 fi

-if [[ -n "$CHECK_WARNINGS" ]]; then
-    TEST_CMD="$TEST_CMD -Werror::DeprecationWarning -Werror::FutureWarning"
+if [[ "$BUILD_REASON" == "Schedule" ]]; then
+    # Enable global random seed randomization to discover seed-sensitive tests
+    # only on nightly builds.
+    # https://scikit-learn.org/stable/computing/parallelism.html#environment-variables
+    export SKLEARN_TESTS_GLOBAL_RANDOM_SEED="any"
+
+    # Enable global dtype fixture for all nightly builds to discover
+    # numerical-sensitive tests.
+    # https://scikit-learn.org/stable/computing/parallelism.html#environment-variables
+    export SKLEARN_RUN_FLOAT32_TESTS=1
 fi

 mkdir -p $TEST_DIR
 cp setup.cfg $TEST_DIR
 cd $TEST_DIR

+python -c "import joblib; print(f'Number of cores: {joblib.cpu_count()}')"
+python -c "import sklearn; sklearn.show_versions()"
+
+show_installed_libraries
+
+TEST_CMD="python -m pytest --showlocals --durations=20 --junitxml=$JUNITXML"
+
+if [[ "$COVERAGE" == "true" ]]; then
+    # Note: --cov-report= is used to disable to long text output report in the
+    # CI logs. The coverage data is consolidated by codecov to get an online
+    # web report across all the platforms so there is no need for this text
+    # report that otherwise hides the test failures and forces long scrolls in
+    # the CI logs.
+    export COVERAGE_PROCESS_START="$BUILD_SOURCESDIRECTORY/.coveragerc"
+    TEST_CMD="$TEST_CMD --cov-config='$COVERAGE_PROCESS_START' --cov sklearn --cov-report="
+fi
+
+if [[ -n "$CHECK_WARNINGS" ]]; then
+    TEST_CMD="$TEST_CMD -Werror::DeprecationWarning -Werror::FutureWarning"
+
+    # numpy's 1.19.0's tostring() deprecation is ignored until scipy and joblib
+    # removes its usage
+    TEST_CMD="$TEST_CMD -Wignore:tostring:DeprecationWarning"
+
+    # Python 3.10 deprecates distutils, which is imported by numpy internally
+    TEST_CMD="$TEST_CMD -Wignore:The\ distutils:DeprecationWarning"
+
+    # Ignore distutils deprecation warning, used by joblib internally
+    TEST_CMD="$TEST_CMD -Wignore:distutils\ Version\ classes\ are\ deprecated:DeprecationWarning"
+fi
+
+if [[ "$PYTEST_XDIST_VERSION" != "none" ]]; then
+    TEST_CMD="$TEST_CMD -n$CPU_COUNT"
+fi
+
+if [[ "$SHOW_SHORT_SUMMARY" == "true" ]]; then
+    TEST_CMD="$TEST_CMD -ra"
+fi
+
+if [[ -n "$SELECTED_TESTS" ]]; then
+    TEST_CMD="$TEST_CMD -k $SELECTED_TESTS"
+
+    # Override to make selected tests run on all random seeds
+    export SKLEARN_TESTS_GLOBAL_RANDOM_SEED="all"
+fi
+
 set -x
-$TEST_CMD --pyargs sklearn
+eval "$TEST_CMD --pyargs sklearn"
 set +x
('build_tools/azure', 'upload_codecov.sh')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -9,7 +9,7 @@
 # Need to run codecov from a git checkout, so we copy .coverage
 # from TEST_DIR where pytest has been run
 pushd $TEST_DIR
-coverage combine
+coverage combine --append
 popd
 cp $TEST_DIR/.coverage $BUILD_REPOSITORY_LOCALPATH

('build_tools/circle', 'push_doc.sh')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,5 +1,5 @@
 #!/bin/bash
-# This script is meant to be called in the "deploy" step defined in
+# This script is meant to be called in the "deploy" step defined in
 # circle.yml. See https://circleci.com/docs/ for more details.
 # The behavior of the script is controlled by environment variable defined
 # in the circle.yml in the top level folder of the project.
@@ -23,7 +23,7 @@
 # Absolute path needed because we use cd further down in this script
 GENERATED_DOC_DIR=$(readlink -f $GENERATED_DOC_DIR)

-if [ "$CIRCLE_BRANCH" = "master" ]
+if [ "$CIRCLE_BRANCH" = "main" ]
 then
     dir=dev
 else
@@ -49,8 +49,8 @@
 	touch $dir/index.html
 	git add $dir
 fi
-git checkout master
-git reset --hard origin/master
+git checkout main
+git reset --hard origin/main
 if [ -d $dir ]
 then
 	git rm -rf $dir/ && rm -rf $dir/
@@ -62,4 +62,4 @@
 git add -f $dir/
 git commit -m "$MSG" $dir
 git push
-echo $MSG
+echo $MSG
('build_tools/circle', 'list_versions.py')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -8,11 +8,12 @@
 from distutils.version import LooseVersion
 from urllib.request import urlopen

+
 def json_urlread(url):
     try:
-        return json.loads(urlopen(url).read().decode('utf8'))
+        return json.loads(urlopen(url).read().decode("utf8"))
     except Exception:
-        print('Error reading', url, file=sys.stderr)
+        print("Error reading", url, file=sys.stderr)
         raise


@@ -20,8 +21,7 @@
     # https://stackoverflow.com/questions/1094841/reusable-library-to-get-human-readable-version-of-file-size
     if quantity == 0:
         quantity = +0
-    SUFFIXES = ["B"] + [i + {1000: "B", 1024: "iB"}[multiple]
-                        for i in "KMGTPEZY"]
+    SUFFIXES = ["B"] + [i + {1000: "B", 1024: "iB"}[multiple] for i in "KMGTPEZY"]
     for suffix in SUFFIXES:
         if quantity < multiple or suffix == SUFFIXES[-1]:
             if suffix == SUFFIXES[0]:
@@ -32,43 +32,58 @@
             quantity /= multiple


-def get_pdf_size(version):
-    api_url = ROOT_URL + '%s/_downloads' % version
-    for path_details in json_urlread(api_url):
-        if path_details['name'] == 'scikit-learn-docs.pdf':
-            return human_readable_data_quantity(path_details['size'], 1000)
+def get_file_extension(version):
+    if "dev" in version:
+        # The 'dev' branch should be explicitly handled
+        return "zip"
+
+    current_version = LooseVersion(version)
+    min_zip_version = LooseVersion("0.24")
+
+    return "zip" if current_version >= min_zip_version else "pdf"


-print(':orphan:')
+def get_file_size(version):
+    api_url = ROOT_URL + "%s/_downloads" % version
+    for path_details in json_urlread(api_url):
+        file_extension = get_file_extension(version)
+        file_path = f"scikit-learn-docs.{file_extension}"
+        if path_details["name"] == file_path:
+            return human_readable_data_quantity(path_details["size"], 1000)
+
+
+print(":orphan:")
 print()
-heading = 'Available documentation for Scikit-learn'
+heading = "Available documentation for Scikit-learn"
 print(heading)
-print('=' * len(heading))
+print("=" * len(heading))
 print()
-print('Web-based documentation is available for versions listed below:')
+print("Web-based documentation is available for versions listed below:")
 print()

-ROOT_URL = 'https://api.github.com/repos/scikit-learn/scikit-learn.github.io/contents/'  # noqa
-RAW_FMT = 'https://raw.githubusercontent.com/scikit-learn/scikit-learn.github.io/master/%s/documentation.html'  # noqa
-VERSION_RE = re.compile(r"\bVERSION:\s*'([^']+)'")
-NAMED_DIRS = ['dev', 'stable']
+ROOT_URL = (
+    "https://api.github.com/repos/scikit-learn/scikit-learn.github.io/contents/"  # noqa
+)
+RAW_FMT = "https://raw.githubusercontent.com/scikit-learn/scikit-learn.github.io/master/%s/index.html"  # noqa
+VERSION_RE = re.compile(r"scikit-learn ([\w\.\-]+) documentation</title>")
+NAMED_DIRS = ["dev", "stable"]

 # Gather data for each version directory, including symlinks
 dirs = {}
 symlinks = {}
 root_listing = json_urlread(ROOT_URL)
 for path_details in root_listing:
-    name = path_details['name']
+    name = path_details["name"]
     if not (name[:1].isdigit() or name in NAMED_DIRS):
         continue
-    if path_details['type'] == 'dir':
-        html = urlopen(RAW_FMT % name).read().decode('utf8')
+    if path_details["type"] == "dir":
+        html = urlopen(RAW_FMT % name).read().decode("utf8")
         version_num = VERSION_RE.search(html).group(1)
-        pdf_size = get_pdf_size(name)
-        dirs[name] = (version_num, pdf_size)
+        file_size = get_file_size(name)
+        dirs[name] = (version_num, file_size)

-    if path_details['type'] == 'symlink':
-        symlinks[name] = json_urlread(path_details['_links']['self'])['target']
+    if path_details["type"] == "symlink":
+        symlinks[name] = json_urlread(path_details["_links"]["self"])["target"]


 # Symlinks should have same data as target
@@ -78,20 +93,26 @@

 # Output in order: dev, stable, decreasing other version
 seen = set()
-for name in (NAMED_DIRS +
-             sorted((k for k in dirs if k[:1].isdigit()),
-                    key=LooseVersion, reverse=True)):
-    version_num, pdf_size = dirs[name]
+for name in NAMED_DIRS + sorted(
+    (k for k in dirs if k[:1].isdigit()), key=LooseVersion, reverse=True
+):
+    version_num, file_size = dirs[name]
     if version_num in seen:
         # symlink came first
         continue
     else:
         seen.add(version_num)
-    name_display = '' if name[:1].isdigit() else ' (%s)' % name
-    path = 'http://scikit-learn.org/%s' % name
-    out = ('* `Scikit-learn %s%s documentation <%s/documentation.html>`_'
-           % (version_num, name_display, path))
-    if pdf_size is not None:
-        out += (' (`PDF %s <%s/_downloads/scikit-learn-docs.pdf>`_)'
-                % (pdf_size, path))
+    name_display = "" if name[:1].isdigit() else " (%s)" % name
+    path = "https://scikit-learn.org/%s/" % name
+    out = "* `Scikit-learn %s%s documentation <%s>`_" % (
+        version_num,
+        name_display,
+        path,
+    )
+    if file_size is not None:
+        file_extension = get_file_extension(version_num)
+        out += (
+            f" (`{file_extension.upper()} {file_size} <{path}/"
+            f"_downloads/scikit-learn-docs.{file_extension}>`_)"
+        )
     print(out)
('build_tools/circle', 'checkout_merge_commit.sh')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,9 +1,9 @@
 #!/bin/bash


-# Add `master` branch to the update list.
+# Add `main` branch to the update list.
 # Otherwise CircleCI will give us a cached one.
-FETCH_REFS="+master:master"
+FETCH_REFS="+main:main"

 # Update PR refs for testing.
 if [[ -n "${CIRCLE_PR_NUMBER}" ]]
@@ -20,13 +20,13 @@
 then
     git checkout -qf "pr/${CIRCLE_PR_NUMBER}/merge" || (
         echo Could not fetch merge commit. >&2
-        echo There may be conflicts in merging PR \#${CIRCLE_PR_NUMBER} with master. >&2;
+        echo There may be conflicts in merging PR \#${CIRCLE_PR_NUMBER} with main. >&2;
         exit 1)
 fi

 # Check for merge conflicts.
 if [[ -n "${CIRCLE_PR_NUMBER}" ]]
 then
-    git branch --merged | grep master > /dev/null
+    git branch --merged | grep main > /dev/null
     git branch --merged | grep "pr/${CIRCLE_PR_NUMBER}/head" > /dev/null
 fi
('build_tools/circle', 'build_doc.sh')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -9,7 +9,7 @@
 # instead of relying on the subsequent rules.
 #
 # We always build the documentation for jobs that are not related to a specific
-# PR (e.g. a merge to master or a maintenance branch).
+# PR (e.g. a merge to main or a maintenance branch).
 #
 # If this is a PR, do a full build if there are some files in this PR that are
 # under the "doc/" or "examples/" folders, otherwise perform a quick build.
@@ -49,8 +49,8 @@
         echo BUILD: not a pull request
         return
     fi
-    git_range="origin/master...$CIRCLE_SHA1"
-    git fetch origin master >&2 || (echo QUICK BUILD: failed to get changed filenames for $git_range; return)
+    git_range="origin/main...$CIRCLE_SHA1"
+    git fetch origin main >&2 || (echo QUICK BUILD: failed to get changed filenames for $git_range; return)
     filenames=$(git diff --name-only $git_range)
     if [ -z "$filenames" ]
     then
@@ -58,6 +58,44 @@
         return
     fi
     changed_examples=$(echo "$filenames" | grep -E "^examples/(.*/)*plot_")
+
+    # The following is used to extract the list of filenames of example python
+    # files that sphinx-gallery needs to run to generate png files used as
+    # figures or images in the .rst files  from the documentation.
+    # If the contributor changes a .rst file in a PR we need to run all
+    # the examples mentioned in that file to get sphinx build the
+    # documentation without generating spurious warnings related to missing
+    # png files.
+
+    if [[ -n "$filenames" ]]
+    then
+        # get rst files
+        rst_files="$(echo "$filenames" | grep -E "rst$")"
+
+        # get lines with figure or images
+        img_fig_lines="$(echo "$rst_files" | xargs grep -shE "(figure|image)::")"
+
+        # get only auto_examples
+        auto_example_files="$(echo "$img_fig_lines" | grep auto_examples | awk -F "/" '{print $NF}')"
+
+        # remove "sphx_glr_" from path and accept replace _(\d\d\d|thumb).png with .py
+        scripts_names="$(echo "$auto_example_files" | sed 's/sphx_glr_//' | sed -E 's/_([[:digit:]][[:digit:]][[:digit:]]|thumb).png/.py/')"
+
+        # get unique values
+        examples_in_rst="$(echo "$scripts_names" | uniq )"
+    fi
+
+    # executed only if there are examples in the modified rst files
+    if [[ -n "$examples_in_rst" ]]
+    then
+        if [[ -n "$changed_examples" ]]
+        then
+            changed_examples="$changed_examples|$examples_in_rst"
+        else
+            changed_examples="$examples_in_rst"
+        fi
+    fi
+
     if [[ -n "$changed_examples" ]]
     then
         echo BUILD: detected examples/ filename modified in $git_range: $changed_examples
@@ -76,10 +114,10 @@
     exit 0
 fi

-if [[ "$CIRCLE_BRANCH" =~ ^master$|^[0-9]+\.[0-9]+\.X$ && -z "$CI_PULL_REQUEST" ]]
-then
-    # PDF linked into HTML
-    make_args="dist LATEXMKOPTS=-halt-on-error"
+if [[ "$CIRCLE_BRANCH" =~ ^main$|^[0-9]+\.[0-9]+\.X$ && -z "$CI_PULL_REQUEST" ]]
+then
+    # ZIP linked into HTML
+    make_args=dist
 elif [[ "$build_type" =~ ^QUICK ]]
 then
     make_args=html-noplot
@@ -95,54 +133,81 @@
 make_args="SPHINXOPTS=-T $make_args"  # show full traceback on exception

 # Installing required system packages to support the rendering of math
-# notation in the HTML documentation
-sudo -E apt-get -yq update
-sudo -E apt-get -yq remove texlive-binaries --purge
+# notation in the HTML documentation and to optimize the image files
+sudo -E apt-get -yq update --allow-releaseinfo-change
 sudo -E apt-get -yq --no-install-suggests --no-install-recommends \
-    install dvipng texlive-latex-base texlive-latex-extra \
-    texlive-latex-recommended texlive-fonts-recommended \
-    latexmk gsfonts
+    install dvipng gsfonts ccache zip optipng

 # deactivate circleci virtualenv and setup a miniconda env instead
 if [[ `type -t deactivate` ]]; then
   deactivate
 fi

+MINICONDA_PATH=$HOME/miniconda
 # Install dependencies with miniconda
-wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh \
-   -O miniconda.sh
+wget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh \
+    -O miniconda.sh
 chmod +x miniconda.sh && ./miniconda.sh -b -p $MINICONDA_PATH
-export PATH="$MINICONDA_PATH/bin:$PATH"
-
-# Configure the conda environment and put it in the path using the
-# provided versions
-conda create -n $CONDA_ENV_NAME --yes --quiet python="${PYTHON_VERSION:-*}" \
-  numpy="${NUMPY_VERSION:-*}" scipy="${SCIPY_VERSION:-*}" cython \
-  pytest coverage matplotlib="${MATPLOTLIB_VERSION:-*}" sphinx=1.6.2 pillow \
-  scikit-image="${SCIKIT_IMAGE_VERSION:-*}" pandas="${PANDAS_VERSION:-*}" \
-  joblib
+export PATH="/usr/lib/ccache:$MINICONDA_PATH/bin:$PATH"
+
+ccache -M 512M
+export CCACHE_COMPRESS=1
+
+# Old packages coming from the 'free' conda channel have been removed but we
+# are using them for our min-dependencies doc generation. See
+# https://www.anaconda.com/why-we-removed-the-free-channel-in-conda-4-7/ for
+# more details.
+if [[ "$CIRCLE_JOB" == "doc-min-dependencies" ]]; then
+    conda config --set restore_free_channel true
+fi
+
+# imports get_dep
+source build_tools/shared.sh
+
+# packaging won't be needed once setuptools starts shipping packaging>=17.0
+mamba create -n $CONDA_ENV_NAME --yes --quiet \
+    python="${PYTHON_VERSION:-*}" \
+    "$(get_dep numpy $NUMPY_VERSION)" \
+    "$(get_dep scipy $SCIPY_VERSION)" \
+    "$(get_dep cython $CYTHON_VERSION)" \
+    "$(get_dep matplotlib $MATPLOTLIB_VERSION)" \
+    "$(get_dep sphinx $SPHINX_VERSION)" \
+    "$(get_dep pandas $PANDAS_VERSION)" \
+    joblib memory_profiler packaging seaborn pillow pytest coverage \
+    compilers

 source activate testenv
-pip install "sphinx-gallery>=0.2,<0.3"
-pip install numpydoc==0.9
-
-# Build and install scikit-learn in dev mode
+pip install "$(get_dep scikit-image $SCIKIT_IMAGE_VERSION)"
+pip install "$(get_dep sphinx-gallery $SPHINX_GALLERY_VERSION)"
+pip install "$(get_dep numpydoc $NUMPYDOC_VERSION)"
+pip install "$(get_dep sphinx-prompt $SPHINX_PROMPT_VERSION)"
+pip install "$(get_dep sphinxext-opengraph $SPHINXEXT_OPENGRAPH_VERSION)"
+
+# Set parallelism to 3 to overlap IO bound tasks with CPU bound tasks on CI
+# workers with 2 cores when building the compiled extensions of scikit-learn.
+export SKLEARN_BUILD_PARALLEL=3
 python setup.py develop

-if [[ "$CIRCLE_BRANCH" =~ ^master$ && -z "$CI_PULL_REQUEST" ]]
-then
-    # List available documentation versions if on master
+export OMP_NUM_THREADS=1
+
+if [[ "$CIRCLE_BRANCH" =~ ^main$ && -z "$CI_PULL_REQUEST" ]]
+then
+    # List available documentation versions if on main
     python build_tools/circle/list_versions.py > doc/versions.rst
 fi

 # The pipefail is requested to propagate exit code
 set -o pipefail && cd doc && make $make_args 2>&1 | tee ~/log.txt

+# Insert the version warning for deployment
+find _build/html/stable -name "*.html" | xargs sed -i '/<\/body>/ i \
+\    <script src="https://scikit-learn.org/versionwarning.js"></script>'
+
 cd -
 set +o pipefail

 affected_doc_paths() {
-    files=$(git diff --name-only origin/master...$CIRCLE_SHA1)
+    files=$(git diff --name-only origin/main...$CIRCLE_SHA1)
     echo "$files" | grep ^doc/.*\.rst | sed 's/^doc\/\(.*\)\.rst$/\1.html/'
     echo "$files" | grep ^examples/.*.py | sed 's/^\(.*\)\.py$/auto_\1.html/'
     sklearn_files=$(echo "$files" | grep '^sklearn/')
@@ -152,14 +217,45 @@
     fi
 }

+affected_doc_warnings() {
+    files=$(git diff --name-only origin/main...$CIRCLE_SHA1)
+    # Look for sphinx warnings only in files affected by the PR
+    if [ -n "$files" ]
+    then
+        for af in ${files[@]}
+        do
+          warn+=`grep WARNING ~/log.txt | grep $af`
+        done
+    fi
+    echo "$warn"
+}
+
 if [ -n "$CI_PULL_REQUEST" ]
 then
+    echo "The following documentation warnings may have been generated by PR #$CI_PULL_REQUEST:"
+    warnings=$(affected_doc_warnings)
+    if [ -z "$warnings" ]
+    then
+        warnings="/home/circleci/project/ no warnings"
+    fi
+    echo "$warnings"
+
     echo "The following documentation files may have been changed by PR #$CI_PULL_REQUEST:"
     affected=$(affected_doc_paths)
     echo "$affected"
     (
     echo '<html><body><ul>'
-    echo "$affected" | sed 's|.*|<li><a href="&">&</a></li>|'
-    echo '</ul><p>General: <a href="index.html">Home</a> | <a href="modules/classes.html">API Reference</a> | <a href="auto_examples/index.html">Examples</a></p></body></html>'
+    echo "$affected" | sed 's|.*|<li><a href="&">&</a> [<a href="https://scikit-learn.org/dev/&">dev</a>, <a href="https://scikit-learn.org/stable/&">stable</a>]</li>|'
+    echo '</ul><p>General: <a href="index.html">Home</a> | <a href="modules/classes.html">API Reference</a> | <a href="auto_examples/index.html">Examples</a></p>'
+    echo '<strong>Sphinx Warnings in affected files</strong><ul>'
+    echo "$warnings" | sed 's/\/home\/circleci\/project\//<li>/g'
+    echo '</ul></body></html>'
     ) > 'doc/_build/html/stable/_changed.html'
-fi
+
+    if [ "$warnings" != "/home/circleci/project/ no warnings" ]
+    then
+        echo "Sphinx generated warnings when building the documentation related to files modified in this PR."
+        echo "Please check doc/_build/html/stable/_changed.html"
+        exit 1
+    fi
+fi
('build_tools/circle', 'build_test_pypy.sh')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -2,35 +2,36 @@
 set -x
 set -e

+# System build tools
 apt-get -yq update
-apt-get -yq install libatlas-dev libatlas-base-dev liblapack-dev gfortran ccache
+apt-get -yq install wget bzip2 build-essential ccache

-pip install virtualenv
+# Install pypy and all the scikit-learn dependencies from conda-forge. In
+# particular, we want to install pypy compatible binary packages for numpy and
+# scipy as it would be to costly to build those from source.
+conda install -y mamba
+mamba create -n pypy -y \
+    pypy numpy scipy cython \
+    joblib threadpoolctl pillow pytest \
+    sphinx numpydoc docutils

-if command -v pypy3; then
-    virtualenv -p $(command -v pypy3) pypy-env
-elif command -v pypy; then
-    virtualenv -p $(command -v pypy) pypy-env
-fi
+eval "$(conda shell.bash hook)"
+conda activate pypy

-source pypy-env/bin/activate
-
+# Check that we are running PyPy instead of CPython in this environment.
 python --version
 which python
+python -c "import platform; assert platform.python_implementation() == 'PyPy'"

-# XXX: numpy version pinning can be reverted once PyPy
-#      compatibility is resolved for numpy v1.6.x. For instance,
-#      when PyPy3 >6.0 is released (see numpy/numpy#12740)
-pip install --extra-index https://antocuni.github.io/pypy-wheels/ubuntu "numpy==1.15.*" Cython pytest
-pip install "scipy>=1.1.0" sphinx numpydoc docutils joblib pillow
-
+# Build and install scikit-learn in dev mode
 ccache -M 512M
 export CCACHE_COMPRESS=1
 export PATH=/usr/lib/ccache:$PATH
 export LOKY_MAX_CPU_COUNT="2"
+export OMP_NUM_THREADS="1"
+# Set parallelism to 3 to overlap IO bound tasks with CPU bound tasks on CI
+# workers with 2 cores when building the compiled extensions of scikit-learn.
+export SKLEARN_BUILD_PARALLEL=3
+pip install --no-build-isolation -e .

-pip install -vv -e .
-
-python -m pytest sklearn/
-python -m pytest doc/sphinxext/
-python -m pytest $(find doc -name '*.rst' | sort)
+python -m pytest sklearn
('build_tools/travis', 'after_success.sh')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,19 +1,35 @@
 #!/bin/bash
-# This script is meant to be called by the "after_success" step defined in
-# .travis.yml. See https://docs.travis-ci.com/ for more details.

-# License: 3-clause BSD
+# This script is meant to be called by the "after_success" step
+# defined in ".travis.yml". In particular, we upload the wheels
+# of the ARM64 architecture for the continuous deployment jobs.

 set -e

-if [[ "$COVERAGE" == "true" ]]; then
-    # Need to run codecov from a git checkout, so we copy .coverage
-    # from TEST_DIR where pytest has been run
-    cp $TEST_DIR/.coverage $TRAVIS_BUILD_DIR
+# The wheels cannot be uploaded on PRs
+if [[ $BUILD_WHEEL == true && $TRAVIS_EVENT_TYPE != pull_request ]]; then
+    # Nightly upload token and staging upload token are set in
+    # Travis settings (originally generated at Anaconda cloud)
+    if [[ $TRAVIS_EVENT_TYPE == cron ]]; then
+        ANACONDA_ORG="scipy-wheels-nightly"
+        ANACONDA_TOKEN="$SCIKIT_LEARN_NIGHTLY_UPLOAD_TOKEN"
+    else
+        ANACONDA_ORG="scikit-learn-wheels-staging"
+        ANACONDA_TOKEN="$SCIKIT_LEARN_STAGING_UPLOAD_TOKEN"
+    fi

-    # Ignore codecov failures as the codecov server is not
-    # very reliable but we don't want travis to report a failure
-    # in the github UI just because the coverage report failed to
-    # be published.
-    codecov --root $TRAVIS_BUILD_DIR || echo "codecov upload failed"
+    MINICONDA_URL="https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-aarch64.sh"
+    wget $MINICONDA_URL -O miniconda.sh
+    MINICONDA_PATH=$HOME/miniconda
+    chmod +x miniconda.sh && ./miniconda.sh -b -p $MINICONDA_PATH
+
+    # Install Python 3.8 because of a bug with Python 3.9
+    export PATH=$MINICONDA_PATH/bin:$PATH
+    conda create -n upload -y python=3.8
+    source activate upload
+    conda install -y anaconda-client
+
+    # Force a replacement if the remote file already exists
+    anaconda -t $ANACONDA_TOKEN upload --force -u $ANACONDA_ORG wheelhouse/*.whl
+    echo "Index: https://pypi.anaconda.org/$ANACONDA_ORG/simple"
 fi
('build_tools/travis', 'install.sh')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,150 +1,11 @@
 #!/bin/bash
-# This script is meant to be called by the "install" step defined in
-# .travis.yml. See https://docs.travis-ci.com/ for more details.
-# The behavior of the script is controlled by environment variabled defined
-# in the .travis.yml in the top level folder of the project.

-# License: 3-clause BSD
+# This script is meant to be called by the "install" step
+# defined in the ".travis.yml" file. In particular, it is
+# important that we call to the right installation script.

-# Travis clone scikit-learn/scikit-learn repository in to a local repository.
-# We use a cached directory with three scikit-learn repositories (one for each
-# matrix entry) from which we pull from local Travis repository. This allows
-# us to keep build artefact for gcc + cython, and gain time
-
-set -e
-
-# Fail fast
-build_tools/travis/travis_fastfail.sh
-
-echo 'List files from cached directories'
-echo 'pip:'
-ls $HOME/.cache/pip
-
-if [ $TRAVIS_OS_NAME = "linux" ]
-then
-	export CC=/usr/lib/ccache/gcc
-	export CXX=/usr/lib/ccache/g++
-	# Useful for debugging how ccache is used
-	# export CCACHE_LOGFILE=/tmp/ccache.log
-	# ~60M is used by .ccache when compiling from scratch at the time of writing
-	ccache --max-size 100M --show-stats
-elif [ $TRAVIS_OS_NAME = "osx" ]
-then
-    # enable OpenMP support for Apple-clang
-    export CC=/usr/bin/clang
-    export CXX=/usr/bin/clang++
-    export CPPFLAGS="$CPPFLAGS -Xpreprocessor -fopenmp"
-    export CFLAGS="$CFLAGS -I/usr/local/opt/libomp/include"
-    export CXXFLAGS="$CXXFLAGS -I/usr/local/opt/libomp/include"
-    export LDFLAGS="$LDFLAGS -L/usr/local/opt/libomp/lib -lomp"
-    export DYLD_LIBRARY_PATH=/usr/local/opt/libomp/lib
+if [[ $BUILD_WHEEL == true ]]; then
+    source build_tools/travis/install_wheels.sh || travis_terminate 1
+else
+    source build_tools/travis/install_main.sh || travis_terminate 1
 fi
-
-make_conda() {
-	TO_INSTALL="$@"
-    # Deactivate the travis-provided virtual environment and setup a
-    # conda-based environment instead
-    # If Travvis has language=generic, deactivate does not exist. `|| :` will pass.
-    deactivate || :
-
-    # Install miniconda
-    if [ $TRAVIS_OS_NAME = "osx" ]
-	then
-		fname=Miniconda3-latest-MacOSX-x86_64.sh
-	else
-		fname=Miniconda3-latest-Linux-x86_64.sh
-	fi
-    wget https://repo.continuum.io/miniconda/$fname \
-        -O miniconda.sh
-    MINICONDA_PATH=$HOME/miniconda
-    chmod +x miniconda.sh && ./miniconda.sh -b -p $MINICONDA_PATH
-    export PATH=$MINICONDA_PATH/bin:$PATH
-    conda update --yes conda
-
-    conda create -n testenv --yes $TO_INSTALL
-    source activate testenv
-}
-
-if [[ "$DISTRIB" == "conda" ]]; then
-    TO_INSTALL="python=$PYTHON_VERSION pip pytest pytest-cov \
-                numpy=$NUMPY_VERSION scipy=$SCIPY_VERSION \
-                cython=$CYTHON_VERSION"
-
-    if [[ "$INSTALL_MKL" == "true" ]]; then
-        TO_INSTALL="$TO_INSTALL mkl"
-    else
-        TO_INSTALL="$TO_INSTALL nomkl"
-    fi
-
-    if [[ -n "$PANDAS_VERSION" ]]; then
-        TO_INSTALL="$TO_INSTALL pandas=$PANDAS_VERSION"
-    fi
-
-    if [[ -n "$PYAMG_VERSION" ]]; then
-        TO_INSTALL="$TO_INSTALL pyamg=$PYAMG_VERSION"
-    fi
-
-    if [[ -n "$PILLOW_VERSION" ]]; then
-        TO_INSTALL="$TO_INSTALL pillow=$PILLOW_VERSION"
-    fi
-
-    if [[ -n "$JOBLIB_VERSION" ]]; then
-        TO_INSTALL="$TO_INSTALL joblib=$JOBLIB_VERSION"
-    fi
-	  make_conda $TO_INSTALL
-
-elif [[ "$DISTRIB" == "ubuntu" ]]; then
-    # At the time of writing numpy 1.9.1 is included in the travis
-    # virtualenv but we want to use the numpy installed through apt-get
-    # install.
-    deactivate
-    # Create a new virtualenv using system site packages for python, numpy
-    # and scipy
-    virtualenv --system-site-packages --python=python3 testvenv
-    source testvenv/bin/activate
-    pip install pytest pytest-cov cython joblib==$JOBLIB_VERSION
-
-elif [[ "$DISTRIB" == "scipy-dev" ]]; then
-    make_conda python=3.7
-    pip install --upgrade pip setuptools
-
-    echo "Installing numpy and scipy master wheels"
-    dev_url=https://7933911d6844c6c53a7d-47bd50c35cd79bd838daf386af554a83.ssl.cf2.rackcdn.com
-    pip install --pre --upgrade --timeout=60 -f $dev_url numpy scipy pandas cython
-    echo "Installing joblib master"
-    pip install https://github.com/joblib/joblib/archive/master.zip
-    echo "Installing pillow master"
-    pip install https://github.com/python-pillow/Pillow/archive/master.zip
-    pip install pytest pytest-cov
-fi
-
-if [[ "$COVERAGE" == "true" ]]; then
-    pip install coverage codecov
-fi
-
-if [[ "$TEST_DOCSTRINGS" == "true" ]]; then
-    pip install sphinx numpydoc  # numpydoc requires sphinx
-fi
-
-# Build scikit-learn in the install.sh script to collapse the verbose
-# build output in the travis output when it succeeds.
-python --version
-python -c "import numpy; print('numpy %s' % numpy.__version__)"
-python -c "import scipy; print('scipy %s' % scipy.__version__)"
-python -c "\
-try:
-    import pandas
-    print('pandas %s' % pandas.__version__)
-except ImportError:
-    pass
-"
-python setup.py develop
-if [ $TRAVIS_OS_NAME = "linux" ]
-then
-	ccache --show-stats
-fi
-# Useful for debugging how ccache is used
-# cat $CCACHE_LOGFILE
-
-# fast fail
-build_tools/travis/travis_fastfail.sh
('build_tools/travis', 'test_docs.sh')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,6 +1,8 @@
 #!/bin/bash

 set -e
-set -x

-make test-doc
+if [[ $TRAVIS_CPU_ARCH != arm64 ]]; then
+    # Faster run of the documentation tests
+    PYTEST="pytest -n $CPU_COUNT" make test-doc
+fi
('build_tools/travis', 'test_script.sh')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,51 +1,39 @@
 #!/bin/bash
-# This script is meant to be called by the "script" step defined in
-# .travis.yml. See https://docs.travis-ci.com/ for more details.
-# The behavior of the script is controlled by environment variabled defined
-# in the .travis.yml in the top level folder of the project.
-
-# License: 3-clause BSD

 set -e

 python --version
-python -c "import numpy; print('numpy %s' % numpy.__version__)"
-python -c "import scipy; print('scipy %s' % scipy.__version__)"
+python -c "import numpy; print(f'numpy {numpy.__version__}')"
+python -c "import scipy; print(f'scipy {scipy.__version__}')"
 python -c "\
 try:
     import pandas
-    print('pandas %s' % pandas.__version__)
+    print(f'pandas {pandas.__version__}')
 except ImportError:
     pass
 "
-python -c "import multiprocessing as mp; print('%d CPUs' % mp.cpu_count())"
+python -c "import joblib; print(f'{joblib.cpu_count()} CPUs')"
+python -c "import platform; print(f'{platform.machine()}')"

-run_tests() {
-    TEST_CMD="pytest --showlocals --durations=20 --pyargs"
+TEST_CMD="pytest --showlocals --durations=20 --pyargs"

-    # Get into a temp directory to run test from the installed scikit-learn and
-    # check if we do not leave artifacts
-    mkdir -p $TEST_DIR
-    # We need the setup.cfg for the pytest settings
-    cp setup.cfg $TEST_DIR
-    cd $TEST_DIR
+# Run the tests on the installed version
+mkdir -p $TEST_DIR

-    # Skip tests that require large downloads over the network to save bandwidth
-    # usage as travis workers are stateless and therefore traditional local
-    # disk caching does not work.
-    export SKLEARN_SKIP_NETWORK_TESTS=1
+# Copy "setup.cfg" for the test settings
+cp setup.cfg $TEST_DIR
+cd $TEST_DIR

-    if [[ "$COVERAGE" == "true" ]]; then
-        TEST_CMD="$TEST_CMD --cov sklearn"
-    fi
+if [[ $TRAVIS_CPU_ARCH == arm64 ]]; then
+    # Faster run of the source code tests
+    TEST_CMD="$TEST_CMD -n $CPU_COUNT"

-    if [[ -n "$CHECK_WARNINGS" ]]; then
-        TEST_CMD="$TEST_CMD -Werror::DeprecationWarning -Werror::FutureWarning"
-    fi
+    # Remove the option to test the docstring
+    sed -i -e 's/--doctest-modules//g' setup.cfg
+fi

-    set -x  # print executed commands to the terminal
+if [[ -n $CHECK_WARNINGS ]]; then
+    TEST_CMD="$TEST_CMD -Werror::DeprecationWarning -Werror::FutureWarning"
+fi

-    $TEST_CMD sklearn
-}
-
-run_tests
+$TEST_CMD sklearn
('.circleci', 'config.yml')
--- /Users/tshi/researchProjs/scikit-learn/scikit-learn-0.21.0/
+++ /Users/tshi/researchProjs/scikit-learn/scikit-learn-1.1.0/
@@ -1,26 +1,42 @@
-version: 2
+version: 2.1

 jobs:
   doc-min-dependencies:
     docker:
-      - image: circleci/python:3.7.3-stretch
+      - image: cimg/python:3.8.12
     environment:
-      - MINICONDA_PATH: ~/miniconda
+      - OMP_NUM_THREADS: 2
+      - MKL_NUM_THREADS: 2
       - CONDA_ENV_NAME: testenv
-      - PYTHON_VERSION: 3.5
-      - NUMPY_VERSION: 1.11.0
-      - SCIPY_VERSION: 0.17.0
-      - PANDAS_VERSION: 0.18.0
-      - MATPLOTLIB_VERSION: 1.5.1
-      - SCIKIT_IMAGE_VERSION: 0.12.3
+      - PYTHON_VERSION: 3.8
+      - NUMPY_VERSION: 'min'
+      - SCIPY_VERSION: 'min'
+      - MATPLOTLIB_VERSION: 'min'
+      - CYTHON_VERSION: 'min'
+      - SCIKIT_IMAGE_VERSION: 'min'
+      - SPHINX_VERSION: 'min'
+      - PANDAS_VERSION: 'min'
+      - SPHINX_GALLERY_VERSION: 'min'
+      - NUMPYDOC_VERSION: 'min'
+      - SPHINX_PROMPT_VERSION: 'min'
+      - SPHINXEXT_OPENGRAPH_VERSION: 'min'
     steps:
       - checkout
       - run: ./build_tools/circle/checkout_merge_commit.sh
       - restore_cache:
-          key: v1-datasets-{{ .Branch }}
+          key: v1-doc-min-deps-datasets-{{ .Branch }}
+      - restore_cache:
+          keys:
+            - doc-min-deps-ccache-{{ .Branch }}
+            - doc-min-deps-ccache
       - run: ./build_tools/circle/build_doc.sh
       - save_cache:
-          key: v1-datasets-{{ .Branch }}
+          key: doc-min-deps-ccache-{{ .Branch }}-{{ .BuildNum }}
+          paths:
+            - ~/.ccache
+            - ~/.cache/pip
+      - save_cache:
+          key: v1-doc-min-deps-datasets-{{ .Branch }}
           paths:
             - ~/scikit_learn_data
       - store_artifacts:
@@ -32,19 +48,42 @@

   doc:
     docker:
-      - image: circleci/python:3.7.3-stretch
+      - image: cimg/python:3.8.12
     environment:
-      - MINICONDA_PATH: ~/miniconda
+      - OMP_NUM_THREADS: 2
+      - MKL_NUM_THREADS: 2
       - CONDA_ENV_NAME: testenv
-      - PYTHON_VERSION: 3
+      - PYTHON_VERSION: '3.9'
+      - NUMPY_VERSION: 'latest'
+      - SCIPY_VERSION: 'latest'
+      - MATPLOTLIB_VERSION: 'latest'
+      - CYTHON_VERSION: 'latest'
+      - SCIKIT_IMAGE_VERSION: 'latest'
+      # Bump the sphinx version from time to time. Avoid latest sphinx version
+      # that tends to break things slightly too often
+      - SPHINX_VERSION: 4.2.0
+      - PANDAS_VERSION: 'latest'
+      - SPHINX_GALLERY_VERSION: 'latest'
+      - NUMPYDOC_VERSION: 'latest'
+      - SPHINX_PROMPT_VERSION: 'latest'
+      - SPHINXEXT_OPENGRAPH_VERSION: 'latest'
     steps:
       - checkout
       - run: ./build_tools/circle/checkout_merge_commit.sh
       - restore_cache:
-          key: v1-datasets-{{ .Branch }}
+          key: v1-doc-datasets-{{ .Branch }}
+      - restore_cache:
+          keys:
+            - doc-ccache-{{ .Branch }}
+            - doc-ccache
       - run: ./build_tools/circle/build_doc.sh
       - save_cache:
-          key: v1-datasets-{{ .Branch }}
+          key: doc-ccache-{{ .Branch }}-{{ .BuildNum }}
+          paths:
+            - ~/.ccache
+            - ~/.cache/pip
+      - save_cache:
+          key: v1-doc-datasets-{{ .Branch }}
           paths:
             - ~/scikit_learn_data
       - store_artifacts:
@@ -61,39 +100,49 @@

   lint:
     docker:
-      - image: circleci/python:3.6
+      - image: cimg/python:3.8.12
     steps:
       - checkout
       - run: ./build_tools/circle/checkout_merge_commit.sh
       - run:
           name: dependencies
-          command: sudo pip install flake8
+          command: pip install flake8
       - run:
-          name: flake8
-          command: ./build_tools/circle/flake8_diff.sh
-      - run:
-          name: deprecated_properties_checks
-          command: ./build_tools/circle/check_deprecated_properties.sh
+          name: linting
+          command: ./build_tools/circle/linting.sh

-  pypy3:
-    docker:
-      - image: pypy:3-7.0.0
+  linux-arm64:
+    machine:
+      image: ubuntu-2004:202101-01
+    resource_class: arm.medium
+    environment:
+      # Use the latest supported version of python
+      - PYTHON_VERSION: '3.9'
+      - OMP_NUM_THREADS: 2
+      - OPENBLAS_NUM_THREADS: 2
+      - NUMPY_VERSION: 'latest'
+      - SCIPY_VERSION: 'latest'
+      - CYTHON_VERSION: 'latest'
+      - JOBLIB_VERSION: 'latest'
+      - THREADPOOLCTL_VERSION: 'latest'
+      - PYTEST_VERSION: 'latest'
+      - PYTEST_XDIST_VERSION: 'latest'
+      - TEST_DOCSTRINGS: 'true'
     steps:
+      - checkout
+      - run: ./build_tools/circle/checkout_merge_commit.sh
       - restore_cache:
-          keys:
-            - pypy3-ccache-{{ .Branch }}
-            - pypy3-ccache
-      - checkout
-      - run: ./build_tools/circle/build_test_pypy.sh
+          key: linux-arm64-{{ .Branch }}
+      - run: ./build_tools/circle/build_test_arm.sh
       - save_cache:
-          key: pypy3-ccache-{{ .Branch }}-{{ .BuildNum }}
+          key: linux-arm64-{{ .Branch }}
           paths:
-            - ~/.ccache
+            - ~/.cache/ccache
             - ~/.cache/pip
-
+            - ~/scikit_learn_data
   deploy:
     docker:
-      - image: circleci/python:3.6
+      - image: cimg/python:3.8.12
     steps:
       - checkout
       - run: ./build_tools/circle/checkout_merge_commit.sh
@@ -104,7 +153,7 @@
       - run: ls -ltrh doc/_build/html/stable
       - deploy:
           command: |
-            if [[ "${CIRCLE_BRANCH}" =~ ^master$|^[0-9]+\.[0-9]+\.X$ ]]; then
+            if [[ "${CIRCLE_BRANCH}" =~ ^main$|^[0-9]+\.[0-9]+\.X$ ]]; then
               bash build_tools/circle/push_doc.sh doc/_build/html/stable
             fi

@@ -119,21 +168,9 @@
       - doc-min-dependencies:
           requires:
             - lint
-      - pypy3:
-          filters:
-            branches:
-              only:
-                - 0.20.X
       - deploy:
           requires:
             - doc
-  pypy:
-    triggers:
-      - schedule:
-          cron: "0 0 * * *"
-          filters:
-            branches:
-              only:
-                - master
+  linux-arm64:
     jobs:
-      - pypy3
+      - linux-arm64
