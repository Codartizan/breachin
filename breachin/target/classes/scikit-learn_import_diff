('', 'conftest.py')
-import platform

('', 'conftest.py')
-from distutils.version import LooseVersion

('', 'conftest.py')
-import pytest

('', 'conftest.py')
-from _pytest.doctest import DoctestItem

('', 'conftest.py')
-        import numpy as np

('', 'conftest.py')
-    import sys

('', 'conftest.py')
-    import sys

('', 'setup.py')
+# We need to import setuptools before because it monkey-patches distutils

('', 'setup.py')
+import setuptools  # noqa

('', 'setup.py')
-from pkg_resources import parse_version

('', 'setup.py')
+from distutils.command.sdist import sdist

('', 'setup.py')
+import importlib

('', 'setup.py')
-    # Python import machinery.

('', 'setup.py')
+    import __builtin__ as builtins

('', 'setup.py')
+# Python import machinery.

('', 'setup.py')
-import sklearn

('', 'setup.py')
+import sklearn  # noqa

('', 'setup.py')
+import sklearn._min_dependencies as min_deps  # noqa

('', 'setup.py')
+from sklearn.externals._packaging.version import parse as parse_version  # noqa

('', 'setup.py')
-# We need to import setuptools early, if we want setuptools features,

('', 'setup.py')
-    import setuptools

('', 'setup.py')
-from numpy.distutils.command.build_ext import build_ext  # noqa

('', 'setup.py')
-        from sklearn._build_utils.openmp_helpers import get_openmp_flag

('', 'setup.py')
-    import wheelhouse_uploader.cmd

('', 'setup.py')
+    from numpy.distutils.command.build_ext import build_ext  # noqa

('', 'setup.py')
+            from sklearn._build_utils.openmp_helpers import get_openmp_flag

('', 'setup.py')
+    from sklearn._build_utils import _check_cython_version

('', 'setup.py')
-        import numpy

('', 'setup.py')
+        module = importlib.import_module(package)

('', 'setup.py')
-            from setuptools import setup

('', 'setup.py')
-            from distutils.core import setup

('', 'setup.py')
+        from setuptools import setup

('', 'setup.py')
+        from numpy.distutils.ccompiler import replace_method

('', 'setup.py')
+        from distutils.ccompiler import CCompiler

('', 'setup.py')
+        from sklearn.externals._numpy_compiler_patch import CCompiler_spawn

('', 'setup.cfg')
+# It's fine not to put the import at the top of the file in the examples

('', 'setup.cfg')
+ignore_missing_imports = True

('sklearn', 'conftest.py')
+from os import environ

('sklearn', 'conftest.py')
+from functools import wraps

('sklearn', 'conftest.py')
+import platform

('sklearn', 'conftest.py')
+import sys

('sklearn', 'conftest.py')
+import numpy as np

('sklearn', 'conftest.py')
+from threadpoolctl import threadpool_limits

('sklearn', 'conftest.py')
+from _pytest.doctest import DoctestItem

('sklearn', 'conftest.py')
+from sklearn.utils import _IS_32BIT

('sklearn', 'conftest.py')
+from sklearn.utils._openmp_helpers import _openmp_effective_n_threads

('sklearn', 'conftest.py')
+from sklearn._min_dependencies import PYTEST_MIN_VERSION

('sklearn', 'conftest.py')
+from sklearn.utils.fixes import parse_version

('sklearn', 'conftest.py')
+from sklearn.datasets import fetch_20newsgroups

('sklearn', 'conftest.py')
+from sklearn.datasets import fetch_20newsgroups_vectorized

('sklearn', 'conftest.py')
+from sklearn.datasets import fetch_california_housing

('sklearn', 'conftest.py')
+from sklearn.datasets import fetch_covtype

('sklearn', 'conftest.py')
+from sklearn.datasets import fetch_kddcup99

('sklearn', 'conftest.py')
+from sklearn.datasets import fetch_olivetti_faces

('sklearn', 'conftest.py')
+from sklearn.datasets import fetch_rcv1

('sklearn', 'conftest.py')
+from sklearn.tests import random_seed

('sklearn', 'conftest.py')
+        import matplotlib  # noqa

('sklearn', 'conftest.py')
+        import PIL  # noqa

('sklearn', 'conftest.py')
-    matplotlib = pytest.importorskip('matplotlib')

('sklearn', 'conftest.py')
-    pyplot = pytest.importorskip('matplotlib.pyplot')

('sklearn', 'conftest.py')
+    pyplot = pytest.importorskip("matplotlib.pyplot")

('sklearn', 'conftest.py')
+        import matplotlib

('sklearn', 'multiclass.py')
+from .utils._tags import _safe_tags

('sklearn', 'multiclass.py')
-from .utils.validation import check_X_y, check_array

('sklearn', 'multiclass.py')
-from .utils.multiclass import (_check_partial_fit_first_call,

('sklearn', 'multiclass.py')
-from .utils.metaestimators import _safe_split, if_delegate_has_method

('sklearn', 'multiclass.py')
-from .utils._joblib import Parallel

('sklearn', 'multiclass.py')
-from .utils._joblib import delayed

('sklearn', 'multiclass.py')
+from .utils.multiclass import (

('sklearn', 'multiclass.py')
+from .utils.metaestimators import _safe_split, available_if

('sklearn', 'multiclass.py')
+from .utils.fixes import delayed

('sklearn', 'multiclass.py')
+from joblib import Parallel

('sklearn', 'multiclass.py')
+    >>> import numpy as np

('sklearn', 'multiclass.py')
+    >>> from sklearn.multiclass import OneVsRestClassifier

('sklearn', 'multiclass.py')
+    >>> from sklearn.svm import SVC

('sklearn', 'multiclass.py')
+    >>> from sklearn.datasets import load_iris

('sklearn', 'multiclass.py')
+    >>> from sklearn.model_selection import train_test_split

('sklearn', 'multiclass.py')
+    >>> from sklearn.multiclass import OneVsOneClassifier

('sklearn', 'multiclass.py')
+    >>> from sklearn.svm import LinearSVC

('sklearn', 'multiclass.py')
+    >>> from sklearn.multiclass import OutputCodeClassifier

('sklearn', 'multiclass.py')
+    >>> from sklearn.ensemble import RandomForestClassifier

('sklearn', 'multiclass.py')
+    >>> from sklearn.datasets import make_classification

('sklearn', '_isotonic.pyx')
+np.import_array()

('sklearn', 'kernel_approximation.py')
+    from scipy.fft import fft, ifft

('sklearn', 'kernel_approximation.py')
+    from scipy.fftpack import fft, ifft

('sklearn', 'kernel_approximation.py')
-from .utils import check_array, check_random_state, as_float_array

('sklearn', 'kernel_approximation.py')
+from .base import _ClassNamePrefixFeaturesOutMixin

('sklearn', 'kernel_approximation.py')
+from .utils import check_random_state

('sklearn', 'kernel_approximation.py')
+from .utils.validation import _check_feature_names_in

('sklearn', 'kernel_approximation.py')
+from .utils.validation import check_non_negative

('sklearn', 'kernel_approximation.py')
+    >>> from sklearn.kernel_approximation import PolynomialCountSketch

('sklearn', 'kernel_approximation.py')
+    >>> from sklearn.linear_model import SGDClassifier

('sklearn', 'random_projection.py')
-from numpy.testing import assert_equal

('sklearn', 'random_projection.py')
+from scipy import linalg

('sklearn', 'random_projection.py')
+from .base import _ClassNamePrefixFeaturesOutMixin

('sklearn', 'random_projection.py')
+    >>> from sklearn.random_projection import johnson_lindenstrauss_min_dim

('sklearn', 'isotonic.py')
+import warnings

('sklearn', 'isotonic.py')
+import math

('sklearn', 'isotonic.py')
+from .utils.validation import _check_sample_weight

('sklearn', 'isotonic.py')
-import warnings

('sklearn', 'isotonic.py')
-import math

('sklearn', 'multioutput.py')
+from joblib import Parallel

('sklearn', 'multioutput.py')
-from .utils import check_array, check_X_y, check_random_state

('sklearn', 'multioutput.py')
-from .utils.fixes import parallel_helper

('sklearn', 'multioutput.py')
-from .utils.metaestimators import if_delegate_has_method

('sklearn', 'multioutput.py')
-from .utils.validation import check_is_fitted, has_fit_parameter

('sklearn', 'multioutput.py')
+from .utils.metaestimators import available_if

('sklearn', 'multioutput.py')
+from .utils import check_random_state

('sklearn', 'multioutput.py')
+from .utils.validation import check_is_fitted, has_fit_parameter, _check_fit_params

('sklearn', 'multioutput.py')
-from .utils._joblib import Parallel, delayed

('sklearn', 'multioutput.py')
+from .utils.fixes import delayed

('sklearn', 'multioutput.py')
+    >>> import numpy as np

('sklearn', 'multioutput.py')
+    >>> from sklearn.datasets import load_linnerud

('sklearn', 'multioutput.py')
+    >>> from sklearn.multioutput import MultiOutputRegressor

('sklearn', 'multioutput.py')
+    >>> from sklearn.linear_model import Ridge

('sklearn', 'multioutput.py')
-        from .metrics import r2_score

('sklearn', 'multioutput.py')
+    >>> import numpy as np

('sklearn', 'multioutput.py')
+    >>> from sklearn.datasets import make_multilabel_classification

('sklearn', 'multioutput.py')
+    >>> from sklearn.multioutput import MultiOutputClassifier

('sklearn', 'multioutput.py')
+    >>> from sklearn.linear_model import LogisticRegression

('sklearn', 'multioutput.py')
+    >>> from sklearn.datasets import make_multilabel_classification

('sklearn', 'multioutput.py')
+    >>> from sklearn.linear_model import LogisticRegression

('sklearn', 'multioutput.py')
+    >>> from sklearn.model_selection import train_test_split

('sklearn', 'multioutput.py')
+    >>> from sklearn.multioutput import ClassifierChain

('sklearn', 'multioutput.py')
+    >>> from sklearn.multioutput import RegressorChain

('sklearn', 'multioutput.py')
+    >>> from sklearn.linear_model import LogisticRegression

('sklearn', '__init__.py')
-import re

('sklearn', '__init__.py')
-import warnings

('sklearn', '__init__.py')
+import random

('sklearn', '__init__.py')
-    sys.stderr.write('Partial import of sklearn during the build process.\n')

('sklearn', '__init__.py')
+    sys.stderr.write("Partial import of sklearn during the build process.\n")

('sklearn', '__init__.py')
-    from . import __check_build

('sklearn', '__init__.py')
+    # It is necessary to do this prior to importing show_versions as the

('sklearn', '__init__.py')
+    # it and importing it first would fail if the OpenMP dll cannot be found.

('sklearn', '__init__.py')
+    from . import _distributor_init  # noqa: F401

('sklearn', '__init__.py')
+    from . import __check_build  # noqa: F401

('sklearn', '__init__.py')
-    import os

('sklearn', '__init__.py')
-    import random

('sklearn', 'kernel_ridge.py')
-from .linear_model.ridge import _solve_cholesky_kernel

('sklearn', 'kernel_ridge.py')
-from .utils import check_array, check_X_y

('sklearn', 'kernel_ridge.py')
-from .utils.validation import check_is_fitted

('sklearn', 'kernel_ridge.py')
+from .linear_model._ridge import _solve_cholesky_kernel

('sklearn', 'kernel_ridge.py')
+from .utils.validation import check_is_fitted, _check_sample_weight

('sklearn', 'naive_bayes.py')
-from scipy.sparse import issparse

('sklearn', 'naive_bayes.py')
+from scipy.special import logsumexp

('sklearn', 'naive_bayes.py')
-from .utils import check_X_y, check_array, check_consistent_length

('sklearn', 'naive_bayes.py')
+from .utils import deprecated

('sklearn', 'naive_bayes.py')
-from .utils.fixes import logsumexp

('sklearn', 'naive_bayes.py')
-from .utils.validation import check_is_fitted

('sklearn', 'naive_bayes.py')
+from .utils.validation import check_is_fitted, check_non_negative

('sklearn', 'naive_bayes.py')
+from .utils.validation import _check_sample_weight

('sklearn', 'naive_bayes.py')
-    >>> import numpy as np

('sklearn', 'naive_bayes.py')
-    >>> from sklearn.naive_bayes import MultinomialNB

('sklearn', 'naive_bayes.py')
+    >>> import numpy as np

('sklearn', 'naive_bayes.py')
+    >>> from sklearn.naive_bayes import MultinomialNB

('sklearn', 'naive_bayes.py')
-    >>> import numpy as np

('sklearn', 'naive_bayes.py')
-    >>> from sklearn.naive_bayes import ComplementNB

('sklearn', 'naive_bayes.py')
+    >>> import numpy as np

('sklearn', 'naive_bayes.py')
+    >>> from sklearn.naive_bayes import ComplementNB

('sklearn', 'naive_bayes.py')
+    >>> import numpy as np

('sklearn', 'naive_bayes.py')
+    >>> from sklearn.naive_bayes import CategoricalNB

('sklearn', 'setup.py')
+import sys

('sklearn', 'setup.py')
-from sklearn._build_utils import maybe_cythonize_extensions

('sklearn', 'setup.py')
+from sklearn._build_utils import cythonize_extensions

('sklearn', 'setup.py')
-    from numpy.distutils.system_info import get_info

('sklearn', 'pipeline.py')
+from joblib import Parallel

('sklearn', 'pipeline.py')
-from .utils._joblib import Parallel, delayed

('sklearn', 'pipeline.py')
-from .utils.metaestimators import if_delegate_has_method

('sklearn', 'pipeline.py')
-from .utils import Bunch, _print_elapsed_time

('sklearn', 'pipeline.py')
+from .preprocessing import FunctionTransformer

('sklearn', 'pipeline.py')
+from .utils._estimator_html_repr import _VisualBlock

('sklearn', 'pipeline.py')
+from .utils.metaestimators import available_if

('sklearn', 'pipeline.py')
+from .utils import (

('sklearn', 'pipeline.py')
+from .utils.deprecation import deprecated

('sklearn', 'pipeline.py')
+from .utils._tags import _safe_tags

('sklearn', 'pipeline.py')
+from .utils.validation import check_is_fitted

('sklearn', 'pipeline.py')
+from .utils.fixes import delayed

('sklearn', 'pipeline.py')
+from .exceptions import NotFittedError

('sklearn', 'pipeline.py')
-    >>> from sklearn import svm

('sklearn', 'pipeline.py')
-    >>> from sklearn.datasets import samples_generator

('sklearn', 'pipeline.py')
-    >>> from sklearn.feature_selection import SelectKBest

('sklearn', 'pipeline.py')
-    >>> from sklearn.feature_selection import f_regression

('sklearn', 'pipeline.py')
+    >>> from sklearn.svm import SVC

('sklearn', 'pipeline.py')
+    >>> from sklearn.preprocessing import StandardScaler

('sklearn', 'pipeline.py')
+    >>> from sklearn.datasets import make_classification

('sklearn', 'pipeline.py')
+    >>> from sklearn.model_selection import train_test_split

('sklearn', 'pipeline.py')
+    >>> from sklearn.pipeline import make_pipeline

('sklearn', 'discriminant_analysis.py')
-from .exceptions import ChangedBehaviorWarning

('sklearn', 'discriminant_analysis.py')
+from numbers import Real

('sklearn', 'discriminant_analysis.py')
-from .linear_model.base import LinearClassifierMixin

('sklearn', 'discriminant_analysis.py')
+from .base import _ClassNamePrefixFeaturesOutMixin

('sklearn', 'discriminant_analysis.py')
+from .linear_model._base import LinearClassifierMixin

('sklearn', 'discriminant_analysis.py')
-from .utils import check_array, check_X_y

('sklearn', 'exceptions.py')
-    >>> from sklearn.model_selection import GridSearchCV

('sklearn', 'exceptions.py')
-    >>> from sklearn.svm import LinearSVC

('sklearn', 'exceptions.py')
-    >>> from sklearn.exceptions import FitFailedWarning

('sklearn', 'exceptions.py')
-    >>> import warnings

('sklearn', '_config.py')
+import threading

('sklearn', 'dummy.py')
+from .utils import deprecated

('sklearn', 'dummy.py')
-from .utils.validation import check_is_fitted

('sklearn', 'dummy.py')
-from .utils.random import random_choice_csc

('sklearn', 'dummy.py')
+from .utils.validation import check_is_fitted, _check_sample_weight

('sklearn', 'dummy.py')
+from .utils.random import _random_choice_csc

('sklearn', 'dummy.py')
+    >>> import numpy as np

('sklearn', 'dummy.py')
+    >>> from sklearn.dummy import DummyClassifier

('sklearn', 'dummy.py')
+    >>> import numpy as np

('sklearn', 'dummy.py')
+    >>> from sklearn.dummy import DummyRegressor

('sklearn', 'base.py')
+from ._config import get_config

('sklearn', 'base.py')
+from .utils._tags import (

('sklearn', 'base.py')
+from .utils.validation import check_X_y

('sklearn', 'base.py')
+from .utils.validation import check_array

('sklearn', 'base.py')
+from .utils.validation import _check_y

('sklearn', 'base.py')
+from .utils.validation import _num_features

('sklearn', 'base.py')
+from .utils.validation import _check_feature_names_in

('sklearn', 'base.py')
+from .utils.validation import _generate_get_feature_names_out

('sklearn', 'base.py')
+from .utils.validation import check_is_fitted

('sklearn', 'base.py')
+from .utils._estimator_html_repr import estimator_html_repr

('sklearn', 'base.py')
+from .utils.validation import _get_feature_names

('sklearn', 'base.py')
-        from .metrics.regression import _check_reg_targets

('sklearn', 'calibration.py')
+from functools import partial

('sklearn', 'calibration.py')
+from joblib import Parallel

('sklearn', 'calibration.py')
-from .preprocessing import LabelEncoder

('sklearn', 'calibration.py')
-from .base import BaseEstimator, ClassifierMixin, RegressorMixin, clone

('sklearn', 'calibration.py')
-from .preprocessing import label_binarize, LabelBinarizer

('sklearn', 'calibration.py')
-from .utils import check_X_y, check_array, indexable, column_or_1d

('sklearn', 'calibration.py')
-from .utils.validation import check_is_fitted, check_consistent_length

('sklearn', 'calibration.py')
+from .base import (

('sklearn', 'calibration.py')
+from .preprocessing import label_binarize, LabelEncoder

('sklearn', 'calibration.py')
+from .utils import (

('sklearn', 'calibration.py')
+from .utils.multiclass import check_classification_targets

('sklearn', 'calibration.py')
+from .utils.fixes import delayed

('sklearn', 'calibration.py')
+from .utils.validation import (

('sklearn', 'calibration.py')
+from .utils import _safe_indexing

('sklearn', 'calibration.py')
-from .model_selection import check_cv

('sklearn', 'calibration.py')
+from .model_selection import check_cv, cross_val_predict

('sklearn', 'calibration.py')
+from .metrics._base import _check_pos_label_consistency

('sklearn', 'calibration.py')
+from .metrics._plot.base import _get_response

('sklearn', 'calibration.py')
+    >>> from sklearn.datasets import make_classification

('sklearn', 'calibration.py')
+    >>> from sklearn.naive_bayes import GaussianNB

('sklearn', 'calibration.py')
+    >>> from sklearn.calibration import CalibratedClassifierCV

('sklearn', 'calibration.py')
+    >>> from sklearn.model_selection import train_test_split

('sklearn', 'calibration.py')
+    >>> import numpy as np

('sklearn', 'calibration.py')
+    >>> from sklearn.calibration import calibration_curve

('sklearn', 'calibration.py')
+    >>> from sklearn.datasets import make_classification

('sklearn', 'calibration.py')
+    >>> from sklearn.model_selection import train_test_split

('sklearn', 'calibration.py')
+    >>> from sklearn.linear_model import LogisticRegression

('sklearn', 'calibration.py')
+    >>> from sklearn.calibration import calibration_curve, CalibrationDisplay

('sklearn', 'calibration.py')
+        import matplotlib.pyplot as plt

('sklearn', 'calibration.py')
+        >>> import matplotlib.pyplot as plt

('sklearn', 'calibration.py')
+        >>> from sklearn.datasets import make_classification

('sklearn', 'calibration.py')
+        >>> from sklearn.model_selection import train_test_split

('sklearn', 'calibration.py')
+        >>> from sklearn.linear_model import LogisticRegression

('sklearn', 'calibration.py')
+        >>> from sklearn.calibration import CalibrationDisplay

('sklearn', 'calibration.py')
+        >>> import matplotlib.pyplot as plt

('sklearn', 'calibration.py')
+        >>> from sklearn.datasets import make_classification

('sklearn', 'calibration.py')
+        >>> from sklearn.model_selection import train_test_split

('sklearn', 'calibration.py')
+        >>> from sklearn.linear_model import LogisticRegression

('sklearn', 'calibration.py')
+        >>> from sklearn.calibration import CalibrationDisplay

('sklearn/tree', '_criterion.pyx')
-from libc.stdlib cimport calloc

('sklearn/tree', '_criterion.pyx')
-from libc.stdlib cimport free

('sklearn/tree', '_criterion.pyx')
+from numpy.math cimport INFINITY

('sklearn/tree', '_criterion.pyx')
+from scipy.special.cython_special cimport xlogy

('sklearn/tree', '_criterion.pyx')
-from ._utils cimport safe_realloc

('sklearn/tree', '_criterion.pyx')
-from ._utils cimport sizet_ptr_to_ndarray

('sklearn/tree', '_splitter.pyx')
+from ..utils._sorting cimport simultaneous_sort

('sklearn/tree', '__init__.py')
-from .tree import DecisionTreeClassifier

('sklearn/tree', '__init__.py')
-from .tree import DecisionTreeRegressor

('sklearn/tree', '__init__.py')
-from .tree import ExtraTreeClassifier

('sklearn/tree', '__init__.py')
-from .tree import ExtraTreeRegressor

('sklearn/tree', '__init__.py')
-from .export import export_graphviz, plot_tree, export_text

('sklearn/tree', '__init__.py')
+from ._classes import BaseDecisionTree

('sklearn/tree', '__init__.py')
+from ._classes import DecisionTreeClassifier

('sklearn/tree', '__init__.py')
+from ._classes import DecisionTreeRegressor

('sklearn/tree', '__init__.py')
+from ._classes import ExtraTreeClassifier

('sklearn/tree', '__init__.py')
+from ._classes import ExtraTreeRegressor

('sklearn/tree', '__init__.py')
+from ._export import export_graphviz, plot_tree, export_text

('sklearn/tree', '_utils.pxd')
-from ..neighbors.quad_tree cimport Cell

('sklearn/tree', '_utils.pxd')
+from ..neighbors._quad_tree cimport Cell

('sklearn/tree', '_tree.pyx')
-from cpython cimport Py_INCREF, PyObject

('sklearn/tree', '_tree.pyx')
+from cpython cimport Py_INCREF, PyObject, PyTypeObject

('sklearn/tree', '_tree.pyx')
+from libc.stdint cimport SIZE_MAX

('sklearn/tree', '_tree.pyx')
+from libcpp.vector cimport vector

('sklearn/tree', '_tree.pyx')
+from libcpp.algorithm cimport pop_heap

('sklearn/tree', '_tree.pyx')
+from libcpp.algorithm cimport push_heap

('sklearn/tree', '_tree.pyx')
+from libcpp cimport bool

('sklearn/tree', '_tree.pyx')
+import struct

('sklearn/tree', '_tree.pyx')
-from scipy.sparse import csc_matrix

('sklearn/tree', '_tree.pyx')
-from ._utils cimport Stack

('sklearn/tree', '_tree.pyx')
-from ._utils cimport StackRecord

('sklearn/tree', '_tree.pyx')
-from ._utils cimport PriorityHeap

('sklearn/tree', '_tree.pyx')
-from ._utils cimport PriorityHeapRecord

('sklearn/metrics', '__init__.py')
-from .ranking import auc

('sklearn/metrics', '__init__.py')
-from .ranking import average_precision_score

('sklearn/metrics', '__init__.py')
-from .ranking import coverage_error

('sklearn/metrics', '__init__.py')
-from .ranking import label_ranking_average_precision_score

('sklearn/metrics', '__init__.py')
-from .ranking import label_ranking_loss

('sklearn/metrics', '__init__.py')
-from .ranking import precision_recall_curve

('sklearn/metrics', '__init__.py')
-from .ranking import roc_auc_score

('sklearn/metrics', '__init__.py')
-from .ranking import roc_curve

('sklearn/metrics', '__init__.py')
+from ._ranking import auc

('sklearn/metrics', '__init__.py')
+from ._ranking import average_precision_score

('sklearn/metrics', '__init__.py')
+from ._ranking import coverage_error

('sklearn/metrics', '__init__.py')
+from ._ranking import det_curve

('sklearn/metrics', '__init__.py')
+from ._ranking import dcg_score

('sklearn/metrics', '__init__.py')
+from ._ranking import label_ranking_average_precision_score

('sklearn/metrics', '__init__.py')
+from ._ranking import label_ranking_loss

('sklearn/metrics', '__init__.py')
+from ._ranking import ndcg_score

('sklearn/metrics', '__init__.py')
+from ._ranking import precision_recall_curve

('sklearn/metrics', '__init__.py')
+from ._ranking import roc_auc_score

('sklearn/metrics', '__init__.py')
+from ._ranking import roc_curve

('sklearn/metrics', '__init__.py')
+from ._ranking import top_k_accuracy_score

('sklearn/metrics', '__init__.py')
-from .classification import accuracy_score

('sklearn/metrics', '__init__.py')
-from .classification import balanced_accuracy_score

('sklearn/metrics', '__init__.py')
-from .classification import classification_report

('sklearn/metrics', '__init__.py')
-from .classification import cohen_kappa_score

('sklearn/metrics', '__init__.py')
-from .classification import confusion_matrix

('sklearn/metrics', '__init__.py')
-from .classification import f1_score

('sklearn/metrics', '__init__.py')
-from .classification import fbeta_score

('sklearn/metrics', '__init__.py')
-from .classification import hamming_loss

('sklearn/metrics', '__init__.py')
-from .classification import hinge_loss

('sklearn/metrics', '__init__.py')
-from .classification import jaccard_similarity_score

('sklearn/metrics', '__init__.py')
-from .classification import jaccard_score

('sklearn/metrics', '__init__.py')
-from .classification import log_loss

('sklearn/metrics', '__init__.py')
-from .classification import matthews_corrcoef

('sklearn/metrics', '__init__.py')
-from .classification import precision_recall_fscore_support

('sklearn/metrics', '__init__.py')
-from .classification import precision_score

('sklearn/metrics', '__init__.py')
-from .classification import recall_score

('sklearn/metrics', '__init__.py')
-from .classification import zero_one_loss

('sklearn/metrics', '__init__.py')
-from .classification import brier_score_loss

('sklearn/metrics', '__init__.py')
-from .classification import multilabel_confusion_matrix

('sklearn/metrics', '__init__.py')
+from ._classification import accuracy_score

('sklearn/metrics', '__init__.py')
+from ._classification import balanced_accuracy_score

('sklearn/metrics', '__init__.py')
+from ._classification import classification_report

('sklearn/metrics', '__init__.py')
+from ._classification import cohen_kappa_score

('sklearn/metrics', '__init__.py')
+from ._classification import confusion_matrix

('sklearn/metrics', '__init__.py')
+from ._classification import f1_score

('sklearn/metrics', '__init__.py')
+from ._classification import fbeta_score

('sklearn/metrics', '__init__.py')
+from ._classification import hamming_loss

('sklearn/metrics', '__init__.py')
+from ._classification import hinge_loss

('sklearn/metrics', '__init__.py')
+from ._classification import jaccard_score

('sklearn/metrics', '__init__.py')
+from ._classification import log_loss

('sklearn/metrics', '__init__.py')
+from ._classification import matthews_corrcoef

('sklearn/metrics', '__init__.py')
+from ._classification import precision_recall_fscore_support

('sklearn/metrics', '__init__.py')
+from ._classification import precision_score

('sklearn/metrics', '__init__.py')
+from ._classification import recall_score

('sklearn/metrics', '__init__.py')
+from ._classification import zero_one_loss

('sklearn/metrics', '__init__.py')
+from ._classification import brier_score_loss

('sklearn/metrics', '__init__.py')
+from ._classification import multilabel_confusion_matrix

('sklearn/metrics', '__init__.py')
+from ._dist_metrics import DistanceMetric

('sklearn/metrics', '__init__.py')
+from .cluster import rand_score

('sklearn/metrics', '__init__.py')
+from .cluster import pair_confusion_matrix

('sklearn/metrics', '__init__.py')
-from .cluster import calinski_harabaz_score

('sklearn/metrics', '__init__.py')
+from .pairwise import nan_euclidean_distances

('sklearn/metrics', '__init__.py')
-from .regression import explained_variance_score

('sklearn/metrics', '__init__.py')
-from .regression import max_error

('sklearn/metrics', '__init__.py')
-from .regression import mean_absolute_error

('sklearn/metrics', '__init__.py')
-from .regression import mean_squared_error

('sklearn/metrics', '__init__.py')
-from .regression import mean_squared_log_error

('sklearn/metrics', '__init__.py')
-from .regression import median_absolute_error

('sklearn/metrics', '__init__.py')
-from .regression import r2_score

('sklearn/metrics', '__init__.py')
+from ._regression import explained_variance_score

('sklearn/metrics', '__init__.py')
+from ._regression import max_error

('sklearn/metrics', '__init__.py')
+from ._regression import mean_absolute_error

('sklearn/metrics', '__init__.py')
+from ._regression import mean_squared_error

('sklearn/metrics', '__init__.py')
+from ._regression import mean_squared_log_error

('sklearn/metrics', '__init__.py')
+from ._regression import median_absolute_error

('sklearn/metrics', '__init__.py')
+from ._regression import mean_absolute_percentage_error

('sklearn/metrics', '__init__.py')
+from ._regression import mean_pinball_loss

('sklearn/metrics', '__init__.py')
+from ._regression import r2_score

('sklearn/metrics', '__init__.py')
+from ._regression import mean_tweedie_deviance

('sklearn/metrics', '__init__.py')
+from ._regression import mean_poisson_deviance

('sklearn/metrics', '__init__.py')
+from ._regression import mean_gamma_deviance

('sklearn/metrics', '__init__.py')
+from ._regression import d2_tweedie_score

('sklearn/metrics', '__init__.py')
+from ._regression import d2_pinball_score

('sklearn/metrics', '__init__.py')
+from ._regression import d2_absolute_error_score

('sklearn/metrics', '__init__.py')
-from .scorer import check_scoring

('sklearn/metrics', '__init__.py')
-from .scorer import make_scorer

('sklearn/metrics', '__init__.py')
-from .scorer import SCORERS

('sklearn/metrics', '__init__.py')
-from .scorer import get_scorer

('sklearn/metrics', '__init__.py')
+from ._scorer import check_scoring

('sklearn/metrics', '__init__.py')
+from ._scorer import make_scorer

('sklearn/metrics', '__init__.py')
+from ._scorer import SCORERS

('sklearn/metrics', '__init__.py')
+from ._scorer import get_scorer

('sklearn/metrics', '__init__.py')
+from ._scorer import get_scorer_names

('sklearn/metrics', '__init__.py')
+from ._plot.det_curve import plot_det_curve

('sklearn/metrics', '__init__.py')
+from ._plot.det_curve import DetCurveDisplay

('sklearn/metrics', '__init__.py')
+from ._plot.roc_curve import plot_roc_curve

('sklearn/metrics', '__init__.py')
+from ._plot.roc_curve import RocCurveDisplay

('sklearn/metrics', '__init__.py')
+from ._plot.precision_recall_curve import plot_precision_recall_curve

('sklearn/metrics', '__init__.py')
+from ._plot.precision_recall_curve import PrecisionRecallDisplay

('sklearn/metrics', '__init__.py')
+from ._plot.confusion_matrix import plot_confusion_matrix

('sklearn/metrics', '__init__.py')
+from ._plot.confusion_matrix import ConfusionMatrixDisplay

('sklearn/metrics', 'setup.py')
+import numpy as np

('sklearn/metrics', 'pairwise.py')
+from joblib import Parallel, effective_n_jobs

('sklearn/metrics', 'pairwise.py')
+from .. import config_context

('sklearn/metrics', 'pairwise.py')
+from ..utils import is_scalar_nan

('sklearn/metrics', 'pairwise.py')
-from ..utils._joblib import Parallel

('sklearn/metrics', 'pairwise.py')
-from ..utils._joblib import delayed

('sklearn/metrics', 'pairwise.py')
-from ..utils._joblib import effective_n_jobs

('sklearn/metrics', 'pairwise.py')
-from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan

('sklearn/metrics', 'pairwise.py')
+from ..utils._mask import _get_mask

('sklearn/metrics', 'pairwise.py')
+from ..utils.fixes import delayed

('sklearn/metrics', 'pairwise.py')
+from ..utils.fixes import sp_version, parse_version

('sklearn/metrics', 'pairwise.py')
+from ._pairwise_distances_reduction import PairwiseDistancesArgKmin

('sklearn/metrics', 'pairwise.py')
+from ._pairwise_fast import _chi2_kernel_fast, _sparse_manhattan

('sklearn/metrics', 'pairwise.py')
+    >>> from sklearn.metrics.pairwise import nan_euclidean_distances

('sklearn/metrics', 'pairwise.py')
+    >>> from math import radians

('sklearn/metrics', 'pairwise.py')
-    from sklearn.neighbors import DistanceMetric

('sklearn/metrics', 'pairwise.py')
+    from ..metrics import DistanceMetric

('sklearn/metrics/cluster', '__init__.py')
-from .supervised import adjusted_mutual_info_score

('sklearn/metrics/cluster', '__init__.py')
-from .supervised import normalized_mutual_info_score

('sklearn/metrics/cluster', '__init__.py')
-from .supervised import adjusted_rand_score

('sklearn/metrics/cluster', '__init__.py')
-from .supervised import completeness_score

('sklearn/metrics/cluster', '__init__.py')
-from .supervised import contingency_matrix

('sklearn/metrics/cluster', '__init__.py')
-from .supervised import expected_mutual_information

('sklearn/metrics/cluster', '__init__.py')
-from .supervised import homogeneity_completeness_v_measure

('sklearn/metrics/cluster', '__init__.py')
-from .supervised import homogeneity_score

('sklearn/metrics/cluster', '__init__.py')
-from .supervised import mutual_info_score

('sklearn/metrics/cluster', '__init__.py')
-from .supervised import v_measure_score

('sklearn/metrics/cluster', '__init__.py')
-from .supervised import fowlkes_mallows_score

('sklearn/metrics/cluster', '__init__.py')
-from .supervised import entropy

('sklearn/metrics/cluster', '__init__.py')
-from .unsupervised import silhouette_samples

('sklearn/metrics/cluster', '__init__.py')
-from .unsupervised import silhouette_score

('sklearn/metrics/cluster', '__init__.py')
-from .unsupervised import calinski_harabasz_score

('sklearn/metrics/cluster', '__init__.py')
-from .unsupervised import calinski_harabaz_score

('sklearn/metrics/cluster', '__init__.py')
-from .unsupervised import davies_bouldin_score

('sklearn/metrics/cluster', '__init__.py')
-from .bicluster import consensus_score

('sklearn/metrics/cluster', '__init__.py')
+from ._supervised import adjusted_mutual_info_score

('sklearn/metrics/cluster', '__init__.py')
+from ._supervised import normalized_mutual_info_score

('sklearn/metrics/cluster', '__init__.py')
+from ._supervised import adjusted_rand_score

('sklearn/metrics/cluster', '__init__.py')
+from ._supervised import rand_score

('sklearn/metrics/cluster', '__init__.py')
+from ._supervised import completeness_score

('sklearn/metrics/cluster', '__init__.py')
+from ._supervised import contingency_matrix

('sklearn/metrics/cluster', '__init__.py')
+from ._supervised import pair_confusion_matrix

('sklearn/metrics/cluster', '__init__.py')
+from ._supervised import expected_mutual_information

('sklearn/metrics/cluster', '__init__.py')
+from ._supervised import homogeneity_completeness_v_measure

('sklearn/metrics/cluster', '__init__.py')
+from ._supervised import homogeneity_score

('sklearn/metrics/cluster', '__init__.py')
+from ._supervised import mutual_info_score

('sklearn/metrics/cluster', '__init__.py')
+from ._supervised import v_measure_score

('sklearn/metrics/cluster', '__init__.py')
+from ._supervised import fowlkes_mallows_score

('sklearn/metrics/cluster', '__init__.py')
+from ._supervised import entropy

('sklearn/metrics/cluster', '__init__.py')
+from ._unsupervised import silhouette_samples

('sklearn/metrics/cluster', '__init__.py')
+from ._unsupervised import silhouette_score

('sklearn/metrics/cluster', '__init__.py')
+from ._unsupervised import calinski_harabasz_score

('sklearn/metrics/cluster', '__init__.py')
+from ._unsupervised import davies_bouldin_score

('sklearn/metrics/cluster', '__init__.py')
+from ._bicluster import consensus_score

('sklearn/ensemble', '__init__.py')
+from ._base import BaseEnsemble

('sklearn/ensemble', '__init__.py')
+from ._forest import RandomForestClassifier

('sklearn/ensemble', '__init__.py')
+from ._forest import RandomForestRegressor

('sklearn/ensemble', '__init__.py')
+from ._forest import RandomTreesEmbedding

('sklearn/ensemble', '__init__.py')
+from ._forest import ExtraTreesClassifier

('sklearn/ensemble', '__init__.py')
+from ._forest import ExtraTreesRegressor

('sklearn/ensemble', '__init__.py')
+from ._bagging import BaggingClassifier

('sklearn/ensemble', '__init__.py')
+from ._bagging import BaggingRegressor

('sklearn/ensemble', '__init__.py')
+from ._iforest import IsolationForest

('sklearn/ensemble', '__init__.py')
+from ._weight_boosting import AdaBoostClassifier

('sklearn/ensemble', '__init__.py')
+from ._weight_boosting import AdaBoostRegressor

('sklearn/ensemble', '__init__.py')
+from ._gb import GradientBoostingClassifier

('sklearn/ensemble', '__init__.py')
+from ._gb import GradientBoostingRegressor

('sklearn/ensemble', '__init__.py')
+from ._voting import VotingClassifier

('sklearn/ensemble', '__init__.py')
+from ._voting import VotingRegressor

('sklearn/ensemble', '__init__.py')
+from ._stacking import StackingClassifier

('sklearn/ensemble', '__init__.py')
+from ._stacking import StackingRegressor

('sklearn/ensemble', '__init__.py')
+from ._hist_gradient_boosting.gradient_boosting import (

('sklearn/ensemble', '__init__.py')
-from .base import BaseEnsemble

('sklearn/ensemble', '__init__.py')
-from .forest import RandomForestClassifier

('sklearn/ensemble', '__init__.py')
-from .forest import RandomForestRegressor

('sklearn/ensemble', '__init__.py')
-from .forest import RandomTreesEmbedding

('sklearn/ensemble', '__init__.py')
-from .forest import ExtraTreesClassifier

('sklearn/ensemble', '__init__.py')
-from .forest import ExtraTreesRegressor

('sklearn/ensemble', '__init__.py')
-from .bagging import BaggingClassifier

('sklearn/ensemble', '__init__.py')
-from .bagging import BaggingRegressor

('sklearn/ensemble', '__init__.py')
-from .iforest import IsolationForest

('sklearn/ensemble', '__init__.py')
-from .weight_boosting import AdaBoostClassifier

('sklearn/ensemble', '__init__.py')
-from .weight_boosting import AdaBoostRegressor

('sklearn/ensemble', '__init__.py')
-from .gradient_boosting import GradientBoostingClassifier

('sklearn/ensemble', '__init__.py')
-from .gradient_boosting import GradientBoostingRegressor

('sklearn/ensemble', '__init__.py')
-from .voting import VotingClassifier

('sklearn/ensemble', '__init__.py')
-from .voting import VotingRegressor

('sklearn/ensemble', '__init__.py')
-from . import bagging

('sklearn/ensemble', '__init__.py')
-from . import forest

('sklearn/ensemble', '__init__.py')
-from . import weight_boosting

('sklearn/ensemble', '__init__.py')
-from . import gradient_boosting

('sklearn/ensemble', '__init__.py')
-from . import partial_dependence

('sklearn/ensemble', '_gradient_boosting.pyx')
-from numpy import bool as np_bool

('sklearn/ensemble', '_gb_losses.py')
-from scipy.special import expit

('sklearn/ensemble', '_gb_losses.py')
+from scipy.special import expit, logsumexp

('sklearn/ensemble', '_gb_losses.py')
-from ..utils.fixes import logsumexp

('sklearn/ensemble/_hist_gradient_boosting', 'predictor.py')
-from .types import X_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', 'predictor.py')
-from .types import Y_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', 'predictor.py')
-from .types import X_BINNED_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', 'predictor.py')
-from ._predictor import _predict_from_numeric_data

('sklearn/ensemble/_hist_gradient_boosting', 'predictor.py')
+from .common import Y_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', 'predictor.py')
+from ._predictor import _predict_from_raw_data

('sklearn/ensemble/_hist_gradient_boosting', 'predictor.py')
+from ._predictor import _compute_partial_dependence

('sklearn/ensemble/_hist_gradient_boosting', 'binning.py')
+from ...utils.fixes import percentile

('sklearn/ensemble/_hist_gradient_boosting', 'binning.py')
+from ...utils._openmp_helpers import _openmp_effective_n_threads

('sklearn/ensemble/_hist_gradient_boosting', 'binning.py')
-from .types import X_DTYPE, X_BINNED_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', 'binning.py')
+from .common import X_DTYPE, X_BINNED_DTYPE, ALMOST_INF, X_BITSET_INNER_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', 'binning.py')
+from ._bitset import set_bitset_memoryview

('sklearn/ensemble/_hist_gradient_boosting', '_predictor.pyx')
+from libc.math cimport isnan

('sklearn/ensemble/_hist_gradient_boosting', '_predictor.pyx')
-from .types cimport X_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', '_predictor.pyx')
-from .types cimport Y_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', '_predictor.pyx')
-from .types cimport X_BINNED_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', '_predictor.pyx')
+from numpy.math cimport INFINITY

('sklearn/ensemble/_hist_gradient_boosting', '_predictor.pyx')
+from .common cimport X_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', '_predictor.pyx')
+from .common cimport Y_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', '_predictor.pyx')
+from .common import Y_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', '_predictor.pyx')
+from .common cimport X_BINNED_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', '_predictor.pyx')
+from .common cimport BITSET_INNER_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', '_predictor.pyx')
+from .common cimport BITSET_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', '_predictor.pyx')
+from .common cimport node_struct

('sklearn/ensemble/_hist_gradient_boosting', '_predictor.pyx')
+from ._bitset cimport in_bitset_2d_memoryview

('sklearn/ensemble/_hist_gradient_boosting', '_predictor.pyx')
+np.import_array()

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
-    from openmp cimport omp_get_max_threads

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
-from libc.stdlib cimport malloc, free

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
+from libc.stdlib cimport malloc, free, qsort

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
-from .types cimport X_BINNED_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
-from .types cimport Y_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
-from .types cimport hist_struct

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
-from .types import HISTOGRAM_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
+from numpy.math cimport INFINITY

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
+from .common cimport X_BINNED_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
+from .common cimport Y_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
+from .common cimport hist_struct

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
+from .common import HISTOGRAM_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
+from .common cimport BITSET_INNER_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
+from .common cimport BITSET_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
+from .common cimport MonotonicConstraint

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
+from ._bitset cimport init_bitset

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
+from ._bitset cimport set_bitset

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
+from ._bitset cimport in_bitset

('sklearn/ensemble/_hist_gradient_boosting', 'splitting.pyx')
+np.import_array()

('sklearn/ensemble/_hist_gradient_boosting', 'histogram.pyx')
-from .types import HISTOGRAM_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', 'histogram.pyx')
-from .types cimport hist_struct

('sklearn/ensemble/_hist_gradient_boosting', 'histogram.pyx')
-from .types cimport X_BINNED_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', 'histogram.pyx')
-from .types cimport G_H_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', 'histogram.pyx')
-from .types cimport hist_struct

('sklearn/ensemble/_hist_gradient_boosting', 'histogram.pyx')
+from .common import HISTOGRAM_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', 'histogram.pyx')
+from .common cimport hist_struct

('sklearn/ensemble/_hist_gradient_boosting', 'histogram.pyx')
+from .common cimport X_BINNED_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', 'histogram.pyx')
+from .common cimport G_H_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', 'histogram.pyx')
+np.import_array()

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
+from functools import partial

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
+import warnings

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-from sklearn.utils import check_X_y, check_random_state, check_array

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-from sklearn.utils.validation import check_is_fitted

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-from sklearn.utils.multiclass import check_classification_targets

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-from sklearn.metrics import check_scoring

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-from sklearn.model_selection import train_test_split

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-from sklearn.preprocessing import LabelEncoder

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
+from ..._loss.loss import (

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
+from ...base import BaseEstimator, RegressorMixin, ClassifierMixin, is_classifier

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
+from ...utils import check_random_state, resample

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
+from ...utils.validation import (

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
+from ...utils._openmp_helpers import _openmp_effective_n_threads

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
+from ...utils.multiclass import check_classification_targets

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
+from ...metrics import check_scoring

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
+from ...model_selection import train_test_split

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
+from ...preprocessing import LabelEncoder

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-from .types import Y_DTYPE, X_DTYPE, X_BINNED_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
+from .common import Y_DTYPE, X_DTYPE, G_H_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-from .loss import _LOSSES

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-      you need to explicitly import ``enable_hist_gradient_boosting``::

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-        >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-        >>> # now you can import normally from ensemble

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-        >>> from sklearn.ensemble import HistGradientBoostingClassifier

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-    >>> from sklearn.datasets import load_boston

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
+    >>> from sklearn.datasets import load_diabetes

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-      you need to explicitly import ``enable_hist_gradient_boosting``::

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-        >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-        >>> # now you can import normally from ensemble

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-        >>> from sklearn.ensemble import HistGradientBoostingClassifier

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
-    >>> from sklearn.ensemble import HistGradientBoostingRegressor

('sklearn/ensemble/_hist_gradient_boosting', 'gradient_boosting.py')
+    >>> from sklearn.ensemble import HistGradientBoostingClassifier

('sklearn/ensemble/_hist_gradient_boosting', '_gradient_boosting.pyx')
-from .types import Y_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', '_gradient_boosting.pyx')
-from .types cimport Y_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', '_gradient_boosting.pyx')
+from .common import Y_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', '_gradient_boosting.pyx')
+from .common cimport Y_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', '_gradient_boosting.pyx')
+np.import_array()

('sklearn/ensemble/_hist_gradient_boosting', 'grower.py')
-from .predictor import TreePredictor, PREDICTOR_RECORD_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', 'grower.py')
+from .predictor import TreePredictor

('sklearn/ensemble/_hist_gradient_boosting', 'grower.py')
+from .common import PREDICTOR_RECORD_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', 'grower.py')
+from .common import X_BITSET_INNER_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', 'grower.py')
+from .common import Y_DTYPE

('sklearn/ensemble/_hist_gradient_boosting', 'grower.py')
+from .common import MonotonicConstraint

('sklearn/ensemble/_hist_gradient_boosting', 'grower.py')
+from ._bitset import set_raw_bitset_from_binned_bitset

('sklearn/ensemble/_hist_gradient_boosting', 'grower.py')
+from sklearn.utils._openmp_helpers import _openmp_effective_n_threads

('sklearn/ensemble/_hist_gradient_boosting', '_binning.pyx')
+from numpy.math cimport INFINITY

('sklearn/ensemble/_hist_gradient_boosting', '_binning.pyx')
+from libc.math cimport isnan

('sklearn/ensemble/_hist_gradient_boosting', '_binning.pyx')
-from .types cimport X_DTYPE_C, X_BINNED_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', '_binning.pyx')
+from .common cimport X_DTYPE_C, X_BINNED_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', '_binning.pyx')
+np.import_array()

('sklearn/ensemble/_hist_gradient_boosting', 'utils.pyx')
-from .types cimport G_H_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', 'utils.pyx')
-from .types cimport Y_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', 'utils.pyx')
+from .common cimport G_H_DTYPE_C

('sklearn/ensemble/_hist_gradient_boosting', 'utils.pyx')
+from .common cimport Y_DTYPE_C

('sklearn/experimental', 'enable_hist_gradient_boosting.py')
-    >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa

('sklearn/experimental', 'enable_hist_gradient_boosting.py')
-    >>> # now you can import normally from ensemble

('sklearn/experimental', 'enable_hist_gradient_boosting.py')
-    >>> from sklearn.ensemble import HistGradientBoostingClassifier

('sklearn/experimental', 'enable_hist_gradient_boosting.py')
-    >>> from sklearn.ensemble import HistGradientBoostingRegressor

('sklearn/experimental', 'enable_hist_gradient_boosting.py')
+:term:`experimental`, but these estimators are now stable and can be imported

('sklearn/experimental', 'enable_hist_gradient_boosting.py')
-flake8 to ignore the import, which appears as unused.

('sklearn/experimental', 'enable_hist_gradient_boosting.py')
+import warnings

('sklearn/experimental', 'enable_hist_gradient_boosting.py')
-from ..ensemble._hist_gradient_boosting.gradient_boosting import (

('sklearn/experimental', 'enable_hist_gradient_boosting.py')
+    "it is not needed to import enable_hist_gradient_boosting anymore. "

('sklearn/experimental', 'enable_hist_gradient_boosting.py')
+    "stable and can be normally imported from sklearn.ensemble."

('sklearn/experimental', 'enable_hist_gradient_boosting.py')
-from .. import ensemble

('sklearn/cluster', '_feature_agglomeration.py')
-from ..utils import check_array

('sklearn/cluster', '__init__.py')
-from .spectral import spectral_clustering, SpectralClustering

('sklearn/cluster', '__init__.py')
-from .mean_shift_ import (mean_shift, MeanShift,

('sklearn/cluster', '__init__.py')
-from .affinity_propagation_ import affinity_propagation, AffinityPropagation

('sklearn/cluster', '__init__.py')
-from .hierarchical import (ward_tree, AgglomerativeClustering, linkage_tree,

('sklearn/cluster', '__init__.py')
-from .k_means_ import k_means, KMeans, MiniBatchKMeans

('sklearn/cluster', '__init__.py')
-from .dbscan_ import dbscan, DBSCAN

('sklearn/cluster', '__init__.py')
-from .optics_ import (OPTICS, cluster_optics_dbscan, compute_optics_graph,

('sklearn/cluster', '__init__.py')
-from .bicluster import SpectralBiclustering, SpectralCoclustering

('sklearn/cluster', '__init__.py')
-from .birch import Birch

('sklearn/cluster', '__init__.py')
+from ._spectral import spectral_clustering, SpectralClustering

('sklearn/cluster', '__init__.py')
+from ._mean_shift import mean_shift, MeanShift, estimate_bandwidth, get_bin_seeds

('sklearn/cluster', '__init__.py')
+from ._affinity_propagation import affinity_propagation, AffinityPropagation

('sklearn/cluster', '__init__.py')
+from ._agglomerative import (

('sklearn/cluster', '__init__.py')
+from ._kmeans import k_means, KMeans, MiniBatchKMeans, kmeans_plusplus

('sklearn/cluster', '__init__.py')
+from ._bisect_k_means import BisectingKMeans

('sklearn/cluster', '__init__.py')
+from ._dbscan import dbscan, DBSCAN

('sklearn/cluster', '__init__.py')
+from ._optics import (

('sklearn/cluster', '__init__.py')
+from ._bicluster import SpectralBiclustering, SpectralCoclustering

('sklearn/cluster', '__init__.py')
+from ._birch import Birch

('sklearn/cluster', '_dbscan_inner.pyx')
+np.import_array()

('sklearn/cluster', '_k_means_elkan.pyx')
+    cimport openmp

('sklearn/cluster', '_k_means_elkan.pyx')
+from cython.parallel import prange, parallel

('sklearn/cluster', '_k_means_elkan.pyx')
-from ..metrics import euclidean_distances

('sklearn/cluster', '_k_means_elkan.pyx')
-from ._k_means import _centers_dense

('sklearn/cluster', '_k_means_elkan.pyx')
+from libc.stdlib cimport calloc, free

('sklearn/cluster', '_k_means_elkan.pyx')
+from libc.string cimport memset, memcpy

('sklearn/cluster', '_k_means_elkan.pyx')
+from ..utils.extmath import row_norms

('sklearn/cluster', '_k_means_elkan.pyx')
+from ._k_means_common import CHUNK_SIZE

('sklearn/cluster', '_k_means_elkan.pyx')
+from ._k_means_common cimport _relocate_empty_clusters_dense

('sklearn/cluster', '_k_means_elkan.pyx')
+from ._k_means_common cimport _relocate_empty_clusters_sparse

('sklearn/cluster', '_k_means_elkan.pyx')
+from ._k_means_common cimport _euclidean_dense_dense

('sklearn/cluster', '_k_means_elkan.pyx')
+from ._k_means_common cimport _euclidean_sparse_dense

('sklearn/cluster', '_k_means_elkan.pyx')
+from ._k_means_common cimport _average_centers

('sklearn/cluster', '_k_means_elkan.pyx')
+from ._k_means_common cimport _center_shift

('sklearn/cluster', '_k_means_elkan.pyx')
+np.import_array()

('sklearn/feature_extraction', '__init__.py')
-from .dict_vectorizer import DictVectorizer

('sklearn/feature_extraction', '__init__.py')
-from .hashing import FeatureHasher

('sklearn/feature_extraction', '__init__.py')
+from ._dict_vectorizer import DictVectorizer

('sklearn/feature_extraction', '__init__.py')
+from ._hash import FeatureHasher

('sklearn/feature_extraction', 'setup.py')
-import platform

('sklearn/feature_extraction', 'text.py')
+from functools import partial

('sklearn/feature_extraction', 'text.py')
-from ..base import BaseEstimator, TransformerMixin

('sklearn/feature_extraction', 'text.py')
+from ..base import BaseEstimator, TransformerMixin, _OneToOneFeatureMixin

('sklearn/feature_extraction', 'text.py')
-from .hashing import FeatureHasher

('sklearn/feature_extraction', 'text.py')
-from .stop_words import ENGLISH_STOP_WORDS

('sklearn/feature_extraction', 'text.py')
-from ..utils.validation import check_is_fitted, check_array, FLOAT_DTYPES

('sklearn/feature_extraction', 'text.py')
+from ._hash import FeatureHasher

('sklearn/feature_extraction', 'text.py')
+from ._stop_words import ENGLISH_STOP_WORDS

('sklearn/feature_extraction', 'text.py')
+from ..utils.validation import check_is_fitted, check_array, FLOAT_DTYPES, check_scalar

('sklearn/feature_extraction', 'text.py')
+from ..utils.deprecation import deprecated

('sklearn/feature_extraction', 'text.py')
-from ..utils.fixes import _astype_copy_false

('sklearn/feature_extraction', 'text.py')
-from ..exceptions import ChangedBehaviorWarning

('sklearn/feature_extraction', 'text.py')
+from ..exceptions import NotFittedError

('sklearn/feature_extraction', 'text.py')
-        import tempfile

('sklearn/feature_extraction', 'text.py')
+    >>> from sklearn.feature_extraction.text import TfidfTransformer

('sklearn/feature_extraction', 'text.py')
+    >>> from sklearn.feature_extraction.text import CountVectorizer

('sklearn/feature_extraction', 'text.py')
+    >>> from sklearn.pipeline import Pipeline

('sklearn/semi_supervised', '__init__.py')
-from .label_propagation import LabelPropagation, LabelSpreading

('sklearn/semi_supervised', '__init__.py')
+from ._label_propagation import LabelPropagation, LabelSpreading

('sklearn/semi_supervised', '__init__.py')
+from ._self_training import SelfTrainingClassifier

('sklearn/gaussian_process', '__init__.py')
-from .gpr import GaussianProcessRegressor

('sklearn/gaussian_process', '__init__.py')
-from .gpc import GaussianProcessClassifier

('sklearn/gaussian_process', '__init__.py')
+from ._gpr import GaussianProcessRegressor

('sklearn/gaussian_process', '__init__.py')
+from ._gpc import GaussianProcessClassifier

('sklearn/gaussian_process', '__init__.py')
-from . import correlation_models

('sklearn/gaussian_process', '__init__.py')
-from . import regression_models

('sklearn/gaussian_process', 'kernels.py')
+from ..utils.validation import _num_samples

('sklearn/gaussian_process', 'kernels.py')
+from ..exceptions import ConvergenceWarning

('sklearn/gaussian_process', 'kernels.py')
+import warnings

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process.kernels import ConstantKernel

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.datasets import make_friedman2

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process.kernels import Hyperparameter

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process.kernels import WhiteKernel

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process.kernels import RBF

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process.kernels import CompoundKernel

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.datasets import make_friedman2

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process.kernels import RBF, Sum, ConstantKernel

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.datasets import make_friedman2

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process.kernels import (RBF, Product,

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.datasets import make_friedman2

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process.kernels import (RationalQuadratic,

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.datasets import make_friedman2

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process.kernels import RBF, ConstantKernel

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.datasets import make_friedman2

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.datasets import load_iris

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process import GaussianProcessClassifier

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process.kernels import RBF

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.datasets import load_iris

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process import GaussianProcessClassifier

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process.kernels import Matern

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.datasets import load_iris

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process import GaussianProcessClassifier

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process.kernels import RationalQuadratic

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.datasets import make_friedman2

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process.kernels import ExpSineSquared

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.datasets import make_friedman2

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process import GaussianProcessRegressor

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.datasets import load_iris

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process import GaussianProcessClassifier

('sklearn/gaussian_process', 'kernels.py')
+    >>> from sklearn.gaussian_process.kernels import PairwiseKernel

('sklearn/compose', '_target.py')
-from ..utils import check_array, safe_indexing

('sklearn/compose', '_target.py')
+from ..utils._tags import _safe_tags

('sklearn/compose', '_target.py')
+from ..utils import check_array, _safe_indexing

('sklearn/compose', '_target.py')
+from ..exceptions import NotFittedError

('sklearn/compose', '_target.py')
+            from ..linear_model import LinearRegression

('sklearn/compose', '__init__.py')
-from ._column_transformer import ColumnTransformer, make_column_transformer

('sklearn/compose', '__init__.py')
+from ._column_transformer import (

('sklearn/compose', '_column_transformer.py')
+from collections import Counter

('sklearn/compose', '_column_transformer.py')
-import warnings

('sklearn/compose', '_column_transformer.py')
+from joblib import Parallel

('sklearn/compose', '_column_transformer.py')
-from ..utils._joblib import Parallel, delayed

('sklearn/compose', '_column_transformer.py')
+from ..utils._estimator_html_repr import _VisualBlock

('sklearn/compose', '_column_transformer.py')
+from ..utils import _safe_indexing

('sklearn/compose', '_column_transformer.py')
+from ..utils import _get_column_indices

('sklearn/compose', '_column_transformer.py')
+from ..utils.deprecation import deprecated

('sklearn/compose', '_column_transformer.py')
-from ..utils.validation import check_array, check_is_fitted

('sklearn/compose', '_column_transformer.py')
+from ..utils.validation import check_array, check_is_fitted, _check_feature_names_in

('sklearn/compose', '_column_transformer.py')
+from ..utils.fixes import delayed

('sklearn/compose', '_column_transformer.py')
+    >>> from sklearn.preprocessing import StandardScaler, OneHotEncoder

('sklearn/compose', '_column_transformer.py')
+    >>> from sklearn.compose import make_column_transformer

('sklearn/compose', '_column_transformer.py')
+    >>> from sklearn.compose import make_column_selector

('sklearn/compose', '_column_transformer.py')
+    >>> import numpy as np

('sklearn/compose', '_column_transformer.py')
+    >>> import pandas as pd  # doctest: +SKIP

('sklearn/datasets', '__init__.py')
-from .base import load_breast_cancer

('sklearn/datasets', '__init__.py')
-from .base import load_boston

('sklearn/datasets', '__init__.py')
-from .base import load_diabetes

('sklearn/datasets', '__init__.py')
-from .base import load_digits

('sklearn/datasets', '__init__.py')
-from .base import load_files

('sklearn/datasets', '__init__.py')
-from .base import load_iris

('sklearn/datasets', '__init__.py')
-from .base import load_linnerud

('sklearn/datasets', '__init__.py')
-from .base import load_sample_images

('sklearn/datasets', '__init__.py')
-from .base import load_sample_image

('sklearn/datasets', '__init__.py')
-from .base import load_wine

('sklearn/datasets', '__init__.py')
-from .base import get_data_home

('sklearn/datasets', '__init__.py')
-from .base import clear_data_home

('sklearn/datasets', '__init__.py')
-from .covtype import fetch_covtype

('sklearn/datasets', '__init__.py')
-from .kddcup99 import fetch_kddcup99

('sklearn/datasets', '__init__.py')
-from .lfw import fetch_lfw_pairs

('sklearn/datasets', '__init__.py')
-from .lfw import fetch_lfw_people

('sklearn/datasets', '__init__.py')
-from .twenty_newsgroups import fetch_20newsgroups

('sklearn/datasets', '__init__.py')
-from .twenty_newsgroups import fetch_20newsgroups_vectorized

('sklearn/datasets', '__init__.py')
-from .mldata import fetch_mldata, mldata_filename

('sklearn/datasets', '__init__.py')
-from .openml import fetch_openml

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_classification

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_multilabel_classification

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_hastie_10_2

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_regression

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_blobs

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_moons

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_circles

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_friedman1

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_friedman2

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_friedman3

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_low_rank_matrix

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_sparse_coded_signal

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_sparse_uncorrelated

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_spd_matrix

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_swiss_roll

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_s_curve

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_sparse_spd_matrix

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_gaussian_quantiles

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_biclusters

('sklearn/datasets', '__init__.py')
-from .samples_generator import make_checkerboard

('sklearn/datasets', '__init__.py')
-from .svmlight_format import load_svmlight_file

('sklearn/datasets', '__init__.py')
-from .svmlight_format import load_svmlight_files

('sklearn/datasets', '__init__.py')
-from .svmlight_format import dump_svmlight_file

('sklearn/datasets', '__init__.py')
-from .olivetti_faces import fetch_olivetti_faces

('sklearn/datasets', '__init__.py')
-from .species_distributions import fetch_species_distributions

('sklearn/datasets', '__init__.py')
-from .california_housing import fetch_california_housing

('sklearn/datasets', '__init__.py')
-from .rcv1 import fetch_rcv1

('sklearn/datasets', '__init__.py')
+from ._base import load_breast_cancer

('sklearn/datasets', '__init__.py')
+from ._base import load_boston

('sklearn/datasets', '__init__.py')
+from ._base import load_diabetes

('sklearn/datasets', '__init__.py')
+from ._base import load_digits

('sklearn/datasets', '__init__.py')
+from ._base import load_files

('sklearn/datasets', '__init__.py')
+from ._base import load_iris

('sklearn/datasets', '__init__.py')
+from ._base import load_linnerud

('sklearn/datasets', '__init__.py')
+from ._base import load_sample_images

('sklearn/datasets', '__init__.py')
+from ._base import load_sample_image

('sklearn/datasets', '__init__.py')
+from ._base import load_wine

('sklearn/datasets', '__init__.py')
+from ._base import get_data_home

('sklearn/datasets', '__init__.py')
+from ._base import clear_data_home

('sklearn/datasets', '__init__.py')
+from ._covtype import fetch_covtype

('sklearn/datasets', '__init__.py')
+from ._kddcup99 import fetch_kddcup99

('sklearn/datasets', '__init__.py')
+from ._lfw import fetch_lfw_pairs

('sklearn/datasets', '__init__.py')
+from ._lfw import fetch_lfw_people

('sklearn/datasets', '__init__.py')
+from ._twenty_newsgroups import fetch_20newsgroups

('sklearn/datasets', '__init__.py')
+from ._twenty_newsgroups import fetch_20newsgroups_vectorized

('sklearn/datasets', '__init__.py')
+from ._openml import fetch_openml

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_classification

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_multilabel_classification

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_hastie_10_2

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_regression

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_blobs

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_moons

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_circles

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_friedman1

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_friedman2

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_friedman3

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_low_rank_matrix

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_sparse_coded_signal

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_sparse_uncorrelated

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_spd_matrix

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_swiss_roll

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_s_curve

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_sparse_spd_matrix

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_gaussian_quantiles

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_biclusters

('sklearn/datasets', '__init__.py')
+from ._samples_generator import make_checkerboard

('sklearn/datasets', '__init__.py')
+from ._svmlight_format_io import load_svmlight_file

('sklearn/datasets', '__init__.py')
+from ._svmlight_format_io import load_svmlight_files

('sklearn/datasets', '__init__.py')
+from ._svmlight_format_io import dump_svmlight_file

('sklearn/datasets', '__init__.py')
+from ._olivetti_faces import fetch_olivetti_faces

('sklearn/datasets', '__init__.py')
+from ._species_distributions import fetch_species_distributions

('sklearn/datasets', '__init__.py')
+from ._california_housing import fetch_california_housing

('sklearn/datasets', '__init__.py')
+from ._rcv1 import fetch_rcv1

('sklearn/externals', '_arff.py')
-import sys

('sklearn/externals', '_arff.py')
+from typing import TYPE_CHECKING

('sklearn/externals', '_arff.py')
+from typing import Optional, List, Dict, Any, Iterator, Union, Tuple

('sklearn/externals', '_arff.py')
+    from typing_extensions import TypedDict

('sklearn/externals', '_arff.py')
-    from itertools import izip as zip

('sklearn/linear_model', '__init__.py')
-from .base import LinearRegression

('sklearn/linear_model', '__init__.py')
+from ._base import LinearRegression

('sklearn/linear_model', '__init__.py')
+from ._bayes import BayesianRidge, ARDRegression

('sklearn/linear_model', '__init__.py')
+from ._least_angle import (

('sklearn/linear_model', '__init__.py')
+from ._coordinate_descent import (

('sklearn/linear_model', '__init__.py')
+from ._glm import PoissonRegressor, GammaRegressor, TweedieRegressor

('sklearn/linear_model', '__init__.py')
+from ._huber import HuberRegressor

('sklearn/linear_model', '__init__.py')
+from ._sgd_fast import Hinge, Log, ModifiedHuber, SquaredLoss, Huber

('sklearn/linear_model', '__init__.py')
+from ._stochastic_gradient import SGDClassifier, SGDRegressor, SGDOneClassSVM

('sklearn/linear_model', '__init__.py')
+from ._ridge import Ridge, RidgeCV, RidgeClassifier, RidgeClassifierCV, ridge_regression

('sklearn/linear_model', '__init__.py')
+from ._logistic import LogisticRegression, LogisticRegressionCV

('sklearn/linear_model', '__init__.py')
+from ._omp import (

('sklearn/linear_model', '__init__.py')
+from ._passive_aggressive import PassiveAggressiveClassifier

('sklearn/linear_model', '__init__.py')
+from ._passive_aggressive import PassiveAggressiveRegressor

('sklearn/linear_model', '__init__.py')
+from ._perceptron import Perceptron

('sklearn/linear_model', '__init__.py')
-from .bayes import BayesianRidge, ARDRegression

('sklearn/linear_model', '__init__.py')
-from .least_angle import (Lars, LassoLars, lars_path, lars_path_gram, LarsCV,

('sklearn/linear_model', '__init__.py')
-from .coordinate_descent import (Lasso, ElasticNet, LassoCV, ElasticNetCV,

('sklearn/linear_model', '__init__.py')
-from .huber import HuberRegressor

('sklearn/linear_model', '__init__.py')
-from .sgd_fast import Hinge, Log, ModifiedHuber, SquaredLoss, Huber

('sklearn/linear_model', '__init__.py')
-from .stochastic_gradient import SGDClassifier, SGDRegressor

('sklearn/linear_model', '__init__.py')
-from .ridge import (Ridge, RidgeCV, RidgeClassifier, RidgeClassifierCV,

('sklearn/linear_model', '__init__.py')
-from .logistic import (LogisticRegression, LogisticRegressionCV,

('sklearn/linear_model', '__init__.py')
-from .omp import (orthogonal_mp, orthogonal_mp_gram, OrthogonalMatchingPursuit,

('sklearn/linear_model', '__init__.py')
-from .passive_aggressive import PassiveAggressiveClassifier

('sklearn/linear_model', '__init__.py')
-from .passive_aggressive import PassiveAggressiveRegressor

('sklearn/linear_model', '__init__.py')
-from .perceptron import Perceptron

('sklearn/linear_model', '__init__.py')
+from ._quantile import QuantileRegressor

('sklearn/linear_model', '__init__.py')
+from ._ransac import RANSACRegressor

('sklearn/linear_model', '__init__.py')
+from ._theil_sen import TheilSenRegressor

('sklearn/linear_model', '__init__.py')
-from .ransac import RANSACRegressor

('sklearn/linear_model', '__init__.py')
-from .theil_sen import TheilSenRegressor

('sklearn/linear_model', 'setup.py')
-from Cython import Tempita

('sklearn/linear_model', 'setup.py')
+from sklearn._build_utils import gen_from_templates

('sklearn/impute', '_base.py')
-from __future__ import division

('sklearn/impute', '_base.py')
+import numbers

('sklearn/impute', '_base.py')
-import numbers

('sklearn/impute', '_base.py')
+from collections import Counter

('sklearn/impute', '_base.py')
-from scipy import sparse

('sklearn/impute', '_base.py')
+from scipy import sparse as sp

('sklearn/impute', '_base.py')
-from ..utils.fixes import _object_dtype_isnan

('sklearn/impute', '_base.py')
+from ..utils.validation import _check_feature_names_in

('sklearn/impute', '_base.py')
+from ..utils._mask import _get_mask

('sklearn/impute', '_base.py')
+from ..utils import _is_pandas_na

('sklearn/impute', '_base.py')
-from ..utils import check_array

('sklearn/impute', '__init__.py')
+import typing

('sklearn/impute', '__init__.py')
+from ._knn import KNNImputer

('sklearn/impute', '__init__.py')
+    from ._iterative import IterativeImputer  # noqa

('sklearn/impute', '_iterative.py')
-from distutils.version import LooseVersion

('sklearn/impute', '_iterative.py')
-import scipy

('sklearn/impute', '_iterative.py')
-from ..base import clone, BaseEstimator, TransformerMixin

('sklearn/impute', '_iterative.py')
+from ..base import clone

('sklearn/impute', '_iterative.py')
-from ..utils import check_array, check_random_state, safe_indexing

('sklearn/impute', '_iterative.py')
+from ..utils import check_array, check_random_state, _safe_indexing, is_scalar_nan

('sklearn/impute', '_iterative.py')
-from ..utils import is_scalar_nan

('sklearn/impute', '_iterative.py')
-from ._base import (_get_mask, MissingIndicator, SimpleImputer,

('sklearn/impute', '_iterative.py')
+from ..utils.validation import _check_feature_names_in

('sklearn/impute', '_iterative.py')
+from ..utils._mask import _get_mask

('sklearn/impute', '_iterative.py')
+from ._base import _BaseImputer

('sklearn/impute', '_iterative.py')
+from ._base import SimpleImputer

('sklearn/impute', '_iterative.py')
+from ._base import _check_inputs_dtype

('sklearn/impute', '_iterative.py')
-      you need to explicitly import ``enable_iterative_imputer``::

('sklearn/impute', '_iterative.py')
+      you need to explicitly import `enable_iterative_imputer`::

('sklearn/impute', '_iterative.py')
+    >>> import numpy as np

('sklearn/impute', '_iterative.py')
+    >>> from sklearn.experimental import enable_iterative_imputer

('sklearn/impute', '_iterative.py')
+    >>> from sklearn.impute import IterativeImputer

('sklearn/utils', 'optimize.py')
-from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1

('sklearn/utils', 'optimize.py')
+from .fixes import line_search_wolfe1, line_search_wolfe2

('sklearn/utils', 'fixes.py')
-from distutils.version import LooseVersion

('sklearn/utils', 'fixes.py')
+from functools import update_wrapper

('sklearn/utils', 'fixes.py')
+import functools

('sklearn/utils', 'fixes.py')
+import sklearn

('sklearn/utils', 'fixes.py')
-import scipy.sparse as sp

('sklearn/utils', 'fixes.py')
-from scipy.sparse.linalg import lsqr as sparse_lsqr  # noqa

('sklearn/utils', 'fixes.py')
+import scipy.stats

('sklearn/utils', 'fixes.py')
+import threadpoolctl

('sklearn/utils', 'fixes.py')
+from .._config import config_context, get_config

('sklearn/utils', 'fixes.py')
+from ..externals._packaging.version import parse as parse_version

('sklearn/utils', 'fixes.py')
+    from scipy.sparse.linalg import lobpcg

('sklearn/utils', 'fixes.py')
+    # mypy error: Name 'lobpcg' already defined (possibly by an import)

('sklearn/utils', 'fixes.py')
+    from ..externals._lobpcg import lobpcg  # type: ignore  # noqa

('sklearn/utils', 'fixes.py')
+    from scipy.optimize._linesearch import line_search_wolfe2, line_search_wolfe1

('sklearn/utils', 'fixes.py')
+    from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1  # type: ignore  # noqa

('sklearn/utils', 'fixes.py')
-    from scipy.special import comb, logsumexp

('sklearn/utils', 'fixes.py')
-    from scipy.misc import comb, logsumexp  # noqa

('sklearn/utils', 'fixes.py')
+    >>> from sklearn.utils.fixes import loguniform

('sklearn/utils', 'fixes.py')
-    from numpy.ma import MaskedArray    # noqa

('sklearn/utils', 'fixes.py')
+    from numpy import percentile  # type: ignore  # noqa

('sklearn/utils', 'fixes.py')
-    from . import _joblib

('sklearn/utils', 'sparsefuncs_fast.pyx')
-import scipy.sparse as sp

('sklearn/utils', 'estimator_checks.py')
-import sys

('sklearn/utils', 'estimator_checks.py')
-import traceback

('sklearn/utils', 'estimator_checks.py')
+import re

('sklearn/utils', 'estimator_checks.py')
-from functools import partial

('sklearn/utils', 'estimator_checks.py')
+from functools import partial, wraps

('sklearn/utils', 'estimator_checks.py')
+import joblib

('sklearn/utils', 'estimator_checks.py')
-from . import _joblib

('sklearn/utils', 'estimator_checks.py')
-from .testing import assert_raises, _get_args

('sklearn/utils', 'estimator_checks.py')
-from .testing import assert_raises_regex

('sklearn/utils', 'estimator_checks.py')
-from .testing import assert_raise_message

('sklearn/utils', 'estimator_checks.py')
-from .testing import assert_equal

('sklearn/utils', 'estimator_checks.py')
-from .testing import assert_not_equal

('sklearn/utils', 'estimator_checks.py')
-from .testing import assert_in

('sklearn/utils', 'estimator_checks.py')
-from .testing import assert_array_equal

('sklearn/utils', 'estimator_checks.py')
-from .testing import assert_array_almost_equal

('sklearn/utils', 'estimator_checks.py')
-from .testing import assert_allclose

('sklearn/utils', 'estimator_checks.py')
-from .testing import assert_allclose_dense_sparse

('sklearn/utils', 'estimator_checks.py')
-from .testing import assert_warns_message

('sklearn/utils', 'estimator_checks.py')
-from .testing import set_random_state

('sklearn/utils', 'estimator_checks.py')
-from .testing import assert_greater

('sklearn/utils', 'estimator_checks.py')
-from .testing import assert_greater_equal

('sklearn/utils', 'estimator_checks.py')
-from .testing import SkipTest

('sklearn/utils', 'estimator_checks.py')
-from .testing import ignore_warnings

('sklearn/utils', 'estimator_checks.py')
-from .testing import assert_dict_equal

('sklearn/utils', 'estimator_checks.py')
-from .testing import create_memmap_backed_data

('sklearn/utils', 'estimator_checks.py')
+from .. import config_context

('sklearn/utils', 'estimator_checks.py')
+from ._testing import _get_args

('sklearn/utils', 'estimator_checks.py')
+from ._testing import assert_raise_message

('sklearn/utils', 'estimator_checks.py')
+from ._testing import assert_array_equal

('sklearn/utils', 'estimator_checks.py')
+from ._testing import assert_array_almost_equal

('sklearn/utils', 'estimator_checks.py')
+from ._testing import assert_allclose

('sklearn/utils', 'estimator_checks.py')
+from ._testing import assert_allclose_dense_sparse

('sklearn/utils', 'estimator_checks.py')
+from ._testing import assert_array_less

('sklearn/utils', 'estimator_checks.py')
+from ._testing import set_random_state

('sklearn/utils', 'estimator_checks.py')
+from ._testing import SkipTest

('sklearn/utils', 'estimator_checks.py')
+from ._testing import ignore_warnings

('sklearn/utils', 'estimator_checks.py')
+from ._testing import create_memmap_backed_data

('sklearn/utils', 'estimator_checks.py')
+from ._testing import raises

('sklearn/utils', 'estimator_checks.py')
-from ..discriminant_analysis import LinearDiscriminantAnalysis

('sklearn/utils', 'estimator_checks.py')
+from ..linear_model import LinearRegression

('sklearn/utils', 'estimator_checks.py')
+from ..linear_model import LogisticRegression

('sklearn/utils', 'estimator_checks.py')
+from ..linear_model import RANSACRegressor

('sklearn/utils', 'estimator_checks.py')
-from ..base import (clone, ClusterMixin, is_classifier, is_regressor,

('sklearn/utils', 'estimator_checks.py')
+from ..base import (

('sklearn/utils', 'estimator_checks.py')
+from ..exceptions import NotFittedError

('sklearn/utils', 'estimator_checks.py')
-from ..metrics.pairwise import (rbf_kernel, linear_kernel,

('sklearn/utils', 'estimator_checks.py')
-from .import shuffle

('sklearn/utils', 'estimator_checks.py')
+from ..metrics.pairwise import rbf_kernel, linear_kernel, pairwise_distances

('sklearn/utils', 'estimator_checks.py')
+from ..utils.fixes import threadpool_info

('sklearn/utils', 'estimator_checks.py')
+from ..utils.validation import check_is_fitted

('sklearn/utils', 'estimator_checks.py')
+from . import shuffle

('sklearn/utils', 'estimator_checks.py')
+from ._tags import (

('sklearn/utils', 'estimator_checks.py')
-from ..datasets import load_iris, load_boston, make_blobs

('sklearn/utils', 'estimator_checks.py')
+from ..preprocessing import scale

('sklearn/utils', 'estimator_checks.py')
+from ..datasets import (

('sklearn/utils', 'estimator_checks.py')
+    >>> from sklearn.utils.estimator_checks import parametrize_with_checks

('sklearn/utils', 'estimator_checks.py')
+    >>> from sklearn.linear_model import LogisticRegression

('sklearn/utils', 'estimator_checks.py')
+    >>> from sklearn.tree import DecisionTreeRegressor

('sklearn/utils', 'estimator_checks.py')
+    import pytest

('sklearn/utils', 'estimator_checks.py')
-            # being able to import pandas.

('sklearn/utils', 'estimator_checks.py')
+            # SkipTest is thrown when pandas can't be imported, or by checks

('sklearn/utils', 'estimator_checks.py')
+        import pandas as pd

('sklearn/utils', 'estimator_checks.py')
-            import pandas as pd

('sklearn/utils', 'estimator_checks.py')
+            import pandas as pd

('sklearn/utils', 'estimator_checks.py')
+        import pandas as pd

('sklearn/utils', 'estimator_checks.py')
+        import pandas as pd

('sklearn/utils', 'multiclass.py')
+import warnings

('sklearn/utils', 'multiclass.py')
-from scipy.sparse.base import spmatrix

('sklearn/utils', 'multiclass.py')
+    >>> from sklearn.utils.multiclass import type_of_target

('sklearn/utils', 'graph.py')
+import numpy as np

('sklearn/utils', 'graph.py')
-from .graph_shortest_path import graph_shortest_path  # noqa

('sklearn/utils', 'graph.py')
+from .deprecation import deprecated

('sklearn/utils', 'graph.py')
+from ..metrics.pairwise import pairwise_distances

('sklearn/utils', '_joblib.py')
-import os as _os

('sklearn/utils', '__init__.py')
+import pkgutil

('sklearn/utils', '__init__.py')
+import inspect

('sklearn/utils', '__init__.py')
+from importlib import import_module

('sklearn/utils', '__init__.py')
+from operator import itemgetter

('sklearn/utils', '__init__.py')
+from itertools import compress

('sklearn/utils', '__init__.py')
+from itertools import islice

('sklearn/utils', '__init__.py')
+import math

('sklearn/utils', '__init__.py')
+from pathlib import Path

('sklearn/utils', '__init__.py')
+from contextlib import suppress

('sklearn/utils', '__init__.py')
-from .validation import (as_float_array,

('sklearn/utils', '__init__.py')
+from .fixes import parse_version, threadpool_info

('sklearn/utils', '__init__.py')
+from ._estimator_html_repr import estimator_html_repr

('sklearn/utils', '__init__.py')
+from .validation import (

('sklearn/utils', '__init__.py')
+from ._bunch import Bunch

('sklearn/utils', '__init__.py')
-       "Please import this functionality directly from joblib, which can "

('sklearn/utils', '__init__.py')
+    import numpy  # noqa

('sklearn/utils', '__init__.py')
+    import scipy  # noqa

('sklearn/utils', '__init__.py')
+      >>> import numpy as np

('sklearn/utils', '__init__.py')
+      >>> import numpy as np

('sklearn/utils', '__init__.py')
+    >>> import numpy as np

('sklearn/utils', '__init__.py')
+    >>> from sklearn.utils import _to_object_array

('sklearn/utils', '__init__.py')
+        from pandas import NA

('sklearn/utils', '__init__.py')
+    >>> import numpy as np

('sklearn/utils', '__init__.py')
+    >>> from sklearn.utils import is_scalar_nan

('sklearn/utils', '__init__.py')
-    Plot utilities like :func:`plot_partial_dependence` should lazily import

('sklearn/utils', '__init__.py')
+    Plot utilities like any of the Display's plotting functions should lazily import

('sklearn/utils', '__init__.py')
+    Plot utilities like :func:`fetch_openml` should lazily import

('sklearn/utils', '__init__.py')
+        import pandas  # noqa

('sklearn/utils', '__init__.py')
+    # lazy import to avoid circular imports from sklearn.base

('sklearn/utils', '__init__.py')
+    from ._testing import ignore_warnings

('sklearn/utils', '__init__.py')
+    from ..base import (

('sklearn/utils', '__init__.py')
+    # Ignore deprecation warnings triggered at import time and from walking

('sklearn/utils', '__init__.py')
+        for importer, modname, ispkg in pkgutil.walk_packages(

('sklearn/utils', '__init__.py')
+            module = import_module(modname)

('sklearn/utils', '_pprint.py')
-from inspect import signature

('sklearn/utils', '_pprint.py')
+import inspect

('sklearn/utils', 'setup.py')
+from sklearn._build_utils import gen_from_templates

('sklearn/utils', 'setup.py')
-    from Cython import Tempita

('sklearn/utils', '_show_versions.py')
-import importlib

('sklearn/utils', '_show_versions.py')
+from ..utils.fixes import threadpool_info

('sklearn/utils', '_show_versions.py')
+from .. import __version__

('sklearn/utils', '_show_versions.py')
+from ._openmp_helpers import _openmp_parallelism_enabled

('sklearn/utils', '_show_versions.py')
+    This function does not import the modules to collect the version numbers

('sklearn/utils', '_show_versions.py')
+            from pkg_resources import get_distribution, DistributionNotFound

('sklearn/utils', '_show_versions.py')
-                mod = importlib.import_module(modname)

('sklearn/utils', '_show_versions.py')
+        from importlib.metadata import version, PackageNotFoundError

('sklearn/utils', '_show_versions.py')
-    from .._build_utils import get_blas_info

('sklearn/utils', 'metaestimators.py')
+from typing import List, Any

('sklearn/utils', 'metaestimators.py')
+from types import MethodType

('sklearn/utils', 'metaestimators.py')
+import warnings

('sklearn/utils', 'metaestimators.py')
+from functools import wraps

('sklearn/utils', 'metaestimators.py')
-from ..utils import safe_indexing

('sklearn/utils', 'metaestimators.py')
+from contextlib import suppress

('sklearn/utils', 'metaestimators.py')
+from ..utils import _safe_indexing

('sklearn/utils', 'metaestimators.py')
+from ..utils._tags import _safe_tags

('sklearn/utils', 'metaestimators.py')
+    >>> from sklearn.utils.metaestimators import available_if

('sklearn/utils', 'extmath.py')
+    >>> from sklearn.utils.extmath import cartesian

('sklearn/utils', 'sparsefuncs.py')
+from ..utils.validation import _check_sample_weight

('sklearn/utils', '_logistic_sigmoid.pyx')
+np.import_array()

('sklearn/utils', 'validation.py')
+from functools import wraps

('sklearn/utils', 'validation.py')
+import operator

('sklearn/utils', 'validation.py')
-from distutils.version import LooseVersion

('sklearn/utils', 'validation.py')
-from inspect import signature

('sklearn/utils', 'validation.py')
-from numpy.core.numeric import ComplexWarning

('sklearn/utils', 'validation.py')
+from inspect import signature, isclass, Parameter

('sklearn/utils', 'validation.py')
+from numpy.core.numeric import ComplexWarning  # type: ignore

('sklearn/utils', 'validation.py')
+import joblib

('sklearn/utils', 'validation.py')
+from contextlib import suppress

('sklearn/utils', 'validation.py')
-from ..exceptions import NonBLASDotWarning

('sklearn/utils', 'validation.py')
+from ..exceptions import PositiveSpectrumWarning

('sklearn/utils', 'validation.py')
-from ._joblib import Memory

('sklearn/utils', 'validation.py')
-from ._joblib import __version__ as joblib_version

('sklearn/utils', 'validation.py')
+    from pandas.api.types import (

('sklearn/utils', 'validation.py')
+        from pandas.api.types import is_extension_array_dtype

('sklearn/utils', 'validation.py')
+            from pandas.api.types import is_sparse

('sklearn/utils', 'validation.py')
+    >>> from sklearn.utils.validation import has_fit_parameter

('sklearn/utils', 'validation.py')
+    >>> from sklearn.utils.validation import _check_psd_eigenvalues

('sklearn/utils', 'validation.py')
+    from . import _safe_indexing

('sklearn/covariance', '__init__.py')
-from .empirical_covariance_ import empirical_covariance, EmpiricalCovariance, \

('sklearn/covariance', '__init__.py')
-from .shrunk_covariance_ import shrunk_covariance, ShrunkCovariance, \

('sklearn/covariance', '__init__.py')
-from .robust_covariance import fast_mcd, MinCovDet

('sklearn/covariance', '__init__.py')
-from .graph_lasso_ import graph_lasso, GraphLasso, GraphLassoCV,\

('sklearn/covariance', '__init__.py')
-from .elliptic_envelope import EllipticEnvelope

('sklearn/covariance', '__init__.py')
+from ._empirical_covariance import (

('sklearn/covariance', '__init__.py')
+from ._shrunk_covariance import (

('sklearn/covariance', '__init__.py')
+from ._robust_covariance import fast_mcd, MinCovDet

('sklearn/covariance', '__init__.py')
+from ._graph_lasso import graphical_lasso, GraphicalLasso, GraphicalLassoCV

('sklearn/covariance', '__init__.py')
+from ._elliptic_envelope import EllipticEnvelope

('sklearn/neural_network', '__init__.py')
-from .rbm import BernoulliRBM

('sklearn/neural_network', '__init__.py')
+from ._rbm import BernoulliRBM

('sklearn/neural_network', '__init__.py')
-from .multilayer_perceptron import MLPClassifier

('sklearn/neural_network', '__init__.py')
-from .multilayer_perceptron import MLPRegressor

('sklearn/neural_network', '__init__.py')
+from ._multilayer_perceptron import MLPClassifier

('sklearn/neural_network', '__init__.py')
+from ._multilayer_perceptron import MLPRegressor

('sklearn/feature_selection', '__init__.py')
-from .univariate_selection import chi2

('sklearn/feature_selection', '__init__.py')
-from .univariate_selection import f_classif

('sklearn/feature_selection', '__init__.py')
-from .univariate_selection import f_oneway

('sklearn/feature_selection', '__init__.py')
-from .univariate_selection import f_regression

('sklearn/feature_selection', '__init__.py')
-from .univariate_selection import SelectPercentile

('sklearn/feature_selection', '__init__.py')
-from .univariate_selection import SelectKBest

('sklearn/feature_selection', '__init__.py')
-from .univariate_selection import SelectFpr

('sklearn/feature_selection', '__init__.py')
-from .univariate_selection import SelectFdr

('sklearn/feature_selection', '__init__.py')
-from .univariate_selection import SelectFwe

('sklearn/feature_selection', '__init__.py')
-from .univariate_selection import GenericUnivariateSelect

('sklearn/feature_selection', '__init__.py')
+from ._univariate_selection import chi2

('sklearn/feature_selection', '__init__.py')
+from ._univariate_selection import f_classif

('sklearn/feature_selection', '__init__.py')
+from ._univariate_selection import f_oneway

('sklearn/feature_selection', '__init__.py')
+from ._univariate_selection import f_regression

('sklearn/feature_selection', '__init__.py')
+from ._univariate_selection import r_regression

('sklearn/feature_selection', '__init__.py')
+from ._univariate_selection import SelectPercentile

('sklearn/feature_selection', '__init__.py')
+from ._univariate_selection import SelectKBest

('sklearn/feature_selection', '__init__.py')
+from ._univariate_selection import SelectFpr

('sklearn/feature_selection', '__init__.py')
+from ._univariate_selection import SelectFdr

('sklearn/feature_selection', '__init__.py')
+from ._univariate_selection import SelectFwe

('sklearn/feature_selection', '__init__.py')
+from ._univariate_selection import GenericUnivariateSelect

('sklearn/feature_selection', '__init__.py')
-from .variance_threshold import VarianceThreshold

('sklearn/feature_selection', '__init__.py')
+from ._variance_threshold import VarianceThreshold

('sklearn/feature_selection', '__init__.py')
-from .rfe import RFE

('sklearn/feature_selection', '__init__.py')
-from .rfe import RFECV

('sklearn/feature_selection', '__init__.py')
+from ._rfe import RFE

('sklearn/feature_selection', '__init__.py')
+from ._rfe import RFECV

('sklearn/feature_selection', '__init__.py')
-from .from_model import SelectFromModel

('sklearn/feature_selection', '__init__.py')
+from ._from_model import SelectFromModel

('sklearn/feature_selection', '__init__.py')
-from .mutual_info_ import mutual_info_regression, mutual_info_classif

('sklearn/feature_selection', '__init__.py')
+from ._sequential import SequentialFeatureSelector

('sklearn/feature_selection', '__init__.py')
+from ._mutual_info import mutual_info_regression, mutual_info_classif

('sklearn/feature_selection', '__init__.py')
+from ._base import SelectorMixin

('sklearn/inspection', '__init__.py')
-from .partial_dependence import partial_dependence

('sklearn/inspection', '__init__.py')
-from .partial_dependence import plot_partial_dependence

('sklearn/inspection', '__init__.py')
+from ._permutation_importance import permutation_importance

('sklearn/inspection', '__init__.py')
+from ._plot.decision_boundary import DecisionBoundaryDisplay

('sklearn/inspection', '__init__.py')
+from ._partial_dependence import partial_dependence

('sklearn/inspection', '__init__.py')
+from ._plot.partial_dependence import plot_partial_dependence

('sklearn/inspection', '__init__.py')
+from ._plot.partial_dependence import PartialDependenceDisplay

('sklearn/inspection', '__init__.py')
+    "permutation_importance",

('sklearn/svm', '__init__.py')
-from .classes import SVC, NuSVC, SVR, NuSVR, OneClassSVM, LinearSVC, \

('sklearn/svm', '__init__.py')
-from .bounds import l1_min_c

('sklearn/svm', '__init__.py')
-from . import libsvm, liblinear, libsvm_sparse

('sklearn/svm', '__init__.py')
+from ._classes import SVC, NuSVC, SVR, NuSVR, OneClassSVM, LinearSVC, LinearSVR

('sklearn/svm', '__init__.py')
+from ._bounds import l1_min_c

('sklearn/manifold', '_utils.pyx')
+np.import_array()

('sklearn/manifold', '__init__.py')
-from .locally_linear import locally_linear_embedding, LocallyLinearEmbedding

('sklearn/manifold', '__init__.py')
-from .isomap import Isomap

('sklearn/manifold', '__init__.py')
-from .mds import MDS, smacof

('sklearn/manifold', '__init__.py')
-from .spectral_embedding_ import SpectralEmbedding, spectral_embedding

('sklearn/manifold', '__init__.py')
-from .t_sne import TSNE

('sklearn/manifold', '__init__.py')
+from ._locally_linear import locally_linear_embedding, LocallyLinearEmbedding

('sklearn/manifold', '__init__.py')
+from ._isomap import Isomap

('sklearn/manifold', '__init__.py')
+from ._mds import MDS, smacof

('sklearn/manifold', '__init__.py')
+from ._spectral_embedding import SpectralEmbedding, spectral_embedding

('sklearn/manifold', '__init__.py')
+from ._t_sne import TSNE, trustworthiness

('sklearn/manifold', '_barnes_hut_tsne.pyx')
-from libc.stdlib cimport malloc, free

('sklearn/manifold', '_barnes_hut_tsne.pyx')
+import numpy as np

('sklearn/manifold', '_barnes_hut_tsne.pyx')
+cimport numpy as np

('sklearn/manifold', '_barnes_hut_tsne.pyx')
-import numpy as np

('sklearn/manifold', '_barnes_hut_tsne.pyx')
-cimport numpy as np

('sklearn/manifold', '_barnes_hut_tsne.pyx')
-from ..neighbors.quad_tree cimport _QuadTree

('sklearn/manifold', '_barnes_hut_tsne.pyx')
+from libc.stdlib cimport malloc, free

('sklearn/manifold', '_barnes_hut_tsne.pyx')
+from cython.parallel cimport prange, parallel

('sklearn/manifold', '_barnes_hut_tsne.pyx')
+from ..neighbors._quad_tree cimport _QuadTree

('sklearn/manifold', '_barnes_hut_tsne.pyx')
+np.import_array()

('sklearn/mixture', '__init__.py')
-from .gaussian_mixture import GaussianMixture

('sklearn/mixture', '__init__.py')
-from .bayesian_mixture import BayesianGaussianMixture

('sklearn/mixture', '__init__.py')
+from ._gaussian_mixture import GaussianMixture

('sklearn/mixture', '__init__.py')
+from ._bayesian_mixture import BayesianGaussianMixture

('sklearn/preprocessing', '_encoders.py')
-from .. import get_config as _get_config

('sklearn/preprocessing', '_encoders.py')
-from ..base import BaseEstimator, TransformerMixin

('sklearn/preprocessing', '_encoders.py')
-from ..utils import check_array

('sklearn/preprocessing', '_encoders.py')
-from ..utils import deprecated

('sklearn/preprocessing', '_encoders.py')
-from ..utils.fixes import _argmax, _object_dtype_isnan

('sklearn/preprocessing', '_encoders.py')
+from ..base import BaseEstimator, TransformerMixin, _OneToOneFeatureMixin

('sklearn/preprocessing', '_encoders.py')
+from ..utils import check_array, is_scalar_nan

('sklearn/preprocessing', '_encoders.py')
+from ..utils.deprecation import deprecated

('sklearn/preprocessing', '_encoders.py')
-from .base import _transform_selected

('sklearn/preprocessing', '_encoders.py')
-from .label import _encode, _encode_check_unknown

('sklearn/preprocessing', '_encoders.py')
+from ..utils.validation import _check_feature_names_in

('sklearn/preprocessing', '_encoders.py')
+from ..utils._mask import _get_mask

('sklearn/preprocessing', '_encoders.py')
+from ..utils._encode import _encode, _check_unknown, _unique, _get_counts

('sklearn/preprocessing', '_encoders.py')
+    >>> import numpy as np

('sklearn/preprocessing', '_encoders.py')
+    >>> import numpy as np

('sklearn/preprocessing', '__init__.py')
-from .data import Binarizer

('sklearn/preprocessing', '__init__.py')
-from .data import KernelCenterer

('sklearn/preprocessing', '__init__.py')
-from .data import MinMaxScaler

('sklearn/preprocessing', '__init__.py')
-from .data import MaxAbsScaler

('sklearn/preprocessing', '__init__.py')
-from .data import Normalizer

('sklearn/preprocessing', '__init__.py')
-from .data import RobustScaler

('sklearn/preprocessing', '__init__.py')
-from .data import StandardScaler

('sklearn/preprocessing', '__init__.py')
-from .data import QuantileTransformer

('sklearn/preprocessing', '__init__.py')
-from .data import add_dummy_feature

('sklearn/preprocessing', '__init__.py')
-from .data import binarize

('sklearn/preprocessing', '__init__.py')
-from .data import normalize

('sklearn/preprocessing', '__init__.py')
-from .data import scale

('sklearn/preprocessing', '__init__.py')
-from .data import robust_scale

('sklearn/preprocessing', '__init__.py')
-from .data import maxabs_scale

('sklearn/preprocessing', '__init__.py')
-from .data import minmax_scale

('sklearn/preprocessing', '__init__.py')
-from .data import quantile_transform

('sklearn/preprocessing', '__init__.py')
-from .data import power_transform

('sklearn/preprocessing', '__init__.py')
-from .data import PowerTransformer

('sklearn/preprocessing', '__init__.py')
-from .data import PolynomialFeatures

('sklearn/preprocessing', '__init__.py')
+from ._data import Binarizer

('sklearn/preprocessing', '__init__.py')
+from ._data import KernelCenterer

('sklearn/preprocessing', '__init__.py')
+from ._data import MinMaxScaler

('sklearn/preprocessing', '__init__.py')
+from ._data import MaxAbsScaler

('sklearn/preprocessing', '__init__.py')
+from ._data import Normalizer

('sklearn/preprocessing', '__init__.py')
+from ._data import RobustScaler

('sklearn/preprocessing', '__init__.py')
+from ._data import StandardScaler

('sklearn/preprocessing', '__init__.py')
+from ._data import QuantileTransformer

('sklearn/preprocessing', '__init__.py')
+from ._data import add_dummy_feature

('sklearn/preprocessing', '__init__.py')
+from ._data import binarize

('sklearn/preprocessing', '__init__.py')
+from ._data import normalize

('sklearn/preprocessing', '__init__.py')
+from ._data import scale

('sklearn/preprocessing', '__init__.py')
+from ._data import robust_scale

('sklearn/preprocessing', '__init__.py')
+from ._data import maxabs_scale

('sklearn/preprocessing', '__init__.py')
+from ._data import minmax_scale

('sklearn/preprocessing', '__init__.py')
+from ._data import quantile_transform

('sklearn/preprocessing', '__init__.py')
+from ._data import power_transform

('sklearn/preprocessing', '__init__.py')
+from ._data import PowerTransformer

('sklearn/preprocessing', '__init__.py')
-from .label import label_binarize

('sklearn/preprocessing', '__init__.py')
-from .label import LabelBinarizer

('sklearn/preprocessing', '__init__.py')
-from .label import LabelEncoder

('sklearn/preprocessing', '__init__.py')
-from .label import MultiLabelBinarizer

('sklearn/preprocessing', '__init__.py')
+from ._label import label_binarize

('sklearn/preprocessing', '__init__.py')
+from ._label import LabelBinarizer

('sklearn/preprocessing', '__init__.py')
+from ._label import LabelEncoder

('sklearn/preprocessing', '__init__.py')
+from ._label import MultiLabelBinarizer

('sklearn/preprocessing', '__init__.py')
-from .imputation import Imputer

('sklearn/preprocessing', '__init__.py')
+from ._polynomial import PolynomialFeatures

('sklearn/preprocessing', '__init__.py')
+from ._polynomial import SplineTransformer

('sklearn/preprocessing', '_discretization.py')
-from ..utils.validation import FLOAT_DTYPES

('sklearn/preprocessing', '_discretization.py')
+from ..utils.validation import check_random_state

('sklearn/preprocessing', '_discretization.py')
+from ..utils.validation import _check_feature_names_in

('sklearn/preprocessing', '_discretization.py')
+from ..utils.validation import check_scalar

('sklearn/preprocessing', '_discretization.py')
+from ..utils import _safe_indexing

('sklearn/preprocessing', '_discretization.py')
+    >>> from sklearn.preprocessing import KBinsDiscretizer

('sklearn/preprocessing', '_csr_polynomial_expansion.pyx')
+np.import_array()

('sklearn/preprocessing', '_function_transformer.py')
+import numpy as np

('sklearn/preprocessing', '_function_transformer.py')
-from ..utils import check_array

('sklearn/preprocessing', '_function_transformer.py')
-from ..utils.testing import assert_allclose_dense_sparse

('sklearn/preprocessing', '_function_transformer.py')
+from ..utils.metaestimators import available_if

('sklearn/preprocessing', '_function_transformer.py')
+from ..utils.validation import (

('sklearn/preprocessing', '_function_transformer.py')
+    >>> import numpy as np

('sklearn/preprocessing', '_function_transformer.py')
+    >>> from sklearn.preprocessing import FunctionTransformer

('sklearn/model_selection', '_search.py')
+import numbers

('sklearn/model_selection', '_search.py')
+from numpy.ma import MaskedArray

('sklearn/model_selection', '_search.py')
+from ._validation import _insert_error_scores

('sklearn/model_selection', '_search.py')
+from ._validation import _normalize_score_results

('sklearn/model_selection', '_search.py')
+from ._validation import _warn_or_raise_about_fit_failures

('sklearn/model_selection', '_search.py')
-from ..utils._joblib import Parallel, delayed

('sklearn/model_selection', '_search.py')
+from joblib import Parallel

('sklearn/model_selection', '_search.py')
-from ..utils.fixes import MaskedArray

('sklearn/model_selection', '_search.py')
-from ..utils.validation import indexable, check_is_fitted

('sklearn/model_selection', '_search.py')
-from ..utils.metaestimators import if_delegate_has_method

('sklearn/model_selection', '_search.py')
-from ..metrics.scorer import _check_multimetric_scoring

('sklearn/model_selection', '_search.py')
-from ..metrics.scorer import check_scoring

('sklearn/model_selection', '_search.py')
+from ..utils._tags import _safe_tags

('sklearn/model_selection', '_search.py')
+from ..utils.validation import indexable, check_is_fitted, _check_fit_params

('sklearn/model_selection', '_search.py')
+from ..utils.metaestimators import available_if

('sklearn/model_selection', '_search.py')
+from ..utils.fixes import delayed

('sklearn/model_selection', '_search.py')
+from ..metrics._scorer import _check_multimetric_scoring

('sklearn/model_selection', '_search.py')
+from ..metrics import check_scoring

('sklearn/model_selection', '_search.py')
-    >>> from sklearn import svm, datasets

('sklearn/model_selection', '_search.py')
-    >>> from sklearn.model_selection import GridSearchCV

('sklearn/model_selection', '_search.py')
+    >>> from sklearn import svm, datasets

('sklearn/model_selection', '_search.py')
+    >>> from sklearn.model_selection import GridSearchCV

('sklearn/model_selection', '_search.py')
+    >>> from sklearn.datasets import load_iris

('sklearn/model_selection', '_search.py')
+    >>> from sklearn.linear_model import LogisticRegression

('sklearn/model_selection', '_search.py')
+    >>> from sklearn.model_selection import RandomizedSearchCV

('sklearn/model_selection', '_search.py')
+    >>> from scipy.stats import uniform

('sklearn/model_selection', '__init__.py')
+import typing

('sklearn/model_selection', '__init__.py')
+from ._split import BaseShuffleSplit

('sklearn/model_selection', '__init__.py')
+from ._split import StratifiedGroupKFold

('sklearn/model_selection', '__init__.py')
-from ._search import fit_grid_point

('sklearn/model_selection', '__init__.py')
+    from ._search_successive_halving import (  # noqa

('sklearn/model_selection', '_validation.py')
-from traceback import format_exception_only

('sklearn/model_selection', '_validation.py')
+from functools import partial

('sklearn/model_selection', '_validation.py')
+from traceback import format_exc

('sklearn/model_selection', '_validation.py')
+from contextlib import suppress

('sklearn/model_selection', '_validation.py')
+from collections import Counter

('sklearn/model_selection', '_validation.py')
+from joblib import Parallel, logger

('sklearn/model_selection', '_validation.py')
-from ..utils import (indexable, check_random_state, safe_indexing,

('sklearn/model_selection', '_validation.py')
-from ..utils.validation import _is_arraylike, _num_samples

('sklearn/model_selection', '_validation.py')
+from ..utils import indexable, check_random_state, _safe_indexing

('sklearn/model_selection', '_validation.py')
+from ..utils.validation import _check_fit_params

('sklearn/model_selection', '_validation.py')
+from ..utils.validation import _num_samples

('sklearn/model_selection', '_validation.py')
+from ..utils.fixes import delayed

('sklearn/model_selection', '_validation.py')
-from ..utils._joblib import Parallel, delayed

('sklearn/model_selection', '_validation.py')
-from ..metrics.scorer import check_scoring, _check_multimetric_scoring

('sklearn/model_selection', '_validation.py')
+from ..metrics import check_scoring

('sklearn/model_selection', '_validation.py')
+from ..metrics._scorer import _check_multimetric_scoring, _MultimetricScorer

('sklearn/model_selection', '_validation.py')
-    >>> from sklearn.metrics.scorer import make_scorer

('sklearn/model_selection', '_validation.py')
+    >>> from sklearn.metrics import make_scorer

('sklearn/model_selection', '_split.py')
+from collections import defaultdict

('sklearn/model_selection', '_split.py')
-from ..utils import indexable, check_random_state, safe_indexing

('sklearn/model_selection', '_split.py')
+from scipy.special import comb

('sklearn/model_selection', '_split.py')
+from ..utils import indexable, check_random_state, _safe_indexing

('sklearn/model_selection', '_split.py')
-from ..utils.fixes import comb

('sklearn/model_selection', '_split.py')
+    >>> import numpy as np

('sklearn/model_selection', '_split.py')
+    >>> from sklearn.model_selection import StratifiedGroupKFold

('sklearn/model_selection', '_split.py')
+    >>> import numpy as np

('sklearn/model_selection', '_split.py')
+    >>> from sklearn.model_selection import GroupShuffleSplit

('sklearn/_build_utils', 'openmp_helpers.py')
-import glob

('sklearn/_build_utils', 'openmp_helpers.py')
-import tempfile

('sklearn/_build_utils', 'openmp_helpers.py')
+import warnings

('sklearn/_build_utils', 'openmp_helpers.py')
-from numpy.distutils.ccompiler import new_compiler

('sklearn/_build_utils', 'openmp_helpers.py')
-from distutils.sysconfig import customize_compiler

('sklearn/_build_utils', 'openmp_helpers.py')
+from .pre_build_helpers import compile_test_program

('sklearn/_build_utils', '__init__.py')
+import sklearn

('sklearn/_build_utils', '__init__.py')
+import contextlib

('sklearn/_build_utils', '__init__.py')
-from numpy.distutils.system_info import get_info

('sklearn/_build_utils', '__init__.py')
+from .pre_build_helpers import basic_check_build

('sklearn/_build_utils', '__init__.py')
+from .._min_dependencies import CYTHON_MIN_VERSION

('sklearn/_build_utils', '__init__.py')
+        import Cython

('sklearn/_build_utils', '__init__.py')
+    from Cython.Build import cythonize

('sklearn/_build_utils', '__init__.py')
+        import joblib

('sklearn/_build_utils', '__init__.py')
+    # Lazy import because cython is not a runtime dependency.

('sklearn/_build_utils', '__init__.py')
+    from Cython import Tempita

('sklearn/_build_utils', '__init__.py')
-            import Cython

('sklearn/_build_utils', '__init__.py')
-            from Cython.Build import cythonize

('sklearn/decomposition', '__init__.py')
-from .nmf import NMF, non_negative_factorization

('sklearn/decomposition', '__init__.py')
-from .pca import PCA

('sklearn/decomposition', '__init__.py')
-from .incremental_pca import IncrementalPCA

('sklearn/decomposition', '__init__.py')
-from .kernel_pca import KernelPCA

('sklearn/decomposition', '__init__.py')
-from .sparse_pca import SparsePCA, MiniBatchSparsePCA

('sklearn/decomposition', '__init__.py')
-from .truncated_svd import TruncatedSVD

('sklearn/decomposition', '__init__.py')
-from .fastica_ import FastICA, fastica

('sklearn/decomposition', '__init__.py')
-from .dict_learning import (dict_learning, dict_learning_online, sparse_encode,

('sklearn/decomposition', '__init__.py')
-from .factor_analysis import FactorAnalysis

('sklearn/decomposition', '__init__.py')
+from ._nmf import (

('sklearn/decomposition', '__init__.py')
+from ._pca import PCA

('sklearn/decomposition', '__init__.py')
+from ._incremental_pca import IncrementalPCA

('sklearn/decomposition', '__init__.py')
+from ._kernel_pca import KernelPCA

('sklearn/decomposition', '__init__.py')
+from ._sparse_pca import SparsePCA, MiniBatchSparsePCA

('sklearn/decomposition', '__init__.py')
+from ._truncated_svd import TruncatedSVD

('sklearn/decomposition', '__init__.py')
+from ._fastica import FastICA, fastica

('sklearn/decomposition', '__init__.py')
+from ._dict_learning import (

('sklearn/decomposition', '__init__.py')
+from ._factor_analysis import FactorAnalysis

('sklearn/decomposition', '__init__.py')
-from .online_lda import LatentDirichletAllocation

('sklearn/decomposition', '__init__.py')
+from ._lda import LatentDirichletAllocation

('sklearn/cross_decomposition', '__init__.py')
-from .pls_ import *  # noqa

('sklearn/cross_decomposition', '__init__.py')
-from .cca_ import *  # noqa

('sklearn/cross_decomposition', '__init__.py')
+from ._pls import PLSCanonical, PLSRegression, PLSSVD, CCA

('sklearn/neighbors', '__init__.py')
-from .ball_tree import BallTree

('sklearn/neighbors', '__init__.py')
-from .kd_tree import KDTree

('sklearn/neighbors', '__init__.py')
-from .dist_metrics import DistanceMetric

('sklearn/neighbors', '__init__.py')
-from .graph import kneighbors_graph, radius_neighbors_graph

('sklearn/neighbors', '__init__.py')
-from .unsupervised import NearestNeighbors

('sklearn/neighbors', '__init__.py')
-from .classification import KNeighborsClassifier, RadiusNeighborsClassifier

('sklearn/neighbors', '__init__.py')
-from .regression import KNeighborsRegressor, RadiusNeighborsRegressor

('sklearn/neighbors', '__init__.py')
-from .nearest_centroid import NearestCentroid

('sklearn/neighbors', '__init__.py')
-from .kde import KernelDensity

('sklearn/neighbors', '__init__.py')
-from .lof import LocalOutlierFactor

('sklearn/neighbors', '__init__.py')
-from .nca import NeighborhoodComponentsAnalysis

('sklearn/neighbors', '__init__.py')
-from .base import VALID_METRICS, VALID_METRICS_SPARSE

('sklearn/neighbors', '__init__.py')
+from ._ball_tree import BallTree

('sklearn/neighbors', '__init__.py')
+from ._kd_tree import KDTree

('sklearn/neighbors', '__init__.py')
+from ._distance_metric import DistanceMetric

('sklearn/neighbors', '__init__.py')
+from ._graph import kneighbors_graph, radius_neighbors_graph

('sklearn/neighbors', '__init__.py')
+from ._graph import KNeighborsTransformer, RadiusNeighborsTransformer

('sklearn/neighbors', '__init__.py')
+from ._unsupervised import NearestNeighbors

('sklearn/neighbors', '__init__.py')
+from ._classification import KNeighborsClassifier, RadiusNeighborsClassifier

('sklearn/neighbors', '__init__.py')
+from ._regression import KNeighborsRegressor, RadiusNeighborsRegressor

('sklearn/neighbors', '__init__.py')
+from ._nearest_centroid import NearestCentroid

('sklearn/neighbors', '__init__.py')
+from ._kde import KernelDensity

('sklearn/neighbors', '__init__.py')
+from ._lof import LocalOutlierFactor

('sklearn/neighbors', '__init__.py')
+from ._nca import NeighborhoodComponentsAnalysis

('sklearn/neighbors', '__init__.py')
+from ._base import VALID_METRICS, VALID_METRICS_SPARSE

('examples/bicluster', 'plot_bicluster_newsgroups.py')
-from sklearn.cluster.bicluster import SpectralCoclustering

('examples/bicluster', 'plot_bicluster_newsgroups.py')
+from sklearn.cluster import SpectralCoclustering

('examples/bicluster', 'plot_bicluster_newsgroups.py')
-from sklearn.datasets.twenty_newsgroups import fetch_20newsgroups

('examples/bicluster', 'plot_bicluster_newsgroups.py')
+from sklearn.datasets import fetch_20newsgroups

('examples/bicluster', 'plot_bicluster_newsgroups.py')
-    important_words = list(feature_names[cluster_words[i]]

('examples/bicluster', 'plot_bicluster_newsgroups.py')
+    important_words = list(

('examples/bicluster', 'plot_bicluster_newsgroups.py')
-    print("words        : {}\n".format(', '.join(important_words)))

('examples/bicluster', 'plot_bicluster_newsgroups.py')
+    print("words        : {}\n".format(", ".join(important_words)))

('examples/bicluster', 'plot_spectral_biclustering.py')
-from sklearn.datasets import samples_generator as sg

('examples/bicluster', 'plot_spectral_biclustering.py')
-from sklearn.cluster.bicluster import SpectralBiclustering

('examples/bicluster', 'plot_spectral_biclustering.py')
+from sklearn.cluster import SpectralBiclustering

('examples/bicluster', 'plot_spectral_coclustering.py')
-from sklearn.datasets import samples_generator as sg

('examples/bicluster', 'plot_spectral_coclustering.py')
-from sklearn.cluster.bicluster import SpectralCoclustering

('examples/bicluster', 'plot_spectral_coclustering.py')
+from sklearn.cluster import SpectralCoclustering

('examples/classification', 'plot_classifier_comparison.py')
+from sklearn.inspection import DecisionBoundaryDisplay

('examples/classification', 'plot_lda.py')
+from sklearn.covariance import OAS

('examples/classification', 'plot_lda_qda.py')
-from scipy import linalg

('examples/classification', 'plot_lda_qda.py')
-import numpy as np

('examples/classification', 'plot_lda_qda.py')
-from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

('examples/classification', 'plot_lda_qda.py')
-from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

('examples/classification', 'plot_lda_qda.py')
+import numpy as np

('examples/classification', 'plot_lda_qda.py')
+from scipy import linalg

('examples/classification', 'plot_lda_qda.py')
+from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

('examples/classification', 'plot_lda_qda.py')
+from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

('examples/classification', 'plot_digits_classification.py')
+from sklearn.model_selection import train_test_split

('examples/tree', 'plot_unveil_tree_structure.py')
+from matplotlib import pyplot as plt

('examples/tree', 'plot_unveil_tree_structure.py')
+from sklearn import tree

('examples/tree', 'plot_iris_dtc.py')
+from sklearn.datasets import load_iris

('examples/tree', 'plot_iris_dtc.py')
-from sklearn.tree import DecisionTreeClassifier, plot_tree

('examples/tree', 'plot_iris_dtc.py')
+from sklearn.tree import DecisionTreeClassifier

('examples/tree', 'plot_iris_dtc.py')
+from sklearn.inspection import DecisionBoundaryDisplay

('examples/tree', 'plot_iris_dtc.py')
+from sklearn.tree import plot_tree

('examples/ensemble', 'plot_forest_importances.py')
-Feature importances with forests of trees

('examples/ensemble', 'plot_forest_importances.py')
+Feature importances with a forest of trees

('examples/ensemble', 'plot_forest_importances.py')
-This examples shows the use of forests of trees to evaluate the importance of

('examples/ensemble', 'plot_forest_importances.py')
-importances of the forest, along with their inter-trees variability.

('examples/ensemble', 'plot_forest_importances.py')
+This example shows the use of a forest of trees to evaluate the importance of

('examples/ensemble', 'plot_forest_importances.py')
+importances of the forest, along with their inter-trees variability represented

('examples/ensemble', 'plot_forest_importances.py')
-import numpy as np

('examples/ensemble', 'plot_forest_importances.py')
-from sklearn.ensemble import ExtraTreesClassifier

('examples/ensemble', 'plot_forest_importances.py')
+from sklearn.model_selection import train_test_split

('examples/ensemble', 'plot_forest_importances.py')
-# Build a forest and compute the feature importances

('examples/ensemble', 'plot_forest_importances.py')
+# A random forest classifier will be fitted to compute the feature importances.

('examples/ensemble', 'plot_forest_importances.py')
+from sklearn.ensemble import RandomForestClassifier

('examples/ensemble', 'plot_forest_importances.py')
+# Feature importance based on mean decrease in impurity

('examples/ensemble', 'plot_forest_importances.py')
+# Feature importances are provided by the fitted attribute

('examples/ensemble', 'plot_forest_importances.py')
+# `feature_importances_` and they are computed as the mean and standard

('examples/ensemble', 'plot_forest_importances.py')
+#     Impurity-based feature importances can be misleading for **high

('examples/ensemble', 'plot_forest_importances.py')
+#     :ref:`permutation_importance` as an alternative below.

('examples/ensemble', 'plot_forest_importances.py')
+import time

('examples/ensemble', 'plot_forest_importances.py')
+import numpy as np

('examples/ensemble', 'plot_forest_importances.py')
-std = np.std([tree.feature_importances_ for tree in forest.estimators_],

('examples/ensemble', 'plot_forest_importances.py')
-indices = np.argsort(importances)[::-1]

('examples/ensemble', 'plot_forest_importances.py')
+std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)

('examples/ensemble', 'plot_forest_importances.py')
+print(f"Elapsed time to compute the importances: {elapsed_time:.3f} seconds")

('examples/ensemble', 'plot_forest_importances.py')
-    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))

('examples/ensemble', 'plot_forest_importances.py')
+# Let's plot the impurity-based importance.

('examples/ensemble', 'plot_forest_importances.py')
+import pandas as pd

('examples/ensemble', 'plot_forest_importances.py')
-# Plot the feature importances of the forest

('examples/ensemble', 'plot_forest_importances.py')
-plt.title("Feature importances")

('examples/ensemble', 'plot_forest_importances.py')
-plt.bar(range(X.shape[1]), importances[indices],

('examples/ensemble', 'plot_forest_importances.py')
+forest_importances = pd.Series(importances, index=feature_names)

('examples/ensemble', 'plot_forest_importances.py')
+forest_importances.plot.bar(yerr=std, ax=ax)

('examples/ensemble', 'plot_forest_importances.py')
+ax.set_title("Feature importances using MDI")

('examples/ensemble', 'plot_forest_importances.py')
+# We observe that, as expected, the three first features are found important.

('examples/ensemble', 'plot_forest_importances.py')
+# Feature importance based on feature permutation

('examples/ensemble', 'plot_forest_importances.py')
+# Permutation feature importance overcomes limitations of the impurity-based

('examples/ensemble', 'plot_forest_importances.py')
+# feature importance: they do not have a bias toward high-cardinality features

('examples/ensemble', 'plot_forest_importances.py')
+from sklearn.inspection import permutation_importance

('examples/ensemble', 'plot_forest_importances.py')
+result = permutation_importance(

('examples/ensemble', 'plot_forest_importances.py')
+print(f"Elapsed time to compute the importances: {elapsed_time:.3f} seconds")

('examples/ensemble', 'plot_forest_importances.py')
+forest_importances = pd.Series(result.importances_mean, index=feature_names)

('examples/ensemble', 'plot_forest_importances.py')
+# The computation for full permutation importance is more costly. Features are

('examples/ensemble', 'plot_forest_importances.py')
+# shuffled n times and the model refitted to estimate the importance of it.

('examples/ensemble', 'plot_forest_importances.py')
+# Please see :ref:`permutation_importance` for more details. We can now plot

('examples/ensemble', 'plot_forest_importances.py')
+# the importance ranking.

('examples/ensemble', 'plot_forest_importances.py')
+forest_importances.plot.bar(yerr=result.importances_std, ax=ax)

('examples/ensemble', 'plot_forest_importances.py')
+ax.set_title("Feature importances using permutation on full model")

('examples/ensemble', 'plot_forest_importances.py')
+# The same features are detected as most important using both methods. Although

('examples/ensemble', 'plot_forest_importances.py')
+# the relative importances vary. As seen on the plots, MDI is less likely than

('examples/ensemble', 'plot_forest_importances.py')
+# permutation importance to fully omit a feature.

('examples/ensemble', 'plot_adaboost_twoclass.py')
+from sklearn.inspection import DecisionBoundaryDisplay

('examples/ensemble', 'plot_ensemble_oob.py')
-import matplotlib.pyplot as plt

('examples/ensemble', 'plot_ensemble_oob.py')
-from collections import OrderedDict

('examples/ensemble', 'plot_ensemble_oob.py')
-from sklearn.datasets import make_classification

('examples/ensemble', 'plot_ensemble_oob.py')
-from sklearn.ensemble import RandomForestClassifier

('examples/ensemble', 'plot_ensemble_oob.py')
+import matplotlib.pyplot as plt

('examples/ensemble', 'plot_ensemble_oob.py')
+from collections import OrderedDict

('examples/ensemble', 'plot_ensemble_oob.py')
+from sklearn.datasets import make_classification

('examples/ensemble', 'plot_ensemble_oob.py')
+from sklearn.ensemble import RandomForestClassifier

('examples/ensemble', 'plot_forest_importances_faces.py')
-This example shows the use of forests of trees to evaluate the importance

('examples/ensemble', 'plot_forest_importances_faces.py')
-the more important.

('examples/ensemble', 'plot_forest_importances_faces.py')
+based importance of the pixels in an image classification task on the faces

('examples/ensemble', 'plot_forest_importances_faces.py')
+dataset. The hotter the pixel, the more important it is.

('examples/ensemble', 'plot_forest_importances_faces.py')
-from time import time

('examples/ensemble', 'plot_forest_importances_faces.py')
-import matplotlib.pyplot as plt

('examples/ensemble', 'plot_forest_importances_faces.py')
+# and evaluate the impurity-based feature importance. One drawback of this

('examples/ensemble', 'plot_forest_importances_faces.py')
+from sklearn.datasets import fetch_olivetti_faces

('examples/ensemble', 'plot_forest_importances_faces.py')
-from sklearn.datasets import fetch_olivetti_faces

('examples/ensemble', 'plot_forest_importances_faces.py')
-from sklearn.ensemble import ExtraTreesClassifier

('examples/ensemble', 'plot_forest_importances_faces.py')
-# Build a forest and compute the pixel importances

('examples/ensemble', 'plot_forest_importances_faces.py')
+# A random forest classifier will be fitted to compute the feature importances.

('examples/ensemble', 'plot_forest_importances_faces.py')
+from sklearn.ensemble import RandomForestClassifier

('examples/ensemble', 'plot_forest_importances_faces.py')
+# Feature importance based on mean decrease in impurity (MDI)

('examples/ensemble', 'plot_forest_importances_faces.py')
+# Feature importances are provided by the fitted attribute

('examples/ensemble', 'plot_forest_importances_faces.py')
+# `feature_importances_` and they are computed as the mean and standard

('examples/ensemble', 'plot_forest_importances_faces.py')
+#     Impurity-based feature importances can be misleading for **high

('examples/ensemble', 'plot_forest_importances_faces.py')
+#     :ref:`permutation_importance` as an alternative.

('examples/ensemble', 'plot_forest_importances_faces.py')
+import time

('examples/ensemble', 'plot_forest_importances_faces.py')
+import matplotlib.pyplot as plt

('examples/ensemble', 'plot_forest_importances_faces.py')
-importances = importances.reshape(data.images[0].shape)

('examples/ensemble', 'plot_forest_importances_faces.py')
-# Plot pixel importances

('examples/ensemble', 'plot_forest_importances_faces.py')
-plt.matshow(importances, cmap=plt.cm.hot)

('examples/ensemble', 'plot_forest_importances_faces.py')
-plt.title("Pixel importances with forests of trees")

('examples/ensemble', 'plot_forest_importances_faces.py')
+print(f"Elapsed time to compute the importances: {elapsed_time:.3f} seconds")

('examples/ensemble', 'plot_forest_importances_faces.py')
+imp_reshaped = importances.reshape(img_shape)

('examples/ensemble', 'plot_forest_importances_faces.py')
+plt.title("Pixel importances using impurity values")

('examples/ensemble', 'plot_forest_importances_faces.py')
+# the :func:`~sklearn.inspection.permutation_importance`.

('examples/ensemble', 'plot_gradient_boosting_regression.py')
+import matplotlib.pyplot as plt

('examples/ensemble', 'plot_gradient_boosting_regression.py')
-import matplotlib.pyplot as plt

('examples/ensemble', 'plot_gradient_boosting_regression.py')
+from sklearn import datasets, ensemble

('examples/ensemble', 'plot_gradient_boosting_regression.py')
+from sklearn.inspection import permutation_importance

('examples/ensemble', 'plot_gradient_boosting_regression.py')
+from sklearn.metrics import mean_squared_error

('examples/ensemble', 'plot_gradient_boosting_regression.py')
+from sklearn.model_selection import train_test_split

('examples/ensemble', 'plot_gradient_boosting_regression.py')
-from sklearn import ensemble

('examples/ensemble', 'plot_gradient_boosting_regression.py')
-from sklearn import datasets

('examples/ensemble', 'plot_gradient_boosting_regression.py')
-from sklearn.utils import shuffle

('examples/ensemble', 'plot_gradient_boosting_regression.py')
-from sklearn.metrics import mean_squared_error

('examples/ensemble', 'plot_gradient_boosting_regression.py')
+# Plot feature importance

('examples/ensemble', 'plot_gradient_boosting_regression.py')
+#    Careful, impurity-based feature importances can be misleading for

('examples/ensemble', 'plot_gradient_boosting_regression.py')
+#    the permutation importances of ``reg`` can be computed on a

('examples/ensemble', 'plot_gradient_boosting_regression.py')
+#    held out test set. See :ref:`permutation_importance` for more details.

('examples/ensemble', 'plot_gradient_boosting_regression.py')
+feature_importance = reg.feature_importances_

('examples/ensemble', 'plot_gradient_boosting_regression.py')
+sorted_idx = np.argsort(feature_importance)

('examples/ensemble', 'plot_gradient_boosting_regression.py')
+plt.barh(pos, feature_importance[sorted_idx], align="center")

('examples/ensemble', 'plot_gradient_boosting_regression.py')
-# Plot feature importance

('examples/ensemble', 'plot_gradient_boosting_regression.py')
-feature_importance = clf.feature_importances_

('examples/ensemble', 'plot_gradient_boosting_regression.py')
-# make importances relative to max importance

('examples/ensemble', 'plot_gradient_boosting_regression.py')
-feature_importance = 100.0 * (feature_importance / feature_importance.max())

('examples/ensemble', 'plot_gradient_boosting_regression.py')
-sorted_idx = np.argsort(feature_importance)

('examples/ensemble', 'plot_gradient_boosting_regression.py')
+result = permutation_importance(

('examples/ensemble', 'plot_gradient_boosting_regression.py')
+sorted_idx = result.importances_mean.argsort()

('examples/ensemble', 'plot_gradient_boosting_regression.py')
-plt.barh(pos, feature_importance[sorted_idx], align='center')

('examples/ensemble', 'plot_gradient_boosting_regression.py')
+    result.importances[sorted_idx].T,

('examples/ensemble', 'plot_feature_transformation.py')
-import numpy as np

('examples/ensemble', 'plot_feature_transformation.py')
+# It is important to split the data in such way to avoid overfitting by leaking

('examples/ensemble', 'plot_feature_transformation.py')
+from sklearn.datasets import make_classification

('examples/ensemble', 'plot_feature_transformation.py')
+from sklearn.model_selection import train_test_split

('examples/ensemble', 'plot_feature_transformation.py')
+from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

('examples/ensemble', 'plot_feature_transformation.py')
+from sklearn.ensemble import RandomTreesEmbedding

('examples/ensemble', 'plot_feature_transformation.py')
+from sklearn.linear_model import LogisticRegression

('examples/ensemble', 'plot_feature_transformation.py')
+from sklearn.pipeline import make_pipeline

('examples/ensemble', 'plot_feature_transformation.py')
+from sklearn.preprocessing import FunctionTransformer

('examples/ensemble', 'plot_feature_transformation.py')
+from sklearn.preprocessing import OneHotEncoder

('examples/ensemble', 'plot_feature_transformation.py')
+from sklearn.metrics import RocCurveDisplay

('examples/ensemble', 'plot_feature_transformation.py')
-from sklearn.datasets import make_classification

('examples/ensemble', 'plot_feature_transformation.py')
-from sklearn.linear_model import LogisticRegression

('examples/ensemble', 'plot_feature_transformation.py')
-from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,

('examples/ensemble', 'plot_feature_transformation.py')
-from sklearn.preprocessing import OneHotEncoder

('examples/ensemble', 'plot_feature_transformation.py')
-from sklearn.model_selection import train_test_split

('examples/ensemble', 'plot_feature_transformation.py')
-from sklearn.metrics import roc_curve

('examples/ensemble', 'plot_feature_transformation.py')
-from sklearn.pipeline import make_pipeline

('examples/ensemble', 'plot_feature_transformation.py')
-# It is important to train the ensemble of trees on a different subset

('examples/ensemble', 'plot_gradient_boosting_quantile.py')
-import matplotlib.pyplot as plt

('examples/ensemble', 'plot_gradient_boosting_quantile.py')
-from sklearn.ensemble import GradientBoostingRegressor

('examples/ensemble', 'plot_gradient_boosting_quantile.py')
+from sklearn.model_selection import train_test_split

('examples/ensemble', 'plot_gradient_boosting_quantile.py')
+from sklearn.ensemble import GradientBoostingRegressor

('examples/ensemble', 'plot_gradient_boosting_quantile.py')
+from sklearn.metrics import mean_pinball_loss, mean_squared_error

('examples/ensemble', 'plot_gradient_boosting_quantile.py')
+import matplotlib.pyplot as plt

('examples/ensemble', 'plot_gradient_boosting_quantile.py')
+import pandas as pd

('examples/ensemble', 'plot_gradient_boosting_quantile.py')
+from sklearn.experimental import enable_halving_search_cv  # noqa

('examples/ensemble', 'plot_gradient_boosting_quantile.py')
+from sklearn.model_selection import HalvingRandomSearchCV

('examples/ensemble', 'plot_gradient_boosting_quantile.py')
+from sklearn.metrics import make_scorer

('examples/ensemble', 'plot_gradient_boosting_quantile.py')
+from pprint import pprint

('examples/ensemble', 'plot_gradient_boosting_quantile.py')
+from sklearn.base import clone

('examples/ensemble', 'plot_forest_iris.py')
-from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,

('examples/ensemble', 'plot_forest_iris.py')
+from sklearn.ensemble import (

('examples/ensemble', 'plot_gradient_boosting_regularization.py')
+from sklearn.model_selection import train_test_split

('examples/ensemble', 'plot_voting_decision_regions.py')
-import numpy as np

('examples/ensemble', 'plot_voting_decision_regions.py')
+from sklearn.inspection import DecisionBoundaryDisplay

('examples/ensemble', 'plot_voting_regressor.py')
-from sklearn import datasets

('examples/ensemble', 'plot_voting_regressor.py')
+from sklearn.datasets import load_diabetes

('examples/cluster', 'plot_affinity_propagation.py')
-from sklearn.datasets.samples_generator import make_blobs

('examples/cluster', 'plot_affinity_propagation.py')
+from sklearn.datasets import make_blobs

('examples/cluster', 'plot_inductive_clustering.py')
-import numpy as np

('examples/cluster', 'plot_inductive_clustering.py')
-from sklearn.utils.metaestimators import if_delegate_has_method

('examples/cluster', 'plot_inductive_clustering.py')
+from sklearn.inspection import DecisionBoundaryDisplay

('examples/cluster', 'plot_inductive_clustering.py')
+from sklearn.utils.metaestimators import available_if

('examples/cluster', 'plot_inductive_clustering.py')
+from sklearn.utils.validation import check_is_fitted

('examples/cluster', 'plot_digits_linkage.py')
-from scipy import ndimage

('examples/cluster', 'plot_birch_vs_minibatchkmeans.py')
+from joblib import cpu_count

('examples/cluster', 'plot_birch_vs_minibatchkmeans.py')
-from sklearn.datasets.samples_generator import make_blobs

('examples/cluster', 'plot_birch_vs_minibatchkmeans.py')
+from sklearn.datasets import make_blobs

('examples/cluster', 'plot_mini_batch_kmeans.py')
-import time

('examples/cluster', 'plot_mini_batch_kmeans.py')
-import matplotlib.pyplot as plt

('examples/cluster', 'plot_mini_batch_kmeans.py')
+from sklearn.datasets import make_blobs

('examples/cluster', 'plot_mini_batch_kmeans.py')
-from sklearn.cluster import MiniBatchKMeans, KMeans

('examples/cluster', 'plot_mini_batch_kmeans.py')
-from sklearn.metrics.pairwise import pairwise_distances_argmin

('examples/cluster', 'plot_mini_batch_kmeans.py')
-from sklearn.datasets.samples_generator import make_blobs

('examples/cluster', 'plot_mini_batch_kmeans.py')
+import time

('examples/cluster', 'plot_mini_batch_kmeans.py')
+from sklearn.cluster import KMeans

('examples/cluster', 'plot_mini_batch_kmeans.py')
+from sklearn.cluster import MiniBatchKMeans

('examples/cluster', 'plot_mini_batch_kmeans.py')
+from sklearn.metrics.pairwise import pairwise_distances_argmin

('examples/cluster', 'plot_mini_batch_kmeans.py')
+import matplotlib.pyplot as plt

('examples/cluster', 'plot_mean_shift.py')
-from sklearn.datasets.samples_generator import make_blobs

('examples/cluster', 'plot_mean_shift.py')
+from sklearn.datasets import make_blobs

('examples/cluster', 'plot_dict_face_patches.py')
+from sklearn import datasets

('examples/cluster', 'plot_dict_face_patches.py')
-import matplotlib.pyplot as plt

('examples/cluster', 'plot_dict_face_patches.py')
-from sklearn import datasets

('examples/cluster', 'plot_dict_face_patches.py')
+import matplotlib.pyplot as plt

('examples/cluster', 'plot_coin_segmentation.py')
-from distutils.version import LooseVersion

('examples/cluster', 'plot_coin_segmentation.py')
-from scipy.ndimage.filters import gaussian_filter

('examples/cluster', 'plot_coin_segmentation.py')
+from scipy.ndimage import gaussian_filter

('examples/cluster', 'plot_coin_segmentation.py')
-import skimage

('examples/cluster', 'plot_ward_structured_vs_unstructured.py')
+import time as time

('examples/cluster', 'plot_ward_structured_vs_unstructured.py')
-import time as time

('examples/cluster', 'plot_ward_structured_vs_unstructured.py')
+import matplotlib.pyplot as plt

('examples/cluster', 'plot_ward_structured_vs_unstructured.py')
+# The following import is required

('examples/cluster', 'plot_ward_structured_vs_unstructured.py')
+import mpl_toolkits.mplot3d  # noqa: F401

('examples/cluster', 'plot_ward_structured_vs_unstructured.py')
-import matplotlib.pyplot as plt

('examples/cluster', 'plot_ward_structured_vs_unstructured.py')
-import mpl_toolkits.mplot3d.axes3d as p3

('examples/cluster', 'plot_ward_structured_vs_unstructured.py')
-from sklearn.datasets.samples_generator import make_swiss_roll

('examples/cluster', 'plot_ward_structured_vs_unstructured.py')
+from sklearn.datasets import make_swiss_roll

('examples/cluster', 'plot_cluster_iris.py')
-from mpl_toolkits.mplot3d import Axes3D

('examples/cluster', 'plot_cluster_iris.py')
+import mpl_toolkits.mplot3d  # noqa: F401

('examples/cluster', 'plot_kmeans_digits.py')
+import numpy as np

('examples/cluster', 'plot_kmeans_digits.py')
+from sklearn.datasets import load_digits

('examples/cluster', 'plot_kmeans_digits.py')
-import numpy as np

('examples/cluster', 'plot_kmeans_digits.py')
+from sklearn import metrics

('examples/cluster', 'plot_kmeans_digits.py')
+from sklearn.pipeline import make_pipeline

('examples/cluster', 'plot_kmeans_digits.py')
+from sklearn.preprocessing import StandardScaler

('examples/cluster', 'plot_kmeans_digits.py')
+from sklearn.cluster import KMeans

('examples/cluster', 'plot_kmeans_digits.py')
+from sklearn.decomposition import PCA

('examples/cluster', 'plot_kmeans_digits.py')
-from sklearn import metrics

('examples/cluster', 'plot_kmeans_digits.py')
-from sklearn.cluster import KMeans

('examples/cluster', 'plot_kmeans_digits.py')
-from sklearn.datasets import load_digits

('examples/cluster', 'plot_kmeans_digits.py')
-from sklearn.decomposition import PCA

('examples/cluster', 'plot_kmeans_digits.py')
-from sklearn.preprocessing import scale

('examples/cluster', 'plot_coin_ward_segmentation.py')
+from skimage.data import coins

('examples/cluster', 'plot_coin_ward_segmentation.py')
+import numpy as np

('examples/cluster', 'plot_coin_ward_segmentation.py')
+from scipy.ndimage import gaussian_filter

('examples/cluster', 'plot_coin_ward_segmentation.py')
+from skimage.transform import rescale

('examples/cluster', 'plot_coin_ward_segmentation.py')
+from sklearn.feature_extraction.image import grid_to_graph

('examples/cluster', 'plot_coin_ward_segmentation.py')
-import numpy as np

('examples/cluster', 'plot_coin_ward_segmentation.py')
-from distutils.version import LooseVersion

('examples/cluster', 'plot_coin_ward_segmentation.py')
-from scipy.ndimage.filters import gaussian_filter

('examples/cluster', 'plot_coin_ward_segmentation.py')
+from sklearn.cluster import AgglomerativeClustering

('examples/cluster', 'plot_coin_ward_segmentation.py')
-import skimage

('examples/cluster', 'plot_coin_ward_segmentation.py')
-from skimage.data import coins

('examples/cluster', 'plot_coin_ward_segmentation.py')
-from skimage.transform import rescale

('examples/cluster', 'plot_coin_ward_segmentation.py')
-from sklearn.feature_extraction.image import grid_to_graph

('examples/cluster', 'plot_coin_ward_segmentation.py')
-from sklearn.cluster import AgglomerativeClustering

('examples/cluster', 'plot_dbscan.py')
-from sklearn.datasets.samples_generator import make_blobs

('examples/cluster', 'plot_dbscan.py')
+from sklearn.datasets import make_blobs

('examples/semi_supervised', 'plot_label_propagation_structure.py')
-import matplotlib.pyplot as plt

('examples/semi_supervised', 'plot_label_propagation_structure.py')
-from sklearn.semi_supervised import label_propagation

('examples/semi_supervised', 'plot_label_propagation_structure.py')
+import matplotlib.pyplot as plt

('examples/semi_supervised', 'plot_label_propagation_structure.py')
+from sklearn.semi_supervised import LabelSpreading

('examples/semi_supervised', 'plot_label_propagation_digits_active_learning.py')
-from sklearn.semi_supervised import label_propagation

('examples/semi_supervised', 'plot_label_propagation_digits_active_learning.py')
+from sklearn.semi_supervised import LabelSpreading

('examples/semi_supervised', 'plot_label_propagation_digits.py')
+from sklearn import datasets

('examples/semi_supervised', 'plot_label_propagation_digits.py')
-import matplotlib.pyplot as plt

('examples/semi_supervised', 'plot_label_propagation_digits.py')
-from scipy import stats

('examples/semi_supervised', 'plot_label_propagation_digits.py')
-from sklearn import datasets

('examples/semi_supervised', 'plot_label_propagation_digits.py')
-from sklearn.semi_supervised import label_propagation

('examples/semi_supervised', 'plot_label_propagation_digits.py')
-from sklearn.metrics import confusion_matrix, classification_report

('examples/semi_supervised', 'plot_label_propagation_digits.py')
+from sklearn.semi_supervised import LabelSpreading

('examples/semi_supervised', 'plot_label_propagation_digits.py')
+from sklearn.metrics import classification_report

('examples/semi_supervised', 'plot_label_propagation_digits.py')
+from sklearn.metrics import ConfusionMatrixDisplay

('examples/semi_supervised', 'plot_label_propagation_digits.py')
+from scipy import stats

('examples/semi_supervised', 'plot_label_propagation_digits.py')
+import matplotlib.pyplot as plt

('examples/calibration', 'plot_calibration.py')
-import matplotlib.pyplot as plt

('examples/calibration', 'plot_calibration.py')
-from matplotlib import cm

('examples/calibration', 'plot_calibration.py')
-from sklearn.naive_bayes import GaussianNB

('examples/calibration', 'plot_calibration.py')
-from sklearn.metrics import brier_score_loss

('examples/calibration', 'plot_calibration.py')
-from sklearn.calibration import CalibratedClassifierCV

('examples/calibration', 'plot_calibration.py')
+from sklearn.calibration import CalibratedClassifierCV

('examples/calibration', 'plot_calibration.py')
+from sklearn.metrics import brier_score_loss

('examples/calibration', 'plot_calibration.py')
+from sklearn.naive_bayes import GaussianNB

('examples/calibration', 'plot_calibration.py')
+from matplotlib import cm

('examples/calibration', 'plot_calibration.py')
+import matplotlib.pyplot as plt

('examples/calibration', 'plot_compare_calibration.py')
+from sklearn.datasets import make_classification

('examples/calibration', 'plot_compare_calibration.py')
+from sklearn.model_selection import train_test_split

('examples/calibration', 'plot_compare_calibration.py')
+from sklearn.svm import LinearSVC

('examples/calibration', 'plot_compare_calibration.py')
+from sklearn.calibration import CalibrationDisplay

('examples/calibration', 'plot_compare_calibration.py')
+from sklearn.ensemble import RandomForestClassifier

('examples/calibration', 'plot_compare_calibration.py')
+from sklearn.linear_model import LogisticRegression

('examples/calibration', 'plot_compare_calibration.py')
+from sklearn.naive_bayes import GaussianNB

('examples/calibration', 'plot_compare_calibration.py')
-from sklearn import datasets

('examples/calibration', 'plot_compare_calibration.py')
-from sklearn.naive_bayes import GaussianNB

('examples/calibration', 'plot_compare_calibration.py')
-from sklearn.linear_model import LogisticRegression

('examples/calibration', 'plot_compare_calibration.py')
-from sklearn.ensemble import RandomForestClassifier

('examples/calibration', 'plot_compare_calibration.py')
-from sklearn.svm import LinearSVC

('examples/calibration', 'plot_compare_calibration.py')
-from sklearn.calibration import calibration_curve

('examples/calibration', 'plot_compare_calibration.py')
+from matplotlib.gridspec import GridSpec

('examples/calibration', 'plot_calibration_curve.py')
+from sklearn.datasets import make_classification

('examples/calibration', 'plot_calibration_curve.py')
+from sklearn.model_selection import train_test_split

('examples/calibration', 'plot_calibration_curve.py')
-from sklearn import datasets

('examples/calibration', 'plot_calibration_curve.py')
+from matplotlib.gridspec import GridSpec

('examples/calibration', 'plot_calibration_curve.py')
+from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay

('examples/calibration', 'plot_calibration_curve.py')
+from sklearn.linear_model import LogisticRegression

('examples/calibration', 'plot_calibration_curve.py')
+from collections import defaultdict

('examples/calibration', 'plot_calibration_curve.py')
+import pandas as pd

('examples/calibration', 'plot_calibration_curve.py')
+from sklearn.metrics import (

('examples/calibration', 'plot_calibration_curve.py')
+import numpy as np

('examples/calibration', 'plot_calibration_curve.py')
-from sklearn.linear_model import LogisticRegression

('examples/calibration', 'plot_calibration_curve.py')
-from sklearn.metrics import (brier_score_loss, precision_score, recall_score,

('examples/calibration', 'plot_calibration_curve.py')
-from sklearn.calibration import CalibratedClassifierCV, calibration_curve

('examples/calibration', 'plot_calibration_curve.py')
-from sklearn.model_selection import train_test_split

('examples/calibration', 'plot_calibration_multiclass.py')
-import matplotlib.pyplot as plt

('examples/calibration', 'plot_calibration_multiclass.py')
-from sklearn.calibration import CalibratedClassifierCV

('examples/calibration', 'plot_calibration_multiclass.py')
-from sklearn.metrics import log_loss

('examples/calibration', 'plot_calibration_multiclass.py')
+from sklearn.calibration import CalibratedClassifierCV

('examples/calibration', 'plot_calibration_multiclass.py')
+import matplotlib.pyplot as plt

('examples/calibration', 'plot_calibration_multiclass.py')
+from sklearn.metrics import log_loss

('examples/gaussian_process', 'plot_compare_gpr_krr.py')
+import numpy as np

('examples/gaussian_process', 'plot_compare_gpr_krr.py')
+import matplotlib.pyplot as plt

('examples/gaussian_process', 'plot_compare_gpr_krr.py')
+from sklearn.linear_model import Ridge

('examples/gaussian_process', 'plot_compare_gpr_krr.py')
-import numpy as np

('examples/gaussian_process', 'plot_compare_gpr_krr.py')
-import matplotlib.pyplot as plt

('examples/gaussian_process', 'plot_compare_gpr_krr.py')
+from sklearn.gaussian_process.kernels import ExpSineSquared

('examples/gaussian_process', 'plot_compare_gpr_krr.py')
-from sklearn.model_selection import GridSearchCV

('examples/gaussian_process', 'plot_compare_gpr_krr.py')
+from sklearn.model_selection import RandomizedSearchCV

('examples/gaussian_process', 'plot_compare_gpr_krr.py')
+from sklearn.utils.fixes import loguniform

('examples/gaussian_process', 'plot_compare_gpr_krr.py')
-from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared

('examples/gaussian_process', 'plot_compare_gpr_krr.py')
+from sklearn.gaussian_process.kernels import WhiteKernel

('examples/gaussian_process', 'plot_compare_gpr_krr.py')
+from sklearn.gaussian_process.kernels import RBF

('examples/gaussian_process', 'plot_gpr_prior_posterior.py')
+import matplotlib.pyplot as plt

('examples/gaussian_process', 'plot_gpr_prior_posterior.py')
-from matplotlib import pyplot as plt

('examples/gaussian_process', 'plot_gpr_prior_posterior.py')
-from sklearn.gaussian_process.kernels import (RBF, Matern, RationalQuadratic,

('examples/gaussian_process', 'plot_gpr_prior_posterior.py')
+from sklearn.gaussian_process.kernels import RBF

('examples/gaussian_process', 'plot_gpr_prior_posterior.py')
+from sklearn.gaussian_process.kernels import RationalQuadratic

('examples/gaussian_process', 'plot_gpr_prior_posterior.py')
+from sklearn.gaussian_process.kernels import ExpSineSquared

('examples/gaussian_process', 'plot_gpr_prior_posterior.py')
+from sklearn.gaussian_process.kernels import ConstantKernel, DotProduct

('examples/gaussian_process', 'plot_gpr_prior_posterior.py')
+from sklearn.gaussian_process.kernels import Matern

('examples/gaussian_process', 'plot_gpr_noisy.py')
-high-noise solution. It is thus important to repeat the optimization several

('examples/gaussian_process', 'plot_gpr_noisy.py')
+level in the data. Moreover, we show the importance of kernel hyperparameters

('examples/gaussian_process', 'plot_gpr_noisy.py')
-from matplotlib import pyplot as plt

('examples/gaussian_process', 'plot_gpr_noisy.py')
-from matplotlib.colors import LogNorm

('examples/gaussian_process', 'plot_gpr_noisy.py')
+import matplotlib.pyplot as plt

('examples/gaussian_process', 'plot_gpr_noisy.py')
+# minima. It will highlights the importance of initial hyperparameter values.

('examples/gaussian_process', 'plot_gpr_noisy.py')
+from matplotlib.colors import LogNorm

('examples/gaussian_process', 'plot_gpr_noisy.py')
+# not to the best model. It is thus important to repeat the optimization

('examples/gaussian_process', 'plot_gpc.py')
-from sklearn.metrics.classification import accuracy_score, log_loss

('examples/gaussian_process', 'plot_gpc.py')
+from sklearn.metrics import accuracy_score, log_loss

('examples/gaussian_process', 'plot_gpr_co2.py')
+from sklearn.datasets import fetch_openml

('examples/gaussian_process', 'plot_gpr_co2.py')
+import pandas as pd

('examples/gaussian_process', 'plot_gpr_co2.py')
+import matplotlib.pyplot as plt

('examples/gaussian_process', 'plot_gpr_co2.py')
+from sklearn.gaussian_process.kernels import RBF

('examples/gaussian_process', 'plot_gpr_co2.py')
+from sklearn.gaussian_process.kernels import ExpSineSquared

('examples/gaussian_process', 'plot_gpr_co2.py')
+from sklearn.gaussian_process.kernels import RationalQuadratic

('examples/gaussian_process', 'plot_gpr_co2.py')
+from sklearn.gaussian_process.kernels import WhiteKernel

('examples/gaussian_process', 'plot_gpr_co2.py')
+from sklearn.gaussian_process import GaussianProcessRegressor

('examples/gaussian_process', 'plot_gpr_co2.py')
+import datetime

('examples/gaussian_process', 'plot_gpr_co2.py')
-from matplotlib import pyplot as plt

('examples/gaussian_process', 'plot_gpr_co2.py')
-from sklearn.datasets import fetch_openml

('examples/gaussian_process', 'plot_gpr_co2.py')
-from sklearn.gaussian_process import GaussianProcessRegressor

('examples/gaussian_process', 'plot_gpr_co2.py')
-    import RBF, WhiteKernel, RationalQuadratic, ExpSineSquared

('examples/gaussian_process', 'plot_gpr_noisy_targets.py')
-from matplotlib import pyplot as plt

('examples/gaussian_process', 'plot_gpr_noisy_targets.py')
+import matplotlib.pyplot as plt

('examples/gaussian_process', 'plot_gpr_noisy_targets.py')
-from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C

('examples/gaussian_process', 'plot_gpr_noisy_targets.py')
+from sklearn.gaussian_process.kernels import RBF

('examples/compose', 'plot_digits_pipe.py')
-from sklearn.linear_model import SGDClassifier

('examples/compose', 'plot_digits_pipe.py')
+from sklearn.linear_model import LogisticRegression

('examples/compose', 'plot_digits_pipe.py')
+from sklearn.preprocessing import StandardScaler

('examples/compose', 'plot_column_transformer.py')
-from sklearn.base import BaseEstimator, TransformerMixin

('examples/compose', 'plot_column_transformer.py')
+from sklearn.preprocessing import FunctionTransformer

('examples/compose', 'plot_column_transformer.py')
-from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer

('examples/compose', 'plot_column_transformer.py')
-from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting

('examples/compose', 'plot_transformed_target.py')
-import matplotlib

('examples/compose', 'plot_transformed_target.py')
-from distutils.version import LooseVersion

('examples/compose', 'plot_transformed_target.py')
-from sklearn.datasets import load_boston

('examples/compose', 'plot_transformed_target.py')
+from sklearn.datasets import fetch_openml

('examples/compose', 'plot_column_transformer_mixed_types.py')
-import pandas as pd

('examples/compose', 'plot_column_transformer_mixed_types.py')
+from sklearn.datasets import fetch_openml

('examples/compose', 'plot_column_transformer_mixed_types.py')
+from sklearn.compose import make_column_selector as selector

('examples/compose', 'plot_column_transformer_mixed_types.py')
+import pandas as pd

('examples/compose', 'plot_compare_reduction.py')
-from tempfile import mkdtemp

('examples/compose', 'plot_compare_reduction.py')
+from joblib import Memory

('examples/compose', 'plot_compare_reduction.py')
-from joblib import Memory

('examples/datasets', 'plot_iris_dataset.py')
-from mpl_toolkits.mplot3d import Axes3D

('examples/datasets', 'plot_iris_dataset.py')
+# unused but required import for doing 3d projections with matplotlib < 3.2

('examples/datasets', 'plot_iris_dataset.py')
+import mpl_toolkits.mplot3d  # noqa: F401

('examples/linear_model', 'plot_sgd_early_stopping.py')
-from sklearn.utils.testing import ignore_warnings

('examples/linear_model', 'plot_sgd_early_stopping.py')
+from sklearn.utils._testing import ignore_warnings

('examples/linear_model', 'plot_sgd_separating_hyperplane.py')
-from sklearn.datasets.samples_generator import make_blobs

('examples/linear_model', 'plot_sgd_separating_hyperplane.py')
+from sklearn.datasets import make_blobs

('examples/linear_model', 'plot_polynomial_interpolation.py')
-from sklearn.preprocessing import PolynomialFeatures

('examples/linear_model', 'plot_polynomial_interpolation.py')
+from sklearn.preprocessing import PolynomialFeatures, SplineTransformer

('examples/linear_model', 'plot_logistic_path.py')
-from time import time

('examples/linear_model', 'plot_logistic_path.py')
-import numpy as np

('examples/linear_model', 'plot_logistic_path.py')
-import matplotlib.pyplot as plt

('examples/linear_model', 'plot_logistic_path.py')
-from sklearn import linear_model

('examples/linear_model', 'plot_logistic_path.py')
-from sklearn.svm import l1_min_c

('examples/linear_model', 'plot_logistic_path.py')
+import numpy as np

('examples/linear_model', 'plot_logistic_path.py')
+from sklearn import linear_model

('examples/linear_model', 'plot_logistic_path.py')
+from sklearn.svm import l1_min_c

('examples/linear_model', 'plot_logistic_path.py')
+import matplotlib.pyplot as plt

('examples/linear_model', 'plot_ard.py')
+from sklearn.datasets import make_regression

('examples/linear_model', 'plot_ard.py')
+import pandas as pd

('examples/linear_model', 'plot_ard.py')
+from sklearn.linear_model import ARDRegression, LinearRegression, BayesianRidge

('examples/linear_model', 'plot_ard.py')
+import matplotlib.pyplot as plt

('examples/linear_model', 'plot_ard.py')
+import seaborn as sns

('examples/linear_model', 'plot_ard.py')
+from matplotlib.colors import SymLogNorm

('examples/linear_model', 'plot_ard.py')
-import matplotlib.pyplot as plt

('examples/linear_model', 'plot_ard.py')
-from scipy import stats

('examples/linear_model', 'plot_ard.py')
-from sklearn.linear_model import ARDRegression, LinearRegression

('examples/linear_model', 'plot_ard.py')
+from sklearn.pipeline import make_pipeline

('examples/linear_model', 'plot_ard.py')
+from sklearn.preprocessing import PolynomialFeatures, StandardScaler

('examples/linear_model', 'plot_lasso_model_selection.py')
+from sklearn.datasets import load_diabetes

('examples/linear_model', 'plot_lasso_model_selection.py')
+import numpy as np

('examples/linear_model', 'plot_lasso_model_selection.py')
+import pandas as pd

('examples/linear_model', 'plot_lasso_model_selection.py')
-import numpy as np

('examples/linear_model', 'plot_lasso_model_selection.py')
+from sklearn.preprocessing import StandardScaler

('examples/linear_model', 'plot_lasso_model_selection.py')
+from sklearn.linear_model import LassoLarsIC

('examples/linear_model', 'plot_lasso_model_selection.py')
+from sklearn.pipeline import make_pipeline

('examples/linear_model', 'plot_lasso_model_selection.py')
+from sklearn.linear_model import LassoCV

('examples/linear_model', 'plot_lasso_model_selection.py')
-from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC

('examples/linear_model', 'plot_lasso_model_selection.py')
-from sklearn import datasets

('examples/linear_model', 'plot_lasso_model_selection.py')
+from sklearn.linear_model import LassoLarsCV

('examples/linear_model', 'plot_sgd_iris.py')
+from sklearn.inspection import DecisionBoundaryDisplay

('examples/linear_model', 'plot_ols_3d.py')
-import matplotlib.pyplot as plt

('examples/linear_model', 'plot_ols_3d.py')
+from sklearn import datasets

('examples/linear_model', 'plot_ols_3d.py')
-from mpl_toolkits.mplot3d import Axes3D

('examples/linear_model', 'plot_ols_3d.py')
-from sklearn import datasets, linear_model

('examples/linear_model', 'plot_ols_3d.py')
+from sklearn import linear_model

('examples/linear_model', 'plot_ols_3d.py')
+import matplotlib.pyplot as plt

('examples/linear_model', 'plot_logistic.py')
-from sklearn import linear_model

('examples/linear_model', 'plot_logistic.py')
+from sklearn.linear_model import LogisticRegression, LinearRegression

('examples/linear_model', 'plot_lasso_dense_vs_sparse_data.py')
-from sklearn.datasets.samples_generator import make_regression

('examples/linear_model', 'plot_lasso_dense_vs_sparse_data.py')
+from sklearn.datasets import make_regression

('examples/linear_model', 'plot_logistic_multinomial.py')
+from sklearn.inspection import DecisionBoundaryDisplay

('examples/linear_model', 'plot_iris_logistic.py')
-import numpy as np

('examples/linear_model', 'plot_iris_logistic.py')
+from sklearn.inspection import DecisionBoundaryDisplay

('examples/impute', 'plot_missing_values.py')
-import matplotlib.pyplot as plt

('examples/impute', 'plot_missing_values.py')
-from sklearn.experimental import enable_iterative_imputer  # noqa

('examples/impute', 'plot_missing_values.py')
+from sklearn.datasets import fetch_california_housing

('examples/impute', 'plot_missing_values.py')
-from sklearn.datasets import load_boston

('examples/impute', 'plot_missing_values.py')
-from sklearn.ensemble import RandomForestRegressor

('examples/impute', 'plot_missing_values.py')
-from sklearn.pipeline import make_pipeline, make_union

('examples/impute', 'plot_missing_values.py')
-from sklearn.impute import SimpleImputer, IterativeImputer, MissingIndicator

('examples/impute', 'plot_missing_values.py')
-from sklearn.model_selection import cross_val_score

('examples/impute', 'plot_missing_values.py')
+from sklearn.ensemble import RandomForestRegressor

('examples/impute', 'plot_missing_values.py')
+from sklearn.experimental import enable_iterative_imputer  # noqa

('examples/impute', 'plot_missing_values.py')
+from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer

('examples/impute', 'plot_missing_values.py')
+from sklearn.model_selection import cross_val_score

('examples/impute', 'plot_missing_values.py')
+from sklearn.pipeline import make_pipeline

('examples/impute', 'plot_missing_values.py')
+import matplotlib.pyplot as plt

('examples/impute', 'plot_iterative_imputer_variants_comparison.py')
-from sklearn.linear_model import BayesianRidge

('examples/impute', 'plot_iterative_imputer_variants_comparison.py')
-from sklearn.tree import DecisionTreeRegressor

('examples/impute', 'plot_iterative_imputer_variants_comparison.py')
-from sklearn.ensemble import ExtraTreesRegressor

('examples/impute', 'plot_iterative_imputer_variants_comparison.py')
+from sklearn.linear_model import BayesianRidge, Ridge

('examples/impute', 'plot_iterative_imputer_variants_comparison.py')
+from sklearn.kernel_approximation import Nystroem

('examples/impute', 'plot_iterative_imputer_variants_comparison.py')
+from sklearn.ensemble import RandomForestRegressor

('examples/covariance', 'plot_mahalanobis_distances.py')
-import matplotlib.pyplot as plt

('examples/covariance', 'plot_mahalanobis_distances.py')
-from sklearn.covariance import EmpiricalCovariance, MinCovDet

('examples/covariance', 'plot_mahalanobis_distances.py')
+import matplotlib.pyplot as plt

('examples/covariance', 'plot_mahalanobis_distances.py')
+from sklearn.covariance import EmpiricalCovariance, MinCovDet

('examples/covariance', 'plot_covariance_estimation.py')
-from sklearn.covariance import LedoitWolf, OAS, ShrunkCovariance, \

('examples/covariance', 'plot_covariance_estimation.py')
+from sklearn.covariance import (

('examples/covariance', 'plot_sparse_cov.py')
-from sklearn.covariance import GraphicalLassoCV, ledoit_wolf

('examples/covariance', 'plot_sparse_cov.py')
-import matplotlib.pyplot as plt

('examples/covariance', 'plot_sparse_cov.py')
+from sklearn.covariance import GraphicalLassoCV, ledoit_wolf

('examples/covariance', 'plot_sparse_cov.py')
+import matplotlib.pyplot as plt

('examples/feature_selection', 'plot_feature_selection.py')
+from sklearn.datasets import load_iris

('examples/feature_selection', 'plot_feature_selection.py')
+from sklearn.model_selection import train_test_split

('examples/feature_selection', 'plot_feature_selection.py')
+from sklearn.feature_selection import SelectKBest, f_classif

('examples/feature_selection', 'plot_feature_selection.py')
-from sklearn import datasets, svm

('examples/feature_selection', 'plot_feature_selection.py')
-from sklearn.feature_selection import SelectPercentile, f_classif

('examples/feature_selection', 'plot_feature_selection.py')
+from sklearn.pipeline import make_pipeline

('examples/feature_selection', 'plot_feature_selection.py')
+from sklearn.preprocessing import MinMaxScaler

('examples/feature_selection', 'plot_feature_selection.py')
+from sklearn.svm import LinearSVC

('examples/feature_selection', 'plot_feature_selection_pipeline.py')
-from sklearn import svm

('examples/feature_selection', 'plot_feature_selection_pipeline.py')
-from sklearn.datasets import samples_generator

('examples/feature_selection', 'plot_feature_selection_pipeline.py')
-from sklearn.feature_selection import SelectKBest, f_regression

('examples/feature_selection', 'plot_feature_selection_pipeline.py')
+from sklearn.datasets import make_classification

('examples/feature_selection', 'plot_feature_selection_pipeline.py')
+from sklearn.model_selection import train_test_split

('examples/feature_selection', 'plot_feature_selection_pipeline.py')
+from sklearn.feature_selection import SelectKBest, f_classif

('examples/feature_selection', 'plot_feature_selection_pipeline.py')
-from sklearn.model_selection import train_test_split

('examples/feature_selection', 'plot_feature_selection_pipeline.py')
+from sklearn.svm import LinearSVC

('examples/feature_selection', 'plot_feature_selection_pipeline.py')
-# import some data to play with

('examples/inspection', 'plot_partial_dependence.py')
-important features.

('examples/inspection', 'plot_partial_dependence.py')
+thus they are usually chosen among the most important features.

('examples/inspection', 'plot_partial_dependence.py')
+import pandas as pd

('examples/inspection', 'plot_partial_dependence.py')
+from sklearn.datasets import fetch_california_housing

('examples/inspection', 'plot_partial_dependence.py')
+from sklearn.model_selection import train_test_split

('examples/inspection', 'plot_partial_dependence.py')
+from time import time

('examples/inspection', 'plot_partial_dependence.py')
+from sklearn.pipeline import make_pipeline

('examples/inspection', 'plot_partial_dependence.py')
+from sklearn.preprocessing import QuantileTransformer

('examples/inspection', 'plot_partial_dependence.py')
+from sklearn.neural_network import MLPRegressor

('examples/inspection', 'plot_partial_dependence.py')
+# Note that it is important to check that the model is accurate enough on a

('examples/inspection', 'plot_partial_dependence.py')
+from sklearn.inspection import PartialDependenceDisplay

('examples/inspection', 'plot_partial_dependence.py')
+from sklearn.ensemble import HistGradientBoostingRegressor

('examples/inspection', 'plot_partial_dependence.py')
+import matplotlib.pyplot as plt

('examples/inspection', 'plot_partial_dependence.py')
-import matplotlib.pyplot as plt

('examples/inspection', 'plot_partial_dependence.py')
-from mpl_toolkits.mplot3d import Axes3D

('examples/inspection', 'plot_partial_dependence.py')
+# unused but required import for doing 3d projections with matplotlib < 3.2

('examples/inspection', 'plot_partial_dependence.py')
+import mpl_toolkits.mplot3d  # noqa: F401

('examples/inspection', 'plot_partial_dependence.py')
-from sklearn.inspection import plot_partial_dependence

('examples/inspection', 'plot_partial_dependence.py')
-from sklearn.ensemble import GradientBoostingRegressor

('examples/inspection', 'plot_partial_dependence.py')
-from sklearn.neural_network import MLPRegressor

('examples/inspection', 'plot_partial_dependence.py')
-from sklearn.datasets.california_housing import fetch_california_housing

('examples/svm', 'plot_separating_hyperplane.py')
-import numpy as np

('examples/svm', 'plot_separating_hyperplane.py')
+from sklearn.inspection import DecisionBoundaryDisplay

('examples/svm', 'plot_custom_kernel.py')
+from sklearn.inspection import DecisionBoundaryDisplay

('examples/svm', 'plot_svm_margin.py')
+from matplotlib import cm

('examples/svm', 'plot_iris_svc.py')
-import numpy as np

('examples/svm', 'plot_iris_svc.py')
+from sklearn.inspection import DecisionBoundaryDisplay

('examples/svm', 'plot_svm_anova.py')
-import matplotlib.pyplot as plt

('examples/svm', 'plot_svm_anova.py')
+from sklearn.pipeline import Pipeline

('examples/svm', 'plot_svm_anova.py')
-from sklearn.model_selection import cross_val_score

('examples/svm', 'plot_svm_anova.py')
-from sklearn.pipeline import Pipeline

('examples/svm', 'plot_svm_anova.py')
+import matplotlib.pyplot as plt

('examples/svm', 'plot_svm_anova.py')
+from sklearn.model_selection import cross_val_score

('examples/svm', 'plot_rbf_parameters.py')
-import matplotlib.pyplot as plt

('examples/svm', 'plot_rbf_parameters.py')
-from sklearn.svm import SVC

('examples/svm', 'plot_rbf_parameters.py')
-from sklearn.preprocessing import StandardScaler

('examples/svm', 'plot_rbf_parameters.py')
-from sklearn.datasets import load_iris

('examples/svm', 'plot_rbf_parameters.py')
-from sklearn.model_selection import StratifiedShuffleSplit

('examples/svm', 'plot_rbf_parameters.py')
-from sklearn.model_selection import GridSearchCV

('examples/svm', 'plot_rbf_parameters.py')
+from sklearn.datasets import load_iris

('examples/svm', 'plot_rbf_parameters.py')
+from sklearn.preprocessing import StandardScaler

('examples/svm', 'plot_rbf_parameters.py')
+from sklearn.svm import SVC

('examples/svm', 'plot_rbf_parameters.py')
+from sklearn.model_selection import StratifiedShuffleSplit

('examples/svm', 'plot_rbf_parameters.py')
+from sklearn.model_selection import GridSearchCV

('examples/svm', 'plot_rbf_parameters.py')
+import matplotlib.pyplot as plt

('examples/svm', 'plot_separating_hyperplane_unbalanced.py')
-import numpy as np

('examples/svm', 'plot_separating_hyperplane_unbalanced.py')
+from sklearn.inspection import DecisionBoundaryDisplay

('examples/manifold', 'plot_compare_methods.py')
-from time import time

('examples/manifold', 'plot_compare_methods.py')
+from numpy.random import RandomState

('examples/manifold', 'plot_compare_methods.py')
-from mpl_toolkits.mplot3d import Axes3D

('examples/manifold', 'plot_compare_methods.py')
-from matplotlib.ticker import NullFormatter

('examples/manifold', 'plot_compare_methods.py')
+from matplotlib import ticker

('examples/manifold', 'plot_compare_methods.py')
+# unused but required import for doing 3d projections with matplotlib < 3.2

('examples/manifold', 'plot_compare_methods.py')
+import mpl_toolkits.mplot3d  # noqa: F401

('examples/manifold', 'plot_compare_methods.py')
-# Next line to silence pyflakes. This import is needed.

('examples/manifold', 'plot_lle_digits.py')
-from time import time

('examples/manifold', 'plot_lle_digits.py')
-import numpy as np

('examples/manifold', 'plot_lle_digits.py')
-import matplotlib.pyplot as plt

('examples/manifold', 'plot_lle_digits.py')
-from matplotlib import offsetbox

('examples/manifold', 'plot_lle_digits.py')
-from sklearn import (manifold, datasets, decomposition, ensemble,

('examples/manifold', 'plot_lle_digits.py')
+from sklearn.datasets import load_digits

('examples/manifold', 'plot_lle_digits.py')
+import matplotlib.pyplot as plt

('examples/manifold', 'plot_lle_digits.py')
+import numpy as np

('examples/manifold', 'plot_lle_digits.py')
+from matplotlib import offsetbox

('examples/manifold', 'plot_lle_digits.py')
+from sklearn.preprocessing import MinMaxScaler

('examples/manifold', 'plot_lle_digits.py')
+from sklearn.decomposition import TruncatedSVD

('examples/manifold', 'plot_lle_digits.py')
+from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

('examples/manifold', 'plot_lle_digits.py')
+from sklearn.ensemble import RandomTreesEmbedding

('examples/manifold', 'plot_lle_digits.py')
+from sklearn.manifold import (

('examples/manifold', 'plot_lle_digits.py')
+from sklearn.neighbors import NeighborhoodComponentsAnalysis

('examples/manifold', 'plot_lle_digits.py')
+from sklearn.pipeline import make_pipeline

('examples/manifold', 'plot_lle_digits.py')
+from sklearn.random_projection import SparseRandomProjection

('examples/manifold', 'plot_lle_digits.py')
+from time import time

('examples/manifold', 'plot_manifold_sphere.py')
-from mpl_toolkits.mplot3d import Axes3D

('examples/manifold', 'plot_manifold_sphere.py')
+# Unused but required import for doing 3d projections with matplotlib < 3.2

('examples/manifold', 'plot_manifold_sphere.py')
+import mpl_toolkits.mplot3d  # noqa: F401

('examples/manifold', 'plot_manifold_sphere.py')
+import warnings

('examples/manifold', 'plot_swissroll.py')
+from sklearn import manifold, datasets

('examples/manifold', 'plot_swissroll.py')
-# This import is needed to modify the way figure behaves

('examples/manifold', 'plot_swissroll.py')
-from mpl_toolkits.mplot3d import Axes3D

('examples/manifold', 'plot_swissroll.py')
-from sklearn import manifold, datasets

('examples/exercises', 'plot_cv_diabetes.py')
+import matplotlib.pyplot as plt

('examples/exercises', 'plot_cv_diabetes.py')
-import matplotlib.pyplot as plt

('examples/exercises', 'plot_cv_diabetes.py')
-from sklearn.linear_model import LassoCV

('examples/exercises', 'plot_cv_diabetes.py')
-from sklearn.model_selection import KFold

('examples/exercises', 'plot_cv_diabetes.py')
+from sklearn.linear_model import LassoCV

('examples/exercises', 'plot_cv_diabetes.py')
+from sklearn.model_selection import KFold

('examples/applications', 'plot_face_recognition.py')
-import logging

('examples/applications', 'plot_face_recognition.py')
-from sklearn.model_selection import GridSearchCV

('examples/applications', 'plot_face_recognition.py')
+from sklearn.model_selection import RandomizedSearchCV

('examples/applications', 'plot_face_recognition.py')
-from sklearn.metrics import confusion_matrix

('examples/applications', 'plot_face_recognition.py')
+from sklearn.metrics import ConfusionMatrixDisplay

('examples/applications', 'plot_face_recognition.py')
+from sklearn.preprocessing import StandardScaler

('examples/applications', 'plot_face_recognition.py')
+from sklearn.utils.fixes import loguniform

('examples/applications', 'plot_stock_market.py')
-import sys

('examples/applications', 'plot_stock_market.py')
-import numpy as np

('examples/applications', 'plot_stock_market.py')
-import matplotlib.pyplot as plt

('examples/applications', 'plot_stock_market.py')
-from matplotlib.collections import LineCollection

('examples/applications', 'plot_stock_market.py')
-import pandas as pd

('examples/applications', 'plot_stock_market.py')
-from sklearn import cluster, covariance, manifold

('examples/applications', 'plot_stock_market.py')
+import sys

('examples/applications', 'plot_stock_market.py')
+import numpy as np

('examples/applications', 'plot_stock_market.py')
+import pandas as pd

('examples/applications', 'plot_stock_market.py')
+from sklearn import covariance

('examples/applications', 'plot_stock_market.py')
+from sklearn import cluster

('examples/applications', 'plot_stock_market.py')
+from sklearn import manifold

('examples/applications', 'plot_stock_market.py')
+import matplotlib.pyplot as plt

('examples/applications', 'plot_stock_market.py')
+from matplotlib.collections import LineCollection

('examples/applications', 'plot_prediction_latency.py')
-from sklearn.datasets.samples_generator import make_regression

('examples/applications', 'plot_prediction_latency.py')
-from sklearn.ensemble.forest import RandomForestRegressor

('examples/applications', 'plot_prediction_latency.py')
-from sklearn.linear_model.ridge import Ridge

('examples/applications', 'plot_prediction_latency.py')
-from sklearn.linear_model.stochastic_gradient import SGDRegressor

('examples/applications', 'plot_prediction_latency.py')
-from sklearn.svm.classes import SVR

('examples/applications', 'plot_prediction_latency.py')
+from sklearn.datasets import make_regression

('examples/applications', 'plot_prediction_latency.py')
+from sklearn.ensemble import RandomForestRegressor

('examples/applications', 'plot_prediction_latency.py')
+from sklearn.linear_model import Ridge

('examples/applications', 'plot_prediction_latency.py')
+from sklearn.linear_model import SGDRegressor

('examples/applications', 'plot_prediction_latency.py')
+from sklearn.svm import SVR

('examples/applications', 'plot_species_distribution_modeling.py')
-from sklearn.datasets.base import Bunch

('examples/applications', 'plot_species_distribution_modeling.py')
+from sklearn.utils import Bunch

('examples/applications', 'plot_species_distribution_modeling.py')
-from sklearn.datasets.species_distributions import construct_grids

('examples/applications', 'wikipedia_principal_eigenvector.py')
-from joblib import Memory

('examples/applications', 'plot_model_complexity_influence.py')
-from mpl_toolkits.axes_grid1.parasite_axes import host_subplot

('examples/applications', 'plot_model_complexity_influence.py')
-from mpl_toolkits.axisartist.axislines import Axes

('examples/applications', 'plot_model_complexity_influence.py')
-from scipy.sparse.csr import csr_matrix

('examples/applications', 'plot_model_complexity_influence.py')
-from sklearn.utils import shuffle

('examples/applications', 'plot_model_complexity_influence.py')
+from sklearn.model_selection import train_test_split

('examples/applications', 'plot_model_complexity_influence.py')
-from sklearn.svm.classes import NuSVR

('examples/applications', 'plot_model_complexity_influence.py')
-from sklearn.ensemble.gradient_boosting import GradientBoostingRegressor

('examples/applications', 'plot_model_complexity_influence.py')
-from sklearn.linear_model.stochastic_gradient import SGDClassifier

('examples/applications', 'plot_model_complexity_influence.py')
+from sklearn.svm import NuSVR

('examples/applications', 'plot_model_complexity_influence.py')
+from sklearn.ensemble import GradientBoostingRegressor

('examples/applications', 'plot_model_complexity_influence.py')
+from sklearn.linear_model import SGDClassifier

('examples/applications', 'svm_gui.py')
-from matplotlib.backends.backend_tkagg import NavigationToolbar2TkAgg

('examples/applications', 'svm_gui.py')
+    from matplotlib.backends.backend_tkagg import NavigationToolbar2Tk

('examples/applications', 'svm_gui.py')
+    from matplotlib.backends.backend_tkagg import (

('examples/applications', 'plot_topics_extraction_with_nmf_lda.py')
+import matplotlib.pyplot as plt

('examples/applications', 'plot_topics_extraction_with_nmf_lda.py')
-from sklearn.decomposition import NMF, LatentDirichletAllocation

('examples/applications', 'plot_topics_extraction_with_nmf_lda.py')
+from sklearn.decomposition import NMF, MiniBatchNMF, LatentDirichletAllocation

('examples/neural_networks', 'plot_mlp_training_curves.py')
+import warnings

('examples/neural_networks', 'plot_mlp_training_curves.py')
+from sklearn.exceptions import ConvergenceWarning

('examples/neural_networks', 'plot_rbm_logistic_classification.py')
-import matplotlib.pyplot as plt

('examples/neural_networks', 'plot_rbm_logistic_classification.py')
-from sklearn import linear_model, datasets, metrics

('examples/neural_networks', 'plot_rbm_logistic_classification.py')
+from sklearn import datasets

('examples/neural_networks', 'plot_rbm_logistic_classification.py')
+from sklearn.preprocessing import minmax_scale

('examples/neural_networks', 'plot_rbm_logistic_classification.py')
-from sklearn.neural_network import BernoulliRBM

('examples/neural_networks', 'plot_rbm_logistic_classification.py')
-from sklearn.pipeline import Pipeline

('examples/neural_networks', 'plot_rbm_logistic_classification.py')
-from sklearn.base import clone

('examples/neural_networks', 'plot_rbm_logistic_classification.py')
+from sklearn import linear_model

('examples/neural_networks', 'plot_rbm_logistic_classification.py')
+from sklearn.neural_network import BernoulliRBM

('examples/neural_networks', 'plot_rbm_logistic_classification.py')
+from sklearn.pipeline import Pipeline

('examples/neural_networks', 'plot_rbm_logistic_classification.py')
+from sklearn.base import clone

('examples/neural_networks', 'plot_rbm_logistic_classification.py')
+from sklearn import metrics

('examples/neural_networks', 'plot_rbm_logistic_classification.py')
+import matplotlib.pyplot as plt

('examples/neural_networks', 'plot_mnist_filters.py')
+import warnings

('examples/neural_networks', 'plot_mnist_filters.py')
+from sklearn.exceptions import ConvergenceWarning

('examples/neural_networks', 'plot_mnist_filters.py')
+from sklearn.model_selection import train_test_split

('examples/neural_networks', 'plot_mlp_alpha.py')
+from sklearn.pipeline import make_pipeline

('examples/preprocessing', 'plot_discretization_classification.py')
-from sklearn.utils.testing import ignore_warnings

('examples/preprocessing', 'plot_discretization_classification.py')
+from sklearn.utils._testing import ignore_warnings

('examples/preprocessing', 'plot_scaling_importance.py')
+import matplotlib.pyplot as plt

('examples/preprocessing', 'plot_scaling_importance.py')
-from sklearn import metrics

('examples/preprocessing', 'plot_scaling_importance.py')
-import matplotlib.pyplot as plt

('examples/preprocessing', 'plot_scaling_importance.py')
+from sklearn.metrics import accuracy_score

('examples/text', 'plot_document_classification_20newsgroups.py')
-import logging

('examples/text', 'plot_document_classification_20newsgroups.py')
-import numpy as np

('examples/text', 'plot_document_classification_20newsgroups.py')
-from optparse import OptionParser

('examples/text', 'plot_document_classification_20newsgroups.py')
-import sys

('examples/text', 'plot_document_classification_20newsgroups.py')
+from sklearn.datasets import fetch_20newsgroups

('examples/text', 'plot_document_classification_20newsgroups.py')
-import matplotlib.pyplot as plt

('examples/text', 'plot_document_classification_20newsgroups.py')
-from sklearn.datasets import fetch_20newsgroups

('examples/text', 'plot_document_classification_20newsgroups.py')
+from sklearn.feature_selection import SelectKBest, chi2

('examples/text', 'plot_document_classification_20newsgroups.py')
+import numpy as np

('examples/text', 'plot_document_classification_20newsgroups.py')
+from sklearn import metrics

('examples/text', 'plot_document_classification_20newsgroups.py')
+from sklearn.utils.extmath import density

('examples/text', 'plot_document_classification_20newsgroups.py')
-from sklearn.feature_selection import SelectKBest, chi2

('examples/text', 'plot_document_classification_20newsgroups.py')
-from sklearn.utils.extmath import density

('examples/text', 'plot_document_classification_20newsgroups.py')
-from sklearn import metrics

('examples/text', 'plot_document_classification_20newsgroups.py')
+import matplotlib.pyplot as plt

('examples/model_selection', 'plot_roc.py')
-from scipy import interp

('examples/model_selection', 'plot_roc.py')
+from sklearn.metrics import roc_auc_score

('examples/model_selection', 'plot_confusion_matrix.py')
-from sklearn.metrics import confusion_matrix

('examples/model_selection', 'plot_confusion_matrix.py')
-from sklearn.utils.multiclass import unique_labels

('examples/model_selection', 'plot_confusion_matrix.py')
+from sklearn.metrics import ConfusionMatrixDisplay

('examples/model_selection', 'plot_train_error_vs_test_error.py')
+from sklearn.datasets import make_regression

('examples/model_selection', 'plot_train_error_vs_test_error.py')
+from sklearn.model_selection import train_test_split

('examples/model_selection', 'plot_roc_crossval.py')
-from scipy import interp

('examples/model_selection', 'plot_roc_crossval.py')
-import matplotlib.pyplot as plt

('examples/model_selection', 'plot_roc_crossval.py')
-from sklearn import svm, datasets

('examples/model_selection', 'plot_roc_crossval.py')
-from sklearn.metrics import roc_curve, auc

('examples/model_selection', 'plot_roc_crossval.py')
-from sklearn.model_selection import StratifiedKFold

('examples/model_selection', 'plot_roc_crossval.py')
+from sklearn import datasets

('examples/model_selection', 'plot_roc_crossval.py')
+import matplotlib.pyplot as plt

('examples/model_selection', 'plot_roc_crossval.py')
+from sklearn import svm

('examples/model_selection', 'plot_roc_crossval.py')
+from sklearn.metrics import auc

('examples/model_selection', 'plot_roc_crossval.py')
+from sklearn.metrics import RocCurveDisplay

('examples/model_selection', 'plot_roc_crossval.py')
+from sklearn.model_selection import StratifiedKFold

('examples/model_selection', 'plot_cv_indices.py')
-from sklearn.model_selection import (TimeSeriesSplit, KFold, ShuffleSplit,

('examples/model_selection', 'plot_cv_indices.py')
+from sklearn.model_selection import (

('examples/model_selection', 'plot_randomized_search.py')
-from scipy.stats import randint as sp_randint

('examples/model_selection', 'plot_randomized_search.py')
+import scipy.stats as stats

('examples/model_selection', 'plot_randomized_search.py')
+from sklearn.utils.fixes import loguniform

('examples/model_selection', 'plot_randomized_search.py')
-from sklearn.model_selection import GridSearchCV

('examples/model_selection', 'plot_randomized_search.py')
-from sklearn.model_selection import RandomizedSearchCV

('examples/model_selection', 'plot_randomized_search.py')
+from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

('examples/model_selection', 'plot_randomized_search.py')
-from sklearn.ensemble import RandomForestClassifier

('examples/model_selection', 'plot_randomized_search.py')
+from sklearn.linear_model import SGDClassifier

('examples/model_selection', 'plot_precision_recall.py')
-from sklearn import svm, datasets

('examples/model_selection', 'plot_precision_recall.py')
+import numpy as np

('examples/model_selection', 'plot_precision_recall.py')
+from sklearn.datasets import load_iris

('examples/model_selection', 'plot_precision_recall.py')
-import numpy as np

('examples/model_selection', 'plot_precision_recall.py')
+from sklearn.pipeline import make_pipeline

('examples/model_selection', 'plot_precision_recall.py')
+from sklearn.preprocessing import StandardScaler

('examples/model_selection', 'plot_precision_recall.py')
+from sklearn.svm import LinearSVC

('examples/model_selection', 'plot_precision_recall.py')
+from sklearn.metrics import PrecisionRecallDisplay

('examples/model_selection', 'plot_precision_recall.py')
-from sklearn.metrics import average_precision_score

('examples/model_selection', 'plot_precision_recall.py')
-from sklearn.metrics import precision_recall_curve

('examples/model_selection', 'plot_precision_recall.py')
-import matplotlib.pyplot as plt

('examples/model_selection', 'plot_precision_recall.py')
-from inspect import signature

('examples/model_selection', 'plot_precision_recall.py')
+import matplotlib.pyplot as plt

('examples/decomposition', 'plot_kernel_pca.py')
-import numpy as np

('examples/decomposition', 'plot_kernel_pca.py')
+from sklearn.datasets import make_circles

('examples/decomposition', 'plot_kernel_pca.py')
+from sklearn.model_selection import train_test_split

('examples/decomposition', 'plot_kernel_pca.py')
-from sklearn.datasets import make_circles

('examples/decomposition', 'plot_faces_decomposition.py')
-from time import time

('examples/decomposition', 'plot_faces_decomposition.py')
-from sklearn.cluster import MiniBatchKMeans

('examples/decomposition', 'plot_faces_decomposition.py')
+from sklearn import cluster

('examples/decomposition', 'plot_image_denoising.py')
-from time import time

('examples/decomposition', 'plot_image_denoising.py')
-import matplotlib.pyplot as plt

('examples/decomposition', 'plot_image_denoising.py')
-from sklearn.decomposition import MiniBatchDictionaryLearning

('examples/decomposition', 'plot_image_denoising.py')
-from sklearn.feature_extraction.image import extract_patches_2d

('examples/decomposition', 'plot_image_denoising.py')
-from sklearn.feature_extraction.image import reconstruct_from_patches_2d

('examples/decomposition', 'plot_image_denoising.py')
+import matplotlib.pyplot as plt

('examples/decomposition', 'plot_image_denoising.py')
+from time import time

('examples/decomposition', 'plot_image_denoising.py')
+from sklearn.feature_extraction.image import extract_patches_2d

('examples/decomposition', 'plot_image_denoising.py')
+from sklearn.decomposition import MiniBatchDictionaryLearning

('examples/decomposition', 'plot_image_denoising.py')
+from sklearn.feature_extraction.image import reconstruct_from_patches_2d

('examples/decomposition', 'plot_pca_iris.py')
-from mpl_toolkits.mplot3d import Axes3D

('examples/decomposition', 'plot_beta_divergence.py')
-from sklearn.decomposition.nmf import _beta_divergence

('examples/decomposition', 'plot_beta_divergence.py')
+from sklearn.decomposition._nmf import _beta_divergence

('examples/decomposition', 'plot_pca_3d.py')
-from sklearn.decomposition import PCA

('examples/decomposition', 'plot_pca_3d.py')
-from mpl_toolkits.mplot3d import Axes3D

('examples/decomposition', 'plot_pca_3d.py')
-import matplotlib.pyplot as plt

('examples/decomposition', 'plot_pca_3d.py')
+from sklearn.decomposition import PCA

('examples/decomposition', 'plot_pca_3d.py')
+import matplotlib.pyplot as plt

('examples/decomposition', 'plot_pca_3d.py')
+# unused but required import for doing 3d projections with matplotlib < 3.2

('examples/decomposition', 'plot_pca_3d.py')
+import mpl_toolkits.mplot3d  # noqa: F401

('examples/decomposition', 'plot_sparse_coding.py')
-from distutils.version import LooseVersion

('examples/decomposition', 'plot_ica_vs_pca.py')
-import matplotlib.pyplot as plt

('examples/decomposition', 'plot_ica_vs_pca.py')
+import matplotlib.pyplot as plt

('examples/neighbors', 'plot_species_kde.py')
-from sklearn.datasets.species_distributions import construct_grids

('examples/neighbors', 'plot_nca_classification.py')
-import numpy as np

('examples/neighbors', 'plot_nca_classification.py')
-from sklearn.neighbors import (KNeighborsClassifier,

('examples/neighbors', 'plot_nca_classification.py')
+from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis

('examples/neighbors', 'plot_nca_classification.py')
+from sklearn.inspection import DecisionBoundaryDisplay

('examples/neighbors', 'plot_nearest_centroid.py')
+from sklearn.inspection import DecisionBoundaryDisplay

('examples/neighbors', 'plot_nca_dim_reduction.py')
-from sklearn.neighbors import (KNeighborsClassifier,

('examples/neighbors', 'plot_nca_dim_reduction.py')
+from sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis

('examples/neighbors', 'plot_nca_illustration.py')
-from sklearn.utils.fixes import logsumexp

('examples/neighbors', 'plot_nca_illustration.py')
+from scipy.special import logsumexp

('examples/neighbors', 'plot_classification.py')
-import numpy as np

('examples/neighbors', 'plot_classification.py')
+import seaborn as sns

('examples/neighbors', 'plot_classification.py')
+from sklearn.inspection import DecisionBoundaryDisplay

('examples/neighbors', 'plot_kde_1d.py')
-import matplotlib

('examples/neighbors', 'plot_kde_1d.py')
-from distutils.version import LooseVersion

('benchmarks', 'bench_mnist.py')
-from sklearn.datasets import fetch_mldata

('benchmarks', 'bench_mnist.py')
+from sklearn.datasets import fetch_openml

('benchmarks', 'bench_rcv1_logreg_convergence.py')
-from sklearn.linear_model import (LogisticRegression, SGDClassifier)

('benchmarks', 'bench_rcv1_logreg_convergence.py')
+from sklearn.linear_model import LogisticRegression, SGDClassifier

('benchmarks', 'bench_rcv1_logreg_convergence.py')
-from sklearn.linear_model.sag import get_auto_step_size

('benchmarks', 'bench_rcv1_logreg_convergence.py')
+from sklearn.linear_model._sag import get_auto_step_size

('benchmarks', 'bench_tsne_mnist.py')
-from sklearn.datasets import fetch_mldata

('benchmarks', 'bench_tsne_mnist.py')
+from sklearn.datasets import fetch_openml

('benchmarks', 'bench_tsne_mnist.py')
+from sklearn.utils._openmp_helpers import _openmp_effective_n_threads

('benchmarks', 'bench_plot_randomized_svd.py')
+from sklearn.utils._arpack import _init_arpack_v0

('benchmarks', 'bench_plot_randomized_svd.py')
-from sklearn.datasets.samples_generator import (make_low_rank_matrix,

('benchmarks', 'bench_plot_randomized_svd.py')
-from sklearn.datasets import (fetch_lfw_people,

('benchmarks', 'bench_plot_randomized_svd.py')
+from sklearn.datasets import make_low_rank_matrix, make_sparse_uncorrelated

('benchmarks', 'bench_plot_randomized_svd.py')
+from sklearn.datasets import (

('benchmarks', 'bench_hist_gradient_boosting_higgsboson.py')
-from sklearn.experimental import enable_hist_gradient_boosting  # noqa

('benchmarks', 'bench_hist_gradient_boosting_higgsboson.py')
-from sklearn.ensemble._hist_gradient_boosting.utils import (

('benchmarks', 'bench_hist_gradient_boosting_higgsboson.py')
+from sklearn.ensemble._hist_gradient_boosting.utils import get_equivalent_estimator

('benchmarks', 'bench_isolation_forest.py')
-from sklearn.datasets import fetch_kddcup99, fetch_covtype, fetch_mldata

('benchmarks', 'bench_isolation_forest.py')
+from sklearn.datasets import fetch_kddcup99, fetch_covtype, fetch_openml

('benchmarks', 'bench_plot_omp_lars.py')
-from sklearn.datasets.samples_generator import make_sparse_coded_signal

('benchmarks', 'bench_plot_omp_lars.py')
+from sklearn.datasets import make_sparse_coded_signal

('benchmarks', 'bench_lasso.py')
-from sklearn.datasets.samples_generator import make_regression

('benchmarks', 'bench_lasso.py')
+from sklearn.datasets import make_regression

('benchmarks', 'bench_plot_svd.py')
-from sklearn.datasets.samples_generator import make_low_rank_matrix

('benchmarks', 'bench_plot_svd.py')
+from sklearn.datasets import make_low_rank_matrix

('benchmarks', 'bench_plot_svd.py')
-    from mpl_toolkits.mplot3d import axes3d  # register the 3d projection

('benchmarks', 'bench_plot_svd.py')
+    from mpl_toolkits.mplot3d import axes3d  # noqa register the 3d projection

('benchmarks', 'bench_hist_gradient_boosting.py')
+import numpy as np

('benchmarks', 'bench_hist_gradient_boosting.py')
-from sklearn.experimental import enable_hist_gradient_boosting  # noqa

('benchmarks', 'bench_hist_gradient_boosting.py')
-from sklearn.ensemble._hist_gradient_boosting.utils import (

('benchmarks', 'bench_hist_gradient_boosting.py')
+from sklearn.ensemble._hist_gradient_boosting.utils import get_equivalent_estimator

('benchmarks', 'bench_plot_fastkmeans.py')
-from sklearn.cluster.k_means_ import KMeans, MiniBatchKMeans

('benchmarks', 'bench_plot_fastkmeans.py')
+from sklearn.cluster import KMeans, MiniBatchKMeans

('benchmarks', 'bench_plot_fastkmeans.py')
-    from mpl_toolkits.mplot3d import axes3d  # register the 3d projection

('benchmarks', 'bench_plot_fastkmeans.py')
+    from mpl_toolkits.mplot3d import axes3d  # noqa register the 3d projection

('benchmarks', 'bench_plot_nmf.py')
-from sklearn.utils.testing import ignore_warnings

('benchmarks', 'bench_plot_nmf.py')
+from sklearn.utils._testing import ignore_warnings

('benchmarks', 'bench_plot_nmf.py')
-from sklearn.decomposition.nmf import NMF

('benchmarks', 'bench_plot_nmf.py')
-from sklearn.decomposition.nmf import _initialize_nmf

('benchmarks', 'bench_plot_nmf.py')
-from sklearn.decomposition.nmf import _beta_divergence

('benchmarks', 'bench_plot_nmf.py')
-from sklearn.decomposition.nmf import INTEGER_TYPES, _check_init

('benchmarks', 'bench_plot_nmf.py')
+from sklearn.decomposition import NMF

('benchmarks', 'bench_plot_nmf.py')
+from sklearn.decomposition._nmf import _initialize_nmf

('benchmarks', 'bench_plot_nmf.py')
+from sklearn.decomposition._nmf import _beta_divergence

('benchmarks', 'bench_plot_nmf.py')
+from sklearn.decomposition._nmf import _check_init

('benchmarks', 'bench_plot_lasso_path.py')
-from sklearn.datasets.samples_generator import make_regression

('benchmarks', 'bench_plot_lasso_path.py')
+from sklearn.datasets import make_regression

('benchmarks', 'bench_plot_lasso_path.py')
-    from mpl_toolkits.mplot3d import axes3d  # register the 3d projection

('benchmarks', 'bench_plot_lasso_path.py')
+    from mpl_toolkits.mplot3d import axes3d  # noqa register the 3d projection

('benchmarks', 'bench_multilabel_metrics.py')
-from sklearn.metrics import (f1_score, accuracy_score, hamming_loss,

('benchmarks', 'bench_multilabel_metrics.py')
-from sklearn.utils.testing import ignore_warnings

('benchmarks', 'bench_multilabel_metrics.py')
+from sklearn.metrics import (

('benchmarks', 'bench_multilabel_metrics.py')
+from sklearn.utils._testing import ignore_warnings

('benchmarks', 'bench_sparsify.py')
-from scipy.sparse.csr import csr_matrix

('benchmarks', 'bench_sparsify.py')
+from scipy.sparse import csr_matrix

('benchmarks', 'bench_sparsify.py')
-from sklearn.linear_model.stochastic_gradient import SGDRegressor

('benchmarks', 'bench_sparsify.py')
+from sklearn.linear_model import SGDRegressor

('benchmarks', 'bench_sgd_regression.py')
-from sklearn.datasets.samples_generator import make_regression

('benchmarks', 'bench_sgd_regression.py')
+from sklearn.datasets import make_regression

('benchmarks', 'bench_saga.py')
-from joblib import delayed, Parallel, Memory

('benchmarks', 'bench_saga.py')
+from joblib import Parallel

('benchmarks', 'bench_saga.py')
+from sklearn.utils.fixes import delayed

('benchmarks', 'bench_saga.py')
-from sklearn.datasets import fetch_rcv1, load_iris, load_digits, \

('benchmarks', 'bench_saga.py')
+from sklearn.datasets import (

('benchmarks', 'bench_random_projections.py')
-from sklearn.random_projection import (SparseRandomProjection,

('benchmarks', 'bench_random_projections.py')
+from sklearn.random_projection import (

('benchmarks', 'bench_lof.py')
-from sklearn.datasets import fetch_kddcup99, fetch_covtype, fetch_mldata

('benchmarks', 'bench_lof.py')
+from sklearn.datasets import fetch_kddcup99, fetch_covtype, fetch_openml

('benchmarks', 'bench_glmnet.py')
-from sklearn.datasets.samples_generator import make_regression

('benchmarks', 'bench_glmnet.py')
+from sklearn.datasets import make_regression

('benchmarks', 'bench_text_vectorizers.py')
-from sklearn.feature_extraction.text import (CountVectorizer, TfidfVectorizer,

('benchmarks', 'bench_text_vectorizers.py')
+from sklearn.feature_extraction.text import (

('build_tools', 'generate_authors_table.py')
+import time

('build_tools', 'generate_authors_table.py')
+from pathlib import Path

('build_tools', 'generate_authors_table.py')
+from os import path

('build_tools/azure', 'install.sh')
-python -c "import numpy; print('numpy %s' % numpy.__version__)"

('build_tools/azure', 'install.sh')
-python -c "import scipy; print('scipy %s' % scipy.__version__)"

('build_tools/azure', 'install.sh')
-    import pandas

('build_tools/azure', 'test_script.sh')
-python -c "import numpy; print('numpy %s' % numpy.__version__)"

('build_tools/azure', 'test_script.sh')
-python -c "import scipy; print('scipy %s' % scipy.__version__)"

('build_tools/azure', 'test_script.sh')
-    import pandas

('build_tools/azure', 'test_script.sh')
-python -c "import multiprocessing as mp; print('%d CPUs' % mp.cpu_count())"

('build_tools/azure', 'test_script.sh')
+python -c "import joblib; print(f'Number of cores: {joblib.cpu_count()}')"

('build_tools/azure', 'test_script.sh')
+python -c "import sklearn; sklearn.show_versions()"

('build_tools/azure', 'test_script.sh')
+    # Python 3.10 deprecates distutils, which is imported by numpy internally

('build_tools/circle', 'build_doc.sh')
+# imports get_dep

('build_tools/circle', 'build_test_pypy.sh')
+python -c "import platform; assert platform.python_implementation() == 'PyPy'"

('build_tools/travis', 'install.sh')
+# important that we call to the right installation script.

('build_tools/travis', 'install.sh')
-python -c "import numpy; print('numpy %s' % numpy.__version__)"

('build_tools/travis', 'install.sh')
-python -c "import scipy; print('scipy %s' % scipy.__version__)"

('build_tools/travis', 'install.sh')
-    import pandas

('build_tools/travis', 'test_script.sh')
-python -c "import numpy; print('numpy %s' % numpy.__version__)"

('build_tools/travis', 'test_script.sh')
-python -c "import scipy; print('scipy %s' % scipy.__version__)"

('build_tools/travis', 'test_script.sh')
+python -c "import numpy; print(f'numpy {numpy.__version__}')"

('build_tools/travis', 'test_script.sh')
+python -c "import scipy; print(f'scipy {scipy.__version__}')"

('build_tools/travis', 'test_script.sh')
-python -c "import multiprocessing as mp; print('%d CPUs' % mp.cpu_count())"

('build_tools/travis', 'test_script.sh')
+python -c "import joblib; print(f'{joblib.cpu_count()} CPUs')"

('build_tools/travis', 'test_script.sh')
+python -c "import platform; print(f'{platform.machine()}')"










