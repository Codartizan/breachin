('', 'setup.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,10 +2,10 @@
 import sys
 from distutils.sysconfig import get_python_lib

-from setuptools import find_packages, setup
+from setuptools import setup

 CURRENT_PYTHON = sys.version_info[:2]
-REQUIRED_PYTHON = (3, 5)
+REQUIRED_PYTHON = (3, 6)

 # This check and everything above must remain compatible with Python 2.7.
 if CURRENT_PYTHON < REQUIRED_PYTHON:
@@ -52,69 +52,7 @@
             break


-EXCLUDE_FROM_PACKAGES = ['django.conf.project_template',
-                         'django.conf.app_template',
-                         'django.bin']
-
-
-# Dynamically calculate the version based on django.VERSION.
-version = __import__('django').get_version()
-
-
-def read(fname):
-    with open(os.path.join(os.path.dirname(__file__), fname)) as f:
-        return f.read()
-
-
-setup(
-    name='Django',
-    version=version,
-    python_requires='>={}.{}'.format(*REQUIRED_PYTHON),
-    url='https://www.djangoproject.com/',
-    author='Django Software Foundation',
-    author_email='foundation@djangoproject.com',
-    description=('A high-level Python Web framework that encourages '
-                 'rapid development and clean, pragmatic design.'),
-    long_description=read('README.rst'),
-    license='BSD',
-    packages=find_packages(exclude=EXCLUDE_FROM_PACKAGES),
-    include_package_data=True,
-    scripts=['django/bin/django-admin.py'],
-    entry_points={'console_scripts': [
-        'django-admin = django.core.management:execute_from_command_line',
-    ]},
-    install_requires=['pytz', 'sqlparse'],
-    extras_require={
-        "bcrypt": ["bcrypt"],
-        "argon2": ["argon2-cffi >= 16.1.0"],
-    },
-    zip_safe=False,
-    classifiers=[
-        'Development Status :: 5 - Production/Stable',
-        'Environment :: Web Environment',
-        'Framework :: Django',
-        'Intended Audience :: Developers',
-        'License :: OSI Approved :: BSD License',
-        'Operating System :: OS Independent',
-        'Programming Language :: Python',
-        'Programming Language :: Python :: 3',
-        'Programming Language :: Python :: 3.5',
-        'Programming Language :: Python :: 3.6',
-        'Programming Language :: Python :: 3.7',
-        'Programming Language :: Python :: 3 :: Only',
-        'Topic :: Internet :: WWW/HTTP',
-        'Topic :: Internet :: WWW/HTTP :: Dynamic Content',
-        'Topic :: Internet :: WWW/HTTP :: WSGI',
-        'Topic :: Software Development :: Libraries :: Application Frameworks',
-        'Topic :: Software Development :: Libraries :: Python Modules',
-    ],
-    project_urls={
-        'Documentation': 'https://docs.djangoproject.com/',
-        'Funding': 'https://www.djangoproject.com/fundraising/',
-        'Source': 'https://github.com/django/django',
-        'Tracker': 'https://code.djangoproject.com/',
-    },
-)
+setup()


 if overlay_warning:
('', 'setup.cfg')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,9 +1,65 @@
+[metadata]
+name = Django
+version = attr: django.__version__
+url = https://www.djangoproject.com/
+author = Django Software Foundation
+author_email = foundation@djangoproject.com
+description = A high-level Python Web framework that encourages rapid development and clean, pragmatic design.
+long_description = file: README.rst
+license = BSD-3-Clause
+classifiers =
+    Development Status :: 5 - Production/Stable
+    Environment :: Web Environment
+    Framework :: Django
+    Intended Audience :: Developers
+    License :: OSI Approved :: BSD License
+    Operating System :: OS Independent
+    Programming Language :: Python
+    Programming Language :: Python :: 3
+    Programming Language :: Python :: 3 :: Only
+    Programming Language :: Python :: 3.6
+    Programming Language :: Python :: 3.7
+    Programming Language :: Python :: 3.8
+    Programming Language :: Python :: 3.9
+    Programming Language :: Python :: 3.10
+    Topic :: Internet :: WWW/HTTP
+    Topic :: Internet :: WWW/HTTP :: Dynamic Content
+    Topic :: Internet :: WWW/HTTP :: WSGI
+    Topic :: Software Development :: Libraries :: Application Frameworks
+    Topic :: Software Development :: Libraries :: Python Modules
+project_urls =
+    Documentation = https://docs.djangoproject.com/
+    Release notes = https://docs.djangoproject.com/en/stable/releases/
+    Funding = https://www.djangoproject.com/fundraising/
+    Source = https://github.com/django/django
+    Tracker = https://code.djangoproject.com/
+
+[options]
+python_requires = >=3.6
+packages = find:
+# When the django-admin.py deprecation ends, remove "scripts".
+scripts = django/bin/django-admin.py
+include_package_data = true
+zip_safe = false
+install_requires =
+    asgiref >= 3.3.2, < 4
+    pytz
+    sqlparse >= 0.2.2
+
+[options.entry_points]
+console_scripts =
+    django-admin = django.core.management:execute_from_command_line
+
+[options.extras_require]
+argon2 = argon2-cffi >= 19.1.0
+bcrypt = bcrypt
+
 [bdist_rpm]
 doc_files = docs extras AUTHORS INSTALL LICENSE README.rst
 install-script = scripts/rpm-install.sh

 [flake8]
-exclude = build,.git,.tox,./django/utils/six.py,./django/conf/app_template/*,./tests/.env
+exclude = build,.git,.tox,./tests/.env
 ignore = W504,W601
 max-line-length = 119

@@ -14,7 +70,3 @@
 known_first_party = django
 line_length = 79
 multi_line_output = 5
-not_skip = __init__.py
-
-[metadata]
-license-file = LICENSE
('scripts', 'manage_translations.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -20,7 +20,7 @@

 import os
 from argparse import ArgumentParser
-from subprocess import PIPE, Popen, call
+from subprocess import PIPE, run

 import django
 from django.conf import settings
@@ -73,10 +73,9 @@
     """
     po_path = '%(path)s/en/LC_MESSAGES/django%(ext)s.po' % {
         'path': base_path, 'ext': 'js' if cat_name.endswith('-js') else ''}
-    p = Popen("git diff -U0 %s | egrep '^[-+]msgid' | wc -l" % po_path,
-              stdout=PIPE, stderr=PIPE, shell=True)
-    output, errors = p.communicate()
-    num_changes = int(output.strip())
+    p = run("git diff -U0 %s | egrep '^[-+]msgid' | wc -l" % po_path,
+            stdout=PIPE, stderr=PIPE, shell=True)
+    num_changes = int(p.stdout.strip())
     print("%d changed/added messages in '%s' catalog." % (num_changes, cat_name))


@@ -122,18 +121,20 @@
             po_path = '{path}/{lang}/LC_MESSAGES/django{ext}.po'.format(
                 path=dir_, lang=lang, ext='js' if name.endswith('-js') else ''
             )
-            p = Popen(
+            p = run(
                 ['msgfmt', '-vc', '-o', '/dev/null', po_path],
                 stdout=PIPE, stderr=PIPE,
-                env={'LANG': 'C'}
+                env={'LANG': 'C'},
+                encoding='utf-8',
             )
-            output, errors = p.communicate()
             if p.returncode == 0:
                 # msgfmt output stats on stderr
-                print("%s: %s" % (lang, errors.decode().strip()))
+                print('%s: %s' % (lang, p.stderr.strip()))
             else:
-                print("Errors happened when checking %s translation for %s:\n%s" % (
-                    lang, name, errors.decode()))
+                print(
+                    'Errors happened when checking %s translation for %s:\n%s'
+                    % (lang, name, p.stderr)
+                )


 def fetch(resources=None, languages=None):
@@ -146,12 +147,11 @@
     for name, dir_ in locale_dirs:
         # Transifex pull
         if languages is None:
-            call('tx pull -r %(res)s -a -f  --minimum-perc=5' % {'res': _tx_resource_for_name(name)}, shell=True)
+            run(['tx', 'pull', '-r', _tx_resource_for_name(name), '-a', '-f', '--minimum-perc=5'])
             target_langs = sorted(d for d in os.listdir(dir_) if not d.startswith('_') and d != 'en')
         else:
             for lang in languages:
-                call('tx pull -r %(res)s -f -l %(lang)s' % {
-                    'res': _tx_resource_for_name(name), 'lang': lang}, shell=True)
+                run(['tx', 'pull', '-r', _tx_resource_for_name(name), '-f', '-l', lang])
             target_langs = languages

         # msgcat to wrap lines and msgfmt for compilation of .mo file
@@ -162,9 +162,9 @@
                 print("No %(lang)s translation for resource %(name)s" % {
                     'lang': lang, 'name': name})
                 continue
-            call('msgcat --no-location -o %s %s' % (po_path, po_path), shell=True)
-            res = call('msgfmt -c -o %s.mo %s' % (po_path[:-3], po_path), shell=True)
-            if res != 0:
+            run(['msgcat', '--no-location', '-o', po_path, po_path])
+            msgfmt = run(['msgfmt', '-c', '-o', '%s.mo' % po_path[:-3], po_path])
+            if msgfmt.returncode != 0:
                 errors.append((name, lang))
     if errors:
         print("\nWARNING: Errors have occurred in following cases:")
('django', 'shortcuts.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,29 +3,12 @@
 of MVC. In other words, these functions/classes introduce controlled coupling
 for convenience's sake.
 """
-import warnings
-
 from django.http import (
     Http404, HttpResponse, HttpResponsePermanentRedirect, HttpResponseRedirect,
 )
 from django.template import loader
 from django.urls import NoReverseMatch, reverse
-from django.utils.deprecation import RemovedInDjango30Warning
 from django.utils.functional import Promise
-
-
-def render_to_response(template_name, context=None, content_type=None, status=None, using=None):
-    """
-    Return a HttpResponse whose content is filled with the result of calling
-    django.template.loader.render_to_string() with the passed arguments.
-    """
-    warnings.warn(
-        'render_to_response() is deprecated in favor of render(). It has the '
-        'same signature except that it also requires a request.',
-        RemovedInDjango30Warning, stacklevel=2,
-    )
-    content = loader.render_to_string(template_name, context, using=using)
-    return HttpResponse(content, content_type, status)


 def render(request, template_name, context=None, content_type=None, status=None, using=None):
@@ -138,10 +121,9 @@
         # further to some Python functions like urlparse.
         to = str(to)

-    if isinstance(to, str):
-        # Handle relative URLs
-        if to.startswith(('./', '../')):
-            return to
+    # Handle relative URLs
+    if isinstance(to, str) and to.startswith(('./', '../')):
+        return to

     # Next try a reverse URL resolution.
     try:
('django', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,6 @@
 from django.utils.version import get_version

-VERSION = (2, 2, 0, 'final', 0)
+VERSION = (3, 2, 14, 'final', 0)

 __version__ = get_version(VERSION)

('django/templatetags', 'i18n.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,5 @@
+from decimal import Decimal
+
 from django.conf import settings
 from django.template import Library, Node, TemplateSyntaxError, Variable
 from django.template.base import TokenType, render_value_in_context
@@ -98,7 +100,8 @@
 class BlockTranslateNode(Node):

     def __init__(self, extra_context, singular, plural=None, countervar=None,
-                 counter=None, message_context=None, trimmed=False, asvar=None):
+                 counter=None, message_context=None, trimmed=False, asvar=None,
+                 tag_name='blocktranslate'):
         self.extra_context = extra_context
         self.singular = singular
         self.plural = plural
@@ -107,6 +110,7 @@
         self.message_context = message_context
         self.trimmed = trimmed
         self.asvar = asvar
+        self.tag_name = tag_name

     def render_token_list(self, tokens):
         result = []
@@ -133,6 +137,11 @@
         singular, vars = self.render_token_list(self.singular)
         if self.plural and self.countervar and self.counter:
             count = self.counter.resolve(context)
+            if not isinstance(count, (Decimal, float, int)):
+                raise TemplateSyntaxError(
+                    "%r argument to %r tag must be a number."
+                    % (self.countervar, self.tag_name)
+                )
             context[self.countervar] = count
             plural, plural_vars = self.render_token_list(self.plural)
             if message_context:
@@ -163,8 +172,8 @@
             if nested:
                 # Either string is malformed, or it's a bug
                 raise TemplateSyntaxError(
-                    "'blocktrans' is unable to format string returned by gettext: %r using %r"
-                    % (result, data)
+                    '%r is unable to format string returned by gettext: %r '
+                    'using %r' % (self.tag_name, result, data)
                 )
             with translation.override(None):
                 result = self.render(context, nested=True)
@@ -313,6 +322,7 @@
     return GetCurrentLanguageBidiNode(args[2])


+@register.tag("translate")
 @register.tag("trans")
 def do_translate(parser, token):
     """
@@ -321,7 +331,7 @@

     Usage::

-        {% trans "this is a test" %}
+        {% translate "this is a test" %}

     This marks the string for translation so it will be pulled out by
     makemessages into the .po files and runs the string through the translation
@@ -329,7 +339,7 @@

     There is a second form::

-        {% trans "this is a test" noop %}
+        {% translate "this is a test" noop %}

     This marks the string for translation, but returns the string unchanged.
     Use it when you need to store values into forms that should be translated
@@ -338,19 +348,19 @@
     You can use variables instead of constant strings
     to translate stuff you marked somewhere else::

-        {% trans variable %}
+        {% translate variable %}

     This tries to translate the contents of the variable ``variable``. Make
     sure that the string in there is something that is in the .po file.

     It is possible to store the translated string into a variable::

-        {% trans "this is a test" as var %}
+        {% translate "this is a test" as var %}
         {{ var }}

     Contextual translations are also supported::

-        {% trans "this is a test" context "greeting" %}
+        {% translate "this is a test" context "greeting" %}

     This is equivalent to calling pgettext instead of (u)gettext.
     """
@@ -406,6 +416,7 @@
     return TranslateNode(message_string, noop, asvar, message_context)


+@register.tag("blocktranslate")
 @register.tag("blocktrans")
 def do_block_translate(parser, token):
     """
@@ -413,37 +424,37 @@

     Usage::

-        {% blocktrans with bar=foo|filter boo=baz|filter %}
+        {% blocktranslate with bar=foo|filter boo=baz|filter %}
         This is {{ bar }} and {{ boo }}.
-        {% endblocktrans %}
+        {% endblocktranslate %}

     Additionally, this supports pluralization::

-        {% blocktrans count count=var|length %}
+        {% blocktranslate count count=var|length %}
         There is {{ count }} object.
         {% plural %}
         There are {{ count }} objects.
-        {% endblocktrans %}
+        {% endblocktranslate %}

     This is much like ngettext, only in template syntax.

     The "var as value" legacy format is still supported::

-        {% blocktrans with foo|filter as bar and baz|filter as boo %}
-        {% blocktrans count var|length as count %}
+        {% blocktranslate with foo|filter as bar and baz|filter as boo %}
+        {% blocktranslate count var|length as count %}

     The translated string can be stored in a variable using `asvar`::

-        {% blocktrans with bar=foo|filter boo=baz|filter asvar var %}
+        {% blocktranslate with bar=foo|filter boo=baz|filter asvar var %}
         This is {{ bar }} and {{ boo }}.
-        {% endblocktrans %}
+        {% endblocktranslate %}
         {{ var }}

     Contextual translations are supported::

-        {% blocktrans with bar=foo|filter context "greeting" %}
+        {% blocktranslate with bar=foo|filter context "greeting" %}
             This is {{ bar }}.
-        {% endblocktrans %}
+        {% endblocktranslate %}

     This is equivalent to calling pgettext/npgettext instead of
     (u)gettext/(u)ngettext.
@@ -513,19 +524,20 @@
             break
     if countervar and counter:
         if token.contents.strip() != 'plural':
-            raise TemplateSyntaxError("'blocktrans' doesn't allow other block tags inside it")
+            raise TemplateSyntaxError("%r doesn't allow other block tags inside it" % bits[0])
         while parser.tokens:
             token = parser.next_token()
             if token.token_type in (TokenType.VAR, TokenType.TEXT):
                 plural.append(token)
             else:
                 break
-    if token.contents.strip() != 'endblocktrans':
-        raise TemplateSyntaxError("'blocktrans' doesn't allow other block tags (seen %r) inside it" % token.contents)
+    end_tag_name = 'end%s' % bits[0]
+    if token.contents.strip() != end_tag_name:
+        raise TemplateSyntaxError("%r doesn't allow other block tags (seen %r) inside it" % (bits[0], token.contents))

     return BlockTranslateNode(extra_context, singular, plural, countervar,
                               counter, message_context, trimmed=trimmed,
-                              asvar=asvar)
+                              asvar=asvar, tag_name=bits[0])


 @register.tag
('django/middleware', 'clickjacking.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -30,16 +30,18 @@
         if getattr(response, 'xframe_options_exempt', False):
             return response

-        response['X-Frame-Options'] = self.get_xframe_options_value(request,
-                                                                    response)
+        response.headers['X-Frame-Options'] = self.get_xframe_options_value(
+            request,
+            response,
+        )
         return response

     def get_xframe_options_value(self, request, response):
         """
         Get the value to set for the X_FRAME_OPTIONS header. Use the value from
-        the X_FRAME_OPTIONS setting, or 'SAMEORIGIN' if not set.
+        the X_FRAME_OPTIONS setting, or 'DENY' if not set.

         This method can be overridden if needed, allowing it to vary based on
         the request or response.
         """
-        return getattr(settings, 'X_FRAME_OPTIONS', 'SAMEORIGIN').upper()
+        return getattr(settings, 'X_FRAME_OPTIONS', 'DENY').upper()
('django/middleware', 'gzip.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,10 +1,9 @@
-import re
-
 from django.utils.cache import patch_vary_headers
 from django.utils.deprecation import MiddlewareMixin
+from django.utils.regex_helper import _lazy_re_compile
 from django.utils.text import compress_sequence, compress_string

-re_accepts_gzip = re.compile(r'\bgzip\b')
+re_accepts_gzip = _lazy_re_compile(r'\bgzip\b')


 class GZipMiddleware(MiddlewareMixin):
@@ -32,21 +31,21 @@
             # Delete the `Content-Length` header for streaming content, because
             # we won't know the compressed size until we stream it.
             response.streaming_content = compress_sequence(response.streaming_content)
-            del response['Content-Length']
+            del response.headers['Content-Length']
         else:
             # Return the compressed content only if it's actually shorter.
             compressed_content = compress_string(response.content)
             if len(compressed_content) >= len(response.content):
                 return response
             response.content = compressed_content
-            response['Content-Length'] = str(len(response.content))
+            response.headers['Content-Length'] = str(len(response.content))

         # If there is a strong ETag, make it weak to fulfill the requirements
         # of RFC 7232 section-2.1 while also allowing conditional request
         # matches on ETags.
         etag = response.get('ETag')
         if etag and etag.startswith('"'):
-            response['ETag'] = 'W/' + etag
-        response['Content-Encoding'] = 'gzip'
+            response.headers['ETag'] = 'W/' + etag
+        response.headers['Content-Encoding'] = 'gzip'

         return response
('django/middleware', 'csrf.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -42,34 +42,33 @@
     return get_random_string(CSRF_SECRET_LENGTH, allowed_chars=CSRF_ALLOWED_CHARS)


-def _salt_cipher_secret(secret):
+def _mask_cipher_secret(secret):
     """
     Given a secret (assumed to be a string of CSRF_ALLOWED_CHARS), generate a
-    token by adding a salt and using it to encrypt the secret.
-    """
-    salt = _get_new_csrf_string()
+    token by adding a mask and applying it to the secret.
+    """
+    mask = _get_new_csrf_string()
     chars = CSRF_ALLOWED_CHARS
-    pairs = zip((chars.index(x) for x in secret), (chars.index(x) for x in salt))
+    pairs = zip((chars.index(x) for x in secret), (chars.index(x) for x in mask))
     cipher = ''.join(chars[(x + y) % len(chars)] for x, y in pairs)
-    return salt + cipher
-
-
-def _unsalt_cipher_token(token):
+    return mask + cipher
+
+
+def _unmask_cipher_token(token):
     """
     Given a token (assumed to be a string of CSRF_ALLOWED_CHARS, of length
-    CSRF_TOKEN_LENGTH, and that its first half is a salt), use it to decrypt
+    CSRF_TOKEN_LENGTH, and that its first half is a mask), use it to decrypt
     the second half to produce the original secret.
     """
-    salt = token[:CSRF_SECRET_LENGTH]
+    mask = token[:CSRF_SECRET_LENGTH]
     token = token[CSRF_SECRET_LENGTH:]
     chars = CSRF_ALLOWED_CHARS
-    pairs = zip((chars.index(x) for x in token), (chars.index(x) for x in salt))
-    secret = ''.join(chars[x - y] for x, y in pairs)  # Note negative values are ok
-    return secret
+    pairs = zip((chars.index(x) for x in token), (chars.index(x) for x in mask))
+    return ''.join(chars[x - y] for x, y in pairs)  # Note negative values are ok


 def _get_new_csrf_token():
-    return _salt_cipher_secret(_get_new_csrf_string())
+    return _mask_cipher_secret(_get_new_csrf_string())


 def get_token(request):
@@ -84,11 +83,11 @@
     """
     if "CSRF_COOKIE" not in request.META:
         csrf_secret = _get_new_csrf_string()
-        request.META["CSRF_COOKIE"] = _salt_cipher_secret(csrf_secret)
+        request.META["CSRF_COOKIE"] = _mask_cipher_secret(csrf_secret)
     else:
-        csrf_secret = _unsalt_cipher_token(request.META["CSRF_COOKIE"])
+        csrf_secret = _unmask_cipher_token(request.META["CSRF_COOKIE"])
     request.META["CSRF_COOKIE_USED"] = True
-    return _salt_cipher_secret(csrf_secret)
+    return _mask_cipher_secret(csrf_secret)


 def rotate_token(request):
@@ -112,20 +111,20 @@
     elif len(token) == CSRF_SECRET_LENGTH:
         # Older Django versions set cookies to values of CSRF_SECRET_LENGTH
         # alphanumeric characters. For backwards compatibility, accept
-        # such values as unsalted secrets.
-        # It's easier to salt here and be consistent later, rather than add
+        # such values as unmasked secrets.
+        # It's easier to mask here and be consistent later, rather than add
         # different code paths in the checks, although that might be a tad more
         # efficient.
-        return _salt_cipher_secret(token)
+        return _mask_cipher_secret(token)
     return _get_new_csrf_token()


-def _compare_salted_tokens(request_csrf_token, csrf_token):
+def _compare_masked_tokens(request_csrf_token, csrf_token):
     # Assume both arguments are sanitized -- that is, strings of
     # length CSRF_TOKEN_LENGTH, all CSRF_ALLOWED_CHARS.
     return constant_time_compare(
-        _unsalt_cipher_token(request_csrf_token),
-        _unsalt_cipher_token(csrf_token),
+        _unmask_cipher_token(request_csrf_token),
+        _unmask_cipher_token(csrf_token),
     )


@@ -164,7 +163,7 @@
                 raise ImproperlyConfigured(
                     'CSRF_USE_SESSIONS is enabled, but request.session is not '
                     'set. SessionMiddleware must appear before CsrfViewMiddleware '
-                    'in MIDDLEWARE%s.' % ('_CLASSES' if settings.MIDDLEWARE is None else '')
+                    'in MIDDLEWARE.'
                 )
         else:
             try:
@@ -281,7 +280,10 @@
                     reason = REASON_BAD_REFERER % referer.geturl()
                     return self._reject(request, reason)

-            csrf_token = request.META.get('CSRF_COOKIE')
+            # Access csrf_token via self._get_token() as rotate_token() may
+            # have been called by an authentication middleware during the
+            # process_request() phase.
+            csrf_token = self._get_token(request)
             if csrf_token is None:
                 # No CSRF cookie. For POST requests, we insist on a CSRF cookie,
                 # and in this way we can avoid all CSRF attacks, including login
@@ -293,7 +295,7 @@
             if request.method == "POST":
                 try:
                     request_csrf_token = request.POST.get('csrfmiddlewaretoken', '')
-                except IOError:
+                except OSError:
                     # Handle a broken connection before we've completed reading
                     # the POST data. process_view shouldn't raise any
                     # exceptions, so we'll ignore and serve the user a 403
@@ -307,7 +309,7 @@
                 request_csrf_token = request.META.get(settings.CSRF_HEADER_NAME, '')

             request_csrf_token = _sanitize_token(request_csrf_token)
-            if not _compare_salted_tokens(request_csrf_token, csrf_token):
+            if not _compare_masked_tokens(request_csrf_token, csrf_token):
                 return self._reject(request, REASON_BAD_TOKEN)

         return self._accept(request)
('django/middleware', 'security.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -6,7 +6,10 @@


 class SecurityMiddleware(MiddlewareMixin):
+    # RemovedInDjango40Warning: when the deprecation ends, replace with:
+    #   def __init__(self, get_response):
     def __init__(self, get_response=None):
+        super().__init__(get_response)
         self.sts_seconds = settings.SECURE_HSTS_SECONDS
         self.sts_include_subdomains = settings.SECURE_HSTS_INCLUDE_SUBDOMAINS
         self.sts_preload = settings.SECURE_HSTS_PRELOAD
@@ -15,7 +18,7 @@
         self.redirect = settings.SECURE_SSL_REDIRECT
         self.redirect_host = settings.SECURE_SSL_HOST
         self.redirect_exempt = [re.compile(r) for r in settings.SECURE_REDIRECT_EXEMPT]
-        self.get_response = get_response
+        self.referrer_policy = settings.SECURE_REFERRER_POLICY

     def process_request(self, request):
         path = request.path.lstrip("/")
@@ -35,12 +38,20 @@
                 sts_header = sts_header + "; includeSubDomains"
             if self.sts_preload:
                 sts_header = sts_header + "; preload"
-            response['Strict-Transport-Security'] = sts_header
+            response.headers['Strict-Transport-Security'] = sts_header

         if self.content_type_nosniff:
-            response.setdefault('X-Content-Type-Options', 'nosniff')
+            response.headers.setdefault('X-Content-Type-Options', 'nosniff')

         if self.xss_filter:
-            response.setdefault('X-XSS-Protection', '1; mode=block')
+            response.headers.setdefault('X-XSS-Protection', '1; mode=block')
+
+        if self.referrer_policy:
+            # Support a comma-separated string or iterable of values to allow
+            # fallback.
+            response.headers.setdefault('Referrer-Policy', ','.join(
+                [v.strip() for v in self.referrer_policy.split(',')]
+                if isinstance(self.referrer_policy, str) else self.referrer_policy
+            ))

         return response
('django/middleware', 'cache.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -61,12 +61,15 @@
     UpdateCacheMiddleware must be the first piece of middleware in MIDDLEWARE
     so that it'll get called last during the response phase.
     """
+    # RemovedInDjango40Warning: when the deprecation ends, replace with:
+    #   def __init__(self, get_response):
     def __init__(self, get_response=None):
+        super().__init__(get_response)
         self.cache_timeout = settings.CACHE_MIDDLEWARE_SECONDS
+        self.page_timeout = None
         self.key_prefix = settings.CACHE_MIDDLEWARE_KEY_PREFIX
         self.cache_alias = settings.CACHE_MIDDLEWARE_ALIAS
         self.cache = caches[self.cache_alias]
-        self.get_response = get_response

     def _should_update_cache(self, request, response):
         return hasattr(request, '_cache_update_cache') and request._cache_update_cache
@@ -89,15 +92,18 @@
         if 'private' in response.get('Cache-Control', ()):
             return response

-        # Try to get the timeout from the "max-age" section of the "Cache-
-        # Control" header before reverting to using the default cache_timeout
-        # length.
-        timeout = get_max_age(response)
+        # Page timeout takes precedence over the "max-age" and the default
+        # cache timeout.
+        timeout = self.page_timeout
         if timeout is None:
-            timeout = self.cache_timeout
-        elif timeout == 0:
-            # max-age was set to 0, don't bother caching.
-            return response
+            # The timeout from the "max-age" section of the "Cache-Control"
+            # header takes precedence over the default cache timeout.
+            timeout = get_max_age(response)
+            if timeout is None:
+                timeout = self.cache_timeout
+            elif timeout == 0:
+                # max-age was set to 0, don't cache.
+                return response
         patch_response_headers(response, timeout)
         if timeout and response.status_code == 200:
             cache_key = learn_cache_key(request, response, timeout, self.key_prefix, cache=self.cache)
@@ -118,11 +124,13 @@
     FetchFromCacheMiddleware must be the last piece of middleware in MIDDLEWARE
     so that it'll get called last during the request phase.
     """
+    # RemovedInDjango40Warning: when the deprecation ends, replace with:
+    #   def __init__(self, get_response):
     def __init__(self, get_response=None):
+        super().__init__(get_response)
         self.key_prefix = settings.CACHE_MIDDLEWARE_KEY_PREFIX
         self.cache_alias = settings.CACHE_MIDDLEWARE_ALIAS
         self.cache = caches[self.cache_alias]
-        self.get_response = get_response

     def process_request(self, request):
         """
@@ -160,8 +168,10 @@
     Also used as the hook point for the cache decorator, which is generated
     using the decorator-from-middleware utility.
     """
-    def __init__(self, get_response=None, cache_timeout=None, **kwargs):
-        self.get_response = get_response
+    # RemovedInDjango40Warning: when the deprecation ends, replace with:
+    #   def __init__(self, get_response, cache_timeout=None, page_timeout=None, **kwargs):
+    def __init__(self, get_response=None, cache_timeout=None, page_timeout=None, **kwargs):
+        super().__init__(get_response)
         # We need to differentiate between "provided, but using default value",
         # and "not provided". If the value is provided using a default, then
         # we fall back to system defaults. If it is not provided at all,
@@ -171,19 +181,18 @@
             key_prefix = kwargs['key_prefix']
             if key_prefix is None:
                 key_prefix = ''
+            self.key_prefix = key_prefix
         except KeyError:
-            key_prefix = settings.CACHE_MIDDLEWARE_KEY_PREFIX
-        self.key_prefix = key_prefix
-
+            pass
         try:
             cache_alias = kwargs['cache_alias']
             if cache_alias is None:
                 cache_alias = DEFAULT_CACHE_ALIAS
+            self.cache_alias = cache_alias
+            self.cache = caches[self.cache_alias]
         except KeyError:
-            cache_alias = settings.CACHE_MIDDLEWARE_ALIAS
-        self.cache_alias = cache_alias
+            pass

-        if cache_timeout is None:
-            cache_timeout = settings.CACHE_MIDDLEWARE_SECONDS
-        self.cache_timeout = cache_timeout
-        self.cache = caches[self.cache_alias]
+        if cache_timeout is not None:
+            self.cache_timeout = cache_timeout
+        self.page_timeout = page_timeout
('django/middleware', 'common.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -38,9 +38,10 @@
         """

         # Check for denied User-Agents
-        if 'HTTP_USER_AGENT' in request.META:
+        user_agent = request.META.get('HTTP_USER_AGENT')
+        if user_agent is not None:
             for user_agent_regex in settings.DISALLOWED_USER_AGENTS:
-                if user_agent_regex.search(request.META['HTTP_USER_AGENT']):
+                if user_agent_regex.search(user_agent):
                     raise PermissionDenied('Forbidden user agent')

         # Check for a redirect based on settings.PREPEND_WWW
@@ -66,10 +67,11 @@
         """
         if settings.APPEND_SLASH and not request.path_info.endswith('/'):
             urlconf = getattr(request, 'urlconf', None)
-            return (
-                not is_valid_path(request.path_info, urlconf) and
-                is_valid_path('%s/' % request.path_info, urlconf)
-            )
+            if not is_valid_path(request.path_info, urlconf):
+                match = is_valid_path('%s/' % request.path_info, urlconf)
+                if match:
+                    view = match.func
+                    return getattr(view, 'should_append_slash', True)
         return False

     def get_full_path_with_slash(self, request):
@@ -102,14 +104,13 @@
         """
         # If the given URL is "Not Found", then check if we should redirect to
         # a path with a slash appended.
-        if response.status_code == 404:
-            if self.should_redirect_with_slash(request):
-                return self.response_redirect_class(self.get_full_path_with_slash(request))
+        if response.status_code == 404 and self.should_redirect_with_slash(request):
+            return self.response_redirect_class(self.get_full_path_with_slash(request))

         # Add the Content-Length header to non-streaming responses if not
         # already set.
         if not response.streaming and not response.has_header('Content-Length'):
-            response['Content-Length'] = str(len(response.content))
+            response.headers['Content-Length'] = str(len(response.content))

         return response

('django/middleware', 'locale.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -11,7 +11,7 @@
     """
     Parse a request and decide what translation object to install in the
     current thread context. This allows pages to be dynamically translated to
-    the language the user desires (if the language is available, of course).
+    the language the user desires (if the language is available).
     """
     response_redirect_class = HttpResponseRedirect

@@ -57,5 +57,5 @@

         if not (i18n_patterns_used and language_from_path):
             patch_vary_headers(response, ('Accept-Language',))
-        response.setdefault('Content-Language', language)
+        response.headers.setdefault('Content-Language', language)
         return response
('django/forms', 'models.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,8 +2,7 @@
 Helper functions for creating Form classes from Django models
 and database field objects.
 """
-
-from collections import OrderedDict
+import warnings
 from itertools import chain

 from django.core.exceptions import (
@@ -14,8 +13,9 @@
 from django.forms.formsets import BaseFormSet, formset_factory
 from django.forms.utils import ErrorList
 from django.forms.widgets import (
-    HiddenInput, MultipleHiddenInput, SelectMultiple,
+    HiddenInput, MultipleHiddenInput, RadioSelect, SelectMultiple,
 )
+from django.utils.deprecation import RemovedInDjango40Warning
 from django.utils.text import capfirst, get_text_list
 from django.utils.translation import gettext, gettext_lazy as _

@@ -49,8 +49,11 @@
             continue
         # Leave defaults for fields that aren't in POST data, except for
         # checkbox inputs because they don't appear in POST data if not checked.
-        if (f.has_default() and
-                form[f.name].field.widget.value_omitted_from_data(form.data, form.files, form.add_prefix(f.name))):
+        if (
+            f.has_default() and
+            form[f.name].field.widget.value_omitted_from_data(form.data, form.files, form.add_prefix(f.name)) and
+            cleaned_data.get(f.name) in form[f.name].field.empty_values
+        ):
             continue
         # Defer saving file-type fields until after the other fields, so a
         # callable upload_to can use the values from other fields.
@@ -84,7 +87,7 @@
     for f in chain(opts.concrete_fields, opts.private_fields, opts.many_to_many):
         if not getattr(f, 'editable', False):
             continue
-        if fields and f.name not in fields:
+        if fields is not None and f.name not in fields:
             continue
         if exclude and f.name in exclude:
             continue
@@ -94,10 +97,18 @@

 def apply_limit_choices_to_to_formfield(formfield):
     """Apply limit_choices_to to the formfield's queryset if needed."""
+    from django.db.models import Exists, OuterRef, Q
     if hasattr(formfield, 'queryset') and hasattr(formfield, 'get_limit_choices_to'):
         limit_choices_to = formfield.get_limit_choices_to()
-        if limit_choices_to is not None:
-            formfield.queryset = formfield.queryset.complex_filter(limit_choices_to)
+        if limit_choices_to:
+            complex_filter = limit_choices_to
+            if not isinstance(complex_filter, Q):
+                complex_filter = Q(**limit_choices_to)
+            complex_filter &= Q(pk=OuterRef('pk'))
+            # Use Exists() to avoid potential duplicates.
+            formfield.queryset = formfield.queryset.filter(
+                Exists(formfield.queryset.model._base_manager.filter(complex_filter)),
+            )


 def fields_for_model(model, fields=None, exclude=None, widgets=None,
@@ -105,7 +116,7 @@
                      labels=None, help_texts=None, error_messages=None,
                      field_classes=None, *, apply_limit_choices_to=True):
     """
-    Return an ``OrderedDict`` containing form fields for the given model.
+    Return a dictionary containing form fields for the given model.

     ``fields`` is an optional list of field names. If provided, return only the
     named fields.
@@ -134,11 +145,11 @@
     ``apply_limit_choices_to`` is a boolean indicating if limit_choices_to
     should be applied to a field's queryset.
     """
-    field_list = []
+    field_dict = {}
     ignored = []
     opts = model._meta
     # Avoid circular import
-    from django.db.models.fields import Field as ModelField
+    from django.db.models import Field as ModelField
     sortable_private_fields = [f for f in opts.private_fields if isinstance(f, ModelField)]
     for f in sorted(chain(opts.concrete_fields, sortable_private_fields, opts.many_to_many)):
         if not getattr(f, 'editable', False):
@@ -178,15 +189,14 @@
         if formfield:
             if apply_limit_choices_to:
                 apply_limit_choices_to_to_formfield(formfield)
-            field_list.append((f.name, formfield))
+            field_dict[f.name] = formfield
         else:
             ignored.append(f.name)
-    field_dict = OrderedDict(field_list)
     if fields:
-        field_dict = OrderedDict(
-            [(f, field_dict.get(f)) for f in fields
-                if ((not exclude) or (exclude and f not in exclude)) and (f not in ignored)]
-        )
+        field_dict = {
+            f: field_dict.get(f) for f in fields
+            if (not exclude or f not in exclude) and f not in ignored
+        }
     return field_dict


@@ -213,7 +223,7 @@

         formfield_callback = attrs.pop('formfield_callback', base_formfield_callback)

-        new_class = super(ModelFormMetaclass, mcs).__new__(mcs, name, bases, attrs)
+        new_class = super().__new__(mcs, name, bases, attrs)

         if bases == (BaseModelForm,):
             return new_class
@@ -475,7 +485,9 @@
                       labels=None, help_texts=None, error_messages=None,
                       field_classes=None):
     """
-    Return a ModelForm containing form fields for the given model.
+    Return a ModelForm containing form fields for the given model. You can
+    optionally pass a `form` argument to use as a starting point for
+    constructing the ModelForm.

     ``fields`` is an optional list of field names. If provided, include only
     the named fields in the returned fields. If omitted or '__all__', use all
@@ -811,7 +823,7 @@

     def add_fields(self, form, index):
         """Add a hidden field for the object's primary key."""
-        from django.db.models import AutoField, OneToOneField, ForeignKey
+        from django.db.models import AutoField, ForeignKey, OneToOneField
         self._pk_field = pk = self.model._meta.pk
         # If a pk isn't editable, then it won't be on the form, so we need to
         # add it here so we can tell which object is which when we get the
@@ -858,7 +870,8 @@
                          can_order=False, max_num=None, fields=None, exclude=None,
                          widgets=None, validate_max=False, localized_fields=None,
                          labels=None, help_texts=None, error_messages=None,
-                         min_num=None, validate_min=False, field_classes=None):
+                         min_num=None, validate_min=False, field_classes=None,
+                         absolute_max=None, can_delete_extra=True):
     """Return a FormSet class for the given Django model class."""
     meta = getattr(form, 'Meta', None)
     if (getattr(meta, 'fields', fields) is None and
@@ -875,7 +888,8 @@
                              error_messages=error_messages, field_classes=field_classes)
     FormSet = formset_factory(form, formset, extra=extra, min_num=min_num, max_num=max_num,
                               can_order=can_order, can_delete=can_delete,
-                              validate_min=validate_min, validate_max=validate_max)
+                              validate_min=validate_min, validate_max=validate_max,
+                              absolute_max=absolute_max, can_delete_extra=can_delete_extra)
     FormSet.model = model
     return FormSet

@@ -1029,7 +1043,8 @@
             )
         else:
             raise ValueError(
-                "'%s' has more than one ForeignKey to '%s'." % (
+                "'%s' has more than one ForeignKey to '%s'. You must specify "
+                "a 'fk_name' attribute." % (
                     model._meta.label,
                     parent_model._meta.label,
                 )
@@ -1043,7 +1058,8 @@
                           can_delete=True, max_num=None, formfield_callback=None,
                           widgets=None, validate_max=False, localized_fields=None,
                           labels=None, help_texts=None, error_messages=None,
-                          min_num=None, validate_min=False, field_classes=None):
+                          min_num=None, validate_min=False, field_classes=None,
+                          absolute_max=None, can_delete_extra=True):
     """
     Return an ``InlineFormSet`` for the given kwargs.

@@ -1073,6 +1089,8 @@
         'help_texts': help_texts,
         'error_messages': error_messages,
         'field_classes': field_classes,
+        'absolute_max': absolute_max,
+        'can_delete_extra': can_delete_extra,
     }
     FormSet = modelformset_factory(model, **kwargs)
     FormSet.fk = fk
@@ -1122,6 +1140,20 @@
         return False


+class ModelChoiceIteratorValue:
+    def __init__(self, value, instance):
+        self.value = value
+        self.instance = instance
+
+    def __str__(self):
+        return str(self.value)
+
+    def __eq__(self, other):
+        if isinstance(other, ModelChoiceIteratorValue):
+            other = other.value
+        return self.value == other
+
+
 class ModelChoiceIterator:
     def __init__(self, field):
         self.field = field
@@ -1147,7 +1179,10 @@
         return self.field.empty_label is not None or self.queryset.exists()

     def choice(self, obj):
-        return (self.field.prepare_value(obj), self.field.label_from_instance(obj))
+        return (
+            ModelChoiceIteratorValue(self.field.prepare_value(obj), obj),
+            self.field.label_from_instance(obj),
+        )


 class ModelChoiceField(ChoiceField):
@@ -1163,18 +1198,20 @@
     def __init__(self, queryset, *, empty_label="---------",
                  required=True, widget=None, label=None, initial=None,
                  help_text='', to_field_name=None, limit_choices_to=None,
-                 **kwargs):
-        if required and (initial is not None):
-            self.empty_label = None
-        else:
-            self.empty_label = empty_label
-
+                 blank=False, **kwargs):
         # Call Field instead of ChoiceField __init__() because we don't need
         # ChoiceField.__init__().
         Field.__init__(
             self, required=required, widget=widget, label=label,
             initial=initial, help_text=help_text, **kwargs
         )
+        if (
+            (required and initial is not None) or
+            (isinstance(self.widget, RadioSelect) and not blank)
+        ):
+            self.empty_label = None
+        else:
+            self.empty_label = empty_label
         self.queryset = queryset
         self.limit_choices_to = limit_choices_to   # limit the queryset later.
         self.to_field_name = to_field_name
@@ -1245,6 +1282,8 @@
             return None
         try:
             key = self.to_field_name or 'pk'
+            if isinstance(value, self.queryset.model):
+                value = getattr(value, key)
             value = self.queryset.get(**{key: value})
         except (ValueError, TypeError, self.queryset.model.DoesNotExist):
             raise ValidationError(self.error_messages['invalid_choice'], code='invalid_choice')
@@ -1266,14 +1305,21 @@
     widget = SelectMultiple
     hidden_widget = MultipleHiddenInput
     default_error_messages = {
-        'list': _('Enter a list of values.'),
+        'invalid_list': _('Enter a list of values.'),
         'invalid_choice': _('Select a valid choice. %(value)s is not one of the'
                             ' available choices.'),
-        'invalid_pk_value': _('"%(pk)s" is not a valid value.')
+        'invalid_pk_value': _('“%(pk)s” is not a valid value.')
     }

     def __init__(self, queryset, **kwargs):
         super().__init__(queryset, empty_label=None, **kwargs)
+        if self.error_messages.get('list') is not None:
+            warnings.warn(
+                "The 'list' error message key is deprecated in favor of "
+                "'invalid_list'.",
+                RemovedInDjango40Warning, stacklevel=2,
+            )
+            self.error_messages['invalid_list'] = self.error_messages['list']

     def to_python(self, value):
         if not value:
@@ -1287,7 +1333,10 @@
         elif not self.required and not value:
             return self.queryset.none()
         if not isinstance(value, (list, tuple)):
-            raise ValidationError(self.error_messages['list'], code='list')
+            raise ValidationError(
+                self.error_messages['invalid_list'],
+                code='invalid_list',
+            )
         qs = self._check_values(value)
         # Since this overrides the inherited ModelChoiceField.clean
         # we run custom validators here
@@ -1308,8 +1357,8 @@
         except TypeError:
             # list of lists isn't hashable, for example
             raise ValidationError(
-                self.error_messages['list'],
-                code='list',
+                self.error_messages['invalid_list'],
+                code='invalid_list',
             )
         for pk in value:
             try:
('django/forms', 'fields.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,6 +4,7 @@

 import copy
 import datetime
+import json
 import math
 import operator
 import os
@@ -15,21 +16,20 @@

 from django.core import validators
 from django.core.exceptions import ValidationError
-# Provide this import for backwards compatibility.
-from django.core.validators import EMPTY_VALUES  # NOQA
 from django.forms.boundfield import BoundField
 from django.forms.utils import from_current_timezone, to_current_timezone
 from django.forms.widgets import (
     FILE_INPUT_CONTRADICTION, CheckboxInput, ClearableFileInput, DateInput,
     DateTimeInput, EmailInput, FileInput, HiddenInput, MultipleHiddenInput,
     NullBooleanSelect, NumberInput, Select, SelectMultiple,
-    SplitDateTimeWidget, SplitHiddenDateTimeWidget, TextInput, TimeInput,
-    URLInput,
+    SplitDateTimeWidget, SplitHiddenDateTimeWidget, Textarea, TextInput,
+    TimeInput, URLInput,
 )
 from django.utils import formats
-from django.utils.dateparse import parse_duration
+from django.utils.dateparse import parse_datetime, parse_duration
 from django.utils.duration import duration_string
 from django.utils.ipv6 import clean_ipv6_address
+from django.utils.regex_helper import _lazy_re_compile
 from django.utils.translation import gettext_lazy as _, ngettext_lazy

 __all__ = (
@@ -39,7 +39,8 @@
     'BooleanField', 'NullBooleanField', 'ChoiceField', 'MultipleChoiceField',
     'ComboField', 'MultiValueField', 'FloatField', 'DecimalField',
     'SplitDateTimeField', 'GenericIPAddressField', 'FilePathField',
-    'SlugField', 'TypedChoiceField', 'TypedMultipleChoiceField', 'UUIDField',
+    'JSONField', 'SlugField', 'TypedChoiceField', 'TypedMultipleChoiceField',
+    'UUIDField',
 )


@@ -201,6 +202,7 @@
         result = copy.copy(self)
         memo[id(self)] = result
         result.widget = copy.deepcopy(self.widget, memo)
+        result.error_messages = self.error_messages.copy()
         result.validators = self.validators[:]
         return result

@@ -244,7 +246,7 @@
     default_error_messages = {
         'invalid': _('Enter a whole number.'),
     }
-    re_decimal = re.compile(r'\.0*\s*$')
+    re_decimal = _lazy_re_compile(r'\.0*\s*$')

     def __init__(self, *, max_value=None, min_value=None, **kwargs):
         self.max_value, self.min_value = max_value, min_value
@@ -353,7 +355,11 @@
         if value in self.empty_values:
             return
         if not value.is_finite():
-            raise ValidationError(self.error_messages['invalid'], code='invalid')
+            raise ValidationError(
+                self.error_messages['invalid'],
+                code='invalid',
+                params={'value': value},
+            )

     def widget_attrs(self, widget):
         attrs = super().widget_attrs(widget)
@@ -435,9 +441,15 @@
         return datetime.datetime.strptime(value, format).time()


+class DateTimeFormatsIterator:
+    def __iter__(self):
+        yield from formats.get_format('DATETIME_INPUT_FORMATS')
+        yield from formats.get_format('DATE_INPUT_FORMATS')
+
+
 class DateTimeField(BaseTemporalField):
     widget = DateTimeInput
-    input_formats = formats.get_format_lazy('DATETIME_INPUT_FORMATS')
+    input_formats = DateTimeFormatsIterator()
     default_error_messages = {
         'invalid': _('Enter a valid date/time.'),
     }
@@ -459,7 +471,12 @@
         if isinstance(value, datetime.date):
             result = datetime.datetime(value.year, value.month, value.day)
             return from_current_timezone(result)
-        result = super().to_python(value)
+        try:
+            result = parse_datetime(value.strip())
+        except ValueError:
+            raise ValidationError(self.error_messages['invalid'], code='invalid')
+        if not result:
+            result = super().to_python(value)
         return from_current_timezone(result)

     def strptime(self, value, format):
@@ -1200,3 +1217,66 @@
             except ValueError:
                 raise ValidationError(self.error_messages['invalid'], code='invalid')
         return value
+
+
+class InvalidJSONInput(str):
+    pass
+
+
+class JSONString(str):
+    pass
+
+
+class JSONField(CharField):
+    default_error_messages = {
+        'invalid': _('Enter a valid JSON.'),
+    }
+    widget = Textarea
+
+    def __init__(self, encoder=None, decoder=None, **kwargs):
+        self.encoder = encoder
+        self.decoder = decoder
+        super().__init__(**kwargs)
+
+    def to_python(self, value):
+        if self.disabled:
+            return value
+        if value in self.empty_values:
+            return None
+        elif isinstance(value, (list, dict, int, float, JSONString)):
+            return value
+        try:
+            converted = json.loads(value, cls=self.decoder)
+        except json.JSONDecodeError:
+            raise ValidationError(
+                self.error_messages['invalid'],
+                code='invalid',
+                params={'value': value},
+            )
+        if isinstance(converted, str):
+            return JSONString(converted)
+        else:
+            return converted
+
+    def bound_data(self, data, initial):
+        if self.disabled:
+            return initial
+        try:
+            return json.loads(data, cls=self.decoder)
+        except json.JSONDecodeError:
+            return InvalidJSONInput(data)
+
+    def prepare_value(self, value):
+        if isinstance(value, InvalidJSONInput):
+            return value
+        return json.dumps(value, ensure_ascii=False, cls=self.encoder)
+
+    def has_changed(self, initial, data):
+        if super().has_changed(initial, data):
+            return True
+        # For purposes of seeing whether something has changed, True isn't the
+        # same as 1 and the order of keys doesn't matter.
+        return (
+            json.dumps(initial, sort_keys=True, cls=self.encoder) !=
+            json.dumps(self.to_python(data), sort_keys=True, cls=self.encoder)
+        )
('django/forms', 'boundfield.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,4 +1,5 @@
 import datetime
+import re

 from django.forms.utils import flatatt, pretty_name
 from django.forms.widgets import Textarea, TextInput
@@ -63,7 +64,10 @@
         # Prevent unnecessary reevaluation when accessing BoundField's attrs
         # from templates.
         if not isinstance(idx, (int, slice)):
-            raise TypeError
+            raise TypeError(
+                'BoundField indices must be integers or slices, not %s.'
+                % type(idx).__name__
+            )
         return self.subwidgets[idx]

     @property
@@ -224,6 +228,10 @@
             attrs['disabled'] = True
         return attrs

+    @property
+    def widget_type(self):
+        return re.sub(r'widget$|input$', '', self.field.widget.__class__.__name__.lower())
+

 @html_safe
 class BoundWidget:
('django/forms', 'renderers.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -6,12 +6,6 @@
 from django.template.loader import get_template
 from django.utils.functional import cached_property
 from django.utils.module_loading import import_string
-
-try:
-    from django.template.backends.jinja2 import Jinja2
-except ImportError:
-    def Jinja2(params):
-        raise ImportError("jinja2 isn't installed")

 ROOT = Path(__file__).parent

@@ -39,7 +33,7 @@
     def engine(self):
         return self.backend({
             'APP_DIRS': True,
-            'DIRS': [str(ROOT / self.backend.app_dirname)],
+            'DIRS': [ROOT / self.backend.app_dirname],
             'NAME': 'djangoforms',
             'OPTIONS': {},
         })
@@ -58,7 +52,10 @@
     Load Jinja2 templates from the built-in widget templates in
     django/forms/jinja2 and from apps' 'jinja2' directory.
     """
-    backend = Jinja2
+    @cached_property
+    def backend(self):
+        from django.template.backends.jinja2 import Jinja2
+        return Jinja2


 class TemplatesSetting(BaseRenderer):
('django/forms', 'widgets.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,12 +4,10 @@

 import copy
 import datetime
-import re
 import warnings
 from collections import defaultdict
 from itertools import chain

-from django.conf import settings
 from django.forms.utils import to_current_timezone
 from django.templatetags.static import static
 from django.utils import datetime_safe, formats
@@ -17,6 +15,7 @@
 from django.utils.dates import MONTHS
 from django.utils.formats import get_format
 from django.utils.html import format_html, html_safe
+from django.utils.regex_helper import _lazy_re_compile
 from django.utils.safestring import mark_safe
 from django.utils.topological_sort import (
     CyclicDependencyError, stable_topological_sort,
@@ -80,7 +79,7 @@
     def render_js(self):
         return [
             format_html(
-                '<script type="text/javascript" src="{}"></script>',
+                '<script src="{}"></script>',
                 self.absolute_path(path)
             ) for path in self._js
         ]
@@ -140,15 +139,21 @@
         except CyclicDependencyError:
             warnings.warn(
                 'Detected duplicate Media files in an opposite order: {}'.format(
-                    ', '.join(repr(l) for l in lists)
+                    ', '.join(repr(list_) for list_ in lists)
                 ), MediaOrderConflictWarning,
             )
             return list(all_items)

     def __add__(self, other):
         combined = Media()
-        combined._css_lists = self._css_lists + other._css_lists
-        combined._js_lists = self._js_lists + other._js_lists
+        combined._css_lists = self._css_lists[:]
+        combined._js_lists = self._js_lists[:]
+        for item in other._css_lists:
+            if item and item not in self._css_lists:
+                combined._css_lists.append(item)
+        for item in other._js_lists:
+            if item and item not in self._js_lists:
+                combined._js_lists.append(item)
         return combined


@@ -183,7 +188,7 @@
     Metaclass for classes that can have media definitions.
     """
     def __new__(mcs, name, bases, attrs):
-        new_class = super(MediaDefiningClass, mcs).__new__(mcs, name, bases, attrs)
+        new_class = super().__new__(mcs, name, bases, attrs)

         if 'media' not in attrs:
             new_class.media = media_property(new_class)
@@ -225,16 +230,16 @@
         return str(value)

     def get_context(self, name, value, attrs):
-        context = {}
-        context['widget'] = {
-            'name': name,
-            'is_hidden': self.is_hidden,
-            'required': self.is_required,
-            'value': self.format_value(value),
-            'attrs': self.build_attrs(self.attrs, attrs),
-            'template_name': self.template_name,
+        return {
+            'widget': {
+                'name': name,
+                'is_hidden': self.is_hidden,
+                'required': self.is_required,
+                'value': self.format_value(value),
+                'attrs': self.build_attrs(self.attrs, attrs),
+                'template_name': self.template_name,
+            },
         }
-        return context

     def render(self, name, value, attrs=None, renderer=None):
         """Render the widget as an HTML string."""
@@ -386,6 +391,9 @@

     def value_omitted_from_data(self, data, files, name):
         return name not in files
+
+    def use_required_attribute(self, initial):
+        return super().use_required_attribute(initial) and not initial


 FILE_INPUT_CONTRADICTION = object()
@@ -451,9 +459,6 @@
             return False
         return upload

-    def use_required_attribute(self, initial):
-        return super().use_required_attribute(initial) and not initial
-
     def value_omitted_from_data(self, data, files, name):
         return (
             super().value_omitted_from_data(data, files, name) and
@@ -522,9 +527,7 @@

     def get_context(self, name, value, attrs):
         if self.check_test(value):
-            if attrs is None:
-                attrs = {}
-            attrs['checked'] = True
+            attrs = {**(attrs or {}), 'checked': True}
         return super().get_context(name, value, attrs)

     def value_from_datadict(self, data, files, name):
@@ -779,7 +782,7 @@
         return False

     def id_for_label(self, id_, index=None):
-        """"
+        """
         Don't include for="field_0" in <label> because clicking such a label
         would toggle the first checkbox.
         """
@@ -801,6 +804,13 @@
     template_name = 'django/forms/widgets/multiwidget.html'

     def __init__(self, widgets, attrs=None):
+        if isinstance(widgets, dict):
+            self.widgets_names = [
+                ('_%s' % name) if name else '' for name in widgets
+            ]
+            widgets = widgets.values()
+        else:
+            self.widgets_names = ['_%s' % i for i in range(len(widgets))]
         self.widgets = [w() if isinstance(w, type) else w for w in widgets]
         super().__init__(attrs)

@@ -822,10 +832,10 @@
         input_type = final_attrs.pop('type', None)
         id_ = final_attrs.get('id')
         subwidgets = []
-        for i, widget in enumerate(self.widgets):
+        for i, (widget_name, widget) in enumerate(zip(self.widgets_names, self.widgets)):
             if input_type is not None:
                 widget.input_type = input_type
-            widget_name = '%s_%s' % (name, i)
+            widget_name = name + widget_name
             try:
                 widget_value = value[i]
             except IndexError:
@@ -845,12 +855,15 @@
         return id_

     def value_from_datadict(self, data, files, name):
-        return [widget.value_from_datadict(data, files, name + '_%s' % i) for i, widget in enumerate(self.widgets)]
+        return [
+            widget.value_from_datadict(data, files, name + widget_name)
+            for widget_name, widget in zip(self.widgets_names, self.widgets)
+        ]

     def value_omitted_from_data(self, data, files, name):
         return all(
-            widget.value_omitted_from_data(data, files, name + '_%s' % i)
-            for i, widget in enumerate(self.widgets)
+            widget.value_omitted_from_data(data, files, name + widget_name)
+            for widget_name, widget in zip(self.widgets_names, self.widgets)
         )

     def decompress(self, value):
@@ -935,7 +948,7 @@
     template_name = 'django/forms/widgets/select_date.html'
     input_type = 'select'
     select_widget = Select
-    date_re = re.compile(r'(\d{4}|0)-(\d\d?)-(\d\d?)$')
+    date_re = _lazy_re_compile(r'(\d{4}|0)-(\d\d?)-(\d\d?)$')

     def __init__(self, attrs=None, years=None, months=None, empty_label=None):
         self.attrs = attrs or {}
@@ -1020,7 +1033,7 @@
                 # Convert any zeros in the date to empty strings to match the
                 # empty option value.
                 year, month, day = [int(val) or '' for val in match.groups()]
-            elif settings.USE_L10N:
+            else:
                 input_format = get_format('DATE_INPUT_FORMATS')[0]
                 try:
                     d = datetime.datetime.strptime(value, input_format)
@@ -1058,18 +1071,15 @@
         if y == m == d == '':
             return None
         if y is not None and m is not None and d is not None:
-            if settings.USE_L10N:
-                input_format = get_format('DATE_INPUT_FORMATS')[0]
-                try:
-                    date_value = datetime.date(int(y), int(m), int(d))
-                except ValueError:
-                    pass
-                else:
-                    date_value = datetime_safe.new_date(date_value)
-                    return date_value.strftime(input_format)
-            # Return pseudo-ISO dates with zeros for any unselected values,
-            # e.g. '2017-0-23'.
-            return '%s-%s-%s' % (y or 0, m or 0, d or 0)
+            input_format = get_format('DATE_INPUT_FORMATS')[0]
+            try:
+                date_value = datetime.date(int(y), int(m), int(d))
+            except ValueError:
+                # Return pseudo-ISO dates with zeros for any unselected values,
+                # e.g. '2017-0-23'.
+                return '%s-%s-%s' % (y or 0, m or 0, d or 0)
+            date_value = datetime_safe.new_date(date_value)
+            return date_value.strftime(input_format)
         return data.get(name)

     def value_omitted_from_data(self, data, files, name):
('django/forms', 'formsets.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,11 +2,11 @@
 from django.forms import Form
 from django.forms.fields import BooleanField, IntegerField
 from django.forms.utils import ErrorList
-from django.forms.widgets import HiddenInput
+from django.forms.widgets import HiddenInput, NumberInput
 from django.utils.functional import cached_property
 from django.utils.html import html_safe
 from django.utils.safestring import mark_safe
-from django.utils.translation import gettext as _, ngettext
+from django.utils.translation import gettext_lazy as _, ngettext

 __all__ = ('BaseFormSet', 'formset_factory', 'all_valid')

@@ -41,14 +41,31 @@
         self.base_fields[MAX_NUM_FORM_COUNT] = IntegerField(required=False, widget=HiddenInput)
         super().__init__(*args, **kwargs)

+    def clean(self):
+        cleaned_data = super().clean()
+        # When the management form is invalid, we don't know how many forms
+        # were submitted.
+        cleaned_data.setdefault(TOTAL_FORM_COUNT, 0)
+        cleaned_data.setdefault(INITIAL_FORM_COUNT, 0)
+        return cleaned_data
+

 @html_safe
 class BaseFormSet:
     """
     A collection of instances of the same Form class.
     """
+    ordering_widget = NumberInput
+    default_error_messages = {
+        'missing_management_form': _(
+            'ManagementForm data is missing or has been tampered with. Missing fields: '
+            '%(field_names)s. You may need to file a bug report if the issue persists.'
+        ),
+    }
+
     def __init__(self, data=None, files=None, auto_id='id_%s', prefix=None,
-                 initial=None, error_class=ErrorList, form_kwargs=None):
+                 initial=None, error_class=ErrorList, form_kwargs=None,
+                 error_messages=None):
         self.is_bound = data is not None or files is not None
         self.prefix = prefix or self.get_default_prefix()
         self.auto_id = auto_id
@@ -60,6 +77,13 @@
         self._errors = None
         self._non_form_errors = None

+        messages = {}
+        for cls in reversed(type(self).__mro__):
+            messages.update(getattr(cls, 'default_error_messages', {}))
+        if error_messages is not None:
+            messages.update(error_messages)
+        self.error_messages = messages
+
     def __str__(self):
         return self.as_table()

@@ -86,11 +110,7 @@
         """Return the ManagementForm instance for this FormSet."""
         if self.is_bound:
             form = ManagementForm(self.data, auto_id=self.auto_id, prefix=self.prefix)
-            if not form.is_valid():
-                raise ValidationError(
-                    _('ManagementForm data is missing or has been tampered with'),
-                    code='missing_management_form',
-                )
+            form.full_clean()
         else:
             form = ManagementForm(auto_id=self.auto_id, prefix=self.prefix, initial={
                 TOTAL_FORM_COUNT: self.total_form_count(),
@@ -132,9 +152,10 @@
     def forms(self):
         """Instantiate forms at first property access."""
         # DoS protection is included in total_form_count()
-        forms = [self._construct_form(i, **self.get_form_kwargs(i))
-                 for i in range(self.total_form_count())]
-        return forms
+        return [
+            self._construct_form(i, **self.get_form_kwargs(i))
+            for i in range(self.total_form_count())
+        ]

     def get_form_kwargs(self, index):
         """
@@ -213,8 +234,7 @@
         # that have had their deletion widget set to True
         if not hasattr(self, '_deleted_form_indexes'):
             self._deleted_form_indexes = []
-            for i in range(0, self.total_form_count()):
-                form = self.forms[i]
+            for i, form in enumerate(self.forms):
                 # if this is an extra form and hasn't changed, don't consider it
                 if i >= self.initial_form_count() and not form.has_changed():
                     continue
@@ -236,8 +256,7 @@
         # by the form data.
         if not hasattr(self, '_ordering'):
             self._ordering = []
-            for i in range(0, self.total_form_count()):
-                form = self.forms[i]
+            for i, form in enumerate(self.forms):
                 # if this is an extra form and hasn't changed, don't consider it
                 if i >= self.initial_form_count() and not form.has_changed():
                     continue
@@ -264,6 +283,10 @@
     def get_default_prefix(cls):
         return 'form'

+    @classmethod
+    def get_ordering_widget(cls):
+        return cls.ordering_widget
+
     def non_form_errors(self):
         """
         Return an ErrorList of errors that aren't associated with a particular
@@ -294,18 +317,14 @@
         """Return True if every form in self.forms is valid."""
         if not self.is_bound:
             return False
-        # We loop over every form.errors here rather than short circuiting on the
-        # first failure to make sure validation gets triggered for every form.
-        forms_valid = True
-        # This triggers a full clean.
+        # Accessing errors triggers a full clean the first time only.
         self.errors
-        for i in range(0, self.total_form_count()):
-            form = self.forms[i]
-            if self.can_delete and self._should_delete_form(form):
-                # This form is going to be deleted so any of its errors
-                # shouldn't cause the entire formset to be invalid.
-                continue
-            forms_valid &= form.is_valid()
+        # List comprehension ensures is_valid() is called for all forms.
+        # Forms due to be deleted shouldn't cause the formset to be invalid.
+        forms_valid = all([
+            form.is_valid() for form in self.forms
+            if not (self.can_delete and self._should_delete_form(form))
+        ])
         return forms_valid and not self.non_form_errors()

     def full_clean(self):
@@ -319,8 +338,21 @@

         if not self.is_bound:  # Stop further processing.
             return
-        for i in range(0, self.total_form_count()):
-            form = self.forms[i]
+
+        if not self.management_form.is_valid():
+            error = ValidationError(
+                self.error_messages['missing_management_form'],
+                params={
+                    'field_names': ', '.join(
+                        self.management_form.add_prefix(field_name)
+                        for field_name in self.management_form.errors
+                    ),
+                },
+                code='missing_management_form',
+            )
+            self._non_form_errors.append(error)
+
+        for i, form in enumerate(self.forms):
             # Empty forms are unchanged forms beyond those with initial data.
             if not form.has_changed() and i >= self.initial_form_count():
                 empty_forms_count += 1
@@ -335,15 +367,15 @@
                     self.total_form_count() - len(self.deleted_forms) > self.max_num) or \
                     self.management_form.cleaned_data[TOTAL_FORM_COUNT] > self.absolute_max:
                 raise ValidationError(ngettext(
-                    "Please submit %d or fewer forms.",
-                    "Please submit %d or fewer forms.", self.max_num) % self.max_num,
+                    "Please submit at most %d form.",
+                    "Please submit at most %d forms.", self.max_num) % self.max_num,
                     code='too_many_forms',
                 )
             if (self.validate_min and
                     self.total_form_count() - len(self.deleted_forms) - empty_forms_count < self.min_num):
                 raise ValidationError(ngettext(
-                    "Please submit %d or more forms.",
-                    "Please submit %d or more forms.", self.min_num) % self.min_num,
+                    "Please submit at least %d form.",
+                    "Please submit at least %d forms.", self.min_num) % self.min_num,
                     code='too_few_forms')
             # Give self.clean() a chance to do cross-form validation.
             self.clean()
@@ -365,13 +397,23 @@

     def add_fields(self, form, index):
         """A hook for adding extra fields on to each form instance."""
+        initial_form_count = self.initial_form_count()
         if self.can_order:
             # Only pre-fill the ordering field for initial forms.
-            if index is not None and index < self.initial_form_count():
-                form.fields[ORDERING_FIELD_NAME] = IntegerField(label=_('Order'), initial=index + 1, required=False)
+            if index is not None and index < initial_form_count:
+                form.fields[ORDERING_FIELD_NAME] = IntegerField(
+                    label=_('Order'),
+                    initial=index + 1,
+                    required=False,
+                    widget=self.get_ordering_widget(),
+                )
             else:
-                form.fields[ORDERING_FIELD_NAME] = IntegerField(label=_('Order'), required=False)
-        if self.can_delete:
+                form.fields[ORDERING_FIELD_NAME] = IntegerField(
+                    label=_('Order'),
+                    required=False,
+                    widget=self.get_ordering_widget(),
+                )
+        if self.can_delete and (self.can_delete_extra or index < initial_form_count):
             form.fields[DELETION_FIELD_NAME] = BooleanField(label=_('Delete'), required=False)

     def add_prefix(self, index):
@@ -417,21 +459,28 @@

 def formset_factory(form, formset=BaseFormSet, extra=1, can_order=False,
                     can_delete=False, max_num=None, validate_max=False,
-                    min_num=None, validate_min=False):
+                    min_num=None, validate_min=False, absolute_max=None,
+                    can_delete_extra=True):
     """Return a FormSet for the given form class."""
     if min_num is None:
         min_num = DEFAULT_MIN_NUM
     if max_num is None:
         max_num = DEFAULT_MAX_NUM
-    # hard limit on forms instantiated, to prevent memory-exhaustion attacks
-    # limit is simply max_num + DEFAULT_MAX_NUM (which is 2*DEFAULT_MAX_NUM
-    # if max_num is None in the first place)
-    absolute_max = max_num + DEFAULT_MAX_NUM
+    # absolute_max is a hard limit on forms instantiated, to prevent
+    # memory-exhaustion attacks. Default to max_num + DEFAULT_MAX_NUM
+    # (which is 2 * DEFAULT_MAX_NUM if max_num is None in the first place).
+    if absolute_max is None:
+        absolute_max = max_num + DEFAULT_MAX_NUM
+    if max_num > absolute_max:
+        raise ValueError(
+            "'absolute_max' must be greater or equal to 'max_num'."
+        )
     attrs = {
         'form': form,
         'extra': extra,
         'can_order': can_order,
         'can_delete': can_delete,
+        'can_delete_extra': can_delete_extra,
         'min_num': min_num,
         'max_num': max_num,
         'absolute_max': absolute_max,
@@ -443,7 +492,5 @@

 def all_valid(formsets):
     """Validate every formset and return True if all are valid."""
-    valid = True
-    for formset in formsets:
-        valid &= formset.is_valid()
-    return valid
+    # List comprehension ensures is_valid() is called for all formsets.
+    return all([formset.is_valid() for formset in formsets])
('django/forms', 'forms.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,15 +3,12 @@
 """

 import copy
-from collections import OrderedDict

 from django.core.exceptions import NON_FIELD_ERRORS, ValidationError
-# BoundField is imported for backwards compatibility in Django 1.9
-from django.forms.boundfield import BoundField  # NOQA
 from django.forms.fields import Field, FileField
-# pretty_name is imported for backwards compatibility in Django 1.9
-from django.forms.utils import ErrorDict, ErrorList, pretty_name  # NOQA
+from django.forms.utils import ErrorDict, ErrorList
 from django.forms.widgets import Media, MediaDefiningClass
+from django.utils.datastructures import MultiValueDict
 from django.utils.functional import cached_property
 from django.utils.html import conditional_escape, html_safe
 from django.utils.safestring import mark_safe
@@ -25,18 +22,16 @@
 class DeclarativeFieldsMetaclass(MediaDefiningClass):
     """Collect Fields declared on the base classes."""
     def __new__(mcs, name, bases, attrs):
-        # Collect fields from current class.
-        current_fields = []
-        for key, value in list(attrs.items()):
-            if isinstance(value, Field):
-                current_fields.append((key, value))
-                attrs.pop(key)
-        attrs['declared_fields'] = OrderedDict(current_fields)
-
-        new_class = super(DeclarativeFieldsMetaclass, mcs).__new__(mcs, name, bases, attrs)
+        # Collect fields from current class and remove them from attrs.
+        attrs['declared_fields'] = {
+            key: attrs.pop(key) for key, value in list(attrs.items())
+            if isinstance(value, Field)
+        }
+
+        new_class = super().__new__(mcs, name, bases, attrs)

         # Walk through the MRO.
-        declared_fields = OrderedDict()
+        declared_fields = {}
         for base in reversed(new_class.__mro__):
             # Collect fields from base class.
             if hasattr(base, 'declared_fields'):
@@ -51,11 +46,6 @@
         new_class.declared_fields = declared_fields

         return new_class
-
-    @classmethod
-    def __prepare__(metacls, name, bases, **kwds):
-        # Remember the order in which form fields are defined.
-        return OrderedDict()


 @html_safe
@@ -75,8 +65,8 @@
                  initial=None, error_class=ErrorList, label_suffix=None,
                  empty_permitted=False, field_order=None, use_required_attribute=None, renderer=None):
         self.is_bound = data is not None or files is not None
-        self.data = {} if data is None else data
-        self.files = {} if files is None else files
+        self.data = MultiValueDict() if data is None else data
+        self.files = MultiValueDict() if files is None else files
         self.auto_id = auto_id
         if prefix is not None:
             self.prefix = prefix
@@ -129,7 +119,7 @@
         """
         if field_order is None:
             return
-        fields = OrderedDict()
+        fields = {}
         for key in field_order:
             try:
                 fields[key] = self.fields.pop(key)
@@ -166,7 +156,7 @@
                 "Key '%s' not found in '%s'. Choices are: %s." % (
                     name,
                     self.__class__.__name__,
-                    ', '.join(sorted(f for f in self.fields)),
+                    ', '.join(sorted(self.fields)),
                 )
             )
         if name not in self._bound_fields_cache:
@@ -194,12 +184,13 @@
         return '%s-%s' % (self.prefix, field_name) if self.prefix else field_name

     def add_initial_prefix(self, field_name):
-        """Add a 'initial' prefix for checking dynamic initial values."""
+        """Add an 'initial' prefix for checking dynamic initial values."""
         return 'initial-%s' % self.add_prefix(field_name)

     def _html_output(self, normal_row, error_row, row_ender, help_text_html, errors_on_separate_row):
         "Output HTML. Used by as_table(), as_ul(), as_p()."
-        top_errors = self.non_field_errors()  # Errors that should be displayed above all fields.
+        # Errors that should be displayed above all fields.
+        top_errors = self.non_field_errors().copy()
         output, hidden_fields = [], []

         for name, field in self.fields.items():
('django/forms', 'utils.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,7 +2,7 @@
 from collections import UserList

 from django.conf import settings
-from django.core.exceptions import ValidationError  # backwards compatibility
+from django.core.exceptions import ValidationError
 from django.utils import timezone
 from django.utils.html import escape, format_html, format_html_join, html_safe
 from django.utils.translation import gettext_lazy as _
@@ -92,6 +92,11 @@
     def as_data(self):
         return ValidationError(self.data).error_list

+    def copy(self):
+        copy = super().copy()
+        copy.error_class = self.error_class
+        return copy
+
     def get_json_data(self, escape_html=False):
         errors = []
         for error in self.as_data():
@@ -156,10 +161,15 @@
     if settings.USE_TZ and value is not None and timezone.is_naive(value):
         current_timezone = timezone.get_current_timezone()
         try:
+            if (
+                not timezone._is_pytz_zone(current_timezone) and
+                timezone._datetime_ambiguous_or_imaginary(value, current_timezone)
+            ):
+                raise ValueError('Ambiguous or non-existent time.')
             return timezone.make_aware(value, current_timezone)
         except Exception as exc:
             raise ValidationError(
-                _('%(datetime)s couldn\'t be interpreted '
+                _('%(datetime)s couldn’t be interpreted '
                   'in time zone %(current_timezone)s; it '
                   'may be ambiguous or it may not exist.'),
                 code='ambiguous_timezone',
('django/core', 'signing.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -36,7 +36,6 @@
 import base64
 import datetime
 import json
-import re
 import time
 import zlib

@@ -45,8 +44,9 @@
 from django.utils.crypto import constant_time_compare, salted_hmac
 from django.utils.encoding import force_bytes
 from django.utils.module_loading import import_string
-
-_SEP_UNSAFE = re.compile(r'^[A-z0-9-_=]*$')
+from django.utils.regex_helper import _lazy_re_compile
+
+_SEP_UNSAFE = _lazy_re_compile(r'^[A-z0-9-_=]*$')


 class BadSignature(Exception):
@@ -68,8 +68,8 @@
     return base64.urlsafe_b64decode(s + pad)


-def base64_hmac(salt, value, key):
-    return b64_encode(salted_hmac(salt, value, key).digest()).decode()
+def base64_hmac(salt, value, key, algorithm='sha1'):
+    return b64_encode(salted_hmac(salt, value, key, algorithm=algorithm).digest()).decode()


 def get_cookie_signer(salt='django.core.signing.get_cookie_signer'):
@@ -92,8 +92,9 @@

 def dumps(obj, key=None, salt='django.core.signing', serializer=JSONSerializer, compress=False):
     """
-    Return URL-safe, hmac/SHA1 signed base64 compressed JSON string. If key is
-    None, use settings.SECRET_KEY instead.
+    Return URL-safe, hmac signed base64 compressed JSON string. If key is
+    None, use settings.SECRET_KEY instead. The hmac algorithm is the default
+    Signer algorithm.

     If compress is True (not the default), check if compressing using zlib can
     save some space. Prepend a '.' to signify compression. This is included
@@ -106,21 +107,7 @@

     The serializer is expected to return a bytestring.
     """
-    data = serializer().dumps(obj)
-
-    # Flag for if it's been compressed or not
-    is_compressed = False
-
-    if compress:
-        # Avoid zlib dependency unless compress is being used
-        compressed = zlib.compress(data)
-        if len(compressed) < (len(data) - 1):
-            data = compressed
-            is_compressed = True
-    base64d = b64_encode(data).decode()
-    if is_compressed:
-        base64d = '.' + base64d
-    return TimestampSigner(key, salt=salt).sign(base64d)
+    return TimestampSigner(key, salt=salt).sign_object(obj, serializer=serializer, compress=compress)


 def loads(s, key=None, salt='django.core.signing', serializer=JSONSerializer, max_age=None):
@@ -129,23 +116,14 @@

     The serializer is expected to accept a bytestring.
     """
-    # TimestampSigner.unsign() returns str but base64 and zlib compression
-    # operate on bytes.
-    base64d = TimestampSigner(key, salt=salt).unsign(s, max_age=max_age).encode()
-    decompress = base64d[:1] == b'.'
-    if decompress:
-        # It's compressed; uncompress it first
-        base64d = base64d[1:]
-    data = b64_decode(base64d)
-    if decompress:
-        data = zlib.decompress(data)
-    return serializer().loads(data)
+    return TimestampSigner(key, salt=salt).unsign_object(s, serializer=serializer, max_age=max_age)


 class Signer:
-
-    def __init__(self, key=None, sep=':', salt=None):
-        # Use of native strings in all versions of Python
+    # RemovedInDjango40Warning.
+    legacy_algorithm = 'sha1'
+
+    def __init__(self, key=None, sep=':', salt=None, algorithm=None):
         self.key = key or settings.SECRET_KEY
         self.sep = sep
         if _SEP_UNSAFE.match(self.sep):
@@ -154,9 +132,16 @@
                 'only A-z0-9-_=)' % sep,
             )
         self.salt = salt or '%s.%s' % (self.__class__.__module__, self.__class__.__name__)
+        # RemovedInDjango40Warning: when the deprecation ends, replace with:
+        # self.algorithm = algorithm or 'sha256'
+        self.algorithm = algorithm or settings.DEFAULT_HASHING_ALGORITHM

     def signature(self, value):
-        return base64_hmac(self.salt + 'signer', value, self.key)
+        return base64_hmac(self.salt + 'signer', value, self.key, algorithm=self.algorithm)
+
+    def _legacy_signature(self, value):
+        # RemovedInDjango40Warning.
+        return base64_hmac(self.salt + 'signer', value, self.key, algorithm=self.legacy_algorithm)

     def sign(self, value):
         return '%s%s%s' % (value, self.sep, self.signature(value))
@@ -165,9 +150,52 @@
         if self.sep not in signed_value:
             raise BadSignature('No "%s" found in value' % self.sep)
         value, sig = signed_value.rsplit(self.sep, 1)
-        if constant_time_compare(sig, self.signature(value)):
+        if (
+            constant_time_compare(sig, self.signature(value)) or (
+                self.legacy_algorithm and
+                constant_time_compare(sig, self._legacy_signature(value))
+            )
+        ):
             return value
         raise BadSignature('Signature "%s" does not match' % sig)
+
+    def sign_object(self, obj, serializer=JSONSerializer, compress=False):
+        """
+        Return URL-safe, hmac signed base64 compressed JSON string.
+
+        If compress is True (not the default), check if compressing using zlib
+        can save some space. Prepend a '.' to signify compression. This is
+        included in the signature, to protect against zip bombs.
+
+        The serializer is expected to return a bytestring.
+        """
+        data = serializer().dumps(obj)
+        # Flag for if it's been compressed or not.
+        is_compressed = False
+
+        if compress:
+            # Avoid zlib dependency unless compress is being used.
+            compressed = zlib.compress(data)
+            if len(compressed) < (len(data) - 1):
+                data = compressed
+                is_compressed = True
+        base64d = b64_encode(data).decode()
+        if is_compressed:
+            base64d = '.' + base64d
+        return self.sign(base64d)
+
+    def unsign_object(self, signed_obj, serializer=JSONSerializer, **kwargs):
+        # Signer.unsign() returns str but base64 and zlib compression operate
+        # on bytes.
+        base64d = self.unsign(signed_obj, **kwargs).encode()
+        decompress = base64d[:1] == b'.'
+        if decompress:
+            # It's compressed; uncompress it first.
+            base64d = base64d[1:]
+        data = b64_decode(base64d)
+        if decompress:
+            data = zlib.decompress(data)
+        return serializer().loads(data)


 class TimestampSigner(Signer):
('django/core', 'signals.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,6 @@
 from django.dispatch import Signal

-request_started = Signal(providing_args=["environ"])
+request_started = Signal()
 request_finished = Signal()
-got_request_exception = Signal(providing_args=["request"])
-setting_changed = Signal(providing_args=["setting", "value", "enter"])
+got_request_exception = Signal()
+setting_changed = Signal()
('django/core', 'validators.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,28 +1,19 @@
 import ipaddress
 import re
+import warnings
 from pathlib import Path
 from urllib.parse import urlsplit, urlunsplit

 from django.core.exceptions import ValidationError
 from django.utils.deconstruct import deconstructible
-from django.utils.functional import SimpleLazyObject
+from django.utils.deprecation import RemovedInDjango41Warning
+from django.utils.encoding import punycode
 from django.utils.ipv6 import is_valid_ipv6_address
+from django.utils.regex_helper import _lazy_re_compile
 from django.utils.translation import gettext_lazy as _, ngettext_lazy

 # These values, if given to validate(), will trigger the self.required check.
 EMPTY_VALUES = (None, '', [], (), {})
-
-
-def _lazy_re_compile(regex, flags=0):
-    """Lazily compile a regex with flags."""
-    def _compile():
-        # Compile the regex if it was not passed pre-compiled.
-        if isinstance(regex, str):
-            return re.compile(regex, flags)
-        else:
-            assert not flags, "flags must be empty if regex is passed pre-compiled"
-            return regex
-    return SimpleLazyObject(_compile)


 @deconstructible
@@ -57,7 +48,7 @@
         regex_matches = self.regex.search(str(value))
         invalid_input = regex_matches if self.inverse_match else not regex_matches
         if invalid_input:
-            raise ValidationError(self.message, code=self.code)
+            raise ValidationError(self.message, code=self.code, params={'value': value})

     def __eq__(self, other):
         return (
@@ -72,11 +63,11 @@

 @deconstructible
 class URLValidator(RegexValidator):
-    ul = '\u00a1-\uffff'  # unicode letters range (must not be a raw string)
+    ul = '\u00a1-\uffff'  # Unicode letters range (must not be a raw string).

     # IP patterns
-    ipv4_re = r'(?:25[0-5]|2[0-4]\d|[0-1]?\d?\d)(?:\.(?:25[0-5]|2[0-4]\d|[0-1]?\d?\d)){3}'
-    ipv6_re = r'\[[0-9a-f:\.]+\]'  # (simple regex, validated later)
+    ipv4_re = r'(?:0|25[0-5]|2[0-4]\d|1\d?\d?|[1-9]\d?)(?:\.(?:0|25[0-5]|2[0-4]\d|1\d?\d?|[1-9]\d?)){3}'
+    ipv6_re = r'\[[0-9a-f:.]+\]'  # (simple regex, validated later)

     # Host patterns
     hostname_re = r'[a-z' + ul + r'0-9](?:[a-z' + ul + r'0-9-]{0,61}[a-z' + ul + r'0-9])?'
@@ -93,7 +84,7 @@
     host_re = '(' + hostname_re + domain_re + tld_re + '|localhost)'

     regex = _lazy_re_compile(
-        r'^(?:[a-z0-9\.\-\+]*)://'  # scheme is validated separately
+        r'^(?:[a-z0-9.+-]*)://'  # scheme is validated separately
         r'(?:[^\s:@/]+(?::[^\s:@/]*)?@)?'  # user:pass authentication
         r'(?:' + ipv4_re + '|' + ipv6_re + '|' + host_re + ')'
         r'(?::\d{2,5})?'  # port
@@ -101,6 +92,7 @@
         r'\Z', re.IGNORECASE)
     message = _('Enter a valid URL.')
     schemes = ['http', 'https', 'ftp', 'ftps']
+    unsafe_chars = frozenset('\t\r\n')

     def __init__(self, schemes=None, **kwargs):
         super().__init__(**kwargs)
@@ -108,10 +100,14 @@
             self.schemes = schemes

     def __call__(self, value):
-        # Check first if the scheme is valid
+        if not isinstance(value, str):
+            raise ValidationError(self.message, code=self.code, params={'value': value})
+        if self.unsafe_chars.intersection(value):
+            raise ValidationError(self.message, code=self.code, params={'value': value})
+        # Check if the scheme is valid.
         scheme = value.split('://')[0].lower()
         if scheme not in self.schemes:
-            raise ValidationError(self.message, code=self.code)
+            raise ValidationError(self.message, code=self.code, params={'value': value})

         # Then check full URL
         try:
@@ -122,9 +118,9 @@
                 try:
                     scheme, netloc, path, query, fragment = urlsplit(value)
                 except ValueError:  # for example, "Invalid IPv6 URL"
-                    raise ValidationError(self.message, code=self.code)
+                    raise ValidationError(self.message, code=self.code, params={'value': value})
                 try:
-                    netloc = netloc.encode('idna').decode('ascii')  # IDN -> ACE
+                    netloc = punycode(netloc)  # IDN -> ACE
                 except UnicodeError:  # invalid domain part
                     raise e
                 url = urlunsplit((scheme, netloc, path, query, fragment))
@@ -135,18 +131,18 @@
             # Now verify IPv6 in the netloc part
             host_match = re.search(r'^\[(.+)\](?::\d{2,5})?$', urlsplit(value).netloc)
             if host_match:
-                potential_ip = host_match.groups()[0]
+                potential_ip = host_match[1]
                 try:
                     validate_ipv6_address(potential_ip)
                 except ValidationError:
-                    raise ValidationError(self.message, code=self.code)
+                    raise ValidationError(self.message, code=self.code, params={'value': value})

         # The maximum length of a full host name is 253 characters per RFC 1034
         # section 3.1. It's defined to be 255 bytes or less, but this includes
         # one byte for the length of the name and one byte for the trailing dot
         # that's used to indicate absolute names in DNS.
-        if len(urlsplit(value).netloc) > 253:
-            raise ValidationError(self.message, code=self.code)
+        if len(urlsplit(value).hostname) > 253:
+            raise ValidationError(self.message, code=self.code, params={'value': value})


 integer_validator = RegexValidator(
@@ -174,38 +170,65 @@
         re.IGNORECASE)
     literal_regex = _lazy_re_compile(
         # literal form, ipv4 or ipv6 address (SMTP 4.1.3)
-        r'\[([A-f0-9:\.]+)\]\Z',
+        r'\[([A-f0-9:.]+)\]\Z',
         re.IGNORECASE)
-    domain_whitelist = ['localhost']
-
-    def __init__(self, message=None, code=None, whitelist=None):
+    domain_allowlist = ['localhost']
+
+    @property
+    def domain_whitelist(self):
+        warnings.warn(
+            'The domain_whitelist attribute is deprecated in favor of '
+            'domain_allowlist.',
+            RemovedInDjango41Warning,
+            stacklevel=2,
+        )
+        return self.domain_allowlist
+
+    @domain_whitelist.setter
+    def domain_whitelist(self, allowlist):
+        warnings.warn(
+            'The domain_whitelist attribute is deprecated in favor of '
+            'domain_allowlist.',
+            RemovedInDjango41Warning,
+            stacklevel=2,
+        )
+        self.domain_allowlist = allowlist
+
+    def __init__(self, message=None, code=None, allowlist=None, *, whitelist=None):
+        if whitelist is not None:
+            allowlist = whitelist
+            warnings.warn(
+                'The whitelist argument is deprecated in favor of allowlist.',
+                RemovedInDjango41Warning,
+                stacklevel=2,
+            )
         if message is not None:
             self.message = message
         if code is not None:
             self.code = code
-        if whitelist is not None:
-            self.domain_whitelist = whitelist
+        if allowlist is not None:
+            self.domain_allowlist = allowlist

     def __call__(self, value):
         if not value or '@' not in value:
-            raise ValidationError(self.message, code=self.code)
+            raise ValidationError(self.message, code=self.code, params={'value': value})

         user_part, domain_part = value.rsplit('@', 1)

         if not self.user_regex.match(user_part):
-            raise ValidationError(self.message, code=self.code)
-
-        if (domain_part not in self.domain_whitelist and
+            raise ValidationError(self.message, code=self.code, params={'value': value})
+
+        if (domain_part not in self.domain_allowlist and
                 not self.validate_domain_part(domain_part)):
             # Try for possible IDN domain-part
             try:
-                domain_part = domain_part.encode('idna').decode('ascii')
+                domain_part = punycode(domain_part)
             except UnicodeError:
                 pass
             else:
                 if self.validate_domain_part(domain_part):
                     return
-            raise ValidationError(self.message, code=self.code)
+            raise ValidationError(self.message, code=self.code, params={'value': value})

     def validate_domain_part(self, domain_part):
         if self.domain_regex.match(domain_part):
@@ -213,7 +236,7 @@

         literal_match = self.literal_regex.match(domain_part)
         if literal_match:
-            ip_address = literal_match.group(1)
+            ip_address = literal_match[1]
             try:
                 validate_ipv46_address(ip_address)
                 return True
@@ -224,7 +247,7 @@
     def __eq__(self, other):
         return (
             isinstance(other, EmailValidator) and
-            (self.domain_whitelist == other.domain_whitelist) and
+            (self.domain_allowlist == other.domain_allowlist) and
             (self.message == other.message) and
             (self.code == other.code)
         )
@@ -236,14 +259,14 @@
 validate_slug = RegexValidator(
     slug_re,
     # Translators: "letters" means latin letters: a-z and A-Z.
-    _("Enter a valid 'slug' consisting of letters, numbers, underscores or hyphens."),
+    _('Enter a valid “slug” consisting of letters, numbers, underscores or hyphens.'),
     'invalid'
 )

 slug_unicode_re = _lazy_re_compile(r'^[-\w]+\Z')
 validate_unicode_slug = RegexValidator(
     slug_unicode_re,
-    _("Enter a valid 'slug' consisting of Unicode letters, numbers, underscores, or hyphens."),
+    _('Enter a valid “slug” consisting of Unicode letters, numbers, underscores, or hyphens.'),
     'invalid'
 )

@@ -252,12 +275,25 @@
     try:
         ipaddress.IPv4Address(value)
     except ValueError:
-        raise ValidationError(_('Enter a valid IPv4 address.'), code='invalid')
+        raise ValidationError(_('Enter a valid IPv4 address.'), code='invalid', params={'value': value})
+    else:
+        # Leading zeros are forbidden to avoid ambiguity with the octal
+        # notation. This restriction is included in Python 3.9.5+.
+        # TODO: Remove when dropping support for PY39.
+        if any(
+            octet != '0' and octet[0] == '0'
+            for octet in value.split('.')
+        ):
+            raise ValidationError(
+                _('Enter a valid IPv4 address.'),
+                code='invalid',
+                params={'value': value},
+            )


 def validate_ipv6_address(value):
     if not is_valid_ipv6_address(value):
-        raise ValidationError(_('Enter a valid IPv6 address.'), code='invalid')
+        raise ValidationError(_('Enter a valid IPv6 address.'), code='invalid', params={'value': value})


 def validate_ipv46_address(value):
@@ -267,7 +303,7 @@
         try:
             validate_ipv6_address(value)
         except ValidationError:
-            raise ValidationError(_('Enter a valid IPv4 or IPv6 address.'), code='invalid')
+            raise ValidationError(_('Enter a valid IPv4 or IPv6 address.'), code='invalid', params={'value': value})


 ip_address_validator_map = {
@@ -323,8 +359,9 @@
             raise ValidationError(self.message, code=self.code, params=params)

     def __eq__(self, other):
+        if not isinstance(other, self.__class__):
+            return NotImplemented
         return (
-            isinstance(other, self.__class__) and
             self.limit_value == other.limit_value and
             self.message == other.message and
             self.code == other.code
@@ -417,7 +454,7 @@
     def __call__(self, value):
         digit_tuple, exponent = value.as_tuple()[1:]
         if exponent in {'F', 'n', 'N'}:
-            raise ValidationError(self.messages['invalid'])
+            raise ValidationError(self.messages['invalid'], code='invalid', params={'value': value})
         if exponent >= 0:
             # A positive exponent adds that many trailing zeros.
             digits = len(digit_tuple) + exponent
@@ -439,20 +476,20 @@
             raise ValidationError(
                 self.messages['max_digits'],
                 code='max_digits',
-                params={'max': self.max_digits},
+                params={'max': self.max_digits, 'value': value},
             )
         if self.decimal_places is not None and decimals > self.decimal_places:
             raise ValidationError(
                 self.messages['max_decimal_places'],
                 code='max_decimal_places',
-                params={'max': self.decimal_places},
+                params={'max': self.decimal_places, 'value': value},
             )
         if (self.max_digits is not None and self.decimal_places is not None and
                 whole_digits > (self.max_digits - self.decimal_places)):
             raise ValidationError(
                 self.messages['max_whole_digits'],
                 code='max_whole_digits',
-                params={'max': (self.max_digits - self.decimal_places)},
+                params={'max': (self.max_digits - self.decimal_places), 'value': value},
             )

     def __eq__(self, other):
@@ -466,8 +503,8 @@
 @deconstructible
 class FileExtensionValidator:
     message = _(
-        "File extension '%(extension)s' is not allowed. "
-        "Allowed extensions are: '%(allowed_extensions)s'."
+        'File extension “%(extension)s” is not allowed. '
+        'Allowed extensions are: %(allowed_extensions)s.'
     )
     code = 'invalid_extension'

@@ -488,7 +525,8 @@
                 code=self.code,
                 params={
                     'extension': extension,
-                    'allowed_extensions': ', '.join(self.allowed_extensions)
+                    'allowed_extensions': ', '.join(self.allowed_extensions),
+                    'value': value,
                 }
             )

@@ -529,7 +567,7 @@

     def __call__(self, value):
         if '\x00' in str(value):
-            raise ValidationError(self.message, code=self.code)
+            raise ValidationError(self.message, code=self.code, params={'value': value})

     def __eq__(self, other):
         return (
('django/core', 'paginator.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,7 +3,6 @@
 import warnings
 from math import ceil

-from django.utils.deprecation import RemovedInDjango31Warning
 from django.utils.functional import cached_property
 from django.utils.inspect import method_has_no_args
 from django.utils.translation import gettext_lazy as _
@@ -26,6 +25,9 @@


 class Paginator:
+    # Translators: String used to replace omitted page numbers in elided page
+    # range generated by paginators, e.g. [1, 2, '…', 5, 6, 7, '…', 9, 10].
+    ELLIPSIS = _('…')

     def __init__(self, object_list, per_page, orphans=0,
                  allow_empty_first_page=True):
@@ -34,6 +36,10 @@
         self.per_page = int(per_page)
         self.orphans = int(orphans)
         self.allow_empty_first_page = allow_empty_first_page
+
+    def __iter__(self):
+        for page_number in self.page_range:
+            yield self.page(page_number)

     def validate_number(self, number):
         """Validate the given 1-based page number."""
@@ -125,15 +131,36 @@
                 stacklevel=3
             )

-
-class QuerySetPaginator(Paginator):
-
-    def __init__(self, *args, **kwargs):
-        warnings.warn(
-            'The QuerySetPaginator alias of Paginator is deprecated.',
-            RemovedInDjango31Warning, stacklevel=2,
-        )
-        super().__init__(*args, **kwargs)
+    def get_elided_page_range(self, number=1, *, on_each_side=3, on_ends=2):
+        """
+        Return a 1-based range of pages with some values elided.
+
+        If the page range is larger than a given size, the whole range is not
+        provided and a compact form is returned instead, e.g. for a paginator
+        with 50 pages, if page 43 were the current page, the output, with the
+        default arguments, would be:
+
+            1, 2, …, 40, 41, 42, 43, 44, 45, 46, …, 49, 50.
+        """
+        number = self.validate_number(number)
+
+        if self.num_pages <= (on_each_side + on_ends) * 2:
+            yield from self.page_range
+            return
+
+        if number > (1 + on_each_side + on_ends) + 1:
+            yield from range(1, on_ends + 1)
+            yield self.ELLIPSIS
+            yield from range(number - on_each_side, number + 1)
+        else:
+            yield from range(1, number + 1)
+
+        if number < (self.num_pages - on_each_side - on_ends) - 1:
+            yield from range(number + 1, number + on_each_side + 1)
+            yield self.ELLIPSIS
+            yield from range(self.num_pages - on_ends + 1, self.num_pages + 1)
+        else:
+            yield from range(number + 1, self.num_pages + 1)


 class Page(collections.abc.Sequence):
@@ -151,7 +178,10 @@

     def __getitem__(self, index):
         if not isinstance(index, (int, slice)):
-            raise TypeError
+            raise TypeError(
+                'Page indices must be integers or slices, not %s.'
+                % type(index).__name__
+            )
         # The object_list is converted to a list so that if it was a QuerySet
         # it won't be a database hit per __getitem__.
         if not isinstance(self.object_list, list):
('django/core', 'exceptions.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,9 @@
 """
 Global Django exception and warning classes.
 """
+import operator
+
+from django.utils.hashable import make_hashable


 class FieldDoesNotExist(Exception):
@@ -60,6 +63,16 @@
     The size of the request (excluding any file uploads) exceeded
     settings.DATA_UPLOAD_MAX_MEMORY_SIZE.
     """
+    pass
+
+
+class RequestAborted(Exception):
+    """The request was closed before it was completed, or timed out."""
+    pass
+
+
+class BadRequest(Exception):
+    """The request is malformed and cannot be processed."""
     pass


@@ -177,7 +190,28 @@
     def __repr__(self):
         return 'ValidationError(%s)' % self

+    def __eq__(self, other):
+        if not isinstance(other, ValidationError):
+            return NotImplemented
+        return hash(self) == hash(other)
+
+    def __hash__(self):
+        if hasattr(self, 'message'):
+            return hash((
+                self.message,
+                self.code,
+                make_hashable(self.params),
+            ))
+        if hasattr(self, 'error_dict'):
+            return hash(make_hashable(self.error_dict))
+        return hash(tuple(sorted(self.error_list, key=operator.attrgetter('message'))))
+

 class EmptyResultSet(Exception):
     """A database query predicate is impossible."""
     pass
+
+
+class SynchronousOnlyOperation(Exception):
+    """The user tried to call a sync-only function from an async context."""
+    pass
('django/core/cache', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -12,112 +12,54 @@

 See docs/topics/cache.txt for information on the public API.
 """
-from threading import local
-
-from django.conf import settings
 from django.core import signals
 from django.core.cache.backends.base import (
-    BaseCache, CacheKeyWarning, InvalidCacheBackendError,
+    BaseCache, CacheKeyWarning, InvalidCacheBackendError, InvalidCacheKey,
 )
+from django.utils.connection import BaseConnectionHandler, ConnectionProxy
 from django.utils.module_loading import import_string

 __all__ = [
     'cache', 'caches', 'DEFAULT_CACHE_ALIAS', 'InvalidCacheBackendError',
-    'CacheKeyWarning', 'BaseCache',
+    'CacheKeyWarning', 'BaseCache', 'InvalidCacheKey',
 ]

 DEFAULT_CACHE_ALIAS = 'default'


-def _create_cache(backend, **kwargs):
-    try:
-        # Try to get the CACHES entry for the given backend name first
+class CacheHandler(BaseConnectionHandler):
+    settings_name = 'CACHES'
+    exception_class = InvalidCacheBackendError
+
+    def create_connection(self, alias):
+        params = self.settings[alias].copy()
+        backend = params.pop('BACKEND')
+        location = params.pop('LOCATION', '')
         try:
-            conf = settings.CACHES[backend]
-        except KeyError:
-            try:
-                # Trying to import the given backend, in case it's a dotted path
-                import_string(backend)
-            except ImportError as e:
-                raise InvalidCacheBackendError("Could not find backend '%s': %s" % (
-                    backend, e))
-            location = kwargs.pop('LOCATION', '')
-            params = kwargs
-        else:
-            params = {**conf, **kwargs}
-            backend = params.pop('BACKEND')
-            location = params.pop('LOCATION', '')
-        backend_cls = import_string(backend)
-    except ImportError as e:
-        raise InvalidCacheBackendError(
-            "Could not find backend '%s': %s" % (backend, e))
-    return backend_cls(location, params)
+            backend_cls = import_string(backend)
+        except ImportError as e:
+            raise InvalidCacheBackendError(
+                "Could not find backend '%s': %s" % (backend, e)
+            ) from e
+        return backend_cls(location, params)

-
-class CacheHandler:
-    """
-    A Cache Handler to manage access to Cache instances.
-
-    Ensure only one instance of each alias exists per thread.
-    """
-    def __init__(self):
-        self._caches = local()
-
-    def __getitem__(self, alias):
-        try:
-            return self._caches.caches[alias]
-        except AttributeError:
-            self._caches.caches = {}
-        except KeyError:
-            pass
-
-        if alias not in settings.CACHES:
-            raise InvalidCacheBackendError(
-                "Could not find config for '%s' in settings.CACHES" % alias
-            )
-
-        cache = _create_cache(alias)
-        self._caches.caches[alias] = cache
-        return cache
-
-    def all(self):
-        return getattr(self._caches, 'caches', {}).values()
+    def all(self, initialized_only=False):
+        return [
+            self[alias] for alias in self
+            # If initialized_only is True, return only initialized caches.
+            if not initialized_only or hasattr(self._connections, alias)
+        ]


 caches = CacheHandler()

-
-class DefaultCacheProxy:
-    """
-    Proxy access to the default Cache object's attributes.
-
-    This allows the legacy `cache` object to be thread-safe using the new
-    ``caches`` API.
-    """
-    def __getattr__(self, name):
-        return getattr(caches[DEFAULT_CACHE_ALIAS], name)
-
-    def __setattr__(self, name, value):
-        return setattr(caches[DEFAULT_CACHE_ALIAS], name, value)
-
-    def __delattr__(self, name):
-        return delattr(caches[DEFAULT_CACHE_ALIAS], name)
-
-    def __contains__(self, key):
-        return key in caches[DEFAULT_CACHE_ALIAS]
-
-    def __eq__(self, other):
-        return caches[DEFAULT_CACHE_ALIAS] == other
-
-
-cache = DefaultCacheProxy()
+cache = ConnectionProxy(caches, DEFAULT_CACHE_ALIAS)


 def close_caches(**kwargs):
-    # Some caches -- python-memcached in particular -- need to do a cleanup at the
-    # end of a request cycle. If not implemented in a particular backend
-    # cache.close is a no-op
-    for cache in caches.all():
+    # Some caches need to do a cleanup at the end of a request cycle. If not
+    # implemented in a particular backend cache.close() is a no-op.
+    for cache in caches.all(initialized_only=True):
         cache.close()


('django/core/cache', 'utils.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,12 +1,12 @@
 import hashlib
-from urllib.parse import quote

 TEMPLATE_FRAGMENT_KEY_TEMPLATE = 'template.cache.%s.%s'


 def make_template_fragment_key(fragment_name, vary_on=None):
-    if vary_on is None:
-        vary_on = ()
-    key = ':'.join(quote(str(var)) for var in vary_on)
-    args = hashlib.md5(key.encode())
-    return TEMPLATE_FRAGMENT_KEY_TEMPLATE % (fragment_name, args.hexdigest())
+    hasher = hashlib.md5()
+    if vary_on is not None:
+        for arg in vary_on:
+            hasher.update(str(arg).encode())
+            hasher.update(b':')
+    return TEMPLATE_FRAGMENT_KEY_TEMPLATE % (fragment_name, hasher.hexdigest())
('django/core/cache/backends', 'memcached.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,8 +3,12 @@
 import pickle
 import re
 import time
-
-from django.core.cache.backends.base import DEFAULT_TIMEOUT, BaseCache
+import warnings
+
+from django.core.cache.backends.base import (
+    DEFAULT_TIMEOUT, BaseCache, InvalidCacheKey, memcache_key_warnings,
+)
+from django.utils.deprecation import RemovedInDjango41Warning
 from django.utils.functional import cached_property


@@ -16,24 +20,24 @@
         else:
             self._servers = server

-        # The exception type to catch from the underlying library for a key
-        # that was not found. This is a ValueError for python-memcache,
-        # pylibmc.NotFound for pylibmc, and cmemcache will return None without
-        # raising an exception.
+        # Exception type raised by the underlying client library for a
+        # nonexistent key.
         self.LibraryValueNotFoundException = value_not_found_exception

         self._lib = library
+        self._class = library.Client
         self._options = params.get('OPTIONS') or {}

     @property
+    def client_servers(self):
+        return self._servers
+
+    @cached_property
     def _cache(self):
         """
         Implement transparent thread-safe access to a memcached client.
         """
-        if getattr(self, '_client', None) is None:
-            self._client = self._lib.Client(self._servers, **self._options)
-
-        return self._client
+        return self._class(self.client_servers, **self._options)

     def get_backend_timeout(self, timeout=DEFAULT_TIMEOUT):
         """
@@ -64,27 +68,35 @@

     def add(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):
         key = self.make_key(key, version=version)
+        self.validate_key(key)
         return self._cache.add(key, value, self.get_backend_timeout(timeout))

     def get(self, key, default=None, version=None):
         key = self.make_key(key, version=version)
-        val = self._cache.get(key)
-        if val is None:
-            return default
-        return val
+        self.validate_key(key)
+        return self._cache.get(key, default)

     def set(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):
         key = self.make_key(key, version=version)
+        self.validate_key(key)
         if not self._cache.set(key, value, self.get_backend_timeout(timeout)):
             # make sure the key doesn't keep its old value in case of failure to set (memcached's 1MB limit)
             self._cache.delete(key)

+    def touch(self, key, timeout=DEFAULT_TIMEOUT, version=None):
+        key = self.make_key(key, version=version)
+        self.validate_key(key)
+        return bool(self._cache.touch(key, self.get_backend_timeout(timeout)))
+
     def delete(self, key, version=None):
         key = self.make_key(key, version=version)
-        self._cache.delete(key)
+        self.validate_key(key)
+        return bool(self._cache.delete(key))

     def get_many(self, keys, version=None):
         key_map = {self.make_key(key, version=version): key for key in keys}
+        for key in key_map:
+            self.validate_key(key)
         ret = self._cache.get_multi(key_map.keys())
         return {key_map[k]: v for k, v in ret.items()}

@@ -94,16 +106,15 @@

     def incr(self, key, delta=1, version=None):
         key = self.make_key(key, version=version)
+        self.validate_key(key)
         # memcached doesn't support a negative delta
         if delta < 0:
             return self._cache.decr(key, -delta)
         try:
             val = self._cache.incr(key, delta)

-        # python-memcache responds to incr on nonexistent keys by
-        # raising a ValueError, pylibmc by raising a pylibmc.NotFound
-        # and Cmemcache returns None. In all cases,
-        # we should raise a ValueError though.
+        # Normalize an exception raised by the underlying client library to
+        # ValueError in the event of a nonexistent key when calling incr().
         except self.LibraryValueNotFoundException:
             val = None
         if val is None:
@@ -112,16 +123,15 @@

     def decr(self, key, delta=1, version=None):
         key = self.make_key(key, version=version)
+        self.validate_key(key)
         # memcached doesn't support a negative delta
         if delta < 0:
             return self._cache.incr(key, -delta)
         try:
             val = self._cache.decr(key, delta)

-        # python-memcache responds to incr on nonexistent keys by
-        # raising a ValueError, pylibmc by raising a pylibmc.NotFound
-        # and Cmemcache returns None. In all cases,
-        # we should raise a ValueError though.
+        # Normalize an exception raised by the underlying client library to
+        # ValueError in the event of a nonexistent key when calling decr().
         except self.LibraryValueNotFoundException:
             val = None
         if val is None:
@@ -133,35 +143,63 @@
         original_keys = {}
         for key, value in data.items():
             safe_key = self.make_key(key, version=version)
+            self.validate_key(safe_key)
             safe_data[safe_key] = value
             original_keys[safe_key] = key
         failed_keys = self._cache.set_multi(safe_data, self.get_backend_timeout(timeout))
         return [original_keys[k] for k in failed_keys]

     def delete_many(self, keys, version=None):
-        self._cache.delete_multi(self.make_key(key, version=version) for key in keys)
+        keys = [self.make_key(key, version=version) for key in keys]
+        for key in keys:
+            self.validate_key(key)
+        self._cache.delete_multi(keys)

     def clear(self):
         self._cache.flush_all()

+    def validate_key(self, key):
+        for warning in memcache_key_warnings(key):
+            raise InvalidCacheKey(warning)
+

 class MemcachedCache(BaseMemcachedCache):
     "An implementation of a cache binding using python-memcached"
+
+    # python-memcached doesn't support default values in get().
+    # https://github.com/linsomniac/python-memcached/issues/159
+    _missing_key = None
+
     def __init__(self, server, params):
+        warnings.warn(
+            'MemcachedCache is deprecated in favor of PyMemcacheCache and '
+            'PyLibMCCache.',
+            RemovedInDjango41Warning, stacklevel=2,
+        )
+        # python-memcached ≥ 1.45 returns None for a nonexistent key in
+        # incr/decr(), python-memcached < 1.45 raises ValueError.
         import memcache
         super().__init__(server, params, library=memcache, value_not_found_exception=ValueError)
-
-    @property
-    def _cache(self):
-        if getattr(self, '_client', None) is None:
-            client_kwargs = {'pickleProtocol': pickle.HIGHEST_PROTOCOL}
-            client_kwargs.update(self._options)
-            self._client = self._lib.Client(self._servers, **client_kwargs)
-        return self._client
-
-    def touch(self, key, timeout=DEFAULT_TIMEOUT, version=None):
-        key = self.make_key(key, version=version)
-        return self._cache.touch(key, self.get_backend_timeout(timeout)) != 0
+        self._options = {'pickleProtocol': pickle.HIGHEST_PROTOCOL, **self._options}
+
+    def get(self, key, default=None, version=None):
+        key = self.make_key(key, version=version)
+        self.validate_key(key)
+        val = self._cache.get(key)
+        # python-memcached doesn't support default values in get().
+        # https://github.com/linsomniac/python-memcached/issues/159
+        # Remove this method if that issue is fixed.
+        if val is None:
+            return default
+        return val
+
+    def delete(self, key, version=None):
+        # python-memcached's delete() returns True when key doesn't exist.
+        # https://github.com/linsomniac/python-memcached/issues/170
+        # Call _deletetouch() without the NOT_FOUND in expected results.
+        key = self.make_key(key, version=version)
+        self.validate_key(key)
+        return bool(self._cache._deletetouch([b'DELETED'], 'delete', key))


 class PyLibMCCache(BaseMemcachedCache):
@@ -170,12 +208,16 @@
         import pylibmc
         super().__init__(server, params, library=pylibmc, value_not_found_exception=pylibmc.NotFound)

-    @cached_property
-    def _cache(self):
-        return self._lib.Client(self._servers, **self._options)
+    @property
+    def client_servers(self):
+        output = []
+        for server in self._servers:
+            output.append(server[5:] if server.startswith('unix:') else server)
+        return output

     def touch(self, key, timeout=DEFAULT_TIMEOUT, version=None):
         key = self.make_key(key, version=version)
+        self.validate_key(key)
         if timeout == 0:
             return self._cache.delete(key)
         return self._cache.touch(key, self.get_backend_timeout(timeout))
@@ -184,3 +226,17 @@
         # libmemcached manages its own connections. Don't call disconnect_all()
         # as it resets the failover state and creates unnecessary reconnects.
         pass
+
+
+class PyMemcacheCache(BaseMemcachedCache):
+    """An implementation of a cache binding using pymemcache."""
+    def __init__(self, server, params):
+        import pymemcache.serde
+        super().__init__(server, params, library=pymemcache, value_not_found_exception=KeyError)
+        self._class = self._lib.HashClient
+        self._options = {
+            'allow_unicode_keys': True,
+            'default_noreply': False,
+            'serde': pymemcache.serde.pickle_serde,
+            **self._options,
+        }
('django/core/cache/backends', 'db.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -7,7 +7,6 @@
 from django.core.cache.backends.base import DEFAULT_TIMEOUT, BaseCache
 from django.db import DatabaseError, connections, models, router, transaction
 from django.utils import timezone
-from django.utils.inspect import func_supports_parameter


 class Options:
@@ -85,10 +84,7 @@
         converters = (connection.ops.get_db_converters(expression) + expression.get_db_converters(connection))
         for key, value, expires in rows:
             for converter in converters:
-                if func_supports_parameter(converter, 'context'):  # RemovedInDjango30Warning
-                    expires = converter(expires, expression, connection, {})
-                else:
-                    expires = converter(expires, expression, connection)
+                expires = converter(expires, expression, connection)
             if expires < timezone.now():
                 expired_keys.append(key)
             else:
@@ -160,10 +156,7 @@
                         expression = models.Expression(output_field=models.DateTimeField())
                         for converter in (connection.ops.get_db_converters(expression) +
                                           expression.get_db_converters(connection)):
-                            if func_supports_parameter(converter, 'context'):  # RemovedInDjango30Warning
-                                current_expires = converter(current_expires, expression, connection, {})
-                            else:
-                                current_expires = converter(current_expires, expression, connection)
+                            current_expires = converter(current_expires, expression, connection)

                     exp = connection.ops.adapt_datetimefield_value(exp)
                     if result and mode == 'touch':
@@ -204,7 +197,8 @@
                 return True

     def delete(self, key, version=None):
-        self.delete_many([key], version)
+        self.validate_key(key)
+        return self._base_delete_many([self.make_key(key, version)])

     def delete_many(self, keys, version=None):
         key_list = []
@@ -215,7 +209,7 @@

     def _base_delete_many(self, keys):
         if not keys:
-            return
+            return False

         db = router.db_for_write(self.cache_model_class)
         connection = connections[db]
@@ -231,6 +225,7 @@
                 ),
                 keys,
             )
+        return bool(cursor.rowcount)

     def has_key(self, key, version=None):
         key = self.make_key(key, version=version)
@@ -272,9 +267,12 @@
                 cursor.execute(
                     connection.ops.cache_key_culling_sql() % table,
                     [cull_num])
-                cursor.execute("DELETE FROM %s "
-                               "WHERE cache_key < %%s" % table,
-                               [cursor.fetchone()[0]])
+                last_cache_key = cursor.fetchone()
+                if last_cache_key:
+                    cursor.execute(
+                        'DELETE FROM %s WHERE cache_key < %%s' % table,
+                        [last_cache_key[0]],
+                    )

     def clear(self):
         db = router.db_for_write(self.cache_model_class)
('django/core/cache/backends', 'filebased.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -76,16 +76,17 @@
             return False

     def delete(self, key, version=None):
-        self._delete(self._key_to_file(key, version))
+        return self._delete(self._key_to_file(key, version))

     def _delete(self, fname):
         if not fname.startswith(self._dir) or not os.path.exists(fname):
-            return
+            return False
         try:
             os.remove(fname)
         except FileNotFoundError:
             # The file may have been removed by another process.
-            pass
+            return False
+        return True

     def has_key(self, key, version=None):
         fname = self._key_to_file(key, version)
@@ -113,11 +114,13 @@
             self._delete(fname)

     def _createdir(self):
-        if not os.path.exists(self._dir):
-            try:
-                os.makedirs(self._dir, 0o700)
-            except FileExistsError:
-                pass
+        # Set the umask because os.makedirs() doesn't apply the "mode" argument
+        # to intermediate-level directories.
+        old_umask = os.umask(0o077)
+        try:
+            os.makedirs(self._dir, 0o700, exist_ok=True)
+        finally:
+            os.umask(old_umask)

     def _key_to_file(self, key, version=None):
         """
@@ -133,8 +136,6 @@
         """
         Remove all the cache files.
         """
-        if not os.path.exists(self._dir):
-            return
         for fname in self._list_cache_files():
             self._delete(fname)

@@ -157,8 +158,7 @@
         Get a list of paths to all the cache files. These are all the files
         in the root cache dir that end on the cache_suffix.
         """
-        if not os.path.exists(self._dir):
-            return []
-        filelist = [os.path.join(self._dir, fname) for fname
-                    in glob.glob1(self._dir, '*%s' % self.cache_suffix)]
-        return filelist
+        return [
+            os.path.join(self._dir, fname)
+            for fname in glob.glob1(self._dir, '*%s' % self.cache_suffix)
+        ]
('django/core/cache/backends', 'dummy.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -28,6 +28,7 @@
     def delete(self, key, version=None):
         key = self.make_key(key, version=version)
         self.validate_key(key)
+        return False

     def has_key(self, key, version=None):
         key = self.make_key(key, version=version)
('django/core/cache/backends', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -14,6 +14,10 @@
     pass


+class InvalidCacheKey(ValueError):
+    pass
+
+
 # Stub class to ensure not passing in a `timeout` argument results in
 # the default timeout
 DEFAULT_TIMEOUT = object()
@@ -27,7 +31,7 @@
     Default function to generate keys.

     Construct the key used by all other methods. By default, prepend
-    the `key_prefix'. KEY_FUNCTION can be used to specify an alternate
+    the `key_prefix`. KEY_FUNCTION can be used to specify an alternate
     function with custom key making behavior.
     """
     return '%s:%s:%s' % (key_prefix, version, key)
@@ -48,6 +52,8 @@


 class BaseCache:
+    _missing_key = object()
+
     def __init__(self, params):
         timeout = params.get('timeout', params.get('TIMEOUT', 300))
         if timeout is not None:
@@ -97,8 +103,7 @@
         if version is None:
             version = self.version

-        new_key = self.key_func(key, self.key_prefix, version)
-        return new_key
+        return self.key_func(key, self.key_prefix, version)

     def add(self, key, value, timeout=DEFAULT_TIMEOUT, version=None):
         """
@@ -133,7 +138,8 @@

     def delete(self, key, version=None):
         """
-        Delete a key from the cache, failing silently.
+        Delete a key from the cache and return whether it succeeded, failing
+        silently.
         """
         raise NotImplementedError('subclasses of BaseCache must provide a delete() method')

@@ -147,8 +153,8 @@
         """
         d = {}
         for k in keys:
-            val = self.get(k, version=version)
-            if val is not None:
+            val = self.get(k, self._missing_key, version=version)
+            if val is not self._missing_key:
                 d[k] = val
         return d

@@ -161,31 +167,29 @@

         Return the value of the key stored or retrieved.
         """
-        val = self.get(key, version=version)
-        if val is None:
+        val = self.get(key, self._missing_key, version=version)
+        if val is self._missing_key:
             if callable(default):
                 default = default()
-            if default is not None:
-                self.add(key, default, timeout=timeout, version=version)
-                # Fetch the value again to avoid a race condition if another
-                # caller added a value between the first get() and the add()
-                # above.
-                return self.get(key, default, version=version)
+            self.add(key, default, timeout=timeout, version=version)
+            # Fetch the value again to avoid a race condition if another caller
+            # added a value between the first get() and the add() above.
+            return self.get(key, default, version=version)
         return val

     def has_key(self, key, version=None):
         """
         Return True if the key is in the cache and has not expired.
         """
-        return self.get(key, version=version) is not None
+        return self.get(key, self._missing_key, version=version) is not self._missing_key

     def incr(self, key, delta=1, version=None):
         """
         Add delta to value in the cache. If the key does not exist, raise a
         ValueError exception.
         """
-        value = self.get(key, version=version)
-        if value is None:
+        value = self.get(key, self._missing_key, version=version)
+        if value is self._missing_key:
             raise ValueError("Key '%s' not found" % key)
         new_value = value + delta
         self.set(key, new_value, version=version)
@@ -242,18 +246,8 @@
         backend. This encourages (but does not force) writing backend-portable
         cache code.
         """
-        if len(key) > MEMCACHE_MAX_KEY_LENGTH:
-            warnings.warn(
-                'Cache key will cause errors if used with memcached: %r '
-                '(longer than %s)' % (key, MEMCACHE_MAX_KEY_LENGTH), CacheKeyWarning
-            )
-        for char in key:
-            if ord(char) < 33 or ord(char) == 127:
-                warnings.warn(
-                    'Cache key contains characters that will cause errors if '
-                    'used with memcached: %r' % key, CacheKeyWarning
-                )
-                break
+        for warning in memcache_key_warnings(key):
+            warnings.warn(warning, CacheKeyWarning)

     def incr_version(self, key, delta=1, version=None):
         """
@@ -263,8 +257,8 @@
         if version is None:
             version = self.version

-        value = self.get(key, version=version)
-        if value is None:
+        value = self.get(key, self._missing_key, version=version)
+        if value is self._missing_key:
             raise ValueError("Key '%s' not found" % key)

         self.set(key, value, version=version + delta)
@@ -281,3 +275,18 @@
     def close(self, **kwargs):
         """Close the cache connection"""
         pass
+
+
+def memcache_key_warnings(key):
+    if len(key) > MEMCACHE_MAX_KEY_LENGTH:
+        yield (
+            'Cache key will cause errors if used with memcached: %r '
+            '(longer than %s)' % (key, MEMCACHE_MAX_KEY_LENGTH)
+        )
+    for char in key:
+        if ord(char) < 33 or ord(char) == 127:
+            yield (
+                'Cache key contains characters that will cause errors if '
+                'used with memcached: %r' % key
+            )
+            break
('django/core/cache/backends', 'locmem.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -59,6 +59,7 @@

     def touch(self, key, timeout=DEFAULT_TIMEOUT, version=None):
         key = self.make_key(key, version=version)
+        self.validate_key(key)
         with self._lock:
             if self._has_expired(key):
                 return False
@@ -108,13 +109,14 @@
             del self._cache[key]
             del self._expire_info[key]
         except KeyError:
-            pass
+            return False
+        return True

     def delete(self, key, version=None):
         key = self.make_key(key, version=version)
         self.validate_key(key)
         with self._lock:
-            self._delete(key)
+            return self._delete(key)

     def clear(self):
         with self._lock:
('django/core/mail', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -42,6 +42,7 @@
     Easy wrapper for sending a single message to a recipient list. All members
     of the recipient list will see the other recipients in the 'To' field.

+    If from_email is None, use the DEFAULT_FROM_EMAIL setting.
     If auth_user is None, use the EMAIL_HOST_USER setting.
     If auth_password is None, use the EMAIL_HOST_PASSWORD setting.

@@ -91,6 +92,8 @@
     """Send a message to the admins, as defined by the ADMINS setting."""
     if not settings.ADMINS:
         return
+    if not all(isinstance(a, (list, tuple)) and len(a) == 2 for a in settings.ADMINS):
+        raise ValueError('The ADMINS setting must be a list of 2-tuples.')
     mail = EmailMultiAlternatives(
         '%s%s' % (settings.EMAIL_SUBJECT_PREFIX, subject), message,
         settings.SERVER_EMAIL, [a[1] for a in settings.ADMINS],
@@ -106,6 +109,8 @@
     """Send a message to the managers, as defined by the MANAGERS setting."""
     if not settings.MANAGERS:
         return
+    if not all(isinstance(a, (list, tuple)) and len(a) == 2 for a in settings.MANAGERS):
+        raise ValueError('The MANAGERS setting must be a list of 2-tuples.')
     mail = EmailMultiAlternatives(
         '%s%s' % (settings.EMAIL_SUBJECT_PREFIX, subject), message,
         settings.SERVER_EMAIL, [a[1] for a in settings.MANAGERS],
('django/core/mail', 'message.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,21 +2,21 @@
 from email import (
     charset as Charset, encoders as Encoders, generator, message_from_string,
 )
-from email.errors import InvalidHeaderDefect, NonASCIILocalPartDefect
+from email.errors import HeaderParseError
 from email.header import Header
-from email.headerregistry import Address
+from email.headerregistry import Address, parser
 from email.message import Message
 from email.mime.base import MIMEBase
 from email.mime.message import MIMEMessage
 from email.mime.multipart import MIMEMultipart
 from email.mime.text import MIMEText
-from email.utils import formatdate, getaddresses, make_msgid, parseaddr
+from email.utils import formataddr, formatdate, getaddresses, make_msgid
 from io import BytesIO, StringIO
 from pathlib import Path

 from django.conf import settings
 from django.core.mail.utils import DNS_NAME
-from django.utils.encoding import force_text
+from django.utils.encoding import force_str, punycode

 # Don't BASE64-encode UTF-8 messages so that we avoid unwanted attention from
 # some spam filters.
@@ -71,56 +71,49 @@
     return name, val


-def split_addr(addr, encoding):
-    """
-    Split the address into local part and domain and encode them.
-
-    When non-ascii characters are present in the local part, it must be
-    MIME-word encoded. The domain name must be idna-encoded if it contains
-    non-ascii characters.
-    """
-    if '@' in addr:
-        localpart, domain = addr.split('@', 1)
-        # Try to get the simplest encoding - ascii if possible so that
-        # to@example.com doesn't become =?utf-8?q?to?=@example.com. This
-        # makes unit testing a bit easier and more readable.
-        try:
-            localpart.encode('ascii')
-        except UnicodeEncodeError:
-            localpart = Header(localpart, encoding).encode()
-        domain = domain.encode('idna').decode('ascii')
-    else:
-        localpart = Header(addr, encoding).encode()
-        domain = ''
-    return (localpart, domain)
-
-
 def sanitize_address(addr, encoding):
     """
     Format a pair of (name, address) or an email address string.
     """
+    address = None
     if not isinstance(addr, tuple):
-        addr = parseaddr(addr)
-    nm, addr = addr
-    localpart, domain = None, None
-    nm = Header(nm, encoding).encode()
+        addr = force_str(addr)
+        try:
+            token, rest = parser.get_mailbox(addr)
+        except (HeaderParseError, ValueError, IndexError):
+            raise ValueError('Invalid address "%s"' % addr)
+        else:
+            if rest:
+                # The entire email address must be parsed.
+                raise ValueError(
+                    'Invalid address; only %s could be parsed from "%s"'
+                    % (token, addr)
+                )
+            nm = token.display_name or ''
+            localpart = token.local_part
+            domain = token.domain or ''
+    else:
+        nm, address = addr
+        localpart, domain = address.rsplit('@', 1)
+
+    address_parts = nm + localpart + domain
+    if '\n' in address_parts or '\r' in address_parts:
+        raise ValueError('Invalid address; address parts cannot contain newlines.')
+
+    # Avoid UTF-8 encode, if it's possible.
     try:
-        addr.encode('ascii')
-    except UnicodeEncodeError:  # IDN or non-ascii in the local part
-        localpart, domain = split_addr(addr, encoding)
-
-    # An `email.headerregistry.Address` object is used since
-    # email.utils.formataddr() naively encodes the name as ascii (see #25986).
-    if localpart and domain:
-        address = Address(nm, username=localpart, domain=domain)
-        return str(address)
-
+        nm.encode('ascii')
+        nm = Header(nm).encode()
+    except UnicodeEncodeError:
+        nm = Header(nm, encoding).encode()
     try:
-        address = Address(nm, addr_spec=addr)
-    except (InvalidHeaderDefect, NonASCIILocalPartDefect):
-        localpart, domain = split_addr(addr, encoding)
-        address = Address(nm, username=localpart, domain=domain)
-    return str(address)
+        localpart.encode('ascii')
+    except UnicodeEncodeError:
+        localpart = Header(localpart, encoding).encode()
+    domain = punycode(domain)
+
+    parsed_address = Address(username=localpart, domain=domain)
+    return formataddr((nm, parsed_address.addr_spec))


 class MIMEMixin:
@@ -172,8 +165,8 @@
     def set_payload(self, payload, charset=None):
         if charset == 'utf-8' and not isinstance(charset, Charset.Charset):
             has_long_lines = any(
-                len(l.encode()) > RFC5322_EMAIL_LINE_LENGTH_LIMIT
-                for l in payload.splitlines()
+                len(line.encode()) > RFC5322_EMAIL_LINE_LENGTH_LIMIT
+                for line in payload.splitlines()
             )
             # Quoted-Printable encoding has the side effect of shortening long
             # lines, if any (#22561).
@@ -375,7 +368,7 @@
             elif not isinstance(content, Message):
                 # For compatibility with existing code, parse the message
                 # into an email.Message object if it is not one already.
-                content = message_from_string(force_text(content))
+                content = message_from_string(force_str(content))

             attachment = SafeMIMEMessage(content, subtype)
         else:
('django/core/mail', 'utils.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,6 +3,8 @@
 """

 import socket
+
+from django.utils.encoding import punycode


 # Cache the hostname, but do it lazily: socket.getfqdn() can take a couple of
@@ -13,7 +15,7 @@

     def get_fqdn(self):
         if not hasattr(self, '_fqdn'):
-            self._fqdn = socket.getfqdn()
+            self._fqdn = punycode(socket.getfqdn())
         return self._fqdn


('django/core/mail/backends', 'filebased.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -17,23 +17,17 @@
             self.file_path = file_path
         else:
             self.file_path = getattr(settings, 'EMAIL_FILE_PATH', None)
-        # Make sure self.file_path is a string.
-        if not isinstance(self.file_path, str):
-            raise ImproperlyConfigured('Path for saving emails is invalid: %r' % self.file_path)
         self.file_path = os.path.abspath(self.file_path)
-        # Make sure that self.file_path is a directory if it exists.
-        if os.path.exists(self.file_path) and not os.path.isdir(self.file_path):
+        try:
+            os.makedirs(self.file_path, exist_ok=True)
+        except FileExistsError:
             raise ImproperlyConfigured(
                 'Path for saving email messages exists, but is not a directory: %s' % self.file_path
             )
-        # Try to create it, if it not exists.
-        elif not os.path.exists(self.file_path):
-            try:
-                os.makedirs(self.file_path)
-            except OSError as err:
-                raise ImproperlyConfigured(
-                    'Could not create directory for saving email messages: %s (%s)' % (self.file_path, err)
-                )
+        except OSError as err:
+            raise ImproperlyConfigured(
+                'Could not create directory for saving email messages: %s (%s)' % (self.file_path, err)
+            )
         # Make sure that self.file_path is writable.
         if not os.access(self.file_path, os.W_OK):
             raise ImproperlyConfigured('Could not write to directory: %s' % self.file_path)
('django/core/mail/backends', 'smtp.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,5 @@
 """SMTP email backend class."""
 import smtplib
-import socket
 import ssl
 import threading

@@ -69,7 +68,7 @@
             if self.username and self.password:
                 self.connection.login(self.username, self.password)
             return True
-        except (smtplib.SMTPException, socket.error):
+        except OSError:
             if not self.fail_silently:
                 raise

('django/core/checks', 'caches.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,7 +1,10 @@
+import pathlib
+
 from django.conf import settings
-from django.core.cache import DEFAULT_CACHE_ALIAS
+from django.core.cache import DEFAULT_CACHE_ALIAS, caches
+from django.core.cache.backends.filebased import FileBasedCache

-from . import Error, Tags, register
+from . import Error, Tags, Warning, register

 E001 = Error(
     "You must define a '%s' cache in your CACHES setting." % DEFAULT_CACHE_ALIAS,
@@ -14,3 +17,56 @@
     if DEFAULT_CACHE_ALIAS not in settings.CACHES:
         return [E001]
     return []
+
+
+@register(Tags.caches, deploy=True)
+def check_cache_location_not_exposed(app_configs, **kwargs):
+    errors = []
+    for name in ('MEDIA_ROOT', 'STATIC_ROOT', 'STATICFILES_DIRS'):
+        setting = getattr(settings, name, None)
+        if not setting:
+            continue
+        if name == 'STATICFILES_DIRS':
+            paths = set()
+            for staticfiles_dir in setting:
+                if isinstance(staticfiles_dir, (list, tuple)):
+                    _, staticfiles_dir = staticfiles_dir
+                paths.add(pathlib.Path(staticfiles_dir).resolve())
+        else:
+            paths = {pathlib.Path(setting).resolve()}
+        for alias in settings.CACHES:
+            cache = caches[alias]
+            if not isinstance(cache, FileBasedCache):
+                continue
+            cache_path = pathlib.Path(cache._dir).resolve()
+            if any(path == cache_path for path in paths):
+                relation = 'matches'
+            elif any(path in cache_path.parents for path in paths):
+                relation = 'is inside'
+            elif any(cache_path in path.parents for path in paths):
+                relation = 'contains'
+            else:
+                continue
+            errors.append(Warning(
+                f"Your '{alias}' cache configuration might expose your cache "
+                f"or lead to corruption of your data because its LOCATION "
+                f"{relation} {name}.",
+                id='caches.W002',
+            ))
+    return errors
+
+
+@register(Tags.caches)
+def check_file_based_cache_is_absolute(app_configs, **kwargs):
+    errors = []
+    for alias, config in settings.CACHES.items():
+        cache = caches[alias]
+        if not isinstance(cache, FileBasedCache):
+            continue
+        if not pathlib.Path(config['LOCATION']).is_absolute():
+            errors.append(Warning(
+                f"Your '{alias}' cache LOCATION path is relative. Use an "
+                f"absolute path instead.",
+                id='caches.W003',
+            ))
+    return errors
('django/core/checks', 'registry.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,6 @@
 from itertools import chain

+from django.utils.inspect import func_accepts_kwargs
 from django.utils.itercompat import is_iterable


@@ -8,12 +9,15 @@
     Built-in tags for internal checks.
     """
     admin = 'admin'
+    async_support = 'async_support'
     caches = 'caches'
     compatibility = 'compatibility'
     database = 'database'
     models = 'models'
     security = 'security'
     signals = 'signals'
+    sites = 'sites'
+    staticfiles = 'staticfiles'
     templates = 'templates'
     translation = 'translation'
     urls = 'urls'
@@ -35,13 +39,17 @@

             registry = CheckRegistry()
             @registry.register('mytag', 'anothertag')
-            def my_check(apps, **kwargs):
+            def my_check(app_configs, **kwargs):
                 # ... perform checks and collect `errors` ...
                 return errors
             # or
             registry.register(my_check, 'mytag', 'anothertag')
         """
         def inner(check):
+            if not func_accepts_kwargs(check):
+                raise TypeError(
+                    'Check functions must accept keyword arguments (**kwargs).'
+                )
             check.tags = tags
             checks = self.deployment_checks if kwargs.get('deploy') else self.registered_checks
             checks.add(check)
@@ -54,7 +62,7 @@
                 tags += (check,)
             return inner

-    def run_checks(self, app_configs=None, tags=None, include_deployment_checks=False):
+    def run_checks(self, app_configs=None, tags=None, include_deployment_checks=False, databases=None):
         """
         Run all registered checks and return list of Errors and Warnings.
         """
@@ -63,13 +71,9 @@

         if tags is not None:
             checks = [check for check in checks if not set(check.tags).isdisjoint(tags)]
-        else:
-            # By default, 'database'-tagged checks are not run as they do more
-            # than mere static code analysis.
-            checks = [check for check in checks if Tags.database not in check.tags]

         for check in checks:
-            new_errors = check(app_configs=app_configs)
+            new_errors = check(app_configs=app_configs, databases=databases)
             assert is_iterable(new_errors), (
                 "The function %r did not return a list. All functions registered "
                 "with the checks registry must return a list." % check)
('django/core/checks', 'model_checks.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,12 +4,15 @@
 from itertools import chain

 from django.apps import apps
-from django.core.checks import Error, Tags, register
+from django.conf import settings
+from django.core.checks import Error, Tags, Warning, register


 @register(Tags.models)
 def check_all_models(app_configs=None, **kwargs):
     db_table_models = defaultdict(list)
+    indexes = defaultdict(list)
+    constraints = defaultdict(list)
     errors = []
     if app_configs is None:
         models = apps.get_models()
@@ -29,15 +32,56 @@
             )
         else:
             errors.extend(model.check(**kwargs))
+        for model_index in model._meta.indexes:
+            indexes[model_index.name].append(model._meta.label)
+        for model_constraint in model._meta.constraints:
+            constraints[model_constraint.name].append(model._meta.label)
+    if settings.DATABASE_ROUTERS:
+        error_class, error_id = Warning, 'models.W035'
+        error_hint = (
+            'You have configured settings.DATABASE_ROUTERS. Verify that %s '
+            'are correctly routed to separate databases.'
+        )
+    else:
+        error_class, error_id = Error, 'models.E028'
+        error_hint = None
     for db_table, model_labels in db_table_models.items():
         if len(model_labels) != 1:
+            model_labels_str = ', '.join(model_labels)
+            errors.append(
+                error_class(
+                    "db_table '%s' is used by multiple models: %s."
+                    % (db_table, model_labels_str),
+                    obj=db_table,
+                    hint=(error_hint % model_labels_str) if error_hint else None,
+                    id=error_id,
+                )
+            )
+    for index_name, model_labels in indexes.items():
+        if len(model_labels) > 1:
+            model_labels = set(model_labels)
             errors.append(
                 Error(
-                    "db_table '%s' is used by multiple models: %s."
-                    % (db_table, ', '.join(db_table_models[db_table])),
-                    obj=db_table,
-                    id='models.E028',
-                )
+                    "index name '%s' is not unique %s %s." % (
+                        index_name,
+                        'for model' if len(model_labels) == 1 else 'among models:',
+                        ', '.join(sorted(model_labels)),
+                    ),
+                    id='models.E029' if len(model_labels) == 1 else 'models.E030',
+                ),
+            )
+    for constraint_name, model_labels in constraints.items():
+        if len(model_labels) > 1:
+            model_labels = set(model_labels)
+            errors.append(
+                Error(
+                    "constraint name '%s' is not unique %s %s." % (
+                        constraint_name,
+                        'for model' if len(model_labels) == 1 else 'among models:',
+                        ', '.join(sorted(model_labels)),
+                    ),
+                    id='models.E031' if len(model_labels) == 1 else 'models.E032',
+                ),
             )
     return errors

@@ -77,10 +121,8 @@
         """
         operation, args, keywords = obj, [], {}
         while hasattr(operation, 'func'):
-            # The or clauses are redundant but work around a bug (#25945) in
-            # functools.partial in Python <= 3.5.1.
-            args.extend(getattr(operation, 'args', []) or [])
-            keywords.update(getattr(operation, 'keywords', {}) or {})
+            args.extend(getattr(operation, 'args', []))
+            keywords.update(getattr(operation, 'keywords', {}))
             operation = operation.func
         return operation, args, keywords

('django/core/checks', 'database.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,8 +4,11 @@


 @register(Tags.database)
-def check_database_backends(*args, **kwargs):
+def check_database_backends(databases=None, **kwargs):
+    if databases is None:
+        return []
     issues = []
-    for conn in connections.all():
+    for alias in databases:
+        conn = connections[alias]
         issues.extend(conn.validation.check(**kwargs))
     return issues
('django/core/checks', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -5,6 +5,7 @@
 from .registry import Tags, register, run_checks, tag_exists

 # Import these to force registration of checks
+import django.core.checks.async_checks  # NOQA isort:skip
 import django.core.checks.caches  # NOQA isort:skip
 import django.core.checks.database  # NOQA isort:skip
 import django.core.checks.model_checks  # NOQA isort:skip
('django/core/checks', 'translation.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,19 +1,64 @@
 from django.conf import settings
+from django.utils.translation import get_supported_language_variant
 from django.utils.translation.trans_real import language_code_re

 from . import Error, Tags, register

 E001 = Error(
-    'You have provided an invalid value for the LANGUAGE_CODE setting.',
+    'You have provided an invalid value for the LANGUAGE_CODE setting: {!r}.',
     id='translation.E001',
+)
+
+E002 = Error(
+    'You have provided an invalid language code in the LANGUAGES setting: {!r}.',
+    id='translation.E002',
+)
+
+E003 = Error(
+    'You have provided an invalid language code in the LANGUAGES_BIDI setting: {!r}.',
+    id='translation.E003',
+)
+
+E004 = Error(
+    'You have provided a value for the LANGUAGE_CODE setting that is not in '
+    'the LANGUAGES setting.',
+    id='translation.E004',
 )


 @register(Tags.translation)
 def check_setting_language_code(app_configs, **kwargs):
-    """
-    Errors if language code setting is invalid.
-    """
-    if not language_code_re.match(settings.LANGUAGE_CODE):
-        return [E001]
+    """Error if LANGUAGE_CODE setting is invalid."""
+    tag = settings.LANGUAGE_CODE
+    if not isinstance(tag, str) or not language_code_re.match(tag):
+        return [Error(E001.msg.format(tag), id=E001.id)]
     return []
+
+
+@register(Tags.translation)
+def check_setting_languages(app_configs, **kwargs):
+    """Error if LANGUAGES setting is invalid."""
+    return [
+        Error(E002.msg.format(tag), id=E002.id)
+        for tag, _ in settings.LANGUAGES if not isinstance(tag, str) or not language_code_re.match(tag)
+    ]
+
+
+@register(Tags.translation)
+def check_setting_languages_bidi(app_configs, **kwargs):
+    """Error if LANGUAGES_BIDI setting is invalid."""
+    return [
+        Error(E003.msg.format(tag), id=E003.id)
+        for tag in settings.LANGUAGES_BIDI if not isinstance(tag, str) or not language_code_re.match(tag)
+    ]
+
+
+@register(Tags.translation)
+def check_language_settings_consistent(app_configs, **kwargs):
+    """Error if language settings are not consistent with each other."""
+    try:
+        get_supported_language_variant(settings.LANGUAGE_CODE)
+    except LookupError:
+        return [E004]
+    else:
+        return []
('django/core/checks/security', 'csrf.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,8 @@
+import inspect
+
 from django.conf import settings

-from .. import Tags, Warning, register
+from .. import Error, Tags, Warning, register

 W003 = Warning(
     "You don't appear to be using Django's built-in "
@@ -38,3 +40,28 @@
         settings.CSRF_COOKIE_SECURE
     )
     return [] if passed_check else [W016]
+
+
+@register(Tags.security)
+def check_csrf_failure_view(app_configs, **kwargs):
+    from django.middleware.csrf import _get_failure_view
+
+    errors = []
+    try:
+        view = _get_failure_view()
+    except ImportError:
+        msg = (
+            "The CSRF failure view '%s' could not be imported." %
+            settings.CSRF_FAILURE_VIEW
+        )
+        errors.append(Error(msg, id='security.E102'))
+    else:
+        try:
+            inspect.signature(view).bind(None, reason=None)
+        except TypeError:
+            msg = (
+                "The CSRF failure view '%s' does not take the correct number of arguments." %
+                settings.CSRF_FAILURE_VIEW
+            )
+            errors.append(Error(msg, id='security.E101'))
+    return errors
('django/core/checks/security', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,16 +1,24 @@
 from django.conf import settings
-
-from .. import Tags, Warning, register
-
+from django.core.exceptions import ImproperlyConfigured
+
+from .. import Error, Tags, Warning, register
+
+REFERRER_POLICY_VALUES = {
+    'no-referrer', 'no-referrer-when-downgrade', 'origin',
+    'origin-when-cross-origin', 'same-origin', 'strict-origin',
+    'strict-origin-when-cross-origin', 'unsafe-url',
+}
+
+SECRET_KEY_INSECURE_PREFIX = 'django-insecure-'
 SECRET_KEY_MIN_LENGTH = 50
 SECRET_KEY_MIN_UNIQUE_CHARACTERS = 5

 W001 = Warning(
     "You do not have 'django.middleware.security.SecurityMiddleware' "
     "in your MIDDLEWARE so the SECURE_HSTS_SECONDS, "
-    "SECURE_CONTENT_TYPE_NOSNIFF, "
-    "SECURE_BROWSER_XSS_FILTER, and SECURE_SSL_REDIRECT settings "
-    "will have no effect.",
+    "SECURE_CONTENT_TYPE_NOSNIFF, SECURE_BROWSER_XSS_FILTER, "
+    "SECURE_REFERRER_POLICY, and SECURE_SSL_REDIRECT settings will have no "
+    "effect.",
     id='security.W001',
 )

@@ -51,15 +59,6 @@
     id='security.W006',
 )

-W007 = Warning(
-    "Your SECURE_BROWSER_XSS_FILTER setting is not set to True, "
-    "so your pages will not be served with an "
-    "'X-XSS-Protection: 1; mode=block' header. "
-    "You should consider enabling this header to activate the "
-    "browser's XSS filtering and help prevent XSS attacks.",
-    id='security.W007',
-)
-
 W008 = Warning(
     "Your SECURE_SSL_REDIRECT setting is not set to True. "
     "Unless your site should be available over both SSL and non-SSL "
@@ -70,12 +69,14 @@
 )

 W009 = Warning(
-    "Your SECRET_KEY has less than %(min_length)s characters or less than "
-    "%(min_unique_chars)s unique characters. Please generate a long and random "
-    "SECRET_KEY, otherwise many of Django's security-critical features will be "
-    "vulnerable to attack." % {
+    "Your SECRET_KEY has less than %(min_length)s characters, less than "
+    "%(min_unique_chars)s unique characters, or it's prefixed with "
+    "'%(insecure_prefix)s' indicating that it was generated automatically by "
+    "Django. Please generate a long and random SECRET_KEY, otherwise many of "
+    "Django's security-critical features will be vulnerable to attack." % {
         'min_length': SECRET_KEY_MIN_LENGTH,
         'min_unique_chars': SECRET_KEY_MIN_UNIQUE_CHARACTERS,
+        'insecure_prefix': SECRET_KEY_INSECURE_PREFIX,
     },
     id='security.W009',
 )
@@ -89,9 +90,8 @@
     "You have "
     "'django.middleware.clickjacking.XFrameOptionsMiddleware' in your "
     "MIDDLEWARE, but X_FRAME_OPTIONS is not set to 'DENY'. "
-    "The default is 'SAMEORIGIN', but unless there is a good reason for "
-    "your site to serve other parts of itself in a frame, you should "
-    "change it to 'DENY'.",
+    "Unless there is a good reason for your site to serve other parts of "
+    "itself in a frame, you should change it to 'DENY'.",
     id='security.W019',
 )

@@ -106,6 +106,24 @@
     id='security.W021',
 )

+W022 = Warning(
+    'You have not set the SECURE_REFERRER_POLICY setting. Without this, your '
+    'site will not send a Referrer-Policy header. You should consider '
+    'enabling this header to protect user privacy.',
+    id='security.W022',
+)
+
+E023 = Error(
+    'You have set the SECURE_REFERRER_POLICY setting to an invalid value.',
+    hint='Valid values are: {}.'.format(', '.join(sorted(REFERRER_POLICY_VALUES))),
+    id='security.E023',
+)
+
+E100 = Error(
+    "DEFAULT_HASHING_ALGORITHM must be 'sha1' or 'sha256'.",
+    id='security.E100',
+)
+

 def _security_middleware():
     return 'django.middleware.security.SecurityMiddleware' in settings.MIDDLEWARE
@@ -163,15 +181,6 @@


 @register(Tags.security, deploy=True)
-def check_xss_filter(app_configs, **kwargs):
-    passed_check = (
-        not _security_middleware() or
-        settings.SECURE_BROWSER_XSS_FILTER is True
-    )
-    return [] if passed_check else [W007]
-
-
-@register(Tags.security, deploy=True)
 def check_ssl_redirect(app_configs, **kwargs):
     passed_check = (
         not _security_middleware() or
@@ -182,11 +191,16 @@

 @register(Tags.security, deploy=True)
 def check_secret_key(app_configs, **kwargs):
-    passed_check = (
-        getattr(settings, 'SECRET_KEY', None) and
-        len(set(settings.SECRET_KEY)) >= SECRET_KEY_MIN_UNIQUE_CHARACTERS and
-        len(settings.SECRET_KEY) >= SECRET_KEY_MIN_LENGTH
-    )
+    try:
+        secret_key = settings.SECRET_KEY
+    except (ImproperlyConfigured, AttributeError):
+        passed_check = False
+    else:
+        passed_check = (
+            len(set(secret_key)) >= SECRET_KEY_MIN_UNIQUE_CHARACTERS and
+            len(secret_key) >= SECRET_KEY_MIN_LENGTH and
+            not secret_key.startswith(SECRET_KEY_INSECURE_PREFIX)
+        )
     return [] if passed_check else [W009]


@@ -208,3 +222,26 @@
 @register(Tags.security, deploy=True)
 def check_allowed_hosts(app_configs, **kwargs):
     return [] if settings.ALLOWED_HOSTS else [W020]
+
+
+@register(Tags.security, deploy=True)
+def check_referrer_policy(app_configs, **kwargs):
+    if _security_middleware():
+        if settings.SECURE_REFERRER_POLICY is None:
+            return [W022]
+        # Support a comma-separated string or iterable of values to allow fallback.
+        if isinstance(settings.SECURE_REFERRER_POLICY, str):
+            values = {v.strip() for v in settings.SECURE_REFERRER_POLICY.split(',')}
+        else:
+            values = set(settings.SECURE_REFERRER_POLICY)
+        if not values <= REFERRER_POLICY_VALUES:
+            return [E023]
+    return []
+
+
+# RemovedInDjango40Warning
+@register(Tags.security)
+def check_default_hashing_algorithm(app_configs, **kwargs):
+    if settings.DEFAULT_HASHING_ALGORITHM not in {'sha1', 'sha256'}:
+        return [E100]
+    return []
('django/core/management', 'color.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -8,18 +8,52 @@

 from django.utils import termcolors

+try:
+    import colorama
+    colorama.init()
+except (ImportError, OSError):
+    HAS_COLORAMA = False
+else:
+    HAS_COLORAMA = True
+

 def supports_color():
     """
     Return True if the running system's terminal supports color,
     and False otherwise.
     """
-    plat = sys.platform
-    supported_platform = plat != 'Pocket PC' and (plat != 'win32' or 'ANSICON' in os.environ)
+    def vt_codes_enabled_in_windows_registry():
+        """
+        Check the Windows Registry to see if VT code handling has been enabled
+        by default, see https://superuser.com/a/1300251/447564.
+        """
+        try:
+            # winreg is only available on Windows.
+            import winreg
+        except ImportError:
+            return False
+        else:
+            reg_key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, 'Console')
+            try:
+                reg_key_value, _ = winreg.QueryValueEx(reg_key, 'VirtualTerminalLevel')
+            except FileNotFoundError:
+                return False
+            else:
+                return reg_key_value == 1

     # isatty is not always implemented, #6223.
     is_a_tty = hasattr(sys.stdout, 'isatty') and sys.stdout.isatty()
-    return supported_platform and is_a_tty
+
+    return is_a_tty and (
+        sys.platform != 'win32' or
+        HAS_COLORAMA or
+        'ANSICON' in os.environ or
+        # Windows Terminal supports VT codes.
+        'WT_SESSION' in os.environ or
+        # Microsoft Visual Studio Code's built-in terminal supports colors.
+        os.environ.get('TERM_PROGRAM') == 'vscode' or
+        vt_codes_enabled_in_windows_registry()
+    )


 class Style:
('django/core/management', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,7 +2,10 @@
 import os
 import pkgutil
 import sys
-from collections import OrderedDict, defaultdict
+from argparse import (
+    _AppendConstAction, _CountAction, _StoreConstAction, _SubParsersAction,
+)
+from collections import defaultdict
 from difflib import get_close_matches
 from importlib import import_module

@@ -117,18 +120,48 @@
         for s_opt in parser._actions if s_opt.option_strings
     }
     arg_options = {opt_mapping.get(key, key): value for key, value in options.items()}
-    parse_args = [str(a) for a in args]
+    parse_args = []
+    for arg in args:
+        if isinstance(arg, (list, tuple)):
+            parse_args += map(str, arg)
+        else:
+            parse_args.append(str(arg))
+
+    def get_actions(parser):
+        # Parser actions and actions from sub-parser choices.
+        for opt in parser._actions:
+            if isinstance(opt, _SubParsersAction):
+                for sub_opt in opt.choices.values():
+                    yield from get_actions(sub_opt)
+            else:
+                yield opt
+
+    parser_actions = list(get_actions(parser))
+    mutually_exclusive_required_options = {
+        opt
+        for group in parser._mutually_exclusive_groups
+        for opt in group._group_actions if group.required
+    }
     # Any required arguments which are passed in via **options must be passed
     # to parse_args().
-    parse_args += [
-        '{}={}'.format(min(opt.option_strings), arg_options[opt.dest])
-        for opt in parser._actions if opt.required and opt.dest in options
-    ]
+    for opt in parser_actions:
+        if (
+            opt.dest in options and
+            (opt.required or opt in mutually_exclusive_required_options)
+        ):
+            parse_args.append(min(opt.option_strings))
+            if isinstance(opt, (_AppendConstAction, _CountAction, _StoreConstAction)):
+                continue
+            value = arg_options[opt.dest]
+            if isinstance(value, (list, tuple)):
+                parse_args += map(str, value)
+            else:
+                parse_args.append(str(value))
     defaults = parser.parse_args(args=parse_args)
     defaults = dict(defaults._get_kwargs(), **arg_options)
     # Raise an error if any unknown options were passed.
     stealth_options = set(command.base_stealth_options + command.stealth_options)
-    dest_parameters = {action.dest for action in parser._actions}
+    dest_parameters = {action.dest for action in parser_actions}
     valid_options = (dest_parameters | stealth_options).union(opt_mapping)
     unknown_options = set(options) - valid_options
     if unknown_options:
@@ -209,7 +242,7 @@
                 # (get_commands() swallows the original one) so the user is
                 # informed about it.
                 settings.INSTALLED_APPS
-            else:
+            elif not settings.configured:
                 sys.stderr.write("No Django settings specified.\n")
             possible_matches = get_close_matches(subcommand, commands)
             sys.stderr.write('Unknown command: %r' % subcommand)
@@ -311,7 +344,12 @@
         # Preprocess options to extract --settings and --pythonpath.
         # These options could affect the commands that are available, so they
         # must be processed early.
-        parser = CommandParser(usage='%(prog)s subcommand [options] [args]', add_help=False, allow_abbrev=False)
+        parser = CommandParser(
+            prog=self.prog_name,
+            usage='%(prog)s subcommand [options] [args]',
+            add_help=False,
+            allow_abbrev=False,
+        )
         parser.add_argument('--settings')
         parser.add_argument('--pythonpath')
         parser.add_argument('args', nargs='*')  # catch-all
@@ -339,8 +377,8 @@
                     # The exception will be raised later in the child process
                     # started by the autoreloader. Pretend it didn't happen by
                     # loading an empty list of applications.
-                    apps.all_models = defaultdict(OrderedDict)
-                    apps.app_configs = OrderedDict()
+                    apps.all_models = defaultdict(dict)
+                    apps.app_configs = {}
                     apps.apps_ready = apps.models_ready = apps.ready = True

                     # Remove options not compatible with the built-in runserver
('django/core/management', 'templates.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -6,7 +6,6 @@
 import stat
 import tempfile
 from importlib import import_module
-from os import path
 from urllib.request import urlretrieve

 import django
@@ -29,7 +28,7 @@
     :param directory: The directory to which the template should be copied.
     :param options: The additional variables passed to project or app templates
     """
-    requires_system_checks = False
+    requires_system_checks = []
     # The supported URL schemes
     url_schemes = ['http', 'https', 'ftp']
     # Rewrite the following suffixes when determining the target filename.
@@ -58,14 +57,15 @@

     def handle(self, app_or_project, name, target=None, **options):
         self.app_or_project = app_or_project
+        self.a_or_an = 'an' if app_or_project == 'app' else 'a'
         self.paths_to_remove = []
         self.verbosity = options['verbosity']

-        self.validate_name(name, app_or_project)
+        self.validate_name(name)

         # if some directory is given, make sure it's nicely expanded
         if target is None:
-            top_dir = path.join(os.getcwd(), name)
+            top_dir = os.path.join(os.getcwd(), name)
             try:
                 os.makedirs(top_dir)
             except FileExistsError:
@@ -73,7 +73,9 @@
             except OSError as e:
                 raise CommandError(e)
         else:
-            top_dir = os.path.abspath(path.expanduser(target))
+            if app_or_project == 'app':
+                self.validate_name(os.path.basename(target), 'directory')
+            top_dir = os.path.abspath(os.path.expanduser(target))
             if not os.path.exists(top_dir):
                 raise CommandError("Destination directory '%s' does not "
                                    "exist, please create it first." % top_dir)
@@ -83,13 +85,14 @@
         for file in options['files']:
             extra_files.extend(map(lambda x: x.strip(), file.split(',')))
         if self.verbosity >= 2:
-            self.stdout.write("Rendering %s template files with "
-                              "extensions: %s\n" %
-                              (app_or_project, ', '.join(extensions)))
-            self.stdout.write("Rendering %s template files with "
-                              "filenames: %s\n" %
-                              (app_or_project, ', '.join(extra_files)))
-
+            self.stdout.write(
+                'Rendering %s template files with extensions: %s'
+                % (app_or_project, ', '.join(extensions))
+            )
+            self.stdout.write(
+                'Rendering %s template files with filenames: %s'
+                % (app_or_project, ', '.join(extra_files))
+            )
         base_name = '%s_name' % app_or_project
         base_subdir = '%s_template' % app_or_project
         base_directory = '%s_directory' % app_or_project
@@ -119,9 +122,8 @@
             path_rest = root[prefix_length:]
             relative_dir = path_rest.replace(base_name, name)
             if relative_dir:
-                target_dir = path.join(top_dir, relative_dir)
-                if not path.exists(target_dir):
-                    os.mkdir(target_dir)
+                target_dir = os.path.join(top_dir, relative_dir)
+                os.makedirs(target_dir, exist_ok=True)

             for dirname in dirs[:]:
                 if dirname.startswith('.') or dirname == '__pycache__':
@@ -131,24 +133,27 @@
                 if filename.endswith(('.pyo', '.pyc', '.py.class')):
                     # Ignore some files as they cause various breakages.
                     continue
-                old_path = path.join(root, filename)
-                new_path = path.join(top_dir, relative_dir,
-                                     filename.replace(base_name, name))
+                old_path = os.path.join(root, filename)
+                new_path = os.path.join(
+                    top_dir, relative_dir, filename.replace(base_name, name)
+                )
                 for old_suffix, new_suffix in self.rewrite_template_suffixes:
                     if new_path.endswith(old_suffix):
                         new_path = new_path[:-len(old_suffix)] + new_suffix
                         break  # Only rewrite once

-                if path.exists(new_path):
-                    raise CommandError("%s already exists, overlaying a "
-                                       "project or app into an existing "
-                                       "directory won't replace conflicting "
-                                       "files" % new_path)
+                if os.path.exists(new_path):
+                    raise CommandError(
+                        "%s already exists. Overlaying %s %s into an existing "
+                        "directory won't replace conflicting files." % (
+                            new_path, self.a_or_an, app_or_project,
+                        )
+                    )

                 # Only render the Python files, as we don't want to
                 # accidentally render Django templates files
                 if new_path.endswith(extensions) or filename in extra_files:
-                    with open(old_path, 'r', encoding='utf-8') as template_file:
+                    with open(old_path, encoding='utf-8') as template_file:
                         content = template_file.read()
                     template = Engine().from_string(content)
                     content = template.render(context)
@@ -158,7 +163,7 @@
                     shutil.copyfile(old_path, new_path)

                 if self.verbosity >= 2:
-                    self.stdout.write("Creating %s\n" % new_path)
+                    self.stdout.write('Creating %s' % new_path)
                 try:
                     shutil.copymode(old_path, new_path)
                     self.make_writeable(new_path)
@@ -170,9 +175,9 @@

         if self.paths_to_remove:
             if self.verbosity >= 2:
-                self.stdout.write("Cleaning up temporary files.\n")
+                self.stdout.write('Cleaning up temporary files.')
             for path_to_remove in self.paths_to_remove:
-                if path.isfile(path_to_remove):
+                if os.path.isfile(path_to_remove):
                     os.remove(path_to_remove)
                 else:
                     shutil.rmtree(path_to_remove)
@@ -184,39 +189,39 @@
         directory isn't known.
         """
         if template is None:
-            return path.join(django.__path__[0], 'conf', subdir)
+            return os.path.join(django.__path__[0], 'conf', subdir)
         else:
             if template.startswith('file://'):
                 template = template[7:]
-            expanded_template = path.expanduser(template)
-            expanded_template = path.normpath(expanded_template)
-            if path.isdir(expanded_template):
+            expanded_template = os.path.expanduser(template)
+            expanded_template = os.path.normpath(expanded_template)
+            if os.path.isdir(expanded_template):
                 return expanded_template
             if self.is_url(template):
                 # downloads the file and returns the path
                 absolute_path = self.download(template)
             else:
-                absolute_path = path.abspath(expanded_template)
-            if path.exists(absolute_path):
+                absolute_path = os.path.abspath(expanded_template)
+            if os.path.exists(absolute_path):
                 return self.extract(absolute_path)

         raise CommandError("couldn't handle %s template %s." %
                            (self.app_or_project, template))

-    def validate_name(self, name, app_or_project):
-        a_or_an = 'an' if app_or_project == 'app' else 'a'
+    def validate_name(self, name, name_or_dir='name'):
         if name is None:
             raise CommandError('you must provide {an} {app} name'.format(
-                an=a_or_an,
-                app=app_or_project,
+                an=self.a_or_an,
+                app=self.app_or_project,
             ))
         # Check it's a valid directory name.
         if not name.isidentifier():
             raise CommandError(
-                "'{name}' is not a valid {app} name. Please make sure the "
-                "name is a valid identifier.".format(
+                "'{name}' is not a valid {app} {type}. Please make sure the "
+                "{type} is a valid identifier.".format(
                     name=name,
-                    app=app_or_project,
+                    app=self.app_or_project,
+                    type=name_or_dir,
                 )
             )
         # Check it cannot be imported.
@@ -227,11 +232,12 @@
         else:
             raise CommandError(
                 "'{name}' conflicts with the name of an existing Python "
-                "module and cannot be used as {an} {app} name. Please try "
-                "another name.".format(
+                "module and cannot be used as {an} {app} {type}. Please try "
+                "another {type}.".format(
                     name=name,
-                    an=a_or_an,
-                    app=app_or_project,
+                    an=self.a_or_an,
+                    app=self.app_or_project,
+                    type=name_or_dir,
                 )
             )

@@ -254,10 +260,10 @@
         filename, display_url = cleanup_url(url)

         if self.verbosity >= 2:
-            self.stdout.write("Downloading %s\n" % display_url)
+            self.stdout.write('Downloading %s' % display_url)
         try:
-            the_path, info = urlretrieve(url, path.join(tempdir, filename))
-        except IOError as e:
+            the_path, info = urlretrieve(url, os.path.join(tempdir, filename))
+        except OSError as e:
             raise CommandError("couldn't download URL %s to %s: %s" %
                                (url, filename, e))

@@ -282,7 +288,7 @@
         # Move the temporary file to a filename that has better
         # chances of being recognized by the archive utils
         if used_name != guessed_filename:
-            guessed_path = path.join(tempdir, guessed_filename)
+            guessed_path = os.path.join(tempdir, guessed_filename)
             shutil.move(the_path, guessed_path)
             return guessed_path

@@ -301,18 +307,18 @@

     def extract(self, filename):
         """
-        Extract the given file to a temporarily and return
+        Extract the given file to a temporary directory and return
         the path of the directory with the extracted content.
         """
         prefix = 'django_%s_template_' % self.app_or_project
         tempdir = tempfile.mkdtemp(prefix=prefix, suffix='_extract')
         self.paths_to_remove.append(tempdir)
         if self.verbosity >= 2:
-            self.stdout.write("Extracting %s\n" % filename)
+            self.stdout.write('Extracting %s' % filename)
         try:
             archive.extract(filename, tempdir)
             return tempdir
-        except (archive.ArchiveException, IOError) as e:
+        except (archive.ArchiveException, OSError) as e:
             raise CommandError("couldn't extract file %s to %s: %s" %
                                (filename, tempdir, e))

('django/core/management', 'utils.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,7 @@
+import fnmatch
 import os
-from subprocess import PIPE, Popen
+from pathlib import Path
+from subprocess import PIPE, run

 from django.apps import apps as installed_apps
 from django.utils.crypto import get_random_string
@@ -15,13 +17,12 @@
     Return stdout output, stderr output, and OS status code.
     """
     try:
-        p = Popen(args, shell=False, stdout=PIPE, stderr=PIPE, close_fds=os.name != 'nt')
+        p = run(args, stdout=PIPE, stderr=PIPE, close_fds=os.name != 'nt')
     except OSError as err:
         raise CommandError('Error executing %s' % args[0]) from err
-    output, errors = p.communicate()
     return (
-        output.decode(stdout_encoding),
-        errors.decode(DEFAULT_LOCALE_ENCODING, errors='replace'),
+        p.stdout.decode(stdout_encoding),
+        p.stderr.decode(DEFAULT_LOCALE_ENCODING, errors='replace'),
         p.returncode
     )

@@ -111,7 +112,7 @@
 def get_command_line_option(argv, option):
     """
     Return the value of a command line option (which should include leading
-    dashes, e.g. '--testrunnner') from an argument list. Return None if the
+    dashes, e.g. '--testrunner') from an argument list. Return None if the
     option wasn't passed or if the argument list couldn't be parsed.
     """
     parser = CommandParser(add_help=False, allow_abbrev=False)
@@ -122,3 +123,31 @@
         return None
     else:
         return options.value
+
+
+def normalize_path_patterns(patterns):
+    """Normalize an iterable of glob style patterns based on OS."""
+    patterns = [os.path.normcase(p) for p in patterns]
+    dir_suffixes = {'%s*' % path_sep for path_sep in {'/', os.sep}}
+    norm_patterns = []
+    for pattern in patterns:
+        for dir_suffix in dir_suffixes:
+            if pattern.endswith(dir_suffix):
+                norm_patterns.append(pattern[:-len(dir_suffix)])
+                break
+        else:
+            norm_patterns.append(pattern)
+    return norm_patterns
+
+
+def is_ignored_path(path, ignore_patterns):
+    """
+    Check if the given path should be ignored or not based on matching
+    one of the glob style `ignore_patterns`.
+    """
+    path = Path(path)
+
+    def ignore(pattern):
+        return fnmatch.fnmatchcase(path.name, pattern) or fnmatch.fnmatchcase(str(path), pattern)
+
+    return any(ignore(pattern) for pattern in normalize_path_patterns(ignore_patterns))
('django/core/management', 'sql.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,20 +2,17 @@
 from django.db import models


-def sql_flush(style, connection, only_django=False, reset_sequences=True, allow_cascade=False):
+def sql_flush(style, connection, reset_sequences=True, allow_cascade=False):
     """
     Return a list of the SQL statements used to flush the database.
-
-    If only_django is True, only include the table names that have associated
-    Django models and are in INSTALLED_APPS .
     """
-    if only_django:
-        tables = connection.introspection.django_table_names(only_existing=True, include_views=False)
-    else:
-        tables = connection.introspection.table_names(include_views=False)
-    seqs = connection.introspection.sequence_list() if reset_sequences else ()
-    statements = connection.ops.sql_flush(style, tables, seqs, allow_cascade)
-    return statements
+    tables = connection.introspection.django_table_names(only_existing=True, include_views=False)
+    return connection.ops.sql_flush(
+        style,
+        tables,
+        reset_sequences=reset_sequences,
+        allow_cascade=allow_cascade,
+    )


 def emit_pre_migrate_signal(verbosity, interactive, db, **kwargs):
('django/core/management', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,6 +4,7 @@
 """
 import os
 import sys
+import warnings
 from argparse import ArgumentParser, HelpFormatter
 from io import TextIOBase

@@ -12,6 +13,9 @@
 from django.core.exceptions import ImproperlyConfigured
 from django.core.management.color import color_style, no_style
 from django.db import DEFAULT_DB_ALIAS, connections
+from django.utils.deprecation import RemovedInDjango41Warning
+
+ALL_CHECKS = '__all__'


 class CommandError(Exception):
@@ -26,7 +30,9 @@
     error) is the preferred way to indicate that something has gone
     wrong in the execution of a command.
     """
-    pass
+    def __init__(self, *args, returncode=1, **kwargs):
+        self.returncode = returncode
+        super().__init__(*args, **kwargs)


 class SystemCheckError(CommandError):
@@ -95,7 +101,7 @@
     """
     show_last = {
         '--version', '--verbosity', '--traceback', '--settings', '--pythonpath',
-        '--no-color', '--force-color',
+        '--no-color', '--force-color', '--skip-checks',
     }

     def _reordered_actions(self, actions):
@@ -126,7 +132,7 @@
         else:
             self._style_func = lambda x: x

-    def __init__(self, out, style_func=None, ending='\n'):
+    def __init__(self, out, ending='\n'):
         self._out = out
         self.style_func = None
         self.ending = ending
@@ -134,10 +140,14 @@
     def __getattr__(self, name):
         return getattr(self._out, name)

+    def flush(self):
+        if hasattr(self._out, 'flush'):
+            self._out.flush()
+
     def isatty(self):
         return hasattr(self._out, 'isatty') and self._out.isatty()

-    def write(self, msg, style_func=None, ending=None):
+    def write(self, msg='', style_func=None, ending=None):
         ending = self.ending if ending is None else ending
         if ending and not msg.endswith(ending):
             msg += ending
@@ -201,8 +211,11 @@
         migrations on disk don't match the migrations in the database.

     ``requires_system_checks``
-        A boolean; if ``True``, entire Django project will be checked for errors
-        prior to executing the command. Default value is ``True``.
+        A list or tuple of tags, e.g. [Tags.staticfiles, Tags.models]. System
+        checks registered in the chosen tags will be checked for errors prior
+        to executing the command. The value '__all__' can be used to specify
+        that all system checks should be performed. Default value is '__all__'.
+
         To validate an individual application's models
         rather than all applications' models, call
         ``self.check(app_configs)`` from ``handle()``, where ``app_configs``
@@ -220,10 +233,10 @@
     _called_from_command_line = False
     output_transaction = False  # Whether to wrap the output in a "BEGIN; COMMIT;"
     requires_migrations_checks = False
-    requires_system_checks = True
+    requires_system_checks = '__all__'
     # Arguments, common to all commands, which aren't defined by the argument
     # parser.
-    base_stealth_options = ('skip_checks', 'stderr', 'stdout')
+    base_stealth_options = ('stderr', 'stdout')
     # Command-specific options not defined by the argument parser.
     stealth_options = ()

@@ -237,6 +250,19 @@
         else:
             self.style = color_style(force_color)
             self.stderr.style_func = self.style.ERROR
+        if self.requires_system_checks in [False, True]:
+            warnings.warn(
+                "Using a boolean value for requires_system_checks is "
+                "deprecated. Use '__all__' instead of True, and [] (an empty "
+                "list) instead of False.",
+                RemovedInDjango41Warning,
+            )
+            self.requires_system_checks = ALL_CHECKS if self.requires_system_checks else []
+        if (
+            not isinstance(self.requires_system_checks, (list, tuple)) and
+            self.requires_system_checks != ALL_CHECKS
+        ):
+            raise TypeError('requires_system_checks must be a list or tuple.')

     def get_version(self):
         """
@@ -286,6 +312,11 @@
             '--force-color', action='store_true',
             help='Force colorization of the command output.',
         )
+        if self.requires_system_checks:
+            parser.add_argument(
+                '--skip-checks', action='store_true',
+                help='Skip system checks.',
+            )
         self.add_arguments(parser)
         return parser

@@ -321,8 +352,8 @@
         handle_default_options(options)
         try:
             self.execute(*args, **cmd_options)
-        except Exception as e:
-            if options.traceback or not isinstance(e, CommandError):
+        except CommandError as e:
+            if options.traceback:
                 raise

             # SystemCheckError takes care of its own formatting.
@@ -330,7 +361,7 @@
                 self.stderr.write(str(e), lambda x: x)
             else:
                 self.stderr.write('%s: %s' % (e.__class__.__name__, e))
-            sys.exit(1)
+            sys.exit(e.returncode)
         finally:
             try:
                 connections.close_all()
@@ -355,10 +386,13 @@
         if options.get('stdout'):
             self.stdout = OutputWrapper(options['stdout'])
         if options.get('stderr'):
-            self.stderr = OutputWrapper(options['stderr'], self.stderr.style_func)
-
-        if self.requires_system_checks and not options.get('skip_checks'):
-            self.check()
+            self.stderr = OutputWrapper(options['stderr'])
+
+        if self.requires_system_checks and not options['skip_checks']:
+            if self.requires_system_checks == ALL_CHECKS:
+                self.check()
+            else:
+                self.check(tags=self.requires_system_checks)
         if self.requires_migrations_checks:
             self.check_migrations()
         output = self.handle(*args, **options)
@@ -373,21 +407,20 @@
             self.stdout.write(output)
         return output

-    def _run_checks(self, **kwargs):
-        return checks.run_checks(**kwargs)
-
     def check(self, app_configs=None, tags=None, display_num_errors=False,
-              include_deployment_checks=False, fail_level=checks.ERROR):
+              include_deployment_checks=False, fail_level=checks.ERROR,
+              databases=None):
         """
         Use the system check framework to validate entire Django project.
         Raise CommandError for any serious message (error or critical errors).
         If there are only light messages (like warnings), print them to stderr
         and don't raise an exception.
         """
-        all_issues = self._run_checks(
+        all_issues = checks.run_checks(
             app_configs=app_configs,
             tags=tags,
             include_deployment_checks=include_deployment_checks,
+            databases=databases,
         )

         header, body, footer = "", "", ""
@@ -460,15 +493,15 @@
             apps_waiting_migration = sorted({migration.app_label for migration, backwards in plan})
             self.stdout.write(
                 self.style.NOTICE(
-                    "\nYou have %(unpplied_migration_count)s unapplied migration(s). "
+                    "\nYou have %(unapplied_migration_count)s unapplied migration(s). "
                     "Your project may not work properly until you apply the "
                     "migrations for app(s): %(apps_waiting_migration)s." % {
-                        "unpplied_migration_count": len(plan),
+                        "unapplied_migration_count": len(plan),
                         "apps_waiting_migration": ", ".join(apps_waiting_migration),
                     }
                 )
             )
-            self.stdout.write(self.style.NOTICE("Run 'python manage.py migrate' to apply them.\n"))
+            self.stdout.write(self.style.NOTICE("Run 'python manage.py migrate' to apply them."))

     def handle(self, *args, **options):
         """
('django/core/management/commands', 'createcachetable.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,15 +3,14 @@
 from django.core.cache.backends.db import BaseDatabaseCache
 from django.core.management.base import BaseCommand, CommandError
 from django.db import (
-    DEFAULT_DB_ALIAS, connections, models, router, transaction,
+    DEFAULT_DB_ALIAS, DatabaseError, connections, models, router, transaction,
 )
-from django.db.utils import DatabaseError


 class Command(BaseCommand):
     help = "Creates the tables needed to use the SQL cache backend."

-    requires_system_checks = False
+    requires_system_checks = []

     def add_arguments(self, parser):
         parser.add_argument(
('django/core/management/commands', 'inspectdb.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,5 @@
 import keyword
 import re
-from collections import OrderedDict

 from django.core.management.base import BaseCommand, CommandError
 from django.db import DEFAULT_DB_ALIAS, connections
@@ -9,7 +8,7 @@

 class Command(BaseCommand):
     help = "Introspects the database tables in the given database and outputs a Django model module."
-    requires_system_checks = False
+    requires_system_checks = []
     stealth_options = ('table_name_filter',)
     db_module = 'django.db'

@@ -32,7 +31,7 @@
     def handle(self, **options):
         try:
             for line in self.handle_inspection(options):
-                self.stdout.write("%s\n" % line)
+                self.stdout.write(line)
         except NotImplementedError:
             raise CommandError("Database inspection isn't supported for the currently selected database backend.")

@@ -49,7 +48,7 @@
             yield "# You'll have to do the following manually to clean this up:"
             yield "#   * Rearrange models' order"
             yield "#   * Make sure each model has one field with primary_key=True"
-            yield "#   * Make sure each ForeignKey has `on_delete` set to the desired behavior."
+            yield "#   * Make sure each ForeignKey and OneToOneField has `on_delete` set to the desired behavior"
             yield (
                 "#   * Remove `managed = False` lines if you wish to allow "
                 "Django to create, modify, and delete the table"
@@ -98,7 +97,7 @@
                 column_to_field_name = {}  # Maps column names to names of model fields
                 for row in table_description:
                     comment_notes = []  # Holds Field notes, to be displayed in a Python comment.
-                    extra_params = OrderedDict()  # Holds Field parameters such as 'db_column'.
+                    extra_params = {}  # Holds Field parameters such as 'db_column'.
                     column_name = row.name
                     is_relation = column_name in relations

@@ -117,14 +116,18 @@
                         extra_params['unique'] = True

                     if is_relation:
+                        if extra_params.pop('unique', False) or extra_params.get('primary_key'):
+                            rel_type = 'OneToOneField'
+                        else:
+                            rel_type = 'ForeignKey'
                         rel_to = (
                             "self" if relations[column_name][1] == table_name
                             else table2model(relations[column_name][1])
                         )
                         if rel_to in known_models:
-                            field_type = 'ForeignKey(%s' % rel_to
+                            field_type = '%s(%s' % (rel_type, rel_to)
                         else:
-                            field_type = "ForeignKey('%s'" % rel_to
+                            field_type = "%s('%s'" % (rel_type, rel_to)
                     else:
                         # Calling `get_field_type` to get the field type string and any
                         # additional parameters and notes.
@@ -139,7 +142,7 @@
                     if att_name == 'id' and extra_params == {'primary_key': True}:
                         if field_type == 'AutoField(':
                             continue
-                        elif field_type == 'IntegerField(' and not connection.features.can_introspect_autofield:
+                        elif field_type == connection.features.introspected_field_types['AutoField'] + '(':
                             comment_notes.append('AutoField?')

                     # Add 'null' and 'blank', if the 'null_ok' flag was present in the
@@ -154,7 +157,7 @@
                         '' if '.' in field_type else 'models.',
                         field_type,
                     )
-                    if field_type.startswith('ForeignKey('):
+                    if field_type.startswith(('ForeignKey(', 'OneToOneField(')):
                         field_desc += ', models.DO_NOTHING'

                     if extra_params:
@@ -167,8 +170,7 @@
                     yield '    %s' % field_desc
                 is_view = any(info.name == table_name and info.type == 'v' for info in table_info)
                 is_partition = any(info.name == table_name and info.type == 'p' for info in table_info)
-                for meta_line in self.get_meta(table_name, constraints, column_to_field_name, is_view, is_partition):
-                    yield meta_line
+                yield from self.get_meta(table_name, constraints, column_to_field_name, is_view, is_partition)

     def normalize_col_name(self, col_name, used_column_names, is_relation):
         """
@@ -232,7 +234,7 @@
         description, this routine will return the given field type name, as
         well as any additional keyword parameters and notes for the field.
         """
-        field_params = OrderedDict()
+        field_params = {}
         field_notes = []

         try:
@@ -241,15 +243,12 @@
             field_type = 'TextField'
             field_notes.append('This field type is a guess.')

-        # This is a hook for data_types_reverse to return a tuple of
-        # (field_type, field_params_dict).
-        if type(field_type) is tuple:
-            field_type, new_params = field_type
-            field_params.update(new_params)
-
         # Add max_length for all CharFields.
         if field_type == 'CharField' and row.internal_size:
             field_params['max_length'] = int(row.internal_size)
+
+        if field_type in {'CharField', 'TextField'} and row.collation:
+            field_params['db_collation'] = row.collation

         if field_type == 'DecimalField':
             if row.precision is None or row.scale is None:
('django/core/management/commands', 'squashmigrations.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -188,16 +188,20 @@
             fh.write(writer.as_string())

         if self.verbosity > 0:
-            self.stdout.write(self.style.MIGRATE_HEADING("Created new squashed migration %s" % writer.path))
-            self.stdout.write("  You should commit this migration but leave the old ones in place;")
-            self.stdout.write("  the new migration will be used for new installs. Once you are sure")
-            self.stdout.write("  all instances of the codebase have applied the migrations you squashed,")
-            self.stdout.write("  you can delete them.")
+            self.stdout.write(
+                self.style.MIGRATE_HEADING('Created new squashed migration %s' % writer.path) + '\n'
+                '  You should commit this migration but leave the old ones in place;\n'
+                '  the new migration will be used for new installs. Once you are sure\n'
+                '  all instances of the codebase have applied the migrations you squashed,\n'
+                '  you can delete them.'
+            )
             if writer.needs_manual_porting:
-                self.stdout.write(self.style.MIGRATE_HEADING("Manual porting required"))
-                self.stdout.write("  Your migrations contained functions that must be manually copied over,")
-                self.stdout.write("  as we could not safely copy their implementation.")
-                self.stdout.write("  See the comment at the top of the squashed migration for details.")
+                self.stdout.write(
+                    self.style.MIGRATE_HEADING('Manual porting required') + '\n'
+                    '  Your migrations contained functions that must be manually copied over,\n'
+                    '  as we could not safely copy their implementation.\n'
+                    '  See the comment at the top of the squashed migration for details.'
+                )

     def find_migration(self, loader, app_label, name):
         try:
('django/core/management/commands', 'check.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -7,7 +7,7 @@
 class Command(BaseCommand):
     help = "Checks the entire Django project for potential problems."

-    requires_system_checks = False
+    requires_system_checks = []

     def add_arguments(self, parser):
         parser.add_argument('args', metavar='app_label', nargs='*')
@@ -31,6 +31,10 @@
                 'Message level that will cause the command to exit with a '
                 'non-zero status. Default is ERROR.'
             ),
+        )
+        parser.add_argument(
+            '--database', action='append', dest='databases',
+            help='Run database related checks against these aliases.',
         )

     def handle(self, *app_labels, **options):
@@ -62,4 +66,5 @@
             display_num_errors=True,
             include_deployment_checks=include_deployment_checks,
             fail_level=getattr(checks, options['fail_level']),
+            databases=options['databases'],
         )
('django/core/management/commands', 'sqlmigrate.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,8 +1,7 @@
 from django.apps import apps
 from django.core.management.base import BaseCommand, CommandError
 from django.db import DEFAULT_DB_ALIAS, connections
-from django.db.migrations.executor import MigrationExecutor
-from django.db.migrations.loader import AmbiguityError
+from django.db.migrations.loader import AmbiguityError, MigrationLoader


 class Command(BaseCommand):
@@ -33,8 +32,9 @@
         # Get the database we're operating from
         connection = connections[options['database']]

-        # Load up an executor to get all the migration data
-        executor = MigrationExecutor(connection)
+        # Load up an loader to get all the migration data, but don't replace
+        # migrations.
+        loader = MigrationLoader(connection, replace_migrations=False)

         # Resolve command-line arguments into a migration
         app_label, migration_name = options['app_label'], options['migration_name']
@@ -43,23 +43,26 @@
             apps.get_app_config(app_label)
         except LookupError as err:
             raise CommandError(str(err))
-        if app_label not in executor.loader.migrated_apps:
+        if app_label not in loader.migrated_apps:
             raise CommandError("App '%s' does not have migrations" % app_label)
         try:
-            migration = executor.loader.get_migration_by_prefix(app_label, migration_name)
+            migration = loader.get_migration_by_prefix(app_label, migration_name)
         except AmbiguityError:
             raise CommandError("More than one migration matches '%s' in app '%s'. Please be more specific." % (
                 migration_name, app_label))
         except KeyError:
             raise CommandError("Cannot find a migration matching '%s' from app '%s'. Is it in INSTALLED_APPS?" % (
                 migration_name, app_label))
-        targets = [(app_label, migration.name)]
+        target = (app_label, migration.name)

-        # Show begin/end around output only for atomic migrations
-        self.output_transaction = migration.atomic
+        # Show begin/end around output for atomic migrations, if the database
+        # supports transactional DDL.
+        self.output_transaction = migration.atomic and connection.features.can_rollback_ddl

         # Make a plan that represents just the requested migrations and show SQL
         # for it
-        plan = [(executor.loader.graph.nodes[targets[0]], options['backwards'])]
-        sql_statements = executor.collect_sql(plan)
+        plan = [(loader.graph.nodes[target], options['backwards'])]
+        sql_statements = loader.collect_sql(plan)
+        if not sql_statements and options['verbosity'] >= 1:
+            self.stderr.write('No operations found.')
         return '\n'.join(sql_statements)
('django/core/management/commands', 'makemigrations.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,6 @@
 import os
 import sys
+import warnings
 from itertools import takewhile

 from django.apps import apps
@@ -7,7 +8,7 @@
 from django.core.management.base import (
     BaseCommand, CommandError, no_translations,
 )
-from django.db import DEFAULT_DB_ALIAS, connections, router
+from django.db import DEFAULT_DB_ALIAS, OperationalError, connections, router
 from django.db.migrations import Migration
 from django.db.migrations.autodetector import MigrationAutodetector
 from django.db.migrations.loader import MigrationLoader
@@ -98,8 +99,15 @@
                     for app_label in consistency_check_labels
                     for model in apps.get_app_config(app_label).get_models()
             )):
-                loader.check_consistent_history(connection)
-
+                try:
+                    loader.check_consistent_history(connection)
+                except OperationalError as error:
+                    warnings.warn(
+                        "Got an error checking a consistent migration history "
+                        "performed for database connection '%s': %s"
+                        % (alias, error),
+                        RuntimeWarning,
+                    )
         # Before anything else, see if there's conflicting apps and drop out
         # hard if there are any and they don't want to merge
         conflicts = loader.detect_conflicts()
@@ -190,7 +198,7 @@
         directory_created = {}
         for app_label, app_migrations in changes.items():
             if self.verbosity >= 1:
-                self.stdout.write(self.style.MIGRATE_HEADING("Migrations for '%s':" % app_label) + "\n")
+                self.stdout.write(self.style.MIGRATE_HEADING("Migrations for '%s':" % app_label))
             for migration in app_migrations:
                 # Describe the migration
                 writer = MigrationWriter(migration, self.include_header)
@@ -203,15 +211,14 @@
                         migration_string = writer.path
                     if migration_string.startswith('..'):
                         migration_string = writer.path
-                    self.stdout.write("  %s\n" % (self.style.MIGRATE_LABEL(migration_string),))
+                    self.stdout.write('  %s\n' % self.style.MIGRATE_LABEL(migration_string))
                     for operation in migration.operations:
-                        self.stdout.write("    - %s\n" % operation.describe())
+                        self.stdout.write('    - %s' % operation.describe())
                 if not self.dry_run:
                     # Write the migrations file to the disk.
                     migrations_directory = os.path.dirname(writer.path)
                     if not directory_created.get(app_label):
-                        if not os.path.isdir(migrations_directory):
-                            os.mkdir(migrations_directory)
+                        os.makedirs(migrations_directory, exist_ok=True)
                         init_path = os.path.join(migrations_directory, "__init__.py")
                         if not os.path.isfile(init_path):
                             open(init_path, "w").close()
@@ -225,9 +232,9 @@
                     # will output the migrations to stdout rather than saving
                     # the file to the disk.
                     self.stdout.write(self.style.MIGRATE_HEADING(
-                        "Full migrations file '%s':" % writer.filename) + "\n"
-                    )
-                    self.stdout.write("%s\n" % writer.as_string())
+                        "Full migrations file '%s':" % writer.filename
+                    ))
+                    self.stdout.write(writer.as_string())

     def handle_merge(self, loader, conflicts):
         """
@@ -273,7 +280,7 @@
                 for migration in merge_migrations:
                     self.stdout.write(self.style.MIGRATE_LABEL("  Branch %s" % migration.name))
                     for operation in migration.merged_operations:
-                        self.stdout.write("    - %s\n" % operation.describe())
+                        self.stdout.write('    - %s' % operation.describe())
             if questioner.ask_merge(app_label):
                 # If they still want to merge it, then write out an empty
                 # file depending on the migrations needing merging.
@@ -288,10 +295,17 @@
                 subclass = type("Migration", (Migration,), {
                     "dependencies": [(app_label, migration.name) for migration in merge_migrations],
                 })
-                migration_name = "%04i_%s" % (
-                    biggest_number + 1,
-                    self.migration_name or ("merge_%s" % get_migration_name_timestamp())
-                )
+                parts = ['%04i' % (biggest_number + 1)]
+                if self.migration_name:
+                    parts.append(self.migration_name)
+                else:
+                    parts.append('merge')
+                    leaf_names = '_'.join(sorted(migration.name for migration in merge_migrations))
+                    if len(leaf_names) > 47:
+                        parts.append(get_migration_name_timestamp())
+                    else:
+                        parts.append(leaf_names)
+                migration_name = '_'.join(parts)
                 new_migration = subclass(migration_name, app_label)
                 writer = MigrationWriter(new_migration, self.include_header)

@@ -306,6 +320,6 @@
                     # will output the merge migrations to stdout rather than saving
                     # the file to the disk.
                     self.stdout.write(self.style.MIGRATE_HEADING(
-                        "Full merge migrations file '%s':" % writer.filename) + "\n"
-                    )
-                    self.stdout.write("%s\n" % writer.as_string())
+                        "Full merge migrations file '%s':" % writer.filename
+                    ))
+                    self.stdout.write(writer.as_string())
('django/core/management/commands', 'sqlflush.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -19,4 +19,7 @@
         )

     def handle(self, **options):
-        return '\n'.join(sql_flush(self.style, connections[options['database']], only_django=True))
+        sql_statements = sql_flush(self.style, connections[options['database']])
+        if not sql_statements and options['verbosity'] >= 1:
+            self.stderr.write('No tables found.')
+        return '\n'.join(sql_statements)
('django/core/management/commands', 'makemessages.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,4 +1,3 @@
-import fnmatch
 import glob
 import os
 import re
@@ -12,15 +11,16 @@
 from django.core.files.temp import NamedTemporaryFile
 from django.core.management.base import BaseCommand, CommandError
 from django.core.management.utils import (
-    find_command, handle_extensions, popen_wrapper,
+    find_command, handle_extensions, is_ignored_path, popen_wrapper,
 )
 from django.utils.encoding import DEFAULT_LOCALE_ENCODING
 from django.utils.functional import cached_property
 from django.utils.jslex import prepare_js_for_gettext
+from django.utils.regex_helper import _lazy_re_compile
 from django.utils.text import get_text_list
 from django.utils.translation import templatize

-plural_forms_re = re.compile(r'^(?P<value>"Plural-Forms.+?\\n")\s*$', re.MULTILINE | re.DOTALL)
+plural_forms_re = _lazy_re_compile(r'^(?P<value>"Plural-Forms.+?\\n")\s*$', re.MULTILINE | re.DOTALL)
 STATUS_OK = 0
 NO_LOCALE_DIR = object()

@@ -103,8 +103,7 @@
         if not self.is_templatized:
             return

-        encoding = settings.FILE_CHARSET if self.command.settings_available else 'utf-8'
-        with open(self.path, 'r', encoding=encoding) as fp:
+        with open(self.path, encoding='utf-8') as fp:
             src_data = fp.read()

         if self.domain == 'djangojs':
@@ -136,7 +135,7 @@

         return re.sub(
             r'^(#: .*)(' + re.escape(old_path) + r')',
-            lambda match: match.group().replace(old_path, new_path),
+            lambda match: match[0].replace(old_path, new_path),
             msgs,
             flags=re.MULTILINE
         )
@@ -207,7 +206,7 @@
     translatable_file_class = TranslatableFile
     build_file_class = BuildFile

-    requires_system_checks = False
+    requires_system_checks = []

     msgmerge_options = ['-q', '--previous']
     msguniq_options = ['--to-code=utf-8']
@@ -330,7 +329,7 @@
             exts = extensions or ['html', 'txt', 'py']
         self.extensions = handle_extensions(exts)

-        if (locale is None and not exclude and not process_all) or self.domain is None:
+        if (not locale and not exclude and not process_all) or self.domain is None:
             raise CommandError(
                 "Type '%s help %s' for usage information."
                 % (os.path.basename(sys.argv[0]), sys.argv[1])
@@ -338,7 +337,7 @@

         if self.verbosity > 1:
             self.stdout.write(
-                'examining files with the extensions: %s\n'
+                'examining files with the extensions: %s'
                 % get_text_list(list(self.extensions), 'and')
             )

@@ -357,8 +356,7 @@
                 self.locale_paths.append(os.path.abspath('locale'))
             if self.locale_paths:
                 self.default_locale_path = self.locale_paths[0]
-                if not os.path.exists(self.default_locale_path):
-                    os.makedirs(self.default_locale_path)
+                os.makedirs(self.default_locale_path, exist_ok=True)

         # Build locale list
         looks_like_locale = re.compile(r'[a-z]{2}')
@@ -385,8 +383,16 @@

             # Build po files for each selected locale
             for locale in locales:
+                if '-' in locale:
+                    self.stdout.write(
+                        'invalid locale %s, did you mean %s?' % (
+                            locale,
+                            locale.replace('-', '_'),
+                        ),
+                    )
+                    continue
                 if self.verbosity > 0:
-                    self.stdout.write("processing locale %s\n" % locale)
+                    self.stdout.write('processing locale %s' % locale)
                 for potfile in potfiles:
                     self.write_po_file(potfile, locale)
         finally:
@@ -454,48 +460,26 @@
         Get all files in the given root. Also check that there is a matching
         locale dir for each file.
         """
-        def is_ignored(path, ignore_patterns):
-            """
-            Check if the given path should be ignored or not.
-            """
-            filename = os.path.basename(path)
-
-            def ignore(pattern):
-                return fnmatch.fnmatchcase(filename, pattern) or fnmatch.fnmatchcase(path, pattern)
-
-            return any(ignore(pattern) for pattern in ignore_patterns)
-
-        ignore_patterns = [os.path.normcase(p) for p in self.ignore_patterns]
-        dir_suffixes = {'%s*' % path_sep for path_sep in {'/', os.sep}}
-        norm_patterns = []
-        for p in ignore_patterns:
-            for dir_suffix in dir_suffixes:
-                if p.endswith(dir_suffix):
-                    norm_patterns.append(p[:-len(dir_suffix)])
-                    break
-            else:
-                norm_patterns.append(p)
-
         all_files = []
         ignored_roots = []
         if self.settings_available:
             ignored_roots = [os.path.normpath(p) for p in (settings.MEDIA_ROOT, settings.STATIC_ROOT) if p]
         for dirpath, dirnames, filenames in os.walk(root, topdown=True, followlinks=self.symlinks):
             for dirname in dirnames[:]:
-                if (is_ignored(os.path.normpath(os.path.join(dirpath, dirname)), norm_patterns) or
+                if (is_ignored_path(os.path.normpath(os.path.join(dirpath, dirname)), self.ignore_patterns) or
                         os.path.join(os.path.abspath(dirpath), dirname) in ignored_roots):
                     dirnames.remove(dirname)
                     if self.verbosity > 1:
-                        self.stdout.write('ignoring directory %s\n' % dirname)
+                        self.stdout.write('ignoring directory %s' % dirname)
                 elif dirname == 'locale':
                     dirnames.remove(dirname)
                     self.locale_paths.insert(0, os.path.join(os.path.abspath(dirpath), dirname))
             for filename in filenames:
                 file_path = os.path.normpath(os.path.join(dirpath, filename))
                 file_ext = os.path.splitext(filename)[1]
-                if file_ext not in self.extensions or is_ignored(file_path, self.ignore_patterns):
+                if file_ext not in self.extensions or is_ignored_path(file_path, self.ignore_patterns):
                     if self.verbosity > 1:
-                        self.stdout.write('ignoring file %s in %s\n' % (filename, dirpath))
+                        self.stdout.write('ignoring file %s in %s' % (filename, dirpath))
                 else:
                     locale_dir = None
                     for path in self.locale_paths:
@@ -528,7 +512,7 @@
         build_files = []
         for translatable in files:
             if self.verbosity > 1:
-                self.stdout.write('processing file %s in %s\n' % (
+                self.stdout.write('processing file %s in %s' % (
                     translatable.file, translatable.dirpath
                 ))
             if self.domain not in ('djangojs', 'django'):
@@ -580,7 +564,7 @@

         input_files = [bf.work_path for bf in build_files]
         with NamedTemporaryFile(mode='w+') as input_files_list:
-            input_files_list.write(('\n'.join(input_files)))
+            input_files_list.write('\n'.join(input_files))
             input_files_list.flush()
             args.extend(['--files-from', input_files_list.name])
             args.extend(self.xgettext_options)
@@ -621,8 +605,7 @@
         Use msgmerge and msgattrib GNU gettext utilities.
         """
         basedir = os.path.join(os.path.dirname(potfile), locale, 'LC_MESSAGES')
-        if not os.path.isdir(basedir):
-            os.makedirs(basedir)
+        os.makedirs(basedir, exist_ok=True)
         pofile = os.path.join(basedir, '%s.po' % self.domain)

         if os.path.exists(pofile):
@@ -635,7 +618,7 @@
                 elif self.verbosity > 0:
                     self.stdout.write(errors)
         else:
-            with open(potfile, 'r', encoding='utf-8') as fp:
+            with open(potfile, encoding='utf-8') as fp:
                 msgs = fp.read()
             if not self.invoked_for_django:
                 msgs = self.copy_plural_forms(msgs, locale)
@@ -669,12 +652,12 @@
         for domain in domains:
             django_po = os.path.join(django_dir, 'conf', 'locale', locale, 'LC_MESSAGES', '%s.po' % domain)
             if os.path.exists(django_po):
-                with open(django_po, 'r', encoding='utf-8') as fp:
+                with open(django_po, encoding='utf-8') as fp:
                     m = plural_forms_re.search(fp.read())
                 if m:
-                    plural_form_line = m.group('value')
+                    plural_form_line = m['value']
                     if self.verbosity > 1:
-                        self.stdout.write("copying plural forms: %s\n" % plural_form_line)
+                        self.stdout.write('copying plural forms: %s' % plural_form_line)
                     lines = []
                     found = False
                     for line in msgs.splitlines():
('django/core/management/commands', 'shell.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -14,7 +14,7 @@
         "as code."
     )

-    requires_system_checks = False
+    requires_system_checks = []
     shells = ['ipython', 'bpython', 'python']

     def add_arguments(self, parser):
@@ -41,6 +41,7 @@

     def python(self, options):
         import code
+
         # Set up a dictionary to serve as the environment for the shell, so
         # that tab completion works on objects that are imported at runtime.
         imported_objects = {}
@@ -83,13 +84,13 @@
     def handle(self, **options):
         # Execute the command and exit.
         if options['command']:
-            exec(options['command'])
+            exec(options['command'], globals())
             return

         # Execute stdin if it has anything to read and exit.
         # Not supported on Windows due to select.select() limitations.
         if sys.platform != 'win32' and not sys.stdin.isatty() and select.select([sys.stdin], [], [], 0)[0]:
-            exec(sys.stdin.read())
+            exec(sys.stdin.read(), globals())
             return

         available_shells = [options['interface']] if options['interface'] else self.shells
('django/core/management/commands', 'dumpdata.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,11 +1,24 @@
+import gzip
+import os
 import warnings
-from collections import OrderedDict

 from django.apps import apps
 from django.core import serializers
 from django.core.management.base import BaseCommand, CommandError
 from django.core.management.utils import parse_apps_and_model_labels
 from django.db import DEFAULT_DB_ALIAS, router
+
+try:
+    import bz2
+    has_bz2 = True
+except ImportError:
+    has_bz2 = False
+
+try:
+    import lzma
+    has_lzma = True
+except ImportError:
+    has_lzma = False


 class ProxyModelWarning(Warning):
@@ -87,14 +100,14 @@
         if not app_labels:
             if primary_keys:
                 raise CommandError("You can only use --pks option with one model")
-            app_list = OrderedDict.fromkeys(
+            app_list = dict.fromkeys(
                 app_config for app_config in apps.get_app_configs()
                 if app_config.models_module is not None and app_config not in excluded_apps
             )
         else:
             if len(app_labels) > 1 and primary_keys:
                 raise CommandError("You can only use --pks option with one model")
-            app_list = OrderedDict()
+            app_list = {}
             for label in app_labels:
                 try:
                     app_label, model_label = label.split('.')
@@ -111,12 +124,11 @@

                     app_list_value = app_list.setdefault(app_config, [])

-                    # We may have previously seen a "all-models" request for
+                    # We may have previously seen an "all-models" request for
                     # this app (no model qualifier was given). In this case
                     # there is no need adding specific models to the list.
-                    if app_list_value is not None:
-                        if model not in app_list_value:
-                            app_list_value.append(model)
+                    if app_list_value is not None and model not in app_list_value:
+                        app_list_value.append(model)
                 except ValueError:
                     if primary_keys:
                         raise CommandError("You can only use --pks option with one model")
@@ -145,7 +157,17 @@
             Collate the objects to be serialized. If count_only is True, just
             count the number of objects to be serialized.
             """
-            models = serializers.sort_dependencies(app_list.items())
+            if use_natural_foreign_keys:
+                models = serializers.sort_dependencies(app_list.items(), allow_cycles=True)
+            else:
+                # There is no need to sort dependencies when natural foreign
+                # keys are not used.
+                models = []
+                for (app_config, model_list) in app_list.items():
+                    if model_list is None:
+                        models.extend(app_config.get_models())
+                    else:
+                        models.extend(model_list)
             for model in models:
                 if model in excluded_models:
                     continue
@@ -176,7 +198,36 @@
             if output and self.stdout.isatty() and options['verbosity'] > 0:
                 progress_output = self.stdout
                 object_count = sum(get_objects(count_only=True))
-            stream = open(output, 'w') if output else None
+            if output:
+                file_root, file_ext = os.path.splitext(output)
+                compression_formats = {
+                    '.bz2': (open, {}, file_root),
+                    '.gz': (gzip.open, {}, output),
+                    '.lzma': (open, {}, file_root),
+                    '.xz': (open, {}, file_root),
+                    '.zip': (open, {}, file_root),
+                }
+                if has_bz2:
+                    compression_formats['.bz2'] = (bz2.open, {}, output)
+                if has_lzma:
+                    compression_formats['.lzma'] = (
+                        lzma.open, {'format': lzma.FORMAT_ALONE}, output
+                    )
+                    compression_formats['.xz'] = (lzma.open, {}, output)
+                try:
+                    open_method, kwargs, file_path = compression_formats[file_ext]
+                except KeyError:
+                    open_method, kwargs, file_path = (open, {}, output)
+                if file_path != output:
+                    file_name = os.path.basename(file_path)
+                    warnings.warn(
+                        f"Unsupported file extension ({file_ext}). "
+                        f"Fixtures saved in '{file_name}'.",
+                        RuntimeWarning,
+                    )
+                stream = open_method(file_path, 'wt', **kwargs)
+            else:
+                stream = None
             try:
                 serializers.serialize(
                     format, get_objects(), indent=indent,
('django/core/management/commands', 'test.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,14 +3,14 @@
 from django.conf import settings
 from django.core.management.base import BaseCommand
 from django.core.management.utils import get_command_line_option
-from django.test.utils import get_runner
+from django.test.utils import NullTimeKeeper, TimeKeeper, get_runner


 class Command(BaseCommand):
     help = 'Discover and run tests in the specified modules or the current directory.'

     # DiscoverRunner runs the checks after databases are set up.
-    requires_system_checks = False
+    requires_system_checks = []
     test_runner = None

     def run_from_argv(self, argv):
@@ -49,8 +49,10 @@
     def handle(self, *test_labels, **options):
         TestRunner = get_runner(settings, options['testrunner'])

+        time_keeper = TimeKeeper() if options.get('timing', False) else NullTimeKeeper()
         test_runner = TestRunner(**options)
-        failures = test_runner.run_tests(test_labels)
-
+        with time_keeper.timed('Total run'):
+            failures = test_runner.run_tests(test_labels)
+        time_keeper.print_results()
         if failures:
             sys.exit(1)
('django/core/management/commands', 'flush.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -44,13 +44,13 @@
             except ImportError:
                 pass

-        sql_list = sql_flush(self.style, connection, only_django=True,
+        sql_list = sql_flush(self.style, connection,
                              reset_sequences=reset_sequences,
                              allow_cascade=allow_cascade)

         if interactive:
             confirm = input("""You have requested a flush of the database.
-This will IRREVERSIBLY DESTROY all data currently in the %r database,
+This will IRREVERSIBLY DESTROY all data currently in the "%s" database,
 and return each table to an empty state.
 Are you sure you want to do this?

@@ -60,7 +60,7 @@

         if confirm == 'yes':
             try:
-                connection.ops.execute_sql_flush(database, sql_list)
+                connection.ops.execute_sql_flush(sql_list)
             except Exception as exc:
                 raise CommandError(
                     "Database %s couldn't be flushed. Possible reasons:\n"
@@ -68,7 +68,7 @@
                     "  * At least one of the expected database tables doesn't exist.\n"
                     "  * The SQL was invalid.\n"
                     "Hint: Look at the output of 'django-admin sqlflush'. "
-                    "That's the SQL this command wasn't able to run.\n" % (
+                    "That's the SQL this command wasn't able to run." % (
                         connection.settings_dict['NAME'],
                     )
                 ) from exc
@@ -79,4 +79,4 @@
                 # respond as if the database had been migrated from scratch.
                 emit_post_migrate_signal(verbosity, interactive, database)
         else:
-            self.stdout.write("Flush cancelled.\n")
+            self.stdout.write('Flush cancelled.')
('django/core/management/commands', 'loaddata.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -26,6 +26,12 @@
 except ImportError:
     has_bz2 = False

+try:
+    import lzma
+    has_lzma = True
+except ImportError:
+    has_lzma = False
+
 READ_STDIN = '-'


@@ -97,6 +103,9 @@
         }
         if has_bz2:
             self.compression_formats['bz2'] = (bz2.BZ2File, 'r')
+        if has_lzma:
+            self.compression_formats['lzma'] = (lzma.LZMAFile, 'r')
+            self.compression_formats['xz'] = (lzma.LZMAFile, 'r')

         # Django's test suite repeatedly tries to load initial_data fixtures
         # from apps that don't have any fixtures. Because disabling constraint
@@ -130,7 +139,7 @@
             sequence_sql = connection.ops.sequence_reset_sql(no_style(), self.models)
             if sequence_sql:
                 if self.verbosity >= 2:
-                    self.stdout.write("Resetting sequences\n")
+                    self.stdout.write('Resetting sequences')
                 with connection.cursor() as cursor:
                     for line in sequence_sql:
                         cursor.execute(line)
@@ -186,9 +195,8 @@
                                 )
                         # psycopg2 raises ValueError if data contains NUL chars.
                         except (DatabaseError, IntegrityError, ValueError) as e:
-                            e.args = ("Could not load %(app_label)s.%(object_name)s(pk=%(pk)s): %(error_msg)s" % {
-                                'app_label': obj.object._meta.app_label,
-                                'object_name': obj.object._meta.object_name,
+                            e.args = ("Could not load %(object_label)s(pk=%(pk)s): %(error_msg)s" % {
+                                'object_label': obj.object._meta.label,
                                 'pk': obj.object.pk,
                                 'error_msg': e,
                             },)
@@ -196,7 +204,7 @@
                     if obj.deferred_fields:
                         self.objs_with_deferred_fields.append(obj)
                 if objects and show_progress:
-                    self.stdout.write('')  # add a newline after progress indicator
+                    self.stdout.write()  # Add a newline after progress indicator.
                 self.loaded_object_count += loaded_objects_in_fixture
                 self.fixture_object_count += objects_in_fixture
             except Exception as e:
@@ -223,7 +231,7 @@
         fixture_name, ser_fmt, cmp_fmt = self.parse_name(fixture_label)
         databases = [self.using, None]
         cmp_fmts = list(self.compression_formats) if cmp_fmt is None else [cmp_fmt]
-        ser_fmts = serializers.get_public_serializer_formats() if ser_fmt is None else [ser_fmt]
+        ser_fmts = self.serialization_formats if ser_fmt is None else [ser_fmt]

         if self.verbosity >= 2:
             self.stdout.write("Loading '%s' fixtures..." % fixture_name)
@@ -300,8 +308,7 @@
                 dirs.append(app_dir)
         dirs.extend(fixture_dirs)
         dirs.append('')
-        dirs = [os.path.abspath(os.path.realpath(d)) for d in dirs]
-        return dirs
+        return [os.path.realpath(d) for d in dirs]

     def parse_name(self, fixture_name):
         """
('django/core/management/commands', 'runserver.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -11,8 +11,9 @@
     WSGIServer, get_internal_wsgi_application, run,
 )
 from django.utils import autoreload
+from django.utils.regex_helper import _lazy_re_compile

-naiveip_re = re.compile(r"""^(?:
+naiveip_re = _lazy_re_compile(r"""^(?:
 (?P<addr>
     (?P<ipv4>\d{1,3}(?:\.\d{1,3}){3}) |         # IPv4 address
     (?P<ipv6>\[[a-fA-F0-9:]+\]) |               # IPv6 address
@@ -24,7 +25,7 @@
     help = "Starts a lightweight Web server for development."

     # Validation is called explicitly each time the server is reloaded.
-    requires_system_checks = False
+    requires_system_checks = []
     stealth_options = ('shutdown_message',)

     default_addr = '127.0.0.1'
@@ -123,7 +124,7 @@
         self.stdout.write((
             "Django version %(version)s, using settings %(settings)r\n"
             "Starting development server at %(protocol)s://%(addr)s:%(port)s/\n"
-            "Quit the server with %(quit_command)s.\n"
+            "Quit the server with %(quit_command)s."
         ) % {
             "version": self.get_version(),
             "settings": settings.SETTINGS_MODULE,
@@ -137,7 +138,7 @@
             handler = self.get_handler(*args, **options)
             run(self.addr, int(self.port), handler,
                 ipv6=self.use_ipv6, threading=threading, server_cls=self.server_cls)
-        except socket.error as e:
+        except OSError as e:
             # Use helpful error messages instead of ugly tracebacks.
             ERRORS = {
                 errno.EACCES: "You don't have permission to access that port.",
@@ -155,7 +156,3 @@
             if shutdown_message:
                 self.stdout.write(shutdown_message)
             sys.exit(0)
-
-
-# Kept for backward compatibility
-BaseRunserverCommand = Command
('django/core/management/commands', 'showmigrations.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -22,7 +22,11 @@
         formats = parser.add_mutually_exclusive_group()
         formats.add_argument(
             '--list', '-l', action='store_const', dest='format', const='list',
-            help='Shows a list of all migrations and which are applied.',
+            help=(
+                'Shows a list of all migrations and which are applied. '
+                'With a verbosity level of 2 or above, the applied datetimes '
+                'will be included.'
+            ),
         )
         formats.add_argument(
             '--plan', '-p', action='store_const', dest='format', const='plan',
@@ -84,9 +88,13 @@
                         title = plan_node[1]
                         if graph.nodes[plan_node].replaces:
                             title += " (%s squashed migrations)" % len(graph.nodes[plan_node].replaces)
+                        applied_migration = loader.applied_migrations.get(plan_node)
                         # Mark it as applied/unapplied
-                        if plan_node in loader.applied_migrations:
-                            self.stdout.write(" [X] %s" % title)
+                        if applied_migration:
+                            output = ' [X] %s' % title
+                            if self.verbosity >= 2:
+                                output += ' (applied at %s)' % applied_migration.applied.strftime('%Y-%m-%d %H:%M:%S')
+                            self.stdout.write(output)
                         else:
                             self.stdout.write(" [ ] %s" % title)
                         shown.add(plan_node)
('django/core/management/commands', 'sqlsequencereset.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -20,4 +20,6 @@
         connection = connections[options['database']]
         models = app_config.get_models(include_auto_created=True)
         statements = connection.ops.sequence_reset_sql(self.style, models)
+        if not statements and options['verbosity'] >= 1:
+            self.stderr.write('No sequences found.')
         return '\n'.join(statements)
('django/core/management/commands', 'dbshell.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,5 @@
+import subprocess
+
 from django.core.management.base import BaseCommand, CommandError
 from django.db import DEFAULT_DB_ALIAS, connections

@@ -8,24 +10,34 @@
         "default database if none is provided."
     )

-    requires_system_checks = False
+    requires_system_checks = []

     def add_arguments(self, parser):
         parser.add_argument(
             '--database', default=DEFAULT_DB_ALIAS,
             help='Nominates a database onto which to open a shell. Defaults to the "default" database.',
         )
+        parameters = parser.add_argument_group('parameters', prefix_chars='--')
+        parameters.add_argument('parameters', nargs='*')

     def handle(self, **options):
         connection = connections[options['database']]
         try:
-            connection.client.runshell()
-        except OSError:
-            # Note that we're assuming OSError means that the client program
-            # isn't installed. There's a possibility OSError would be raised
-            # for some other reason, in which case this error message would be
-            # inaccurate. Still, this message catches the common case.
+            connection.client.runshell(options['parameters'])
+        except FileNotFoundError:
+            # Note that we're assuming the FileNotFoundError relates to the
+            # command missing. It could be raised for some other reason, in
+            # which case this error message would be inaccurate. Still, this
+            # message catches the common case.
             raise CommandError(
                 'You appear not to have the %r program installed or on your path.' %
                 connection.client.executable_name
             )
+        except subprocess.CalledProcessError as e:
+            raise CommandError(
+                '"%s" returned non-zero exit status %s.' % (
+                    ' '.join(e.cmd),
+                    e.returncode,
+                ),
+                returncode=e.returncode,
+            )
('django/core/management/commands', 'startproject.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,4 @@
+from django.core.checks.security.base import SECRET_KEY_INSECURE_PREFIX
 from django.core.management.templates import TemplateCommand

 from ..utils import get_random_secret_key
@@ -15,6 +16,6 @@
         target = options.pop('directory')

         # Create a random SECRET_KEY to put it in the main settings.
-        options['secret_key'] = get_random_secret_key()
+        options['secret_key'] = SECRET_KEY_INSECURE_PREFIX + get_random_secret_key()

         super().handle('project', project_name, target, **options)
('django/core/management/commands', 'migrate.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,9 +1,8 @@
+import sys
 import time
-from collections import OrderedDict
 from importlib import import_module

 from django.apps import apps
-from django.core.checks import Tags, run_checks
 from django.core.management.base import (
     BaseCommand, CommandError, no_translations,
 )
@@ -21,8 +20,13 @@

 class Command(BaseCommand):
     help = "Updates database schema. Manages both apps with migrations and those without."
+    requires_system_checks = []

     def add_arguments(self, parser):
+        parser.add_argument(
+            '--skip-checks', action='store_true',
+            help='Skip system checks.',
+        )
         parser.add_argument(
             'app_label', nargs='?',
             help='App label of an application to synchronize the state.',
@@ -59,14 +63,16 @@
             '--run-syncdb', action='store_true',
             help='Creates tables for apps without migrations.',
         )
-
-    def _run_checks(self, **kwargs):
-        issues = run_checks(tags=[Tags.database])
-        issues.extend(super()._run_checks(**kwargs))
-        return issues
+        parser.add_argument(
+            '--check', action='store_true', dest='check_unapplied',
+            help='Exits with a non-zero status if unapplied migrations exist.',
+        )

     @no_translations
     def handle(self, *args, **options):
+        database = options['database']
+        if not options['skip_checks']:
+            self.check(databases=[database])

         self.verbosity = options['verbosity']
         self.interactive = options['interactive']
@@ -78,8 +84,7 @@
                 import_module('.management', app_config.name)

         # Get the database we're operating from
-        db = options['database']
-        connection = connections[db]
+        connection = connections[database]

         # Hook for backends needing any database preparation
         connection.prepare_database()
@@ -143,6 +148,7 @@
             targets = executor.loader.graph.leaf_nodes()

         plan = executor.migration_plan(targets)
+        exit_dry = plan and options['check_unapplied']

         if options['plan']:
             self.stdout.write('Planned operations:', self.style.MIGRATE_LABEL)
@@ -154,7 +160,11 @@
                     message, is_error = self.describe_operation(operation, backwards)
                     style = self.style.WARNING if is_error else None
                     self.stdout.write('    ' + message, style)
+            if exit_dry:
+                sys.exit(1)
             return
+        if exit_dry:
+            sys.exit(1)

         # At this point, ignore run_syncdb if there aren't any apps to sync.
         run_syncdb = options['run_syncdb'] and executor.loader.unmigrated_apps
@@ -178,8 +188,9 @@
                 )
             else:
                 if targets[0][1] is None:
-                    self.stdout.write(self.style.MIGRATE_LABEL(
-                        "  Unapply all migrations: ") + "%s" % (targets[0][0],)
+                    self.stdout.write(
+                        self.style.MIGRATE_LABEL('  Unapply all migrations: ') +
+                        str(targets[0][0])
                     )
                 else:
                     self.stdout.write(self.style.MIGRATE_LABEL(
@@ -216,8 +227,9 @@
                 changes = autodetector.changes(graph=executor.loader.graph)
                 if changes:
                     self.stdout.write(self.style.NOTICE(
-                        "  Your models have changes that are not yet reflected "
-                        "in a migration, and so won't be applied."
+                        "  Your models in app(s): %s have changes that are not "
+                        "yet reflected in a migration, and so won't be "
+                        "applied." % ", ".join(repr(app) for app in sorted(changes))
                     ))
                     self.stdout.write(self.style.NOTICE(
                         "  Run 'manage.py makemigrations' to make new "
@@ -262,33 +274,33 @@
             compute_time = self.verbosity > 1
             if action == "apply_start":
                 if compute_time:
-                    self.start = time.time()
+                    self.start = time.monotonic()
                 self.stdout.write("  Applying %s..." % migration, ending="")
                 self.stdout.flush()
             elif action == "apply_success":
-                elapsed = " (%.3fs)" % (time.time() - self.start) if compute_time else ""
+                elapsed = " (%.3fs)" % (time.monotonic() - self.start) if compute_time else ""
                 if fake:
                     self.stdout.write(self.style.SUCCESS(" FAKED" + elapsed))
                 else:
                     self.stdout.write(self.style.SUCCESS(" OK" + elapsed))
             elif action == "unapply_start":
                 if compute_time:
-                    self.start = time.time()
+                    self.start = time.monotonic()
                 self.stdout.write("  Unapplying %s..." % migration, ending="")
                 self.stdout.flush()
             elif action == "unapply_success":
-                elapsed = " (%.3fs)" % (time.time() - self.start) if compute_time else ""
+                elapsed = " (%.3fs)" % (time.monotonic() - self.start) if compute_time else ""
                 if fake:
                     self.stdout.write(self.style.SUCCESS(" FAKED" + elapsed))
                 else:
                     self.stdout.write(self.style.SUCCESS(" OK" + elapsed))
             elif action == "render_start":
                 if compute_time:
-                    self.start = time.time()
+                    self.start = time.monotonic()
                 self.stdout.write("  Rendering model states...", ending="")
                 self.stdout.flush()
             elif action == "render_success":
-                elapsed = " (%.3fs)" % (time.time() - self.start) if compute_time else ""
+                elapsed = " (%.3fs)" % (time.monotonic() - self.start) if compute_time else ""
                 self.stdout.write(self.style.SUCCESS(" DONE" + elapsed))

     def sync_apps(self, connection, app_labels):
@@ -314,14 +326,14 @@
                 (opts.auto_created and converter(opts.auto_created._meta.db_table) in tables)
             )

-        manifest = OrderedDict(
-            (app_name, list(filter(model_installed, model_list)))
+        manifest = {
+            app_name: list(filter(model_installed, model_list))
             for app_name, model_list in all_models
-        )
+        }

         # Create the tables for each model
         if self.verbosity >= 1:
-            self.stdout.write("  Creating tables...\n")
+            self.stdout.write('  Creating tables...')
         with connection.schema_editor() as editor:
             for app_name, model_list in manifest.items():
                 for model in model_list:
@@ -330,35 +342,35 @@
                         continue
                     if self.verbosity >= 3:
                         self.stdout.write(
-                            "    Processing %s.%s model\n" % (app_name, model._meta.object_name)
+                            '    Processing %s.%s model' % (app_name, model._meta.object_name)
                         )
                     if self.verbosity >= 1:
-                        self.stdout.write("    Creating table %s\n" % model._meta.db_table)
+                        self.stdout.write('    Creating table %s' % model._meta.db_table)
                     editor.create_model(model)

             # Deferred SQL is executed when exiting the editor's context.
             if self.verbosity >= 1:
-                self.stdout.write("    Running deferred SQL...\n")
+                self.stdout.write('    Running deferred SQL...')

     @staticmethod
     def describe_operation(operation, backwards):
         """Return a string that describes a migration operation for --plan."""
         prefix = ''
+        is_error = False
         if hasattr(operation, 'code'):
             code = operation.reverse_code if backwards else operation.code
-            action = code.__doc__ if code else ''
+            action = (code.__doc__ or '') if code else None
         elif hasattr(operation, 'sql'):
             action = operation.reverse_sql if backwards else operation.sql
         else:
             action = ''
             if backwards:
                 prefix = 'Undo '
-        if action is None:
+        if action is not None:
+            action = str(action).replace('\n', '')
+        elif backwards:
             action = 'IRREVERSIBLE'
             is_error = True
-        else:
-            action = str(action).replace('\n', '')
-            is_error = False
         if action:
             action = ' -> ' + action
         truncated = Truncator(action)
('django/core/management/commands', 'compilemessages.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,13 +2,16 @@
 import concurrent.futures
 import glob
 import os
+from pathlib import Path

 from django.core.management.base import BaseCommand, CommandError
-from django.core.management.utils import find_command, popen_wrapper
+from django.core.management.utils import (
+    find_command, is_ignored_path, popen_wrapper,
+)


 def has_bom(fn):
-    with open(fn, 'rb') as f:
+    with fn.open('rb') as f:
         sample = f.read(4)
     return sample.startswith((codecs.BOM_UTF8, codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE))

@@ -19,7 +22,7 @@
     try:
         with open(path, 'a'):
             os.utime(path, None)
-    except (IOError, OSError):
+    except OSError:
         return False
     return True

@@ -27,7 +30,7 @@
 class Command(BaseCommand):
     help = 'Compiles .po files to .mo files for use with builtin gettext support.'

-    requires_system_checks = False
+    requires_system_checks = []

     program = 'msgfmt'
     program_options = ['--check-format']
@@ -46,10 +49,17 @@
             '--use-fuzzy', '-f', dest='fuzzy', action='store_true',
             help='Use fuzzy translations.',
         )
+        parser.add_argument(
+            '--ignore', '-i', action='append', dest='ignore_patterns',
+            default=[], metavar='PATTERN',
+            help='Ignore directories matching this glob-style pattern. '
+                 'Use multiple times to ignore more.',
+        )

     def handle(self, **options):
         locale = options['locale']
         exclude = options['exclude']
+        ignore_patterns = set(options['ignore_patterns'])
         self.verbosity = options['verbosity']
         if options['fuzzy']:
             self.program_options = self.program_options + ['-f']
@@ -66,7 +76,9 @@
         # Walk entire tree, looking for locale directories
         for dirpath, dirnames, filenames in os.walk('.', topdown=True):
             for dirname in dirnames:
-                if dirname == 'locale':
+                if is_ignored_path(os.path.normpath(os.path.join(dirpath, dirname)), ignore_patterns):
+                    dirnames.remove(dirname)
+                elif dirname == 'locale':
                     basedirs.append(os.path.join(dirpath, dirname))

         # Gather existing directories.
@@ -90,7 +102,7 @@
         self.has_errors = False
         for basedir in basedirs:
             if locales:
-                dirs = [os.path.join(basedir, l, 'LC_MESSAGES') for l in locales]
+                dirs = [os.path.join(basedir, locale, 'LC_MESSAGES') for locale in locales]
             else:
                 dirs = [basedir]
             locations = []
@@ -110,9 +122,21 @@
         with concurrent.futures.ThreadPoolExecutor() as executor:
             futures = []
             for i, (dirpath, f) in enumerate(locations):
+                po_path = Path(dirpath) / f
+                mo_path = po_path.with_suffix('.mo')
+                try:
+                    if mo_path.stat().st_mtime >= po_path.stat().st_mtime:
+                        if self.verbosity > 0:
+                            self.stdout.write(
+                                'File “%s” is already compiled and up to date.'
+                                % po_path
+                            )
+                        continue
+                except FileNotFoundError:
+                    pass
                 if self.verbosity > 0:
-                    self.stdout.write('processing file %s in %s\n' % (f, dirpath))
-                po_path = os.path.join(dirpath, f)
+                    self.stdout.write('processing file %s in %s' % (f, dirpath))
+
                 if has_bom(po_path):
                     self.stderr.write(
                         'The %s file has a BOM (Byte Order Mark). Django only '
@@ -120,10 +144,9 @@
                     )
                     self.has_errors = True
                     continue
-                base_path = os.path.splitext(po_path)[0]

                 # Check writability on first location
-                if i == 0 and not is_writable(base_path + '.mo'):
+                if i == 0 and not is_writable(mo_path):
                     self.stderr.write(
                         'The po files under %s are in a seemingly not writable location. '
                         'mo files will not be updated/created.' % dirpath
@@ -131,9 +154,9 @@
                     self.has_errors = True
                     return

-                args = [self.program] + self.program_options + [
-                    '-o', base_path + '.mo', base_path + '.po'
-                ]
+                # PY37: Remove str() when dropping support for PY37.
+                # https://bugs.python.org/issue31961
+                args = [self.program, *self.program_options, '-o', str(mo_path), str(po_path)]
                 futures.append(executor.submit(popen_wrapper, args))

             for future in concurrent.futures.as_completed(futures):
('django/core/management/commands', 'diffsettings.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -10,7 +10,7 @@
     help = """Displays differences between the current settings.py and Django's
     default settings."""

-    requires_system_checks = False
+    requires_system_checks = []

     def add_arguments(self, parser):
         parser.add_argument(
@@ -39,7 +39,7 @@
         )

     def handle(self, **options):
-        from django.conf import settings, Settings, global_settings
+        from django.conf import Settings, global_settings, settings

         # Because settings are imported lazily, we need to explicitly load them.
         if not settings.configured:
('django/core/management/commands', 'testserver.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -6,7 +6,7 @@
 class Command(BaseCommand):
     help = 'Runs a development server with data from the given fixture(s).'

-    requires_system_checks = False
+    requires_system_checks = []

     def add_arguments(self, parser):
         parser.add_argument(
('django/core/serializers', 'pyyaml.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -18,10 +18,9 @@

 # Use the C (faster) implementation if possible
 try:
-    from yaml import CSafeLoader as SafeLoader
-    from yaml import CSafeDumper as SafeDumper
+    from yaml import CSafeDumper as SafeDumper, CSafeLoader as SafeLoader
 except ImportError:
-    from yaml import SafeLoader, SafeDumper
+    from yaml import SafeDumper, SafeLoader


 class DjangoSafeDumper(SafeDumper):
@@ -34,6 +33,9 @@

 DjangoSafeDumper.add_representer(decimal.Decimal, DjangoSafeDumper.represent_decimal)
 DjangoSafeDumper.add_representer(collections.OrderedDict, DjangoSafeDumper.represent_ordered_dict)
+# Workaround to represent dictionaries in insertion order.
+# See https://github.com/yaml/pyyaml/pull/143.
+DjangoSafeDumper.add_representer(dict, DjangoSafeDumper.represent_ordered_dict)


 class Serializer(PythonSerializer):
@@ -54,6 +56,7 @@
             super().handle_field(obj, field)

     def end_serialization(self):
+        self.options.setdefault('allow_unicode', True)
         yaml.dump(self.objects, self.stream, Dumper=DjangoSafeDumper, **self.options)

     def getvalue(self):
('django/core/serializers', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -28,6 +28,7 @@
     "python": "django.core.serializers.python",
     "json": "django.core.serializers.json",
     "yaml": "django.core.serializers.pyyaml",
+    "jsonl": "django.core.serializers.jsonl",
 }

 _serializers = {}
@@ -156,12 +157,15 @@
     _serializers = serializers


-def sort_dependencies(app_list):
+def sort_dependencies(app_list, allow_cycles=False):
     """Sort a list of (app_config, models) pairs into a single list of models.

     The single list of models is sorted so that any model with a natural key
     is serialized before a normal model, and any model with a natural key
     dependency has it's dependencies serialized first.
+
+    If allow_cycles is True, return the best-effort ordering that will respect
+    most of dependencies but ignore some of them to break the cycles.
     """
     # Process the list of models, and get the list of dependencies
     model_dependencies = []
@@ -222,13 +226,20 @@
             else:
                 skipped.append((model, deps))
         if not changed:
-            raise RuntimeError(
-                "Can't resolve dependencies for %s in serialized app list." %
-                ', '.join(
-                    '%s.%s' % (model._meta.app_label, model._meta.object_name)
-                    for model, deps in sorted(skipped, key=lambda obj: obj[0].__name__)
+            if allow_cycles:
+                # If cycles are allowed, add the last skipped model and ignore
+                # its dependencies. This could be improved by some graph
+                # analysis to ignore as few dependencies as possible.
+                model, _ = skipped.pop()
+                model_list.append(model)
+            else:
+                raise RuntimeError(
+                    "Can't resolve dependencies for %s in serialized app list."
+                    % ', '.join(
+                        model._meta.label
+                        for model, deps in sorted(skipped, key=lambda obj: obj[0].__name__)
+                    ),
                 )
-            )
         model_dependencies = skipped

     return model_list
('django/core/serializers', 'xml_serializer.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,7 +1,7 @@
 """
 XML serializer.
 """
-
+import json
 from xml.dom import pulldom
 from xml.sax import handler
 from xml.sax.expatreader import ExpatParser as _ExpatParser
@@ -75,8 +75,13 @@

         # Get a "string version" of the object's data.
         if getattr(obj, field.name) is not None:
+            value = field.value_to_string(obj)
+            if field.get_internal_type() == 'JSONField':
+                # Dump value since JSONField.value_to_string() doesn't output
+                # strings.
+                value = json.dumps(value, cls=field.encoder)
             try:
-                self.xml.characters(field.value_to_string(obj))
+                self.xml.characters(value)
             except UnserializableContentError:
                 raise ValueError("%s.%s (pk:%s) contains unserializable characters" % (
                     obj.__class__.__name__, field.name, obj.pk))
@@ -132,7 +137,11 @@
                     self.xml.addQuickElement("object", attrs={
                         'pk': str(value.pk)
                     })
-            for relobj in getattr(obj, field.name).iterator():
+            m2m_iter = getattr(obj, '_prefetched_objects_cache', {}).get(
+                field.name,
+                getattr(obj, field.name).iterator(),
+            )
+            for relobj in m2m_iter:
                 handle_m2m(relobj)

             self.xml.endElement("field")
@@ -228,6 +237,9 @@
                     value = None
                 else:
                     value = field.to_python(getInnerText(field_node).strip())
+                    # Load value since JSONField.to_python() outputs strings.
+                    if field.get_internal_type() == 'JSONField':
+                        value = json.loads(value, cls=field.decoder)
                 data[field.name] = value

         obj = base.build_instance(Model, data, self.db)
('django/core/serializers', 'python.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,7 +3,6 @@
 and from basic Python data types (lists, dicts, strings, etc.). Useful as a basis for
 other serializers.
 """
-from collections import OrderedDict

 from django.apps import apps
 from django.core.serializers import base
@@ -26,14 +25,14 @@
         pass

     def start_object(self, obj):
-        self._current = OrderedDict()
+        self._current = {}

     def end_object(self, obj):
         self.objects.append(self.get_dump_object(obj))
         self._current = None

     def get_dump_object(self, obj):
-        data = OrderedDict([('model', str(obj._meta))])
+        data = {'model': str(obj._meta)}
         if not self.use_natural_primary_keys or not hasattr(obj, 'natural_key'):
             data["pk"] = self._value_from_field(obj, obj._meta.pk)
         data['fields'] = self._current
@@ -68,9 +67,11 @@
             else:
                 def m2m_value(value):
                     return self._value_from_field(value, value._meta.pk)
-            self._current[field.name] = [
-                m2m_value(related) for related in getattr(obj, field.name).iterator()
-            ]
+            m2m_iter = getattr(obj, '_prefetched_objects_cache', {}).get(
+                field.name,
+                getattr(obj, field.name).iterator(),
+            )
+            self._current[field.name] = [m2m_value(related) for related in m2m_iter]

     def getvalue(self):
         return self.objects
('django/core/serializers', 'json.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -29,6 +29,7 @@
             # Prevent trailing spaces
             self.json_kwargs['separators'] = (',', ': ')
         self.json_kwargs.setdefault('cls', DjangoJSONEncoder)
+        self.json_kwargs.setdefault('ensure_ascii', False)

     def start_serialization(self):
         self._init_options()
('django/core/serializers', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -108,7 +108,7 @@
                     else:
                         if self.selected_fields is None or field.attname[:-3] in self.selected_fields:
                             self.handle_fk_field(obj, field)
-            for field in concrete_model._meta.many_to_many:
+            for field in concrete_model._meta.local_many_to_many:
                 if field.serialize:
                     if self.selected_fields is None or field.attname in self.selected_fields:
                         self.handle_m2m_field(obj, field)
@@ -146,19 +146,19 @@
         """
         Called to handle each individual (non-relational) field on an object.
         """
-        raise NotImplementedError('subclasses of Serializer must provide an handle_field() method')
+        raise NotImplementedError('subclasses of Serializer must provide a handle_field() method')

     def handle_fk_field(self, obj, field):
         """
         Called to handle a ForeignKey field.
         """
-        raise NotImplementedError('subclasses of Serializer must provide an handle_fk_field() method')
+        raise NotImplementedError('subclasses of Serializer must provide a handle_fk_field() method')

     def handle_m2m_field(self, obj, field):
         """
         Called to handle a ManyToManyField.
         """
-        raise NotImplementedError('subclasses of Serializer must provide an handle_m2m_field() method')
+        raise NotImplementedError('subclasses of Serializer must provide a handle_m2m_field() method')

     def getvalue(self):
         """
@@ -257,7 +257,7 @@
     natural keys, try to retrieve it from the database.
     """
     default_manager = Model._meta.default_manager
-    pk = data.get(Model._meta.pk.name)
+    pk = data.get(Model._meta.pk.attname)
     if (pk is None and hasattr(default_manager, 'get_by_natural_key') and
             hasattr(Model, 'natural_key')):
         natural_key = Model(**data).natural_key()
@@ -283,8 +283,12 @@
             return model._meta.pk.to_python(v)

     try:
+        pks_iter = iter(field_value)
+    except TypeError as e:
+        raise M2MDeserializationError(e, field_value)
+    try:
         values = []
-        for pk in field_value:
+        for pk in pks_iter:
             values.append(m2m_convert(pk))
         return values
     except Exception as e:
('django/core/files', 'locks.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -28,8 +28,10 @@

 if os.name == 'nt':
     import msvcrt
-    from ctypes import (sizeof, c_ulong, c_void_p, c_int64,
-                        Structure, Union, POINTER, windll, byref)
+    from ctypes import (
+        POINTER, Structure, Union, byref, c_int64, c_ulong, c_void_p, sizeof,
+        windll,
+    )
     from ctypes.wintypes import BOOL, DWORD, HANDLE

     LOCK_SH = 0  # the default
@@ -105,9 +107,12 @@
             return True
     else:
         def lock(f, flags):
-            ret = fcntl.flock(_fd(f), flags)
-            return ret == 0
+            try:
+                fcntl.flock(_fd(f), flags)
+                return True
+            except BlockingIOError:
+                return False

         def unlock(f):
-            ret = fcntl.flock(_fd(f), fcntl.LOCK_UN)
-            return ret == 0
+            fcntl.flock(_fd(f), fcntl.LOCK_UN)
+            return True
('django/core/files', 'uploadhandler.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,7 +1,7 @@
 """
 Base file upload handler classes, and the built-in concrete subclasses
 """
-
+import os
 from io import BytesIO

 from django.conf import settings
@@ -52,7 +52,7 @@

 class StopFutureHandlers(UploadFileException):
     """
-    Upload handers that have handled a file and do not want future handlers to
+    Upload handlers that have handled a file and do not want future handlers to
     run should raise this exception instead of returning None.
     """
     pass
@@ -127,6 +127,13 @@
         """
         pass

+    def upload_interrupted(self):
+        """
+        Signal that the upload was interrupted. Subclasses should perform
+        cleanup that is necessary for this handler.
+        """
+        pass
+

 class TemporaryFileUploadHandler(FileUploadHandler):
     """
@@ -146,6 +153,15 @@
         self.file.seek(0)
         self.file.size = file_size
         return self.file
+
+    def upload_interrupted(self):
+        if hasattr(self, 'file'):
+            temp_location = self.file.temporary_file_path()
+            try:
+                self.file.close()
+                os.remove(temp_location)
+            except FileNotFoundError:
+                pass


 class MemoryFileUploadHandler(FileUploadHandler):
('django/core/files', 'utils.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,29 @@
+import os
+import pathlib
+
+from django.core.exceptions import SuspiciousFileOperation
+
+
+def validate_file_name(name, allow_relative_path=False):
+    # Remove potentially dangerous names
+    if os.path.basename(name) in {'', '.', '..'}:
+        raise SuspiciousFileOperation("Could not derive file name from '%s'" % name)
+
+    if allow_relative_path:
+        # Use PurePosixPath() because this branch is checked only in
+        # FileField.generate_filename() where all file paths are expected to be
+        # Unix style (with forward slashes).
+        path = pathlib.PurePosixPath(name)
+        if path.is_absolute() or '..' in path.parts:
+            raise SuspiciousFileOperation(
+                "Detected path traversal attempt in '%s'" % name
+            )
+    elif name != os.path.basename(name):
+        raise SuspiciousFileOperation("File name '%s' includes path elements" % name)
+
+    return name
+
+
 class FileProxyMixin:
     """
     A mixin class used to forward file methods to an underlaying file
('django/core/files', 'uploadedfile.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -8,6 +8,7 @@
 from django.conf import settings
 from django.core.files import temp as tempfile
 from django.core.files.base import File
+from django.core.files.utils import validate_file_name

 __all__ = ('UploadedFile', 'TemporaryUploadedFile', 'InMemoryUploadedFile',
            'SimpleUploadedFile')
@@ -46,6 +47,8 @@
                 name, ext = os.path.splitext(name)
                 ext = ext[:255]
                 name = name[:255 - len(ext)] + ext
+
+            name = validate_file_name(name)

         self._name = name

('django/core/files', 'temp.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -50,7 +50,7 @@
                 self.close_called = True
                 try:
                     self.file.close()
-                except (OSError, IOError):
+                except OSError:
                     pass
                 try:
                     self.unlink(self.name)
('django/core/files', 'storage.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,4 +1,5 @@
 import os
+import pathlib
 from datetime import datetime
 from urllib.parse import urljoin

@@ -6,6 +7,7 @@
 from django.core.exceptions import SuspiciousFileOperation
 from django.core.files import File, locks
 from django.core.files.move import file_move_safe
+from django.core.files.utils import validate_file_name
 from django.core.signals import setting_changed
 from django.utils import timezone
 from django.utils._os import safe_join
@@ -49,7 +51,10 @@
             content = File(content, name)

         name = self.get_available_name(name, max_length=max_length)
-        return self._save(name, content)
+        name = self._save(name, content)
+        # Ensure that the name returned from the storage system is still valid.
+        validate_file_name(name, allow_relative_path=True)
+        return name

     # These methods are part of the public API, with default implementations.

@@ -60,21 +65,32 @@
         """
         return get_valid_filename(name)

+    def get_alternative_name(self, file_root, file_ext):
+        """
+        Return an alternative filename, by adding an underscore and a random 7
+        character alphanumeric string (before the file extension, if one
+        exists) to the filename.
+        """
+        return '%s_%s%s' % (file_root, get_random_string(7), file_ext)
+
     def get_available_name(self, name, max_length=None):
         """
         Return a filename that's free on the target storage system and
         available for new content to be written to.
         """
+        name = str(name).replace('\\', '/')
         dir_name, file_name = os.path.split(name)
+        if '..' in pathlib.PurePath(dir_name).parts:
+            raise SuspiciousFileOperation("Detected path traversal attempt in '%s'" % dir_name)
+        validate_file_name(file_name)
         file_root, file_ext = os.path.splitext(file_name)
-        # If the filename already exists, add an underscore and a random 7
-        # character alphanumeric string (before the file extension, if one
-        # exists) to the filename until the generated filename doesn't exist.
+        # If the filename already exists, generate an alternative filename
+        # until it doesn't exist.
         # Truncate original name if required, so the new filename does not
         # exceed the max_length.
         while self.exists(name) or (max_length and len(name) > max_length):
             # file_ext includes the dot.
-            name = os.path.join(dir_name, "%s_%s%s" % (file_root, get_random_string(7), file_ext))
+            name = os.path.join(dir_name, self.get_alternative_name(file_root, file_ext))
             if max_length is None:
                 continue
             # Truncate file_root if max_length exceeded.
@@ -88,7 +104,7 @@
                         'Please make sure that the corresponding file field '
                         'allows sufficient "max_length".' % name
                     )
-                name = os.path.join(dir_name, "%s_%s%s" % (file_root, get_random_string(7), file_ext))
+                name = os.path.join(dir_name, self.get_alternative_name(file_root, file_ext))
         return name

     def generate_filename(self, filename):
@@ -96,8 +112,11 @@
         Validate the filename by calling get_valid_name() and return a filename
         to be passed to the save() method.
         """
+        filename = str(filename).replace('\\', '/')
         # `filename` may include a path as returned by FileField.upload_to.
         dirname, filename = os.path.split(filename)
+        if '..' in pathlib.PurePath(dirname).parts:
+            raise SuspiciousFileOperation("Detected path traversal attempt in '%s'" % dirname)
         return os.path.normpath(os.path.join(dirname, self.get_valid_name(filename)))

     def path(self, name):
@@ -228,25 +247,19 @@

         # Create any intermediate directories that do not exist.
         directory = os.path.dirname(full_path)
-        if not os.path.exists(directory):
-            try:
-                if self.directory_permissions_mode is not None:
-                    # os.makedirs applies the global umask, so we reset it,
-                    # for consistency with file_permissions_mode behavior.
-                    old_umask = os.umask(0)
-                    try:
-                        os.makedirs(directory, self.directory_permissions_mode)
-                    finally:
-                        os.umask(old_umask)
-                else:
-                    os.makedirs(directory)
-            except FileExistsError:
-                # There's a race between os.path.exists() and os.makedirs().
-                # If os.makedirs() fails with FileExistsError, the directory
-                # was created concurrently.
-                pass
-        if not os.path.isdir(directory):
-            raise IOError("%s exists and is not a directory." % directory)
+        try:
+            if self.directory_permissions_mode is not None:
+                # Set the umask because os.makedirs() doesn't apply the "mode"
+                # argument to intermediate-level directories.
+                old_umask = os.umask(0o777 & ~self.directory_permissions_mode)
+                try:
+                    os.makedirs(directory, self.directory_permissions_mode, exist_ok=True)
+                finally:
+                    os.umask(old_umask)
+            else:
+                os.makedirs(directory, exist_ok=True)
+        except FileExistsError:
+            raise FileExistsError('%s exists and is not a directory.' % directory)

         # There's a potential race condition between get_available_name and
         # saving the file; it's possible that two threads might return the
@@ -289,8 +302,10 @@
         if self.file_permissions_mode is not None:
             os.chmod(full_path, self.file_permissions_mode)

+        # Ensure the saved path is always relative to the storage root.
+        name = os.path.relpath(full_path, self.location)
         # Store filenames with forward slashes, even on Windows.
-        return name.replace('\\', '/')
+        return str(name).replace('\\', '/')

     def delete(self, name):
         assert name, "The name argument is not allowed to be empty."
('django/core/files', 'move.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -35,7 +35,7 @@
     If that fails, stream manually from one file to another in pure Python.

     If the destination file exists and ``allow_overwrite`` is ``False``, raise
-    ``IOError``.
+    ``FileExistsError``.
     """
     # There's no reason to move if we don't have to.
     if _samefile(old_file_name, new_file_name):
@@ -43,7 +43,7 @@

     try:
         if not allow_overwrite and os.access(new_file_name, os.F_OK):
-            raise IOError("Destination file %s exists and allow_overwrite is False" % new_file_name)
+            raise FileExistsError('Destination file %s exists and allow_overwrite is False.' % new_file_name)

         os.rename(old_file_name, new_file_name)
         return
('django/core/handlers', 'exception.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,11 +1,14 @@
+import asyncio
 import logging
 import sys
 from functools import wraps

+from asgiref.sync import sync_to_async
+
 from django.conf import settings
 from django.core import signals
 from django.core.exceptions import (
-    PermissionDenied, RequestDataTooBig, SuspiciousOperation,
+    BadRequest, PermissionDenied, RequestDataTooBig, SuspiciousOperation,
     TooManyFieldsSent,
 )
 from django.http import Http404
@@ -28,14 +31,24 @@
     no middleware leaks an exception and that the next middleware in the stack
     can rely on getting a response instead of an exception.
     """
-    @wraps(get_response)
-    def inner(request):
-        try:
-            response = get_response(request)
-        except Exception as exc:
-            response = response_for_exception(request, exc)
-        return response
-    return inner
+    if asyncio.iscoroutinefunction(get_response):
+        @wraps(get_response)
+        async def inner(request):
+            try:
+                response = await get_response(request)
+            except Exception as exc:
+                response = await sync_to_async(response_for_exception, thread_sensitive=False)(request, exc)
+            return response
+        return inner
+    else:
+        @wraps(get_response)
+        def inner(request):
+            try:
+                response = get_response(request)
+            except Exception as exc:
+                response = response_for_exception(request, exc)
+            return response
+        return inner


 def response_for_exception(request, exc):
@@ -63,6 +76,17 @@
             exc_info=sys.exc_info(),
         )

+    elif isinstance(exc, BadRequest):
+        if settings.DEBUG:
+            response = debug.technical_500_response(request, *sys.exc_info(), status_code=400)
+        else:
+            response = get_exception_response(request, get_resolver(get_urlconf()), 400, exc)
+        log_response(
+            '%s: %s', str(exc), request.path,
+            response=response,
+            request=request,
+            exc_info=sys.exc_info(),
+        )
     elif isinstance(exc, SuspiciousOperation):
         if isinstance(exc, (RequestDataTooBig, TooManyFieldsSent)):
             # POST data can't be accessed again, otherwise the original
@@ -104,8 +128,8 @@

 def get_exception_response(request, resolver, status_code, exception):
     try:
-        callback, param_dict = resolver.resolve_error_handler(status_code)
-        response = callback(request, **{**param_dict, 'exception': exception})
+        callback = resolver.resolve_error_handler(status_code)
+        response = callback(request, exception=exception)
     except Exception:
         signals.got_request_exception.send(sender=None, request=request)
         response = handle_uncaught_exception(request, resolver, sys.exc_info())
@@ -125,5 +149,5 @@
         return debug.technical_500_response(request, *exc_info)

     # Return an HttpResponse that displays a friendly error message.
-    callback, param_dict = resolver.resolve_error_handler(500)
-    return callback(request, **param_dict)
+    callback = resolver.resolve_error_handler(500)
+    return callback(request)
('django/core/handlers', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,8 @@
+import asyncio
 import logging
 import types
+
+from asgiref.sync import async_to_sync, sync_to_async

 from django.conf import settings
 from django.core.exceptions import ImproperlyConfigured, MiddlewareNotUsed
@@ -20,7 +23,7 @@
     _exception_middleware = None
     _middleware_chain = None

-    def load_middleware(self):
+    def load_middleware(self, is_async=False):
         """
         Populate middleware lists from settings.MIDDLEWARE.

@@ -30,11 +33,29 @@
         self._template_response_middleware = []
         self._exception_middleware = []

-        handler = convert_exception_to_response(self._get_response)
+        get_response = self._get_response_async if is_async else self._get_response
+        handler = convert_exception_to_response(get_response)
+        handler_is_async = is_async
         for middleware_path in reversed(settings.MIDDLEWARE):
             middleware = import_string(middleware_path)
-            try:
-                mw_instance = middleware(handler)
+            middleware_can_sync = getattr(middleware, 'sync_capable', True)
+            middleware_can_async = getattr(middleware, 'async_capable', False)
+            if not middleware_can_sync and not middleware_can_async:
+                raise RuntimeError(
+                    'Middleware %s must have at least one of '
+                    'sync_capable/async_capable set to True.' % middleware_path
+                )
+            elif not handler_is_async and middleware_can_sync:
+                middleware_is_async = False
+            else:
+                middleware_is_async = middleware_can_async
+            try:
+                # Adapt handler, if needed.
+                adapted_handler = self.adapt_method_mode(
+                    middleware_is_async, handler, handler_is_async,
+                    debug=settings.DEBUG, name='middleware %s' % middleware_path,
+                )
+                mw_instance = middleware(adapted_handler)
             except MiddlewareNotUsed as exc:
                 if settings.DEBUG:
                     if str(exc):
@@ -42,6 +63,8 @@
                     else:
                         logger.debug('MiddlewareNotUsed: %r', middleware_path)
                 continue
+            else:
+                handler = adapted_handler

             if mw_instance is None:
                 raise ImproperlyConfigured(
@@ -49,31 +72,63 @@
                 )

             if hasattr(mw_instance, 'process_view'):
-                self._view_middleware.insert(0, mw_instance.process_view)
+                self._view_middleware.insert(
+                    0,
+                    self.adapt_method_mode(is_async, mw_instance.process_view),
+                )
             if hasattr(mw_instance, 'process_template_response'):
-                self._template_response_middleware.append(mw_instance.process_template_response)
+                self._template_response_middleware.append(
+                    self.adapt_method_mode(is_async, mw_instance.process_template_response),
+                )
             if hasattr(mw_instance, 'process_exception'):
-                self._exception_middleware.append(mw_instance.process_exception)
+                # The exception-handling stack is still always synchronous for
+                # now, so adapt that way.
+                self._exception_middleware.append(
+                    self.adapt_method_mode(False, mw_instance.process_exception),
+                )

             handler = convert_exception_to_response(mw_instance)
-
+            handler_is_async = middleware_is_async
+
+        # Adapt the top of the stack, if needed.
+        handler = self.adapt_method_mode(is_async, handler, handler_is_async)
         # We only assign to this when initialization is complete as it is used
         # as a flag for initialization being complete.
         self._middleware_chain = handler

-    def make_view_atomic(self, view):
-        non_atomic_requests = getattr(view, '_non_atomic_requests', set())
-        for db in connections.all():
-            if db.settings_dict['ATOMIC_REQUESTS'] and db.alias not in non_atomic_requests:
-                view = transaction.atomic(using=db.alias)(view)
-        return view
+    def adapt_method_mode(
+        self, is_async, method, method_is_async=None, debug=False, name=None,
+    ):
+        """
+        Adapt a method to be in the correct "mode":
+        - If is_async is False:
+          - Synchronous methods are left alone
+          - Asynchronous methods are wrapped with async_to_sync
+        - If is_async is True:
+          - Synchronous methods are wrapped with sync_to_async()
+          - Asynchronous methods are left alone
+        """
+        if method_is_async is None:
+            method_is_async = asyncio.iscoroutinefunction(method)
+        if debug and not name:
+            name = name or 'method %s()' % method.__qualname__
+        if is_async:
+            if not method_is_async:
+                if debug:
+                    logger.debug('Synchronous %s adapted.', name)
+                return sync_to_async(method, thread_sensitive=True)
+        elif method_is_async:
+            if debug:
+                logger.debug('Asynchronous %s adapted.', name)
+            return async_to_sync(method)
+        return method

     def get_response(self, request):
         """Return an HttpResponse object for the given HttpRequest."""
         # Setup default url resolver for this thread
         set_urlconf(settings.ROOT_URLCONF)
         response = self._middleware_chain(request)
-        response._closable_objects.append(request)
+        response._resource_closers.append(request.close)
         if response.status_code >= 400:
             log_response(
                 '%s: %s', response.reason_phrase, request.path,
@@ -82,6 +137,26 @@
             )
         return response

+    async def get_response_async(self, request):
+        """
+        Asynchronous version of get_response.
+
+        Funneling everything, including WSGI, into a single async
+        get_response() is too slow. Avoid the context switch by using
+        a separate async response path.
+        """
+        # Setup default url resolver for this thread.
+        set_urlconf(settings.ROOT_URLCONF)
+        response = await self._middleware_chain(request)
+        response._resource_closers.append(request.close)
+        if response.status_code >= 400:
+            await sync_to_async(log_response, thread_sensitive=False)(
+                '%s: %s', response.reason_phrase, request.path,
+                response=response,
+                request=request,
+            )
+        return response
+
     def _get_response(self, request):
         """
         Resolve and call the view, then apply view, exception, and
@@ -89,73 +164,182 @@
         inside the request/response middleware.
         """
         response = None
-
+        callback, callback_args, callback_kwargs = self.resolve_request(request)
+
+        # Apply view middleware
+        for middleware_method in self._view_middleware:
+            response = middleware_method(request, callback, callback_args, callback_kwargs)
+            if response:
+                break
+
+        if response is None:
+            wrapped_callback = self.make_view_atomic(callback)
+            # If it is an asynchronous view, run it in a subthread.
+            if asyncio.iscoroutinefunction(wrapped_callback):
+                wrapped_callback = async_to_sync(wrapped_callback)
+            try:
+                response = wrapped_callback(request, *callback_args, **callback_kwargs)
+            except Exception as e:
+                response = self.process_exception_by_middleware(e, request)
+                if response is None:
+                    raise
+
+        # Complain if the view returned None (a common error).
+        self.check_response(response, callback)
+
+        # If the response supports deferred rendering, apply template
+        # response middleware and then render the response
+        if hasattr(response, 'render') and callable(response.render):
+            for middleware_method in self._template_response_middleware:
+                response = middleware_method(request, response)
+                # Complain if the template response middleware returned None (a common error).
+                self.check_response(
+                    response,
+                    middleware_method,
+                    name='%s.process_template_response' % (
+                        middleware_method.__self__.__class__.__name__,
+                    )
+                )
+            try:
+                response = response.render()
+            except Exception as e:
+                response = self.process_exception_by_middleware(e, request)
+                if response is None:
+                    raise
+
+        return response
+
+    async def _get_response_async(self, request):
+        """
+        Resolve and call the view, then apply view, exception, and
+        template_response middleware. This method is everything that happens
+        inside the request/response middleware.
+        """
+        response = None
+        callback, callback_args, callback_kwargs = self.resolve_request(request)
+
+        # Apply view middleware.
+        for middleware_method in self._view_middleware:
+            response = await middleware_method(request, callback, callback_args, callback_kwargs)
+            if response:
+                break
+
+        if response is None:
+            wrapped_callback = self.make_view_atomic(callback)
+            # If it is a synchronous view, run it in a subthread
+            if not asyncio.iscoroutinefunction(wrapped_callback):
+                wrapped_callback = sync_to_async(wrapped_callback, thread_sensitive=True)
+            try:
+                response = await wrapped_callback(request, *callback_args, **callback_kwargs)
+            except Exception as e:
+                response = await sync_to_async(
+                    self.process_exception_by_middleware,
+                    thread_sensitive=True,
+                )(e, request)
+                if response is None:
+                    raise
+
+        # Complain if the view returned None or an uncalled coroutine.
+        self.check_response(response, callback)
+
+        # If the response supports deferred rendering, apply template
+        # response middleware and then render the response
+        if hasattr(response, 'render') and callable(response.render):
+            for middleware_method in self._template_response_middleware:
+                response = await middleware_method(request, response)
+                # Complain if the template response middleware returned None or
+                # an uncalled coroutine.
+                self.check_response(
+                    response,
+                    middleware_method,
+                    name='%s.process_template_response' % (
+                        middleware_method.__self__.__class__.__name__,
+                    )
+                )
+            try:
+                if asyncio.iscoroutinefunction(response.render):
+                    response = await response.render()
+                else:
+                    response = await sync_to_async(response.render, thread_sensitive=True)()
+            except Exception as e:
+                response = await sync_to_async(
+                    self.process_exception_by_middleware,
+                    thread_sensitive=True,
+                )(e, request)
+                if response is None:
+                    raise
+
+        # Make sure the response is not a coroutine
+        if asyncio.iscoroutine(response):
+            raise RuntimeError('Response is still a coroutine.')
+        return response
+
+    def resolve_request(self, request):
+        """
+        Retrieve/set the urlconf for the request. Return the view resolved,
+        with its args and kwargs.
+        """
+        # Work out the resolver.
         if hasattr(request, 'urlconf'):
             urlconf = request.urlconf
             set_urlconf(urlconf)
             resolver = get_resolver(urlconf)
         else:
             resolver = get_resolver()
-
+        # Resolve the view, and assign the match object back to the request.
         resolver_match = resolver.resolve(request.path_info)
-        callback, callback_args, callback_kwargs = resolver_match
         request.resolver_match = resolver_match
-
-        # Apply view middleware
-        for middleware_method in self._view_middleware:
-            response = middleware_method(request, callback, callback_args, callback_kwargs)
-            if response:
-                break
-
+        return resolver_match
+
+    def check_response(self, response, callback, name=None):
+        """
+        Raise an error if the view returned None or an uncalled coroutine.
+        """
+        if not(response is None or asyncio.iscoroutine(response)):
+            return
+        if not name:
+            if isinstance(callback, types.FunctionType):  # FBV
+                name = 'The view %s.%s' % (callback.__module__, callback.__name__)
+            else:  # CBV
+                name = 'The view %s.%s.__call__' % (
+                    callback.__module__,
+                    callback.__class__.__name__,
+                )
         if response is None:
-            wrapped_callback = self.make_view_atomic(callback)
-            try:
-                response = wrapped_callback(request, *callback_args, **callback_kwargs)
-            except Exception as e:
-                response = self.process_exception_by_middleware(e, request)
-
-        # Complain if the view returned None (a common error).
-        if response is None:
-            if isinstance(callback, types.FunctionType):    # FBV
-                view_name = callback.__name__
-            else:                                           # CBV
-                view_name = callback.__class__.__name__ + '.__call__'
-
             raise ValueError(
-                "The view %s.%s didn't return an HttpResponse object. It "
-                "returned None instead." % (callback.__module__, view_name)
+                "%s didn't return an HttpResponse object. It returned None "
+                "instead." % name
             )
-
-        # If the response supports deferred rendering, apply template
-        # response middleware and then render the response
-        elif hasattr(response, 'render') and callable(response.render):
-            for middleware_method in self._template_response_middleware:
-                response = middleware_method(request, response)
-                # Complain if the template response middleware returned None (a common error).
-                if response is None:
-                    raise ValueError(
-                        "%s.process_template_response didn't return an "
-                        "HttpResponse object. It returned None instead."
-                        % (middleware_method.__self__.__class__.__name__)
+        elif asyncio.iscoroutine(response):
+            raise ValueError(
+                "%s didn't return an HttpResponse object. It returned an "
+                "unawaited coroutine instead. You may need to add an 'await' "
+                "into your view." % name
+            )
+
+    # Other utility methods.
+
+    def make_view_atomic(self, view):
+        non_atomic_requests = getattr(view, '_non_atomic_requests', set())
+        for db in connections.all():
+            if db.settings_dict['ATOMIC_REQUESTS'] and db.alias not in non_atomic_requests:
+                if asyncio.iscoroutinefunction(view):
+                    raise RuntimeError(
+                        'You cannot use ATOMIC_REQUESTS with async views.'
                     )
-
-            try:
-                response = response.render()
-            except Exception as e:
-                response = self.process_exception_by_middleware(e, request)
-
-        return response
+                view = transaction.atomic(using=db.alias)(view)
+        return view

     def process_exception_by_middleware(self, exception, request):
         """
         Pass the exception to the exception middleware. If no middleware
-        return a response for this exception, raise it.
+        return a response for this exception, return None.
         """
         for middleware_method in self._exception_middleware:
             response = middleware_method(request, exception)
             if response:
                 return response
-        raise
+        return None


 def reset_urlconf(sender, **kwargs):
('django/core/handlers', 'wsgi.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,3 @@
-import cgi
-import codecs
-import re
 from io import BytesIO

 from django.conf import settings
@@ -10,8 +7,9 @@
 from django.urls import set_script_prefix
 from django.utils.encoding import repercent_broken_unicode
 from django.utils.functional import cached_property
-
-_slashes_re = re.compile(br'/+')
+from django.utils.regex_helper import _lazy_re_compile
+
+_slashes_re = _lazy_re_compile(br'/+')


 class LimitedStream:
@@ -80,14 +78,8 @@
         self.META['PATH_INFO'] = path_info
         self.META['SCRIPT_NAME'] = script_name
         self.method = environ['REQUEST_METHOD'].upper()
-        self.content_type, self.content_params = cgi.parse_header(environ.get('CONTENT_TYPE', ''))
-        if 'charset' in self.content_params:
-            try:
-                codecs.lookup(self.content_params['charset'])
-            except LookupError:
-                pass
-            else:
-                self.encoding = self.content_params['charset']
+        # Set content_type, content_params, and encoding.
+        self._set_content_type_params(environ)
         try:
             content_length = int(environ.get('CONTENT_LENGTH'))
         except (ValueError, TypeError):
@@ -149,7 +141,11 @@
         ]
         start_response(status, response_headers)
         if getattr(response, 'file_to_stream', None) is not None and environ.get('wsgi.file_wrapper'):
-            response = environ['wsgi.file_wrapper'](response.file_to_stream)
+            # If `wsgi.file_wrapper` is used the WSGI server does not call
+            # .close on the response, but on the file wrapper. Patch it to use
+            # response.close instead which takes care of closing all files.
+            response.file_to_stream.close = response.close
+            response = environ['wsgi.file_wrapper'](response.file_to_stream, response.block_size)
         return response


('django/core/servers', 'basehttp.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -51,8 +51,12 @@


 def is_broken_pipe_error():
-    exc_type, exc_value = sys.exc_info()[:2]
-    return issubclass(exc_type, socket.error) and exc_value.args[0] == 32
+    exc_type, _, _ = sys.exc_info()
+    return issubclass(exc_type, (
+        BrokenPipeError,
+        ConnectionAbortedError,
+        ConnectionResetError,
+    ))


 class WSGIServer(simple_server.WSGIServer):
@@ -100,6 +104,9 @@
         # the content length is unknown to prevent clients from reusing the
         # connection.
         if 'Content-Length' not in self.headers:
+            self.headers['Connection'] = 'close'
+        # Persistent connections require threading server.
+        elif not isinstance(self.request_handler.server, socketserver.ThreadingMixIn):
             self.headers['Connection'] = 'close'
         # Mark the connection for closing if it's set as such above or if the
         # application sent the header.
@@ -171,7 +178,7 @@
             self.handle_one_request()
         try:
             self.connection.shutdown(socket.SHUT_WR)
-        except (socket.error, AttributeError):
+        except (AttributeError, OSError):
             pass

     def handle_one_request(self):
('django/bin', 'django-admin.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,21 @@
 #!/usr/bin/env python
+# When the django-admin.py deprecation ends, remove this script.
+import warnings
+
 from django.core import management

+try:
+    from django.utils.deprecation import RemovedInDjango40Warning
+except ImportError:
+    raise ImportError(
+        'django-admin.py was deprecated in Django 3.1 and removed in Django '
+        '4.0. Please manually remove this script from your virtual environment '
+        'and use django-admin instead.'
+    )
+
 if __name__ == "__main__":
+    warnings.warn(
+        'django-admin.py is deprecated in favor of django-admin.',
+        RemovedInDjango40Warning,
+    )
     management.execute_from_command_line()
('django/dispatch', 'dispatcher.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,7 +1,12 @@
+import logging
 import threading
+import warnings
 import weakref

+from django.utils.deprecation import RemovedInDjango40Warning
 from django.utils.inspect import func_accepts_kwargs
+
+logger = logging.getLogger('django.dispatch')


 def _make_id(target):
@@ -28,14 +33,16 @@
     def __init__(self, providing_args=None, use_caching=False):
         """
         Create a new signal.
-
-        providing_args
-            A list of the arguments this signal can pass along in a send() call.
         """
         self.receivers = []
-        if providing_args is None:
-            providing_args = []
-        self.providing_args = set(providing_args)
+        if providing_args is not None:
+            warnings.warn(
+                'The providing_args argument is deprecated. As it is purely '
+                'documentational, it has no replacement. If you rely on this '
+                'argument as documentation, you can move the text to a code '
+                'comment or docstring.',
+                RemovedInDjango40Warning, stacklevel=2,
+            )
         self.lock = threading.Lock()
         self.use_caching = use_caching
         # For convenience we create empty caches even if they are not used.
@@ -187,9 +194,7 @@
                 occur).

             named
-                Named arguments which will be passed to receivers. These
-                arguments must be a subset of the argument names defined in
-                providing_args.
+                Named arguments which will be passed to receivers.

         Return a list of tuple pairs [(receiver, response), ... ].

@@ -206,6 +211,12 @@
             try:
                 response = receiver(signal=self, sender=sender, **named)
             except Exception as err:
+                logger.error(
+                    'Error calling %s in Signal.send_robust() (%s)',
+                    receiver.__qualname__,
+                    err,
+                    exc_info=err,
+                )
                 responses.append((receiver, err))
             else:
                 responses.append((receiver, response))
('django/template', 'library.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,6 @@
 import functools
 from importlib import import_module
-from inspect import getfullargspec
+from inspect import getfullargspec, unwrap

 from django.utils.html import conditional_escape
 from django.utils.itercompat import is_iterable
@@ -106,7 +106,7 @@
             return 'world'
         """
         def dec(func):
-            params, varargs, varkw, defaults, kwonly, kwonly_defaults, _ = getfullargspec(func)
+            params, varargs, varkw, defaults, kwonly, kwonly_defaults, _ = getfullargspec(unwrap(func))
             function_name = (name or getattr(func, '_decorated_function', func).__name__)

             @functools.wraps(func)
@@ -143,7 +143,7 @@
             return {'choices': choices}
         """
         def dec(func):
-            params, varargs, varkw, defaults, kwonly, kwonly_defaults, _ = getfullargspec(func)
+            params, varargs, varkw, defaults, kwonly, kwonly_defaults, _ = getfullargspec(unwrap(func))
             function_name = (name or getattr(func, '_decorated_function', func).__name__)

             @functools.wraps(func)
@@ -261,7 +261,7 @@
         if kwarg:
             # The kwarg was successfully extracted
             param, value = kwarg.popitem()
-            if param not in params and param not in unhandled_kwargs and varkw is None:
+            if param not in params and param not in kwonly and varkw is None:
                 # An unexpected keyword argument was supplied
                 raise TemplateSyntaxError(
                     "'%s' received unexpected keyword argument '%s'" %
('django/template', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -53,16 +53,19 @@

 # Public exceptions
 from .base import VariableDoesNotExist                                  # NOQA isort:skip
-from .context import ContextPopException                                # NOQA isort:skip
+from .context import Context, ContextPopException, RequestContext       # NOQA isort:skip
 from .exceptions import TemplateDoesNotExist, TemplateSyntaxError       # NOQA isort:skip

 # Template parts
 from .base import (                                                     # NOQA isort:skip
-    Context, Node, NodeList, Origin, RequestContext, Template, Variable,
+    Node, NodeList, Origin, Template, Variable,
 )

 # Library management
 from .library import Library                                            # NOQA isort:skip

+# Import the .autoreload module to trigger the registrations of signals.
+from . import autoreload                                                # NOQA isort:skip
+

 __all__ += ('Template', 'Context', 'RequestContext')
('django/template', 'response.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -11,7 +11,7 @@
     rendering_attrs = ['template_name', 'context_data', '_post_render_callbacks']

     def __init__(self, template, context=None, content_type=None, status=None,
-                 charset=None, using=None):
+                 charset=None, using=None, headers=None):
         # It would seem obvious to call these next two members 'template' and
         # 'context', but those names are reserved as part of the test Client
         # API. To avoid the name collision, we use different names.
@@ -33,7 +33,7 @@
         # content argument doesn't make sense here because it will be replaced
         # with rendered template so we always pass empty string in order to
         # prevent errors and provide shorter signature.
-        super().__init__('', content_type, status, charset=charset)
+        super().__init__('', content_type, status, charset=charset, headers=headers)

         # _is_rendered tracks whether the template and context has been baked
         # into a final response.
@@ -80,8 +80,7 @@
         """
         template = self.resolve_template(self.template_name)
         context = self.resolve_context(self.context_data)
-        content = template.render(context, self._request)
-        return content
+        return template.render(context, self._request)

     def add_post_render_callback(self, callback):
         """Add a new post-rendering callback.
@@ -140,6 +139,6 @@
     rendering_attrs = SimpleTemplateResponse.rendering_attrs + ['_request']

     def __init__(self, request, template, context=None, content_type=None,
-                 status=None, charset=None, using=None):
-        super().__init__(template, context, content_type, status, charset, using)
+                 status=None, charset=None, using=None, headers=None):
+        super().__init__(template, context, content_type, status, charset, using, headers=headers)
         self._request = request
('django/template', 'context_processors.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -40,6 +40,7 @@
     if settings.DEBUG and request.META.get('REMOTE_ADDR') in settings.INTERNAL_IPS:
         context_extras['debug'] = True
         from django.db import connections
+
         # Return a lazy reference that computes connection.queries on access,
         # to ensure it contains queries triggered after this function runs.
         context_extras['sql_queries'] = lazy(
('django/template', 'defaultfilters.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -22,7 +22,7 @@
 from django.utils.timesince import timesince, timeuntil
 from django.utils.translation import gettext, ngettext

-from .base import Variable, VariableDoesNotExist
+from .base import VARIABLE_ATTRIBUTE_SEPARATOR
 from .library import Library

 register = Library()
@@ -119,9 +119,20 @@
     * {{ num2|floatformat:"-3" }} displays "34"
     * {{ num3|floatformat:"-3" }} displays "34.260"

+    If arg has the 'g' suffix, force the result to be grouped by the
+    THOUSAND_SEPARATOR for the active locale. When the active locale is
+    en (English):
+
+    * {{ 6666.6666|floatformat:"2g" }} displays "6,666.67"
+    * {{ 10000|floatformat:"g" }} displays "10,000"
+
     If the input float is infinity or NaN, display the string representation
     of that value.
     """
+    force_grouping = False
+    if isinstance(arg, str) and arg.endswith('g'):
+        force_grouping = True
+        arg = arg[:-1] or -1
     try:
         input_val = repr(text)
         d = Decimal(input_val)
@@ -141,7 +152,9 @@
         return input_val

     if not m and p < 0:
-        return mark_safe(formats.number_format('%d' % (int(d)), 0))
+        return mark_safe(
+            formats.number_format('%d' % (int(d)), 0, force_grouping=force_grouping),
+        )

     exp = Decimal(1).scaleb(-abs(p))
     # Set the precision high enough to avoid an exception (#15789).
@@ -152,15 +165,18 @@

     # Avoid conversion to scientific notation by accessing `sign`, `digits`,
     # and `exponent` from Decimal.as_tuple() directly.
-    sign, digits, exponent = d.quantize(exp, ROUND_HALF_UP, Context(prec=prec)).as_tuple()
+    rounded_d = d.quantize(exp, ROUND_HALF_UP, Context(prec=prec))
+    sign, digits, exponent = rounded_d.as_tuple()
     digits = [str(digit) for digit in reversed(digits)]
     while len(digits) <= abs(exponent):
         digits.append('0')
     digits.insert(-exponent, '.')
-    if sign:
+    if sign and rounded_d:
         digits.append('-')
     number = ''.join(reversed(digits))
-    return mark_safe(formats.number_format(number, abs(p)))
+    return mark_safe(
+        formats.number_format(number, abs(p), force_grouping=force_grouping),
+    )


 @register.filter(is_safe=True)
@@ -240,8 +256,8 @@
 @stringfilter
 def title(value):
     """Convert a string into titlecase."""
-    t = re.sub("([a-z])'([A-Z])", lambda m: m.group(0).lower(), value.title())
-    return re.sub(r"\d([A-Z])", lambda m: m.group(0).lower(), t)
+    t = re.sub("([a-z])'([A-Z])", lambda m: m[0].lower(), value.title())
+    return re.sub(r'\d([A-Z])', lambda m: m[0].lower(), t)


 @register.filter(is_safe=True)
@@ -465,7 +481,7 @@
 def _property_resolver(arg):
     """
     When arg is convertible to float, behave like operator.itemgetter(arg)
-    Otherwise, behave like Variable(arg).resolve
+    Otherwise, chain __getitem__() and getattr().

     >>> _property_resolver(1)('abc')
     'b'
@@ -483,7 +499,19 @@
     try:
         float(arg)
     except ValueError:
-        return Variable(arg).resolve
+        if VARIABLE_ATTRIBUTE_SEPARATOR + '_' in arg or arg[0] == '_':
+            raise AttributeError('Access to private variables is forbidden.')
+        parts = arg.split(VARIABLE_ATTRIBUTE_SEPARATOR)
+
+        def resolve(value):
+            for part in parts:
+                try:
+                    value = value[part]
+                except (AttributeError, IndexError, KeyError, TypeError, ValueError):
+                    value = getattr(value, part)
+            return value
+
+        return resolve
     else:
         return itemgetter(arg)

@@ -496,7 +524,7 @@
     """
     try:
         return sorted(value, key=_property_resolver(arg))
-    except (TypeError, VariableDoesNotExist):
+    except (AttributeError, TypeError):
         return ''


@@ -508,7 +536,7 @@
     """
     try:
         return sorted(value, key=_property_resolver(arg), reverse=True)
-    except (TypeError, VariableDoesNotExist):
+    except (AttributeError, TypeError):
         return ''


@@ -785,6 +813,7 @@
     ==========  ======================  ==================================
     """
     if arg is None:
+        # Translators: Please do not add spaces around commas.
         arg = gettext('yes,no,maybe')
     bits = arg.split(',')
     if len(bits) < 2:
@@ -812,7 +841,7 @@
     102 bytes, etc.).
     """
     try:
-        bytes_ = float(bytes_)
+        bytes_ = int(bytes_)
     except (TypeError, ValueError, UnicodeDecodeError):
         value = ngettext("%(size)d byte", "%(size)d bytes", 0) % {'size': 0}
         return avoid_wrapping(value)
@@ -851,8 +880,8 @@
 @register.filter(is_safe=False)
 def pluralize(value, arg='s'):
     """
-    Return a plural suffix if the value is not 1. By default, use 's' as the
-    suffix:
+    Return a plural suffix if the value is not 1, '1', or an object of
+    length 1. By default, use 's' as the suffix:

     * If value is 0, vote{{ value|pluralize }} display "votes".
     * If value is 1, vote{{ value|pluralize }} display "vote".
@@ -879,17 +908,15 @@
     singular_suffix, plural_suffix = bits[:2]

     try:
-        if float(value) != 1:
-            return plural_suffix
+        return singular_suffix if float(value) == 1 else plural_suffix
     except ValueError:  # Invalid string that's not a number.
         pass
     except TypeError:  # Value isn't a string or a number; maybe it's a list?
         try:
-            if len(value) != 1:
-                return plural_suffix
+            return singular_suffix if len(value) == 1 else plural_suffix
         except TypeError:  # len() of unsized object.
             pass
-    return singular_suffix
+    return ''


 @register.filter("phone2numeric", is_safe=True)
('django/template', 'engine.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,8 +4,8 @@
 from django.utils.functional import cached_property
 from django.utils.module_loading import import_string

-from .base import Context, Template
-from .context import _builtin_context_processors
+from .base import Template
+from .context import Context, _builtin_context_processors
 from .exceptions import TemplateDoesNotExist
 from .library import import_library

@@ -160,7 +160,7 @@
         if isinstance(context, Context):
             return t.render(context)
         else:
-            return t.render(Context(context))
+            return t.render(Context(context, autoescape=self.autoescape))

     def select_template(self, template_name_list):
         """
('django/template', 'context.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -124,12 +124,10 @@
         """
         Compare two contexts by comparing theirs 'dicts' attributes.
         """
-        return (
-            isinstance(other, BaseContext) and
-            # because dictionaries can be put in different order
-            # we have to flatten them like in templates
-            self.flatten() == other.flatten()
-        )
+        if not isinstance(other, BaseContext):
+            return NotImplemented
+        # flatten dictionaries because they can be put in a different order.
+        return self.flatten() == other.flatten()


 class Context(BaseContext):
('django/template', 'utils.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,5 @@
 import functools
-from collections import Counter, OrderedDict
+from collections import Counter
 from pathlib import Path

 from django.apps import apps
@@ -27,7 +27,7 @@
         if self._templates is None:
             self._templates = settings.TEMPLATES

-        templates = OrderedDict()
+        templates = {}
         backend_names = []
         for tpl in self._templates:
             try:
@@ -99,7 +99,7 @@
     installed applications.
     """
     template_dirs = [
-        str(Path(app_config.path) / dirname)
+        Path(app_config.path) / dirname
         for app_config in apps.get_app_configs()
         if app_config.path and (Path(app_config.path) / dirname).is_dir()
     ]
('django/template', 'loader_tags.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -168,12 +168,19 @@
         template = self.template.resolve(context)
         # Does this quack like a Template?
         if not callable(getattr(template, 'render', None)):
-            # If not, try the cache and get_template().
-            template_name = template
+            # If not, try the cache and select_template().
+            template_name = template or ()
+            if isinstance(template_name, str):
+                template_name = (construct_relative_path(
+                    self.origin.template_name,
+                    template_name,
+                ),)
+            else:
+                template_name = tuple(template_name)
             cache = context.render_context.dicts[0].setdefault(self, {})
             template = cache.get(template_name)
             if template is None:
-                template = context.template.engine.get_template(template_name)
+                template = context.template.engine.select_template(template_name)
                 cache[template_name] = template
         # Use the base.Template of a backends.django.Template.
         elif hasattr(template, 'template'):
@@ -222,7 +229,12 @@
     Convert a relative path (starting with './' or '../') to the full template
     name based on the current_template_name.
     """
-    if not relative_name.startswith(("'./", "'../", '"./', '"../')):
+    has_quotes = (
+        (relative_name.startswith('"') and relative_name.endswith('"')) or
+        (relative_name.startswith("'") and relative_name.endswith("'"))
+    )
+    new_name = relative_name.strip('\'"')
+    if not new_name.startswith(('./', '../')):
         # relative_name is a variable or a literal that doesn't contain a
         # relative path.
         return relative_name
@@ -230,7 +242,7 @@
     new_name = posixpath.normpath(
         posixpath.join(
             posixpath.dirname(current_template_name.lstrip('/')),
-            relative_name.strip('\'"')
+            new_name,
         )
     )
     if new_name.startswith('../'):
@@ -244,7 +256,7 @@
             "same template in which the tag appears."
             % (relative_name, current_template_name)
         )
-    return '"%s"' % new_name
+    return f'"{new_name}"' if has_quotes else new_name


 @register.tag('extends')
('django/template', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -50,16 +50,15 @@
 '<html></html>'
 """

+import inspect
 import logging
 import re
 from enum import Enum
-from inspect import getcallargs, getfullargspec, unwrap
-
-from django.template.context import (  # NOQA: imported for backwards compatibility
-    BaseContext, Context, ContextPopException, RequestContext,
-)
+
+from django.template.context import BaseContext
 from django.utils.formats import localize
 from django.utils.html import conditional_escape, escape
+from django.utils.regex_helper import _lazy_re_compile
 from django.utils.safestring import SafeData, mark_safe
 from django.utils.text import (
     get_text_list, smart_split, unescape_string_literal,
@@ -89,7 +88,7 @@

 # match a variable or block tag and capture the entire tag, including start/end
 # delimiters
-tag_re = (re.compile('(%s.*?%s|%s.*?%s|%s.*?%s)' %
+tag_re = (_lazy_re_compile('(%s.*?%s|%s.*?%s|%s.*?%s)' %
           (re.escape(BLOCK_TAG_START), re.escape(BLOCK_TAG_END),
            re.escape(VARIABLE_TAG_START), re.escape(VARIABLE_TAG_END),
            re.escape(COMMENT_TAG_START), re.escape(COMMENT_TAG_END))))
@@ -176,7 +175,7 @@
         """
         Parse and compile the template source into a nodelist. If debug
         is True and an exception occurs during parsing, the exception is
-        is annotated with contextual line information where it occurred in the
+        annotated with contextual line information where it occurred in the
         template source.
         """
         if self.engine.debug:
@@ -395,7 +394,6 @@
                 token_string = self.template_string[upto:start]
                 result.append(self.create_token(token_string, (upto, start), lineno, in_tag=False))
                 lineno += token_string.count('\n')
-                upto = start
             token_string = self.template_string[start:end]
             result.append(self.create_token(token_string, (start, end), lineno, in_tag=True))
             lineno += token_string.count('\n')
@@ -408,7 +406,9 @@

 class Parser:
     def __init__(self, tokens, libraries=None, builtins=None, origin=None):
-        self.tokens = tokens
+        # Reverse the tokens so delete_first_token(), prepend_token(), and
+        # next_token() can operate at the end of the list in constant time.
+        self.tokens = list(reversed(tokens))
         self.tags = {}
         self.filters = {}
         self.command_stack = []
@@ -544,13 +544,13 @@
         raise self.error(token, msg)

     def next_token(self):
-        return self.tokens.pop(0)
+        return self.tokens.pop()

     def prepend_token(self, token):
-        self.tokens.insert(0, token)
+        self.tokens.append(token)

     def delete_first_token(self):
-        del self.tokens[0]
+        del self.tokens[-1]

     def add_library(self, lib):
         self.tags.update(lib.tags)
@@ -604,7 +604,7 @@
     'arg_sep': re.escape(FILTER_ARGUMENT_SEPARATOR),
 }

-filter_re = re.compile(filter_raw_string, re.VERBOSE)
+filter_re = _lazy_re_compile(filter_raw_string, re.VERBOSE)


 class FilterExpression:
@@ -635,7 +635,7 @@
                                           (token[:upto], token[upto:start],
                                            token[start:]))
             if var_obj is None:
-                var, constant = match.group("var", "constant")
+                var, constant = match['var'], match['constant']
                 if constant:
                     try:
                         var_obj = Variable(constant).resolve({})
@@ -647,9 +647,9 @@
                 else:
                     var_obj = Variable(var)
             else:
-                filter_name = match.group("filter_name")
+                filter_name = match['filter_name']
                 args = []
-                constant_arg, var_arg = match.group("constant_arg", "var_arg")
+                constant_arg, var_arg = match['constant_arg'], match['var_arg']
                 if constant_arg:
                     args.append((False, Variable(constant_arg).resolve({})))
                 elif var_arg:
@@ -707,9 +707,9 @@
         # First argument, filter input, is implied.
         plen = len(provided) + 1
         # Check to see if a decorator is providing the real function.
-        func = unwrap(func)
-
-        args, _, _, defaults, _, _, _ = getfullargspec(func)
+        func = inspect.unwrap(func)
+
+        args, _, _, defaults, _, _, _ = inspect.getfullargspec(func)
         alen = len(args)
         dlen = len(defaults or [])
         # Not enough OR Too many
@@ -857,8 +857,9 @@
                         try:  # method call (assuming no args required)
                             current = current()
                         except TypeError:
+                            signature = inspect.signature(current)
                             try:
-                                getcallargs(current)
+                                signature.bind()
                             except TypeError:  # arguments *were* required
                                 current = context.template.engine.string_if_invalid  # invalid method call
                             else:
@@ -994,7 +995,7 @@


 # Regex for token keyword arguments
-kwarg_re = re.compile(r"(?:(\w+)=)?(.+)")
+kwarg_re = _lazy_re_compile(r"(?:(\w+)=)?(.+)")


 def token_kwargs(bits, parser, support_legacy=False):
@@ -1016,7 +1017,7 @@
     if not bits:
         return {}
     match = kwarg_re.match(bits[0])
-    kwarg_format = match and match.group(1)
+    kwarg_format = match and match[1]
     if not kwarg_format:
         if not support_legacy:
             return {}
@@ -1027,7 +1028,7 @@
     while bits:
         if kwarg_format:
             match = kwarg_re.match(bits[0])
-            if not match or not match.group(1):
+            if not match or not match[1]:
                 return kwargs
             key, value = match.groups()
             del bits[:1]
('django/template', 'defaulttags.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -8,17 +8,19 @@

 from django.conf import settings
 from django.utils import timezone
-from django.utils.html import conditional_escape, format_html
+from django.utils.deprecation import RemovedInDjango40Warning
+from django.utils.html import conditional_escape, escape, format_html
 from django.utils.lorem_ipsum import paragraphs, words
 from django.utils.safestring import mark_safe

 from .base import (
     BLOCK_TAG_END, BLOCK_TAG_START, COMMENT_TAG_END, COMMENT_TAG_START,
     FILTER_SEPARATOR, SINGLE_BRACE_END, SINGLE_BRACE_START,
-    VARIABLE_ATTRIBUTE_SEPARATOR, VARIABLE_TAG_END, VARIABLE_TAG_START,
-    Context, Node, NodeList, TemplateSyntaxError, VariableDoesNotExist,
-    kwarg_re, render_value_in_context, token_kwargs,
+    VARIABLE_ATTRIBUTE_SEPARATOR, VARIABLE_TAG_END, VARIABLE_TAG_START, Node,
+    NodeList, TemplateSyntaxError, VariableDoesNotExist, kwarg_re,
+    render_value_in_context, token_kwargs,
 )
+from .context import Context
 from .defaultfilters import date
 from .library import Library
 from .smartif import IfParser, Literal
@@ -94,10 +96,13 @@

 class DebugNode(Node):
     def render(self, context):
+        if not settings.DEBUG:
+            return ''
+
         from pprint import pformat
-        output = [pformat(val) for val in context]
+        output = [escape(pformat(val)) for val in context]
         output.append('\n\n')
-        output.append(pformat(sys.modules))
+        output.append(escape(pformat(sys.modules)))
         return ''.join(output)


@@ -260,6 +265,7 @@


 class IfEqualNode(Node):
+    # RemovedInDjango40Warning.
     child_nodelists = ('nodelist_true', 'nodelist_false')

     def __init__(self, var1, var2, nodelist_true, nodelist_false, negate):
@@ -425,7 +431,7 @@
         self.asvar = asvar

     def render(self, context):
-        from django.urls import reverse, NoReverseMatch
+        from django.urls import NoReverseMatch, reverse
         args = [arg.resolve(context) for arg in self.args]
         kwargs = {k: v.resolve(context) for k, v in self.kwargs.items()}
         view_name = self.view_name.resolve(context)
@@ -698,7 +704,7 @@
             {{ var3 }}
         {% endif %}

-    but obviously much cleaner!
+    but much cleaner!

     You can also use a literal string as a fallback value in case all
     passed variables are False::
@@ -819,6 +825,7 @@


 def do_ifequal(parser, token, negate):
+    # RemovedInDjango40Warning.
     bits = list(token.split_contents())
     if len(bits) != 3:
         raise TemplateSyntaxError("%r takes two arguments" % bits[0])
@@ -852,6 +859,10 @@
             ...
         {% endifnotequal %}
     """
+    warnings.warn(
+        'The {% ifequal %} template tag is deprecated in favor of {% if %}.',
+        RemovedInDjango40Warning,
+    )
     return do_ifequal(parser, token, False)


@@ -861,6 +872,11 @@
     Output the contents of the block if the two arguments are not equal.
     See ifequal.
     """
+    warnings.warn(
+        'The {% ifnotequal %} template tag is deprecated in favor of '
+        '{% if %}.',
+        RemovedInDjango40Warning,
+    )
     return do_ifequal(parser, token, True)


@@ -969,7 +985,7 @@

     # {% endif %}
     if token.contents != 'endif':
-        raise TemplateSyntaxError('Malformed template tag at line {0}: "{1}"'.format(token.lineno, token.contents))
+        raise TemplateSyntaxError('Malformed template tag at line {}: "{}"'.format(token.lineno, token.contents))

     return IfNode(conditions_nodelists)

@@ -1417,10 +1433,10 @@
     (because 175/200 = .875; .875 * 100 = 87.5 which is rounded up to 88).

     In some cases you might want to capture the result of widthratio in a
-    variable. It can be useful for instance in a blocktrans like this::
+    variable. It can be useful for instance in a blocktranslate like this::

         {% widthratio this_value max_value max_width as width %}
-        {% blocktrans %}The width is: {{ width }}{% endblocktrans %}
+        {% blocktranslate %}The width is: {{ width }}{% endblocktranslate %}
     """
     bits = token.split_contents()
     if len(bits) == 4:
('django/template/backends', 'django.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -20,7 +20,7 @@
         options = params.pop('OPTIONS').copy()
         options.setdefault('autoescape', True)
         options.setdefault('debug', settings.DEBUG)
-        options.setdefault('file_charset', settings.FILE_CHARSET)
+        options.setdefault('file_charset', 'utf-8')
         libraries = options.get('libraries', {})
         options['libraries'] = self.get_templatetag_libraries(libraries)
         super().__init__(params)
@@ -123,7 +123,7 @@
             raise InvalidTemplateLibrary(
                 "Invalid template library specified. ImportError raised when "
                 "trying to load '%s': %s" % (entry[1], e)
-            )
+            ) from e

         if hasattr(module, 'register'):
             yield entry[1]
('django/template/backends', 'jinja2.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,5 @@
+from pathlib import Path
+
 import jinja2

 from django.conf import settings
@@ -68,7 +70,12 @@
             context['csrf_token'] = csrf_token_lazy(request)
             for context_processor in self.backend.template_context_processors:
                 context.update(context_processor(request))
-        return self.template.render(context)
+        try:
+            return self.template.render(context)
+        except jinja2.TemplateSyntaxError as exc:
+            new = TemplateSyntaxError(exc.args)
+            new.template_debug = get_exception_info(exc)
+            raise new from exc


 class Origin:
@@ -88,12 +95,22 @@
     """
     context_lines = 10
     lineno = exception.lineno
-    lines = list(enumerate(exception.source.strip().split("\n"), start=1))
-    during = lines[lineno - 1][1]
-    total = len(lines)
-    top = max(0, lineno - context_lines - 1)
-    bottom = min(total, lineno + context_lines)
-
+    source = exception.source
+    if source is None:
+        exception_file = Path(exception.filename)
+        if exception_file.exists():
+            with open(exception_file, 'r') as fp:
+                source = fp.read()
+    if source is not None:
+        lines = list(enumerate(source.strip().split('\n'), start=1))
+        during = lines[lineno - 1][1]
+        total = len(lines)
+        top = max(0, lineno - context_lines - 1)
+        bottom = min(total, lineno + context_lines)
+    else:
+        during = ''
+        lines = []
+        total = top = bottom = 0
     return {
         'name': exception.filename,
         'message': exception.message,
('django/template/backends', 'utils.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,7 +1,7 @@
 from django.middleware.csrf import get_token
 from django.utils.functional import lazy
 from django.utils.html import format_html
-from django.utils.safestring import SafeText
+from django.utils.safestring import SafeString


 def csrf_input(request):
@@ -10,5 +10,5 @@
         get_token(request))


-csrf_input_lazy = lazy(csrf_input, SafeText, str)
+csrf_input_lazy = lazy(csrf_input, SafeString, str)
 csrf_token_lazy = lazy(get_token, str)
('django/template/backends', 'dummy.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,5 @@
 import string

-from django.conf import settings
 from django.core.exceptions import ImproperlyConfigured
 from django.template import Origin, TemplateDoesNotExist
 from django.utils.html import conditional_escape
@@ -28,7 +27,7 @@
         tried = []
         for template_file in self.iter_template_filenames(template_name):
             try:
-                with open(template_file, encoding=settings.FILE_CHARSET) as fp:
+                with open(template_file, encoding='utf-8') as fp:
                     template_code = fp.read()
             except FileNotFoundError:
                 tried.append((
('django/template/loaders', 'cached.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -14,10 +14,14 @@
 class Loader(BaseLoader):

     def __init__(self, engine, loaders):
-        self.template_cache = {}
         self.get_template_cache = {}
         self.loaders = engine.get_template_loaders(loaders)
         super().__init__(engine)
+
+    def get_dirs(self):
+        for loader in self.loaders:
+            if hasattr(loader, "get_dirs"):
+                yield from loader.get_dirs()

     def get_contents(self, origin):
         return origin.loader.get_contents(origin)
@@ -66,7 +70,7 @@

     def cache_key(self, template_name, skip=None):
         """
-        Generate a cache key for the template name, dirs, and skip.
+        Generate a cache key for the template name and skip.

         If skip is provided, only origins that match template_name are included
         in the cache key. This ensures each template is only parsed and cached
@@ -76,7 +80,6 @@
             y -> a -> a
             z -> a -> a
         """
-        dirs_prefix = ''
         skip_prefix = ''

         if skip:
@@ -84,12 +87,11 @@
             if matching:
                 skip_prefix = self.generate_hash(matching)

-        return '-'.join(s for s in (str(template_name), skip_prefix, dirs_prefix) if s)
+        return '-'.join(s for s in (str(template_name), skip_prefix) if s)

     def generate_hash(self, values):
         return hashlib.sha1('|'.join(values).encode()).hexdigest()

     def reset(self):
         "Empty the template cache."
-        self.template_cache.clear()
         self.get_template_cache.clear()
('django/template/loaders', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -17,7 +17,7 @@

         for origin in self.get_template_sources(template_name):
             if skip is not None and origin in skip:
-                tried.append((origin, 'Skipped'))
+                tried.append((origin, 'Skipped to avoid recursion'))
                 continue

             try:
('django/utils', '_os.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,24 +1,9 @@
 import os
 import tempfile
 from os.path import abspath, dirname, join, normcase, sep
+from pathlib import Path

 from django.core.exceptions import SuspiciousFileOperation
-
-# For backwards-compatibility in Django 2.0
-abspathu = abspath
-
-
-def upath(path):
-    """Always return a unicode path (did something for Python 2)."""
-    return path
-
-
-def npath(path):
-    """
-    Always return a native path, that is unicode on Python 3 and bytestring on
-    Python 2. Noop for Python 3.
-    """
-    return path


 def safe_join(base, *paths):
@@ -63,3 +48,12 @@
         except (OSError, NotImplementedError):
             supported = False
         return supported
+
+
+def to_path(value):
+    """Convert value to a pathlib.Path instance, if not already a Path."""
+    if isinstance(value, Path):
+        return value
+    elif not isinstance(value, str):
+        raise TypeError('Invalid path type: %s' % type(value).__name__)
+    return Path(value)
('django/utils', 'termcolors.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -144,7 +144,7 @@
     where:
         palette is a named palette; one of 'light', 'dark', or 'nocolor'.
         role is a named style used by Django
-        fg is a background color.
+        fg is a foreground color.
         bg is a background color.
         option is a display options.

('django/utils', 'topological_sort.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -27,10 +27,10 @@
                 todo.items() if node not in current}


-def stable_topological_sort(l, dependency_graph):
+def stable_topological_sort(nodes, dependency_graph):
     result = []
     for layer in topological_sort_as_sets(dependency_graph):
-        for node in l:
+        for node in nodes:
             if node in layer:
                 result.append(node)
     return result
('django/utils', 'tree.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -90,7 +90,7 @@
         If `squash` is False the data is prepared and added as a child to
         this tree without further logic.
         """
-        if data in self.children:
+        if self.connector == conn_type and data in self.children:
             return data
         if not squash:
             self.children.append(data)
('django/utils', 'hashable.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,10 +2,15 @@


 def make_hashable(value):
+    """
+    Attempt to make value hashable or raise a TypeError if it fails.
+
+    The returned value should generate the same hash for equal values.
+    """
     if isinstance(value, dict):
         return tuple([
             (key, make_hashable(nested_value))
-            for key, nested_value in value.items()
+            for key, nested_value in sorted(value.items())
         ])
     # Try hash to avoid converting a hashable iterable (e.g. string, frozenset)
     # to a tuple.
('django/utils', 'version.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,7 +3,8 @@
 import os
 import subprocess
 import sys
-from distutils.version import LooseVersion
+
+from django.utils.regex_helper import _lazy_re_compile

 # Private, stable API for detecting the Python version. PYXY means "Python X.Y
 # or later". So that third-party apps can use these values, each constant
@@ -13,6 +14,7 @@
 PY37 = sys.version_info >= (3, 7)
 PY38 = sys.version_info >= (3, 8)
 PY39 = sys.version_info >= (3, 9)
+PY310 = sys.version_info >= (3, 10)


 def get_version(version=None):
@@ -77,12 +79,12 @@
     so it's sufficient for generating the development version numbers.
     """
     repo_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
-    git_log = subprocess.Popen(
-        'git log --pretty=format:%ct --quiet -1 HEAD',
+    git_log = subprocess.run(
+        ['git', 'log', '--pretty=format:%ct', '--quiet', '-1', 'HEAD'],
         stdout=subprocess.PIPE, stderr=subprocess.PIPE,
         shell=True, cwd=repo_dir, universal_newlines=True,
     )
-    timestamp = git_log.communicate()[0]
+    timestamp = git_log.stdout
     try:
         timestamp = datetime.datetime.utcfromtimestamp(int(timestamp))
     except ValueError:
@@ -90,15 +92,21 @@
     return timestamp.strftime('%Y%m%d%H%M%S')


+version_component_re = _lazy_re_compile(r'(\d+|[a-z]+|\.)')
+
+
 def get_version_tuple(version):
     """
     Return a tuple of version numbers (e.g. (1, 2, 3)) from the version
     string (e.g. '1.2.3').
     """
-    loose_version = LooseVersion(version)
     version_numbers = []
-    for item in loose_version.version:
-        if not isinstance(item, int):
-            break
-        version_numbers.append(item)
+    for item in version_component_re.split(version):
+        if item and item != '.':
+            try:
+                component = int(item)
+            except ValueError:
+                break
+            else:
+                version_numbers.append(component)
     return tuple(version_numbers)
('django/utils', 'encoding.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,10 +1,11 @@
 import codecs
 import datetime
 import locale
+import warnings
 from decimal import Decimal
 from urllib.parse import quote

-from django.utils import six
+from django.utils.deprecation import RemovedInDjango40Warning
 from django.utils.functional import Promise


@@ -17,11 +18,7 @@
         return '%s. You passed in %r (%s)' % (super().__str__(), self.obj, type(self.obj))


-# For backwards compatibility. (originally in Django, then added to six 1.9)
-python_2_unicode_compatible = six.python_2_unicode_compatible
-
-
-def smart_text(s, encoding='utf-8', strings_only=False, errors='strict'):
+def smart_str(s, encoding='utf-8', strings_only=False, errors='strict'):
     """
     Return a string representing 's'. Treat bytestrings using the 'encoding'
     codec.
@@ -31,7 +28,7 @@
     if isinstance(s, Promise):
         # The input is the result of a gettext_lazy() call.
         return s
-    return force_text(s, encoding, strings_only, errors)
+    return force_str(s, encoding, strings_only, errors)


 _PROTECTED_TYPES = (
@@ -43,14 +40,14 @@
     """Determine if the object instance is of a protected type.

     Objects of protected types are preserved as-is when passed to
-    force_text(strings_only=True).
+    force_str(strings_only=True).
     """
     return isinstance(obj, _PROTECTED_TYPES)


-def force_text(s, encoding='utf-8', strings_only=False, errors='strict'):
-    """
-    Similar to smart_text, except that lazy instances are resolved to
+def force_str(s, encoding='utf-8', strings_only=False, errors='strict'):
+    """
+    Similar to smart_str(), except that lazy instances are resolved to
     strings, rather than kept as lazy objects.

     If strings_only is True, don't convert (some) non-string-like objects.
@@ -102,18 +99,20 @@
     return str(s).encode(encoding, errors)


-smart_str = smart_text
-force_str = force_text
-
-smart_str.__doc__ = """
-Apply smart_text in Python 3 and smart_bytes in Python 2.
-
-This is suitable for writing to sys.stdout (for instance).
-"""
-
-force_str.__doc__ = """
-Apply force_text in Python 3 and force_bytes in Python 2.
-"""
+def smart_text(s, encoding='utf-8', strings_only=False, errors='strict'):
+    warnings.warn(
+        'smart_text() is deprecated in favor of smart_str().',
+        RemovedInDjango40Warning, stacklevel=2,
+    )
+    return smart_str(s, encoding, strings_only, errors)
+
+
+def force_text(s, encoding='utf-8', strings_only=False, errors='strict'):
+    warnings.warn(
+        'force_text() is deprecated in favor of force_str().',
+        RemovedInDjango40Warning, stacklevel=2,
+    )
+    return force_str(s, encoding, strings_only, errors)


 def iri_to_uri(iri):
@@ -158,7 +157,7 @@
     for fmt in ['%02x', '%02X']
 }
 # And then everything above 128, because bytes ≥ 128 are part of multibyte
-# unicode characters.
+# Unicode characters.
 _hexdig = '0123456789ABCDEFabcdef'
 _hextobyte.update({
     (a + b).encode(): bytes.fromhex(a + b)
@@ -219,19 +218,27 @@
     return quote(path, safe="/:@&+$,-_.!~*'()")


+def punycode(domain):
+    """Return the Punycode of the given domain if it's non-ASCII."""
+    return domain.encode('idna').decode('ascii')
+
+
 def repercent_broken_unicode(path):
     """
     As per section 3.2 of RFC 3987, step three of converting a URI into an IRI,
     repercent-encode any octet produced that is not part of a strictly legal
     UTF-8 octet sequence.
     """
-    try:
-        path.decode()
-    except UnicodeDecodeError as e:
-        repercent = quote(path[e.start:e.end], safe=b"/#%[]=:;$&()+,!?*@'~")
-        path = repercent_broken_unicode(
-            path[:e.start] + force_bytes(repercent) + path[e.end:])
-    return path
+    while True:
+        try:
+            path.decode()
+        except UnicodeDecodeError as e:
+            # CVE-2019-14235: A recursion shouldn't be used since the exception
+            # handling uses massive amounts of memory
+            repercent = quote(path[e.start:e.end], safe=b"/#%[]=:;$&()+,!?*@'~")
+            path = path[:e.start] + repercent.encode() + path[e.end:]
+        else:
+            return path


 def filepath_to_uri(path):
@@ -246,7 +253,7 @@
         return path
     # I know about `os.sep` and `os.altsep` but I want to leave
     # some flexibility for hardcoding separators.
-    return quote(path.replace("\\", "/"), safe="/~!*()'")
+    return quote(str(path).replace("\\", "/"), safe="/~!*()'")


 def get_system_encoding():
('django/utils', 'jslex.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -62,7 +62,7 @@
             for match in regexes[state].finditer(text, start):
                 name = match.lastgroup
                 tok = toks[name]
-                toktext = match.group(name)
+                toktext = match[name]
                 start += len(toktext)
                 yield (tok.name, toktext)

@@ -192,7 +192,7 @@
     """
     def escape_quotes(m):
         """Used in a regex to properly escape double quotes."""
-        s = m.group(0)
+        s = m[0]
         if s == '"':
             return r'\"'
         else:
('django/utils', 'log.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -7,7 +7,6 @@
 from django.core.mail import get_connection
 from django.core.management.color import color_style
 from django.utils.module_loading import import_string
-from django.views.debug import ExceptionReporter

 request_logger = logging.getLogger('django.request')

@@ -83,10 +82,11 @@
     request data will be provided in the email report.
     """

-    def __init__(self, include_html=False, email_backend=None):
+    def __init__(self, include_html=False, email_backend=None, reporter_class=None):
         super().__init__()
         self.include_html = include_html
         self.email_backend = email_backend
+        self.reporter_class = import_string(reporter_class or settings.DEFAULT_EXCEPTION_REPORTER)

     def emit(self, record):
         try:
@@ -116,7 +116,7 @@
         else:
             exc_info = (None, record.getMessage(), None)

-        reporter = ExceptionReporter(request, is_email=True, *exc_info)
+        reporter = self.reporter_class(request, is_email=True, *exc_info)
         message = "%s\n\n%s" % (self.format(no_exc_record), reporter.get_traceback_text())
         html_message = reporter.get_traceback_html() if self.include_html else None
         self.send_mail(subject, message, fail_silently=True, html_message=html_message)
@@ -160,6 +160,8 @@


 class ServerFormatter(logging.Formatter):
+    default_time_format = '%d/%b/%Y %H:%M:%S'
+
     def __init__(self, *args, **kwargs):
         self.style = color_style()
         super().__init__(*args, **kwargs)
('django/utils', 'deprecation.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,16 +1,19 @@
+import asyncio
 import inspect
 import warnings

+from asgiref.sync import sync_to_async

-class RemovedInDjango30Warning(DeprecationWarning):
+
+class RemovedInDjango40Warning(DeprecationWarning):
     pass


-class RemovedInDjango31Warning(PendingDeprecationWarning):
+class RemovedInDjango41Warning(PendingDeprecationWarning):
     pass


-RemovedInNextVersionWarning = RemovedInDjango30Warning
+RemovedInNextVersionWarning = RemovedInDjango40Warning


 class warn_about_renamed_method:
@@ -83,11 +86,31 @@


 class MiddlewareMixin:
+    sync_capable = True
+    async_capable = True
+
+    # RemovedInDjango40Warning: when the deprecation ends, replace with:
+    #   def __init__(self, get_response):
     def __init__(self, get_response=None):
+        self._get_response_none_deprecation(get_response)
         self.get_response = get_response
+        self._async_check()
         super().__init__()

+    def _async_check(self):
+        """
+        If get_response is a coroutine function, turns us into async mode so
+        a thread is not consumed during a whole request.
+        """
+        if asyncio.iscoroutinefunction(self.get_response):
+            # Mark the class as async-capable, but do the actual switch
+            # inside __call__ to avoid swapping out dunder methods
+            self._is_coroutine = asyncio.coroutines._is_coroutine
+
     def __call__(self, request):
+        # Exit out to async mode, if needed
+        if asyncio.iscoroutinefunction(self.get_response):
+            return self.__acall__(request)
         response = None
         if hasattr(self, 'process_request'):
             response = self.process_request(request)
@@ -95,3 +118,30 @@
         if hasattr(self, 'process_response'):
             response = self.process_response(request, response)
         return response
+
+    async def __acall__(self, request):
+        """
+        Async version of __call__ that is swapped in when an async request
+        is running.
+        """
+        response = None
+        if hasattr(self, 'process_request'):
+            response = await sync_to_async(
+                self.process_request,
+                thread_sensitive=True,
+            )(request)
+        response = response or await self.get_response(request)
+        if hasattr(self, 'process_response'):
+            response = await sync_to_async(
+                self.process_response,
+                thread_sensitive=True,
+            )(request, response)
+        return response
+
+    def _get_response_none_deprecation(self, get_response):
+        if get_response is None:
+            warnings.warn(
+                'Passing None for the middleware get_response argument is '
+                'deprecated.',
+                RemovedInDjango40Warning, stacklevel=3,
+            )
('django/utils', 'timesince.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -24,26 +24,30 @@
 )


-def timesince(d, now=None, reversed=False, time_strings=None):
+def timesince(d, now=None, reversed=False, time_strings=None, depth=2):
     """
     Take two datetime objects and return the time between d and now as a nicely
     formatted string, e.g. "10 minutes". If d occurs after now, return
     "0 minutes".

     Units used are years, months, weeks, days, hours, and minutes.
-    Seconds and microseconds are ignored.  Up to two adjacent units will be
+    Seconds and microseconds are ignored. Up to `depth` adjacent units will be
     displayed.  For example, "2 weeks, 3 days" and "1 year, 3 months" are
     possible outputs, but "2 weeks, 3 hours" and "1 year, 5 days" are not.

     `time_strings` is an optional dict of strings to replace the default
     TIME_STRINGS dict.

+    `depth` is an optional integer to control the number of adjacent time
+    units returned.
+
     Adapted from
     https://web.archive.org/web/20060617175230/http://blog.natbat.co.uk/archive/2003/Jun/14/time_since
     """
     if time_strings is None:
         time_strings = TIME_STRINGS
-
+    if depth <= 0:
+        raise ValueError('depth must be greater than 0.')
     # Convert datetime.date to datetime.datetime for comparison.
     if not isinstance(d, datetime.datetime):
         d = datetime.datetime(d.year, d.month, d.day)
@@ -69,23 +73,29 @@
     since = delta.days * 24 * 60 * 60 + delta.seconds
     if since <= 0:
         # d is in the future compared to now, stop processing.
-        return avoid_wrapping(gettext('0 minutes'))
+        return avoid_wrapping(time_strings['minute'] % 0)
     for i, (seconds, name) in enumerate(TIMESINCE_CHUNKS):
         count = since // seconds
         if count != 0:
             break
-    result = avoid_wrapping(time_strings[name] % count)
-    if i + 1 < len(TIMESINCE_CHUNKS):
-        # Now get the second item
-        seconds2, name2 = TIMESINCE_CHUNKS[i + 1]
-        count2 = (since - (seconds * count)) // seconds2
-        if count2 != 0:
-            result += gettext(', ') + avoid_wrapping(time_strings[name2] % count2)
-    return result
+    else:
+        return avoid_wrapping(time_strings['minute'] % 0)
+    result = []
+    current_depth = 0
+    while i < len(TIMESINCE_CHUNKS) and current_depth < depth:
+        seconds, name = TIMESINCE_CHUNKS[i]
+        count = since // seconds
+        if count == 0:
+            break
+        result.append(avoid_wrapping(time_strings[name] % count))
+        since -= seconds * count
+        current_depth += 1
+        i += 1
+    return gettext(', ').join(result)


-def timeuntil(d, now=None, time_strings=None):
+def timeuntil(d, now=None, time_strings=None, depth=2):
     """
     Like timesince, but return a string measuring the time until the given time.
     """
-    return timesince(d, now, reversed=True, time_strings=time_strings)
+    return timesince(d, now, reversed=True, time_strings=time_strings, depth=depth)
('django/utils', 'numberformat.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -26,7 +26,18 @@
         return mark_safe(number)
     # sign
     sign = ''
+    # Treat potentially very large/small floats as Decimals.
+    if isinstance(number, float) and 'e' in str(number).lower():
+        number = Decimal(str(number))
     if isinstance(number, Decimal):
+
+        if decimal_pos is not None:
+            # If the provided number is too small to affect any of the visible
+            # decimal places, consider it equal to '0'.
+            cutoff = Decimal('0.' + '1'.rjust(decimal_pos, '0'))
+            if abs(number) < cutoff:
+                number = Decimal('0')
+
         # Format values with more than 200 digits (an arbitrary cutoff) using
         # scientific notation to avoid high memory usage in {:f}'.format().
         _, digits, exponent = number.as_tuple()
('django/utils', 'html.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,6 @@
 """HTML utilities suitable for global use."""

+import html
 import json
 import re
 from html.parser import HTMLParser
@@ -7,9 +8,11 @@
     parse_qsl, quote, unquote, urlencode, urlsplit, urlunsplit,
 )

+from django.utils.encoding import punycode
 from django.utils.functional import Promise, keep_lazy, keep_lazy_text
 from django.utils.http import RFC3986_GENDELIMS, RFC3986_SUBDELIMS
-from django.utils.safestring import SafeData, SafeText, mark_safe
+from django.utils.regex_helper import _lazy_re_compile
+from django.utils.safestring import SafeData, SafeString, mark_safe
 from django.utils.text import normalize_newlines

 # Configuration for urlize() function.
@@ -19,21 +22,15 @@
 # List of possible strings used for bullets in bulleted lists.
 DOTS = ['&middot;', '*', '\u2022', '&#149;', '&bull;', '&#8226;']

-unencoded_ampersands_re = re.compile(r'&(?!(\w+|#\d+);)')
-word_split_re = re.compile(r'''([\s<>"']+)''')
-simple_url_re = re.compile(r'^https?://\[?\w', re.IGNORECASE)
-simple_url_2_re = re.compile(r'^www\.|^(?!http)\w[^@]+\.(com|edu|gov|int|mil|net|org)($|/.*)$', re.IGNORECASE)
-
-_html_escapes = {
-    ord('&'): '&amp;',
-    ord('<'): '&lt;',
-    ord('>'): '&gt;',
-    ord('"'): '&quot;',
-    ord("'"): '&#39;',
-}
-
-
-@keep_lazy(str, SafeText)
+word_split_re = _lazy_re_compile(r'''([\s<>"']+)''')
+simple_url_re = _lazy_re_compile(r'^https?://\[?\w', re.IGNORECASE)
+simple_url_2_re = _lazy_re_compile(
+    r'^www\.|^(?!http)\w[^@]+\.(com|edu|gov|int|mil|net|org)($|/.*)$',
+    re.IGNORECASE
+)
+
+
+@keep_lazy(str, SafeString)
 def escape(text):
     """
     Return the given text with ampersands, quotes and angle brackets encoded
@@ -43,7 +40,7 @@
     This may result in double-escaping. If this is a concern, use
     conditional_escape() instead.
     """
-    return mark_safe(str(text).translate(_html_escapes))
+    return mark_safe(html.escape(str(text)))


 _js_escapes = {
@@ -65,7 +62,7 @@
 _js_escapes.update((ord('%c' % z), '\\u%04X' % z) for z in range(32))


-@keep_lazy(str, SafeText)
+@keep_lazy(str, SafeString)
 def escapejs(value):
     """Hex encode characters for use in JavaScript strings."""
     return mark_safe(str(value).translate(_js_escapes))
@@ -187,8 +184,8 @@
     value = str(value)
     while '<' in value and '>' in value:
         new_value = _strip_once(value)
-        if len(new_value) >= len(value):
-            # _strip_once was not able to detect more tags
+        if value.count('<') == new_value.count('<'):
+            # _strip_once wasn't able to detect more tags.
             break
         value = new_value
     return value
@@ -217,7 +214,7 @@
         return unquote_quote(url)

     try:
-        netloc = netloc.encode('idna').decode('ascii')  # IDN -> ACE
+        netloc = punycode(netloc)  # IDN -> ACE
     except UnicodeError:  # invalid domain part
         return unquote_quote(url)

@@ -258,15 +255,6 @@
         if limit is None or len(x) <= limit:
             return x
         return '%s…' % x[:max(0, limit - 1)]
-
-    def unescape(text):
-        """
-        If input URL is HTML-escaped, unescape it so that it can be safely fed
-        to smart_urlquote. For example:
-        http://example.com?x=1&amp;y=&lt;2&gt; => http://example.com?x=1&y=<2>
-        """
-        return text.replace('&amp;', '&').replace('&lt;', '<').replace(
-            '&gt;', '>').replace('&quot;', '"').replace('&#39;', "'")

     def trim_punctuation(lead, middle, trail):
         """
@@ -290,9 +278,9 @@
                     trail = closing + trail
                     trimmed_something = True
             # Trim trailing punctuation (after trimming wrapping punctuation,
-            # as encoded entities contain ';'). Unescape entites to avoid
+            # as encoded entities contain ';'). Unescape entities to avoid
             # breaking them by removing ';'.
-            middle_unescaped = unescape(middle)
+            middle_unescaped = html.unescape(middle)
             stripped = middle_unescaped.rstrip(TRAILING_PUNCTUATION_CHARS)
             if middle_unescaped != stripped:
                 trail = middle[len(stripped):] + trail
@@ -329,13 +317,13 @@
             url = None
             nofollow_attr = ' rel="nofollow"' if nofollow else ''
             if simple_url_re.match(middle):
-                url = smart_urlquote(unescape(middle))
+                url = smart_urlquote(html.unescape(middle))
             elif simple_url_2_re.match(middle):
-                url = smart_urlquote('http://%s' % unescape(middle))
+                url = smart_urlquote('http://%s' % html.unescape(middle))
             elif ':' not in middle and is_email_simple(middle):
                 local, domain = middle.rsplit('@', 1)
                 try:
-                    domain = domain.encode('idna').decode('ascii')
+                    domain = punycode(domain)
                 except UnicodeError:
                     continue
                 url = 'mailto:%s@%s' % (local, domain)
@@ -372,7 +360,7 @@
 def html_safe(klass):
     """
     A decorator that defines the __html__ method. This helps non-Django
-    templates to detect classes whose __str__ methods return SafeText.
+    templates to detect classes whose __str__ methods return SafeString.
     """
     if '__html__' in klass.__dict__:
         raise ValueError(
('django/utils', 'cache.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -17,21 +17,21 @@
 "Accept-language" header.
 """
 import hashlib
-import re
 import time
+from collections import defaultdict

 from django.conf import settings
 from django.core.cache import caches
 from django.http import HttpResponse, HttpResponseNotModified
-from django.utils.encoding import iri_to_uri
 from django.utils.http import (
     http_date, parse_etags, parse_http_date_safe, quote_etag,
 )
 from django.utils.log import log_response
+from django.utils.regex_helper import _lazy_re_compile
 from django.utils.timezone import get_current_timezone_name
 from django.utils.translation import get_language

-cc_delim_re = re.compile(r'\s*,\s*')
+cc_delim_re = _lazy_re_compile(r'\s*,\s*')


 def patch_cache_control(response, **kwargs):
@@ -53,17 +53,21 @@
         else:
             return (t[0].lower(), True)

-    def dictvalue(t):
+    def dictvalue(*t):
         if t[1] is True:
             return t[0]
         else:
             return '%s=%s' % (t[0], t[1])

+    cc = defaultdict(set)
     if response.get('Cache-Control'):
-        cc = cc_delim_re.split(response['Cache-Control'])
-        cc = dict(dictitem(el) for el in cc)
-    else:
-        cc = {}
+        for field in cc_delim_re.split(response.headers['Cache-Control']):
+            directive, value = dictitem(field)
+            if directive == 'no-cache':
+                # no-cache supports multiple field names.
+                cc[directive].add(value)
+            else:
+                cc[directive] = value

     # If there's already a max-age header but we're being asked to set a new
     # max-age, use the minimum of the two ages. In practice this happens when
@@ -78,9 +82,24 @@
         del cc['public']

     for (k, v) in kwargs.items():
-        cc[k.replace('_', '-')] = v
-    cc = ', '.join(dictvalue(el) for el in cc.items())
-    response['Cache-Control'] = cc
+        directive = k.replace('_', '-')
+        if directive == 'no-cache':
+            # no-cache supports multiple field names.
+            cc[directive].add(v)
+        else:
+            cc[directive] = v
+
+    directives = []
+    for directive, values in cc.items():
+        if isinstance(values, set):
+            if True in values:
+                # True takes precedence.
+                values = {True}
+            directives.extend([dictvalue(directive, value) for value in values])
+        else:
+            directives.append(dictvalue(directive, values))
+    cc = ', '.join(directives)
+    response.headers['Cache-Control'] = cc


 def get_max_age(response):
@@ -90,7 +109,7 @@
     """
     if not response.has_header('Cache-Control'):
         return
-    cc = dict(_to_tuple(el) for el in cc_delim_re.split(response['Cache-Control']))
+    cc = dict(_to_tuple(el) for el in cc_delim_re.split(response.headers['Cache-Control']))
     try:
         return int(cc['max-age'])
     except (ValueError, TypeError, KeyError):
@@ -98,8 +117,8 @@


 def set_response_etag(response):
-    if not response.streaming:
-        response['ETag'] = quote_etag(hashlib.md5(response.content).hexdigest())
+    if not response.streaming and response.content:
+        response.headers['ETag'] = quote_etag(hashlib.md5(response.content).hexdigest())
     return response


@@ -120,7 +139,7 @@
         # Last-Modified.
         for header in ('Cache-Control', 'Content-Location', 'Date', 'ETag', 'Expires', 'Last-Modified', 'Vary'):
             if header in response:
-                new_response[header] = response[header]
+                new_response.headers[header] = response.headers[header]

         # Preserve cookies as per the cookie specification: "If a proxy server
         # receives a response which contains a Set-cookie header, it should
@@ -161,10 +180,13 @@
             return _precondition_failed(request)

     # Step 4: Test the If-Modified-Since precondition.
-    if (not if_none_match_etags and if_modified_since and
-            not _if_modified_since_passes(last_modified, if_modified_since)):
-        if request.method in ('GET', 'HEAD'):
-            return _not_modified(request, response)
+    if (
+        not if_none_match_etags and
+        if_modified_since and
+        not _if_modified_since_passes(last_modified, if_modified_since) and
+        request.method in ('GET', 'HEAD')
+    ):
+        return _not_modified(request, response)

     # Step 5: Test the If-Range precondition (not supported).
     # Step 6: Return original response since there isn't a conditional response.
@@ -241,7 +263,7 @@
     if cache_timeout < 0:
         cache_timeout = 0  # Can't have max-age negative
     if not response.has_header('Expires'):
-        response['Expires'] = http_date(time.time() + cache_timeout)
+        response.headers['Expires'] = http_date(time.time() + cache_timeout)
     patch_cache_control(response, max_age=cache_timeout)


@@ -250,27 +272,32 @@
     Add headers to a response to indicate that a page should never be cached.
     """
     patch_response_headers(response, cache_timeout=-1)
-    patch_cache_control(response, no_cache=True, no_store=True, must_revalidate=True)
+    patch_cache_control(response, no_cache=True, no_store=True, must_revalidate=True, private=True)


 def patch_vary_headers(response, newheaders):
     """
     Add (or update) the "Vary" header in the given HttpResponse object.
-    newheaders is a list of header names that should be in "Vary". Existing
-    headers in "Vary" aren't removed.
+    newheaders is a list of header names that should be in "Vary". If headers
+    contains an asterisk, then "Vary" header will consist of a single asterisk
+    '*'. Otherwise, existing headers in "Vary" aren't removed.
     """
     # Note that we need to keep the original order intact, because cache
     # implementations may rely on the order of the Vary contents in, say,
     # computing an MD5 hash.
     if response.has_header('Vary'):
-        vary_headers = cc_delim_re.split(response['Vary'])
+        vary_headers = cc_delim_re.split(response.headers['Vary'])
     else:
         vary_headers = []
     # Use .lower() here so we treat headers as case-insensitive.
     existing_headers = {header.lower() for header in vary_headers}
     additional_headers = [newheader for newheader in newheaders
                           if newheader.lower() not in existing_headers]
-    response['Vary'] = ', '.join(vary_headers + additional_headers)
+    vary_headers += additional_headers
+    if '*' in vary_headers:
+        response.headers['Vary'] = '*'
+    else:
+        response.headers['Vary'] = ', '.join(vary_headers)


 def has_vary_header(response, header_query):
@@ -279,14 +306,14 @@
     """
     if not response.has_header('Vary'):
         return False
-    vary_headers = cc_delim_re.split(response['Vary'])
+    vary_headers = cc_delim_re.split(response.headers['Vary'])
     existing_headers = {header.lower() for header in vary_headers}
     return header_query.lower() in existing_headers


 def _i18n_cache_key_suffix(request, cache_key):
     """If necessary, add the current locale or time zone to the cache key."""
-    if settings.USE_I18N or settings.USE_L10N:
+    if settings.USE_I18N:
         # first check if LocaleMiddleware or another middleware added
         # LANGUAGE_CODE to request, then fall back to the active language
         # which in turn can also fall back to settings.LANGUAGE_CODE
@@ -303,7 +330,7 @@
         value = request.META.get(header)
         if value is not None:
             ctx.update(value.encode())
-    url = hashlib.md5(iri_to_uri(request.build_absolute_uri()).encode('ascii'))
+    url = hashlib.md5(request.build_absolute_uri().encode('ascii'))
     cache_key = 'views.decorators.cache.cache_page.%s.%s.%s.%s' % (
         key_prefix, method, url.hexdigest(), ctx.hexdigest())
     return _i18n_cache_key_suffix(request, cache_key)
@@ -311,7 +338,7 @@

 def _generate_cache_header_key(key_prefix, request):
     """Return a cache key for the header cache."""
-    url = hashlib.md5(iri_to_uri(request.build_absolute_uri()).encode('ascii'))
+    url = hashlib.md5(request.build_absolute_uri().encode('ascii'))
     cache_key = 'views.decorators.cache.cache_header.%s.%s' % (
         key_prefix, url.hexdigest())
     return _i18n_cache_key_suffix(request, cache_key)
@@ -360,13 +387,13 @@
     if cache is None:
         cache = caches[settings.CACHE_MIDDLEWARE_ALIAS]
     if response.has_header('Vary'):
-        is_accept_language_redundant = settings.USE_I18N or settings.USE_L10N
-        # If i18n or l10n are used, the generated cache key will be suffixed
-        # with the current locale. Adding the raw value of Accept-Language is
-        # redundant in that case and would result in storing the same content
-        # under multiple keys in the cache. See #18191 for details.
+        is_accept_language_redundant = settings.USE_I18N
+        # If i18n is used, the generated cache key will be suffixed with the
+        # current locale. Adding the raw value of Accept-Language is redundant
+        # in that case and would result in storing the same content under
+        # multiple keys in the cache. See #18191 for details.
         headerlist = []
-        for header in cc_delim_re.split(response['Vary']):
+        for header in cc_delim_re.split(response.headers['Vary']):
             header = header.upper().replace('-', '_')
             if header != 'ACCEPT_LANGUAGE' or not is_accept_language_redundant:
                 headerlist.append('HTTP_' + header)
('django/utils', 'datetime_safe.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -7,11 +7,12 @@
 # >>> datetime_safe.date(10, 8, 2).strftime("%Y/%m/%d was a %A")
 # '0010/08/02 was a Monday'

-import re
 import time as ttime
 from datetime import (
     date as real_date, datetime as real_datetime, time as real_time,
 )
+
+from django.utils.regex_helper import _lazy_re_compile


 class date(real_date):
@@ -54,7 +55,7 @@

 # This library does not support strftime's "%s" or "%y" format strings.
 # Allowed if there's an even number of "%"s because they are escaped.
-_illegal_formatting = re.compile(r"((^|[^%])(%%)*%[sy])")
+_illegal_formatting = _lazy_re_compile(r"((^|[^%])(%%)*%[sy])")


 def _findall(text, substr):
@@ -75,7 +76,7 @@
         return super(type(dt), dt).strftime(fmt)
     illegal_formatting = _illegal_formatting.search(fmt)
     if illegal_formatting:
-        raise TypeError("strftime of dates before 1000 does not handle " + illegal_formatting.group(0))
+        raise TypeError('strftime of dates before 1000 does not handle ' + illegal_formatting[0])

     year = dt.year
     # For every non-leap year century, advance by
@@ -99,7 +100,7 @@
             sites.append(site)

     s = s1
-    syear = "%04d" % (dt.year,)
+    syear = "%04d" % dt.year
     for site in sites:
         s = s[:site] + syear + s[site + 4:]
     return s
('django/utils', 'inspect.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,10 +1,25 @@
+import functools
 import inspect


+@functools.lru_cache(maxsize=512)
+def _get_func_parameters(func, remove_first):
+    parameters = tuple(inspect.signature(func).parameters.values())
+    if remove_first:
+        parameters = parameters[1:]
+    return parameters
+
+
+def _get_callable_parameters(meth_or_func):
+    is_method = inspect.ismethod(meth_or_func)
+    func = meth_or_func.__func__ if is_method else meth_or_func
+    return _get_func_parameters(func, remove_first=is_method)
+
+
 def get_func_args(func):
-    sig = inspect.signature(func)
+    params = _get_callable_parameters(func)
     return [
-        arg_name for arg_name, param in sig.parameters.items()
+        param.name for param in params
         if param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD
     ]

@@ -15,10 +30,10 @@
     does not have a default value, omit it in the tuple. Arguments such as
     *args and **kwargs are also included.
     """
-    sig = inspect.signature(func)
+    params = _get_callable_parameters(func)
     args = []
-    for arg_name, param in sig.parameters.items():
-        name = arg_name
+    for param in params:
+        name = param.name
         # Ignore 'self'
         if name == 'self':
             continue
@@ -34,8 +49,9 @@


 def func_accepts_kwargs(func):
+    """Return True if function 'func' accepts keyword arguments **kwargs."""
     return any(
-        p for p in inspect.signature(func).parameters.values()
+        p for p in _get_callable_parameters(func)
         if p.kind == p.VAR_KEYWORD
     )

@@ -45,7 +61,7 @@
     Return True if function 'func' accepts positional arguments *args.
     """
     return any(
-        p for p in inspect.signature(func).parameters.values()
+        p for p in _get_callable_parameters(func)
         if p.kind == p.VAR_POSITIONAL
     )

@@ -53,11 +69,11 @@
 def method_has_no_args(meth):
     """Return True if a method only accepts 'self'."""
     count = len([
-        p for p in inspect.signature(meth).parameters.values()
+        p for p in _get_callable_parameters(meth)
         if p.kind == p.POSITIONAL_OR_KEYWORD
     ])
     return count == 0 if inspect.ismethod(meth) else count == 1


-def func_supports_parameter(func, parameter):
-    return parameter in inspect.signature(func).parameters
+def func_supports_parameter(func, name):
+    return any(param.name == name for param in _get_callable_parameters(func))
('django/utils', 'functional.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,17 +3,6 @@
 import operator
 from functools import total_ordering, wraps

-from django.utils.version import PY36, get_docs_version
-
-
-# You can't trivially replace this with `functools.partial` because this binds
-# to classes and returns bound instances, whereas functools.partial (on
-# CPython) is a type and its instances don't bind.
-def curry(_curried_func, *args, **kwargs):
-    def _curried(*moreargs, **morekwargs):
-        return _curried_func(*args, *moreargs, **{**kwargs, **morekwargs})
-    return _curried
-

 class cached_property:
     """
@@ -22,8 +11,8 @@

     A cached property can be made out of an existing method:
     (e.g. ``url = cached_property(get_absolute_url)``).
-    On Python < 3.6, the optional ``name`` argument must be provided, e.g.
-    ``url = cached_property(get_absolute_url, name='url')``.
+    The optional ``name`` argument is obsolete as of Python 3.6 and will be
+    deprecated in Django 4.0 (#30127).
     """
     name = None

@@ -34,29 +23,8 @@
             '__set_name__() on it.'
         )

-    @staticmethod
-    def _is_mangled(name):
-        return name.startswith('__') and not name.endswith('__')
-
     def __init__(self, func, name=None):
-        if PY36:
-            self.real_func = func
-        else:
-            func_name = func.__name__
-            name = name or func_name
-            if not (isinstance(name, str) and name.isidentifier()):
-                raise ValueError(
-                    "%r can't be used as the name of a cached_property." % name,
-                )
-            if self._is_mangled(name):
-                raise ValueError(
-                    'cached_property does not work with mangled methods on '
-                    'Python < 3.6 without the appropriate `name` argument. See '
-                    'https://docs.djangoproject.com/en/%s/ref/utils/'
-                    '#cached-property-mangled-name' % get_docs_version(),
-                )
-            self.name = name
-            self.func = func
+        self.real_func = func
         self.__doc__ = getattr(func, '__doc__')

     def __set_name__(self, owner, name):
@@ -81,6 +49,22 @@
         return res


+class classproperty:
+    """
+    Decorator that converts a method with a single cls argument into a property
+    that can be accessed directly from the class.
+    """
+    def __init__(self, method=None):
+        self.fget = method
+
+    def __get__(self, instance, cls=None):
+        return self.fget(cls)
+
+    def getter(self, method):
+        self.fget = method
+        return self
+
+
 class Promise:
     """
     Base class for the proxy class created in the closure of the lazy function.
@@ -111,7 +95,7 @@
             self.__kw = kw
             if not self.__prepared:
                 self.__prepare_class__()
-            self.__prepared = True
+            self.__class__.__prepared = True

         def __reduce__(self):
             return (
@@ -192,6 +176,12 @@
                 return str(self) % rhs
             return self.__cast() % rhs

+        def __add__(self, other):
+            return self.__cast() + other
+
+        def __radd__(self, other):
+            return other + self.__cast()
+
         def __deepcopy__(self, memo):
             # Instances of this class are effectively immutable. It's just a
             # collection of functions. So we don't need to do anything
('django/utils', 'crypto.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,27 +3,24 @@
 """
 import hashlib
 import hmac
-import random
-import time
+import secrets
+import warnings

 from django.conf import settings
+from django.utils.deprecation import RemovedInDjango40Warning
 from django.utils.encoding import force_bytes

-# Use the system PRNG if possible
-try:
-    random = random.SystemRandom()
-    using_sysrandom = True
-except NotImplementedError:
-    import warnings
-    warnings.warn('A secure pseudo-random number generator is not available '
-                  'on your system. Falling back to Mersenne Twister.')
-    using_sysrandom = False
+
+class InvalidAlgorithm(ValueError):
+    """Algorithm is not supported by hashlib."""
+    pass


-def salted_hmac(key_salt, value, secret=None):
+def salted_hmac(key_salt, value, secret=None, *, algorithm='sha1'):
     """
-    Return the HMAC-SHA1 of 'value', using a key generated from key_salt and a
-    secret (which defaults to settings.SECRET_KEY).
+    Return the HMAC of 'value', using a key generated from key_salt and a
+    secret (which defaults to settings.SECRET_KEY). Default algorithm is SHA1,
+    but any algorithm name supported by hashlib can be passed.

     A different key_salt should be passed in for every application of HMAC.
     """
@@ -32,46 +29,52 @@

     key_salt = force_bytes(key_salt)
     secret = force_bytes(secret)
-
+    try:
+        hasher = getattr(hashlib, algorithm)
+    except AttributeError as e:
+        raise InvalidAlgorithm(
+            '%r is not an algorithm accepted by the hashlib module.'
+            % algorithm
+        ) from e
     # We need to generate a derived key from our base key.  We can do this by
-    # passing the key_salt and our base key through a pseudo-random function and
-    # SHA1 works nicely.
-    key = hashlib.sha1(key_salt + secret).digest()
-
-    # If len(key_salt + secret) > sha_constructor().block_size, the above
+    # passing the key_salt and our base key through a pseudo-random function.
+    key = hasher(key_salt + secret).digest()
+    # If len(key_salt + secret) > block size of the hash algorithm, the above
     # line is redundant and could be replaced by key = key_salt + secret, since
     # the hmac module does the same thing for keys longer than the block size.
     # However, we need to ensure that we *always* do this.
-    return hmac.new(key, msg=force_bytes(value), digestmod=hashlib.sha1)
+    return hmac.new(key, msg=force_bytes(value), digestmod=hasher)


-def get_random_string(length=12,
-                      allowed_chars='abcdefghijklmnopqrstuvwxyz'
-                                    'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'):
+NOT_PROVIDED = object()  # RemovedInDjango40Warning.
+RANDOM_STRING_CHARS = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'
+
+
+# RemovedInDjango40Warning: when the deprecation ends, replace with:
+#   def get_random_string(length, allowed_chars=RANDOM_STRING_CHARS):
+def get_random_string(length=NOT_PROVIDED, allowed_chars=RANDOM_STRING_CHARS):
     """
     Return a securely generated random string.

-    The default length of 12 with the a-z, A-Z, 0-9 character set returns
-    a 71-bit value. log_2((26+26+10)^12) =~ 71 bits
+    The bit length of the returned value can be calculated with the formula:
+        log_2(len(allowed_chars)^length)
+
+    For example, with default `allowed_chars` (26+26+10), this gives:
+      * length: 12, bit length =~ 71 bits
+      * length: 22, bit length =~ 131 bits
     """
-    if not using_sysrandom:
-        # This is ugly, and a hack, but it makes things better than
-        # the alternative of predictability. This re-seeds the PRNG
-        # using a value that is hard for an attacker to predict, every
-        # time a random string is required. This may change the
-        # properties of the chosen random sequence slightly, but this
-        # is better than absolute predictability.
-        random.seed(
-            hashlib.sha256(
-                ('%s%s%s' % (random.getstate(), time.time(), settings.SECRET_KEY)).encode()
-            ).digest()
+    if length is NOT_PROVIDED:
+        warnings.warn(
+            'Not providing a length argument is deprecated.',
+            RemovedInDjango40Warning,
         )
-    return ''.join(random.choice(allowed_chars) for i in range(length))
+        length = 12
+    return ''.join(secrets.choice(allowed_chars) for i in range(length))


 def constant_time_compare(val1, val2):
     """Return True if the two strings are equal, False otherwise."""
-    return hmac.compare_digest(force_bytes(val1), force_bytes(val2))
+    return secrets.compare_digest(force_bytes(val1), force_bytes(val2))


 def pbkdf2(password, salt, iterations, dklen=0, digest=None):
('django/utils', 'regex_helper.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -5,6 +5,10 @@
 This is not, and is not intended to be, a complete reg-exp decompiler. It
 should be good enough for a large class of URLS, however.
 """
+import re
+
+from django.utils.functional import SimpleLazyObject
+
 # Mapping of an escape character to a representative of that class. So, e.g.,
 # "\w" is replaced by "x" in a reverse URL. A value of None means to ignore
 # this sequence. Any missing key is mapped to itself.
@@ -331,3 +335,17 @@
         for i in range(len(result)):
             result[i] += piece
     return result, result_args
+
+
+def _lazy_re_compile(regex, flags=0):
+    """Lazily compile a regex with flags."""
+    def _compile():
+        # Compile the regex if it was not passed pre-compiled.
+        if isinstance(regex, (str, bytes)):
+            return re.compile(regex, flags)
+        else:
+            assert not flags, (
+                'flags must be empty if regex is passed pre-compiled'
+            )
+            return regex
+    return SimpleLazyObject(_compile)
('django/utils', 'http.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -12,13 +12,13 @@
     urlencode as original_urlencode, uses_params,
 )

-from django.core.exceptions import TooManyFieldsSent
 from django.utils.datastructures import MultiValueDict
-from django.utils.deprecation import RemovedInDjango30Warning
+from django.utils.deprecation import RemovedInDjango40Warning
 from django.utils.functional import keep_lazy_text
+from django.utils.regex_helper import _lazy_re_compile

 # based on RFC 7232, Appendix C
-ETAG_MATCH = re.compile(r'''
+ETAG_MATCH = _lazy_re_compile(r'''
     \A(      # start of string and capture group
     (?:W/)?  # optional weak indicator
     "        # opening quote
@@ -34,15 +34,13 @@
 __Y = r'(?P<year>\d{4})'
 __Y2 = r'(?P<year>\d{2})'
 __T = r'(?P<hour>\d{2}):(?P<min>\d{2}):(?P<sec>\d{2})'
-RFC1123_DATE = re.compile(r'^\w{3}, %s %s %s %s GMT$' % (__D, __M, __Y, __T))
-RFC850_DATE = re.compile(r'^\w{6,9}, %s-%s-%s %s GMT$' % (__D, __M, __Y2, __T))
-ASCTIME_DATE = re.compile(r'^\w{3} %s %s %s %s$' % (__M, __D2, __T, __Y))
+RFC1123_DATE = _lazy_re_compile(r'^\w{3}, %s %s %s %s GMT$' % (__D, __M, __Y, __T))
+RFC850_DATE = _lazy_re_compile(r'^\w{6,9}, %s-%s-%s %s GMT$' % (__D, __M, __Y2, __T))
+ASCTIME_DATE = _lazy_re_compile(r'^\w{3} %s %s %s %s$' % (__M, __D2, __T, __Y))

 RFC3986_GENDELIMS = ":/?#[]@"
 RFC3986_SUBDELIMS = "!$&'()*+,;="

-FIELDS_MATCH = re.compile('[&;]')
-

 @keep_lazy_text
 def urlquote(url, safe='/'):
@@ -50,6 +48,11 @@
     A legacy compatibility wrapper to Python's urllib.parse.quote() function.
     (was used for unicode handling on Python 2)
     """
+    warnings.warn(
+        'django.utils.http.urlquote() is deprecated in favor of '
+        'urllib.parse.quote().',
+        RemovedInDjango40Warning, stacklevel=2,
+    )
     return quote(url, safe)


@@ -59,6 +62,11 @@
     A legacy compatibility wrapper to Python's urllib.parse.quote_plus()
     function. (was used for unicode handling on Python 2)
     """
+    warnings.warn(
+        'django.utils.http.urlquote_plus() is deprecated in favor of '
+        'urllib.parse.quote_plus(),',
+        RemovedInDjango40Warning, stacklevel=2,
+    )
     return quote_plus(url, safe)


@@ -68,6 +76,11 @@
     A legacy compatibility wrapper to Python's urllib.parse.unquote() function.
     (was used for unicode handling on Python 2)
     """
+    warnings.warn(
+        'django.utils.http.urlunquote() is deprecated in favor of '
+        'urllib.parse.unquote().',
+        RemovedInDjango40Warning, stacklevel=2,
+    )
     return unquote(quoted_url)


@@ -77,6 +90,11 @@
     A legacy compatibility wrapper to Python's urllib.parse.unquote_plus()
     function. (was used for unicode handling on Python 2)
     """
+    warnings.warn(
+        'django.utils.http.urlunquote_plus() is deprecated in favor of '
+        'urllib.parse.unquote_plus().',
+        RemovedInDjango40Warning, stacklevel=2,
+    )
     return unquote_plus(quoted_url)


@@ -93,10 +111,10 @@
     for key, value in query:
         if value is None:
             raise TypeError(
-                'Cannot encode None in a query string. Did you mean to pass '
-                'an empty string or omit the value?'
+                "Cannot encode None for key '%s' in a query string. Did you "
+                "mean to pass an empty string or omit the value?" % key
             )
-        elif isinstance(value, (str, bytes)):
+        elif not doseq or isinstance(value, (str, bytes)):
             query_val = value
         else:
             try:
@@ -104,39 +122,21 @@
             except TypeError:
                 query_val = value
             else:
-                # Consume generators and iterators, even when doseq=True, to
+                # Consume generators and iterators, when doseq=True, to
                 # work around https://bugs.python.org/issue31706.
                 query_val = []
                 for item in itr:
                     if item is None:
                         raise TypeError(
-                            'Cannot encode None in a query string. Did you '
-                            'mean to pass an empty string or omit the value?'
+                            "Cannot encode None for key '%s' in a query "
+                            "string. Did you mean to pass an empty string or "
+                            "omit the value?" % key
                         )
                     elif not isinstance(item, bytes):
                         item = str(item)
                     query_val.append(item)
         query_params.append((key, query_val))
     return original_urlencode(query_params, doseq)
-
-
-def cookie_date(epoch_seconds=None):
-    """
-    Format the time to ensure compatibility with Netscape's cookie standard.
-
-    `epoch_seconds` is a floating point number expressed in seconds since the
-    epoch, in UTC - such as that outputted by time.time(). If set to None, it
-    defaults to the current time.
-
-    Output a string in the format 'Wdy, DD-Mon-YYYY HH:MM:SS GMT'.
-    """
-    warnings.warn(
-        'cookie_date() is deprecated in favor of http_date(), which follows '
-        'the format of the latest RFC.',
-        RemovedInDjango30Warning, stacklevel=2,
-    )
-    rfcdate = formatdate(epoch_seconds)
-    return '%s-%s-%s GMT' % (rfcdate[:7], rfcdate[8:11], rfcdate[12:25])


 def http_date(epoch_seconds=None):
@@ -172,17 +172,21 @@
     else:
         raise ValueError("%r is not in a valid HTTP date format" % date)
     try:
-        year = int(m.group('year'))
+        year = int(m['year'])
         if year < 100:
-            if year < 70:
-                year += 2000
+            current_year = datetime.datetime.utcnow().year
+            current_century = current_year - (current_year % 100)
+            if year - (current_year % 100) > 50:
+                # year that appears to be more than 50 years in the future are
+                # interpreted as representing the past.
+                year += current_century - 100
             else:
-                year += 1900
-        month = MONTHS.index(m.group('mon').lower()) + 1
-        day = int(m.group('day'))
-        hour = int(m.group('hour'))
-        min = int(m.group('min'))
-        sec = int(m.group('sec'))
+                year += current_century
+        month = MONTHS.index(m['mon'].lower()) + 1
+        day = int(m['day'])
+        hour = int(m['hour'])
+        min = int(m['min'])
+        sec = int(m['sec'])
         result = datetime.datetime(year, month, day, hour, min, sec)
         return calendar.timegm(result.utctimetuple())
     except Exception as exc:
@@ -259,7 +263,7 @@
     else:
         # Parse each ETag individually, and return any that are valid.
         etag_matches = (ETAG_MATCH.match(etag.strip()) for etag in etag_str.split(','))
-        return [match.group(1) for match in etag_matches if match]
+        return [match[1] for match in etag_matches if match]


 def quote_etag(etag_str):
@@ -292,15 +296,18 @@
     )


-def is_safe_url(url, allowed_hosts, require_https=False):
-    """
-    Return ``True`` if the url is a safe redirection (i.e. it doesn't point to
-    a different host and uses a safe scheme).
+def url_has_allowed_host_and_scheme(url, allowed_hosts, require_https=False):
+    """
+    Return ``True`` if the url uses an allowed host and a safe scheme.

     Always return ``False`` on an empty url.

     If ``require_https`` is ``True``, only 'https' will be considered a valid
     scheme, as opposed to 'http' and 'https' with the default, ``False``.
+
+    Note: "True" doesn't entail that a URL is "safe". It may still be e.g.
+    quoted incorrectly. Ensure to also use django.utils.encoding.iri_to_uri()
+    on the path component of untrusted URLs.
     """
     if url is not None:
         url = url.strip()
@@ -312,8 +319,19 @@
         allowed_hosts = {allowed_hosts}
     # Chrome treats \ completely as / in paths but it could be part of some
     # basic auth credentials so we need to check both URLs.
-    return (_is_safe_url(url, allowed_hosts, require_https=require_https) and
-            _is_safe_url(url.replace('\\', '/'), allowed_hosts, require_https=require_https))
+    return (
+        _url_has_allowed_host_and_scheme(url, allowed_hosts, require_https=require_https) and
+        _url_has_allowed_host_and_scheme(url.replace('\\', '/'), allowed_hosts, require_https=require_https)
+    )
+
+
+def is_safe_url(url, allowed_hosts, require_https=False):
+    warnings.warn(
+        'django.utils.http.is_safe_url() is deprecated in favor of '
+        'url_has_allowed_host_and_scheme().',
+        RemovedInDjango40Warning, stacklevel=2,
+    )
+    return url_has_allowed_host_and_scheme(url, allowed_hosts, require_https)


 # Copied from urllib.parse.urlparse() but uses fixed urlsplit() function.
@@ -365,7 +383,7 @@
     return _coerce_result(v)


-def _is_safe_url(url, allowed_hosts, require_https=False):
+def _url_has_allowed_host_and_scheme(url, allowed_hosts, require_https=False):
     # Chrome considers any URL with more than two slashes to be absolute, but
     # urlparse is not so flexible. Treat any url with three slashes as unsafe.
     if url.startswith('///'):
@@ -394,13 +412,20 @@
             (not scheme or scheme in valid_schemes))


-def limited_parse_qsl(qs, keep_blank_values=False, encoding='utf-8',
-                      errors='replace', fields_limit=None):
+# TODO: Remove when dropping support for PY37.
+def parse_qsl(
+    qs, keep_blank_values=False, strict_parsing=False, encoding='utf-8',
+    errors='replace', max_num_fields=None, separator='&',
+):
     """
     Return a list of key/value tuples parsed from query string.

-    Copied from urlparse with an additional "fields_limit" argument.
-    Copyright (C) 2013 Python Software Foundation (see LICENSE.python).
+    Backport of urllib.parse.parse_qsl() from Python 3.8.8.
+    Copyright (C) 2021 Python Software Foundation (see LICENSE.python).
+
+    ----
+
+    Parse a query given as a string argument.

     Arguments:

@@ -412,37 +437,55 @@
         strings. The default false value indicates that blank values
         are to be ignored and treated as if they were  not included.

+    strict_parsing: flag indicating what to do with parsing errors. If false
+        (the default), errors are silently ignored. If true, errors raise a
+        ValueError exception.
+
     encoding and errors: specify how to decode percent-encoded sequences
         into Unicode characters, as accepted by the bytes.decode() method.

-    fields_limit: maximum number of fields parsed or an exception
-        is raised. None means no limit and is the default.
-    """
-    if fields_limit:
-        pairs = FIELDS_MATCH.split(qs, fields_limit)
-        if len(pairs) > fields_limit:
-            raise TooManyFieldsSent(
-                'The number of GET/POST parameters exceeded '
-                'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'
-            )
-    else:
-        pairs = FIELDS_MATCH.split(qs)
+    max_num_fields: int. If set, then throws a ValueError if there are more
+        than n fields read by parse_qsl().
+
+    separator: str. The symbol to use for separating the query arguments.
+        Defaults to &.
+
+    Returns a list, as G-d intended.
+    """
+    qs, _coerce_result = _coerce_args(qs)
+
+    if not separator or not isinstance(separator, (str, bytes)):
+        raise ValueError('Separator must be of type string or bytes.')
+
+    # If max_num_fields is defined then check that the number of fields is less
+    # than max_num_fields. This prevents a memory exhaustion DOS attack via
+    # post bodies with many fields.
+    if max_num_fields is not None:
+        num_fields = 1 + qs.count(separator)
+        if max_num_fields < num_fields:
+            raise ValueError('Max number of fields exceeded')
+
+    pairs = [s1 for s1 in qs.split(separator)]
     r = []
     for name_value in pairs:
-        if not name_value:
+        if not name_value and not strict_parsing:
             continue
         nv = name_value.split('=', 1)
         if len(nv) != 2:
-            # Handle case of a control-name with no equal sign
+            if strict_parsing:
+                raise ValueError("bad query field: %r" % (name_value,))
+            # Handle case of a control-name with no equal sign.
             if keep_blank_values:
                 nv.append('')
             else:
                 continue
-        if nv[1] or keep_blank_values:
+        if len(nv[1]) or keep_blank_values:
             name = nv[0].replace('+', ' ')
             name = unquote(name, encoding=encoding, errors=errors)
+            name = _coerce_result(name)
             value = nv[1].replace('+', ' ')
             value = unquote(value, encoding=encoding, errors=errors)
+            value = _coerce_result(value)
             r.append((name, value))
     return r

('django/utils', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -197,6 +197,8 @@
     elif isinstance(value, bool):  # Make sure booleans don't get treated as numbers
         return str(value)
     elif isinstance(value, (decimal.Decimal, float, int)):
+        if use_l10n is False:
+            return str(value)
         return number_format(value, use_l10n=use_l10n)
     elif isinstance(value, datetime.datetime):
         return date_format(value, 'DATETIME_FORMAT', use_l10n=use_l10n)
('django/utils', 'text.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,10 +1,14 @@
 import html.entities
 import re
 import unicodedata
+import warnings
 from gzip import GzipFile
 from io import BytesIO

+from django.core.exceptions import SuspiciousFileOperation
+from django.utils.deprecation import RemovedInDjango40Warning
 from django.utils.functional import SimpleLazyObject, keep_lazy_text, lazy
+from django.utils.regex_helper import _lazy_re_compile
 from django.utils.translation import gettext as _, gettext_lazy, pgettext


@@ -15,11 +19,11 @@


 # Set up regular expressions
-re_words = re.compile(r'<.*?>|((?:\w[-\w]*|&.*?;)+)', re.S)
-re_chars = re.compile(r'<.*?>|(.)', re.S)
-re_tag = re.compile(r'<(/)?(\S+?)(?:(\s*/)|\s.*?)?>', re.S)
-re_newlines = re.compile(r'\r\n|\r')  # Used in normalize_newlines
-re_camel_case = re.compile(r'(((?<=[a-z])[A-Z])|([A-Z](?![A-Z]|$)))')
+re_words = _lazy_re_compile(r'<[^>]+?>|([^<>\s]+)', re.S)
+re_chars = _lazy_re_compile(r'<[^>]+?>|(.)', re.S)
+re_tag = _lazy_re_compile(r'<(/)?(\S+?)(?:(\s*/)|\s.*?)?>', re.S)
+re_newlines = _lazy_re_compile(r'\r\n|\r')  # Used in normalize_newlines
+re_camel_case = _lazy_re_compile(r'(((?<=[a-z])[A-Z])|([A-Z](?![A-Z]|$)))')


 @keep_lazy_text
@@ -172,14 +176,14 @@
                 # Checked through whole string
                 break
             pos = m.end(0)
-            if m.group(1):
+            if m[1]:
                 # It's an actual non-HTML word or char
                 current_len += 1
                 if current_len == truncate_len:
                     end_text_pos = pos
                 continue
             # Check for tag
-            tag = re_tag.match(m.group(0))
+            tag = re_tag.match(m[0])
             if not tag or current_len >= truncate_len:
                 # Don't worry about non tags or tags after our truncate point
                 continue
@@ -216,7 +220,7 @@


 @keep_lazy_text
-def get_valid_filename(s):
+def get_valid_filename(name):
     """
     Return the given string converted to a string that can be used for a clean
     filename. Remove leading and trailing spaces; convert other spaces to
@@ -225,8 +229,11 @@
     >>> get_valid_filename("john's portrait in 2004.jpg")
     'johns_portrait_in_2004.jpg'
     """
-    s = str(s).strip().replace(' ', '_')
-    return re.sub(r'(?u)[^-\w.]', '', s)
+    s = str(name).strip().replace(' ', '_')
+    s = re.sub(r'(?u)[^-\w.]', '', s)
+    if s in {'', '.', '..'}:
+        raise SuspiciousFileOperation("Could not derive file name from '%s'" % name)
+    return s


 @keep_lazy_text
@@ -304,7 +311,7 @@

 # Expression to match some_token and some_token="with spaces" (and similarly
 # for single-quoted strings).
-smart_split_re = re.compile(r"""
+smart_split_re = _lazy_re_compile(r"""
     ((?:
         [^\s'"]*
         (?:
@@ -331,11 +338,11 @@
     ['A', '"\\"funky\\" style"', 'test.']
     """
     for bit in smart_split_re.finditer(str(text)):
-        yield bit.group(0)
+        yield bit[0]


 def _replace_entity(match):
-    text = match.group(1)
+    text = match[1]
     if text[0] == '#':
         text = text[1:]
         try:
@@ -345,19 +352,24 @@
                 c = int(text)
             return chr(c)
         except ValueError:
-            return match.group(0)
+            return match[0]
     else:
         try:
             return chr(html.entities.name2codepoint[text])
-        except (ValueError, KeyError):
-            return match.group(0)
-
-
-_entity_re = re.compile(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));")
+        except KeyError:
+            return match[0]
+
+
+_entity_re = _lazy_re_compile(r"&(#?[xX]?(?:[0-9a-fA-F]+|\w{1,8}));")


 @keep_lazy_text
 def unescape_entities(text):
+    warnings.warn(
+        'django.utils.text.unescape_entities() is deprecated in favor of '
+        'html.unescape().',
+        RemovedInDjango40Warning, stacklevel=2,
+    )
     return _entity_re.sub(_replace_entity, str(text))


@@ -385,17 +397,18 @@
 @keep_lazy_text
 def slugify(value, allow_unicode=False):
     """
-    Convert to ASCII if 'allow_unicode' is False. Convert spaces to hyphens.
-    Remove characters that aren't alphanumerics, underscores, or hyphens.
-    Convert to lowercase. Also strip leading and trailing whitespace.
+    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated
+    dashes to single dashes. Remove characters that aren't alphanumerics,
+    underscores, or hyphens. Convert to lowercase. Also strip leading and
+    trailing whitespace, dashes, and underscores.
     """
     value = str(value)
     if allow_unicode:
         value = unicodedata.normalize('NFKC', value)
     else:
         value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')
-    value = re.sub(r'[^\w\s-]', '', value).strip().lower()
-    return re.sub(r'[-\s]+', '-', value)
+    value = re.sub(r'[^\w\s-]', '', value.lower())
+    return re.sub(r'[-\s]+', '-', value).strip('-_')


 def camel_case_to_spaces(value):
('django/utils', 'archive.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -27,6 +27,8 @@
 import tarfile
 import zipfile

+from django.core.exceptions import SuspiciousOperation
+

 class ArchiveException(Exception):
     """
@@ -40,7 +42,7 @@
     """


-def extract(path, to_path=''):
+def extract(path, to_path):
     """
     Unpack the tar or zip file at the specified path to the directory
     specified by to_path.
@@ -83,7 +85,7 @@
     def __exit__(self, exc_type, exc_value, traceback):
         self.close()

-    def extract(self, to_path=''):
+    def extract(self, to_path):
         self._archive.extract(to_path)

     def list(self):
@@ -133,6 +135,13 @@
                 return False
         return True

+    def target_filename(self, to_path, name):
+        target_path = os.path.abspath(to_path)
+        filename = os.path.abspath(os.path.join(target_path, name))
+        if not filename.startswith(target_path):
+            raise SuspiciousOperation("Archive contains invalid path: '%s'" % name)
+        return filename
+
     def extract(self):
         raise NotImplementedError('subclasses of BaseArchive must provide an extract() method')

@@ -155,10 +164,10 @@
             name = member.name
             if leading:
                 name = self.split_leading_dir(name)[1]
-            filename = os.path.join(to_path, name)
+            filename = self.target_filename(to_path, name)
             if member.isdir():
-                if filename and not os.path.exists(filename):
-                    os.makedirs(filename)
+                if filename:
+                    os.makedirs(filename, exist_ok=True)
             else:
                 try:
                     extracted = self._archive.extractfile(member)
@@ -169,8 +178,8 @@
                           (name, member.name, exc))
                 else:
                     dirname = os.path.dirname(filename)
-                    if dirname and not os.path.exists(dirname):
-                        os.makedirs(dirname)
+                    if dirname:
+                        os.makedirs(dirname, exist_ok=True)
                     with open(filename, 'wb') as outfile:
                         shutil.copyfileobj(extracted, outfile)
                         self._copy_permissions(member.mode, filename)
@@ -198,15 +207,16 @@
             info = self._archive.getinfo(name)
             if leading:
                 name = self.split_leading_dir(name)[1]
-            filename = os.path.join(to_path, name)
-            dirname = os.path.dirname(filename)
-            if dirname and not os.path.exists(dirname):
-                os.makedirs(dirname)
-            if filename.endswith(('/', '\\')):
+            if not name:
+                continue
+            filename = self.target_filename(to_path, name)
+            if name.endswith(('/', '\\')):
                 # A directory
-                if not os.path.exists(filename):
-                    os.makedirs(filename)
+                os.makedirs(filename, exist_ok=True)
             else:
+                dirname = os.path.dirname(filename)
+                if dirname:
+                    os.makedirs(dirname, exist_ok=True)
                 with open(filename, 'wb') as outfile:
                     outfile.write(data)
                 # Convert ZipInfo.external_attr to mode
@@ -217,11 +227,11 @@
         self._archive.close()


-extension_map = {
-    '.tar': TarArchive,
-    '.tar.bz2': TarArchive,
-    '.tar.gz': TarArchive,
-    '.tgz': TarArchive,
-    '.tz2': TarArchive,
-    '.zip': ZipArchive,
-}
+extension_map = dict.fromkeys((
+    '.tar',
+    '.tar.bz2', '.tbz2', '.tbz', '.tz2',
+    '.tar.gz', '.tgz', '.taz',
+    '.tar.lzma', '.tlz',
+    '.tar.xz', '.txz',
+), TarArchive)
+extension_map['.zip'] = ZipArchive
('django/utils', 'safestring.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -5,7 +5,7 @@
 be interpreted by the HTML engine (e.g. '<') into the appropriate entities.
 """

-from django.utils.functional import wraps
+from functools import wraps


 class SafeData:
@@ -18,28 +18,7 @@
         return self


-class SafeBytes(bytes, SafeData):
-    """
-    A bytes subclass that has been specifically marked as "safe" (requires no
-    further escaping) for HTML output purposes.
-
-    Kept in Django 2.0 for usage by apps supporting Python 2. Shouldn't be used
-    in Django anymore.
-    """
-    def __add__(self, rhs):
-        """
-        Concatenating a safe byte string with another safe byte string or safe
-        string is safe. Otherwise, the result is no longer safe.
-        """
-        t = super().__add__(rhs)
-        if isinstance(rhs, SafeText):
-            return SafeText(t)
-        elif isinstance(rhs, SafeBytes):
-            return SafeBytes(t)
-        return t
-
-
-class SafeText(str, SafeData):
+class SafeString(str, SafeData):
     """
     A str subclass that has been specifically marked as "safe" for HTML output
     purposes.
@@ -51,14 +30,14 @@
         """
         t = super().__add__(rhs)
         if isinstance(rhs, SafeData):
-            return SafeText(t)
+            return SafeString(t)
         return t

     def __str__(self):
         return self


-SafeString = SafeText
+SafeText = SafeString  # For backwards compatibility since Django 2.0.


 def _safety_decorator(safety_marker, func):
@@ -81,4 +60,4 @@
         return s
     if callable(s):
         return _safety_decorator(mark_safe, s)
-    return SafeText(s)
+    return SafeString(s)
('django/utils', 'autoreload.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,7 +2,6 @@
 import itertools
 import logging
 import os
-import pathlib
 import signal
 import subprocess
 import sys
@@ -15,6 +14,7 @@
 from types import ModuleType
 from zipimport import zipimporter

+import django
 from django.apps import apps
 from django.core.signals import request_finished
 from django.dispatch import Signal
@@ -22,7 +22,7 @@
 from django.utils.version import get_version_tuple

 autoreload_started = Signal()
-file_changed = Signal(providing_args=['file_path', 'kind'])
+file_changed = Signal()

 DJANGO_AUTORELOAD_ENV = 'RUN_MAIN'

@@ -46,6 +46,16 @@
     pywatchman = None


+def is_django_module(module):
+    """Return True if the given module is nested under Django."""
+    return module.__name__.startswith('django.')
+
+
+def is_django_path(path):
+    """Return True if the given file path is nested under Django."""
+    return Path(django.__file__).parent in Path(path).parents
+
+
 def check_errors(fn):
     @functools.wraps(fn)
     def wrapper(*args, **kwargs):
@@ -74,23 +84,26 @@
 def raise_last_exception():
     global _exception
     if _exception is not None:
-        raise _exception[0](_exception[1]).with_traceback(_exception[2])
+        raise _exception[1]


 def ensure_echo_on():
-    if termios:
-        fd = sys.stdin
-        if fd.isatty():
-            attr_list = termios.tcgetattr(fd)
-            if not attr_list[3] & termios.ECHO:
-                attr_list[3] |= termios.ECHO
-                if hasattr(signal, 'SIGTTOU'):
-                    old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN)
-                else:
-                    old_handler = None
-                termios.tcsetattr(fd, termios.TCSANOW, attr_list)
-                if old_handler is not None:
-                    signal.signal(signal.SIGTTOU, old_handler)
+    """
+    Ensure that echo mode is enabled. Some tools such as PDB disable
+    it which causes usability issues after reload.
+    """
+    if not termios or not sys.stdin.isatty():
+        return
+    attr_list = termios.tcgetattr(sys.stdin)
+    if not attr_list[3] & termios.ECHO:
+        attr_list[3] |= termios.ECHO
+        if hasattr(signal, 'SIGTTOU'):
+            old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN)
+        else:
+            old_handler = None
+        termios.tcsetattr(sys.stdin, termios.TCSANOW, attr_list)
+        if old_handler is not None:
+            signal.signal(signal.SIGTTOU, old_handler)


 def iter_all_python_module_files():
@@ -98,8 +111,8 @@
     # modules based on the module name and pass it to iter_modules_and_files().
     # This ensures cached results are returned in the usual case that modules
     # aren't loaded on the fly.
-    modules_view = sorted(list(sys.modules.items()), key=lambda i: i[0])
-    modules = tuple(m[1] for m in modules_view if not isinstance(m[1], weakref.ProxyTypes))
+    keys = sorted(sys.modules)
+    modules = tuple(m for m in map(sys.modules.__getitem__, keys) if not isinstance(m, weakref.ProxyTypes))
     return iter_modules_and_files(modules, frozenset(_error_files))


@@ -111,7 +124,17 @@
         # During debugging (with PyDev) the 'typing.io' and 'typing.re' objects
         # are added to sys.modules, however they are types not modules and so
         # cause issues here.
-        if not isinstance(module, ModuleType) or getattr(module, '__spec__', None) is None:
+        if not isinstance(module, ModuleType):
+            continue
+        if module.__name__ == '__main__':
+            # __main__ (usually manage.py) doesn't always have a __spec__ set.
+            # Handle this by falling back to using __file__, resolved below.
+            # See https://docs.python.org/reference/import.html#main-spec
+            # __file__ may not exists, e.g. when running ipdb debugger.
+            if hasattr(module, '__file__'):
+                sys_file_paths.append(module.__file__)
+            continue
+        if getattr(module, '__spec__', None) is None:
             continue
         spec = module.__spec__
         # Modules could be loaded from places without a concrete location. If
@@ -124,12 +147,18 @@
     for filename in itertools.chain(sys_file_paths, extra_files):
         if not filename:
             continue
-        path = pathlib.Path(filename)
-        if not path.exists():
-            # The module could have been removed, don't fail loudly if this
-            # is the case.
+        path = Path(filename)
+        try:
+            if not path.exists():
+                # The module could have been removed, don't fail loudly if this
+                # is the case.
+                continue
+        except ValueError as e:
+            # Network filesystems may return null bytes in file paths.
+            logger.debug('"%s" raised when resolving path: "%s"', e, path)
             continue
-        results.add(path.resolve().absolute())
+        resolved_path = path.resolve().absolute()
+        results.add(resolved_path)
     return frozenset(results)


@@ -173,12 +202,12 @@
         path = Path(path)
         if not path.exists():
             continue
-        path = path.resolve().absolute()
+        resolved_path = path.resolve().absolute()
         # If the path is a file (like a zip file), watch the parent directory.
-        if path.is_file():
-            yield path.parent
+        if resolved_path.is_file():
+            yield resolved_path.parent
         else:
-            yield path
+            yield resolved_path


 def get_child_arguments():
@@ -187,13 +216,32 @@
     executable is reported to not have the .exe extension which can cause bugs
     on reloading.
     """
-    import django.__main__
+    import __main__
+    py_script = Path(sys.argv[0])

     args = [sys.executable] + ['-W%s' % o for o in sys.warnoptions]
-    if sys.argv[0] == django.__main__.__file__:
-        # The server was started with `python -m django runserver`.
-        args += ['-m', 'django']
+    # __spec__ is set when the server was started with the `-m` option,
+    # see https://docs.python.org/3/reference/import.html#main-spec
+    # __spec__ may not exist, e.g. when running in a Conda env.
+    if getattr(__main__, '__spec__', None) is not None and __main__.__spec__.parent:
+        args += ['-m', __main__.__spec__.parent]
         args += sys.argv[1:]
+    elif not py_script.exists():
+        # sys.argv[0] may not exist for several reasons on Windows.
+        # It may exist with a .exe extension or have a -script.py suffix.
+        exe_entrypoint = py_script.with_suffix('.exe')
+        if exe_entrypoint.exists():
+            # Should be executed directly, ignoring sys.executable.
+            # TODO: Remove str() when dropping support for PY37.
+            # args parameter accepts path-like on Windows from Python 3.8.
+            return [str(exe_entrypoint), *sys.argv[1:]]
+        script_entrypoint = py_script.with_name('%s-script.py' % py_script.name)
+        if script_entrypoint.exists():
+            # Should be executed as usual.
+            # TODO: Remove str() when dropping support for PY37.
+            # args parameter accepts path-like on Windows from Python 3.8.
+            return [*args, str(script_entrypoint), *sys.argv[1:]]
+        raise RuntimeError('Script %s does not exist.' % py_script)
     else:
         args += sys.argv
     return args
@@ -208,9 +256,9 @@
     new_environ = {**os.environ, DJANGO_AUTORELOAD_ENV: 'true'}
     args = get_child_arguments()
     while True:
-        exit_code = subprocess.call(args, env=new_environ, close_fds=False)
-        if exit_code != 3:
-            return exit_code
+        p = subprocess.run(args, env=new_environ, close_fds=False)
+        if p.returncode != 3:
+            return p.returncode


 class BaseReloader:
@@ -221,17 +269,17 @@

     def watch_dir(self, path, glob):
         path = Path(path)
-        if not path.is_absolute():
-            raise ValueError('%s must be absolute.' % path)
+        try:
+            path = path.absolute()
+        except FileNotFoundError:
+            logger.debug(
+                'Unable to watch directory %s as it cannot be resolved.',
+                path,
+                exc_info=True,
+            )
+            return
         logger.debug('Watching dir %s with glob %s.', path, glob)
         self.directory_globs[path].add(glob)
-
-    def watch_file(self, path):
-        path = Path(path)
-        if not path.is_absolute():
-            raise ValueError('%s must be absolute.' % path)
-        logger.debug('Watching file %s.', path)
-        self.extra_files.add(path)

     def watched_files(self, include_globs=True):
         """
@@ -267,9 +315,15 @@
         logger.debug('Waiting for apps ready_event.')
         self.wait_for_apps_ready(apps, django_main_thread)
         from django.urls import get_resolver
+
         # Prevent a race condition where URL modules aren't loaded when the
         # reloader starts by accessing the urlconf_module property.
-        get_resolver().urlconf_module
+        try:
+            get_resolver().urlconf_module
+        except Exception:
+            # Loading the urlconf can result in errors during development.
+            # If this occurs then swallow the error and continue.
+            pass
         logger.debug('Apps ready_event triggered. Sending autoreload_started signal.')
         autoreload_started.send(sender=self)
         self.run_loop()
@@ -316,41 +370,33 @@
     SLEEP_TIME = 1  # Check for changes once per second.

     def tick(self):
-        state, previous_timestamp = {}, time.time()
+        mtimes = {}
         while True:
-            state.update(self.loop_files(state, previous_timestamp))
-            previous_timestamp = time.time()
+            for filepath, mtime in self.snapshot_files():
+                old_time = mtimes.get(filepath)
+                mtimes[filepath] = mtime
+                if old_time is None:
+                    logger.debug('File %s first seen with mtime %s', filepath, mtime)
+                    continue
+                elif mtime > old_time:
+                    logger.debug('File %s previous mtime: %s, current mtime: %s', filepath, old_time, mtime)
+                    self.notify_file_changed(filepath)
+
             time.sleep(self.SLEEP_TIME)
             yield

-    def loop_files(self, previous_times, previous_timestamp):
-        updated_times = {}
-        for path, mtime in self.snapshot_files():
-            previous_time = previous_times.get(path)
-            # If there are overlapping globs, a file may be iterated twice.
-            if path in updated_times:
+    def snapshot_files(self):
+        # watched_files may produce duplicate paths if globs overlap.
+        seen_files = set()
+        for file in self.watched_files():
+            if file in seen_files:
                 continue
-            # A new file has been detected. This could happen due to it being
-            # imported at runtime and only being polled now, or because the
-            # file was just created. Compare the file's mtime to the
-            # previous_timestamp and send a notification if it was created
-            # since the last poll.
-            is_newly_created = previous_time is None and mtime > previous_timestamp
-            is_changed = previous_time is not None and previous_time != mtime
-            if is_newly_created or is_changed:
-                logger.debug('File %s. is_changed: %s, is_new: %s', path, is_changed, is_newly_created)
-                logger.debug('File %s previous mtime: %s, current mtime: %s', path, previous_time, mtime)
-                self.notify_file_changed(path)
-                updated_times[path] = mtime
-        return updated_times
-
-    def snapshot_files(self):
-        for file in self.watched_files():
             try:
                 mtime = file.stat().st_mtime
             except OSError:
                 # This is thrown when the file does not exist.
                 continue
+            seen_files.add(file)
             yield file, mtime

     @classmethod
@@ -366,11 +412,12 @@
     def __init__(self):
         self.roots = defaultdict(set)
         self.processed_request = threading.Event()
+        self.client_timeout = int(os.environ.get('DJANGO_WATCHMAN_TIMEOUT', 5))
         super().__init__()

     @cached_property
     def client(self):
-        return pywatchman.client()
+        return pywatchman.client(timeout=self.client_timeout)

     def _watch_root(self, root):
         # In practice this shouldn't occur, however, it's possible that a
@@ -399,8 +446,15 @@

     def _subscribe(self, directory, name, expression):
         root, rel_path = self._watch_root(directory)
+        # Only receive notifications of files changing, filtering out other types
+        # like special files: https://facebook.github.io/watchman/docs/type
+        only_files_expression = [
+            'allof',
+            ['anyof', ['type', 'f'], ['type', 'l']],
+            expression
+        ]
         query = {
-            'expression': expression,
+            'expression': only_files_expression,
             'fields': ['name'],
             'since': self._get_clock(root),
             'dedup_results': True,
@@ -505,12 +559,17 @@
                 self.processed_request.clear()
             try:
                 self.client.receive()
+            except pywatchman.SocketTimeout:
+                pass
             except pywatchman.WatchmanError as ex:
+                logger.debug('Watchman error: %s, checking server status.', ex)
                 self.check_server_status(ex)
             else:
                 for sub in list(self.client.subs.keys()):
                     self._check_subscription(sub)
             yield
+            # Protect against busy loops.
+            time.sleep(0.1)

     def stop(self):
         self.client.close()
@@ -528,7 +587,7 @@
     def check_availability(cls):
         if not pywatchman:
             raise WatchmanUnavailable('pywatchman not installed.')
-        client = pywatchman.client(timeout=0.01)
+        client = pywatchman.client(timeout=0.1)
         try:
             result = client.capabilityCheck()
         except Exception:
@@ -555,8 +614,8 @@
     ensure_echo_on()

     main_func = check_errors(main_func)
-    django_main_thread = threading.Thread(target=main_func, args=args, kwargs=kwargs)
-    django_main_thread.setDaemon(True)
+    django_main_thread = threading.Thread(target=main_func, args=args, kwargs=kwargs, name='django-main-thread')
+    django_main_thread.daemon = True
     django_main_thread.start()

     while not reloader.should_stop:
('django/utils', 'datastructures.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,16 +1,14 @@
 import copy
-from collections import OrderedDict
 from collections.abc import Mapping


 class OrderedSet:
     """
     A set which keeps the ordering of the inserted items.
-    Currently backs onto OrderedDict.
     """

     def __init__(self, iterable=None):
-        self.dict = OrderedDict.fromkeys(iterable or ())
+        self.dict = dict.fromkeys(iterable or ())

     def add(self, item):
         self.dict[item] = None
@@ -196,16 +194,15 @@
         if len(args) > 1:
             raise TypeError("update expected at most 1 argument, got %d" % len(args))
         if args:
-            other_dict = args[0]
-            if isinstance(other_dict, MultiValueDict):
-                for key, value_list in other_dict.lists():
+            arg = args[0]
+            if isinstance(arg, MultiValueDict):
+                for key, value_list in arg.lists():
                     self.setlistdefault(key).extend(value_list)
             else:
-                try:
-                    for key, value in other_dict.items():
-                        self.setlistdefault(key).append(value)
-                except TypeError:
-                    raise ValueError("MultiValueDict.update() takes either a MultiValueDict or dictionary")
+                if isinstance(arg, Mapping):
+                    arg = arg.items()
+                for key, value in arg:
+                    self.setlistdefault(key).append(value)
         for key, value in kwargs.items():
             self.setlistdefault(key).append(value)

@@ -232,11 +229,8 @@
         self.warning = warning
         return self

-    def complain(self, *wargs, **kwargs):
-        if isinstance(self.warning, Exception):
-            raise self.warning
-        else:
-            raise AttributeError(self.warning)
+    def complain(self, *args, **kwargs):
+        raise AttributeError(self.warning)

     # All list mutation functions complain.
     __delitem__ = complain
('django/utils', 'dateformat.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -12,17 +12,21 @@
 """
 import calendar
 import datetime
-import re
 import time
+from email.utils import format_datetime as format_datetime_rfc5322

 from django.utils.dates import (
     MONTHS, MONTHS_3, MONTHS_ALT, MONTHS_AP, WEEKDAYS, WEEKDAYS_ABBR,
 )
-from django.utils.timezone import get_default_timezone, is_aware, is_naive
+from django.utils.regex_helper import _lazy_re_compile
+from django.utils.timezone import (
+    _datetime_ambiguous_or_imaginary, get_default_timezone, is_aware, is_naive,
+    make_aware,
+)
 from django.utils.translation import gettext as _

-re_formatchars = re.compile(r'(?<!\\)([aAbBcdDeEfFgGhHiIjlLmMnNoOPrsStTUuwWyYzZ])')
-re_escaped = re.compile(r'\\(.)')
+re_formatchars = _lazy_re_compile(r'(?<!\\)([aAbcdDeEfFgGhHiIjlLmMnNoOPrsStTUuwWyYzZ])')
+re_escaped = _lazy_re_compile(r'\\(.)')


 class Formatter:
@@ -68,10 +72,6 @@
             return _('PM')
         return _('AM')

-    def B(self):
-        "Swatch Internet time"
-        raise NotImplementedError('may be implemented in a future release')
-
     def e(self):
         """
         Timezone name.
@@ -101,11 +101,7 @@

     def g(self):
         "Hour, 12-hour format without leading zeros; i.e. '1' to '12'"
-        if self.data.hour == 0:
-            return 12
-        if self.data.hour > 12:
-            return self.data.hour - 12
-        return self.data.hour
+        return self.data.hour % 12 or 12

     def G(self):
         "Hour, 24-hour format without leading zeros; i.e. '0' to '23'"
@@ -123,7 +119,7 @@
         "Minutes; i.e. '00' to '59'"
         return '%02d' % self.data.minute

-    def O(self):  # NOQA: E743
+    def O(self):  # NOQA: E743, E741
         """
         Difference to Greenwich time in hours; e.g. '+0200', '-0430'.

@@ -165,15 +161,9 @@
         if not self.timezone:
             return ""

-        name = None
-        try:
+        if not _datetime_ambiguous_or_imaginary(self.data, self.timezone):
             name = self.timezone.tzname(self.data)
-        except Exception:
-            # pytz raises AmbiguousTimeError during the autumn DST change.
-            # This happens mainly when __init__ receives a naive datetime
-            # and sets self.timezone = get_default_timezone().
-            pass
-        if name is None:
+        else:
             name = self.format('O')
         return str(name)

@@ -189,16 +179,13 @@

         If timezone information is not available, return an empty string.
         """
-        if not self.timezone:
-            return ""
-
-        try:
-            offset = self.timezone.utcoffset(self.data)
-        except Exception:
-            # pytz raises AmbiguousTimeError during the autumn DST change.
-            # This happens mainly when __init__ receives a naive datetime
-            # and sets self.timezone = get_default_timezone().
-            return ""
+        if (
+            not self.timezone or
+            _datetime_ambiguous_or_imaginary(self.data, self.timezone)
+        ):
+            return ""
+
+        offset = self.timezone.utcoffset(self.data)

         # `offset` is a datetime.timedelta. For negative values (to the west of
         # UTC) only days can be negative (days=-1) and seconds are always
@@ -208,8 +195,6 @@


 class DateFormat(TimeFormat):
-    year_days = [None, 0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334]
-
     def b(self):
         "Month, textual, 3 letters, lowercase; e.g. 'jan'"
         return MONTHS_3[self.data.month]
@@ -237,24 +222,20 @@
         "Month, textual, long; e.g. 'January'"
         return MONTHS[self.data.month]

-    def I(self):  # NOQA: E743
+    def I(self):  # NOQA: E743, E741
         "'1' if Daylight Savings Time, '0' otherwise."
-        try:
-            if self.timezone and self.timezone.dst(self.data):
-                return '1'
-            else:
-                return '0'
-        except Exception:
-            # pytz raises AmbiguousTimeError during the autumn DST change.
-            # This happens mainly when __init__ receives a naive datetime
-            # and sets self.timezone = get_default_timezone().
+        if (
+            not self.timezone or
+            _datetime_ambiguous_or_imaginary(self.data, self.timezone)
+        ):
             return ''
+        return '1' if self.timezone.dst(self.data) else '0'

     def j(self):
         "Day of the month without leading zeros; i.e. '1' to '31'"
         return self.data.day

-    def l(self):  # NOQA: E743
+    def l(self):  # NOQA: E743, E741
         "Day of the week, textual, long; e.g. 'Friday'"
         return WEEKDAYS[self.data.weekday()]

@@ -284,7 +265,16 @@

     def r(self):
         "RFC 5322 formatted date; e.g. 'Thu, 21 Dec 2000 16:01:07 +0200'"
-        return self.format('D, j M Y H:i:s O')
+        if type(self.data) is datetime.date:
+            raise TypeError(
+                "The format for date objects may not contain time-related "
+                "format specifiers (found 'r')."
+            )
+        if is_naive(self.data):
+            dt = make_aware(self.data, timezone=self.timezone)
+        else:
+            dt = self.data
+        return format_datetime_rfc5322(dt)

     def S(self):
         "English ordinal suffix for the day of the month, 2 characters; i.e. 'st', 'nd', 'rd' or 'th'"
@@ -316,43 +306,19 @@

     def W(self):
         "ISO-8601 week number of year, weeks starting on Monday"
-        # Algorithm from http://www.personal.ecu.edu/mccartyr/ISOwdALG.txt
-        jan1_weekday = self.data.replace(month=1, day=1).weekday() + 1
-        weekday = self.data.weekday() + 1
-        day_of_year = self.z()
-        if day_of_year <= (8 - jan1_weekday) and jan1_weekday > 4:
-            if jan1_weekday == 5 or (jan1_weekday == 6 and calendar.isleap(self.data.year - 1)):
-                week_number = 53
-            else:
-                week_number = 52
-        else:
-            if calendar.isleap(self.data.year):
-                i = 366
-            else:
-                i = 365
-            if (i - day_of_year) < (4 - weekday):
-                week_number = 1
-            else:
-                j = day_of_year + (7 - weekday) + (jan1_weekday - 1)
-                week_number = j // 7
-                if jan1_weekday > 4:
-                    week_number -= 1
-        return week_number
+        return self.data.isocalendar()[1]

     def y(self):
-        "Year, 2 digits; e.g. '99'"
-        return str(self.data.year)[2:]
+        """Year, 2 digits with leading zeros; e.g. '99'."""
+        return '%02d' % (self.data.year % 100)

     def Y(self):
         "Year, 4 digits; e.g. '1999'"
         return self.data.year

     def z(self):
-        "Day of the year; i.e. '0' to '365'"
-        doy = self.year_days[self.data.month] + self.data.day
-        if self.L() and self.data.month > 2:
-            doy += 1
-        return doy
+        """Day of the year, i.e. 1 to 366."""
+        return self.data.timetuple().tm_yday


 def format(value, format_string):
('django/utils', 'timezone.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,15 +3,13 @@
 """

 import functools
-import warnings
 from contextlib import ContextDecorator
 from datetime import datetime, timedelta, timezone, tzinfo
-from threading import local

 import pytz
+from asgiref.local import Local

 from django.conf import settings
-from django.utils.deprecation import RemovedInDjango31Warning

 __all__ = [
     'utc', 'get_fixed_timezone',
@@ -23,42 +21,13 @@
 ]


-# UTC and local time zones
-
-ZERO = timedelta(0)
-
-
-class FixedOffset(tzinfo):
-    """
-    Fixed offset in minutes east from UTC. Taken from Python's docs.
-
-    Kept as close as possible to the reference version. __init__ was changed
-    to make its arguments optional, according to Python's requirement that
-    tzinfo subclasses can be instantiated without arguments.
-    """
-
-    def __init__(self, offset=None, name=None):
-        warnings.warn(
-            'FixedOffset is deprecated in favor of datetime.timezone',
-            RemovedInDjango31Warning, stacklevel=2,
-        )
-        if offset is not None:
-            self.__offset = timedelta(minutes=offset)
-        if name is not None:
-            self.__name = name
-
-    def utcoffset(self, dt):
-        return self.__offset
-
-    def tzname(self, dt):
-        return self.__name
-
-    def dst(self, dt):
-        return ZERO
-
-
 # UTC time zone as a tzinfo instance.
 utc = pytz.utc
+
+_PYTZ_BASE_CLASSES = (pytz.tzinfo.BaseTzInfo, pytz._FixedOffset)
+# In releases prior to 2018.4, pytz.UTC was not a subclass of BaseTzInfo
+if not isinstance(pytz.UTC, pytz._FixedOffset):
+    _PYTZ_BASE_CLASSES = _PYTZ_BASE_CLASSES + (type(pytz.UTC),)


 def get_fixed_timezone(offset):
@@ -89,7 +58,7 @@
     return _get_timezone_name(get_default_timezone())


-_active = local()
+_active = Local()


 def get_current_timezone():
@@ -103,8 +72,11 @@


 def _get_timezone_name(timezone):
-    """Return the name of ``timezone``."""
-    return timezone.tzname(None)
+    """
+    Return the offset for fixed offset timezones, or the name of timezone if
+    not set.
+    """
+    return timezone.tzname(None) or str(timezone)

 # Timezone selection functions.

@@ -265,7 +237,7 @@
     """Make a naive datetime.datetime in a given time zone aware."""
     if timezone is None:
         timezone = get_current_timezone()
-    if hasattr(timezone, 'localize'):
+    if _is_pytz_zone(timezone):
         # This method is available for pytz time zones.
         return timezone.localize(value, is_dst=is_dst)
     else:
@@ -285,3 +257,20 @@
     if is_naive(value):
         raise ValueError("make_naive() cannot be applied to a naive datetime")
     return value.astimezone(timezone).replace(tzinfo=None)
+
+
+def _is_pytz_zone(tz):
+    """Checks if a zone is a pytz zone."""
+    return isinstance(tz, _PYTZ_BASE_CLASSES)
+
+
+def _datetime_ambiguous_or_imaginary(dt, tz):
+    if _is_pytz_zone(tz):
+        try:
+            tz.utcoffset(dt)
+        except (pytz.AmbiguousTimeError, pytz.NonExistentTimeError):
+            return True
+        else:
+            return False
+
+    return tz.utcoffset(dt.replace(fold=not dt.fold)) != tz.utcoffset(dt)
('django/utils', 'module_loading.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -72,11 +72,10 @@
     full_module_name = package_name + '.' + module_name
     try:
         return importlib_find(full_module_name, package_path) is not None
-    except (ImportError, AttributeError):
-        # When module_name is an invalid dotted path, Python raises ImportError
-        # (or ModuleNotFoundError in Python 3.6+). AttributeError may be raised
+    except (ModuleNotFoundError, AttributeError):
+        # When module_name is an invalid dotted path, Python raises
+        # ModuleNotFoundError. AttributeError is raised on PY36 (fixed in PY37)
         # if the penultimate part of the path is not a package.
-        # (https://bugs.python.org/issue30436)
         return False


@@ -87,7 +86,7 @@
     Raise ValueError otherwise, e.g. for namespace packages that are split
     over several directories.
     """
-    # Convert to list because _NamespacePath does not support indexing on 3.3.
+    # Convert to list because _NamespacePath does not support indexing.
     paths = list(getattr(module, '__path__', []))
     if len(paths) == 1:
         return paths[0]
('django/utils', 'dateparse.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -6,39 +6,40 @@
 # - The date/datetime/time constructors produce friendlier error messages.

 import datetime
-import re

+from django.utils.regex_helper import _lazy_re_compile
 from django.utils.timezone import get_fixed_timezone, utc

-date_re = re.compile(
+date_re = _lazy_re_compile(
     r'(?P<year>\d{4})-(?P<month>\d{1,2})-(?P<day>\d{1,2})$'
 )

-time_re = re.compile(
+time_re = _lazy_re_compile(
     r'(?P<hour>\d{1,2}):(?P<minute>\d{1,2})'
-    r'(?::(?P<second>\d{1,2})(?:\.(?P<microsecond>\d{1,6})\d{0,6})?)?'
+    r'(?::(?P<second>\d{1,2})(?:[\.,](?P<microsecond>\d{1,6})\d{0,6})?)?'
 )

-datetime_re = re.compile(
+datetime_re = _lazy_re_compile(
     r'(?P<year>\d{4})-(?P<month>\d{1,2})-(?P<day>\d{1,2})'
     r'[T ](?P<hour>\d{1,2}):(?P<minute>\d{1,2})'
-    r'(?::(?P<second>\d{1,2})(?:\.(?P<microsecond>\d{1,6})\d{0,6})?)?'
+    r'(?::(?P<second>\d{1,2})(?:[\.,](?P<microsecond>\d{1,6})\d{0,6})?)?'
     r'(?P<tzinfo>Z|[+-]\d{2}(?::?\d{2})?)?$'
 )

-standard_duration_re = re.compile(
+standard_duration_re = _lazy_re_compile(
     r'^'
     r'(?:(?P<days>-?\d+) (days?, )?)?'
-    r'((?:(?P<hours>-?\d+):)(?=\d+:\d+))?'
-    r'(?:(?P<minutes>-?\d+):)?'
-    r'(?P<seconds>-?\d+)'
-    r'(?:\.(?P<microseconds>\d{1,6})\d{0,6})?'
+    r'(?P<sign>-?)'
+    r'((?:(?P<hours>\d+):)(?=\d+:\d+))?'
+    r'(?:(?P<minutes>\d+):)?'
+    r'(?P<seconds>\d+)'
+    r'(?:[\.,](?P<microseconds>\d{1,6})\d{0,6})?'
     r'$'
 )

 # Support the sections of ISO 8601 date representation that are accepted by
 # timedelta
-iso8601_duration_re = re.compile(
+iso8601_duration_re = _lazy_re_compile(
     r'^(?P<sign>[-+]?)'
     r'P'
     r'(?:(?P<days>\d+(.\d+)?)D)?'
@@ -53,7 +54,7 @@
 # Support PostgreSQL's day-time interval format, e.g. "3 days 04:05:06". The
 # year-month and mixed intervals cannot be converted to a timedelta and thus
 # aren't accepted.
-postgres_interval_re = re.compile(
+postgres_interval_re = _lazy_re_compile(
     r'^'
     r'(?:(?P<days>-?\d+) (days? ?))?'
     r'(?:(?P<sign>[-+])?'
@@ -136,11 +137,13 @@
     )
     if match:
         kw = match.groupdict()
-        days = datetime.timedelta(float(kw.pop('days', 0) or 0))
         sign = -1 if kw.pop('sign', '+') == '-' else 1
         if kw.get('microseconds'):
             kw['microseconds'] = kw['microseconds'].ljust(6, '0')
         if kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-'):
             kw['microseconds'] = '-' + kw['microseconds']
-        kw = {k: float(v) for k, v in kw.items() if v is not None}
+        kw = {k: float(v.replace(',', '.')) for k, v in kw.items() if v is not None}
+        days = datetime.timedelta(kw.pop('days', .0) or .0)
+        if match.re == iso8601_duration_re:
+            days *= sign
         return days + sign * datetime.timedelta(**kw)
('django/utils', 'xmlutils.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,7 +3,6 @@
 """

 import re
-from collections import OrderedDict
 from xml.sax.saxutils import XMLGenerator


@@ -30,5 +29,5 @@

     def startElement(self, name, attrs):
         # Sort attrs for a deterministic output.
-        sorted_attrs = OrderedDict(sorted(attrs.items())) if attrs else attrs
+        sorted_attrs = dict(sorted(attrs.items())) if attrs else attrs
         super().startElement(name, sorted_attrs)
('django/utils', 'decorators.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,8 +1,6 @@
 "Functions that help with dynamically creating decorators for views."

-# For backwards compatibility in Django 2.0.
-from contextlib import ContextDecorator  # noqa
-from functools import WRAPPER_ASSIGNMENTS, partial, update_wrapper, wraps
+from functools import partial, update_wrapper, wraps


 class classonlymethod(classmethod):
@@ -113,21 +111,11 @@
     return make_middleware_decorator(middleware_class)()


-# Unused, for backwards compatibility in Django 2.0.
-def available_attrs(fn):
-    """
-    Return the list of functools-wrappable attributes on a callable.
-    This was required as a workaround for https://bugs.python.org/issue3445
-    under Python 2.
-    """
-    return WRAPPER_ASSIGNMENTS
-
-
 def make_middleware_decorator(middleware_class):
     def _make_decorator(*m_args, **m_kwargs):
-        middleware = middleware_class(*m_args, **m_kwargs)
+        def _decorator(view_func):
+            middleware = middleware_class(view_func, *m_args, **m_kwargs)

-        def _decorator(view_func):
             @wraps(view_func)
             def _wrapped_view(request, *args, **kwargs):
                 if hasattr(middleware, 'process_request'):
@@ -164,13 +152,28 @@
     return _make_decorator


-class classproperty:
-    def __init__(self, method=None):
-        self.fget = method
+def sync_and_async_middleware(func):
+    """
+    Mark a middleware factory as returning a hybrid middleware supporting both
+    types of request.
+    """
+    func.sync_capable = True
+    func.async_capable = True
+    return func

-    def __get__(self, instance, cls=None):
-        return self.fget(cls)

-    def getter(self, method):
-        self.fget = method
-        return self
+def sync_only_middleware(func):
+    """
+    Mark a middleware factory as returning a sync middleware.
+    This is the default.
+    """
+    func.sync_capable = True
+    func.async_capable = False
+    return func
+
+
+def async_only_middleware(func):
+    """Mark a middleware factory as returning an async middleware."""
+    func.sync_capable = False
+    func.async_capable = True
+    return func
('django/utils/translation', 'trans_real.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -5,8 +5,8 @@
 import re
 import sys
 import warnings
-from collections import OrderedDict
-from threading import local
+
+from asgiref.local import Local

 from django.apps import apps
 from django.conf import settings
@@ -14,14 +14,15 @@
 from django.core.exceptions import AppRegistryNotReady
 from django.core.signals import setting_changed
 from django.dispatch import receiver
+from django.utils.regex_helper import _lazy_re_compile
 from django.utils.safestring import SafeData, mark_safe

-from . import LANGUAGE_SESSION_KEY, to_language, to_locale
+from . import to_language, to_locale

 # Translations are cached in a dictionary for every language.
 # The active translations are stored by threadid to make them thread local.
 _translations = {}
-_active = local()
+_active = Local()

 # The default translation is based on the settings file.
 _default = None
@@ -31,18 +32,18 @@

 # Format of Accept-Language header values. From RFC 2616, section 14.4 and 3.9
 # and RFC 3066, section 2.1
-accept_language_re = re.compile(r'''
+accept_language_re = _lazy_re_compile(r'''
         ([A-Za-z]{1,8}(?:-[A-Za-z0-9]{1,8})*|\*)      # "en", "en-au", "x-y-z", "es-419", "*"
         (?:\s*;\s*q=(0(?:\.\d{,3})?|1(?:\.0{,3})?))?  # Optional "q=1.00", "q=0.8"
         (?:\s*,\s*|$)                                 # Multiple accepts per header.
         ''', re.VERBOSE)

-language_code_re = re.compile(
+language_code_re = _lazy_re_compile(
     r'^[a-z]{1,8}(?:-[a-z0-9]{1,8})*(?:@[a-z0-9]{1,20})?$',
     re.IGNORECASE
 )

-language_code_prefix_re = re.compile(r'^/(\w+([@-]\w+)?)(/|$)')
+language_code_prefix_re = _lazy_re_compile(r'^/(\w+([@-]\w+)?)(/|$)')


 @receiver(setting_changed)
@@ -55,6 +56,63 @@
         check_for_language.cache_clear()
         get_languages.cache_clear()
         get_supported_language_variant.cache_clear()
+
+
+class TranslationCatalog:
+    """
+    Simulate a dict for DjangoTranslation._catalog so as multiple catalogs
+    with different plural equations are kept separate.
+    """
+    def __init__(self, trans=None):
+        self._catalogs = [trans._catalog.copy()] if trans else [{}]
+        self._plurals = [trans.plural] if trans else [lambda n: int(n != 1)]
+
+    def __getitem__(self, key):
+        for cat in self._catalogs:
+            try:
+                return cat[key]
+            except KeyError:
+                pass
+        raise KeyError(key)
+
+    def __setitem__(self, key, value):
+        self._catalogs[0][key] = value
+
+    def __contains__(self, key):
+        return any(key in cat for cat in self._catalogs)
+
+    def items(self):
+        for cat in self._catalogs:
+            yield from cat.items()
+
+    def keys(self):
+        for cat in self._catalogs:
+            yield from cat.keys()
+
+    def update(self, trans):
+        # Merge if plural function is the same, else prepend.
+        for cat, plural in zip(self._catalogs, self._plurals):
+            if trans.plural.__code__ == plural.__code__:
+                cat.update(trans._catalog)
+                break
+        else:
+            self._catalogs.insert(0, trans._catalog.copy())
+            self._plurals.insert(0, trans.plural)
+
+    def get(self, key, default=None):
+        missing = object()
+        for cat in self._catalogs:
+            result = cat.get(key, missing)
+            if result is not missing:
+                return result
+        return default
+
+    def plural(self, msgid, num):
+        for cat, plural in zip(self._catalogs, self._plurals):
+            tmsg = cat.get((msgid, plural(num)))
+            if tmsg is not None:
+                return tmsg
+        raise KeyError


 class DjangoTranslation(gettext_module.GNUTranslations):
@@ -99,11 +157,11 @@
         self._add_local_translations()
         if self.__language == settings.LANGUAGE_CODE and self.domain == 'django' and self._catalog is None:
             # default lang should have at least one translation file available.
-            raise IOError("No translation files found for default language %s." % settings.LANGUAGE_CODE)
+            raise OSError('No translation files found for default language %s.' % settings.LANGUAGE_CODE)
         self._add_fallback(localedirs)
         if self._catalog is None:
             # No catalogs found for this language, set an empty catalog.
-            self._catalog = {}
+            self._catalog = TranslationCatalog()

     def __repr__(self):
         return "<DjangoTranslation lang:%s>" % self.__language
@@ -174,9 +232,9 @@
             # Take plural and _info from first catalog found (generally Django's).
             self.plural = other.plural
             self._info = other._info.copy()
-            self._catalog = other._catalog.copy()
+            self._catalog = TranslationCatalog(other)
         else:
-            self._catalog.update(other._catalog)
+            self._catalog.update(other)
         if other._fallback:
             self.add_fallback(other._fallback)

@@ -187,6 +245,18 @@
     def to_language(self):
         """Return the translation language name."""
         return self.__to_language
+
+    def ngettext(self, msgid1, msgid2, n):
+        try:
+            tmsg = self._catalog.plural(msgid1, n)
+        except KeyError:
+            if self._fallback:
+                return self._fallback.ngettext(msgid1, msgid2, n)
+            if n == 1:
+                tmsg = msgid1
+            else:
+                tmsg = msgid2
+        return tmsg


 def translation(language):
@@ -385,9 +455,9 @@
 @functools.lru_cache()
 def get_languages():
     """
-    Cache of settings.LANGUAGES in an OrderedDict for easy lookups by key.
-    """
-    return OrderedDict(settings.LANGUAGES)
+    Cache of settings.LANGUAGES in a dictionary for easy lookups by key.
+    """
+    return dict(settings.LANGUAGES)


 @functools.lru_cache(maxsize=1000)
@@ -435,7 +505,7 @@
     regex_match = language_code_prefix_re.match(path)
     if not regex_match:
         return None
-    lang_code = regex_match.group(1)
+    lang_code = regex_match[1]
     try:
         return get_supported_language_variant(lang_code, strict=strict)
     except LookupError:
@@ -457,14 +527,9 @@
         if lang_code is not None:
             return lang_code

-    supported_lang_codes = get_languages()
-
-    if hasattr(request, 'session'):
-        lang_code = request.session.get(LANGUAGE_SESSION_KEY)
-        if lang_code in supported_lang_codes and lang_code is not None and check_for_language(lang_code):
-            return lang_code
-
     lang_code = request.COOKIES.get(settings.LANGUAGE_COOKIE_NAME)
+    if lang_code is not None and lang_code in get_languages() and check_for_language(lang_code):
+        return lang_code

     try:
         return get_supported_language_variant(lang_code)
('django/utils/translation', 'reloader.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,7 +1,9 @@
-import threading
 from pathlib import Path

+from asgiref.local import Local
+
 from django.apps import apps
+from django.utils.autoreload import is_django_module


 def watch_for_translation_changes(sender, **kwargs):
@@ -10,20 +12,24 @@

     if settings.USE_I18N:
         directories = [Path('locale')]
-        directories.extend(Path(config.path) / 'locale' for config in apps.get_app_configs())
+        directories.extend(
+            Path(config.path) / 'locale'
+            for config in apps.get_app_configs()
+            if not is_django_module(config.module)
+        )
         directories.extend(Path(p) for p in settings.LOCALE_PATHS)
         for path in directories:
-            absolute_path = path.absolute()
-            sender.watch_dir(absolute_path, '**/*.mo')
+            sender.watch_dir(path, '**/*.mo')


 def translation_file_changed(sender, file_path, **kwargs):
     """Clear the internal translations cache if a .mo file is modified."""
     if file_path.suffix == '.mo':
         import gettext
+
         from django.utils.translation import trans_real
         gettext._translations = {}
         trans_real._translations = {}
         trans_real._default = None
-        trans_real._active = threading.local()
+        trans_real._active = Local()
         return True
('django/utils/translation', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,11 +1,14 @@
 """
 Internationalization support.
 """
-import re
+import warnings
 from contextlib import ContextDecorator
+from decimal import ROUND_UP, Decimal

 from django.utils.autoreload import autoreload_started, file_changed
+from django.utils.deprecation import RemovedInDjango40Warning
 from django.utils.functional import lazy
+from django.utils.regex_helper import _lazy_re_compile

 __all__ = [
     'activate', 'deactivate', 'override', 'deactivate_all',
@@ -53,7 +56,9 @@
         from django.conf import settings
         if settings.USE_I18N:
             from django.utils.translation import trans_real as trans
-            from django.utils.translation.reloader import watch_for_translation_changes, translation_file_changed
+            from django.utils.translation.reloader import (
+                translation_file_changed, watch_for_translation_changes,
+            )
             autoreload_started.connect(watch_for_translation_changes, dispatch_uid='translation_file_changed')
             file_changed.connect(translation_file_changed, dispatch_uid='translation_file_changed')
         else:
@@ -72,23 +77,51 @@
     return _trans.gettext_noop(message)


-ugettext_noop = gettext_noop
+def ugettext_noop(message):
+    """
+    A legacy compatibility wrapper for Unicode handling on Python 2.
+    Alias of gettext_noop() since Django 2.0.
+    """
+    warnings.warn(
+        'django.utils.translation.ugettext_noop() is deprecated in favor of '
+        'django.utils.translation.gettext_noop().',
+        RemovedInDjango40Warning, stacklevel=2,
+    )
+    return gettext_noop(message)


 def gettext(message):
     return _trans.gettext(message)


-# An alias since Django 2.0
-ugettext = gettext
+def ugettext(message):
+    """
+    A legacy compatibility wrapper for Unicode handling on Python 2.
+    Alias of gettext() since Django 2.0.
+    """
+    warnings.warn(
+        'django.utils.translation.ugettext() is deprecated in favor of '
+        'django.utils.translation.gettext().',
+        RemovedInDjango40Warning, stacklevel=2,
+    )
+    return gettext(message)


 def ngettext(singular, plural, number):
     return _trans.ngettext(singular, plural, number)


-# An alias since Django 2.0
-ungettext = ngettext
+def ungettext(singular, plural, number):
+    """
+    A legacy compatibility wrapper for Unicode handling on Python 2.
+    Alias of ngettext() since Django 2.0.
+    """
+    warnings.warn(
+        'django.utils.translation.ungettext() is deprecated in favor of '
+        'django.utils.translation.ngettext().',
+        RemovedInDjango40Warning, stacklevel=2,
+    )
+    return ngettext(singular, plural, number)


 def pgettext(context, message):
@@ -99,8 +132,21 @@
     return _trans.npgettext(context, singular, plural, number)


-gettext_lazy = ugettext_lazy = lazy(gettext, str)
+gettext_lazy = lazy(gettext, str)
 pgettext_lazy = lazy(pgettext, str)
+
+
+def ugettext_lazy(message):
+    """
+    A legacy compatibility wrapper for Unicode handling on Python 2. Has been
+    Alias of gettext_lazy since Django 2.0.
+    """
+    warnings.warn(
+        'django.utils.translation.ugettext_lazy() is deprecated in favor of '
+        'django.utils.translation.gettext_lazy().',
+        RemovedInDjango40Warning, stacklevel=2,
+    )
+    return gettext_lazy(message)


 def lazy_number(func, resultclass, number=None, **kwargs):
@@ -158,8 +204,17 @@
     return lazy_number(ngettext, str, singular=singular, plural=plural, number=number)


-# An alias since Django 2.0
-ungettext_lazy = ngettext_lazy
+def ungettext_lazy(singular, plural, number=None):
+    """
+    A legacy compatibility wrapper for Unicode handling on Python 2.
+    An alias of ungettext_lazy() since Django 2.0.
+    """
+    warnings.warn(
+        'django.utils.translation.ungettext_lazy() is deprecated in favor of '
+        'django.utils.translation.ngettext_lazy().',
+        RemovedInDjango40Warning, stacklevel=2,
+    )
+    return ngettext_lazy(singular, plural, number)


 def npgettext_lazy(context, singular, plural, number=None):
@@ -275,8 +330,12 @@
     return info


-trim_whitespace_re = re.compile(r'\s*\n\s*')
+trim_whitespace_re = _lazy_re_compile(r'\s*\n\s*')


 def trim_whitespace(s):
     return trim_whitespace_re.sub(' ', s.strip())
+
+
+def round_away_from_one(value):
+    return int(Decimal(value - 1).quantize(Decimal('0'), rounding=ROUND_UP)) + 1
('django/utils/translation', 'template.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,12 +1,12 @@
-import re
 import warnings
 from io import StringIO

 from django.template.base import TRANSLATOR_COMMENT_MARK, Lexer, TokenType
+from django.utils.regex_helper import _lazy_re_compile

 from . import TranslatorCommentWarning, trim_whitespace

-dot_re = re.compile(r'\S')
+dot_re = _lazy_re_compile(r'\S')


 def blankout(src, char):
@@ -17,19 +17,19 @@
     return dot_re.sub(char, src)


-context_re = re.compile(r"""^\s+.*context\s+((?:"[^"]*?")|(?:'[^']*?'))\s*""")
-inline_re = re.compile(
-    # Match the trans 'some text' part
-    r"""^\s*trans\s+((?:"[^"]*?")|(?:'[^']*?'))"""
+context_re = _lazy_re_compile(r"""^\s+.*context\s+((?:"[^"]*?")|(?:'[^']*?'))\s*""")
+inline_re = _lazy_re_compile(
+    # Match the trans/translate 'some text' part.
+    r"""^\s*trans(?:late)?\s+((?:"[^"]*?")|(?:'[^']*?'))"""
     # Match and ignore optional filters
     r"""(?:\s*\|\s*[^\s:]+(?::(?:[^\s'":]+|(?:"[^"]*?")|(?:'[^']*?')))?)*"""
     # Match the optional context part
     r"""(\s+.*context\s+((?:"[^"]*?")|(?:'[^']*?')))?\s*"""
 )
-block_re = re.compile(r"""^\s*blocktrans(\s+.*context\s+((?:"[^"]*?")|(?:'[^']*?')))?(?:\s+|$)""")
-endblock_re = re.compile(r"""^\s*endblocktrans$""")
-plural_re = re.compile(r"""^\s*plural$""")
-constant_re = re.compile(r"""_\(((?:".*?")|(?:'.*?'))\)""")
+block_re = _lazy_re_compile(r"""^\s*blocktrans(?:late)?(\s+.*context\s+((?:"[^"]*?")|(?:'[^']*?')))?(?:\s+|$)""")
+endblock_re = _lazy_re_compile(r"""^\s*endblocktrans(?:late)?$""")
+plural_re = _lazy_re_compile(r"""^\s*plural$""")
+constant_re = _lazy_re_compile(r"""_\(((?:".*?")|(?:'.*?'))\)""")


 def templatize(src, origin=None):
@@ -165,16 +165,16 @@
                 bmatch = block_re.match(t.contents)
                 cmatches = constant_re.findall(t.contents)
                 if imatch:
-                    g = imatch.group(1)
+                    g = imatch[1]
                     if g[0] == '"':
                         g = g.strip('"')
                     elif g[0] == "'":
                         g = g.strip("'")
                     g = g.replace('%', '%%')
-                    if imatch.group(2):
+                    if imatch[2]:
                         # A context is provided
-                        context_match = context_re.match(imatch.group(2))
-                        message_context = context_match.group(1)
+                        context_match = context_re.match(imatch[2])
+                        message_context = context_match[1]
                         if message_context[0] == '"':
                             message_context = message_context.strip('"')
                         elif message_context[0] == "'":
@@ -188,10 +188,10 @@
                 elif bmatch:
                     for fmatch in constant_re.findall(t.contents):
                         out.write(' _(%s) ' % fmatch)
-                    if bmatch.group(1):
+                    if bmatch[1]:
                         # A context is provided
-                        context_match = context_re.match(bmatch.group(1))
-                        message_context = context_match.group(1)
+                        context_match = context_re.match(bmatch[1])
+                        message_context = context_match[1]
                         if message_context[0] == '"':
                             message_context = message_context.strip('"')
                         elif message_context[0] == "'":
@@ -212,7 +212,7 @@
                 parts = t.contents.split('|')
                 cmatch = constant_re.match(parts[0])
                 if cmatch:
-                    out.write(' _(%s) ' % cmatch.group(1))
+                    out.write(' _(%s) ' % cmatch[1])
                 for p in parts[1:]:
                     if p.find(':_(') >= 0:
                         out.write(' %s ' % p.split(':', 1)[1])
('django/contrib/syndication', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1 +0,0 @@
-default_app_config = 'django.contrib.syndication.apps.SyndicationConfig'
('django/contrib/syndication', 'views.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,5 @@
 from calendar import timegm

-from django.conf import settings
 from django.contrib.sites.shortcuts import get_current_site
 from django.core.exceptions import ImproperlyConfigured, ObjectDoesNotExist
 from django.http import Http404, HttpResponse
@@ -10,6 +9,7 @@
 from django.utils.html import escape
 from django.utils.http import http_date
 from django.utils.timezone import get_default_timezone, is_naive, make_aware
+from django.utils.translation import get_language


 def add_domain(domain, url, secure=False):
@@ -30,6 +30,7 @@
     feed_type = feedgenerator.DefaultFeed
     title_template = None
     description_template = None
+    language = None

     def __call__(self, request, *args, **kwargs):
         try:
@@ -41,7 +42,7 @@
         if hasattr(self, 'item_pubdate') or hasattr(self, 'item_updateddate'):
             # if item_pubdate or item_updateddate is defined for the feed, set
             # header so as ConditionalGetMiddleware is able to send 304 NOT MODIFIED
-            response['Last-Modified'] = http_date(
+            response.headers['Last-Modified'] = http_date(
                 timegm(feedgen.latest_post_date().utctimetuple()))
         feedgen.write(response, 'utf-8')
         return response
@@ -134,7 +135,7 @@
             subtitle=self._get_dynamic_attr('subtitle', obj),
             link=link,
             description=self._get_dynamic_attr('description', obj),
-            language=settings.LANGUAGE_CODE,
+            language=self.language or get_language(),
             feed_url=add_domain(
                 current_site.domain,
                 self._get_dynamic_attr('feed_url', obj) or request.path,
@@ -211,6 +212,7 @@
                 author_name=author_name,
                 author_email=author_email,
                 author_link=author_link,
+                comments=self._get_dynamic_attr('item_comments', item),
                 categories=self._get_dynamic_attr('item_categories', item),
                 item_copyright=self._get_dynamic_attr('item_copyright', item),
                 **self.item_extra_kwargs(item)
('django/contrib/messages', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,4 +1,2 @@
 from django.contrib.messages.api import *  # NOQA
 from django.contrib.messages.constants import *  # NOQA
-
-default_app_config = 'django.contrib.messages.apps.MessagesConfig'
('django/contrib/messages/storage', 'session.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,5 @@
 import json

-from django.conf import settings
 from django.contrib.messages.storage.base import BaseStorage
 from django.contrib.messages.storage.cookie import (
     MessageDecoder, MessageEncoder,
@@ -17,7 +16,7 @@
         assert hasattr(request, 'session'), "The session-based temporary "\
             "message storage requires session middleware to be installed, "\
             "and come before the message middleware in the "\
-            "MIDDLEWARE%s list." % ("_CLASSES" if settings.MIDDLEWARE is None else "")
+            "MIDDLEWARE list."
         super().__init__(request, *args, **kwargs)

     def _get(self, *args, **kwargs):
@@ -39,7 +38,7 @@
         return []

     def serialize_messages(self, messages):
-        encoder = MessageEncoder(separators=(',', ':'))
+        encoder = MessageEncoder()
         return encoder.encode(messages)

     def deserialize_messages(self, data):
('django/contrib/messages/storage', 'cookie.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,7 +1,9 @@
+import binascii
 import json

 from django.conf import settings
 from django.contrib.messages.storage.base import BaseStorage, Message
+from django.core import signing
 from django.http import SimpleCookie
 from django.utils.crypto import constant_time_compare, salted_hmac
 from django.utils.safestring import SafeData, mark_safe
@@ -32,9 +34,6 @@
     def process_messages(self, obj):
         if isinstance(obj, list) and obj:
             if obj[0] == MessageEncoder.message_key:
-                if len(obj) == 3:
-                    # Compatibility with previously-encoded messages
-                    return Message(*obj[1:])
                 if obj[1]:
                     obj[3] = mark_safe(obj[3])
                 return Message(*obj[2:])
@@ -49,6 +48,18 @@
         return self.process_messages(decoded)


+class MessageSerializer:
+    def dumps(self, obj):
+        return json.dumps(
+            obj,
+            separators=(',', ':'),
+            cls=MessageEncoder,
+        ).encode('latin-1')
+
+    def loads(self, data):
+        return json.loads(data.decode('latin-1'), cls=MessageDecoder)
+
+
 class CookieStorage(BaseStorage):
     """
     Store messages in a cookie.
@@ -59,6 +70,11 @@
     # restrict the session cookie to 1/2 of 4kb. See #18781.
     max_cookie_size = 2048
     not_finished = '__messagesnotfinished__'
+    key_salt = 'django.contrib.messages'
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.signer = signing.get_cookie_signer(salt=self.key_salt)

     def _get(self, *args, **kwargs):
         """
@@ -89,7 +105,11 @@
                 samesite=settings.SESSION_COOKIE_SAMESITE,
             )
         else:
-            response.delete_cookie(self.cookie_name, domain=settings.SESSION_COOKIE_DOMAIN)
+            response.delete_cookie(
+                self.cookie_name,
+                domain=settings.SESSION_COOKIE_DOMAIN,
+                samesite=settings.SESSION_COOKIE_SAMESITE,
+            )

     def _store(self, messages, response, remove_oldest=True, *args, **kwargs):
         """
@@ -120,11 +140,15 @@
         self._update_cookie(encoded_data, response)
         return unstored_messages

-    def _hash(self, value):
-        """
+    def _legacy_hash(self, value):
+        """
+        # RemovedInDjango40Warning: pre-Django 3.1 hashes will be invalid.
         Create an HMAC/SHA1 hash based on the value and the project setting's
         SECRET_KEY, modified to make it unique for the present purpose.
         """
+        # The class wide key salt is not reused here since older Django
+        # versions had it fixed and making it dynamic would break old hashes if
+        # self.key_salt is changed.
         key_salt = 'django.contrib.messages'
         return salted_hmac(key_salt, value).hexdigest()

@@ -137,9 +161,7 @@
         also contains a hash to ensure that the data was not tampered with.
         """
         if messages or encode_empty:
-            encoder = MessageEncoder(separators=(',', ':'))
-            value = encoder.encode(messages)
-            return '%s$%s' % (self._hash(value), value)
+            return self.signer.sign_object(messages, serializer=MessageSerializer, compress=True)

     def _decode(self, data):
         """
@@ -150,17 +172,36 @@
         """
         if not data:
             return None
-        bits = data.split('$', 1)
-        if len(bits) == 2:
-            hash, value = bits
-            if constant_time_compare(hash, self._hash(value)):
-                try:
-                    # If we get here (and the JSON decode works), everything is
-                    # good. In any other case, drop back and return None.
-                    return json.loads(value, cls=MessageDecoder)
-                except json.JSONDecodeError:
-                    pass
+        try:
+            return self.signer.unsign_object(data, serializer=MessageSerializer)
+        # RemovedInDjango41Warning: when the deprecation ends, replace with:
+        #
+        # except (signing.BadSignature, json.JSONDecodeError):
+        #     pass
+        except signing.BadSignature:
+            # RemovedInDjango40Warning: when the deprecation ends, replace
+            # with:
+            #   decoded = None.
+            decoded = self._legacy_decode(data)
+        except (binascii.Error, json.JSONDecodeError):
+            decoded = self.signer.unsign(data)
+
+        if decoded:
+            # RemovedInDjango41Warning.
+            try:
+                return json.loads(decoded, cls=MessageDecoder)
+            except json.JSONDecodeError:
+                pass
         # Mark the data as used (so it gets removed) since something was wrong
         # with the data.
         self.used = True
         return None
+
+    def _legacy_decode(self, data):
+        # RemovedInDjango40Warning: pre-Django 3.1 hashes will be invalid.
+        bits = data.split('$', 1)
+        if len(bits) == 2:
+            hash_, value = bits
+            if constant_time_compare(hash_, self._legacy_hash(value)):
+                return value
+        return None
('django/contrib/messages/storage', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -25,8 +25,9 @@
         self.extra_tags = str(self.extra_tags) if self.extra_tags is not None else None

     def __eq__(self, other):
-        return isinstance(other, Message) and self.level == other.level and \
-            self.message == other.message
+        if not isinstance(other, Message):
+            return NotImplemented
+        return self.level == other.level and self.message == other.message

     def __str__(self):
         return str(self.message)
('django/contrib/auth', 'signals.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,5 @@
 from django.dispatch import Signal

-user_logged_in = Signal(providing_args=['request', 'user'])
-user_login_failed = Signal(providing_args=['credentials', 'request'])
-user_logged_out = Signal(providing_args=['request', 'user'])
+user_logged_in = Signal()
+user_login_failed = Signal()
+user_logged_out = Signal()
('django/contrib/auth', 'mixins.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,7 +1,10 @@
+from urllib.parse import urlparse
+
 from django.conf import settings
 from django.contrib.auth import REDIRECT_FIELD_NAME
 from django.contrib.auth.views import redirect_to_login
 from django.core.exceptions import ImproperlyConfigured, PermissionDenied
+from django.shortcuts import resolve_url


 class AccessMixin:
@@ -41,7 +44,23 @@
     def handle_no_permission(self):
         if self.raise_exception or self.request.user.is_authenticated:
             raise PermissionDenied(self.get_permission_denied_message())
-        return redirect_to_login(self.request.get_full_path(), self.get_login_url(), self.get_redirect_field_name())
+
+        path = self.request.build_absolute_uri()
+        resolved_login_url = resolve_url(self.get_login_url())
+        # If the login url is the same scheme and net location then use the
+        # path as the "next" url.
+        login_scheme, login_netloc = urlparse(resolved_login_url)[:2]
+        current_scheme, current_netloc = urlparse(path)[:2]
+        if (
+            (not login_scheme or login_scheme == current_scheme) and
+            (not login_netloc or login_netloc == current_netloc)
+        ):
+            path = self.request.get_full_path()
+        return redirect_to_login(
+            path,
+            resolved_login_url,
+            self.get_redirect_field_name(),
+        )


 class LoginRequiredMixin(AccessMixin):
@@ -93,7 +112,7 @@

     def test_func(self):
         raise NotImplementedError(
-            '{0} is missing the implementation of the test_func() method.'.format(self.__class__.__name__)
+            '{} is missing the implementation of the test_func() method.'.format(self.__class__.__name__)
         )

     def get_test_func(self):
('django/contrib/auth', 'password_validation.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -115,6 +115,36 @@
         ) % {'min_length': self.min_length}


+def exceeds_maximum_length_ratio(password, max_similarity, value):
+    """
+    Test that value is within a reasonable range of password.
+
+    The following ratio calculations are based on testing SequenceMatcher like
+    this:
+
+    for i in range(0,6):
+      print(10**i, SequenceMatcher(a='A', b='A'*(10**i)).quick_ratio())
+
+    which yields:
+
+    1 1.0
+    10 0.18181818181818182
+    100 0.019801980198019802
+    1000 0.001998001998001998
+    10000 0.00019998000199980003
+    100000 1.999980000199998e-05
+
+    This means a length_ratio of 10 should never yield a similarity higher than
+    0.2, for 100 this is down to 0.02 and for 1000 it is 0.002. This can be
+    calculated via 2 / length_ratio. As a result we avoid the potentially
+    expensive sequence matching.
+    """
+    pwd_len = len(password)
+    length_bound_similarity = max_similarity / 2 * pwd_len
+    value_len = len(value)
+    return pwd_len >= 10 * value_len and value_len < length_bound_similarity
+
+
 class UserAttributeSimilarityValidator:
     """
     Validate whether the password is sufficiently different from the user's
@@ -130,19 +160,25 @@

     def __init__(self, user_attributes=DEFAULT_USER_ATTRIBUTES, max_similarity=0.7):
         self.user_attributes = user_attributes
+        if max_similarity < 0.1:
+            raise ValueError('max_similarity must be at least 0.1')
         self.max_similarity = max_similarity

     def validate(self, password, user=None):
         if not user:
             return

+        password = password.lower()
         for attribute_name in self.user_attributes:
             value = getattr(user, attribute_name, None)
             if not value or not isinstance(value, str):
                 continue
-            value_parts = re.split(r'\W+', value) + [value]
+            value_lower = value.lower()
+            value_parts = re.split(r'\W+', value_lower) + [value_lower]
             for value_part in value_parts:
-                if SequenceMatcher(a=password.lower(), b=value_part.lower()).quick_ratio() >= self.max_similarity:
+                if exceeds_maximum_length_ratio(password, self.max_similarity, value_part):
+                    continue
+                if SequenceMatcher(a=password, b=value_part).quick_ratio() >= self.max_similarity:
                     try:
                         verbose_name = str(user._meta.get_field(attribute_name).verbose_name)
                     except FieldDoesNotExist:
@@ -154,7 +190,7 @@
                     )

     def get_help_text(self):
-        return _("Your password can't be too similar to your other personal information.")
+        return _('Your password can’t be too similar to your other personal information.')


 class CommonPasswordValidator:
@@ -171,13 +207,11 @@

     def __init__(self, password_list_path=DEFAULT_PASSWORD_LIST_PATH):
         try:
-            with gzip.open(str(password_list_path)) as f:
-                common_passwords_lines = f.read().decode().splitlines()
-        except IOError:
-            with open(str(password_list_path)) as f:
-                common_passwords_lines = f.readlines()
-
-        self.passwords = {p.strip() for p in common_passwords_lines}
+            with gzip.open(password_list_path, 'rt', encoding='utf-8') as f:
+                self.passwords = {x.strip() for x in f}
+        except OSError:
+            with open(password_list_path) as f:
+                self.passwords = {x.strip() for x in f}

     def validate(self, password, user=None):
         if password.lower().strip() in self.passwords:
@@ -187,7 +221,7 @@
             )

     def get_help_text(self):
-        return _("Your password can't be a commonly used password.")
+        return _('Your password can’t be a commonly used password.')


 class NumericPasswordValidator:
@@ -202,4 +236,4 @@
             )

     def get_help_text(self):
-        return _("Your password can't be entirely numeric.")
+        return _('Your password can’t be entirely numeric.')
('django/contrib/auth', 'models.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,7 @@
+from django.apps import apps
 from django.contrib import auth
 from django.contrib.auth.base_user import AbstractBaseUser, BaseUserManager
+from django.contrib.auth.hashers import make_password
 from django.contrib.contenttypes.models import ContentType
 from django.core.exceptions import PermissionDenied
 from django.core.mail import send_mail
@@ -66,16 +68,11 @@
     class Meta:
         verbose_name = _('permission')
         verbose_name_plural = _('permissions')
-        unique_together = (('content_type', 'codename'),)
-        ordering = ('content_type__app_label', 'content_type__model',
-                    'codename')
+        unique_together = [['content_type', 'codename']]
+        ordering = ['content_type__app_label', 'content_type__model', 'codename']

     def __str__(self):
-        return "%s | %s | %s" % (
-            self.content_type.app_label,
-            self.content_type,
-            self.name,
-        )
+        return '%s | %s' % (self.content_type, self.name)

     def natural_key(self):
         return (self.codename,) + self.content_type.natural_key()
@@ -139,9 +136,13 @@
         if not username:
             raise ValueError('The given username must be set')
         email = self.normalize_email(email)
-        username = self.model.normalize_username(username)
+        # Lookup the real model class from the global app registry so this
+        # manager method can be used in migrations. This is fine because
+        # managers are by definition working on the real model.
+        GlobalUserModel = apps.get_model(self.model._meta.app_label, self.model._meta.object_name)
+        username = GlobalUserModel.normalize_username(username)
         user = self.model(username=username, email=email, **extra_fields)
-        user.set_password(password)
+        user.password = make_password(password)
         user.save(using=self._db)
         return user

@@ -150,7 +151,7 @@
         extra_fields.setdefault('is_superuser', False)
         return self._create_user(username, email, password, **extra_fields)

-    def create_superuser(self, username, email, password, **extra_fields):
+    def create_superuser(self, username, email=None, password=None, **extra_fields):
         extra_fields.setdefault('is_staff', True)
         extra_fields.setdefault('is_superuser', True)

@@ -161,13 +162,40 @@

         return self._create_user(username, email, password, **extra_fields)

+    def with_perm(self, perm, is_active=True, include_superusers=True, backend=None, obj=None):
+        if backend is None:
+            backends = auth._get_backends(return_tuples=True)
+            if len(backends) == 1:
+                backend, _ = backends[0]
+            else:
+                raise ValueError(
+                    'You have multiple authentication backends configured and '
+                    'therefore must provide the `backend` argument.'
+                )
+        elif not isinstance(backend, str):
+            raise TypeError(
+                'backend must be a dotted import path string (got %r).'
+                % backend
+            )
+        else:
+            backend = auth.load_backend(backend)
+        if hasattr(backend, 'with_perm'):
+            return backend.with_perm(
+                perm,
+                is_active=is_active,
+                include_superusers=include_superusers,
+                obj=obj,
+            )
+        return self.none()
+

 # A few helper functions for common logic between User and AnonymousUser.
-def _user_get_all_permissions(user, obj):
+def _user_get_permissions(user, obj, from_name):
     permissions = set()
+    name = 'get_%s_permissions' % from_name
     for backend in auth.get_backends():
-        if hasattr(backend, "get_all_permissions"):
-            permissions.update(backend.get_all_permissions(user, obj))
+        if hasattr(backend, name):
+            permissions.update(getattr(backend, name)(user, obj))
     return permissions


@@ -237,20 +265,24 @@
     class Meta:
         abstract = True

+    def get_user_permissions(self, obj=None):
+        """
+        Return a list of permission strings that this user has directly.
+        Query all available auth backends. If an object is passed in,
+        return only permissions matching this object.
+        """
+        return _user_get_permissions(self, obj, 'user')
+
     def get_group_permissions(self, obj=None):
         """
         Return a list of permission strings that this user has through their
         groups. Query all available auth backends. If an object is passed in,
         return only permissions matching this object.
         """
-        permissions = set()
-        for backend in auth.get_backends():
-            if hasattr(backend, "get_group_permissions"):
-                permissions.update(backend.get_group_permissions(self, obj))
-        return permissions
+        return _user_get_permissions(self, obj, 'group')

     def get_all_permissions(self, obj=None):
-        return _user_get_all_permissions(self, obj)
+        return _user_get_permissions(self, obj, 'all')

     def has_perm(self, perm, obj=None):
         """
@@ -305,7 +337,7 @@
             'unique': _("A user with that username already exists."),
         },
     )
-    first_name = models.CharField(_('first name'), max_length=30, blank=True)
+    first_name = models.CharField(_('first name'), max_length=150, blank=True)
     last_name = models.CharField(_('last name'), max_length=150, blank=True)
     email = models.EmailField(_('email address'), blank=True)
     is_staff = models.BooleanField(
@@ -407,11 +439,14 @@
     def user_permissions(self):
         return self._user_permissions

+    def get_user_permissions(self, obj=None):
+        return _user_get_permissions(self, obj, 'user')
+
     def get_group_permissions(self, obj=None):
         return set()

     def get_all_permissions(self, obj=None):
-        return _user_get_all_permissions(self, obj=obj)
+        return _user_get_permissions(self, obj, 'all')

     def has_perm(self, perm, obj=None):
         return _user_has_perm(self, perm, obj=obj)
('django/contrib/auth', 'validators.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -7,7 +7,7 @@

 @deconstructible
 class ASCIIUsernameValidator(validators.RegexValidator):
-    regex = r'^[\w.@+-]+$'
+    regex = r'^[\w.@+-]+\Z'
     message = _(
         'Enter a valid username. This value may contain only English letters, '
         'numbers, and @/./+/-/_ characters.'
@@ -17,7 +17,7 @@

 @deconstructible
 class UnicodeUsernameValidator(validators.RegexValidator):
-    regex = r'^[\w.@+-]+$'
+    regex = r'^[\w.@+-]+\Z'
     message = _(
         'Enter a valid username. This value may contain only letters, '
         'numbers, and @/./+/-/_ characters.'
('django/contrib/auth', 'checks.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -41,13 +41,21 @@
             checks.Error(
                 "The field named as the 'USERNAME_FIELD' "
                 "for a custom user model must not be included in 'REQUIRED_FIELDS'.",
+                hint=(
+                    "The 'USERNAME_FIELD' is currently set to '%s', you "
+                    "should remove '%s' from the 'REQUIRED_FIELDS'."
+                    % (cls.USERNAME_FIELD, cls.USERNAME_FIELD)
+                ),
                 obj=cls,
                 id='auth.E002',
             )
         )

     # Check that the username field is unique
-    if not cls._meta.get_field(cls.USERNAME_FIELD).unique:
+    if not cls._meta.get_field(cls.USERNAME_FIELD).unique and not any(
+        constraint.fields == (cls.USERNAME_FIELD,)
+        for constraint in cls._meta.total_unique_constraints
+    ):
         if (settings.AUTHENTICATION_BACKENDS ==
                 ['django.contrib.auth.backends.ModelBackend']):
             errors.append(
@@ -102,6 +110,7 @@

     Permission = apps.get_model('auth', 'Permission')
     permission_name_max_length = Permission._meta.get_field('name').max_length
+    permission_codename_max_length = Permission._meta.get_field('codename').max_length
     errors = []

     for model in models:
@@ -118,12 +127,35 @@
             )
             errors.append(
                 checks.Error(
-                    "The verbose_name of model '%s.%s' must be at most %d characters "
-                    "for its builtin permission names to be at most %d characters." % (
-                        opts.app_label, opts.object_name, verbose_name_max_length, permission_name_max_length
+                    "The verbose_name of model '%s' must be at most %d "
+                    "characters for its builtin permission names to be at "
+                    "most %d characters." % (
+                        opts.label, verbose_name_max_length, permission_name_max_length
                     ),
                     obj=model,
                     id='auth.E007',
+                )
+            )
+        # Check builtin permission codename length.
+        max_builtin_permission_codename_length = (
+            max(len(codename) for codename in builtin_permissions.keys())
+            if builtin_permissions else 0
+        )
+        if max_builtin_permission_codename_length > permission_codename_max_length:
+            model_name_max_length = permission_codename_max_length - (
+                max_builtin_permission_codename_length - len(opts.model_name)
+            )
+            errors.append(
+                checks.Error(
+                    "The name of model '%s' must be at most %d characters "
+                    "for its builtin permission codenames to be at most %d "
+                    "characters." % (
+                        opts.label,
+                        model_name_max_length,
+                        permission_codename_max_length,
+                    ),
+                    obj=model,
+                    id='auth.E011',
                 )
             )
         codenames = set()
@@ -132,11 +164,26 @@
             if len(name) > permission_name_max_length:
                 errors.append(
                     checks.Error(
-                        "The permission named '%s' of model '%s.%s' is longer than %d characters." % (
-                            name, opts.app_label, opts.object_name, permission_name_max_length
+                        "The permission named '%s' of model '%s' is longer "
+                        "than %d characters." % (
+                            name, opts.label, permission_name_max_length,
                         ),
                         obj=model,
                         id='auth.E008',
+                    )
+                )
+            # Check custom permission codename length.
+            if len(codename) > permission_codename_max_length:
+                errors.append(
+                    checks.Error(
+                        "The permission codenamed '%s' of model '%s' is "
+                        "longer than %d characters." % (
+                            codename,
+                            opts.label,
+                            permission_codename_max_length,
+                        ),
+                        obj=model,
+                        id='auth.E012',
                     )
                 )
             # Check custom permissions codename clashing.
@@ -144,9 +191,7 @@
                 errors.append(
                     checks.Error(
                         "The permission codenamed '%s' clashes with a builtin permission "
-                        "for model '%s.%s'." % (
-                            codename, opts.app_label, opts.object_name
-                        ),
+                        "for model '%s'." % (codename, opts.label),
                         obj=model,
                         id='auth.E005',
                     )
@@ -154,9 +199,8 @@
             elif codename in codenames:
                 errors.append(
                     checks.Error(
-                        "The permission codenamed '%s' is duplicated for model '%s.%s'." % (
-                            codename, opts.app_label, opts.object_name
-                        ),
+                        "The permission codenamed '%s' is duplicated for "
+                        "model '%s'." % (codename, opts.label),
                         obj=model,
                         id='auth.E006',
                     )
('django/contrib/auth', 'base_user.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,6 +4,7 @@
 """
 import unicodedata

+from django.conf import settings
 from django.contrib.auth import password_validation
 from django.contrib.auth.hashers import (
     check_password, is_password_usable, make_password,
@@ -120,12 +121,24 @@
         """
         return is_password_usable(self.password)

+    def _legacy_get_session_auth_hash(self):
+        # RemovedInDjango40Warning: pre-Django 3.1 hashes will be invalid.
+        key_salt = 'django.contrib.auth.models.AbstractBaseUser.get_session_auth_hash'
+        return salted_hmac(key_salt, self.password, algorithm='sha1').hexdigest()
+
     def get_session_auth_hash(self):
         """
         Return an HMAC of the password field.
         """
         key_salt = "django.contrib.auth.models.AbstractBaseUser.get_session_auth_hash"
-        return salted_hmac(key_salt, self.password).hexdigest()
+        return salted_hmac(
+            key_salt,
+            self.password,
+            # RemovedInDjango40Warning: when the deprecation ends, replace
+            # with:
+            # algorithm='sha256',
+            algorithm=settings.DEFAULT_HASHING_ALGORITHM,
+        ).hexdigest()

     @classmethod
     def get_email_field_name(cls):
('django/contrib/auth', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -7,7 +7,7 @@
 from django.middleware.csrf import rotate_token
 from django.utils.crypto import constant_time_compare
 from django.utils.module_loading import import_string
-from django.utils.translation import LANGUAGE_SESSION_KEY
+from django.views.decorators.debug import sensitive_variables

 from .signals import user_logged_in, user_logged_out, user_login_failed

@@ -38,6 +38,7 @@
     return _get_backends(return_tuples=False)


+@sensitive_variables('credentials')
 def _clean_credentials(credentials):
     """
     Clean a dictionary of credentials of potentially sensitive info before
@@ -59,13 +60,15 @@
     return get_user_model()._meta.pk.to_python(request.session[SESSION_KEY])


+@sensitive_variables('credentials')
 def authenticate(request=None, **credentials):
     """
     If the given credentials are valid, return a User object.
     """
     for backend, backend_path in _get_backends(return_tuples=True):
+        backend_signature = inspect.signature(backend.authenticate)
         try:
-            inspect.getcallargs(backend.authenticate, request, **credentials)
+            backend_signature.bind(request, **credentials)
         except TypeError:
             # This backend doesn't accept these credentials as arguments. Try the next one.
             continue
@@ -143,15 +146,7 @@
     if not getattr(user, 'is_authenticated', True):
         user = None
     user_logged_out.send(sender=user.__class__, request=request, user=user)
-
-    # remember language choice saved to session
-    language = request.session.get(LANGUAGE_SESSION_KEY)
-
     request.session.flush()
-
-    if language is not None:
-        request.session[LANGUAGE_SESSION_KEY] = language
-
     if hasattr(request, 'user'):
         from django.contrib.auth.models import AnonymousUser
         request.user = AnonymousUser()
@@ -195,8 +190,13 @@
                     user.get_session_auth_hash()
                 )
                 if not session_hash_verified:
-                    request.session.flush()
-                    user = None
+                    if not (
+                        session_hash and
+                        hasattr(user, '_legacy_get_session_auth_hash') and
+                        constant_time_compare(session_hash, user._legacy_get_session_auth_hash())
+                    ):
+                        request.session.flush()
+                        user = None

     return user or AnonymousUser()

@@ -220,6 +220,3 @@
     request.session.cycle_key()
     if hasattr(user, 'get_session_auth_hash') and request.user == user:
         request.session[HASH_SESSION_KEY] = user.get_session_auth_hash()
-
-
-default_app_config = 'django.contrib.auth.apps.AuthConfig'
('django/contrib/auth', 'tokens.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,4 +1,4 @@
-from datetime import date
+from datetime import datetime, time

 from django.conf import settings
 from django.utils.crypto import constant_time_compare, salted_hmac
@@ -11,14 +11,21 @@
     reset mechanism.
     """
     key_salt = "django.contrib.auth.tokens.PasswordResetTokenGenerator"
-    secret = settings.SECRET_KEY
+    algorithm = None
+    secret = None
+
+    def __init__(self):
+        self.secret = self.secret or settings.SECRET_KEY
+        # RemovedInDjango40Warning: when the deprecation ends, replace with:
+        # self.algorithm = self.algorithm or 'sha256'
+        self.algorithm = self.algorithm or settings.DEFAULT_HASHING_ALGORITHM

     def make_token(self, user):
         """
         Return a token that can be used once to do a password reset
         for the given user.
         """
-        return self._make_token_with_timestamp(user, self._num_days(self._today()))
+        return self._make_token_with_timestamp(user, self._num_seconds(self._now()))

     def check_token(self, user, token):
         """
@@ -29,6 +36,8 @@
         # Parse the token
         try:
             ts_b36, _ = token.split("-")
+            # RemovedInDjango40Warning.
+            legacy_token = len(ts_b36) < 4
         except ValueError:
             return False

@@ -39,39 +48,52 @@

         # Check that the timestamp/uid has not been tampered with
         if not constant_time_compare(self._make_token_with_timestamp(user, ts), token):
-            return False
+            # RemovedInDjango40Warning: when the deprecation ends, replace
+            # with:
+            #   return False
+            if not constant_time_compare(
+                self._make_token_with_timestamp(user, ts, legacy=True),
+                token,
+            ):
+                return False

-        # Check the timestamp is within limit. Timestamps are rounded to
-        # midnight (server time) providing a resolution of only 1 day. If a
-        # link is generated 5 minutes before midnight and used 6 minutes later,
-        # that counts as 1 day. Therefore, PASSWORD_RESET_TIMEOUT_DAYS = 1 means
-        # "at least 1 day, could be up to 2."
-        if (self._num_days(self._today()) - ts) > settings.PASSWORD_RESET_TIMEOUT_DAYS:
+        # RemovedInDjango40Warning: convert days to seconds and round to
+        # midnight (server time) for pre-Django 3.1 tokens.
+        now = self._now()
+        if legacy_token:
+            ts *= 24 * 60 * 60
+            ts += int((now - datetime.combine(now.date(), time.min)).total_seconds())
+        # Check the timestamp is within limit.
+        if (self._num_seconds(now) - ts) > settings.PASSWORD_RESET_TIMEOUT:
             return False

         return True

-    def _make_token_with_timestamp(self, user, timestamp):
-        # timestamp is number of days since 2001-1-1.  Converted to
-        # base 36, this gives us a 3 digit string until about 2121
+    def _make_token_with_timestamp(self, user, timestamp, legacy=False):
+        # timestamp is number of seconds since 2001-1-1. Converted to base 36,
+        # this gives us a 6 digit string until about 2069.
         ts_b36 = int_to_base36(timestamp)
         hash_string = salted_hmac(
             self.key_salt,
             self._make_hash_value(user, timestamp),
             secret=self.secret,
-        ).hexdigest()[::2]  # Limit to 20 characters to shorten the URL.
+            # RemovedInDjango40Warning: when the deprecation ends, remove the
+            # legacy argument and replace with:
+            #   algorithm=self.algorithm,
+            algorithm='sha1' if legacy else self.algorithm,
+        ).hexdigest()[::2]  # Limit to shorten the URL.
         return "%s-%s" % (ts_b36, hash_string)

     def _make_hash_value(self, user, timestamp):
         """
-        Hash the user's primary key and some user state that's sure to change
-        after a password reset to produce a token that invalidated when it's
-        used:
+        Hash the user's primary key, email (if available), and some user state
+        that's sure to change after a password reset to produce a token that is
+        invalidated when it's used:
         1. The password field will change upon a password reset (even if the
            same password is chosen, due to password salting).
         2. The last_login field will usually be updated very shortly after
            a password reset.
-        Failing those things, settings.PASSWORD_RESET_TIMEOUT_DAYS eventually
+        Failing those things, settings.PASSWORD_RESET_TIMEOUT eventually
         invalidates the token.

         Running this data through salted_hmac() prevents password cracking
@@ -80,14 +102,16 @@
         # Truncate microseconds so that tokens are consistent even if the
         # database doesn't support microseconds.
         login_timestamp = '' if user.last_login is None else user.last_login.replace(microsecond=0, tzinfo=None)
-        return str(user.pk) + user.password + str(login_timestamp) + str(timestamp)
+        email_field = user.get_email_field_name()
+        email = getattr(user, email_field, '') or ''
+        return f'{user.pk}{user.password}{login_timestamp}{timestamp}{email}'

-    def _num_days(self, dt):
-        return (dt - date(2001, 1, 1)).days
+    def _num_seconds(self, dt):
+        return int((dt - datetime(2001, 1, 1)).total_seconds())

-    def _today(self):
+    def _now(self):
         # Used for mocking in tests
-        return date.today()
+        return datetime.now()


 default_token_generator = PasswordResetTokenGenerator()
('django/contrib/auth', 'apps.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -11,6 +11,7 @@


 class AuthConfig(AppConfig):
+    default_auto_field = 'django.db.models.AutoField'
     name = 'django.contrib.auth'
     verbose_name = _("Authentication and Authorization")

('django/contrib/auth', 'forms.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -10,6 +10,7 @@
 from django.contrib.auth.models import User
 from django.contrib.auth.tokens import default_token_generator
 from django.contrib.sites.shortcuts import get_current_site
+from django.core.exceptions import ValidationError
 from django.core.mail import EmailMultiAlternatives
 from django.template import loader
 from django.utils.encoding import force_bytes
@@ -18,6 +19,15 @@
 from django.utils.translation import gettext, gettext_lazy as _

 UserModel = get_user_model()
+
+
+def _unicode_ci_compare(s1, s2):
+    """
+    Perform case-insensitive comparison of two identifiers, using the
+    recommended algorithm from Unicode Technical Report 36, section
+    2.11.2(B)(2).
+    """
+    return unicodedata.normalize('NFKC', s1).casefold() == unicodedata.normalize('NFKC', s2).casefold()


 class ReadOnlyPasswordHashWidget(forms.Widget):
@@ -46,21 +56,21 @@

     def __init__(self, *args, **kwargs):
         kwargs.setdefault("required", False)
-        super().__init__(*args, **kwargs)
-
-    def bound_data(self, data, initial):
-        # Always return initial because the widget doesn't
-        # render an input field.
-        return initial
-
-    def has_changed(self, initial, data):
-        return False
+        kwargs.setdefault('disabled', True)
+        super().__init__(*args, **kwargs)


 class UsernameField(forms.CharField):
     def to_python(self, value):
         return unicodedata.normalize('NFKC', super().to_python(value))

+    def widget_attrs(self, widget):
+        return {
+            **super().widget_attrs(widget),
+            'autocapitalize': 'none',
+            'autocomplete': 'username',
+        }
+

 class UserCreationForm(forms.ModelForm):
     """
@@ -68,17 +78,17 @@
     password.
     """
     error_messages = {
-        'password_mismatch': _("The two password fields didn't match."),
+        'password_mismatch': _('The two password fields didn’t match.'),
     }
     password1 = forms.CharField(
         label=_("Password"),
         strip=False,
-        widget=forms.PasswordInput,
+        widget=forms.PasswordInput(attrs={'autocomplete': 'new-password'}),
         help_text=password_validation.password_validators_help_text_html(),
     )
     password2 = forms.CharField(
         label=_("Password confirmation"),
-        widget=forms.PasswordInput,
+        widget=forms.PasswordInput(attrs={'autocomplete': 'new-password'}),
         strip=False,
         help_text=_("Enter the same password as before, for verification."),
     )
@@ -91,13 +101,13 @@
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         if self._meta.model.USERNAME_FIELD in self.fields:
-            self.fields[self._meta.model.USERNAME_FIELD].widget.attrs.update({'autofocus': True})
+            self.fields[self._meta.model.USERNAME_FIELD].widget.attrs['autofocus'] = True

     def clean_password2(self):
         password1 = self.cleaned_data.get("password1")
         password2 = self.cleaned_data.get("password2")
         if password1 and password2 and password1 != password2:
-            raise forms.ValidationError(
+            raise ValidationError(
                 self.error_messages['password_mismatch'],
                 code='password_mismatch',
             )
@@ -111,7 +121,7 @@
         if password:
             try:
                 password_validation.validate_password(password, self.instance)
-            except forms.ValidationError as error:
+            except ValidationError as error:
                 self.add_error('password2', error)

     def save(self, commit=True):
@@ -126,9 +136,9 @@
     password = ReadOnlyPasswordHashField(
         label=_("Password"),
         help_text=_(
-            "Raw passwords are not stored, so there is no way to see this "
-            "user's password, but you can change the password using "
-            "<a href=\"{}\">this form</a>."
+            'Raw passwords are not stored, so there is no way to see this '
+            'user’s password, but you can change the password using '
+            '<a href="{}">this form</a>.'
         ),
     )

@@ -146,12 +156,6 @@
         if user_permissions:
             user_permissions.queryset = user_permissions.queryset.select_related('content_type')

-    def clean_password(self):
-        # Regardless of what the user provides, return the initial value.
-        # This is done here, rather than on the field, because the
-        # field does not have access to the initial value
-        return self.initial.get('password')
-

 class AuthenticationForm(forms.Form):
     """
@@ -162,7 +166,7 @@
     password = forms.CharField(
         label=_("Password"),
         strip=False,
-        widget=forms.PasswordInput,
+        widget=forms.PasswordInput(attrs={'autocomplete': 'current-password'}),
     )

     error_messages = {
@@ -184,7 +188,9 @@

         # Set the max length and label for the "username" field.
         self.username_field = UserModel._meta.get_field(UserModel.USERNAME_FIELD)
-        self.fields['username'].max_length = self.username_field.max_length or 254
+        username_max_length = self.username_field.max_length or 254
+        self.fields['username'].max_length = username_max_length
+        self.fields['username'].widget.attrs['maxlength'] = username_max_length
         if self.fields['username'].label is None:
             self.fields['username'].label = capfirst(self.username_field.verbose_name)

@@ -208,12 +214,12 @@
         allow login by active users, and reject login by inactive users.

         If the given user cannot log in, this method should raise a
-        ``forms.ValidationError``.
+        ``ValidationError``.

         If the given user may log in, this method should return None.
         """
         if not user.is_active:
-            raise forms.ValidationError(
+            raise ValidationError(
                 self.error_messages['inactive'],
                 code='inactive',
             )
@@ -222,7 +228,7 @@
         return self.user_cache

     def get_invalid_login_error(self):
-        return forms.ValidationError(
+        return ValidationError(
             self.error_messages['invalid_login'],
             code='invalid_login',
             params={'username': self.username_field.verbose_name},
@@ -230,7 +236,11 @@


 class PasswordResetForm(forms.Form):
-    email = forms.EmailField(label=_("Email"), max_length=254)
+    email = forms.EmailField(
+        label=_("Email"),
+        max_length=254,
+        widget=forms.EmailInput(attrs={'autocomplete': 'email'})
+    )

     def send_mail(self, subject_template_name, email_template_name,
                   context, from_email, to_email, html_email_template_name=None):
@@ -256,11 +266,16 @@
         that prevent inactive users and users with unusable passwords from
         resetting their password.
         """
+        email_field_name = UserModel.get_email_field_name()
         active_users = UserModel._default_manager.filter(**{
-            '%s__iexact' % UserModel.get_email_field_name(): email,
+            '%s__iexact' % email_field_name: email,
             'is_active': True,
         })
-        return (u for u in active_users if u.has_usable_password())
+        return (
+            u for u in active_users
+            if u.has_usable_password() and
+            _unicode_ci_compare(email, getattr(u, email_field_name))
+        )

     def save(self, domain_override=None,
              subject_template_name='registration/password_reset_subject.txt',
@@ -273,15 +288,17 @@
         user.
         """
         email = self.cleaned_data["email"]
+        if not domain_override:
+            current_site = get_current_site(request)
+            site_name = current_site.name
+            domain = current_site.domain
+        else:
+            site_name = domain = domain_override
+        email_field_name = UserModel.get_email_field_name()
         for user in self.get_users(email):
-            if not domain_override:
-                current_site = get_current_site(request)
-                site_name = current_site.name
-                domain = current_site.domain
-            else:
-                site_name = domain = domain_override
+            user_email = getattr(user, email_field_name)
             context = {
-                'email': email,
+                'email': user_email,
                 'domain': domain,
                 'site_name': site_name,
                 'uid': urlsafe_base64_encode(force_bytes(user.pk)),
@@ -292,7 +309,7 @@
             }
             self.send_mail(
                 subject_template_name, email_template_name, context, from_email,
-                email, html_email_template_name=html_email_template_name,
+                user_email, html_email_template_name=html_email_template_name,
             )


@@ -302,18 +319,18 @@
     password
     """
     error_messages = {
-        'password_mismatch': _("The two password fields didn't match."),
+        'password_mismatch': _('The two password fields didn’t match.'),
     }
     new_password1 = forms.CharField(
         label=_("New password"),
-        widget=forms.PasswordInput,
+        widget=forms.PasswordInput(attrs={'autocomplete': 'new-password'}),
         strip=False,
         help_text=password_validation.password_validators_help_text_html(),
     )
     new_password2 = forms.CharField(
         label=_("New password confirmation"),
         strip=False,
-        widget=forms.PasswordInput,
+        widget=forms.PasswordInput(attrs={'autocomplete': 'new-password'}),
     )

     def __init__(self, user, *args, **kwargs):
@@ -325,7 +342,7 @@
         password2 = self.cleaned_data.get('new_password2')
         if password1 and password2:
             if password1 != password2:
-                raise forms.ValidationError(
+                raise ValidationError(
                     self.error_messages['password_mismatch'],
                     code='password_mismatch',
                 )
@@ -352,7 +369,7 @@
     old_password = forms.CharField(
         label=_("Old password"),
         strip=False,
-        widget=forms.PasswordInput(attrs={'autofocus': True}),
+        widget=forms.PasswordInput(attrs={'autocomplete': 'current-password', 'autofocus': True}),
     )

     field_order = ['old_password', 'new_password1', 'new_password2']
@@ -363,7 +380,7 @@
         """
         old_password = self.cleaned_data["old_password"]
         if not self.user.check_password(old_password):
-            raise forms.ValidationError(
+            raise ValidationError(
                 self.error_messages['password_incorrect'],
                 code='password_incorrect',
             )
@@ -375,18 +392,18 @@
     A form used to change the password of a user in the admin interface.
     """
     error_messages = {
-        'password_mismatch': _("The two password fields didn't match."),
+        'password_mismatch': _('The two password fields didn’t match.'),
     }
     required_css_class = 'required'
     password1 = forms.CharField(
         label=_("Password"),
-        widget=forms.PasswordInput(attrs={'autofocus': True}),
+        widget=forms.PasswordInput(attrs={'autocomplete': 'new-password', 'autofocus': True}),
         strip=False,
         help_text=password_validation.password_validators_help_text_html(),
     )
     password2 = forms.CharField(
         label=_("Password (again)"),
-        widget=forms.PasswordInput,
+        widget=forms.PasswordInput(attrs={'autocomplete': 'new-password'}),
         strip=False,
         help_text=_("Enter the same password as before, for verification."),
     )
@@ -398,12 +415,11 @@
     def clean_password2(self):
         password1 = self.cleaned_data.get('password1')
         password2 = self.cleaned_data.get('password2')
-        if password1 and password2:
-            if password1 != password2:
-                raise forms.ValidationError(
-                    self.error_messages['password_mismatch'],
-                    code='password_mismatch',
-                )
+        if password1 and password2 and password1 != password2:
+            raise ValidationError(
+                self.error_messages['password_mismatch'],
+                code='password_mismatch',
+            )
         password_validation.validate_password(password2, self.user)
         return password2

('django/contrib/auth', 'backends.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,14 +1,34 @@
-import inspect
-import warnings
-
 from django.contrib.auth import get_user_model
 from django.contrib.auth.models import Permission
-from django.utils.deprecation import RemovedInDjango31Warning
+from django.db.models import Exists, OuterRef, Q

 UserModel = get_user_model()


-class ModelBackend:
+class BaseBackend:
+    def authenticate(self, request, **kwargs):
+        return None
+
+    def get_user(self, user_id):
+        return None
+
+    def get_user_permissions(self, user_obj, obj=None):
+        return set()
+
+    def get_group_permissions(self, user_obj, obj=None):
+        return set()
+
+    def get_all_permissions(self, user_obj, obj=None):
+        return {
+            *self.get_user_permissions(user_obj, obj=obj),
+            *self.get_group_permissions(user_obj, obj=obj),
+        }
+
+    def has_perm(self, user_obj, perm, obj=None):
+        return perm in self.get_all_permissions(user_obj, obj=obj)
+
+
+class ModelBackend(BaseBackend):
     """
     Authenticates against settings.AUTH_USER_MODEL.
     """
@@ -16,6 +36,8 @@
     def authenticate(self, request, username=None, password=None, **kwargs):
         if username is None:
             username = kwargs.get(UserModel.USERNAME_FIELD)
+        if username is None or password is None:
+            return
         try:
             user = UserModel._default_manager.get_by_natural_key(username)
         except UserModel.DoesNotExist:
@@ -79,14 +101,11 @@
         if not user_obj.is_active or user_obj.is_anonymous or obj is not None:
             return set()
         if not hasattr(user_obj, '_perm_cache'):
-            user_obj._perm_cache = {
-                *self.get_user_permissions(user_obj),
-                *self.get_group_permissions(user_obj),
-            }
+            user_obj._perm_cache = super().get_all_permissions(user_obj)
         return user_obj._perm_cache

     def has_perm(self, user_obj, perm, obj=None):
-        return user_obj.is_active and perm in self.get_all_permissions(user_obj, obj)
+        return user_obj.is_active and super().has_perm(user_obj, perm, obj=obj)

     def has_module_perms(self, user_obj, app_label):
         """
@@ -96,6 +115,42 @@
             perm[:perm.index('.')] == app_label
             for perm in self.get_all_permissions(user_obj)
         )
+
+    def with_perm(self, perm, is_active=True, include_superusers=True, obj=None):
+        """
+        Return users that have permission "perm". By default, filter out
+        inactive users and include superusers.
+        """
+        if isinstance(perm, str):
+            try:
+                app_label, codename = perm.split('.')
+            except ValueError:
+                raise ValueError(
+                    'Permission name should be in the form '
+                    'app_label.permission_codename.'
+                )
+        elif not isinstance(perm, Permission):
+            raise TypeError(
+                'The `perm` argument must be a string or a permission instance.'
+            )
+
+        UserModel = get_user_model()
+        if obj is not None:
+            return UserModel._default_manager.none()
+
+        permission_q = Q(group__user=OuterRef('pk')) | Q(user=OuterRef('pk'))
+        if isinstance(perm, Permission):
+            permission_q &= Q(pk=perm.pk)
+        else:
+            permission_q &= Q(codename=codename, content_type__app_label=app_label)
+
+        user_q = Exists(Permission.objects.filter(permission_q))
+        if include_superusers:
+            user_q |= Q(is_superuser=True)
+        if is_active is not None:
+            user_q &= Q(is_active=is_active)
+
+        return UserModel._default_manager.filter(user_q)

     def get_user(self, user_id):
         try:
@@ -147,17 +202,7 @@
                 UserModel.USERNAME_FIELD: username
             })
             if created:
-                args = (request, user)
-                try:
-                    inspect.getcallargs(self.configure_user, request, user)
-                except TypeError:
-                    args = (user,)
-                    warnings.warn(
-                        'Update %s.configure_user() to accept `request` as '
-                        'the first argument.'
-                        % self.__class__.__name__, RemovedInDjango31Warning
-                    )
-                user = self.configure_user(*args)
+                user = self.configure_user(request, user)
         else:
             try:
                 user = UserModel._default_manager.get_by_natural_key(username)
('django/contrib/auth', 'hashers.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,15 +3,15 @@
 import functools
 import hashlib
 import importlib
+import math
 import warnings
-from collections import OrderedDict

 from django.conf import settings
 from django.core.exceptions import ImproperlyConfigured
 from django.core.signals import setting_changed
 from django.dispatch import receiver
 from django.utils.crypto import (
-    constant_time_compare, get_random_string, pbkdf2,
+    RANDOM_STRING_CHARS, constant_time_compare, get_random_string, pbkdf2,
 )
 from django.utils.module_loading import import_string
 from django.utils.translation import gettext_noop as _
@@ -73,6 +73,11 @@
     """
     if password is None:
         return UNUSABLE_PASSWORD_PREFIX + get_random_string(UNUSABLE_PASSWORD_SUFFIX_LENGTH)
+    if not isinstance(password, (bytes, str)):
+        raise TypeError(
+            'Password must be a string or bytes, got %s.'
+            % type(password).__qualname__
+        )
     hasher = get_hasher(hasher)
     salt = salt or hasher.salt()
     return hasher.encode(password, salt)
@@ -157,6 +162,11 @@
     return masked


+def must_update_salt(salt, expected_entropy):
+    # Each character in the salt provides log_2(len(alphabet)) bits of entropy.
+    return len(salt) * math.log2(len(RANDOM_STRING_CHARS)) < expected_entropy
+
+
 class BasePasswordHasher:
     """
     Abstract base class for password hashers
@@ -168,6 +178,7 @@
     """
     algorithm = None
     library = None
+    salt_entropy = 128

     def _load_library(self):
         if self.library is not None:
@@ -185,8 +196,14 @@
                          self.__class__.__name__)

     def salt(self):
-        """Generate a cryptographically secure nonce salt in ASCII."""
-        return get_random_string()
+        """
+        Generate a cryptographically secure nonce salt in ASCII with an entropy
+        of at least `salt_entropy` bits.
+        """
+        # Each character in the salt provides
+        # log_2(len(alphabet)) bits of entropy.
+        char_count = math.ceil(self.salt_entropy / math.log2(len(RANDOM_STRING_CHARS)))
+        return get_random_string(char_count, allowed_chars=RANDOM_STRING_CHARS)

     def verify(self, password, encoded):
         """Check if the given password is correct."""
@@ -200,6 +217,18 @@
         must be fewer than 128 characters.
         """
         raise NotImplementedError('subclasses of BasePasswordHasher must provide an encode() method')
+
+    def decode(self, encoded):
+        """
+        Return a decoded database value.
+
+        The result is a dictionary and should contain `algorithm`, `hash`, and
+        `salt`. Extra keys can be algorithm specific like `iterations` or
+        `work_factor`.
+        """
+        raise NotImplementedError(
+            'subclasses of BasePasswordHasher must provide a decode() method.'
+        )

     def safe_summary(self, encoded):
         """
@@ -236,7 +265,7 @@
     safely but you must rename the algorithm if you change SHA256.
     """
     algorithm = "pbkdf2_sha256"
-    iterations = 150000
+    iterations = 260000
     digest = hashlib.sha256

     def encode(self, password, salt, iterations=None):
@@ -247,31 +276,40 @@
         hash = base64.b64encode(hash).decode('ascii').strip()
         return "%s$%d$%s$%s" % (self.algorithm, iterations, salt, hash)

-    def verify(self, password, encoded):
+    def decode(self, encoded):
         algorithm, iterations, salt, hash = encoded.split('$', 3)
         assert algorithm == self.algorithm
-        encoded_2 = self.encode(password, salt, int(iterations))
+        return {
+            'algorithm': algorithm,
+            'hash': hash,
+            'iterations': int(iterations),
+            'salt': salt,
+        }
+
+    def verify(self, password, encoded):
+        decoded = self.decode(encoded)
+        encoded_2 = self.encode(password, decoded['salt'], decoded['iterations'])
         return constant_time_compare(encoded, encoded_2)

     def safe_summary(self, encoded):
-        algorithm, iterations, salt, hash = encoded.split('$', 3)
-        assert algorithm == self.algorithm
-        return OrderedDict([
-            (_('algorithm'), algorithm),
-            (_('iterations'), iterations),
-            (_('salt'), mask_hash(salt)),
-            (_('hash'), mask_hash(hash)),
-        ])
+        decoded = self.decode(encoded)
+        return {
+            _('algorithm'): decoded['algorithm'],
+            _('iterations'): decoded['iterations'],
+            _('salt'): mask_hash(decoded['salt']),
+            _('hash'): mask_hash(decoded['hash']),
+        }

     def must_update(self, encoded):
-        algorithm, iterations, salt, hash = encoded.split('$', 3)
-        return int(iterations) != self.iterations
-
-    def harden_runtime(self, password, encoded):
-        algorithm, iterations, salt, hash = encoded.split('$', 3)
-        extra_iterations = self.iterations - int(iterations)
+        decoded = self.decode(encoded)
+        update_salt = must_update_salt(decoded['salt'], self.salt_entropy)
+        return (decoded['iterations'] != self.iterations) or update_salt
+
+    def harden_runtime(self, password, encoded):
+        decoded = self.decode(encoded)
+        extra_iterations = self.iterations - decoded['iterations']
         if extra_iterations > 0:
-            self.encode(password, salt, extra_iterations)
+            self.encode(password, decoded['salt'], extra_iterations)


 class PBKDF2SHA1PasswordHasher(PBKDF2PasswordHasher):
@@ -297,92 +335,92 @@
     library = 'argon2'

     time_cost = 2
-    memory_cost = 512
-    parallelism = 2
+    memory_cost = 102400
+    parallelism = 8

     def encode(self, password, salt):
         argon2 = self._load_library()
+        params = self.params()
         data = argon2.low_level.hash_secret(
             password.encode(),
             salt.encode(),
-            time_cost=self.time_cost,
-            memory_cost=self.memory_cost,
-            parallelism=self.parallelism,
-            hash_len=argon2.DEFAULT_HASH_LENGTH,
-            type=argon2.low_level.Type.I,
+            time_cost=params.time_cost,
+            memory_cost=params.memory_cost,
+            parallelism=params.parallelism,
+            hash_len=params.hash_len,
+            type=params.type,
         )
         return self.algorithm + data.decode('ascii')
+
+    def decode(self, encoded):
+        argon2 = self._load_library()
+        algorithm, rest = encoded.split('$', 1)
+        assert algorithm == self.algorithm
+        params = argon2.extract_parameters('$' + rest)
+        variety, *_, b64salt, hash = rest.split('$')
+        # Add padding.
+        b64salt += '=' * (-len(b64salt) % 4)
+        salt = base64.b64decode(b64salt).decode('latin1')
+        return {
+            'algorithm': algorithm,
+            'hash': hash,
+            'memory_cost': params.memory_cost,
+            'parallelism': params.parallelism,
+            'salt': salt,
+            'time_cost': params.time_cost,
+            'variety': variety,
+            'version': params.version,
+            'params': params,
+        }

     def verify(self, password, encoded):
         argon2 = self._load_library()
         algorithm, rest = encoded.split('$', 1)
         assert algorithm == self.algorithm
         try:
-            return argon2.low_level.verify_secret(
-                ('$' + rest).encode('ascii'),
-                password.encode(),
-                type=argon2.low_level.Type.I,
-            )
+            return argon2.PasswordHasher().verify('$' + rest, password)
         except argon2.exceptions.VerificationError:
             return False

     def safe_summary(self, encoded):
-        (algorithm, variety, version, time_cost, memory_cost, parallelism,
-            salt, data) = self._decode(encoded)
-        assert algorithm == self.algorithm
-        return OrderedDict([
-            (_('algorithm'), algorithm),
-            (_('variety'), variety),
-            (_('version'), version),
-            (_('memory cost'), memory_cost),
-            (_('time cost'), time_cost),
-            (_('parallelism'), parallelism),
-            (_('salt'), mask_hash(salt)),
-            (_('hash'), mask_hash(data)),
-        ])
+        decoded = self.decode(encoded)
+        return {
+            _('algorithm'): decoded['algorithm'],
+            _('variety'): decoded['variety'],
+            _('version'): decoded['version'],
+            _('memory cost'): decoded['memory_cost'],
+            _('time cost'): decoded['time_cost'],
+            _('parallelism'): decoded['parallelism'],
+            _('salt'): mask_hash(decoded['salt']),
+            _('hash'): mask_hash(decoded['hash']),
+        }

     def must_update(self, encoded):
-        (algorithm, variety, version, time_cost, memory_cost, parallelism,
-            salt, data) = self._decode(encoded)
-        assert algorithm == self.algorithm
-        argon2 = self._load_library()
-        return (
-            argon2.low_level.ARGON2_VERSION != version or
-            self.time_cost != time_cost or
-            self.memory_cost != memory_cost or
-            self.parallelism != parallelism
-        )
+        decoded = self.decode(encoded)
+        current_params = decoded['params']
+        new_params = self.params()
+        # Set salt_len to the salt_len of the current parameters because salt
+        # is explicitly passed to argon2.
+        new_params.salt_len = current_params.salt_len
+        update_salt = must_update_salt(decoded['salt'], self.salt_entropy)
+        return (current_params != new_params) or update_salt

     def harden_runtime(self, password, encoded):
         # The runtime for Argon2 is too complicated to implement a sensible
         # hardening algorithm.
         pass

-    def _decode(self, encoded):
-        """
-        Split an encoded hash and return: (
-            algorithm, variety, version, time_cost, memory_cost,
-            parallelism, salt, data,
-        ).
-        """
-        bits = encoded.split('$')
-        if len(bits) == 5:
-            # Argon2 < 1.3
-            algorithm, variety, raw_params, salt, data = bits
-            version = 0x10
-        else:
-            assert len(bits) == 6
-            algorithm, variety, raw_version, raw_params, salt, data = bits
-            assert raw_version.startswith('v=')
-            version = int(raw_version[len('v='):])
-        params = dict(bit.split('=', 1) for bit in raw_params.split(','))
-        assert len(params) == 3 and all(x in params for x in ('t', 'm', 'p'))
-        time_cost = int(params['t'])
-        memory_cost = int(params['m'])
-        parallelism = int(params['p'])
-        return (
-            algorithm, variety, version, time_cost, memory_cost, parallelism,
-            salt, data,
+    def params(self):
+        argon2 = self._load_library()
+        # salt_len is a noop, because we provide our own salt.
+        return argon2.Parameters(
+            type=argon2.low_level.Type.ID,
+            version=argon2.low_level.ARGON2_VERSION,
+            salt_len=argon2.DEFAULT_RANDOM_SALT_LENGTH,
+            hash_len=argon2.DEFAULT_HASH_LENGTH,
+            time_cost=self.time_cost,
+            memory_cost=self.memory_cost,
+            parallelism=self.parallelism,
         )


@@ -416,6 +454,17 @@
         data = bcrypt.hashpw(password, salt)
         return "%s$%s" % (self.algorithm, data.decode('ascii'))

+    def decode(self, encoded):
+        algorithm, empty, algostr, work_factor, data = encoded.split('$', 4)
+        assert algorithm == self.algorithm
+        return {
+            'algorithm': algorithm,
+            'algostr': algostr,
+            'checksum': data[22:],
+            'salt': data[:22],
+            'work_factor': int(work_factor),
+        }
+
     def verify(self, password, encoded):
         algorithm, data = encoded.split('$', 1)
         assert algorithm == self.algorithm
@@ -423,19 +472,17 @@
         return constant_time_compare(encoded, encoded_2)

     def safe_summary(self, encoded):
-        algorithm, empty, algostr, work_factor, data = encoded.split('$', 4)
-        assert algorithm == self.algorithm
-        salt, checksum = data[:22], data[22:]
-        return OrderedDict([
-            (_('algorithm'), algorithm),
-            (_('work factor'), work_factor),
-            (_('salt'), mask_hash(salt)),
-            (_('checksum'), mask_hash(checksum)),
-        ])
+        decoded = self.decode(encoded)
+        return {
+            _('algorithm'): decoded['algorithm'],
+            _('work factor'): decoded['work_factor'],
+            _('salt'): mask_hash(decoded['salt']),
+            _('checksum'): mask_hash(decoded['checksum']),
+        }

     def must_update(self, encoded):
-        algorithm, empty, algostr, rounds, data = encoded.split('$', 4)
-        return int(rounds) != self.rounds
+        decoded = self.decode(encoded)
+        return decoded['work_factor'] != self.rounds

     def harden_runtime(self, password, encoded):
         _, data = encoded.split('$', 1)
@@ -477,20 +524,31 @@
         hash = hashlib.sha1((salt + password).encode()).hexdigest()
         return "%s$%s$%s" % (self.algorithm, salt, hash)

-    def verify(self, password, encoded):
+    def decode(self, encoded):
         algorithm, salt, hash = encoded.split('$', 2)
         assert algorithm == self.algorithm
-        encoded_2 = self.encode(password, salt)
+        return {
+            'algorithm': algorithm,
+            'hash': hash,
+            'salt': salt,
+        }
+
+    def verify(self, password, encoded):
+        decoded = self.decode(encoded)
+        encoded_2 = self.encode(password, decoded['salt'])
         return constant_time_compare(encoded, encoded_2)

     def safe_summary(self, encoded):
-        algorithm, salt, hash = encoded.split('$', 2)
-        assert algorithm == self.algorithm
-        return OrderedDict([
-            (_('algorithm'), algorithm),
-            (_('salt'), mask_hash(salt, show=2)),
-            (_('hash'), mask_hash(hash)),
-        ])
+        decoded = self.decode(encoded)
+        return {
+            _('algorithm'): decoded['algorithm'],
+            _('salt'): mask_hash(decoded['salt'], show=2),
+            _('hash'): mask_hash(decoded['hash']),
+        }
+
+    def must_update(self, encoded):
+        decoded = self.decode(encoded)
+        return must_update_salt(decoded['salt'], self.salt_entropy)

     def harden_runtime(self, password, encoded):
         pass
@@ -508,20 +566,31 @@
         hash = hashlib.md5((salt + password).encode()).hexdigest()
         return "%s$%s$%s" % (self.algorithm, salt, hash)

-    def verify(self, password, encoded):
+    def decode(self, encoded):
         algorithm, salt, hash = encoded.split('$', 2)
         assert algorithm == self.algorithm
-        encoded_2 = self.encode(password, salt)
+        return {
+            'algorithm': algorithm,
+            'hash': hash,
+            'salt': salt,
+        }
+
+    def verify(self, password, encoded):
+        decoded = self.decode(encoded)
+        encoded_2 = self.encode(password, decoded['salt'])
         return constant_time_compare(encoded, encoded_2)

     def safe_summary(self, encoded):
-        algorithm, salt, hash = encoded.split('$', 2)
-        assert algorithm == self.algorithm
-        return OrderedDict([
-            (_('algorithm'), algorithm),
-            (_('salt'), mask_hash(salt, show=2)),
-            (_('hash'), mask_hash(hash)),
-        ])
+        decoded = self.decode(encoded)
+        return {
+            _('algorithm'): decoded['algorithm'],
+            _('salt'): mask_hash(decoded['salt'], show=2),
+            _('hash'): mask_hash(decoded['hash']),
+        }
+
+    def must_update(self, encoded):
+        decoded = self.decode(encoded)
+        return must_update_salt(decoded['salt'], self.salt_entropy)

     def harden_runtime(self, password, encoded):
         pass
@@ -546,17 +615,24 @@
         hash = hashlib.sha1(password.encode()).hexdigest()
         return 'sha1$$%s' % hash

+    def decode(self, encoded):
+        assert encoded.startswith('sha1$$')
+        return {
+            'algorithm': self.algorithm,
+            'hash': encoded[6:],
+            'salt': None,
+        }
+
     def verify(self, password, encoded):
         encoded_2 = self.encode(password, '')
         return constant_time_compare(encoded, encoded_2)

     def safe_summary(self, encoded):
-        assert encoded.startswith('sha1$$')
-        hash = encoded[6:]
-        return OrderedDict([
-            (_('algorithm'), self.algorithm),
-            (_('hash'), mask_hash(hash)),
-        ])
+        decoded = self.decode(encoded)
+        return {
+            _('algorithm'): decoded['algorithm'],
+            _('hash'): mask_hash(decoded['hash']),
+        }

     def harden_runtime(self, password, encoded):
         pass
@@ -582,6 +658,13 @@
         assert salt == ''
         return hashlib.md5(password.encode()).hexdigest()

+    def decode(self, encoded):
+        return {
+            'algorithm': self.algorithm,
+            'hash': encoded,
+            'salt': None,
+        }
+
     def verify(self, password, encoded):
         if len(encoded) == 37 and encoded.startswith('md5$$'):
             encoded = encoded[5:]
@@ -589,10 +672,11 @@
         return constant_time_compare(encoded, encoded_2)

     def safe_summary(self, encoded):
-        return OrderedDict([
-            (_('algorithm'), self.algorithm),
-            (_('hash'), mask_hash(encoded, show=3)),
-        ])
+        decoded = self.decode(encoded)
+        return {
+            _('algorithm'): decoded['algorithm'],
+            _('hash'): mask_hash(decoded['hash'], show=3),
+        }

     def harden_runtime(self, password, encoded):
         pass
@@ -613,25 +697,33 @@
     def encode(self, password, salt):
         crypt = self._load_library()
         assert len(salt) == 2
-        data = crypt.crypt(password, salt)
-        assert data is not None  # A platform like OpenBSD with a dummy crypt module.
+        hash = crypt.crypt(password, salt)
+        assert hash is not None  # A platform like OpenBSD with a dummy crypt module.
         # we don't need to store the salt, but Django used to do this
-        return "%s$%s$%s" % (self.algorithm, '', data)
+        return '%s$%s$%s' % (self.algorithm, '', hash)
+
+    def decode(self, encoded):
+        algorithm, salt, hash = encoded.split('$', 2)
+        assert algorithm == self.algorithm
+        return {
+            'algorithm': algorithm,
+            'hash': hash,
+            'salt': salt,
+        }

     def verify(self, password, encoded):
         crypt = self._load_library()
-        algorithm, salt, data = encoded.split('$', 2)
-        assert algorithm == self.algorithm
-        return constant_time_compare(data, crypt.crypt(password, data))
-
-    def safe_summary(self, encoded):
-        algorithm, salt, data = encoded.split('$', 2)
-        assert algorithm == self.algorithm
-        return OrderedDict([
-            (_('algorithm'), algorithm),
-            (_('salt'), salt),
-            (_('hash'), mask_hash(data, show=3)),
-        ])
+        decoded = self.decode(encoded)
+        data = crypt.crypt(password, decoded['hash'])
+        return constant_time_compare(decoded['hash'], data)
+
+    def safe_summary(self, encoded):
+        decoded = self.decode(encoded)
+        return {
+            _('algorithm'): decoded['algorithm'],
+            _('salt'): decoded['salt'],
+            _('hash'): mask_hash(decoded['hash'], show=3),
+        }

     def harden_runtime(self, password, encoded):
         pass
('django/contrib/auth', 'urls.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,4 +1,4 @@
-# The views used below are normally mapped in django.contrib.admin.urls.py
+# The views used below are normally mapped in the AdminSite instance.
 # This URLs file is used to provide a reliable view deployment for test purposes.
 # It is also provided as a convenience to those who want to deploy these URLs
 # elsewhere.
('django/contrib/auth', 'middleware.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,4 +1,3 @@
-from django.conf import settings
 from django.contrib import auth
 from django.contrib.auth import load_backend
 from django.contrib.auth.backends import RemoteUserBackend
@@ -17,10 +16,10 @@
     def process_request(self, request):
         assert hasattr(request, 'session'), (
             "The Django authentication middleware requires session middleware "
-            "to be installed. Edit your MIDDLEWARE%s setting to insert "
+            "to be installed. Edit your MIDDLEWARE setting to insert "
             "'django.contrib.sessions.middleware.SessionMiddleware' before "
             "'django.contrib.auth.middleware.AuthenticationMiddleware'."
-        ) % ("_CLASSES" if settings.MIDDLEWARE is None else "")
+        )
         request.user = SimpleLazyObject(lambda: get_user(request))


('django/contrib/auth', 'views.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -17,7 +17,9 @@
 from django.shortcuts import resolve_url
 from django.urls import reverse_lazy
 from django.utils.decorators import method_decorator
-from django.utils.http import is_safe_url, urlsafe_base64_decode
+from django.utils.http import (
+    url_has_allowed_host_and_scheme, urlsafe_base64_decode,
+)
 from django.utils.translation import gettext_lazy as _
 from django.views.decorators.cache import never_cache
 from django.views.decorators.csrf import csrf_protect
@@ -70,7 +72,7 @@
             self.redirect_field_name,
             self.request.GET.get(self.redirect_field_name, '')
         )
-        url_is_safe = is_safe_url(
+        url_is_safe = url_has_allowed_host_and_scheme(
             url=redirect_to,
             allowed_hosts=self.get_success_url_allowed_hosts(),
             require_https=self.request.is_secure(),
@@ -138,7 +140,7 @@
                 self.redirect_field_name,
                 self.request.GET.get(self.redirect_field_name)
             )
-            url_is_safe = is_safe_url(
+            url_is_safe = url_has_allowed_host_and_scheme(
                 url=next_page,
                 allowed_hosts=self.get_success_url_allowed_hosts(),
                 require_https=self.request.is_secure(),
@@ -234,7 +236,6 @@
         return super().form_valid(form)


-INTERNAL_RESET_URL_TOKEN = 'set-password'
 INTERNAL_RESET_SESSION_TOKEN = '_password_reset_token'


@@ -247,6 +248,7 @@
     form_class = SetPasswordForm
     post_reset_login = False
     post_reset_login_backend = None
+    reset_url_token = 'set-password'
     success_url = reverse_lazy('password_reset_complete')
     template_name = 'registration/password_reset_confirm.html'
     title = _('Enter new password')
@@ -262,7 +264,7 @@

         if self.user is not None:
             token = kwargs['token']
-            if token == INTERNAL_RESET_URL_TOKEN:
+            if token == self.reset_url_token:
                 session_token = self.request.session.get(INTERNAL_RESET_SESSION_TOKEN)
                 if self.token_generator.check_token(self.user, session_token):
                     # If the token is valid, display the password reset form.
@@ -275,7 +277,7 @@
                     # avoids the possibility of leaking the token in the
                     # HTTP Referer header.
                     self.request.session[INTERNAL_RESET_SESSION_TOKEN] = token
-                    redirect_url = self.request.path.replace(token, INTERNAL_RESET_URL_TOKEN)
+                    redirect_url = self.request.path.replace(token, self.reset_url_token)
                     return HttpResponseRedirect(redirect_url)

         # Display the "Password reset unsuccessful" page.
('django/contrib/auth/migrations', '0011_update_proxy_permissions.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,17 @@
-from django.db import migrations
+import sys
+
+from django.core.management.color import color_style
+from django.db import IntegrityError, migrations, transaction
 from django.db.models import Q
+
+WARNING = """
+    A problem arose migrating proxy model permissions for {old} to {new}.
+
+      Permission(s) for {new} already existed.
+      Codenames Q: {query}
+
+    Ensure to audit ALL permissions for {old} and {new}.
+"""


 def update_proxy_model_permissions(apps, schema_editor, reverse=False):
@@ -7,8 +19,10 @@
     Update the content_type of proxy model permissions to use the ContentType
     of the proxy model.
     """
+    style = color_style()
     Permission = apps.get_model('auth', 'Permission')
     ContentType = apps.get_model('contenttypes', 'ContentType')
+    alias = schema_editor.connection.alias
     for Model in apps.get_models():
         opts = Model._meta
         if not opts.proxy:
@@ -20,14 +34,21 @@
         permissions_query = Q(codename__in=proxy_default_permissions_codenames)
         for codename, name in opts.permissions:
             permissions_query = permissions_query | Q(codename=codename, name=name)
-        concrete_content_type = ContentType.objects.get_for_model(Model, for_concrete_model=True)
-        proxy_content_type = ContentType.objects.get_for_model(Model, for_concrete_model=False)
+        content_type_manager = ContentType.objects.db_manager(alias)
+        concrete_content_type = content_type_manager.get_for_model(Model, for_concrete_model=True)
+        proxy_content_type = content_type_manager.get_for_model(Model, for_concrete_model=False)
         old_content_type = proxy_content_type if reverse else concrete_content_type
         new_content_type = concrete_content_type if reverse else proxy_content_type
-        Permission.objects.filter(
-            permissions_query,
-            content_type=old_content_type,
-        ).update(content_type=new_content_type)
+        try:
+            with transaction.atomic(using=alias):
+                Permission.objects.using(alias).filter(
+                    permissions_query,
+                    content_type=old_content_type,
+                ).update(content_type=new_content_type)
+        except IntegrityError:
+            old = '{}_{}'.format(old_content_type.app_label, old_content_type.model)
+            new = '{}_{}'.format(new_content_type.app_label, new_content_type.model)
+            sys.stdout.write(style.WARNING(WARNING.format(old=old, new=new, query=permissions_query)))


 def revert_proxy_model_permissions(apps, schema_editor):
('django/contrib/auth/migrations', '0001_initial.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -25,7 +25,7 @@
                 ('codename', models.CharField(max_length=100, verbose_name='codename')),
             ],
             options={
-                'ordering': ('content_type__app_label', 'content_type__model', 'codename'),
+                'ordering': ['content_type__app_label', 'content_type__model', 'codename'],
                 'unique_together': {('content_type', 'codename')},
                 'verbose_name': 'permission',
                 'verbose_name_plural': 'permissions',
('django/contrib/auth/management', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -101,14 +101,15 @@
     return result


-def get_default_username(check_db=True):
+def get_default_username(check_db=True, database=DEFAULT_DB_ALIAS):
     """
     Try to determine the current system user's username to use as a default.

     :param check_db: If ``True``, requires that the username does not match an
         existing ``auth.User`` (otherwise returns an empty string).
+    :param database: The database where the unique check will be performed.
     :returns: The username, or an empty string if no username can be
-        determined.
+        determined or the suggested username is already taken.
     """
     # This file is used in apps.py, it should not trigger models import.
     from django.contrib.auth import models as auth_app
@@ -137,7 +138,9 @@
     # Don't return the default username if it is already taken.
     if check_db and default_username:
         try:
-            auth_app.User._default_manager.get(username=default_username)
+            auth_app.User._default_manager.db_manager(database).get(
+                username=default_username,
+            )
         except auth_app.User.DoesNotExist:
             pass
         else:
('django/contrib/auth/management/commands', 'createsuperuser.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,6 +2,7 @@
 Management utility to create superusers.
 """
 import getpass
+import os
 import sys

 from django.contrib.auth import get_user_model
@@ -50,11 +51,28 @@
             default=DEFAULT_DB_ALIAS,
             help='Specifies the database to use. Default is "default".',
         )
-        for field in self.UserModel.REQUIRED_FIELDS:
-            parser.add_argument(
-                '--%s' % field,
-                help='Specifies the %s for the superuser.' % field,
-            )
+        for field_name in self.UserModel.REQUIRED_FIELDS:
+            field = self.UserModel._meta.get_field(field_name)
+            if field.many_to_many:
+                if field.remote_field.through and not field.remote_field.through._meta.auto_created:
+                    raise CommandError(
+                        "Required field '%s' specifies a many-to-many "
+                        "relation through model, which is not supported."
+                        % field_name
+                    )
+                else:
+                    parser.add_argument(
+                        '--%s' % field_name, action='append',
+                        help=(
+                            'Specifies the %s for the superuser. Can be used '
+                            'multiple times.' % field_name,
+                        ),
+                    )
+            else:
+                parser.add_argument(
+                    '--%s' % field_name,
+                    help='Specifies the %s for the superuser.' % field_name,
+                )

     def execute(self, *args, **options):
         self.stdin = options.get('stdin', sys.stdin)  # Used for testing
@@ -74,12 +92,12 @@
             user_data[PASSWORD_FIELD] = None
         try:
             if options['interactive']:
-                # Same as user_data but with foreign keys as fake model
-                # instances instead of raw IDs.
+                # Same as user_data but without many to many fields and with
+                # foreign keys as fake model instances instead of raw IDs.
                 fake_user_data = {}
                 if hasattr(self.stdin, 'isatty') and not self.stdin.isatty():
                     raise NotRunningInTTYException
-                default_username = get_default_username()
+                default_username = get_default_username(database=database)
                 if username:
                     error_msg = self._validate_username(username, verbose_field_name, database)
                     if error_msg:
@@ -110,10 +128,17 @@
                         message = self._get_input_message(field)
                         input_value = self.get_input_data(field, message)
                         user_data[field_name] = input_value
-                        fake_user_data[field_name] = input_value
+                        if field.many_to_many and input_value:
+                            if not input_value.strip():
+                                user_data[field_name] = None
+                                self.stderr.write('Error: This field cannot be blank.')
+                                continue
+                            user_data[field_name] = [pk.strip() for pk in input_value.split(',')]
+                        if not field.many_to_many:
+                            fake_user_data[field_name] = input_value

                         # Wrap any foreign keys in fake model instances
-                        if field.remote_field:
+                        if field.many_to_one:
                             fake_user_data[field_name] = field.remote_field.model(input_value)

                 # Prompt for a password if the model has one.
@@ -138,6 +163,13 @@
                     user_data[PASSWORD_FIELD] = password
             else:
                 # Non-interactive mode.
+                # Use password from environment variable, if provided.
+                if PASSWORD_FIELD in user_data and 'DJANGO_SUPERUSER_PASSWORD' in os.environ:
+                    user_data[PASSWORD_FIELD] = os.environ['DJANGO_SUPERUSER_PASSWORD']
+                # Use username from environment variable, if not provided in
+                # options.
+                if username is None:
+                    username = os.environ.get('DJANGO_SUPERUSER_' + self.UserModel.USERNAME_FIELD.upper())
                 if username is None:
                     raise CommandError('You must use --%s with --noinput.' % self.UserModel.USERNAME_FIELD)
                 else:
@@ -147,11 +179,12 @@

                 user_data[self.UserModel.USERNAME_FIELD] = username
                 for field_name in self.UserModel.REQUIRED_FIELDS:
-                    if options[field_name]:
-                        field = self.UserModel._meta.get_field(field_name)
-                        user_data[field_name] = field.clean(options[field_name], None)
-                    else:
+                    env_var = 'DJANGO_SUPERUSER_' + field_name.upper()
+                    value = options[field_name] or os.environ.get(env_var)
+                    if not value:
                         raise CommandError('You must use --%s with --noinput.' % field_name)
+                    field = self.UserModel._meta.get_field(field_name)
+                    user_data[field_name] = field.clean(value, None)

             self.UserModel._default_manager.db_manager(database).create_superuser(**user_data)
             if options['verbosity'] >= 1:
@@ -190,7 +223,7 @@
             " (leave blank to use '%s')" % default if default else '',
             ' (%s.%s)' % (
                 field.remote_field.model._meta.object_name,
-                field.remote_field.field_name,
+                field.m2m_target_field_name() if field.many_to_many else field.remote_field.field_name,
             ) if field.remote_field else '',
         )

('django/contrib/auth/management/commands', 'changepassword.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -12,7 +12,7 @@
 class Command(BaseCommand):
     help = "Change a user's password for django.contrib.auth."
     requires_migrations_checks = True
-    requires_system_checks = False
+    requires_system_checks = []

     def _get_pass(self, prompt="Password: "):
         p = getpass.getpass(prompt=prompt)
@@ -44,7 +44,7 @@
         except UserModel.DoesNotExist:
             raise CommandError("user '%s' does not exist" % username)

-        self.stdout.write("Changing password for user '%s'\n" % u)
+        self.stdout.write("Changing password for user '%s'" % u)

         MAX_TRIES = 3
         count = 0
@@ -54,7 +54,7 @@
             p1 = self._get_pass()
             p2 = self._get_pass("Password (again): ")
             if p1 != p2:
-                self.stdout.write("Passwords do not match. Please try again.\n")
+                self.stdout.write('Passwords do not match. Please try again.')
                 count += 1
                 # Don't validate passwords that don't match.
                 continue
('django/contrib/admin', 'options.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,7 +2,6 @@
 import json
 import operator
 import re
-from collections import OrderedDict
 from functools import partial, reduce, update_wrapper
 from urllib.parse import quote as urlquote

@@ -13,6 +12,7 @@
 from django.contrib.admin.checks import (
     BaseModelAdminChecks, InlineModelAdminChecks, ModelAdminChecks,
 )
+from django.contrib.admin.decorators import display
 from django.contrib.admin.exceptions import DisallowedModelAdminToField
 from django.contrib.admin.templatetags.admin_urls import add_preserved_filters
 from django.contrib.admin.utils import (
@@ -20,7 +20,6 @@
     get_deleted_objects, lookup_needs_distinct, model_format_dict,
     model_ngettext, quote, unquote,
 )
-from django.contrib.admin.views.autocomplete import AutocompleteJsonView
 from django.contrib.admin.widgets import (
     AutocompleteSelect, AutocompleteSelectMultiple,
 )
@@ -31,7 +30,6 @@
 from django.core.paginator import Paginator
 from django.db import models, router, transaction
 from django.db.models.constants import LOOKUP_SEP
-from django.db.models.fields import BLANK_CHOICE_DASH
 from django.forms.formsets import DELETION_FIELD_NAME, all_valid
 from django.forms.models import (
     BaseInlineFormSet, inlineformset_factory, modelform_defines_fields,
@@ -45,9 +43,10 @@
 from django.utils.decorators import method_decorator
 from django.utils.html import format_html
 from django.utils.http import urlencode
-from django.utils.inspect import get_func_args
 from django.utils.safestring import mark_safe
-from django.utils.text import capfirst, format_lazy, get_text_list
+from django.utils.text import (
+    capfirst, format_lazy, get_text_list, smart_split, unescape_string_literal,
+)
 from django.utils.translation import gettext as _, ngettext
 from django.views.decorators.csrf import csrf_protect
 from django.views.generic import RedirectView
@@ -226,7 +225,7 @@

         if 'widget' not in kwargs:
             if db_field.name in self.get_autocomplete_fields(request):
-                kwargs['widget'] = AutocompleteSelect(db_field.remote_field, self.admin_site, using=db)
+                kwargs['widget'] = AutocompleteSelect(db_field, self.admin_site, using=db)
             elif db_field.name in self.raw_id_fields:
                 kwargs['widget'] = widgets.ForeignKeyRawIdWidget(db_field.remote_field, self.admin_site, using=db)
             elif db_field.name in self.radio_fields:
@@ -252,17 +251,25 @@
             return None
         db = kwargs.get('using')

-        autocomplete_fields = self.get_autocomplete_fields(request)
-        if db_field.name in autocomplete_fields:
-            kwargs['widget'] = AutocompleteSelectMultiple(db_field.remote_field, self.admin_site, using=db)
-        elif db_field.name in self.raw_id_fields:
-            kwargs['widget'] = widgets.ManyToManyRawIdWidget(db_field.remote_field, self.admin_site, using=db)
-        elif db_field.name in [*self.filter_vertical, *self.filter_horizontal]:
-            kwargs['widget'] = widgets.FilteredSelectMultiple(
-                db_field.verbose_name,
-                db_field.name in self.filter_vertical
-            )
-
+        if 'widget' not in kwargs:
+            autocomplete_fields = self.get_autocomplete_fields(request)
+            if db_field.name in autocomplete_fields:
+                kwargs['widget'] = AutocompleteSelectMultiple(
+                    db_field,
+                    self.admin_site,
+                    using=db,
+                )
+            elif db_field.name in self.raw_id_fields:
+                kwargs['widget'] = widgets.ManyToManyRawIdWidget(
+                    db_field.remote_field,
+                    self.admin_site,
+                    using=db,
+                )
+            elif db_field.name in [*self.filter_vertical, *self.filter_horizontal]:
+                kwargs['widget'] = widgets.FilteredSelectMultiple(
+                    db_field.verbose_name,
+                    db_field.name in self.filter_vertical
+                )
         if 'queryset' not in kwargs:
             queryset = self.get_field_queryset(db, db_field, request)
             if queryset is not None:
@@ -271,7 +278,7 @@
         form_field = db_field.formfield(**kwargs)
         if (isinstance(form_field.widget, SelectMultiple) and
                 not isinstance(form_field.widget, (CheckboxSelectMultiple, AutocompleteSelectMultiple))):
-            msg = _('Hold down "Control", or "Command" on a Mac, to select more than one.')
+            msg = _('Hold down “Control”, or “Command” on a Mac, to select more than one.')
             help_text = form_field.help_text
             form_field.help_text = format_lazy('{} {}', help_text, msg) if help_text else msg
         return form_field
@@ -289,7 +296,7 @@

         if callable(self.view_on_site):
             return self.view_on_site(obj)
-        elif self.view_on_site and hasattr(obj, 'get_absolute_url'):
+        elif hasattr(obj, 'get_absolute_url'):
             # use the ContentType lookup if view_on_site is True
             return reverse('admin:view_on_site', kwargs={
                 'content_type_id': get_content_type_for_model(obj).pk,
@@ -328,6 +335,10 @@
         if self.fieldsets:
             return self.fieldsets
         return [(None, {'fields': self.get_fields(request, obj)})]
+
+    def get_inlines(self, request, obj):
+        """Hook for specifying custom inlines."""
+        return self.inlines

     def get_ordering(self, request):
         """
@@ -584,15 +595,14 @@

     def get_inline_instances(self, request, obj=None):
         inline_instances = []
-        for inline_class in self.inlines:
+        for inline_class in self.get_inlines(request, obj):
             inline = inline_class(self.model, self.admin_site)
             if request:
-                inline_has_add_permission = inline._has_add_permission(request, obj)
                 if not (inline.has_view_or_change_permission(request, obj) or
-                        inline_has_add_permission or
+                        inline.has_add_permission(request, obj) or
                         inline.has_delete_permission(request, obj)):
                     continue
-                if not inline_has_add_permission:
+                if not inline.has_add_permission(request, obj):
                     inline.max_num = 0
             inline_instances.append(inline)

@@ -609,10 +619,9 @@

         info = self.model._meta.app_label, self.model._meta.model_name

-        urlpatterns = [
+        return [
             path('', wrap(self.changelist_view), name='%s_%s_changelist' % info),
             path('add/', wrap(self.add_view), name='%s_%s_add' % info),
-            path('autocomplete/', wrap(self.autocomplete_view), name='%s_%s_autocomplete' % info),
             path('<path:object_id>/history/', wrap(self.history_view), name='%s_%s_history' % info),
             path('<path:object_id>/delete/', wrap(self.delete_view), name='%s_%s_delete' % info),
             path('<path:object_id>/change/', wrap(self.change_view), name='%s_%s_change' % info),
@@ -621,7 +630,6 @@
                 pattern_name='%s:%s_%s_change' % ((self.admin_site.name,) + info)
             ))),
         ]
-        return urlpatterns

     @property
     def urls(self):
@@ -635,9 +643,9 @@
             'jquery.init.js',
             'core.js',
             'admin/RelatedObjectLookups.js',
-            'actions%s.js' % extra,
+            'actions.js',
             'urlify.js',
-            'prepopulate%s.js' % extra,
+            'prepopulate.js',
             'vendor/xregexp/xregexp%s.js' % extra,
         ]
         return forms.Media(js=['admin/js/%s' % url for url in js])
@@ -684,10 +692,7 @@
         exclude = exclude or None

         # Remove declared form fields which are in readonly_fields.
-        new_attrs = OrderedDict.fromkeys(
-            f for f in readonly_fields
-            if f in self.form.declared_fields
-        )
+        new_attrs = dict.fromkeys(f for f in readonly_fields if f in self.form.declared_fields)
         form = type(self.form.__name__, (self.form,), new_attrs)

         defaults = {
@@ -802,7 +807,7 @@

         The default implementation creates an admin LogEntry object.
         """
-        from django.contrib.admin.models import LogEntry, ADDITION
+        from django.contrib.admin.models import ADDITION, LogEntry
         return LogEntry.objects.log_action(
             user_id=request.user.pk,
             content_type_id=get_content_type_for_model(object).pk,
@@ -818,7 +823,7 @@

         The default implementation creates an admin LogEntry object.
         """
-        from django.contrib.admin.models import LogEntry, CHANGE
+        from django.contrib.admin.models import CHANGE, LogEntry
         return LogEntry.objects.log_action(
             user_id=request.user.pk,
             content_type_id=get_content_type_for_model(object).pk,
@@ -835,7 +840,7 @@

         The default implementation creates an admin LogEntry object.
         """
-        from django.contrib.admin.models import LogEntry, DELETION
+        from django.contrib.admin.models import DELETION, LogEntry
         return LogEntry.objects.log_action(
             user_id=request.user.pk,
             content_type_id=get_content_type_for_model(object).pk,
@@ -844,25 +849,34 @@
             action_flag=DELETION,
         )

+    @display(description=mark_safe('<input type="checkbox" id="action-toggle">'))
     def action_checkbox(self, obj):
         """
         A list_display column containing a checkbox widget.
         """
         return helpers.checkbox.render(helpers.ACTION_CHECKBOX_NAME, str(obj.pk))
-    action_checkbox.short_description = mark_safe('<input type="checkbox" id="action-toggle">')
+
+    @staticmethod
+    def _get_action_description(func, name):
+        return getattr(func, 'short_description', capfirst(name.replace('_', ' ')))

     def _get_base_actions(self):
         """Return the list of actions, prior to any request-based filtering."""
         actions = []
+        base_actions = (self.get_action(action) for action in self.actions or [])
+        # get_action might have returned None, so filter any of those out.
+        base_actions = [action for action in base_actions if action]
+        base_action_names = {name for _, name, _ in base_actions}

         # Gather actions from the admin site first
         for (name, func) in self.admin_site.actions:
-            description = getattr(func, 'short_description', name.replace('_', ' '))
+            if name in base_action_names:
+                continue
+            description = self._get_action_description(func, name)
             actions.append((func, name, description))
         # Add actions from this ModelAdmin.
-        actions.extend(self.get_action(action) for action in self.actions or [])
-        # get_action might have returned None, so filter any of those out.
-        return filter(None, actions)
+        actions.extend(base_actions)
+        return actions

     def _filter_actions_by_permissions(self, request, actions):
         """Filter out any actions that the user doesn't have access to."""
@@ -888,15 +902,11 @@
         # If self.actions is set to None that means actions are disabled on
         # this page.
         if self.actions is None or IS_POPUP_VAR in request.GET:
-            return OrderedDict()
+            return {}
         actions = self._filter_actions_by_permissions(request, self._get_base_actions())
-        # Convert the actions into an OrderedDict keyed by name.
-        return OrderedDict(
-            (name, (func, name, desc))
-            for func, name, desc in actions
-        )
-
-    def get_action_choices(self, request, default_choices=BLANK_CHOICE_DASH):
+        return {name: (func, name, desc) for func, name, desc in actions}
+
+    def get_action_choices(self, request, default_choices=models.BLANK_CHOICE_DASH):
         """
         Return a list of choices for use in a form object.  Each choice is a
         tuple (name, description).
@@ -931,10 +941,7 @@
             except KeyError:
                 return None

-        if hasattr(func, 'short_description'):
-            description = func.short_description
-        else:
-            description = capfirst(action.replace('_', ' '))
+        description = self._get_action_description(func, action)
         return func, action, description

     def get_list_display(self, request):
@@ -1012,18 +1019,22 @@
             # Otherwise, use the field with icontains.
             return "%s__icontains" % field_name

-        use_distinct = False
+        may_have_duplicates = False
         search_fields = self.get_search_fields(request)
         if search_fields and search_term:
             orm_lookups = [construct_search(str(search_field))
                            for search_field in search_fields]
-            for bit in search_term.split():
+            for bit in smart_split(search_term):
+                if bit.startswith(('"', "'")) and bit[0] == bit[-1]:
+                    bit = unescape_string_literal(bit)
                 or_queries = [models.Q(**{orm_lookup: bit})
                               for orm_lookup in orm_lookups]
                 queryset = queryset.filter(reduce(operator.or_, or_queries))
-            use_distinct |= any(lookup_needs_distinct(self.opts, search_spec) for search_spec in orm_lookups)
-
-        return queryset, use_distinct
+            may_have_duplicates |= any(
+                lookup_needs_distinct(self.opts, search_spec)
+                for search_spec in orm_lookups
+            )
+        return queryset, may_have_duplicates

     def get_preserved_filters(self, request):
         """
@@ -1066,7 +1077,7 @@
                 level = getattr(messages.constants, level.upper())
             except AttributeError:
                 levels = messages.constants.DEFAULT_TAGS.values()
-                levels_repr = ', '.join('`%s`' % l for l in levels)
+                levels_repr = ', '.join('`%s`' % level for level in levels)
                 raise ValueError(
                     'Bad message level string: `%s`. Possible values are: %s'
                     % (level, levels_repr)
@@ -1135,7 +1146,7 @@
             'has_delete_permission': self.has_delete_permission(request, obj),
             'has_editable_inline_admin_formsets': has_editable_inline_admin_formsets,
             'has_file_field': context['adminform'].form.is_multipart() or any(
-                admin_formset.formset.form().is_multipart()
+                admin_formset.formset.is_multipart()
                 for admin_formset in context['inline_admin_formsets']
             ),
             'has_absolute_url': view_on_site_url is not None,
@@ -1209,7 +1220,7 @@
                 "_saveasnew" in request.POST and self.save_as_continue and
                 self.has_change_permission(request, obj)
         ):
-            msg = _('The {name} "{obj}" was added successfully.')
+            msg = _('The {name} “{obj}” was added successfully.')
             if self.has_change_permission(request, obj):
                 msg += ' ' + _('You may edit it again below.')
             self.message_user(request, format_html(msg, **msg_dict), messages.SUCCESS)
@@ -1223,7 +1234,7 @@

         elif "_addanother" in request.POST:
             msg = format_html(
-                _('The {name} "{obj}" was added successfully. You may add another {name} below.'),
+                _('The {name} “{obj}” was added successfully. You may add another {name} below.'),
                 **msg_dict
             )
             self.message_user(request, msg, messages.SUCCESS)
@@ -1233,7 +1244,7 @@

         else:
             msg = format_html(
-                _('The {name} "{obj}" was added successfully.'),
+                _('The {name} “{obj}” was added successfully.'),
                 **msg_dict
             )
             self.message_user(request, msg, messages.SUCCESS)
@@ -1273,7 +1284,7 @@
         }
         if "_continue" in request.POST:
             msg = format_html(
-                _('The {name} "{obj}" was changed successfully. You may edit it again below.'),
+                _('The {name} “{obj}” was changed successfully. You may edit it again below.'),
                 **msg_dict
             )
             self.message_user(request, msg, messages.SUCCESS)
@@ -1283,7 +1294,7 @@

         elif "_saveasnew" in request.POST:
             msg = format_html(
-                _('The {name} "{obj}" was added successfully. You may edit it again below.'),
+                _('The {name} “{obj}” was added successfully. You may edit it again below.'),
                 **msg_dict
             )
             self.message_user(request, msg, messages.SUCCESS)
@@ -1296,7 +1307,7 @@

         elif "_addanother" in request.POST:
             msg = format_html(
-                _('The {name} "{obj}" was changed successfully. You may add another {name} below.'),
+                _('The {name} “{obj}” was changed successfully. You may add another {name} below.'),
                 **msg_dict
             )
             self.message_user(request, msg, messages.SUCCESS)
@@ -1308,7 +1319,7 @@

         else:
             msg = format_html(
-                _('The {name} "{obj}" was changed successfully.'),
+                _('The {name} “{obj}” was changed successfully.'),
                 **msg_dict
             )
             self.message_user(request, msg, messages.SUCCESS)
@@ -1429,7 +1440,7 @@

         self.message_user(
             request,
-            _('The %(name)s "%(obj)s" was deleted successfully.') % {
+            _('The %(name)s “%(obj)s” was deleted successfully.') % {
                 'name': opts.verbose_name,
                 'obj': obj_display,
             },
@@ -1471,13 +1482,20 @@
         )

     def get_inline_formsets(self, request, formsets, inline_instances, obj=None):
+        # Edit permissions on parent model are required for editable inlines.
+        can_edit_parent = self.has_change_permission(request, obj) if obj else self.has_add_permission(request)
         inline_admin_formsets = []
         for inline, formset in zip(inline_instances, formsets):
             fieldsets = list(inline.get_fieldsets(request, obj))
             readonly = list(inline.get_readonly_fields(request, obj))
-            has_add_permission = inline._has_add_permission(request, obj)
-            has_change_permission = inline.has_change_permission(request, obj)
-            has_delete_permission = inline.has_delete_permission(request, obj)
+            if can_edit_parent:
+                has_add_permission = inline.has_add_permission(request, obj)
+                has_change_permission = inline.has_change_permission(request, obj)
+                has_delete_permission = inline.has_delete_permission(request, obj)
+            else:
+                # Disable all edit-permissions, and overide formset settings.
+                has_add_permission = has_change_permission = has_delete_permission = False
+                formset.extra = formset.max_num = 0
             has_view_permission = inline.has_view_permission(request, obj)
             prepopulated = dict(inline.get_prepopulated_fields(request, obj))
             inline_admin_formset = helpers.InlineAdminFormSet(
@@ -1508,7 +1526,7 @@
         Create a message informing the user that the object doesn't exist
         and return a redirect to the admin index page.
         """
-        msg = _("""%(name)s with ID "%(key)s" doesn't exist. Perhaps it was deleted?""") % {
+        msg = _('%(name)s with ID “%(key)s” doesn’t exist. Perhaps it was deleted?') % {
             'name': opts.verbose_name,
             'key': unquote(object_id),
         }
@@ -1542,13 +1560,20 @@
         else:
             obj = self.get_object(request, unquote(object_id), to_field)

-            if not self.has_view_or_change_permission(request, obj):
-                raise PermissionDenied
+            if request.method == 'POST':
+                if not self.has_change_permission(request, obj):
+                    raise PermissionDenied
+            else:
+                if not self.has_view_or_change_permission(request, obj):
+                    raise PermissionDenied

             if obj is None:
                 return self._get_obj_does_not_exist_redirect(request, opts, object_id)

-        ModelForm = self.get_form(request, obj, change=not add)
+        fieldsets = self.get_fieldsets(request, obj)
+        ModelForm = self.get_form(
+            request, obj, change=not add, fields=flatten_fieldsets(fieldsets)
+        )
         if request.method == 'POST':
             form = ModelForm(request.POST, request.FILES, instance=obj)
             form_validated = form.is_valid()
@@ -1579,12 +1604,12 @@
                 formsets, inline_instances = self._create_formsets(request, obj, change=True)

         if not add and not self.has_change_permission(request, obj):
-            readonly_fields = flatten_fieldsets(self.get_fieldsets(request, obj))
+            readonly_fields = flatten_fieldsets(fieldsets)
         else:
             readonly_fields = self.get_readonly_fields(request, obj)
         adminForm = helpers.AdminForm(
             form,
-            list(self.get_fieldsets(request, obj)),
+            list(fieldsets),
             # Clear prepopulated fields on a view-only form to avoid a crash.
             self.get_prepopulated_fields(request, obj) if add or self.has_change_permission(request, obj) else {},
             readonly_fields,
@@ -1604,6 +1629,7 @@
         context = {
             **self.admin_site.each_context(request),
             'title': title % opts.verbose_name,
+            'subtitle': str(obj) if obj else None,
             'adminform': adminForm,
             'object_id': object_id,
             'original': obj,
@@ -1627,9 +1653,6 @@

         return self.render_change_form(request, context, add=add, change=not add, obj=obj, form_url=form_url)

-    def autocomplete_view(self, request):
-        return AutocompleteJsonView.as_view(model_admin=self)(request)
-
     def add_view(self, request, form_url='', extra_context=None):
         return self.changeform_view(request, None, form_url, extra_context)

@@ -1638,7 +1661,9 @@

     def _get_edited_object_pks(self, request, prefix):
         """Return POST data values of list_editable primary keys."""
-        pk_pattern = re.compile(r'{}-\d+-{}$'.format(prefix, self.model._meta.pk.name))
+        pk_pattern = re.compile(
+            r'{}-\d+-{}$'.format(re.escape(prefix), self.model._meta.pk.name)
+        )
         return [value for key, value in request.POST.items() if pk_pattern.match(key)]

     def _get_list_editable_queryset(self, request, prefix):
@@ -1790,6 +1815,7 @@
             'selection_note': _('0 of %(cnt)s selected') % {'cnt': len(cl.result_list)},
             'selection_note_all': selection_note_all % {'total_count': cl.result_count},
             'title': cl.title,
+            'subtitle': None,
             'is_popup': cl.is_popup,
             'to_field': cl.to_field,
             'cl': cl,
@@ -1866,6 +1892,7 @@
         context = {
             **self.admin_site.each_context(request),
             'title': title,
+            'subtitle': None,
             'object_name': object_name,
             'object': obj,
             'deleted_objects': deleted_objects,
@@ -1885,6 +1912,7 @@
     def history_view(self, request, object_id, extra_context=None):
         "The 'history' admin view for this model."
         from django.contrib.admin.models import LogEntry
+
         # First check if the user can see this history.
         model = self.model
         obj = self.get_object(request, unquote(object_id))
@@ -1905,6 +1933,7 @@
         context = {
             **self.admin_site.each_context(request),
             'title': _('Change history: %s') % obj,
+            'subtitle': None,
             'action_list': action_list,
             'module_name': str(capfirst(opts.verbose_name_plural)),
             'object': obj,
@@ -2003,18 +2032,12 @@
     @property
     def media(self):
         extra = '' if settings.DEBUG else '.min'
-        js = ['vendor/jquery/jquery%s.js' % extra, 'jquery.init.js',
-              'inlines%s.js' % extra]
+        js = ['vendor/jquery/jquery%s.js' % extra, 'jquery.init.js', 'inlines.js']
         if self.filter_vertical or self.filter_horizontal:
             js.extend(['SelectBox.js', 'SelectFilter2.js'])
         if self.classes and 'collapse' in self.classes:
-            js.append('collapse%s.js' % extra)
+            js.append('collapse.js')
         return forms.Media(js=['admin/js/%s' % url for url in js])
-
-    def _has_add_permission(self, request, obj):
-        # RemovedInDjango30Warning: obj will be a required argument.
-        args = get_func_args(self.has_add_permission)
-        return self.has_add_permission(request, obj) if 'obj' in args else self.has_add_permission(request)

     def get_extra(self, request, obj=None, **kwargs):
         """Hook for customizing the number of extra inline forms."""
@@ -2061,7 +2084,7 @@

         base_model_form = defaults['form']
         can_change = self.has_change_permission(request, obj) if request else True
-        can_add = self._has_add_permission(request, obj) if request else True
+        can_add = self.has_add_permission(request, obj) if request else True

         class DeleteProtectedModelForm(base_model_form):

@@ -2144,8 +2167,7 @@
             for perm in perms
         )

-    def has_add_permission(self, request, obj=None):
-        # RemovedInDjango30Warning: obj becomes a mandatory argument.
+    def has_add_permission(self, request, obj):
         if self.opts.auto_created:
             # Auto-created intermediate models don't have their own
             # permissions. The user needs to have the change permission for the
('django/contrib/admin', 'models.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -66,21 +66,21 @@
         verbose_name = _('log entry')
         verbose_name_plural = _('log entries')
         db_table = 'django_admin_log'
-        ordering = ('-action_time',)
+        ordering = ['-action_time']

     def __repr__(self):
         return str(self.action_time)

     def __str__(self):
         if self.is_addition():
-            return gettext('Added "%(object)s".') % {'object': self.object_repr}
+            return gettext('Added “%(object)s”.') % {'object': self.object_repr}
         elif self.is_change():
-            return gettext('Changed "%(object)s" - %(changes)s') % {
+            return gettext('Changed “%(object)s” — %(changes)s') % {
                 'object': self.object_repr,
                 'changes': self.get_change_message(),
             }
         elif self.is_deletion():
-            return gettext('Deleted "%(object)s."') % {'object': self.object_repr}
+            return gettext('Deleted “%(object)s.”') % {'object': self.object_repr}

         return gettext('LogEntry Object')

@@ -108,17 +108,17 @@
                 if 'added' in sub_message:
                     if sub_message['added']:
                         sub_message['added']['name'] = gettext(sub_message['added']['name'])
-                        messages.append(gettext('Added {name} "{object}".').format(**sub_message['added']))
+                        messages.append(gettext('Added {name} “{object}”.').format(**sub_message['added']))
                     else:
                         messages.append(gettext('Added.'))

                 elif 'changed' in sub_message:
                     sub_message['changed']['fields'] = get_text_list(
-                        sub_message['changed']['fields'], gettext('and')
+                        [gettext(field_name) for field_name in sub_message['changed']['fields']], gettext('and')
                     )
                     if 'name' in sub_message['changed']:
                         sub_message['changed']['name'] = gettext(sub_message['changed']['name'])
-                        messages.append(gettext('Changed {fields} for {name} "{object}".').format(
+                        messages.append(gettext('Changed {fields} for {name} “{object}”.').format(
                             **sub_message['changed']
                         ))
                     else:
@@ -126,7 +126,7 @@

                 elif 'deleted' in sub_message:
                     sub_message['deleted']['name'] = gettext(sub_message['deleted']['name'])
-                    messages.append(gettext('Deleted {name} "{object}".').format(**sub_message['deleted']))
+                    messages.append(gettext('Deleted {name} “{object}”.').format(**sub_message['deleted']))

             change_message = ' '.join(msg[0].upper() + msg[1:] for msg in messages)
             return change_message or gettext('No fields changed.')
('django/contrib/admin', 'checks.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,4 +1,4 @@
-import warnings
+import collections
 from itertools import chain

 from django.apps import apps
@@ -10,14 +10,12 @@
 from django.core.exceptions import FieldDoesNotExist
 from django.db import models
 from django.db.models.constants import LOOKUP_SEP
-from django.db.models.expressions import Combinable, F, OrderBy
+from django.db.models.expressions import Combinable
 from django.forms.models import (
     BaseModelForm, BaseModelFormSet, _get_foreign_key,
 )
 from django.template import engines
 from django.template.backends.django import DjangoTemplates
-from django.utils.deprecation import RemovedInDjango30Warning
-from django.utils.inspect import get_func_args
 from django.utils.module_loading import import_string


@@ -61,6 +59,7 @@
     """
     Check that the admin's dependencies are correctly installed.
     """
+    from django.contrib.admin.sites import all_sites
     if not apps.is_installed('django.contrib.admin'):
         return []
     errors = []
@@ -68,7 +67,6 @@
         ('django.contrib.contenttypes', 401),
         ('django.contrib.auth', 405),
         ('django.contrib.messages', 406),
-        ('django.contrib.sessions', 407),
     )
     for app_name, error_code in app_dependencies:
         if not apps.is_installed(app_name):
@@ -108,6 +106,15 @@
                 "the admin application.",
                 id='admin.E404',
             ))
+        sidebar_enabled = any(site.enable_nav_sidebar for site in all_sites)
+        if (sidebar_enabled and 'django.template.context_processors.request'
+                not in django_templates_instance.context_processors):
+            errors.append(checks.Warning(
+                "'django.template.context_processors.request' must be enabled "
+                "in DjangoTemplates (TEMPLATES) in order to use the admin "
+                "navigation sidebar.",
+                id='admin.W411',
+            ))

     if not _contains_subclass('django.contrib.auth.middleware.AuthenticationMiddleware', settings.MIDDLEWARE):
         errors.append(checks.Error(
@@ -120,6 +127,18 @@
             "'django.contrib.messages.middleware.MessageMiddleware' must "
             "be in MIDDLEWARE in order to use the admin application.",
             id='admin.E409',
+        ))
+    if not _contains_subclass('django.contrib.sessions.middleware.SessionMiddleware', settings.MIDDLEWARE):
+        errors.append(checks.Error(
+            "'django.contrib.sessions.middleware.SessionMiddleware' must "
+            "be in MIDDLEWARE in order to use the admin application.",
+            hint=(
+                "Insert "
+                "'django.contrib.sessions.middleware.SessionMiddleware' "
+                "before "
+                "'django.contrib.auth.middleware.AuthenticationMiddleware'."
+            ),
+            id='admin.E410',
         ))
     return errors

@@ -543,10 +562,10 @@

     def _check_ordering_item(self, obj, field_name, label):
         """ Check that `ordering` refers to existing fields. """
-        if isinstance(field_name, (Combinable, OrderBy)):
-            if not isinstance(field_name, OrderBy):
+        if isinstance(field_name, (Combinable, models.OrderBy)):
+            if not isinstance(field_name, models.OrderBy):
                 field_name = field_name.asc()
-            if isinstance(field_name.expression, F):
+            if isinstance(field_name.expression, models.F):
                 field_name = field_name.expression.name
             else:
                 return []
@@ -604,8 +623,9 @@
             except FieldDoesNotExist:
                 return [
                     checks.Error(
-                        "The value of '%s' is not a callable, an attribute of '%s', or an attribute of '%s.%s'." % (
-                            label, obj.__class__.__name__, obj.model._meta.app_label, obj.model._meta.object_name
+                        "The value of '%s' is not a callable, an attribute of "
+                        "'%s', or an attribute of '%s'." % (
+                            label, obj.__class__.__name__, obj.model._meta.label,
                         ),
                         obj=obj.__class__,
                         id='admin.E035',
@@ -718,33 +738,33 @@
             return []
         elif hasattr(obj, item):
             return []
-        elif hasattr(obj.model, item):
+        try:
+            field = obj.model._meta.get_field(item)
+        except FieldDoesNotExist:
             try:
-                field = obj.model._meta.get_field(item)
-            except FieldDoesNotExist:
-                return []
-            else:
-                if isinstance(field, models.ManyToManyField):
-                    return [
-                        checks.Error(
-                            "The value of '%s' must not be a ManyToManyField." % label,
-                            obj=obj.__class__,
-                            id='admin.E109',
-                        )
-                    ]
-                return []
-        else:
-            return [
-                checks.Error(
-                    "The value of '%s' refers to '%s', which is not a callable, "
-                    "an attribute of '%s', or an attribute or method on '%s.%s'." % (
-                        label, item, obj.__class__.__name__,
-                        obj.model._meta.app_label, obj.model._meta.object_name,
-                    ),
-                    obj=obj.__class__,
-                    id='admin.E108',
-                )
-            ]
+                field = getattr(obj.model, item)
+            except AttributeError:
+                return [
+                    checks.Error(
+                        "The value of '%s' refers to '%s', which is not a "
+                        "callable, an attribute of '%s', or an attribute or "
+                        "method on '%s'." % (
+                            label, item, obj.__class__.__name__,
+                            obj.model._meta.label,
+                        ),
+                        obj=obj.__class__,
+                        id='admin.E108',
+                    )
+                ]
+        if isinstance(field, models.ManyToManyField):
+            return [
+                checks.Error(
+                    "The value of '%s' must not be a ManyToManyField." % label,
+                    obj=obj.__class__,
+                    id='admin.E109',
+                )
+            ]
+        return []

     def _check_list_display_links(self, obj):
         """ Check that list_display_links is a unique subset of list_display.
@@ -794,8 +814,7 @@
         2. ('field', SomeFieldListFilter) - a field-based list filter class
         3. SomeListFilter - a non-field list filter class
         """
-
-        from django.contrib.admin import ListFilter, FieldListFilter
+        from django.contrib.admin import FieldListFilter, ListFilter

         if callable(item) and not isinstance(item, models.Field):
             # If item is option 3, it should be a ListFilter...
@@ -983,21 +1002,25 @@

     def _check_actions_uniqueness(self, obj):
         """Check that every action has a unique __name__."""
-        names = [name for _, name, _ in obj._get_base_actions()]
-        if len(names) != len(set(names)):
-            return [checks.Error(
-                '__name__ attributes of actions defined in %s must be '
-                'unique.' % obj.__class__,
-                obj=obj.__class__,
-                id='admin.E130',
-            )]
-        return []
+        errors = []
+        names = collections.Counter(name for _, name, _ in obj._get_base_actions())
+        for name, count in names.items():
+            if count > 1:
+                errors.append(checks.Error(
+                    '__name__ attributes of actions defined in %s must be '
+                    'unique. Name %r is not unique.' % (
+                        obj.__class__.__name__,
+                        name,
+                    ),
+                    obj=obj.__class__,
+                    id='admin.E130',
+                ))
+        return errors


 class InlineModelAdminChecks(BaseModelAdminChecks):

     def check(self, inline_obj, **kwargs):
-        self._check_has_add_permission(inline_obj)
         parent_model = inline_obj.parent_model
         return [
             *super().check(inline_obj),
@@ -1028,8 +1051,8 @@
             return [
                 checks.Error(
                     "Cannot exclude the field '%s', because it is the foreign key "
-                    "to the parent model '%s.%s'." % (
-                        fk.name, parent_model._meta.app_label, parent_model._meta.object_name
+                    "to the parent model '%s'." % (
+                        fk.name, parent_model._meta.label,
                     ),
                     obj=obj.__class__,
                     id='admin.E201',
@@ -1081,20 +1104,6 @@
             return must_inherit_from(parent='BaseModelFormSet', option='formset', obj=obj, id='admin.E206')
         else:
             return []
-
-    def _check_has_add_permission(self, obj):
-        cls = obj.__class__
-        try:
-            func = cls.has_add_permission
-        except AttributeError:
-            pass
-        else:
-            args = get_func_args(func)
-            if 'obj' not in args:
-                warnings.warn(
-                    "Update %s.has_add_permission() to accept a positional "
-                    "`obj` argument." % cls.__name__, RemovedInDjango30Warning
-                )


 def must_be(type, option, obj, id):
@@ -1120,9 +1129,8 @@
 def refer_to_missing_field(field, option, obj, id):
     return [
         checks.Error(
-            "The value of '%s' refers to '%s', which is not an attribute of '%s.%s'." % (
-                option, field, obj.model._meta.app_label, obj.model._meta.object_name
-            ),
+            "The value of '%s' refers to '%s', which is not an attribute of "
+            "'%s'." % (option, field, obj.model._meta.label),
             obj=obj.__class__,
             id=id,
         ),
('django/contrib/admin', 'actions.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,12 +4,17 @@

 from django.contrib import messages
 from django.contrib.admin import helpers
+from django.contrib.admin.decorators import action
 from django.contrib.admin.utils import model_ngettext
 from django.core.exceptions import PermissionDenied
 from django.template.response import TemplateResponse
 from django.utils.translation import gettext as _, gettext_lazy


+@action(
+    permissions=['delete'],
+    description=gettext_lazy('Delete selected %(verbose_name_plural)s'),
+)
 def delete_selected(modeladmin, request, queryset):
     """
     Default action which deletes the selected objects.
@@ -73,7 +78,3 @@
         "admin/%s/delete_selected_confirmation.html" % app_label,
         "admin/delete_selected_confirmation.html"
     ], context)
-
-
-delete_selected.allowed_permissions = ('delete',)
-delete_selected.short_description = gettext_lazy("Delete selected %(verbose_name_plural)s")
('django/contrib/admin', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,12 +1,9 @@
-# ACTION_CHECKBOX_NAME is unused, but should stay since its import from here
-# has been referenced in documentation.
-from django.contrib.admin.decorators import register
+from django.contrib.admin.decorators import action, display, register
 from django.contrib.admin.filters import (
     AllValuesFieldListFilter, BooleanFieldListFilter, ChoicesFieldListFilter,
-    DateFieldListFilter, FieldListFilter, ListFilter, RelatedFieldListFilter,
-    RelatedOnlyFieldListFilter, SimpleListFilter,
+    DateFieldListFilter, EmptyFieldListFilter, FieldListFilter, ListFilter,
+    RelatedFieldListFilter, RelatedOnlyFieldListFilter, SimpleListFilter,
 )
-from django.contrib.admin.helpers import ACTION_CHECKBOX_NAME
 from django.contrib.admin.options import (
     HORIZONTAL, VERTICAL, ModelAdmin, StackedInline, TabularInline,
 )
@@ -14,16 +11,14 @@
 from django.utils.module_loading import autodiscover_modules

 __all__ = [
-    "register", "ACTION_CHECKBOX_NAME", "ModelAdmin", "HORIZONTAL", "VERTICAL",
+    "action", "display", "register", "ModelAdmin", "HORIZONTAL", "VERTICAL",
     "StackedInline", "TabularInline", "AdminSite", "site", "ListFilter",
     "SimpleListFilter", "FieldListFilter", "BooleanFieldListFilter",
     "RelatedFieldListFilter", "ChoicesFieldListFilter", "DateFieldListFilter",
-    "AllValuesFieldListFilter", "RelatedOnlyFieldListFilter", "autodiscover",
+    "AllValuesFieldListFilter", "EmptyFieldListFilter",
+    "RelatedOnlyFieldListFilter", "autodiscover",
 ]


 def autodiscover():
     autodiscover_modules('admin', register_to=site)
-
-
-default_app_config = 'django.contrib.admin.apps.AdminConfig'
('django/contrib/admin', 'apps.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -7,6 +7,7 @@
 class SimpleAdminConfig(AppConfig):
     """Simple AppConfig which does not do automatic discovery."""

+    default_auto_field = 'django.db.models.AutoField'
     default_site = 'django.contrib.admin.sites.AdminSite'
     name = 'django.contrib.admin'
     verbose_name = _("Administration")
@@ -19,6 +20,8 @@
 class AdminConfig(SimpleAdminConfig):
     """The default AppConfig for admin which does autodiscovery."""

+    default = True
+
     def ready(self):
         super().ready()
         self.module.autodiscover()
('django/contrib/admin', 'widgets.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -7,11 +7,12 @@
 from django import forms
 from django.conf import settings
 from django.core.exceptions import ValidationError
-from django.db.models.deletion import CASCADE
+from django.core.validators import URLValidator
+from django.db.models import CASCADE
 from django.urls import reverse
 from django.urls.exceptions import NoReverseMatch
 from django.utils.html import smart_urlquote
-from django.utils.safestring import mark_safe
+from django.utils.http import urlencode
 from django.utils.text import Truncator
 from django.utils.translation import get_language, gettext as _

@@ -23,17 +24,12 @@
     Note that the resulting JavaScript assumes that the jsi18n
     catalog has been loaded in the page
     """
-    @property
-    def media(self):
-        extra = '' if settings.DEBUG else '.min'
+    class Media:
         js = [
-            'vendor/jquery/jquery%s.js' % extra,
-            'jquery.init.js',
-            'core.js',
-            'SelectBox.js',
-            'SelectFilter2.js',
+            'admin/js/core.js',
+            'admin/js/SelectBox.js',
+            'admin/js/SelectFilter2.js',
         ]
-        return forms.Media(js=["admin/js/%s" % path for path in js])

     def __init__(self, verbose_name, is_stacked, attrs=None, choices=()):
         self.verbose_name = verbose_name
@@ -149,8 +145,8 @@

             params = self.url_parameters()
             if params:
-                related_url += '?' + '&amp;'.join('%s=%s' % (k, v) for k, v in params.items())
-            context['related_url'] = mark_safe(related_url)
+                related_url += '?' + urlencode(params)
+            context['related_url'] = related_url
             context['link_title'] = _('Lookup')
             # The JavaScript code looks for this class.
             context['widget']['attrs'].setdefault('class', 'vForeignKeyRawIdAdminField')
@@ -330,14 +326,21 @@
 class AdminURLFieldWidget(forms.URLInput):
     template_name = 'admin/widgets/url.html'

-    def __init__(self, attrs=None):
+    def __init__(self, attrs=None, validator_class=URLValidator):
         super().__init__(attrs={'class': 'vURLField', **(attrs or {})})
-
-    def get_context(self, name, value, attrs):
+        self.validator = validator_class()
+
+    def get_context(self, name, value, attrs):
+        try:
+            self.validator(value if value else '')
+            url_valid = True
+        except ValidationError:
+            url_valid = False
         context = super().get_context(name, value, attrs)
         context['current_label'] = _('Currently:')
         context['change_label'] = _('Change:')
         context['widget']['href'] = smart_urlquote(context['widget']['value']) if value else ''
+        context['url_valid'] = url_valid
         return context


@@ -377,18 +380,17 @@
     Renders the necessary data attributes for select2 and adds the static form
     media.
     """
-    url_name = '%s:%s_%s_autocomplete'
-
-    def __init__(self, rel, admin_site, attrs=None, choices=(), using=None):
-        self.rel = rel
+    url_name = '%s:autocomplete'
+
+    def __init__(self, field, admin_site, attrs=None, choices=(), using=None):
+        self.field = field
         self.admin_site = admin_site
         self.db = using
         self.choices = choices
         self.attrs = {} if attrs is None else attrs.copy()

     def get_url(self):
-        model = self.rel.model
-        return reverse(self.url_name % (self.admin_site.name, model._meta.app_label, model._meta.model_name))
+        return reverse(self.url_name % self.admin_site.name)

     def build_attrs(self, base_attrs, extra_attrs=None):
         """
@@ -402,8 +404,12 @@
         attrs.setdefault('class', '')
         attrs.update({
             'data-ajax--cache': 'true',
+            'data-ajax--delay': 250,
             'data-ajax--type': 'GET',
             'data-ajax--url': self.get_url(),
+            'data-app-label': self.field.model._meta.app_label,
+            'data-model-name': self.field.model._meta.model_name,
+            'data-field-name': self.field.name,
             'data-theme': 'admin-autocomplete',
             'data-allow-clear': json.dumps(not self.is_required),
             'data-placeholder': '',  # Allows clearing of the input.
@@ -422,9 +428,12 @@
         }
         if not self.is_required and not self.allow_multiple_selected:
             default[1].append(self.create_option(name, '', '', False, 0))
+        remote_model_opts = self.field.remote_field.model._meta
+        to_field_name = getattr(self.field.remote_field, 'field_name', remote_model_opts.pk.attname)
+        to_field_name = remote_model_opts.get_field(to_field_name).attname
         choices = (
-            (obj.pk, self.choices.field.label_from_instance(obj))
-            for obj in self.choices.queryset.using(self.db).filter(pk__in=selected_choices)
+            (getattr(obj, to_field_name), self.choices.field.label_from_instance(obj))
+            for obj in self.choices.queryset.using(self.db).filter(**{'%s__in' % to_field_name: selected_choices})
         )
         for option_value, option_label in choices:
             selected = (
('django/contrib/admin', 'forms.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,5 @@
-from django import forms
 from django.contrib.auth.forms import AuthenticationForm, PasswordChangeForm
+from django.core.exceptions import ValidationError
 from django.utils.translation import gettext_lazy as _


@@ -19,7 +19,7 @@
     def confirm_login_allowed(self, user):
         super().confirm_login_allowed(user)
         if not user.is_staff:
-            raise forms.ValidationError(
+            raise ValidationError(
                 self.error_messages['invalid_login'],
                 code='invalid_login',
                 params={'username': self.username_field.verbose_name}
('django/contrib/admin', 'utils.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,6 @@
 import datetime
 import decimal
-import re
+import json
 from collections import defaultdict

 from django.core.exceptions import FieldDoesNotExist
@@ -11,12 +11,13 @@
 from django.urls import NoReverseMatch, reverse
 from django.utils import formats, timezone
 from django.utils.html import format_html
+from django.utils.regex_helper import _lazy_re_compile
 from django.utils.text import capfirst
 from django.utils.translation import ngettext, override as translation_override

 QUOTE_MAP = {i: '_%02X' % i for i in b'":/_#?;@&=+$,"[]<>%\n\\'}
 UNQUOTE_MAP = {v: chr(k) for k, v in QUOTE_MAP.items()}
-UNQUOTE_RE = re.compile('_(?:%s)' % '|'.join([x[1:] for x in UNQUOTE_MAP]))
+UNQUOTE_RE = _lazy_re_compile('_(?:%s)' % '|'.join([x[1:] for x in UNQUOTE_MAP]))


 class FieldIsAForeignKeyColumnName(Exception):
@@ -74,7 +75,7 @@

 def unquote(s):
     """Undo the effects of quote()."""
-    return UNQUOTE_RE.sub(lambda m: UNQUOTE_MAP[m.group(0)], s)
+    return UNQUOTE_RE.sub(lambda m: UNQUOTE_MAP[m[0]], s)


 def flatten(fields):
@@ -181,10 +182,12 @@
             return super().collect(objs, source_attr=source_attr, **kwargs)
         except models.ProtectedError as e:
             self.protected.update(e.protected_objects)
-
-    def related_objects(self, related, objs):
-        qs = super().related_objects(related, objs)
-        return qs.select_related(related.field.name)
+        except models.RestrictedError as e:
+            self.protected.update(e.restricted_objects)
+
+    def related_objects(self, related_model, related_fields, objs):
+        qs = super().related_objects(related_model, related_fields, objs)
+        return qs.select_related(*[related_field.name for related_field in related_fields])

     def _nested(self, obj, seen, format_callback):
         if obj in seen:
@@ -335,7 +338,7 @@
             else:
                 message = "Unable to lookup '%s' on %s" % (name, model._meta.object_name)
                 if model_admin:
-                    message += " or %s" % (model_admin.__class__.__name__,)
+                    message += " or %s" % model_admin.__class__.__name__
                 if form:
                     message += " or %s" % form.__class__.__name__
                 raise AttributeError(message)
@@ -396,6 +399,11 @@
         return formats.number_format(value)
     elif isinstance(field, models.FileField) and value:
         return format_html('<a href="{}">{}</a>', value.url, value)
+    elif isinstance(field, models.JSONField) and value:
+        try:
+            return json.dumps(value, ensure_ascii=False, cls=field.encoder)
+        except TypeError:
+            return display_for_value(value, empty_value_display)
     else:
         return display_for_value(value, empty_value_display)

@@ -489,12 +497,21 @@
     Translations are deactivated so that strings are stored untranslated.
     Translation happens later on LogEntry access.
     """
+    # Evaluating `form.changed_data` prior to disabling translations is required
+    # to avoid fields affected by localization from being included incorrectly,
+    # e.g. where date formats differ such as MM/DD/YYYY vs DD/MM/YYYY.
+    changed_data = form.changed_data
+    with translation_override(None):
+        # Deactivate translations while fetching verbose_name for form
+        # field labels and using `field_name`, if verbose_name is not provided.
+        # Translations will happen later on LogEntry access.
+        changed_field_labels = _get_changed_field_labels_from_form(form, changed_data)
+
     change_message = []
     if add:
         change_message.append({'added': {}})
     elif form.changed_data:
-        change_message.append({'changed': {'fields': form.changed_data}})
-
+        change_message.append({'changed': {'fields': changed_field_labels}})
     if formsets:
         with translation_override(None):
             for formset in formsets:
@@ -510,7 +527,7 @@
                         'changed': {
                             'name': str(changed_object._meta.verbose_name),
                             'object': str(changed_object),
-                            'fields': changed_fields,
+                            'fields': _get_changed_field_labels_from_form(formset.forms[0], changed_fields),
                         }
                     })
                 for deleted_object in formset.deleted_objects:
@@ -521,3 +538,14 @@
                         }
                     })
     return change_message
+
+
+def _get_changed_field_labels_from_form(form, changed_data):
+    changed_field_labels = []
+    for field_name in changed_data:
+        try:
+            verbose_field_name = form.fields[field_name].label or field_name
+        except KeyError:
+            verbose_field_name = field_name
+        changed_field_labels.append(str(verbose_field_name))
+    return changed_field_labels
('django/contrib/admin', 'sites.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,19 +1,25 @@
+import re
 from functools import update_wrapper
 from weakref import WeakSet

 from django.apps import apps
+from django.conf import settings
 from django.contrib.admin import ModelAdmin, actions
+from django.contrib.admin.views.autocomplete import AutocompleteJsonView
 from django.contrib.auth import REDIRECT_FIELD_NAME
 from django.core.exceptions import ImproperlyConfigured
 from django.db.models.base import ModelBase
-from django.http import Http404, HttpResponseRedirect
+from django.http import (
+    Http404, HttpResponsePermanentRedirect, HttpResponseRedirect,
+)
 from django.template.response import TemplateResponse
-from django.urls import NoReverseMatch, reverse
+from django.urls import NoReverseMatch, Resolver404, resolve, reverse
 from django.utils.functional import LazyObject
 from django.utils.module_loading import import_string
 from django.utils.text import capfirst
 from django.utils.translation import gettext as _, gettext_lazy
 from django.views.decorators.cache import never_cache
+from django.views.decorators.common import no_append_slash
 from django.views.decorators.csrf import csrf_protect
 from django.views.i18n import JavaScriptCatalog

@@ -49,7 +55,9 @@
     # URL for the "View site" link at the top of each admin page.
     site_url = '/'

-    _empty_value_display = '-'
+    enable_nav_sidebar = True
+
+    empty_value_display = '-'

     login_form = None
     index_template = None
@@ -58,6 +66,8 @@
     logout_template = None
     password_change_template = None
     password_change_done_template = None
+
+    final_catch_all_view = True

     def __init__(self, name='admin'):
         self._registry = {}  # model_class class -> admin_class instance
@@ -106,7 +116,14 @@
                 )

             if model in self._registry:
-                raise AlreadyRegistered('The model %s is already registered' % model.__name__)
+                registered_admin = str(self._registry[model])
+                msg = 'The model %s is already registered ' % model.__name__
+                if registered_admin.endswith('.ModelAdmin'):
+                    # Most likely registered without a ModelAdmin subclass.
+                    msg += 'in app %r.' % re.sub(r'\.ModelAdmin$', '', registered_admin)
+                else:
+                    msg += 'with %r.' % registered_admin
+                raise AlreadyRegistered(msg)

             # Ignore the registration if the model has been
             # swapped out.
@@ -169,14 +186,6 @@
         Get all the enabled actions as an iterable of (name, func).
         """
         return self._actions.items()
-
-    @property
-    def empty_value_display(self):
-        return self._empty_value_display
-
-    @empty_value_display.setter
-    def empty_value_display(self, empty_value_display):
-        self._empty_value_display = empty_value_display

     def has_permission(self, request):
         """
@@ -230,11 +239,11 @@
         return update_wrapper(inner, view)

     def get_urls(self):
-        from django.urls import include, path, re_path
         # Since this module gets imported in the application's root package,
         # it cannot import models from other applications at the module level,
         # and django.contrib.contenttypes.views imports ContentType.
         from django.contrib.contenttypes import views as contenttype_views
+        from django.urls import include, path, re_path

         def wrap(view, cacheable=False):
             def wrapper(*args, **kwargs):
@@ -253,6 +262,7 @@
                 wrap(self.password_change_done, cacheable=True),
                 name='password_change_done',
             ),
+            path('autocomplete/', wrap(self.autocomplete_view), name='autocomplete'),
             path('jsi18n/', wrap(self.i18n_javascript, cacheable=True), name='jsi18n'),
             path(
                 'r/<int:content_type_id>/<path:object_id>/',
@@ -278,6 +288,10 @@
             urlpatterns += [
                 re_path(regex, wrap(self.app_index), name='app_list'),
             ]
+
+        if self.final_catch_all_view:
+            urlpatterns.append(re_path(r'(?P<url>.*)$', wrap(self.catch_all_view)))
+
         return urlpatterns

     @property
@@ -301,6 +315,7 @@
             'has_permission': self.has_permission(request),
             'available_apps': self.get_app_list(request),
             'is_popup': False,
+            'is_nav_sidebar_enabled': self.enable_nav_sidebar,
         }

     def password_change(self, request, extra_context=None):
@@ -374,11 +389,11 @@
             index_path = reverse('admin:index', current_app=self.name)
             return HttpResponseRedirect(index_path)

-        from django.contrib.auth.views import LoginView
         # Since this module gets imported in the application's root package,
         # it cannot import models from other applications at the module level,
         # and django.contrib.admin.forms eventually imports User.
         from django.contrib.admin.forms import AdminAuthenticationForm
+        from django.contrib.auth.views import LoginView
         context = {
             **self.each_context(request),
             'title': _('Log in'),
@@ -397,6 +412,22 @@
         }
         request.current_app = self.name
         return LoginView.as_view(**defaults)(request)
+
+    def autocomplete_view(self, request):
+        return AutocompleteJsonView.as_view(admin_site=self)(request)
+
+    @no_append_slash
+    def catch_all_view(self, request, url):
+        if settings.APPEND_SLASH and not url.endswith('/'):
+            urlconf = getattr(request, 'urlconf', None)
+            try:
+                match = resolve('%s/' % request.path_info, urlconf)
+            except Resolver404:
+                pass
+            else:
+                if getattr(match.func, 'should_append_slash', True):
+                    return HttpResponsePermanentRedirect('%s/' % request.path)
+        raise Http404

     def _build_app_dict(self, request, label=None):
         """
@@ -493,6 +524,7 @@
         context = {
             **self.each_context(request),
             'title': self.index_title,
+            'subtitle': None,
             'app_list': app_list,
             **(extra_context or {}),
         }
@@ -507,10 +539,10 @@
             raise Http404('The requested admin page does not exist.')
         # Sort the models alphabetically within each app.
         app_dict['models'].sort(key=lambda x: x['name'])
-        app_name = apps.get_app_config(app_label).verbose_name
         context = {
             **self.each_context(request),
-            'title': _('%(app)s administration') % {'app': app_name},
+            'title': _('%(app)s administration') % {'app': app_dict['name']},
+            'subtitle': None,
             'app_list': [app_dict],
             'app_label': app_label,
             **(extra_context or {}),
('django/contrib/admin', 'tests.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,5 @@
+from contextlib import contextmanager
+
 from django.contrib.staticfiles.testing import StaticLiveServerTestCase
 from django.test import modify_settings
 from django.test.selenium import SeleniumTestCase
@@ -8,7 +10,7 @@
 class CSPMiddleware(MiddlewareMixin):
     """The admin's JavaScript should be compatible with CSP."""
     def process_response(self, request, response):
-        response['Content-Security-Policy'] = "default-src 'self'"
+        response.headers['Content-Security-Policy'] = "default-src 'self'"
         return response


@@ -33,12 +35,15 @@
         from selenium.webdriver.support.wait import WebDriverWait
         WebDriverWait(self.selenium, timeout).until(callback)

-    def wait_for_popup(self, num_windows=2, timeout=10):
+    def wait_for_and_switch_to_popup(self, num_windows=2, timeout=10):
         """
-        Block until `num_windows` are present (usually 2, but can be
-        overridden in the case of pop-ups opening other pop-ups).
+        Block until `num_windows` are present and are ready (usually 2, but can
+        be overridden in the case of pop-ups opening other pop-ups). Switch the
+        current window to the new pop-up.
         """
         self.wait_until(lambda d: len(d.window_handles) == num_windows, timeout)
+        self.selenium.switch_to.window(self.selenium.window_handles[-1])
+        self.wait_page_ready()

     def wait_for(self, css_selector, timeout=10):
         """
@@ -97,19 +102,26 @@
             timeout
         )

-    def wait_page_loaded(self):
+    def wait_page_ready(self, timeout=10):
         """
-        Block until page has started to load.
+        Block until the  page is ready.
         """
-        from selenium.common.exceptions import TimeoutException
-        try:
-            # Wait for the next page to be loaded
-            self.wait_for('body')
-        except TimeoutException:
-            # IE7 occasionally returns an error "Internet Explorer cannot
-            # display the webpage" and doesn't load the next page. We just
-            # ignore it.
-            pass
+        self.wait_until(
+            lambda driver: driver.execute_script('return document.readyState;') == 'complete',
+            timeout,
+        )
+
+    @contextmanager
+    def wait_page_loaded(self, timeout=10):
+        """
+        Block until a new page has loaded and is ready.
+        """
+        from selenium.webdriver.support import expected_conditions as ec
+        old_page = self.selenium.find_element_by_tag_name('html')
+        yield
+        # Wait for the next page to be loaded
+        self.wait_until(ec.staleness_of(old_page), timeout=timeout)
+        self.wait_page_ready(timeout=timeout)

     def admin_login(self, username, password, login_url='/admin/'):
         """
@@ -121,29 +133,26 @@
         password_input = self.selenium.find_element_by_name('password')
         password_input.send_keys(password)
         login_text = _('Log in')
-        self.selenium.find_element_by_xpath(
-            '//input[@value="%s"]' % login_text).click()
-        self.wait_page_loaded()
+        with self.wait_page_loaded():
+            self.selenium.find_element_by_xpath('//input[@value="%s"]' % login_text).click()

-    def get_css_value(self, selector, attribute):
+    def select_option(self, selector, value):
         """
-        Return the value for the CSS attribute of a DOM element specified by
-        the given selector. Uses the jQuery that ships with Django.
-        """
-        return self.selenium.execute_script(
-            'return django.jQuery("%s").css("%s")' % (selector, attribute))
-
-    def get_select_option(self, selector, value):
-        """
-        Return the <OPTION> with the value `value` inside the <SELECT> widget
+        Select the <OPTION> with the value `value` inside the <SELECT> widget
         identified by the CSS selector `selector`.
         """
-        from selenium.common.exceptions import NoSuchElementException
-        options = self.selenium.find_elements_by_css_selector('%s > option' % selector)
-        for option in options:
-            if option.get_attribute('value') == value:
-                return option
-        raise NoSuchElementException('Option "%s" not found in "%s"' % (value, selector))
+        from selenium.webdriver.support.ui import Select
+        select = Select(self.selenium.find_element_by_css_selector(selector))
+        select.select_by_value(value)
+
+    def deselect_option(self, selector, value):
+        """
+        Deselect the <OPTION> with the value `value` inside the <SELECT> widget
+        identified by the CSS selector `selector`.
+        """
+        from selenium.webdriver.support.ui import Select
+        select = Select(self.selenium.find_element_by_css_selector(selector))
+        select.deselect_by_value(value)

     def _assertOptionsValues(self, options_selector, values):
         if values:
('django/contrib/admin', 'helpers.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,15 +1,17 @@
 import json

 from django import forms
-from django.conf import settings
 from django.contrib.admin.utils import (
     display_for_field, flatten_fieldsets, help_text_for_field, label_for_field,
-    lookup_field,
+    lookup_field, quote,
 )
 from django.core.exceptions import ObjectDoesNotExist
-from django.db.models.fields.related import ManyToManyRel
+from django.db.models.fields.related import (
+    ForeignObjectRel, ManyToManyRel, OneToOneField,
+)
 from django.forms.utils import flatatt
 from django.template.defaultfilters import capfirst, linebreaksbr
+from django.urls import NoReverseMatch, reverse
 from django.utils.html import conditional_escape, format_html
 from django.utils.safestring import mark_safe
 from django.utils.translation import gettext, gettext_lazy as _
@@ -80,8 +82,7 @@
     @property
     def media(self):
         if 'collapse' in self.classes:
-            extra = '' if settings.DEBUG else '.min'
-            return forms.Media(js=['admin/js/collapse%s.js' % extra])
+            return forms.Media(js=['admin/js/collapse.js'])
         return forms.Media()

     def __iter__(self):
@@ -187,7 +188,22 @@
         if not self.is_first:
             attrs["class"] = "inline"
         label = self.field['label']
-        return format_html('<label{}>{}:</label>', flatatt(attrs), capfirst(label))
+        return format_html('<label{}>{}{}</label>', flatatt(attrs), capfirst(label), self.form.label_suffix)
+
+    def get_admin_url(self, remote_field, remote_obj):
+        url_name = 'admin:%s_%s_change' % (
+            remote_field.model._meta.app_label,
+            remote_field.model._meta.model_name,
+        )
+        try:
+            url = reverse(
+                url_name,
+                args=[quote(remote_obj.pk)],
+                current_app=self.model_admin.admin_site.name,
+            )
+            return format_html('<a href="{}">{}</a>', url, remote_obj)
+        except NoReverseMatch:
+            return str(remote_obj)

     def contents(self):
         from django.contrib.admin.templatetags.admin_list import _boolean_icon
@@ -214,6 +230,11 @@
             else:
                 if isinstance(f.remote_field, ManyToManyRel) and value is not None:
                     result_repr = ", ".join(map(str, value.all()))
+                elif (
+                    isinstance(f.remote_field, (ForeignObjectRel, OneToOneField)) and
+                    value is not None
+                ):
+                    result_repr = self.get_admin_url(f.remote_field, value)
                 else:
                     result_repr = display_for_field(value, f, self.empty_value_display)
                 result_repr = linebreaksbr(result_repr)
@@ -280,7 +301,12 @@
             if not self.has_change_permission or field_name in self.readonly_fields:
                 yield {
                     'name': field_name,
-                    'label': meta_labels.get(field_name) or label_for_field(field_name, self.opts.model, self.opts),
+                    'label': meta_labels.get(field_name) or label_for_field(
+                        field_name,
+                        self.opts.model,
+                        self.opts,
+                        form=empty_form,
+                    ),
                     'widget': {'is_hidden': False},
                     'required': False,
                     'help_text': meta_help_texts.get(field_name) or help_text_for_field(field_name, self.opts.model),
@@ -289,7 +315,7 @@
                 form_field = empty_form.fields[field_name]
                 label = form_field.label
                 if label is None:
-                    label = label_for_field(field_name, self.opts.model, self.opts)
+                    label = label_for_field(field_name, self.opts.model, self.opts, form=empty_form)
                 yield {
                     'name': field_name,
                     'label': label,
('django/contrib/admin', 'filters.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -193,11 +193,17 @@
     def expected_parameters(self):
         return [self.lookup_kwarg, self.lookup_kwarg_isnull]

-    def field_choices(self, field, request, model_admin):
-        ordering = ()
+    def field_admin_ordering(self, field, request, model_admin):
+        """
+        Return the model admin's ordering for related field, if provided.
+        """
         related_admin = model_admin.admin_site._registry.get(field.remote_field.model)
         if related_admin is not None:
-            ordering = related_admin.get_ordering(request)
+            return related_admin.get_ordering(request)
+        return ()
+
+    def field_choices(self, field, request, model_admin):
+        ordering = self.field_admin_ordering(field, request, model_admin)
         return field.get_choices(include_blank=False, ordering=ordering)

     def choices(self, changelist):
@@ -238,10 +244,12 @@
         return [self.lookup_kwarg, self.lookup_kwarg2]

     def choices(self, changelist):
+        field_choices = dict(self.field.flatchoices)
         for lookup, title in (
-                (None, _('All')),
-                ('1', _('Yes')),
-                ('0', _('No'))):
+            (None, _('All')),
+            ('1', field_choices.get(True, _('Yes'))),
+            ('0', field_choices.get(False, _('No'))),
+        ):
             yield {
                 'selected': self.lookup_val == lookup and not self.lookup_val2,
                 'query_string': changelist.get_query_string({self.lookup_kwarg: lookup}, [self.lookup_kwarg2]),
@@ -251,7 +259,7 @@
             yield {
                 'selected': self.lookup_val2 == 'True',
                 'query_string': changelist.get_query_string({self.lookup_kwarg2: 'True'}, [self.lookup_kwarg]),
-                'display': _('Unknown'),
+                'display': field_choices.get(None, _('Unknown')),
             }


@@ -419,4 +427,50 @@
 class RelatedOnlyFieldListFilter(RelatedFieldListFilter):
     def field_choices(self, field, request, model_admin):
         pk_qs = model_admin.get_queryset(request).distinct().values_list('%s__pk' % self.field_path, flat=True)
-        return field.get_choices(include_blank=False, limit_choices_to={'pk__in': pk_qs})
+        ordering = self.field_admin_ordering(field, request, model_admin)
+        return field.get_choices(include_blank=False, limit_choices_to={'pk__in': pk_qs}, ordering=ordering)
+
+
+class EmptyFieldListFilter(FieldListFilter):
+    def __init__(self, field, request, params, model, model_admin, field_path):
+        if not field.empty_strings_allowed and not field.null:
+            raise ImproperlyConfigured(
+                "The list filter '%s' cannot be used with field '%s' which "
+                "doesn't allow empty strings and nulls." % (
+                    self.__class__.__name__,
+                    field.name,
+                )
+            )
+        self.lookup_kwarg = '%s__isempty' % field_path
+        self.lookup_val = params.get(self.lookup_kwarg)
+        super().__init__(field, request, params, model, model_admin, field_path)
+
+    def queryset(self, request, queryset):
+        if self.lookup_kwarg not in self.used_parameters:
+            return queryset
+        if self.lookup_val not in ('0', '1'):
+            raise IncorrectLookupParameters
+
+        lookup_condition = models.Q()
+        if self.field.empty_strings_allowed:
+            lookup_condition |= models.Q(**{self.field_path: ''})
+        if self.field.null:
+            lookup_condition |= models.Q(**{'%s__isnull' % self.field_path: True})
+        if self.lookup_val == '1':
+            return queryset.filter(lookup_condition)
+        return queryset.exclude(lookup_condition)
+
+    def expected_parameters(self):
+        return [self.lookup_kwarg]
+
+    def choices(self, changelist):
+        for lookup, title in (
+            (None, _('All')),
+            ('1', _('Empty')),
+            ('0', _('Not empty')),
+        ):
+            yield {
+                'selected': self.lookup_val == lookup,
+                'query_string': changelist.get_query_string({self.lookup_kwarg: lookup}),
+                'display': title,
+            }
('django/contrib/admin', 'decorators.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,76 @@
+def action(function=None, *, permissions=None, description=None):
+    """
+    Conveniently add attributes to an action function::
+
+        @admin.action(
+            permissions=['publish'],
+            description='Mark selected stories as published',
+        )
+        def make_published(self, request, queryset):
+            queryset.update(status='p')
+
+    This is equivalent to setting some attributes (with the original, longer
+    names) on the function directly::
+
+        def make_published(self, request, queryset):
+            queryset.update(status='p')
+        make_published.allowed_permissions = ['publish']
+        make_published.short_description = 'Mark selected stories as published'
+    """
+    def decorator(func):
+        if permissions is not None:
+            func.allowed_permissions = permissions
+        if description is not None:
+            func.short_description = description
+        return func
+    if function is None:
+        return decorator
+    else:
+        return decorator(function)
+
+
+def display(function=None, *, boolean=None, ordering=None, description=None, empty_value=None):
+    """
+    Conveniently add attributes to a display function::
+
+        @admin.display(
+            boolean=True,
+            ordering='-publish_date',
+            description='Is Published?',
+        )
+        def is_published(self, obj):
+            return obj.publish_date is not None
+
+    This is equivalent to setting some attributes (with the original, longer
+    names) on the function directly::
+
+        def is_published(self, obj):
+            return obj.publish_date is not None
+        is_published.boolean = True
+        is_published.admin_order_field = '-publish_date'
+        is_published.short_description = 'Is Published?'
+    """
+    def decorator(func):
+        if boolean is not None and empty_value is not None:
+            raise ValueError(
+                'The boolean and empty_value arguments to the @display '
+                'decorator are mutually exclusive.'
+            )
+        if boolean is not None:
+            func.boolean = boolean
+        if ordering is not None:
+            func.admin_order_field = ordering
+        if description is not None:
+            func.short_description = description
+        if empty_value is not None:
+            func.empty_value_display = empty_value
+        return func
+    if function is None:
+        return decorator
+    else:
+        return decorator(function)
+
+
 def register(*models, site=None):
     """
     Register the given model(s) classes and wrapped ModelAdmin class with
@@ -10,7 +83,7 @@
     The `site` kwarg is an admin site to use instead of the default admin site.
     """
     from django.contrib.admin import ModelAdmin
-    from django.contrib.admin.sites import site as default_site, AdminSite
+    from django.contrib.admin.sites import AdminSite, site as default_site

     def _model_admin_wrapper(admin_class):
         if not models:
('django/contrib/admin/templatetags', 'admin_list.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,7 +2,8 @@

 from django.contrib.admin.templatetags.admin_urls import add_preserved_filters
 from django.contrib.admin.utils import (
-    display_for_field, display_for_value, label_for_field, lookup_field,
+    display_for_field, display_for_value, get_fields_from_path,
+    label_for_field, lookup_field,
 )
 from django.contrib.admin.views.main import (
     ALL_VAR, ORDER_VAR, PAGE_VAR, SEARCH_VAR,
@@ -13,7 +14,7 @@
 from django.template.loader import get_template
 from django.templatetags.static import static
 from django.urls import NoReverseMatch
-from django.utils import formats
+from django.utils import formats, timezone
 from django.utils.html import format_html
 from django.utils.safestring import mark_safe
 from django.utils.text import capfirst
@@ -23,24 +24,22 @@

 register = Library()

-DOT = '.'
-

 @register.simple_tag
 def paginator_number(cl, i):
     """
     Generate an individual page index link in a paginated list.
     """
-    if i == DOT:
-        return '… '
+    if i == cl.paginator.ELLIPSIS:
+        return format_html('{} ', cl.paginator.ELLIPSIS)
     elif i == cl.page_num:
-        return format_html('<span class="this-page">{}</span> ', i + 1)
+        return format_html('<span class="this-page">{}</span> ', i)
     else:
         return format_html(
             '<a href="{}"{}>{}</a> ',
             cl.get_query_string({PAGE_VAR: i}),
-            mark_safe(' class="end"' if i == cl.paginator.num_pages - 1 else ''),
-            i + 1,
+            mark_safe(' class="end"' if i == cl.paginator.num_pages else ''),
+            i,
         )


@@ -48,39 +47,8 @@
     """
     Generate the series of links to the pages in a paginated list.
     """
-    paginator, page_num = cl.paginator, cl.page_num
-
     pagination_required = (not cl.show_all or not cl.can_show_all) and cl.multi_page
-    if not pagination_required:
-        page_range = []
-    else:
-        ON_EACH_SIDE = 3
-        ON_ENDS = 2
-
-        # If there are 10 or fewer pages, display links to every page.
-        # Otherwise, do some fancy
-        if paginator.num_pages <= 10:
-            page_range = range(paginator.num_pages)
-        else:
-            # Insert "smart" pagination links, so that there are always ON_ENDS
-            # links at either end of the list of pages, and there are always
-            # ON_EACH_SIDE links at either end of the "current page" link.
-            page_range = []
-            if page_num > (ON_EACH_SIDE + ON_ENDS):
-                page_range += [
-                    *range(0, ON_ENDS), DOT,
-                    *range(page_num - ON_EACH_SIDE, page_num + 1),
-                ]
-            else:
-                page_range.extend(range(0, page_num + 1))
-            if page_num < (paginator.num_pages - ON_EACH_SIDE - ON_ENDS - 1):
-                page_range += [
-                    *range(page_num + 1, page_num + ON_EACH_SIDE + 1), DOT,
-                    *range(paginator.num_pages - ON_ENDS, paginator.num_pages)
-                ]
-            else:
-                page_range.extend(range(page_num + 1, paginator.num_pages))
-
+    page_range = cl.paginator.get_elided_page_range(cl.page_num) if pagination_required else []
     need_show_all_link = cl.can_show_all and not cl.show_all and cl.multi_page
     return {
         'cl': cl,
@@ -128,6 +96,9 @@
                 continue

             admin_order_field = getattr(attr, "admin_order_field", None)
+            # Set ordering for attr that is a property, if defined.
+            if isinstance(attr, property) and hasattr(attr, 'fget'):
+                admin_order_field = getattr(attr.fget, 'admin_order_field', None)
             if not admin_order_field:
                 is_field_sortable = False

@@ -249,8 +220,6 @@
                     result_repr = display_for_field(value, f, empty_value_display)
                 if isinstance(f, (models.DateField, models.TimeField, models.ForeignKey)):
                     row_classes.append('nowrap')
-        if str(result_repr) == '':
-            result_repr = mark_safe('&nbsp;')
         row_class = mark_safe(' class="%s"' % ' '.join(row_classes))
         # If list_display_links not defined, add the link tag to the first field
         if link_in_col(first, field_name, cl):
@@ -356,6 +325,13 @@
     """
     if cl.date_hierarchy:
         field_name = cl.date_hierarchy
+        field = get_fields_from_path(cl.model, field_name)[-1]
+        if isinstance(field, models.DateTimeField):
+            dates_or_datetimes = 'datetimes'
+            qs_kwargs = {'is_dst': True}
+        else:
+            dates_or_datetimes = 'dates'
+            qs_kwargs = {}
         year_field = '%s__year' % field_name
         month_field = '%s__month' % field_name
         day_field = '%s__day' % field_name
@@ -372,6 +348,11 @@
             date_range = cl.queryset.aggregate(first=models.Min(field_name),
                                                last=models.Max(field_name))
             if date_range['first'] and date_range['last']:
+                if dates_or_datetimes == 'datetimes':
+                    date_range = {
+                        k: timezone.localtime(v) if timezone.is_aware(v) else v
+                        for k, v in date_range.items()
+                    }
                 if date_range['first'].year == date_range['last'].year:
                     year_lookup = date_range['first'].year
                     if date_range['first'].month == date_range['last'].month:
@@ -388,7 +369,7 @@
                 'choices': [{'title': capfirst(formats.date_format(day, 'MONTH_DAY_FORMAT'))}]
             }
         elif year_lookup and month_lookup:
-            days = getattr(cl.queryset, 'dates')(field_name, 'day')
+            days = getattr(cl.queryset, dates_or_datetimes)(field_name, 'day', **qs_kwargs)
             return {
                 'show': True,
                 'back': {
@@ -401,7 +382,7 @@
                 } for day in days]
             }
         elif year_lookup:
-            months = getattr(cl.queryset, 'dates')(field_name, 'month')
+            months = getattr(cl.queryset, dates_or_datetimes)(field_name, 'month', **qs_kwargs)
             return {
                 'show': True,
                 'back': {
@@ -414,7 +395,7 @@
                 } for month in months]
             }
         else:
-            years = getattr(cl.queryset, 'dates')(field_name, 'year')
+            years = getattr(cl.queryset, dates_or_datetimes)(field_name, 'year', **qs_kwargs)
             return {
                 'show': True,
                 'back': None,
('django/contrib/admin/templatetags', 'admin_modify.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -54,12 +54,20 @@
     is_popup = context['is_popup']
     save_as = context['save_as']
     show_save = context.get('show_save', True)
+    show_save_and_add_another = context.get('show_save_and_add_another', True)
     show_save_and_continue = context.get('show_save_and_continue', True)
     has_add_permission = context['has_add_permission']
     has_change_permission = context['has_change_permission']
     has_view_permission = context['has_view_permission']
     has_editable_inline_admin_formsets = context['has_editable_inline_admin_formsets']
     can_save = (has_change_permission and change) or (has_add_permission and add) or has_editable_inline_admin_formsets
+    can_save_and_add_another = (
+        has_add_permission and
+        not is_popup and
+        (not save_as or add) and
+        can_save and
+        show_save_and_add_another
+    )
     can_save_and_continue = not is_popup and can_save and has_view_permission and show_save_and_continue
     can_change = has_change_permission or has_editable_inline_admin_formsets
     ctx = Context(context)
@@ -70,10 +78,7 @@
             change and context.get('show_delete', True)
         ),
         'show_save_as_new': not is_popup and has_change_permission and change and save_as,
-        'show_save_and_add_another': (
-            has_add_permission and not is_popup and
-            (not save_as or add) and can_save
-        ),
+        'show_save_and_add_another': can_save_and_add_another,
         'show_save_and_continue': can_save_and_continue,
         'show_save': show_save and can_save,
         'show_close': not(show_save and can_save)
('django/contrib/admin/templatetags', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -28,6 +28,6 @@
         context.render_context[self] = context.template.engine.select_template([
             'admin/%s/%s/%s' % (app_label, object_name, self.template_name),
             'admin/%s/%s' % (app_label, self.template_name),
-            'admin/%s' % (self.template_name,),
+            'admin/%s' % self.template_name,
         ])
         return super().render(context)
('django/contrib/admin/migrations', '0001_initial.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -34,7 +34,7 @@
                 )),
             ],
             options={
-                'ordering': ('-action_time',),
+                'ordering': ['-action_time'],
                 'db_table': 'django_admin_log',
                 'verbose_name': 'log entry',
                 'verbose_name_plural': 'log entries',
('django/contrib/admin/views', 'autocomplete.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,5 @@
+from django.apps import apps
+from django.core.exceptions import FieldDoesNotExist, PermissionDenied
 from django.http import Http404, JsonResponse
 from django.views.generic.list import BaseListView

@@ -5,7 +7,7 @@
 class AutocompleteJsonView(BaseListView):
     """Handle AutocompleteWidget's AJAX requests for data."""
     paginate_by = 20
-    model_admin = None
+    admin_site = None

     def get(self, request, *args, **kwargs):
         """
@@ -15,21 +17,16 @@
             pagination: {more: true}
         }
         """
-        if not self.model_admin.get_search_fields(request):
-            raise Http404(
-                '%s must have search_fields for the autocomplete_view.' %
-                type(self.model_admin).__name__
-            )
+        self.term, self.model_admin, self.source_field, to_field_name = self.process_request(request)
+
         if not self.has_perm(request):
-            return JsonResponse({'error': '403 Forbidden'}, status=403)
+            raise PermissionDenied

-        self.term = request.GET.get('term', '')
-        self.paginator_class = self.model_admin.paginator
         self.object_list = self.get_queryset()
         context = self.get_context_data()
         return JsonResponse({
             'results': [
-                {'id': str(obj.pk), 'text': str(obj)}
+                {'id': str(getattr(obj, to_field_name)), 'text': str(obj)}
                 for obj in context['object_list']
             ],
             'pagination': {'more': context['page_obj'].has_next()},
@@ -42,11 +39,64 @@
     def get_queryset(self):
         """Return queryset based on ModelAdmin.get_search_results()."""
         qs = self.model_admin.get_queryset(self.request)
+        qs = qs.complex_filter(self.source_field.get_limit_choices_to())
         qs, search_use_distinct = self.model_admin.get_search_results(self.request, qs, self.term)
         if search_use_distinct:
             qs = qs.distinct()
         return qs

+    def process_request(self, request):
+        """
+        Validate request integrity, extract and return request parameters.
+
+        Since the subsequent view permission check requires the target model
+        admin, which is determined here, raise PermissionDenied if the
+        requested app, model or field are malformed.
+
+        Raise Http404 if the target model admin is not configured properly with
+        search_fields.
+        """
+        term = request.GET.get('term', '')
+        try:
+            app_label = request.GET['app_label']
+            model_name = request.GET['model_name']
+            field_name = request.GET['field_name']
+        except KeyError as e:
+            raise PermissionDenied from e
+
+        # Retrieve objects from parameters.
+        try:
+            source_model = apps.get_model(app_label, model_name)
+        except LookupError as e:
+            raise PermissionDenied from e
+
+        try:
+            source_field = source_model._meta.get_field(field_name)
+        except FieldDoesNotExist as e:
+            raise PermissionDenied from e
+        try:
+            remote_model = source_field.remote_field.model
+        except AttributeError as e:
+            raise PermissionDenied from e
+        try:
+            model_admin = self.admin_site._registry[remote_model]
+        except KeyError as e:
+            raise PermissionDenied from e
+
+        # Validate suitability of objects.
+        if not model_admin.get_search_fields(request):
+            raise Http404(
+                '%s must have search_fields for the autocomplete_view.' %
+                type(model_admin).__qualname__
+            )
+
+        to_field_name = getattr(source_field.remote_field, 'field_name', remote_model._meta.pk.attname)
+        to_field_name = remote_model._meta.get_field(to_field_name).attname
+        if not model_admin.to_field_allowed(request, to_field_name):
+            raise PermissionDenied
+
+        return term, model_admin, source_field, to_field_name
+
     def has_perm(self, request, obj=None):
         """Check if user has permission to access the related model."""
         return self.model_admin.has_view_permission(request, obj=obj)
('django/contrib/admin/views', 'main.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,7 +1,8 @@
-from collections import OrderedDict
 from datetime import datetime, timedelta

+from django import forms
 from django.conf import settings
+from django.contrib import messages
 from django.contrib.admin import FieldListFilter
 from django.contrib.admin.exceptions import (
     DisallowedModelAdminLookup, DisallowedModelAdminToField,
@@ -16,8 +17,8 @@
     FieldDoesNotExist, ImproperlyConfigured, SuspiciousOperation,
 )
 from django.core.paginator import InvalidPage
-from django.db import models
-from django.db.models.expressions import Combinable, F, OrderBy
+from django.db.models import Exists, F, Field, ManyToOneRel, OrderBy, OuterRef
+from django.db.models.expressions import Combinable
 from django.urls import reverse
 from django.utils.http import urlencode
 from django.utils.timezone import make_aware
@@ -35,7 +36,18 @@
     ALL_VAR, ORDER_VAR, ORDER_TYPE_VAR, SEARCH_VAR, IS_POPUP_VAR, TO_FIELD_VAR)


+class ChangeListSearchForm(forms.Form):
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        # Populate "fields" dynamically because SEARCH_VAR is a variable:
+        self.fields = {
+            SEARCH_VAR: forms.CharField(required=False, strip=False),
+        }
+
+
 class ChangeList:
+    search_form_class = ChangeListSearchForm
+
     def __init__(self, request, model, list_display, list_display_links,
                  list_filter, date_hierarchy, search_fields, list_select_related,
                  list_per_page, list_max_show_all, list_editable, model_admin, sortable_by):
@@ -47,6 +59,8 @@
         self.list_display_links = list_display_links
         self.list_filter = list_filter
         self.has_filters = None
+        self.has_active_filters = None
+        self.clear_all_filters_qs = None
         self.date_hierarchy = date_hierarchy
         self.search_fields = search_fields
         self.list_select_related = list_select_related
@@ -57,10 +71,15 @@
         self.sortable_by = sortable_by

         # Get search parameters from the query string.
+        _search_form = self.search_form_class(request.GET)
+        if not _search_form.is_valid():
+            for error in _search_form.errors.values():
+                messages.error(request, ', '.join(error))
+        self.query = _search_form.cleaned_data.get(SEARCH_VAR) or ''
         try:
-            self.page_num = int(request.GET.get(PAGE_VAR, 0))
+            self.page_num = int(request.GET.get(PAGE_VAR, 1))
         except ValueError:
-            self.page_num = 0
+            self.page_num = 1
         self.show_all = ALL_VAR in request.GET
         self.is_popup = IS_POPUP_VAR in request.GET
         to_field = request.GET.get(TO_FIELD_VAR)
@@ -77,7 +96,6 @@
             self.list_editable = ()
         else:
             self.list_editable = list_editable
-        self.query = request.GET.get(SEARCH_VAR, '')
         self.queryset = self.get_queryset(request)
         self.get_results(request)
         if self.is_popup:
@@ -104,7 +122,8 @@

     def get_filters(self, request):
         lookup_params = self.get_filters_params()
-        use_distinct = False
+        may_have_duplicates = False
+        has_active_filters = False

         for key, value in lookup_params.items():
             if not self.model_admin.lookup_allowed(key, value):
@@ -112,6 +131,7 @@

         filter_specs = []
         for list_filter in self.list_filter:
+            lookup_params_count = len(lookup_params)
             if callable(list_filter):
                 # This is simply a custom list filter class.
                 spec = list_filter(request, lookup_params, self.model, self.model_admin)
@@ -125,11 +145,10 @@
                     # FieldListFilter class that has been registered for the
                     # type of the given field.
                     field, field_list_filter_class = list_filter, FieldListFilter.create
-                if not isinstance(field, models.Field):
+                if not isinstance(field, Field):
                     field_path = field
                     field = get_fields_from_path(self.model, field_path)[-1]

-                lookup_params_count = len(lookup_params)
                 spec = field_list_filter_class(
                     field, request, lookup_params,
                     self.model, self.model_admin, field_path=field_path,
@@ -138,9 +157,11 @@
                 # processes. If that happened, check if distinct() is needed to
                 # remove duplicate results.
                 if lookup_params_count > len(lookup_params):
-                    use_distinct = use_distinct or lookup_needs_distinct(self.lookup_opts, field_path)
+                    may_have_duplicates |= lookup_needs_distinct(self.lookup_opts, field_path)
             if spec and spec.has_output():
                 filter_specs.append(spec)
+                if lookup_params_count > len(lookup_params):
+                    has_active_filters = True

         if self.date_hierarchy:
             # Create bounded lookup parameters so that the query is more
@@ -157,8 +178,6 @@
                     )
                 except ValueError as e:
                     raise IncorrectLookupParameters(e) from e
-                if settings.USE_TZ:
-                    from_date = make_aware(from_date)
                 if day:
                     to_date = from_date + timedelta(days=1)
                 elif month:
@@ -167,6 +186,9 @@
                     to_date = (from_date + timedelta(days=32)).replace(day=1)
                 else:
                     to_date = from_date.replace(year=from_date.year + 1)
+                if settings.USE_TZ:
+                    from_date = make_aware(from_date)
+                    to_date = make_aware(to_date)
                 lookup_params.update({
                     '%s__gte' % self.date_hierarchy: from_date,
                     '%s__lt' % self.date_hierarchy: to_date,
@@ -181,8 +203,11 @@
         try:
             for key, value in lookup_params.items():
                 lookup_params[key] = prepare_lookup_value(key, value)
-                use_distinct = use_distinct or lookup_needs_distinct(self.lookup_opts, key)
-            return filter_specs, bool(filter_specs), lookup_params, use_distinct
+                may_have_duplicates |= lookup_needs_distinct(self.lookup_opts, key)
+            return (
+                filter_specs, bool(filter_specs), lookup_params, may_have_duplicates,
+                has_active_filters,
+            )
         except FieldDoesNotExist as e:
             raise IncorrectLookupParameters(e) from e

@@ -222,7 +247,7 @@
             result_list = self.queryset._clone()
         else:
             try:
-                result_list = paginator.page(self.page_num + 1).object_list
+                result_list = paginator.page(self.page_num).object_list
             except InvalidPage:
                 raise IncorrectLookupParameters

@@ -265,6 +290,8 @@
                 attr = getattr(self.model_admin, field_name)
             else:
                 attr = getattr(self.model, field_name)
+            if isinstance(attr, property) and hasattr(attr, 'fget'):
+                attr = attr.fget
             return getattr(attr, 'admin_order_field', None)

     def get_ordering(self, request, queryset):
@@ -289,7 +316,12 @@
                     order_field = self.get_ordering_field(field_name)
                     if not order_field:
                         continue  # No 'admin_order_field', skip it
-                    if hasattr(order_field, 'as_sql'):
+                    if isinstance(order_field, OrderBy):
+                        if pfx == '-':
+                            order_field = order_field.copy()
+                            order_field.reverse_ordering()
+                        ordering.append(order_field)
+                    elif hasattr(order_field, 'resolve_expression'):
                         # order_field is an expression.
                         ordering.append(order_field.desc() if pfx == '-' else order_field.asc())
                     # reverse order if order_field has already "-" as prefix
@@ -343,8 +375,16 @@
                     break
                 ordering_fields.add(field.attname)
         else:
-            # No single total ordering field, try unique_together.
-            for field_names in self.lookup_opts.unique_together:
+            # No single total ordering field, try unique_together and total
+            # unique constraints.
+            constraint_field_names = (
+                *self.lookup_opts.unique_together,
+                *(
+                    constraint.fields
+                    for constraint in self.lookup_opts.total_unique_constraints
+                ),
+            )
+            for field_names in constraint_field_names:
                 # Normalize attname references by using get_field().
                 fields = [self.lookup_opts.get_field(field_name) for field_name in field_names]
                 # Composite unique constraints containing a nullable column
@@ -361,12 +401,12 @@

     def get_ordering_field_columns(self):
         """
-        Return an OrderedDict of ordering field column numbers and asc/desc.
+        Return a dictionary of ordering field column numbers and asc/desc.
         """
         # We must cope with more than one column having the same underlying sort
         # field, so we base things on column numbers.
         ordering = self._get_default_ordering()
-        ordering_fields = OrderedDict()
+        ordering_fields = {}
         if ORDER_VAR not in self.params:
             # for ordering specified on ModelAdmin or model Meta, we don't know
             # the right column numbers absolutely, because there might be more
@@ -401,9 +441,13 @@

     def get_queryset(self, request):
         # First, we collect all the declared list filters.
-        (self.filter_specs, self.has_filters, remaining_lookup_params,
-         filters_use_distinct) = self.get_filters(request)
-
+        (
+            self.filter_specs,
+            self.has_filters,
+            remaining_lookup_params,
+            filters_may_have_duplicates,
+            self.has_active_filters,
+        ) = self.get_filters(request)
         # Then, we let every list filter modify the queryset to its liking.
         qs = self.root_queryset
         for filter_spec in self.filter_specs:
@@ -428,21 +472,29 @@
             # ValueError, ValidationError, or ?.
             raise IncorrectLookupParameters(e)

-        if not qs.query.select_related:
-            qs = self.apply_select_related(qs)
+        # Apply search results
+        qs, search_may_have_duplicates = self.model_admin.get_search_results(
+            request, qs, self.query,
+        )
+
+        # Set query string for clearing all filters.
+        self.clear_all_filters_qs = self.get_query_string(
+            new_params=remaining_lookup_params,
+            remove=self.get_filters_params(),
+        )
+        # Remove duplicates from results, if necessary
+        if filters_may_have_duplicates | search_may_have_duplicates:
+            qs = qs.filter(pk=OuterRef('pk'))
+            qs = self.root_queryset.filter(Exists(qs))

         # Set ordering.
         ordering = self.get_ordering(request, qs)
         qs = qs.order_by(*ordering)

-        # Apply search results
-        qs, search_use_distinct = self.model_admin.get_search_results(request, qs, self.query)
-
-        # Remove duplicates from results, if necessary
-        if filters_use_distinct | search_use_distinct:
-            return qs.distinct()
-        else:
-            return qs
+        if not qs.query.select_related:
+            qs = self.apply_select_related(qs)
+
+        return qs

     def apply_select_related(self, qs):
         if self.list_select_related is True:
@@ -463,7 +515,7 @@
             except FieldDoesNotExist:
                 pass
             else:
-                if isinstance(field.remote_field, models.ManyToOneRel):
+                if isinstance(field.remote_field, ManyToOneRel):
                     # <FK>_id field names don't require a join.
                     if field_name != field.get_attname():
                         return True
('django/contrib/staticfiles', 'finders.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,5 @@
 import functools
 import os
-from collections import OrderedDict

 from django.apps import apps
 from django.conf import settings
@@ -54,7 +53,7 @@
         # List of locations with static files
         self.locations = []
         # Maps dir paths to an appropriate storage instance
-        self.storages = OrderedDict()
+        self.storages = {}
         for root in settings.STATICFILES_DIRS:
             if isinstance(root, (list, tuple)):
                 prefix, root = root
@@ -144,7 +143,7 @@
         # The list of apps that are handled
         self.apps = []
         # Mapping of app names to storage instances
-        self.storages = OrderedDict()
+        self.storages = {}
         app_configs = apps.get_app_configs()
         if app_names:
             app_names = set(app_names)
@@ -188,12 +187,11 @@
         Find a requested static file in an app's static locations.
         """
         storage = self.storages.get(app)
-        if storage:
-            # only try to find a file if the source dir actually exists
-            if storage.exists(path):
-                matched_path = storage.path(path)
-                if matched_path:
-                    return matched_path
+        # Only try to find a file if the source dir actually exists.
+        if storage and storage.exists(path):
+            matched_path = storage.path(path)
+            if matched_path:
+                return matched_path


 class BaseStorageFinder(BaseFinder):
('django/contrib/staticfiles', 'handlers.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,26 +1,24 @@
 from urllib.parse import urlparse
 from urllib.request import url2pathname
+
+from asgiref.sync import sync_to_async

 from django.conf import settings
 from django.contrib.staticfiles import utils
 from django.contrib.staticfiles.views import serve
+from django.core.handlers.asgi import ASGIHandler
 from django.core.handlers.exception import response_for_exception
 from django.core.handlers.wsgi import WSGIHandler, get_path_info
+from django.http import Http404


-class StaticFilesHandler(WSGIHandler):
+class StaticFilesHandlerMixin:
     """
-    WSGI middleware that intercepts calls to the static files directory, as
-    defined by the STATIC_URL setting, and serves those files.
+    Common methods used by WSGI and ASGI handlers.
     """
     # May be used to differentiate between handler types (e.g. in a
     # request_finished signal)
     handles_files = True
-
-    def __init__(self, application):
-        self.application = application
-        self.base_url = urlparse(self.get_base_url())
-        super().__init__()

     def load_middleware(self):
         # Middleware are already loaded for self.application; no need to reload
@@ -51,16 +49,48 @@
         return serve(request, self.file_path(request.path), insecure=True)

     def get_response(self, request):
-        from django.http import Http404
+        try:
+            return self.serve(request)
+        except Http404 as e:
+            return response_for_exception(request, e)

-        if self._should_handle(request.path):
-            try:
-                return self.serve(request)
-            except Http404 as e:
-                return response_for_exception(request, e)
-        return super().get_response(request)
+    async def get_response_async(self, request):
+        try:
+            return await sync_to_async(self.serve, thread_sensitive=False)(request)
+        except Http404 as e:
+            return await sync_to_async(response_for_exception, thread_sensitive=False)(request, e)
+
+
+class StaticFilesHandler(StaticFilesHandlerMixin, WSGIHandler):
+    """
+    WSGI middleware that intercepts calls to the static files directory, as
+    defined by the STATIC_URL setting, and serves those files.
+    """
+    def __init__(self, application):
+        self.application = application
+        self.base_url = urlparse(self.get_base_url())
+        super().__init__()

     def __call__(self, environ, start_response):
         if not self._should_handle(get_path_info(environ)):
             return self.application(environ, start_response)
         return super().__call__(environ, start_response)
+
+
+class ASGIStaticFilesHandler(StaticFilesHandlerMixin, ASGIHandler):
+    """
+    ASGI application which wraps another and intercepts requests for static
+    files, passing them off to Django's static file serving.
+    """
+    def __init__(self, application):
+        self.application = application
+        self.base_url = urlparse(self.get_base_url())
+
+    async def __call__(self, scope, receive, send):
+        # Only even look at HTTP requests
+        if scope['type'] == 'http' and self._should_handle(scope['path']):
+            # Serve static content
+            # (the one thing super() doesn't do is __call__, apparently)
+            return await super().__call__(scope, receive, send)
+        # Hand off to the main app
+        return await self.application(scope, receive, send)
('django/contrib/staticfiles', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1 +0,0 @@
-default_app_config = 'django.contrib.staticfiles.apps.StaticFilesConfig'
('django/contrib/staticfiles', 'apps.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -10,4 +10,4 @@
     ignore_patterns = ['CVS', '.*', '*~']

     def ready(self):
-        checks.register(check_finders, 'staticfiles')
+        checks.register(check_finders, checks.Tags.staticfiles)
('django/contrib/staticfiles', 'utils.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -5,12 +5,12 @@
 from django.core.exceptions import ImproperlyConfigured


-def matches_patterns(path, patterns=None):
+def matches_patterns(path, patterns):
     """
     Return True or False depending on whether the ``path`` should be
     ignored (if it matches any pattern in ``ignore_patterns``).
     """
-    return any(fnmatch.fnmatchcase(path, pattern) for pattern in (patterns or []))
+    return any(fnmatch.fnmatchcase(path, pattern) for pattern in patterns)


 def get_files(storage, ignore_patterns=None, location=''):
('django/contrib/staticfiles', 'storage.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,19 +3,13 @@
 import os
 import posixpath
 import re
-import warnings
-from collections import OrderedDict
 from urllib.parse import unquote, urldefrag, urlsplit, urlunsplit

 from django.conf import settings
 from django.contrib.staticfiles.utils import check_settings, matches_patterns
-from django.core.cache import (
-    InvalidCacheBackendError, cache as default_cache, caches,
-)
 from django.core.exceptions import ImproperlyConfigured
 from django.core.files.base import ContentFile
 from django.core.files.storage import FileSystemStorage, get_storage_class
-from django.utils.deprecation import RemovedInDjango31Warning
 from django.utils.functional import LazyObject


@@ -56,10 +50,11 @@
             (r"""(@import\s*["']\s*(.*?)["'])""", """@import url("%s")"""),
         )),
     )
+    keep_intermediate_files = True

     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
-        self._patterns = OrderedDict()
+        self._patterns = {}
         self.hashed_files = {}
         for extension, patterns in self.patterns:
             for pattern in patterns:
@@ -93,7 +88,7 @@
                 raise ValueError("The file '%s' could not be found with %r." % (filename, self))
             try:
                 content = self.open(filename)
-            except IOError:
+            except OSError:
                 # Handle directory paths and fragments
                 return name
         try:
@@ -103,8 +98,7 @@
                 content.close()
         path, filename = os.path.split(clean_name)
         root, ext = os.path.splitext(filename)
-        if file_hash is not None:
-            file_hash = ".%s" % file_hash
+        file_hash = ('.%s' % file_hash) if file_hash else ''
         hashed_name = os.path.join(path, "%s%s%s" %
                                    (root, file_hash, ext))
         unparsed_name = list(parsed_name)
@@ -208,7 +202,7 @@

     def post_process(self, paths, dry_run=False, **options):
         """
-        Post process the given OrderedDict of files (called from collectstatic).
+        Post process the given dictionary of files (called from collectstatic).

         Processing is actually two separate operations:

@@ -225,7 +219,7 @@
             return

         # where to store the new paths
-        hashed_files = OrderedDict()
+        hashed_files = {}

         # build a list of adjustable files
         adjustable_paths = [
@@ -285,7 +279,7 @@
                 # ..to apply each replacement pattern to the content
                 if name in adjustable_paths:
                     old_hashed_name = hashed_name
-                    content = original_file.read().decode(settings.FILE_CHARSET)
+                    content = original_file.read().decode('utf-8')
                     for extension, patterns in self._patterns.items():
                         if matches_patterns(path, (extension,)):
                             for pattern, template in patterns:
@@ -298,8 +292,9 @@
                         self.delete(hashed_name)
                     # then save the processed result
                     content_file = ContentFile(content.encode())
-                    # Save intermediate file for reference
-                    saved_name = self._save(hashed_name, content_file)
+                    if self.keep_intermediate_files:
+                        # Save intermediate file for reference
+                        self._save(hashed_name, content_file)
                     hashed_name = self.hashed_name(name, content_file)

                     if self.exists(hashed_name):
@@ -371,6 +366,7 @@
     manifest_version = '1.0'  # the manifest format standard
     manifest_name = 'staticfiles.json'
     manifest_strict = True
+    keep_intermediate_files = False

     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
@@ -380,28 +376,29 @@
         try:
             with self.open(self.manifest_name) as manifest:
                 return manifest.read().decode()
-        except IOError:
+        except FileNotFoundError:
             return None

     def load_manifest(self):
         content = self.read_manifest()
         if content is None:
-            return OrderedDict()
+            return {}
         try:
-            stored = json.loads(content, object_pairs_hook=OrderedDict)
+            stored = json.loads(content)
         except json.JSONDecodeError:
             pass
         else:
             version = stored.get('version')
             if version == '1.0':
-                return stored.get('paths', OrderedDict())
+                return stored.get('paths', {})
         raise ValueError("Couldn't load manifest '%s' (version %s)" %
                          (self.manifest_name, self.manifest_version))

     def post_process(self, *args, **kwargs):
-        self.hashed_files = OrderedDict()
+        self.hashed_files = {}
         yield from super().post_process(*args, **kwargs)
-        self.save_manifest()
+        if not kwargs.get('dry_run'):
+            self.save_manifest()

     def save_manifest(self):
         payload = {'paths': self.hashed_files, 'version': self.manifest_version}
@@ -428,63 +425,6 @@
         return urlunsplit(unparsed_name)


-class _MappingCache:
-    """
-    A small dict-like wrapper for a given cache backend instance.
-    """
-    def __init__(self, cache):
-        self.cache = cache
-
-    def __setitem__(self, key, value):
-        self.cache.set(key, value)
-
-    def __getitem__(self, key):
-        value = self.cache.get(key)
-        if value is None:
-            raise KeyError("Couldn't find a file name '%s'" % key)
-        return value
-
-    def clear(self):
-        self.cache.clear()
-
-    def update(self, data):
-        self.cache.set_many(data)
-
-    def get(self, key, default=None):
-        try:
-            return self[key]
-        except KeyError:
-            return default
-
-
-class CachedFilesMixin(HashedFilesMixin):
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        try:
-            self.hashed_files = _MappingCache(caches['staticfiles'])
-        except InvalidCacheBackendError:
-            # Use the default backend
-            self.hashed_files = _MappingCache(default_cache)
-
-    def hash_key(self, name):
-        key = hashlib.md5(self.clean_name(name).encode()).hexdigest()
-        return 'staticfiles:%s' % key
-
-
-class CachedStaticFilesStorage(CachedFilesMixin, StaticFilesStorage):
-    """
-    A static file system storage backend which also saves
-    hashed copies of the files it saves.
-    """
-    def __init__(self, *args, **kwargs):
-        warnings.warn(
-            'CachedStaticFilesStorage is deprecated in favor of '
-            'ManifestStaticFilesStorage.',
-            RemovedInDjango31Warning, stacklevel=2,
-        )
-        super().__init__(*args, **kwargs)
-
-
 class ManifestStaticFilesStorage(ManifestFilesMixin, StaticFilesStorage):
     """
     A static file system storage backend which also saves
('django/contrib/staticfiles', 'views.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -21,7 +21,7 @@

         from django.contrib.staticfiles import views

-        url(r'^(?P<path>.*)$', views.serve)
+        path('<path:path>', views.serve)

     in your URLconf.

('django/contrib/staticfiles/management/commands', 'findstatic.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -21,7 +21,7 @@
         if verbosity >= 2:
             searched_locations = (
                 "\nLooking in the following locations:\n  %s" %
-                "\n  ".join(finders.searched_locations)
+                "\n  ".join([str(loc) for loc in finders.searched_locations])
             )
         else:
             searched_locations = ''
('django/contrib/staticfiles/management/commands', 'collectstatic.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,9 +1,9 @@
 import os
-from collections import OrderedDict

 from django.apps import apps
 from django.contrib.staticfiles.finders import get_finders
 from django.contrib.staticfiles.storage import staticfiles_storage
+from django.core.checks import Tags
 from django.core.files.storage import FileSystemStorage
 from django.core.management.base import BaseCommand, CommandError
 from django.core.management.color import no_style
@@ -16,7 +16,7 @@
     settings.STATIC_ROOT.
     """
     help = "Collect static files in a single location."
-    requires_system_checks = False
+    requires_system_checks = [Tags.staticfiles]

     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
@@ -80,7 +80,7 @@
         ignore_patterns = options['ignore_patterns']
         if options['use_default_ignore_patterns']:
             ignore_patterns += apps.get_app_config('staticfiles').ignore_patterns
-        self.ignore_patterns = list(set(os.path.normpath(p) for p in ignore_patterns))
+        self.ignore_patterns = list({os.path.normpath(p) for p in ignore_patterns})
         self.post_process = options['post_process']

     def collect(self):
@@ -100,7 +100,7 @@
         else:
             handler = self.copy_file

-        found_files = OrderedDict()
+        found_files = {}
         for finder in get_finders():
             for path, storage in finder.list(self.ignore_patterns):
                 # Prefix the relative path if the source storage contains it
@@ -130,7 +130,7 @@
                     self.stderr.write("Post-processing '%s' failed!" % original_path)
                     # Add a blank line before the traceback, otherwise it's
                     # too easy to miss the relevant part of the error message.
-                    self.stderr.write("")
+                    self.stderr.write()
                     raise processed
                 if processed:
                     self.log("Post-processed '%s' as '%s'" %
@@ -147,7 +147,6 @@

     def handle(self, **options):
         self.set_options(**options)
-
         message = ['\n']
         if self.dry_run:
             message.append(
@@ -186,14 +185,15 @@
                 raise CommandError("Collecting static files cancelled.")

         collected = self.collect()
-        modified_count = len(collected['modified'])
-        unmodified_count = len(collected['unmodified'])
-        post_processed_count = len(collected['post_processed'])

         if self.verbosity >= 1:
-            template = ("\n%(modified_count)s %(identifier)s %(action)s"
-                        "%(destination)s%(unmodified)s%(post_processed)s.\n")
-            summary = template % {
+            modified_count = len(collected['modified'])
+            unmodified_count = len(collected['unmodified'])
+            post_processed_count = len(collected['post_processed'])
+            return (
+                "\n%(modified_count)s %(identifier)s %(action)s"
+                "%(destination)s%(unmodified)s%(post_processed)s."
+            ) % {
                 'modified_count': modified_count,
                 'identifier': 'static file' + ('' if modified_count == 1 else 's'),
                 'action': 'symlinked' if self.symlink else 'copied',
@@ -203,7 +203,6 @@
                                    ', %s post-processed'
                                    % post_processed_count or ''),
             }
-            return summary

     def log(self, msg, level=2):
         """
@@ -310,10 +309,7 @@
         else:
             self.log("Linking '%s'" % source_path, level=2)
             full_path = self.storage.path(prefixed_path)
-            try:
-                os.makedirs(os.path.dirname(full_path))
-            except OSError:
-                pass
+            os.makedirs(os.path.dirname(full_path), exist_ok=True)
             try:
                 if os.path.lexists(full_path):
                     os.unlink(full_path)
('django/contrib/flatpages', 'models.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,6 @@
 from django.contrib.sites.models import Site
 from django.db import models
-from django.urls import get_script_prefix
+from django.urls import NoReverseMatch, get_script_prefix, reverse
 from django.utils.encoding import iri_to_uri
 from django.utils.translation import gettext_lazy as _

@@ -15,8 +15,8 @@
         max_length=70,
         blank=True,
         help_text=_(
-            "Example: 'flatpages/contact_page.html'. If this isn't provided, "
-            "the system will use 'flatpages/default.html'."
+            'Example: “flatpages/contact_page.html”. If this isn’t provided, '
+            'the system will use “flatpages/default.html”.'
         ),
     )
     registration_required = models.BooleanField(
@@ -30,11 +30,18 @@
         db_table = 'django_flatpage'
         verbose_name = _('flat page')
         verbose_name_plural = _('flat pages')
-        ordering = ('url',)
+        ordering = ['url']

     def __str__(self):
         return "%s -- %s" % (self.url, self.title)

     def get_absolute_url(self):
+        from .views import flatpage
+
+        for url in (self.url.lstrip('/'), self.url):
+            try:
+                return reverse(flatpage, kwargs={'url': url})
+            except NoReverseMatch:
+                pass
         # Handle script prefix manually because we bypass reverse()
         return iri_to_uri(get_script_prefix().rstrip('/') + self.url)
('django/contrib/flatpages', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1 +0,0 @@
-default_app_config = 'django.contrib.flatpages.apps.FlatPagesConfig'
('django/contrib/flatpages', 'apps.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,5 +3,6 @@


 class FlatPagesConfig(AppConfig):
+    default_auto_field = 'django.db.models.AutoField'
     name = 'django.contrib.flatpages'
     verbose_name = _("Flat Pages")
('django/contrib/flatpages', 'forms.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,7 @@
 from django import forms
 from django.conf import settings
 from django.contrib.flatpages.models import FlatPage
+from django.core.exceptions import ValidationError
 from django.utils.translation import gettext, gettext_lazy as _


@@ -9,7 +10,7 @@
         label=_("URL"),
         max_length=100,
         regex=r'^[-\w/\.~]+$',
-        help_text=_("Example: '/about/contact/'. Make sure to have leading and trailing slashes."),
+        help_text=_('Example: “/about/contact/”. Make sure to have leading and trailing slashes.'),
         error_messages={
             "invalid": _(
                 "This value must contain only letters, numbers, dots, "
@@ -26,7 +27,7 @@
         super().__init__(*args, **kwargs)
         if not self._trailing_slash_required():
             self.fields['url'].help_text = _(
-                "Example: '/about/contact'. Make sure to have a leading slash."
+                'Example: “/about/contact”. Make sure to have a leading slash.'
             )

     def _trailing_slash_required(self):
@@ -38,12 +39,12 @@
     def clean_url(self):
         url = self.cleaned_data['url']
         if not url.startswith('/'):
-            raise forms.ValidationError(
+            raise ValidationError(
                 gettext("URL is missing a leading slash."),
                 code='missing_leading_slash',
             )
         if self._trailing_slash_required() and not url.endswith('/'):
-            raise forms.ValidationError(
+            raise ValidationError(
                 gettext("URL is missing a trailing slash."),
                 code='missing_trailing_slash',
             )
@@ -60,7 +61,7 @@
         if sites and same_url.filter(sites__in=sites).exists():
             for site in sites:
                 if same_url.filter(sites=site).exists():
-                    raise forms.ValidationError(
+                    raise ValidationError(
                         _('Flatpage with url %(url)s already exists for site %(site)s'),
                         code='duplicate_url',
                         params={'url': url, 'site': site},
('django/contrib/flatpages', 'views.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -66,5 +66,4 @@
     f.title = mark_safe(f.title)
     f.content = mark_safe(f.content)

-    response = HttpResponse(template.render({'flatpage': f}, request))
-    return response
+    return HttpResponse(template.render({'flatpage': f}, request))
('django/contrib/flatpages/migrations', '0001_initial.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -18,8 +18,8 @@
                 ('enable_comments', models.BooleanField(default=False, verbose_name='enable comments')),
                 ('template_name', models.CharField(
                     help_text=(
-                        "Example: 'flatpages/contact_page.html'. If this isn't provided, the system will use "
-                        "'flatpages/default.html'."
+                        'Example: “flatpages/contact_page.html”. If this isn’t provided, the system will use '
+                        '“flatpages/default.html”.'
                     ), max_length=70, verbose_name='template name', blank=True
                 )),
                 ('registration_required', models.BooleanField(
@@ -29,7 +29,7 @@
                 ('sites', models.ManyToManyField(to='sites.Site', verbose_name='sites')),
             ],
             options={
-                'ordering': ('url',),
+                'ordering': ['url'],
                 'db_table': 'django_flatpage',
                 'verbose_name': 'flat page',
                 'verbose_name_plural': 'flat pages',
('django/contrib/sites', 'models.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -91,7 +91,7 @@
         db_table = 'django_site'
         verbose_name = _('site')
         verbose_name_plural = _('sites')
-        ordering = ('domain',)
+        ordering = ['domain']

     def __str__(self):
         return self.domain
('django/contrib/sites', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1 +0,0 @@
-default_app_config = 'django.contrib.sites.apps.SitesConfig'
('django/contrib/sites', 'apps.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,4 +1,6 @@
 from django.apps import AppConfig
+from django.contrib.sites.checks import check_site_id
+from django.core import checks
 from django.db.models.signals import post_migrate
 from django.utils.translation import gettext_lazy as _

@@ -6,8 +8,10 @@


 class SitesConfig(AppConfig):
+    default_auto_field = 'django.db.models.AutoField'
     name = 'django.contrib.sites'
     verbose_name = _("Sites")

     def ready(self):
         post_migrate.connect(create_default_site, sender=self)
+        checks.register(check_site_id, checks.Tags.sites)
('django/contrib/sites/migrations', '0001_initial.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -18,7 +18,7 @@
                 ('name', models.CharField(max_length=50, verbose_name='display name')),
             ],
             options={
-                'ordering': ('domain',),
+                'ordering': ['domain'],
                 'db_table': 'django_site',
                 'verbose_name': 'site',
                 'verbose_name_plural': 'sites',
('django/contrib/postgres', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1 +0,0 @@
-default_app_config = 'django.contrib.postgres.apps.PostgresConfig'
('django/contrib/postgres', 'lookups.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,41 +1,33 @@
-from django.db.models import Lookup, Transform
-from django.db.models.lookups import Exact
+from django.db.models import Transform
+from django.db.models.lookups import PostgresOperatorLookup

 from .search import SearchVector, SearchVectorExact, SearchVectorField


-class PostgresSimpleLookup(Lookup):
-    def as_sql(self, qn, connection):
-        lhs, lhs_params = self.process_lhs(qn, connection)
-        rhs, rhs_params = self.process_rhs(qn, connection)
-        params = lhs_params + rhs_params
-        return '%s %s %s' % (lhs, self.operator, rhs), params
+class DataContains(PostgresOperatorLookup):
+    lookup_name = 'contains'
+    postgres_operator = '@>'


-class DataContains(PostgresSimpleLookup):
-    lookup_name = 'contains'
-    operator = '@>'
+class ContainedBy(PostgresOperatorLookup):
+    lookup_name = 'contained_by'
+    postgres_operator = '<@'


-class ContainedBy(PostgresSimpleLookup):
-    lookup_name = 'contained_by'
-    operator = '<@'
+class Overlap(PostgresOperatorLookup):
+    lookup_name = 'overlap'
+    postgres_operator = '&&'


-class Overlap(PostgresSimpleLookup):
-    lookup_name = 'overlap'
-    operator = '&&'
-
-
-class HasKey(PostgresSimpleLookup):
+class HasKey(PostgresOperatorLookup):
     lookup_name = 'has_key'
-    operator = '?'
+    postgres_operator = '?'
     prepare_rhs = False


-class HasKeys(PostgresSimpleLookup):
+class HasKeys(PostgresOperatorLookup):
     lookup_name = 'has_keys'
-    operator = '?&'
+    postgres_operator = '?&'

     def get_prep_lookup(self):
         return [str(item) for item in self.rhs]
@@ -43,7 +35,7 @@

 class HasAnyKeys(HasKeys):
     lookup_name = 'has_any_keys'
-    operator = '?|'
+    postgres_operator = '?|'


 class Unaccent(Transform):
@@ -57,20 +49,12 @@

     def process_lhs(self, qn, connection):
         if not isinstance(self.lhs.output_field, SearchVectorField):
-            self.lhs = SearchVector(self.lhs)
+            config = getattr(self.rhs, 'config', None)
+            self.lhs = SearchVector(self.lhs, config=config)
         lhs, lhs_params = super().process_lhs(qn, connection)
         return lhs, lhs_params


-class TrigramSimilar(PostgresSimpleLookup):
+class TrigramSimilar(PostgresOperatorLookup):
     lookup_name = 'trigram_similar'
-    operator = '%%'
-
-
-class JSONExact(Exact):
-    can_use_none_as_rhs = True
-
-    def process_rhs(self, compiler, connection):
-        result = super().process_rhs(compiler, connection)
-        # Treat None lookup values as null.
-        return ("'null'", []) if result == ('%s', [None]) else result
+    postgres_operator = '%%'
('django/contrib/postgres', 'apps.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -6,10 +6,13 @@
 from django.db import connections
 from django.db.backends.signals import connection_created
 from django.db.migrations.writer import MigrationWriter
-from django.db.models import CharField, TextField
+from django.db.models import CharField, OrderBy, TextField
+from django.db.models.functions import Collate
+from django.db.models.indexes import IndexExpression
 from django.test.signals import setting_changed
 from django.utils.translation import gettext_lazy as _

+from .indexes import OpClass
 from .lookups import SearchLookup, TrigramSimilar, Unaccent
 from .serializers import RangeSerializer
 from .signals import register_type_handlers
@@ -47,7 +50,6 @@
         for conn in connections.all():
             if conn.vendor == 'postgresql':
                 conn.introspection.data_types_reverse.update({
-                    3802: 'django.contrib.postgres.fields.JSONField',
                     3904: 'django.contrib.postgres.fields.IntegerRangeField',
                     3906: 'django.contrib.postgres.fields.DecimalRangeField',
                     3910: 'django.contrib.postgres.fields.DateTimeRangeField',
@@ -64,3 +66,4 @@
         CharField.register_lookup(TrigramSimilar)
         TextField.register_lookup(TrigramSimilar)
         MigrationWriter.register_serializer(RANGE_TYPES, RangeSerializer)
+        IndexExpression.register_wrappers(OrderBy, OpClass, Collate)
('django/contrib/postgres', 'operations.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,8 @@
 from django.contrib.postgres.signals import (
     get_citext_oids, get_hstore_oids, register_type_handlers,
 )
+from django.db import NotSupportedError, router
+from django.db.migrations import AddIndex, RemoveIndex
 from django.db.migrations.operations.base import Operation


@@ -14,9 +16,15 @@
         pass

     def database_forwards(self, app_label, schema_editor, from_state, to_state):
-        if schema_editor.connection.vendor != 'postgresql':
-            return
-        schema_editor.execute("CREATE EXTENSION IF NOT EXISTS %s" % schema_editor.quote_name(self.name))
+        if (
+            schema_editor.connection.vendor != 'postgresql' or
+            not router.allow_migrate(schema_editor.connection.alias, app_label)
+        ):
+            return
+        if not self.extension_exists(schema_editor, self.name):
+            schema_editor.execute(
+                'CREATE EXTENSION IF NOT EXISTS %s' % schema_editor.quote_name(self.name)
+            )
         # Clear cached, stale oids.
         get_hstore_oids.cache_clear()
         get_citext_oids.cache_clear()
@@ -26,14 +34,37 @@
         register_type_handlers(schema_editor.connection)

     def database_backwards(self, app_label, schema_editor, from_state, to_state):
-        schema_editor.execute("DROP EXTENSION %s" % schema_editor.quote_name(self.name))
+        if not router.allow_migrate(schema_editor.connection.alias, app_label):
+            return
+        if self.extension_exists(schema_editor, self.name):
+            schema_editor.execute(
+                'DROP EXTENSION IF EXISTS %s' % schema_editor.quote_name(self.name)
+            )
         # Clear cached, stale oids.
         get_hstore_oids.cache_clear()
         get_citext_oids.cache_clear()

+    def extension_exists(self, schema_editor, extension):
+        with schema_editor.connection.cursor() as cursor:
+            cursor.execute(
+                'SELECT 1 FROM pg_extension WHERE extname = %s',
+                [extension],
+            )
+            return bool(cursor.fetchone())
+
     def describe(self):
         return "Creates extension %s" % self.name

+    @property
+    def migration_name_fragment(self):
+        return 'create_extension_%s' % self.name
+
+
+class BloomExtension(CreateExtension):
+
+    def __init__(self):
+        self.name = 'bloom'
+

 class BtreeGinExtension(CreateExtension):

@@ -75,3 +106,158 @@

     def __init__(self):
         self.name = 'unaccent'
+
+
+class NotInTransactionMixin:
+    def _ensure_not_in_transaction(self, schema_editor):
+        if schema_editor.connection.in_atomic_block:
+            raise NotSupportedError(
+                'The %s operation cannot be executed inside a transaction '
+                '(set atomic = False on the migration).'
+                % self.__class__.__name__
+            )
+
+
+class AddIndexConcurrently(NotInTransactionMixin, AddIndex):
+    """Create an index using PostgreSQL's CREATE INDEX CONCURRENTLY syntax."""
+    atomic = False
+
+    def describe(self):
+        return 'Concurrently create index %s on field(s) %s of model %s' % (
+            self.index.name,
+            ', '.join(self.index.fields),
+            self.model_name,
+        )
+
+    def database_forwards(self, app_label, schema_editor, from_state, to_state):
+        self._ensure_not_in_transaction(schema_editor)
+        model = to_state.apps.get_model(app_label, self.model_name)
+        if self.allow_migrate_model(schema_editor.connection.alias, model):
+            schema_editor.add_index(model, self.index, concurrently=True)
+
+    def database_backwards(self, app_label, schema_editor, from_state, to_state):
+        self._ensure_not_in_transaction(schema_editor)
+        model = from_state.apps.get_model(app_label, self.model_name)
+        if self.allow_migrate_model(schema_editor.connection.alias, model):
+            schema_editor.remove_index(model, self.index, concurrently=True)
+
+
+class RemoveIndexConcurrently(NotInTransactionMixin, RemoveIndex):
+    """Remove an index using PostgreSQL's DROP INDEX CONCURRENTLY syntax."""
+    atomic = False
+
+    def describe(self):
+        return 'Concurrently remove index %s from %s' % (self.name, self.model_name)
+
+    def database_forwards(self, app_label, schema_editor, from_state, to_state):
+        self._ensure_not_in_transaction(schema_editor)
+        model = from_state.apps.get_model(app_label, self.model_name)
+        if self.allow_migrate_model(schema_editor.connection.alias, model):
+            from_model_state = from_state.models[app_label, self.model_name_lower]
+            index = from_model_state.get_index_by_name(self.name)
+            schema_editor.remove_index(model, index, concurrently=True)
+
+    def database_backwards(self, app_label, schema_editor, from_state, to_state):
+        self._ensure_not_in_transaction(schema_editor)
+        model = to_state.apps.get_model(app_label, self.model_name)
+        if self.allow_migrate_model(schema_editor.connection.alias, model):
+            to_model_state = to_state.models[app_label, self.model_name_lower]
+            index = to_model_state.get_index_by_name(self.name)
+            schema_editor.add_index(model, index, concurrently=True)
+
+
+class CollationOperation(Operation):
+    def __init__(self, name, locale, *, provider='libc', deterministic=True):
+        self.name = name
+        self.locale = locale
+        self.provider = provider
+        self.deterministic = deterministic
+
+    def state_forwards(self, app_label, state):
+        pass
+
+    def deconstruct(self):
+        kwargs = {'name': self.name, 'locale': self.locale}
+        if self.provider and self.provider != 'libc':
+            kwargs['provider'] = self.provider
+        if self.deterministic is False:
+            kwargs['deterministic'] = self.deterministic
+        return (
+            self.__class__.__qualname__,
+            [],
+            kwargs,
+        )
+
+    def create_collation(self, schema_editor):
+        if (
+            self.deterministic is False and
+            not schema_editor.connection.features.supports_non_deterministic_collations
+        ):
+            raise NotSupportedError(
+                'Non-deterministic collations require PostgreSQL 12+.'
+            )
+        if (
+            self.provider != 'libc' and
+            not schema_editor.connection.features.supports_alternate_collation_providers
+        ):
+            raise NotSupportedError('Non-libc providers require PostgreSQL 10+.')
+        args = {'locale': schema_editor.quote_name(self.locale)}
+        if self.provider != 'libc':
+            args['provider'] = schema_editor.quote_name(self.provider)
+        if self.deterministic is False:
+            args['deterministic'] = 'false'
+        schema_editor.execute('CREATE COLLATION %(name)s (%(args)s)' % {
+            'name': schema_editor.quote_name(self.name),
+            'args': ', '.join(f'{option}={value}' for option, value in args.items()),
+        })
+
+    def remove_collation(self, schema_editor):
+        schema_editor.execute(
+            'DROP COLLATION %s' % schema_editor.quote_name(self.name),
+        )
+
+
+class CreateCollation(CollationOperation):
+    """Create a collation."""
+    def database_forwards(self, app_label, schema_editor, from_state, to_state):
+        if (
+            schema_editor.connection.vendor != 'postgresql' or
+            not router.allow_migrate(schema_editor.connection.alias, app_label)
+        ):
+            return
+        self.create_collation(schema_editor)
+
+    def database_backwards(self, app_label, schema_editor, from_state, to_state):
+        if not router.allow_migrate(schema_editor.connection.alias, app_label):
+            return
+        self.remove_collation(schema_editor)
+
+    def describe(self):
+        return f'Create collation {self.name}'
+
+    @property
+    def migration_name_fragment(self):
+        return 'create_collation_%s' % self.name.lower()
+
+
+class RemoveCollation(CollationOperation):
+    """Remove a collation."""
+    def database_forwards(self, app_label, schema_editor, from_state, to_state):
+        if (
+            schema_editor.connection.vendor != 'postgresql' or
+            not router.allow_migrate(schema_editor.connection.alias, app_label)
+        ):
+            return
+        self.remove_collation(schema_editor)
+
+    def database_backwards(self, app_label, schema_editor, from_state, to_state):
+        if not router.allow_migrate(schema_editor.connection.alias, app_label):
+            return
+        self.create_collation(schema_editor)
+
+    def describe(self):
+        return f'Remove collation {self.name}'
+
+    @property
+    def migration_name_fragment(self):
+        return 'remove_collation_%s' % self.name.lower()
('django/contrib/postgres', 'indexes.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,10 +1,10 @@
-from django.db.models import Index
-from django.db.utils import NotSupportedError
+from django.db import NotSupportedError
+from django.db.models import Func, Index
 from django.utils.functional import cached_property

 __all__ = [
-    'BrinIndex', 'BTreeIndex', 'GinIndex', 'GistIndex', 'HashIndex',
-    'SpGistIndex',
+    'BloomIndex', 'BrinIndex', 'BTreeIndex', 'GinIndex', 'GistIndex',
+    'HashIndex', 'SpGistIndex',
 ]


@@ -18,9 +18,9 @@
         # indexes.
         return Index.max_name_length - len(Index.suffix) + len(self.suffix)

-    def create_sql(self, model, schema_editor, using=''):
+    def create_sql(self, model, schema_editor, using='', **kwargs):
         self.check_supported(schema_editor)
-        statement = super().create_sql(model, schema_editor, using=' USING %s' % self.suffix)
+        statement = super().create_sql(model, schema_editor, using=' USING %s' % self.suffix, **kwargs)
         with_params = self.get_with_params()
         if with_params:
             statement.parts['extra'] = 'WITH (%s) %s' % (
@@ -36,15 +36,59 @@
         return []


+class BloomIndex(PostgresIndex):
+    suffix = 'bloom'
+
+    def __init__(self, *expressions, length=None, columns=(), **kwargs):
+        super().__init__(*expressions, **kwargs)
+        if len(self.fields) > 32:
+            raise ValueError('Bloom indexes support a maximum of 32 fields.')
+        if not isinstance(columns, (list, tuple)):
+            raise ValueError('BloomIndex.columns must be a list or tuple.')
+        if len(columns) > len(self.fields):
+            raise ValueError(
+                'BloomIndex.columns cannot have more values than fields.'
+            )
+        if not all(0 < col <= 4095 for col in columns):
+            raise ValueError(
+                'BloomIndex.columns must contain integers from 1 to 4095.',
+            )
+        if length is not None and not 0 < length <= 4096:
+            raise ValueError(
+                'BloomIndex.length must be None or an integer from 1 to 4096.',
+            )
+        self.length = length
+        self.columns = columns
+
+    def deconstruct(self):
+        path, args, kwargs = super().deconstruct()
+        if self.length is not None:
+            kwargs['length'] = self.length
+        if self.columns:
+            kwargs['columns'] = self.columns
+        return path, args, kwargs
+
+    def get_with_params(self):
+        with_params = []
+        if self.length is not None:
+            with_params.append('length = %d' % self.length)
+        if self.columns:
+            with_params.extend(
+                'col%d = %d' % (i, v)
+                for i, v in enumerate(self.columns, start=1)
+            )
+        return with_params
+
+
 class BrinIndex(PostgresIndex):
     suffix = 'brin'

-    def __init__(self, *, autosummarize=None, pages_per_range=None, **kwargs):
+    def __init__(self, *expressions, autosummarize=None, pages_per_range=None, **kwargs):
         if pages_per_range is not None and pages_per_range <= 0:
             raise ValueError('pages_per_range must be None or a positive integer')
         self.autosummarize = autosummarize
         self.pages_per_range = pages_per_range
-        super().__init__(**kwargs)
+        super().__init__(*expressions, **kwargs)

     def deconstruct(self):
         path, args, kwargs = super().deconstruct()
@@ -55,8 +99,6 @@
         return path, args, kwargs

     def check_supported(self, schema_editor):
-        if not schema_editor.connection.features.has_brin_index_support:
-            raise NotSupportedError('BRIN indexes require PostgreSQL 9.5+.')
         if self.autosummarize and not schema_editor.connection.features.has_brin_autosummarize:
             raise NotSupportedError('BRIN option autosummarize requires PostgreSQL 10+.')

@@ -72,9 +114,9 @@
 class BTreeIndex(PostgresIndex):
     suffix = 'btree'

-    def __init__(self, *, fillfactor=None, **kwargs):
-        self.fillfactor = fillfactor
-        super().__init__(**kwargs)
+    def __init__(self, *expressions, fillfactor=None, **kwargs):
+        self.fillfactor = fillfactor
+        super().__init__(*expressions, **kwargs)

     def deconstruct(self):
         path, args, kwargs = super().deconstruct()
@@ -92,10 +134,10 @@
 class GinIndex(PostgresIndex):
     suffix = 'gin'

-    def __init__(self, *, fastupdate=None, gin_pending_list_limit=None, **kwargs):
+    def __init__(self, *expressions, fastupdate=None, gin_pending_list_limit=None, **kwargs):
         self.fastupdate = fastupdate
         self.gin_pending_list_limit = gin_pending_list_limit
-        super().__init__(**kwargs)
+        super().__init__(*expressions, **kwargs)

     def deconstruct(self):
         path, args, kwargs = super().deconstruct()
@@ -105,10 +147,6 @@
             kwargs['gin_pending_list_limit'] = self.gin_pending_list_limit
         return path, args, kwargs

-    def check_supported(self, schema_editor):
-        if self.gin_pending_list_limit and not schema_editor.connection.features.has_gin_pending_list_limit:
-            raise NotSupportedError('GIN option gin_pending_list_limit requires PostgreSQL 9.5+.')
-
     def get_with_params(self):
         with_params = []
         if self.gin_pending_list_limit is not None:
@@ -121,10 +159,10 @@
 class GistIndex(PostgresIndex):
     suffix = 'gist'

-    def __init__(self, *, buffering=None, fillfactor=None, **kwargs):
+    def __init__(self, *expressions, buffering=None, fillfactor=None, **kwargs):
         self.buffering = buffering
         self.fillfactor = fillfactor
-        super().__init__(**kwargs)
+        super().__init__(*expressions, **kwargs)

     def deconstruct(self):
         path, args, kwargs = super().deconstruct()
@@ -142,13 +180,17 @@
             with_params.append('fillfactor = %d' % self.fillfactor)
         return with_params

+    def check_supported(self, schema_editor):
+        if self.include and not schema_editor.connection.features.supports_covering_gist_indexes:
+            raise NotSupportedError('Covering GiST indexes requires PostgreSQL 12+.')
+

 class HashIndex(PostgresIndex):
     suffix = 'hash'

-    def __init__(self, *, fillfactor=None, **kwargs):
-        self.fillfactor = fillfactor
-        super().__init__(**kwargs)
+    def __init__(self, *expressions, fillfactor=None, **kwargs):
+        self.fillfactor = fillfactor
+        super().__init__(*expressions, **kwargs)

     def deconstruct(self):
         path, args, kwargs = super().deconstruct()
@@ -166,18 +208,25 @@
 class SpGistIndex(PostgresIndex):
     suffix = 'spgist'

-    def __init__(self, *, fillfactor=None, **kwargs):
-        self.fillfactor = fillfactor
-        super().__init__(**kwargs)
-
-    def deconstruct(self):
-        path, args, kwargs = super().deconstruct()
-        if self.fillfactor is not None:
-            kwargs['fillfactor'] = self.fillfactor
-        return path, args, kwargs
-
-    def get_with_params(self):
-        with_params = []
-        if self.fillfactor is not None:
-            with_params.append('fillfactor = %d' % self.fillfactor)
-        return with_params
+    def __init__(self, *expressions, fillfactor=None, **kwargs):
+        self.fillfactor = fillfactor
+        super().__init__(*expressions, **kwargs)
+
+    def deconstruct(self):
+        path, args, kwargs = super().deconstruct()
+        if self.fillfactor is not None:
+            kwargs['fillfactor'] = self.fillfactor
+        return path, args, kwargs
+
+    def get_with_params(self):
+        with_params = []
+        if self.fillfactor is not None:
+            with_params.append('fillfactor = %d' % self.fillfactor)
+        return with_params
+
+
+class OpClass(Func):
+    template = '%(expressions)s %(name)s'
+
+    def __init__(self, expression, name):
+        super().__init__(expression, name=name)
('django/contrib/postgres', 'search.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,13 +1,17 @@
-from django.db.models import Field, FloatField
-from django.db.models.expressions import CombinedExpression, Func, Value
-from django.db.models.lookups import Lookup
+import psycopg2
+
+from django.db.models import (
+    CharField, Expression, Field, FloatField, Func, Lookup, TextField, Value,
+)
+from django.db.models.expressions import CombinedExpression
+from django.db.models.functions import Cast, Coalesce


 class SearchVectorExact(Lookup):
     lookup_name = 'exact'

     def process_rhs(self, qn, connection):
-        if not hasattr(self.rhs, 'resolve_expression'):
+        if not isinstance(self.rhs, (SearchQuery, CombinedSearchQuery)):
             config = getattr(self.lhs, 'config', None)
             self.rhs = SearchQuery(self.rhs, config=config)
         rhs, rhs_params = super().process_rhs(qn, connection)
@@ -17,7 +21,7 @@
         lhs, lhs_params = self.process_lhs(qn, connection)
         rhs, rhs_params = self.process_rhs(qn, connection)
         params = lhs_params + rhs_params
-        return '%s @@ %s = true' % (lhs, rhs), params
+        return '%s @@ %s' % (lhs, rhs), params


 class SearchVectorField(Field):
@@ -32,12 +36,39 @@
         return 'tsquery'


+class SearchConfig(Expression):
+    def __init__(self, config):
+        super().__init__()
+        if not hasattr(config, 'resolve_expression'):
+            config = Value(config)
+        self.config = config
+
+    @classmethod
+    def from_parameter(cls, config):
+        if config is None or isinstance(config, cls):
+            return config
+        return cls(config)
+
+    def get_source_expressions(self):
+        return [self.config]
+
+    def set_source_expressions(self, exprs):
+        self.config, = exprs
+
+    def as_sql(self, compiler, connection):
+        sql, params = compiler.compile(self.config)
+        return '%s::regconfig' % sql, params
+
+
 class SearchVectorCombinable:
     ADD = '||'

     def _combine(self, other, connector, reversed):
-        if not isinstance(other, SearchVectorCombinable) or not self.config == other.config:
-            raise TypeError('SearchVector can only be combined with other SearchVectors')
+        if not isinstance(other, SearchVectorCombinable):
+            raise TypeError(
+                'SearchVector can only be combined with other SearchVector '
+                'instances, got %s.' % type(other).__name__
+            )
         if reversed:
             return CombinedSearchVector(other, connector, self, self.config)
         return CombinedSearchVector(self, connector, other, self.config)
@@ -45,15 +76,12 @@

 class SearchVector(SearchVectorCombinable, Func):
     function = 'to_tsvector'
-    arg_joiner = ", ' ',"
-    template = '%(function)s(concat(%(expressions)s))'
+    arg_joiner = " || ' ' || "
     output_field = SearchVectorField()
-    config = None
-
-    def __init__(self, *expressions, **extra):
-        super().__init__(*expressions, **extra)
-        self.config = self.extra.get('config', self.config)
-        weight = self.extra.get('weight')
+
+    def __init__(self, *expressions, config=None, weight=None):
+        super().__init__(*expressions)
+        self.config = SearchConfig.from_parameter(config)
         if weight is not None and not hasattr(weight, 'resolve_expression'):
             weight = Value(weight)
         self.weight = weight
@@ -61,24 +89,34 @@
     def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):
         resolved = super().resolve_expression(query, allow_joins, reuse, summarize, for_save)
         if self.config:
-            if not hasattr(self.config, 'resolve_expression'):
-                resolved.config = Value(self.config).resolve_expression(query, allow_joins, reuse, summarize, for_save)
-            else:
-                resolved.config = self.config.resolve_expression(query, allow_joins, reuse, summarize, for_save)
+            resolved.config = self.config.resolve_expression(query, allow_joins, reuse, summarize, for_save)
         return resolved

     def as_sql(self, compiler, connection, function=None, template=None):
+        clone = self.copy()
+        clone.set_source_expressions([
+            Coalesce(
+                expression
+                if isinstance(expression.output_field, (CharField, TextField))
+                else Cast(expression, TextField()),
+                Value('')
+            ) for expression in clone.get_source_expressions()
+        ])
+        config_sql = None
         config_params = []
         if template is None:
-            if self.config:
-                config_sql, config_params = compiler.compile(self.config)
-                template = "%(function)s({}::regconfig, concat(%(expressions)s))".format(config_sql.replace('%', '%%'))
+            if clone.config:
+                config_sql, config_params = compiler.compile(clone.config)
+                template = '%(function)s(%(config)s, %(expressions)s)'
             else:
-                template = self.template
-        sql, params = super().as_sql(compiler, connection, function=function, template=template)
+                template = clone.template
+        sql, params = super(SearchVector, clone).as_sql(
+            compiler, connection, function=function, template=template,
+            config=config_sql,
+        )
         extra_params = []
-        if self.weight:
-            weight_sql, extra_params = compiler.compile(self.weight)
+        if clone.weight:
+            weight_sql, extra_params = compiler.compile(clone.weight)
             sql = 'setweight({}, {})'.format(sql, weight_sql)
         return sql, config_params + params + extra_params

@@ -96,8 +134,8 @@
     def _combine(self, other, connector, reversed):
         if not isinstance(other, SearchQueryCombinable):
             raise TypeError(
-                'SearchQuery can only be combined with other SearchQuerys, '
-                'got {}.'.format(type(other))
+                'SearchQuery can only be combined with other SearchQuery '
+                'instances, got %s.' % type(other).__name__
             )
         if reversed:
             return CombinedSearchQuery(other, connector, self, self.config)
@@ -119,51 +157,38 @@
         return self._combine(other, self.BITAND, True)


-class SearchQuery(SearchQueryCombinable, Value):
+class SearchQuery(SearchQueryCombinable, Func):
     output_field = SearchQueryField()
     SEARCH_TYPES = {
         'plain': 'plainto_tsquery',
         'phrase': 'phraseto_tsquery',
         'raw': 'to_tsquery',
+        'websearch': 'websearch_to_tsquery',
     }

     def __init__(self, value, output_field=None, *, config=None, invert=False, search_type='plain'):
-        self.config = config
+        self.function = self.SEARCH_TYPES.get(search_type)
+        if self.function is None:
+            raise ValueError("Unknown search_type argument '%s'." % search_type)
+        if not hasattr(value, 'resolve_expression'):
+            value = Value(value)
+        expressions = (value,)
+        self.config = SearchConfig.from_parameter(config)
+        if self.config is not None:
+            expressions = (self.config,) + expressions
         self.invert = invert
-        if search_type not in self.SEARCH_TYPES:
-            raise ValueError("Unknown search_type argument '%s'." % search_type)
-        self.search_type = search_type
-        super().__init__(value, output_field=output_field)
-
-    def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):
-        resolved = super().resolve_expression(query, allow_joins, reuse, summarize, for_save)
-        if self.config:
-            if not hasattr(self.config, 'resolve_expression'):
-                resolved.config = Value(self.config).resolve_expression(query, allow_joins, reuse, summarize, for_save)
-            else:
-                resolved.config = self.config.resolve_expression(query, allow_joins, reuse, summarize, for_save)
-        return resolved
-
-    def as_sql(self, compiler, connection):
-        params = [self.value]
-        function = self.SEARCH_TYPES[self.search_type]
-        if self.config:
-            config_sql, config_params = compiler.compile(self.config)
-            template = '{}({}::regconfig, %s)'.format(function, config_sql)
-            params = config_params + [self.value]
-        else:
-            template = '{}(%s)'.format(function)
+        super().__init__(*expressions, output_field=output_field)
+
+    def as_sql(self, compiler, connection, function=None, template=None):
+        sql, params = super().as_sql(compiler, connection, function, template)
         if self.invert:
-            template = '!!({})'.format(template)
-        return template, params
-
-    def _combine(self, other, connector, reversed):
-        combined = super()._combine(other, connector, reversed)
-        combined.output_field = SearchQueryField()
-        return combined
+            sql = '!!(%s)' % sql
+        return sql, params

     def __invert__(self):
-        return type(self)(self.value, config=self.config, invert=not self.invert)
+        clone = self.copy()
+        clone.invert = not self.invert
+        return clone

     def __str__(self):
         result = super().__str__()
@@ -183,30 +208,77 @@
     function = 'ts_rank'
     output_field = FloatField()

-    def __init__(self, vector, query, **extra):
+    def __init__(
+        self, vector, query, weights=None, normalization=None,
+        cover_density=False,
+    ):
         if not hasattr(vector, 'resolve_expression'):
             vector = SearchVector(vector)
         if not hasattr(query, 'resolve_expression'):
             query = SearchQuery(query)
-        weights = extra.get('weights')
-        if weights is not None and not hasattr(weights, 'resolve_expression'):
-            weights = Value(weights)
-        self.weights = weights
-        super().__init__(vector, query, **extra)
+        expressions = (vector, query)
+        if weights is not None:
+            if not hasattr(weights, 'resolve_expression'):
+                weights = Value(weights)
+            expressions = (weights,) + expressions
+        if normalization is not None:
+            if not hasattr(normalization, 'resolve_expression'):
+                normalization = Value(normalization)
+            expressions += (normalization,)
+        if cover_density:
+            self.function = 'ts_rank_cd'
+        super().__init__(*expressions)
+
+
+class SearchHeadline(Func):
+    function = 'ts_headline'
+    template = '%(function)s(%(expressions)s%(options)s)'
+    output_field = TextField()
+
+    def __init__(
+        self, expression, query, *, config=None, start_sel=None, stop_sel=None,
+        max_words=None, min_words=None, short_word=None, highlight_all=None,
+        max_fragments=None, fragment_delimiter=None,
+    ):
+        if not hasattr(query, 'resolve_expression'):
+            query = SearchQuery(query)
+        options = {
+            'StartSel': start_sel,
+            'StopSel': stop_sel,
+            'MaxWords': max_words,
+            'MinWords': min_words,
+            'ShortWord': short_word,
+            'HighlightAll': highlight_all,
+            'MaxFragments': max_fragments,
+            'FragmentDelimiter': fragment_delimiter,
+        }
+        self.options = {
+            option: value
+            for option, value in options.items() if value is not None
+        }
+        expressions = (expression, query)
+        if config is not None:
+            config = SearchConfig.from_parameter(config)
+            expressions = (config,) + expressions
+        super().__init__(*expressions)

     def as_sql(self, compiler, connection, function=None, template=None):
-        extra_params = []
-        extra_context = {}
-        if template is None and self.extra.get('weights'):
-            if self.weights:
-                template = '%(function)s(%(weights)s, %(expressions)s)'
-                weight_sql, extra_params = compiler.compile(self.weights)
-                extra_context['weights'] = weight_sql
+        options_sql = ''
+        options_params = []
+        if self.options:
+            # getquoted() returns a quoted bytestring of the adapted value.
+            options_params.append(', '.join(
+                '%s=%s' % (
+                    option,
+                    psycopg2.extensions.adapt(value).getquoted().decode(),
+                ) for option, value in self.options.items()
+            ))
+            options_sql = ', %s'
         sql, params = super().as_sql(
-            compiler, connection,
-            function=function, template=template, **extra_context
+            compiler, connection, function=function, template=template,
+            options=options_sql,
         )
-        return sql, extra_params + params
+        return sql, params + options_params


 SearchVectorField.register_lookup(SearchVectorExact)
('django/contrib/postgres/forms', 'ranges.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,17 +1,31 @@
-import warnings
-
 from psycopg2.extras import DateRange, DateTimeTZRange, NumericRange

 from django import forms
 from django.core import exceptions
-from django.forms.widgets import MultiWidget
-from django.utils.deprecation import RemovedInDjango31Warning
+from django.forms.widgets import HiddenInput, MultiWidget
 from django.utils.translation import gettext_lazy as _

 __all__ = [
     'BaseRangeField', 'IntegerRangeField', 'DecimalRangeField',
-    'DateTimeRangeField', 'DateRangeField', 'FloatRangeField', 'RangeWidget',
+    'DateTimeRangeField', 'DateRangeField', 'HiddenRangeWidget', 'RangeWidget',
 ]
+
+
+class RangeWidget(MultiWidget):
+    def __init__(self, base_widget, attrs=None):
+        widgets = (base_widget, base_widget)
+        super().__init__(widgets, attrs)
+
+    def decompress(self, value):
+        if value:
+            return (value.lower, value.upper)
+        return (None, None)
+
+
+class HiddenRangeWidget(RangeWidget):
+    """A widget that splits input into two <input type="hidden"> inputs."""
+    def __init__(self, attrs=None):
+        super().__init__(HiddenInput, attrs)


 class BaseRangeField(forms.MultiValueField):
@@ -19,6 +33,7 @@
         'invalid': _('Enter two valid values.'),
         'bound_ordering': _('The start of the range must not exceed the end of the range.'),
     }
+    hidden_widget = HiddenRangeWidget

     def __init__(self, **kwargs):
         if 'widget' not in kwargs:
@@ -75,17 +90,6 @@
     range_type = NumericRange


-class FloatRangeField(DecimalRangeField):
-    base_field = forms.FloatField
-
-    def __init__(self, **kwargs):
-        warnings.warn(
-            'FloatRangeField is deprecated in favor of DecimalRangeField.',
-            RemovedInDjango31Warning, stacklevel=2,
-        )
-        super().__init__(**kwargs)
-
-
 class DateTimeRangeField(BaseRangeField):
     default_error_messages = {'invalid': _('Enter two valid date/times.')}
     base_field = forms.DateTimeField
@@ -96,14 +100,3 @@
     default_error_messages = {'invalid': _('Enter two valid dates.')}
     base_field = forms.DateField
     range_type = DateRange
-
-
-class RangeWidget(MultiWidget):
-    def __init__(self, base_widget, attrs=None):
-        widgets = (base_widget, base_widget)
-        super().__init__(widgets, attrs)
-
-    def decompress(self, value):
-        if value:
-            return (value.lower, value.upper)
-        return (None, None)
('django/contrib/postgres/forms', 'jsonb.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,62 +1,16 @@
-import json
+import warnings

-from django import forms
-from django.utils.translation import gettext_lazy as _
+from django.forms import JSONField as BuiltinJSONField
+from django.utils.deprecation import RemovedInDjango40Warning

 __all__ = ['JSONField']


-class InvalidJSONInput(str):
-    pass
-
-
-class JSONString(str):
-    pass
-
-
-class JSONField(forms.CharField):
-    default_error_messages = {
-        'invalid': _("'%(value)s' value must be valid JSON."),
-    }
-    widget = forms.Textarea
-
-    def to_python(self, value):
-        if self.disabled:
-            return value
-        if value in self.empty_values:
-            return None
-        elif isinstance(value, (list, dict, int, float, JSONString)):
-            return value
-        try:
-            converted = json.loads(value)
-        except json.JSONDecodeError:
-            raise forms.ValidationError(
-                self.error_messages['invalid'],
-                code='invalid',
-                params={'value': value},
-            )
-        if isinstance(converted, str):
-            return JSONString(converted)
-        else:
-            return converted
-
-    def bound_data(self, data, initial):
-        if self.disabled:
-            return initial
-        try:
-            return json.loads(data)
-        except json.JSONDecodeError:
-            return InvalidJSONInput(data)
-
-    def prepare_value(self, value):
-        if isinstance(value, InvalidJSONInput):
-            return value
-        return json.dumps(value)
-
-    def has_changed(self, initial, data):
-        if super().has_changed(initial, data):
-            return True
-        # For purposes of seeing whether something has changed, True isn't the
-        # same as 1 and the order of keys doesn't matter.
-        data = self.to_python(data)
-        return json.dumps(initial, sort_keys=True) != json.dumps(data, sort_keys=True)
+class JSONField(BuiltinJSONField):
+    def __init__(self, *args, **kwargs):
+        warnings.warn(
+            'django.contrib.postgres.forms.JSONField is deprecated in favor '
+            'of django.forms.JSONField.',
+            RemovedInDjango40Warning, stacklevel=2,
+        )
+        super().__init__(*args, **kwargs)
('django/contrib/postgres/forms', 'array.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -178,6 +178,22 @@
         kwargs.setdefault('widget', widget)
         super().__init__(**kwargs)

+    def _remove_trailing_nulls(self, values):
+        index = None
+        if self.remove_trailing_nulls:
+            for i, value in reversed(list(enumerate(values))):
+                if value in self.base_field.empty_values:
+                    index = i
+                else:
+                    break
+            if index is not None:
+                values = values[:index]
+        return values, index
+
+    def to_python(self, value):
+        value = super().to_python(value)
+        return [self.base_field.to_python(item) for item in value]
+
     def clean(self, value):
         cleaned_data = []
         errors = []
@@ -198,17 +214,21 @@
                 cleaned_data.append(None)
             else:
                 errors.append(None)
-        if self.remove_trailing_nulls:
-            null_index = None
-            for i, value in reversed(list(enumerate(cleaned_data))):
-                if value in self.base_field.empty_values:
-                    null_index = i
-                else:
-                    break
-            if null_index is not None:
-                cleaned_data = cleaned_data[:null_index]
-                errors = errors[:null_index]
+        cleaned_data, null_index = self._remove_trailing_nulls(cleaned_data)
+        if null_index is not None:
+            errors = errors[:null_index]
         errors = list(filter(None, errors))
         if errors:
             raise ValidationError(list(chain.from_iterable(errors)))
         return cleaned_data
+
+    def has_changed(self, initial, data):
+        try:
+            data = self.to_python(data)
+        except ValidationError:
+            pass
+        else:
+            data, _ = self._remove_trailing_nulls(data)
+            if initial in self.empty_values and data in self.empty_values:
+                return False
+        return super().has_changed(initial, data)
('django/contrib/postgres/aggregates', 'mixins.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,9 +1,9 @@
-from django.db.models.expressions import F, OrderBy
+from django.db.models import F, OrderBy


 class OrderableAggMixin:

-    def __init__(self, expression, ordering=(), **extra):
+    def __init__(self, *expressions, ordering=(), **extra):
         if not isinstance(ordering, (list, tuple)):
             ordering = [ordering]
         ordering = ordering or []
@@ -12,7 +12,7 @@
             (OrderBy(F(o[1:]), descending=True) if isinstance(o, str) and o[0] == '-' else o)
             for o in ordering
         )
-        super().__init__(expression, **extra)
+        super().__init__(*expressions, **extra)
         self.ordering = self._parse_expressions(*ordering)

     def resolve_expression(self, *args, **kwargs):
@@ -21,25 +21,26 @@

     def as_sql(self, compiler, connection):
         if self.ordering:
-            self.extra['ordering'] = 'ORDER BY ' + ', '.join((
-                ordering_element.as_sql(compiler, connection)[0]
-                for ordering_element in self.ordering
+            ordering_params = []
+            ordering_expr_sql = []
+            for expr in self.ordering:
+                expr_sql, expr_params = compiler.compile(expr)
+                ordering_expr_sql.append(expr_sql)
+                ordering_params.extend(expr_params)
+            sql, sql_params = super().as_sql(compiler, connection, ordering=(
+                'ORDER BY ' + ', '.join(ordering_expr_sql)
             ))
-        else:
-            self.extra['ordering'] = ''
-        return super().as_sql(compiler, connection)
+            return sql, sql_params + ordering_params
+        return super().as_sql(compiler, connection, ordering='')
+
+    def set_source_expressions(self, exprs):
+        # Extract the ordering expressions because ORDER BY clause is handled
+        # in a custom way.
+        self.ordering = exprs[self._get_ordering_expressions_index():]
+        return super().set_source_expressions(exprs[:self._get_ordering_expressions_index()])

     def get_source_expressions(self):
-        return self.source_expressions + self.ordering
-
-    def get_source_fields(self):
-        # Filter out fields contributed by the ordering expressions as
-        # these should not be used to determine which the return type of the
-        # expression.
-        return [
-            e._output_field_or_none
-            for e in self.get_source_expressions()[:self._get_ordering_expressions_index()]
-        ]
+        return super().get_source_expressions() + self.ordering

     def _get_ordering_expressions_index(self):
         """Return the index at which the ordering expressions start."""
('django/contrib/postgres/aggregates', 'statistics.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,4 @@
-from django.db.models import FloatField, IntegerField
-from django.db.models.aggregates import Aggregate
+from django.db.models import Aggregate, FloatField, IntegerField

 __all__ = [
     'CovarPop', 'Corr', 'RegrAvgX', 'RegrAvgY', 'RegrCount', 'RegrIntercept',
@@ -14,9 +13,6 @@
         if not x or not y:
             raise ValueError('Both y and x must be provided.')
         super().__init__(y, x, output_field=output_field, filter=filter)
-
-    def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):
-        return super().resolve_expression(query, allow_joins, reuse, summarize)


 class Corr(StatAggregate):
('django/contrib/postgres/aggregates', 'general.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,5 @@
-from django.contrib.postgres.fields import ArrayField, JSONField
-from django.db.models.aggregates import Aggregate
+from django.contrib.postgres.fields import ArrayField
+from django.db.models import Aggregate, BooleanField, JSONField, Value

 from .mixins import OrderableAggMixin

@@ -33,29 +33,34 @@

 class BoolAnd(Aggregate):
     function = 'BOOL_AND'
+    output_field = BooleanField()


 class BoolOr(Aggregate):
     function = 'BOOL_OR'
+    output_field = BooleanField()


-class JSONBAgg(Aggregate):
+class JSONBAgg(OrderableAggMixin, Aggregate):
     function = 'JSONB_AGG'
+    template = '%(function)s(%(distinct)s%(expressions)s %(ordering)s)'
+    allow_distinct = True
     output_field = JSONField()

     def convert_value(self, value, expression, connection):
         if not value:
-            return []
+            return '[]'
         return value


 class StringAgg(OrderableAggMixin, Aggregate):
     function = 'STRING_AGG'
-    template = "%(function)s(%(distinct)s%(expressions)s, '%(delimiter)s'%(ordering)s)"
+    template = '%(function)s(%(distinct)s%(expressions)s %(ordering)s)'
     allow_distinct = True

     def __init__(self, expression, delimiter, **extra):
-        super().__init__(expression, delimiter=delimiter, **extra)
+        delimiter_expr = Value(str(delimiter))
+        super().__init__(expression, delimiter_expr, **extra)

     def convert_value(self, value, expression, connection):
         if not value:
('django/contrib/postgres/fields', 'ranges.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -5,14 +5,39 @@

 from django.contrib.postgres import forms, lookups
 from django.db import models
+from django.db.models.lookups import PostgresOperatorLookup

 from .utils import AttributeSetter

 __all__ = [
     'RangeField', 'IntegerRangeField', 'BigIntegerRangeField',
     'DecimalRangeField', 'DateTimeRangeField', 'DateRangeField',
-    'FloatRangeField',
+    'RangeBoundary', 'RangeOperators',
 ]
+
+
+class RangeBoundary(models.Expression):
+    """A class that represents range boundaries."""
+    def __init__(self, inclusive_lower=True, inclusive_upper=False):
+        self.lower = '[' if inclusive_lower else '('
+        self.upper = ']' if inclusive_upper else ')'
+
+    def as_sql(self, compiler, connection):
+        return "'%s%s'" % (self.lower, self.upper), []
+
+
+class RangeOperators:
+    # https://www.postgresql.org/docs/current/functions-range.html#RANGE-OPERATORS-TABLE
+    EQUAL = '='
+    NOT_EQUAL = '<>'
+    CONTAINS = '@>'
+    CONTAINED_BY = '<@'
+    OVERLAPS = '&&'
+    FULLY_LT = '<<'
+    FULLY_GT = '>>'
+    NOT_LT = '&>'
+    NOT_GT = '&<'
+    ADJACENT_TO = '-|-'


 class RangeField(models.Field):
@@ -35,6 +60,10 @@
     def model(self, model):
         self.__dict__['model'] = model
         self.base_field.model = model
+
+    @classmethod
+    def _choices_is_value(cls, value):
+        return isinstance(value, (list, tuple)) or super()._choices_is_value(value)

     def get_prep_value(self, value):
         if value is None:
@@ -110,22 +139,6 @@
         return 'numrange'


-class FloatRangeField(RangeField):
-    system_check_deprecated_details = {
-        'msg': (
-            'FloatRangeField is deprecated and will be removed in Django 3.1.'
-        ),
-        'hint': 'Use DecimalRangeField instead.',
-        'id': 'fields.W902',
-    }
-    base_field = models.FloatField
-    range_type = NumericRange
-    form_field = forms.FloatRangeField
-
-    def db_type(self, connection):
-        return 'numrange'
-
-
 class DateTimeRangeField(RangeField):
     base_field = models.DateTimeField
     range_type = DateTimeTZRange
@@ -149,57 +162,67 @@
 RangeField.register_lookup(lookups.Overlap)


-class DateTimeRangeContains(models.Lookup):
+class DateTimeRangeContains(PostgresOperatorLookup):
     """
     Lookup for Date/DateTimeRange containment to cast the rhs to the correct
     type.
     """
     lookup_name = 'contains'
+    postgres_operator = RangeOperators.CONTAINS

     def process_rhs(self, compiler, connection):
         # Transform rhs value for db lookup.
         if isinstance(self.rhs, datetime.date):
-            output_field = models.DateTimeField() if isinstance(self.rhs, datetime.datetime) else models.DateField()
-            value = models.Value(self.rhs, output_field=output_field)
+            value = models.Value(self.rhs)
             self.rhs = value.resolve_expression(compiler.query)
         return super().process_rhs(compiler, connection)

-    def as_sql(self, compiler, connection):
-        lhs, lhs_params = self.process_lhs(compiler, connection)
-        rhs, rhs_params = self.process_rhs(compiler, connection)
-        params = lhs_params + rhs_params
+    def as_postgresql(self, compiler, connection):
+        sql, params = super().as_postgresql(compiler, connection)
         # Cast the rhs if needed.
         cast_sql = ''
-        if isinstance(self.rhs, models.Expression) and self.rhs._output_field_or_none:
+        if (
+            isinstance(self.rhs, models.Expression) and
+            self.rhs._output_field_or_none and
+            # Skip cast if rhs has a matching range type.
+            not isinstance(self.rhs._output_field_or_none, self.lhs.output_field.__class__)
+        ):
             cast_internal_type = self.lhs.output_field.base_field.get_internal_type()
             cast_sql = '::{}'.format(connection.data_types.get(cast_internal_type))
-        return '%s @> %s%s' % (lhs, rhs, cast_sql), params
+        return '%s%s' % (sql, cast_sql), params


 DateRangeField.register_lookup(DateTimeRangeContains)
 DateTimeRangeField.register_lookup(DateTimeRangeContains)


-class RangeContainedBy(models.Lookup):
+class RangeContainedBy(PostgresOperatorLookup):
     lookup_name = 'contained_by'
     type_mapping = {
+        'smallint': 'int4range',
         'integer': 'int4range',
         'bigint': 'int8range',
         'double precision': 'numrange',
+        'numeric': 'numrange',
         'date': 'daterange',
         'timestamp with time zone': 'tstzrange',
     }
-
-    def as_sql(self, qn, connection):
-        field = self.lhs.output_field
-        if isinstance(field, models.FloatField):
-            sql = '%s::numeric <@ %s::{}'.format(self.type_mapping[field.db_type(connection)])
-        else:
-            sql = '%s <@ %s::{}'.format(self.type_mapping[field.db_type(connection)])
-        lhs, lhs_params = self.process_lhs(qn, connection)
-        rhs, rhs_params = self.process_rhs(qn, connection)
-        params = lhs_params + rhs_params
-        return sql % (lhs, rhs), params
+    postgres_operator = RangeOperators.CONTAINED_BY
+
+    def process_rhs(self, compiler, connection):
+        rhs, rhs_params = super().process_rhs(compiler, connection)
+        # Ignore precision for DecimalFields.
+        db_type = self.lhs.output_field.cast_db_type(connection).split('(')[0]
+        cast_type = self.type_mapping[db_type]
+        return '%s::%s' % (rhs, cast_type), rhs_params
+
+    def process_lhs(self, compiler, connection):
+        lhs, lhs_params = super().process_lhs(compiler, connection)
+        if isinstance(self.lhs.output_field, models.FloatField):
+            lhs = '%s::numeric' % lhs
+        elif isinstance(self.lhs.output_field, models.SmallIntegerField):
+            lhs = '%s::integer' % lhs
+        return lhs, lhs_params

     def get_prep_lookup(self):
         return RangeField().get_prep_value(self.rhs)
@@ -208,38 +231,38 @@
 models.DateField.register_lookup(RangeContainedBy)
 models.DateTimeField.register_lookup(RangeContainedBy)
 models.IntegerField.register_lookup(RangeContainedBy)
-models.BigIntegerField.register_lookup(RangeContainedBy)
 models.FloatField.register_lookup(RangeContainedBy)
-
-
-@RangeField.register_lookup
-class FullyLessThan(lookups.PostgresSimpleLookup):
+models.DecimalField.register_lookup(RangeContainedBy)
+
+
+@RangeField.register_lookup
+class FullyLessThan(PostgresOperatorLookup):
     lookup_name = 'fully_lt'
-    operator = '<<'
-
-
-@RangeField.register_lookup
-class FullGreaterThan(lookups.PostgresSimpleLookup):
+    postgres_operator = RangeOperators.FULLY_LT
+
+
+@RangeField.register_lookup
+class FullGreaterThan(PostgresOperatorLookup):
     lookup_name = 'fully_gt'
-    operator = '>>'
-
-
-@RangeField.register_lookup
-class NotLessThan(lookups.PostgresSimpleLookup):
+    postgres_operator = RangeOperators.FULLY_GT
+
+
+@RangeField.register_lookup
+class NotLessThan(PostgresOperatorLookup):
     lookup_name = 'not_lt'
-    operator = '&>'
-
-
-@RangeField.register_lookup
-class NotGreaterThan(lookups.PostgresSimpleLookup):
+    postgres_operator = RangeOperators.NOT_LT
+
+
+@RangeField.register_lookup
+class NotGreaterThan(PostgresOperatorLookup):
     lookup_name = 'not_gt'
-    operator = '&<'
-
-
-@RangeField.register_lookup
-class AdjacentToLookup(lookups.PostgresSimpleLookup):
+    postgres_operator = RangeOperators.NOT_GT
+
+
+@RangeField.register_lookup
+class AdjacentToLookup(PostgresOperatorLookup):
     lookup_name = 'adjacent_to'
-    operator = '-|-'
+    postgres_operator = RangeOperators.ADJACENT_TO


 @RangeField.register_lookup
@@ -267,3 +290,31 @@
     lookup_name = 'isempty'
     function = 'isempty'
     output_field = models.BooleanField()
+
+
+@RangeField.register_lookup
+class LowerInclusive(models.Transform):
+    lookup_name = 'lower_inc'
+    function = 'LOWER_INC'
+    output_field = models.BooleanField()
+
+
+@RangeField.register_lookup
+class LowerInfinite(models.Transform):
+    lookup_name = 'lower_inf'
+    function = 'LOWER_INF'
+    output_field = models.BooleanField()
+
+
+@RangeField.register_lookup
+class UpperInclusive(models.Transform):
+    lookup_name = 'upper_inc'
+    function = 'UPPER_INC'
+    output_field = models.BooleanField()
+
+
+@RangeField.register_lookup
+class UpperInfinite(models.Transform):
+    lookup_name = 'upper_inf'
+    function = 'UPPER_INF'
+    output_field = models.BooleanField()
('django/contrib/postgres/fields', 'jsonb.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,188 +1,43 @@
-import json
+import warnings

-from psycopg2.extras import Json
-
-from django.contrib.postgres import forms, lookups
-from django.core import exceptions
-from django.db.models import (
-    Field, TextField, Transform, lookups as builtin_lookups,
+from django.db.models import JSONField as BuiltinJSONField
+from django.db.models.fields.json import (
+    KeyTextTransform as BuiltinKeyTextTransform,
+    KeyTransform as BuiltinKeyTransform,
 )
-from django.utils.translation import gettext_lazy as _
-
-from .mixins import CheckFieldDefaultMixin
+from django.utils.deprecation import RemovedInDjango40Warning

 __all__ = ['JSONField']


-class JsonAdapter(Json):
-    """
-    Customized psycopg2.extras.Json to allow for a custom encoder.
-    """
-    def __init__(self, adapted, dumps=None, encoder=None):
-        self.encoder = encoder
-        super().__init__(adapted, dumps=dumps)
-
-    def dumps(self, obj):
-        options = {'cls': self.encoder} if self.encoder else {}
-        return json.dumps(obj, **options)
+class JSONField(BuiltinJSONField):
+    system_check_deprecated_details = {
+        'msg': (
+            'django.contrib.postgres.fields.JSONField is deprecated. Support '
+            'for it (except in historical migrations) will be removed in '
+            'Django 4.0.'
+        ),
+        'hint': 'Use django.db.models.JSONField instead.',
+        'id': 'fields.W904',
+    }


-class JSONField(CheckFieldDefaultMixin, Field):
-    empty_strings_allowed = False
-    description = _('A JSON object')
-    default_error_messages = {
-        'invalid': _("Value must be valid JSON."),
-    }
-    _default_hint = ('dict', '{}')
-
-    def __init__(self, verbose_name=None, name=None, encoder=None, **kwargs):
-        if encoder and not callable(encoder):
-            raise ValueError("The encoder parameter must be a callable object.")
-        self.encoder = encoder
-        super().__init__(verbose_name, name, **kwargs)
-
-    def db_type(self, connection):
-        return 'jsonb'
-
-    def deconstruct(self):
-        name, path, args, kwargs = super().deconstruct()
-        if self.encoder is not None:
-            kwargs['encoder'] = self.encoder
-        return name, path, args, kwargs
-
-    def get_transform(self, name):
-        transform = super().get_transform(name)
-        if transform:
-            return transform
-        return KeyTransformFactory(name)
-
-    def get_prep_value(self, value):
-        if value is not None:
-            return JsonAdapter(value, encoder=self.encoder)
-        return value
-
-    def validate(self, value, model_instance):
-        super().validate(value, model_instance)
-        options = {'cls': self.encoder} if self.encoder else {}
-        try:
-            json.dumps(value, **options)
-        except TypeError:
-            raise exceptions.ValidationError(
-                self.error_messages['invalid'],
-                code='invalid',
-                params={'value': value},
-            )
-
-    def value_to_string(self, obj):
-        return self.value_from_object(obj)
-
-    def formfield(self, **kwargs):
-        return super().formfield(**{
-            'form_class': forms.JSONField,
-            **kwargs,
-        })
+class KeyTransform(BuiltinKeyTransform):
+    def __init__(self, *args, **kwargs):
+        warnings.warn(
+            'django.contrib.postgres.fields.jsonb.KeyTransform is deprecated '
+            'in favor of django.db.models.fields.json.KeyTransform.',
+            RemovedInDjango40Warning, stacklevel=2,
+        )
+        super().__init__(*args, **kwargs)


-JSONField.register_lookup(lookups.DataContains)
-JSONField.register_lookup(lookups.ContainedBy)
-JSONField.register_lookup(lookups.HasKey)
-JSONField.register_lookup(lookups.HasKeys)
-JSONField.register_lookup(lookups.HasAnyKeys)
-JSONField.register_lookup(lookups.JSONExact)
-
-
-class KeyTransform(Transform):
-    operator = '->'
-    nested_operator = '#>'
-
-    def __init__(self, key_name, *args, **kwargs):
+class KeyTextTransform(BuiltinKeyTextTransform):
+    def __init__(self, *args, **kwargs):
+        warnings.warn(
+            'django.contrib.postgres.fields.jsonb.KeyTextTransform is '
+            'deprecated in favor of '
+            'django.db.models.fields.json.KeyTextTransform.',
+            RemovedInDjango40Warning, stacklevel=2,
+        )
         super().__init__(*args, **kwargs)
-        self.key_name = key_name
-
-    def as_sql(self, compiler, connection):
-        key_transforms = [self.key_name]
-        previous = self.lhs
-        while isinstance(previous, KeyTransform):
-            key_transforms.insert(0, previous.key_name)
-            previous = previous.lhs
-        lhs, params = compiler.compile(previous)
-        if len(key_transforms) > 1:
-            return "(%s %s %%s)" % (lhs, self.nested_operator), [key_transforms] + params
-        try:
-            int(self.key_name)
-        except ValueError:
-            lookup = "'%s'" % self.key_name
-        else:
-            lookup = "%s" % self.key_name
-        return "(%s %s %s)" % (lhs, self.operator, lookup), params
-
-
-class KeyTextTransform(KeyTransform):
-    operator = '->>'
-    nested_operator = '#>>'
-    output_field = TextField()
-
-
-class KeyTransformTextLookupMixin:
-    """
-    Mixin for combining with a lookup expecting a text lhs from a JSONField
-    key lookup. Make use of the ->> operator instead of casting key values to
-    text and performing the lookup on the resulting representation.
-    """
-    def __init__(self, key_transform, *args, **kwargs):
-        assert isinstance(key_transform, KeyTransform)
-        key_text_transform = KeyTextTransform(
-            key_transform.key_name, *key_transform.source_expressions, **key_transform.extra
-        )
-        super().__init__(key_text_transform, *args, **kwargs)
-
-
-class KeyTransformIExact(KeyTransformTextLookupMixin, builtin_lookups.IExact):
-    pass
-
-
-class KeyTransformIContains(KeyTransformTextLookupMixin, builtin_lookups.IContains):
-    pass
-
-
-class KeyTransformStartsWith(KeyTransformTextLookupMixin, builtin_lookups.StartsWith):
-    pass
-
-
-class KeyTransformIStartsWith(KeyTransformTextLookupMixin, builtin_lookups.IStartsWith):
-    pass
-
-
-class KeyTransformEndsWith(KeyTransformTextLookupMixin, builtin_lookups.EndsWith):
-    pass
-
-
-class KeyTransformIEndsWith(KeyTransformTextLookupMixin, builtin_lookups.IEndsWith):
-    pass
-
-
-class KeyTransformRegex(KeyTransformTextLookupMixin, builtin_lookups.Regex):
-    pass
-
-
-class KeyTransformIRegex(KeyTransformTextLookupMixin, builtin_lookups.IRegex):
-    pass
-
-
-KeyTransform.register_lookup(KeyTransformIExact)
-KeyTransform.register_lookup(KeyTransformIContains)
-KeyTransform.register_lookup(KeyTransformStartsWith)
-KeyTransform.register_lookup(KeyTransformIStartsWith)
-KeyTransform.register_lookup(KeyTransformEndsWith)
-KeyTransform.register_lookup(KeyTransformIEndsWith)
-KeyTransform.register_lookup(KeyTransformRegex)
-KeyTransform.register_lookup(KeyTransformIRegex)
-
-
-class KeyTransformFactory:
-
-    def __init__(self, key_name):
-        self.key_name = key_name
-
-    def __call__(self, *args, **kwargs):
-        return KeyTransform(self.key_name, *args, **kwargs)
('django/contrib/postgres/fields', 'array.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,13 +4,12 @@
 from django.contrib.postgres.forms import SimpleArrayField
 from django.contrib.postgres.validators import ArrayMaxLengthValidator
 from django.core import checks, exceptions
-from django.db.models import Field, IntegerField, Transform
+from django.db.models import Field, Func, IntegerField, Transform, Value
+from django.db.models.fields.mixins import CheckFieldDefaultMixin
 from django.db.models.lookups import Exact, In
-from django.utils.inspect import func_supports_parameter
 from django.utils.translation import gettext_lazy as _

 from ..utils import prefix_validation_error
-from .mixins import CheckFieldDefaultMixin
 from .utils import AttributeSetter

 __all__ = ['ArrayField']
@@ -46,6 +45,10 @@
     def model(self, model):
         self.__dict__['model'] = model
         self.base_field.model = model
+
+    @classmethod
+    def _choices_is_value(cls, value):
+        return isinstance(value, (list, tuple)) or super()._choices_is_value(value)

     def check(self, **kwargs):
         errors = super().check(**kwargs)
@@ -83,6 +86,10 @@
         size = self.size or ''
         return '%s[%s]' % (self.base_field.db_type(connection), size)

+    def cast_db_type(self, connection):
+        size = self.size or ''
+        return '%s[%s]' % (self.base_field.cast_db_type(connection), size)
+
     def get_placeholder(self, value, compiler, connection):
         return '%s::{}'.format(self.db_type(connection))

@@ -112,9 +119,7 @@
         if value is None:
             return value
         return [
-            self.base_field.from_db_value(item, expression, connection, {})
-            if func_supports_parameter(self.base_field.from_db_value, 'context')  # RemovedInDjango30Warning
-            else self.base_field.from_db_value(item, expression, connection)
+            self.base_field.from_db_value(item, expression, connection)
             for item in value
         ]

@@ -193,36 +198,46 @@
         })


-@ArrayField.register_lookup
-class ArrayContains(lookups.DataContains):
-    def as_sql(self, qn, connection):
-        sql, params = super().as_sql(qn, connection)
-        sql = '%s::%s' % (sql, self.lhs.output_field.db_type(connection))
-        return sql, params
-
-
-@ArrayField.register_lookup
-class ArrayContainedBy(lookups.ContainedBy):
-    def as_sql(self, qn, connection):
-        sql, params = super().as_sql(qn, connection)
-        sql = '%s::%s' % (sql, self.lhs.output_field.db_type(connection))
-        return sql, params
-
-
-@ArrayField.register_lookup
-class ArrayExact(Exact):
-    def as_sql(self, qn, connection):
-        sql, params = super().as_sql(qn, connection)
-        sql = '%s::%s' % (sql, self.lhs.output_field.db_type(connection))
-        return sql, params
-
-
-@ArrayField.register_lookup
-class ArrayOverlap(lookups.Overlap):
-    def as_sql(self, qn, connection):
-        sql, params = super().as_sql(qn, connection)
-        sql = '%s::%s' % (sql, self.lhs.output_field.db_type(connection))
-        return sql, params
+class ArrayRHSMixin:
+    def __init__(self, lhs, rhs):
+        if isinstance(rhs, (tuple, list)):
+            expressions = []
+            for value in rhs:
+                if not hasattr(value, 'resolve_expression'):
+                    field = lhs.output_field
+                    value = Value(field.base_field.get_prep_value(value))
+                expressions.append(value)
+            rhs = Func(
+                *expressions,
+                function='ARRAY',
+                template='%(function)s[%(expressions)s]',
+            )
+        super().__init__(lhs, rhs)
+
+    def process_rhs(self, compiler, connection):
+        rhs, rhs_params = super().process_rhs(compiler, connection)
+        cast_type = self.lhs.output_field.cast_db_type(connection)
+        return '%s::%s' % (rhs, cast_type), rhs_params
+
+
+@ArrayField.register_lookup
+class ArrayContains(ArrayRHSMixin, lookups.DataContains):
+    pass
+
+
+@ArrayField.register_lookup
+class ArrayContainedBy(ArrayRHSMixin, lookups.ContainedBy):
+    pass
+
+
+@ArrayField.register_lookup
+class ArrayExact(ArrayRHSMixin, Exact):
+    pass
+
+
+@ArrayField.register_lookup
+class ArrayOverlap(ArrayRHSMixin, lookups.Overlap):
+    pass


 @ArrayField.register_lookup
@@ -243,8 +258,7 @@
 class ArrayInLookup(In):
     def get_prep_lookup(self):
         values = super().get_prep_lookup()
-        if hasattr(self.rhs, '_prepare'):
-            # Subqueries don't need further preparation.
+        if hasattr(values, 'resolve_expression'):
             return values
         # In.process_rhs() expects values to be hashable, so convert lists
         # to tuples.
@@ -266,7 +280,7 @@

     def as_sql(self, compiler, connection):
         lhs, params = compiler.compile(self.lhs)
-        return '%s[%s]' % (lhs, self.index), params
+        return '%s[%%s]' % lhs, params + [self.index]

     @property
     def output_field(self):
@@ -292,7 +306,7 @@

     def as_sql(self, compiler, connection):
         lhs, params = compiler.compile(self.lhs)
-        return '%s[%s:%s]' % (lhs, self.start, self.end), params
+        return '%s[%%s:%%s]' % lhs, params + [self.start, self.end]


 class SliceTransformFactory:
('django/contrib/postgres/fields', 'hstore.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,9 +4,8 @@
 from django.contrib.postgres.fields.array import ArrayField
 from django.core import exceptions
 from django.db.models import Field, TextField, Transform
+from django.db.models.fields.mixins import CheckFieldDefaultMixin
 from django.utils.translation import gettext_lazy as _
-
-from .mixins import CheckFieldDefaultMixin

 __all__ = ['HStoreField']

@@ -15,7 +14,7 @@
     empty_strings_allowed = False
     description = _('Map of strings to strings/nulls')
     default_error_messages = {
-        'not_a_string': _('The value of "%(key)s" is not a string or null.'),
+        'not_a_string': _('The value of “%(key)s” is not a string or null.'),
     }
     _default_hint = ('dict', '{}')

@@ -86,7 +85,7 @@

     def as_sql(self, compiler, connection):
         lhs, params = compiler.compile(self.lhs)
-        return "(%s -> '%s')" % (lhs, self.key_name), params
+        return '(%s -> %%s)' % lhs, tuple(params) + (self.key_name,)


 class KeyTransformFactory:
('django/contrib/redirects', 'models.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -9,21 +9,24 @@
         _('redirect from'),
         max_length=200,
         db_index=True,
-        help_text=_("This should be an absolute path, excluding the domain name. Example: '/events/search/'."),
+        help_text=_('This should be an absolute path, excluding the domain name. Example: “/events/search/”.'),
     )
     new_path = models.CharField(
         _('redirect to'),
         max_length=200,
         blank=True,
-        help_text=_("This can be either an absolute path (as above) or a full URL starting with 'http://'."),
+        help_text=_(
+            'This can be either an absolute path (as above) or a full URL '
+            'starting with a scheme such as “https://”.'
+        ),
     )

     class Meta:
         verbose_name = _('redirect')
         verbose_name_plural = _('redirects')
         db_table = 'django_redirect'
-        unique_together = (('site', 'old_path'),)
-        ordering = ('old_path',)
+        unique_together = [['site', 'old_path']]
+        ordering = ['old_path']

     def __str__(self):
         return "%s ---> %s" % (self.old_path, self.new_path)
('django/contrib/redirects', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1 +0,0 @@
-default_app_config = 'django.contrib.redirects.apps.RedirectsConfig'
('django/contrib/redirects', 'apps.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,5 +3,6 @@


 class RedirectsConfig(AppConfig):
+    default_auto_field = 'django.db.models.AutoField'
     name = 'django.contrib.redirects'
     verbose_name = _("Redirects")
('django/contrib/redirects/migrations', '0001_initial.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -20,16 +20,16 @@
                 )),
                 ('old_path', models.CharField(
                     help_text=(
-                        "This should be an absolute path, excluding the domain name. Example: '/events/search/'."
+                        'This should be an absolute path, excluding the domain name. Example: “/events/search/”.'
                     ), max_length=200, verbose_name='redirect from', db_index=True
                 )),
                 ('new_path', models.CharField(
-                    help_text="This can be either an absolute path (as above) or a full URL starting with 'http://'.",
+                    help_text='This can be either an absolute path (as above) or a full URL starting with “http://”.',
                     max_length=200, verbose_name='redirect to', blank=True
                 )),
             ],
             options={
-                'ordering': ('old_path',),
+                'ordering': ['old_path'],
                 'unique_together': {('site', 'old_path')},
                 'db_table': 'django_redirect',
                 'verbose_name': 'redirect',
('django/contrib/sessions', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1 +0,0 @@
-default_app_config = 'django.contrib.sessions.apps.SessionsConfig'
('django/contrib/sessions', 'exceptions.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,4 +1,4 @@
-from django.core.exceptions import SuspiciousOperation
+from django.core.exceptions import BadRequest, SuspiciousOperation


 class InvalidSessionKey(SuspiciousOperation):
@@ -9,3 +9,8 @@
 class SuspiciousSession(SuspiciousOperation):
     """The session may be tampered with"""
     pass
+
+
+class SessionInterrupted(BadRequest):
+    """The session was interrupted."""
+    pass
('django/contrib/sessions', 'middleware.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,15 +3,17 @@

 from django.conf import settings
 from django.contrib.sessions.backends.base import UpdateError
-from django.core.exceptions import SuspiciousOperation
+from django.contrib.sessions.exceptions import SessionInterrupted
 from django.utils.cache import patch_vary_headers
 from django.utils.deprecation import MiddlewareMixin
 from django.utils.http import http_date


 class SessionMiddleware(MiddlewareMixin):
+    # RemovedInDjango40Warning: when the deprecation ends, replace with:
+    #   def __init__(self, get_response):
     def __init__(self, get_response=None):
-        self.get_response = get_response
+        super().__init__(get_response)
         engine = import_module(settings.SESSION_ENGINE)
         self.SessionStore = engine.SessionStore

@@ -30,45 +32,46 @@
             modified = request.session.modified
             empty = request.session.is_empty()
         except AttributeError:
-            pass
+            return response
+        # First check if we need to delete this cookie.
+        # The session should be deleted only if the session is entirely empty.
+        if settings.SESSION_COOKIE_NAME in request.COOKIES and empty:
+            response.delete_cookie(
+                settings.SESSION_COOKIE_NAME,
+                path=settings.SESSION_COOKIE_PATH,
+                domain=settings.SESSION_COOKIE_DOMAIN,
+                samesite=settings.SESSION_COOKIE_SAMESITE,
+            )
+            patch_vary_headers(response, ('Cookie',))
         else:
-            # First check if we need to delete this cookie.
-            # The session should be deleted only if the session is entirely empty
-            if settings.SESSION_COOKIE_NAME in request.COOKIES and empty:
-                response.delete_cookie(
-                    settings.SESSION_COOKIE_NAME,
-                    path=settings.SESSION_COOKIE_PATH,
-                    domain=settings.SESSION_COOKIE_DOMAIN,
-                )
-            else:
-                if accessed:
-                    patch_vary_headers(response, ('Cookie',))
-                if (modified or settings.SESSION_SAVE_EVERY_REQUEST) and not empty:
-                    if request.session.get_expire_at_browser_close():
-                        max_age = None
-                        expires = None
-                    else:
-                        max_age = request.session.get_expiry_age()
-                        expires_time = time.time() + max_age
-                        expires = http_date(expires_time)
-                    # Save the session data and refresh the client cookie.
-                    # Skip session save for 500 responses, refs #3881.
-                    if response.status_code != 500:
-                        try:
-                            request.session.save()
-                        except UpdateError:
-                            raise SuspiciousOperation(
-                                "The request's session was deleted before the "
-                                "request completed. The user may have logged "
-                                "out in a concurrent request, for example."
-                            )
-                        response.set_cookie(
-                            settings.SESSION_COOKIE_NAME,
-                            request.session.session_key, max_age=max_age,
-                            expires=expires, domain=settings.SESSION_COOKIE_DOMAIN,
-                            path=settings.SESSION_COOKIE_PATH,
-                            secure=settings.SESSION_COOKIE_SECURE or None,
-                            httponly=settings.SESSION_COOKIE_HTTPONLY or None,
-                            samesite=settings.SESSION_COOKIE_SAMESITE,
+            if accessed:
+                patch_vary_headers(response, ('Cookie',))
+            if (modified or settings.SESSION_SAVE_EVERY_REQUEST) and not empty:
+                if request.session.get_expire_at_browser_close():
+                    max_age = None
+                    expires = None
+                else:
+                    max_age = request.session.get_expiry_age()
+                    expires_time = time.time() + max_age
+                    expires = http_date(expires_time)
+                # Save the session data and refresh the client cookie.
+                # Skip session save for 500 responses, refs #3881.
+                if response.status_code != 500:
+                    try:
+                        request.session.save()
+                    except UpdateError:
+                        raise SessionInterrupted(
+                            "The request's session was deleted before the "
+                            "request completed. The user may have logged "
+                            "out in a concurrent request, for example."
                         )
+                    response.set_cookie(
+                        settings.SESSION_COOKIE_NAME,
+                        request.session.session_key, max_age=max_age,
+                        expires=expires, domain=settings.SESSION_COOKIE_DOMAIN,
+                        path=settings.SESSION_COOKIE_PATH,
+                        secure=settings.SESSION_COOKIE_SECURE or None,
+                        httponly=settings.SESSION_COOKIE_HTTPONLY or None,
+                        samesite=settings.SESSION_COOKIE_SAMESITE,
+                    )
         return response
('django/contrib/sessions/backends', 'signed_cookies.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,4 +1,3 @@
-from django.conf import settings
 from django.contrib.sessions.backends.base import SessionBase
 from django.core import signing

@@ -16,7 +15,7 @@
                 self.session_key,
                 serializer=self.serializer,
                 # This doesn't handle non-default expiry dates, see #19201
-                max_age=settings.SESSION_COOKIE_AGE,
+                max_age=self.get_session_cookie_age(),
                 salt='django.contrib.sessions.backends.signed_cookies',
             )
         except Exception:
('django/contrib/sessions/backends', 'file.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -18,7 +18,7 @@
     Implement a file based session store.
     """
     def __init__(self, session_key=None):
-        self.storage_path = type(self)._get_storage_path()
+        self.storage_path = self._get_storage_path()
         self.file_prefix = settings.SESSION_COOKIE_NAME
         super().__init__(session_key)

@@ -69,13 +69,13 @@
         Return the expiry time of the file storing the session's content.
         """
         return session_data.get('_session_expiry') or (
-            self._last_modification() + datetime.timedelta(seconds=settings.SESSION_COOKIE_AGE)
+            self._last_modification() + datetime.timedelta(seconds=self.get_session_cookie_age())
         )

     def load(self):
         session_data = {}
         try:
-            with open(self._key_to_file(), "r", encoding="ascii") as session_file:
+            with open(self._key_to_file(), encoding='ascii') as session_file:
                 file_data = session_file.read()
             # Don't fail if there is no data in the session file.
             # We may have opened the empty placeholder file.
@@ -94,7 +94,7 @@
                     session_data = {}
                     self.delete()
                     self.create()
-        except (IOError, SuspiciousOperation):
+        except (OSError, SuspiciousOperation):
             self._session_key = None
         return session_data

@@ -166,7 +166,7 @@
             finally:
                 if not renamed:
                     os.unlink(output_file_name)
-        except (OSError, IOError, EOFError):
+        except (EOFError, OSError):
             pass

     def exists(self, session_key):
('django/contrib/sessions/backends', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,16 +1,20 @@
 import base64
 import logging
 import string
+import warnings
 from datetime import datetime, timedelta

 from django.conf import settings
 from django.contrib.sessions.exceptions import SuspiciousSession
+from django.core import signing
 from django.core.exceptions import SuspiciousOperation
 from django.utils import timezone
 from django.utils.crypto import (
     constant_time_compare, get_random_string, salted_hmac,
 )
+from django.utils.deprecation import RemovedInDjango40Warning
 from django.utils.module_loading import import_string
+from django.utils.translation import LANGUAGE_SESSION_KEY

 # session_key should not be case sensitive because some backends can store it
 # on case insensitive file systems.
@@ -51,6 +55,13 @@
         return key in self._session

     def __getitem__(self, key):
+        if key == LANGUAGE_SESSION_KEY:
+            warnings.warn(
+                'The user language will no longer be stored in '
+                'request.session in Django 4.0. Read it from '
+                'request.COOKIES[settings.LANGUAGE_COOKIE_NAME] instead.',
+                RemovedInDjango40Warning, stacklevel=2,
+            )
         return self._session[key]

     def __setitem__(self, key, value):
@@ -60,6 +71,10 @@
     def __delitem__(self, key):
         del self._session[key]
         self.modified = True
+
+    @property
+    def key_salt(self):
+        return 'django.contrib.sessions.' + self.__class__.__qualname__

     def get(self, key, default=None):
         return self._session.get(key, default)
@@ -87,16 +102,45 @@
         del self[self.TEST_COOKIE_NAME]

     def _hash(self, value):
+        # RemovedInDjango40Warning: pre-Django 3.1 format will be invalid.
         key_salt = "django.contrib.sessions" + self.__class__.__name__
         return salted_hmac(key_salt, value).hexdigest()

     def encode(self, session_dict):
         "Return the given session dictionary serialized and encoded as a string."
+        # RemovedInDjango40Warning: DEFAULT_HASHING_ALGORITHM will be removed.
+        if settings.DEFAULT_HASHING_ALGORITHM == 'sha1':
+            return self._legacy_encode(session_dict)
+        return signing.dumps(
+            session_dict, salt=self.key_salt, serializer=self.serializer,
+            compress=True,
+        )
+
+    def decode(self, session_data):
+        try:
+            return signing.loads(session_data, salt=self.key_salt, serializer=self.serializer)
+        # RemovedInDjango40Warning: when the deprecation ends, handle here
+        # exceptions similar to what _legacy_decode() does now.
+        except signing.BadSignature:
+            try:
+                # Return an empty session if data is not in the pre-Django 3.1
+                # format.
+                return self._legacy_decode(session_data)
+            except Exception:
+                logger = logging.getLogger('django.security.SuspiciousSession')
+                logger.warning('Session data corrupted')
+                return {}
+        except Exception:
+            return self._legacy_decode(session_data)
+
+    def _legacy_encode(self, session_dict):
+        # RemovedInDjango40Warning.
         serialized = self.serializer().dumps(session_dict)
         hash = self._hash(serialized)
-        return base64.b64encode(hash.encode() + b":" + serialized).decode('ascii')
-
-    def decode(self, session_data):
+        return base64.b64encode(hash.encode() + b':' + serialized).decode('ascii')
+
+    def _legacy_decode(self, session_data):
+        # RemovedInDjango40Warning: pre-Django 3.1 format will be invalid.
         encoded_data = base64.b64decode(session_data.encode('ascii'))
         try:
             # could produce ValueError if there is no ':'
@@ -196,6 +240,9 @@

     _session = property(_get_session)

+    def get_session_cookie_age(self):
+        return settings.SESSION_COOKIE_AGE
+
     def get_expiry_age(self, **kwargs):
         """Get the number of seconds until the session expires.

@@ -215,7 +262,7 @@
             expiry = self.get('_session_expiry')

         if not expiry:   # Checks both None and 0 cases
-            return settings.SESSION_COOKIE_AGE
+            return self.get_session_cookie_age()
         if not isinstance(expiry, datetime):
             return expiry
         delta = expiry - modification
@@ -239,7 +286,7 @@

         if isinstance(expiry, datetime):
             return expiry
-        expiry = expiry or settings.SESSION_COOKIE_AGE   # Checks both None and 0 cases
+        expiry = expiry or self.get_session_cookie_age()
         return modification + timedelta(seconds=expiry)

     def set_expiry(self, value):
('django/contrib/sessions/management/commands', 'clearsessions.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,7 +1,7 @@
 from importlib import import_module

 from django.conf import settings
-from django.core.management.base import BaseCommand
+from django.core.management.base import BaseCommand, CommandError


 class Command(BaseCommand):
@@ -15,5 +15,7 @@
         try:
             engine.SessionStore.clear_expired()
         except NotImplementedError:
-            self.stderr.write("Session engine '%s' doesn't support clearing "
-                              "expired sessions.\n" % settings.SESSION_ENGINE)
+            raise CommandError(
+                "Session engine '%s' doesn't support clearing expired "
+                "sessions." % settings.SESSION_ENGINE
+            )
('django/contrib/sitemaps', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -60,32 +60,71 @@
     # with which the sitemap was requested.
     protocol = None

-    def __get(self, name, obj, default=None):
+    # Enables generating URLs for all languages.
+    i18n = False
+
+    # Override list of languages to use.
+    languages = None
+
+    # Enables generating alternate/hreflang links.
+    alternates = False
+
+    # Add an alternate/hreflang link with value 'x-default'.
+    x_default = False
+
+    def _get(self, name, item, default=None):
         try:
             attr = getattr(self, name)
         except AttributeError:
             return default
         if callable(attr):
-            return attr(obj)
+            if self.i18n:
+                # Split the (item, lang_code) tuples again for the location,
+                # priority, lastmod and changefreq method calls.
+                item, lang_code = item
+            return attr(item)
         return attr
+
+    def _languages(self):
+        if self.languages is not None:
+            return self.languages
+        return [lang_code for lang_code, _ in settings.LANGUAGES]
+
+    def _items(self):
+        if self.i18n:
+            # Create (item, lang_code) tuples for all items and languages.
+            # This is necessary to paginate with all languages already considered.
+            items = [
+                (item, lang_code)
+                for lang_code in self._languages()
+                for item in self.items()
+            ]
+            return items
+        return self.items()
+
+    def _location(self, item, force_lang_code=None):
+        if self.i18n:
+            obj, lang_code = item
+            # Activate language from item-tuple or forced one before calling location.
+            with translation.override(force_lang_code or lang_code):
+                return self._get('location', item)
+        return self._get('location', item)
+
+    @property
+    def paginator(self):
+        return paginator.Paginator(self._items(), self.limit)

     def items(self):
         return []

-    def location(self, obj):
-        return obj.get_absolute_url()
-
-    @property
-    def paginator(self):
-        return paginator.Paginator(self.items(), self.limit)
-
-    def get_urls(self, page=1, site=None, protocol=None):
+    def location(self, item):
+        return item.get_absolute_url()
+
+    def get_protocol(self, protocol=None):
         # Determine protocol
-        if self.protocol is not None:
-            protocol = self.protocol
-        if protocol is None:
-            protocol = 'http'
-
+        return self.protocol or protocol or 'http'
+
+    def get_domain(self, site=None):
         # Determine domain
         if site is None:
             if django_apps.is_installed('django.contrib.sites'):
@@ -99,43 +138,60 @@
                     "To use sitemaps, either enable the sites framework or pass "
                     "a Site/RequestSite object in your view."
                 )
-        domain = site.domain
-
-        if getattr(self, 'i18n', False):
-            urls = []
-            current_lang_code = translation.get_language()
-            for lang_code, lang_name in settings.LANGUAGES:
-                translation.activate(lang_code)
-                urls += self._urls(page, protocol, domain)
-            translation.activate(current_lang_code)
-        else:
-            urls = self._urls(page, protocol, domain)
-
-        return urls
+        return site.domain
+
+    def get_urls(self, page=1, site=None, protocol=None):
+        protocol = self.get_protocol(protocol)
+        domain = self.get_domain(site)
+        return self._urls(page, protocol, domain)

     def _urls(self, page, protocol, domain):
         urls = []
         latest_lastmod = None
         all_items_lastmod = True  # track if all items have a lastmod
-        for item in self.paginator.page(page).object_list:
-            loc = "%s://%s%s" % (protocol, domain, self.__get('location', item))
-            priority = self.__get('priority', item)
-            lastmod = self.__get('lastmod', item)
+
+        paginator_page = self.paginator.page(page)
+        for item in paginator_page.object_list:
+            loc = f'{protocol}://{domain}{self._location(item)}'
+            priority = self._get('priority', item)
+            lastmod = self._get('lastmod', item)
+
             if all_items_lastmod:
                 all_items_lastmod = lastmod is not None
                 if (all_items_lastmod and
                         (latest_lastmod is None or lastmod > latest_lastmod)):
                     latest_lastmod = lastmod
+
             url_info = {
                 'item': item,
                 'location': loc,
                 'lastmod': lastmod,
-                'changefreq': self.__get('changefreq', item),
+                'changefreq': self._get('changefreq', item),
                 'priority': str(priority if priority is not None else ''),
+                'alternates': [],
             }
+
+            if self.i18n and self.alternates:
+                for lang_code in self._languages():
+                    loc = f'{protocol}://{domain}{self._location(item, lang_code)}'
+                    url_info['alternates'].append({
+                        'location': loc,
+                        'lang_code': lang_code,
+                    })
+                if self.x_default:
+                    lang_code = settings.LANGUAGE_CODE
+                    loc = f'{protocol}://{domain}{self._location(item, lang_code)}'
+                    loc = loc.replace(f'/{lang_code}/', '/', 1)
+                    url_info['alternates'].append({
+                        'location': loc,
+                        'lang_code': 'x-default',
+                    })
+
             urls.append(url_info)
+
         if all_items_lastmod and latest_lastmod:
             self.latest_lastmod = latest_lastmod
+
         return urls


@@ -146,9 +202,9 @@
     def __init__(self, info_dict, priority=None, changefreq=None, protocol=None):
         self.queryset = info_dict['queryset']
         self.date_field = info_dict.get('date_field')
-        self.priority = priority
-        self.changefreq = changefreq
-        self.protocol = protocol
+        self.priority = self.priority or priority
+        self.changefreq = self.changefreq or changefreq
+        self.protocol = self.protocol or protocol

     def items(self):
         # Make sure to return a clone; we don't want premature evaluation.
@@ -158,6 +214,3 @@
         if self.date_field is not None:
             return getattr(item, self.date_field)
         return None
-
-
-default_app_config = 'django.contrib.sitemaps.apps.SiteMapsConfig'
('django/contrib/sitemaps', 'apps.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,5 +3,6 @@


 class SiteMapsConfig(AppConfig):
+    default_auto_field = 'django.db.models.AutoField'
     name = 'django.contrib.sitemaps'
     verbose_name = _("Site Maps")
('django/contrib/sitemaps', 'views.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -14,7 +14,7 @@
     @wraps(func)
     def inner(request, *args, **kwargs):
         response = func(request, *args, **kwargs)
-        response['X-Robots-Tag'] = 'noindex, noodp, noarchive'
+        response.headers['X-Robots-Tag'] = 'noindex, noodp, noarchive'
         return response
     return inner

@@ -88,5 +88,5 @@
     if all_sites_lastmod and lastmod is not None:
         # if lastmod is defined for all sites, set header so as
         # ConditionalGetMiddleware is able to send 304 NOT MODIFIED
-        response['Last-Modified'] = http_date(timegm(lastmod))
+        response.headers['Last-Modified'] = http_date(timegm(lastmod))
     return response
('django/contrib/sitemaps/templates', 'sitemap.xml')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,5 @@
 <?xml version="1.0" encoding="UTF-8"?>
-<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
+<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9" xmlns:xhtml="http://www.w3.org/1999/xhtml">
 {% spaceless %}
 {% for url in urlset %}
   <url>
@@ -7,7 +7,10 @@
     {% if url.lastmod %}<lastmod>{{ url.lastmod|date:"Y-m-d" }}</lastmod>{% endif %}
     {% if url.changefreq %}<changefreq>{{ url.changefreq }}</changefreq>{% endif %}
     {% if url.priority %}<priority>{{ url.priority }}</priority>{% endif %}
-   </url>
+    {% for alternate in url.alternates %}
+    <xhtml:link rel="alternate" hreflang="{{ alternate.lang_code }}" href="{{ alternate.location }}"/>
+    {% endfor %}
+  </url>
 {% endfor %}
 {% endspaceless %}
 </urlset>
('django/contrib/humanize', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1 +0,0 @@
-default_app_config = 'django.contrib.humanize.apps.HumanizeConfig'
('django/contrib/humanize/templatetags', 'humanize.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,14 +3,13 @@
 from decimal import Decimal

 from django import template
-from django.conf import settings
 from django.template import defaultfilters
 from django.utils.formats import number_format
 from django.utils.safestring import mark_safe
 from django.utils.timezone import is_aware, utc
 from django.utils.translation import (
     gettext as _, gettext_lazy, ngettext, ngettext_lazy, npgettext_lazy,
-    pgettext,
+    pgettext, round_away_from_one,
 )

 register = template.Library()
@@ -63,14 +62,14 @@
     Convert an integer to a string containing commas every three digits.
     For example, 3000 becomes '3,000' and 45000 becomes '45,000'.
     """
-    if settings.USE_L10N and use_l10n:
+    if use_l10n:
         try:
             if not isinstance(value, (float, Decimal)):
                 value = int(value)
         except (TypeError, ValueError):
             return intcomma(value, False)
         else:
-            return number_format(value, force_grouping=True)
+            return number_format(value, use_l10n=True, force_grouping=True)
     orig = str(value)
     new = re.sub(r"^(-?\d+)(\d{3})", r'\g<1>,\g<2>', orig)
     if orig == new:
@@ -81,50 +80,17 @@

 # A tuple of standard large number to their converters
 intword_converters = (
-    (6, lambda number: (
-        ngettext('%(value).1f million', '%(value).1f million', number),
-        ngettext('%(value)s million', '%(value)s million', number),
-    )),
-    (9, lambda number: (
-        ngettext('%(value).1f billion', '%(value).1f billion', number),
-        ngettext('%(value)s billion', '%(value)s billion', number),
-    )),
-    (12, lambda number: (
-        ngettext('%(value).1f trillion', '%(value).1f trillion', number),
-        ngettext('%(value)s trillion', '%(value)s trillion', number),
-    )),
-    (15, lambda number: (
-        ngettext('%(value).1f quadrillion', '%(value).1f quadrillion', number),
-        ngettext('%(value)s quadrillion', '%(value)s quadrillion', number),
-    )),
-    (18, lambda number: (
-        ngettext('%(value).1f quintillion', '%(value).1f quintillion', number),
-        ngettext('%(value)s quintillion', '%(value)s quintillion', number),
-    )),
-    (21, lambda number: (
-        ngettext('%(value).1f sextillion', '%(value).1f sextillion', number),
-        ngettext('%(value)s sextillion', '%(value)s sextillion', number),
-    )),
-    (24, lambda number: (
-        ngettext('%(value).1f septillion', '%(value).1f septillion', number),
-        ngettext('%(value)s septillion', '%(value)s septillion', number),
-    )),
-    (27, lambda number: (
-        ngettext('%(value).1f octillion', '%(value).1f octillion', number),
-        ngettext('%(value)s octillion', '%(value)s octillion', number),
-    )),
-    (30, lambda number: (
-        ngettext('%(value).1f nonillion', '%(value).1f nonillion', number),
-        ngettext('%(value)s nonillion', '%(value)s nonillion', number),
-    )),
-    (33, lambda number: (
-        ngettext('%(value).1f decillion', '%(value).1f decillion', number),
-        ngettext('%(value)s decillion', '%(value)s decillion', number),
-    )),
-    (100, lambda number: (
-        ngettext('%(value).1f googol', '%(value).1f googol', number),
-        ngettext('%(value)s googol', '%(value)s googol', number),
-    )),
+    (6, lambda number: ngettext('%(value)s million', '%(value)s million', number)),
+    (9, lambda number: ngettext('%(value)s billion', '%(value)s billion', number)),
+    (12, lambda number: ngettext('%(value)s trillion', '%(value)s trillion', number)),
+    (15, lambda number: ngettext('%(value)s quadrillion', '%(value)s quadrillion', number)),
+    (18, lambda number: ngettext('%(value)s quintillion', '%(value)s quintillion', number)),
+    (21, lambda number: ngettext('%(value)s sextillion', '%(value)s sextillion', number)),
+    (24, lambda number: ngettext('%(value)s septillion', '%(value)s septillion', number)),
+    (27, lambda number: ngettext('%(value)s octillion', '%(value)s octillion', number)),
+    (30, lambda number: ngettext('%(value)s nonillion', '%(value)s nonillion', number)),
+    (33, lambda number: ngettext('%(value)s decillion', '%(value)s decillion', number)),
+    (100, lambda number: ngettext('%(value)s googol', '%(value)s googol', number)),
 )


@@ -140,25 +106,18 @@
     except (TypeError, ValueError):
         return value

-    if value < 1000000:
-        return value
-
-    def _check_for_i18n(value, float_formatted, string_formatted):
-        """
-        Use the i18n enabled defaultfilters.floatformat if possible
-        """
-        if settings.USE_L10N:
-            value = defaultfilters.floatformat(value, 1)
-            template = string_formatted
-        else:
-            template = float_formatted
-        return template % {'value': value}
-
-    for exponent, converters in intword_converters:
+    abs_value = abs(value)
+    if abs_value < 1000000:
+        return value
+
+    for exponent, converter in intword_converters:
         large_number = 10 ** exponent
-        if value < large_number * 1000:
+        if abs_value < large_number * 1000:
             new_value = value / large_number
-            return _check_for_i18n(new_value, *converters(new_value))
+            rounded_value = round_away_from_one(new_value)
+            return converter(abs(rounded_value)) % {
+                'value': defaultfilters.floatformat(new_value, 1),
+            }
     return value


('django/contrib/contenttypes', 'models.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -104,7 +104,7 @@
     def get_for_id(self, id):
         """
         Lookup a ContentType by ID. Use the same shared cache as get_for_model
-        (though ContentTypes are obviously not created on-the-fly by get_by_id).
+        (though ContentTypes are not created on-the-fly by get_by_id).
         """
         try:
             ct = self._cache[self.db][id]
@@ -139,10 +139,10 @@
         verbose_name = _('content type')
         verbose_name_plural = _('content types')
         db_table = 'django_content_type'
-        unique_together = (('app_label', 'model'),)
+        unique_together = [['app_label', 'model']]

     def __str__(self):
-        return self.name
+        return self.app_labeled_name

     @property
     def name(self):
@@ -150,6 +150,13 @@
         if not model:
             return self.model
         return str(model._meta.verbose_name)
+
+    @property
+    def app_labeled_name(self):
+        model = self.model_class()
+        if not model:
+            return self.model
+        return '%s | %s' % (model._meta.app_label, model._meta.verbose_name)

     def model_class(self):
         """Return the model class for this type of content."""
('django/contrib/contenttypes', 'fields.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,15 +1,17 @@
+import functools
+import itertools
+import operator
 from collections import defaultdict

 from django.contrib.contenttypes.models import ContentType
 from django.core import checks
 from django.core.exceptions import FieldDoesNotExist, ObjectDoesNotExist
 from django.db import DEFAULT_DB_ALIAS, models, router, transaction
-from django.db.models import DO_NOTHING
+from django.db.models import DO_NOTHING, ForeignObject, ForeignObjectRel
 from django.db.models.base import ModelBase, make_foreign_order_accessors
 from django.db.models.fields.mixins import FieldCacheMixin
 from django.db.models.fields.related import (
-    ForeignObject, ForeignObjectRel, ReverseManyToOneDescriptor,
-    lazy_related_operation,
+    ReverseManyToOneDescriptor, lazy_related_operation,
 )
 from django.db.models.query_utils import PathInfo
 from django.utils.functional import cached_property
@@ -68,8 +70,7 @@

     def __str__(self):
         model = self.model
-        app = model._meta.app_label
-        return '%s.%s.%s' % (app, model._meta.object_name, self.name)
+        return '%s.%s' % (model._meta.label, self.name)

     def check(self, **kwargs):
         return [
@@ -275,6 +276,7 @@

     # Field flags
     auto_created = False
+    empty_strings_allowed = False

     many_to_many = False
     many_to_one = False
@@ -293,6 +295,9 @@
             limit_choices_to=limit_choices_to,
         )

+        # Reverse relations are always nullable (Django can't enforce that a
+        # foreign key on the related model points to this model).
+        kwargs['null'] = True
         kwargs['blank'] = True
         kwargs['on_delete'] = models.CASCADE
         kwargs['editable'] = False
@@ -337,9 +342,8 @@
                 return [
                     checks.Error(
                         "The GenericRelation defines a relation with the model "
-                        "'%s.%s', but that model does not have a GenericForeignKey." % (
-                            target._meta.app_label, target._meta.object_name
-                        ),
+                        "'%s', but that model does not have a GenericForeignKey."
+                        % target._meta.label,
                         obj=self,
                         id='contenttypes.E004',
                     )
@@ -515,17 +519,18 @@
             self.instance = instance

             self.model = rel.model
-
-            content_type = ContentType.objects.db_manager(instance._state.db).get_for_model(
-                instance, for_concrete_model=rel.field.for_concrete_model)
-            self.content_type = content_type
+            self.get_content_type = functools.partial(
+                ContentType.objects.db_manager(instance._state.db).get_for_model,
+                for_concrete_model=rel.field.for_concrete_model,
+            )
+            self.content_type = self.get_content_type(instance)
             self.content_type_field_name = rel.field.content_type_field_name
             self.object_id_field_name = rel.field.object_id_field_name
             self.prefetch_cache_name = rel.field.attname
             self.pk_val = instance.pk

             self.core_filters = {
-                '%s__pk' % self.content_type_field_name: content_type.id,
+                '%s__pk' % self.content_type_field_name: self.content_type.id,
                 self.object_id_field_name: self.pk_val,
             }

@@ -564,19 +569,29 @@

             queryset._add_hints(instance=instances[0])
             queryset = queryset.using(queryset._db or self._db)
-
-            query = {
-                '%s__pk' % self.content_type_field_name: self.content_type.id,
-                '%s__in' % self.object_id_field_name: {obj.pk for obj in instances}
-            }
-
+            # Group instances by content types.
+            content_type_queries = (
+                models.Q(**{
+                    '%s__pk' % self.content_type_field_name: content_type_id,
+                    '%s__in' % self.object_id_field_name: {obj.pk for obj in objs}
+                })
+                for content_type_id, objs in itertools.groupby(
+                    sorted(instances, key=lambda obj: self.get_content_type(obj).pk),
+                    lambda obj: self.get_content_type(obj).pk,
+                )
+            )
+            query = functools.reduce(operator.or_, content_type_queries)
             # We (possibly) need to convert object IDs to the type of the
             # instances' PK in order to match up instances:
             object_id_converter = instances[0]._meta.pk.to_python
+            content_type_id_field_name = '%s_id' % self.content_type_field_name
             return (
-                queryset.filter(**query),
-                lambda relobj: object_id_converter(getattr(relobj, self.object_id_field_name)),
-                lambda obj: obj.pk,
+                queryset.filter(query),
+                lambda relobj: (
+                    object_id_converter(getattr(relobj, self.object_id_field_name)),
+                    getattr(relobj, content_type_id_field_name),
+                ),
+                lambda obj: (obj.pk, self.get_content_type(obj).pk),
                 False,
                 self.prefetch_cache_name,
                 False,
('django/contrib/contenttypes', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1 +0,0 @@
-default_app_config = 'django.contrib.contenttypes.apps.ContentTypesConfig'
('django/contrib/contenttypes', 'apps.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -12,6 +12,7 @@


 class ContentTypesConfig(AppConfig):
+    default_auto_field = 'django.db.models.AutoField'
     name = 'django.contrib.contenttypes'
     verbose_name = _("Content Types")

('django/contrib/contenttypes', 'forms.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -56,7 +56,8 @@
                                   extra=3, can_order=False, can_delete=True,
                                   max_num=None, formfield_callback=None,
                                   validate_max=False, for_concrete_model=True,
-                                  min_num=None, validate_min=False):
+                                  min_num=None, validate_min=False,
+                                  absolute_max=None, can_delete_extra=True):
     """
     Return a ``GenericInlineFormSet`` for the given kwargs.

@@ -75,6 +76,7 @@
         formset=formset, extra=extra, can_delete=can_delete,
         can_order=can_order, fields=fields, exclude=exclude, max_num=max_num,
         validate_max=validate_max, min_num=min_num, validate_min=validate_min,
+        absolute_max=absolute_max, can_delete_extra=can_delete_extra,
     )
     FormSet.ct_field = ct_field
     FormSet.ct_fk_field = fk_field
('django/contrib/contenttypes', 'admin.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -28,9 +28,7 @@
         if not gfks:
             return [
                 checks.Error(
-                    "'%s.%s' has no GenericForeignKey." % (
-                        obj.model._meta.app_label, obj.model._meta.object_name
-                    ),
+                    "'%s' has no GenericForeignKey." % obj.model._meta.label,
                     obj=obj.__class__,
                     id='admin.E301'
                 )
@@ -42,8 +40,8 @@
             except FieldDoesNotExist:
                 return [
                     checks.Error(
-                        "'ct_field' references '%s', which is not a field on '%s.%s'." % (
-                            obj.ct_field, obj.model._meta.app_label, obj.model._meta.object_name
+                        "'ct_field' references '%s', which is not a field on '%s'." % (
+                            obj.ct_field, obj.model._meta.label,
                         ),
                         obj=obj.__class__,
                         id='admin.E302'
@@ -55,8 +53,8 @@
             except FieldDoesNotExist:
                 return [
                     checks.Error(
-                        "'ct_fk_field' references '%s', which is not a field on '%s.%s'." % (
-                            obj.ct_fk_field, obj.model._meta.app_label, obj.model._meta.object_name
+                        "'ct_fk_field' references '%s', which is not a field on '%s'." % (
+                            obj.ct_fk_field, obj.model._meta.label,
                         ),
                         obj=obj.__class__,
                         id='admin.E303'
@@ -71,8 +69,8 @@

             return [
                 checks.Error(
-                    "'%s.%s' has no GenericForeignKey using content type field '%s' and object ID field '%s'." % (
-                        obj.model._meta.app_label, obj.model._meta.object_name, obj.ct_field, obj.ct_fk_field
+                    "'%s' has no GenericForeignKey using content type field '%s' and object ID field '%s'." % (
+                        obj.model._meta.label, obj.ct_field, obj.ct_fk_field,
                     ),
                     obj=obj.__class__,
                     id='admin.E304'
('django/contrib/contenttypes', 'views.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -21,7 +21,7 @@
         obj = content_type.get_object_for_this_type(pk=object_id)
     except (ObjectDoesNotExist, ValueError):
         raise Http404(
-            _("Content type %(ct_id)s object %(obj_id)s doesn't exist") %
+            _('Content type %(ct_id)s object %(obj_id)s doesn’t exist') %
             {'ct_id': content_type_id, 'obj_id': object_id}
         )

@@ -29,7 +29,7 @@
         get_absolute_url = obj.get_absolute_url
     except AttributeError:
         raise Http404(
-            _("%(ct_name)s objects don't have a get_absolute_url() method") %
+            _('%(ct_name)s objects don’t have a get_absolute_url() method') %
             {'ct_name': content_type.name}
         )
     absurl = get_absolute_url()
('django/contrib/contenttypes/management', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,7 @@
 from django.apps import apps as global_apps
-from django.db import DEFAULT_DB_ALIAS, migrations, router, transaction
-from django.db.utils import IntegrityError
+from django.db import (
+    DEFAULT_DB_ALIAS, IntegrityError, migrations, router, transaction,
+)


 class RenameContentType(migrations.RunPython):
@@ -24,14 +25,14 @@
             content_type.model = new_model
             try:
                 with transaction.atomic(using=db):
-                    content_type.save(update_fields={'model'})
+                    content_type.save(using=db, update_fields={'model'})
             except IntegrityError:
                 # Gracefully fallback if a stale content type causes a
                 # conflict as remove_stale_contenttypes will take care of
                 # asking the user what should be done next.
                 content_type.model = old_model
             else:
-                # Clear the cache as the `get_by_natual_key()` call will cache
+                # Clear the cache as the `get_by_natural_key()` call will cache
                 # the renamed ContentType instance by its old model name.
                 ContentType.objects.clear_cache()

('django/contrib/contenttypes/management/commands', 'remove_stale_contenttypes.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,10 +1,10 @@
+import itertools
+
 from django.apps import apps
 from django.contrib.contenttypes.models import ContentType
 from django.core.management import BaseCommand
 from django.db import DEFAULT_DB_ALIAS, router
 from django.db.models.deletion import Collector
-
-from ...management import get_contenttypes_and_models


 class Command(BaseCommand):
@@ -18,18 +18,32 @@
             '--database', default=DEFAULT_DB_ALIAS,
             help='Nominates the database to use. Defaults to the "default" database.',
         )
+        parser.add_argument(
+            '--include-stale-apps', action='store_true', default=False,
+            help=(
+                "Deletes stale content types including ones from previously "
+                "installed apps that have been removed from INSTALLED_APPS."
+            ),
+        )

     def handle(self, **options):
         db = options['database']
+        include_stale_apps = options['include_stale_apps']
         interactive = options['interactive']
         verbosity = options['verbosity']

-        for app_config in apps.get_app_configs():
-            content_types, app_models = get_contenttypes_and_models(app_config, db, ContentType)
-            to_remove = [
-                ct for (model_name, ct) in content_types.items()
-                if model_name not in app_models
-            ]
+        if not router.allow_migrate_model(db, ContentType):
+            return
+        ContentType.objects.clear_cache()
+
+        apps_content_types = itertools.groupby(
+            ContentType.objects.using(db).order_by('app_label', 'model'),
+            lambda obj: obj.app_label,
+        )
+        for app_label, content_types in apps_content_types:
+            if not include_stale_apps and app_label not in apps.app_configs:
+                continue
+            to_remove = [ct for ct in content_types if ct.model_class() is None]
             # Confirm that the content type is stale before deletion.
             using = router.db_for_write(ContentType)
             if to_remove:
@@ -57,7 +71,7 @@
 models (uncommon).

 Are you sure you want to delete these content types?
-If you're unsure, answer 'no'.\n""" % content_type_display)
+If you're unsure, answer 'no'.""" % content_type_display)
                     ok_to_delete = input("Type 'yes' to continue, or 'no' to cancel: ")
                 else:
                     ok_to_delete = 'yes'
('django/contrib/gis', 'measure.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -88,6 +88,9 @@
             return self.standard == other.standard
         else:
             return NotImplemented
+
+    def __hash__(self):
+        return hash(self.standard)

     def __lt__(self, other):
         if isinstance(other, self.__class__):
@@ -233,6 +236,7 @@
         'clarke_link': 0.201166195164,
         'fathom': 1.8288,
         'ft': 0.3048,
+        'furlong': 201.168,
         'german_m': 1.0000135965,
         'gold_coast_ft': 0.304799710181508,
         'indian_yd': 0.914398530744,
@@ -280,6 +284,7 @@
         'Chain (Benoit)': 'chain_benoit',
         'Chain (Sears)': 'chain_sears',
         'Foot (International)': 'ft',
+        'Furrow Long': 'furlong',
         'German legal metre': 'german_m',
         'Gold Coast foot': 'gold_coast_ft',
         'Indian yard': 'indian_yd',
('django/contrib/gis', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1 +0,0 @@
-default_app_config = 'django.contrib.gis.apps.GISConfig'
('django/contrib/gis', 'apps.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,6 +4,7 @@


 class GISConfig(AppConfig):
+    default_auto_field = 'django.db.models.AutoField'
     name = 'django.contrib.gis'
     verbose_name = _("GIS")

('django/contrib/gis', 'geometry.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,13 +1,17 @@
 import re
+
+from django.utils.regex_helper import _lazy_re_compile

 # Regular expression for recognizing HEXEWKB and WKT.  A prophylactic measure
 # to prevent potentially malicious input from reaching the underlying C
 # library.  Not a substitute for good Web security programming practices.
-hex_regex = re.compile(r'^[0-9A-F]+$', re.I)
-wkt_regex = re.compile(r'^(SRID=(?P<srid>\-?\d+);)?'
-                       r'(?P<wkt>'
-                       r'(?P<type>POINT|LINESTRING|LINEARRING|POLYGON|MULTIPOINT|'
-                       r'MULTILINESTRING|MULTIPOLYGON|GEOMETRYCOLLECTION)'
-                       r'[ACEGIMLONPSRUTYZ\d,\.\-\+\(\) ]+)$',
-                       re.I)
-json_regex = re.compile(r'^(\s+)?\{.*}(\s+)?$', re.DOTALL)
+hex_regex = _lazy_re_compile(r'^[0-9A-F]+$', re.I)
+wkt_regex = _lazy_re_compile(
+    r'^(SRID=(?P<srid>\-?\d+);)?'
+    r'(?P<wkt>'
+    r'(?P<type>POINT|LINESTRING|LINEARRING|POLYGON|MULTIPOINT|'
+    r'MULTILINESTRING|MULTIPOLYGON|GEOMETRYCOLLECTION)'
+    r'[ACEGIMLONPSRUTYZ\d,\.\-\+\(\) ]+)$',
+    re.I
+)
+json_regex = _lazy_re_compile(r'^(\s+)?\{.*}(\s+)?$', re.DOTALL)
('django/contrib/gis', 'views.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -11,7 +11,7 @@
     try:
         f = feed_dict[slug]
     except KeyError:
-        raise Http404(_("Slug %r isn't registered.") % slug)
+        raise Http404(_('Slug %r isn’t registered.') % slug)

     instance = f()
     instance.feed_url = getattr(f, 'feed_url', None) or request.path
('django/contrib/gis/geos', 'linestring.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -91,7 +91,8 @@

     def __iter__(self):
         "Allow iteration over this LineString."
-        return iter(self._cs)
+        for i in range(len(self)):
+            yield self[i]

     def __len__(self):
         "Return the number of points in this LineString."
@@ -105,6 +106,7 @@
     def _set_list(self, length, items):
         ndim = self._cs.dims
         hasz = self._cs.hasz  # I don't understand why these are different
+        srid = self.srid

         # create a new coordinate sequence and populate accordingly
         cs = GEOSCoordSeq(capi.create_cs(length, ndim), z=hasz)
@@ -115,6 +117,8 @@
         if ptr:
             capi.destroy_geom(self.ptr)
             self.ptr = ptr
+            if srid is not None:
+                self.srid = srid
             self._post_init()
         else:
             # can this happen?
@@ -173,3 +177,11 @@
 class LinearRing(LineString):
     _minlength = 4
     _init_func = capi.create_linearring
+
+    @property
+    def is_counterclockwise(self):
+        if self.empty:
+            raise ValueError(
+                'Orientation of an empty LinearRing cannot be determined.'
+            )
+        return self._cs.is_counterclockwise
('django/contrib/gis/geos', 'coordseq.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,12 +3,12 @@
  by GEOSGeometry to house the actual coordinates of the Point,
  LineString, and LinearRing geometries.
 """
-from ctypes import byref, c_double, c_uint
+from ctypes import byref, c_byte, c_double, c_uint

 from django.contrib.gis.geos import prototypes as capi
 from django.contrib.gis.geos.base import GEOSBase
 from django.contrib.gis.geos.error import GEOSException
-from django.contrib.gis.geos.libgeos import CS_PTR
+from django.contrib.gis.geos.libgeos import CS_PTR, geos_version_tuple
 from django.contrib.gis.shortcuts import numpy


@@ -31,7 +31,7 @@

     def __len__(self):
         "Return the number of points in the coordinate sequence."
-        return int(self.size)
+        return self.size

     def __str__(self):
         "Return the string representation of the coordinate sequence."
@@ -194,3 +194,23 @@
         if n == 1:
             return get_point(0)
         return tuple(get_point(i) for i in range(n))
+
+    @property
+    def is_counterclockwise(self):
+        """Return whether this coordinate sequence is counterclockwise."""
+        if geos_version_tuple() < (3, 7):
+            # A modified shoelace algorithm to determine polygon orientation.
+            # See https://en.wikipedia.org/wiki/Shoelace_formula.
+            area = 0.0
+            n = len(self)
+            for i in range(n):
+                j = (i + 1) % n
+                area += self[i][0] * self[j][1]
+                area -= self[j][0] * self[i][1]
+            return area > 0.0
+        ret = c_byte()
+        if not capi.cs_is_ccw(self.ptr, byref(ret)):
+            raise GEOSException(
+                'Error encountered in GEOS C function "%s".' % capi.cs_is_ccw.func_name
+            )
+        return ret.value == 1
('django/contrib/gis/geos', 'point.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -76,8 +76,11 @@
     def _set_list(self, length, items):
         ptr = self._create_point(length, items)
         if ptr:
+            srid = self.srid
             capi.destroy_geom(self.ptr)
             self._ptr = ptr
+            if srid is not None:
+                self.srid = srid
             self._post_init()
         else:
             # can this happen?
('django/contrib/gis/geos', 'collections.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -5,9 +5,8 @@
 from ctypes import byref, c_int, c_uint

 from django.contrib.gis.geos import prototypes as capi
-from django.contrib.gis.geos.error import GEOSException
 from django.contrib.gis.geos.geometry import GEOSGeometry, LinearGeometryMixin
-from django.contrib.gis.geos.libgeos import GEOM_PTR, geos_version_tuple
+from django.contrib.gis.geos.libgeos import GEOM_PTR
 from django.contrib.gis.geos.linestring import LinearRing, LineString
 from django.contrib.gis.geos.point import Point
 from django.contrib.gis.geos.polygon import Polygon
@@ -98,12 +97,6 @@
     _allowed = (LineString, LinearRing)
     _typeid = 5

-    @property
-    def closed(self):
-        if geos_version_tuple() < (3, 5):
-            raise GEOSException("MultiLineString.closed requires GEOS >= 3.5.0.")
-        return super().closed
-

 class MultiPolygon(GeometryCollection):
     _allowed = Polygon
('django/contrib/gis/geos', 'geometry.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -18,7 +18,7 @@
     ewkb_w, wkb_r, wkb_w, wkt_r, wkt_w,
 )
 from django.utils.deconstruct import deconstructible
-from django.utils.encoding import force_bytes, force_text
+from django.utils.encoding import force_bytes, force_str


 class GEOSGeometryBase(GEOSBase):
@@ -37,12 +37,13 @@
             if cls is None:
                 if GEOSGeometryBase._GEOS_CLASSES is None:
                     # Inner imports avoid import conflicts with GEOSGeometry.
-                    from .linestring import LineString, LinearRing
+                    from .collections import (
+                        GeometryCollection, MultiLineString, MultiPoint,
+                        MultiPolygon,
+                    )
+                    from .linestring import LinearRing, LineString
                     from .point import Point
                     from .polygon import Polygon
-                    from .collections import (
-                        GeometryCollection, MultiPoint, MultiLineString, MultiPolygon,
-                    )
                     GEOSGeometryBase._GEOS_CLASSES = {
                         0: Point,
                         1: LineString,
@@ -122,7 +123,7 @@
             match = re.match(br'SRID=(?P<srid>\-?\d+)', srid_part)
             if not match:
                 raise ValueError('EWKT has invalid SRID part.')
-            srid = int(match.group('srid'))
+            srid = int(match['srid'])
         else:
             wkt = ewkt
         if not wkt:
@@ -434,7 +435,7 @@
         if self.srid:
             try:
                 return gdal.SpatialReference(self.srid)
-            except gdal.SRSException:
+            except (gdal.GDALException, gdal.SRSException):
                 pass
         return None

@@ -447,7 +448,7 @@
         """
         Requires GDAL. Transform the geometry according to the given
         transformation object, which may be an integer SRID, and WKT or
-        PROJ.4 string. By default, transform the geometry in-place and return
+        PROJ string. By default, transform the geometry in-place and return
         nothing. However if the `clone` keyword is set, don't modify the
         geometry and return a transformed clone instead.
         """
@@ -695,14 +696,14 @@
         """
         input_srid = None
         if isinstance(geo_input, bytes):
-            geo_input = force_text(geo_input)
+            geo_input = force_str(geo_input)
         if isinstance(geo_input, str):
             wkt_m = wkt_regex.match(geo_input)
             if wkt_m:
                 # Handle WKT input.
-                if wkt_m.group('srid'):
-                    input_srid = int(wkt_m.group('srid'))
-                g = self._from_wkt(force_bytes(wkt_m.group('wkt')))
+                if wkt_m['srid']:
+                    input_srid = int(wkt_m['srid'])
+                g = self._from_wkt(force_bytes(wkt_m['wkt']))
             elif hex_regex.match(geo_input):
                 # Handle HEXEWKB input.
                 g = wkb_r().read(force_bytes(geo_input))
('django/contrib/gis/geos', 'libgeos.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -23,8 +23,7 @@
     try:
         from django.conf import settings
         lib_path = settings.GEOS_LIBRARY_PATH
-    except (AttributeError, EnvironmentError,
-            ImportError, ImproperlyConfigured):
+    except (AttributeError, ImportError, ImproperlyConfigured, OSError):
         lib_path = None

     # Setting the appropriate names for the GEOS-C library.
@@ -58,7 +57,7 @@
     # Getting the GEOS C library.  The C interface (CDLL) is used for
     # both *NIX and Windows.
     # See the GEOS C API source code for more details on the library function calls:
-    #  http://geos.refractions.net/ro/doxygen_docs/html/geos__c_8h-source.html
+    # https://geos.osgeo.org/doxygen/geos__c_8h_source.html
     _lgeos = CDLL(lib_path)
     # Here we set up the prototypes for the initGEOS_r and finishGEOS_r
     # routines.  These functions aren't actually called until they are
@@ -140,7 +139,7 @@
     restype = None
     errcheck = None

-    def __init__(self, func_name, *args, restype=None, errcheck=None, argtypes=None, **kwargs):
+    def __init__(self, func_name, *, restype=None, errcheck=None, argtypes=None):
         self.func_name = func_name
         if restype is not None:
             self.restype = restype
@@ -148,11 +147,9 @@
             self.errcheck = errcheck
         if argtypes is not None:
             self.argtypes = argtypes
-        self.args = args
-        self.kwargs = kwargs

-    def __call__(self, *args, **kwargs):
-        return self.func(*args, **kwargs)
+    def __call__(self, *args):
+        return self.func(*args)

     @cached_property
     def func(self):
('django/contrib/gis/geos/prototypes', 'geom.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,8 +1,8 @@
-from ctypes import POINTER, c_char_p, c_int, c_size_t, c_ubyte
+from ctypes import POINTER, c_char_p, c_int, c_ubyte

 from django.contrib.gis.geos.libgeos import CS_PTR, GEOM_PTR, GEOSFuncFactory
 from django.contrib.gis.geos.prototypes.errcheck import (
-    check_geom, check_minus_one, check_sized_string, check_string,
+    check_geom, check_minus_one, check_string,
 )

 # This is the return type used by binary output (WKB, HEX) routines.
@@ -21,21 +21,6 @@


 # ### ctypes factory classes ###
-class BinConstructor(GEOSFuncFactory):
-    "Generate a prototype for binary construction (HEX, WKB) GEOS routines."
-    argtypes = [c_char_p, c_size_t]
-    restype = GEOM_PTR
-    errcheck = staticmethod(check_geom)
-
-
-# HEX & WKB output
-class BinOutput(GEOSFuncFactory):
-    "Generate a prototype for the routines that return a sized string."
-    argtypes = [GEOM_PTR, POINTER(c_size_t)]
-    restype = c_uchar_p
-    errcheck = staticmethod(check_sized_string)
-
-
 class GeomOutput(GEOSFuncFactory):
     "For GEOS routines that return a geometry."
     restype = GEOM_PTR
('django/contrib/gis/geos/prototypes', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -6,7 +6,8 @@

 from django.contrib.gis.geos.prototypes.coordseq import (  # NOQA
     create_cs, cs_clone, cs_getdims, cs_getordinate, cs_getsize, cs_getx,
-    cs_gety, cs_getz, cs_setordinate, cs_setx, cs_sety, cs_setz, get_cs,
+    cs_gety, cs_getz, cs_is_ccw, cs_setordinate, cs_setx, cs_sety, cs_setz,
+    get_cs,
 )
 from django.contrib.gis.geos.prototypes.geom import (  # NOQA
     create_collection, create_empty_polygon, create_linearring,
('django/contrib/gis/geos/prototypes', 'coordseq.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,4 +1,4 @@
-from ctypes import POINTER, c_double, c_int, c_uint
+from ctypes import POINTER, c_byte, c_double, c_int, c_uint

 from django.contrib.gis.geos.libgeos import CS_PTR, GEOM_PTR, GEOSFuncFactory
 from django.contrib.gis.geos.prototypes.errcheck import (
@@ -89,3 +89,5 @@
 # These routines return size & dimensions.
 cs_getsize = CsInt('GEOSCoordSeq_getSize')
 cs_getdims = CsInt('GEOSCoordSeq_getDimensions')
+
+cs_is_ccw = GEOSFuncFactory('GEOSCoordSeq_isCCW', restype=c_int, argtypes=[CS_PTR, POINTER(c_byte)])
('django/contrib/gis/forms', 'fields.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,7 @@
 from django import forms
 from django.contrib.gis.gdal import GDALException
 from django.contrib.gis.geos import GEOSException, GEOSGeometry
+from django.core.exceptions import ValidationError
 from django.utils.translation import gettext_lazy as _

 from .widgets import OpenLayersWidget
@@ -47,7 +48,7 @@
                 except (GEOSException, ValueError, TypeError):
                     value = None
             if value is None:
-                raise forms.ValidationError(self.error_messages['invalid_geom'], code='invalid_geom')
+                raise ValidationError(self.error_messages['invalid_geom'], code='invalid_geom')

         # Try to set the srid
         if not value.srid:
@@ -70,15 +71,15 @@

         # Ensuring that the geometry is of the correct type (indicated
         # using the OGC string label).
-        if str(geom.geom_type).upper() != self.geom_type and not self.geom_type == 'GEOMETRY':
-            raise forms.ValidationError(self.error_messages['invalid_geom_type'], code='invalid_geom_type')
+        if str(geom.geom_type).upper() != self.geom_type and self.geom_type != 'GEOMETRY':
+            raise ValidationError(self.error_messages['invalid_geom_type'], code='invalid_geom_type')

         # Transforming the geometry if the SRID was set.
         if self.srid and self.srid != -1 and self.srid != geom.srid:
             try:
                 geom.transform(self.srid)
             except GEOSException:
-                raise forms.ValidationError(
+                raise ValidationError(
                     self.error_messages['transform_error'], code='transform_error')

         return geom
@@ -89,7 +90,7 @@
         try:
             data = self.to_python(data)
             initial = self.to_python(initial)
-        except forms.ValidationError:
+        except ValidationError:
             return True

         # Only do a geographic comparison if both values are available
('django/contrib/gis/forms', 'widgets.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -61,11 +61,12 @@
                         value.srid, self.map_srid, err
                     )

+        geom_type = gdal.OGRGeomType(self.attrs['geom_type']).name
         context.update(self.build_attrs(self.attrs, {
             'name': name,
             'module': 'geodjango_%s' % name.replace('-', '_'),  # JS-safe
             'serialized': self.serialize(value),
-            'geom_type': gdal.OGRGeomType(self.attrs['geom_type']),
+            'geom_type': 'Geometry' if geom_type == 'Unknown' else geom_type,
             'STATIC_URL': settings.STATIC_URL,
             'LANGUAGE_BIDI': translation.get_language_bidi(),
             **(attrs or {}),
('django/contrib/gis/gdal', 'field.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,11 +4,11 @@
 from django.contrib.gis.gdal.base import GDALBase
 from django.contrib.gis.gdal.error import GDALException
 from django.contrib.gis.gdal.prototypes import ds as capi
-from django.utils.encoding import force_text
+from django.utils.encoding import force_str


 # For more information, see the OGR C API source code:
-#  https://www.gdal.org/ogr__api_8h.html
+#  https://gdal.org/api/vector_c_api.html
 #
 # The OGR_Fld_* routines are relevant here.
 class Field(GDALBase):
@@ -34,11 +34,6 @@
         # Setting the class depending upon the OGR Field Type (OFT)
         self.__class__ = OGRFieldTypes[self.type]

-        # OFTReal with no precision should be an OFTInteger.
-        if isinstance(self, OFTReal) and self.precision == 0:
-            self.__class__ = OFTInteger
-            self._double = True
-
     def __str__(self):
         "Return the string representation of the Field."
         return str(self.value).strip()
@@ -46,22 +41,26 @@
     # #### Field Methods ####
     def as_double(self):
         "Retrieve the Field's value as a double (float)."
-        return capi.get_field_as_double(self._feat.ptr, self._index)
+        return capi.get_field_as_double(self._feat.ptr, self._index) if self.is_set else None

     def as_int(self, is_64=False):
         "Retrieve the Field's value as an integer."
         if is_64:
-            return capi.get_field_as_integer64(self._feat.ptr, self._index)
+            return capi.get_field_as_integer64(self._feat.ptr, self._index) if self.is_set else None
         else:
-            return capi.get_field_as_integer(self._feat.ptr, self._index)
+            return capi.get_field_as_integer(self._feat.ptr, self._index) if self.is_set else None

     def as_string(self):
         "Retrieve the Field's value as a string."
+        if not self.is_set:
+            return None
         string = capi.get_field_as_string(self._feat.ptr, self._index)
-        return force_text(string, encoding=self._feat.encoding, strings_only=True)
+        return force_str(string, encoding=self._feat.encoding, strings_only=True)

     def as_datetime(self):
         "Retrieve the Field's value as a tuple of date & time components."
+        if not self.is_set:
+            return None
         yy, mm, dd, hh, mn, ss, tz = [c_int() for i in range(7)]
         status = capi.get_field_as_datetime(
             self._feat.ptr, self._index, byref(yy), byref(mm), byref(dd),
@@ -73,10 +72,15 @@

     # #### Field Properties ####
     @property
+    def is_set(self):
+        "Return True if the value of this field isn't null, False otherwise."
+        return capi.is_field_set(self._feat.ptr, self._index)
+
+    @property
     def name(self):
         "Return the name of this Field."
         name = capi.get_field_name(self.ptr)
-        return force_text(name, encoding=self._feat.encoding, strings_only=True)
+        return force_str(name, encoding=self._feat.encoding, strings_only=True)

     @property
     def precision(self):
@@ -107,18 +111,12 @@

 # ### The Field sub-classes for each OGR Field type. ###
 class OFTInteger(Field):
-    _double = False
     _bit64 = False

     @property
     def value(self):
         "Return an integer contained in this field."
-        if self._double:
-            # If this is really from an OFTReal field with no precision,
-            # read as a double and cast as Python int (to prevent overflow).
-            return int(self.as_double())
-        else:
-            return self.as_int(self._bit64)
+        return self.as_int(self._bit64)

     @property
     def type(self):
@@ -158,7 +156,7 @@
         try:
             yy, mm, dd, hh, mn, ss, tz = self.as_datetime()
             return date(yy.value, mm.value, dd.value)
-        except (ValueError, GDALException):
+        except (TypeError, ValueError, GDALException):
             return None


@@ -173,7 +171,7 @@
         try:
             yy, mm, dd, hh, mn, ss, tz = self.as_datetime()
             return datetime(yy.value, mm.value, dd.value, hh.value, mn.value, ss.value)
-        except (ValueError, GDALException):
+        except (TypeError, ValueError, GDALException):
             return None


('django/contrib/gis/gdal', 'srs.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -27,11 +27,18 @@
   NAD83 / Texas South Central
 """
 from ctypes import byref, c_char_p, c_int
+from enum import IntEnum

 from django.contrib.gis.gdal.base import GDALBase
 from django.contrib.gis.gdal.error import SRSException
+from django.contrib.gis.gdal.libgdal import GDAL_VERSION
 from django.contrib.gis.gdal.prototypes import srs as capi
-from django.utils.encoding import force_bytes, force_text
+from django.utils.encoding import force_bytes, force_str
+
+
+class AxisOrder(IntEnum):
+    TRADITIONAL = 0
+    AUTHORITY = 1


 class SpatialReference(GDALBase):
@@ -42,17 +49,25 @@
     """
     destructor = capi.release_srs

-    def __init__(self, srs_input='', srs_type='user'):
+    def __init__(self, srs_input='', srs_type='user', axis_order=None):
         """
         Create a GDAL OSR Spatial Reference object from the given input.
         The input may be string of OGC Well Known Text (WKT), an integer
-        EPSG code, a PROJ.4 string, and/or a projection "well known" shorthand
+        EPSG code, a PROJ string, and/or a projection "well known" shorthand
         string (one of 'WGS84', 'WGS72', 'NAD27', 'NAD83').
         """
-
+        if not isinstance(axis_order, (type(None), AxisOrder)):
+            raise ValueError(
+                'SpatialReference.axis_order must be an AxisOrder instance.'
+            )
+        self.axis_order = axis_order or AxisOrder.TRADITIONAL
         if srs_type == 'wkt':
             self.ptr = capi.new_srs(c_char_p(b''))
             self.import_wkt(srs_input)
+            if self.axis_order == AxisOrder.TRADITIONAL and GDAL_VERSION >= (3, 0):
+                capi.set_axis_strategy(self.ptr, self.axis_order)
+            elif self.axis_order != AxisOrder.TRADITIONAL and GDAL_VERSION < (3, 0):
+                raise ValueError('%s is not supported in GDAL < 3.0.' % self.axis_order)
             return
         elif isinstance(srs_input, str):
             try:
@@ -85,6 +100,10 @@
         else:
             self.ptr = srs

+        if self.axis_order == AxisOrder.TRADITIONAL and GDAL_VERSION >= (3, 0):
+            capi.set_axis_strategy(self.ptr, self.axis_order)
+        elif self.axis_order != AxisOrder.TRADITIONAL and GDAL_VERSION < (3, 0):
+            raise ValueError('%s is not supported in GDAL < 3.0.' % self.axis_order)
         # Importing from either the user input string or an integer SRID.
         if srs_type == 'user':
             self.import_user_input(srs_input)
@@ -143,7 +162,7 @@

     def clone(self):
         "Return a clone of this SpatialReference object."
-        return SpatialReference(capi.clone_srs(self.ptr))
+        return SpatialReference(capi.clone_srs(self.ptr), axis_order=self.axis_order)

     def from_esri(self):
         "Morph this SpatialReference from ESRI's format to EPSG."
@@ -222,7 +241,7 @@
         elif self.geographic:
             units, name = capi.angular_units(self.ptr, byref(c_char_p()))
         if name is not None:
-            name = force_text(name)
+            name = force_str(name)
         return (units, name)

     # #### Spheroid/Ellipsoid Properties ####
@@ -277,7 +296,7 @@
         capi.from_epsg(self.ptr, epsg)

     def import_proj(self, proj):
-        "Import the Spatial Reference from a PROJ.4 string."
+        """Import the Spatial Reference from a PROJ string."""
         capi.from_proj(self.ptr, proj)

     def import_user_input(self, user_input):
@@ -305,7 +324,7 @@

     @property
     def proj(self):
-        "Return the PROJ.4 representation for this Spatial Reference."
+        """Return the PROJ representation for this Spatial Reference."""
         return capi.to_proj(self.ptr, byref(c_char_p()))

     @property
('django/contrib/gis/gdal', 'error.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -29,7 +29,7 @@
 }

 # CPL Error Codes
-# https://www.gdal.org/cpl__error_8h.html
+# https://gdal.org/api/cpl.html#cpl-error-h
 CPLERR_DICT = {
     1: (GDALException, 'AppDefined'),
     2: (GDALException, 'OutOfMemory'),
('django/contrib/gis/gdal', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -37,11 +37,13 @@
     GDAL_VERSION, gdal_full_version, gdal_version,
 )
 from django.contrib.gis.gdal.raster.source import GDALRaster
-from django.contrib.gis.gdal.srs import CoordTransform, SpatialReference
+from django.contrib.gis.gdal.srs import (
+    AxisOrder, CoordTransform, SpatialReference,
+)

 __all__ = (
-    'Driver', 'DataSource', 'CoordTransform', 'Envelope', 'GDALException',
-    'GDALRaster', 'GDAL_VERSION', 'OGRGeometry', 'OGRGeomType',
-    'SpatialReference', 'SRSException', 'check_err', 'gdal_version',
-    'gdal_full_version',
+    'AxisOrder', 'Driver', 'DataSource', 'CoordTransform', 'Envelope',
+    'GDALException', 'GDALRaster', 'GDAL_VERSION', 'OGRGeometry',
+    'OGRGeomType', 'SpatialReference', 'SRSException', 'check_err',
+    'gdal_version', 'gdal_full_version',
 )
('django/contrib/gis/gdal', 'feature.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,11 +3,11 @@
 from django.contrib.gis.gdal.field import Field
 from django.contrib.gis.gdal.geometries import OGRGeometry, OGRGeomType
 from django.contrib.gis.gdal.prototypes import ds as capi, geom as geom_api
-from django.utils.encoding import force_bytes, force_text
+from django.utils.encoding import force_bytes, force_str


 # For more information, see the OGR C API source code:
-#  https://www.gdal.org/ogr__api_8h.html
+#  https://gdal.org/api/vector_c_api.html
 #
 # The OGR_F_* routines are relevant here.
 class Feature(GDALBase):
@@ -67,7 +67,7 @@
     def layer_name(self):
         "Return the name of the layer for the feature."
         name = capi.get_feat_name(self._layer._ldefn)
-        return force_text(name, self.encoding, strings_only=True)
+        return force_str(name, self.encoding, strings_only=True)

     @property
     def num_fields(self):
@@ -78,7 +78,7 @@
     def fields(self):
         "Return a list of fields in the Feature."
         return [
-            force_text(
+            force_str(
                 capi.get_field_name(capi.get_field_defn(self._layer._ldefn, i)),
                 self.encoding,
                 strings_only=True
('django/contrib/gis/gdal', 'datasource.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -34,17 +34,18 @@
               val = field.value
 """
 from ctypes import byref
+from pathlib import Path

 from django.contrib.gis.gdal.base import GDALBase
 from django.contrib.gis.gdal.driver import Driver
 from django.contrib.gis.gdal.error import GDALException
 from django.contrib.gis.gdal.layer import Layer
 from django.contrib.gis.gdal.prototypes import ds as capi
-from django.utils.encoding import force_bytes, force_text
+from django.utils.encoding import force_bytes, force_str


-# For more information, see the OGR C API source code:
-#  https://www.gdal.org/ogr__api_8h.html
+# For more information, see the OGR C API documentation:
+#  https://gdal.org/api/vector_c_api.html
 #
 # The OGR_DS_* routines are relevant here.
 class DataSource(GDALBase):
@@ -62,7 +63,7 @@

         Driver.ensure_registered()

-        if isinstance(ds_input, str):
+        if isinstance(ds_input, (str, Path)):
             # The data source driver is a void pointer.
             ds_driver = Driver.ptr_type()
             try:
@@ -117,4 +118,4 @@
     def name(self):
         "Return the name of the data source."
         name = capi.get_ds_name(self._ptr)
-        return force_text(name, self.encoding, strings_only=True)
+        return force_str(name, self.encoding, strings_only=True)
('django/contrib/gis/gdal', 'layer.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -11,11 +11,11 @@
     ds as capi, geom as geom_api, srs as srs_api,
 )
 from django.contrib.gis.gdal.srs import SpatialReference
-from django.utils.encoding import force_bytes, force_text
+from django.utils.encoding import force_bytes, force_str


 # For more information, see the OGR C API source code:
-#  https://www.gdal.org/ogr__api_8h.html
+#  https://gdal.org/api/vector_c_api.html
 #
 # The OGR_L_* routines are relevant here.
 class Layer(GDALBase):
@@ -101,7 +101,7 @@
     def name(self):
         "Return the name of this layer in the Data Source."
         name = capi.get_fd_name(self._ldefn)
-        return force_text(name, self._ds.encoding, strings_only=True)
+        return force_str(name, self._ds.encoding, strings_only=True)

     @property
     def num_feat(self, force=1):
@@ -133,9 +133,10 @@
         Return a list of string names corresponding to each of the Fields
         available in this Layer.
         """
-        return [force_text(capi.get_field_name(capi.get_field_defn(self._ldefn, i)),
-                           self._ds.encoding, strings_only=True)
-                for i in range(self.num_fields)]
+        return [force_str(
+            capi.get_field_name(capi.get_field_defn(self._ldefn, i)),
+            self._ds.encoding, strings_only=True,
+        ) for i in range(self.num_fields)]

     @property
     def field_types(self):
('django/contrib/gis/gdal', 'driver.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,20 +3,21 @@
 from django.contrib.gis.gdal.base import GDALBase
 from django.contrib.gis.gdal.error import GDALException
 from django.contrib.gis.gdal.prototypes import ds as vcapi, raster as rcapi
-from django.utils.encoding import force_bytes, force_text
+from django.utils.encoding import force_bytes, force_str


 class Driver(GDALBase):
     """
     Wrap a GDAL/OGR Data Source Driver.
-    For more information, see the C API source code:
-    https://www.gdal.org/gdal_8h.html - https://www.gdal.org/ogr__api_8h.html
+    For more information, see the C API documentation:
+    https://gdal.org/api/vector_c_api.html
+    https://gdal.org/api/raster_c_api.html
     """

     # Case-insensitive aliases for some GDAL/OGR Drivers.
     # For a complete list of original driver names see
-    # https://www.gdal.org/ogr_formats.html (vector)
-    # https://www.gdal.org/formats_list.html (raster)
+    # https://gdal.org/drivers/vector/
+    # https://gdal.org/drivers/raster/
     _alias = {
         # vector
         'esri': 'ESRI Shapefile',
@@ -94,4 +95,4 @@
         """
         Return description/name string for this driver.
         """
-        return force_text(rcapi.get_driver_description(self.ptr))
+        return force_str(rcapi.get_driver_description(self.ptr))
('django/contrib/gis/gdal', 'geometries.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,7 +1,7 @@
 """
  The OGRGeometry is a wrapper for using the OGR Geometry class
- (see https://www.gdal.org/classOGRGeometry.html).  OGRGeometry
- may be instantiated when reading geometries from OGR Data Sources
+ (see https://gdal.org/api/ogrgeometry_cpp.html#_CPPv411OGRGeometry).
+ OGRGeometry may be instantiated when reading geometries from OGR Data Sources
  (e.g. SHP files), or when given OGC WKT (a string).

  While the 'full' API is not present yet, the API is "pythonic" unlike
@@ -46,7 +46,6 @@
 from django.contrib.gis.gdal.envelope import Envelope, OGREnvelope
 from django.contrib.gis.gdal.error import GDALException, SRSException
 from django.contrib.gis.gdal.geomtype import OGRGeomType
-from django.contrib.gis.gdal.libgdal import GDAL_VERSION
 from django.contrib.gis.gdal.prototypes import geom as capi, srs as srs_api
 from django.contrib.gis.gdal.srs import CoordTransform, SpatialReference
 from django.contrib.gis.geometry import hex_regex, json_regex, wkt_regex
@@ -54,7 +53,7 @@


 # For more information, see the OGR C API source code:
-#  https://www.gdal.org/ogr__api_8h.html
+#  https://gdal.org/api/vector_c_api.html
 #
 # The OGR_G_* routines are relevant here.
 class OGRGeometry(GDALBase):
@@ -75,16 +74,16 @@
             wkt_m = wkt_regex.match(geom_input)
             json_m = json_regex.match(geom_input)
             if wkt_m:
-                if wkt_m.group('srid'):
+                if wkt_m['srid']:
                     # If there's EWKT, set the SRS w/value of the SRID.
-                    srs = int(wkt_m.group('srid'))
-                if wkt_m.group('type').upper() == 'LINEARRING':
+                    srs = int(wkt_m['srid'])
+                if wkt_m['type'].upper() == 'LINEARRING':
                     # OGR_G_CreateFromWkt doesn't work with LINEARRING WKT.
                     #  See https://trac.osgeo.org/gdal/ticket/1992.
-                    g = capi.create_geom(OGRGeomType(wkt_m.group('type')).num)
-                    capi.import_wkt(g, byref(c_char_p(wkt_m.group('wkt').encode())))
+                    g = capi.create_geom(OGRGeomType(wkt_m['type']).num)
+                    capi.import_wkt(g, byref(c_char_p(wkt_m['wkt'].encode())))
                 else:
-                    g = capi.from_wkt(byref(c_char_p(wkt_m.group('wkt').encode())), None, byref(c_void_p()))
+                    g = capi.from_wkt(byref(c_char_p(wkt_m['wkt'].encode())), None, byref(c_void_p()))
             elif json_m:
                 g = self._from_json(geom_input.encode())
             else:
@@ -140,14 +139,7 @@

     @staticmethod
     def _from_json(geom_input):
-        ptr = capi.from_json(geom_input)
-        if GDAL_VERSION < (2, 0):
-            try:
-                capi.get_geom_srs(ptr)
-            except SRSException:
-                srs = SpatialReference(4326)
-                capi.assign_srs(ptr, srs.ptr)
-        return ptr
+        return capi.from_json(geom_input)

     @classmethod
     def from_bbox(cls, bbox):
@@ -391,7 +383,7 @@
         """
         Transform this geometry to a different spatial reference system.
         May take a CoordTransform object, a SpatialReference object, string
-        WKT or PROJ.4, and/or an integer SRID.  By default, return nothing
+        WKT or PROJ, and/or an integer SRID.  By default, return nothing
         and transform the geometry in-place. However, if the `clone` keyword is
         set, return a transformed clone of this geometry.
         """
('django/contrib/gis/gdal', 'libgdal.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -13,18 +13,24 @@
 try:
     from django.conf import settings
     lib_path = settings.GDAL_LIBRARY_PATH
-except (AttributeError, EnvironmentError,
-        ImportError, ImproperlyConfigured):
+except (AttributeError, ImportError, ImproperlyConfigured, OSError):
     lib_path = None

 if lib_path:
     lib_names = None
 elif os.name == 'nt':
     # Windows NT shared libraries
-    lib_names = ['gdal203', 'gdal202', 'gdal201', 'gdal20', 'gdal111']
+    lib_names = [
+        'gdal302', 'gdal301', 'gdal300',
+        'gdal204', 'gdal203', 'gdal202', 'gdal201', 'gdal20',
+    ]
 elif os.name == 'posix':
     # *NIX library names.
-    lib_names = ['gdal', 'GDAL', 'gdal2.3.0', 'gdal2.2.0', 'gdal2.1.0', 'gdal2.0.0', 'gdal1.11.0']
+    lib_names = [
+        'gdal', 'GDAL',
+        'gdal3.2.0', 'gdal3.1.0', 'gdal3.0.0',
+        'gdal2.4.0', 'gdal2.3.0', 'gdal2.2.0', 'gdal2.1.0', 'gdal2.0.0',
+    ]
 else:
     raise ImproperlyConfigured('GDAL is unsupported on OS "%s".' % os.name)

@@ -81,26 +87,19 @@

 def gdal_full_version():
     "Return the full GDAL version information."
-    return _version_info('')
-
-
-version_regex = re.compile(r'^(?P<major>\d+)\.(?P<minor>\d+)(\.(?P<subminor>\d+))?')
+    return _version_info(b'')


 def gdal_version_info():
-    ver = gdal_version().decode()
-    m = version_regex.match(ver)
+    ver = gdal_version()
+    m = re.match(br'^(?P<major>\d+)\.(?P<minor>\d+)(?:\.(?P<subminor>\d+))?', ver)
     if not m:
         raise GDALException('Could not parse GDAL version string "%s"' % ver)
-    return {key: m.group(key) for key in ('major', 'minor', 'subminor')}
+    major, minor, subminor = m.groups()
+    return (int(major), int(minor), subminor and int(subminor))


-_verinfo = gdal_version_info()
-GDAL_MAJOR_VERSION = int(_verinfo['major'])
-GDAL_MINOR_VERSION = int(_verinfo['minor'])
-GDAL_SUBMINOR_VERSION = _verinfo['subminor'] and int(_verinfo['subminor'])
-GDAL_VERSION = (GDAL_MAJOR_VERSION, GDAL_MINOR_VERSION, GDAL_SUBMINOR_VERSION)
-del _verinfo
+GDAL_VERSION = gdal_version_info()

 # Set library error handling so as errors are logged
 CPLErrorHandler = CFUNCTYPE(None, c_int, c_int, c_char_p)
('django/contrib/gis/gdal', 'envelope.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -17,7 +17,7 @@

 # The OGR definition of an Envelope is a C structure containing four doubles.
 #  See the 'ogr_core.h' source file for more information:
-#   https://www.gdal.org/ogr__core_8h_source.html
+#   https://gdal.org/doxygen/ogr__core_8h_source.html
 class OGREnvelope(Structure):
     "Represent the OGREnvelope C Structure."
     _fields_ = [("MinX", c_double),
('django/contrib/gis/gdal/prototypes', 'srs.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,6 @@
 from ctypes import POINTER, c_char_p, c_int, c_void_p

-from django.contrib.gis.gdal.libgdal import lgdal, std_call
+from django.contrib.gis.gdal.libgdal import GDAL_VERSION, lgdal, std_call
 from django.contrib.gis.gdal.prototypes.generation import (
     const_string_output, double_output, int_output, srs_output, string_output,
     void_output,
@@ -31,6 +31,9 @@
 destroy_srs = void_output(std_call('OSRDestroySpatialReference'), [c_void_p], errcheck=False)
 srs_validate = void_output(lgdal.OSRValidate, [c_void_p])

+if GDAL_VERSION >= (3, 0):
+    set_axis_strategy = void_output(lgdal.OSRSetAxisMappingStrategy, [c_void_p, c_int], errcheck=False)
+
 # Getting the semi_major, semi_minor, and flattening functions.
 semi_major = srs_double(lgdal.OSRGetSemiMajor)
 semi_minor = srs_double(lgdal.OSRGetSemiMinor)
@@ -54,7 +57,7 @@
 linear_units = units_func(lgdal.OSRGetLinearUnits)
 angular_units = units_func(lgdal.OSRGetAngularUnits)

-# For exporting to WKT, PROJ.4, "Pretty" WKT, and XML.
+# For exporting to WKT, PROJ, "Pretty" WKT, and XML.
 to_wkt = string_output(std_call('OSRExportToWkt'), [c_void_p, POINTER(c_char_p)], decoding='utf-8')
 to_proj = string_output(std_call('OSRExportToProj4'), [c_void_p, POINTER(c_char_p)], decoding='ascii')
 to_pretty_wkt = string_output(
@@ -65,7 +68,7 @@
 # Memory leak fixed in GDAL 1.5; still exists in 1.4.
 to_xml = string_output(lgdal.OSRExportToXML, [c_void_p, POINTER(c_char_p), c_char_p], offset=-2, decoding='utf-8')

-# String attribute retrival routines.
+# String attribute retrieval routines.
 get_attr_value = const_string_output(std_call('OSRGetAttrValue'), [c_void_p, c_char_p, c_int], decoding='utf-8')
 get_auth_name = const_string_output(lgdal.OSRGetAuthorityName, [c_void_p, c_char_p], decoding='ascii')
 get_auth_code = const_string_output(lgdal.OSRGetAuthorityCode, [c_void_p, c_char_p], decoding='ascii')
('django/contrib/gis/gdal/prototypes', 'generation.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,7 +2,9 @@
  This module contains functions that generate ctypes prototypes for the
  GDAL routines.
 """
-from ctypes import POINTER, c_char_p, c_double, c_int, c_int64, c_void_p
+from ctypes import (
+    POINTER, c_bool, c_char_p, c_double, c_int, c_int64, c_void_p,
+)
 from functools import partial

 from django.contrib.gis.gdal.prototypes.errcheck import (
@@ -13,6 +15,15 @@

 class gdal_char_p(c_char_p):
     pass
+
+
+def bool_output(func, argtypes, errcheck=None):
+    """Generate a ctypes function that returns a boolean value."""
+    func.argtypes = argtypes
+    func.restype = c_bool
+    if errcheck:
+        func.errcheck = errcheck
+    return func


 def double_output(func, argtypes, errcheck=False, strarg=False, cpl=False):
('django/contrib/gis/gdal/prototypes', 'ds.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -8,8 +8,8 @@
 from django.contrib.gis.gdal.envelope import OGREnvelope
 from django.contrib.gis.gdal.libgdal import GDAL_VERSION, lgdal
 from django.contrib.gis.gdal.prototypes.generation import (
-    const_string_output, double_output, geom_output, int64_output, int_output,
-    srs_output, void_output, voidptr_output,
+    bool_output, const_string_output, double_output, geom_output, int64_output,
+    int_output, srs_output, void_output, voidptr_output,
 )

 c_int_p = POINTER(c_int)  # shortcut type
@@ -68,8 +68,11 @@
 )
 get_field_as_double = double_output(lgdal.OGR_F_GetFieldAsDouble, [c_void_p, c_int])
 get_field_as_integer = int_output(lgdal.OGR_F_GetFieldAsInteger, [c_void_p, c_int])
-if GDAL_VERSION >= (2, 0):
-    get_field_as_integer64 = int64_output(lgdal.OGR_F_GetFieldAsInteger64, [c_void_p, c_int])
+get_field_as_integer64 = int64_output(lgdal.OGR_F_GetFieldAsInteger64, [c_void_p, c_int])
+if GDAL_VERSION >= (2, 2):
+    is_field_set = bool_output(lgdal.OGR_F_IsFieldSetAndNotNull, [c_void_p, c_int])
+else:
+    is_field_set = bool_output(lgdal.OGR_F_IsFieldSet, [c_void_p, c_int])
 get_field_as_string = const_string_output(lgdal.OGR_F_GetFieldAsString, [c_void_p, c_int])
 get_field_index = int_output(lgdal.OGR_F_GetFieldIndex, [c_void_p, c_char_p])

('django/contrib/gis/gdal/prototypes', 'raster.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -12,9 +12,9 @@
 )

 # For more detail about c function names and definitions see
-# https://gdal.org/gdal_8h.html
-# https://gdal.org/gdalwarper_8h.html
-# https://www.gdal.org/gdal__utils_8h.html
+# https://gdal.org/api/raster_c_api.html
+# https://gdal.org/doxygen/gdalwarper_8h.html
+# https://gdal.org/api/gdal_utils.html

 # Prepare partial functions that use cpl error codes
 void_output = partial(void_output, cpl=True)
@@ -102,7 +102,7 @@
 )

 # Create VSI gdal raster files from in-memory buffers.
-# https://gdal.org/cpl__vsi_8h.html
+# https://gdal.org/api/cpl.html#cpl-vsi-h
 create_vsi_file_from_mem_buffer = voidptr_output(std_call('VSIFileFromMemBuffer'), [c_char_p, c_void_p, c_int, c_int])
 get_mem_buffer_from_vsi_file = voidptr_output(std_call('VSIGetMemFileBuffer'), [c_char_p, POINTER(c_int), c_bool])
 unlink_vsi_file = int_output(std_call('VSIUnlink'), [c_char_p])
('django/contrib/gis/gdal/prototypes', 'errcheck.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -69,8 +69,7 @@
 # ### Envelope checking ###
 def check_envelope(result, func, cargs, offset=-1):
     "Check a function that returns an OGR Envelope by reference."
-    env = ptr_byref(cargs, offset)
-    return env
+    return ptr_byref(cargs, offset)


 # ### Geometry error-checking routines ###
('django/contrib/gis/gdal/raster', 'band.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,7 +4,7 @@
 from django.contrib.gis.gdal.prototypes import raster as capi
 from django.contrib.gis.gdal.raster.base import GDALRasterBase
 from django.contrib.gis.shortcuts import numpy
-from django.utils.encoding import force_text
+from django.utils.encoding import force_str

 from .const import (
     GDAL_COLOR_TYPES, GDAL_INTEGER_TYPES, GDAL_PIXEL_TYPES, GDAL_TO_CTYPES,
@@ -32,7 +32,7 @@
         """
         Return the description string of the band.
         """
-        return force_text(capi.get_band_description(self._ptr))
+        return force_str(capi.get_band_description(self._ptr))

     @property
     def width(self):
('django/contrib/gis/gdal/raster', 'source.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -17,7 +17,7 @@
 )
 from django.contrib.gis.gdal.srs import SpatialReference, SRSException
 from django.contrib.gis.geometry import json_regex
-from django.utils.encoding import force_bytes, force_text
+from django.utils.encoding import force_bytes, force_str
 from django.utils.functional import cached_property


@@ -73,6 +73,13 @@

         # If input is a valid file path, try setting file as source.
         if isinstance(ds_input, str):
+            if (
+                not ds_input.startswith(VSI_FILESYSTEM_BASE_PATH) and
+                not os.path.exists(ds_input)
+            ):
+                raise GDALException(
+                    'Unable to read raster source input "%s".' % ds_input
+                )
             try:
                 # GDALOpen will auto-detect the data source type.
                 self._ptr = capi.open_ds(force_bytes(ds_input), self._write)
@@ -225,7 +232,7 @@

     @cached_property
     def is_vsi_based(self):
-        return self.name.startswith(VSI_FILESYSTEM_BASE_PATH)
+        return self._ptr and self.name.startswith(VSI_FILESYSTEM_BASE_PATH)

     @property
     def name(self):
@@ -233,7 +240,7 @@
         Return the name of this raster. Corresponds to filename
         for file-based rasters.
         """
-        return force_text(capi.get_ds_description(self._ptr))
+        return force_str(capi.get_ds_description(self._ptr))

     @cached_property
     def driver(self):
@@ -418,17 +425,48 @@

         return target

-    def transform(self, srid, driver=None, name=None, resampling='NearestNeighbour',
+    def clone(self, name=None):
+        """Return a clone of this GDALRaster."""
+        if name:
+            clone_name = name
+        elif self.driver.name != 'MEM':
+            clone_name = self.name + '_copy.' + self.driver.name
+        else:
+            clone_name = os.path.join(VSI_FILESYSTEM_BASE_PATH, str(uuid.uuid4()))
+        return GDALRaster(
+            capi.copy_ds(
+                self.driver._ptr,
+                force_bytes(clone_name),
+                self._ptr,
+                c_int(),
+                c_char_p(),
+                c_void_p(),
+                c_void_p(),
+            ),
+            write=self._write,
+        )
+
+    def transform(self, srs, driver=None, name=None, resampling='NearestNeighbour',
                   max_error=0.0):
         """
-        Return a copy of this raster reprojected into the given SRID.
+        Return a copy of this raster reprojected into the given spatial
+        reference system.
         """
         # Convert the resampling algorithm name into an algorithm id
         algorithm = GDAL_RESAMPLE_ALGORITHMS[resampling]

-        # Instantiate target spatial reference system
-        target_srs = SpatialReference(srid)
-
+        if isinstance(srs, SpatialReference):
+            target_srs = srs
+        elif isinstance(srs, (int, str)):
+            target_srs = SpatialReference(srs)
+        else:
+            raise TypeError(
+                'Transform only accepts SpatialReference, string, and integer '
+                'objects.'
+            )
+
+        if target_srs.srid == self.srid and (not driver or driver == self.driver.name):
+            return self.clone(name)
         # Create warped virtual dataset in the target reference system
         target = capi.auto_create_warped_vrt(
             self._ptr, self.srs.wkt.encode(), target_srs.wkt.encode(),
@@ -438,7 +476,7 @@

         # Construct the target warp dictionary from the virtual raster
         data = {
-            'srid': srid,
+            'srid': target_srs.srid,
             'width': target.width,
             'height': target.height,
             'origin': [target.origin.x, target.origin.y],
('django/contrib/gis/gdal/raster', 'const.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -5,7 +5,7 @@
     c_double, c_float, c_int16, c_int32, c_ubyte, c_uint16, c_uint32,
 )

-# See https://www.gdal.org/gdal_8h.html#a22e22ce0a55036a96f652765793fb7a4
+# See https://gdal.org/api/raster_c_api.html#_CPPv412GDALDataType
 GDAL_PIXEL_TYPES = {
     0: 'GDT_Unknown',  # Unknown or unspecified type
     1: 'GDT_Byte',  # Eight bit unsigned integer
@@ -44,7 +44,7 @@
     'Mode': 6,
 }

-# See https://www.gdal.org/gdal_8h.html#ace76452d94514561fffa8ea1d2a5968c
+# See https://gdal.org/api/raster_c_api.html#_CPPv415GDALColorInterp
 GDAL_COLOR_TYPES = {
     0: 'GCI_Undefined',  # Undefined, default value, i.e. not known
     1: 'GCI_GrayIndex',  # Greyscale
('django/contrib/gis/admin', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,12 +1,12 @@
 from django.contrib.admin import (
     HORIZONTAL, VERTICAL, AdminSite, ModelAdmin, StackedInline, TabularInline,
-    autodiscover, register, site,
+    action, autodiscover, display, register, site,
 )
 from django.contrib.gis.admin.options import GeoModelAdmin, OSMGeoAdmin
 from django.contrib.gis.admin.widgets import OpenLayersWidget

 __all__ = [
     'HORIZONTAL', 'VERTICAL', 'AdminSite', 'ModelAdmin', 'StackedInline',
-    'TabularInline', 'autodiscover', 'register', 'site',
+    'TabularInline', 'action', 'autodiscover', 'display', 'register', 'site',
     'GeoModelAdmin', 'OSMGeoAdmin', 'OpenLayersWidget',
 ]
('django/contrib/gis/management/commands', 'inspectdb.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -9,9 +9,8 @@
     def get_field_type(self, connection, table_name, row):
         field_type, field_params, field_notes = super().get_field_type(connection, table_name, row)
         if field_type == 'GeometryField':
-            geo_col = row[0]
             # Getting a more specific field type and any additional parameters
             # from the `get_geometry_type` routine for the spatial backend.
-            field_type, geo_params = connection.introspection.get_geometry_type(table_name, geo_col)
+            field_type, geo_params = connection.introspection.get_geometry_type(table_name, row)
             field_params.update(geo_params)
         return field_type, field_params, field_notes
('django/contrib/gis/management/commands', 'ogrinspect.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -37,7 +37,7 @@
         ' ./manage.py ogrinspect zipcode.shp Zipcode'
     )

-    requires_system_checks = False
+    requires_system_checks = []

     def add_arguments(self, parser):
         parser.add_argument('data_source', help='Path to the data source.')
@@ -107,6 +107,7 @@
         # Returning the output of ogrinspect with the given arguments
         # and options.
         from django.contrib.gis.utils.ogrinspect import _ogrinspect, mapping
+
         # Filter options to params accepted by `_ogrinspect`
         ogr_options = {k: v for k, v in options.items()
                        if k in get_func_args(_ogrinspect) and v is not None}
@@ -130,4 +131,4 @@
                 rev_mapping[ogr_fld], ogr_fld) for ogr_fld in ds[options['layer_key']].fields
             )
             output.extend(["    '%s': '%s'," % (options['geom_name'], mapping_dict[options['geom_name']]), '}'])
-        return '\n'.join(output) + '\n'
+        return '\n'.join(output)
('django/contrib/gis/utils', 'ogrinspect.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -116,7 +116,7 @@

     Note: Call the _ogrinspect() helper to do the heavy lifting.
     """
-    return '\n'.join(s for s in _ogrinspect(*args, **kwargs))
+    return '\n'.join(_ogrinspect(*args, **kwargs))


 def _ogrinspect(data_source, model_name, geom_name='geom', layer_key=0, srid=None,
('django/contrib/gis/utils', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -9,6 +9,8 @@
 try:
     # LayerMapping requires DJANGO_SETTINGS_MODULE to be set,
     # and ImproperlyConfigured is raised if that's not the case.
-    from django.contrib.gis.utils.layermapping import LayerMapping, LayerMapError  # NOQA
+    from django.contrib.gis.utils.layermapping import (  # NOQA
+        LayerMapError, LayerMapping,
+    )
 except ImproperlyConfigured:
     pass
('django/contrib/gis/utils', 'layermapping.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -8,6 +8,7 @@
 """
 import sys
 from decimal import Decimal, InvalidOperation as DecimalInvalidOperation
+from pathlib import Path

 from django.contrib.gis.db.models import GeometryField
 from django.contrib.gis.gdal import (
@@ -20,7 +21,7 @@
 )
 from django.core.exceptions import FieldDoesNotExist, ObjectDoesNotExist
 from django.db import connections, models, router, transaction
-from django.utils.encoding import force_text
+from django.utils.encoding import force_str


 # LayerMapping exceptions.
@@ -61,6 +62,8 @@
     FIELD_TYPES = {
         models.AutoField: OFTInteger,
         models.BigAutoField: OFTInteger64,
+        models.SmallAutoField: OFTInteger,
+        models.BooleanField: (OFTInteger, OFTReal, OFTString),
         models.IntegerField: (OFTInteger, OFTReal, OFTString),
         models.FloatField: (OFTInteger, OFTReal),
         models.DateField: OFTDate,
@@ -72,8 +75,11 @@
         models.SlugField: OFTString,
         models.TextField: OFTString,
         models.URLField: OFTString,
+        models.UUIDField: OFTString,
         models.BigIntegerField: (OFTInteger, OFTReal, OFTString),
         models.SmallIntegerField: (OFTInteger, OFTReal, OFTString),
+        models.PositiveBigIntegerField: (OFTInteger, OFTReal, OFTString),
+        models.PositiveIntegerField: (OFTInteger, OFTReal, OFTString),
         models.PositiveSmallIntegerField: (OFTInteger, OFTReal, OFTString),
     }

@@ -88,7 +94,7 @@
         argument usage.
         """
         # Getting the DataSource and the associated Layer.
-        if isinstance(data, str):
+        if isinstance(data, (str, Path)):
             self.ds = DataSource(data, encoding=encoding)
         else:
             self.ds = data
@@ -343,13 +349,13 @@
         """
         if (isinstance(ogr_field, OFTString) and
                 isinstance(model_field, (models.CharField, models.TextField))):
-            if self.encoding:
+            if self.encoding and ogr_field.value is not None:
                 # The encoding for OGR data sources may be specified here
                 # (e.g., 'cp437' for Census Bureau boundary files).
-                val = force_text(ogr_field.value, self.encoding)
+                val = force_str(ogr_field.value, self.encoding)
             else:
                 val = ogr_field.value
-            if model_field.max_length and len(val) > model_field.max_length:
+            if model_field.max_length and val is not None and len(val) > model_field.max_length:
                 raise InvalidString('%s model field maximum string length is %s, given %s characters.' %
                                     (model_field.name, model_field.max_length, len(val)))
         elif isinstance(ogr_field, OFTReal) and isinstance(model_field, models.DecimalField):
('django/contrib/gis/sitemaps', 'kml.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,5 @@
 from django.apps import apps
-from django.contrib.gis.db.models.fields import GeometryField
+from django.contrib.gis.db.models import GeometryField
 from django.contrib.sitemaps import Sitemap
 from django.db import models
 from django.urls import reverse
('django/contrib/gis/sitemaps', 'views.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,5 @@
 from django.apps import apps
-from django.contrib.gis.db.models.fields import GeometryField
+from django.contrib.gis.db.models import GeometryField
 from django.contrib.gis.db.models.functions import AsKML, Transform
 from django.contrib.gis.shortcuts import render_to_kml, render_to_kmz
 from django.core.exceptions import FieldDoesNotExist
('django/contrib/gis/db/backends/oracle', 'adapter.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -16,42 +16,48 @@
          * Inner ring(s) - clockwise
         """
         if isinstance(geom, Polygon):
-            self._fix_polygon(geom)
+            if self._polygon_must_be_fixed(geom):
+                geom = self._fix_polygon(geom)
         elif isinstance(geom, GeometryCollection):
-            self._fix_geometry_collection(geom)
+            if any(isinstance(g, Polygon) and self._polygon_must_be_fixed(g) for g in geom):
+                geom = self._fix_geometry_collection(geom)

         self.wkt = geom.wkt
         self.srid = geom.srid

-    def _fix_polygon(self, poly):
+    @staticmethod
+    def _polygon_must_be_fixed(poly):
+        return (
+            not poly.empty and
+            (
+                not poly.exterior_ring.is_counterclockwise or
+                any(x.is_counterclockwise for x in poly)
+            )
+        )
+
+    @classmethod
+    def _fix_polygon(cls, poly, clone=True):
         """Fix single polygon orientation as described in __init__()."""
-        if self._isClockwise(poly.exterior_ring):
+        if clone:
+            poly = poly.clone()
+
+        if not poly.exterior_ring.is_counterclockwise:
             poly.exterior_ring = list(reversed(poly.exterior_ring))

         for i in range(1, len(poly)):
-            if not self._isClockwise(poly[i]):
+            if poly[i].is_counterclockwise:
                 poly[i] = list(reversed(poly[i]))

         return poly

-    def _fix_geometry_collection(self, coll):
+    @classmethod
+    def _fix_geometry_collection(cls, coll):
         """
         Fix polygon orientations in geometry collections as described in
         __init__().
         """
+        coll = coll.clone()
         for i, geom in enumerate(coll):
             if isinstance(geom, Polygon):
-                coll[i] = self._fix_polygon(geom)
-
-    def _isClockwise(self, coords):
-        """
-        A modified shoelace algorithm to determine polygon orientation.
-        See https://en.wikipedia.org/wiki/Shoelace_formula.
-        """
-        n = len(coords)
-        area = 0.0
-        for i in range(n):
-            j = (i + 1) % n
-            area += coords[i][0] * coords[j][1]
-            area -= coords[j][0] * coords[i][1]
-        return area < 0.0
+                coll[i] = cls._fix_polygon(geom, clone=False)
+        return coll
('django/contrib/gis/db/backends/oracle', 'features.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -9,3 +9,6 @@
     supports_geometry_field_introspection = False
     supports_geometry_field_unique_index = False
     supports_perimeter_geodetic = True
+    supports_dwithin_distance_expr = False
+    supports_tolerance_parameter = True
+    unsupported_geojson_options = {'bbox', 'crs', 'precision'}
('django/contrib/gis/db/backends/oracle', 'operations.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -9,12 +9,12 @@
 """
 import re

+from django.contrib.gis.db import models
 from django.contrib.gis.db.backends.base.operations import (
     BaseSpatialOperations,
 )
 from django.contrib.gis.db.backends.oracle.adapter import OracleSpatialAdapter
 from django.contrib.gis.db.backends.utils import SpatialOperator
-from django.contrib.gis.db.models import aggregates
 from django.contrib.gis.geos.geometry import GEOSGeometry, GEOSGeometryBase
 from django.contrib.gis.geos.prototypes.io import wkb_r
 from django.contrib.gis.measure import Distance
@@ -45,15 +45,15 @@
             raise ValueError('Invalid SDO_RELATE mask: "%s"' % arg)

     def as_sql(self, connection, lookup, template_params, sql_params):
-        template_params['mask'] = sql_params.pop()
-        return super().as_sql(connection, lookup, template_params, sql_params)
+        template_params['mask'] = sql_params[-1]
+        return super().as_sql(connection, lookup, template_params, sql_params[:-1])


 class OracleOperations(BaseSpatialOperations, DatabaseOperations):

     name = 'oracle'
     oracle = True
-    disallowed_aggregates = (aggregates.Collect, aggregates.Extent3D, aggregates.MakeLine)
+    disallowed_aggregates = (models.Collect, models.Extent3D, models.MakeLine)

     Adapter = OracleSpatialAdapter

@@ -64,6 +64,9 @@

     function_names = {
         'Area': 'SDO_GEOM.SDO_AREA',
+        'AsGeoJSON': 'SDO_UTIL.TO_GEOJSON',
+        'AsWKB': 'SDO_UTIL.TO_WKBGEOMETRY',
+        'AsWKT': 'SDO_UTIL.TO_WKTGEOMETRY',
         'BoundingCircle': 'SDO_GEOM.SDO_MBC',
         'Centroid': 'SDO_GEOM.SDO_CENTROID',
         'Difference': 'SDO_GEOM.SDO_DIFFERENCE',
@@ -106,9 +109,9 @@
     }

     unsupported_functions = {
-        'AsGeoJSON', 'AsKML', 'AsSVG', 'Azimuth',
-        'ForcePolygonCW', 'ForceRHR', 'GeoHash', 'LineLocatePoint',
-        'MakeValid', 'MemSize', 'Scale', 'SnapToGrid', 'Translate',
+        'AsKML', 'AsSVG', 'Azimuth', 'ForcePolygonCW', 'GeoHash',
+        'GeometryDistance', 'LineLocatePoint', 'MakeValid', 'MemSize',
+        'Scale', 'SnapToGrid', 'Translate',
     }

     def geo_quote_name(self, name):
@@ -183,11 +186,15 @@

     # Routines for getting the OGC-compliant models.
     def geometry_columns(self):
-        from django.contrib.gis.db.backends.oracle.models import OracleGeometryColumns
+        from django.contrib.gis.db.backends.oracle.models import (
+            OracleGeometryColumns,
+        )
         return OracleGeometryColumns

     def spatial_ref_sys(self):
-        from django.contrib.gis.db.backends.oracle.models import OracleSpatialRefSys
+        from django.contrib.gis.db.backends.oracle.models import (
+            OracleSpatialRefSys,
+        )
         return OracleSpatialRefSys

     def modify_insert_params(self, placeholder, params):
('django/contrib/gis/db/backends/oracle', 'introspection.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,29 +1,34 @@
 import cx_Oracle

 from django.db.backends.oracle.introspection import DatabaseIntrospection
+from django.utils.functional import cached_property


 class OracleIntrospection(DatabaseIntrospection):
-    # Associating any OBJECTVAR instances with GeometryField.  Of course,
-    # this won't work right on Oracle objects that aren't MDSYS.SDO_GEOMETRY,
-    # but it is the only object type supported within Django anyways.
-    data_types_reverse = DatabaseIntrospection.data_types_reverse.copy()
-    data_types_reverse[cx_Oracle.OBJECT] = 'GeometryField'
+    # Associating any OBJECTVAR instances with GeometryField. This won't work
+    # right on Oracle objects that aren't MDSYS.SDO_GEOMETRY, but it is the
+    # only object type supported within Django anyways.
+    @cached_property
+    def data_types_reverse(self):
+        return {
+            **super().data_types_reverse,
+            cx_Oracle.OBJECT: 'GeometryField',
+        }

-    def get_geometry_type(self, table_name, geo_col):
+    def get_geometry_type(self, table_name, description):
         with self.connection.cursor() as cursor:
             # Querying USER_SDO_GEOM_METADATA to get the SRID and dimension information.
             try:
                 cursor.execute(
                     'SELECT "DIMINFO", "SRID" FROM "USER_SDO_GEOM_METADATA" '
                     'WHERE "TABLE_NAME"=%s AND "COLUMN_NAME"=%s',
-                    (table_name.upper(), geo_col.upper())
+                    (table_name.upper(), description.name.upper())
                 )
                 row = cursor.fetchone()
             except Exception as exc:
                 raise Exception(
                     'Could not find entry in USER_SDO_GEOM_METADATA '
-                    'corresponding to "%s"."%s"' % (table_name, geo_col)
+                    'corresponding to "%s"."%s"' % (table_name, description.name)
                 ) from exc

             # TODO: Research way to find a more specific geometry field type for
('django/contrib/gis/db/backends/oracle', 'schema.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,4 +1,4 @@
-from django.contrib.gis.db.models.fields import GeometryField
+from django.contrib.gis.db.models import GeometryField
 from django.db.backends.oracle.schema import DatabaseSchemaEditor
 from django.db.backends.utils import strip_quotes, truncate_name

('django/contrib/gis/db/backends/postgis', 'adapter.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -42,6 +42,10 @@
     def __str__(self):
         return self.getquoted()

+    @classmethod
+    def _fix_polygon(cls, poly):
+        return poly
+
     def prepare(self, conn):
         """
         This method allows escaping the binary in the style required by the
('django/contrib/gis/db/backends/postgis', 'pgraster.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,10 +1,10 @@
 import struct

-from django.forms import ValidationError
+from django.core.exceptions import ValidationError

 from .const import (
-    GDAL_TO_POSTGIS, GDAL_TO_STRUCT, POSTGIS_HEADER_STRUCTURE, POSTGIS_TO_GDAL,
-    STRUCT_SIZE,
+    BANDTYPE_FLAG_HASNODATA, BANDTYPE_PIXTYPE_MASK, GDAL_TO_POSTGIS,
+    GDAL_TO_STRUCT, POSTGIS_HEADER_STRUCTURE, POSTGIS_TO_GDAL, STRUCT_SIZE,
 )


@@ -45,13 +45,9 @@
     pixeltypes = []
     while data:
         # Get pixel type for this band
-        pixeltype, data = chunk(data, 2)
-        pixeltype = unpack('B', pixeltype)[0]
-
-        # Subtract nodata byte from band nodata value if it exists
-        has_nodata = pixeltype >= 64
-        if has_nodata:
-            pixeltype -= 64
+        pixeltype_with_flags, data = chunk(data, 2)
+        pixeltype_with_flags = unpack('B', pixeltype_with_flags)[0]
+        pixeltype = pixeltype_with_flags & BANDTYPE_PIXTYPE_MASK

         # Convert datatype from PostGIS to GDAL & get pack type and size
         pixeltype = POSTGIS_TO_GDAL[pixeltype]
@@ -68,8 +64,8 @@
         band, data = chunk(data, pack_size * header[10] * header[11])
         band_result = {'data': bytes.fromhex(band)}

-        # If the nodata flag is True, set the nodata value.
-        if has_nodata:
+        # Set the nodata value if the nodata flag is set.
+        if pixeltype_with_flags & BANDTYPE_FLAG_HASNODATA:
             band_result['nodata_value'] = nodata

         # Append band data to band list
@@ -116,12 +112,13 @@
         # and the nodata value.
         #
         # The 8BUI stores both the PostGIS pixel data type and a nodata flag.
-        # It is composed as the datatype integer plus 64 as a flag for existing
-        # nodata values:
-        # 8BUI_VALUE = PG_PIXEL_TYPE (0-11) + FLAG (0 or 64)
+        # It is composed as the datatype with BANDTYPE_FLAG_HASNODATA (1 << 6)
+        # for existing nodata values:
+        #   8BUI_VALUE = PG_PIXEL_TYPE (0-11) | BANDTYPE_FLAG_HASNODATA
         #
         # For example, if the byte value is 71, then the datatype is
-        # 71-64 = 7 (32BSI) and the nodata value is True.
+        #   71 & ~BANDTYPE_FLAG_HASNODATA = 7 (32BSI)
+        # and the nodata value is True.
         structure = 'B' + GDAL_TO_STRUCT[band.datatype()]

         # Get band pixel type in PostGIS notation
@@ -129,7 +126,7 @@

         # Set the nodata flag
         if band.nodata_value is not None:
-            pixeltype += 64
+            pixeltype |= BANDTYPE_FLAG_HASNODATA

         # Pack band header
         bandheader = pack(structure, (pixeltype, band.nodata_value or 0))
('django/contrib/gis/db/backends/postgis', 'features.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -5,8 +5,9 @@


 class DatabaseFeatures(BaseSpatialFeatures, Psycopg2DatabaseFeatures):
+    supports_geography = True
     supports_3d_storage = True
     supports_3d_functions = True
-    supports_left_right_lookups = True
     supports_raster = True
     supports_empty_geometries = True
+    empty_intersection_returns_none = False
('django/contrib/gis/db/backends/postgis', 'operations.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -11,9 +11,9 @@
 from django.contrib.gis.geos.prototypes.io import wkb_r
 from django.contrib.gis.measure import Distance
 from django.core.exceptions import ImproperlyConfigured
+from django.db import NotSupportedError, ProgrammingError
 from django.db.backends.postgresql.operations import DatabaseOperations
 from django.db.models import Func, Value
-from django.db.utils import NotSupportedError, ProgrammingError
 from django.utils.functional import cached_property
 from django.utils.version import get_version_tuple

@@ -98,7 +98,6 @@
 class PostGISOperations(BaseSpatialOperations, DatabaseOperations):
     name = 'postgis'
     postgis = True
-    geography = True
     geom_func_prefix = 'ST_'

     Adapter = PostGISAdapter
@@ -148,16 +147,11 @@
     @cached_property
     def function_names(self):
         function_names = {
+            'AsWKB': 'ST_AsBinary',
+            'AsWKT': 'ST_AsText',
             'BoundingCircle': 'ST_MinimumBoundingCircle',
             'NumPoints': 'ST_NPoints',
         }
-        if self.spatial_version < (2, 2, 0):
-            function_names.update({
-                'DistanceSphere': 'ST_distance_sphere',
-                'DistanceSpheroid': 'ST_distance_spheroid',
-                'LengthSpheroid': 'ST_length_spheroid',
-                'MemSize': 'ST_mem_size',
-            })
         if self.spatial_version < (2, 4, 0):
             function_names['ForcePolygonCW'] = 'ST_ForceRHR'
         return function_names
@@ -185,7 +179,7 @@
                 raise ImproperlyConfigured(
                     'Cannot determine PostGIS version for database "%s" '
                     'using command "SELECT postgis_lib_version()". '
-                    'GeoDjango requires at least PostGIS version 2.1. '
+                    'GeoDjango requires at least PostGIS version 2.3. '
                     'Was the database created from a spatial database '
                     'template?' % self.connection.settings_dict['NAME']
                 )
@@ -319,7 +313,7 @@
         return self._get_postgis_func('postgis_lib_version')

     def postgis_proj_version(self):
-        "Return the version of the PROJ.4 library used with PostGIS."
+        """Return the version of the PROJ library used with PostGIS."""
         return self._get_postgis_func('postgis_proj_version')

     def postgis_version(self):
@@ -340,16 +334,16 @@

     def proj_version_tuple(self):
         """
-        Return the version of PROJ.4 used by PostGIS as a tuple of the
+        Return the version of PROJ used by PostGIS as a tuple of the
         major, minor, and subminor release numbers.
         """
         proj_regex = re.compile(r'(\d+)\.(\d+)\.(\d+)')
         proj_ver_str = self.postgis_proj_version()
         m = proj_regex.search(proj_ver_str)
         if m:
-            return tuple(map(int, [m.group(1), m.group(2), m.group(3)]))
-        else:
-            raise Exception('Could not determine PROJ.4 version from PostGIS.')
+            return tuple(map(int, m.groups()))
+        else:
+            raise Exception('Could not determine PROJ version from PostGIS.')

     def spatial_aggregate_name(self, agg_name):
         if agg_name == 'Extent3D':
('django/contrib/gis/db/backends/postgis', 'introspection.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,14 +2,8 @@
 from django.db.backends.postgresql.introspection import DatabaseIntrospection


-class GeoIntrospectionError(Exception):
-    pass
-
-
 class PostGISIntrospection(DatabaseIntrospection):
-    # Reverse dictionary for PostGIS geometry types not populated until
-    # introspection is actually performed.
-    postgis_types_reverse = {}
+    postgis_oid_lookup = {}  # Populated when introspection is performed.

     ignored_tables = DatabaseIntrospection.ignored_tables + [
         'geography_columns',
@@ -19,45 +13,20 @@
         'raster_overviews',
     ]

-    def get_postgis_types(self):
-        """
-        Return a dictionary with keys that are the PostgreSQL object
-        identification integers for the PostGIS geometry and/or
-        geography types (if supported).
-        """
-        field_types = [
-            ('geometry', 'GeometryField'),
-            # The value for the geography type is actually a tuple
-            # to pass in the `geography=True` keyword to the field
-            # definition.
-            ('geography', ('GeometryField', {'geography': True})),
-        ]
-        postgis_types = {}
-
-        # The OID integers associated with the geometry type may
-        # be different across versions; hence, this is why we have
-        # to query the PostgreSQL pg_type table corresponding to the
-        # PostGIS custom data types.
-        oid_sql = 'SELECT "oid" FROM "pg_type" WHERE "typname" = %s'
-        with self.connection.cursor() as cursor:
-            for field_type in field_types:
-                cursor.execute(oid_sql, (field_type[0],))
-                for result in cursor.fetchall():
-                    postgis_types[result[0]] = field_type[1]
-        return postgis_types
-
     def get_field_type(self, data_type, description):
-        if not self.postgis_types_reverse:
-            # If the PostGIS types reverse dictionary is not populated, do so
-            # now.  In order to prevent unnecessary requests upon connection
-            # initialization, the `data_types_reverse` dictionary is not updated
-            # with the PostGIS custom types until introspection is actually
-            # performed -- in other words, when this function is called.
-            self.postgis_types_reverse = self.get_postgis_types()
-            self.data_types_reverse.update(self.postgis_types_reverse)
+        if not self.postgis_oid_lookup:
+            # Query PostgreSQL's pg_type table to determine the OID integers
+            # for the PostGIS data types used in reverse lookup (the integers
+            # may be different across versions). To prevent unnecessary
+            # requests upon connection initialization, the `data_types_reverse`
+            # dictionary isn't updated until introspection is performed here.
+            with self.connection.cursor() as cursor:
+                cursor.execute("SELECT oid, typname FROM pg_type WHERE typname IN ('geometry', 'geography')")
+                self.postgis_oid_lookup = dict(cursor.fetchall())
+            self.data_types_reverse.update((oid, 'GeometryField') for oid in self.postgis_oid_lookup)
         return super().get_field_type(data_type, description)

-    def get_geometry_type(self, table_name, geo_col):
+    def get_geometry_type(self, table_name, description):
         """
         The geometry type OID used by PostGIS does not indicate the particular
         type of field that a geometry column is (e.g., whether it's a
@@ -65,34 +34,25 @@
         metadata tables to determine the geometry type.
         """
         with self.connection.cursor() as cursor:
-            try:
-                # First seeing if this geometry column is in the `geometry_columns`
-                cursor.execute('SELECT "coord_dimension", "srid", "type" '
-                               'FROM "geometry_columns" '
-                               'WHERE "f_table_name"=%s AND "f_geometry_column"=%s',
-                               (table_name, geo_col))
-                row = cursor.fetchone()
-                if not row:
-                    raise GeoIntrospectionError
-            except GeoIntrospectionError:
-                cursor.execute('SELECT "coord_dimension", "srid", "type" '
-                               'FROM "geography_columns" '
-                               'WHERE "f_table_name"=%s AND "f_geography_column"=%s',
-                               (table_name, geo_col))
-                row = cursor.fetchone()
-
+            cursor.execute("""
+                SELECT t.coord_dimension, t.srid, t.type FROM (
+                    SELECT * FROM geometry_columns
+                    UNION ALL
+                    SELECT * FROM geography_columns
+                ) AS t WHERE t.f_table_name = %s AND t.f_geometry_column = %s
+            """, (table_name, description.name))
+            row = cursor.fetchone()
             if not row:
                 raise Exception('Could not find a geometry or geography column for "%s"."%s"' %
-                                (table_name, geo_col))
-
+                                (table_name, description.name))
+            dim, srid, field_type = row
             # OGRGeomType does not require GDAL and makes it easy to convert
             # from OGC geom type name to Django field.
-            field_type = OGRGeomType(row[2]).django
-
+            field_type = OGRGeomType(field_type).django
             # Getting any GeometryField keyword arguments that are not the default.
-            dim = row[0]
-            srid = row[1]
             field_params = {}
+            if self.postgis_oid_lookup.get(description.type_code) == 'geography':
+                field_params['geography'] = True
             if srid != 4326:
                 field_params['srid'] = srid
             if dim != 2:
('django/contrib/gis/db/backends/postgis', 'const.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -41,3 +41,12 @@
     'f': 4,  # Float
     'd': 8,  # Double
 }
+
+# Pixel type specifies type of pixel values in a band. Storage flag specifies
+# whether the band data is stored as part of the datum or is to be found on the
+# server's filesystem. There are currently 11 supported pixel value types, so 4
+# bits are enough to account for all. Reserve the upper 4 bits for generic
+# flags.
+# See https://trac.osgeo.org/postgis/wiki/WKTRaster/RFC/RFC1_V0SerialFormat#Pixeltypeandstorageflag
+BANDTYPE_PIXTYPE_MASK = 0x0F
+BANDTYPE_FLAG_HASNODATA = 1 << 6
('django/contrib/gis/db/backends/postgis', 'schema.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,4 @@
+from django.db.backends.ddl_references import Statement
 from django.db.backends.postgresql.schema import DatabaseSchemaEditor


@@ -17,9 +18,9 @@
             return True
         return super()._field_should_be_indexed(model, field)

-    def _create_index_sql(self, model, fields, **kwargs):
-        if len(fields) != 1 or not hasattr(fields[0], 'geodetic'):
-            return super()._create_index_sql(model, fields, **kwargs)
+    def _create_index_sql(self, model, *, fields=None, **kwargs):
+        if fields is None or len(fields) != 1 or not hasattr(fields[0], 'geodetic'):
+            return super()._create_index_sql(model, fields=fields, **kwargs)

         field = fields[0]
         field_column = self.quote_name(field.column)
@@ -31,15 +32,21 @@
         elif field.dim > 2 and not field.geography:
             # Use "nd" ops which are fast on multidimensional cases
             field_column = "%s %s" % (field_column, self.geom_index_ops_nd)
+        if kwargs.get('name') is None:
+            index_name = '%s_%s_id' % (model._meta.db_table, field.column)
+        else:
+            index_name = kwargs['name']

-        return self.sql_create_index % {
-            "name": self.quote_name('%s_%s_id' % (model._meta.db_table, field.column)),
-            "table": self.quote_name(model._meta.db_table),
-            "using": "USING %s" % self.geom_index_type,
-            "columns": field_column,
-            "extra": '',
-            "condition": '',
-        }
+        return Statement(
+            self.sql_create_index,
+            name=self.quote_name(index_name),
+            table=self.quote_name(model._meta.db_table),
+            using=' USING %s' % self.geom_index_type,
+            columns=field_column,
+            extra='',
+            condition='',
+            include='',
+        )

     def _alter_column_type_sql(self, table, old_field, new_field, new_type):
         """
('django/contrib/gis/db/backends/mysql', 'features.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -12,15 +12,32 @@
     supports_length_geodetic = False
     supports_area_geodetic = False
     supports_transform = False
-    supports_real_shape_operations = False
     supports_null_geometries = False
     supports_num_points_poly = False
+    unsupported_geojson_options = {'crs'}

     @cached_property
-    def supports_empty_geometry_collection(self):
-        return self.connection.mysql_version >= (5, 7, 5)
+    def empty_intersection_returns_none(self):
+        return (
+            not self.connection.mysql_is_mariadb and
+            self.connection.mysql_version < (5, 7, 5)
+        )

     @cached_property
     def supports_geometry_field_unique_index(self):
         # Not supported in MySQL since https://dev.mysql.com/worklog/task/?id=11808
         return self.connection.mysql_is_mariadb
+
+    @cached_property
+    def django_test_skips(self):
+        skips = super().django_test_skips
+        if (
+            not self.connection.mysql_is_mariadb and
+            self.connection.mysql_version < (8, 0, 0)
+        ):
+            skips.update({
+                'MySQL < 8 gives different results.': {
+                    'gis_tests.geoapp.tests.GeoLookupTest.test_disjoint_lookup',
+                },
+            })
+        return skips
('django/contrib/gis/db/backends/mysql', 'operations.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,9 +1,9 @@
+from django.contrib.gis.db import models
 from django.contrib.gis.db.backends.base.adapter import WKTAdapter
 from django.contrib.gis.db.backends.base.operations import (
     BaseSpatialOperations,
 )
 from django.contrib.gis.db.backends.utils import SpatialOperator
-from django.contrib.gis.db.models import aggregates
 from django.contrib.gis.geos.geometry import GEOSGeometryBase
 from django.contrib.gis.geos.prototypes.io import wkb_r
 from django.contrib.gis.measure import Distance
@@ -12,12 +12,18 @@


 class MySQLOperations(BaseSpatialOperations, DatabaseOperations):
-
-    mysql = True
     name = 'mysql'
     geom_func_prefix = 'ST_'

     Adapter = WKTAdapter
+
+    @cached_property
+    def mariadb(self):
+        return self.connection.mysql_is_mariadb
+
+    @cached_property
+    def mysql(self):
+        return not self.connection.mysql_is_mariadb

     @cached_property
     def select(self):
@@ -29,38 +35,40 @@

     @cached_property
     def gis_operators(self):
-        MBREquals = 'MBREqual' if (
-            self.connection.mysql_is_mariadb or self.connection.mysql_version < (5, 7, 6)
-        ) else 'MBREquals'
-        return {
+        operators = {
             'bbcontains': SpatialOperator(func='MBRContains'),  # For consistency w/PostGIS API
             'bboverlaps': SpatialOperator(func='MBROverlaps'),  # ...
             'contained': SpatialOperator(func='MBRWithin'),  # ...
-            'contains': SpatialOperator(func='MBRContains'),
-            'disjoint': SpatialOperator(func='MBRDisjoint'),
-            'equals': SpatialOperator(func=MBREquals),
-            'exact': SpatialOperator(func=MBREquals),
-            'intersects': SpatialOperator(func='MBRIntersects'),
-            'overlaps': SpatialOperator(func='MBROverlaps'),
-            'same_as': SpatialOperator(func=MBREquals),
-            'touches': SpatialOperator(func='MBRTouches'),
-            'within': SpatialOperator(func='MBRWithin'),
+            'contains': SpatialOperator(func='ST_Contains'),
+            'crosses': SpatialOperator(func='ST_Crosses'),
+            'disjoint': SpatialOperator(func='ST_Disjoint'),
+            'equals': SpatialOperator(func='ST_Equals'),
+            'exact': SpatialOperator(func='ST_Equals'),
+            'intersects': SpatialOperator(func='ST_Intersects'),
+            'overlaps': SpatialOperator(func='ST_Overlaps'),
+            'same_as': SpatialOperator(func='ST_Equals'),
+            'touches': SpatialOperator(func='ST_Touches'),
+            'within': SpatialOperator(func='ST_Within'),
         }
+        if self.connection.mysql_is_mariadb:
+            operators['relate'] = SpatialOperator(func='ST_Relate')
+        return operators

     disallowed_aggregates = (
-        aggregates.Collect, aggregates.Extent, aggregates.Extent3D,
-        aggregates.MakeLine, aggregates.Union,
+        models.Collect, models.Extent, models.Extent3D, models.MakeLine,
+        models.Union,
     )

     @cached_property
     def unsupported_functions(self):
         unsupported = {
             'AsGML', 'AsKML', 'AsSVG', 'Azimuth', 'BoundingCircle',
-            'ForcePolygonCW', 'ForceRHR', 'LineLocatePoint', 'MakeValid',
-            'MemSize', 'Perimeter', 'PointOnSurface', 'Reverse', 'Scale',
-            'SnapToGrid', 'Transform', 'Translate',
+            'ForcePolygonCW', 'GeometryDistance', 'LineLocatePoint',
+            'MakeValid', 'MemSize', 'Perimeter', 'PointOnSurface', 'Reverse',
+            'Scale', 'SnapToGrid', 'Transform', 'Translate',
         }
         if self.connection.mysql_is_mariadb:
+            unsupported.remove('PointOnSurface')
             unsupported.update({'GeoHash', 'IsValid'})
             if self.connection.mysql_version < (10, 2, 4):
                 unsupported.add('AsGeoJSON')
('django/contrib/gis/db/backends/mysql', 'introspection.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -10,7 +10,7 @@
     data_types_reverse = DatabaseIntrospection.data_types_reverse.copy()
     data_types_reverse[FIELD_TYPE.GEOMETRY] = 'GeometryField'

-    def get_geometry_type(self, table_name, geo_col):
+    def get_geometry_type(self, table_name, description):
         with self.connection.cursor() as cursor:
             # In order to get the specific geometry type of the field,
             # we introspect on the table definition using `DESCRIBE`.
@@ -19,7 +19,7 @@
             # Increment over description info until we get to the geometry
             # column.
             for column, typ, null, key, default, extra in cursor.fetchall():
-                if column == geo_col:
+                if column == description.name:
                     # Using OGRGeomType to convert from OGC name to Django field.
                     # MySQL does not support 3D or SRIDs, so the field params
                     # are empty.
('django/contrib/gis/db/backends/mysql', 'schema.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,8 +1,8 @@
 import logging

-from django.contrib.gis.db.models.fields import GeometryField
+from django.contrib.gis.db.models import GeometryField
+from django.db import OperationalError
 from django.db.backends.mysql.schema import DatabaseSchemaEditor
-from django.db.utils import OperationalError

 logger = logging.getLogger('django.contrib.gis')

@@ -16,11 +16,11 @@
         self.geometry_sql = []

     def skip_default(self, field):
-        return (
-            super().skip_default(field) or
-            # Geometry fields are stored as BLOB/TEXT and can't have defaults.
-            isinstance(field, GeometryField)
-        )
+        # Geometry fields are stored as BLOB/TEXT, for which MySQL < 8.0.13 and
+        # MariaDB < 10.2.1 don't support defaults.
+        if isinstance(field, GeometryField) and not self._supports_limited_data_type_defaults:
+            return True
+        return super().skip_default(field)

     def column_sql(self, model, field, include_default=False):
         column_sql = super().column_sql(model, field, include_default)
('django/contrib/gis/db/backends/spatialite', 'features.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -6,8 +6,19 @@


 class DatabaseFeatures(BaseSpatialFeatures, SQLiteDatabaseFeatures):
+    can_alter_geometry_field = False  # Not implemented
     supports_3d_storage = True

     @cached_property
     def supports_area_geodetic(self):
         return bool(self.connection.ops.lwgeom_version())
+
+    @cached_property
+    def django_test_skips(self):
+        skips = super().django_test_skips
+        skips.update({
+            "SpatiaLite doesn't support distance lookups with Distance objects.": {
+                'gis_tests.geogapp.tests.GeographyTest.test02_distance_lookup',
+            },
+        })
+        return skips
('django/contrib/gis/db/backends/spatialite', 'operations.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,15 +1,15 @@
 """
 SQL functions reference lists:
-https://www.gaia-gis.it/gaia-sins/spatialite-sql-4.2.1.html
+https://www.gaia-gis.it/gaia-sins/spatialite-sql-4.3.0.html
 """
+from django.contrib.gis.db import models
 from django.contrib.gis.db.backends.base.operations import (
     BaseSpatialOperations,
 )
 from django.contrib.gis.db.backends.spatialite.adapter import SpatiaLiteAdapter
 from django.contrib.gis.db.backends.utils import SpatialOperator
-from django.contrib.gis.db.models import aggregates
 from django.contrib.gis.geos.geometry import GEOSGeometry, GEOSGeometryBase
-from django.contrib.gis.geos.prototypes.io import wkb_r, wkt_r
+from django.contrib.gis.geos.prototypes.io import wkb_r
 from django.contrib.gis.measure import Distance
 from django.core.exceptions import ImproperlyConfigured
 from django.db.backends.sqlite3.operations import DatabaseOperations
@@ -62,13 +62,12 @@
         'dwithin': SpatialOperator(func='PtDistWithin'),
     }

-    disallowed_aggregates = (aggregates.Extent3D,)
-
-    @cached_property
-    def select(self):
-        return 'CAST (AsEWKB(%s) AS BLOB)' if self.spatial_version >= (4, 3, 0) else 'AsText(%s)'
+    disallowed_aggregates = (models.Extent3D,)
+
+    select = 'CAST (AsEWKB(%s) AS BLOB)'

     function_names = {
+        'AsWKB': 'St_AsBinary',
         'ForcePolygonCW': 'ST_ForceLHR',
         'Length': 'ST_Length',
         'LineLocatePoint': 'ST_Line_Locate_Point',
@@ -81,7 +80,7 @@

     @cached_property
     def unsupported_functions(self):
-        unsupported = {'BoundingCircle', 'ForceRHR', 'MemSize'}
+        unsupported = {'BoundingCircle', 'GeometryDistance', 'MemSize'}
         if not self.lwgeom_version():
             unsupported |= {'Azimuth', 'GeoHash', 'IsValid', 'MakeValid'}
         return unsupported
@@ -98,8 +97,8 @@
                     self.connection.settings_dict['NAME'],
                 )
             ) from exc
-        if version < (4, 1, 0):
-            raise ImproperlyConfigured('GeoDjango only supports SpatiaLite versions 4.1.0 and above.')
+        if version < (4, 3, 0):
+            raise ImproperlyConfigured('GeoDjango supports SpatiaLite 4.3.0 and above.')
         return version

     def convert_extent(self, box):
@@ -160,8 +159,8 @@
         "Return the version of GEOS used by SpatiaLite as a string."
         return self._get_spatialite_func('geos_version()')

-    def proj4_version(self):
-        "Return the version of the PROJ.4 library used by SpatiaLite."
+    def proj_version(self):
+        """Return the version of the PROJ library used by SpatiaLite."""
         return self._get_spatialite_func('proj4_version()')

     def lwgeom_version(self):
@@ -190,30 +189,21 @@

     # Routines for getting the OGC-compliant models.
     def geometry_columns(self):
-        from django.contrib.gis.db.backends.spatialite.models import SpatialiteGeometryColumns
+        from django.contrib.gis.db.backends.spatialite.models import (
+            SpatialiteGeometryColumns,
+        )
         return SpatialiteGeometryColumns

     def spatial_ref_sys(self):
-        from django.contrib.gis.db.backends.spatialite.models import SpatialiteSpatialRefSys
+        from django.contrib.gis.db.backends.spatialite.models import (
+            SpatialiteSpatialRefSys,
+        )
         return SpatialiteSpatialRefSys

     def get_geometry_converter(self, expression):
         geom_class = expression.output_field.geom_class
-        if self.spatial_version >= (4, 3, 0):
-            read = wkb_r().read
-
-            def converter(value, expression, connection):
-                return None if value is None else GEOSGeometryBase(read(value), geom_class)
-        else:
-            read = wkt_r().read
-            srid = expression.output_field.srid
-            if srid == -1:
-                srid = None
-
-            def converter(value, expression, connection):
-                if value is not None:
-                    geom = GEOSGeometryBase(read(value), geom_class)
-                    if srid:
-                        geom.srid = srid
-                    return geom
+        read = wkb_r().read
+
+        def converter(value, expression, connection):
+            return None if value is None else GEOSGeometryBase(read(value), geom_class)
         return converter
('django/contrib/gis/db/backends/spatialite', 'introspection.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -6,7 +6,7 @@

 class GeoFlexibleFieldLookupDict(FlexibleFieldLookupDict):
     """
-    Sublcass that includes updates the `base_data_types_reverse` dict
+    Subclass that includes updates the `base_data_types_reverse` dict
     for geometry field types.
     """
     base_data_types_reverse = {
@@ -24,17 +24,17 @@
 class SpatiaLiteIntrospection(DatabaseIntrospection):
     data_types_reverse = GeoFlexibleFieldLookupDict()

-    def get_geometry_type(self, table_name, geo_col):
+    def get_geometry_type(self, table_name, description):
         with self.connection.cursor() as cursor:
             # Querying the `geometry_columns` table to get additional metadata.
             cursor.execute('SELECT coord_dimension, srid, geometry_type '
                            'FROM geometry_columns '
                            'WHERE f_table_name=%s AND f_geometry_column=%s',
-                           (table_name, geo_col))
+                           (table_name, description.name))
             row = cursor.fetchone()
             if not row:
                 raise Exception('Could not find a geometry column for "%s"."%s"' %
-                                (table_name, geo_col))
+                                (table_name, description.name))

             # OGRGeomType does not require GDAL and makes it easy to convert
             # from OGC geom type name to Django field.
('django/contrib/gis/db/backends/spatialite', 'schema.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,5 @@
+from django.db import DatabaseError
 from django.db.backends.sqlite3.schema import DatabaseSchemaEditor
-from django.db.utils import DatabaseError


 class SpatialiteSchemaEditor(DatabaseSchemaEditor):
@@ -35,7 +35,7 @@
         return self.connection.ops.geo_quote_name(name)

     def column_sql(self, model, field, include_default=False):
-        from django.contrib.gis.db.models.fields import GeometryField
+        from django.contrib.gis.db.models import GeometryField
         if not isinstance(field, GeometryField):
             return super().column_sql(model, field, include_default)

@@ -82,7 +82,8 @@
         self.geometry_sql = []

     def delete_model(self, model, **kwargs):
-        from django.contrib.gis.db.models.fields import GeometryField
+        from django.contrib.gis.db.models import GeometryField
+
         # Drop spatial metadata (dropping the table does not automatically remove them)
         for field in model._meta.local_fields:
             if isinstance(field, GeometryField):
@@ -101,7 +102,7 @@
         super().delete_model(model, **kwargs)

     def add_field(self, model, field):
-        from django.contrib.gis.db.models.fields import GeometryField
+        from django.contrib.gis.db.models import GeometryField
         if isinstance(field, GeometryField):
             # Populate self.geometry_sql
             self.column_sql(model, field)
@@ -112,7 +113,8 @@
             super().add_field(model, field)

     def remove_field(self, model, field):
-        from django.contrib.gis.db.models.fields import GeometryField
+        from django.contrib.gis.db.models import GeometryField
+
         # NOTE: If the field is a geometry field, the table is just recreated,
         # the parent's remove_field can't be used cause it will skip the
         # recreation if the field does not have a database type. Geometry fields
@@ -124,7 +126,8 @@
             super().remove_field(model, field)

     def alter_db_table(self, model, old_db_table, new_db_table, disable_constraints=True):
-        from django.contrib.gis.db.models.fields import GeometryField
+        from django.contrib.gis.db.models import GeometryField
+
         # Remove geometry-ness from temp table
         for field in model._meta.local_fields:
             if isinstance(field, GeometryField):
('django/contrib/gis/db/backends/base', 'adapter.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -17,3 +17,8 @@

     def __str__(self):
         return self.wkt
+
+    @classmethod
+    def _fix_polygon(cls, poly):
+        # Hook for Oracle.
+        return poly
('django/contrib/gis/db/backends/base', 'features.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,6 @@
 import re

-from django.contrib.gis.db.models import aggregates
+from django.contrib.gis.db import models


 class BaseSpatialFeatures:
@@ -14,6 +14,8 @@
     # Does the backend introspect GeometryField to its subtypes?
     supports_geometry_field_introspection = True

+    # Does the database have a geography type?
+    supports_geography = False
     # Does the backend support storing 3D geometries?
     supports_3d_storage = False
     # Reference implementation of 3D functions is:
@@ -21,8 +23,6 @@
     supports_3d_functions = False
     # Does the database support SRID transform operations?
     supports_transform = True
-    # Do geometric relationship operations operate on real shapes (or only on bounding boxes)?
-    supports_real_shape_operations = True
     # Can geometry fields be null?
     supports_null_geometries = True
     # Are empty geometries supported?
@@ -35,15 +35,28 @@
     # Is the database able to count vertices on polygons (with `num_points`)?
     supports_num_points_poly = True

-    # The following properties indicate if the database backend support
-    # certain lookups (dwithin, left and right, relate, ...)
-    supports_left_right_lookups = False
+    # Does the backend support expressions for specifying distance in the
+    # dwithin lookup?
+    supports_dwithin_distance_expr = True

     # Does the database have raster support?
     supports_raster = False

     # Does the database support a unique index on geometry fields?
     supports_geometry_field_unique_index = True
+
+    # Can SchemaEditor alter geometry fields?
+    can_alter_geometry_field = True
+
+    # Do the database functions/aggregates support the tolerance parameter?
+    supports_tolerance_parameter = False
+
+    # Set of options that AsGeoJSON() doesn't support.
+    unsupported_geojson_options = {}
+
+    # Does Intersection() return None (rather than an empty GeometryCollection)
+    # for empty results?
+    empty_intersection_returns_none = True

     @property
     def supports_bbcontains_lookup(self):
@@ -76,23 +89,23 @@
     # Is the aggregate supported by the database?
     @property
     def supports_collect_aggr(self):
-        return aggregates.Collect not in self.connection.ops.disallowed_aggregates
+        return models.Collect not in self.connection.ops.disallowed_aggregates

     @property
     def supports_extent_aggr(self):
-        return aggregates.Extent not in self.connection.ops.disallowed_aggregates
+        return models.Extent not in self.connection.ops.disallowed_aggregates

     @property
     def supports_make_line_aggr(self):
-        return aggregates.MakeLine not in self.connection.ops.disallowed_aggregates
+        return models.MakeLine not in self.connection.ops.disallowed_aggregates

     @property
     def supports_union_aggr(self):
-        return aggregates.Union not in self.connection.ops.disallowed_aggregates
+        return models.Union not in self.connection.ops.disallowed_aggregates

     def __getattr__(self, name):
         m = re.match(r'has_(\w*)_function$', name)
         if m:
-            func_name = m.group(1)
+            func_name = m[1]
             return func_name not in self.connection.ops.unsupported_functions
         raise AttributeError
('django/contrib/gis/db/backends/base', 'operations.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,7 +3,7 @@
 from django.contrib.gis.measure import (
     Area as AreaMeasure, Distance as DistanceMeasure,
 )
-from django.db.utils import NotSupportedError
+from django.db import NotSupportedError
 from django.utils.functional import cached_property


@@ -12,6 +12,7 @@
     # an attribute for the spatial database version tuple (if applicable)
     postgis = False
     spatialite = False
+    mariadb = False
     mysql = False
     oracle = False
     spatial_version = None
@@ -23,10 +24,6 @@
     def select_extent(self):
         return self.select

-    # Does the spatial database have a geometry or geography type?
-    geography = False
-    geometry = False
-
     # Aggregates
     disallowed_aggregates = ()

@@ -36,11 +33,11 @@
     # match; used in spatial_function_name().
     function_names = {}

-    # Blacklist/set of known unsupported functions of the backend
+    # Set of known unsupported functions of the backend
     unsupported_functions = {
         'Area', 'AsGeoJSON', 'AsGML', 'AsKML', 'AsSVG', 'Azimuth',
         'BoundingCircle', 'Centroid', 'Difference', 'Distance', 'Envelope',
-        'ForceRHR', 'GeoHash', 'Intersection', 'IsValid', 'Length',
+        'GeoHash', 'GeometryDistance', 'Intersection', 'IsValid', 'Length',
         'LineLocatePoint', 'MakeValid', 'MemSize', 'NumGeometries',
         'NumPoints', 'Perimeter', 'PointOnSurface', 'Reverse', 'Scale',
         'SnapToGrid', 'SymDifference', 'Transform', 'Translate', 'Union',
('django/contrib/gis/db/models', 'functions.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,17 +1,15 @@
-import warnings
 from decimal import Decimal

 from django.contrib.gis.db.models.fields import BaseSpatialField, GeometryField
 from django.contrib.gis.db.models.sql import AreaField, DistanceField
 from django.contrib.gis.geos import GEOSGeometry
 from django.core.exceptions import FieldError
+from django.db import NotSupportedError
 from django.db.models import (
-    BooleanField, FloatField, IntegerField, TextField, Transform,
+    BinaryField, BooleanField, FloatField, Func, IntegerField, TextField,
+    Transform, Value,
 )
-from django.db.models.expressions import Func, Value
 from django.db.models.functions import Cast
-from django.db.utils import NotSupportedError
-from django.utils.deprecation import RemovedInDjango30Warning
 from django.utils.functional import cached_property

 NUMERIC_TYPES = (int, float, Decimal)
@@ -50,7 +48,7 @@
         return self.source_expressions[self.geom_param_pos[0]].field

     def as_sql(self, compiler, connection, function=None, **extra_context):
-        if not self.function and not function:
+        if self.function is None and function is None:
             function = connection.ops.spatial_function_name(self.name)
         return super().as_sql(compiler, connection, function=function, **extra_context)

@@ -103,22 +101,27 @@
     is not acceptable by the GIS functions expecting numeric values.
     """
     def as_sqlite(self, compiler, connection, **extra_context):
-        for expr in self.get_source_expressions():
-            if hasattr(expr, 'value') and isinstance(expr.value, Decimal):
-                expr.value = float(expr.value)
-        return super().as_sql(compiler, connection, **extra_context)
+        copy = self.copy()
+        copy.set_source_expressions([
+            Value(float(expr.value)) if hasattr(expr, 'value') and isinstance(expr.value, Decimal)
+            else expr
+            for expr in copy.get_source_expressions()
+        ])
+        return copy.as_sql(compiler, connection, **extra_context)


 class OracleToleranceMixin:
     tolerance = 0.05

     def as_oracle(self, compiler, connection, **extra_context):
-        tol = self.extra.get('tolerance', self.tolerance)
-        return self.as_sql(
-            compiler, connection,
-            template="%%(function)s(%%(expressions)s, %s)" % tol,
-            **extra_context
-        )
+        tolerance = Value(self._handle_param(
+            self.extra.get('tolerance', self.tolerance),
+            'tolerance',
+            NUMERIC_TYPES,
+        ))
+        clone = self.copy()
+        clone.set_source_expressions([*self.get_source_expressions(), tolerance])
+        return clone.as_sql(compiler, connection, **extra_context)


 class Area(OracleToleranceMixin, GeoFunc):
@@ -164,6 +167,12 @@
             expressions.append(options)
         super().__init__(*expressions, **extra)

+    def as_oracle(self, compiler, connection, **extra_context):
+        source_expressions = self.get_source_expressions()
+        clone = self.copy()
+        clone.set_source_expressions(source_expressions[:1])
+        return super(AsGeoJSON, clone).as_sql(compiler, connection, **extra_context)
+

 class AsGML(GeoFunc):
     geom_param_pos = (1,)
@@ -184,12 +193,14 @@
         return super(AsGML, clone).as_sql(compiler, connection, **extra_context)


-class AsKML(AsGML):
-    def as_sqlite(self, compiler, connection, **extra_context):
-        # No version parameter
-        clone = self.copy()
-        clone.set_source_expressions(self.get_source_expressions()[1:])
-        return clone.as_sql(compiler, connection, **extra_context)
+class AsKML(GeoFunc):
+    output_field = TextField()
+
+    def __init__(self, expression, precision=8, **extra):
+        expressions = [expression]
+        if precision is not None:
+            expressions.append(self._handle_param(precision, 'precision', int))
+        super().__init__(*expressions, **extra)


 class AsSVG(GeoFunc):
@@ -205,7 +216,17 @@
         super().__init__(*expressions, **extra)


-class BoundingCircle(OracleToleranceMixin, GeoFunc):
+class AsWKB(GeoFunc):
+    output_field = BinaryField()
+    arity = 1
+
+
+class AsWKT(GeoFunc):
+    output_field = TextField()
+    arity = 1
+
+
+class BoundingCircle(OracleToleranceMixin, GeomOutputGeoFunc):
     def __init__(self, expression, num_seg=48, **extra):
         super().__init__(expression, num_seg, **extra)

@@ -284,17 +305,6 @@
     arity = 1


-class ForceRHR(GeomOutputGeoFunc):
-    arity = 1
-
-    def __init__(self, *args, **kwargs):
-        warnings.warn(
-            'ForceRHR is deprecated in favor of ForcePolygonCW.',
-            RemovedInDjango30Warning, stacklevel=2,
-        )
-        super().__init__(*args, **kwargs)
-
-
 class GeoHash(GeoFunc):
     output_field = TextField()

@@ -310,6 +320,14 @@
         if len(clone.source_expressions) < 2:
             clone.source_expressions.append(Value(100))
         return clone.as_sql(compiler, connection, **extra_context)
+
+
+class GeometryDistance(GeoFunc):
+    output_field = FloatField()
+    arity = 2
+    function = ''
+    arg_joiner = ' <-> '
+    geom_param_pos = (0, 1)


 class Intersection(OracleToleranceMixin, GeomOutputGeoFunc):
@@ -365,7 +383,7 @@
     geom_param_pos = (0, 1)


-class MakeValid(GeoFunc):
+class MakeValid(GeomOutputGeoFunc):
     pass


('django/contrib/gis/db/models', 'fields.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -8,7 +8,7 @@
     MultiLineString, MultiPoint, MultiPolygon, Point, Polygon,
 )
 from django.core.exceptions import ImproperlyConfigured
-from django.db.models.fields import Field
+from django.db.models import Field
 from django.utils.translation import gettext_lazy as _

 # Local cache of the spatial_ref_sys table, which holds SRID data for each
@@ -145,7 +145,11 @@
             return None
         return connection.ops.Adapter(
             super().get_db_prep_value(value, connection, *args, **kwargs),
-            **({'geography': True} if self.geography and connection.ops.geography else {})
+            **(
+                {'geography': True}
+                if self.geography and connection.features.supports_geography
+                else {}
+            )
         )

     def get_raster_prep_value(self, value, is_candidate):
@@ -198,7 +202,7 @@
     """
     The base Geometry field -- maps to the OpenGIS Specification Geometry type.
     """
-    description = _("The base Geometry field -- maps to the OpenGIS Specification Geometry type.")
+    description = _('The base Geometry field — maps to the OpenGIS Specification Geometry type.')
     form_class = forms.GeometryField
     # The OpenGIS Geometry name.
     geom_type = 'GEOMETRY'
@@ -272,7 +276,9 @@
         of the spatial backend. For example, Oracle and MySQL require custom
         selection formats in order to retrieve geometries in OGC WKB.
         """
-        return compiler.connection.ops.select % sql, params
+        if not compiler.query.subquery:
+            return compiler.connection.ops.select % sql, params
+        return sql, params


 # The OpenGIS Geometry Type Fields
('django/contrib/gis/db/models', 'proxy.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -14,10 +14,9 @@
         Initialize on the given Geometry or Raster class (not an instance)
         and the corresponding field.
         """
-        self._field = field
         self._klass = klass
         self._load_func = load_func or klass
-        super().__init__(field.attname)
+        super().__init__(field)

     def __get__(self, instance, cls=None):
         """
@@ -32,7 +31,7 @@

         # Getting the value of the field.
         try:
-            geo_value = instance.__dict__[self._field.attname]
+            geo_value = instance.__dict__[self.field.attname]
         except KeyError:
             geo_value = super().__get__(instance, cls)

@@ -44,7 +43,7 @@
             # Otherwise, a geometry or raster object is built using the field's
             # contents, and the model's corresponding attribute is set.
             geo_obj = self._load_func(geo_value)
-            setattr(instance, self._field.attname, geo_obj)
+            setattr(instance, self.field.attname, geo_obj)
         return geo_obj

     def __set__(self, instance, value):
@@ -56,7 +55,7 @@
         To set rasters, use JSON or dict values.
         """
         # The geographic type of the field.
-        gtype = self._field.geom_type
+        gtype = self.field.geom_type

         if gtype == 'RASTER' and (value is None or isinstance(value, (str, dict, self._klass))):
             # For raster fields, assure input is None or a string, dict, or
@@ -67,7 +66,7 @@
             # general GeometryField is used.
             if value.srid is None:
                 # Assigning the field SRID if the geometry has no SRID.
-                value.srid = self._field.srid
+                value.srid = self.field.srid
         elif value is None or isinstance(value, (str, memoryview)):
             # Set geometries with None, WKT, HEX, or WKB
             pass
@@ -76,5 +75,5 @@
                 instance.__class__.__name__, gtype, type(value)))

         # Setting the objects dictionary with the value, and returning.
-        instance.__dict__[self._field.attname] = value
+        instance.__dict__[self.field.attname] = value
         return value
('django/contrib/gis/db/models', 'lookups.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,9 +1,9 @@
-import re
-
 from django.contrib.gis.db.models.fields import BaseSpatialField
-from django.db.models.expressions import Expression
-from django.db.models.lookups import Lookup, Transform
+from django.contrib.gis.measure import Distance
+from django.db import NotSupportedError
+from django.db.models import Expression, Lookup, Transform
 from django.db.models.sql.query import Query
+from django.utils.regex_helper import _lazy_re_compile


 class RasterBandTransform(Transform):
@@ -74,9 +74,9 @@
         return connection.ops.gis_operators[self.lookup_name]

     def as_sql(self, compiler, connection):
-        lhs_sql, sql_params = self.process_lhs(compiler, connection)
+        lhs_sql, lhs_params = self.process_lhs(compiler, connection)
         rhs_sql, rhs_params = self.process_rhs(compiler, connection)
-        sql_params.extend(rhs_params)
+        sql_params = (*lhs_params, *rhs_params)

         template_params = {'lhs': lhs_sql, 'rhs': rhs_sql, 'value': '%s', **self.template_params}
         rhs_op = self.get_rhs_op(connection, rhs_sql)
@@ -251,7 +251,7 @@
 class RelateLookup(GISLookup):
     lookup_name = 'relate'
     sql_template = '%(func)s(%(lhs)s, %(rhs)s, %%s)'
-    pattern_regex = re.compile(r'^[012TF\*]{9}$')
+    pattern_regex = _lazy_re_compile(r'^[012TF\*]{9}$')

     def process_rhs(self, compiler, connection):
         # Check the pattern argument
@@ -301,7 +301,20 @@
 @BaseSpatialField.register_lookup
 class DWithinLookup(DistanceLookupBase):
     lookup_name = 'dwithin'
-    sql_template = '%(func)s(%(lhs)s, %(rhs)s, %%s)'
+    sql_template = '%(func)s(%(lhs)s, %(rhs)s, %(value)s)'
+
+    def process_distance(self, compiler, connection):
+        dist_param = self.rhs_params[0]
+        if (
+            not connection.features.supports_dwithin_distance_expr and
+            hasattr(dist_param, 'resolve_expression') and
+            not isinstance(dist_param, Distance)
+        ):
+            raise NotSupportedError(
+                'This backend does not support expressions for specifying '
+                'distance in the dwithin lookup.'
+            )
+        return super().process_distance(compiler, connection)

     def process_rhs(self, compiler, connection):
         dist_sql, dist_params = self.process_distance(compiler, connection)
('django/contrib/gis/db/models', 'aggregates.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,7 +1,7 @@
 from django.contrib.gis.db.models.fields import (
     ExtentField, GeometryCollectionField, GeometryField, LineStringField,
 )
-from django.db.models.aggregates import Aggregate
+from django.db.models import Aggregate, Value
 from django.utils.functional import cached_property

 __all__ = ['Collect', 'Extent', 'Extent3D', 'MakeLine', 'Union']
@@ -27,9 +27,16 @@
         )

     def as_oracle(self, compiler, connection, **extra_context):
-        tolerance = self.extra.get('tolerance') or getattr(self, 'tolerance', 0.05)
-        template = None if self.is_extent else '%(function)s(SDOAGGRTYPE(%(expressions)s,%(tolerance)s))'
-        return self.as_sql(compiler, connection, template=template, tolerance=tolerance, **extra_context)
+        if not self.is_extent:
+            tolerance = self.extra.get('tolerance') or getattr(self, 'tolerance', 0.05)
+            clone = self.copy()
+            clone.set_source_expressions([
+                *self.get_source_expressions(),
+                Value(tolerance),
+            ])
+            template = '%(function)s(SDOAGGRTYPE(%(expressions)s))'
+            return clone.as_sql(compiler, connection, template=template, **extra_context)
+        return self.as_sql(compiler, connection, **extra_context)

     def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):
         c = super().resolve_expression(query, allow_joins, reuse, summarize, for_save)
('django/contrib/gis/geoip2', 'resources.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -6,6 +6,7 @@
         'country_code': response.country.iso_code,
         'country_name': response.country.name,
         'dma_code': response.location.metro_code,
+        'is_in_european_union': response.country.is_in_european_union,
         'latitude': response.location.latitude,
         'longitude': response.location.longitude,
         'postal_code': response.postal.code,
('django/contrib/gis/geoip2', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,11 +1,11 @@
 import socket
-from pathlib import Path

 import geoip2.database

 from django.conf import settings
 from django.core.exceptions import ValidationError
 from django.core.validators import validate_ipv46_address
+from django.utils._os import to_path

 from .resources import City, Country

@@ -76,10 +76,8 @@
         path = path or GEOIP_SETTINGS['GEOIP_PATH']
         if not path:
             raise GeoIP2Exception('GeoIP path must be provided via parameter or the GEOIP_PATH setting.')
-        if not isinstance(path, str):
-            raise TypeError('Invalid path type: %s' % type(path).__name__)
-
-        path = Path(path)
+
+        path = to_path(path)
         if path.is_dir():
             # Constructing the GeoIP database filenames using the settings
             # dictionary. If the database files for the GeoLite country
('django/http', 'multipartparser.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -7,6 +7,8 @@
 import base64
 import binascii
 import cgi
+import collections
+import html
 from urllib.parse import unquote

 from django.conf import settings
@@ -17,8 +19,7 @@
     SkipFile, StopFutureHandlers, StopUpload,
 )
 from django.utils.datastructures import MultiValueDict
-from django.utils.encoding import force_text
-from django.utils.text import unescape_entities
+from django.utils.encoding import force_str

 __all__ = ('MultiPartParser', 'MultiPartParserError', 'InputStreamExhausted')

@@ -66,10 +67,13 @@
             raise MultiPartParserError('Invalid Content-Type: %s' % content_type)

         # Parse the header to get the boundary to split the parts.
-        ctypes, opts = parse_header(content_type.encode('ascii'))
+        try:
+            ctypes, opts = parse_header(content_type.encode('ascii'))
+        except UnicodeEncodeError:
+            raise MultiPartParserError('Invalid non-ASCII Content-Type in multipart: %s' % force_str(content_type))
         boundary = opts.get('boundary')
         if not boundary or not cgi.valid_boundary(boundary):
-            raise MultiPartParserError('Invalid boundary in multipart: %s' % boundary.decode())
+            raise MultiPartParserError('Invalid boundary in multipart: %s' % force_str(boundary))

         # Content-Length should contain the length of the body we are about
         # to receive.
@@ -145,6 +149,8 @@
         num_post_keys = 0
         # To limit the amount of data read from the request.
         read_size = None
+        # Whether a file upload is finished.
+        uploaded_file = True

         try:
             for item_type, meta_data, field_stream in Parser(stream, self._boundary):
@@ -154,6 +160,7 @@
                     # we hit the next boundary/part of the multipart content.
                     self.handle_file_complete(old_field_name, counters)
                     old_field_name = None
+                    uploaded_file = True

                 try:
                     disposition = meta_data['content-disposition'][1]
@@ -164,7 +171,7 @@
                 transfer_encoding = meta_data.get('content-transfer-encoding')
                 if transfer_encoding is not None:
                     transfer_encoding = transfer_encoding[0].strip()
-                field_name = force_text(field_name, encoding, errors='replace')
+                field_name = force_str(field_name, encoding, errors='replace')

                 if item_type == FIELD:
                     # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.
@@ -199,13 +206,13 @@
                             num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE):
                         raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')

-                    self._post.appendlist(field_name, force_text(data, encoding, errors='replace'))
+                    self._post.appendlist(field_name, force_str(data, encoding, errors='replace'))
                 elif item_type == FILE:
                     # This is a file, use the handler...
                     file_name = disposition.get('filename')
                     if file_name:
-                        file_name = force_text(file_name, encoding, errors='replace')
-                        file_name = self.IE_sanitize(unescape_entities(file_name))
+                        file_name = force_str(file_name, encoding, errors='replace')
+                        file_name = self.sanitize_file_name(file_name)
                     if not file_name:
                         continue

@@ -219,6 +226,7 @@
                         content_length = None

                     counters = [0] * len(handlers)
+                    uploaded_file = False
                     try:
                         for handler in handlers:
                             try:
@@ -240,6 +248,8 @@
                                 remaining = len(stripped_chunk) % 4
                                 while remaining != 0:
                                     over_chunk = field_stream.read(4 - remaining)
+                                    if not over_chunk:
+                                        break
                                     stripped_chunk += b"".join(over_chunk.split())
                                     remaining = len(stripped_chunk) % 4

@@ -273,6 +283,9 @@
             if not e.connection_reset:
                 exhaust(self._input_data)
         else:
+            if not uploaded_file:
+                for handler in handlers:
+                    handler.upload_interrupted()
             # Make sure that the request data is all fed
             exhaust(self._input_data)

@@ -290,12 +303,31 @@
             file_obj = handler.file_complete(counters[i])
             if file_obj:
                 # If it returns a file object, then set the files dict.
-                self._files.appendlist(force_text(old_field_name, self._encoding, errors='replace'), file_obj)
+                self._files.appendlist(force_str(old_field_name, self._encoding, errors='replace'), file_obj)
                 break

-    def IE_sanitize(self, filename):
-        """Cleanup filename from Internet Explorer full paths."""
-        return filename and filename[filename.rfind("\\") + 1:].strip()
+    def sanitize_file_name(self, file_name):
+        """
+        Sanitize the filename of an upload.
+
+        Remove all possible path separators, even though that might remove more
+        than actually required by the target system. Filenames that could
+        potentially cause problems (current/parent dir) are also discarded.
+
+        It should be noted that this function could still return a "filepath"
+        like "C:some_file.txt" which is handled later on by the storage layer.
+        So while this function does sanitize filenames to some extent, the
+        resulting filename should still be considered as untrusted user input.
+        """
+        file_name = html.unescape(file_name)
+        file_name = file_name.rsplit('/')[-1]
+        file_name = file_name.rsplit('\\')[-1]
+
+        if file_name in {'', '.', '..'}:
+            return None
+        return file_name
+
+    IE_sanitize = sanitize_file_name

     def _close_files(self):
         # Free up all file handles.
@@ -356,8 +388,7 @@
                     remaining -= len(emitting)
                     yield emitting

-        out = b''.join(parts())
-        return out
+        return b''.join(parts())

     def __next__(self):
         """
@@ -565,9 +596,7 @@
         iterator = iter(stream_or_iterable)
     except TypeError:
         iterator = ChunkIter(stream_or_iterable, 16384)
-
-    for __ in iterator:
-        pass
+    collections.deque(iterator, maxlen=0)  # consume iterator quickly.


 def parse_boundary_stream(stream, max_header_size):
@@ -663,12 +692,12 @@
                 if p.count(b"'") == 2:
                     has_encoding = True
             value = p[i + 1:].strip()
+            if len(value) >= 2 and value[:1] == value[-1:] == b'"':
+                value = value[1:-1]
+                value = value.replace(b'\\\\', b'\\').replace(b'\\"', b'"')
             if has_encoding:
                 encoding, lang, value = value.split(b"'")
                 value = unquote(value.decode(), encoding=encoding.decode())
-            if len(value) >= 2 and value[:1] == value[-1:] == b'"':
-                value = value[1:-1]
-                value = value.replace(b'\\\\', b'\\').replace(b'\\"', b'"')
             pdict[name] = value
     return key, pdict

('django/http', 'request.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,30 +1,46 @@
+import cgi
+import codecs
 import copy
-import re
 import warnings
 from io import BytesIO
 from itertools import chain
-from urllib.parse import quote, urlencode, urljoin, urlsplit
+from urllib.parse import parse_qsl, quote, urlencode, urljoin, urlsplit

 from django.conf import settings
 from django.core import signing
 from django.core.exceptions import (
-    DisallowedHost, ImproperlyConfigured, RequestDataTooBig,
+    DisallowedHost, ImproperlyConfigured, RequestDataTooBig, TooManyFieldsSent,
 )
 from django.core.files import uploadhandler
 from django.http.multipartparser import MultiPartParser, MultiPartParserError
 from django.utils.datastructures import (
     CaseInsensitiveMapping, ImmutableList, MultiValueDict,
 )
-from django.utils.deprecation import RemovedInDjango30Warning
+from django.utils.deprecation import RemovedInDjango40Warning
 from django.utils.encoding import escape_uri_path, iri_to_uri
 from django.utils.functional import cached_property
-from django.utils.http import is_same_domain, limited_parse_qsl
+from django.utils.http import is_same_domain
+from django.utils.inspect import func_supports_parameter
+from django.utils.regex_helper import _lazy_re_compile
+
+from .multipartparser import parse_header
+
+# TODO: Remove when dropping support for PY37. inspect.signature() is used to
+# detect whether the max_num_fields argument is available as this security fix
+# was backported to Python 3.6.8 and 3.7.2, and may also have been applied by
+# downstream package maintainers to other versions in their repositories.
+if (
+    not func_supports_parameter(parse_qsl, 'max_num_fields') or
+    not func_supports_parameter(parse_qsl, 'separator')
+):
+    from django.utils.http import parse_qsl
+

 RAISE_ERROR = object()
-host_validation_re = re.compile(r"^([a-z0-9.-]+|\[[a-f0-9]*:[a-f0-9\.:]+\])(:\d+)?$")
-
-
-class UnreadablePostError(IOError):
+host_validation_re = _lazy_re_compile(r"^([a-z0-9.-]+|\[[a-f0-9]*:[a-f0-9\.:]+\])(:\d+)?$")
+
+
+class UnreadablePostError(OSError):
     pass


@@ -70,6 +86,28 @@
     @cached_property
     def headers(self):
         return HttpHeaders(self.META)
+
+    @cached_property
+    def accepted_types(self):
+        """Return a list of MediaType instances."""
+        return parse_accept_header(self.headers.get('Accept', '*/*'))
+
+    def accepts(self, media_type):
+        return any(
+            accepted_type.match(media_type)
+            for accepted_type in self.accepted_types
+        )
+
+    def _set_content_type_params(self, meta):
+        """Set content_type, content_params, and encoding."""
+        self.content_type, self.content_params = cgi.parse_header(meta.get('CONTENT_TYPE', ''))
+        if 'charset' in self.content_params:
+            try:
+                codecs.lookup(self.content_params['charset'])
+            except LookupError:
+                pass
+            else:
+                self.encoding = self.content_params['charset']

     def _get_raw_host(self):
         """
@@ -97,7 +135,7 @@
         # Allow variants of localhost if ALLOWED_HOSTS is empty and DEBUG=True.
         allowed_hosts = settings.ALLOWED_HOSTS
         if settings.DEBUG and not allowed_hosts:
-            allowed_hosts = ['localhost', '127.0.0.1', '[::1]']
+            allowed_hosts = ['.localhost', '127.0.0.1', '[::1]']

         domain, port = split_domain_port(host)
         if domain and validate_host(domain, allowed_hosts):
@@ -180,6 +218,9 @@
             # Make it an absolute url (but schemeless and domainless) for the
             # edge case that the path starts with '//'.
             location = '//%s' % self.get_full_path()
+        else:
+            # Coerce lazy locations.
+            location = str(location)
         bits = urlsplit(location)
         if not (bits.scheme and bits.netloc):
             # Handle the simple, most common case. If the location is absolute
@@ -215,19 +256,26 @@
     def scheme(self):
         if settings.SECURE_PROXY_SSL_HEADER:
             try:
-                header, value = settings.SECURE_PROXY_SSL_HEADER
+                header, secure_value = settings.SECURE_PROXY_SSL_HEADER
             except ValueError:
                 raise ImproperlyConfigured(
                     'The SECURE_PROXY_SSL_HEADER setting must be a tuple containing two values.'
                 )
-            if self.META.get(header) == value:
-                return 'https'
+            header_value = self.META.get(header)
+            if header_value is not None:
+                return 'https' if header_value == secure_value else 'http'
         return self._get_scheme()

     def is_secure(self):
         return self.scheme == 'https'

     def is_ajax(self):
+        warnings.warn(
+            'request.is_ajax() is deprecated. See Django 3.1 release notes '
+            'for more details about this deprecation.',
+            RemovedInDjango40Warning,
+            stacklevel=2,
+        )
         return self.META.get('HTTP_X_REQUESTED_WITH') == 'XMLHttpRequest'

     @property
@@ -286,7 +334,7 @@

             try:
                 self._body = self.read()
-            except IOError as e:
+            except OSError as e:
                 raise UnreadablePostError(*e.args) from e
             self._stream = BytesIO(self._body)
         return self._body
@@ -326,7 +374,7 @@

     def close(self):
         if hasattr(self, '_files'):
-            for f in chain.from_iterable(l[1] for l in self._files.lists()):
+            for f in chain.from_iterable(list_[1] for list_ in self._files.lists()):
                 f.close()

     # File-like and iterator interface.
@@ -341,25 +389,18 @@
         self._read_started = True
         try:
             return self._stream.read(*args, **kwargs)
-        except IOError as e:
+        except OSError as e:
             raise UnreadablePostError(*e.args) from e

     def readline(self, *args, **kwargs):
         self._read_started = True
         try:
             return self._stream.readline(*args, **kwargs)
-        except IOError as e:
+        except OSError as e:
             raise UnreadablePostError(*e.args) from e

     def __iter__(self):
         return iter(self.readline, b'')
-
-    def xreadlines(self):
-        warnings.warn(
-            'HttpRequest.xreadlines() is deprecated in favor of iterating the '
-            'request.', RemovedInDjango30Warning, stacklevel=2,
-        )
-        yield from self

     def readlines(self):
         return list(self)
@@ -378,6 +419,10 @@
                 headers[name] = value
         super().__init__(headers)

+    def __getitem__(self, key):
+        """Allow header lookup using underscores in place of hyphens."""
+        return super().__getitem__(key.replace('_', '-'))
+
     @classmethod
     def parse_header_name(cls, header):
         if header.startswith(cls.HTTP_PREFIX):
@@ -413,8 +458,8 @@
         query_string = query_string or ''
         parse_qsl_kwargs = {
             'keep_blank_values': True,
-            'fields_limit': settings.DATA_UPLOAD_MAX_NUMBER_FIELDS,
             'encoding': self.encoding,
+            'max_num_fields': settings.DATA_UPLOAD_MAX_NUMBER_FIELDS,
         }
         if isinstance(query_string, bytes):
             # query_string normally contains URL-encoded data, a subset of ASCII.
@@ -423,8 +468,18 @@
             except UnicodeDecodeError:
                 # ... but some user agents are misbehaving :-(
                 query_string = query_string.decode('iso-8859-1')
-        for key, value in limited_parse_qsl(query_string, **parse_qsl_kwargs):
-            self.appendlist(key, value)
+        try:
+            for key, value in parse_qsl(query_string, **parse_qsl_kwargs):
+                self.appendlist(key, value)
+        except ValueError as e:
+            # ValueError can also be raised if the strict_parsing argument to
+            # parse_qsl() is True. As that is not used by Django, assume that
+            # the exception was raised by exceeding the value of max_num_fields
+            # instead of fragile checks of exception message strings.
+            raise TooManyFieldsSent(
+                'The number of GET/POST parameters exceeded '
+                'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'
+            ) from e
         self._mutable = mutable

     @classmethod
@@ -545,8 +600,42 @@
         return '&'.join(output)


+class MediaType:
+    def __init__(self, media_type_raw_line):
+        full_type, self.params = parse_header(
+            media_type_raw_line.encode('ascii') if media_type_raw_line else b''
+        )
+        self.main_type, _, self.sub_type = full_type.partition('/')
+
+    def __str__(self):
+        params_str = ''.join(
+            '; %s=%s' % (k, v.decode('ascii'))
+            for k, v in self.params.items()
+        )
+        return '%s%s%s' % (
+            self.main_type,
+            ('/%s' % self.sub_type) if self.sub_type else '',
+            params_str,
+        )
+
+    def __repr__(self):
+        return '<%s: %s>' % (self.__class__.__qualname__, self)
+
+    @property
+    def is_all_types(self):
+        return self.main_type == '*' and self.sub_type == '*'
+
+    def match(self, other):
+        if self.is_all_types:
+            return True
+        other = MediaType(other)
+        if self.main_type == other.main_type and self.sub_type in {'*', other.sub_type}:
+            return True
+        return False
+
+
 # It's neither necessary nor appropriate to use
-# django.utils.encoding.force_text for parsing URLs and form inputs. Thus,
+# django.utils.encoding.force_str() for parsing URLs and form inputs. Thus,
 # this slightly more restricted function, used by QueryDict.
 def bytes_to_text(s, encoding):
     """
@@ -600,3 +689,7 @@
     Return ``True`` for a valid host, ``False`` otherwise.
     """
     return any(pattern == '*' or is_same_domain(host, pattern) for pattern in allowed_hosts)
+
+
+def parse_accept_header(header):
+    return [MediaType(token) for token in header.split(',') if token.strip()]
('django/http', 'response.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -5,6 +5,7 @@
 import re
 import sys
 import time
+from collections.abc import Mapping
 from email.header import Header
 from http.client import responses
 from urllib.parse import quote, urlparse
@@ -15,107 +16,40 @@
 from django.core.serializers.json import DjangoJSONEncoder
 from django.http.cookie import SimpleCookie
 from django.utils import timezone
+from django.utils.datastructures import (
+    CaseInsensitiveMapping, _destruct_iterable_mapping_values,
+)
 from django.utils.encoding import iri_to_uri
 from django.utils.http import http_date
-
-_charset_from_content_type_re = re.compile(r';\s*charset=(?P<charset>[^\s;]+)', re.I)
-
-
-class BadHeaderError(ValueError):
-    pass
-
-
-class HttpResponseBase:
-    """
-    An HTTP response base class with dictionary-accessed headers.
-
-    This class doesn't handle content. It should not be used directly.
-    Use the HttpResponse and StreamingHttpResponse subclasses instead.
-    """
-
-    status_code = 200
-
-    def __init__(self, content_type=None, status=None, reason=None, charset=None):
-        # _headers is a mapping of the lowercase name to the original case of
-        # the header (required for working with legacy systems) and the header
-        # value. Both the name of the header and its value are ASCII strings.
-        self._headers = {}
-        self._closable_objects = []
-        # This parameter is set by the handler. It's necessary to preserve the
-        # historical behavior of request_finished.
-        self._handler_class = None
-        self.cookies = SimpleCookie()
-        self.closed = False
-        if status is not None:
-            try:
-                self.status_code = int(status)
-            except (ValueError, TypeError):
-                raise TypeError('HTTP status code must be an integer.')
-
-            if not 100 <= self.status_code <= 599:
-                raise ValueError('HTTP status code must be an integer from 100 to 599.')
-        self._reason_phrase = reason
-        self._charset = charset
-        if content_type is None:
-            content_type = '%s; charset=%s' % (settings.DEFAULT_CONTENT_TYPE,
-                                               self.charset)
-        self['Content-Type'] = content_type
-
-    @property
-    def reason_phrase(self):
-        if self._reason_phrase is not None:
-            return self._reason_phrase
-        # Leave self._reason_phrase unset in order to use the default
-        # reason phrase for status code.
-        return responses.get(self.status_code, 'Unknown Status Code')
-
-    @reason_phrase.setter
-    def reason_phrase(self, value):
-        self._reason_phrase = value
-
-    @property
-    def charset(self):
-        if self._charset is not None:
-            return self._charset
-        content_type = self.get('Content-Type', '')
-        matched = _charset_from_content_type_re.search(content_type)
-        if matched:
-            # Extract the charset and strip its double quotes
-            return matched.group('charset').replace('"', '')
-        return settings.DEFAULT_CHARSET
-
-    @charset.setter
-    def charset(self, value):
-        self._charset = value
-
-    def serialize_headers(self):
-        """HTTP headers as a bytestring."""
-        def to_bytes(val, encoding):
-            return val if isinstance(val, bytes) else val.encode(encoding)
-
-        headers = [
-            (to_bytes(key, 'ascii') + b': ' + to_bytes(value, 'latin-1'))
-            for key, value in self._headers.values()
-        ]
-        return b'\r\n'.join(headers)
-
-    __bytes__ = serialize_headers
-
-    @property
-    def _content_type_for_repr(self):
-        return ', "%s"' % self['Content-Type'] if 'Content-Type' in self else ''
+from django.utils.regex_helper import _lazy_re_compile
+
+_charset_from_content_type_re = _lazy_re_compile(r';\s*charset=(?P<charset>[^\s;]+)', re.I)
+
+
+class ResponseHeaders(CaseInsensitiveMapping):
+    def __init__(self, data):
+        """
+        Populate the initial data using __setitem__ to ensure values are
+        correctly encoded.
+        """
+        if not isinstance(data, Mapping):
+            data = {k: v for k, v in _destruct_iterable_mapping_values(data)}
+        self._store = {}
+        for header, value in data.items():
+            self[header] = value

     def _convert_to_charset(self, value, charset, mime_encode=False):
         """
         Convert headers key/value to ascii/latin-1 native strings.
-
         `charset` must be 'ascii' or 'latin-1'. If `mime_encode` is True and
         `value` can't be represented in the given charset, apply MIME-encoding.
         """
         if not isinstance(value, (bytes, str)):
             value = str(value)
-        if ((isinstance(value, bytes) and (b'\n' in value or b'\r' in value)) or
-                isinstance(value, str) and ('\n' in value or '\r' in value)):
+        if (
+            (isinstance(value, bytes) and (b'\n' in value or b'\r' in value)) or
+            (isinstance(value, str) and ('\n' in value or '\r' in value))
+        ):
             raise BadHeaderError("Header values can't contain newlines (got %r)" % value)
         try:
             if isinstance(value, str):
@@ -132,28 +66,128 @@
                 raise
         return value

+    def __delitem__(self, key):
+        self.pop(key)
+
+    def __setitem__(self, key, value):
+        key = self._convert_to_charset(key, 'ascii')
+        value = self._convert_to_charset(value, 'latin-1', mime_encode=True)
+        self._store[key.lower()] = (key, value)
+
+    def pop(self, key, default=None):
+        return self._store.pop(key.lower(), default)
+
+    def setdefault(self, key, value):
+        if key not in self:
+            self[key] = value
+
+
+class BadHeaderError(ValueError):
+    pass
+
+
+class HttpResponseBase:
+    """
+    An HTTP response base class with dictionary-accessed headers.
+
+    This class doesn't handle content. It should not be used directly.
+    Use the HttpResponse and StreamingHttpResponse subclasses instead.
+    """
+
+    status_code = 200
+
+    def __init__(self, content_type=None, status=None, reason=None, charset=None, headers=None):
+        self.headers = ResponseHeaders(headers or {})
+        self._charset = charset
+        if content_type and 'Content-Type' in self.headers:
+            raise ValueError(
+                "'headers' must not contain 'Content-Type' when the "
+                "'content_type' parameter is provided."
+            )
+        if 'Content-Type' not in self.headers:
+            if content_type is None:
+                content_type = 'text/html; charset=%s' % self.charset
+            self.headers['Content-Type'] = content_type
+        self._resource_closers = []
+        # This parameter is set by the handler. It's necessary to preserve the
+        # historical behavior of request_finished.
+        self._handler_class = None
+        self.cookies = SimpleCookie()
+        self.closed = False
+        if status is not None:
+            try:
+                self.status_code = int(status)
+            except (ValueError, TypeError):
+                raise TypeError('HTTP status code must be an integer.')
+
+            if not 100 <= self.status_code <= 599:
+                raise ValueError('HTTP status code must be an integer from 100 to 599.')
+        self._reason_phrase = reason
+
+    @property
+    def reason_phrase(self):
+        if self._reason_phrase is not None:
+            return self._reason_phrase
+        # Leave self._reason_phrase unset in order to use the default
+        # reason phrase for status code.
+        return responses.get(self.status_code, 'Unknown Status Code')
+
+    @reason_phrase.setter
+    def reason_phrase(self, value):
+        self._reason_phrase = value
+
+    @property
+    def charset(self):
+        if self._charset is not None:
+            return self._charset
+        content_type = self.get('Content-Type', '')
+        matched = _charset_from_content_type_re.search(content_type)
+        if matched:
+            # Extract the charset and strip its double quotes
+            return matched['charset'].replace('"', '')
+        return settings.DEFAULT_CHARSET
+
+    @charset.setter
+    def charset(self, value):
+        self._charset = value
+
+    def serialize_headers(self):
+        """HTTP headers as a bytestring."""
+        def to_bytes(val, encoding):
+            return val if isinstance(val, bytes) else val.encode(encoding)
+
+        headers = [
+            (to_bytes(key, 'ascii') + b': ' + to_bytes(value, 'latin-1'))
+            for key, value in self.headers.items()
+        ]
+        return b'\r\n'.join(headers)
+
+    __bytes__ = serialize_headers
+
+    @property
+    def _content_type_for_repr(self):
+        return ', "%s"' % self.headers['Content-Type'] if 'Content-Type' in self.headers else ''
+
     def __setitem__(self, header, value):
-        header = self._convert_to_charset(header, 'ascii')
-        value = self._convert_to_charset(value, 'latin-1', mime_encode=True)
-        self._headers[header.lower()] = (header, value)
+        self.headers[header] = value

     def __delitem__(self, header):
-        self._headers.pop(header.lower(), False)
+        del self.headers[header]

     def __getitem__(self, header):
-        return self._headers[header.lower()][1]
+        return self.headers[header]

     def has_header(self, header):
         """Case-insensitive check for a header."""
-        return header.lower() in self._headers
+        return header in self.headers

     __contains__ = has_header

     def items(self):
-        return self._headers.values()
+        return self.headers.items()

     def get(self, header, alternate=None):
-        return self._headers.get(header.lower(), (None, alternate))[1]
+        return self.headers.get(header, alternate)

     def set_cookie(self, key, value='', max_age=None, expires=None, path='/',
                    domain=None, secure=False, httponly=False, samesite=None):
@@ -184,7 +218,7 @@
         else:
             self.cookies[key]['expires'] = ''
         if max_age is not None:
-            self.cookies[key]['max-age'] = max_age
+            self.cookies[key]['max-age'] = int(max_age)
             # IE requires expires, so set it if hasn't been already.
             if not expires:
                 self.cookies[key]['expires'] = http_date(time.time() + max_age)
@@ -197,26 +231,30 @@
         if httponly:
             self.cookies[key]['httponly'] = True
         if samesite:
-            if samesite.lower() not in ('lax', 'strict'):
-                raise ValueError('samesite must be "lax" or "strict".')
+            if samesite.lower() not in ('lax', 'none', 'strict'):
+                raise ValueError('samesite must be "lax", "none", or "strict".')
             self.cookies[key]['samesite'] = samesite

     def setdefault(self, key, value):
         """Set a header unless it has already been set."""
-        if key not in self:
-            self[key] = value
+        self.headers.setdefault(key, value)

     def set_signed_cookie(self, key, value, salt='', **kwargs):
         value = signing.get_cookie_signer(salt=key + salt).sign(value)
         return self.set_cookie(key, value, **kwargs)

-    def delete_cookie(self, key, path='/', domain=None):
-        # Most browsers ignore the Set-Cookie header if the cookie name starts
-        # with __Host- or __Secure- and the cookie doesn't use the secure flag.
-        secure = key.startswith(('__Secure-', '__Host-'))
+    def delete_cookie(self, key, path='/', domain=None, samesite=None):
+        # Browsers can ignore the Set-Cookie header if the cookie doesn't use
+        # the secure flag and:
+        # - the cookie name starts with "__Host-" or "__Secure-", or
+        # - the samesite is "none".
+        secure = (
+            key.startswith(('__Secure-', '__Host-')) or
+            (samesite and samesite.lower() == 'none')
+        )
         self.set_cookie(
             key, max_age=0, path=path, domain=domain, secure=secure,
-            expires='Thu, 01 Jan 1970 00:00:00 GMT',
+            expires='Thu, 01 Jan 1970 00:00:00 GMT', samesite=samesite,
         )

     # Common methods used by subclasses
@@ -230,7 +268,7 @@
         # Handle string types -- we can't rely on force_bytes here because:
         # - Python attempts str conversion first
         # - when self._charset != 'utf-8' it re-encodes the content
-        if isinstance(value, bytes):
+        if isinstance(value, (bytes, memoryview)):
             return bytes(value)
         if isinstance(value, str):
             return bytes(value.encode(self.charset))
@@ -243,22 +281,24 @@
     # The WSGI server must call this method upon completion of the request.
     # See http://blog.dscpl.com.au/2012/10/obligations-for-calling-close-on.html
     def close(self):
-        for closable in self._closable_objects:
+        for closer in self._resource_closers:
             try:
-                closable.close()
+                closer()
             except Exception:
                 pass
+        # Free resources that were still referenced.
+        self._resource_closers.clear()
         self.closed = True
         signals.request_finished.send(sender=self._handler_class)

     def write(self, content):
-        raise IOError("This %s instance is not writable" % self.__class__.__name__)
+        raise OSError('This %s instance is not writable' % self.__class__.__name__)

     def flush(self):
         pass

     def tell(self):
-        raise IOError("This %s instance cannot tell its position" % self.__class__.__name__)
+        raise OSError('This %s instance cannot tell its position' % self.__class__.__name__)

     # These methods partially implement a stream-like object interface.
     # See https://docs.python.org/library/io.html#io.IOBase
@@ -273,14 +313,14 @@
         return False

     def writelines(self, lines):
-        raise IOError("This %s instance is not writable" % self.__class__.__name__)
+        raise OSError('This %s instance is not writable' % self.__class__.__name__)


 class HttpResponse(HttpResponseBase):
     """
     An HTTP response class with a string as content.

-    This content that can be read, appended to, or replaced.
+    This content can be read, appended to, or replaced.
     """

     streaming = False
@@ -310,7 +350,10 @@
     @content.setter
     def content(self, value):
         # Consume iterators upon assignment to allow repeated iteration.
-        if hasattr(value, '__iter__') and not isinstance(value, (bytes, str)):
+        if (
+            hasattr(value, '__iter__') and
+            not isinstance(value, (bytes, memoryview, str))
+        ):
             content = b''.join(self.make_bytes(chunk) for chunk in value)
             if hasattr(value, 'close'):
                 try:
@@ -378,7 +421,7 @@
         # Ensure we can never iterate on "value" more than once.
         self._iterator = iter(value)
         if hasattr(value, 'close'):
-            self._closable_objects.append(value)
+            self._resource_closers.append(value.close)

     def __iter__(self):
         return self.streaming_content
@@ -405,7 +448,7 @@

         self.file_to_stream = filelike = value
         if hasattr(filelike, 'close'):
-            self._closable_objects.append(filelike)
+            self._resource_closers.append(filelike.close)
         value = iter(lambda: filelike.read(self.block_size), b'')
         self.set_headers(filelike)
         super()._set_streaming_content(value)
@@ -423,29 +466,31 @@
         filename = getattr(filelike, 'name', None)
         filename = filename if (isinstance(filename, str) and filename) else self.filename
         if os.path.isabs(filename):
-            self['Content-Length'] = os.path.getsize(filelike.name)
+            self.headers['Content-Length'] = os.path.getsize(filelike.name)
         elif hasattr(filelike, 'getbuffer'):
-            self['Content-Length'] = filelike.getbuffer().nbytes
-
-        if self.get('Content-Type', '').startswith(settings.DEFAULT_CONTENT_TYPE):
+            self.headers['Content-Length'] = filelike.getbuffer().nbytes
+
+        if self.headers.get('Content-Type', '').startswith('text/html'):
             if filename:
                 content_type, encoding = mimetypes.guess_type(filename)
                 # Encoding isn't set to prevent browsers from automatically
                 # uncompressing files.
                 content_type = encoding_map.get(encoding, content_type)
-                self['Content-Type'] = content_type or 'application/octet-stream'
+                self.headers['Content-Type'] = content_type or 'application/octet-stream'
             else:
-                self['Content-Type'] = 'application/octet-stream'
-
-        if self.as_attachment:
-            filename = self.filename or os.path.basename(filename)
-            if filename:
-                try:
-                    filename.encode('ascii')
-                    file_expr = 'filename="{}"'.format(filename)
-                except UnicodeEncodeError:
-                    file_expr = "filename*=utf-8''{}".format(quote(filename))
-                self['Content-Disposition'] = 'attachment; {}'.format(file_expr)
+                self.headers['Content-Type'] = 'application/octet-stream'
+
+        filename = self.filename or os.path.basename(filename)
+        if filename:
+            disposition = 'attachment' if self.as_attachment else 'inline'
+            try:
+                filename.encode('ascii')
+                file_expr = 'filename="{}"'.format(filename)
+            except UnicodeEncodeError:
+                file_expr = "filename*=utf-8''{}".format(quote(filename))
+            self.headers['Content-Disposition'] = '{}; {}'.format(disposition, file_expr)
+        elif self.as_attachment:
+            self.headers['Content-Disposition'] = 'attachment'


 class HttpResponseRedirectBase(HttpResponse):
('django/urls', 'resolvers.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -8,18 +8,20 @@
 import functools
 import inspect
 import re
-import threading
+import string
 from importlib import import_module
 from urllib.parse import quote
+
+from asgiref.local import Local

 from django.conf import settings
 from django.core.checks import Error, Warning
 from django.core.checks.urls import check_resolver
-from django.core.exceptions import ImproperlyConfigured
+from django.core.exceptions import ImproperlyConfigured, ViewDoesNotExist
 from django.utils.datastructures import MultiValueDict
 from django.utils.functional import cached_property
 from django.utils.http import RFC3986_SUBDELIMS, escape_leading_slashes
-from django.utils.regex_helper import normalize
+from django.utils.regex_helper import _lazy_re_compile, normalize
 from django.utils.translation import get_language

 from .converters import get_converter
@@ -28,12 +30,13 @@


 class ResolverMatch:
-    def __init__(self, func, args, kwargs, url_name=None, app_names=None, namespaces=None, route=None):
+    def __init__(self, func, args, kwargs, url_name=None, app_names=None, namespaces=None, route=None, tried=None):
         self.func = func
         self.args = args
         self.kwargs = kwargs
         self.url_name = url_name
         self.route = route
+        self.tried = tried

         # If a URLRegexResolver doesn't have a namespace or app_name, it passes
         # in an empty value.
@@ -62,10 +65,14 @@
         )


-@functools.lru_cache(maxsize=None)
 def get_resolver(urlconf=None):
     if urlconf is None:
         urlconf = settings.ROOT_URLCONF
+    return _get_cached_resolver(urlconf)
+
+
+@functools.lru_cache(maxsize=None)
+def _get_cached_resolver(urlconf=None):
     return URLResolver(RegexPattern(r'^/'), urlconf)


@@ -147,13 +154,18 @@
         self.converters = {}

     def match(self, path):
-        match = self.regex.search(path)
+        match = (
+            self.regex.fullmatch(path)
+            if self._is_endpoint and self.regex.pattern.endswith('$')
+            else self.regex.search(path)
+        )
         if match:
             # If there are any named groups, use those as kwargs, ignoring
             # non-named groups. Otherwise, pass all non-named arguments as
             # positional arguments.
             kwargs = match.groupdict()
             args = () if kwargs else match.groups()
+            kwargs = {k: v for k, v in kwargs.items() if v is not None}
             return path[match.end():], args, kwargs
         return None

@@ -183,14 +195,14 @@
         except re.error as e:
             raise ImproperlyConfigured(
                 '"%s" is not a valid regular expression: %s' % (regex, e)
-            )
+            ) from e

     def __str__(self):
         return str(self._regex)


-_PATH_PARAMETER_COMPONENT_RE = re.compile(
-    r'<(?:(?P<converter>[^>:]+):)?(?P<parameter>\w+)>'
+_PATH_PARAMETER_COMPONENT_RE = _lazy_re_compile(
+    r'<(?:(?P<converter>[^>:]+):)?(?P<parameter>[^>]+)>'
 )


@@ -209,15 +221,20 @@
         if not match:
             parts.append(re.escape(route))
             break
+        elif not set(match.group()).isdisjoint(string.whitespace):
+            raise ImproperlyConfigured(
+                "URL route '%s' cannot contain whitespace in angle brackets "
+                "<…>." % original_route
+            )
         parts.append(re.escape(route[:match.start()]))
         route = route[match.end():]
-        parameter = match.group('parameter')
+        parameter = match['parameter']
         if not parameter.isidentifier():
             raise ImproperlyConfigured(
                 "URL route '%s' uses parameter name %r which isn't a valid "
                 "Python identifier." % (original_route, parameter)
             )
-        raw_converter = match.group('converter')
+        raw_converter = match['converter']
         if raw_converter is None:
             # If a converter isn't specified, the default is `str`.
             raw_converter = 'str'
@@ -225,12 +242,13 @@
             converter = get_converter(raw_converter)
         except KeyError as e:
             raise ImproperlyConfigured(
-                "URL route '%s' uses invalid converter %s." % (original_route, e)
-            )
+                'URL route %r uses invalid converter %r.'
+                % (original_route, raw_converter)
+            ) from e
         converters[parameter] = converter
         parts.append('(?P<' + parameter + '>' + converter.regex + ')')
     if is_endpoint:
-        parts.append('$')
+        parts.append(r'\Z')
     return ''.join(parts), converters


@@ -380,7 +398,7 @@
         # urlpatterns
         self._callback_strs = set()
         self._populated = False
-        self._local = threading.local()
+        self._local = Local()

     def __repr__(self):
         if isinstance(self.urlconf_name, list) and self.urlconf_name:
@@ -405,7 +423,15 @@
         # All handlers take (request, exception) arguments except handler500
         # which takes (request).
         for status_code, num_parameters in [(400, 2), (403, 2), (404, 2), (500, 1)]:
-            handler, param_dict = self.resolve_error_handler(status_code)
+            try:
+                handler = self.resolve_error_handler(status_code)
+            except (ImportError, ViewDoesNotExist) as e:
+                path = getattr(self.urlconf_module, 'handler%s' % status_code)
+                msg = (
+                    "The custom handler{status_code} view '{path}' could not be imported."
+                ).format(status_code=status_code, path=path)
+                messages.append(Error(msg, hint=str(e), id='urls.E008'))
+                continue
             signature = inspect.signature(handler)
             args = [None] * num_parameters
             try:
@@ -505,6 +531,13 @@
         return self._app_dict[language_code]

     @staticmethod
+    def _extend_tried(tried, pattern, sub_tried=None):
+        if sub_tried is None:
+            tried.append([pattern])
+        else:
+            tried.extend([pattern, *t] for t in sub_tried)
+
+    @staticmethod
     def _join_route(route1, route2):
         """Join two routes, without the starting ^ in the second route."""
         if not route1:
@@ -528,11 +561,7 @@
                 try:
                     sub_match = pattern.resolve(new_path)
                 except Resolver404 as e:
-                    sub_tried = e.args[0].get('tried')
-                    if sub_tried is not None:
-                        tried.extend([pattern] + t for t in sub_tried)
-                    else:
-                        tried.append([pattern])
+                    self._extend_tried(tried, pattern, e.args[0].get('tried'))
                 else:
                     if sub_match:
                         # Merge captured arguments in match with submatch
@@ -545,6 +574,7 @@
                         if not sub_match_dict:
                             sub_match_args = args + sub_match.args
                         current_route = '' if isinstance(pattern, URLPattern) else str(pattern.pattern)
+                        self._extend_tried(tried, pattern, sub_match.tried)
                         return ResolverMatch(
                             sub_match.func,
                             sub_match_args,
@@ -553,6 +583,7 @@
                             [self.app_name] + sub_match.app_names,
                             [self.namespace] + sub_match.namespaces,
                             self._join_route(current_route, sub_match.route),
+                            tried,
                         )
                     tried.append([pattern])
             raise Resolver404({'tried': tried, 'path': new_path})
@@ -571,13 +602,13 @@
         patterns = getattr(self.urlconf_module, "urlpatterns", self.urlconf_module)
         try:
             iter(patterns)
-        except TypeError:
+        except TypeError as e:
             msg = (
                 "The included URLconf '{name}' does not appear to have any "
                 "patterns in it. If you see valid patterns in the file then "
                 "the issue is probably caused by a circular import."
             )
-            raise ImproperlyConfigured(msg.format(name=self.urlconf_name))
+            raise ImproperlyConfigured(msg.format(name=self.urlconf_name)) from e
         return patterns

     def resolve_error_handler(self, view_type):
@@ -587,7 +618,7 @@
             # django.conf.urls imports this file.
             from django.conf import urls
             callback = getattr(urls, 'handler%s' % view_type)
-        return get_callable(callback), {}
+        return get_callable(callback)

     def reverse(self, lookup_view, *args, **kwargs):
         return self._reverse_with_prefix(lookup_view, '', *args, **kwargs)
@@ -615,11 +646,18 @@
                     candidate_subs = kwargs
                 # Convert the candidate subs to text using Converter.to_url().
                 text_candidate_subs = {}
+                match = True
                 for k, v in candidate_subs.items():
                     if k in converters:
-                        text_candidate_subs[k] = converters[k].to_url(v)
+                        try:
+                            text_candidate_subs[k] = converters[k].to_url(v)
+                        except ValueError:
+                            match = False
+                            break
                     else:
                         text_candidate_subs[k] = str(v)
+                if not match:
+                    continue
                 # WSGI provides decoded URLs, without %xx escapes, and the URL
                 # resolver operates on such URLs. First substitute arguments
                 # without quoting to build a decoded URL and look for a match.
@@ -645,7 +683,7 @@
             if args:
                 arg_msg = "arguments '%s'" % (args,)
             elif kwargs:
-                arg_msg = "keyword arguments '%s'" % (kwargs,)
+                arg_msg = "keyword arguments '%s'" % kwargs
             else:
                 arg_msg = "no arguments"
             msg = (
('django/urls', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,21 +1,21 @@
-from threading import local
-from urllib.parse import urlsplit, urlunsplit
+from urllib.parse import unquote, urlsplit, urlunsplit

-from django.utils.encoding import iri_to_uri
+from asgiref.local import Local
+
 from django.utils.functional import lazy
 from django.utils.translation import override

 from .exceptions import NoReverseMatch, Resolver404
-from .resolvers import get_ns_resolver, get_resolver
+from .resolvers import _get_cached_resolver, get_ns_resolver, get_resolver
 from .utils import get_callable

 # SCRIPT_NAME prefixes for each thread are stored here. If there's no entry for
 # the current thread (which is the only one we ever access), it is assumed to
 # be empty.
-_prefixes = local()
+_prefixes = Local()

 # Overridden URLconfs for each thread are stored here.
-_urlconfs = local()
+_urlconfs = Local()


 def resolve(path, urlconf=None):
@@ -36,10 +36,7 @@
     if not isinstance(viewname, str):
         view = viewname
     else:
-        parts = viewname.split(':')
-        parts.reverse()
-        view = parts[0]
-        path = parts[1:]
+        *path, view = viewname.split(':')

         if current_app:
             current_path = current_app.split(':')
@@ -50,8 +47,7 @@
         resolved_path = []
         ns_pattern = ''
         ns_converters = {}
-        while path:
-            ns = path.pop()
+        for ns in path:
             current_ns = current_path.pop() if current_path else None
             # Lookup the name to see if it could be an app identifier.
             try:
@@ -87,7 +83,7 @@
         if ns_pattern:
             resolver = get_ns_resolver(ns_pattern, resolver, tuple(ns_converters.items()))

-    return iri_to_uri(resolver._reverse_with_prefix(view, prefix, *args, **kwargs))
+    return resolver._reverse_with_prefix(view, prefix, *args, **kwargs)


 reverse_lazy = lazy(reverse, str)
@@ -95,7 +91,7 @@

 def clear_url_caches():
     get_callable.cache_clear()
-    get_resolver.cache_clear()
+    _get_cached_resolver.cache_clear()
     get_ns_resolver.cache_clear()


@@ -149,13 +145,12 @@

 def is_valid_path(path, urlconf=None):
     """
-    Return True if the given path resolves against the default URL resolver,
-    False otherwise. This is a convenience method to make working with "is
-    this a match?" cases easier, avoiding try...except blocks.
+    Return the ResolverMatch if the given path resolves against the default URL
+    resolver, False otherwise. This is a convenience method to make working
+    with "is this a match?" cases easier, avoiding try...except blocks.
     """
     try:
-        resolve(path, urlconf)
-        return True
+        return resolve(path, urlconf)
     except Resolver404:
         return False

@@ -168,7 +163,8 @@
     """
     parsed = urlsplit(url)
     try:
-        match = resolve(parsed.path)
+        # URL may be encoded.
+        match = resolve(unquote(parsed.path))
     except Resolver404:
         pass
     else:
('django/db', 'transaction.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -158,29 +158,35 @@

     Since database connections are thread-local, this is thread-safe.

+    An atomic block can be tagged as durable. In this case, raise a
+    RuntimeError if it's nested within another atomic block. This guarantees
+    that database changes in a durable block are committed to the database when
+    the block exists without error.
+
     This is a private API.
     """
-
-    def __init__(self, using, savepoint):
+    # This private flag is provided only to disable the durability checks in
+    # TestCase.
+    _ensure_durability = True
+
+    def __init__(self, using, savepoint, durable):
         self.using = using
         self.savepoint = savepoint
+        self.durable = durable

     def __enter__(self):
         connection = get_connection(self.using)

+        if self.durable and self._ensure_durability and connection.in_atomic_block:
+            raise RuntimeError(
+                'A durable atomic block cannot be nested within another '
+                'atomic block.'
+            )
         if not connection.in_atomic_block:
             # Reset state when entering an outermost atomic block.
             connection.commit_on_exit = True
             connection.needs_rollback = False
             if not connection.get_autocommit():
-                # sqlite3 in Python < 3.6 doesn't handle transactions and
-                # savepoints properly when autocommit is off.
-                # Turning autocommit back on isn't an option; it would trigger
-                # a premature commit. Give up if that happens.
-                if connection.features.autocommits_when_autocommit_is_off:
-                    raise TransactionManagementError(
-                        "Your database backend doesn't behave properly when "
-                        "autocommit is off. Turn it on before using 'atomic'.")
                 # Pretend we're already in an atomic block to bypass the code
                 # that disables autocommit to enter a transaction, and make a
                 # note to deal with this case in __exit__.
@@ -290,14 +296,14 @@
                     connection.in_atomic_block = False


-def atomic(using=None, savepoint=True):
+def atomic(using=None, savepoint=True, durable=False):
     # Bare decorator: @atomic -- although the first argument is called
     # `using`, it's actually the function being decorated.
     if callable(using):
-        return Atomic(DEFAULT_DB_ALIAS, savepoint)(using)
+        return Atomic(DEFAULT_DB_ALIAS, savepoint, durable)(using)
     # Decorator: @atomic(...) or context manager: with atomic(...): ...
     else:
-        return Atomic(using, savepoint)
+        return Atomic(using, savepoint, durable)


 def _non_atomic_requests(view, using):
('django/db', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -5,6 +5,7 @@
     InterfaceError, InternalError, NotSupportedError, OperationalError,
     ProgrammingError,
 )
+from django.utils.connection import ConnectionProxy

 __all__ = [
     'connection', 'connections', 'router', 'DatabaseError', 'IntegrityError',
@@ -17,28 +18,8 @@

 router = ConnectionRouter()

-
-class DefaultConnectionProxy:
-    """
-    Proxy for accessing the default DatabaseWrapper object's attributes. If you
-    need to access the DatabaseWrapper object itself, use
-    connections[DEFAULT_DB_ALIAS] instead.
-    """
-    def __getattr__(self, item):
-        return getattr(connections[DEFAULT_DB_ALIAS], item)
-
-    def __setattr__(self, name, value):
-        return setattr(connections[DEFAULT_DB_ALIAS], name, value)
-
-    def __delattr__(self, name):
-        return delattr(connections[DEFAULT_DB_ALIAS], name)
-
-    def __eq__(self, other):
-        return connections[DEFAULT_DB_ALIAS] == other
-
-
 # For backwards compatibility. Prefer connections['default'] instead.
-connection = DefaultConnectionProxy()
+connection = ConnectionProxy(connections, DEFAULT_DB_ALIAS)


 # Register an event to reset saved queries when a Django request is started.
('django/db', 'utils.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,10 +1,11 @@
 import pkgutil
 from importlib import import_module
-from pathlib import Path
-from threading import local

 from django.conf import settings
 from django.core.exceptions import ImproperlyConfigured
+# For backwards compatibility with Django < 3.2
+from django.utils.connection import ConnectionDoesNotExist  # NOQA: F401
+from django.utils.connection import BaseConnectionHandler
 from django.utils.functional import cached_property
 from django.utils.module_loading import import_string

@@ -111,16 +112,18 @@
     except ImportError as e_user:
         # The database backend wasn't found. Display a helpful error message
         # listing all built-in database backends.
-        backend_dir = str(Path(__file__).parent / 'backends')
+        import django.db.backends
         builtin_backends = [
-            name for _, name, ispkg in pkgutil.iter_modules([backend_dir])
-            if ispkg and name not in {'base', 'dummy', 'postgresql_psycopg2'}
+            name for _, name, ispkg in pkgutil.iter_modules(django.db.backends.__path__)
+            if ispkg and name not in {'base', 'dummy'}
         ]
         if backend_name not in ['django.db.backends.%s' % b for b in builtin_backends]:
             backend_reprs = map(repr, sorted(builtin_backends))
             raise ImproperlyConfigured(
-                "%r isn't an available database backend.\n"
-                "Try using 'django.db.backends.XXX', where XXX is one of:\n"
+                "%r isn't an available database backend or couldn't be "
+                "imported. Check the above exception. To use one of the "
+                "built-in backends, use 'django.db.backends.XXX', where XXX "
+                "is one of:\n"
                 "    %s" % (backend_name, ", ".join(backend_reprs))
             ) from e_user
         else:
@@ -128,34 +131,30 @@
             raise


-class ConnectionDoesNotExist(Exception):
-    pass
-
-
-class ConnectionHandler:
-    def __init__(self, databases=None):
-        """
-        databases is an optional dictionary of database definitions (structured
-        like settings.DATABASES).
-        """
-        self._databases = databases
-        self._connections = local()
-
-    @cached_property
+class ConnectionHandler(BaseConnectionHandler):
+    settings_name = 'DATABASES'
+    # Connections needs to still be an actual thread local, as it's truly
+    # thread-critical. Database backends should use @async_unsafe to protect
+    # their code from async contexts, but this will give those contexts
+    # separate connections in case it's needed as well. There's no cleanup
+    # after async contexts, though, so we don't allow that if we can help it.
+    thread_critical = True
+
+    def configure_settings(self, databases):
+        databases = super().configure_settings(databases)
+        if databases == {}:
+            databases[DEFAULT_DB_ALIAS] = {'ENGINE': 'django.db.backends.dummy'}
+        elif DEFAULT_DB_ALIAS not in databases:
+            raise ImproperlyConfigured(
+                f"You must define a '{DEFAULT_DB_ALIAS}' database."
+            )
+        elif databases[DEFAULT_DB_ALIAS] == {}:
+            databases[DEFAULT_DB_ALIAS]['ENGINE'] = 'django.db.backends.dummy'
+        return databases
+
+    @property
     def databases(self):
-        if self._databases is None:
-            self._databases = settings.DATABASES
-        if self._databases == {}:
-            self._databases = {
-                DEFAULT_DB_ALIAS: {
-                    'ENGINE': 'django.db.backends.dummy',
-                },
-            }
-        if DEFAULT_DB_ALIAS not in self._databases:
-            raise ImproperlyConfigured("You must define a '%s' database." % DEFAULT_DB_ALIAS)
-        if self._databases[DEFAULT_DB_ALIAS] == {}:
-            self._databases[DEFAULT_DB_ALIAS]['ENGINE'] = 'django.db.backends.dummy'
-        return self._databases
+        return self.settings

     def ensure_defaults(self, alias):
         """
@@ -165,7 +164,7 @@
         try:
             conn = self.databases[alias]
         except KeyError:
-            raise ConnectionDoesNotExist("The connection %s doesn't exist" % alias)
+            raise self.exception_class(f"The connection '{alias}' doesn't exist.")

         conn.setdefault('ATOMIC_REQUESTS', False)
         conn.setdefault('AUTOCOMMIT', True)
@@ -185,35 +184,25 @@
         try:
             conn = self.databases[alias]
         except KeyError:
-            raise ConnectionDoesNotExist("The connection %s doesn't exist" % alias)
+            raise self.exception_class(f"The connection '{alias}' doesn't exist.")

         test_settings = conn.setdefault('TEST', {})
-        for key in ['CHARSET', 'COLLATION', 'NAME', 'MIRROR']:
-            test_settings.setdefault(key, None)
-
-    def __getitem__(self, alias):
-        if hasattr(self._connections, alias):
-            return getattr(self._connections, alias)
-
+        default_test_settings = [
+            ('CHARSET', None),
+            ('COLLATION', None),
+            ('MIGRATE', True),
+            ('MIRROR', None),
+            ('NAME', None),
+        ]
+        for key, value in default_test_settings:
+            test_settings.setdefault(key, value)
+
+    def create_connection(self, alias):
         self.ensure_defaults(alias)
         self.prepare_test_settings(alias)
         db = self.databases[alias]
         backend = load_backend(db['ENGINE'])
-        conn = backend.DatabaseWrapper(db, alias)
-        setattr(self._connections, alias, conn)
-        return conn
-
-    def __setitem__(self, key, value):
-        setattr(self._connections, key, value)
-
-    def __delitem__(self, key):
-        delattr(self._connections, key)
-
-    def __iter__(self):
-        return iter(self.databases)
-
-    def all(self):
-        return [self[alias] for alias in self]
+        return backend.DatabaseWrapper(db, alias)

     def close_all(self):
         for alias in self:
('django/db/migrations', 'questioner.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,7 +4,7 @@
 import sys

 from django.apps import apps
-from django.db.models.fields import NOT_PROVIDED
+from django.db.models import NOT_PROVIDED
 from django.utils import timezone

 from .loader import MigrationLoader
('django/db/migrations', 'recorder.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,7 +1,6 @@
 from django.apps.registry import Apps
-from django.db import models
-from django.db.utils import DatabaseError
-from django.utils.decorators import classproperty
+from django.db import DatabaseError, models
+from django.utils.functional import classproperty
 from django.utils.timezone import now

 from .exceptions import MigrationSchemaMissing
@@ -53,7 +52,9 @@

     def has_table(self):
         """Return True if the django_migrations table exists."""
-        return self.Migration._meta.db_table in self.connection.introspection.table_names(self.connection.cursor())
+        with self.connection.cursor() as cursor:
+            tables = self.connection.introspection.table_names(cursor)
+        return self.Migration._meta.db_table in tables

     def ensure_schema(self):
         """Ensure the table exists and has the correct schema."""
@@ -69,13 +70,16 @@
             raise MigrationSchemaMissing("Unable to create the django_migrations table (%s)" % exc)

     def applied_migrations(self):
-        """Return a set of (app, name) of applied migrations."""
+        """
+        Return a dict mapping (app_name, migration_name) to Migration instances
+        for all applied migrations.
+        """
         if self.has_table():
-            return {tuple(x) for x in self.migration_qs.values_list('app', 'name')}
+            return {(migration.app, migration.name): migration for migration in self.migration_qs}
         else:
             # If the django_migrations table doesn't exist, then no migrations
             # are applied.
-            return set()
+            return {}

     def record_applied(self, app, name):
         """Record that a migration was applied."""
('django/db/migrations', 'autodetector.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -9,9 +9,7 @@
 from django.db.migrations.operations.models import AlterModelOptions
 from django.db.migrations.optimizer import MigrationOptimizer
 from django.db.migrations.questioner import MigrationQuestioner
-from django.db.migrations.utils import (
-    COMPILED_REGEX_TYPE, RegexObject, get_migration_name_timestamp,
-)
+from django.db.migrations.utils import COMPILED_REGEX_TYPE, RegexObject
 from django.utils.topological_sort import stable_topological_sort


@@ -89,11 +87,11 @@
     def only_relation_agnostic_fields(self, fields):
         """
         Return a definition of the fields that ignores field names and
-        what related fields actually relate to. Used for detecting renames (as,
-        of course, the related fields change during renames).
+        what related fields actually relate to. Used for detecting renames (as
+        the related fields change during renames).
         """
         fields_def = []
-        for name, field in sorted(fields):
+        for name, field in sorted(fields.items()):
             deconstruction = self.deep_deconstruct(field)
             if field.remote_field and field.remote_field.model:
                 del deconstruction[2]['to']
@@ -184,12 +182,12 @@
         self.generate_removed_fields()
         self.generate_added_fields()
         self.generate_altered_fields()
+        self.generate_altered_order_with_respect_to()
         self.generate_altered_unique_together()
         self.generate_altered_index_together()
         self.generate_added_indexes()
         self.generate_added_constraints()
         self.generate_altered_db_table()
-        self.generate_altered_order_with_respect_to()

         self._sort_migrations()
         self._build_migration_list(graph)
@@ -208,17 +206,17 @@
         self.kept_unmanaged_keys = self.old_unmanaged_keys & self.new_unmanaged_keys
         self.through_users = {}
         self.old_field_keys = {
-            (app_label, model_name, x)
+            (app_label, model_name, field_name)
             for app_label, model_name in self.kept_model_keys
-            for x, y in self.from_state.models[
+            for field_name in self.from_state.models[
                 app_label,
                 self.renamed_models.get((app_label, model_name), model_name)
             ].fields
         }
         self.new_field_keys = {
-            (app_label, model_name, x)
+            (app_label, model_name, field_name)
             for app_label, model_name in self.kept_model_keys
-            for x, y in self.to_state.models[app_label, model_name].fields
+            for field_name in self.to_state.models[app_label, model_name].fields
         }

     def _generate_through_model_map(self):
@@ -226,7 +224,7 @@
         for app_label, model_name in sorted(self.old_model_keys):
             old_model_name = self.renamed_models.get((app_label, model_name), model_name)
             old_model_state = self.from_state.models[app_label, old_model_name]
-            for field_name, field in old_model_state.fields:
+            for field_name in old_model_state.fields:
                 old_field = self.old_apps.get_model(app_label, old_model_name)._meta.get_field(field_name)
                 if (hasattr(old_field, "remote_field") and getattr(old_field.remote_field, "through", None) and
                         not old_field.remote_field.through._meta.auto_created):
@@ -369,7 +367,7 @@
         # Optimize migrations
         for app_label, migrations in self.migrations.items():
             for migration in migrations:
-                migration.operations = MigrationOptimizer().optimize(migration.operations, app_label=app_label)
+                migration.operations = MigrationOptimizer().optimize(migration.operations, app_label)

     def check_dependency(self, operation, dependency):
         """
@@ -496,10 +494,13 @@
                                 dependencies=dependencies,
                             )
                             self.renamed_models[app_label, model_name] = rem_model_name
-                            renamed_models_rel_key = '%s.%s' % (rem_model_state.app_label, rem_model_state.name)
+                            renamed_models_rel_key = '%s.%s' % (
+                                rem_model_state.app_label,
+                                rem_model_state.name_lower,
+                            )
                             self.renamed_models_rel[renamed_models_rel_key] = '%s.%s' % (
                                 model_state.app_label,
-                                model_state.name,
+                                model_state.name_lower,
                             )
                             self.old_model_keys.remove((rem_app_label, rem_model_name))
                             self.old_model_keys.add((app_label, model_name))
@@ -560,6 +561,16 @@
                 if isinstance(base, str) and "." in base:
                     base_app_label, base_name = base.split(".", 1)
                     dependencies.append((base_app_label, base_name, None, True))
+                    # Depend on the removal of base fields if the new model has
+                    # a field with the same name.
+                    old_base_model_state = self.from_state.models.get((base_app_label, base_name))
+                    new_base_model_state = self.to_state.models.get((base_app_label, base_name))
+                    if old_base_model_state and new_base_model_state:
+                        removed_base_fields = set(old_base_model_state.fields).difference(
+                            new_base_model_state.fields,
+                        ).intersection(model_state.fields)
+                        for removed_base_field in removed_base_fields:
+                            dependencies.append((base_app_label, base_name, removed_base_field, False))
             # Depend on the other end of the primary key if it's a relation
             if primary_key_rel:
                 dependencies.append((
@@ -573,7 +584,7 @@
                 app_label,
                 operations.CreateModel(
                     name=model_state.name,
-                    fields=[d for d in model_state.fields if d[0] not in related_fields],
+                    fields=[d for d in model_state.fields.items() if d[0] not in related_fields],
                     options=model_state.options,
                     bases=model_state.bases,
                     managers=model_state.managers,
@@ -602,47 +613,6 @@
                     dependencies=list(set(dependencies)),
                 )
             # Generate other opns
-            related_dependencies = [
-                (app_label, model_name, name, True)
-                for name in sorted(related_fields)
-            ]
-            related_dependencies.append((app_label, model_name, None, True))
-            for index in indexes:
-                self.add_operation(
-                    app_label,
-                    operations.AddIndex(
-                        model_name=model_name,
-                        index=index,
-                    ),
-                    dependencies=related_dependencies,
-                )
-            for constraint in constraints:
-                self.add_operation(
-                    app_label,
-                    operations.AddConstraint(
-                        model_name=model_name,
-                        constraint=constraint,
-                    ),
-                    dependencies=related_dependencies,
-                )
-            if unique_together:
-                self.add_operation(
-                    app_label,
-                    operations.AlterUniqueTogether(
-                        name=model_name,
-                        unique_together=unique_together,
-                    ),
-                    dependencies=related_dependencies
-                )
-            if index_together:
-                self.add_operation(
-                    app_label,
-                    operations.AlterIndexTogether(
-                        name=model_name,
-                        index_together=index_together,
-                    ),
-                    dependencies=related_dependencies
-                )
             if order_with_respect_to:
                 self.add_operation(
                     app_label,
@@ -655,7 +625,47 @@
                         (app_label, model_name, None, True),
                     ]
                 )
-
+            related_dependencies = [
+                (app_label, model_name, name, True)
+                for name in sorted(related_fields)
+            ]
+            related_dependencies.append((app_label, model_name, None, True))
+            for index in indexes:
+                self.add_operation(
+                    app_label,
+                    operations.AddIndex(
+                        model_name=model_name,
+                        index=index,
+                    ),
+                    dependencies=related_dependencies,
+                )
+            for constraint in constraints:
+                self.add_operation(
+                    app_label,
+                    operations.AddConstraint(
+                        model_name=model_name,
+                        constraint=constraint,
+                    ),
+                    dependencies=related_dependencies,
+                )
+            if unique_together:
+                self.add_operation(
+                    app_label,
+                    operations.AlterUniqueTogether(
+                        name=model_name,
+                        unique_together=unique_together,
+                    ),
+                    dependencies=related_dependencies
+                )
+            if index_together:
+                self.add_operation(
+                    app_label,
+                    operations.AlterIndexTogether(
+                        name=model_name,
+                        index_together=index_together,
+                    ),
+                    dependencies=related_dependencies
+                )
             # Fix relationships if the model changed from a proxy model to a
             # concrete model.
             if (app_label, model_name) in self.old_proxy_keys:
@@ -673,9 +683,8 @@
     def generate_created_proxies(self):
         """
         Make CreateModel statements for proxy models. Use the same statements
-        as that way there's less code duplication, but of course for proxy
-        models it's safe to skip all the pointless field stuff and just chuck
-        out an operation.
+        as that way there's less code duplication, but for proxy models it's
+        safe to skip all the pointless field stuff and chuck out an operation.
         """
         added = self.new_proxy_keys - self.old_proxy_keys
         for app_label, model_name in sorted(added):
@@ -817,7 +826,7 @@
             field_dec = self.deep_deconstruct(field)
             for rem_app_label, rem_model_name, rem_field_name in sorted(self.old_field_keys - self.new_field_keys):
                 if rem_app_label == app_label and rem_model_name == model_name:
-                    old_field = old_model_state.get_field_by_name(rem_field_name)
+                    old_field = old_model_state.fields[rem_field_name]
                     old_field_dec = self.deep_deconstruct(old_field)
                     if field.remote_field and field.remote_field.model and 'to' in old_field_dec[2]:
                         old_rel_to = old_field_dec[2]['to']
@@ -912,6 +921,7 @@
             old_field_name = self.renamed_fields.get((app_label, model_name, field_name), field_name)
             old_field = self.old_apps.get_model(app_label, old_model_name)._meta.get_field(old_field_name)
             new_field = self.new_apps.get_model(app_label, model_name)._meta.get_field(field_name)
+            dependencies = []
             # Implement any model renames on relations; these are handled by RenameModel
             # so we need to exclude them from the comparison
             if hasattr(new_field, "remote_field") and getattr(new_field.remote_field, "model", None):
@@ -926,6 +936,10 @@
                 if remote_field_name:
                     to_field_rename_key = rename_key + (remote_field_name,)
                     if to_field_rename_key in self.renamed_fields:
+                        # Repoint both model and field name because to_field
+                        # inclusion in ForeignKey.deconstruct() is based on
+                        # both.
+                        new_field.remote_field.model = old_field.remote_field.model
                         new_field.remote_field.field_name = old_field.remote_field.field_name
                 # Handle ForeignObjects which can have multiple from_fields/to_fields.
                 from_fields = getattr(new_field, 'from_fields', None)
@@ -939,6 +953,7 @@
                         self.renamed_fields.get(rename_key + (to_field,), to_field)
                         for to_field in new_field.to_fields
                     ])
+                dependencies.extend(self._get_dependencies_for_foreign_key(new_field))
             if hasattr(new_field, "remote_field") and getattr(new_field.remote_field, "through", None):
                 rename_key = (
                     new_field.remote_field.through._meta.app_label,
@@ -970,7 +985,8 @@
                             name=field_name,
                             field=field,
                             preserve_default=preserve_default,
-                        )
+                        ),
+                        dependencies=dependencies,
                     )
                 else:
                     # We cannot alter between m2m and concrete fields
@@ -1246,13 +1262,14 @@
             for i, migration in enumerate(migrations):
                 if i == 0 and app_leaf:
                     migration.dependencies.append(app_leaf)
-                if i == 0 and not app_leaf:
-                    new_name = "0001_%s" % migration_name if migration_name else "0001_initial"
+                new_name_parts = ['%04i' % next_number]
+                if migration_name:
+                    new_name_parts.append(migration_name)
+                elif i == 0 and not app_leaf:
+                    new_name_parts.append('initial')
                 else:
-                    new_name = "%04i_%s" % (
-                        next_number,
-                        migration_name or self.suggest_name(migration.operations)[:100],
-                    )
+                    new_name_parts.append(migration.suggest_name()[:100])
+                new_name = '_'.join(new_name_parts)
                 name_map[(app_label, migration.name)] = (app_label, new_name)
                 next_number += 1
                 migration.name = new_name
@@ -1288,27 +1305,6 @@
         return changes

     @classmethod
-    def suggest_name(cls, ops):
-        """
-        Given a set of operations, suggest a name for the migration they might
-        represent. Names are not guaranteed to be unique, but put some effort
-        into the fallback name to avoid VCS conflicts if possible.
-        """
-        if len(ops) == 1:
-            if isinstance(ops[0], operations.CreateModel):
-                return ops[0].name_lower
-            elif isinstance(ops[0], operations.DeleteModel):
-                return "delete_%s" % ops[0].name_lower
-            elif isinstance(ops[0], operations.AddField):
-                return "%s_%s" % (ops[0].model_name_lower, ops[0].name_lower)
-            elif isinstance(ops[0], operations.RemoveField):
-                return "remove_%s_%s" % (ops[0].model_name_lower, ops[0].name_lower)
-        elif ops:
-            if all(isinstance(o, operations.CreateModel) for o in ops):
-                return "_".join(sorted(o.name_lower for o in ops))
-        return "auto_%s" % get_migration_name_timestamp()
-
-    @classmethod
     def parse_number(cls, name):
         """
         Given a migration name, try to extract a number from the beginning of
@@ -1316,5 +1312,5 @@
         """
         match = re.match(r'^\d+', name)
         if match:
-            return int(match.group())
+            return int(match[0])
         return None
('django/db/migrations', 'loader.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -40,11 +40,15 @@
     in memory.
     """

-    def __init__(self, connection, load=True, ignore_no_migrations=False):
+    def __init__(
+        self, connection, load=True, ignore_no_migrations=False,
+        replace_migrations=True,
+    ):
         self.connection = connection
         self.disk_migrations = None
         self.applied_migrations = None
         self.ignore_no_migrations = ignore_no_migrations
+        self.replace_migrations = replace_migrations
         if load:
             self.build_graph()

@@ -75,22 +79,26 @@
             was_loaded = module_name in sys.modules
             try:
                 module = import_module(module_name)
-            except ImportError as e:
-                # I hate doing this, but I don't want to squash other import errors.
-                # Might be better to try a directory check directly.
-                if ((explicit and self.ignore_no_migrations) or (
-                        not explicit and "No module named" in str(e) and MIGRATIONS_MODULE_NAME in str(e))):
+            except ModuleNotFoundError as e:
+                if (
+                    (explicit and self.ignore_no_migrations) or
+                    (not explicit and MIGRATIONS_MODULE_NAME in e.name.split('.'))
+                ):
                     self.unmigrated_apps.add(app_config.label)
                     continue
                 raise
             else:
-                # Empty directories are namespaces.
-                # getattr() needed on PY36 and older (replace w/attribute access).
-                if getattr(module, '__file__', None) is None:
+                # Module is not a package (e.g. migrations.py).
+                if not hasattr(module, '__path__'):
                     self.unmigrated_apps.add(app_config.label)
                     continue
-                # Module is not a package (e.g. migrations.py).
-                if not hasattr(module, '__path__'):
+                # Empty directories are namespaces. Namespace packages have no
+                # __file__ and don't use a list for __path__. See
+                # https://docs.python.org/3/reference/import.html#namespace-packages
+                if (
+                    getattr(module, '__file__', None) is None and
+                    not isinstance(module.__path__, list)
+                ):
                     self.unmigrated_apps.add(app_config.label)
                     continue
                 # Force a reload if it's already loaded (tests need this)
@@ -206,7 +214,7 @@
         self.load_disk()
         # Load database data
         if self.connection is None:
-            self.applied_migrations = set()
+            self.applied_migrations = {}
         else:
             recorder = MigrationRecorder(self.connection)
             self.applied_migrations = recorder.applied_migrations()
@@ -225,24 +233,27 @@
         # Add external dependencies now that the internal ones have been resolved.
         for key, migration in self.disk_migrations.items():
             self.add_external_dependencies(key, migration)
-        # Carry out replacements where possible.
-        for key, migration in self.replacements.items():
-            # Get applied status of each of this migration's replacement targets.
-            applied_statuses = [(target in self.applied_migrations) for target in migration.replaces]
-            # Ensure the replacing migration is only marked as applied if all of
-            # its replacement targets are.
-            if all(applied_statuses):
-                self.applied_migrations.add(key)
-            else:
-                self.applied_migrations.discard(key)
-            # A replacing migration can be used if either all or none of its
-            # replacement targets have been applied.
-            if all(applied_statuses) or (not any(applied_statuses)):
-                self.graph.remove_replaced_nodes(key, migration.replaces)
-            else:
-                # This replacing migration cannot be used because it is partially applied.
-                # Remove it from the graph and remap dependencies to it (#25945).
-                self.graph.remove_replacement_node(key, migration.replaces)
+        # Carry out replacements where possible and if enabled.
+        if self.replace_migrations:
+            for key, migration in self.replacements.items():
+                # Get applied status of each of this migration's replacement
+                # targets.
+                applied_statuses = [(target in self.applied_migrations) for target in migration.replaces]
+                # The replacing migration is only marked as applied if all of
+                # its replacement targets are.
+                if all(applied_statuses):
+                    self.applied_migrations[key] = migration
+                else:
+                    self.applied_migrations.pop(key, None)
+                # A replacing migration can be used if either all or none of
+                # its replacement targets have been applied.
+                if all(applied_statuses) or (not any(applied_statuses)):
+                    self.graph.remove_replaced_nodes(key, migration.replaces)
+                else:
+                    # This replacing migration cannot be used because it is
+                    # partially applied. Remove it from the graph and remap
+                    # dependencies to it (#25945).
+                    self.graph.remove_replacement_node(key, migration.replaces)
         # Ensure the graph is consistent.
         try:
             self.graph.validate_consistency()
@@ -271,7 +282,7 @@
                         ),
                         exc.node
                     ) from exc
-            raise exc
+            raise
         self.graph.ensure_not_cyclic()

     def check_consistent_history(self, connection):
@@ -312,7 +323,7 @@
             if app_label in seen_apps:
                 conflicting_apps.add(app_label)
             seen_apps.setdefault(app_label, set()).add(migration_name)
-        return {app_label: seen_apps[app_label] for app_label in conflicting_apps}
+        return {app_label: sorted(seen_apps[app_label]) for app_label in conflicting_apps}

     def project_state(self, nodes=None, at_end=True):
         """
@@ -322,3 +333,21 @@
         See graph.make_state() for the meaning of "nodes" and "at_end".
         """
         return self.graph.make_state(nodes=nodes, at_end=at_end, real_apps=list(self.unmigrated_apps))
+
+    def collect_sql(self, plan):
+        """
+        Take a migration plan and return a list of collected SQL statements
+        that represent the best-efforts version of that plan.
+        """
+        statements = []
+        state = None
+        for migration, backwards in plan:
+            with self.connection.schema_editor(collect_sql=True, atomic=migration.atomic) as schema_editor:
+                if state is None:
+                    state = self.project_state((migration.app_label, migration.name), at_end=False)
+                if not backwards:
+                    state = migration.apply(state, schema_editor, collect_sql=True)
+                else:
+                    state = migration.unapply(state, schema_editor, collect_sql=True)
+            statements.extend(schema_editor.collected_sql)
+        return statements
('django/db/migrations', 'optimizer.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -9,7 +9,7 @@
     nothing.
     """

-    def optimize(self, operations, app_label=None):
+    def optimize(self, operations, app_label):
         """
         Main optimization entry point. Pass in a list of Operation instances,
         get out a new list of Operation instances.
@@ -25,11 +25,10 @@
         The inner loop is run until the starting list is the same as the result
         list, and then the result is returned. This means that operation
         optimization must be stable and always return an equal or shorter list.
-
-        The app_label argument is optional, but if you pass it you'll get more
-        efficient optimization.
         """
         # Internal tracking variable for test assertions about # of loops
+        if app_label is None:
+            raise TypeError('app_label must be a str.')
         self._iterations = 0
         while True:
             result = self.optimize_inner(operations, app_label)
@@ -38,16 +37,16 @@
                 return result
             operations = result

-    def optimize_inner(self, operations, app_label=None):
+    def optimize_inner(self, operations, app_label):
         """Inner optimization loop."""
         new_operations = []
         for i, operation in enumerate(operations):
             right = True  # Should we reduce on the right or on the left.
             # Compare it to each operation after it
             for j, other in enumerate(operations[i + 1:]):
-                in_between = operations[i + 1:i + j + 1]
                 result = operation.reduce(other, app_label)
                 if isinstance(result, list):
+                    in_between = operations[i + 1:i + j + 1]
                     if right:
                         new_operations.extend(in_between)
                         new_operations.extend(result)
('django/db/migrations', 'serializer.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -5,10 +5,11 @@
 import enum
 import functools
 import math
+import os
+import pathlib
 import re
 import types
 import uuid
-from collections import OrderedDict

 from django.conf import SettingsReference
 from django.db import models
@@ -45,6 +46,11 @@
 class BaseSimpleSerializer(BaseSerializer):
     def serialize(self):
         return repr(self.value), set()
+
+
+class ChoicesSerializer(BaseSerializer):
+    def serialize(self):
+        return serializer_factory(self.value.value).serialize()


 class DateTimeSerializer(BaseSerializer):
@@ -116,9 +122,10 @@
     def serialize(self):
         enum_class = self.value.__class__
         module = enum_class.__module__
-        v_string, v_imports = serializer_factory(self.value.value).serialize()
-        imports = {'import %s' % module, *v_imports}
-        return "%s.%s(%s)" % (module, enum_class.__name__, v_string), imports
+        return (
+            '%s.%s[%r]' % (module, enum_class.__qualname__, self.value.name),
+            {'import %s' % module},
+        )


 class FloatSerializer(BaseSimpleSerializer):
@@ -212,6 +219,19 @@
         return string.rstrip(','), imports


+class PathLikeSerializer(BaseSerializer):
+    def serialize(self):
+        return repr(os.fspath(self.value)), {}
+
+
+class PathSerializer(BaseSerializer):
+    def serialize(self):
+        # Convert concrete paths to pure paths to avoid issues with migrations
+        # generated on one platform being used on a different platform.
+        prefix = 'Pure' if isinstance(self.value, pathlib.Path) else ''
+        return 'pathlib.%s%r' % (prefix, self.value), {'import pathlib'}
+
+
 class RegexSerializer(BaseSerializer):
     def serialize(self):
         regex_pattern, pattern_imports = serializer_factory(self.value.pattern).serialize()
@@ -264,7 +284,7 @@
             if module == builtins.__name__:
                 return self.value.__name__, set()
             else:
-                return "%s.%s" % (module, self.value.__name__), {"import %s" % module}
+                return "%s.%s" % (module, self.value.__qualname__), {"import %s" % module}


 class UUIDSerializer(BaseSerializer):
@@ -273,25 +293,29 @@


 class Serializer:
-    _registry = OrderedDict([
-        (frozenset, FrozensetSerializer),
-        (list, SequenceSerializer),
-        (set, SetSerializer),
-        (tuple, TupleSerializer),
-        (dict, DictionarySerializer),
-        (enum.Enum, EnumSerializer),
-        (datetime.datetime, DatetimeDatetimeSerializer),
-        ((datetime.date, datetime.timedelta, datetime.time), DateTimeSerializer),
-        (SettingsReference, SettingsReferenceSerializer),
-        (float, FloatSerializer),
-        ((bool, int, type(None), bytes, str), BaseSimpleSerializer),
-        (decimal.Decimal, DecimalSerializer),
-        ((functools.partial, functools.partialmethod), FunctoolsPartialSerializer),
-        ((types.FunctionType, types.BuiltinFunctionType, types.MethodType), FunctionTypeSerializer),
-        (collections.abc.Iterable, IterableSerializer),
-        ((COMPILED_REGEX_TYPE, RegexObject), RegexSerializer),
-        (uuid.UUID, UUIDSerializer),
-    ])
+    _registry = {
+        # Some of these are order-dependent.
+        frozenset: FrozensetSerializer,
+        list: SequenceSerializer,
+        set: SetSerializer,
+        tuple: TupleSerializer,
+        dict: DictionarySerializer,
+        models.Choices: ChoicesSerializer,
+        enum.Enum: EnumSerializer,
+        datetime.datetime: DatetimeDatetimeSerializer,
+        (datetime.date, datetime.timedelta, datetime.time): DateTimeSerializer,
+        SettingsReference: SettingsReferenceSerializer,
+        float: FloatSerializer,
+        (bool, int, type(None), bytes, str, range): BaseSimpleSerializer,
+        decimal.Decimal: DecimalSerializer,
+        (functools.partial, functools.partialmethod): FunctoolsPartialSerializer,
+        (types.FunctionType, types.BuiltinFunctionType, types.MethodType): FunctionTypeSerializer,
+        collections.abc.Iterable: IterableSerializer,
+        (COMPILED_REGEX_TYPE, RegexObject): RegexSerializer,
+        uuid.UUID: UUIDSerializer,
+        pathlib.PurePath: PathSerializer,
+        os.PathLike: PathLikeSerializer,
+    }

     @classmethod
     def register(cls, type_, serializer):
('django/db/migrations', 'exceptions.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,4 +1,4 @@
-from django.db.utils import DatabaseError
+from django.db import DatabaseError


 class AmbiguityError(Exception):
('django/db/migrations', 'writer.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,4 @@
+
 import os
 import re
 from importlib import import_module
@@ -249,8 +250,7 @@
                 migrations_package_name)

         final_dir = os.path.join(base_dir, *missing_dirs)
-        if not os.path.isdir(final_dir):
-            os.makedirs(final_dir)
+        os.makedirs(final_dir, exist_ok=True)
         for missing_dir in missing_dirs:
             base_dir = os.path.join(base_dir, missing_dir)
             with open(os.path.join(base_dir, "__init__.py"), "w"):
('django/db/migrations', 'migration.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,5 @@
+from django.db.migrations import operations
+from django.db.migrations.utils import get_migration_name_timestamp
 from django.db.transaction import atomic

 from .exceptions import IrreversibleError
@@ -175,6 +177,24 @@
                 operation.database_backwards(self.app_label, schema_editor, from_state, to_state)
         return project_state

+    def suggest_name(self):
+        """
+        Suggest a name for the operations this migration might represent. Names
+        are not guaranteed to be unique, but put some effort into the fallback
+        name to avoid VCS conflicts if possible.
+        """
+        name = None
+        if len(self.operations) == 1:
+            name = self.operations[0].migration_name_fragment
+        elif (
+            len(self.operations) > 1 and
+            all(isinstance(o, operations.CreateModel) for o in self.operations)
+        ):
+            name = '_'.join(sorted(o.migration_name_fragment for o in self.operations))
+        if name is None:
+            name = 'initial' if self.initial else 'auto_%s' % get_migration_name_timestamp()
+        return name
+

 class SwappableTuple(tuple):
     """
('django/db/migrations', 'executor.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -25,9 +25,9 @@
         """
         plan = []
         if clean_start:
-            applied = set()
-        else:
-            applied = set(self.loader.applied_migrations)
+            applied = {}
+        else:
+            applied = dict(self.loader.applied_migrations)
         for target in targets:
             # If the target is (app_label, None), that means unmigrate everything
             if target[1] is None:
@@ -36,7 +36,7 @@
                         for migration in self.loader.graph.backwards_plan(root):
                             if migration in applied:
                                 plan.append((self.loader.graph.nodes[migration], True))
-                                applied.remove(migration)
+                                applied.pop(migration)
             # If the migration is already applied, do backwards mode,
             # otherwise do forwards mode.
             elif target in applied:
@@ -53,12 +53,12 @@
                     for migration in self.loader.graph.backwards_plan(node):
                         if migration in applied:
                             plan.append((self.loader.graph.nodes[migration], True))
-                            applied.remove(migration)
+                            applied.pop(migration)
             else:
                 for migration in self.loader.graph.forwards_plan(target):
                     if migration not in applied:
                         plan.append((self.loader.graph.nodes[migration], False))
-                        applied.add(migration)
+                        applied[migration] = self.loader.graph.nodes[migration]
         return plan

     def _create_project_state(self, with_applied_migrations=False):
@@ -210,24 +210,6 @@

         return state

-    def collect_sql(self, plan):
-        """
-        Take a migration plan and return a list of collected SQL statements
-        that represent the best-efforts version of that plan.
-        """
-        statements = []
-        state = None
-        for migration, backwards in plan:
-            with self.connection.schema_editor(collect_sql=True, atomic=migration.atomic) as schema_editor:
-                if state is None:
-                    state = self.loader.project_state((migration.app_label, migration.name), at_end=False)
-                if not backwards:
-                    state = migration.apply(state, schema_editor, collect_sql=True)
-                else:
-                    state = migration.unapply(state, schema_editor, collect_sql=True)
-            statements.extend(schema_editor.collected_sql)
-        return statements
-
     def apply_migration(self, state, migration, fake=False, fake_initial=False):
         """Run a migration forwards."""
         migration_recorded = False
@@ -243,8 +225,9 @@
                 # Alright, do it normally
                 with self.connection.schema_editor(atomic=migration.atomic) as schema_editor:
                     state = migration.apply(state, schema_editor)
-                    self.record_migration(migration)
-                    migration_recorded = True
+                    if not schema_editor.deferred_sql:
+                        self.record_migration(migration)
+                        migration_recorded = True
         if not migration_recorded:
             self.record_migration(migration)
         # Report progress
@@ -329,8 +312,11 @@
         apps = after_state.apps
         found_create_model_migration = False
         found_add_field_migration = False
+        fold_identifier_case = self.connection.features.ignores_table_name_case
         with self.connection.cursor() as cursor:
-            existing_table_names = self.connection.introspection.table_names(cursor)
+            existing_table_names = set(self.connection.introspection.table_names(cursor))
+            if fold_identifier_case:
+                existing_table_names = {name.casefold() for name in existing_table_names}
         # Make sure all create model and add field operations are done
         for operation in migration.operations:
             if isinstance(operation, migrations.CreateModel):
@@ -341,7 +327,10 @@
                     model = global_apps.get_model(model._meta.swapped)
                 if should_skip_detecting_model(migration, model):
                     continue
-                if model._meta.db_table not in existing_table_names:
+                db_table = model._meta.db_table
+                if fold_identifier_case:
+                    db_table = db_table.casefold()
+                if db_table not in existing_table_names:
                     return False, project_state
                 found_create_model_migration = True
             elif isinstance(operation, migrations.AddField):
@@ -358,19 +347,27 @@

                 # Handle implicit many-to-many tables created by AddField.
                 if field.many_to_many:
-                    if field.remote_field.through._meta.db_table not in existing_table_names:
+                    through_db_table = field.remote_field.through._meta.db_table
+                    if fold_identifier_case:
+                        through_db_table = through_db_table.casefold()
+                    if through_db_table not in existing_table_names:
                         return False, project_state
                     else:
                         found_add_field_migration = True
                         continue
-
-                column_names = [
-                    column.name for column in
-                    self.connection.introspection.get_table_description(self.connection.cursor(), table)
-                ]
-                if field.column not in column_names:
+                with self.connection.cursor() as cursor:
+                    columns = self.connection.introspection.get_table_description(cursor, table)
+                for column in columns:
+                    field_column = field.column
+                    column_name = column.name
+                    if fold_identifier_case:
+                        column_name = column_name.casefold()
+                        field_column = field_column.casefold()
+                    if column_name == field_column:
+                        found_add_field_migration = True
+                        break
+                else:
                     return False, project_state
-                found_add_field_migration = True
         # If we get this far and we found at least one CreateModel or AddField migration,
         # the migration is considered implicitly applied.
         return (found_create_model_migration or found_add_field_migration), after_state
('django/db/migrations', 'state.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,12 +1,10 @@
 import copy
-from collections import OrderedDict
 from contextlib import contextmanager

 from django.apps import AppConfig
 from django.apps.registry import Apps, apps as global_apps
 from django.conf import settings
 from django.db import models
-from django.db.models.fields.proxy import OrderWrt
 from django.db.models.fields.related import RECURSIVE_RELATIONSHIP_CONSTANT
 from django.db.models.options import DEFAULT_NAMES, normalize_together
 from django.db.models.utils import make_model_tuple
@@ -127,7 +125,7 @@
         # Directly related models are the models pointed to by ForeignKeys,
         # OneToOneFields, and ManyToManyFields.
         direct_related_models = set()
-        for name, field in model_state.fields:
+        for field in model_state.fields.values():
             if field.is_relation:
                 if field.remote_field.model == RECURSIVE_RELATIONSHIP_CONSTANT:
                     continue
@@ -229,15 +227,14 @@

 class AppConfigStub(AppConfig):
     """Stub of an AppConfig. Only provides a label and a dict of models."""
-    # Not used, but required by AppConfig.__init__
-    path = ''
-
     def __init__(self, label):
-        self.label = label
+        self.apps = None
+        self.models = {}
         # App-label and app-name are not the same thing, so technically passing
         # in the label here is wrong. In practice, migrations don't care about
         # the app name, but we need something unique, and the label works fine.
-        super().__init__(label, None)
+        self.label = label
+        self.name = label

     def import_models(self):
         self.models = self.apps.all_models[self.label]
@@ -334,7 +331,6 @@
         if app_label not in self.app_configs:
             self.app_configs[app_label] = AppConfigStub(app_label)
             self.app_configs[app_label].apps = self
-            self.app_configs[app_label].models = OrderedDict()
         self.app_configs[app_label].models[model._meta.model_name] = model
         self.do_pending_operations(model)
         self.clear_cache()
@@ -361,16 +357,13 @@
     def __init__(self, app_label, name, fields, options=None, bases=None, managers=None):
         self.app_label = app_label
         self.name = name
-        self.fields = fields
+        self.fields = dict(fields)
         self.options = options or {}
         self.options.setdefault('indexes', [])
         self.options.setdefault('constraints', [])
         self.bases = bases or (models.Model,)
         self.managers = managers or []
-        # Sanity-check that fields is NOT a dict. It must be ordered.
-        if isinstance(self.fields, dict):
-            raise ValueError("ModelState.fields cannot be a dict - it must be a list of 2-tuples.")
-        for name, field in fields:
+        for name, field in self.fields.items():
             # Sanity-check that fields are NOT already bound to a model.
             if hasattr(field, 'model'):
                 raise ValueError(
@@ -407,7 +400,7 @@
         for field in model._meta.local_fields:
             if getattr(field, "remote_field", None) and exclude_rels:
                 continue
-            if isinstance(field, OrderWrt):
+            if isinstance(field, models.OrderWrt):
                 continue
             name = field.name
             try:
@@ -546,7 +539,7 @@
         return self.__class__(
             app_label=self.app_label,
             name=self.name,
-            fields=list(self.fields),
+            fields=dict(self.fields),
             # Since options are shallow-copied here, operations such as
             # AddIndex must replace their option (e.g 'indexes') rather
             # than mutating it.
@@ -568,8 +561,8 @@
             )
         except LookupError:
             raise InvalidBasesError("Cannot resolve one or more bases from %r" % (self.bases,))
-        # Turn fields into a dict for the body, add other bits
-        body = {name: field.clone() for name, field in self.fields}
+        # Clone fields for the body, add other bits.
+        body = {name: field.clone() for name, field in self.fields.items()}
         body['Meta'] = meta
         body['__module__'] = "__fake__"

@@ -577,12 +570,6 @@
         body.update(self.construct_managers())
         # Then, make a Model object (apps.register_model is called in __new__)
         return type(self.name, bases, body)
-
-    def get_field_by_name(self, name):
-        for fname, field in self.fields:
-            if fname == name:
-                return field
-        raise ValueError("No field called %s on model %s" % (name, self.name))

     def get_index_by_name(self, name):
         for index in self.options['indexes']:
@@ -604,8 +591,13 @@
             (self.app_label == other.app_label) and
             (self.name == other.name) and
             (len(self.fields) == len(other.fields)) and
-            all((k1 == k2 and (f1.deconstruct()[1:] == f2.deconstruct()[1:]))
-                for (k1, f1), (k2, f2) in zip(self.fields, other.fields)) and
+            all(
+                k1 == k2 and f1.deconstruct()[1:] == f2.deconstruct()[1:]
+                for (k1, f1), (k2, f2) in zip(
+                    sorted(self.fields.items()),
+                    sorted(other.fields.items()),
+                )
+            ) and
             (self.options == other.options) and
             (self.bases == other.bases) and
             (self.managers == other.managers)
('django/db/migrations/operations', 'models.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -7,7 +7,7 @@
 from .fields import (
     AddField, AlterField, FieldOperation, RemoveField, RenameField,
 )
-from .utils import ModelTuple, field_references_model
+from .utils import field_references, get_references, resolve_relation


 def _check_for_duplicates(arg_name, objs):
@@ -28,12 +28,12 @@
     def name_lower(self):
         return self.name.lower()

-    def references_model(self, name, app_label=None):
+    def references_model(self, name, app_label):
         return name.lower() == self.name_lower

-    def reduce(self, operation, app_label=None):
-        return (
-            super().reduce(operation, app_label=app_label) or
+    def reduce(self, operation, app_label):
+        return (
+            super().reduce(operation, app_label) or
             not operation.references_model(self.name, app_label)
         )

@@ -99,25 +99,29 @@
     def describe(self):
         return "Create %smodel %s" % ("proxy " if self.options.get("proxy", False) else "", self.name)

-    def references_model(self, name, app_label=None):
+    @property
+    def migration_name_fragment(self):
+        return self.name_lower
+
+    def references_model(self, name, app_label):
         name_lower = name.lower()
         if name_lower == self.name_lower:
             return True

         # Check we didn't inherit from the model
-        model_tuple = ModelTuple(app_label, name_lower)
+        reference_model_tuple = (app_label, name_lower)
         for base in self.bases:
             if (base is not models.Model and isinstance(base, (models.base.ModelBase, str)) and
-                    ModelTuple.from_model(base) == model_tuple):
+                    resolve_relation(base, app_label) == reference_model_tuple):
                 return True

         # Check we have no FKs/M2Ms with it
         for _name, field in self.fields:
-            if field_references_model(field, model_tuple):
+            if field_references((app_label, self.name_lower), field, reference_model_tuple):
                 return True
         return False

-    def reduce(self, operation, app_label=None):
+    def reduce(self, operation, app_label):
         if (isinstance(operation, DeleteModel) and
                 self.name_lower == operation.name_lower and
                 not self.options.get("proxy", False)):
@@ -133,11 +137,15 @@
                 ),
             ]
         elif isinstance(operation, AlterModelOptions) and self.name_lower == operation.name_lower:
+            options = {**self.options, **operation.options}
+            for key in operation.ALTER_OPTION_KEYS:
+                if key not in operation.options:
+                    options.pop(key, None)
             return [
                 CreateModel(
                     self.name,
                     fields=self.fields,
-                    options={**self.options, **operation.options},
+                    options=options,
                     bases=self.bases,
                     managers=self.managers,
                 ),
@@ -236,7 +244,7 @@
                         managers=self.managers,
                     ),
                 ]
-        return super().reduce(operation, app_label=app_label)
+        return super().reduce(operation, app_label)


 class DeleteModel(ModelOperation):
@@ -265,13 +273,17 @@
         if self.allow_migrate_model(schema_editor.connection.alias, model):
             schema_editor.create_model(model)

-    def references_model(self, name, app_label=None):
+    def references_model(self, name, app_label):
         # The deleted model could be referencing the specified model through
         # related fields.
         return True

     def describe(self):
         return "Delete model %s" % self.name
+
+    @property
+    def migration_name_fragment(self):
+        return 'delete_%s' % self.name_lower


 class RenameModel(ModelOperation):
@@ -307,35 +319,21 @@
         renamed_model.name = self.new_name
         state.models[app_label, self.new_name_lower] = renamed_model
         # Repoint all fields pointing to the old model to the new one.
-        old_model_tuple = ModelTuple(app_label, self.old_name_lower)
+        old_model_tuple = (app_label, self.old_name_lower)
         new_remote_model = '%s.%s' % (app_label, self.new_name)
-        to_reload = []
-        for (model_app_label, model_name), model_state in state.models.items():
-            model_changed = False
-            for index, (name, field) in enumerate(model_state.fields):
-                changed_field = None
-                remote_field = field.remote_field
-                if remote_field:
-                    remote_model_tuple = ModelTuple.from_model(
-                        remote_field.model, model_app_label, model_name
-                    )
-                    if remote_model_tuple == old_model_tuple:
-                        changed_field = field.clone()
-                        changed_field.remote_field.model = new_remote_model
-                    through_model = getattr(remote_field, 'through', None)
-                    if through_model:
-                        through_model_tuple = ModelTuple.from_model(
-                            through_model, model_app_label, model_name
-                        )
-                        if through_model_tuple == old_model_tuple:
-                            if changed_field is None:
-                                changed_field = field.clone()
-                            changed_field.remote_field.through = new_remote_model
-                if changed_field:
-                    model_state.fields[index] = name, changed_field
-                    model_changed = True
-            if model_changed:
-                to_reload.append((model_app_label, model_name))
+        to_reload = set()
+        for model_state, name, field, reference in get_references(state, old_model_tuple):
+            changed_field = None
+            if reference.to:
+                changed_field = field.clone()
+                changed_field.remote_field.model = new_remote_model
+            if reference.through:
+                if changed_field is None:
+                    changed_field = field.clone()
+                changed_field.remote_field.through = new_remote_model
+            if changed_field:
+                model_state.fields[name] = changed_field
+                to_reload.add((model_state.app_label, model_state.name_lower))
         # Reload models related to old model before removing the old model.
         state.reload_models(to_reload, delay=True)
         # Remove the old model.
@@ -402,7 +400,7 @@
         self.new_name_lower, self.old_name_lower = self.old_name_lower, self.new_name_lower
         self.new_name, self.old_name = self.old_name, self.new_name

-    def references_model(self, name, app_label=None):
+    def references_model(self, name, app_label):
         return (
             name.lower() == self.old_name_lower or
             name.lower() == self.new_name_lower
@@ -411,7 +409,11 @@
     def describe(self):
         return "Rename model %s to %s" % (self.old_name, self.new_name)

-    def reduce(self, operation, app_label=None):
+    @property
+    def migration_name_fragment(self):
+        return 'rename_%s_%s' % (self.old_name_lower, self.new_name_lower)
+
+    def reduce(self, operation, app_label):
         if (isinstance(operation, RenameModel) and
                 self.new_name_lower == operation.old_name_lower):
             return [
@@ -423,12 +425,19 @@
         # Skip `ModelOperation.reduce` as we want to run `references_model`
         # against self.new_name.
         return (
-            super(ModelOperation, self).reduce(operation, app_label=app_label) or
+            super(ModelOperation, self).reduce(operation, app_label) or
             not operation.references_model(self.new_name, app_label)
         )


-class AlterModelTable(ModelOperation):
+class ModelOptionOperation(ModelOperation):
+    def reduce(self, operation, app_label):
+        if isinstance(operation, (self.__class__, DeleteModel)) and self.name_lower == operation.name_lower:
+            return [operation]
+        return super().reduce(operation, app_label)
+
+
+class AlterModelTable(ModelOptionOperation):
     """Rename a model's table."""

     def __init__(self, name, table):
@@ -477,17 +486,9 @@
             self.table if self.table is not None else "(default)"
         )

-    def reduce(self, operation, app_label=None):
-        if isinstance(operation, (AlterModelTable, DeleteModel)) and self.name_lower == operation.name_lower:
-            return [operation]
-        return super().reduce(operation, app_label=app_label)
-
-
-class ModelOptionOperation(ModelOperation):
-    def reduce(self, operation, app_label=None):
-        if isinstance(operation, (self.__class__, DeleteModel)) and self.name_lower == operation.name_lower:
-            return [operation]
-        return super().reduce(operation, app_label=app_label)
+    @property
+    def migration_name_fragment(self):
+        return 'alter_%s_table' % self.name_lower


 class AlterTogetherOptionOperation(ModelOptionOperation):
@@ -533,7 +534,7 @@
     def database_backwards(self, app_label, schema_editor, from_state, to_state):
         return self.database_forwards(app_label, schema_editor, from_state, to_state)

-    def references_field(self, model_name, name, app_label=None):
+    def references_field(self, model_name, name, app_label):
         return (
             self.references_model(model_name, app_label) and
             (
@@ -544,6 +545,10 @@

     def describe(self):
         return "Alter %s for %s (%s constraint(s))" % (self.option_name, self.name, len(self.option_value or ''))
+
+    @property
+    def migration_name_fragment(self):
+        return 'alter_%s_%s' % (self.name_lower, self.option_name)


 class AlterUniqueTogether(AlterTogetherOptionOperation):
@@ -614,7 +619,7 @@
     def database_backwards(self, app_label, schema_editor, from_state, to_state):
         self.database_forwards(app_label, schema_editor, from_state, to_state)

-    def references_field(self, model_name, name, app_label=None):
+    def references_field(self, model_name, name, app_label):
         return (
             self.references_model(model_name, app_label) and
             (
@@ -625,6 +630,10 @@

     def describe(self):
         return "Set order_with_respect_to on %s to %s" % (self.name, self.order_with_respect_to)
+
+    @property
+    def migration_name_fragment(self):
+        return 'alter_%s_order_with_respect_to' % self.name_lower


 class AlterModelOptions(ModelOptionOperation):
@@ -681,6 +690,10 @@
     def describe(self):
         return "Change Meta options on %s" % self.name

+    @property
+    def migration_name_fragment(self):
+        return 'alter_%s_options' % self.name_lower
+

 class AlterModelManagers(ModelOptionOperation):
     """Alter the model's managers."""
@@ -711,6 +724,10 @@

     def describe(self):
         return "Change managers on %s" % self.name
+
+    @property
+    def migration_name_fragment(self):
+        return 'alter_%s_managers' % self.name_lower


 class IndexOperation(Operation):
@@ -760,12 +777,22 @@
         )

     def describe(self):
+        if self.index.expressions:
+            return 'Create index %s on %s on model %s' % (
+                self.index.name,
+                ', '.join([str(expression) for expression in self.index.expressions]),
+                self.model_name,
+            )
         return 'Create index %s on field(s) %s of model %s' % (
             self.index.name,
             ', '.join(self.index.fields),
             self.model_name,
         )

+    @property
+    def migration_name_fragment(self):
+        return '%s_%s' % (self.model_name_lower, self.index.name.lower())
+

 class RemoveIndex(IndexOperation):
     """Remove an index from a model."""
@@ -808,6 +835,10 @@
     def describe(self):
         return 'Remove index %s from %s' % (self.name, self.model_name)

+    @property
+    def migration_name_fragment(self):
+        return 'remove_%s_%s' % (self.model_name_lower, self.name.lower())
+

 class AddConstraint(IndexOperation):
     option_name = 'constraints'
@@ -839,6 +870,10 @@

     def describe(self):
         return 'Create constraint %s on model %s' % (self.constraint.name, self.model_name)
+
+    @property
+    def migration_name_fragment(self):
+        return '%s_%s' % (self.model_name_lower, self.constraint.name.lower())


 class RemoveConstraint(IndexOperation):
@@ -876,3 +911,7 @@

     def describe(self):
         return 'Remove constraint %s from model %s' % (self.name, self.model_name)
+
+    @property
+    def migration_name_fragment(self):
+        return 'remove_%s_%s' % (self.model_name_lower, self.name.lower())
('django/db/migrations/operations', 'fields.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,11 +1,9 @@
 from django.core.exceptions import FieldDoesNotExist
-from django.db.models.fields import NOT_PROVIDED
+from django.db.models import NOT_PROVIDED
 from django.utils.functional import cached_property

 from .base import Operation
-from .utils import (
-    ModelTuple, field_references_model, is_referenced_by_foreign_key,
-)
+from .utils import field_is_referenced, field_references, get_references


 class FieldOperation(Operation):
@@ -28,15 +26,17 @@
     def is_same_field_operation(self, operation):
         return self.is_same_model_operation(operation) and self.name_lower == operation.name_lower

-    def references_model(self, name, app_label=None):
+    def references_model(self, name, app_label):
         name_lower = name.lower()
         if name_lower == self.model_name_lower:
             return True
         if self.field:
-            return field_references_model(self.field, ModelTuple(app_label, name_lower))
+            return bool(field_references(
+                (app_label, self.model_name_lower), self.field, (app_label, name_lower)
+            ))
         return False

-    def references_field(self, model_name, name, app_label=None):
+    def references_field(self, model_name, name, app_label):
         model_name_lower = model_name.lower()
         # Check if this operation locally references the field.
         if model_name_lower == self.model_name_lower:
@@ -45,24 +45,18 @@
             elif self.field and hasattr(self.field, 'from_fields') and name in self.field.from_fields:
                 return True
         # Check if this operation remotely references the field.
-        if self.field:
-            model_tuple = ModelTuple(app_label, model_name_lower)
-            remote_field = self.field.remote_field
-            if remote_field:
-                if (ModelTuple.from_model(remote_field.model) == model_tuple and
-                        (not hasattr(self.field, 'to_fields') or
-                            name in self.field.to_fields or None in self.field.to_fields)):
-                    return True
-                through = getattr(remote_field, 'through', None)
-                if (through and ModelTuple.from_model(through) == model_tuple and
-                        (getattr(remote_field, 'through_fields', None) is None or
-                            name in remote_field.through_fields)):
-                    return True
-        return False
-
-    def reduce(self, operation, app_label=None):
-        return (
-            super().reduce(operation, app_label=app_label) or
+        if self.field is None:
+            return False
+        return bool(field_references(
+            (app_label, self.model_name_lower),
+            self.field,
+            (app_label, model_name_lower),
+            name,
+        ))
+
+    def reduce(self, operation, app_label):
+        return (
+            super().reduce(operation, app_label) or
             not operation.references_field(self.model_name, self.name, app_label)
         )

@@ -95,7 +89,7 @@
             field.default = NOT_PROVIDED
         else:
             field = self.field
-        state.models[app_label, self.model_name_lower].fields.append((self.name, field))
+        state.models[app_label, self.model_name_lower].fields[self.name] = field
         # Delay rendering of relationships if it's not a relational field
         delay = not field.is_relation
         state.reload_model(app_label, self.model_name_lower, delay=delay)
@@ -122,7 +116,11 @@
     def describe(self):
         return "Add field %s to %s" % (self.name, self.model_name)

-    def reduce(self, operation, app_label=None):
+    @property
+    def migration_name_fragment(self):
+        return '%s_%s' % (self.model_name_lower, self.name_lower)
+
+    def reduce(self, operation, app_label):
         if isinstance(operation, FieldOperation) and self.is_same_field_operation(operation):
             if isinstance(operation, AlterField):
                 return [
@@ -142,7 +140,7 @@
                         field=self.field,
                     ),
                 ]
-        return super().reduce(operation, app_label=app_label)
+        return super().reduce(operation, app_label)


 class RemoveField(FieldOperation):
@@ -160,14 +158,8 @@
         )

     def state_forwards(self, app_label, state):
-        new_fields = []
-        old_field = None
-        for name, instance in state.models[app_label, self.model_name_lower].fields:
-            if name != self.name:
-                new_fields.append((name, instance))
-            else:
-                old_field = instance
-        state.models[app_label, self.model_name_lower].fields = new_fields
+        model_state = state.models[app_label, self.model_name_lower]
+        old_field = model_state.fields.pop(self.name)
         # Delay rendering of relationships if it's not a relational field
         delay = not old_field.is_relation
         state.reload_model(app_label, self.model_name_lower, delay=delay)
@@ -186,11 +178,15 @@
     def describe(self):
         return "Remove field %s from %s" % (self.name, self.model_name)

-    def reduce(self, operation, app_label=None):
+    @property
+    def migration_name_fragment(self):
+        return 'remove_%s_%s' % (self.model_name_lower, self.name_lower)
+
+    def reduce(self, operation, app_label):
         from .models import DeleteModel
         if isinstance(operation, DeleteModel) and operation.name_lower == self.model_name_lower:
             return [operation]
-        return super().reduce(operation, app_label=app_label)
+        return super().reduce(operation, app_label)


 class AlterField(FieldOperation):
@@ -223,18 +219,17 @@
             field.default = NOT_PROVIDED
         else:
             field = self.field
-        state.models[app_label, self.model_name_lower].fields = [
-            (n, field if n == self.name else f)
-            for n, f in
-            state.models[app_label, self.model_name_lower].fields
-        ]
+        model_state = state.models[app_label, self.model_name_lower]
+        model_state.fields[self.name] = field
         # TODO: investigate if old relational fields must be reloaded or if it's
         # sufficient if the new field is (#27737).
         # Delay rendering of relationships if it's not a relational field and
         # not referenced by a foreign key.
         delay = (
             not field.is_relation and
-            not is_referenced_by_foreign_key(state, self.model_name_lower, self.field, self.name)
+            not field_is_referenced(
+                state, (app_label, self.model_name_lower), (self.name, field),
+            )
         )
         state.reload_model(app_label, self.model_name_lower, delay=delay)

@@ -256,7 +251,11 @@
     def describe(self):
         return "Alter field %s on %s" % (self.name, self.model_name)

-    def reduce(self, operation, app_label=None):
+    @property
+    def migration_name_fragment(self):
+        return 'alter_%s_%s' % (self.model_name_lower, self.name_lower)
+
+    def reduce(self, operation, app_label):
         if isinstance(operation, RemoveField) and self.is_same_field_operation(operation):
             return [operation]
         elif isinstance(operation, RenameField) and self.is_same_field_operation(operation):
@@ -268,7 +267,7 @@
                     field=self.field,
                 ),
             ]
-        return super().reduce(operation, app_label=app_label)
+        return super().reduce(operation, app_label)


 class RenameField(FieldOperation):
@@ -303,12 +302,14 @@
         model_state = state.models[app_label, self.model_name_lower]
         # Rename the field
         fields = model_state.fields
-        found = False
-        delay = True
-        for index, (name, field) in enumerate(fields):
-            if not found and name == self.old_name:
-                fields[index] = (self.new_name, field)
-                found = True
+        try:
+            found = fields.pop(self.old_name)
+        except KeyError:
+            raise FieldDoesNotExist(
+                "%s.%s has no field named '%s'" % (app_label, self.model_name, self.old_name)
+            )
+        fields[self.new_name] = found
+        for field in fields.values():
             # Fix from_fields to refer to the new field.
             from_fields = getattr(field, 'from_fields', None)
             if from_fields:
@@ -316,16 +317,6 @@
                     self.new_name if from_field_name == self.old_name else from_field_name
                     for from_field_name in from_fields
                 ])
-            # Delay rendering of relationships if it's not a relational
-            # field and not referenced by a foreign key.
-            delay = delay and (
-                not field.is_relation and
-                not is_referenced_by_foreign_key(state, self.model_name_lower, field, self.name)
-            )
-        if not found:
-            raise FieldDoesNotExist(
-                "%s.%s has no field named '%s'" % (app_label, self.model_name, self.old_name)
-            )
         # Fix index/unique_together to refer to the new field
         options = model_state.options
         for option in ('index_together', 'unique_together'):
@@ -335,23 +326,21 @@
                     for together in options[option]
                 ]
         # Fix to_fields to refer to the new field.
-        model_tuple = app_label, self.model_name_lower
-        for (model_app_label, model_name), model_state in state.models.items():
-            for index, (name, field) in enumerate(model_state.fields):
-                remote_field = field.remote_field
-                if remote_field:
-                    remote_model_tuple = self._get_model_tuple(
-                        remote_field.model, model_app_label, model_name
-                    )
-                    if remote_model_tuple == model_tuple:
-                        if getattr(remote_field, 'field_name', None) == self.old_name:
-                            remote_field.field_name = self.new_name
-                        to_fields = getattr(field, 'to_fields', None)
-                        if to_fields:
-                            field.to_fields = tuple([
-                                self.new_name if to_field_name == self.old_name else to_field_name
-                                for to_field_name in to_fields
-                            ])
+        delay = True
+        references = get_references(
+            state, (app_label, self.model_name_lower), (self.old_name, found),
+        )
+        for *_, field, reference in references:
+            delay = False
+            if reference.to:
+                remote_field, to_fields = reference.to
+                if getattr(remote_field, 'field_name', None) == self.old_name:
+                    remote_field.field_name = self.new_name
+                if to_fields:
+                    field.to_fields = tuple([
+                        self.new_name if to_field_name == self.old_name else to_field_name
+                        for to_field_name in to_fields
+                    ])
         state.reload_model(app_label, self.model_name_lower, delay=delay)

     def database_forwards(self, app_label, schema_editor, from_state, to_state):
@@ -377,13 +366,21 @@
     def describe(self):
         return "Rename field %s on %s to %s" % (self.old_name, self.model_name, self.new_name)

-    def references_field(self, model_name, name, app_label=None):
-        return self.references_model(model_name) and (
+    @property
+    def migration_name_fragment(self):
+        return 'rename_%s_%s_%s' % (
+            self.old_name_lower,
+            self.model_name_lower,
+            self.new_name_lower,
+        )
+
+    def references_field(self, model_name, name, app_label):
+        return self.references_model(model_name, app_label) and (
             name.lower() == self.old_name_lower or
             name.lower() == self.new_name_lower
         )

-    def reduce(self, operation, app_label=None):
+    def reduce(self, operation, app_label):
         if (isinstance(operation, RenameField) and
                 self.is_same_model_operation(operation) and
                 self.new_name_lower == operation.old_name_lower):
@@ -397,6 +394,6 @@
         # Skip `FieldOperation.reduce` as we want to run `references_field`
         # against self.new_name.
         return (
-            super(FieldOperation, self).reduce(operation, app_label=app_label) or
+            super(FieldOperation, self).reduce(operation, app_label) or
             not operation.references_field(self.model_name, self.new_name, app_label)
         )
('django/db/migrations/operations', 'utils.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,51 +3,100 @@
 from django.db.models.fields.related import RECURSIVE_RELATIONSHIP_CONSTANT


-def is_referenced_by_foreign_key(state, model_name_lower, field, field_name):
-    for state_app_label, state_model in state.models:
-        for _, f in state.models[state_app_label, state_model].fields:
-            if (f.related_model and
-                    '%s.%s' % (state_app_label, model_name_lower) == f.related_model.lower() and
-                    hasattr(f, 'to_fields')):
-                if (f.to_fields[0] is None and field.primary_key) or field_name in f.to_fields:
-                    return True
-    return False
+def resolve_relation(model, app_label=None, model_name=None):
+    """
+    Turn a model class or model reference string and return a model tuple.
+
+    app_label and model_name are used to resolve the scope of recursive and
+    unscoped model relationship.
+    """
+    if isinstance(model, str):
+        if model == RECURSIVE_RELATIONSHIP_CONSTANT:
+            if app_label is None or model_name is None:
+                raise TypeError(
+                    'app_label and model_name must be provided to resolve '
+                    'recursive relationships.'
+                )
+            return app_label, model_name
+        if '.' in model:
+            app_label, model_name = model.split('.', 1)
+            return app_label, model_name.lower()
+        if app_label is None:
+            raise TypeError(
+                'app_label must be provided to resolve unscoped model '
+                'relationships.'
+            )
+        return app_label, model.lower()
+    return model._meta.app_label, model._meta.model_name


-class ModelTuple(namedtuple('ModelTupleBase', ('app_label', 'model_name'))):
-    @classmethod
-    def from_model(cls, model, app_label=None, model_name=None):
-        """
-        Take a model class or a 'app_label.ModelName' string and return a
-        ModelTuple('app_label', 'modelname'). The optional app_label and
-        model_name arguments are the defaults if "self" or "ModelName" are
-        passed.
-        """
-        if isinstance(model, str):
-            if model == RECURSIVE_RELATIONSHIP_CONSTANT:
-                return cls(app_label, model_name)
-            if '.' in model:
-                return cls(*model.lower().split('.', 1))
-            return cls(app_label, model.lower())
-        return cls(model._meta.app_label, model._meta.model_name)
-
-    def __eq__(self, other):
-        if isinstance(other, ModelTuple):
-            # Consider ModelTuple equal if their model_name is equal and either
-            # one of them is missing an app_label.
-            return self.model_name == other.model_name and (
-                self.app_label is None or other.app_label is None or self.app_label == other.app_label
-            )
-        return super().__eq__(other)
+FieldReference = namedtuple('FieldReference', 'to through')


-def field_references_model(field, model_tuple):
-    """Return whether or not field references model_tuple."""
+def field_references(
+    model_tuple,
+    field,
+    reference_model_tuple,
+    reference_field_name=None,
+    reference_field=None,
+):
+    """
+    Return either False or a FieldReference if `field` references provided
+    context.
+
+    False positives can be returned if `reference_field_name` is provided
+    without `reference_field` because of the introspection limitation it
+    incurs. This should not be an issue when this function is used to determine
+    whether or not an optimization can take place.
+    """
     remote_field = field.remote_field
-    if remote_field:
-        if ModelTuple.from_model(remote_field.model) == model_tuple:
-            return True
-        through = getattr(remote_field, 'through', None)
-        if through and ModelTuple.from_model(through) == model_tuple:
-            return True
-    return False
+    if not remote_field:
+        return False
+    references_to = None
+    references_through = None
+    if resolve_relation(remote_field.model, *model_tuple) == reference_model_tuple:
+        to_fields = getattr(field, 'to_fields', None)
+        if (
+            reference_field_name is None or
+            # Unspecified to_field(s).
+            to_fields is None or
+            # Reference to primary key.
+            (None in to_fields and (reference_field is None or reference_field.primary_key)) or
+            # Reference to field.
+            reference_field_name in to_fields
+        ):
+            references_to = (remote_field, to_fields)
+    through = getattr(remote_field, 'through', None)
+    if through and resolve_relation(through, *model_tuple) == reference_model_tuple:
+        through_fields = remote_field.through_fields
+        if (
+            reference_field_name is None or
+            # Unspecified through_fields.
+            through_fields is None or
+            # Reference to field.
+            reference_field_name in through_fields
+        ):
+            references_through = (remote_field, through_fields)
+    if not (references_to or references_through):
+        return False
+    return FieldReference(references_to, references_through)
+
+
+def get_references(state, model_tuple, field_tuple=()):
+    """
+    Generator of (model_state, name, field, reference) referencing
+    provided context.
+
+    If field_tuple is provided only references to this particular field of
+    model_tuple will be generated.
+    """
+    for state_model_tuple, model_state in state.models.items():
+        for name, field in model_state.fields.items():
+            reference = field_references(state_model_tuple, field, model_tuple, *field_tuple)
+            if reference:
+                yield model_state, name, field, reference
+
+
+def field_is_referenced(state, model_tuple, field_tuple):
+    """Return whether `field_tuple` is referenced by any state models."""
+    return next(get_references(state, model_tuple, field_tuple), None) is not None
('django/db/migrations/operations', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,4 @@
 from django.db import router
-from django.db.models.fields.related import RECURSIVE_RELATIONSHIP_CONSTANT


 class Operation:
@@ -80,10 +79,18 @@
         """
         return "%s: %s" % (self.__class__.__name__, self._constructor_args)

-    def references_model(self, name, app_label=None):
+    @property
+    def migration_name_fragment(self):
+        """
+        A filename part suitable for automatically naming a migration
+        containing this operation, or None if not applicable.
+        """
+        return None
+
+    def references_model(self, name, app_label):
         """
         Return True if there is a chance this operation references the given
-        model name (as a string), with an optional app label for accuracy.
+        model name (as a string), with an app label for accuracy.

         Used for optimization. If in doubt, return True;
         returning a false positive will merely make the optimizer a little
@@ -92,10 +99,10 @@
         """
         return True

-    def references_field(self, model_name, name, app_label=None):
+    def references_field(self, model_name, name, app_label):
         """
         Return True if there is a chance this operation references the given
-        field name, with an optional app label for accuracy.
+        field name, with an app label for accuracy.

         Used for optimization. If in doubt, return True.
         """
@@ -113,7 +120,7 @@

         return router.allow_migrate_model(connection_alias, model)

-    def reduce(self, operation, app_label=None):
+    def reduce(self, operation, app_label):
         """
         Return either a list of operations the actual operation should be
         replaced with or a boolean that indicates whether or not the specified
@@ -125,14 +132,6 @@
             return [self]
         return False

-    def _get_model_tuple(self, remote_model, app_label, model_name):
-        if remote_model == RECURSIVE_RELATIONSHIP_CONSTANT:
-            return app_label, model_name.lower()
-        elif '.' in remote_model:
-            return tuple(remote_model.lower().split('.'))
-        else:
-            return app_label, remote_model.lower()
-
     def __repr__(self):
         return "<%s %s%s>" % (
             self.__class__.__name__,
('django/db/backends', 'signals.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,3 @@
 from django.dispatch import Signal

-connection_created = Signal(providing_args=["connection"])
+connection_created = Signal()
('django/db/backends', 'ddl_references.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,6 +2,7 @@
 Helpers to manipulate deferred DDL statements that might need to be adjusted or
 discarded within when executing a migration.
 """
+from copy import deepcopy


 class Reference:
@@ -83,10 +84,14 @@

     def __str__(self):
         def col_str(column, idx):
+            col = self.quote_name(column)
             try:
-                return self.quote_name(column) + self.col_suffixes[idx]
+                suffix = self.col_suffixes[idx]
+                if suffix:
+                    col = '{} {}'.format(col, suffix)
             except IndexError:
-                return self.quote_name(column)
+                pass
+            return col

         return ', '.join(col_str(column, idx) for idx, column in enumerate(self.columns))

@@ -110,13 +115,16 @@

     def __str__(self):
         def col_str(column, idx):
-            try:
-                col = self.quote_name(column) + self.col_suffixes[idx]
-            except IndexError:
-                col = self.quote_name(column)
             # Index.__init__() guarantees that self.opclasses is the same
             # length as self.columns.
-            return '{} {}'.format(col, self.opclasses[idx])
+            col = '{} {}'.format(self.quote_name(column), self.opclasses[idx])
+            try:
+                suffix = self.col_suffixes[idx]
+                if suffix:
+                    col = '{} {}'.format(col, suffix)
+            except IndexError:
+                pass
+            return col

         return ', '.join(col_str(column, idx) for idx, column in enumerate(self.columns))

@@ -191,3 +199,34 @@

     def __str__(self):
         return self.template % self.parts
+
+
+class Expressions(TableColumns):
+    def __init__(self, table, expressions, compiler, quote_value):
+        self.compiler = compiler
+        self.expressions = expressions
+        self.quote_value = quote_value
+        columns = [col.target.column for col in self.compiler.query._gen_cols([self.expressions])]
+        super().__init__(table, columns)
+
+    def rename_table_references(self, old_table, new_table):
+        if self.table != old_table:
+            return
+        self.expressions = self.expressions.relabeled_clone({old_table: new_table})
+        super().rename_table_references(old_table, new_table)
+
+    def rename_column_references(self, table, old_column, new_column):
+        if self.table != table:
+            return
+        expressions = deepcopy(self.expressions)
+        self.columns = []
+        for col in self.compiler.query._gen_cols([expressions]):
+            if col.target.column == old_column:
+                col.target.column = new_column
+            self.columns.append(col.target.column)
+        self.expressions = expressions
+
+    def __str__(self):
+        sql, params = self.compiler.compile(self.expressions)
+        params = map(self.quote_value, params)
+        return sql % tuple(params)
('django/db/backends', 'utils.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,11 +3,10 @@
 import functools
 import hashlib
 import logging
-from time import time
-
-from django.conf import settings
-from django.db.utils import NotSupportedError
-from django.utils.timezone import utc
+import time
+from contextlib import contextmanager
+
+from django.db import NotSupportedError

 logger = logging.getLogger('django.db.backends')

@@ -79,6 +78,7 @@
         self.db.validate_no_broken_transaction()
         with self.db.wrap_database_errors:
             if params is None:
+                # params default might be backend specific.
                 return self.cursor.execute(sql)
             else:
                 return self.cursor.execute(sql, params)
@@ -94,40 +94,38 @@
     # XXX callproc isn't instrumented at this time.

     def execute(self, sql, params=None):
-        start = time()
+        with self.debug_sql(sql, params, use_last_executed_query=True):
+            return super().execute(sql, params)
+
+    def executemany(self, sql, param_list):
+        with self.debug_sql(sql, param_list, many=True):
+            return super().executemany(sql, param_list)
+
+    @contextmanager
+    def debug_sql(self, sql=None, params=None, use_last_executed_query=False, many=False):
+        start = time.monotonic()
         try:
-            return super().execute(sql, params)
+            yield
         finally:
-            stop = time()
+            stop = time.monotonic()
             duration = stop - start
-            sql = self.db.ops.last_executed_query(self.cursor, sql, params)
+            if use_last_executed_query:
+                sql = self.db.ops.last_executed_query(self.cursor, sql, params)
+            try:
+                times = len(params) if many else ''
+            except TypeError:
+                # params could be an iterator.
+                times = '?'
             self.db.queries_log.append({
-                'sql': sql,
-                'time': "%.3f" % duration,
+                'sql': '%s times: %s' % (times, sql) if many else sql,
+                'time': '%.3f' % duration,
             })
             logger.debug(
-                '(%.3f) %s; args=%s', duration, sql, params,
-                extra={'duration': duration, 'sql': sql, 'params': params}
-            )
-
-    def executemany(self, sql, param_list):
-        start = time()
-        try:
-            return super().executemany(sql, param_list)
-        finally:
-            stop = time()
-            duration = stop - start
-            try:
-                times = len(param_list)
-            except TypeError:           # param_list could be an iterator
-                times = '?'
-            self.db.queries_log.append({
-                'sql': '%s times: %s' % (times, sql),
-                'time': "%.3f" % duration,
-            })
-            logger.debug(
-                '(%.3f) %s; args=%s', duration, sql, param_list,
-                extra={'duration': duration, 'sql': sql, 'params': param_list}
+                '(%.3f) %s; args=%s',
+                duration,
+                sql,
+                params,
+                extra={'duration': duration, 'sql': sql, 'params': params},
             )


@@ -170,11 +168,10 @@
         seconds, microseconds = seconds.split('.')
     else:
         microseconds = '0'
-    tzinfo = utc if settings.USE_TZ else None
     return datetime.datetime(
         int(dates[0]), int(dates[1]), int(dates[2]),
         int(times[0]), int(times[1]), int(seconds),
-        int((microseconds + '000000')[:6]), tzinfo
+        int((microseconds + '000000')[:6])
     )


@@ -184,7 +181,7 @@

 def split_identifier(identifier):
     """
-    Split a SQL identifier into a two element tuple of (namespace, name).
+    Split an SQL identifier into a two element tuple of (namespace, name).

     The identifier could be a table, column, or sequence name might be prefixed
     by a namespace.
@@ -198,7 +195,7 @@

 def truncate_name(identifier, length=None, hash_len=4):
     """
-    Shorten a SQL identifier to a repeatable mangled version with the given
+    Shorten an SQL identifier to a repeatable mangled version with the given
     length.

     If a quote stripped name contains a namespace, e.g. USERNAME"."TABLE,
('django/db/backends/postgresql', 'creation.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -48,7 +48,7 @@
             elif not keepdb:
                 # If the database should be kept, ignore "database already
                 # exists".
-                raise e
+                raise

     def _clone_test_db(self, suffix, verbosity, keepdb=False):
         # CREATE DATABASE ... WITH TEMPLATE ... requires closing connections
@@ -61,7 +61,7 @@
             'dbname': self._quote_name(target_database_name),
             'suffix': self._get_database_create_suffix(template=source_database_name),
         }
-        with self._nodb_connection.cursor() as cursor:
+        with self._nodb_cursor() as cursor:
             try:
                 self._execute_create_test_db(cursor, test_db_params, keepdb)
             except Exception:
('django/db/backends/postgresql', 'client.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,30 +1,25 @@
-import os
 import signal
-import subprocess

-from django.core.files.temp import NamedTemporaryFile
 from django.db.backends.base.client import BaseDatabaseClient
-
-
-def _escape_pgpass(txt):
-    """
-    Escape a fragment of a PostgreSQL .pgpass file.
-    """
-    return txt.replace('\\', '\\\\').replace(':', '\\:')


 class DatabaseClient(BaseDatabaseClient):
     executable_name = 'psql'

     @classmethod
-    def runshell_db(cls, conn_params):
+    def settings_to_cmd_args_env(cls, settings_dict, parameters):
         args = [cls.executable_name]
+        options = settings_dict.get('OPTIONS', {})

-        host = conn_params.get('host', '')
-        port = conn_params.get('port', '')
-        dbname = conn_params.get('database', '')
-        user = conn_params.get('user', '')
-        passwd = conn_params.get('password', '')
+        host = settings_dict.get('HOST')
+        port = settings_dict.get('PORT')
+        dbname = settings_dict.get('NAME') or 'postgres'
+        user = settings_dict.get('USER')
+        passwd = settings_dict.get('PASSWORD')
+        sslmode = options.get('sslmode')
+        sslrootcert = options.get('sslrootcert')
+        sslcert = options.get('sslcert')
+        sslkey = options.get('sslkey')

         if user:
             args += ['-U', user]
@@ -33,39 +28,27 @@
         if port:
             args += ['-p', str(port)]
         args += [dbname]
+        args.extend(parameters)

-        temp_pgpass = None
+        env = {}
+        if passwd:
+            env['PGPASSWORD'] = str(passwd)
+        if sslmode:
+            env['PGSSLMODE'] = str(sslmode)
+        if sslrootcert:
+            env['PGSSLROOTCERT'] = str(sslrootcert)
+        if sslcert:
+            env['PGSSLCERT'] = str(sslcert)
+        if sslkey:
+            env['PGSSLKEY'] = str(sslkey)
+        return args, (env or None)
+
+    def runshell(self, parameters):
         sigint_handler = signal.getsignal(signal.SIGINT)
         try:
-            if passwd:
-                # Create temporary .pgpass file.
-                temp_pgpass = NamedTemporaryFile(mode='w+')
-                try:
-                    print(
-                        _escape_pgpass(host) or '*',
-                        str(port) or '*',
-                        _escape_pgpass(dbname) or '*',
-                        _escape_pgpass(user) or '*',
-                        _escape_pgpass(passwd),
-                        file=temp_pgpass,
-                        sep=':',
-                        flush=True,
-                    )
-                    os.environ['PGPASSFILE'] = temp_pgpass.name
-                except UnicodeEncodeError:
-                    # If the current locale can't encode the data, let the
-                    # user input the password manually.
-                    pass
             # Allow SIGINT to pass to psql to abort queries.
             signal.signal(signal.SIGINT, signal.SIG_IGN)
-            subprocess.check_call(args)
+            super().runshell(parameters)
         finally:
             # Restore the original SIGINT handler.
             signal.signal(signal.SIGINT, sigint_handler)
-            if temp_pgpass:
-                temp_pgpass.close()
-                if 'PGPASSFILE' in os.environ:  # unit tests need cleanup
-                    del os.environ['PGPASSFILE']
-
-    def runshell(self):
-        DatabaseClient.runshell_db(self.connection.get_connection_params())
('django/db/backends/postgresql', 'features.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,28 +1,28 @@
 import operator

+from django.db import InterfaceError
 from django.db.backends.base.features import BaseDatabaseFeatures
-from django.db.utils import InterfaceError
 from django.utils.functional import cached_property


 class DatabaseFeatures(BaseDatabaseFeatures):
     allows_group_by_selected_pks = True
-    can_return_id_from_insert = True
-    can_return_ids_from_bulk_insert = True
+    can_return_columns_from_insert = True
+    can_return_rows_from_bulk_insert = True
     has_real_datatype = True
     has_native_uuid_field = True
     has_native_duration_field = True
+    has_native_json_field = True
     can_defer_constraint_checks = True
     has_select_for_update = True
     has_select_for_update_nowait = True
     has_select_for_update_of = True
+    has_select_for_update_skip_locked = True
+    has_select_for_no_key_update = True
     can_release_savepoints = True
     supports_tablespaces = True
     supports_transactions = True
-    can_introspect_autofield = True
-    can_introspect_ip_address_field = True
     can_introspect_materialized_views = True
-    can_introspect_small_integer_field = True
     can_distinct_on_fields = True
     can_rollback_ddl = True
     supports_combined_alters = True
@@ -51,27 +51,58 @@
     $$ LANGUAGE plpgsql;"""
     requires_casted_case_in_updates = True
     supports_over_clause = True
+    only_supports_unbounded_with_preceding_and_following = True
     supports_aggregate_filter_clause = True
     supported_explain_formats = {'JSON', 'TEXT', 'XML', 'YAML'}
-    validates_explain_options = False  # A query will error on invalid options.
+    supports_deferrable_unique_constraints = True
+    has_json_operators = True
+    json_key_contains_list_matching_requires_list = True
+
+    django_test_skips = {
+        'opclasses are PostgreSQL only.': {
+            'indexes.tests.SchemaIndexesNotPostgreSQLTests.test_create_index_ignores_opclasses',
+        },
+    }

     @cached_property
-    def is_postgresql_9_5(self):
-        return self.connection.pg_version >= 90500
+    def test_collations(self):
+        # PostgreSQL < 10 doesn't support ICU collations.
+        if self.is_postgresql_10:
+            return {
+                'non_default': 'sv-x-icu',
+                'swedish_ci': 'sv-x-icu',
+            }
+        return {}

     @cached_property
-    def is_postgresql_9_6(self):
-        return self.connection.pg_version >= 90600
+    def introspected_field_types(self):
+        return {
+            **super().introspected_field_types,
+            'PositiveBigIntegerField': 'BigIntegerField',
+            'PositiveIntegerField': 'IntegerField',
+            'PositiveSmallIntegerField': 'SmallIntegerField',
+        }

     @cached_property
     def is_postgresql_10(self):
         return self.connection.pg_version >= 100000

-    has_select_for_update_skip_locked = property(operator.attrgetter('is_postgresql_9_5'))
-    has_brin_index_support = property(operator.attrgetter('is_postgresql_9_5'))
-    has_jsonb_agg = property(operator.attrgetter('is_postgresql_9_5'))
+    @cached_property
+    def is_postgresql_11(self):
+        return self.connection.pg_version >= 110000
+
+    @cached_property
+    def is_postgresql_12(self):
+        return self.connection.pg_version >= 120000
+
+    @cached_property
+    def is_postgresql_13(self):
+        return self.connection.pg_version >= 130000
+
     has_brin_autosummarize = property(operator.attrgetter('is_postgresql_10'))
-    has_gin_pending_list_limit = property(operator.attrgetter('is_postgresql_9_5'))
-    supports_ignore_conflicts = property(operator.attrgetter('is_postgresql_9_5'))
-    has_phraseto_tsquery = property(operator.attrgetter('is_postgresql_9_6'))
+    has_websearch_to_tsquery = property(operator.attrgetter('is_postgresql_11'))
     supports_table_partitions = property(operator.attrgetter('is_postgresql_10'))
+    supports_covering_indexes = property(operator.attrgetter('is_postgresql_11'))
+    supports_covering_gist_indexes = property(operator.attrgetter('is_postgresql_12'))
+    supports_non_deterministic_collations = property(operator.attrgetter('is_postgresql_12'))
+    supports_alternate_collation_providers = property(operator.attrgetter('is_postgresql_10'))
('django/db/backends/postgresql', 'operations.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,16 +1,28 @@
 from psycopg2.extras import Inet

 from django.conf import settings
-from django.db import NotSupportedError
 from django.db.backends.base.operations import BaseDatabaseOperations


 class DatabaseOperations(BaseDatabaseOperations):
     cast_char_field_without_max_length = 'varchar'
     explain_prefix = 'EXPLAIN'
+    explain_options = frozenset(
+        [
+            "ANALYZE",
+            "BUFFERS",
+            "COSTS",
+            "SETTINGS",
+            "SUMMARY",
+            "TIMING",
+            "VERBOSE",
+            "WAL",
+        ]
+    )
     cast_data_types = {
         'AutoField': 'integer',
         'BigAutoField': 'bigint',
+        'SmallAutoField': 'smallint',
     }

     def unification_cast_sql(self, output_field):
@@ -31,18 +43,28 @@
         if lookup_type == 'week_day':
             # For consistency across backends, we return Sunday=1, Saturday=7.
             return "EXTRACT('dow' FROM %s) + 1" % field_name
+        elif lookup_type == 'iso_week_day':
+            return "EXTRACT('isodow' FROM %s)" % field_name
         elif lookup_type == 'iso_year':
             return "EXTRACT('isoyear' FROM %s)" % field_name
         else:
             return "EXTRACT('%s' FROM %s)" % (lookup_type, field_name)

-    def date_trunc_sql(self, lookup_type, field_name):
+    def date_trunc_sql(self, lookup_type, field_name, tzname=None):
+        field_name = self._convert_field_to_tz(field_name, tzname)
         # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC
         return "DATE_TRUNC('%s', %s)" % (lookup_type, field_name)

+    def _prepare_tzname_delta(self, tzname):
+        if '+' in tzname:
+            return tzname.replace('+', '-')
+        elif '-' in tzname:
+            return tzname.replace('-', '+')
+        return tzname
+
     def _convert_field_to_tz(self, field_name, tzname):
-        if settings.USE_TZ:
-            field_name = "%s AT TIME ZONE '%s'" % (field_name, tzname)
+        if tzname and settings.USE_TZ:
+            field_name = "%s AT TIME ZONE '%s'" % (field_name, self._prepare_tzname_delta(tzname))
         return field_name

     def datetime_cast_date_sql(self, field_name, tzname):
@@ -62,19 +84,19 @@
         # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC
         return "DATE_TRUNC('%s', %s)" % (lookup_type, field_name)

-    def time_trunc_sql(self, lookup_type, field_name):
+    def time_trunc_sql(self, lookup_type, field_name, tzname=None):
+        field_name = self._convert_field_to_tz(field_name, tzname)
         return "DATE_TRUNC('%s', %s)::time" % (lookup_type, field_name)

     def deferrable_sql(self):
         return " DEFERRABLE INITIALLY DEFERRED"

-    def fetch_returned_insert_ids(self, cursor):
+    def fetch_returned_insert_rows(self, cursor):
         """
         Given a cursor object that has just performed an INSERT...RETURNING
-        statement into a table that has an auto-incrementing ID, return the
-        list of newly created IDs.
-        """
-        return [item[0] for item in cursor.fetchall()]
+        statement into a table, return the tuple of returned data.
+        """
+        return cursor.fetchall()

     def lookup_cast(self, lookup_type, internal_type=None):
         lookup = '%s'
@@ -109,28 +131,21 @@
     def set_time_zone_sql(self):
         return "SET TIME ZONE %s"

-    def sql_flush(self, style, tables, sequences, allow_cascade=False):
-        if tables:
-            # Perform a single SQL 'TRUNCATE x, y, z...;' statement.  It allows
-            # us to truncate tables referenced by a foreign key in any other
-            # table.
-            tables_sql = ', '.join(
-                style.SQL_FIELD(self.quote_name(table)) for table in tables)
-            if allow_cascade:
-                sql = ['%s %s %s;' % (
-                    style.SQL_KEYWORD('TRUNCATE'),
-                    tables_sql,
-                    style.SQL_KEYWORD('CASCADE'),
-                )]
-            else:
-                sql = ['%s %s;' % (
-                    style.SQL_KEYWORD('TRUNCATE'),
-                    tables_sql,
-                )]
-            sql.extend(self.sequence_reset_by_name_sql(style, sequences))
-            return sql
-        else:
+    def sql_flush(self, style, tables, *, reset_sequences=False, allow_cascade=False):
+        if not tables:
             return []
+
+        # Perform a single SQL 'TRUNCATE x, y, z...;' statement. It allows us
+        # to truncate tables referenced by a foreign key in any other table.
+        sql_parts = [
+            style.SQL_KEYWORD('TRUNCATE'),
+            ', '.join(style.SQL_FIELD(self.quote_name(table)) for table in tables),
+        ]
+        if reset_sequences:
+            sql_parts.append(style.SQL_KEYWORD('RESTART IDENTITY'))
+        if allow_cascade:
+            sql_parts.append(style.SQL_KEYWORD('CASCADE'))
+        return ['%s;' % ' '.join(sql_parts)]

     def sequence_reset_by_name_sql(self, style, sequences):
         # 'ALTER SEQUENCE sequence_name RESTART WITH 1;'... style SQL statements
@@ -181,21 +196,6 @@
                         )
                     )
                     break  # Only one AutoField is allowed per model, so don't bother continuing.
-            for f in model._meta.many_to_many:
-                if not f.remote_field.through:
-                    output.append(
-                        "%s setval(pg_get_serial_sequence('%s','%s'), "
-                        "coalesce(max(%s), 1), max(%s) %s null) %s %s;" % (
-                            style.SQL_KEYWORD('SELECT'),
-                            style.SQL_TABLE(qn(f.m2m_db_table())),
-                            style.SQL_FIELD('id'),
-                            style.SQL_FIELD(qn('id')),
-                            style.SQL_FIELD(qn('id')),
-                            style.SQL_KEYWORD('IS NOT'),
-                            style.SQL_KEYWORD('FROM'),
-                            style.SQL_TABLE(qn(f.m2m_db_table()))
-                        )
-                    )
         return output

     def prep_for_iexact_query(self, x):
@@ -222,14 +222,22 @@
             return ['DISTINCT'], []

     def last_executed_query(self, cursor, sql, params):
-        # http://initd.org/psycopg/docs/cursor.html#cursor.query
+        # https://www.psycopg.org/docs/cursor.html#cursor.query
         # The query attribute is a Psycopg extension to the DB API 2.0.
         if cursor.query is not None:
             return cursor.query.decode()
         return None

-    def return_insert_id(self):
-        return "RETURNING %s", ()
+    def return_insert_columns(self, fields):
+        if not fields:
+            return '', ()
+        columns = [
+            '%s.%s' % (
+                self.quote_name(field.model._meta.db_table),
+                self.quote_name(field.column),
+            ) for field in fields
+        ]
+        return 'RETURNING %s' % ', '.join(columns), ()

     def bulk_insert_sql(self, fields, placeholder_rows):
         placeholder_rows_sql = (", ".join(row) for row in placeholder_rows)
@@ -245,6 +253,9 @@
     def adapt_timefield_value(self, value):
         return value

+    def adapt_decimalfield_value(self, value, max_digits=None, decimal_places=None):
+        return value
+
     def adapt_ipaddressfield_value(self, value):
         if value:
             return Inet(value)
@@ -254,28 +265,25 @@
         if internal_type == 'DateField':
             lhs_sql, lhs_params = lhs
             rhs_sql, rhs_params = rhs
-            return "(interval '1 day' * (%s - %s))" % (lhs_sql, rhs_sql), lhs_params + rhs_params
+            params = (*lhs_params, *rhs_params)
+            return "(interval '1 day' * (%s - %s))" % (lhs_sql, rhs_sql), params
         return super().subtract_temporals(internal_type, lhs, rhs)

-    def window_frame_range_start_end(self, start=None, end=None):
-        start_, end_ = super().window_frame_range_start_end(start, end)
-        if (start and start < 0) or (end and end > 0):
-            raise NotSupportedError(
-                'PostgreSQL only supports UNBOUNDED together with PRECEDING '
-                'and FOLLOWING.'
-            )
-        return start_, end_
-
     def explain_query_prefix(self, format=None, **options):
-        prefix = super().explain_query_prefix(format)
         extra = {}
+        # Normalize options.
+        if options:
+            options = {
+                name.upper(): "true" if value else "false"
+                for name, value in options.items()
+            }
+            for valid_option in self.explain_options:
+                value = options.pop(valid_option, None)
+                if value is not None:
+                    extra[valid_option.upper()] = value
+        prefix = super().explain_query_prefix(format, **options)
         if format:
             extra['FORMAT'] = format
-        if options:
-            extra.update({
-                name.upper(): 'true' if value else 'false'
-                for name, value in options.items()
-            })
         if extra:
             prefix += ' (%s)' % ', '.join('%s %s' % i for i in extra.items())
         return prefix
('django/db/backends/postgresql', 'introspection.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,7 +1,7 @@
 from django.db.backends.base.introspection import (
     BaseDatabaseIntrospection, FieldInfo, TableInfo,
 )
-from django.db.models.indexes import Index
+from django.db.models import Index


 class DatabaseIntrospection(BaseDatabaseIntrospection):
@@ -26,7 +26,10 @@
         1266: 'TimeField',
         1700: 'DecimalField',
         2950: 'UUIDField',
+        3802: 'JSONField',
     }
+    # A hook for subclasses.
+    index_default_access_method = 'btree'

     ignored_tables = []

@@ -37,6 +40,8 @@
                 return 'AutoField'
             elif field_type == 'BigIntegerField':
                 return 'BigAutoField'
+            elif field_type == 'SmallIntegerField':
+                return 'SmallAutoField'
         return field_type

     def get_table_list(self, cursor):
@@ -64,9 +69,11 @@
             SELECT
                 a.attname AS column_name,
                 NOT (a.attnotnull OR (t.typtype = 'd' AND t.typnotnull)) AS is_nullable,
-                pg_get_expr(ad.adbin, ad.adrelid) AS column_default
+                pg_get_expr(ad.adbin, ad.adrelid) AS column_default,
+                CASE WHEN collname = 'default' THEN NULL ELSE collname END AS collation
             FROM pg_attribute a
             LEFT JOIN pg_attrdef ad ON a.attrelid = ad.adrelid AND a.attnum = ad.adnum
+            LEFT JOIN pg_collation co ON a.attcollation = co.oid
             JOIN pg_type t ON a.atttypid = t.oid
             JOIN pg_class c ON a.attrelid = c.oid
             JOIN pg_namespace n ON c.relnamespace = n.oid
@@ -77,7 +84,18 @@
         """, [table_name])
         field_map = {line[0]: line[1:] for line in cursor.fetchall()}
         cursor.execute("SELECT * FROM %s LIMIT 1" % self.connection.ops.quote_name(table_name))
-        return [FieldInfo(*line[0:6], *field_map[line.name]) for line in cursor.description]
+        return [
+            FieldInfo(
+                line.name,
+                line.type_code,
+                line.display_size,
+                line.internal_size,
+                line.precision,
+                line.scale,
+                *field_map[line.name],
+            )
+            for line in cursor.description
+        ]

     def get_sequences(self, cursor, table_name, table_fields=()):
         cursor.execute("""
@@ -88,10 +106,9 @@
                 JOIN pg_attrdef ad ON ad.oid = d.objid AND d.classid = 'pg_attrdef'::regclass
                 JOIN pg_attribute col ON col.attrelid = ad.adrelid AND col.attnum = ad.adnum
                 JOIN pg_class tbl ON tbl.oid = ad.adrelid
-                JOIN pg_namespace n ON n.oid = tbl.relnamespace
             WHERE s.relkind = 'S'
               AND d.deptype in ('a', 'n')
-              AND n.nspname = 'public'
+              AND pg_catalog.pg_table_is_visible(tbl.oid)
               AND tbl.relname = %s
         """, [table_name])
         return [
@@ -104,30 +121,21 @@
         Return a dictionary of {field_name: (field_name_other_table, other_table)}
         representing all relationships to the given table.
         """
-        cursor.execute("""
-            SELECT c2.relname, a1.attname, a2.attname
+        return {row[0]: (row[2], row[1]) for row in self.get_key_columns(cursor, table_name)}
+
+    def get_key_columns(self, cursor, table_name):
+        cursor.execute("""
+            SELECT a1.attname, c2.relname, a2.attname
             FROM pg_constraint con
             LEFT JOIN pg_class c1 ON con.conrelid = c1.oid
             LEFT JOIN pg_class c2 ON con.confrelid = c2.oid
             LEFT JOIN pg_attribute a1 ON c1.oid = a1.attrelid AND a1.attnum = con.conkey[1]
             LEFT JOIN pg_attribute a2 ON c2.oid = a2.attrelid AND a2.attnum = con.confkey[1]
-            WHERE c1.relname = %s AND con.contype = 'f'
-        """, [table_name])
-        return {row[1]: (row[2], row[0]) for row in cursor.fetchall()}
-
-    def get_key_columns(self, cursor, table_name):
-        cursor.execute("""
-            SELECT kcu.column_name, ccu.table_name AS referenced_table, ccu.column_name AS referenced_column
-            FROM information_schema.constraint_column_usage ccu
-            LEFT JOIN information_schema.key_column_usage kcu
-                ON ccu.constraint_catalog = kcu.constraint_catalog
-                    AND ccu.constraint_schema = kcu.constraint_schema
-                    AND ccu.constraint_name = kcu.constraint_name
-            LEFT JOIN information_schema.table_constraints tc
-                ON ccu.constraint_catalog = tc.constraint_catalog
-                    AND ccu.constraint_schema = tc.constraint_schema
-                    AND ccu.constraint_name = tc.constraint_name
-            WHERE kcu.table_name = %s AND tc.constraint_type = 'FOREIGN KEY'
+            WHERE
+                c1.relname = %s AND
+                con.contype = 'f' AND
+                c1.relnamespace = c2.relnamespace AND
+                pg_catalog.pg_table_is_visible(c1.oid)
         """, [table_name])
         return cursor.fetchall()

@@ -159,9 +167,8 @@
                 cl.reloptions
             FROM pg_constraint AS c
             JOIN pg_class AS cl ON c.conrelid = cl.oid
-            JOIN pg_namespace AS ns ON cl.relnamespace = ns.oid
-            WHERE ns.nspname = %s AND cl.relname = %s
-        """, ["public", table_name])
+            WHERE cl.relname = %s AND pg_catalog.pg_table_is_visible(cl.oid)
+        """, [table_name])
         for constraint, columns, kind, used_cols, options in cursor.fetchall():
             constraints[constraint] = {
                 "columns": columns,
@@ -186,7 +193,7 @@
                             pg_get_indexdef(idx.indexrelid)
                     END AS exprdef,
                     CASE am.amname
-                        WHEN 'btree' THEN
+                        WHEN %s THEN
                             CASE (option & 1)
                                 WHEN 1 THEN 'DESC' ELSE 'ASC'
                             END
@@ -200,13 +207,18 @@
                 LEFT JOIN pg_class c2 ON idx.indexrelid = c2.oid
                 LEFT JOIN pg_am am ON c2.relam = am.oid
                 LEFT JOIN pg_attribute attr ON attr.attrelid = c.oid AND attr.attnum = idx.key
-                WHERE c.relname = %s
+                WHERE c.relname = %s AND pg_catalog.pg_table_is_visible(c.oid)
             ) s2
             GROUP BY indexname, indisunique, indisprimary, amname, exprdef, attoptions;
-        """, [table_name])
+        """, [self.index_default_access_method, table_name])
         for index, columns, unique, primary, orders, type_, definition, options in cursor.fetchall():
             if index not in constraints:
-                basic_index = type_ == 'btree' and not index.endswith('_btree') and options is None
+                basic_index = (
+                    type_ == self.index_default_access_method and
+                    # '_btree' references
+                    # django.contrib.postgres.indexes.BTreeIndex.suffix.
+                    not index.endswith('_btree') and options is None
+                )
                 constraints[index] = {
                     "columns": columns if columns != [None] else [],
                     "orders": orders if orders != [None] else [],
('django/db/backends/postgresql', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,19 +1,24 @@
 """
 PostgreSQL database backend for Django.

-Requires psycopg 2: http://initd.org/projects/psycopg2
+Requires psycopg 2: https://www.psycopg.org/
 """

+import asyncio
 import threading
 import warnings
+from contextlib import contextmanager

 from django.conf import settings
 from django.core.exceptions import ImproperlyConfigured
-from django.db import connections
+from django.db import DatabaseError as WrappedDatabaseError, connections
 from django.db.backends.base.base import BaseDatabaseWrapper
-from django.db.utils import DatabaseError as WrappedDatabaseError
+from django.db.backends.utils import (
+    CursorDebugWrapper as BaseCursorDebugWrapper,
+)
+from django.utils.asyncio import async_unsafe
 from django.utils.functional import cached_property
-from django.utils.safestring import SafeText
+from django.utils.safestring import SafeString
 from django.utils.version import get_version_tuple

 try:
@@ -36,15 +41,14 @@


 # Some of these import psycopg2, so import them after checking if it's installed.
-from .client import DatabaseClient                          # NOQA isort:skip
-from .creation import DatabaseCreation                      # NOQA isort:skip
-from .features import DatabaseFeatures                      # NOQA isort:skip
-from .introspection import DatabaseIntrospection            # NOQA isort:skip
-from .operations import DatabaseOperations                  # NOQA isort:skip
-from .schema import DatabaseSchemaEditor                    # NOQA isort:skip
-from .utils import utc_tzinfo_factory                       # NOQA isort:skip
-
-psycopg2.extensions.register_adapter(SafeText, psycopg2.extensions.QuotedString)
+from .client import DatabaseClient  # NOQA
+from .creation import DatabaseCreation  # NOQA
+from .features import DatabaseFeatures  # NOQA
+from .introspection import DatabaseIntrospection  # NOQA
+from .operations import DatabaseOperations  # NOQA
+from .schema import DatabaseSchemaEditor  # NOQA
+
+psycopg2.extensions.register_adapter(SafeString, psycopg2.extensions.QuotedString)
 psycopg2.extras.register_uuid()

 # Register support for inet[] manually so we don't have to handle the Inet()
@@ -82,17 +86,21 @@
         'BigIntegerField': 'bigint',
         'IPAddressField': 'inet',
         'GenericIPAddressField': 'inet',
+        'JSONField': 'jsonb',
         'NullBooleanField': 'boolean',
         'OneToOneField': 'integer',
+        'PositiveBigIntegerField': 'bigint',
         'PositiveIntegerField': 'integer',
         'PositiveSmallIntegerField': 'smallint',
         'SlugField': 'varchar(%(max_length)s)',
+        'SmallAutoField': 'smallserial',
         'SmallIntegerField': 'smallint',
         'TextField': 'text',
         'TimeField': 'time',
         'UUIDField': 'uuid',
     }
     data_type_check_constraints = {
+        'PositiveBigIntegerField': '"%(column)s" >= 0',
         'PositiveIntegerField': '"%(column)s" >= 0',
         'PositiveSmallIntegerField': '"%(column)s" >= 0',
     }
@@ -174,6 +182,7 @@
             conn_params['port'] = settings_dict['PORT']
         return conn_params

+    @async_unsafe
     def get_new_connection(self, conn_params):
         connection = Database.connect(**conn_params)

@@ -191,7 +200,10 @@
             # Set the isolation level to the value from OPTIONS.
             if self.isolation_level != connection.isolation_level:
                 connection.set_session(isolation_level=self.isolation_level)
-
+        # Register dummy loads() to avoid a round trip from psycopg2's decode
+        # to json.dumps() to json.loads(), when using a custom decoder in
+        # JSONField.
+        psycopg2.extras.register_default_jsonb(conn_or_curs=connection, loads=lambda x: x)
         return connection

     def ensure_timezone(self):
@@ -214,6 +226,7 @@
             if not self.get_autocommit():
                 self.connection.commit()

+    @async_unsafe
     def create_cursor(self, name=None):
         if name:
             # In autocommit mode, the cursor will be used outside of a
@@ -221,15 +234,40 @@
             cursor = self.connection.cursor(name, scrollable=False, withhold=self.connection.autocommit)
         else:
             cursor = self.connection.cursor()
-        cursor.tzinfo_factory = utc_tzinfo_factory if settings.USE_TZ else None
+        cursor.tzinfo_factory = self.tzinfo_factory if settings.USE_TZ else None
         return cursor

+    def tzinfo_factory(self, offset):
+        return self.timezone
+
+    @async_unsafe
     def chunked_cursor(self):
         self._named_cursor_idx += 1
+        # Get the current async task
+        # Note that right now this is behind @async_unsafe, so this is
+        # unreachable, but in future we'll start loosening this restriction.
+        # For now, it's here so that every use of "threading" is
+        # also async-compatible.
+        try:
+            if hasattr(asyncio, 'current_task'):
+                # Python 3.7 and up
+                current_task = asyncio.current_task()
+            else:
+                # Python 3.6
+                current_task = asyncio.Task.current_task()
+        except RuntimeError:
+            current_task = None
+        # Current task can be none even if the current_task call didn't error
+        if current_task:
+            task_ident = str(id(current_task))
+        else:
+            task_ident = 'sync'
+        # Use that and the thread ident to get a unique name
         return self._cursor(
-            name='_django_curs_%d_%d' % (
-                # Avoid reusing name in other threads
+            name='_django_curs_%d_%s_%d' % (
+                # Avoid reusing name in other threads / tasks
                 threading.current_thread().ident,
+                task_ident,
                 self._named_cursor_idx,
             )
         )
@@ -243,23 +281,25 @@
         Check constraints by setting them to immediate. Return them to deferred
         afterward.
         """
-        self.cursor().execute('SET CONSTRAINTS ALL IMMEDIATE')
-        self.cursor().execute('SET CONSTRAINTS ALL DEFERRED')
+        with self.cursor() as cursor:
+            cursor.execute('SET CONSTRAINTS ALL IMMEDIATE')
+            cursor.execute('SET CONSTRAINTS ALL DEFERRED')

     def is_usable(self):
         try:
             # Use a psycopg cursor directly, bypassing Django's utilities.
-            self.connection.cursor().execute("SELECT 1")
+            with self.connection.cursor() as cursor:
+                cursor.execute('SELECT 1')
         except Database.Error:
             return False
         else:
             return True

-    @property
-    def _nodb_connection(self):
-        nodb_connection = super()._nodb_connection
+    @contextmanager
+    def _nodb_cursor(self):
         try:
-            nodb_connection.ensure_connection()
+            with super()._nodb_cursor() as cursor:
+                yield cursor
         except (Database.DatabaseError, WrappedDatabaseError):
             warnings.warn(
                 "Normally Django will use a connection to the 'postgres' database "
@@ -271,13 +311,33 @@
             )
             for connection in connections.all():
                 if connection.vendor == 'postgresql' and connection.settings_dict['NAME'] != 'postgres':
-                    return self.__class__(
+                    conn = self.__class__(
                         {**self.settings_dict, 'NAME': connection.settings_dict['NAME']},
                         alias=self.alias,
                     )
-        return nodb_connection
+                    try:
+                        with conn.cursor() as cursor:
+                            yield cursor
+                    finally:
+                        conn.close()
+                    break
+            else:
+                raise

     @cached_property
     def pg_version(self):
         with self.temporary_connection():
             return self.connection.server_version
+
+    def make_debug_cursor(self, cursor):
+        return CursorDebugWrapper(cursor, self)
+
+
+class CursorDebugWrapper(BaseCursorDebugWrapper):
+    def copy_expert(self, sql, file, *args):
+        with self.debug_sql(sql):
+            return self.cursor.copy_expert(sql, file, *args)
+
+    def copy_to(self, file, table, *args, **kwargs):
+        with self.debug_sql(sql='COPY %s TO STDOUT' % table):
+            return self.cursor.copy_to(file, table, *args, **kwargs)
('django/db/backends/postgresql', 'schema.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,19 +2,33 @@

 from django.db.backends.base.schema import BaseDatabaseSchemaEditor
 from django.db.backends.ddl_references import IndexColumns
+from django.db.backends.utils import strip_quotes


 class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):
-
-    sql_alter_column_type = "ALTER COLUMN %(column)s TYPE %(type)s USING %(column)s::%(type)s"

     sql_create_sequence = "CREATE SEQUENCE %(sequence)s"
     sql_delete_sequence = "DROP SEQUENCE IF EXISTS %(sequence)s CASCADE"
     sql_set_sequence_max = "SELECT setval('%(sequence)s', MAX(%(column)s)) FROM %(table)s"
-
-    sql_create_index = "CREATE INDEX %(name)s ON %(table)s%(using)s (%(columns)s)%(extra)s%(condition)s"
+    sql_set_sequence_owner = 'ALTER SEQUENCE %(sequence)s OWNED BY %(table)s.%(column)s'
+
+    sql_create_index = (
+        'CREATE INDEX %(name)s ON %(table)s%(using)s '
+        '(%(columns)s)%(include)s%(extra)s%(condition)s'
+    )
+    sql_create_index_concurrently = (
+        'CREATE INDEX CONCURRENTLY %(name)s ON %(table)s%(using)s '
+        '(%(columns)s)%(include)s%(extra)s%(condition)s'
+    )
     sql_delete_index = "DROP INDEX IF EXISTS %(name)s"
-
+    sql_delete_index_concurrently = "DROP INDEX CONCURRENTLY IF EXISTS %(name)s"
+
+    # Setting the constraint to IMMEDIATE to allow changing data in the same
+    # transaction.
+    sql_create_column_inline_fk = (
+        'CONSTRAINT %(name)s REFERENCES %(to_table)s(%(to_column)s)%(deferrable)s'
+        '; SET CONSTRAINTS %(namespace)s%(name)s IMMEDIATE'
+    )
     # Setting the constraint to IMMEDIATE runs any deferred checks to allow
     # dropping it in the same transaction.
     sql_delete_fk = "SET CONSTRAINTS %(name)s IMMEDIATE; ALTER TABLE %(table)s DROP CONSTRAINT %(name)s"
@@ -22,8 +36,13 @@
     sql_delete_procedure = 'DROP FUNCTION %(procedure)s(%(param_types)s)'

     def quote_value(self, value):
+        if isinstance(value, str):
+            value = value.replace('%', '%%')
+        adapted = psycopg2.extensions.adapt(value)
+        if hasattr(adapted, 'encoding'):
+            adapted.encoding = 'utf8'
         # getquoted() returns a quoted bytestring of the adapted value.
-        return psycopg2.extensions.adapt(value).getquoted().decode()
+        return adapted.getquoted().decode()

     def _field_indexes_sql(self, model, field):
         output = super()._field_indexes_sql(model, field)
@@ -31,6 +50,21 @@
         if like_index_statement is not None:
             output.append(like_index_statement)
         return output
+
+    def _field_data_type(self, field):
+        if field.is_relation:
+            return field.rel_db_type(self.connection)
+        return self.connection.data_types.get(
+            field.get_internal_type(),
+            field.db_type(self.connection),
+        )
+
+    def _field_base_data_types(self, field):
+        # Yield base data types for array fields.
+        if field.base_field.get_internal_type() == 'ArrayField':
+            yield from self._field_base_data_types(field.base_field)
+        else:
+            yield self._field_data_type(field.base_field)

     def _create_like_index_sql(self, model, field):
         """
@@ -49,23 +83,44 @@
             if '[' in db_type:
                 return None
             if db_type.startswith('varchar'):
-                return self._create_index_sql(model, [field], suffix='_like', opclasses=['varchar_pattern_ops'])
+                return self._create_index_sql(
+                    model,
+                    fields=[field],
+                    suffix='_like',
+                    opclasses=['varchar_pattern_ops'],
+                )
             elif db_type.startswith('text'):
-                return self._create_index_sql(model, [field], suffix='_like', opclasses=['text_pattern_ops'])
+                return self._create_index_sql(
+                    model,
+                    fields=[field],
+                    suffix='_like',
+                    opclasses=['text_pattern_ops'],
+                )
         return None

     def _alter_column_type_sql(self, model, old_field, new_field, new_type):
-        """Make ALTER TYPE with SERIAL make sense."""
-        table = model._meta.db_table
-        if new_type.lower() in ("serial", "bigserial"):
-            column = new_field.column
+        self.sql_alter_column_type = 'ALTER COLUMN %(column)s TYPE %(type)s'
+        # Cast when data type changed.
+        using_sql = ' USING %(column)s::%(type)s'
+        new_internal_type = new_field.get_internal_type()
+        old_internal_type = old_field.get_internal_type()
+        if new_internal_type == 'ArrayField' and new_internal_type == old_internal_type:
+            # Compare base data types for array fields.
+            if list(self._field_base_data_types(old_field)) != list(self._field_base_data_types(new_field)):
+                self.sql_alter_column_type += using_sql
+        elif self._field_data_type(old_field) != self._field_data_type(new_field):
+            self.sql_alter_column_type += using_sql
+        # Make ALTER TYPE with SERIAL make sense.
+        table = strip_quotes(model._meta.db_table)
+        serial_fields_map = {'bigserial': 'bigint', 'serial': 'integer', 'smallserial': 'smallint'}
+        if new_type.lower() in serial_fields_map:
+            column = strip_quotes(new_field.column)
             sequence_name = "%s_%s_seq" % (table, column)
-            col_type = "integer" if new_type.lower() == "serial" else "bigint"
             return (
                 (
                     self.sql_alter_column_type % {
                         "column": self.quote_name(column),
-                        "type": col_type,
+                        "type": serial_fields_map[new_type.lower()],
                     },
                     [],
                 ),
@@ -100,8 +155,29 @@
                         },
                         [],
                     ),
+                    (
+                        self.sql_set_sequence_owner % {
+                            'table': self.quote_name(table),
+                            'column': self.quote_name(column),
+                            'sequence': self.quote_name(sequence_name),
+                        },
+                        [],
+                    ),
                 ],
             )
+        elif old_field.db_parameters(connection=self.connection)['type'] in serial_fields_map:
+            # Drop the sequence if migrating away from AutoField.
+            column = strip_quotes(new_field.column)
+            sequence_name = '%s_%s_seq' % (table, column)
+            fragment, _ = super()._alter_column_type_sql(model, old_field, new_field, new_type)
+            return fragment, [
+                (
+                    self.sql_delete_sequence % {
+                        'sequence': self.quote_name(sequence_name),
+                    },
+                    [],
+                ),
+            ]
         else:
             return super()._alter_column_type_sql(model, old_field, new_field, new_type)

@@ -137,3 +213,26 @@
         if opclasses:
             return IndexColumns(table, columns, self.quote_name, col_suffixes=col_suffixes, opclasses=opclasses)
         return super()._index_columns(table, columns, col_suffixes, opclasses)
+
+    def add_index(self, model, index, concurrently=False):
+        self.execute(index.create_sql(model, self, concurrently=concurrently), params=None)
+
+    def remove_index(self, model, index, concurrently=False):
+        self.execute(index.remove_sql(model, self, concurrently=concurrently))
+
+    def _delete_index_sql(self, model, name, sql=None, concurrently=False):
+        sql = self.sql_delete_index_concurrently if concurrently else self.sql_delete_index
+        return super()._delete_index_sql(model, name, sql)
+
+    def _create_index_sql(
+        self, model, *, fields=None, name=None, suffix='', using='',
+        db_tablespace=None, col_suffixes=(), sql=None, opclasses=(),
+        condition=None, concurrently=False, include=None, expressions=None,
+    ):
+        sql = self.sql_create_index if not concurrently else self.sql_create_index_concurrently
+        return super()._create_index_sql(
+            model, fields=fields, name=name, suffix=suffix, using=using,
+            db_tablespace=db_tablespace, col_suffixes=col_suffixes, sql=sql,
+            opclasses=opclasses, condition=condition, include=include,
+            expressions=expressions,
+        )
('django/db/backends/oracle', 'creation.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,8 +1,8 @@
 import sys

 from django.conf import settings
+from django.db import DatabaseError
 from django.db.backends.base.creation import BaseDatabaseCreation
-from django.db.utils import DatabaseError
 from django.utils.crypto import get_random_string
 from django.utils.functional import cached_property

@@ -341,7 +341,7 @@
         password = self._test_settings_get('PASSWORD')
         if password is None and self._test_user_create():
             # Oracle passwords are limited to 30 chars and can't contain symbols.
-            password = get_random_string(length=30)
+            password = get_random_string(30)
         return password

     def _test_database_tblspace(self):
('django/db/backends/oracle', 'client.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,4 @@
 import shutil
-import subprocess

 from django.db.backends.base.client import BaseDatabaseClient

@@ -8,10 +7,21 @@
     executable_name = 'sqlplus'
     wrapper_name = 'rlwrap'

-    def runshell(self):
-        conn_string = self.connection._connect_string()
-        args = [self.executable_name, "-L", conn_string]
-        wrapper_path = shutil.which(self.wrapper_name)
+    @staticmethod
+    def connect_string(settings_dict):
+        from django.db.backends.oracle.utils import dsn
+
+        return '%s/"%s"@%s' % (
+            settings_dict['USER'],
+            settings_dict['PASSWORD'],
+            dsn(settings_dict),
+        )
+
+    @classmethod
+    def settings_to_cmd_args_env(cls, settings_dict, parameters):
+        args = [cls.executable_name, '-L', cls.connect_string(settings_dict)]
+        wrapper_path = shutil.which(cls.wrapper_name)
         if wrapper_path:
             args = [wrapper_path, *args]
-        subprocess.check_call(args)
+        args.extend(parameters)
+        return args, None
('django/db/backends/oracle', 'features.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,28 +1,30 @@
+from django.db import DatabaseError, InterfaceError
 from django.db.backends.base.features import BaseDatabaseFeatures
-from django.db.utils import InterfaceError
 from django.utils.functional import cached_property


 class DatabaseFeatures(BaseDatabaseFeatures):
+    # Oracle crashes with "ORA-00932: inconsistent datatypes: expected - got
+    # BLOB" when grouping by LOBs (#24096).
+    allows_group_by_lob = False
     interprets_empty_strings_as_nulls = True
     has_select_for_update = True
     has_select_for_update_nowait = True
     has_select_for_update_skip_locked = True
     has_select_for_update_of = True
     select_for_update_of_column = True
-    can_return_id_from_insert = True
-    can_introspect_autofield = True
+    can_return_columns_from_insert = True
     supports_subqueries_in_group_by = False
     supports_transactions = True
     supports_timezones = False
     has_native_duration_field = True
     can_defer_constraint_checks = True
     supports_partially_nullable_unique_constraints = False
+    supports_deferrable_unique_constraints = True
     truncates_names = True
     supports_tablespaces = True
     supports_sequence_reset = False
     can_introspect_materialized_views = True
-    can_introspect_time_field = False
     atomic_transactions = False
     supports_combined_alters = False
     nulls_order_largest = True
@@ -53,19 +55,69 @@
     """
     supports_callproc_kwargs = True
     supports_over_clause = True
+    supports_frame_range_fixed_distance = True
     supports_ignore_conflicts = False
     max_query_params = 2**16 - 1
     supports_partial_indexes = False
+    supports_slicing_ordering_in_compound = True
     allows_multiple_constraints_on_same_fields = False
+    supports_boolean_expr_in_select_clause = False
+    supports_primitives_in_json_field = False
+    supports_json_field_contains = False
+    supports_collation_on_textfield = False
+    test_collations = {
+        'ci': 'BINARY_CI',
+        'cs': 'BINARY',
+        'non_default': 'SWEDISH_CI',
+        'swedish_ci': 'SWEDISH_CI',
+    }
+
+    django_test_skips = {
+        "Oracle doesn't support SHA224.": {
+            'db_functions.text.test_sha224.SHA224Tests.test_basic',
+            'db_functions.text.test_sha224.SHA224Tests.test_transform',
+        },
+        "Oracle doesn't support bitwise XOR.": {
+            'expressions.tests.ExpressionOperatorTests.test_lefthand_bitwise_xor',
+            'expressions.tests.ExpressionOperatorTests.test_lefthand_bitwise_xor_null',
+        },
+        "Oracle requires ORDER BY in row_number, ANSI:SQL doesn't.": {
+            'expressions_window.tests.WindowFunctionTests.test_row_number_no_ordering',
+        },
+        'Raises ORA-00600: internal error code on Oracle 18.': {
+            'model_fields.test_jsonfield.TestQuerying.test_usage_in_subquery',
+        },
+    }
+    django_test_expected_failures = {
+        # A bug in Django/cx_Oracle with respect to string handling (#23843).
+        'annotations.tests.NonAggregateAnnotationTestCase.test_custom_functions',
+        'annotations.tests.NonAggregateAnnotationTestCase.test_custom_functions_can_ref_other_functions',
+    }

     @cached_property
-    def has_fetch_offset_support(self):
-        return self.connection.oracle_version >= (12, 2)
+    def introspected_field_types(self):
+        return {
+            **super().introspected_field_types,
+            'GenericIPAddressField': 'CharField',
+            'PositiveBigIntegerField': 'BigIntegerField',
+            'PositiveIntegerField': 'IntegerField',
+            'PositiveSmallIntegerField': 'IntegerField',
+            'SmallIntegerField': 'IntegerField',
+            'TimeField': 'DateTimeField',
+        }

     @cached_property
-    def allow_sliced_subqueries_with_in(self):
-        return self.has_fetch_offset_support
+    def supports_collation_on_charfield(self):
+        with self.connection.cursor() as cursor:
+            try:
+                cursor.execute("SELECT CAST('a' AS VARCHAR2(4001)) FROM dual")
+            except DatabaseError as e:
+                if e.args[0].code == 910:
+                    return False
+                raise
+            return True

     @cached_property
-    def supports_slicing_ordering_in_compound(self):
-        return self.has_fetch_offset_support
+    def has_json_object_function(self):
+        # Oracle < 18 supports JSON_OBJECT() but it's not fully functional.
+        return self.connection.oracle_version >= (18,)
('django/db/backends/oracle', 'operations.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,28 +1,37 @@
 import datetime
-import re
 import uuid
 from functools import lru_cache

 from django.conf import settings
+from django.db import DatabaseError, NotSupportedError
 from django.db.backends.base.operations import BaseDatabaseOperations
 from django.db.backends.utils import strip_quotes, truncate_name
-from django.db.utils import DatabaseError
+from django.db.models import AutoField, Exists, ExpressionWrapper
+from django.db.models.expressions import RawSQL
+from django.db.models.sql.where import WhereNode
 from django.utils import timezone
-from django.utils.encoding import force_bytes
+from django.utils.encoding import force_bytes, force_str
 from django.utils.functional import cached_property
+from django.utils.regex_helper import _lazy_re_compile

 from .base import Database
-from .utils import BulkInsertMapper, InsertIdVar, Oracle_datetime
+from .utils import BulkInsertMapper, InsertVar, Oracle_datetime


 class DatabaseOperations(BaseDatabaseOperations):
-    # Oracle uses NUMBER(11) and NUMBER(19) for integer fields.
+    # Oracle uses NUMBER(5), NUMBER(11), and NUMBER(19) for integer fields.
+    # SmallIntegerField uses NUMBER(11) instead of NUMBER(5), which is used by
+    # SmallAutoField, to preserve backward compatibility.
     integer_field_ranges = {
         'SmallIntegerField': (-99999999999, 99999999999),
         'IntegerField': (-99999999999, 99999999999),
         'BigIntegerField': (-9999999999999999999, 9999999999999999999),
+        'PositiveBigIntegerField': (0, 9999999999999999999),
         'PositiveSmallIntegerField': (0, 99999999999),
         'PositiveIntegerField': (0, 99999999999),
+        'SmallAutoField': (-99999, 99999),
+        'AutoField': (-99999999999, 99999999999),
+        'BigAutoField': (-9999999999999999999, 9999999999999999999),
     }
     set_operators = {**BaseDatabaseOperations.set_operators, 'difference': 'MINUS'}

@@ -56,6 +65,7 @@
     cast_data_types = {
         'AutoField': 'NUMBER(11)',
         'BigAutoField': 'NUMBER(19)',
+        'SmallAutoField': 'NUMBER(5)',
         'TextField': cast_char_field_without_max_length,
     }

@@ -66,6 +76,8 @@
         if lookup_type == 'week_day':
             # TO_CHAR(field, 'D') returns an integer from 1-7, where 1=Sunday.
             return "TO_CHAR(%s, 'D')" % field_name
+        elif lookup_type == 'iso_week_day':
+            return "TO_CHAR(%s - 1, 'D')" % field_name
         elif lookup_type == 'week':
             # IW = ISO week number
             return "TO_CHAR(%s, 'IW')" % field_name
@@ -77,7 +89,8 @@
             # https://docs.oracle.com/en/database/oracle/oracle-database/18/sqlrf/EXTRACT-datetime.html
             return "EXTRACT(%s FROM %s)" % (lookup_type.upper(), field_name)

-    def date_trunc_sql(self, lookup_type, field_name):
+    def date_trunc_sql(self, lookup_type, field_name, tzname=None):
+        field_name = self._convert_field_to_tz(field_name, tzname)
         # https://docs.oracle.com/en/database/oracle/oracle-database/18/sqlrf/ROUND-and-TRUNC-Date-Functions.html
         if lookup_type in ('year', 'month'):
             return "TRUNC(%s, '%s')" % (field_name, lookup_type.upper())
@@ -92,16 +105,30 @@
     # if the time zone name is passed in parameter. Use interpolation instead.
     # https://groups.google.com/forum/#!msg/django-developers/zwQju7hbG78/9l934yelwfsJ
     # This regexp matches all time zone names from the zoneinfo database.
-    _tzname_re = re.compile(r'^[\w/:+-]+$')
+    _tzname_re = _lazy_re_compile(r'^[\w/:+-]+$')
+
+    def _prepare_tzname_delta(self, tzname):
+        if '+' in tzname:
+            return tzname[tzname.find('+'):]
+        elif '-' in tzname:
+            return tzname[tzname.find('-'):]
+        return tzname

     def _convert_field_to_tz(self, field_name, tzname):
-        if not settings.USE_TZ:
+        if not (settings.USE_TZ and tzname):
             return field_name
         if not self._tzname_re.match(tzname):
             raise ValueError("Invalid time zone name: %s" % tzname)
-        # Convert from UTC to local time, returning TIMESTAMP WITH TIME ZONE
-        # and cast it back to TIMESTAMP to strip the TIME ZONE details.
-        return "CAST((FROM_TZ(%s, '0:00') AT TIME ZONE '%s') AS TIMESTAMP)" % (field_name, tzname)
+        # Convert from connection timezone to the local time, returning
+        # TIMESTAMP WITH TIME ZONE and cast it back to TIMESTAMP to strip the
+        # TIME ZONE details.
+        if self.connection.timezone_name != tzname:
+            return "CAST((FROM_TZ(%s, '%s') AT TIME ZONE '%s') AS TIMESTAMP)" % (
+                field_name,
+                self.connection.timezone_name,
+                self._prepare_tzname_delta(tzname),
+            )
+        return field_name

     def datetime_cast_date_sql(self, field_name, tzname):
         field_name = self._convert_field_to_tz(field_name, tzname)
@@ -135,10 +162,11 @@
             sql = "CAST(%s AS DATE)" % field_name  # Cast to DATE removes sub-second precision.
         return sql

-    def time_trunc_sql(self, lookup_type, field_name):
+    def time_trunc_sql(self, lookup_type, field_name, tzname=None):
         # The implementation is similar to `datetime_trunc_sql` as both
         # `DateTimeField` and `TimeField` are stored as TIMESTAMP where
         # the date part of the later is ignored.
+        field_name = self._convert_field_to_tz(field_name, tzname)
         if lookup_type == 'hour':
             sql = "TRUNC(%s, 'HH24')" % field_name
         elif lookup_type == 'minute':
@@ -150,7 +178,7 @@
     def get_db_converters(self, expression):
         converters = super().get_db_converters(expression)
         internal_type = expression.output_field.get_internal_type()
-        if internal_type == 'TextField':
+        if internal_type in ['JSONField', 'TextField']:
             converters.append(self.convert_textfield_value)
         elif internal_type == 'BinaryField':
             converters.append(self.convert_binaryfield_value)
@@ -226,21 +254,24 @@
     def deferrable_sql(self):
         return " DEFERRABLE INITIALLY DEFERRED"

-    def fetch_returned_insert_id(self, cursor):
-        try:
-            value = cursor._insert_id_var.getvalue()
+    def fetch_returned_insert_columns(self, cursor, returning_params):
+        columns = []
+        for param in returning_params:
+            value = param.get_value()
+            if value is None or value == []:
+                # cx_Oracle < 6.3 returns None, >= 6.3 returns empty list.
+                raise DatabaseError(
+                    'The database did not return a new row id. Probably '
+                    '"ORA-1403: no data found" was raised internally but was '
+                    'hidden by the Oracle OCI library (see '
+                    'https://code.djangoproject.com/ticket/28859).'
+                )
             # cx_Oracle < 7 returns value, >= 7 returns list with single value.
-            return int(value[0] if isinstance(value, list) else value)
-        except (IndexError, TypeError):
-            # cx_Oracle < 6.3 returns None, >= 6.3 raises IndexError.
-            raise DatabaseError(
-                'The database did not return a new row id. Probably "ORA-1403: '
-                'no data found" was raised internally but was hidden by the '
-                'Oracle OCI library (see https://code.djangoproject.com/ticket/28859).'
-            )
+            columns.append(value[0] if isinstance(value, list) else value)
+        return tuple(columns)

     def field_cast_sql(self, db_type, internal_type):
-        if db_type and db_type.endswith('LOB'):
+        if db_type and db_type.endswith('LOB') and internal_type != 'JSONField':
             return "DBMS_LOB.SUBSTR(%s)"
         else:
             return "%s"
@@ -250,18 +281,25 @@

     def limit_offset_sql(self, low_mark, high_mark):
         fetch, offset = self._get_limit_offset_params(low_mark, high_mark)
-        return '%s%s' % (
-            (' OFFSET %d ROWS' % offset) if offset else '',
-            (' FETCH FIRST %d ROWS ONLY' % fetch) if fetch else '',
-        )
+        return ' '.join(sql for sql in (
+            ('OFFSET %d ROWS' % offset) if offset else None,
+            ('FETCH FIRST %d ROWS ONLY' % fetch) if fetch else None,
+        ) if sql)

     def last_executed_query(self, cursor, sql, params):
         # https://cx-oracle.readthedocs.io/en/latest/cursor.html#Cursor.statement
         # The DB API definition does not define this attribute.
         statement = cursor.statement
-        # Unlike Psycopg's `query` and MySQLdb`'s `_executed`, CxOracle's
-        # `statement` doesn't contain the query parameters. refs #20010.
-        return super().last_executed_query(cursor, statement, params)
+        # Unlike Psycopg's `query` and MySQLdb`'s `_executed`, cx_Oracle's
+        # `statement` doesn't contain the query parameters. Substitute
+        # parameters manually.
+        if isinstance(params, (tuple, list)):
+            for i, param in enumerate(params):
+                statement = statement.replace(':arg%d' % i, force_str(param, errors='replace'))
+        elif isinstance(params, dict):
+            for key, param in params.items():
+                statement = statement.replace(':%s' % key, force_str(param, errors='replace'))
+        return statement

     def last_insert_id(self, cursor, table_name, pk_name):
         sq_name = self._get_sequence_name(cursor, strip_quotes(table_name), pk_name)
@@ -271,6 +309,8 @@
     def lookup_cast(self, lookup_type, internal_type=None):
         if lookup_type in ('iexact', 'icontains', 'istartswith', 'iendswith'):
             return "UPPER(%s)"
+        if internal_type == 'JSONField' and lookup_type == 'exact':
+            return 'DBMS_LOB.SUBSTR(%s)'
         return "%s"

     def max_in_list_size(self):
@@ -303,9 +343,6 @@
         name = name.replace('%', '%%')
         return name.upper()

-    def random_function_sql(self):
-        return "DBMS_RANDOM.RANDOM"
-
     def regex_lookup(self, lookup_type):
         if lookup_type == 'regex':
             match_option = "'c'"
@@ -313,8 +350,21 @@
             match_option = "'i'"
         return 'REGEXP_LIKE(%%s, %%s, %s)' % match_option

-    def return_insert_id(self):
-        return "RETURNING %s INTO %%s", (InsertIdVar(),)
+    def return_insert_columns(self, fields):
+        if not fields:
+            return '', ()
+        field_names = []
+        params = []
+        for field in fields:
+            field_names.append('%s.%s' % (
+                self.quote_name(field.model._meta.db_table),
+                self.quote_name(field.column),
+            ))
+            params.append(InsertVar(field))
+        return 'RETURNING %s INTO %s' % (
+            ', '.join(field_names),
+            ', '.join(['%s'] * len(params)),
+        ), tuple(params)

     def __foreign_key_constraints(self, table_name, recursive):
         with self.connection.cursor() as cursor:
@@ -355,53 +405,58 @@
         # Django's test suite.
         return lru_cache(maxsize=512)(self.__foreign_key_constraints)

-    def sql_flush(self, style, tables, sequences, allow_cascade=False):
-        if tables:
-            truncated_tables = {table.upper() for table in tables}
-            constraints = set()
-            # Oracle's TRUNCATE CASCADE only works with ON DELETE CASCADE
-            # foreign keys which Django doesn't define. Emulate the
-            # PostgreSQL behavior which truncates all dependent tables by
-            # manually retrieving all foreign key constraints and resolving
-            # dependencies.
-            for table in tables:
-                for foreign_table, constraint in self._foreign_key_constraints(table, recursive=allow_cascade):
-                    if allow_cascade:
-                        truncated_tables.add(foreign_table)
-                    constraints.add((foreign_table, constraint))
-            sql = [
-                "%s %s %s %s %s %s %s %s;" % (
-                    style.SQL_KEYWORD('ALTER'),
-                    style.SQL_KEYWORD('TABLE'),
-                    style.SQL_FIELD(self.quote_name(table)),
-                    style.SQL_KEYWORD('DISABLE'),
-                    style.SQL_KEYWORD('CONSTRAINT'),
-                    style.SQL_FIELD(self.quote_name(constraint)),
-                    style.SQL_KEYWORD('KEEP'),
-                    style.SQL_KEYWORD('INDEX'),
-                ) for table, constraint in constraints
-            ] + [
-                "%s %s %s;" % (
-                    style.SQL_KEYWORD('TRUNCATE'),
-                    style.SQL_KEYWORD('TABLE'),
-                    style.SQL_FIELD(self.quote_name(table)),
-                ) for table in truncated_tables
-            ] + [
-                "%s %s %s %s %s %s;" % (
-                    style.SQL_KEYWORD('ALTER'),
-                    style.SQL_KEYWORD('TABLE'),
-                    style.SQL_FIELD(self.quote_name(table)),
-                    style.SQL_KEYWORD('ENABLE'),
-                    style.SQL_KEYWORD('CONSTRAINT'),
-                    style.SQL_FIELD(self.quote_name(constraint)),
-                ) for table, constraint in constraints
+    def sql_flush(self, style, tables, *, reset_sequences=False, allow_cascade=False):
+        if not tables:
+            return []
+
+        truncated_tables = {table.upper() for table in tables}
+        constraints = set()
+        # Oracle's TRUNCATE CASCADE only works with ON DELETE CASCADE foreign
+        # keys which Django doesn't define. Emulate the PostgreSQL behavior
+        # which truncates all dependent tables by manually retrieving all
+        # foreign key constraints and resolving dependencies.
+        for table in tables:
+            for foreign_table, constraint in self._foreign_key_constraints(table, recursive=allow_cascade):
+                if allow_cascade:
+                    truncated_tables.add(foreign_table)
+                constraints.add((foreign_table, constraint))
+        sql = [
+            '%s %s %s %s %s %s %s %s;' % (
+                style.SQL_KEYWORD('ALTER'),
+                style.SQL_KEYWORD('TABLE'),
+                style.SQL_FIELD(self.quote_name(table)),
+                style.SQL_KEYWORD('DISABLE'),
+                style.SQL_KEYWORD('CONSTRAINT'),
+                style.SQL_FIELD(self.quote_name(constraint)),
+                style.SQL_KEYWORD('KEEP'),
+                style.SQL_KEYWORD('INDEX'),
+            ) for table, constraint in constraints
+        ] + [
+            '%s %s %s;' % (
+                style.SQL_KEYWORD('TRUNCATE'),
+                style.SQL_KEYWORD('TABLE'),
+                style.SQL_FIELD(self.quote_name(table)),
+            ) for table in truncated_tables
+        ] + [
+            '%s %s %s %s %s %s;' % (
+                style.SQL_KEYWORD('ALTER'),
+                style.SQL_KEYWORD('TABLE'),
+                style.SQL_FIELD(self.quote_name(table)),
+                style.SQL_KEYWORD('ENABLE'),
+                style.SQL_KEYWORD('CONSTRAINT'),
+                style.SQL_FIELD(self.quote_name(constraint)),
+            ) for table, constraint in constraints
+        ]
+        if reset_sequences:
+            sequences = [
+                sequence
+                for sequence in self.connection.introspection.sequence_list()
+                if sequence['table'].upper() in truncated_tables
             ]
-            # Since we've just deleted all the rows, running our sequence
-            # ALTER code will reset the sequence to 0.
+            # Since we've just deleted all the rows, running our sequence ALTER
+            # code will reset the sequence to 0.
             sql.extend(self.sequence_reset_by_name_sql(style, sequences))
-            return sql
-        else:
-            return []
+        return sql

     def sequence_reset_by_name_sql(self, style, sequences):
         sql = []
@@ -420,12 +475,11 @@
         return sql

     def sequence_reset_sql(self, style, model_list):
-        from django.db import models
         output = []
         query = self._sequence_reset_sql
         for model in model_list:
             for f in model._meta.local_fields:
-                if isinstance(f, models.AutoField):
+                if isinstance(f, AutoField):
                     no_autofield_sequence_name = self._get_no_autofield_sequence_name(model._meta.db_table)
                     table = self.quote_name(model._meta.db_table)
                     column = self.quote_name(f.column)
@@ -439,18 +493,6 @@
                     # Only one AutoField is allowed per model, so don't
                     # continue to loop
                     break
-            for f in model._meta.many_to_many:
-                if not f.remote_field.through:
-                    no_autofield_sequence_name = self._get_no_autofield_sequence_name(f.m2m_db_table())
-                    table = self.quote_name(f.m2m_db_table())
-                    column = self.quote_name('id')
-                    output.append(query % {
-                        'no_autofield_sequence_name': no_autofield_sequence_name,
-                        'table': table,
-                        'column': column,
-                        'table_name': strip_quotes(table),
-                        'column_name': 'ID',
-                    })
         return output

     def start_transaction_sql(self):
@@ -514,6 +556,9 @@

         return Oracle_datetime(1900, 1, 1, value.hour, value.minute,
                                value.second, value.microsecond)
+
+    def adapt_decimalfield_value(self, value, max_digits=None, decimal_places=None):
+        return value

     def combine_expression(self, connector, sub_expressions):
         lhs, rhs = sub_expressions
@@ -529,6 +574,8 @@
             return 'FLOOR(%(lhs)s / POWER(2, %(rhs)s))' % {'lhs': lhs, 'rhs': rhs}
         elif connector == '^':
             return 'POWER(%s)' % ','.join(sub_expressions)
+        elif connector == '#':
+            raise NotSupportedError('Bitwise XOR is not supported in Oracle.')
         return super().combine_expression(connector, sub_expressions)

     def _get_no_autofield_sequence_name(self, table):
@@ -573,7 +620,8 @@
         if internal_type == 'DateField':
             lhs_sql, lhs_params = lhs
             rhs_sql, rhs_params = rhs
-            return "NUMTODSINTERVAL(TO_NUMBER(%s - %s), 'DAY')" % (lhs_sql, rhs_sql), lhs_params + rhs_params
+            params = (*lhs_params, *rhs_params)
+            return "NUMTODSINTERVAL(TO_NUMBER(%s - %s), 'DAY')" % (lhs_sql, rhs_sql), params
         return super().subtract_temporals(internal_type, lhs, rhs)

     def bulk_batch_size(self, fields, objs):
@@ -582,8 +630,15 @@
             return self.connection.features.max_query_params // len(fields)
         return len(objs)

-    @cached_property
-    def compiler_module(self):
-        if self.connection.features.has_fetch_offset_support:
-            return super().compiler_module
-        return 'django.db.backends.oracle.compiler'
+    def conditional_expression_supported_in_where_clause(self, expression):
+        """
+        Oracle supports only EXISTS(...) or filters in the WHERE clause, others
+        must be compared with True.
+        """
+        if isinstance(expression, (Exists, WhereNode)):
+            return True
+        if isinstance(expression, ExpressionWrapper) and expression.conditional:
+            return self.conditional_expression_supported_in_where_clause(expression.expression)
+        if isinstance(expression, RawSQL) and expression.conditional:
+            return True
+        return False
('django/db/backends/oracle', 'utils.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,17 +3,39 @@
 from .base import Database


-class InsertIdVar:
+class InsertVar:
     """
     A late-binding cursor variable that can be passed to Cursor.execute
     as a parameter, in order to receive the id of the row created by an
     insert statement.
     """
+    types = {
+        'AutoField': int,
+        'BigAutoField': int,
+        'SmallAutoField': int,
+        'IntegerField': int,
+        'BigIntegerField': int,
+        'SmallIntegerField': int,
+        'PositiveBigIntegerField': int,
+        'PositiveSmallIntegerField': int,
+        'PositiveIntegerField': int,
+        'FloatField': Database.NATIVE_FLOAT,
+        'DateTimeField': Database.TIMESTAMP,
+        'DateField': Database.Date,
+        'DecimalField': Database.NUMBER,
+    }
+
+    def __init__(self, field):
+        internal_type = getattr(field, 'target_field', field).get_internal_type()
+        self.db_type = self.types.get(internal_type, str)
+        self.bound_param = None

     def bind_parameter(self, cursor):
-        param = cursor.cursor.var(Database.NUMBER)
-        cursor._insert_id_var = param
-        return param
+        self.bound_param = cursor.cursor.var(self.db_type)
+        return self.bound_param
+
+    def get_value(self):
+        return self.bound_param.getvalue()


 class Oracle_datetime(datetime.datetime):
@@ -33,12 +55,15 @@

 class BulkInsertMapper:
     BLOB = 'TO_BLOB(%s)'
+    CLOB = 'TO_CLOB(%s)'
     DATE = 'TO_DATE(%s)'
     INTERVAL = 'CAST(%s as INTERVAL DAY(9) TO SECOND(6))'
     NUMBER = 'TO_NUMBER(%s)'
     TIMESTAMP = 'TO_TIMESTAMP(%s)'

     types = {
+        'AutoField': NUMBER,
+        'BigAutoField': NUMBER,
         'BigIntegerField': NUMBER,
         'BinaryField': BLOB,
         'BooleanField': NUMBER,
@@ -49,8 +74,18 @@
         'FloatField': NUMBER,
         'IntegerField': NUMBER,
         'NullBooleanField': NUMBER,
+        'PositiveBigIntegerField': NUMBER,
         'PositiveIntegerField': NUMBER,
         'PositiveSmallIntegerField': NUMBER,
+        'SmallAutoField': NUMBER,
         'SmallIntegerField': NUMBER,
+        'TextField': CLOB,
         'TimeField': TIMESTAMP,
     }
+
+
+def dsn(settings_dict):
+    if settings_dict['PORT']:
+        host = settings_dict['HOST'].strip() or 'localhost'
+        return Database.makedsn(host, int(settings_dict['PORT']), settings_dict['NAME'])
+    return settings_dict['NAME']
('django/db/backends/oracle', 'introspection.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -6,28 +6,47 @@
 from django.db.backends.base.introspection import (
     BaseDatabaseIntrospection, FieldInfo as BaseFieldInfo, TableInfo,
 )
-
-FieldInfo = namedtuple('FieldInfo', BaseFieldInfo._fields + ('is_autofield',))
+from django.utils.functional import cached_property
+
+FieldInfo = namedtuple('FieldInfo', BaseFieldInfo._fields + ('is_autofield', 'is_json'))


 class DatabaseIntrospection(BaseDatabaseIntrospection):
+    cache_bust_counter = 1
+
     # Maps type objects to Django Field types.
-    data_types_reverse = {
-        cx_Oracle.BLOB: 'BinaryField',
-        cx_Oracle.CLOB: 'TextField',
-        cx_Oracle.DATETIME: 'DateField',
-        cx_Oracle.FIXED_CHAR: 'CharField',
-        cx_Oracle.FIXED_NCHAR: 'CharField',
-        cx_Oracle.INTERVAL: 'DurationField',
-        cx_Oracle.NATIVE_FLOAT: 'FloatField',
-        cx_Oracle.NCHAR: 'CharField',
-        cx_Oracle.NCLOB: 'TextField',
-        cx_Oracle.NUMBER: 'DecimalField',
-        cx_Oracle.STRING: 'CharField',
-        cx_Oracle.TIMESTAMP: 'DateTimeField',
-    }
-
-    cache_bust_counter = 1
+    @cached_property
+    def data_types_reverse(self):
+        if self.connection.cx_oracle_version < (8,):
+            return {
+                cx_Oracle.BLOB: 'BinaryField',
+                cx_Oracle.CLOB: 'TextField',
+                cx_Oracle.DATETIME: 'DateField',
+                cx_Oracle.FIXED_CHAR: 'CharField',
+                cx_Oracle.FIXED_NCHAR: 'CharField',
+                cx_Oracle.INTERVAL: 'DurationField',
+                cx_Oracle.NATIVE_FLOAT: 'FloatField',
+                cx_Oracle.NCHAR: 'CharField',
+                cx_Oracle.NCLOB: 'TextField',
+                cx_Oracle.NUMBER: 'DecimalField',
+                cx_Oracle.STRING: 'CharField',
+                cx_Oracle.TIMESTAMP: 'DateTimeField',
+            }
+        else:
+            return {
+                cx_Oracle.DB_TYPE_DATE: 'DateField',
+                cx_Oracle.DB_TYPE_BINARY_DOUBLE: 'FloatField',
+                cx_Oracle.DB_TYPE_BLOB: 'BinaryField',
+                cx_Oracle.DB_TYPE_CHAR: 'CharField',
+                cx_Oracle.DB_TYPE_CLOB: 'TextField',
+                cx_Oracle.DB_TYPE_INTERVAL_DS: 'DurationField',
+                cx_Oracle.DB_TYPE_NCHAR: 'CharField',
+                cx_Oracle.DB_TYPE_NCLOB: 'TextField',
+                cx_Oracle.DB_TYPE_NVARCHAR: 'CharField',
+                cx_Oracle.DB_TYPE_NUMBER: 'DecimalField',
+                cx_Oracle.DB_TYPE_TIMESTAMP: 'DateTimeField',
+                cx_Oracle.DB_TYPE_VARCHAR: 'CharField',
+            }

     def get_field_type(self, data_type, description):
         if data_type == cx_Oracle.NUMBER:
@@ -35,6 +54,8 @@
             if scale == 0:
                 if precision > 11:
                     return 'BigAutoField' if description.is_autofield else 'BigIntegerField'
+                elif 1 < precision < 6 and description.is_autofield:
+                    return 'SmallAutoField'
                 elif precision == 1:
                     return 'BooleanField'
                 elif description.is_autofield:
@@ -43,6 +64,8 @@
                     return 'IntegerField'
             elif scale == -127:
                 return 'FloatField'
+        elif data_type == cx_Oracle.NCLOB and description.is_json:
+            return 'JSONField'

         return super().get_field_type(data_type, description)

@@ -72,21 +95,41 @@
         # user_tab_columns gives data default for columns
         cursor.execute("""
             SELECT
-                column_name,
-                data_default,
-                CASE
-                    WHEN char_used IS NULL THEN data_length
-                    ELSE char_length
+                user_tab_cols.column_name,
+                user_tab_cols.data_default,
+                CASE
+                    WHEN user_tab_cols.collation = user_tables.default_collation
+                    THEN NULL
+                    ELSE user_tab_cols.collation
+                END collation,
+                CASE
+                    WHEN user_tab_cols.char_used IS NULL
+                    THEN user_tab_cols.data_length
+                    ELSE user_tab_cols.char_length
                 END as internal_size,
                 CASE
-                    WHEN identity_column = 'YES' THEN 1
-                    ELSE 0
-                END as is_autofield
+                    WHEN user_tab_cols.identity_column = 'YES' THEN 1
+                    ELSE 0
+                END as is_autofield,
+                CASE
+                    WHEN EXISTS (
+                        SELECT  1
+                        FROM user_json_columns
+                        WHERE
+                            user_json_columns.table_name = user_tab_cols.table_name AND
+                            user_json_columns.column_name = user_tab_cols.column_name
+                    )
+                    THEN 1
+                    ELSE 0
+                END as is_json
             FROM user_tab_cols
-            WHERE table_name = UPPER(%s)""", [table_name])
+            LEFT OUTER JOIN
+                user_tables ON user_tables.table_name = user_tab_cols.table_name
+            WHERE user_tab_cols.table_name = UPPER(%s)
+        """, [table_name])
         field_map = {
-            column: (internal_size, default if default != 'NULL' else None, is_autofield)
-            for column, default, internal_size, is_autofield in cursor.fetchall()
+            column: (internal_size, default if default != 'NULL' else None, collation, is_autofield, is_json)
+            for column, default, collation, internal_size, is_autofield, is_json in cursor.fetchall()
         }
         self.cache_bust_counter += 1
         cursor.execute("SELECT * FROM {} WHERE ROWNUM < 2 AND {} > 0".format(
@@ -95,11 +138,11 @@
         description = []
         for desc in cursor.description:
             name = desc[0]
-            internal_size, default, is_autofield = field_map[name]
+            internal_size, default, collation, is_autofield, is_json = field_map[name]
             name = name % {}  # cx_Oracle, for some reason, doubles percent signs.
             description.append(FieldInfo(
                 self.identifier_converter(name), *desc[1:3], internal_size, desc[4] or 0,
-                desc[5] or 0, *desc[6:], default, is_autofield,
+                desc[5] or 0, *desc[6:], default, collation, is_autofield, is_json,
             ))
         return description

@@ -173,6 +216,22 @@
             for row in cursor.fetchall()
         ]

+    def get_primary_key_column(self, cursor, table_name):
+        cursor.execute("""
+            SELECT
+                cols.column_name
+            FROM
+                user_constraints,
+                user_cons_columns cols
+            WHERE
+                user_constraints.constraint_name = cols.constraint_name AND
+                user_constraints.constraint_type = 'P' AND
+                user_constraints.table_name = UPPER(%s) AND
+                cols.position = 1
+        """, [table_name])
+        row = cursor.fetchone()
+        return self.identifier_converter(row[0]) if row else None
+
     def get_constraints(self, cursor, table_name):
         """
         Retrieve any constraints or keys (unique, pk, fk, check, index) across
('django/db/backends/oracle', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -11,9 +11,10 @@

 from django.conf import settings
 from django.core.exceptions import ImproperlyConfigured
-from django.db import utils
+from django.db import IntegrityError
 from django.db.backends.base.base import BaseDatabaseWrapper
-from django.utils.encoding import force_bytes, force_text
+from django.utils.asyncio import async_unsafe
+from django.utils.encoding import force_bytes, force_str
 from django.utils.functional import cached_property


@@ -37,8 +38,8 @@
 _setup_environment([
     # Oracle takes client-side character set encoding from the environment.
     ('NLS_LANG', '.AL32UTF8'),
-    # This prevents unicode from getting mangled by getting encoded into the
-    # potentially non-unicode database character set.
+    # This prevents Unicode from getting mangled by getting encoded into the
+    # potentially non-Unicode database character set.
     ('ORA_NCHAR_LITERAL_REPLACE', 'TRUE'),
 ])

@@ -49,14 +50,14 @@
     raise ImproperlyConfigured("Error loading cx_Oracle module: %s" % e)

 # Some of these import cx_Oracle, so import them after checking if it's installed.
-from .client import DatabaseClient                          # NOQA isort:skip
-from .creation import DatabaseCreation                      # NOQA isort:skip
-from .features import DatabaseFeatures                      # NOQA isort:skip
-from .introspection import DatabaseIntrospection            # NOQA isort:skip
-from .operations import DatabaseOperations                  # NOQA isort:skip
-from .schema import DatabaseSchemaEditor                    # NOQA isort:skip
-from .utils import Oracle_datetime                          # NOQA isort:skip
-from .validation import DatabaseValidation                  # NOQA isort:skip
+from .client import DatabaseClient  # NOQA
+from .creation import DatabaseCreation  # NOQA
+from .features import DatabaseFeatures  # NOQA
+from .introspection import DatabaseIntrospection  # NOQA
+from .operations import DatabaseOperations  # NOQA
+from .schema import DatabaseSchemaEditor  # NOQA
+from .utils import Oracle_datetime, dsn  # NOQA
+from .validation import DatabaseValidation  # NOQA


 @contextmanager
@@ -70,10 +71,18 @@
         #  message = 'ORA-02091: transaction rolled back
         #            'ORA-02291: integrity constraint (TEST_DJANGOTEST.SYS
         #               _C00102056) violated - parent key not found'
+        #            or:
+        #            'ORA-00001: unique constraint (DJANGOTEST.DEFERRABLE_
+        #               PINK_CONSTRAINT) violated
         # Convert that case to Django's IntegrityError exception.
         x = e.args[0]
-        if hasattr(x, 'code') and hasattr(x, 'message') and x.code == 2091 and 'ORA-02291' in x.message:
-            raise utils.IntegrityError(*tuple(e.args))
+        if (
+            hasattr(x, 'code') and
+            hasattr(x, 'message') and
+            x.code == 2091 and
+            ('ORA-02291' in x.message or 'ORA-00001' in x.message)
+        ):
+            raise IntegrityError(*tuple(e.args))
         raise


@@ -114,14 +123,17 @@
         'FilePathField': 'NVARCHAR2(%(max_length)s)',
         'FloatField': 'DOUBLE PRECISION',
         'IntegerField': 'NUMBER(11)',
+        'JSONField': 'NCLOB',
         'BigIntegerField': 'NUMBER(19)',
         'IPAddressField': 'VARCHAR2(15)',
         'GenericIPAddressField': 'VARCHAR2(39)',
         'NullBooleanField': 'NUMBER(1)',
         'OneToOneField': 'NUMBER(11)',
+        'PositiveBigIntegerField': 'NUMBER(19)',
         'PositiveIntegerField': 'NUMBER(11)',
         'PositiveSmallIntegerField': 'NUMBER(11)',
         'SlugField': 'NVARCHAR2(%(max_length)s)',
+        'SmallAutoField': 'NUMBER(5) GENERATED BY DEFAULT ON NULL AS IDENTITY',
         'SmallIntegerField': 'NUMBER(11)',
         'TextField': 'NCLOB',
         'TimeField': 'TIMESTAMP',
@@ -130,7 +142,9 @@
     }
     data_type_check_constraints = {
         'BooleanField': '%(qn_column)s IN (0,1)',
+        'JSONField': '%(qn_column)s IS JSON',
         'NullBooleanField': '%(qn_column)s IN (0,1)',
+        'PositiveBigIntegerField': '%(qn_column)s >= 0',
         'PositiveIntegerField': '%(qn_column)s >= 0',
         'PositiveSmallIntegerField': '%(qn_column)s >= 0',
     }
@@ -202,18 +216,7 @@
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         use_returning_into = self.settings_dict["OPTIONS"].get('use_returning_into', True)
-        self.features.can_return_id_from_insert = use_returning_into
-
-    def _dsn(self):
-        settings_dict = self.settings_dict
-        if not settings_dict['HOST'].strip():
-            settings_dict['HOST'] = 'localhost'
-        if settings_dict['PORT']:
-            return Database.makedsn(settings_dict['HOST'], int(settings_dict['PORT']), settings_dict['NAME'])
-        return settings_dict['NAME']
-
-    def _connect_string(self):
-        return '%s/\\"%s\\"@%s' % (self.settings_dict['USER'], self.settings_dict['PASSWORD'], self._dsn())
+        self.features.can_return_columns_from_insert = use_returning_into

     def get_connection_params(self):
         conn_params = self.settings_dict['OPTIONS'].copy()
@@ -221,11 +224,12 @@
             del conn_params['use_returning_into']
         return conn_params

+    @async_unsafe
     def get_new_connection(self, conn_params):
         return Database.connect(
             user=self.settings_dict['USER'],
             password=self.settings_dict['PASSWORD'],
-            dsn=self._dsn(),
+            dsn=dsn(self.settings_dict),
             **conn_params,
         )

@@ -269,6 +273,7 @@
         if not self.get_autocommit():
             self.commit()

+    @async_unsafe
     def create_cursor(self, name=None):
         return FormatStylePlaceholderCursor(self.connection)

@@ -295,8 +300,9 @@
         Check constraints by setting them to immediate. Return them to deferred
         afterward.
         """
-        self.cursor().execute('SET CONSTRAINTS ALL IMMEDIATE')
-        self.cursor().execute('SET CONSTRAINTS ALL DEFERRED')
+        with self.cursor() as cursor:
+            cursor.execute('SET CONSTRAINTS ALL IMMEDIATE')
+            cursor.execute('SET CONSTRAINTS ALL DEFERRED')

     def is_usable(self):
         try:
@@ -305,6 +311,10 @@
             return False
         else:
             return True
+
+    @cached_property
+    def cx_oracle_version(self):
+        return tuple(int(x) for x in Database.version.split('.'))

     @cached_property
     def oracle_version(self):
@@ -342,7 +352,7 @@
         else:
             # To transmit to the database, we need Unicode if supported
             # To get size right, we must consider bytes.
-            self.force_bytes = force_text(param, cursor.charset, strings_only)
+            self.force_bytes = force_str(param, cursor.charset, strings_only)
             if isinstance(self.force_bytes, str):
                 # We could optimize by only converting up to 4000 bytes here
                 string_size = len(force_bytes(param, cursor.charset, strings_only))
@@ -493,7 +503,10 @@
             # params_dict = {0.75: ':arg0', 2: ':arg1', 'sth': ':arg2'}
             # args = [':arg0', ':arg1', ':arg0', ':arg2', ':arg0']
             # params = {':arg0': 0.75, ':arg1': 2, ':arg2': 'sth'}
-            params_dict = {param: ':arg%d' % i for i, param in enumerate(set(params))}
+            params_dict = {
+                param: ':arg%d' % i
+                for i, param in enumerate(dict.fromkeys(params))
+            }
             args = [params_dict[param] for param in params]
             params = {value: key for key, value in params_dict.items()}
             query = query % tuple(args)
('django/db/backends/oracle', 'schema.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,8 +2,8 @@
 import datetime
 import re

+from django.db import DatabaseError
 from django.db.backends.base.schema import BaseDatabaseSchemaEditor
-from django.db.utils import DatabaseError


 class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):
@@ -14,7 +14,11 @@
     sql_alter_column_not_null = "MODIFY %(column)s NOT NULL"
     sql_alter_column_default = "MODIFY %(column)s DEFAULT %(default)s"
     sql_alter_column_no_default = "MODIFY %(column)s DEFAULT NULL"
+    sql_alter_column_no_default_null = sql_alter_column_no_default
+    sql_alter_column_collate = "MODIFY %(column)s %(type)s%(collation)s"
+
     sql_delete_column = "ALTER TABLE %(table)s DROP COLUMN %(column)s"
+    sql_create_column_inline_fk = 'CONSTRAINT %(name)s REFERENCES %(to_table)s(%(to_column)s)%(deferrable)s'
     sql_delete_table = "DROP TABLE %(table)s CASCADE CONSTRAINTS"
     sql_create_index = "CREATE INDEX %(name)s ON %(table)s (%(columns)s)%(extra)s"

@@ -22,7 +26,7 @@
         if isinstance(value, (datetime.date, datetime.time, datetime.datetime)):
             return "'%s'" % value
         elif isinstance(value, str):
-            return "'%s'" % value.replace("\'", "\'\'")
+            return "'%s'" % value.replace("\'", "\'\'").replace('%', '%%')
         elif isinstance(value, (bytes, bytearray, memoryview)):
             return "'%s'" % value.hex()
         elif isinstance(value, bool):
@@ -89,7 +93,7 @@
         # Make a new field that's like the new one but with a temporary
         # column name.
         new_temp_field = copy.deepcopy(new_field)
-        new_temp_field.null = (new_field.get_internal_type() not in ('AutoField', 'BigAutoField'))
+        new_temp_field.null = (new_field.get_internal_type() not in ('AutoField', 'BigAutoField', 'SmallAutoField'))
         new_temp_field.column = self._generate_temp_name(new_field.column)
         # Add it
         self.add_field(model, new_temp_field)
@@ -120,6 +124,17 @@
         self.remove_field(model, old_field)
         # Rename and possibly make the new field NOT NULL
         super().alter_field(model, new_temp_field, new_field)
+
+    def _alter_column_type_sql(self, model, old_field, new_field, new_type):
+        auto_field_types = {'AutoField', 'BigAutoField', 'SmallAutoField'}
+        # Drop the identity if migrating away from AutoField.
+        if (
+            old_field.get_internal_type() in auto_field_types and
+            new_field.get_internal_type() not in auto_field_types and
+            self._is_identity_column(model._meta.db_table, new_field.column)
+        ):
+            self._drop_identity(model._meta.db_table, new_field.column)
+        return super()._alter_column_type_sql(model, old_field, new_field, new_type)

     def normalize_name(self, name):
         """
@@ -169,3 +184,15 @@
             'table': self.quote_name(table_name),
             'column': self.quote_name(column_name),
         })
+
+    def _get_default_collation(self, table_name):
+        with self.connection.cursor() as cursor:
+            cursor.execute("""
+                SELECT default_collation FROM user_tables WHERE table_name = %s
+            """, [self.normalize_name(table_name)])
+            return cursor.fetchone()[0]
+
+    def _alter_column_collation_sql(self, model, new_field, new_type, new_collation):
+        if new_collation is None:
+            new_collation = self._get_default_collation(model._meta.db_table)
+        return super()._alter_column_collation_sql(model, new_field, new_type, new_collation)
('django/db/backends/sqlite3', 'creation.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,7 @@
 import os
 import shutil
 import sys
+from pathlib import Path

 from django.db.backends.base.creation import BaseDatabaseCreation

@@ -9,7 +10,9 @@

     @staticmethod
     def is_in_memory_db(database_name):
-        return database_name == ':memory:' or 'mode=memory' in database_name
+        return not isinstance(database_name, Path) and (
+            database_name == ':memory:' or 'mode=memory' in database_name
+        )

     def _get_test_db_name(self):
         test_database_name = self.connection.settings_dict['TEST']['NAME'] or ':memory:'
@@ -95,4 +98,6 @@
         sig = [self.connection.settings_dict['NAME']]
         if self.is_in_memory_db(test_database_name):
             sig.append(self.connection.alias)
+        else:
+            sig.append(test_database_name)
         return tuple(sig)
('django/db/backends/sqlite3', 'client.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,12 +1,16 @@
-import subprocess
-
 from django.db.backends.base.client import BaseDatabaseClient


 class DatabaseClient(BaseDatabaseClient):
     executable_name = 'sqlite3'

-    def runshell(self):
-        args = [self.executable_name,
-                self.connection.settings_dict['NAME']]
-        subprocess.check_call(args)
+    @classmethod
+    def settings_to_cmd_args_env(cls, settings_dict, parameters):
+        args = [
+            cls.executable_name,
+            # TODO: Remove str() when dropping support for PY37. args
+            # parameter accepts path-like objects on Windows since Python 3.8.
+            str(settings_dict['NAME']),
+            *parameters,
+        ]
+        return args, None
('django/db/backends/sqlite3', 'features.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,10 @@
-import sys
+import operator
+import platform

+from django.db import transaction
 from django.db.backends.base.features import BaseDatabaseFeatures
+from django.db.utils import OperationalError
+from django.utils.functional import cached_property

 from .base import Database

@@ -15,19 +19,11 @@
     supports_timezones = False
     max_query_params = 999
     supports_mixed_date_datetime_comparisons = False
-    autocommits_when_autocommit_is_off = sys.version_info < (3, 6)
-    can_introspect_autofield = True
-    can_introspect_decimal_field = False
-    can_introspect_duration_field = False
-    can_introspect_positive_integer_field = True
-    can_introspect_small_integer_field = True
-    introspected_big_auto_field_type = 'AutoField'
     supports_transactions = True
     atomic_transactions = False
     can_rollback_ddl = True
-    supports_atomic_references_rename = Database.sqlite_version_info >= (3, 26, 0)
+    can_create_inline_fk = False
     supports_paramstyle_pyformat = False
-    supports_sequence_reset = False
     can_clone_databases = True
     supports_temporal_subtraction = True
     ignores_table_name_case = True
@@ -42,3 +38,76 @@
     supports_pragma_foreign_key_check = Database.sqlite_version_info >= (3, 20, 0)
     can_defer_constraint_checks = supports_pragma_foreign_key_check
     supports_functions_in_partial_indexes = Database.sqlite_version_info >= (3, 15, 0)
+    supports_over_clause = Database.sqlite_version_info >= (3, 25, 0)
+    supports_frame_range_fixed_distance = Database.sqlite_version_info >= (3, 28, 0)
+    supports_aggregate_filter_clause = Database.sqlite_version_info >= (3, 30, 1)
+    supports_order_by_nulls_modifier = Database.sqlite_version_info >= (3, 30, 0)
+    order_by_nulls_first = True
+    supports_json_field_contains = False
+    test_collations = {
+        'ci': 'nocase',
+        'cs': 'binary',
+        'non_default': 'nocase',
+    }
+
+    @cached_property
+    def django_test_skips(self):
+        skips = {
+            'SQLite stores values rounded to 15 significant digits.': {
+                'model_fields.test_decimalfield.DecimalFieldTests.test_fetch_from_db_without_float_rounding',
+            },
+            'SQLite naively remakes the table on field alteration.': {
+                'schema.tests.SchemaTests.test_unique_no_unnecessary_fk_drops',
+                'schema.tests.SchemaTests.test_unique_and_reverse_m2m',
+                'schema.tests.SchemaTests.test_alter_field_default_doesnt_perform_queries',
+                'schema.tests.SchemaTests.test_rename_column_renames_deferred_sql_references',
+            },
+            "SQLite doesn't have a constraint.": {
+                'model_fields.test_integerfield.PositiveIntegerFieldTests.test_negative_values',
+            },
+        }
+        if Database.sqlite_version_info < (3, 27):
+            skips.update({
+                'Nondeterministic failure on SQLite < 3.27.': {
+                    'expressions_window.tests.WindowFunctionTests.test_subquery_row_range_rank',
+                },
+            })
+        if self.connection.is_in_memory_db():
+            skips.update({
+                "the sqlite backend's close() method is a no-op when using an "
+                "in-memory database": {
+                    'servers.test_liveserverthread.LiveServerThreadTest.test_closes_connections',
+                },
+            })
+        return skips
+
+    @cached_property
+    def supports_atomic_references_rename(self):
+        # SQLite 3.28.0 bundled with MacOS 10.15 does not support renaming
+        # references atomically.
+        if platform.mac_ver()[0].startswith('10.15.') and Database.sqlite_version_info == (3, 28, 0):
+            return False
+        return Database.sqlite_version_info >= (3, 26, 0)
+
+    @cached_property
+    def introspected_field_types(self):
+        return{
+            **super().introspected_field_types,
+            'BigAutoField': 'AutoField',
+            'DurationField': 'BigIntegerField',
+            'GenericIPAddressField': 'CharField',
+            'SmallAutoField': 'AutoField',
+        }
+
+    @cached_property
+    def supports_json_field(self):
+        with self.connection.cursor() as cursor:
+            try:
+                with transaction.atomic(self.connection.alias):
+                    cursor.execute('SELECT JSON(\'{"a": "b"}\')')
+            except OperationalError:
+                return False
+        return True
+
+    can_introspect_json_field = property(operator.attrgetter('supports_json_field'))
+    has_json_object_function = property(operator.attrgetter('supports_json_field'))
('django/db/backends/sqlite3', 'operations.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -6,13 +6,11 @@

 from django.conf import settings
 from django.core.exceptions import FieldError
-from django.db import utils
+from django.db import DatabaseError, NotSupportedError, models
 from django.db.backends.base.operations import BaseDatabaseOperations
-from django.db.models import aggregates, fields
 from django.db.models.expressions import Col
 from django.utils import timezone
 from django.utils.dateparse import parse_date, parse_datetime, parse_time
-from django.utils.duration import duration_microseconds
 from django.utils.functional import cached_property


@@ -40,25 +38,29 @@
             return len(objs)

     def check_expression_support(self, expression):
-        bad_fields = (fields.DateField, fields.DateTimeField, fields.TimeField)
-        bad_aggregates = (aggregates.Sum, aggregates.Avg, aggregates.Variance, aggregates.StdDev)
+        bad_fields = (models.DateField, models.DateTimeField, models.TimeField)
+        bad_aggregates = (models.Sum, models.Avg, models.Variance, models.StdDev)
         if isinstance(expression, bad_aggregates):
             for expr in expression.get_source_expressions():
                 try:
                     output_field = expr.output_field
-                except FieldError:
+                except (AttributeError, FieldError):
                     # Not every subexpression has an output_field which is fine
                     # to ignore.
                     pass
                 else:
                     if isinstance(output_field, bad_fields):
-                        raise utils.NotSupportedError(
+                        raise NotSupportedError(
                             'You cannot use Sum, Avg, StdDev, and Variance '
                             'aggregations on date/time fields in sqlite3 '
                             'since date/time is saved as text.'
                         )
-        if isinstance(expression, aggregates.Aggregate) and len(expression.source_expressions) > 1:
-            raise utils.NotSupportedError(
+        if (
+            isinstance(expression, models.Aggregate) and
+            expression.distinct and
+            len(expression.source_expressions) > 1
+        ):
+            raise NotSupportedError(
                 "SQLite doesn't support DISTINCT on aggregate functions "
                 "accepting multiple arguments."
             )
@@ -71,40 +73,47 @@
         """
         return "django_date_extract('%s', %s)" % (lookup_type.lower(), field_name)

-    def date_interval_sql(self, timedelta):
-        return str(duration_microseconds(timedelta))
-
     def format_for_duration_arithmetic(self, sql):
         """Do nothing since formatting is handled in the custom function."""
         return sql

-    def date_trunc_sql(self, lookup_type, field_name):
-        return "django_date_trunc('%s', %s)" % (lookup_type.lower(), field_name)
-
-    def time_trunc_sql(self, lookup_type, field_name):
-        return "django_time_trunc('%s', %s)" % (lookup_type.lower(), field_name)
-
-    def _convert_tzname_to_sql(self, tzname):
-        return "'%s'" % tzname if settings.USE_TZ else 'NULL'
+    def date_trunc_sql(self, lookup_type, field_name, tzname=None):
+        return "django_date_trunc('%s', %s, %s, %s)" % (
+            lookup_type.lower(),
+            field_name,
+            *self._convert_tznames_to_sql(tzname),
+        )
+
+    def time_trunc_sql(self, lookup_type, field_name, tzname=None):
+        return "django_time_trunc('%s', %s, %s, %s)" % (
+            lookup_type.lower(),
+            field_name,
+            *self._convert_tznames_to_sql(tzname),
+        )
+
+    def _convert_tznames_to_sql(self, tzname):
+        if tzname and settings.USE_TZ:
+            return "'%s'" % tzname, "'%s'" % self.connection.timezone_name
+        return 'NULL', 'NULL'

     def datetime_cast_date_sql(self, field_name, tzname):
-        return "django_datetime_cast_date(%s, %s)" % (
-            field_name, self._convert_tzname_to_sql(tzname),
+        return 'django_datetime_cast_date(%s, %s, %s)' % (
+            field_name, *self._convert_tznames_to_sql(tzname),
         )

     def datetime_cast_time_sql(self, field_name, tzname):
-        return "django_datetime_cast_time(%s, %s)" % (
-            field_name, self._convert_tzname_to_sql(tzname),
+        return 'django_datetime_cast_time(%s, %s, %s)' % (
+            field_name, *self._convert_tznames_to_sql(tzname),
         )

     def datetime_extract_sql(self, lookup_type, field_name, tzname):
-        return "django_datetime_extract('%s', %s, %s)" % (
-            lookup_type.lower(), field_name, self._convert_tzname_to_sql(tzname),
+        return "django_datetime_extract('%s', %s, %s, %s)" % (
+            lookup_type.lower(), field_name, *self._convert_tznames_to_sql(tzname),
         )

     def datetime_trunc_sql(self, lookup_type, field_name, tzname):
-        return "django_datetime_trunc('%s', %s, %s)" % (
-            lookup_type.lower(), field_name, self._convert_tzname_to_sql(tzname),
+        return "django_datetime_trunc('%s', %s, %s, %s)" % (
+            lookup_type.lower(), field_name, *self._convert_tznames_to_sql(tzname),
         )

     def time_extract_sql(self, lookup_type, field_name):
@@ -191,7 +200,7 @@
         # Django's test suite.
         return lru_cache(maxsize=512)(self.__references_graph)

-    def sql_flush(self, style, tables, sequences, allow_cascade=False):
+    def sql_flush(self, style, tables, *, reset_sequences=False, allow_cascade=False):
         if tables and allow_cascade:
             # Simulate TRUNCATE CASCADE by recursively collecting the tables
             # referencing the tables to be flushed.
@@ -201,9 +210,28 @@
             style.SQL_KEYWORD('FROM'),
             style.SQL_FIELD(self.quote_name(table))
         ) for table in tables]
-        # Note: No requirement for reset of auto-incremented indices (cf. other
-        # sql_flush() implementations). Just return SQL at this point
+        if reset_sequences:
+            sequences = [{'table': table} for table in tables]
+            sql.extend(self.sequence_reset_by_name_sql(style, sequences))
         return sql
+
+    def sequence_reset_by_name_sql(self, style, sequences):
+        if not sequences:
+            return []
+        return [
+            '%s %s %s %s = 0 %s %s %s (%s);' % (
+                style.SQL_KEYWORD('UPDATE'),
+                style.SQL_TABLE(self.quote_name('sqlite_sequence')),
+                style.SQL_KEYWORD('SET'),
+                style.SQL_FIELD(self.quote_name('seq')),
+                style.SQL_KEYWORD('WHERE'),
+                style.SQL_FIELD(self.quote_name('name')),
+                style.SQL_KEYWORD('IN'),
+                ', '.join([
+                    "'%s'" % sequence_info['table'] for sequence_info in sequences
+                ]),
+            ),
+        ]

     def adapt_datetimefield_value(self, value):
         if value is None:
@@ -308,11 +336,13 @@
         # function that's registered in connect().
         if connector == '^':
             return 'POWER(%s)' % ','.join(sub_expressions)
+        elif connector == '#':
+            return 'BITXOR(%s)' % ','.join(sub_expressions)
         return super().combine_expression(connector, sub_expressions)

     def combine_duration_expression(self, connector, sub_expressions):
         if connector not in ['+', '-']:
-            raise utils.DatabaseError('Invalid connector for timedelta: %s.' % connector)
+            raise DatabaseError('Invalid connector for timedelta: %s.' % connector)
         fn_params = ["'%s'" % connector] + sub_expressions
         if len(fn_params) > 3:
             raise ValueError('Too many params for timedelta operations.')
@@ -325,9 +355,10 @@
     def subtract_temporals(self, internal_type, lhs, rhs):
         lhs_sql, lhs_params = lhs
         rhs_sql, rhs_params = rhs
+        params = (*lhs_params, *rhs_params)
         if internal_type == 'TimeField':
-            return "django_time_diff(%s, %s)" % (lhs_sql, rhs_sql), lhs_params + rhs_params
-        return "django_timestamp_diff(%s, %s)" % (lhs_sql, rhs_sql), lhs_params + rhs_params
+            return 'django_time_diff(%s, %s)' % (lhs_sql, rhs_sql), params
+        return 'django_timestamp_diff(%s, %s)' % (lhs_sql, rhs_sql), params

     def insert_statement(self, ignore_conflicts=False):
         return 'INSERT OR IGNORE INTO' if ignore_conflicts else super().insert_statement(ignore_conflicts)
('django/db/backends/sqlite3', 'introspection.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -6,17 +6,18 @@
 from django.db.backends.base.introspection import (
     BaseDatabaseIntrospection, FieldInfo as BaseFieldInfo, TableInfo,
 )
-from django.db.models.indexes import Index
-
-FieldInfo = namedtuple('FieldInfo', BaseFieldInfo._fields + ('pk',))
-
-field_size_re = re.compile(r'^\s*(?:var)?char\s*\(\s*(\d+)\s*\)\s*$')
+from django.db.models import Index
+from django.utils.regex_helper import _lazy_re_compile
+
+FieldInfo = namedtuple('FieldInfo', BaseFieldInfo._fields + ('pk', 'has_json_constraint'))
+
+field_size_re = _lazy_re_compile(r'^\s*(?:var)?char\s*\(\s*(\d+)\s*\)\s*$')


 def get_field_size(name):
     """ Extract the size number from a "varchar(11)" type name """
     m = field_size_re.search(name)
-    return int(m.group(1)) if m else None
+    return int(m[1]) if m else None


 # This light wrapper "fakes" a dictionary interface, because some SQLite data
@@ -36,6 +37,7 @@
         'integer': 'IntegerField',
         'bigint': 'BigIntegerField',
         'integer unsigned': 'PositiveIntegerField',
+        'bigint unsigned': 'PositiveBigIntegerField',
         'decimal': 'DecimalField',
         'real': 'FloatField',
         'text': 'TextField',
@@ -57,10 +59,12 @@

     def get_field_type(self, data_type, description):
         field_type = super().get_field_type(data_type, description)
-        if description.pk and field_type in {'BigIntegerField', 'IntegerField'}:
-            # No support for BigAutoField as SQLite treats all integer primary
-            # keys as signed 64-bit integers.
+        if description.pk and field_type in {'BigIntegerField', 'IntegerField', 'SmallIntegerField'}:
+            # No support for BigAutoField or SmallAutoField as SQLite treats
+            # all integer primary keys as signed 64-bit integers.
             return 'AutoField'
+        if description.has_json_constraint:
+            return 'JSONField'
         return field_type

     def get_table_list(self, cursor):
@@ -78,18 +82,30 @@
         Return a description of the table with the DB-API cursor.description
         interface.
         """
+        cursor.execute('PRAGMA table_info(%s)' % self.connection.ops.quote_name(table_name))
+        table_info = cursor.fetchall()
+        collations = self._get_column_collations(cursor, table_name)
+        json_columns = set()
+        if self.connection.features.can_introspect_json_field:
+            for line in table_info:
+                column = line[1]
+                json_constraint_sql = '%%json_valid("%s")%%' % column
+                has_json_constraint = cursor.execute("""
+                    SELECT sql
+                    FROM sqlite_master
+                    WHERE
+                        type = 'table' AND
+                        name = %s AND
+                        sql LIKE %s
+                """, [table_name, json_constraint_sql]).fetchone()
+                if has_json_constraint:
+                    json_columns.add(column)
         return [
             FieldInfo(
-                info['name'],
-                info['type'],
-                None,
-                info['size'],
-                None,
-                None,
-                info['null_ok'],
-                info['default'],
-                info['pk'] == 1,
-            ) for info in self._table_info(cursor, table_name)
+                name, data_type, None, get_field_size(data_type), None, None,
+                not notnull, default, collations.get(name), pk == 1, name in json_columns
+            )
+            for cid, name, data_type, notnull, default, pk in table_info
         ]

     def get_sequences(self, cursor, table_name, table_fields=()):
@@ -132,7 +148,7 @@
             if field_desc.startswith("FOREIGN KEY"):
                 # Find name of the target FK field
                 m = re.match(r'FOREIGN KEY\s*\(([^\)]*)\).*', field_desc, re.I)
-                field_name = m.groups()[0].strip('"')
+                field_name = m[1].strip('"')
             else:
                 field_name = field_desc.split()[0].strip('"')

@@ -203,20 +219,8 @@
             field_desc = field_desc.strip()
             m = re.match(r'(?:(?:["`\[])(.*)(?:["`\]])|(\w+)).*PRIMARY KEY.*', field_desc)
             if m:
-                return m.group(1) if m.group(1) else m.group(2)
+                return m[1] if m[1] else m[2]
         return None
-
-    def _table_info(self, cursor, name):
-        cursor.execute('PRAGMA table_info(%s)' % self.connection.ops.quote_name(name))
-        # cid, name, type, notnull, default_value, pk
-        return [{
-            'name': field[1],
-            'type': field[2],
-            'size': get_field_size(field[2]),
-            'null_ok': not field[3],
-            'default': field[4],
-            'pk': field[5],  # undocumented
-        } for field in cursor.fetchall()]

     def _get_foreign_key_constraints(self, cursor, table_name):
         constraints = {}
@@ -409,12 +413,12 @@
                     }
                 constraints[index]['columns'].append(column)
             # Add type and column orders for indexes
-            if constraints[index]['index'] and not constraints[index]['unique']:
+            if constraints[index]['index']:
                 # SQLite doesn't support any index type other than b-tree
                 constraints[index]['type'] = Index.suffix
-                order_info = sql.split('(')[-1].split(')')[0].split(',')
-                orders = ['DESC' if info.endswith('DESC') else 'ASC' for info in order_info]
-                constraints[index]['orders'] = orders
+                orders = self._get_index_columns_orders(sql)
+                if orders is not None:
+                    constraints[index]['orders'] = orders
         # Get the PK
         pk_column = self.get_primary_key_column(cursor, table_name)
         if pk_column:
@@ -432,3 +436,35 @@
             }
         constraints.update(self._get_foreign_key_constraints(cursor, table_name))
         return constraints
+
+    def _get_index_columns_orders(self, sql):
+        tokens = sqlparse.parse(sql)[0]
+        for token in tokens:
+            if isinstance(token, sqlparse.sql.Parenthesis):
+                columns = str(token).strip('()').split(', ')
+                return ['DESC' if info.endswith('DESC') else 'ASC' for info in columns]
+        return None
+
+    def _get_column_collations(self, cursor, table_name):
+        row = cursor.execute("""
+            SELECT sql
+            FROM sqlite_master
+            WHERE type = 'table' AND name = %s
+        """, [table_name]).fetchone()
+        if not row:
+            return {}
+
+        sql = row[0]
+        columns = str(sqlparse.parse(sql)[0][-1]).strip('()').split(', ')
+        collations = {}
+        for column in columns:
+            tokens = column[1:].split()
+            column_name = tokens[0].strip('"')
+            for index, token in enumerate(tokens):
+                if token == 'COLLATE':
+                    collation = tokens[index + 1]
+                    break
+            else:
+                collation = None
+            collations[column_name] = collation
+        return collations
('django/db/backends/sqlite3', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,8 +4,10 @@
 import datetime
 import decimal
 import functools
+import hashlib
 import math
 import operator
+import random
 import re
 import statistics
 import warnings
@@ -15,19 +17,22 @@
 import pytz

 from django.core.exceptions import ImproperlyConfigured
-from django.db import utils
+from django.db import IntegrityError
 from django.db.backends import utils as backend_utils
 from django.db.backends.base.base import BaseDatabaseWrapper
 from django.utils import timezone
+from django.utils.asyncio import async_unsafe
 from django.utils.dateparse import parse_datetime, parse_time
 from django.utils.duration import duration_microseconds
-
-from .client import DatabaseClient                          # isort:skip
-from .creation import DatabaseCreation                      # isort:skip
-from .features import DatabaseFeatures                      # isort:skip
-from .introspection import DatabaseIntrospection            # isort:skip
-from .operations import DatabaseOperations                  # isort:skip
-from .schema import DatabaseSchemaEditor                    # isort:skip
+from django.utils.regex_helper import _lazy_re_compile
+from django.utils.version import PY38
+
+from .client import DatabaseClient
+from .creation import DatabaseCreation
+from .features import DatabaseFeatures
+from .introspection import DatabaseIntrospection
+from .operations import DatabaseOperations
+from .schema import DatabaseSchemaEditor


 def decoder(conv_func):
@@ -59,8 +64,10 @@


 def check_sqlite_version():
-    if Database.sqlite_version_info < (3, 8, 3):
-        raise ImproperlyConfigured('SQLite 3.8.3 or later is required (found %s).' % Database.sqlite_version)
+    if Database.sqlite_version_info < (3, 9, 0):
+        raise ImproperlyConfigured(
+            'SQLite 3.9.0 or later is required (found %s).' % Database.sqlite_version
+        )


 check_sqlite_version()
@@ -69,7 +76,6 @@
 Database.register_converter("time", decoder(parse_time))
 Database.register_converter("datetime", decoder(parse_datetime))
 Database.register_converter("timestamp", decoder(parse_datetime))
-Database.register_converter("TIMESTAMP", decoder(parse_datetime))

 Database.register_adapter(decimal.Decimal, str)

@@ -97,23 +103,29 @@
         'BigIntegerField': 'bigint',
         'IPAddressField': 'char(15)',
         'GenericIPAddressField': 'char(39)',
+        'JSONField': 'text',
         'NullBooleanField': 'bool',
         'OneToOneField': 'integer',
+        'PositiveBigIntegerField': 'bigint unsigned',
         'PositiveIntegerField': 'integer unsigned',
         'PositiveSmallIntegerField': 'smallint unsigned',
         'SlugField': 'varchar(%(max_length)s)',
+        'SmallAutoField': 'integer',
         'SmallIntegerField': 'smallint',
         'TextField': 'text',
         'TimeField': 'time',
         'UUIDField': 'char(32)',
     }
     data_type_check_constraints = {
+        'PositiveBigIntegerField': '"%(column)s" >= 0',
+        'JSONField': '(JSON_VALID("%(column)s") OR "%(column)s" IS NULL)',
         'PositiveIntegerField': '"%(column)s" >= 0',
         'PositiveSmallIntegerField': '"%(column)s" >= 0',
     }
     data_types_suffix = {
         'AutoField': 'AUTOINCREMENT',
         'BigAutoField': 'AUTOINCREMENT',
+        'SmallAutoField': 'AUTOINCREMENT',
     }
     # SQLite requires LIKE statements to include an ESCAPE clause if the value
     # being escaped has a percent or underscore in it.
@@ -169,7 +181,9 @@
                 "settings.DATABASES is improperly configured. "
                 "Please supply the NAME value.")
         kwargs = {
-            'database': settings_dict['NAME'],
+            # TODO: Remove str() when dropping support for PY36.
+            # https://bugs.python.org/issue33496
+            'database': str(settings_dict['NAME']),
             'detect_types': Database.PARSE_DECLTYPES | Database.PARSE_COLNAMES,
             **settings_dict['OPTIONS'],
         }
@@ -190,43 +204,62 @@
         kwargs.update({'check_same_thread': False, 'uri': True})
         return kwargs

+    @async_unsafe
     def get_new_connection(self, conn_params):
         conn = Database.connect(**conn_params)
-        conn.create_function("django_date_extract", 2, _sqlite_datetime_extract)
-        conn.create_function("django_date_trunc", 2, _sqlite_date_trunc)
-        conn.create_function("django_datetime_cast_date", 2, _sqlite_datetime_cast_date)
-        conn.create_function("django_datetime_cast_time", 2, _sqlite_datetime_cast_time)
-        conn.create_function("django_datetime_extract", 3, _sqlite_datetime_extract)
-        conn.create_function("django_datetime_trunc", 3, _sqlite_datetime_trunc)
-        conn.create_function("django_time_extract", 2, _sqlite_time_extract)
-        conn.create_function("django_time_trunc", 2, _sqlite_time_trunc)
-        conn.create_function("django_time_diff", 2, _sqlite_time_diff)
-        conn.create_function("django_timestamp_diff", 2, _sqlite_timestamp_diff)
-        conn.create_function("django_format_dtdelta", 3, _sqlite_format_dtdelta)
-        conn.create_function('regexp', 2, _sqlite_regexp)
-        conn.create_function('ACOS', 1, none_guard(math.acos))
-        conn.create_function('ASIN', 1, none_guard(math.asin))
-        conn.create_function('ATAN', 1, none_guard(math.atan))
-        conn.create_function('ATAN2', 2, none_guard(math.atan2))
-        conn.create_function('CEILING', 1, none_guard(math.ceil))
-        conn.create_function('COS', 1, none_guard(math.cos))
-        conn.create_function('COT', 1, none_guard(lambda x: 1 / math.tan(x)))
-        conn.create_function('DEGREES', 1, none_guard(math.degrees))
-        conn.create_function('EXP', 1, none_guard(math.exp))
-        conn.create_function('FLOOR', 1, none_guard(math.floor))
-        conn.create_function('LN', 1, none_guard(math.log))
-        conn.create_function('LOG', 2, none_guard(lambda x, y: math.log(y, x)))
-        conn.create_function('LPAD', 3, _sqlite_lpad)
-        conn.create_function('MOD', 2, none_guard(math.fmod))
-        conn.create_function('PI', 0, lambda: math.pi)
-        conn.create_function('POWER', 2, none_guard(operator.pow))
-        conn.create_function('RADIANS', 1, none_guard(math.radians))
-        conn.create_function('REPEAT', 2, none_guard(operator.mul))
-        conn.create_function('REVERSE', 1, none_guard(lambda x: x[::-1]))
-        conn.create_function('RPAD', 3, _sqlite_rpad)
-        conn.create_function('SIN', 1, none_guard(math.sin))
-        conn.create_function('SQRT', 1, none_guard(math.sqrt))
-        conn.create_function('TAN', 1, none_guard(math.tan))
+        if PY38:
+            create_deterministic_function = functools.partial(
+                conn.create_function,
+                deterministic=True,
+            )
+        else:
+            create_deterministic_function = conn.create_function
+        create_deterministic_function('django_date_extract', 2, _sqlite_datetime_extract)
+        create_deterministic_function('django_date_trunc', 4, _sqlite_date_trunc)
+        create_deterministic_function('django_datetime_cast_date', 3, _sqlite_datetime_cast_date)
+        create_deterministic_function('django_datetime_cast_time', 3, _sqlite_datetime_cast_time)
+        create_deterministic_function('django_datetime_extract', 4, _sqlite_datetime_extract)
+        create_deterministic_function('django_datetime_trunc', 4, _sqlite_datetime_trunc)
+        create_deterministic_function('django_time_extract', 2, _sqlite_time_extract)
+        create_deterministic_function('django_time_trunc', 4, _sqlite_time_trunc)
+        create_deterministic_function('django_time_diff', 2, _sqlite_time_diff)
+        create_deterministic_function('django_timestamp_diff', 2, _sqlite_timestamp_diff)
+        create_deterministic_function('django_format_dtdelta', 3, _sqlite_format_dtdelta)
+        create_deterministic_function('regexp', 2, _sqlite_regexp)
+        create_deterministic_function('ACOS', 1, none_guard(math.acos))
+        create_deterministic_function('ASIN', 1, none_guard(math.asin))
+        create_deterministic_function('ATAN', 1, none_guard(math.atan))
+        create_deterministic_function('ATAN2', 2, none_guard(math.atan2))
+        create_deterministic_function('BITXOR', 2, none_guard(operator.xor))
+        create_deterministic_function('CEILING', 1, none_guard(math.ceil))
+        create_deterministic_function('COS', 1, none_guard(math.cos))
+        create_deterministic_function('COT', 1, none_guard(lambda x: 1 / math.tan(x)))
+        create_deterministic_function('DEGREES', 1, none_guard(math.degrees))
+        create_deterministic_function('EXP', 1, none_guard(math.exp))
+        create_deterministic_function('FLOOR', 1, none_guard(math.floor))
+        create_deterministic_function('LN', 1, none_guard(math.log))
+        create_deterministic_function('LOG', 2, none_guard(lambda x, y: math.log(y, x)))
+        create_deterministic_function('LPAD', 3, _sqlite_lpad)
+        create_deterministic_function('MD5', 1, none_guard(lambda x: hashlib.md5(x.encode()).hexdigest()))
+        create_deterministic_function('MOD', 2, none_guard(math.fmod))
+        create_deterministic_function('PI', 0, lambda: math.pi)
+        create_deterministic_function('POWER', 2, none_guard(operator.pow))
+        create_deterministic_function('RADIANS', 1, none_guard(math.radians))
+        create_deterministic_function('REPEAT', 2, none_guard(operator.mul))
+        create_deterministic_function('REVERSE', 1, none_guard(lambda x: x[::-1]))
+        create_deterministic_function('RPAD', 3, _sqlite_rpad)
+        create_deterministic_function('SHA1', 1, none_guard(lambda x: hashlib.sha1(x.encode()).hexdigest()))
+        create_deterministic_function('SHA224', 1, none_guard(lambda x: hashlib.sha224(x.encode()).hexdigest()))
+        create_deterministic_function('SHA256', 1, none_guard(lambda x: hashlib.sha256(x.encode()).hexdigest()))
+        create_deterministic_function('SHA384', 1, none_guard(lambda x: hashlib.sha384(x.encode()).hexdigest()))
+        create_deterministic_function('SHA512', 1, none_guard(lambda x: hashlib.sha512(x.encode()).hexdigest()))
+        create_deterministic_function('SIGN', 1, none_guard(lambda x: (x > 0) - (x < 0)))
+        create_deterministic_function('SIN', 1, none_guard(math.sin))
+        create_deterministic_function('SQRT', 1, none_guard(math.sqrt))
+        create_deterministic_function('TAN', 1, none_guard(math.tan))
+        # Don't use the built-in RANDOM() function because it returns a value
+        # in the range [2^63, 2^63 - 1] instead of [0, 1).
+        conn.create_function('RAND', 0, random.random)
         conn.create_aggregate('STDDEV_POP', 1, list_aggregate(statistics.pstdev))
         conn.create_aggregate('STDDEV_SAMP', 1, list_aggregate(statistics.stdev))
         conn.create_aggregate('VAR_POP', 1, list_aggregate(statistics.pvariance))
@@ -240,6 +273,7 @@
     def create_cursor(self, name=None):
         return self.connection.cursor(factory=SQLiteCursorWrapper)

+    @async_unsafe
     def close(self):
         self.validate_thread_sharing()
         # If database is in memory, closing the connection destroys the
@@ -278,7 +312,8 @@
         return not bool(enabled)

     def enable_constraint_checking(self):
-        self.cursor().execute('PRAGMA foreign_keys = ON')
+        with self.cursor() as cursor:
+            cursor.execute('PRAGMA foreign_keys = ON')

     def check_constraints(self, table_names=None):
         """
@@ -291,26 +326,31 @@
         if self.features.supports_pragma_foreign_key_check:
             with self.cursor() as cursor:
                 if table_names is None:
-                    violations = self.cursor().execute('PRAGMA foreign_key_check').fetchall()
+                    violations = cursor.execute('PRAGMA foreign_key_check').fetchall()
                 else:
                     violations = chain.from_iterable(
-                        cursor.execute('PRAGMA foreign_key_check(%s)' % table_name).fetchall()
+                        cursor.execute(
+                            'PRAGMA foreign_key_check(%s)'
+                            % self.ops.quote_name(table_name)
+                        ).fetchall()
                         for table_name in table_names
                     )
                 # See https://www.sqlite.org/pragma.html#pragma_foreign_key_check
                 for table_name, rowid, referenced_table_name, foreign_key_index in violations:
                     foreign_key = cursor.execute(
-                        'PRAGMA foreign_key_list(%s)' % table_name
+                        'PRAGMA foreign_key_list(%s)' % self.ops.quote_name(table_name)
                     ).fetchall()[foreign_key_index]
                     column_name, referenced_column_name = foreign_key[3:5]
                     primary_key_column_name = self.introspection.get_primary_key_column(cursor, table_name)
                     primary_key_value, bad_value = cursor.execute(
                         'SELECT %s, %s FROM %s WHERE rowid = %%s' % (
-                            primary_key_column_name, column_name, table_name
+                            self.ops.quote_name(primary_key_column_name),
+                            self.ops.quote_name(column_name),
+                            self.ops.quote_name(table_name),
                         ),
                         (rowid,),
                     ).fetchone()
-                    raise utils.IntegrityError(
+                    raise IntegrityError(
                         "The row in table '%s' with primary key '%s' has an "
                         "invalid foreign key: %s.%s contains a value '%s' that "
                         "does not have a corresponding value in %s.%s." % (
@@ -342,7 +382,7 @@
                             )
                         )
                         for bad_row in cursor.fetchall():
-                            raise utils.IntegrityError(
+                            raise IntegrityError(
                                 "The row in table '%s' with primary key '%s' has an "
                                 "invalid foreign key: %s.%s contains a value '%s' that "
                                 "does not have a corresponding value in %s.%s." % (
@@ -367,7 +407,7 @@
         return self.creation.is_in_memory_db(self.settings_dict['NAME'])


-FORMAT_QMARK_REGEX = re.compile(r'(?<!%)%s')
+FORMAT_QMARK_REGEX = _lazy_re_compile(r'(?<!%)%s')


 class SQLiteCursorWrapper(Database.Cursor):
@@ -390,20 +430,30 @@
         return FORMAT_QMARK_REGEX.sub('?', query).replace('%%', '%')


-def _sqlite_datetime_parse(dt, tzname=None):
+def _sqlite_datetime_parse(dt, tzname=None, conn_tzname=None):
     if dt is None:
         return None
     try:
         dt = backend_utils.typecast_timestamp(dt)
     except (TypeError, ValueError):
         return None
-    if tzname is not None:
+    if conn_tzname:
+        dt = dt.replace(tzinfo=pytz.timezone(conn_tzname))
+    if tzname is not None and tzname != conn_tzname:
+        sign_index = tzname.find('+') + tzname.find('-') + 1
+        if sign_index > -1:
+            sign = tzname[sign_index]
+            tzname, offset = tzname.split(sign)
+            if offset:
+                hours, minutes = offset.split(':')
+                offset_delta = datetime.timedelta(hours=int(hours), minutes=int(minutes))
+                dt += offset_delta if sign == '+' else -offset_delta
         dt = timezone.localtime(dt, pytz.timezone(tzname))
     return dt


-def _sqlite_date_trunc(lookup_type, dt):
-    dt = _sqlite_datetime_parse(dt)
+def _sqlite_date_trunc(lookup_type, dt, tzname, conn_tzname):
+    dt = _sqlite_datetime_parse(dt, tzname, conn_tzname)
     if dt is None:
         return None
     if lookup_type == 'year':
@@ -420,13 +470,17 @@
         return "%i-%02i-%02i" % (dt.year, dt.month, dt.day)


-def _sqlite_time_trunc(lookup_type, dt):
-    if dt is None:
-        return None
-    try:
-        dt = backend_utils.typecast_time(dt)
-    except (ValueError, TypeError):
-        return None
+def _sqlite_time_trunc(lookup_type, dt, tzname, conn_tzname):
+    if dt is None:
+        return None
+    dt_parsed = _sqlite_datetime_parse(dt, tzname, conn_tzname)
+    if dt_parsed is None:
+        try:
+            dt = backend_utils.typecast_time(dt)
+        except (ValueError, TypeError):
+            return None
+    else:
+        dt = dt_parsed
     if lookup_type == 'hour':
         return "%02i:00:00" % dt.hour
     elif lookup_type == 'minute':
@@ -435,26 +489,28 @@
         return "%02i:%02i:%02i" % (dt.hour, dt.minute, dt.second)


-def _sqlite_datetime_cast_date(dt, tzname):
-    dt = _sqlite_datetime_parse(dt, tzname)
+def _sqlite_datetime_cast_date(dt, tzname, conn_tzname):
+    dt = _sqlite_datetime_parse(dt, tzname, conn_tzname)
     if dt is None:
         return None
     return dt.date().isoformat()


-def _sqlite_datetime_cast_time(dt, tzname):
-    dt = _sqlite_datetime_parse(dt, tzname)
+def _sqlite_datetime_cast_time(dt, tzname, conn_tzname):
+    dt = _sqlite_datetime_parse(dt, tzname, conn_tzname)
     if dt is None:
         return None
     return dt.time().isoformat()


-def _sqlite_datetime_extract(lookup_type, dt, tzname=None):
-    dt = _sqlite_datetime_parse(dt, tzname)
+def _sqlite_datetime_extract(lookup_type, dt, tzname=None, conn_tzname=None):
+    dt = _sqlite_datetime_parse(dt, tzname, conn_tzname)
     if dt is None:
         return None
     if lookup_type == 'week_day':
         return (dt.isoweekday() % 7) + 1
+    elif lookup_type == 'iso_week_day':
+        return dt.isoweekday()
     elif lookup_type == 'week':
         return dt.isocalendar()[1]
     elif lookup_type == 'quarter':
@@ -465,8 +521,8 @@
         return getattr(dt, lookup_type)


-def _sqlite_datetime_trunc(lookup_type, dt, tzname):
-    dt = _sqlite_datetime_parse(dt, tzname)
+def _sqlite_datetime_trunc(lookup_type, dt, tzname, conn_tzname):
+    dt = _sqlite_datetime_parse(dt, tzname, conn_tzname)
     if dt is None:
         return None
     if lookup_type == 'year':
('django/db/backends/sqlite3', 'schema.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,11 +2,12 @@
 from decimal import Decimal

 from django.apps.registry import Apps
+from django.db import NotSupportedError
 from django.db.backends.base.schema import BaseDatabaseSchemaEditor
 from django.db.backends.ddl_references import Statement
+from django.db.backends.utils import strip_quotes
 from django.db.models import UniqueConstraint
 from django.db.transaction import atomic
-from django.db.utils import NotSupportedError


 class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):
@@ -98,6 +99,8 @@
             super().alter_db_table(model, old_db_table, new_db_table)

     def alter_field(self, model, old_field, new_field, strict=False):
+        if not self._field_should_be_altered(old_field, new_field):
+            return
         old_field_name = old_field.name
         table_name = model._meta.db_table
         _, old_column_name = old_field.get_attname_column()
@@ -263,7 +266,7 @@
         body_copy = copy.deepcopy(body)
         meta_contents = {
             'app_label': model._meta.app_label,
-            'db_table': 'new__%s' % model._meta.db_table,
+            'db_table': 'new__%s' % strip_quotes(model._meta.db_table),
             'unique_together': unique_together,
             'index_together': index_together,
             'indexes': indexes,
@@ -357,11 +360,28 @@
             return self.execute(self._rename_field_sql(model._meta.db_table, old_field, new_field, new_type))
         # Alter by remaking table
         self._remake_table(model, alter_field=(old_field, new_field))
-        # Rebuild tables with FKs pointing to this field if the PK type changed.
-        if old_field.primary_key and new_field.primary_key and old_type != new_type:
-            for rel in new_field.model._meta.related_objects:
-                if not rel.many_to_many:
-                    self._remake_table(rel.related_model)
+        # Rebuild tables with FKs pointing to this field.
+        if new_field.unique and old_type != new_type:
+            related_models = set()
+            opts = new_field.model._meta
+            for remote_field in opts.related_objects:
+                # Ignore self-relationship since the table was already rebuilt.
+                if remote_field.related_model == model:
+                    continue
+                if not remote_field.many_to_many:
+                    if remote_field.field_name == new_field.name:
+                        related_models.add(remote_field.related_model)
+                elif new_field.primary_key and remote_field.through._meta.auto_created:
+                    related_models.add(remote_field.through)
+            if new_field.primary_key:
+                for many_to_many in opts.many_to_many:
+                    # Ignore self-relationship since the table was already rebuilt.
+                    if many_to_many.related_model == model:
+                        continue
+                    if many_to_many.remote_field.through._meta.auto_created:
+                        related_models.add(many_to_many.remote_field.through)
+            for related_model in related_models:
+                self._remake_table(related_model)

     def _alter_many_to_many(self, model, old_field, new_field, strict):
         """Alter M2Ms to repoint their to= endpoints."""
@@ -409,3 +429,6 @@
             super().remove_constraint(model, constraint)
         else:
             self._remake_table(model)
+
+    def _collate_sql(self, collation):
+        return ' COLLATE ' + collation
('django/db/backends/mysql', 'compiler.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,5 @@
+from django.core.exceptions import FieldError
+from django.db.models.expressions import Col
 from django.db.models.sql import compiler


@@ -14,11 +16,55 @@


 class SQLDeleteCompiler(compiler.SQLDeleteCompiler, SQLCompiler):
-    pass
+    def as_sql(self):
+        # Prefer the non-standard DELETE FROM syntax over the SQL generated by
+        # the SQLDeleteCompiler's default implementation when multiple tables
+        # are involved since MySQL/MariaDB will generate a more efficient query
+        # plan than when using a subquery.
+        where, having = self.query.where.split_having()
+        if self.single_alias or having:
+            # DELETE FROM cannot be used when filtering against aggregates
+            # since it doesn't allow for GROUP BY and HAVING clauses.
+            return super().as_sql()
+        result = [
+            'DELETE %s FROM' % self.quote_name_unless_alias(
+                self.query.get_initial_alias()
+            )
+        ]
+        from_sql, from_params = self.get_from_clause()
+        result.extend(from_sql)
+        where_sql, where_params = self.compile(where)
+        if where_sql:
+            result.append('WHERE %s' % where_sql)
+        return ' '.join(result), tuple(from_params) + tuple(where_params)


 class SQLUpdateCompiler(compiler.SQLUpdateCompiler, SQLCompiler):
-    pass
+    def as_sql(self):
+        update_query, update_params = super().as_sql()
+        # MySQL and MariaDB support UPDATE ... ORDER BY syntax.
+        if self.query.order_by:
+            order_by_sql = []
+            order_by_params = []
+            db_table = self.query.get_meta().db_table
+            try:
+                for resolved, (sql, params, _) in self.get_order_by():
+                    if (
+                        isinstance(resolved.expression, Col) and
+                        resolved.expression.alias != db_table
+                    ):
+                        # Ignore ordering if it contains joined fields, because
+                        # they cannot be used in the ORDER BY clause.
+                        raise FieldError
+                    order_by_sql.append(sql)
+                    order_by_params.extend(params)
+                update_query += ' ORDER BY ' + ', '.join(order_by_sql)
+                update_params += tuple(order_by_params)
+            except FieldError:
+                # Ignore ordering if it contains annotations, because they're
+                # removed in .update() and cannot be resolved.
+                pass
+        return update_query, update_params


 class SQLAggregateCompiler(compiler.SQLAggregateCompiler, SQLCompiler):
('django/db/backends/mysql', 'creation.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,4 @@
+import os
 import subprocess
 import sys

@@ -26,7 +27,7 @@
                 self.log('Got an error creating the test database: %s' % e)
                 sys.exit(2)
             else:
-                raise e
+                raise

     def _clone_test_db(self, suffix, verbosity, keepdb=False):
         source_database_name = self.connection.settings_dict['NAME']
@@ -35,7 +36,7 @@
             'dbname': self.connection.ops.quote_name(target_database_name),
             'suffix': self.sql_table_creation_suffix(),
         }
-        with self._nodb_connection.cursor() as cursor:
+        with self._nodb_cursor() as cursor:
             try:
                 self._execute_create_test_db(cursor, test_db_params, keepdb)
             except Exception:
@@ -55,13 +56,13 @@
         self._clone_db(source_database_name, target_database_name)

     def _clone_db(self, source_database_name, target_database_name):
-        dump_args = DatabaseClient.settings_to_cmd_args(self.connection.settings_dict)[1:]
-        dump_args[-1] = source_database_name
-        dump_cmd = ['mysqldump', '--routines', '--events'] + dump_args
-        load_cmd = DatabaseClient.settings_to_cmd_args(self.connection.settings_dict)
+        cmd_args, cmd_env = DatabaseClient.settings_to_cmd_args_env(self.connection.settings_dict, [])
+        dump_cmd = ['mysqldump', *cmd_args[1:-1], '--routines', '--events', source_database_name]
+        dump_env = load_env = {**os.environ, **cmd_env} if cmd_env else None
+        load_cmd = cmd_args
         load_cmd[-1] = target_database_name

-        with subprocess.Popen(dump_cmd, stdout=subprocess.PIPE) as dump_proc:
-            with subprocess.Popen(load_cmd, stdin=dump_proc.stdout, stdout=subprocess.DEVNULL):
+        with subprocess.Popen(dump_cmd, stdout=subprocess.PIPE, env=dump_env) as dump_proc:
+            with subprocess.Popen(load_cmd, stdin=dump_proc.stdout, stdout=subprocess.DEVNULL, env=load_env):
                 # Allow dump_proc to receive a SIGPIPE if the load process exits.
                 dump_proc.stdout.close()
('django/db/backends/mysql', 'client.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,3 @@
-import subprocess
-
 from django.db.backends.base.client import BaseDatabaseClient


@@ -7,25 +5,40 @@
     executable_name = 'mysql'

     @classmethod
-    def settings_to_cmd_args(cls, settings_dict):
+    def settings_to_cmd_args_env(cls, settings_dict, parameters):
         args = [cls.executable_name]
-        db = settings_dict['OPTIONS'].get('db', settings_dict['NAME'])
+        env = None
+        database = settings_dict['OPTIONS'].get(
+            'database',
+            settings_dict['OPTIONS'].get('db', settings_dict['NAME']),
+        )
         user = settings_dict['OPTIONS'].get('user', settings_dict['USER'])
-        passwd = settings_dict['OPTIONS'].get('passwd', settings_dict['PASSWORD'])
+        password = settings_dict['OPTIONS'].get(
+            'password',
+            settings_dict['OPTIONS'].get('passwd', settings_dict['PASSWORD'])
+        )
         host = settings_dict['OPTIONS'].get('host', settings_dict['HOST'])
         port = settings_dict['OPTIONS'].get('port', settings_dict['PORT'])
         server_ca = settings_dict['OPTIONS'].get('ssl', {}).get('ca')
         client_cert = settings_dict['OPTIONS'].get('ssl', {}).get('cert')
         client_key = settings_dict['OPTIONS'].get('ssl', {}).get('key')
         defaults_file = settings_dict['OPTIONS'].get('read_default_file')
+        charset = settings_dict['OPTIONS'].get('charset')
         # Seems to be no good way to set sql_mode with CLI.

         if defaults_file:
             args += ["--defaults-file=%s" % defaults_file]
         if user:
             args += ["--user=%s" % user]
-        if passwd:
-            args += ["--password=%s" % passwd]
+        if password:
+            # The MYSQL_PWD environment variable usage is discouraged per
+            # MySQL's documentation due to the possibility of exposure through
+            # `ps` on old Unix flavors but --password suffers from the same
+            # flaw on even more systems. Usage of an environment variable also
+            # prevents password exposure if the subprocess.run(check=True) call
+            # raises a CalledProcessError since the string representation of
+            # the latter includes all of the provided `args`.
+            env = {'MYSQL_PWD': password}
         if host:
             if '/' in host:
                 args += ["--socket=%s" % host]
@@ -39,10 +52,9 @@
             args += ["--ssl-cert=%s" % client_cert]
         if client_key:
             args += ["--ssl-key=%s" % client_key]
-        if db:
-            args += [db]
-        return args
-
-    def runshell(self):
-        args = DatabaseClient.settings_to_cmd_args(self.connection.settings_dict)
-        subprocess.check_call(args)
+        if charset:
+            args += ['--default-character-set=%s' % charset]
+        if database:
+            args += [database]
+        args.extend(parameters)
+        return args, env
('django/db/backends/mysql', 'features.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -6,7 +6,6 @@

 class DatabaseFeatures(BaseDatabaseFeatures):
     empty_fetchmany_value = ()
-    update_can_self_select = False
     allows_group_by_pk = True
     related_fields_match_type = True
     # MySQL doesn't support sliced subqueries with IN/ALL/ANY/SOME.
@@ -15,20 +14,10 @@
     supports_forward_references = False
     supports_regex_backreferencing = False
     supports_date_lookup_using_string = False
-    can_introspect_autofield = True
-    can_introspect_binary_field = False
-    can_introspect_duration_field = False
-    can_introspect_small_integer_field = True
-    can_introspect_positive_integer_field = True
-    introspected_boolean_field_type = 'IntegerField'
-    supports_index_column_ordering = False
     supports_timezones = False
     requires_explicit_null_ordering_when_grouping = True
-    allows_auto_pk_0 = False
     can_release_savepoints = True
     atomic_transactions = False
-    supports_column_check_constraints = False
-    supports_table_check_constraints = False
     can_clone_databases = True
     supports_temporal_subtraction = True
     supports_select_intersection = False
@@ -50,19 +39,86 @@
             SET V_I = P_I;
         END;
     """
-    db_functions_convert_bytes_to_str = True
-    # Alias MySQL's TRADITIONAL to TEXT for consistency with other backends.
-    supported_explain_formats = {'JSON', 'TEXT', 'TRADITIONAL'}
     # Neither MySQL nor MariaDB support partial indexes.
     supports_partial_indexes = False
+    # COLLATE must be wrapped in parentheses because MySQL treats COLLATE as an
+    # indexed expression.
+    collate_as_index_expression = True
+
+    supports_order_by_nulls_modifier = False
+    order_by_nulls_first = True
+    test_collations = {
+        'ci': 'utf8_general_ci',
+        'non_default': 'utf8_esperanto_ci',
+        'swedish_ci': 'utf8_swedish_ci',
+    }
+
+    @cached_property
+    def django_test_skips(self):
+        skips = {
+            "This doesn't work on MySQL.": {
+                'db_functions.comparison.test_greatest.GreatestTests.test_coalesce_workaround',
+                'db_functions.comparison.test_least.LeastTests.test_coalesce_workaround',
+            },
+            'Running on MySQL requires utf8mb4 encoding (#18392).': {
+                'model_fields.test_textfield.TextFieldTests.test_emoji',
+                'model_fields.test_charfield.TestCharField.test_emoji',
+            },
+            "MySQL doesn't support functional indexes on a function that "
+            "returns JSON": {
+                'schema.tests.SchemaTests.test_func_index_json_key_transform',
+            },
+        }
+        if 'ONLY_FULL_GROUP_BY' in self.connection.sql_mode:
+            skips.update({
+                'GROUP BY optimization does not work properly when '
+                'ONLY_FULL_GROUP_BY mode is enabled on MySQL, see #31331.': {
+                    'aggregation.tests.AggregateTestCase.test_aggregation_subquery_annotation_multivalued',
+                    'annotations.tests.NonAggregateAnnotationTestCase.test_annotation_aggregate_with_m2o',
+                },
+            })
+        if (
+            self.connection.mysql_is_mariadb and
+            (10, 4, 3) < self.connection.mysql_version < (10, 5, 2)
+        ):
+            skips.update({
+                'https://jira.mariadb.org/browse/MDEV-19598': {
+                    'schema.tests.SchemaTests.test_alter_not_unique_field_to_primary_key',
+                },
+            })
+        if (
+            self.connection.mysql_is_mariadb and
+            (10, 4, 12) < self.connection.mysql_version < (10, 5)
+        ):
+            skips.update({
+                'https://jira.mariadb.org/browse/MDEV-22775': {
+                    'schema.tests.SchemaTests.test_alter_pk_with_self_referential_field',
+                },
+            })
+        if not self.supports_explain_analyze:
+            skips.update({
+                'MariaDB and MySQL >= 8.0.18 specific.': {
+                    'queries.test_explain.ExplainTests.test_mysql_analyze',
+                },
+            })
+        return skips

     @cached_property
     def _mysql_storage_engine(self):
         "Internal method used in Django tests. Don't rely on this from your code"
-        with self.connection.cursor() as cursor:
-            cursor.execute("SELECT ENGINE FROM INFORMATION_SCHEMA.ENGINES WHERE SUPPORT = 'DEFAULT'")
-            result = cursor.fetchone()
-        return result[0]
+        return self.connection.mysql_server_data['default_storage_engine']
+
+    @cached_property
+    def allows_auto_pk_0(self):
+        """
+        Autoincrement primary key can be set to 0 if it doesn't generate new
+        autoincrement values.
+        """
+        return 'NO_AUTO_VALUE_ON_ZERO' in self.connection.sql_mode
+
+    @cached_property
+    def update_can_self_select(self):
+        return self.connection.mysql_is_mariadb and self.connection.mysql_version >= (10, 3, 2)

     @cached_property
     def can_introspect_foreign_keys(self):
@@ -70,35 +126,78 @@
         return self._mysql_storage_engine != 'MyISAM'

     @cached_property
+    def introspected_field_types(self):
+        return {
+            **super().introspected_field_types,
+            'BinaryField': 'TextField',
+            'BooleanField': 'IntegerField',
+            'DurationField': 'BigIntegerField',
+            'GenericIPAddressField': 'CharField',
+        }
+
+    @cached_property
+    def can_return_columns_from_insert(self):
+        return self.connection.mysql_is_mariadb and self.connection.mysql_version >= (10, 5, 0)
+
+    can_return_rows_from_bulk_insert = property(operator.attrgetter('can_return_columns_from_insert'))
+
+    @cached_property
     def has_zoneinfo_database(self):
-        # Test if the time zone definitions are installed.
-        with self.connection.cursor() as cursor:
-            cursor.execute("SELECT 1 FROM mysql.time_zone LIMIT 1")
-            return cursor.fetchone() is not None
+        return self.connection.mysql_server_data['has_zoneinfo_database']

     @cached_property
     def is_sql_auto_is_null_enabled(self):
-        with self.connection.cursor() as cursor:
-            cursor.execute('SELECT @@SQL_AUTO_IS_NULL')
-            result = cursor.fetchone()
-            return result and result[0] == 1
+        return self.connection.mysql_server_data['sql_auto_is_null']

     @cached_property
     def supports_over_clause(self):
         if self.connection.mysql_is_mariadb:
-            return self.connection.mysql_version >= (10, 2)
+            return True
         return self.connection.mysql_version >= (8, 0, 2)
+
+    supports_frame_range_fixed_distance = property(operator.attrgetter('supports_over_clause'))
+
+    @cached_property
+    def supports_column_check_constraints(self):
+        if self.connection.mysql_is_mariadb:
+            return self.connection.mysql_version >= (10, 2, 1)
+        return self.connection.mysql_version >= (8, 0, 16)
+
+    supports_table_check_constraints = property(operator.attrgetter('supports_column_check_constraints'))
+
+    @cached_property
+    def can_introspect_check_constraints(self):
+        if self.connection.mysql_is_mariadb:
+            version = self.connection.mysql_version
+            return (version >= (10, 2, 22) and version < (10, 3)) or version >= (10, 3, 10)
+        return self.connection.mysql_version >= (8, 0, 16)

     @cached_property
     def has_select_for_update_skip_locked(self):
         return not self.connection.mysql_is_mariadb and self.connection.mysql_version >= (8, 0, 1)

-    has_select_for_update_nowait = property(operator.attrgetter('has_select_for_update_skip_locked'))
-
-    @cached_property
-    def needs_explain_extended(self):
-        # EXTENDED is deprecated (and not required) in MySQL 5.7.
-        return not self.connection.mysql_is_mariadb and self.connection.mysql_version < (5, 7)
+    @cached_property
+    def has_select_for_update_nowait(self):
+        if self.connection.mysql_is_mariadb:
+            return self.connection.mysql_version >= (10, 3, 0)
+        return self.connection.mysql_version >= (8, 0, 1)
+
+    @cached_property
+    def has_select_for_update_of(self):
+        return not self.connection.mysql_is_mariadb and self.connection.mysql_version >= (8, 0, 1)
+
+    @cached_property
+    def supports_explain_analyze(self):
+        return self.connection.mysql_is_mariadb or self.connection.mysql_version >= (8, 0, 18)
+
+    @cached_property
+    def supported_explain_formats(self):
+        # Alias MySQL's TRADITIONAL to TEXT for consistency with other
+        # backends.
+        formats = {'JSON', 'TEXT', 'TRADITIONAL'}
+        if not self.connection.mysql_is_mariadb and self.connection.mysql_version >= (8, 0, 16):
+            formats.add('TREE')
+        return formats

     @cached_property
     def supports_transactions(self):
@@ -109,12 +208,35 @@

     @cached_property
     def ignores_table_name_case(self):
-        with self.connection.cursor() as cursor:
-            cursor.execute('SELECT @@LOWER_CASE_TABLE_NAMES')
-            result = cursor.fetchone()
-            return result and result[0] != 0
+        return self.connection.mysql_server_data['lower_case_table_names']

     @cached_property
     def supports_default_in_lead_lag(self):
         # To be added in https://jira.mariadb.org/browse/MDEV-12981.
         return not self.connection.mysql_is_mariadb
+
+    @cached_property
+    def supports_json_field(self):
+        if self.connection.mysql_is_mariadb:
+            return self.connection.mysql_version >= (10, 2, 7)
+        return self.connection.mysql_version >= (5, 7, 8)
+
+    @cached_property
+    def can_introspect_json_field(self):
+        if self.connection.mysql_is_mariadb:
+            return self.supports_json_field and self.can_introspect_check_constraints
+        return self.supports_json_field
+
+    @cached_property
+    def supports_index_column_ordering(self):
+        return (
+            not self.connection.mysql_is_mariadb and
+            self.connection.mysql_version >= (8, 0, 1)
+        )
+
+    @cached_property
+    def supports_expression_indexes(self):
+        return (
+            not self.connection.mysql_is_mariadb and
+            self.connection.mysql_version >= (8, 0, 13)
+        )
('django/db/backends/mysql', 'operations.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,7 +3,7 @@
 from django.conf import settings
 from django.db.backends.base.operations import BaseDatabaseOperations
 from django.utils import timezone
-from django.utils.duration import duration_microseconds
+from django.utils.encoding import force_str


 class DatabaseOperations(BaseDatabaseOperations):
@@ -14,18 +14,22 @@
         **BaseDatabaseOperations.integer_field_ranges,
         'PositiveSmallIntegerField': (0, 65535),
         'PositiveIntegerField': (0, 4294967295),
+        'PositiveBigIntegerField': (0, 18446744073709551615),
     }
     cast_data_types = {
         'AutoField': 'signed integer',
         'BigAutoField': 'signed integer',
+        'SmallAutoField': 'signed integer',
         'CharField': 'char(%(max_length)s)',
         'DecimalField': 'decimal(%(max_digits)s, %(decimal_places)s)',
         'TextField': 'char',
         'IntegerField': 'signed integer',
         'BigIntegerField': 'signed integer',
         'SmallIntegerField': 'signed integer',
+        'PositiveBigIntegerField': 'unsigned integer',
         'PositiveIntegerField': 'unsigned integer',
         'PositiveSmallIntegerField': 'unsigned integer',
+        'DurationField': 'signed integer',
     }
     cast_char_field_without_max_length = 'char'
     explain_prefix = 'EXPLAIN'
@@ -34,8 +38,10 @@
         # https://dev.mysql.com/doc/mysql/en/date-and-time-functions.html
         if lookup_type == 'week_day':
             # DAYOFWEEK() returns an integer, 1-7, Sunday=1.
-            # Note: WEEKDAY() returns 0-6, Monday=0.
             return "DAYOFWEEK(%s)" % field_name
+        elif lookup_type == 'iso_week_day':
+            # WEEKDAY() returns an integer, 0-6, Monday=0.
+            return "WEEKDAY(%s) + 1" % field_name
         elif lookup_type == 'week':
             # Override the value of default_week_format for consistency with
             # other database backends.
@@ -49,7 +55,8 @@
             # EXTRACT returns 1-53 based on ISO-8601 for the week number.
             return "EXTRACT(%s FROM %s)" % (lookup_type.upper(), field_name)

-    def date_trunc_sql(self, lookup_type, field_name):
+    def date_trunc_sql(self, lookup_type, field_name, tzname=None):
+        field_name = self._convert_field_to_tz(field_name, tzname)
         fields = {
             'year': '%%Y-01-01',
             'month': '%%Y-%%m-01',
@@ -68,9 +75,20 @@
         else:
             return "DATE(%s)" % (field_name)

+    def _prepare_tzname_delta(self, tzname):
+        if '+' in tzname:
+            return tzname[tzname.find('+'):]
+        elif '-' in tzname:
+            return tzname[tzname.find('-'):]
+        return tzname
+
     def _convert_field_to_tz(self, field_name, tzname):
-        if settings.USE_TZ:
-            field_name = "CONVERT_TZ(%s, 'UTC', '%s')" % (field_name, tzname)
+        if tzname and settings.USE_TZ and self.connection.timezone_name != tzname:
+            field_name = "CONVERT_TZ(%s, '%s', '%s')" % (
+                field_name,
+                self.connection.timezone_name,
+                self._prepare_tzname_delta(tzname),
+            )
         return field_name

     def datetime_cast_date_sql(self, field_name, tzname):
@@ -107,11 +125,12 @@
         except ValueError:
             sql = field_name
         else:
-            format_str = ''.join([f for f in format[:i]] + [f for f in format_def[i:]])
+            format_str = ''.join(format[:i] + format_def[i:])
             sql = "CAST(DATE_FORMAT(%s, '%s') AS DATETIME)" % (field_name, format_str)
         return sql

-    def time_trunc_sql(self, lookup_type, field_name):
+    def time_trunc_sql(self, lookup_type, field_name, tzname=None):
+        field_name = self._convert_field_to_tz(field_name, tzname)
         fields = {
             'hour': '%%H:00:00',
             'minute': '%%H:%%i:00',
@@ -123,8 +142,12 @@
         else:
             return "TIME(%s)" % (field_name)

-    def date_interval_sql(self, timedelta):
-        return 'INTERVAL %s MICROSECOND' % duration_microseconds(timedelta)
+    def fetch_returned_insert_rows(self, cursor):
+        """
+        Given a cursor object that has just performed an INSERT...RETURNING
+        statement into a table, return the tuple of returned data.
+        """
+        return cursor.fetchall()

     def format_for_duration_arithmetic(self, sql):
         return 'INTERVAL %s MICROSECOND' % sql
@@ -137,14 +160,15 @@
         """
         return [(None, ("NULL", [], False))]

+    def adapt_decimalfield_value(self, value, max_digits=None, decimal_places=None):
+        return value
+
     def last_executed_query(self, cursor, sql, params):
         # With MySQLdb, cursor objects have an (undocumented) "_executed"
         # attribute where the exact query sent to the database is saved.
         # See MySQLdb/cursors.py in the source distribution.
-        query = getattr(cursor, '_executed', None)
-        if query is not None:
-            query = query.decode(errors='replace')
-        return query
+        # MySQLdb returns string, PyMySQL bytes.
+        return force_str(getattr(cursor, '_executed', None), errors='replace')

     def no_limit_value(self):
         # 2**64 - 1, as recommended by the MySQL documentation
@@ -155,29 +179,60 @@
             return name  # Quoting once is enough.
         return "`%s`" % name

-    def random_function_sql(self):
-        return 'RAND()'
-
-    def sql_flush(self, style, tables, sequences, allow_cascade=False):
-        # NB: The generated SQL below is specific to MySQL
-        # 'TRUNCATE x;', 'TRUNCATE y;', 'TRUNCATE z;'... style SQL statements
-        # to clear all tables of all data
-        if tables:
-            sql = ['SET FOREIGN_KEY_CHECKS = 0;']
-            for table in tables:
-                sql.append('%s %s;' % (
+    def return_insert_columns(self, fields):
+        # MySQL and MariaDB < 10.5.0 don't support an INSERT...RETURNING
+        # statement.
+        if not fields:
+            return '', ()
+        columns = [
+            '%s.%s' % (
+                self.quote_name(field.model._meta.db_table),
+                self.quote_name(field.column),
+            ) for field in fields
+        ]
+        return 'RETURNING %s' % ', '.join(columns), ()
+
+    def sql_flush(self, style, tables, *, reset_sequences=False, allow_cascade=False):
+        if not tables:
+            return []
+
+        sql = ['SET FOREIGN_KEY_CHECKS = 0;']
+        if reset_sequences:
+            # It's faster to TRUNCATE tables that require a sequence reset
+            # since ALTER TABLE AUTO_INCREMENT is slower than TRUNCATE.
+            sql.extend(
+                '%s %s;' % (
                     style.SQL_KEYWORD('TRUNCATE'),
-                    style.SQL_FIELD(self.quote_name(table)),
-                ))
-            sql.append('SET FOREIGN_KEY_CHECKS = 1;')
-            sql.extend(self.sequence_reset_by_name_sql(style, sequences))
-            return sql
-        else:
-            return []
+                    style.SQL_FIELD(self.quote_name(table_name)),
+                ) for table_name in tables
+            )
+        else:
+            # Otherwise issue a simple DELETE since it's faster than TRUNCATE
+            # and preserves sequences.
+            sql.extend(
+                '%s %s %s;' % (
+                    style.SQL_KEYWORD('DELETE'),
+                    style.SQL_KEYWORD('FROM'),
+                    style.SQL_FIELD(self.quote_name(table_name)),
+                ) for table_name in tables
+            )
+        sql.append('SET FOREIGN_KEY_CHECKS = 1;')
+        return sql
+
+    def sequence_reset_by_name_sql(self, style, sequences):
+        return [
+            '%s %s %s %s = 1;' % (
+                style.SQL_KEYWORD('ALTER'),
+                style.SQL_KEYWORD('TABLE'),
+                style.SQL_FIELD(self.quote_name(sequence_info['table'])),
+                style.SQL_FIELD('AUTO_INCREMENT'),
+            ) for sequence_info in sequences
+        ]

     def validate_autopk_value(self, value):
-        # MySQLism: zero in AUTO_INCREMENT field does not work. Refs #17653.
-        if value == 0:
+        # Zero in AUTO_INCREMENT field does not work without the
+        # NO_AUTO_VALUE_ON_ZERO SQL mode.
+        if value == 0 and not self.connection.features.allows_auto_pk_0:
             raise ValueError('The database backend does not accept 0 as a '
                              'value for AutoField.')
         return value
@@ -215,6 +270,9 @@
     def max_name_length(self):
         return 64

+    def pk_default_value(self):
+        return 'NULL'
+
     def bulk_insert_sql(self, fields, placeholder_rows):
         placeholder_rows_sql = (", ".join(row) for row in placeholder_rows)
         values_sql = ", ".join("(%s)" % sql for sql in placeholder_rows_sql)
@@ -225,7 +283,8 @@
             return 'POW(%s)' % ','.join(sub_expressions)
         # Convert the result to a signed integer since MySQL's binary operators
         # return an unsigned integer.
-        elif connector in ('&', '|', '<<'):
+        elif connector in ('&', '|', '<<', '#'):
+            connector = '^' if connector == '#' else connector
             return 'CONVERT(%s, SIGNED)' % connector.join(sub_expressions)
         elif connector == '>>':
             lhs, rhs = sub_expressions
@@ -271,29 +330,34 @@
                 # a decimal. MySQL returns an integer without microseconds.
                 return 'CAST((TIME_TO_SEC(%(lhs)s) - TIME_TO_SEC(%(rhs)s)) * 1000000 AS SIGNED)' % {
                     'lhs': lhs_sql, 'rhs': rhs_sql
-                }, lhs_params + rhs_params
+                }, (*lhs_params, *rhs_params)
             return (
                 "((TIME_TO_SEC(%(lhs)s) * 1000000 + MICROSECOND(%(lhs)s)) -"
                 " (TIME_TO_SEC(%(rhs)s) * 1000000 + MICROSECOND(%(rhs)s)))"
-            ) % {'lhs': lhs_sql, 'rhs': rhs_sql}, lhs_params * 2 + rhs_params * 2
-        else:
-            return "TIMESTAMPDIFF(MICROSECOND, %s, %s)" % (rhs_sql, lhs_sql), rhs_params + lhs_params
+            ) % {'lhs': lhs_sql, 'rhs': rhs_sql}, tuple(lhs_params) * 2 + tuple(rhs_params) * 2
+        params = (*rhs_params, *lhs_params)
+        return "TIMESTAMPDIFF(MICROSECOND, %s, %s)" % (rhs_sql, lhs_sql), params

     def explain_query_prefix(self, format=None, **options):
         # Alias MySQL's TRADITIONAL to TEXT for consistency with other backends.
         if format and format.upper() == 'TEXT':
             format = 'TRADITIONAL'
+        elif not format and 'TREE' in self.connection.features.supported_explain_formats:
+            # Use TREE by default (if supported) as it's more informative.
+            format = 'TREE'
+        analyze = options.pop('analyze', False)
         prefix = super().explain_query_prefix(format, **options)
-        if format:
+        if analyze and self.connection.features.supports_explain_analyze:
+            # MariaDB uses ANALYZE instead of EXPLAIN ANALYZE.
+            prefix = 'ANALYZE' if self.connection.mysql_is_mariadb else prefix + ' ANALYZE'
+        if format and not (analyze and not self.connection.mysql_is_mariadb):
+            # Only MariaDB supports the analyze option with formats.
             prefix += ' FORMAT=%s' % format
-        if self.connection.features.needs_explain_extended and format is None:
-            # EXTENDED and FORMAT are mutually exclusive options.
-            prefix += ' EXTENDED'
         return prefix

     def regex_lookup(self, lookup_type):
         # REGEXP BINARY doesn't work correctly in MySQL 8+ and REGEXP_LIKE
-        # doesn't exist in MySQL 5.6 or in MariaDB.
+        # doesn't exist in MySQL 5.x or in MariaDB.
         if self.connection.mysql_version < (8, 0, 0) or self.connection.mysql_is_mariadb:
             if lookup_type == 'regex':
                 return '%s REGEXP BINARY %s'
@@ -304,3 +368,13 @@

     def insert_statement(self, ignore_conflicts=False):
         return 'INSERT IGNORE INTO' if ignore_conflicts else super().insert_statement(ignore_conflicts)
+
+    def lookup_cast(self, lookup_type, internal_type=None):
+        lookup = '%s'
+        if internal_type == 'JSONField':
+            if self.connection.mysql_is_mariadb or lookup_type in (
+                'iexact', 'contains', 'icontains', 'startswith', 'istartswith',
+                'endswith', 'iendswith', 'regex', 'iregex',
+            ):
+                lookup = 'JSON_UNQUOTE(%s)'
+        return lookup
('django/db/backends/mysql', 'introspection.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,15 +1,20 @@
 from collections import namedtuple

+import sqlparse
 from MySQLdb.constants import FIELD_TYPE

 from django.db.backends.base.introspection import (
     BaseDatabaseIntrospection, FieldInfo as BaseFieldInfo, TableInfo,
 )
-from django.db.models.indexes import Index
+from django.db.models import Index
 from django.utils.datastructures import OrderedSet

-FieldInfo = namedtuple('FieldInfo', BaseFieldInfo._fields + ('extra', 'is_unsigned'))
-InfoLine = namedtuple('InfoLine', 'col_name data_type max_len num_prec num_scale extra column_default is_unsigned')
+FieldInfo = namedtuple('FieldInfo', BaseFieldInfo._fields + ('extra', 'is_unsigned', 'has_json_constraint'))
+InfoLine = namedtuple(
+    'InfoLine',
+    'col_name data_type max_len num_prec num_scale extra column_default '
+    'collation is_unsigned'
+)


 class DatabaseIntrospection(BaseDatabaseIntrospection):
@@ -23,6 +28,7 @@
         FIELD_TYPE.DOUBLE: 'FloatField',
         FIELD_TYPE.FLOAT: 'FloatField',
         FIELD_TYPE.INT24: 'IntegerField',
+        FIELD_TYPE.JSON: 'JSONField',
         FIELD_TYPE.LONG: 'IntegerField',
         FIELD_TYPE.LONGLONG: 'BigIntegerField',
         FIELD_TYPE.SHORT: 'SmallIntegerField',
@@ -43,11 +49,19 @@
                 return 'AutoField'
             elif field_type == 'BigIntegerField':
                 return 'BigAutoField'
+            elif field_type == 'SmallIntegerField':
+                return 'SmallAutoField'
         if description.is_unsigned:
-            if field_type == 'IntegerField':
+            if field_type == 'BigIntegerField':
+                return 'PositiveBigIntegerField'
+            elif field_type == 'IntegerField':
                 return 'PositiveIntegerField'
             elif field_type == 'SmallIntegerField':
                 return 'PositiveSmallIntegerField'
+        # JSON data type is an alias for LONGTEXT in MariaDB, use check
+        # constraints clauses to introspect JSONField.
+        if description.has_json_constraint:
+            return 'JSONField'
         return field_type

     def get_table_list(self, cursor):
@@ -61,6 +75,28 @@
         Return a description of the table with the DB-API cursor.description
         interface."
         """
+        json_constraints = {}
+        if self.connection.mysql_is_mariadb and self.connection.features.can_introspect_json_field:
+            # JSON data type is an alias for LONGTEXT in MariaDB, select
+            # JSON_VALID() constraints to introspect JSONField.
+            cursor.execute("""
+                SELECT c.constraint_name AS column_name
+                FROM information_schema.check_constraints AS c
+                WHERE
+                    c.table_name = %s AND
+                    LOWER(c.check_clause) = 'json_valid(`' + LOWER(c.constraint_name) + '`)' AND
+                    c.constraint_schema = DATABASE()
+            """, [table_name])
+            json_constraints = {row[0] for row in cursor.fetchall()}
+        # A default collation for the given table.
+        cursor.execute("""
+            SELECT  table_collation
+            FROM    information_schema.tables
+            WHERE   table_schema = DATABASE()
+            AND     table_name = %s
+        """, [table_name])
+        row = cursor.fetchone()
+        default_column_collation = row[0] if row else ''
         # information_schema database gives more accurate results for some figures:
         # - varchar length returned by cursor.description is an internal length,
         #   not visible length (#5725)
@@ -71,11 +107,16 @@
                 column_name, data_type, character_maximum_length,
                 numeric_precision, numeric_scale, extra, column_default,
                 CASE
+                    WHEN collation_name = %s THEN NULL
+                    ELSE collation_name
+                END AS collation_name,
+                CASE
                     WHEN column_type LIKE '%% unsigned' THEN 1
                     ELSE 0
                 END AS is_unsigned
             FROM information_schema.columns
-            WHERE table_name = %s AND table_schema = DATABASE()""", [table_name])
+            WHERE table_name = %s AND table_schema = DATABASE()
+        """, [default_column_collation, table_name])
         field_info = {line[0]: InfoLine(*line) for line in cursor.fetchall()}

         cursor.execute("SELECT * FROM %s LIMIT 1" % self.connection.ops.quote_name(table_name))
@@ -93,8 +134,10 @@
                 to_int(info.num_scale) or line[5],
                 line[6],
                 info.column_default,
+                info.collation,
                 info.extra,
                 info.is_unsigned,
+                line[0] in json_constraints,
             ))
         return fields

@@ -145,6 +188,19 @@
         if not result:
             return self.connection.features._mysql_storage_engine
         return result[0]
+
+    def _parse_constraint_columns(self, check_clause, columns):
+        check_columns = OrderedSet()
+        statement = sqlparse.parse(check_clause)[0]
+        tokens = (token for token in statement.flatten() if not token.is_whitespace)
+        for token in tokens:
+            if (
+                token.ttype == sqlparse.tokens.Name and
+                self.connection.ops.quote_name(token.value) == token.value and
+                token.value[1:-1] in columns
+            ):
+                check_columns.add(token.value[1:-1])
+        return check_columns

     def get_constraints(self, cursor, table_name):
         """
@@ -173,6 +229,8 @@
                     'check': False,
                     'foreign_key': (ref_table, ref_column) if ref_column else None,
                 }
+                if self.connection.features.supports_index_column_ordering:
+                    constraints[constraint]['orders'] = []
             constraints[constraint]['columns'].add(column)
         # Now get the constraint types
         type_query = """
@@ -189,9 +247,53 @@
                 constraints[constraint]['unique'] = True
             elif kind.lower() == "unique":
                 constraints[constraint]['unique'] = True
+        # Add check constraints.
+        if self.connection.features.can_introspect_check_constraints:
+            unnamed_constraints_index = 0
+            columns = {info.name for info in self.get_table_description(cursor, table_name)}
+            if self.connection.mysql_is_mariadb:
+                type_query = """
+                    SELECT c.constraint_name, c.check_clause
+                    FROM information_schema.check_constraints AS c
+                    WHERE
+                        c.constraint_schema = DATABASE() AND
+                        c.table_name = %s
+                """
+            else:
+                type_query = """
+                    SELECT cc.constraint_name, cc.check_clause
+                    FROM
+                        information_schema.check_constraints AS cc,
+                        information_schema.table_constraints AS tc
+                    WHERE
+                        cc.constraint_schema = DATABASE() AND
+                        tc.table_schema = cc.constraint_schema AND
+                        cc.constraint_name = tc.constraint_name AND
+                        tc.constraint_type = 'CHECK' AND
+                        tc.table_name = %s
+                """
+            cursor.execute(type_query, [table_name])
+            for constraint, check_clause in cursor.fetchall():
+                constraint_columns = self._parse_constraint_columns(check_clause, columns)
+                # Ensure uniqueness of unnamed constraints. Unnamed unique
+                # and check columns constraints have the same name as
+                # a column.
+                if set(constraint_columns) == {constraint}:
+                    unnamed_constraints_index += 1
+                    constraint = '__unnamed_constraint_%s__' % unnamed_constraints_index
+                constraints[constraint] = {
+                    'columns': constraint_columns,
+                    'primary_key': False,
+                    'unique': False,
+                    'index': False,
+                    'check': True,
+                    'foreign_key': None,
+                }
         # Now add in the indexes
         cursor.execute("SHOW INDEX FROM %s" % self.connection.ops.quote_name(table_name))
-        for table, non_unique, index, colseq, column, type_ in [x[:5] + (x[10],) for x in cursor.fetchall()]:
+        for table, non_unique, index, colseq, column, order, type_ in [
+            x[:6] + (x[10],) for x in cursor.fetchall()
+        ]:
             if index not in constraints:
                 constraints[index] = {
                     'columns': OrderedSet(),
@@ -200,9 +302,13 @@
                     'check': False,
                     'foreign_key': None,
                 }
+                if self.connection.features.supports_index_column_ordering:
+                    constraints[index]['orders'] = []
             constraints[index]['index'] = True
             constraints[index]['type'] = Index.suffix if type_ == 'BTREE' else type_.lower()
             constraints[index]['columns'].add(column)
+            if self.connection.features.supports_index_column_ordering:
+                constraints[index]['orders'].append('DESC' if order == 'D' else 'ASC')
         # Convert the sorted sets to lists
         for constraint in constraints.values():
             constraint['columns'] = list(constraint['columns'])
('django/db/backends/mysql', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,13 +3,13 @@

 Requires mysqlclient: https://pypi.org/project/mysqlclient/
 """
-import re
-
 from django.core.exceptions import ImproperlyConfigured
-from django.db import utils
+from django.db import IntegrityError
 from django.db.backends import utils as backend_utils
 from django.db.backends.base.base import BaseDatabaseWrapper
+from django.utils.asyncio import async_unsafe
 from django.utils.functional import cached_property
+from django.utils.regex_helper import _lazy_re_compile

 try:
     import MySQLdb as Database
@@ -19,21 +19,21 @@
         'Did you install mysqlclient?'
     ) from err

-from MySQLdb.constants import CLIENT, FIELD_TYPE                # isort:skip
-from MySQLdb.converters import conversions                      # isort:skip
+from MySQLdb.constants import CLIENT, FIELD_TYPE
+from MySQLdb.converters import conversions

 # Some of these import MySQLdb, so import them after checking if it's installed.
-from .client import DatabaseClient                          # isort:skip
-from .creation import DatabaseCreation                      # isort:skip
-from .features import DatabaseFeatures                      # isort:skip
-from .introspection import DatabaseIntrospection            # isort:skip
-from .operations import DatabaseOperations                  # isort:skip
-from .schema import DatabaseSchemaEditor                    # isort:skip
-from .validation import DatabaseValidation                  # isort:skip
+from .client import DatabaseClient
+from .creation import DatabaseCreation
+from .features import DatabaseFeatures
+from .introspection import DatabaseIntrospection
+from .operations import DatabaseOperations
+from .schema import DatabaseSchemaEditor
+from .validation import DatabaseValidation

 version = Database.version_info
-if version < (1, 3, 13):
-    raise ImproperlyConfigured('mysqlclient 1.3.13 or newer is required; you have %s.' % Database.__version__)
+if version < (1, 4, 0):
+    raise ImproperlyConfigured('mysqlclient 1.4.0 or newer is required; you have %s.' % Database.__version__)


 # MySQLdb returns TIME columns as timedelta -- they are more like timedelta in
@@ -46,7 +46,7 @@

 # This should match the numerical portion of the version numbers (we can treat
 # versions like 5.0.24 and 5.0.24a as the same).
-server_version_re = re.compile(r'(\d{1,2})\.(\d{1,2})\.(\d{1,2})')
+server_version_re = _lazy_re_compile(r'(\d{1,2})\.(\d{1,2})\.(\d{1,2})')


 class CursorWrapper:
@@ -60,6 +60,8 @@
     codes_for_integrityerror = (
         1048,  # Column cannot be null
         1690,  # BIGINT UNSIGNED value is out of range
+        3819,  # CHECK constraint is violated
+        4025,  # CHECK constraint failed
     )

     def __init__(self, cursor):
@@ -73,7 +75,7 @@
             # Map some error codes to IntegrityError, since they seem to be
             # misclassified and Django would prefer the more logical place.
             if e.args[0] in self.codes_for_integrityerror:
-                raise utils.IntegrityError(*tuple(e.args))
+                raise IntegrityError(*tuple(e.args))
             raise

     def executemany(self, query, args):
@@ -83,7 +85,7 @@
             # Map some error codes to IntegrityError, since they seem to be
             # misclassified and Django would prefer the more logical place.
             if e.args[0] in self.codes_for_integrityerror:
-                raise utils.IntegrityError(*tuple(e.args))
+                raise IntegrityError(*tuple(e.args))
             raise

     def __getattr__(self, attr):
@@ -95,7 +97,6 @@

 class DatabaseWrapper(BaseDatabaseWrapper):
     vendor = 'mysql'
-    display_name = 'MySQL'
     # This dictionary maps Field objects to their associated MySQL column
     # types, as strings. Column-type strings can contain format strings; they'll
     # be interpolated against the values of Field.__dict__ before being output.
@@ -117,20 +118,25 @@
         'BigIntegerField': 'bigint',
         'IPAddressField': 'char(15)',
         'GenericIPAddressField': 'char(39)',
+        'JSONField': 'json',
         'NullBooleanField': 'bool',
         'OneToOneField': 'integer',
+        'PositiveBigIntegerField': 'bigint UNSIGNED',
         'PositiveIntegerField': 'integer UNSIGNED',
         'PositiveSmallIntegerField': 'smallint UNSIGNED',
         'SlugField': 'varchar(%(max_length)s)',
+        'SmallAutoField': 'smallint AUTO_INCREMENT',
         'SmallIntegerField': 'smallint',
         'TextField': 'longtext',
         'TimeField': 'time(6)',
         'UUIDField': 'char(32)',
     }

-    # For these columns, MySQL doesn't:
-    # - accept default values and implicitly treats these columns as nullable
-    # - support a database index
+    # For these data types:
+    # - MySQL < 8.0.13 and MariaDB < 10.2.1 don't accept default values and
+    #   implicitly treat them as nullable
+    # - all versions of MySQL and MariaDB don't support full width database
+    #   indexes
     _limited_data_types = (
         'tinyblob', 'blob', 'mediumblob', 'longblob', 'tinytext', 'text',
         'mediumtext', 'longtext', 'json',
@@ -195,9 +201,9 @@
         if settings_dict['USER']:
             kwargs['user'] = settings_dict['USER']
         if settings_dict['NAME']:
-            kwargs['db'] = settings_dict['NAME']
+            kwargs['database'] = settings_dict['NAME']
         if settings_dict['PASSWORD']:
-            kwargs['passwd'] = settings_dict['PASSWORD']
+            kwargs['password'] = settings_dict['PASSWORD']
         if settings_dict['HOST'].startswith('/'):
             kwargs['unix_socket'] = settings_dict['HOST']
         elif settings_dict['HOST']:
@@ -223,8 +229,16 @@
         kwargs.update(options)
         return kwargs

+    @async_unsafe
     def get_new_connection(self, conn_params):
-        return Database.connect(**conn_params)
+        connection = Database.connect(**conn_params)
+        # bytes encoder in mysqlclient doesn't work and was added only to
+        # prevent KeyErrors in Django < 2.0. We can remove this workaround when
+        # mysqlclient 2.1 becomes the minimal mysqlclient supported by Django.
+        # See https://github.com/PyMySQL/mysqlclient/issues/489
+        if connection.encoders.get(bytes) is bytes:
+            connection.encoders.pop(bytes)
+        return connection

     def init_connection_state(self):
         assignments = []
@@ -242,6 +256,7 @@
             with self.cursor() as cursor:
                 cursor.execute('; '.join(assignments))

+    @async_unsafe
     def create_cursor(self, name=None):
         cursor = self.connection.cursor()
         return CursorWrapper(cursor)
@@ -262,7 +277,8 @@
         forward references. Always return True to indicate constraint checks
         need to be re-enabled.
         """
-        self.cursor().execute('SET foreign_key_checks=0')
+        with self.cursor() as cursor:
+            cursor.execute('SET foreign_key_checks=0')
         return True

     def enable_constraint_checking(self):
@@ -273,7 +289,8 @@
         # nested inside transaction.atomic.
         self.needs_rollback, needs_rollback = False, self.needs_rollback
         try:
-            self.cursor().execute('SET foreign_key_checks=1')
+            with self.cursor() as cursor:
+                cursor.execute('SET foreign_key_checks=1')
         finally:
             self.needs_rollback = needs_rollback

@@ -307,7 +324,7 @@
                         )
                     )
                     for bad_row in cursor.fetchall():
-                        raise utils.IntegrityError(
+                        raise IntegrityError(
                             "The row in table '%s' with primary key '%s' has an invalid "
                             "foreign key: %s.%s contains a value '%s' that does not "
                             "have a corresponding value in %s.%s."
@@ -326,10 +343,51 @@
             return True

     @cached_property
+    def display_name(self):
+        return 'MariaDB' if self.mysql_is_mariadb else 'MySQL'
+
+    @cached_property
+    def data_type_check_constraints(self):
+        if self.features.supports_column_check_constraints:
+            check_constraints = {
+                'PositiveBigIntegerField': '`%(column)s` >= 0',
+                'PositiveIntegerField': '`%(column)s` >= 0',
+                'PositiveSmallIntegerField': '`%(column)s` >= 0',
+            }
+            if self.mysql_is_mariadb and self.mysql_version < (10, 4, 3):
+                # MariaDB < 10.4.3 doesn't automatically use the JSON_VALID as
+                # a check constraint.
+                check_constraints['JSONField'] = 'JSON_VALID(`%(column)s`)'
+            return check_constraints
+        return {}
+
+    @cached_property
+    def mysql_server_data(self):
+        with self.temporary_connection() as cursor:
+            # Select some server variables and test if the time zone
+            # definitions are installed. CONVERT_TZ returns NULL if 'UTC'
+            # timezone isn't loaded into the mysql.time_zone table.
+            cursor.execute("""
+                SELECT VERSION(),
+                       @@sql_mode,
+                       @@default_storage_engine,
+                       @@sql_auto_is_null,
+                       @@lower_case_table_names,
+                       CONVERT_TZ('2001-01-01 01:00:00', 'UTC', 'UTC') IS NOT NULL
+            """)
+            row = cursor.fetchone()
+        return {
+            'version': row[0],
+            'sql_mode': row[1],
+            'default_storage_engine': row[2],
+            'sql_auto_is_null': bool(row[3]),
+            'lower_case_table_names': bool(row[4]),
+            'has_zoneinfo_database': bool(row[5]),
+        }
+
+    @cached_property
     def mysql_server_info(self):
-        with self.temporary_connection() as cursor:
-            cursor.execute('SELECT VERSION()')
-            return cursor.fetchone()[0]
+        return self.mysql_server_data['version']

     @cached_property
     def mysql_version(self):
@@ -340,5 +398,9 @@

     @cached_property
     def mysql_is_mariadb(self):
-        # MariaDB isn't officially supported.
         return 'mariadb' in self.mysql_server_info.lower()
+
+    @cached_property
+    def sql_mode(self):
+        sql_mode = self.mysql_server_data['sql_mode']
+        return set(sql_mode.split(',') if sql_mode else ())
('django/db/backends/mysql', 'schema.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -9,14 +9,17 @@
     sql_alter_column_null = "MODIFY %(column)s %(type)s NULL"
     sql_alter_column_not_null = "MODIFY %(column)s %(type)s NOT NULL"
     sql_alter_column_type = "MODIFY %(column)s %(type)s"
+    sql_alter_column_collate = "MODIFY %(column)s %(type)s%(collation)s"
+    sql_alter_column_no_default_null = 'ALTER COLUMN %(column)s SET DEFAULT NULL'

     # No 'CASCADE' which works as a no-op in MySQL but is undocumented
     sql_delete_column = "ALTER TABLE %(table)s DROP COLUMN %(column)s"

-    sql_rename_column = "ALTER TABLE %(table)s CHANGE %(old_column)s %(new_column)s %(type)s"
-
     sql_delete_unique = "ALTER TABLE %(table)s DROP INDEX %(name)s"
-
+    sql_create_column_inline_fk = (
+        ', ADD CONSTRAINT %(name)s FOREIGN KEY (%(column)s) '
+        'REFERENCES %(to_table)s(%(to_column)s)'
+    )
     sql_delete_fk = "ALTER TABLE %(table)s DROP FOREIGN KEY %(name)s"

     sql_delete_index = "DROP INDEX %(name)s ON %(table)s"
@@ -26,10 +29,33 @@

     sql_create_index = 'CREATE INDEX %(name)s ON %(table)s (%(columns)s)%(extra)s'

+    @property
+    def sql_delete_check(self):
+        if self.connection.mysql_is_mariadb:
+            # The name of the column check constraint is the same as the field
+            # name on MariaDB. Adding IF EXISTS clause prevents migrations
+            # crash. Constraint is removed during a "MODIFY" column statement.
+            return 'ALTER TABLE %(table)s DROP CONSTRAINT IF EXISTS %(name)s'
+        return 'ALTER TABLE %(table)s DROP CHECK %(name)s'
+
+    @property
+    def sql_rename_column(self):
+        # MariaDB >= 10.5.2 and MySQL >= 8.0.4 support an
+        # "ALTER TABLE ... RENAME COLUMN" statement.
+        if self.connection.mysql_is_mariadb:
+            if self.connection.mysql_version >= (10, 5, 2):
+                return super().sql_rename_column
+        elif self.connection.mysql_version >= (8, 0, 4):
+            return super().sql_rename_column
+        return 'ALTER TABLE %(table)s CHANGE %(old_column)s %(new_column)s %(type)s'
+
     def quote_value(self, value):
         self.connection.ensure_connection()
+        if isinstance(value, str):
+            value = value.replace('%', '%%')
+        # MySQLdb escapes to string, PyMySQL to bytes.
         quoted = self.connection.connection.escape(value, self.connection.connection.encoders)
-        if isinstance(value, str):
+        if isinstance(value, str) and isinstance(quoted, bytes):
             quoted = quoted.decode()
         return quoted

@@ -38,7 +64,35 @@
         return db_type is not None and db_type.lower() in self.connection._limited_data_types

     def skip_default(self, field):
-        return self._is_limited_data_type(field)
+        if not self._supports_limited_data_type_defaults:
+            return self._is_limited_data_type(field)
+        return False
+
+    def skip_default_on_alter(self, field):
+        if self._is_limited_data_type(field) and not self.connection.mysql_is_mariadb:
+            # MySQL doesn't support defaults for BLOB and TEXT in the
+            # ALTER COLUMN statement.
+            return True
+        return False
+
+    @property
+    def _supports_limited_data_type_defaults(self):
+        # MariaDB >= 10.2.1 and MySQL >= 8.0.13 supports defaults for BLOB
+        # and TEXT.
+        if self.connection.mysql_is_mariadb:
+            return self.connection.mysql_version >= (10, 2, 1)
+        return self.connection.mysql_version >= (8, 0, 13)
+
+    def _column_default_sql(self, field):
+        if (
+            not self.connection.mysql_is_mariadb and
+            self._supports_limited_data_type_defaults and
+            self._is_limited_data_type(field)
+        ):
+            # MySQL supports defaults for BLOB and TEXT columns only if the
+            # default value is written as an expression i.e. in parentheses.
+            return '(%s)'
+        return super()._column_default_sql(field)

     def add_field(self, model, field):
         super().add_field(model, field)
@@ -80,7 +134,9 @@
         if first_field.get_internal_type() == 'ForeignKey':
             constraint_names = self._constraint_names(model, [first_field.column], index=True)
             if not constraint_names:
-                self.execute(self._create_index_sql(model, [first_field], suffix=""))
+                self.execute(
+                    self._create_index_sql(model, fields=[first_field], suffix='')
+                )
         return super()._delete_composed_index(model, fields, *args)

     def _set_field_new_type_null_status(self, field, new_type):
('django/db/backends/mysql', 'validation.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -10,18 +10,22 @@
         return issues

     def _check_sql_mode(self, **kwargs):
-        with self.connection.cursor() as cursor:
-            cursor.execute("SELECT @@sql_mode")
-            sql_mode = cursor.fetchone()
-        modes = set(sql_mode[0].split(',') if sql_mode else ())
-        if not (modes & {'STRICT_TRANS_TABLES', 'STRICT_ALL_TABLES'}):
+        if not (self.connection.sql_mode & {'STRICT_TRANS_TABLES', 'STRICT_ALL_TABLES'}):
             return [checks.Warning(
-                "MySQL Strict Mode is not set for database connection '%s'" % self.connection.alias,
-                hint="MySQL's Strict Mode fixes many data integrity problems in MySQL, "
-                     "such as data truncation upon insertion, by escalating warnings into "
-                     "errors. It is strongly recommended you activate it. See: "
-                     "https://docs.djangoproject.com/en/%s/ref/databases/#mysql-sql-mode"
-                     % (get_docs_version(),),
+                "%s Strict Mode is not set for database connection '%s'"
+                % (self.connection.display_name, self.connection.alias),
+                hint=(
+                    "%s's Strict Mode fixes many data integrity problems in "
+                    "%s, such as data truncation upon insertion, by "
+                    "escalating warnings into errors. It is strongly "
+                    "recommended you activate it. See: "
+                    "https://docs.djangoproject.com/en/%s/ref/databases/#mysql-sql-mode"
+                    % (
+                        self.connection.display_name,
+                        self.connection.display_name,
+                        get_docs_version(),
+                    ),
+                ),
                 id='mysql.W002',
             )]
         return []
@@ -37,18 +41,23 @@
         if (field_type.startswith('varchar') and field.unique and
                 (field.max_length is None or int(field.max_length) > 255)):
             errors.append(
-                checks.Error(
-                    'MySQL does not allow unique CharFields to have a max_length > 255.',
+                checks.Warning(
+                    '%s may not allow unique CharFields to have a max_length '
+                    '> 255.' % self.connection.display_name,
                     obj=field,
-                    id='mysql.E001',
+                    hint=(
+                        'See: https://docs.djangoproject.com/en/%s/ref/'
+                        'databases/#mysql-character-fields' % get_docs_version()
+                    ),
+                    id='mysql.W003',
                 )
             )

         if field.db_index and field_type.lower() in self.connection._limited_data_types:
             errors.append(
                 checks.Warning(
-                    'MySQL does not support a database index on %s columns.'
-                    % field_type,
+                    '%s does not support a database index on %s columns.'
+                    % (self.connection.display_name, field_type),
                     hint=(
                         "An index won't be created. Silence this warning if "
                         "you don't care about it."
('django/db/backends/base', 'creation.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,11 +1,14 @@
 import os
 import sys
 from io import StringIO
+from unittest import expectedFailure, skip

 from django.apps import apps
 from django.conf import settings
 from django.core import serializers
 from django.db import router
+from django.db.transaction import atomic
+from django.utils.module_loading import import_string

 # The prefix to put on the default database name when creating
 # the test database.
@@ -20,12 +23,8 @@
     def __init__(self, connection):
         self.connection = connection

-    @property
-    def _nodb_connection(self):
-        """
-        Used to be defined here, now moved to DatabaseWrapper.
-        """
-        return self.connection._nodb_connection
+    def _nodb_cursor(self):
+        return self.connection._nodb_cursor()

     def log(self, msg):
         sys.stderr.write(msg + os.linesep)
@@ -61,16 +60,27 @@
         settings.DATABASES[self.connection.alias]["NAME"] = test_database_name
         self.connection.settings_dict["NAME"] = test_database_name

-        # We report migrate messages at one level lower than that requested.
-        # This ensures we don't get flooded with messages during testing
-        # (unless you really ask to be flooded).
-        call_command(
-            'migrate',
-            verbosity=max(verbosity - 1, 0),
-            interactive=False,
-            database=self.connection.alias,
-            run_syncdb=True,
-        )
+        try:
+            if self.connection.settings_dict['TEST']['MIGRATE'] is False:
+                # Disable migrations for all apps.
+                old_migration_modules = settings.MIGRATION_MODULES
+                settings.MIGRATION_MODULES = {
+                    app.label: None
+                    for app in apps.get_app_configs()
+                }
+            # We report migrate messages at one level lower than that
+            # requested. This ensures we don't get flooded with messages during
+            # testing (unless you really ask to be flooded).
+            call_command(
+                'migrate',
+                verbosity=max(verbosity - 1, 0),
+                interactive=False,
+                database=self.connection.alias,
+                run_syncdb=True,
+            )
+        finally:
+            if self.connection.settings_dict['TEST']['MIGRATE'] is False:
+                settings.MIGRATION_MODULES = old_migration_modules

         # We then serialize the current state of the database into a string
         # and store it on the connection. This slightly horrific process is so people
@@ -84,6 +94,9 @@
         # Ensure a connection for the side effect of initializing the test database.
         self.connection.ensure_connection()

+        if os.environ.get('RUNNING_DJANGOS_TEST_SUITE') == 'true':
+            self.mark_expected_failures_and_skips()
+
         return test_database_name

     def set_as_test_mirror(self, primary_settings_dict):
@@ -99,25 +112,25 @@
         Designed only for test runner usage; will not handle large
         amounts of data.
         """
-        # Build list of all apps to serialize
-        from django.db.migrations.loader import MigrationLoader
-        loader = MigrationLoader(self.connection)
-        app_list = []
-        for app_config in apps.get_app_configs():
-            if (
-                app_config.models_module is not None and
-                app_config.label in loader.migrated_apps and
-                app_config.name not in settings.TEST_NON_SERIALIZED_APPS
-            ):
-                app_list.append((app_config, None))
-
-        # Make a function to iteratively return every object
+        # Iteratively return every object for all models to serialize.
         def get_objects():
-            for model in serializers.sort_dependencies(app_list):
-                if (model._meta.can_migrate(self.connection) and
-                        router.allow_migrate_model(self.connection.alias, model)):
-                    queryset = model._default_manager.using(self.connection.alias).order_by(model._meta.pk.name)
-                    yield from queryset.iterator()
+            from django.db.migrations.loader import MigrationLoader
+            loader = MigrationLoader(self.connection)
+            for app_config in apps.get_app_configs():
+                if (
+                    app_config.models_module is not None and
+                    app_config.label in loader.migrated_apps and
+                    app_config.name not in settings.TEST_NON_SERIALIZED_APPS
+                ):
+                    for model in app_config.get_models():
+                        if (
+                            model._meta.can_migrate(self.connection) and
+                            router.allow_migrate_model(self.connection.alias, model)
+                        ):
+                            queryset = model._base_manager.using(
+                                self.connection.alias,
+                            ).order_by(model._meta.pk.name)
+                            yield from queryset.iterator()
         # Serialize to a string
         out = StringIO()
         serializers.serialize("json", get_objects(), indent=None, stream=out)
@@ -129,8 +142,18 @@
         the serialize_db_to_string() method.
         """
         data = StringIO(data)
-        for obj in serializers.deserialize("json", data, using=self.connection.alias):
-            obj.save()
+        table_names = set()
+        # Load data in a transaction to handle forward references and cycles.
+        with atomic(using=self.connection.alias):
+            # Disable constraint checks, because some databases (MySQL) doesn't
+            # support deferred checks.
+            with self.connection.constraint_checks_disabled():
+                for obj in serializers.deserialize('json', data, using=self.connection.alias):
+                    obj.save()
+                    table_names.add(obj.object.__class__._meta.db_table)
+            # Manually check for any invalid keys that might have been added,
+            # because constraint checks were disabled.
+            self.connection.check_constraints(table_names=table_names)

     def _get_database_display_str(self, verbosity, database_name):
         """
@@ -165,7 +188,7 @@
             'suffix': self.sql_table_creation_suffix(),
         }
         # Create the test database and connect to it.
-        with self._nodb_connection.cursor() as cursor:
+        with self._nodb_cursor() as cursor:
             try:
                 self._execute_create_test_db(cursor, test_db_params, keepdb)
             except Exception as e:
@@ -271,9 +294,32 @@
         # ourselves. Connect to the previous database (not the test database)
         # to do so, because it's not allowed to delete a database while being
         # connected to it.
-        with self.connection._nodb_connection.cursor() as cursor:
+        with self._nodb_cursor() as cursor:
             cursor.execute("DROP DATABASE %s"
                            % self.connection.ops.quote_name(test_database_name))
+
+    def mark_expected_failures_and_skips(self):
+        """
+        Mark tests in Django's test suite which are expected failures on this
+        database and test which should be skipped on this database.
+        """
+        for test_name in self.connection.features.django_test_expected_failures:
+            test_case_name, _, test_method_name = test_name.rpartition('.')
+            test_app = test_name.split('.')[0]
+            # Importing a test app that isn't installed raises RuntimeError.
+            if test_app in settings.INSTALLED_APPS:
+                test_case = import_string(test_case_name)
+                test_method = getattr(test_case, test_method_name)
+                setattr(test_case, test_method_name, expectedFailure(test_method))
+        for reason, tests in self.connection.features.django_test_skips.items():
+            for test_name in tests:
+                test_case_name, _, test_method_name = test_name.rpartition('.')
+                test_app = test_name.split('.')[0]
+                # Importing a test app that isn't installed raises RuntimeError.
+                if test_app in settings.INSTALLED_APPS:
+                    test_case = import_string(test_case_name)
+                    test_method = getattr(test_case, test_method_name)
+                    setattr(test_case, test_method_name, skip(reason)(test_method))

     def sql_table_creation_suffix(self):
         """
('django/db/backends/base', 'client.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,7 @@
+import os
+import subprocess
+
+
 class BaseDatabaseClient:
     """Encapsulate backend-specific methods for opening a client shell."""
     # This should be a string representing the name of the executable
@@ -8,5 +12,14 @@
         # connection is an instance of BaseDatabaseWrapper.
         self.connection = connection

-    def runshell(self):
-        raise NotImplementedError('subclasses of BaseDatabaseClient must provide a runshell() method')
+    @classmethod
+    def settings_to_cmd_args_env(cls, settings_dict, parameters):
+        raise NotImplementedError(
+            'subclasses of BaseDatabaseClient must provide a '
+            'settings_to_cmd_args_env() method or override a runshell().'
+        )
+
+    def runshell(self, parameters):
+        args, env = self.settings_to_cmd_args_env(self.connection.settings_dict, parameters)
+        env = {**os.environ, **env} if env else None
+        subprocess.run(args, env=env, check=True)
('django/db/backends/base', 'features.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,9 +1,11 @@
-from django.db.utils import ProgrammingError
+from django.db import ProgrammingError
 from django.utils.functional import cached_property


 class BaseDatabaseFeatures:
     gis_enabled = False
+    # Oracle can't group by LOB (large object) data types.
+    allows_group_by_lob = True
     allows_group_by_pk = False
     allows_group_by_selected_pks = False
     empty_fetchmany_value = []
@@ -20,10 +22,12 @@
     # Does the backend allow inserting duplicate rows when a unique_together
     # constraint exists and some fields are nullable but not all of them?
     supports_partially_nullable_unique_constraints = True
+    # Does the backend support initially deferrable unique constraints?
+    supports_deferrable_unique_constraints = False

     can_use_chunked_reads = True
-    can_return_id_from_insert = False
-    can_return_ids_from_bulk_insert = False
+    can_return_columns_from_insert = False
+    can_return_rows_from_bulk_insert = False
     has_bulk_insert = True
     uses_savepoints = True
     can_release_savepoints = False
@@ -36,6 +40,7 @@
     has_select_for_update_nowait = False
     has_select_for_update_skip_locked = False
     has_select_for_update_of = False
+    has_select_for_no_key_update = False
     # Does the database's SELECT FOR UPDATE OF syntax require a column rather
     # than a table?
     select_for_update_of_column = False
@@ -88,10 +93,16 @@
     # Does the backend order NULL values as largest or smallest?
     nulls_order_largest = False

+    # Does the backend support NULLS FIRST and NULLS LAST in ORDER BY?
+    supports_order_by_nulls_modifier = True
+
+    # Does the backend orders NULLS FIRST by default?
+    order_by_nulls_first = False
+
     # The database's limit on the number of query parameters.
     max_query_params = None

-    # Can an object have an autoincrement primary key of 0? MySQL says No.
+    # Can an object have an autoincrement primary key of 0?
     allows_auto_pk_0 = True

     # Do we need to NULL a ForeignKey out, or can the constraint check be
@@ -116,40 +127,25 @@
     # which can't do it for MyISAM tables
     can_introspect_foreign_keys = True

-    # Can the backend introspect an AutoField, instead of an IntegerField?
-    can_introspect_autofield = False
-
-    # Can the backend introspect a BigIntegerField, instead of an IntegerField?
-    can_introspect_big_integer_field = True
-
-    # Can the backend introspect an BinaryField, instead of an TextField?
-    can_introspect_binary_field = True
-
-    # Can the backend introspect an DecimalField, instead of an FloatField?
-    can_introspect_decimal_field = True
-
-    # Can the backend introspect a DurationField, instead of a BigIntegerField?
-    can_introspect_duration_field = True
-
-    # Can the backend introspect an IPAddressField, instead of an CharField?
-    can_introspect_ip_address_field = False
-
-    # Can the backend introspect a PositiveIntegerField, instead of an IntegerField?
-    can_introspect_positive_integer_field = False
-
-    # Can the backend introspect a SmallIntegerField, instead of an IntegerField?
-    can_introspect_small_integer_field = False
-
-    # Can the backend introspect a TimeField, instead of a DateTimeField?
-    can_introspect_time_field = True
-
-    # Some backends may not be able to differentiate BigAutoField from other
-    # fields such as AutoField.
-    introspected_big_auto_field_type = 'BigAutoField'
-
-    # Some backends may not be able to differentiate BooleanField from other
-    # fields such as IntegerField.
-    introspected_boolean_field_type = 'BooleanField'
+    # Map fields which some backends may not be able to differentiate to the
+    # field it's introspected as.
+    introspected_field_types = {
+        'AutoField': 'AutoField',
+        'BigAutoField': 'BigAutoField',
+        'BigIntegerField': 'BigIntegerField',
+        'BinaryField': 'BinaryField',
+        'BooleanField': 'BooleanField',
+        'CharField': 'CharField',
+        'DurationField': 'DurationField',
+        'GenericIPAddressField': 'GenericIPAddressField',
+        'IntegerField': 'IntegerField',
+        'PositiveBigIntegerField': 'PositiveBigIntegerField',
+        'PositiveIntegerField': 'PositiveIntegerField',
+        'PositiveSmallIntegerField': 'PositiveSmallIntegerField',
+        'SmallAutoField': 'SmallAutoField',
+        'SmallIntegerField': 'SmallIntegerField',
+        'TimeField': 'TimeField',
+    }

     # Can the backend introspect the column order (ASC/DESC) for indexes?
     supports_index_column_ordering = True
@@ -160,10 +156,6 @@
     # Support for the DISTINCT ON clause
     can_distinct_on_fields = False

-    # Does the backend decide to commit before SAVEPOINT statements
-    # when autocommit is disabled? https://bugs.python.org/issue8145#msg109965
-    autocommits_when_autocommit_is_off = False
-
     # Does the backend prevent running SQL queries in broken transactions?
     atomic_transactions = True

@@ -178,10 +170,18 @@

     # Does it support foreign keys?
     supports_foreign_keys = True
+
+    # Can it create foreign key constraints inline when adding columns?
+    can_create_inline_fk = True
+
+    # Does it automatically index foreign keys?
+    indexes_foreign_keys = True

     # Does it support CHECK constraints?
     supports_column_check_constraints = True
     supports_table_check_constraints = True
+    # Does the backend support introspection of CHECK constraints?
+    can_introspect_check_constraints = True

     # Does the backend support 'pyformat' style ("... %(name)s ...", {'name': value})
     # parameter passing? Note this can be provided by the backend even if not
@@ -240,6 +240,8 @@

     # Does the backend support window expressions (expression OVER (...))?
     supports_over_clause = False
+    supports_frame_range_fixed_distance = False
+    only_supports_unbounded_with_preceding_and_following = False

     # Does the backend support CAST with precision?
     supports_cast_with_precision = True
@@ -256,9 +258,6 @@
     # Does the backend support keyword parameters for cursor.callproc()?
     supports_callproc_kwargs = False

-    # Convert CharField results from bytes to str in database functions.
-    db_functions_convert_bytes_to_str = False
-
     # What formats does the backend EXPLAIN syntax support?
     supported_explain_formats = set()

@@ -280,10 +279,60 @@
     # Does the backend support partial indexes (CREATE INDEX ... WHERE ...)?
     supports_partial_indexes = True
     supports_functions_in_partial_indexes = True
+    # Does the backend support covering indexes (CREATE INDEX ... INCLUDE ...)?
+    supports_covering_indexes = False
+    # Does the backend support indexes on expressions?
+    supports_expression_indexes = True
+    # Does the backend treat COLLATE as an indexed expression?
+    collate_as_index_expression = False

     # Does the database allow more than one constraint or index on the same
     # field(s)?
     allows_multiple_constraints_on_same_fields = True
+
+    # Does the backend support boolean expressions in SELECT and GROUP BY
+    # clauses?
+    supports_boolean_expr_in_select_clause = True
+
+    # Does the backend support JSONField?
+    supports_json_field = True
+    # Can the backend introspect a JSONField?
+    can_introspect_json_field = True
+    # Does the backend support primitives in JSONField?
+    supports_primitives_in_json_field = True
+    # Is there a true datatype for JSON?
+    has_native_json_field = False
+    # Does the backend use PostgreSQL-style JSON operators like '->'?
+    has_json_operators = False
+    # Does the backend support __contains and __contained_by lookups for
+    # a JSONField?
+    supports_json_field_contains = True
+    # Does value__d__contains={'f': 'g'} (without a list around the dict) match
+    # {'d': [{'f': 'g'}]}?
+    json_key_contains_list_matching_requires_list = False
+    # Does the backend support JSONObject() database function?
+    has_json_object_function = True
+
+    # Does the backend support column collations?
+    supports_collation_on_charfield = True
+    supports_collation_on_textfield = True
+    # Does the backend support non-deterministic collations?
+    supports_non_deterministic_collations = True
+
+    # Collation names for use by the Django test suite.
+    test_collations = {
+        'ci': None,  # Case-insensitive.
+        'cs': None,  # Case-sensitive.
+        'non_default': None,  # Non-default.
+        'swedish_ci': None  # Swedish case-insensitive.
+    }
+
+    # A set of dotted paths to tests in Django's test suite that are expected
+    # to fail on this database.
+    django_test_expected_failures = set()
+    # A map of reasons to sets of dotted paths to tests in Django's test suite
+    # that should be skipped for this database.
+    django_test_skips = {}

     def __init__(self, connection):
         self.connection = connection
@@ -306,3 +355,8 @@
             count, = cursor.fetchone()
             cursor.execute('DROP TABLE ROLLBACK_TEST')
         return count == 0
+
+    def allows_group_by_selected_pks_on_model(self, model):
+        if not self.allows_group_by_selected_pks:
+            return False
+        return model._meta.managed
('django/db/backends/base', 'operations.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -8,7 +8,8 @@
 from django.db import NotSupportedError, transaction
 from django.db.backends import utils
 from django.utils import timezone
-from django.utils.encoding import force_text
+from django.utils.encoding import force_str
+from django.utils.regex_helper import _lazy_re_compile


 class BaseDatabaseOperations:
@@ -24,8 +25,12 @@
         'SmallIntegerField': (-32768, 32767),
         'IntegerField': (-2147483648, 2147483647),
         'BigIntegerField': (-9223372036854775808, 9223372036854775807),
+        'PositiveBigIntegerField': (0, 9223372036854775807),
         'PositiveSmallIntegerField': (0, 32767),
         'PositiveIntegerField': (0, 2147483647),
+        'SmallAutoField': (-32768, 32767),
+        'AutoField': (-2147483648, 2147483647),
+        'BigAutoField': (-9223372036854775808, 9223372036854775807),
     }
     set_operators = {
         'union': 'UNION',
@@ -49,6 +54,8 @@
     # Prefix for EXPLAIN queries, or None EXPLAIN isn't supported.
     explain_prefix = None

+    extract_trunc_lookup_pattern = _lazy_re_compile(r"[\w\-_()]+")
+
     def __init__(self, connection):
         self.connection = connection
         self._cache = None
@@ -95,17 +102,14 @@
         """
         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a date_extract_sql() method')

-    def date_interval_sql(self, timedelta):
-        """
-        Implement the date interval functionality for expressions.
-        """
-        raise NotImplementedError('subclasses of BaseDatabaseOperations may require a date_interval_sql() method')
-
-    def date_trunc_sql(self, lookup_type, field_name):
+    def date_trunc_sql(self, lookup_type, field_name, tzname=None):
         """
         Given a lookup_type of 'year', 'month', or 'day', return the SQL that
-        truncates the given date field field_name to a date object with only
-        the given specificity.
+        truncates the given date or datetime field field_name to a date object
+        with only the given specificity.
+
+        If `tzname` is provided, the given value is truncated in a specific
+        timezone.
         """
         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a date_trunc_sql() method.')

@@ -140,11 +144,14 @@
         """
         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a datetime_trunc_sql() method')

-    def time_trunc_sql(self, lookup_type, field_name):
+    def time_trunc_sql(self, lookup_type, field_name, tzname=None):
         """
         Given a lookup_type of 'hour', 'minute' or 'second', return the SQL
-        that truncates the given time field field_name to a time object with
-        only the given specificity.
+        that truncates the given time or datetime field field_name to a time
+        object with only the given specificity.
+
+        If `tzname` is provided, the given value is truncated in a specific
+        timezone.
         """
         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a time_trunc_sql() method')

@@ -173,13 +180,12 @@
         else:
             return ['DISTINCT'], []

-    def fetch_returned_insert_id(self, cursor):
+    def fetch_returned_insert_columns(self, cursor, returning_params):
         """
         Given a cursor object that has just performed an INSERT...RETURNING
-        statement into a table that has an auto-incrementing ID, return the
-        newly created ID.
-        """
-        return cursor.fetchone()[0]
+        statement into a table, return the newly created data.
+        """
+        return cursor.fetchone()

     def field_cast_sql(self, db_type, internal_type):
         """
@@ -197,11 +203,12 @@
         """
         return []

-    def for_update_sql(self, nowait=False, skip_locked=False, of=()):
+    def for_update_sql(self, nowait=False, skip_locked=False, of=(), no_key=False):
         """
         Return the FOR UPDATE SQL clause to lock rows for an update operation.
         """
-        return 'FOR UPDATE%s%s%s' % (
+        return 'FOR%s UPDATE%s%s%s' % (
+            ' NO KEY' if no_key else '',
             ' OF %s' % ', '.join(of) if of else '',
             ' NOWAIT' if nowait else '',
             ' SKIP LOCKED' if skip_locked else '',
@@ -218,10 +225,10 @@
     def limit_offset_sql(self, low_mark, high_mark):
         """Return LIMIT/OFFSET SQL clause."""
         limit, offset = self._get_limit_offset_params(low_mark, high_mark)
-        return '%s%s' % (
-            (' LIMIT %d' % limit) if limit else '',
-            (' OFFSET %d' % offset) if offset else '',
-        )
+        return ' '.join(sql for sql in (
+            ('LIMIT %d' % limit) if limit else None,
+            ('OFFSET %d' % offset) if offset else None,
+        ) if sql)

     def last_executed_query(self, cursor, sql, params):
         """
@@ -235,7 +242,7 @@
         """
         # Convert params to contain string values.
         def to_string(s):
-            return force_text(s, strings_only=True, errors='replace')
+            return force_str(s, strings_only=True, errors='replace')
         if isinstance(params, (list, tuple)):
             u_params = tuple(to_string(val) for val in params)
         elif params is None:
@@ -311,12 +318,11 @@
         """
         return value

-    def return_insert_id(self):
-        """
-        For backends that support returning the last insert ID as part of an
-        insert query, return the SQL and params to append to the INSERT query.
-        The returned fragment should contain a format string to hold the
-        appropriate column.
+    def return_insert_columns(self, fields):
+        """
+        For backends that support returning columns as part of an insert query,
+        return the SQL and params to append to the INSERT query. The returned
+        fragment should contain a format string to hold the appropriate column.
         """
         pass

@@ -337,10 +343,6 @@
         """
         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a quote_name() method')

-    def random_function_sql(self):
-        """Return an SQL expression that returns a random value."""
-        return 'RANDOM()'
-
     def regex_lookup(self, lookup_type):
         """
         Return the string to use in a query when performing regular expression
@@ -380,15 +382,17 @@
         """
         return ''

-    def sql_flush(self, style, tables, sequences, allow_cascade=False):
+    def sql_flush(self, style, tables, *, reset_sequences=False, allow_cascade=False):
         """
         Return a list of SQL statements required to remove all data from
         the given database tables (without actually removing the tables
-        themselves) and the SQL statements required to reset the sequences
-        passed in `sequences`.
+        themselves).

         The `style` argument is a Style object as returned by either
         color_style() or no_style() in django.core.management.color.
+
+        If `reset_sequences` is True, the list includes SQL statements required
+        to reset the sequences.

         The `allow_cascade` argument determines whether truncation may cascade
         to tables with foreign keys pointing the tables being truncated.
@@ -396,9 +400,12 @@
         """
         raise NotImplementedError('subclasses of BaseDatabaseOperations must provide an sql_flush() method')

-    def execute_sql_flush(self, using, sql_list):
+    def execute_sql_flush(self, sql_list):
         """Execute a list of SQL statements to flush the database."""
-        with transaction.atomic(using=using, savepoint=self.connection.features.can_rollback_ddl):
+        with transaction.atomic(
+            using=self.connection.alias,
+            savepoint=self.connection.features.can_rollback_ddl,
+        ):
             with self.connection.cursor() as cursor:
                 for sql in sql_list:
                     cursor.execute(sql)
@@ -578,6 +585,13 @@
         """
         pass

+    def conditional_expression_supported_in_where_clause(self, expression):
+        """
+        Return True, if the conditional expression is supported in the WHERE
+        clause.
+        """
+        return True
+
     def combine_expression(self, connector, sub_expressions):
         """
         Combine a list of subexpressions into a single expression, using
@@ -617,7 +631,7 @@
         if self.connection.features.supports_temporal_subtraction:
             lhs_sql, lhs_params = lhs
             rhs_sql, rhs_params = rhs
-            return "(%s - %s)" % (lhs_sql, rhs_sql), lhs_params + rhs_params
+            return '(%s - %s)' % (lhs_sql, rhs_sql), (*lhs_params, *rhs_params)
         raise NotSupportedError("This backend does not support %s subtraction." % internal_type)

     def window_frame_start(self, start):
@@ -649,7 +663,16 @@
         return self.window_frame_start(start), self.window_frame_end(end)

     def window_frame_range_start_end(self, start=None, end=None):
-        return self.window_frame_rows_start_end(start, end)
+        start_, end_ = self.window_frame_rows_start_end(start, end)
+        if (
+            self.connection.features.only_supports_unbounded_with_preceding_and_following and
+            ((start and start < 0) or (end and end > 0))
+        ):
+            raise NotSupportedError(
+                '%s only supports UNBOUNDED together with PRECEDING and '
+                'FOLLOWING.' % self.connection.display_name
+            )
+        return start_, end_

     def explain_query_prefix(self, format=None, **options):
         if not self.connection.features.supports_explaining_query_execution:
('django/db/backends/base', 'introspection.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,7 +4,11 @@
 TableInfo = namedtuple('TableInfo', ['name', 'type'])

 # Structure returned by the DB-API cursor.description interface (PEP 249)
-FieldInfo = namedtuple('FieldInfo', 'name type_code display_size internal_size precision scale null_ok default')
+FieldInfo = namedtuple(
+    'FieldInfo',
+    'name type_code display_size internal_size precision scale null_ok '
+    'default collation'
+)


 class BaseDatabaseIntrospection:
@@ -54,6 +58,26 @@
         """
         raise NotImplementedError('subclasses of BaseDatabaseIntrospection may require a get_table_list() method')

+    def get_table_description(self, cursor, table_name):
+        """
+        Return a description of the table with the DB-API cursor.description
+        interface.
+        """
+        raise NotImplementedError(
+            'subclasses of BaseDatabaseIntrospection may require a '
+            'get_table_description() method.'
+        )
+
+    def get_migratable_models(self):
+        from django.apps import apps
+        from django.db import router
+        return (
+            model
+            for app_config in apps.get_app_configs()
+            for model in router.get_migratable_models(app_config, self.connection.alias)
+            if model._meta.can_migrate(self.connection)
+        )
+
     def django_table_names(self, only_existing=False, include_views=True):
         """
         Return a list of all table names that have associated Django models and
@@ -61,18 +85,15 @@

         If only_existing is True, include only the tables in the database.
         """
-        from django.apps import apps
-        from django.db import router
         tables = set()
-        for app_config in apps.get_app_configs():
-            for model in router.get_migratable_models(app_config, self.connection.alias):
-                if not model._meta.managed:
-                    continue
-                tables.add(model._meta.db_table)
-                tables.update(
-                    f.m2m_db_table() for f in model._meta.local_many_to_many
-                    if f.remote_field.through._meta.managed
-                )
+        for model in self.get_migratable_models():
+            if not model._meta.managed:
+                continue
+            tables.add(model._meta.db_table)
+            tables.update(
+                f.m2m_db_table() for f in model._meta.local_many_to_many
+                if f.remote_field.through._meta.managed
+            )
         tables = list(tables)
         if only_existing:
             existing_tables = set(self.table_names(include_views=include_views))
@@ -88,14 +109,9 @@
         Return a set of all models represented by the provided list of table
         names.
         """
-        from django.apps import apps
-        from django.db import router
-        all_models = []
-        for app_config in apps.get_app_configs():
-            all_models.extend(router.get_migratable_models(app_config, self.connection.alias))
         tables = set(map(self.identifier_converter, tables))
         return {
-            m for m in all_models
+            m for m in self.get_migratable_models()
             if self.identifier_converter(m._meta.db_table) in tables
         }

@@ -104,24 +120,20 @@
         Return a list of information about all DB sequences for all models in
         all apps.
         """
-        from django.apps import apps
-        from django.db import router
-
         sequence_list = []
         with self.connection.cursor() as cursor:
-            for app_config in apps.get_app_configs():
-                for model in router.get_migratable_models(app_config, self.connection.alias):
-                    if not model._meta.managed:
-                        continue
-                    if model._meta.swapped:
-                        continue
-                    sequence_list.extend(self.get_sequences(cursor, model._meta.db_table, model._meta.local_fields))
-                    for f in model._meta.local_many_to_many:
-                        # If this is an m2m using an intermediate table,
-                        # we don't need to reset the sequence.
-                        if f.remote_field.through._meta.auto_created:
-                            sequence = self.get_sequences(cursor, f.m2m_db_table())
-                            sequence_list.extend(sequence or [{'table': f.m2m_db_table(), 'column': None}])
+            for model in self.get_migratable_models():
+                if not model._meta.managed:
+                    continue
+                if model._meta.swapped:
+                    continue
+                sequence_list.extend(self.get_sequences(cursor, model._meta.db_table, model._meta.local_fields))
+                for f in model._meta.local_many_to_many:
+                    # If this is an m2m using an intermediate table,
+                    # we don't need to reset the sequence.
+                    if f.remote_field.through._meta.auto_created:
+                        sequence = self.get_sequences(cursor, f.m2m_db_table())
+                        sequence_list.extend(sequence or [{'table': f.m2m_db_table(), 'column': None}])
         return sequence_list

     def get_sequences(self, cursor, table_name, table_fields=()):
@@ -131,6 +143,17 @@
         'name' key can be added if the backend supports named sequences.
         """
         raise NotImplementedError('subclasses of BaseDatabaseIntrospection may require a get_sequences() method')
+
+    def get_relations(self, cursor, table_name):
+        """
+        Return a dictionary of
+        {field_name: (field_name_other_table, other_table)} representing all
+        relationships to the given table.
+        """
+        raise NotImplementedError(
+            'subclasses of BaseDatabaseIntrospection may require a '
+            'get_relations() method.'
+        )

     def get_key_columns(self, cursor, table_name):
         """
('django/db/backends/base', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,4 @@
+import _thread
 import copy
 import threading
 import time
@@ -5,18 +6,18 @@
 from collections import deque
 from contextlib import contextmanager

-import _thread
 import pytz

 from django.conf import settings
 from django.core.exceptions import ImproperlyConfigured
-from django.db import DEFAULT_DB_ALIAS
+from django.db import DEFAULT_DB_ALIAS, DatabaseError
 from django.db.backends import utils
 from django.db.backends.base.validation import BaseDatabaseValidation
 from django.db.backends.signals import connection_created
 from django.db.transaction import TransactionManagementError
-from django.db.utils import DatabaseError, DatabaseErrorWrapper
+from django.db.utils import DatabaseErrorWrapper
 from django.utils import timezone
+from django.utils.asyncio import async_unsafe
 from django.utils.functional import cached_property

 NO_DB_ALIAS = '__no_db__'
@@ -116,17 +117,20 @@
     @cached_property
     def timezone(self):
         """
-        Time zone for datetimes stored as naive values in the database.
-
-        Return a tzinfo object or None.
-
-        This is only needed when time zone support is enabled and the database
-        doesn't support time zones. (When the database supports time zones,
-        the adapter handles aware datetimes so Django doesn't need to.)
+        Return a tzinfo of the database connection time zone.
+
+        This is only used when time zone support is enabled. When a datetime is
+        read from the database, it is always returned in this time zone.
+
+        When the database backend supports time zones, it doesn't matter which
+        time zone Django uses, as long as aware datetimes are used everywhere.
+        Other users connecting to the database can choose their own time zone.
+
+        When the database backend doesn't support time zones, the time zone
+        Django uses may be constrained by the requirements of other users of
+        the database.
         """
         if not settings.USE_TZ:
-            return None
-        elif self.features.supports_timezones:
             return None
         elif self.settings_dict['TIME_ZONE'] is None:
             return timezone.utc
@@ -177,6 +181,7 @@

     # ##### Backend-specific methods for creating connections #####

+    @async_unsafe
     def connect(self):
         """Connect to the database. Assume that the connection is closed."""
         # Check for invalid configurations.
@@ -187,7 +192,7 @@
         self.needs_rollback = False
         # Reset parameters defining when to close the connection
         max_age = self.settings_dict['CONN_MAX_AGE']
-        self.close_at = None if max_age is None else time.time() + max_age
+        self.close_at = None if max_age is None else time.monotonic() + max_age
         self.closed_in_transaction = False
         self.errors_occurred = False
         # Establish the connection
@@ -200,16 +205,13 @@
         self.run_on_commit = []

     def check_settings(self):
-        if self.settings_dict['TIME_ZONE'] is not None:
-            if not settings.USE_TZ:
-                raise ImproperlyConfigured(
-                    "Connection '%s' cannot set TIME_ZONE because USE_TZ is "
-                    "False." % self.alias)
-            elif self.features.supports_timezones:
-                raise ImproperlyConfigured(
-                    "Connection '%s' cannot set TIME_ZONE because its engine "
-                    "handles time zones conversions natively." % self.alias)
-
+        if self.settings_dict['TIME_ZONE'] is not None and not settings.USE_TZ:
+            raise ImproperlyConfigured(
+                "Connection '%s' cannot set TIME_ZONE because USE_TZ is False."
+                % self.alias
+            )
+
+    @async_unsafe
     def ensure_connection(self):
         """Guarantee that a connection to the database is established."""
         if self.connection is None:
@@ -251,10 +253,12 @@

     # ##### Generic wrappers for PEP-249 connection methods #####

+    @async_unsafe
     def cursor(self):
         """Create a cursor, opening a connection if necessary."""
         return self._cursor()

+    @async_unsafe
     def commit(self):
         """Commit a transaction and reset the dirty flag."""
         self.validate_thread_sharing()
@@ -264,6 +268,7 @@
         self.errors_occurred = False
         self.run_commit_hooks_on_set_autocommit_on = True

+    @async_unsafe
     def rollback(self):
         """Roll back a transaction and reset the dirty flag."""
         self.validate_thread_sharing()
@@ -274,6 +279,7 @@
         self.needs_rollback = False
         self.run_on_commit = []

+    @async_unsafe
     def close(self):
         """Close the connection to the database."""
         self.validate_thread_sharing()
@@ -313,6 +319,7 @@

     # ##### Generic savepoint management methods #####

+    @async_unsafe
     def savepoint(self):
         """
         Create a savepoint inside the current transaction. Return an
@@ -333,6 +340,7 @@

         return sid

+    @async_unsafe
     def savepoint_rollback(self, sid):
         """
         Roll back to a savepoint. Do nothing if savepoints are not supported.
@@ -348,6 +356,7 @@
             (sids, func) for (sids, func) in self.run_on_commit if sid not in sids
         ]

+    @async_unsafe
     def savepoint_commit(self, sid):
         """
         Release a savepoint. Do nothing if savepoints are not supported.
@@ -358,6 +367,7 @@
         self.validate_thread_sharing()
         self._savepoint_commit(sid)

+    @async_unsafe
     def clean_savepoints(self):
         """
         Reset the counter used to generate unique savepoint ids in this thread.
@@ -386,7 +396,7 @@
         The usual way to start a transaction is to turn autocommit off.
         SQLite does not properly start a transaction when disabling
         autocommit. To avoid this buggy behavior and to actually enter a new
-        transaction, an explcit BEGIN is required. Using
+        transaction, an explicit BEGIN is required. Using
         force_begin_transaction_with_broken_autocommit=True will issue an
         explicit BEGIN with SQLite. This option will be ignored for other
         backends.
@@ -510,7 +520,7 @@
                     self.close()
                     return

-            if self.close_at is not None and time.time() >= self.close_at:
+            if self.close_at is not None and time.monotonic() >= self.close_at:
                 self.close()
                 return

@@ -596,16 +606,21 @@
             if must_close:
                 self.close()

-    @property
-    def _nodb_connection(self):
-        """
-        Return an alternative connection to be used when there is no need to
-        access the main database, specifically for test db creation/deletion.
-        This also prevents the production database from being exposed to
-        potential child threads while (or after) the test database is destroyed.
-        Refs #10868, #17786, #16969.
-        """
-        return self.__class__({**self.settings_dict, 'NAME': None}, alias=NO_DB_ALIAS)
+    @contextmanager
+    def _nodb_cursor(self):
+        """
+        Return a cursor from an alternative connection to be used when there is
+        no need to access the main database, specifically for test db
+        creation/deletion. This also prevents the production database from
+        being exposed to potential child threads while (or after) the test
+        database is destroyed. Refs #10868, #17786, #16969.
+        """
+        conn = self.__class__({**self.settings_dict, 'NAME': None}, alias=NO_DB_ALIAS)
+        try:
+            with conn.cursor() as cursor:
+                yield cursor
+        finally:
+            conn.close()

     def schema_editor(self, *args, **kwargs):
         """
@@ -617,6 +632,8 @@
         return self.SchemaEditorClass(self, *args, **kwargs)

     def on_commit(self, func):
+        if not callable(func):
+            raise TypeError("on_commit()'s callback must be a callable.")
         if self.in_atomic_block:
             # Transaction in progress; save for execution on commit.
             self.run_on_commit.append((set(self.savepoint_ids), func))
('django/db/backends/base', 'schema.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,10 +2,11 @@
 from datetime import datetime

 from django.db.backends.ddl_references import (
-    Columns, ForeignKeyName, IndexName, Statement, Table,
+    Columns, Expressions, ForeignKeyName, IndexName, Statement, Table,
 )
 from django.db.backends.utils import names_digest, split_identifier
-from django.db.models import Index
+from django.db.models import Deferrable, Index
+from django.db.models.sql import Query
 from django.db.transaction import TransactionManagementError, atomic
 from django.utils import timezone

@@ -28,12 +29,16 @@
     return altered_field.name in field.to_fields


+def _all_related_fields(model):
+    return model._meta._get_fields(forward=False, reverse=True, include_hidden=True)
+
+
 def _related_non_m2m_objects(old_field, new_field):
     # Filter out m2m objects from reverse relations.
     # Return (old_relation, new_relation) tuples.
     return zip(
-        (obj for obj in old_field.model._meta.related_objects if _is_relevant_relation(obj, old_field)),
-        (obj for obj in new_field.model._meta.related_objects if _is_relevant_relation(obj, new_field))
+        (obj for obj in _all_related_fields(old_field.model) if _is_relevant_relation(obj, old_field)),
+        (obj for obj in _all_related_fields(new_field.model) if _is_relevant_relation(obj, new_field)),
     )


@@ -57,11 +62,13 @@
     sql_alter_column_not_null = "ALTER COLUMN %(column)s SET NOT NULL"
     sql_alter_column_default = "ALTER COLUMN %(column)s SET DEFAULT %(default)s"
     sql_alter_column_no_default = "ALTER COLUMN %(column)s DROP DEFAULT"
+    sql_alter_column_no_default_null = sql_alter_column_no_default
+    sql_alter_column_collate = "ALTER COLUMN %(column)s TYPE %(type)s%(collation)s"
     sql_delete_column = "ALTER TABLE %(table)s DROP COLUMN %(column)s CASCADE"
     sql_rename_column = "ALTER TABLE %(table)s RENAME COLUMN %(old_column)s TO %(new_column)s"
     sql_update_with_default = "UPDATE %(table)s SET %(column)s = %(default)s WHERE %(column)s IS NULL"

-    sql_unique_constraint = "UNIQUE (%(columns)s)"
+    sql_unique_constraint = "UNIQUE (%(columns)s)%(deferrable)s"
     sql_check_constraint = "CHECK (%(check)s)"
     sql_delete_constraint = "ALTER TABLE %(table)s DROP CONSTRAINT %(name)s"
     sql_constraint = "CONSTRAINT %(name)s %(constraint)s"
@@ -69,7 +76,7 @@
     sql_create_check = "ALTER TABLE %(table)s ADD CONSTRAINT %(name)s CHECK (%(check)s)"
     sql_delete_check = sql_delete_constraint

-    sql_create_unique = "ALTER TABLE %(table)s ADD CONSTRAINT %(name)s UNIQUE (%(columns)s)"
+    sql_create_unique = "ALTER TABLE %(table)s ADD CONSTRAINT %(name)s UNIQUE (%(columns)s)%(deferrable)s"
     sql_delete_unique = sql_delete_constraint

     sql_create_fk = (
@@ -77,10 +84,11 @@
         "REFERENCES %(to_table)s (%(to_column)s)%(deferrable)s"
     )
     sql_create_inline_fk = None
+    sql_create_column_inline_fk = None
     sql_delete_fk = sql_delete_constraint

-    sql_create_index = "CREATE INDEX %(name)s ON %(table)s (%(columns)s)%(extra)s%(condition)s"
-    sql_create_unique_index = "CREATE UNIQUE INDEX %(name)s ON %(table)s (%(columns)s)%(condition)s"
+    sql_create_index = "CREATE INDEX %(name)s ON %(table)s (%(columns)s)%(include)s%(extra)s%(condition)s"
+    sql_create_unique_index = "CREATE UNIQUE INDEX %(name)s ON %(table)s (%(columns)s)%(include)s%(condition)s"
     sql_delete_index = "DROP INDEX %(name)s"

     sql_create_pk = "ALTER TABLE %(table)s ADD CONSTRAINT %(name)s PRIMARY KEY (%(columns)s)"
@@ -139,6 +147,63 @@
     def quote_name(self, name):
         return self.connection.ops.quote_name(name)

+    def table_sql(self, model):
+        """Take a model and return its table definition."""
+        # Add any unique_togethers (always deferred, as some fields might be
+        # created afterwards, like geometry fields with some backends).
+        for fields in model._meta.unique_together:
+            columns = [model._meta.get_field(field).column for field in fields]
+            self.deferred_sql.append(self._create_unique_sql(model, columns))
+        # Create column SQL, add FK deferreds if needed.
+        column_sqls = []
+        params = []
+        for field in model._meta.local_fields:
+            # SQL.
+            definition, extra_params = self.column_sql(model, field)
+            if definition is None:
+                continue
+            # Check constraints can go on the column SQL here.
+            db_params = field.db_parameters(connection=self.connection)
+            if db_params['check']:
+                definition += ' ' + self.sql_check_constraint % db_params
+            # Autoincrement SQL (for backends with inline variant).
+            col_type_suffix = field.db_type_suffix(connection=self.connection)
+            if col_type_suffix:
+                definition += ' %s' % col_type_suffix
+            params.extend(extra_params)
+            # FK.
+            if field.remote_field and field.db_constraint:
+                to_table = field.remote_field.model._meta.db_table
+                to_column = field.remote_field.model._meta.get_field(field.remote_field.field_name).column
+                if self.sql_create_inline_fk:
+                    definition += ' ' + self.sql_create_inline_fk % {
+                        'to_table': self.quote_name(to_table),
+                        'to_column': self.quote_name(to_column),
+                    }
+                elif self.connection.features.supports_foreign_keys:
+                    self.deferred_sql.append(self._create_fk_sql(model, field, '_fk_%(to_table)s_%(to_column)s'))
+            # Add the SQL to our big list.
+            column_sqls.append('%s %s' % (
+                self.quote_name(field.column),
+                definition,
+            ))
+            # Autoincrement SQL (for backends with post table definition
+            # variant).
+            if field.get_internal_type() in ('AutoField', 'BigAutoField', 'SmallAutoField'):
+                autoinc_sql = self.connection.ops.autoinc_sql(model._meta.db_table, field.column)
+                if autoinc_sql:
+                    self.deferred_sql.extend(autoinc_sql)
+        constraints = [constraint.constraint_sql(model, self) for constraint in model._meta.constraints]
+        sql = self.sql_create_table % {
+            'table': self.quote_name(model._meta.db_table),
+            'definition': ', '.join(constraint for constraint in (*column_sqls, *constraints) if constraint),
+        }
+        if model._meta.db_tablespace:
+            tablespace_sql = self.connection.ops.tablespace_sql(model._meta.db_tablespace)
+            if tablespace_sql:
+                sql += ' ' + tablespace_sql
+        return sql, params
+
     # Field <-> database mapping functions

     def column_sql(self, model, field, include_default=False):
@@ -153,20 +218,32 @@
         # Check for fields that aren't actually columns (e.g. M2M)
         if sql is None:
             return None, None
+        # Collation.
+        collation = getattr(field, 'db_collation', None)
+        if collation:
+            sql += self._collate_sql(collation)
         # Work out nullability
         null = field.null
         # If we were told to include a default value, do so
-        include_default = include_default and not self.skip_default(field)
+        include_default = (
+            include_default and
+            not self.skip_default(field) and
+            # Don't include a default value if it's a nullable field and the
+            # default cannot be dropped in the ALTER COLUMN statement (e.g.
+            # MySQL longtext and longblob).
+            not (null and self.skip_default_on_alter(field))
+        )
         if include_default:
             default_value = self.effective_default(field)
+            column_default = ' DEFAULT ' + self._column_default_sql(field)
             if default_value is not None:
                 if self.connection.features.requires_literal_defaults:
                     # Some databases can't take defaults as a parameter (oracle)
                     # If this is the case, the individual schema backend should
                     # implement prepare_default
-                    sql += " DEFAULT %s" % self.prepare_default(default_value)
+                    sql += column_default % self.prepare_default(default_value)
                 else:
-                    sql += " DEFAULT %s"
+                    sql += column_default
                     params += [default_value]
         # Oracle treats the empty string ('') as null, so coerce the null
         # option whenever '' is a possible value.
@@ -196,6 +273,13 @@
         """
         return False

+    def skip_default_on_alter(self, field):
+        """
+        Some backends don't accept default values for certain columns types
+        (i.e. MySQL longtext and longblob) in the ALTER COLUMN statement.
+        """
+        return False
+
     def prepare_default(self, value):
         """
         Only used for backends which have requires_literal_defaults feature
@@ -204,6 +288,13 @@
             'subclasses of BaseDatabaseSchemaEditor for backends which have '
             'requires_literal_defaults must provide a prepare_default() method'
         )
+
+    def _column_default_sql(self, field):
+        """
+        Return the SQL to use in a DEFAULT clause. The resulting string should
+        contain a '%s' placeholder for a default value.
+        """
+        return '%s'

     @staticmethod
     def _effective_default(field):
@@ -212,9 +303,9 @@
             default = field.get_default()
         elif not field.null and field.blank and field.empty_strings_allowed:
             if field.get_internal_type() == "BinaryField":
-                default = bytes()
+                default = b''
             else:
-                default = str()
+                default = ''
         elif getattr(field, 'auto_now', False) or getattr(field, 'auto_now_add', False):
             default = datetime.now()
             internal_type = field.get_internal_type()
@@ -249,60 +340,7 @@
         Create a table and any accompanying indexes or unique constraints for
         the given `model`.
         """
-        # Create column SQL, add FK deferreds if needed
-        column_sqls = []
-        params = []
-        for field in model._meta.local_fields:
-            # SQL
-            definition, extra_params = self.column_sql(model, field)
-            if definition is None:
-                continue
-            # Check constraints can go on the column SQL here
-            db_params = field.db_parameters(connection=self.connection)
-            if db_params['check']:
-                definition += " " + self.sql_check_constraint % db_params
-            # Autoincrement SQL (for backends with inline variant)
-            col_type_suffix = field.db_type_suffix(connection=self.connection)
-            if col_type_suffix:
-                definition += " %s" % col_type_suffix
-            params.extend(extra_params)
-            # FK
-            if field.remote_field and field.db_constraint:
-                to_table = field.remote_field.model._meta.db_table
-                to_column = field.remote_field.model._meta.get_field(field.remote_field.field_name).column
-                if self.sql_create_inline_fk:
-                    definition += " " + self.sql_create_inline_fk % {
-                        "to_table": self.quote_name(to_table),
-                        "to_column": self.quote_name(to_column),
-                    }
-                elif self.connection.features.supports_foreign_keys:
-                    self.deferred_sql.append(self._create_fk_sql(model, field, "_fk_%(to_table)s_%(to_column)s"))
-            # Add the SQL to our big list
-            column_sqls.append("%s %s" % (
-                self.quote_name(field.column),
-                definition,
-            ))
-            # Autoincrement SQL (for backends with post table definition variant)
-            if field.get_internal_type() in ("AutoField", "BigAutoField"):
-                autoinc_sql = self.connection.ops.autoinc_sql(model._meta.db_table, field.column)
-                if autoinc_sql:
-                    self.deferred_sql.extend(autoinc_sql)
-
-        # Add any unique_togethers (always deferred, as some fields might be
-        # created afterwards, like geometry fields with some backends)
-        for fields in model._meta.unique_together:
-            columns = [model._meta.get_field(field).column for field in fields]
-            self.deferred_sql.append(self._create_unique_sql(model, columns))
-        constraints = [constraint.constraint_sql(model, self) for constraint in model._meta.constraints]
-        # Make the table
-        sql = self.sql_create_table % {
-            "table": self.quote_name(model._meta.db_table),
-            "definition": ", ".join(constraint for constraint in (*column_sqls, *constraints) if constraint),
-        }
-        if model._meta.db_tablespace:
-            tablespace_sql = self.connection.ops.tablespace_sql(model._meta.db_tablespace)
-            if tablespace_sql:
-                sql += ' ' + tablespace_sql
+        sql, params = self.table_sql(model)
         # Prevent using [] as params, in the case a literal '%' is used in the definition
         self.execute(sql, params or None)

@@ -332,20 +370,34 @@

     def add_index(self, model, index):
         """Add an index on a model."""
+        if (
+            index.contains_expressions and
+            not self.connection.features.supports_expression_indexes
+        ):
+            return None
+        # Index.create_sql returns interpolated SQL which makes params=None a
+        # necessity to avoid escaping attempts on execution.
         self.execute(index.create_sql(model, self), params=None)

     def remove_index(self, model, index):
         """Remove an index from a model."""
+        if (
+            index.contains_expressions and
+            not self.connection.features.supports_expression_indexes
+        ):
+            return None
         self.execute(index.remove_sql(model, self))

     def add_constraint(self, model, constraint):
-        """Add a check constraint to a model."""
+        """Add a constraint to a model."""
         sql = constraint.create_sql(model, self)
         if sql:
-            self.execute(sql)
+            # Constraint.create_sql returns interpolated SQL which makes
+            # params=None a necessity to avoid escaping attempts on execution.
+            self.execute(sql, params=None)

     def remove_constraint(self, model, constraint):
-        """Remove a check constraint from a model."""
+        """Remove a constraint from a model."""
         sql = constraint.remove_sql(model, self)
         if sql:
             self.execute(sql)
@@ -376,11 +428,16 @@
         news = {tuple(fields) for fields in new_index_together}
         # Deleted indexes
         for fields in olds.difference(news):
-            self._delete_composed_index(model, fields, {'index': True}, self.sql_delete_index)
+            self._delete_composed_index(
+                model,
+                fields,
+                {'index': True, 'unique': False},
+                self.sql_delete_index,
+            )
         # Created indexes
         for field_names in news.difference(olds):
             fields = [model._meta.get_field(field) for field in field_names]
-            self.execute(self._create_index_sql(model, fields, suffix="_idx"))
+            self.execute(self._create_index_sql(model, fields=fields, suffix='_idx'))

     def _delete_composed_index(self, model, fields, constraint_kwargs, sql):
         meta_constraint_names = {constraint.name for constraint in model._meta.constraints}
@@ -438,6 +495,24 @@
         db_params = field.db_parameters(connection=self.connection)
         if db_params['check']:
             definition += " " + self.sql_check_constraint % db_params
+        if field.remote_field and self.connection.features.supports_foreign_keys and field.db_constraint:
+            constraint_suffix = '_fk_%(to_table)s_%(to_column)s'
+            # Add FK constraint inline, if supported.
+            if self.sql_create_column_inline_fk:
+                to_table = field.remote_field.model._meta.db_table
+                to_column = field.remote_field.model._meta.get_field(field.remote_field.field_name).column
+                namespace, _ = split_identifier(model._meta.db_table)
+                definition += " " + self.sql_create_column_inline_fk % {
+                    'name': self._fk_constraint_name(model, field, constraint_suffix),
+                    'namespace': '%s.' % self.quote_name(namespace) if namespace else '',
+                    'column': self.quote_name(field.column),
+                    'to_table': self.quote_name(to_table),
+                    'to_column': self.quote_name(to_column),
+                    'deferrable': self.connection.ops.deferrable_sql()
+                }
+            # Otherwise, add FK constraints later.
+            else:
+                self.deferred_sql.append(self._create_fk_sql(model, field, constraint_suffix))
         # Build the SQL and run it
         sql = self.sql_create_column % {
             "table": self.quote_name(model._meta.db_table),
@@ -447,7 +522,7 @@
         self.execute(sql, params)
         # Drop the default if we need to
         # (Django usually does not use in-database defaults)
-        if not self.skip_default(field) and self.effective_default(field) is not None:
+        if not self.skip_default_on_alter(field) and self.effective_default(field) is not None:
             changes_sql, params = self._alter_column_default_sql(model, None, field, drop=True)
             sql = self.sql_alter_column % {
                 "table": self.quote_name(model._meta.db_table),
@@ -456,9 +531,6 @@
             self.execute(sql, params)
         # Add an index, if required
         self.deferred_sql.extend(self._field_indexes_sql(model, field))
-        # Add any FK constraints later
-        if field.remote_field and self.connection.features.supports_foreign_keys and field.db_constraint:
-            self.deferred_sql.append(self._create_fk_sql(model, field, "_fk_%(to_table)s_%(to_column)s"))
         # Reset connection if required
         if self.connection.features.connection_persists_old_columns:
             self.connection.close()
@@ -501,6 +573,8 @@
         If `strict` is True, raise errors if the old column does not match
         `old_field` precisely.
         """
+        if not self._field_should_be_altered(old_field, new_field):
+            return
         # Ensure this field is even column-based
         old_db_params = old_field.db_parameters(connection=self.connection)
         old_type = old_db_params['type']
@@ -539,7 +613,11 @@
         """Perform a "physical" (non-ManyToMany) field update."""
         # Drop any FK constraints, we'll remake them later
         fks_dropped = set()
-        if old_field.remote_field and old_field.db_constraint:
+        if (
+            self.connection.features.supports_foreign_keys and
+            old_field.remote_field and
+            old_field.db_constraint
+        ):
             fk_names = self._constraint_names(model, [old_field.column], foreign_key=True)
             if strict and len(fk_names) != 1:
                 raise ValueError("Found wrong number (%s) of foreign key constraints for %s.%s" % (
@@ -569,7 +647,7 @@
         # Drop incoming FK constraints if the field is a primary key or unique,
         # which might be a to_field target, and things are going to change.
         drop_foreign_keys = (
-            (
+            self.connection.features.supports_foreign_keys and (
                 (old_field.primary_key and new_field.primary_key) or
                 (old_field.unique and new_field.unique)
             ) and old_type != new_type
@@ -633,8 +711,15 @@
         actions = []
         null_actions = []
         post_actions = []
+        # Collation change?
+        old_collation = getattr(old_field, 'db_collation', None)
+        new_collation = getattr(new_field, 'db_collation', None)
+        if old_collation != new_collation:
+            # Collation change handles also a type change.
+            fragment = self._alter_column_collation_sql(model, new_field, new_type, new_collation)
+            actions.append(fragment)
         # Type change?
-        if old_type != new_type:
+        elif old_type != new_type:
             fragment, other_actions = self._alter_column_type_sql(model, old_field, new_field, new_type)
             actions.append(fragment)
             post_actions.extend(other_actions)
@@ -645,17 +730,17 @@
         #  3. Replace NULL constraint with NOT NULL
         #  4. Drop the default again.
         # Default change?
-        old_default = self.effective_default(old_field)
-        new_default = self.effective_default(new_field)
-        needs_database_default = (
-            old_field.null and
-            not new_field.null and
-            old_default != new_default and
-            new_default is not None and
-            not self.skip_default(new_field)
-        )
-        if needs_database_default:
-            actions.append(self._alter_column_default_sql(model, old_field, new_field))
+        needs_database_default = False
+        if old_field.null and not new_field.null:
+            old_default = self.effective_default(old_field)
+            new_default = self.effective_default(new_field)
+            if (
+                not self.skip_default_on_alter(new_field) and
+                old_default != new_default and
+                new_default is not None
+            ):
+                needs_database_default = True
+                actions.append(self._alter_column_default_sql(model, old_field, new_field))
         # Nullability change?
         if old_field.null != new_field.null:
             fragment = self._alter_column_null_sql(model, old_field, new_field)
@@ -723,11 +808,11 @@
         # False              | True             | True               | False
         # True               | True             | True               | False
         if (not old_field.db_index or old_field.unique) and new_field.db_index and not new_field.unique:
-            self.execute(self._create_index_sql(model, [new_field]))
+            self.execute(self._create_index_sql(model, fields=[new_field]))
         # Type alteration on primary key? Then we need to alter the column
         # referring to us.
         rels_to_update = []
-        if old_field.primary_key and new_field.primary_key and old_type != new_type:
+        if drop_foreign_keys:
             rels_to_update.extend(_related_non_m2m_objects(old_field, new_field))
         # Changed to become primary key?
         if self._field_became_primary_key(old_field, new_field):
@@ -752,7 +837,7 @@
             for sql, params in other_actions:
                 self.execute(sql, params)
         # Does it have a foreign key?
-        if (new_field.remote_field and
+        if (self.connection.features.supports_foreign_keys and new_field.remote_field and
                 (fks_dropped or not old_field.remote_field or not old_field.db_constraint) and
                 new_field.db_constraint):
             self.execute(self._create_fk_sql(model, new_field, "_fk_%(to_table)s_%(to_column)s"))
@@ -808,7 +893,7 @@
         argument) a default to new_field's column.
         """
         new_default = self.effective_default(new_field)
-        default = '%s'
+        default = self._column_default_sql(new_field)
         params = [new_default]

         if drop:
@@ -821,7 +906,13 @@
             params = []

         new_db_params = new_field.db_parameters(connection=self.connection)
-        sql = self.sql_alter_column_no_default if drop else self.sql_alter_column_default
+        if drop:
+            if new_field.null:
+                sql = self.sql_alter_column_no_default_null
+            else:
+                sql = self.sql_alter_column_no_default
+        else:
+            sql = self.sql_alter_column_default
         return (
             sql % {
                 'column': self.quote_name(new_field.column),
@@ -849,6 +940,16 @@
                 },
                 [],
             ),
+            [],
+        )
+
+    def _alter_column_collation_sql(self, model, new_field, new_type, new_collation):
+        return (
+            self.sql_alter_column_collate % {
+                'column': self.quote_name(new_field.column),
+                'type': new_type,
+                'collation': self._collate_sql(new_collation) if new_collation else '',
+            },
             [],
         )

@@ -912,14 +1013,32 @@
             return ' ' + self.connection.ops.tablespace_sql(db_tablespace)
         return ''

-    def _create_index_sql(self, model, fields, *, name=None, suffix='', using='',
+    def _index_condition_sql(self, condition):
+        if condition:
+            return ' WHERE ' + condition
+        return ''
+
+    def _index_include_sql(self, model, columns):
+        if not columns or not self.connection.features.supports_covering_indexes:
+            return ''
+        return Statement(
+            ' INCLUDE (%(columns)s)',
+            columns=Columns(model._meta.db_table, columns, self.quote_name),
+        )
+
+    def _create_index_sql(self, model, *, fields=None, name=None, suffix='', using='',
                           db_tablespace=None, col_suffixes=(), sql=None, opclasses=(),
-                          condition=None):
-        """
-        Return the SQL statement to create the index for one or several fields.
-        `sql` can be specified if the syntax differs from the standard (GIS
-        indexes, ...).
-        """
+                          condition=None, include=None, expressions=None):
+        """
+        Return the SQL statement to create the index for one or several fields
+        or expressions. `sql` can be specified if the syntax differs from the
+        standard (GIS indexes, ...).
+        """
+        fields = fields or []
+        expressions = expressions or []
+        compiler = Query(model, alias_cols=False).get_compiler(
+            connection=self.connection,
+        )
         tablespace_sql = self._get_index_tablespace_sql(model, fields, db_tablespace=db_tablespace)
         columns = [field.column for field in fields]
         sql_create_index = sql or self.sql_create_index
@@ -936,14 +1055,19 @@
             table=Table(table, self.quote_name),
             name=IndexName(table, columns, suffix, create_index_name),
             using=using,
-            columns=self._index_columns(table, columns, col_suffixes, opclasses),
+            columns=(
+                self._index_columns(table, columns, col_suffixes, opclasses)
+                if columns
+                else Expressions(table, expressions, compiler, self.quote_value)
+            ),
             extra=tablespace_sql,
-            condition=(' WHERE ' + condition) if condition else '',
-        )
-
-    def _delete_index_sql(self, model, name):
+            condition=self._index_condition_sql(condition),
+            include=self._index_include_sql(model, include),
+        )
+
+    def _delete_index_sql(self, model, name, sql=None):
         return Statement(
-            self.sql_delete_index,
+            sql or self.sql_delete_index,
             table=Table(model._meta.db_table, self.quote_name),
             name=self.quote_name(name),
         )
@@ -964,10 +1088,14 @@

         for field_names in model._meta.index_together:
             fields = [model._meta.get_field(field) for field in field_names]
-            output.append(self._create_index_sql(model, fields, suffix="_idx"))
+            output.append(self._create_index_sql(model, fields=fields, suffix='_idx'))

         for index in model._meta.indexes:
-            output.append(index.create_sql(model, self))
+            if (
+                not index.contains_expressions or
+                self.connection.features.supports_expression_indexes
+            ):
+                output.append(index.create_sql(model, self))
         return output

     def _field_indexes_sql(self, model, field):
@@ -976,8 +1104,37 @@
         """
         output = []
         if self._field_should_be_indexed(model, field):
-            output.append(self._create_index_sql(model, [field]))
+            output.append(self._create_index_sql(model, fields=[field]))
         return output
+
+    def _field_should_be_altered(self, old_field, new_field):
+        _, old_path, old_args, old_kwargs = old_field.deconstruct()
+        _, new_path, new_args, new_kwargs = new_field.deconstruct()
+        # Don't alter when:
+        # - changing only a field name
+        # - changing an attribute that doesn't affect the schema
+        # - adding only a db_column and the column name is not changed
+        non_database_attrs = [
+            'blank',
+            'db_column',
+            'editable',
+            'error_messages',
+            'help_text',
+            'limit_choices_to',
+            # Database-level options are not supported, see #21961.
+            'on_delete',
+            'related_name',
+            'related_query_name',
+            'validators',
+            'verbose_name',
+        ]
+        for attr in non_database_attrs:
+            old_kwargs.pop(attr, None)
+            new_kwargs.pop(attr, None)
+        return (
+            self.quote_name(old_field.column) != self.quote_name(new_field.column) or
+            (old_path, old_args, old_kwargs) != (new_path, new_args, new_kwargs)
+        )

     def _field_should_be_indexed(self, model, field):
         return field.db_index and not field.unique
@@ -999,18 +1156,8 @@
         }

     def _create_fk_sql(self, model, field, suffix):
-        def create_fk_name(*args, **kwargs):
-            return self.quote_name(self._create_index_name(*args, **kwargs))
-
         table = Table(model._meta.db_table, self.quote_name)
-        name = ForeignKeyName(
-            model._meta.db_table,
-            [field.column],
-            split_identifier(field.target_field.model._meta.db_table)[1],
-            [field.target_field.column],
-            suffix,
-            create_fk_name,
-        )
+        name = self._fk_constraint_name(model, field, suffix)
         column = Columns(model._meta.db_table, [field.column], self.quote_name)
         to_table = Table(field.target_field.model._meta.db_table, self.quote_name)
         to_column = Columns(field.target_field.model._meta.db_table, [field.target_field.column], self.quote_name)
@@ -1025,26 +1172,76 @@
             deferrable=deferrable,
         )

+    def _fk_constraint_name(self, model, field, suffix):
+        def create_fk_name(*args, **kwargs):
+            return self.quote_name(self._create_index_name(*args, **kwargs))
+
+        return ForeignKeyName(
+            model._meta.db_table,
+            [field.column],
+            split_identifier(field.target_field.model._meta.db_table)[1],
+            [field.target_field.column],
+            suffix,
+            create_fk_name,
+        )
+
     def _delete_fk_sql(self, model, name):
         return self._delete_constraint_sql(self.sql_delete_fk, model, name)

-    def _unique_sql(self, model, fields, name, condition=None):
-        if condition:
-            # Databases support conditional unique constraints via a unique
-            # index.
-            sql = self._create_unique_sql(model, fields, name=name, condition=condition)
+    def _deferrable_constraint_sql(self, deferrable):
+        if deferrable is None:
+            return ''
+        if deferrable == Deferrable.DEFERRED:
+            return ' DEFERRABLE INITIALLY DEFERRED'
+        if deferrable == Deferrable.IMMEDIATE:
+            return ' DEFERRABLE INITIALLY IMMEDIATE'
+
+    def _unique_sql(
+        self, model, fields, name, condition=None, deferrable=None,
+        include=None, opclasses=None,
+    ):
+        if (
+            deferrable and
+            not self.connection.features.supports_deferrable_unique_constraints
+        ):
+            return None
+        if condition or include or opclasses:
+            # Databases support conditional and covering unique constraints via
+            # a unique index.
+            sql = self._create_unique_sql(
+                model,
+                fields,
+                name=name,
+                condition=condition,
+                include=include,
+                opclasses=opclasses,
+            )
             if sql:
                 self.deferred_sql.append(sql)
             return None
         constraint = self.sql_unique_constraint % {
             'columns': ', '.join(map(self.quote_name, fields)),
+            'deferrable': self._deferrable_constraint_sql(deferrable),
         }
         return self.sql_constraint % {
             'name': self.quote_name(name),
             'constraint': constraint,
         }

-    def _create_unique_sql(self, model, columns, name=None, condition=None):
+    def _create_unique_sql(
+        self, model, columns, name=None, condition=None, deferrable=None,
+        include=None, opclasses=None,
+    ):
+        if (
+            (
+                deferrable and
+                not self.connection.features.supports_deferrable_unique_constraints
+            ) or
+            (condition and not self.connection.features.supports_partial_indexes) or
+            (include and not self.connection.features.supports_covering_indexes)
+        ):
+            return None
+
         def create_unique_name(*args, **kwargs):
             return self.quote_name(self._create_index_name(*args, **kwargs))

@@ -1053,30 +1250,39 @@
             name = IndexName(model._meta.db_table, columns, '_uniq', create_unique_name)
         else:
             name = self.quote_name(name)
-        columns = Columns(table, columns, self.quote_name)
-        if condition:
-            return Statement(
-                self.sql_create_unique_index,
-                table=table,
-                name=name,
-                columns=columns,
-                condition=' WHERE ' + condition,
-            ) if self.connection.features.supports_partial_indexes else None
+        columns = self._index_columns(table, columns, col_suffixes=(), opclasses=opclasses)
+        if condition or include or opclasses:
+            sql = self.sql_create_unique_index
         else:
-            return Statement(
-                self.sql_create_unique,
-                table=table,
-                name=name,
-                columns=columns,
-            )
-
-    def _delete_unique_sql(self, model, name, condition=None):
-        if condition:
-            return (
-                self._delete_constraint_sql(self.sql_delete_index, model, name)
-                if self.connection.features.supports_partial_indexes else None
-            )
-        return self._delete_constraint_sql(self.sql_delete_unique, model, name)
+            sql = self.sql_create_unique
+        return Statement(
+            sql,
+            table=table,
+            name=name,
+            columns=columns,
+            condition=self._index_condition_sql(condition),
+            deferrable=self._deferrable_constraint_sql(deferrable),
+            include=self._index_include_sql(model, include),
+        )
+
+    def _delete_unique_sql(
+        self, model, name, condition=None, deferrable=None, include=None,
+        opclasses=None,
+    ):
+        if (
+            (
+                deferrable and
+                not self.connection.features.supports_deferrable_unique_constraints
+            ) or
+            (condition and not self.connection.features.supports_partial_indexes) or
+            (include and not self.connection.features.supports_covering_indexes)
+        ):
+            return None
+        if condition or include or opclasses:
+            sql = self.sql_delete_index
+        else:
+            sql = self.sql_delete_unique
+        return self._delete_constraint_sql(sql, model, name)

     def _check_sql(self, name, check):
         return self.sql_constraint % {
@@ -1155,6 +1361,9 @@
     def _delete_primary_key_sql(self, model, name):
         return self._delete_constraint_sql(self.sql_delete_pk, model, name)

+    def _collate_sql(self, collation):
+        return ' COLLATE ' + self.quote_name(collation)
+
     def remove_procedure(self, procedure_name, param_types=()):
         sql = self.sql_delete_procedure % {
             'procedure': self.quote_name(procedure_name),
('django/db/models', 'options.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,18 +1,17 @@
+import bisect
 import copy
 import inspect
-from bisect import bisect
-from collections import OrderedDict, defaultdict
+from collections import defaultdict

 from django.apps import apps
 from django.conf import settings
 from django.core.exceptions import FieldDoesNotExist, ImproperlyConfigured
 from django.db import connections
-from django.db.models import Manager
-from django.db.models.fields import AutoField
-from django.db.models.fields.proxy import OrderWrt
+from django.db.models import AutoField, Manager, OrderWrt, UniqueConstraint
 from django.db.models.query_utils import PathInfo
 from django.utils.datastructures import ImmutableList, OrderedSet
 from django.utils.functional import cached_property
+from django.utils.module_loading import import_string
 from django.utils.text import camel_case_to_spaces, format_lazy
 from django.utils.translation import override

@@ -33,8 +32,6 @@
     'select_on_save', 'default_related_name', 'required_db_features',
     'required_db_vendor', 'base_manager_name', 'default_manager_name',
     'indexes', 'constraints',
-    # For backwards compatibility with Django 1.11. RemovedInDjango30Warning
-    'manager_inheritance_from_future',
 )


@@ -119,7 +116,7 @@
         # concrete models, the concrete_model is always the class itself.
         self.concrete_model = None
         self.swappable = None
-        self.parents = OrderedDict()
+        self.parents = {}
         self.auto_created = False

         # List of all lookups defined in ForeignKey 'limit_choices_to' options
@@ -182,6 +179,12 @@

             self.unique_together = normalize_together(self.unique_together)
             self.index_together = normalize_together(self.index_together)
+            # App label/class name interpolation for names of constraints and
+            # indexes.
+            if not getattr(cls._meta, 'abstract', False):
+                for attr_name in {'constraints', 'indexes'}:
+                    objs = getattr(self, attr_name, [])
+                    setattr(self, attr_name, self._format_names_with_class(cls, objs))

             # verbose_name_plural is a special case because it uses a 's'
             # by default.
@@ -202,6 +205,49 @@
         if not self.db_table:
             self.db_table = "%s_%s" % (self.app_label, self.model_name)
             self.db_table = truncate_name(self.db_table, connection.ops.max_name_length())
+
+    def _format_names_with_class(self, cls, objs):
+        """App label/class name interpolation for object names."""
+        new_objs = []
+        for obj in objs:
+            obj = obj.clone()
+            obj.name = obj.name % {
+                'app_label': cls._meta.app_label.lower(),
+                'class': cls.__name__.lower(),
+            }
+            new_objs.append(obj)
+        return new_objs
+
+    def _get_default_pk_class(self):
+        pk_class_path = getattr(
+            self.app_config,
+            'default_auto_field',
+            settings.DEFAULT_AUTO_FIELD,
+        )
+        if self.app_config and self.app_config._is_default_auto_field_overridden:
+            app_config_class = type(self.app_config)
+            source = (
+                f'{app_config_class.__module__}.'
+                f'{app_config_class.__qualname__}.default_auto_field'
+            )
+        else:
+            source = 'DEFAULT_AUTO_FIELD'
+        if not pk_class_path:
+            raise ImproperlyConfigured(f'{source} must not be empty.')
+        try:
+            pk_class = import_string(pk_class_path)
+        except ImportError as e:
+            msg = (
+                f"{source} refers to the module '{pk_class_path}' that could "
+                f"not be imported."
+            )
+            raise ImproperlyConfigured(msg) from e
+        if not issubclass(pk_class, AutoField):
+            raise ValueError(
+                f"Primary key '{pk_class_path}' referred by {source} must "
+                f"subclass AutoField."
+            )
+        return pk_class

     def _prepare(self, model):
         if self.order_with_respect_to:
@@ -235,12 +281,9 @@
                     field = already_created[0]
                 field.primary_key = True
                 self.setup_pk(field)
-                if not field.remote_field.parent_link:
-                    raise ImproperlyConfigured(
-                        'Add parent_link=True to %s.' % field,
-                    )
             else:
-                auto = AutoField(verbose_name='ID', primary_key=True, auto_created=True)
+                pk_class = self._get_default_pk_class()
+                auto = pk_class(verbose_name='ID', primary_key=True, auto_created=True)
                 model.add_to_class('id', auto)

     def add_manager(self, manager):
@@ -255,9 +298,9 @@
         if private:
             self.private_fields.append(field)
         elif field.is_relation and field.many_to_many:
-            self.local_many_to_many.insert(bisect(self.local_many_to_many, field), field)
+            bisect.insort(self.local_many_to_many, field)
         else:
-            self.local_fields.insert(bisect(self.local_fields, field), field)
+            bisect.insort(self.local_fields, field)
             self.setup_pk(field)

         # If the field being added is a relation to another known field,
@@ -295,7 +338,7 @@
         return '<Options for %s>' % self.object_name

     def __str__(self):
-        return "%s.%s" % (self.app_label, self.model_name)
+        return self.label_lower

     def can_migrate(self, connection):
         """
@@ -688,7 +731,8 @@
             )
             for f in fields_with_relations:
                 if not isinstance(f.remote_field.model, str):
-                    related_objects_graph[f.remote_field.model._meta.concrete_model._meta].append(f)
+                    remote_label = f.remote_field.model._meta.concrete_model._meta.label
+                    related_objects_graph[remote_label].append(f)

         for model in all_models:
             # Set the relation_tree using the internal __dict__. In this way
@@ -696,7 +740,7 @@
             # __dict__ takes precedence over a data descriptor (such as
             # @cached_property). This means that the _meta._relation_tree is
             # only called if related_objects is not in __dict__.
-            related_objects = related_objects_graph[model._meta.concrete_model._meta]
+            related_objects = related_objects_graph[model._meta.concrete_model._meta.label]
             model._meta.__dict__['_relation_tree'] = related_objects
         # It seems it is possible that self is not in all_models, so guard
         # against that with default for get().
@@ -818,6 +862,18 @@
         return fields

     @cached_property
+    def total_unique_constraints(self):
+        """
+        Return a list of total unique constraints. Useful for determining set
+        of fields guaranteed to be unique for all rows.
+        """
+        return [
+            constraint
+            for constraint in self.constraints
+            if isinstance(constraint, UniqueConstraint) and constraint.condition is None
+        ]
+
+    @cached_property
     def _property_names(self):
         """Return a set of the names of the properties defined on the model."""
         names = []
@@ -826,3 +882,14 @@
             if isinstance(attr, property):
                 names.append(name)
         return frozenset(names)
+
+    @cached_property
+    def db_returning_fields(self):
+        """
+        Private API intended only to be used by Django itself.
+        Fields to be returned after a database insert.
+        """
+        return [
+            field for field in self._get_fields(forward=True, reverse=False, include_parents=PROXY_PARENTS)
+            if getattr(field, 'db_returning', False)
+        ]
('django/db/models', 'signals.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,7 +3,7 @@
 from django.db.models.utils import make_model_tuple
 from django.dispatch import Signal

-class_prepared = Signal(providing_args=["class"])
+class_prepared = Signal()


 class ModelSignal(Signal):
@@ -34,20 +34,16 @@
         )


-pre_init = ModelSignal(providing_args=["instance", "args", "kwargs"], use_caching=True)
-post_init = ModelSignal(providing_args=["instance"], use_caching=True)
+pre_init = ModelSignal(use_caching=True)
+post_init = ModelSignal(use_caching=True)

-pre_save = ModelSignal(providing_args=["instance", "raw", "using", "update_fields"],
-                       use_caching=True)
-post_save = ModelSignal(providing_args=["instance", "raw", "created", "using", "update_fields"], use_caching=True)
+pre_save = ModelSignal(use_caching=True)
+post_save = ModelSignal(use_caching=True)

-pre_delete = ModelSignal(providing_args=["instance", "using"], use_caching=True)
-post_delete = ModelSignal(providing_args=["instance", "using"], use_caching=True)
+pre_delete = ModelSignal(use_caching=True)
+post_delete = ModelSignal(use_caching=True)

-m2m_changed = ModelSignal(
-    providing_args=["action", "instance", "reverse", "model", "pk_set", "using"],
-    use_caching=True,
-)
+m2m_changed = ModelSignal(use_caching=True)

-pre_migrate = Signal(providing_args=["app_config", "verbosity", "interactive", "using", "apps", "plan"])
-post_migrate = Signal(providing_args=["app_config", "verbosity", "interactive", "using", "apps", "plan"])
+pre_migrate = Signal()
+post_migrate = Signal()
('django/db/models', 'query.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -5,35 +5,31 @@
 import copy
 import operator
 import warnings
-from collections import OrderedDict, namedtuple
-from functools import lru_cache
 from itertools import chain

+import django
 from django.conf import settings
 from django.core import exceptions
 from django.db import (
-    DJANGO_VERSION_PICKLE_KEY, IntegrityError, connections, router,
-    transaction,
+    DJANGO_VERSION_PICKLE_KEY, IntegrityError, NotSupportedError, connections,
+    router, transaction,
 )
-from django.db.models import DateField, DateTimeField, sql
+from django.db.models import AutoField, DateField, DateTimeField, sql
 from django.db.models.constants import LOOKUP_SEP
 from django.db.models.deletion import Collector
-from django.db.models.expressions import Case, Expression, F, Value, When
-from django.db.models.fields import AutoField
+from django.db.models.expressions import Case, Expression, F, Ref, Value, When
 from django.db.models.functions import Cast, Trunc
-from django.db.models.query_utils import FilteredRelation, InvalidQuery, Q
+from django.db.models.query_utils import FilteredRelation, Q
 from django.db.models.sql.constants import CURSOR, GET_ITERATOR_CHUNK_SIZE
-from django.db.utils import NotSupportedError
+from django.db.models.utils import create_namedtuple_class, resolve_callables
 from django.utils import timezone
-from django.utils.deprecation import RemovedInDjango30Warning
 from django.utils.functional import cached_property, partition
-from django.utils.version import get_version
+
+# The maximum number of results to fetch in a get() query.
+MAX_GET_RESULTS = 21

 # The maximum number of items to display in a QuerySet.__repr__
 REPR_OUTPUT_SIZE = 20
-
-# Pull into this namespace for backwards compatibility.
-EmptyResultSet = sql.EmptyResultSet


 class BaseIterable:
@@ -150,13 +146,6 @@
     namedtuple for each row.
     """

-    @staticmethod
-    @lru_cache()
-    def create_namedtuple_class(*names):
-        # Cache namedtuple() with @lru_cache() since it's too slow to be
-        # called for every QuerySet evaluation.
-        return namedtuple('Row', names)
-
     def __iter__(self):
         queryset = self.queryset
         if queryset._fields:
@@ -164,7 +153,7 @@
         else:
             query = queryset.query
             names = [*query.extra_select, *query.values_select, *query.annotation_select]
-        tuple_class = self.create_namedtuple_class(*names)
+        tuple_class = create_namedtuple_class(*names)
         new = tuple.__new__
         for row in super().__iter__():
             yield new(tuple_class, row)
@@ -190,7 +179,7 @@
         self.model = model
         self._db = using
         self._hints = hints or {}
-        self.query = query or sql.Query(self.model)
+        self._query = query or sql.Query(self.model)
         self._result_cache = None
         self._sticky_filter = False
         self._for_write = False
@@ -199,6 +188,22 @@
         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
         self._iterable_class = ModelIterable
         self._fields = None
+        self._defer_next_filter = False
+        self._deferred_filter = None
+
+    @property
+    def query(self):
+        if self._deferred_filter:
+            negate, args, kwargs = self._deferred_filter
+            self._filter_or_exclude_inplace(negate, args, kwargs)
+            self._deferred_filter = None
+        return self._query
+
+    @query.setter
+    def query(self, value):
+        if value.values_select:
+            self._iterable_class = ValuesIterable
+        self._query = value

     def as_manager(cls):
         # Address the circular dependency between `Queryset` and `Manager`.
@@ -226,24 +231,25 @@
     def __getstate__(self):
         # Force the cache to be fully populated.
         self._fetch_all()
-        return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
+        return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}

     def __setstate__(self, state):
-        msg = None
         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
         if pickled_version:
-            current_version = get_version()
-            if current_version != pickled_version:
-                msg = (
+            if pickled_version != django.__version__:
+                warnings.warn(
                     "Pickled queryset instance's Django version %s does not "
-                    "match the current version %s." % (pickled_version, current_version)
+                    "match the current version %s."
+                    % (pickled_version, django.__version__),
+                    RuntimeWarning,
+                    stacklevel=2,
                 )
         else:
-            msg = "Pickled queryset instance's Django version is not specified."
-
-        if msg:
-            warnings.warn(msg, RuntimeWarning, stacklevel=2)
-
+            warnings.warn(
+                "Pickled queryset instance's Django version is not specified.",
+                RuntimeWarning,
+                stacklevel=2,
+            )
         self.__dict__.update(state)

     def __repr__(self):
@@ -281,7 +287,10 @@
     def __getitem__(self, k):
         """Retrieve an item or slice from the set of results."""
         if not isinstance(k, (int, slice)):
-            raise TypeError
+            raise TypeError(
+                'QuerySet indices must be integers or slices, not %s.'
+                % type(k).__name__
+            )
         assert ((not isinstance(k, slice) and (k >= 0)) or
                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
                  (k.stop is None or k.stop >= 0))), \
@@ -307,6 +316,9 @@
         qs.query.set_limits(k, k + 1)
         qs._fetch_all()
         return qs._result_cache[0]
+
+    def __class_getitem__(cls, *args, **kwargs):
+        return cls

     def __and__(self, other):
         self._merge_sanity_check(other)
@@ -374,8 +386,16 @@
         query = self.query.chain()
         for (alias, aggregate_expr) in kwargs.items():
             query.add_annotation(aggregate_expr, alias, is_summary=True)
-            if not query.annotations[alias].contains_aggregate:
+            annotation = query.annotations[alias]
+            if not annotation.contains_aggregate:
                 raise TypeError("%s is not an aggregate expression" % alias)
+            for expr in annotation.get_source_expressions():
+                if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:
+                    name = expr.refs
+                    raise exceptions.FieldError(
+                        "Cannot compute %s('%s'): '%s' is an aggregate"
+                        % (annotation.name, name, name)
+                    )
         return query.get_aggregation(self.db, kwargs)

     def count(self):
@@ -396,9 +416,18 @@
         Perform the query and return a single object matching the given
         keyword arguments.
         """
-        clone = self.filter(*args, **kwargs)
+        if self.query.combinator and (args or kwargs):
+            raise NotSupportedError(
+                'Calling QuerySet.get(...) with filters after %s() is not '
+                'supported.' % self.query.combinator
+            )
+        clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
         if self.query.can_filter() and not self.query.distinct_fields:
             clone = clone.order_by()
+        limit = None
+        if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
+            limit = MAX_GET_RESULTS
+            clone.query.set_limits(high=limit)
         num = len(clone)
         if num == 1:
             return clone._result_cache[0]
@@ -408,8 +437,10 @@
                 self.model._meta.object_name
             )
         raise self.model.MultipleObjectsReturned(
-            "get() returned more than one %s -- it returned %s!" %
-            (self.model._meta.object_name, num)
+            'get() returned more than one %s -- it returned %s!' % (
+                self.model._meta.object_name,
+                num if not limit or num < limit else 'more than %s' % (limit - 1),
+            )
         )

     def create(self, **kwargs):
@@ -422,21 +453,23 @@
         obj.save(force_insert=True, using=self.db)
         return obj

-    def _populate_pk_values(self, objs):
+    def _prepare_for_bulk_create(self, objs):
         for obj in objs:
             if obj.pk is None:
+                # Populate new PK values.
                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
+            obj._prepare_related_fields_for_save(operation_name='bulk_create')

     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
         """
         Insert each of the instances into the database. Do *not* call
         save() on each of the instances, do not send any pre/post_save
         signals, and do not set the primary key attribute if it is an
-        autoincrement field (except if features.can_return_ids_from_bulk_insert=True).
+        autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
         Multi-table models are not supported.
         """
         # When you bulk insert you don't get the primary keys back (if it's an
-        # autoincrement, except if can_return_ids_from_bulk_insert=True), so
+        # autoincrement, except if can_return_rows_from_bulk_insert=True), so
         # you can't insert into the child tables which references this. There
         # are two workarounds:
         # 1) This could be implemented if you didn't have an autoincrement pk
@@ -459,23 +492,33 @@
             return objs
         self._for_write = True
         connection = connections[self.db]
-        fields = self.model._meta.concrete_fields
+        opts = self.model._meta
+        fields = opts.concrete_fields
         objs = list(objs)
-        self._populate_pk_values(objs)
+        self._prepare_for_bulk_create(objs)
         with transaction.atomic(using=self.db, savepoint=False):
             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
             if objs_with_pk:
-                self._batched_insert(objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts)
+                returned_columns = self._batched_insert(
+                    objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
+                )
+                for obj_with_pk, results in zip(objs_with_pk, returned_columns):
+                    for result, field in zip(results, opts.db_returning_fields):
+                        if field != opts.pk:
+                            setattr(obj_with_pk, field.attname, result)
                 for obj_with_pk in objs_with_pk:
                     obj_with_pk._state.adding = False
                     obj_with_pk._state.db = self.db
             if objs_without_pk:
                 fields = [f for f in fields if not isinstance(f, AutoField)]
-                ids = self._batched_insert(objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts)
-                if connection.features.can_return_ids_from_bulk_insert and not ignore_conflicts:
-                    assert len(ids) == len(objs_without_pk)
-                for obj_without_pk, pk in zip(objs_without_pk, ids):
-                    obj_without_pk.pk = pk
+                returned_columns = self._batched_insert(
+                    objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
+                )
+                if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
+                    assert len(returned_columns) == len(objs_without_pk)
+                for obj_without_pk, results in zip(objs_without_pk, returned_columns):
+                    for result, field in zip(results, opts.db_returning_fields):
+                        setattr(obj_without_pk, field.attname, result)
                     obj_without_pk._state.adding = False
                     obj_without_pk._state.db = self.db

@@ -538,7 +581,17 @@
             return self.get(**kwargs), False
         except self.model.DoesNotExist:
             params = self._extract_model_params(defaults, **kwargs)
-            return self._create_object_from_params(kwargs, params)
+            # Try to create an object using passed params.
+            try:
+                with transaction.atomic(using=self.db):
+                    params = dict(resolve_callables(params))
+                    return self.create(**params), True
+            except IntegrityError:
+                try:
+                    return self.get(**kwargs), False
+                except self.model.DoesNotExist:
+                    pass
+                raise

     def update_or_create(self, defaults=None, **kwargs):
         """
@@ -550,42 +603,20 @@
         defaults = defaults or {}
         self._for_write = True
         with transaction.atomic(using=self.db):
-            try:
-                obj = self.select_for_update().get(**kwargs)
-            except self.model.DoesNotExist:
-                params = self._extract_model_params(defaults, **kwargs)
-                # Lock the row so that a concurrent update is blocked until
-                # after update_or_create() has performed its save.
-                obj, created = self._create_object_from_params(kwargs, params, lock=True)
-                if created:
-                    return obj, created
-            for k, v in defaults.items():
-                setattr(obj, k, v() if callable(v) else v)
+            # Lock the row so that a concurrent update is blocked until
+            # update_or_create() has performed its save.
+            obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
+            if created:
+                return obj, created
+            for k, v in resolve_callables(defaults):
+                setattr(obj, k, v)
             obj.save(using=self.db)
         return obj, False

-    def _create_object_from_params(self, lookup, params, lock=False):
-        """
-        Try to create an object using passed params. Used by get_or_create()
-        and update_or_create().
-        """
-        try:
-            with transaction.atomic(using=self.db):
-                params = {k: v() if callable(v) else v for k, v in params.items()}
-                obj = self.create(**params)
-            return obj, True
-        except IntegrityError as e:
-            try:
-                qs = self.select_for_update() if lock else self
-                return qs.get(**lookup), False
-            except self.model.DoesNotExist:
-                pass
-            raise e
-
     def _extract_model_params(self, defaults, **kwargs):
         """
         Prepare `params` for creating a model instance based on the given
-        kwargs; for use by get_or_create() and update_or_create().
+        kwargs; for use by get_or_create().
         """
         defaults = defaults or {}
         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
@@ -607,22 +638,12 @@
                 ))
         return params

-    def _earliest(self, *fields, field_name=None):
+    def _earliest(self, *fields):
         """
         Return the earliest object according to fields (if given) or by the
         model's Meta.get_latest_by.
         """
-        if fields and field_name is not None:
-            raise ValueError('Cannot use both positional arguments and the field_name keyword argument.')
-
-        if field_name is not None:
-            warnings.warn(
-                'The field_name keyword argument to earliest() and latest() '
-                'is deprecated in favor of passing positional arguments.',
-                RemovedInDjango30Warning,
-            )
-            order_by = (field_name,)
-        elif fields:
+        if fields:
             order_by = fields
         else:
             order_by = getattr(self.model._meta, 'get_latest_by')
@@ -634,7 +655,7 @@
                 "arguments or 'get_latest_by' in the model's Meta."
             )

-        assert self.query.can_filter(), \
+        assert not self.query.is_sliced, \
             "Cannot change a query once a slice has been taken."
         obj = self._chain()
         obj.query.set_limits(high=1)
@@ -642,11 +663,11 @@
         obj.query.add_ordering(*order_by)
         return obj.get()

-    def earliest(self, *fields, field_name=None):
-        return self._earliest(*fields, field_name=field_name)
-
-    def latest(self, *fields, field_name=None):
-        return self.reverse()._earliest(*fields, field_name=field_name)
+    def earliest(self, *fields):
+        return self._earliest(*fields)
+
+    def latest(self, *fields):
+        return self.reverse()._earliest(*fields)

     def first(self):
         """Return the first object of a query or None if no match is found."""
@@ -663,9 +684,20 @@
         Return a dictionary mapping each of the given IDs to the object with
         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
         """
-        assert self.query.can_filter(), \
+        assert not self.query.is_sliced, \
             "Cannot use 'limit' or 'offset' with in_bulk"
-        if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
+        opts = self.model._meta
+        unique_fields = [
+            constraint.fields[0]
+            for constraint in opts.total_unique_constraints
+            if len(constraint.fields) == 1
+        ]
+        if (
+            field_name != 'pk' and
+            not opts.get_field(field_name).unique and
+            field_name not in unique_fields and
+            self.query.distinct_fields != (field_name,)
+        ):
             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
         if id_list is not None:
             if not id_list:
@@ -688,9 +720,12 @@

     def delete(self):
         """Delete the records in the current QuerySet."""
-        assert self.query.can_filter(), \
+        self._not_support_combined_queries('delete')
+        assert not self.query.is_sliced, \
             "Cannot use 'limit' or 'offset' with delete."

+        if self.query.distinct or self.query.distinct_fields:
+            raise TypeError('Cannot call delete() after .distinct().')
         if self._fields is not None:
             raise TypeError("Cannot call delete() after .values() or .values_list()")

@@ -722,7 +757,13 @@
         Delete objects found from the given queryset in single direct SQL
         query. No signals are sent and there is no protection for cascades.
         """
-        return sql.DeleteQuery(self.model).delete_qs(self, using)
+        query = self.query.clone()
+        query.__class__ = sql.DeleteQuery
+        cursor = query.get_compiler(using).execute_sql(CURSOR)
+        if cursor:
+            with cursor:
+                return cursor.rowcount
+        return 0
     _raw_delete.alters_data = True

     def update(self, **kwargs):
@@ -730,13 +771,14 @@
         Update all elements in the current QuerySet, setting all the given
         fields to the appropriate values.
         """
-        assert self.query.can_filter(), \
+        self._not_support_combined_queries('update')
+        assert not self.query.is_sliced, \
             "Cannot update a query once a slice has been taken."
         self._for_write = True
         query = self.query.chain(sql.UpdateQuery)
         query.add_update_values(kwargs)
         # Clear any annotations so that they won't be present in subqueries.
-        query._annotations = None
+        query.annotations = {}
         with transaction.mark_for_rollback_on_error(using=self.db):
             rows = query.get_compiler(self.db).execute_sql(CURSOR)
         self._result_cache = None
@@ -750,12 +792,12 @@
         code (it requires too much poking around at model internals to be
         useful at that level).
         """
-        assert self.query.can_filter(), \
+        assert not self.query.is_sliced, \
             "Cannot update a query once a slice has been taken."
         query = self.query.chain(sql.UpdateQuery)
         query.add_update_fields(values)
         # Clear any annotations so that they won't be present in subqueries.
-        query._annotations = None
+        query.annotations = {}
         self._result_cache = None
         return query.get_compiler(self.db).execute_sql(CURSOR)
     _update.alters_data = True
@@ -778,7 +820,7 @@
     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
     ##################################################

-    def raw(self, raw_query, params=None, translations=None, using=None):
+    def raw(self, raw_query, params=(), translations=None, using=None):
         if using is None:
             using = self.db
         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
@@ -846,7 +888,7 @@
             'datefield', flat=True
         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')

-    def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
+    def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
         """
         Return a list of datetime objects representing all available
         datetimes for the given field_name, scoped to 'kind'.
@@ -861,7 +903,13 @@
         else:
             tzinfo = None
         return self.annotate(
-            datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
+            datetimefield=Trunc(
+                field_name,
+                kind,
+                output_field=DateTimeField(),
+                tzinfo=tzinfo,
+                is_dst=is_dst,
+            ),
             plain_field=F(field_name)
         ).values_list(
             'datetimefield', flat=True
@@ -889,26 +937,35 @@
         Return a new QuerySet instance with the args ANDed to the existing
         set.
         """
-        return self._filter_or_exclude(False, *args, **kwargs)
+        self._not_support_combined_queries('filter')
+        return self._filter_or_exclude(False, args, kwargs)

     def exclude(self, *args, **kwargs):
         """
         Return a new QuerySet instance with NOT (args) ANDed to the existing
         set.
         """
-        return self._filter_or_exclude(True, *args, **kwargs)
-
-    def _filter_or_exclude(self, negate, *args, **kwargs):
+        self._not_support_combined_queries('exclude')
+        return self._filter_or_exclude(True, args, kwargs)
+
+    def _filter_or_exclude(self, negate, args, kwargs):
         if args or kwargs:
-            assert self.query.can_filter(), \
+            assert not self.query.is_sliced, \
                 "Cannot filter a query once a slice has been taken."

         clone = self._chain()
+        if self._defer_next_filter:
+            self._defer_next_filter = False
+            clone._deferred_filter = negate, args, kwargs
+        else:
+            clone._filter_or_exclude_inplace(negate, args, kwargs)
+        return clone
+
+    def _filter_or_exclude_inplace(self, negate, args, kwargs):
         if negate:
-            clone.query.add_q(~Q(*args, **kwargs))
+            self._query.add_q(~Q(*args, **kwargs))
         else:
-            clone.query.add_q(Q(*args, **kwargs))
-        return clone
+            self._query.add_q(Q(*args, **kwargs))

     def complex_filter(self, filter_obj):
         """
@@ -925,7 +982,7 @@
             clone.query.add_q(filter_obj)
             return clone
         else:
-            return self._filter_or_exclude(None, **filter_obj)
+            return self._filter_or_exclude(False, args=(), kwargs=filter_obj)

     def _combinator_query(self, combinator, *other_qs, all=False):
         # Clone the query to inherit the select list and everything
@@ -942,7 +999,11 @@
         # If the query is an EmptyQuerySet, combine all nonempty querysets.
         if isinstance(self, EmptyQuerySet):
             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
-            return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
+            if not qs:
+                return self
+            if len(qs) == 1:
+                return qs[0]
+            return qs[0]._combinator_query('union', *qs[1:], all=all)
         return self._combinator_query('union', *other_qs, all=all)

     def intersection(self, *other_qs):
@@ -960,7 +1021,7 @@
             return self
         return self._combinator_query('difference', *other_qs)

-    def select_for_update(self, nowait=False, skip_locked=False, of=()):
+    def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
         """
         Return a new QuerySet instance that will select objects with a
         FOR UPDATE lock.
@@ -973,6 +1034,7 @@
         obj.query.select_for_update_nowait = nowait
         obj.query.select_for_update_skip_locked = skip_locked
         obj.query.select_for_update_of = of
+        obj.query.select_for_no_key_update = no_key
         return obj

     def select_related(self, *fields):
@@ -984,7 +1046,7 @@

         If select_related(None) is called, clear the list.
         """
-
+        self._not_support_combined_queries('select_related')
         if self._fields is not None:
             raise TypeError("Cannot call select_related() after .values() or .values_list()")

@@ -1006,6 +1068,7 @@
         When prefetch_related() is called more than once, append to the list of
         prefetch lookups. If prefetch_related(None) is called, clear the list.
         """
+        self._not_support_combined_queries('prefetch_related')
         clone = self._chain()
         if lookups == (None,):
             clone._prefetch_related_lookups = ()
@@ -1024,8 +1087,19 @@
         Return a query set in which the returned objects have been annotated
         with extra data or aggregations.
         """
+        self._not_support_combined_queries('annotate')
+        return self._annotate(args, kwargs, select=True)
+
+    def alias(self, *args, **kwargs):
+        """
+        Return a query set with added aliases for extra data or aggregations.
+        """
+        self._not_support_combined_queries('alias')
+        return self._annotate(args, kwargs, select=False)
+
+    def _annotate(self, args, kwargs, select=True):
         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
-        annotations = OrderedDict()  # To preserve ordering of args
+        annotations = {}
         for arg in args:
             # The default_alias property may raise a TypeError.
             try:
@@ -1053,8 +1127,9 @@
             if isinstance(annotation, FilteredRelation):
                 clone.query.add_filtered_relation(annotation, alias)
             else:
-                clone.query.add_annotation(annotation, alias, is_summary=False)
-
+                clone.query.add_annotation(
+                    annotation, alias, is_summary=False, select=select,
+                )
         for alias, annotation in clone.query.annotations.items():
             if alias in annotations and annotation.contains_aggregate:
                 if clone._fields is None:
@@ -1067,7 +1142,7 @@

     def order_by(self, *field_names):
         """Return a new QuerySet instance with the ordering changed."""
-        assert self.query.can_filter(), \
+        assert not self.query.is_sliced, \
             "Cannot reorder a query once a slice has been taken."
         obj = self._chain()
         obj.query.clear_ordering(force_empty=False)
@@ -1078,7 +1153,8 @@
         """
         Return a new QuerySet instance that will select only distinct results.
         """
-        assert self.query.can_filter(), \
+        self._not_support_combined_queries('distinct')
+        assert not self.query.is_sliced, \
             "Cannot create distinct fields once a slice has been taken."
         obj = self._chain()
         obj.query.add_distinct_fields(*field_names)
@@ -1087,7 +1163,8 @@
     def extra(self, select=None, where=None, params=None, tables=None,
               order_by=None, select_params=None):
         """Add extra SQL fragments to the query."""
-        assert self.query.can_filter(), \
+        self._not_support_combined_queries('extra')
+        assert not self.query.is_sliced, \
             "Cannot change a query once a slice has been taken"
         clone = self._chain()
         clone.query.add_extra(select, select_params, where, params, tables, order_by)
@@ -1095,7 +1172,7 @@

     def reverse(self):
         """Reverse the ordering of the QuerySet."""
-        if not self.query.can_filter():
+        if self.query.is_sliced:
             raise TypeError('Cannot reverse a query once a slice has been taken.')
         clone = self._chain()
         clone.query.standard_ordering = not clone.query.standard_ordering
@@ -1108,6 +1185,7 @@
         The only exception to this is if None is passed in as the only
         parameter, in which case removal all deferrals.
         """
+        self._not_support_combined_queries('defer')
         if self._fields is not None:
             raise TypeError("Cannot call defer() after .values() or .values_list()")
         clone = self._chain()
@@ -1123,6 +1201,7 @@
         method and that are not already specified as deferred are loaded
         immediately when the queryset is evaluated.
         """
+        self._not_support_combined_queries('only')
         if self._fields is not None:
             raise TypeError("Cannot call only() after .values() or .values_list()")
         if fields == (None,):
@@ -1157,7 +1236,12 @@
             return True
         if self.query.extra_order_by or self.query.order_by:
             return True
-        elif self.query.default_ordering and self.query.get_meta().ordering:
+        elif (
+            self.query.default_ordering and
+            self.query.get_meta().ordering and
+            # A default ordering doesn't affect GROUP BY queries.
+            not self.query.group_by
+        ):
             return True
         else:
             return False
@@ -1173,7 +1257,7 @@
     # PRIVATE METHODS #
     ###################

-    def _insert(self, objs, fields, return_id=False, raw=False, using=None, ignore_conflicts=False):
+    def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
         """
         Insert a new record for the given model. This provides an interface to
         the InsertQuery class and is how Model.save() is implemented.
@@ -1183,7 +1267,7 @@
             using = self.db
         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
         query.insert_values(fields, objs, raw=raw)
-        return query.get_compiler(using=using).execute_sql(return_id)
+        return query.get_compiler(using=using).execute_sql(returning_fields)
     _insert.alters_data = True
     _insert.queryset_only = False

@@ -1194,22 +1278,20 @@
         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
             raise NotSupportedError('This database backend does not support ignoring conflicts.')
         ops = connections[self.db].ops
-        batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))
-        inserted_ids = []
-        bulk_return = connections[self.db].features.can_return_ids_from_bulk_insert
+        max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
+        batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
+        inserted_rows = []
+        bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
             if bulk_return and not ignore_conflicts:
-                inserted_id = self._insert(
-                    item, fields=fields, using=self.db, return_id=True,
+                inserted_rows.extend(self._insert(
+                    item, fields=fields, using=self.db,
+                    returning_fields=self.model._meta.db_returning_fields,
                     ignore_conflicts=ignore_conflicts,
-                )
-                if isinstance(inserted_id, list):
-                    inserted_ids.extend(inserted_id)
-                else:
-                    inserted_ids.append(inserted_id)
+                ))
             else:
                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
-        return inserted_ids
+        return inserted_rows

     def _chain(self, **kwargs):
         """
@@ -1311,6 +1393,13 @@
                 )
             )

+    def _not_support_combined_queries(self, operation_name):
+        if self.query.combinator:
+            raise NotSupportedError(
+                'Calling QuerySet.%s() after %s() is not supported.'
+                % (operation_name, self.query.combinator)
+            )
+

 class InstanceCheckMeta(type):
     def __instancecheck__(self, instance):
@@ -1332,14 +1421,14 @@
     Provide an iterator which converts the results of raw SQL queries into
     annotated model instances.
     """
-    def __init__(self, raw_query, model=None, query=None, params=None,
+    def __init__(self, raw_query, model=None, query=None, params=(),
                  translations=None, using=None, hints=None):
         self.raw_query = raw_query
         self.model = model
         self._db = using
         self._hints = hints or {}
         self.query = query or sql.RawQuery(sql=raw_query, using=self.db, params=params)
-        self.params = params or ()
+        self.params = params
         self.translations = translations or {}
         self._result_cache = None
         self._prefetch_related_lookups = ()
@@ -1407,7 +1496,9 @@
         try:
             model_init_names, model_init_pos, annotation_fields = self.resolve_model_init_order()
             if self.model._meta.pk.attname not in model_init_names:
-                raise InvalidQuery('Raw query must include the primary key')
+                raise exceptions.FieldDoesNotExist(
+                    'Raw query must include the primary key'
+                )
             model_cls = self.model
             fields = [self.model_fields.get(c) for c in self.columns]
             converters = compiler.get_converters([
@@ -1483,8 +1574,16 @@
         self.prefetch_through = lookup
         # `prefetch_to` is the path to the attribute that stores the result.
         self.prefetch_to = lookup
-        if queryset is not None and not issubclass(queryset._iterable_class, ModelIterable):
-            raise ValueError('Prefetch querysets cannot use values().')
+        if queryset is not None and (
+            isinstance(queryset, RawQuerySet) or (
+                hasattr(queryset, '_iterable_class') and
+                not issubclass(queryset._iterable_class, ModelIterable)
+            )
+        ):
+            raise ValueError(
+                'Prefetch querysets cannot use raw(), values(), and '
+                'values_list().'
+            )
         if to_attr:
             self.prefetch_to = LOOKUP_SEP.join(lookup.split(LOOKUP_SEP)[:-1] + [to_attr])

@@ -1520,7 +1619,9 @@
         return None

     def __eq__(self, other):
-        return isinstance(other, Prefetch) and self.prefetch_to == other.prefetch_to
+        if not isinstance(other, Prefetch):
+            return NotImplemented
+        return self.prefetch_to == other.prefetch_to

     def __hash__(self):
         return hash((self.__class__, self.prefetch_to))
@@ -1558,7 +1659,7 @@
     while all_lookups:
         lookup = all_lookups.pop()
         if lookup.prefetch_to in done_queries:
-            if lookup.queryset:
+            if lookup.queryset is not None:
                 raise ValueError("'%s' lookup was already seen with a different queryset. "
                                  "You may need to adjust the ordering of your lookups." % lookup.prefetch_to)

@@ -1621,8 +1722,17 @@
                                  "prefetching - this is an invalid parameter to "
                                  "prefetch_related()." % lookup.prefetch_through)

-            if prefetcher is not None and not is_fetched:
-                obj_list, additional_lookups = prefetch_one_level(obj_list, prefetcher, lookup, level)
+            obj_to_fetch = None
+            if prefetcher is not None:
+                obj_to_fetch = [obj for obj in obj_list if not is_fetched(obj)]
+
+            if obj_to_fetch:
+                obj_list, additional_lookups = prefetch_one_level(
+                    obj_to_fetch,
+                    prefetcher,
+                    lookup,
+                    level,
+                )
                 # We need to ensure we don't keep adding lookups from the
                 # same relationships to stop infinite recursion. So, if we
                 # are already on an automatically added lookup, don't add
@@ -1672,10 +1782,14 @@
     (the object with get_prefetch_queryset (or None),
      the descriptor object representing this relationship (or None),
      a boolean that is False if the attribute was not found at all,
-     a boolean that is True if the attribute has already been fetched)
+     a function that takes an instance and returns a boolean that is True if
+     the attribute has already been fetched for that instance)
     """
+    def has_to_attr_attribute(instance):
+        return hasattr(instance, to_attr)
+
     prefetcher = None
-    is_fetched = False
+    is_fetched = has_to_attr_attribute

     # For singly related objects, we have to avoid getting the attribute
     # from the object, as this will trigger the query. So we first try
@@ -1690,8 +1804,7 @@
             # get_prefetch_queryset() method.
             if hasattr(rel_obj_descriptor, 'get_prefetch_queryset'):
                 prefetcher = rel_obj_descriptor
-                if rel_obj_descriptor.is_cached(instance):
-                    is_fetched = True
+                is_fetched = rel_obj_descriptor.is_cached
             else:
                 # descriptor doesn't support prefetching, so we go ahead and get
                 # the attribute on the instance rather than the class to
@@ -1703,11 +1816,15 @@
                     # Special case cached_property instances because hasattr
                     # triggers attribute computation and assignment.
                     if isinstance(getattr(instance.__class__, to_attr, None), cached_property):
-                        is_fetched = to_attr in instance.__dict__
-                    else:
-                        is_fetched = hasattr(instance, to_attr)
+                        def has_cached_property(instance):
+                            return to_attr in instance.__dict__
+
+                        is_fetched = has_cached_property
                 else:
-                    is_fetched = through_attr in instance._prefetched_objects_cache
+                    def in_prefetched_cache(instance):
+                        return through_attr in instance._prefetched_objects_cache
+
+                    is_fetched = in_prefetched_cache
     return prefetcher, rel_obj_descriptor, attr_found, is_fetched


('django/db/models', 'expressions.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,11 +1,14 @@
 import copy
 import datetime
+import functools
 import inspect
 from decimal import Decimal
+from uuid import UUID

 from django.core.exceptions import EmptyResultSet, FieldError
-from django.db import connection
+from django.db import NotSupportedError, connection
 from django.db.models import fields
+from django.db.models.constants import LOOKUP_SEP
 from django.db.models.query_utils import Q
 from django.utils.deconstruct import deconstructible
 from django.utils.functional import cached_property
@@ -50,14 +53,12 @@
     BITOR = '|'
     BITLEFTSHIFT = '<<'
     BITRIGHTSHIFT = '>>'
+    BITXOR = '#'

     def _combine(self, other, connector, reversed):
         if not hasattr(other, 'resolve_expression'):
             # everything must be resolvable to an expression
-            if isinstance(other, datetime.timedelta):
-                other = DurationValue(other, output_field=fields.DurationField())
-            else:
-                other = Value(other)
+            other = Value(other)

         if reversed:
             return CombinedExpression(other, connector, self)
@@ -89,6 +90,8 @@
         return self._combine(other, self.POW, False)

     def __and__(self, other):
+        if getattr(self, 'conditional', False) and getattr(other, 'conditional', False):
+            return Q(self) & Q(other)
         raise NotImplementedError(
             "Use .bitand() and .bitor() for bitwise logical operations."
         )
@@ -102,7 +105,12 @@
     def bitrightshift(self, other):
         return self._combine(other, self.BITRIGHTSHIFT, False)

+    def bitxor(self, other):
+        return self._combine(other, self.BITXOR, False)
+
     def __or__(self, other):
+        if getattr(self, 'conditional', False) and getattr(other, 'conditional', False):
+            return Q(self) | Q(other)
         raise NotImplementedError(
             "Use .bitand() and .bitor() for bitwise logical operations."
         )
@@ -139,7 +147,6 @@
         )


-@deconstructible
 class BaseExpression:
     """Base class for all query expressions."""

@@ -244,9 +251,9 @@
         ])
         return c

-    def _prepare(self, field):
-        """Hook used by Lookup.get_prep_lookup() to do custom preparation."""
-        return self
+    @property
+    def conditional(self):
+        return isinstance(self.output_field, fields.BooleanField)

     @property
     def field(self):
@@ -289,8 +296,15 @@
         """
         sources_iter = (source for source in self.get_source_fields() if source is not None)
         for output_field in sources_iter:
-            if any(not isinstance(output_field, source.__class__) for source in sources_iter):
-                raise FieldError('Expression contains mixed types. You must set output_field.')
+            for source in sources_iter:
+                if not isinstance(output_field, source.__class__):
+                    raise FieldError(
+                        'Expression contains mixed types: %s, %s. You must '
+                        'set output_field.' % (
+                            output_field.__class__.__name__,
+                            source.__class__.__name__,
+                        )
+                    )
             return output_field

     @staticmethod
@@ -331,7 +345,7 @@
     def copy(self):
         return copy.copy(self)

-    def get_group_by_cols(self):
+    def get_group_by_cols(self, alias=None):
         if not self.contains_aggregate:
             return [self]
         cols = []
@@ -360,7 +374,24 @@
         yield self
         for expr in self.get_source_expressions():
             if expr:
-                yield from expr.flatten()
+                if hasattr(expr, 'flatten'):
+                    yield from expr.flatten()
+                else:
+                    yield expr
+
+    def select_format(self, compiler, sql, params):
+        """
+        Custom format for select clauses. For example, EXISTS expressions need
+        to be wrapped in CASE WHEN on Oracle.
+        """
+        if hasattr(self.output_field, 'select_format'):
+            return self.output_field.select_format(compiler, sql, params)
+        return sql, params
+
+
+@deconstructible
+class Expression(BaseExpression, Combinable):
+    """An expression that can be combined with other expressions."""

     @cached_property
     def identity(self):
@@ -372,22 +403,42 @@
         identity = [self.__class__]
         for arg, value in arguments:
             if isinstance(value, fields.Field):
-                value = type(value)
+                if value.name and value.model:
+                    value = (value.model._meta.label, value.name)
+                else:
+                    value = type(value)
             else:
                 value = make_hashable(value)
             identity.append((arg, value))
         return tuple(identity)

     def __eq__(self, other):
-        return isinstance(other, BaseExpression) and other.identity == self.identity
+        if not isinstance(other, Expression):
+            return NotImplemented
+        return other.identity == self.identity

     def __hash__(self):
         return hash(self.identity)


-class Expression(BaseExpression, Combinable):
-    """An expression that can be combined with other expressions."""
-    pass
+_connector_combinators = {
+    connector: [
+        (fields.IntegerField, fields.IntegerField, fields.IntegerField),
+        (fields.IntegerField, fields.DecimalField, fields.DecimalField),
+        (fields.DecimalField, fields.IntegerField, fields.DecimalField),
+        (fields.IntegerField, fields.FloatField, fields.FloatField),
+        (fields.FloatField, fields.IntegerField, fields.FloatField),
+    ]
+    for connector in (Combinable.ADD, Combinable.SUB, Combinable.MUL, Combinable.DIV)
+}
+
+
+@functools.lru_cache(maxsize=128)
+def _resolve_combined_type(connector, lhs_type, rhs_type):
+    combinators = _connector_combinators.get(connector, ())
+    for combinator_lhs_type, combinator_rhs_type, combined_type in combinators:
+        if issubclass(lhs_type, combinator_lhs_type) and issubclass(rhs_type, combinator_rhs_type):
+            return combined_type


 class CombinedExpression(SQLiteNumericMixin, Expression):
@@ -410,23 +461,20 @@
     def set_source_expressions(self, exprs):
         self.lhs, self.rhs = exprs

+    def _resolve_output_field(self):
+        try:
+            return super()._resolve_output_field()
+        except FieldError:
+            combined_type = _resolve_combined_type(
+                self.connector,
+                type(self.lhs.output_field),
+                type(self.rhs.output_field),
+            )
+            if combined_type is None:
+                raise
+            return combined_type()
+
     def as_sql(self, compiler, connection):
-        try:
-            lhs_output = self.lhs.output_field
-        except FieldError:
-            lhs_output = None
-        try:
-            rhs_output = self.rhs.output_field
-        except FieldError:
-            rhs_output = None
-        if (not connection.features.has_native_duration_field and
-                ((lhs_output and lhs_output.get_internal_type() == 'DurationField') or
-                 (rhs_output and rhs_output.get_internal_type() == 'DurationField'))):
-            return DurationExpression(self.lhs, self.connector, self.rhs).as_sql(compiler, connection)
-        if (lhs_output and rhs_output and self.connector == self.SUB and
-            lhs_output.get_internal_type() in {'DateField', 'DateTimeField', 'TimeField'} and
-                lhs_output.get_internal_type() == rhs_output.get_internal_type()):
-            return TemporalSubtraction(self.lhs, self.rhs).as_sql(compiler, connection)
         expressions = []
         expression_params = []
         sql, params = compiler.compile(self.lhs)
@@ -441,27 +489,48 @@
         return expression_wrapper % sql, expression_params

     def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):
+        lhs = self.lhs.resolve_expression(query, allow_joins, reuse, summarize, for_save)
+        rhs = self.rhs.resolve_expression(query, allow_joins, reuse, summarize, for_save)
+        if not isinstance(self, (DurationExpression, TemporalSubtraction)):
+            try:
+                lhs_type = lhs.output_field.get_internal_type()
+            except (AttributeError, FieldError):
+                lhs_type = None
+            try:
+                rhs_type = rhs.output_field.get_internal_type()
+            except (AttributeError, FieldError):
+                rhs_type = None
+            if 'DurationField' in {lhs_type, rhs_type} and lhs_type != rhs_type:
+                return DurationExpression(self.lhs, self.connector, self.rhs).resolve_expression(
+                    query, allow_joins, reuse, summarize, for_save,
+                )
+            datetime_fields = {'DateField', 'DateTimeField', 'TimeField'}
+            if self.connector == self.SUB and lhs_type in datetime_fields and lhs_type == rhs_type:
+                return TemporalSubtraction(self.lhs, self.rhs).resolve_expression(
+                    query, allow_joins, reuse, summarize, for_save,
+                )
         c = self.copy()
         c.is_summary = summarize
-        c.lhs = c.lhs.resolve_expression(query, allow_joins, reuse, summarize, for_save)
-        c.rhs = c.rhs.resolve_expression(query, allow_joins, reuse, summarize, for_save)
+        c.lhs = lhs
+        c.rhs = rhs
         return c


 class DurationExpression(CombinedExpression):
     def compile(self, side, compiler, connection):
-        if not isinstance(side, DurationValue):
-            try:
-                output = side.output_field
-            except FieldError:
-                pass
-            else:
-                if output.get_internal_type() == 'DurationField':
-                    sql, params = compiler.compile(side)
-                    return connection.ops.format_for_duration_arithmetic(sql), params
+        try:
+            output = side.output_field
+        except FieldError:
+            pass
+        else:
+            if output.get_internal_type() == 'DurationField':
+                sql, params = compiler.compile(side)
+                return connection.ops.format_for_duration_arithmetic(sql), params
         return compiler.compile(side)

     def as_sql(self, compiler, connection):
+        if connection.features.has_native_duration_field:
+            return super().as_sql(compiler, connection)
         connection.ops.check_expression_support(self)
         expressions = []
         expression_params = []
@@ -485,16 +554,14 @@

     def as_sql(self, compiler, connection):
         connection.ops.check_expression_support(self)
-        lhs = compiler.compile(self.lhs, connection)
-        rhs = compiler.compile(self.rhs, connection)
+        lhs = compiler.compile(self.lhs)
+        rhs = compiler.compile(self.rhs)
         return connection.ops.subtract_temporals(self.lhs.output_field.get_internal_type(), lhs, rhs)


 @deconstructible
 class F(Combinable):
     """An object capable of resolving references to existing query objects."""
-    # Can the expression be used in a WHERE clause?
-    filterable = True

     def __init__(self, name):
         """
@@ -507,8 +574,8 @@
         return "{}({})".format(self.__class__.__name__, self.name)

     def resolve_expression(self, query=None, allow_joins=True, reuse=None,
-                           summarize=False, for_save=False, simple_col=False):
-        return query.resolve_ref(self.name, allow_joins, reuse, summarize, simple_col)
+                           summarize=False, for_save=False):
+        return query.resolve_ref(self.name, allow_joins, reuse, summarize)

     def asc(self, **kwargs):
         return OrderBy(self, **kwargs)
@@ -530,27 +597,38 @@
     In this case, the reference to the outer query has been resolved because
     the inner query has been used as a subquery.
     """
+    contains_aggregate = False
+
     def as_sql(self, *args, **kwargs):
         raise ValueError(
             'This queryset contains a reference to an outer query and may '
             'only be used in a subquery.'
         )

-    def _prepare(self, output_field=None):
-        return self
+    def resolve_expression(self, *args, **kwargs):
+        col = super().resolve_expression(*args, **kwargs)
+        # FIXME: Rename possibly_multivalued to multivalued and fix detection
+        # for non-multivalued JOINs (e.g. foreign key fields). This should take
+        # into account only many-to-many and one-to-many relationships.
+        col.possibly_multivalued = LOOKUP_SEP in self.name
+        return col

     def relabeled_clone(self, relabels):
         return self

+    def get_group_by_cols(self, alias=None):
+        return []
+

 class OuterRef(F):
-    def resolve_expression(self, query=None, allow_joins=True, reuse=None,
-                           summarize=False, for_save=False, simple_col=False):
+    contains_aggregate = False
+
+    def resolve_expression(self, *args, **kwargs):
         if isinstance(self.name, self.__class__):
             return self.name
         return ResolvedOuterRef(self.name)

-    def _prepare(self, output_field=None):
+    def relabeled_clone(self, relabels):
         return self


@@ -630,6 +708,10 @@

 class Value(Expression):
     """Represent a wrapped value as a node within an expression."""
+    # Provide a default value for `for_save` in order to allow unresolved
+    # instances to be compiled until a decision is taken in #25425.
+    for_save = False
+
     def __init__(self, value, output_field=None):
         """
         Arguments:
@@ -668,16 +750,32 @@
         c.for_save = for_save
         return c

-    def get_group_by_cols(self):
+    def get_group_by_cols(self, alias=None):
         return []

-
-class DurationValue(Value):
-    def as_sql(self, compiler, connection):
-        connection.ops.check_expression_support(self)
-        if connection.features.has_native_duration_field:
-            return super().as_sql(compiler, connection)
-        return connection.ops.date_interval_sql(self.value), []
+    def _resolve_output_field(self):
+        if isinstance(self.value, str):
+            return fields.CharField()
+        if isinstance(self.value, bool):
+            return fields.BooleanField()
+        if isinstance(self.value, int):
+            return fields.IntegerField()
+        if isinstance(self.value, float):
+            return fields.FloatField()
+        if isinstance(self.value, datetime.datetime):
+            return fields.DateTimeField()
+        if isinstance(self.value, datetime.date):
+            return fields.DateField()
+        if isinstance(self.value, datetime.time):
+            return fields.TimeField()
+        if isinstance(self.value, datetime.timedelta):
+            return fields.DurationField()
+        if isinstance(self.value, Decimal):
+            return fields.DecimalField()
+        if isinstance(self.value, bytes):
+            return fields.BinaryField()
+        if isinstance(self.value, UUID):
+            return fields.UUIDField()


 class RawSQL(Expression):
@@ -693,8 +791,18 @@
     def as_sql(self, compiler, connection):
         return '(%s)' % self.sql, self.params

-    def get_group_by_cols(self):
+    def get_group_by_cols(self, alias=None):
         return [self]
+
+    def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):
+        # Resolve parents fields used in raw SQL.
+        for parent in query.model._meta.get_parent_list():
+            for parent_field in parent._meta.local_fields:
+                _, column_name = parent_field.get_attname_column()
+                if column_name.lower() in self.sql.lower():
+                    query.resolve_ref(parent_field.name, allow_joins, reuse, summarize)
+                    break
+        return super().resolve_expression(query, allow_joins, reuse, summarize, for_save)


 class Star(Expression):
@@ -705,19 +813,10 @@
         return '*', []


-class Random(Expression):
-    output_field = fields.FloatField()
-
-    def __repr__(self):
-        return "Random()"
-
-    def as_sql(self, compiler, connection):
-        return connection.ops.random_function_sql(), []
-
-
 class Col(Expression):

     contains_column_references = True
+    possibly_multivalued = False

     def __init__(self, alias, target, output_field=None):
         if output_field is None:
@@ -726,17 +825,22 @@
         self.alias, self.target = alias, target

     def __repr__(self):
-        return "{}({}, {})".format(
-            self.__class__.__name__, self.alias, self.target)
+        alias, target = self.alias, self.target
+        identifiers = (alias, str(target)) if alias else (str(target),)
+        return '{}({})'.format(self.__class__.__name__, ', '.join(identifiers))

     def as_sql(self, compiler, connection):
-        qn = compiler.quote_name_unless_alias
-        return "%s.%s" % (qn(self.alias), qn(self.target.column)), []
+        alias, column = self.alias, self.target.column
+        identifiers = (alias, column) if alias else (column,)
+        sql = '.'.join(map(compiler.quote_name_unless_alias, identifiers))
+        return sql, []

     def relabeled_clone(self, relabels):
+        if self.alias is None:
+            return self
         return self.__class__(relabels.get(self.alias, self.alias), self.target, self.output_field)

-    def get_group_by_cols(self):
+    def get_group_by_cols(self, alias=None):
         return [self]

     def get_db_converters(self, connection):
@@ -746,40 +850,6 @@
                 self.target.get_db_converters(connection))


-class SimpleCol(Expression):
-    """
-    Represents the SQL of a column name without the table name.
-
-    This variant of Col doesn't include the table name (or an alias) to
-    avoid a syntax error in check constraints.
-    """
-    contains_column_references = True
-
-    def __init__(self, target, output_field=None):
-        if output_field is None:
-            output_field = target
-        super().__init__(output_field=output_field)
-        self.target = target
-
-    def __repr__(self):
-        return '{}({})'.format(self.__class__.__name__, self.target)
-
-    def as_sql(self, compiler, connection):
-        qn = compiler.quote_name_unless_alias
-        return qn(self.target.column), []
-
-    def get_group_by_cols(self):
-        return [self]
-
-    def get_db_converters(self, connection):
-        if self.target == self.output_field:
-            return self.output_field.get_db_converters(connection)
-        return (
-            self.output_field.get_db_converters(connection) +
-            self.target.get_db_converters(connection)
-        )
-
-
 class Ref(Expression):
     """
     Reference to column alias of the query. For example, Ref('sum_cost') in
@@ -809,7 +879,7 @@
     def as_sql(self, compiler, connection):
         return connection.ops.quote_name(self.refs), []

-    def get_group_by_cols(self):
+    def get_group_by_cols(self, alias=None):
         return [self]


@@ -829,6 +899,10 @@
     def __str__(self):
         return self.arg_joiner.join(str(arg) for arg in self.source_expressions)

+    def as_sqlite(self, compiler, connection, **extra_context):
+        # Casting to numeric is unnecessary.
+        return self.as_sql(compiler, connection, **extra_context)
+

 class ExpressionWrapper(Expression):
     """
@@ -846,8 +920,17 @@
     def get_source_expressions(self):
         return [self.expression]

+    def get_group_by_cols(self, alias=None):
+        if isinstance(self.expression, Expression):
+            expression = self.expression.copy()
+            expression.output_field = self.output_field
+            return expression.get_group_by_cols(alias=alias)
+        # For non-expressions e.g. an SQL WHERE clause, the entire
+        # `expression` must be included in the GROUP BY clause.
+        return super().get_group_by_cols()
+
     def as_sql(self, compiler, connection):
-        return self.expression.as_sql(compiler, connection)
+        return compiler.compile(self.expression)

     def __repr__(self):
         return "{}({})".format(self.__class__.__name__, self.expression)
@@ -855,12 +938,20 @@

 class When(Expression):
     template = 'WHEN %(condition)s THEN %(result)s'
+    # This isn't a complete conditional expression, must be used in Case().
+    conditional = False

     def __init__(self, condition=None, then=None, **lookups):
-        if lookups and condition is None:
-            condition, lookups = Q(**lookups), None
+        if lookups:
+            if condition is None:
+                condition, lookups = Q(**lookups), None
+            elif getattr(condition, 'conditional', False):
+                condition, lookups = Q(condition, **lookups), None
         if condition is None or not getattr(condition, 'conditional', False) or lookups:
-            raise TypeError("__init__() takes either a Q object or lookups as keyword arguments")
+            raise TypeError(
+                'When() supports a Q object, a boolean expression, or lookups '
+                'as a condition.'
+            )
         if isinstance(condition, Q) and not condition:
             raise ValueError("An empty Q() can't be used as a When() condition.")
         super().__init__(output_field=None)
@@ -904,7 +995,7 @@
         template = template or self.template
         return template % template_params, sql_params

-    def get_group_by_cols(self):
+    def get_group_by_cols(self, alias=None):
         # This is not a complete expression and cannot be used in GROUP BY.
         cols = []
         for source in self.get_source_expressions():
@@ -987,8 +1078,13 @@
             sql = connection.ops.unification_cast_sql(self.output_field) % sql
         return sql, sql_params

-
-class Subquery(Expression):
+    def get_group_by_cols(self, alias=None):
+        if not self.cases:
+            return self.default.get_group_by_cols(alias)
+        return super().get_group_by_cols(alias)
+
+
+class Subquery(BaseExpression, Combinable):
     """
     An explicit subquery. It may contain OuterRef() references to the outer
     query which will be resolved when it is applied to that query.
@@ -997,125 +1093,90 @@
     contains_aggregate = False

     def __init__(self, queryset, output_field=None, **extra):
-        self.queryset = queryset
+        # Allow the usage of both QuerySet and sql.Query objects.
+        self.query = getattr(queryset, 'query', queryset)
         self.extra = extra
         super().__init__(output_field)

+    def get_source_expressions(self):
+        return [self.query]
+
+    def set_source_expressions(self, exprs):
+        self.query = exprs[0]
+
     def _resolve_output_field(self):
-        if len(self.queryset.query.select) == 1:
-            return self.queryset.query.select[0].field
-        return super()._resolve_output_field()
+        return self.query.output_field

     def copy(self):
         clone = super().copy()
-        clone.queryset = clone.queryset.all()
+        clone.query = clone.query.clone()
         return clone

-    def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):
-        clone = self.copy()
-        clone.is_summary = summarize
-        clone.queryset.query.bump_prefix(query)
-
-        # Need to recursively resolve these.
-        def resolve_all(child):
-            if hasattr(child, 'children'):
-                [resolve_all(_child) for _child in child.children]
-            if hasattr(child, 'rhs'):
-                child.rhs = resolve(child.rhs)
-
-        def resolve(child):
-            if hasattr(child, 'resolve_expression'):
-                resolved = child.resolve_expression(
-                    query=query, allow_joins=allow_joins, reuse=reuse,
-                    summarize=summarize, for_save=for_save,
-                )
-                # Add table alias to the parent query's aliases to prevent
-                # quoting.
-                if hasattr(resolved, 'alias') and resolved.alias != resolved.target.model._meta.db_table:
-                    clone.queryset.query.external_aliases.add(resolved.alias)
-                return resolved
-            return child
-
-        resolve_all(clone.queryset.query.where)
-
-        for key, value in clone.queryset.query.annotations.items():
-            if isinstance(value, Subquery):
-                clone.queryset.query.annotations[key] = resolve(value)
-
-        return clone
-
-    def get_source_expressions(self):
-        return [
-            x for x in [
-                getattr(expr, 'lhs', None)
-                for expr in self.queryset.query.where.children
-            ] if x
-        ]
-
-    def relabeled_clone(self, change_map):
-        clone = self.copy()
-        clone.queryset.query = clone.queryset.query.relabeled_clone(change_map)
-        clone.queryset.query.external_aliases.update(
-            alias for alias in change_map.values()
-            if alias not in clone.queryset.query.alias_map
-        )
-        return clone
-
-    def as_sql(self, compiler, connection, template=None, **extra_context):
+    @property
+    def external_aliases(self):
+        return self.query.external_aliases
+
+    def get_external_cols(self):
+        return self.query.get_external_cols()
+
+    def as_sql(self, compiler, connection, template=None, query=None, **extra_context):
         connection.ops.check_expression_support(self)
         template_params = {**self.extra, **extra_context}
-        template_params['subquery'], sql_params = self.queryset.query.get_compiler(connection=connection).as_sql()
+        query = query or self.query
+        subquery_sql, sql_params = query.as_sql(compiler, connection)
+        template_params['subquery'] = subquery_sql[1:-1]

         template = template or template_params.get('template', self.template)
         sql = template % template_params
         return sql, sql_params

-    def _prepare(self, output_field):
-        # This method will only be called if this instance is the "rhs" in an
-        # expression: the wrapping () must be removed (as the expression that
-        # contains this will provide them). SQLite evaluates ((subquery))
-        # differently than the other databases.
-        if self.template == '(%(subquery)s)':
-            clone = self.copy()
-            clone.template = '%(subquery)s'
-            return clone
-        return self
+    def get_group_by_cols(self, alias=None):
+        if alias:
+            return [Ref(alias, self)]
+        external_cols = self.get_external_cols()
+        if any(col.possibly_multivalued for col in external_cols):
+            return [self]
+        return external_cols


 class Exists(Subquery):
     template = 'EXISTS(%(subquery)s)'
     output_field = fields.BooleanField()

-    def __init__(self, *args, negated=False, **kwargs):
+    def __init__(self, queryset, negated=False, **kwargs):
         self.negated = negated
-        super().__init__(*args, **kwargs)
+        super().__init__(queryset, **kwargs)

     def __invert__(self):
-        return type(self)(self.queryset, negated=(not self.negated), **self.extra)
-
-    def resolve_expression(self, query=None, *args, **kwargs):
-        # As a performance optimization, remove ordering since EXISTS doesn't
-        # care about it, just whether or not a row matches.
-        self.queryset = self.queryset.order_by()
-        return super().resolve_expression(query, *args, **kwargs)
+        clone = self.copy()
+        clone.negated = not self.negated
+        return clone

     def as_sql(self, compiler, connection, template=None, **extra_context):
-        sql, params = super().as_sql(compiler, connection, template, **extra_context)
+        query = self.query.exists(using=connection.alias)
+        sql, params = super().as_sql(
+            compiler,
+            connection,
+            template=template,
+            query=query,
+            **extra_context,
+        )
         if self.negated:
             sql = 'NOT {}'.format(sql)
         return sql, params

-    def as_oracle(self, compiler, connection, template=None, **extra_context):
-        # Oracle doesn't allow EXISTS() in the SELECT list, so wrap it with a
-        # CASE WHEN expression. Change the template since the When expression
-        # requires a left hand side (column) to compare against.
-        sql, params = self.as_sql(compiler, connection, template, **extra_context)
-        sql = 'CASE WHEN {} THEN 1 ELSE 0 END'.format(sql)
+    def select_format(self, compiler, sql, params):
+        # Wrap EXISTS() with a CASE WHEN expression if a database backend
+        # (e.g. Oracle) doesn't support boolean expression in SELECT or GROUP
+        # BY list.
+        if not compiler.connection.features.supports_boolean_expr_in_select_clause:
+            sql = 'CASE WHEN {} THEN 1 ELSE 0 END'.format(sql)
         return sql, params


-class OrderBy(BaseExpression):
+class OrderBy(Expression):
     template = '%(expression)s %(ordering)s'
+    conditional = False

     def __init__(self, expression, descending=False, nulls_first=False, nulls_last=False):
         if nulls_first and nulls_last:
@@ -1138,11 +1199,21 @@
         return [self.expression]

     def as_sql(self, compiler, connection, template=None, **extra_context):
-        if not template:
+        template = template or self.template
+        if connection.features.supports_order_by_nulls_modifier:
             if self.nulls_last:
-                template = '%s NULLS LAST' % self.template
+                template = '%s NULLS LAST' % template
             elif self.nulls_first:
-                template = '%s NULLS FIRST' % self.template
+                template = '%s NULLS FIRST' % template
+        else:
+            if self.nulls_last and not (
+                self.descending and connection.features.order_by_nulls_first
+            ):
+                template = '%%(expression)s IS NULL, %s' % template
+            elif self.nulls_first and not (
+                not self.descending and connection.features.order_by_nulls_first
+            ):
+                template = '%%(expression)s IS NOT NULL, %s' % template
         connection.ops.check_expression_support(self)
         expression_sql, params = compiler.compile(self.expression)
         placeholders = {
@@ -1154,23 +1225,19 @@
         params *= template.count('%(expression)s')
         return (template % placeholders).rstrip(), params

-    def as_sqlite(self, compiler, connection):
-        template = None
-        if self.nulls_last:
-            template = '%(expression)s IS NULL, %(expression)s %(ordering)s'
-        elif self.nulls_first:
-            template = '%(expression)s IS NOT NULL, %(expression)s %(ordering)s'
-        return self.as_sql(compiler, connection, template=template)
-
-    def as_mysql(self, compiler, connection):
-        template = None
-        if self.nulls_last:
-            template = 'IF(ISNULL(%(expression)s),1,0), %(expression)s %(ordering)s '
-        elif self.nulls_first:
-            template = 'IF(ISNULL(%(expression)s),0,1), %(expression)s %(ordering)s '
-        return self.as_sql(compiler, connection, template=template)
-
-    def get_group_by_cols(self):
+    def as_oracle(self, compiler, connection):
+        # Oracle doesn't allow ORDER BY EXISTS() unless it's wrapped in
+        # a CASE WHEN.
+        if isinstance(self.expression, Exists):
+            copy = self.copy()
+            copy.expression = Case(
+                When(self.expression, then=True),
+                default=False,
+            )
+            return copy.as_sql(compiler, connection)
+        return self.as_sql(compiler, connection)
+
+    def get_group_by_cols(self, alias=None):
         cols = []
         for source in self.get_source_expressions():
             cols.extend(source.get_group_by_cols())
@@ -1190,7 +1257,7 @@
         self.descending = True


-class Window(Expression):
+class Window(SQLiteNumericMixin, Expression):
     template = '%(expression)s OVER (%(window)s)'
     # Although the main expression may either be an aggregate or an
     # expression with an aggregate function, the GROUP BY that will
@@ -1237,6 +1304,8 @@

     def as_sql(self, compiler, connection, template=None):
         connection.ops.check_expression_support(self)
+        if not connection.features.supports_over_clause:
+            raise NotSupportedError('This backend does not support window expressions.')
         expr_sql, params = compiler.compile(self.source_expression)
         window_sql, window_params = [], []

@@ -1251,12 +1320,12 @@
         if self.order_by is not None:
             window_sql.append(' ORDER BY ')
             order_sql, order_params = compiler.compile(self.order_by)
-            window_sql.extend(''.join(order_sql))
+            window_sql.extend(order_sql)
             window_params.extend(order_params)

         if self.frame:
             frame_sql, frame_params = compiler.compile(self.frame)
-            window_sql.extend(' ' + frame_sql)
+            window_sql.append(' ' + frame_sql)
             window_params.extend(frame_params)

         params.extend(window_params)
@@ -1266,6 +1335,16 @@
             'expression': expr_sql,
             'window': ''.join(window_sql).strip()
         }, params
+
+    def as_sqlite(self, compiler, connection):
+        if isinstance(self.output_field, fields.DecimalField):
+            # Casting to numeric must be outside of the window expression.
+            copy = self.copy()
+            source_expressions = copy.get_source_expressions()
+            source_expressions[0].output_field = fields.FloatField()
+            copy.set_source_expressions(source_expressions)
+            return super(Window, copy).as_sqlite(compiler, connection)
+        return self.as_sql(compiler, connection)

     def __str__(self):
         return '{} OVER ({}{}{})'.format(
@@ -1278,7 +1357,7 @@
     def __repr__(self):
         return '<%s: %s>' % (self.__class__.__name__, self)

-    def get_group_by_cols(self):
+    def get_group_by_cols(self, alias=None):
         return []


@@ -1314,7 +1393,7 @@
     def __repr__(self):
         return '<%s: %s>' % (self.__class__.__name__, self)

-    def get_group_by_cols(self):
+    def get_group_by_cols(self, alias=None):
         return []

     def __str__(self):
('django/db/models', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -5,44 +5,48 @@
 from django.db.models.constraints import *  # NOQA
 from django.db.models.constraints import __all__ as constraints_all
 from django.db.models.deletion import (
-    CASCADE, DO_NOTHING, PROTECT, SET, SET_DEFAULT, SET_NULL, ProtectedError,
+    CASCADE, DO_NOTHING, PROTECT, RESTRICT, SET, SET_DEFAULT, SET_NULL,
+    ProtectedError, RestrictedError,
 )
+from django.db.models.enums import *  # NOQA
+from django.db.models.enums import __all__ as enums_all
 from django.db.models.expressions import (
     Case, Exists, Expression, ExpressionList, ExpressionWrapper, F, Func,
-    OuterRef, RowRange, Subquery, Value, ValueRange, When, Window, WindowFrame,
+    OrderBy, OuterRef, RowRange, Subquery, Value, ValueRange, When, Window,
+    WindowFrame,
 )
 from django.db.models.fields import *  # NOQA
 from django.db.models.fields import __all__ as fields_all
 from django.db.models.fields.files import FileField, ImageField
+from django.db.models.fields.json import JSONField
 from django.db.models.fields.proxy import OrderWrt
 from django.db.models.indexes import *  # NOQA
 from django.db.models.indexes import __all__ as indexes_all
 from django.db.models.lookups import Lookup, Transform
 from django.db.models.manager import Manager
-from django.db.models.query import (
-    Prefetch, Q, QuerySet, prefetch_related_objects,
-)
-from django.db.models.query_utils import FilteredRelation
+from django.db.models.query import Prefetch, QuerySet, prefetch_related_objects
+from django.db.models.query_utils import FilteredRelation, Q

 # Imports that would create circular imports if sorted
 from django.db.models.base import DEFERRED, Model  # isort:skip
 from django.db.models.fields.related import (  # isort:skip
     ForeignKey, ForeignObject, OneToOneField, ManyToManyField,
-    ManyToOneRel, ManyToManyRel, OneToOneRel,
+    ForeignObjectRel, ManyToOneRel, ManyToManyRel, OneToOneRel,
 )


-__all__ = aggregates_all + constraints_all + fields_all + indexes_all
+__all__ = aggregates_all + constraints_all + enums_all + fields_all + indexes_all
 __all__ += [
     'ObjectDoesNotExist', 'signals',
-    'CASCADE', 'DO_NOTHING', 'PROTECT', 'SET', 'SET_DEFAULT', 'SET_NULL',
-    'ProtectedError',
+    'CASCADE', 'DO_NOTHING', 'PROTECT', 'RESTRICT', 'SET', 'SET_DEFAULT',
+    'SET_NULL', 'ProtectedError', 'RestrictedError',
     'Case', 'Exists', 'Expression', 'ExpressionList', 'ExpressionWrapper', 'F',
-    'Func', 'OuterRef', 'RowRange', 'Subquery', 'Value', 'ValueRange', 'When',
+    'Func', 'OrderBy', 'OuterRef', 'RowRange', 'Subquery', 'Value',
+    'ValueRange', 'When',
     'Window', 'WindowFrame',
-    'FileField', 'ImageField', 'OrderWrt', 'Lookup', 'Transform', 'Manager',
-    'Prefetch', 'Q', 'QuerySet', 'prefetch_related_objects', 'DEFERRED', 'Model',
-    'FilteredRelation',
+    'FileField', 'ImageField', 'JSONField', 'OrderWrt', 'Lookup', 'Transform',
+    'Manager', 'Prefetch', 'Q', 'QuerySet', 'prefetch_related_objects',
+    'DEFERRED', 'Model', 'FilteredRelation',
     'ForeignKey', 'ForeignObject', 'OneToOneField', 'ManyToManyField',
-    'ManyToOneRel', 'ManyToManyRel', 'OneToOneRel',
+    'ForeignObjectRel', 'ManyToOneRel', 'ManyToManyRel', 'OneToOneRel',
 ]
('django/db/models', 'lookups.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,13 +1,18 @@
 import itertools
 import math
+import warnings
 from copy import copy

 from django.core.exceptions import EmptyResultSet
-from django.db.models.expressions import Func, Value
-from django.db.models.fields import DateTimeField, Field, IntegerField
+from django.db.models.expressions import Case, Exists, Func, Value, When
+from django.db.models.fields import (
+    CharField, DateTimeField, Field, IntegerField, UUIDField,
+)
 from django.db.models.query_utils import RegisterLookupMixin
 from django.utils.datastructures import OrderedSet
+from django.utils.deprecation import RemovedInDjango40Warning
 from django.utils.functional import cached_property
+from django.utils.hashable import make_hashable


 class Lookup:
@@ -25,7 +30,9 @@
         if bilateral_transforms:
             # Warn the user as soon as possible if they are trying to apply
             # a bilateral transformation on a nested QuerySet: that won't work.
-            from django.db.models.sql.query import Query  # avoid circular import
+            from django.db.models.sql.query import (  # avoid circular import
+                Query,
+            )
             if isinstance(rhs, Query):
                 raise NotImplementedError("Bilateral transformations on nested querysets are not implemented.")
         self.bilateral_transforms = bilateral_transforms
@@ -64,8 +71,8 @@
             self.lhs, self.rhs = new_exprs

     def get_prep_lookup(self):
-        if hasattr(self.rhs, '_prepare'):
-            return self.rhs._prepare(self.lhs.output_field)
+        if hasattr(self.rhs, 'resolve_expression'):
+            return self.rhs
         if self.prepare_rhs and hasattr(self.lhs.output_field, 'get_prep_value'):
             return self.lhs.output_field.get_prep_value(self.rhs)
         return self.rhs
@@ -89,8 +96,7 @@
             value = self.apply_bilateral_transforms(value)
             value = value.resolve_expression(compiler.query)
         if hasattr(value, 'as_sql'):
-            sql, params = compiler.compile(value)
-            return '(' + sql + ')', params
+            return compiler.compile(value)
         else:
             return self.get_db_prep_lookup(value, connection)

@@ -104,7 +110,7 @@
             new.rhs = new.rhs.relabeled_clone(relabels)
         return new

-    def get_group_by_cols(self):
+    def get_group_by_cols(self, alias=None):
         cols = self.lhs.get_group_by_cols()
         if hasattr(self.rhs, 'get_group_by_cols'):
             cols.extend(self.rhs.get_group_by_cols())
@@ -113,6 +119,19 @@
     def as_sql(self, compiler, connection):
         raise NotImplementedError

+    def as_oracle(self, compiler, connection):
+        # Oracle doesn't allow EXISTS() to be compared to another expression
+        # unless it's wrapped in a CASE WHEN.
+        wrapped = False
+        exprs = []
+        for expr in (self.lhs, self.rhs):
+            if isinstance(expr, Exists):
+                expr = Case(When(expr, then=True), default=False)
+                wrapped = True
+            exprs.append(expr)
+        lookup = type(self)(*exprs) if wrapped else self
+        return lookup.as_sql(compiler, connection)
+
     @cached_property
     def contains_aggregate(self):
         return self.lhs.contains_aggregate or getattr(self.rhs, 'contains_aggregate', False)
@@ -124,6 +143,18 @@
     @property
     def is_summary(self):
         return self.lhs.is_summary or getattr(self.rhs, 'is_summary', False)
+
+    @property
+    def identity(self):
+        return self.__class__, self.lhs, self.rhs
+
+    def __eq__(self, other):
+        if not isinstance(other, Lookup):
+            return NotImplemented
+        return self.identity == other.identity
+
+    def __hash__(self):
+        return hash(make_hashable(self.identity))


 class Transform(RegisterLookupMixin, Func):
@@ -177,8 +208,9 @@
     get_db_prep_lookup_value_is_iterable = False

     def get_db_prep_lookup(self, value, connection):
-        # For relational fields, use the output_field of the 'field' attribute.
-        field = getattr(self.lhs.output_field, 'field', None)
+        # For relational fields, use the 'target_field' attribute of the
+        # output_field.
+        field = getattr(self.lhs.output_field, 'target_field', None)
         get_db_prep_value = getattr(field, 'get_db_prep_value', None) or self.lhs.output_field.get_db_prep_value
         return (
             '%s',
@@ -196,11 +228,9 @@
     get_db_prep_lookup_value_is_iterable = True

     def get_prep_lookup(self):
+        if hasattr(self.rhs, 'resolve_expression'):
+            return self.rhs
         prepared_values = []
-        if hasattr(self.rhs, '_prepare'):
-            # A subquery is like an iterable but its items shouldn't be
-            # prepared independently.
-            return self.rhs._prepare(self.lhs.output_field)
         for rhs_value in self.rhs:
             if hasattr(rhs_value, 'resolve_expression'):
                 # An expression will be handled by the database but can coexist
@@ -224,7 +254,7 @@
         if hasattr(param, 'resolve_expression'):
             param = param.resolve_expression(compiler.query)
         if hasattr(param, 'as_sql'):
-            sql, params = param.as_sql(compiler, connection)
+            sql, params = compiler.compile(param)
         return sql, params

     def batch_process_rhs(self, compiler, connection, rhs=None):
@@ -241,6 +271,17 @@
         return sql, tuple(params)


+class PostgresOperatorLookup(FieldGetDbPrepValueMixin, Lookup):
+    """Lookup defined by operators on PostgreSQL."""
+    postgres_operator = None
+
+    def as_postgresql(self, compiler, connection):
+        lhs, lhs_params = self.process_lhs(compiler, connection)
+        rhs, rhs_params = self.process_rhs(compiler, connection)
+        params = tuple(lhs_params) + tuple(rhs_params)
+        return '%s %s %s' % (lhs, self.postgres_operator, rhs), params
+
+
 @Field.register_lookup
 class Exact(FieldGetDbPrepValueMixin, BuiltinLookup):
     lookup_name = 'exact'
@@ -249,15 +290,29 @@
         from django.db.models.sql.query import Query
         if isinstance(self.rhs, Query):
             if self.rhs.has_limit_one():
-                # The subquery must select only the pk.
-                self.rhs.clear_select_clause()
-                self.rhs.add_fields(['pk'])
+                if not self.rhs.has_select_fields:
+                    self.rhs.clear_select_clause()
+                    self.rhs.add_fields(['pk'])
             else:
                 raise ValueError(
                     'The QuerySet value for an exact lookup must be limited to '
                     'one result using slicing.'
                 )
         return super().process_rhs(compiler, connection)
+
+    def as_sql(self, compiler, connection):
+        # Avoid comparison against direct rhs if lhs is a boolean value. That
+        # turns "boolfield__exact=True" into "WHERE boolean_field" instead of
+        # "WHERE boolean_field = True" when allowed.
+        if (
+            isinstance(self.rhs, bool) and
+            getattr(self.lhs, 'conditional', False) and
+            connection.ops.conditional_expression_supported_in_where_clause(self.lhs)
+        ):
+            lhs_sql, params = self.process_lhs(compiler, connection)
+            template = '%s' if self.rhs else 'NOT %s'
+            return template % lhs_sql, params
+        return super().as_sql(compiler, connection)


 @Field.register_lookup
@@ -326,10 +381,12 @@
             )

         if self.rhs_is_direct_value():
+            # Remove None from the list as NULL is never equal to anything.
             try:
                 rhs = OrderedSet(self.rhs)
+                rhs.discard(None)
             except TypeError:  # Unhashable items in self.rhs
-                rhs = self.rhs
+                rhs = [r for r in self.rhs if r is not None]

             if not rhs:
                 raise EmptyResultSet
@@ -450,6 +507,17 @@
     prepare_rhs = False

     def as_sql(self, compiler, connection):
+        if not isinstance(self.rhs, bool):
+            # When the deprecation ends, replace with:
+            # raise ValueError(
+            #     'The QuerySet value for an isnull lookup must be True or '
+            #     'False.'
+            # )
+            warnings.warn(
+                'Using a non-boolean value for an isnull lookup is '
+                'deprecated, use True or False instead.',
+                RemovedInDjango40Warning,
+            )
         sql, params = compiler.compile(self.lhs)
         if self.rhs:
             return "%s IS NULL" % sql, params
@@ -486,71 +554,102 @@
             bounds = connection.ops.year_lookup_bounds_for_date_field(year)
         return bounds

-
-class YearComparisonLookup(YearLookup):
     def as_sql(self, compiler, connection):
-        # We will need to skip the extract part and instead go
-        # directly with the originating field, that is self.lhs.lhs.
-        lhs_sql, params = self.process_lhs(compiler, connection, self.lhs.lhs)
-        rhs_sql, rhs_params = self.process_rhs(compiler, connection)
-        rhs_sql = self.get_rhs_op(connection, rhs_sql)
-        start, finish = self.year_lookup_bounds(connection, rhs_params[0])
-        params.append(self.get_bound(start, finish))
-        return '%s %s' % (lhs_sql, rhs_sql), params
-
-    def get_rhs_op(self, connection, rhs):
+        # Avoid the extract operation if the rhs is a direct value to allow
+        # indexes to be used.
+        if self.rhs_is_direct_value():
+            # Skip the extract part by directly using the originating field,
+            # that is self.lhs.lhs.
+            lhs_sql, params = self.process_lhs(compiler, connection, self.lhs.lhs)
+            rhs_sql, _ = self.process_rhs(compiler, connection)
+            rhs_sql = self.get_direct_rhs_sql(connection, rhs_sql)
+            start, finish = self.year_lookup_bounds(connection, self.rhs)
+            params.extend(self.get_bound_params(start, finish))
+            return '%s %s' % (lhs_sql, rhs_sql), params
+        return super().as_sql(compiler, connection)
+
+    def get_direct_rhs_sql(self, connection, rhs):
         return connection.operators[self.lookup_name] % rhs

-    def get_bound(self, start, finish):
+    def get_bound_params(self, start, finish):
         raise NotImplementedError(
-            'subclasses of YearComparisonLookup must provide a get_bound() method'
+            'subclasses of YearLookup must provide a get_bound_params() method'
         )


 class YearExact(YearLookup, Exact):
-    lookup_name = 'exact'
-
-    def as_sql(self, compiler, connection):
-        # We will need to skip the extract part and instead go
-        # directly with the originating field, that is self.lhs.lhs.
-        lhs_sql, params = self.process_lhs(compiler, connection, self.lhs.lhs)
-        rhs_sql, rhs_params = self.process_rhs(compiler, connection)
-        try:
-            # Check that rhs_params[0] exists (IndexError),
-            # it isn't None (TypeError), and is a number (ValueError)
-            int(rhs_params[0])
-        except (IndexError, TypeError, ValueError):
-            # Can't determine the bounds before executing the query, so skip
-            # optimizations by falling back to a standard exact comparison.
-            return super().as_sql(compiler, connection)
-        bounds = self.year_lookup_bounds(connection, rhs_params[0])
-        params.extend(bounds)
-        return '%s BETWEEN %%s AND %%s' % lhs_sql, params
-
-
-class YearGt(YearComparisonLookup):
-    lookup_name = 'gt'
-
-    def get_bound(self, start, finish):
-        return finish
-
-
-class YearGte(YearComparisonLookup):
-    lookup_name = 'gte'
-
-    def get_bound(self, start, finish):
-        return start
-
-
-class YearLt(YearComparisonLookup):
-    lookup_name = 'lt'
-
-    def get_bound(self, start, finish):
-        return start
-
-
-class YearLte(YearComparisonLookup):
-    lookup_name = 'lte'
-
-    def get_bound(self, start, finish):
-        return finish
+    def get_direct_rhs_sql(self, connection, rhs):
+        return 'BETWEEN %s AND %s'
+
+    def get_bound_params(self, start, finish):
+        return (start, finish)
+
+
+class YearGt(YearLookup, GreaterThan):
+    def get_bound_params(self, start, finish):
+        return (finish,)
+
+
+class YearGte(YearLookup, GreaterThanOrEqual):
+    def get_bound_params(self, start, finish):
+        return (start,)
+
+
+class YearLt(YearLookup, LessThan):
+    def get_bound_params(self, start, finish):
+        return (start,)
+
+
+class YearLte(YearLookup, LessThanOrEqual):
+    def get_bound_params(self, start, finish):
+        return (finish,)
+
+
+class UUIDTextMixin:
+    """
+    Strip hyphens from a value when filtering a UUIDField on backends without
+    a native datatype for UUID.
+    """
+    def process_rhs(self, qn, connection):
+        if not connection.features.has_native_uuid_field:
+            from django.db.models.functions import Replace
+            if self.rhs_is_direct_value():
+                self.rhs = Value(self.rhs)
+            self.rhs = Replace(self.rhs, Value('-'), Value(''), output_field=CharField())
+        rhs, params = super().process_rhs(qn, connection)
+        return rhs, params
+
+
+@UUIDField.register_lookup
+class UUIDIExact(UUIDTextMixin, IExact):
+    pass
+
+
+@UUIDField.register_lookup
+class UUIDContains(UUIDTextMixin, Contains):
+    pass
+
+
+@UUIDField.register_lookup
+class UUIDIContains(UUIDTextMixin, IContains):
+    pass
+
+
+@UUIDField.register_lookup
+class UUIDStartsWith(UUIDTextMixin, StartsWith):
+    pass
+
+
+@UUIDField.register_lookup
+class UUIDIStartsWith(UUIDTextMixin, IStartsWith):
+    pass
+
+
+@UUIDField.register_lookup
+class UUIDEndsWith(UUIDTextMixin, EndsWith):
+    pass
+
+
+@UUIDField.register_lookup
+class UUIDIEndsWith(UUIDTextMixin, IEndsWith):
+    pass
('django/db/models', 'indexes.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,9 @@
 from django.db.backends.utils import names_digest, split_identifier
+from django.db.models.expressions import Col, ExpressionList, F, Func, OrderBy
+from django.db.models.functions import Collate
 from django.db.models.query_utils import Q
 from django.db.models.sql import Query
+from django.utils.functional import partition

 __all__ = ['Index']

@@ -11,7 +14,16 @@
     # cross-database compatibility with Oracle)
     max_name_length = 30

-    def __init__(self, *, fields=(), name=None, db_tablespace=None, opclasses=(), condition=None):
+    def __init__(
+        self,
+        *expressions,
+        fields=(),
+        name=None,
+        db_tablespace=None,
+        opclasses=(),
+        condition=None,
+        include=None,
+    ):
         if opclasses and not name:
             raise ValueError('An index must be named to use opclasses.')
         if not isinstance(condition, (type(None), Q)):
@@ -22,10 +34,30 @@
             raise ValueError('Index.fields must be a list or tuple.')
         if not isinstance(opclasses, (list, tuple)):
             raise ValueError('Index.opclasses must be a list or tuple.')
+        if not expressions and not fields:
+            raise ValueError(
+                'At least one field or expression is required to define an '
+                'index.'
+            )
+        if expressions and fields:
+            raise ValueError(
+                'Index.fields and expressions are mutually exclusive.',
+            )
+        if expressions and not name:
+            raise ValueError('An index must be named to use expressions.')
+        if expressions and opclasses:
+            raise ValueError(
+                'Index.opclasses cannot be used with expressions. Use '
+                'django.contrib.postgres.indexes.OpClass() instead.'
+            )
         if opclasses and len(fields) != len(opclasses):
             raise ValueError('Index.fields and Index.opclasses must have the same number of elements.')
-        if not fields:
-            raise ValueError('At least one field is required to define an index.')
+        if fields and not all(isinstance(field, str) for field in fields):
+            raise ValueError('Index.fields must contain only strings with field names.')
+        if include and not name:
+            raise ValueError('A covering index must be named.')
+        if not isinstance(include, (type(None), list, tuple)):
+            raise ValueError('Index.include must be a list or tuple.')
         self.fields = list(fields)
         # A list of 2-tuple with the field name and ordering ('' or 'DESC').
         self.fields_orders = [
@@ -33,68 +65,79 @@
             for field_name in self.fields
         ]
         self.name = name or ''
-        if self.name:
-            errors = self.check_name()
-            if len(self.name) > self.max_name_length:
-                errors.append('Index names cannot be longer than %s characters.' % self.max_name_length)
-            if errors:
-                raise ValueError(errors)
         self.db_tablespace = db_tablespace
         self.opclasses = opclasses
         self.condition = condition
-
-    def check_name(self):
-        errors = []
-        # Name can't start with an underscore on Oracle; prepend D if needed.
-        if self.name[0] == '_':
-            errors.append('Index names cannot start with an underscore (_).')
-            self.name = 'D%s' % self.name[1:]
-        # Name can't start with a number on Oracle; prepend D if needed.
-        elif self.name[0].isdigit():
-            errors.append('Index names cannot start with a number (0-9).')
-            self.name = 'D%s' % self.name[1:]
-        return errors
+        self.include = tuple(include) if include else ()
+        self.expressions = tuple(
+            F(expression) if isinstance(expression, str) else expression
+            for expression in expressions
+        )
+
+    @property
+    def contains_expressions(self):
+        return bool(self.expressions)

     def _get_condition_sql(self, model, schema_editor):
         if self.condition is None:
             return None
-        query = Query(model=model)
-        query.add_q(self.condition)
+        query = Query(model=model, alias_cols=False)
+        where = query.build_where(self.condition)
         compiler = query.get_compiler(connection=schema_editor.connection)
-        # Only the WhereNode is of interest for the partial index.
-        sql, params = query.where.as_sql(compiler=compiler, connection=schema_editor.connection)
-        # BaseDatabaseSchemaEditor does the same map on the params, but since
-        # it's handled outside of that class, the work is done here.
-        return sql % tuple(map(schema_editor.quote_value, params))
-
-    def create_sql(self, model, schema_editor, using=''):
-        fields = [model._meta.get_field(field_name) for field_name, _ in self.fields_orders]
-        col_suffixes = [order[1] for order in self.fields_orders]
+        sql, params = where.as_sql(compiler, schema_editor.connection)
+        return sql % tuple(schema_editor.quote_value(p) for p in params)
+
+    def create_sql(self, model, schema_editor, using='', **kwargs):
+        include = [model._meta.get_field(field_name).column for field_name in self.include]
         condition = self._get_condition_sql(model, schema_editor)
+        if self.expressions:
+            index_expressions = []
+            for expression in self.expressions:
+                index_expression = IndexExpression(expression)
+                index_expression.set_wrapper_classes(schema_editor.connection)
+                index_expressions.append(index_expression)
+            expressions = ExpressionList(*index_expressions).resolve_expression(
+                Query(model, alias_cols=False),
+            )
+            fields = None
+            col_suffixes = None
+        else:
+            fields = [
+                model._meta.get_field(field_name)
+                for field_name, _ in self.fields_orders
+            ]
+            col_suffixes = [order[1] for order in self.fields_orders]
+            expressions = None
         return schema_editor._create_index_sql(
-            model, fields, name=self.name, using=using, db_tablespace=self.db_tablespace,
-            col_suffixes=col_suffixes, opclasses=self.opclasses, condition=condition,
-        )
-
-    def remove_sql(self, model, schema_editor):
-        return schema_editor._delete_index_sql(model, self.name)
+            model, fields=fields, name=self.name, using=using,
+            db_tablespace=self.db_tablespace, col_suffixes=col_suffixes,
+            opclasses=self.opclasses, condition=condition, include=include,
+            expressions=expressions, **kwargs,
+        )
+
+    def remove_sql(self, model, schema_editor, **kwargs):
+        return schema_editor._delete_index_sql(model, self.name, **kwargs)

     def deconstruct(self):
         path = '%s.%s' % (self.__class__.__module__, self.__class__.__name__)
         path = path.replace('django.db.models.indexes', 'django.db.models')
-        kwargs = {'fields': self.fields, 'name': self.name}
+        kwargs = {'name': self.name}
+        if self.fields:
+            kwargs['fields'] = self.fields
         if self.db_tablespace is not None:
             kwargs['db_tablespace'] = self.db_tablespace
         if self.opclasses:
             kwargs['opclasses'] = self.opclasses
         if self.condition:
             kwargs['condition'] = self.condition
-        return (path, (), kwargs)
+        if self.include:
+            kwargs['include'] = self.include
+        return (path, self.expressions, kwargs)

     def clone(self):
         """Create a copy of this Index."""
-        _, _, kwargs = self.deconstruct()
-        return self.__class__(**kwargs)
+        _, args, kwargs = self.deconstruct()
+        return self.__class__(*args, **kwargs)

     def set_name_with_model(self, model):
         """
@@ -122,13 +165,103 @@
             'Index too long for multiple database support. Is self.suffix '
             'longer than 3 characters?'
         )
-        self.check_name()
+        if self.name[0] == '_' or self.name[0].isdigit():
+            self.name = 'D%s' % self.name[1:]

     def __repr__(self):
-        return "<%s: fields='%s'%s>" % (
-            self.__class__.__name__, ', '.join(self.fields),
-            '' if self.condition is None else ', condition=%s' % self.condition,
+        return '<%s:%s%s%s%s%s>' % (
+            self.__class__.__name__,
+            '' if not self.fields else " fields='%s'" % ', '.join(self.fields),
+            '' if not self.expressions else " expressions='%s'" % ', '.join([
+                str(expression) for expression in self.expressions
+            ]),
+            '' if self.condition is None else ' condition=%s' % self.condition,
+            '' if not self.include else " include='%s'" % ', '.join(self.include),
+            '' if not self.opclasses else " opclasses='%s'" % ', '.join(self.opclasses),
         )

     def __eq__(self, other):
-        return (self.__class__ == other.__class__) and (self.deconstruct() == other.deconstruct())
+        if self.__class__ == other.__class__:
+            return self.deconstruct() == other.deconstruct()
+        return NotImplemented
+
+
+class IndexExpression(Func):
+    """Order and wrap expressions for CREATE INDEX statements."""
+    template = '%(expressions)s'
+    wrapper_classes = (OrderBy, Collate)
+
+    def set_wrapper_classes(self, connection=None):
+        # Some databases (e.g. MySQL) treats COLLATE as an indexed expression.
+        if connection and connection.features.collate_as_index_expression:
+            self.wrapper_classes = tuple([
+                wrapper_cls
+                for wrapper_cls in self.wrapper_classes
+                if wrapper_cls is not Collate
+            ])
+
+    @classmethod
+    def register_wrappers(cls, *wrapper_classes):
+        cls.wrapper_classes = wrapper_classes
+
+    def resolve_expression(
+        self,
+        query=None,
+        allow_joins=True,
+        reuse=None,
+        summarize=False,
+        for_save=False,
+    ):
+        expressions = list(self.flatten())
+        # Split expressions and wrappers.
+        index_expressions, wrappers = partition(
+            lambda e: isinstance(e, self.wrapper_classes),
+            expressions,
+        )
+        wrapper_types = [type(wrapper) for wrapper in wrappers]
+        if len(wrapper_types) != len(set(wrapper_types)):
+            raise ValueError(
+                "Multiple references to %s can't be used in an indexed "
+                "expression." % ', '.join([
+                    wrapper_cls.__qualname__ for wrapper_cls in self.wrapper_classes
+                ])
+            )
+        if expressions[1:len(wrappers) + 1] != wrappers:
+            raise ValueError(
+                '%s must be topmost expressions in an indexed expression.'
+                % ', '.join([
+                    wrapper_cls.__qualname__ for wrapper_cls in self.wrapper_classes
+                ])
+            )
+        # Wrap expressions in parentheses if they are not column references.
+        root_expression = index_expressions[1]
+        resolve_root_expression = root_expression.resolve_expression(
+            query,
+            allow_joins,
+            reuse,
+            summarize,
+            for_save,
+        )
+        if not isinstance(resolve_root_expression, Col):
+            root_expression = Func(root_expression, template='(%(expressions)s)')
+
+        if wrappers:
+            # Order wrappers and set their expressions.
+            wrappers = sorted(
+                wrappers,
+                key=lambda w: self.wrapper_classes.index(type(w)),
+            )
+            wrappers = [wrapper.copy() for wrapper in wrappers]
+            for i, wrapper in enumerate(wrappers[:-1]):
+                wrapper.set_source_expressions([wrappers[i + 1]])
+            # Set the root expression on the deepest wrapper.
+            wrappers[-1].set_source_expressions([root_expression])
+            self.set_source_expressions([wrappers[0]])
+        else:
+            # Use the root expression, if there are no wrappers.
+            self.set_source_expressions([root_expression])
+        return super().resolve_expression(query, allow_joins, reuse, summarize, for_save)
+
+    def as_sqlite(self, compiler, connection, **extra_context):
+        # Casting to numeric is unnecessary.
+        return self.as_sql(compiler, connection, **extra_context)
('django/db/models', 'utils.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,7 @@
+import functools
+from collections import namedtuple
+
+
 def make_model_tuple(model):
     """
     Take a model or a string of the form "app_label.ModelName" and return a
@@ -19,3 +23,30 @@
             "Invalid model reference '%s'. String model references "
             "must be of the form 'app_label.ModelName'." % model
         )
+
+
+def resolve_callables(mapping):
+    """
+    Generate key/value pairs for the given mapping where the values are
+    evaluated if they're callable.
+    """
+    for k, v in mapping.items():
+        yield k, v() if callable(v) else v
+
+
+def unpickle_named_row(names, values):
+    return create_namedtuple_class(*names)(*values)
+
+
+@functools.lru_cache()
+def create_namedtuple_class(*names):
+    # Cache type() with @lru_cache() since it's too slow to be called for every
+    # QuerySet evaluation.
+    def __reduce__(self):
+        return unpickle_named_row, (names, tuple(self))
+
+    return type(
+        'Row',
+        (namedtuple('Row', names),),
+        {'__reduce__': __reduce__, '__slots__': ()},
+    )
('django/db/models', 'aggregates.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -64,7 +64,7 @@
             return '%s__%s' % (expressions[0].name, self.name.lower())
         raise TypeError("Complex expressions require an alias")

-    def get_group_by_cols(self):
+    def get_group_by_cols(self, alias=None):
         return []

     def as_sql(self, compiler, connection, **extra_context):
@@ -99,6 +99,7 @@
 class Avg(FixDurationInputMixin, NumericOutputFieldMixin, Aggregate):
     function = 'AVG'
     name = 'Avg'
+    allow_distinct = True


 class Count(Aggregate):
@@ -142,6 +143,7 @@
 class Sum(FixDurationInputMixin, Aggregate):
     function = 'SUM'
     name = 'Sum'
+    allow_distinct = True


 class Variance(NumericOutputFieldMixin, Aggregate):
('django/db/models', 'deletion.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,8 +1,11 @@
-from collections import Counter, OrderedDict
+import operator
+from collections import Counter, defaultdict
+from functools import partial, reduce
+from itertools import chain
 from operator import attrgetter

 from django.db import IntegrityError, connections, transaction
-from django.db.models import signals, sql
+from django.db.models import query_utils, signals, sql


 class ProtectedError(IntegrityError):
@@ -11,9 +14,17 @@
         super().__init__(msg, protected_objects)


+class RestrictedError(IntegrityError):
+    def __init__(self, msg, restricted_objects):
+        self.restricted_objects = restricted_objects
+        super().__init__(msg, restricted_objects)
+
+
 def CASCADE(collector, field, sub_objs, using):
-    collector.collect(sub_objs, source=field.remote_field.model,
-                      source_attr=field.name, nullable=field.null)
+    collector.collect(
+        sub_objs, source=field.remote_field.model, source_attr=field.name,
+        nullable=field.null, fail_on_restricted=False,
+    )
     if field.null and not connections[using].features.can_defer_constraint_checks:
         collector.add_field_update(field, None, sub_objs)

@@ -26,6 +37,11 @@
         ),
         sub_objs
     )
+
+
+def RESTRICT(collector, field, sub_objs, using):
+    collector.add_restricted_objects(field, sub_objs)
+    collector.add_dependency(field.remote_field.model, field.model)


 def SET(value):
@@ -64,8 +80,11 @@
     def __init__(self, using):
         self.using = using
         # Initially, {model: {instances}}, later values become lists.
-        self.data = OrderedDict()
-        self.field_updates = {}  # {model: {(field, value): {instances}}}
+        self.data = defaultdict(set)
+        # {model: {(field, value): {instances}}}
+        self.field_updates = defaultdict(partial(defaultdict, set))
+        # {model: {field: {instances}}}
+        self.restricted_objects = defaultdict(partial(defaultdict, set))
         # fast_deletes is a list of queryset-likes that can be deleted without
         # fetching the objects into memory.
         self.fast_deletes = []
@@ -75,7 +94,7 @@
         # should be included, as the dependencies exist only between actual
         # database tables; proxy models are represented here by their concrete
         # parent.
-        self.dependencies = {}  # {model: {models}}
+        self.dependencies = defaultdict(set)  # {model: {models}}

     def add(self, objs, source=None, nullable=False, reverse_dependency=False):
         """
@@ -89,7 +108,7 @@
             return []
         new_objs = []
         model = objs[0].__class__
-        instances = self.data.setdefault(model, set())
+        instances = self.data[model]
         for obj in objs:
             if obj not in instances:
                 new_objs.append(obj)
@@ -98,11 +117,14 @@
         # deleting, and therefore do not affect the order in which objects have
         # to be deleted.
         if source is not None and not nullable:
-            if reverse_dependency:
-                source, model = model, source
-            self.dependencies.setdefault(
-                source._meta.concrete_model, set()).add(model._meta.concrete_model)
+            self.add_dependency(source, model, reverse_dependency=reverse_dependency)
         return new_objs
+
+    def add_dependency(self, model, dependency, reverse_dependency=False):
+        if reverse_dependency:
+            model, dependency = dependency, model
+        self.dependencies[model._meta.concrete_model].add(dependency._meta.concrete_model)
+        self.data.setdefault(dependency, self.data.default_factory())

     def add_field_update(self, field, value, objs):
         """
@@ -112,9 +134,33 @@
         if not objs:
             return
         model = objs[0].__class__
-        self.field_updates.setdefault(
-            model, {}).setdefault(
-            (field, value), set()).update(objs)
+        self.field_updates[model][field, value].update(objs)
+
+    def add_restricted_objects(self, field, objs):
+        if objs:
+            model = objs[0].__class__
+            self.restricted_objects[model][field].update(objs)
+
+    def clear_restricted_objects_from_set(self, model, objs):
+        if model in self.restricted_objects:
+            self.restricted_objects[model] = {
+                field: items - objs
+                for field, items in self.restricted_objects[model].items()
+            }
+
+    def clear_restricted_objects_from_queryset(self, model, qs):
+        if model in self.restricted_objects:
+            objs = set(qs.filter(pk__in=[
+                obj.pk
+                for objs in self.restricted_objects[model].values() for obj in objs
+            ]))
+            self.clear_restricted_objects_from_set(model, objs)
+
+    def _has_signal_listeners(self, model):
+        return (
+            signals.pre_delete.has_listeners(model) or
+            signals.post_delete.has_listeners(model)
+        )

     def can_fast_delete(self, objs, from_field=None):
         """
@@ -130,14 +176,12 @@
         if from_field and from_field.remote_field.on_delete is not CASCADE:
             return False
         if hasattr(objs, '_meta'):
-            model = type(objs)
+            model = objs._meta.model
         elif hasattr(objs, 'model') and hasattr(objs, '_raw_delete'):
             model = objs.model
         else:
             return False
-        if (signals.pre_delete.has_listeners(model) or
-                signals.post_delete.has_listeners(model) or
-                signals.m2m_changed.has_listeners(model)):
+        if self._has_signal_listeners(model):
             return False
         # The use of from_field comes from the need to avoid cascade back to
         # parent when parent delete is cascading to child.
@@ -154,12 +198,13 @@
             )
         )

-    def get_del_batches(self, objs, field):
+    def get_del_batches(self, objs, fields):
         """
         Return the objs in suitably sized batches for the used connection.
         """
+        field_names = [field.name for field in fields]
         conn_batch_size = max(
-            connections[self.using].ops.bulk_batch_size([field.name], objs), 1)
+            connections[self.using].ops.bulk_batch_size(field_names, objs), 1)
         if len(objs) > conn_batch_size:
             return [objs[i:i + conn_batch_size]
                     for i in range(0, len(objs), conn_batch_size)]
@@ -167,7 +212,8 @@
             return [objs]

     def collect(self, objs, source=None, nullable=False, collect_related=True,
-                source_attr=None, reverse_dependency=False, keep_parents=False):
+                source_attr=None, reverse_dependency=False, keep_parents=False,
+                fail_on_restricted=True):
         """
         Add 'objs' to the collection of objects to be deleted as well as all
         parent instances.  'objs' must be a homogeneous iterable collection of
@@ -184,6 +230,12 @@
         direction of an FK rather than the reverse direction.)

         If 'keep_parents' is True, data of parent model's will be not deleted.
+
+        If 'fail_on_restricted' is False, error won't be raised even if it's
+        prohibited to delete such objects due to RESTRICT, that defers
+        restricted object checking in recursive calls where the top-level call
+        may need to collect more objects to determine whether restricted ones
+        can be deleted.
         """
         if self.can_fast_delete(objs):
             self.fast_deletes.append(objs)
@@ -205,36 +257,101 @@
                     self.collect(parent_objs, source=model,
                                  source_attr=ptr.remote_field.related_name,
                                  collect_related=False,
-                                 reverse_dependency=True)
-        if collect_related:
-            parents = model._meta.parents
-            for related in get_candidate_relations_to_delete(model._meta):
-                # Preserve parent reverse relationships if keep_parents=True.
-                if keep_parents and related.model in parents:
-                    continue
-                field = related.field
-                if field.remote_field.on_delete == DO_NOTHING:
-                    continue
-                batches = self.get_del_batches(new_objs, field)
-                for batch in batches:
-                    sub_objs = self.related_objects(related, batch)
-                    if self.can_fast_delete(sub_objs, from_field=field):
-                        self.fast_deletes.append(sub_objs)
-                    elif sub_objs:
+                                 reverse_dependency=True,
+                                 fail_on_restricted=False)
+        if not collect_related:
+            return
+
+        if keep_parents:
+            parents = set(model._meta.get_parent_list())
+        model_fast_deletes = defaultdict(list)
+        protected_objects = defaultdict(list)
+        for related in get_candidate_relations_to_delete(model._meta):
+            # Preserve parent reverse relationships if keep_parents=True.
+            if keep_parents and related.model in parents:
+                continue
+            field = related.field
+            if field.remote_field.on_delete == DO_NOTHING:
+                continue
+            related_model = related.related_model
+            if self.can_fast_delete(related_model, from_field=field):
+                model_fast_deletes[related_model].append(field)
+                continue
+            batches = self.get_del_batches(new_objs, [field])
+            for batch in batches:
+                sub_objs = self.related_objects(related_model, [field], batch)
+                # Non-referenced fields can be deferred if no signal receivers
+                # are connected for the related model as they'll never be
+                # exposed to the user. Skip field deferring when some
+                # relationships are select_related as interactions between both
+                # features are hard to get right. This should only happen in
+                # the rare cases where .related_objects is overridden anyway.
+                if not (sub_objs.query.select_related or self._has_signal_listeners(related_model)):
+                    referenced_fields = set(chain.from_iterable(
+                        (rf.attname for rf in rel.field.foreign_related_fields)
+                        for rel in get_candidate_relations_to_delete(related_model._meta)
+                    ))
+                    sub_objs = sub_objs.only(*tuple(referenced_fields))
+                if sub_objs:
+                    try:
                         field.remote_field.on_delete(self, field, sub_objs, self.using)
-            for field in model._meta.private_fields:
-                if hasattr(field, 'bulk_related_objects'):
-                    # It's something like generic foreign key.
-                    sub_objs = field.bulk_related_objects(new_objs, self.using)
-                    self.collect(sub_objs, source=model, nullable=True)
-
-    def related_objects(self, related, objs):
-        """
-        Get a QuerySet of objects related to `objs` via the relation `related`.
-        """
-        return related.related_model._base_manager.using(self.using).filter(
-            **{"%s__in" % related.field.name: objs}
-        )
+                    except ProtectedError as error:
+                        key = "'%s.%s'" % (field.model.__name__, field.name)
+                        protected_objects[key] += error.protected_objects
+        if protected_objects:
+            raise ProtectedError(
+                'Cannot delete some instances of model %r because they are '
+                'referenced through protected foreign keys: %s.' % (
+                    model.__name__,
+                    ', '.join(protected_objects),
+                ),
+                set(chain.from_iterable(protected_objects.values())),
+            )
+        for related_model, related_fields in model_fast_deletes.items():
+            batches = self.get_del_batches(new_objs, related_fields)
+            for batch in batches:
+                sub_objs = self.related_objects(related_model, related_fields, batch)
+                self.fast_deletes.append(sub_objs)
+        for field in model._meta.private_fields:
+            if hasattr(field, 'bulk_related_objects'):
+                # It's something like generic foreign key.
+                sub_objs = field.bulk_related_objects(new_objs, self.using)
+                self.collect(sub_objs, source=model, nullable=True, fail_on_restricted=False)
+
+        if fail_on_restricted:
+            # Raise an error if collected restricted objects (RESTRICT) aren't
+            # candidates for deletion also collected via CASCADE.
+            for related_model, instances in self.data.items():
+                self.clear_restricted_objects_from_set(related_model, instances)
+            for qs in self.fast_deletes:
+                self.clear_restricted_objects_from_queryset(qs.model, qs)
+            if self.restricted_objects.values():
+                restricted_objects = defaultdict(list)
+                for related_model, fields in self.restricted_objects.items():
+                    for field, objs in fields.items():
+                        if objs:
+                            key = "'%s.%s'" % (related_model.__name__, field.name)
+                            restricted_objects[key] += objs
+                if restricted_objects:
+                    raise RestrictedError(
+                        'Cannot delete some instances of model %r because '
+                        'they are referenced through restricted foreign keys: '
+                        '%s.' % (
+                            model.__name__,
+                            ', '.join(restricted_objects),
+                        ),
+                        set(chain.from_iterable(restricted_objects.values())),
+                    )
+
+    def related_objects(self, related_model, related_fields, objs):
+        """
+        Get a QuerySet of the related model to objs via related fields.
+        """
+        predicate = reduce(operator.or_, (
+            query_utils.Q(**{'%s__in' % related_field.name: objs})
+            for related_field in related_fields
+        ))
+        return related_model._base_manager.using(self.using).filter(predicate)

     def instances_with_model(self):
         for model, instances in self.data.items():
@@ -257,8 +374,7 @@
                     found = True
             if not found:
                 return
-        self.data = OrderedDict((model, self.data[model])
-                                for model in sorted_models)
+        self.data = {model: self.data[model] for model in sorted_models}

     def delete(self):
         # sort instance collections
@@ -276,8 +392,9 @@
         if len(self.data) == 1 and len(instances) == 1:
             instance = list(instances)[0]
             if self.can_fast_delete(instance):
-                with transaction.mark_for_rollback_on_error():
+                with transaction.mark_for_rollback_on_error(self.using):
                     count = sql.DeleteQuery(model).delete_batch([instance.pk], self.using)
+                setattr(instance, model._meta.pk.attname, None)
                 return count, {model._meta.label: count}

         with transaction.atomic(using=self.using, savepoint=False):
@@ -291,7 +408,8 @@
             # fast deletes
             for qs in self.fast_deletes:
                 count = qs._raw_delete(using=self.using)
-                deleted_counter[qs.model._meta.label] += count
+                if count:
+                    deleted_counter[qs.model._meta.label] += count

             # update fields
             for model, instances_for_fieldvalues in self.field_updates.items():
@@ -309,7 +427,8 @@
                 query = sql.DeleteQuery(model)
                 pk_list = [obj.pk for obj in instances]
                 count = query.delete_batch(pk_list, self.using)
-                deleted_counter[model._meta.label] += count
+                if count:
+                    deleted_counter[model._meta.label] += count

                 if not model._meta.auto_created:
                     for obj in instances:
('django/db/models', 'constraints.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,7 +1,9 @@
+from enum import Enum
+
 from django.db.models.query_utils import Q
 from django.db.models.sql.query import Query

-__all__ = ['CheckConstraint', 'UniqueConstraint']
+__all__ = ['CheckConstraint', 'Deferrable', 'UniqueConstraint']


 class BaseConstraint:
@@ -30,10 +32,15 @@
 class CheckConstraint(BaseConstraint):
     def __init__(self, *, check, name):
         self.check = check
+        if not getattr(check, 'conditional', False):
+            raise TypeError(
+                'CheckConstraint.check must be a Q instance or boolean '
+                'expression.'
+            )
         super().__init__(name)

     def _get_check_sql(self, model, schema_editor):
-        query = Query(model=model)
+        query = Query(model=model, alias_cols=False)
         where = query.build_where(self.check)
         compiler = query.get_compiler(connection=schema_editor.connection)
         sql, params = where.as_sql(compiler, schema_editor.connection)
@@ -54,11 +61,9 @@
         return "<%s: check='%s' name=%r>" % (self.__class__.__name__, self.check, self.name)

     def __eq__(self, other):
-        return (
-            isinstance(other, CheckConstraint) and
-            self.name == other.name and
-            self.check == other.check
-        )
+        if isinstance(other, CheckConstraint):
+            return self.name == other.name and self.check == other.check
+        return super().__eq__(other)

     def deconstruct(self):
         path, args, kwargs = super().deconstruct()
@@ -66,20 +71,62 @@
         return path, args, kwargs


+class Deferrable(Enum):
+    DEFERRED = 'deferred'
+    IMMEDIATE = 'immediate'
+
+
 class UniqueConstraint(BaseConstraint):
-    def __init__(self, *, fields, name, condition=None):
+    def __init__(
+        self,
+        *,
+        fields,
+        name,
+        condition=None,
+        deferrable=None,
+        include=None,
+        opclasses=(),
+    ):
         if not fields:
             raise ValueError('At least one field is required to define a unique constraint.')
         if not isinstance(condition, (type(None), Q)):
             raise ValueError('UniqueConstraint.condition must be a Q instance.')
+        if condition and deferrable:
+            raise ValueError(
+                'UniqueConstraint with conditions cannot be deferred.'
+            )
+        if include and deferrable:
+            raise ValueError(
+                'UniqueConstraint with include fields cannot be deferred.'
+            )
+        if opclasses and deferrable:
+            raise ValueError(
+                'UniqueConstraint with opclasses cannot be deferred.'
+            )
+        if not isinstance(deferrable, (type(None), Deferrable)):
+            raise ValueError(
+                'UniqueConstraint.deferrable must be a Deferrable instance.'
+            )
+        if not isinstance(include, (type(None), list, tuple)):
+            raise ValueError('UniqueConstraint.include must be a list or tuple.')
+        if not isinstance(opclasses, (list, tuple)):
+            raise ValueError('UniqueConstraint.opclasses must be a list or tuple.')
+        if opclasses and len(fields) != len(opclasses):
+            raise ValueError(
+                'UniqueConstraint.fields and UniqueConstraint.opclasses must '
+                'have the same number of elements.'
+            )
         self.fields = tuple(fields)
         self.condition = condition
+        self.deferrable = deferrable
+        self.include = tuple(include) if include else ()
+        self.opclasses = opclasses
         super().__init__(name)

     def _get_condition_sql(self, model, schema_editor):
         if self.condition is None:
             return None
-        query = Query(model=model)
+        query = Query(model=model, alias_cols=False)
         where = query.build_where(self.condition)
         compiler = query.get_compiler(connection=schema_editor.connection)
         sql, params = where.as_sql(compiler, schema_editor.connection)
@@ -87,35 +134,62 @@

     def constraint_sql(self, model, schema_editor):
         fields = [model._meta.get_field(field_name).column for field_name in self.fields]
+        include = [model._meta.get_field(field_name).column for field_name in self.include]
         condition = self._get_condition_sql(model, schema_editor)
-        return schema_editor._unique_sql(model, fields, self.name, condition=condition)
+        return schema_editor._unique_sql(
+            model, fields, self.name, condition=condition,
+            deferrable=self.deferrable, include=include,
+            opclasses=self.opclasses,
+        )

     def create_sql(self, model, schema_editor):
         fields = [model._meta.get_field(field_name).column for field_name in self.fields]
+        include = [model._meta.get_field(field_name).column for field_name in self.include]
         condition = self._get_condition_sql(model, schema_editor)
-        return schema_editor._create_unique_sql(model, fields, self.name, condition=condition)
+        return schema_editor._create_unique_sql(
+            model, fields, self.name, condition=condition,
+            deferrable=self.deferrable, include=include,
+            opclasses=self.opclasses,
+        )

     def remove_sql(self, model, schema_editor):
         condition = self._get_condition_sql(model, schema_editor)
-        return schema_editor._delete_unique_sql(model, self.name, condition=condition)
+        include = [model._meta.get_field(field_name).column for field_name in self.include]
+        return schema_editor._delete_unique_sql(
+            model, self.name, condition=condition, deferrable=self.deferrable,
+            include=include, opclasses=self.opclasses,
+        )

     def __repr__(self):
-        return '<%s: fields=%r name=%r%s>' % (
+        return '<%s: fields=%r name=%r%s%s%s%s>' % (
             self.__class__.__name__, self.fields, self.name,
             '' if self.condition is None else ' condition=%s' % self.condition,
+            '' if self.deferrable is None else ' deferrable=%s' % self.deferrable,
+            '' if not self.include else ' include=%s' % repr(self.include),
+            '' if not self.opclasses else ' opclasses=%s' % repr(self.opclasses),
         )

     def __eq__(self, other):
-        return (
-            isinstance(other, UniqueConstraint) and
-            self.name == other.name and
-            self.fields == other.fields and
-            self.condition == other.condition
-        )
+        if isinstance(other, UniqueConstraint):
+            return (
+                self.name == other.name and
+                self.fields == other.fields and
+                self.condition == other.condition and
+                self.deferrable == other.deferrable and
+                self.include == other.include and
+                self.opclasses == other.opclasses
+            )
+        return super().__eq__(other)

     def deconstruct(self):
         path, args, kwargs = super().deconstruct()
         kwargs['fields'] = self.fields
         if self.condition:
             kwargs['condition'] = self.condition
+        if self.deferrable:
+            kwargs['deferrable'] = self.deferrable
+        if self.include:
+            kwargs['include'] = self.include
+        if self.opclasses:
+            kwargs['opclasses'] = self.opclasses
         return path, args, kwargs
('django/db/models', 'manager.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -34,6 +34,9 @@
     def __str__(self):
         """Return "app_label.model_label.manager_name"."""
         return '%s.%s' % (self.model._meta.label, self.name)
+
+    def __class_getitem__(cls, *args, **kwargs):
+        return cls

     def deconstruct(self):
         """
@@ -106,13 +109,13 @@
             **cls._get_queryset_methods(queryset_class),
         })

-    def contribute_to_class(self, model, name):
+    def contribute_to_class(self, cls, name):
         self.name = self.name or name
-        self.model = model
-
-        setattr(model, name, ManagerDescriptor(self))
-
-        model._meta.add_manager(self)
+        self.model = cls
+
+        setattr(cls, name, ManagerDescriptor(self))
+
+        cls._meta.add_manager(self)

     def _set_creation_counter(self):
         """
@@ -182,9 +185,8 @@

         if cls._meta.swapped:
             raise AttributeError(
-                "Manager isn't available; '%s.%s' has been swapped for '%s'" % (
-                    cls._meta.app_label,
-                    cls._meta.object_name,
+                "Manager isn't available; '%s' has been swapped for '%s'" % (
+                    cls._meta.label,
                     cls._meta.swapped,
                 )
             )
('django/db/models', 'query_utils.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -8,10 +8,13 @@
 import copy
 import functools
 import inspect
+import warnings
 from collections import namedtuple

+from django.core.exceptions import FieldDoesNotExist, FieldError
 from django.db.models.constants import LOOKUP_SEP
 from django.utils import tree
+from django.utils.deprecation import RemovedInDjango40Warning

 # PathInfo is used when converting lookups (fk__somecol). The contents
 # describe the relation in Model terms (model Options and Fields for both
@@ -19,8 +22,29 @@
 PathInfo = namedtuple('PathInfo', 'from_opts to_opts target_fields join_field m2m direct filtered_relation')


-class InvalidQuery(Exception):
-    """The query passed to raw() isn't a safe query to use with raw()."""
+class InvalidQueryType(type):
+    @property
+    def _subclasses(self):
+        return (FieldDoesNotExist, FieldError)
+
+    def __warn(self):
+        warnings.warn(
+            'The InvalidQuery exception class is deprecated. Use '
+            'FieldDoesNotExist or FieldError instead.',
+            category=RemovedInDjango40Warning,
+            stacklevel=4,
+        )
+
+    def __instancecheck__(self, instance):
+        self.__warn()
+        return isinstance(instance, self._subclasses) or super().__instancecheck__(instance)
+
+    def __subclasscheck__(self, subclass):
+        self.__warn()
+        return issubclass(subclass, self._subclasses) or super().__subclasscheck__(subclass)
+
+
+class InvalidQuery(Exception, metaclass=InvalidQueryType):
     pass


@@ -28,20 +52,6 @@
     yield cls
     for subclass in cls.__subclasses__():
         yield from subclasses(subclass)
-
-
-class QueryWrapper:
-    """
-    A type that indicates the contents are an SQL fragment and the associate
-    parameters. Can be used to pass opaque data to a where-clause, for example.
-    """
-    contains_aggregate = False
-
-    def __init__(self, sql, params):
-        self.data = sql, list(params)
-
-    def as_sql(self, compiler=None, connection=None):
-        return self.data


 class Q(tree.Node):
@@ -59,15 +69,14 @@
         super().__init__(children=[*args, *sorted(kwargs.items())], connector=_connector, negated=_negated)

     def _combine(self, other, conn):
-        if not isinstance(other, Q):
+        if not(isinstance(other, Q) or getattr(other, 'conditional', False) is True):
             raise TypeError(other)

-        # If the other Q() is empty, ignore it and just use `self`.
-        if not other:
-            return copy.deepcopy(self)
-        # Or if this Q is empty, ignore it and just use `other`.
-        elif not self:
-            return copy.deepcopy(other)
+        if not self:
+            return other.copy() if hasattr(other, 'copy') else copy.copy(other)
+        elif isinstance(other, Q) and not other:
+            _, args, kwargs = self.deconstruct()
+            return type(self)(*args, **kwargs)

         obj = type(self)()
         obj.connector = conn
@@ -90,7 +99,10 @@
     def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):
         # We must promote any new joins to left outer joins so that when Q is
         # used as an expression, rows aren't filtered due to joins.
-        clause, joins = query._add_q(self, reuse, allow_joins=allow_joins, split_subq=False)
+        clause, joins = query._add_q(
+            self, reuse, allow_joins=allow_joins, split_subq=False,
+            check_filterable=False,
+        )
         query.promote_joins(joins)
         return clause

@@ -98,14 +110,10 @@
         path = '%s.%s' % (self.__class__.__module__, self.__class__.__name__)
         if path.startswith('django.db.models.query_utils'):
             path = path.replace('django.db.models.query_utils', 'django.db.models')
-        args, kwargs = (), {}
-        if len(self.children) == 1 and not isinstance(self.children[0], Q):
-            child = self.children[0]
-            kwargs = {child[0]: child[1]}
-        else:
-            args = tuple(self.children)
-            if self.connector != self.default:
-                kwargs = {'_connector': self.connector}
+        args = tuple(self.children)
+        kwargs = {}
+        if self.connector != self.default:
+            kwargs['_connector'] = self.connector
         if self.negated:
             kwargs['_negated'] = True
         return path, args, kwargs
@@ -116,8 +124,8 @@
     A wrapper for a deferred-loading field. When the value is read from this
     object the first time, the query is executed.
     """
-    def __init__(self, field_name):
-        self.field_name = field_name
+    def __init__(self, field):
+        self.field = field

     def __get__(self, instance, cls=None):
         """
@@ -127,26 +135,26 @@
         if instance is None:
             return self
         data = instance.__dict__
-        if data.get(self.field_name, self) is self:
+        field_name = self.field.attname
+        if field_name not in data:
             # Let's see if the field is part of the parent chain. If so we
             # might be able to reuse the already loaded value. Refs #18343.
-            val = self._check_parent_chain(instance, self.field_name)
+            val = self._check_parent_chain(instance)
             if val is None:
-                instance.refresh_from_db(fields=[self.field_name])
-                val = getattr(instance, self.field_name)
-            data[self.field_name] = val
-        return data[self.field_name]
-
-    def _check_parent_chain(self, instance, name):
+                instance.refresh_from_db(fields=[field_name])
+            else:
+                data[field_name] = val
+        return data[field_name]
+
+    def _check_parent_chain(self, instance):
         """
         Check if the field value can be fetched from a parent field already
         loaded in the instance. This can be done if the to-be fetched
         field is a primary key field.
         """
         opts = instance._meta
-        f = opts.get_field(name)
-        link_field = opts.get_ancestor_link(f.model)
-        if f.primary_key and f != link_field:
+        link_field = opts.get_ancestor_link(self.field.model)
+        if self.field.primary_key and self.field != link_field:
             return getattr(instance, link_field.attname)
         return None

@@ -247,10 +255,11 @@
     if load_fields:
         if field.attname not in load_fields:
             if restricted and field.name in requested:
-                raise InvalidQuery("Field %s.%s cannot be both deferred"
-                                   " and traversed using select_related"
-                                   " at the same time." %
-                                   (field.model._meta.object_name, field.name))
+                msg = (
+                    'Field %s.%s cannot be both deferred and traversed using '
+                    'select_related at the same time.'
+                ) % (field.model._meta.object_name, field.name)
+                raise FieldError(msg)
     return True


@@ -283,7 +292,7 @@
     # If the field is a primary key, then doing a query against the field's
     # model is ok, too. Consider the case:
     # class Restaurant(models.Model):
-    #     place = OnetoOneField(Place, primary_key=True):
+    #     place = OneToOneField(Place, primary_key=True):
     # Restaurant.objects.filter(pk__in=Restaurant.objects.all()).
     # If we didn't have the primary key check, then pk__in (== place__in) would
     # give Place's opts as the target opts, but Restaurant isn't compatible
@@ -309,8 +318,9 @@
         self.path = []

     def __eq__(self, other):
+        if not isinstance(other, self.__class__):
+            return NotImplemented
         return (
-            isinstance(other, self.__class__) and
             self.relation_name == other.relation_name and
             self.alias == other.alias and
             self.condition == other.condition
('django/db/models', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,6 +4,7 @@
 from functools import partialmethod
 from itertools import chain

+import django
 from django.apps import apps
 from django.conf import settings
 from django.core import checks
@@ -15,23 +16,27 @@
     DEFAULT_DB_ALIAS, DJANGO_VERSION_PICKLE_KEY, DatabaseError, connection,
     connections, router, transaction,
 )
+from django.db.models import (
+    NOT_PROVIDED, ExpressionWrapper, IntegerField, Max, Value,
+)
 from django.db.models.constants import LOOKUP_SEP
 from django.db.models.constraints import CheckConstraint, UniqueConstraint
 from django.db.models.deletion import CASCADE, Collector
 from django.db.models.fields.related import (
     ForeignObjectRel, OneToOneField, lazy_related_operation, resolve_relation,
 )
+from django.db.models.functions import Coalesce
 from django.db.models.manager import Manager
 from django.db.models.options import Options
-from django.db.models.query import Q
+from django.db.models.query import F, Q
 from django.db.models.signals import (
     class_prepared, post_init, post_save, pre_init, pre_save,
 )
 from django.db.models.utils import make_model_tuple
 from django.utils.encoding import force_str
+from django.utils.hashable import make_hashable
 from django.utils.text import capfirst, get_text_list
 from django.utils.translation import gettext_lazy as _
-from django.utils.version import get_version


 class Deferred:
@@ -86,7 +91,7 @@
         # method to type.__new__() so that they're properly initialized
         # (i.e. __set_name__()).
         contributable_attrs = {}
-        for obj_name, obj in list(attrs.items()):
+        for obj_name, obj in attrs.items():
             if _has_contribute_to_class(obj):
                 contributable_attrs[obj_name] = obj
             else:
@@ -197,7 +202,7 @@
                 continue
             # Locate OneToOneField instances.
             for field in base._meta.local_fields:
-                if isinstance(field, OneToOneField):
+                if isinstance(field, OneToOneField) and field.remote_field.parent_link:
                     related = resolve_relation(new_class, field.remote_field.model)
                     parent_links[make_model_tuple(related)] = field

@@ -404,6 +409,8 @@
         opts = self._meta
         _setattr = setattr
         _DEFERRED = DEFERRED
+        if opts.abstract:
+            raise TypeError('Abstract models cannot be instantiated.')

         pre_init.send(sender=cls, args=args, kwargs=kwargs)

@@ -457,11 +464,6 @@
                             val = kwargs.pop(field.attname)
                         except KeyError:
                             val = field.get_default()
-                    else:
-                        # Object instance was passed in. Special case: You can
-                        # pass in "None" for related objects if it's allowed.
-                        if rel_obj is None and field.null:
-                            val = None
                 else:
                     try:
                         val = kwargs.pop(field.attname)
@@ -523,7 +525,7 @@

     def __eq__(self, other):
         if not isinstance(other, Model):
-            return False
+            return NotImplemented
         if self._meta.concrete_model != other._meta.concrete_model:
             return False
         my_pk = self.pk
@@ -538,30 +540,47 @@

     def __reduce__(self):
         data = self.__getstate__()
-        data[DJANGO_VERSION_PICKLE_KEY] = get_version()
+        data[DJANGO_VERSION_PICKLE_KEY] = django.__version__
         class_id = self._meta.app_label, self._meta.object_name
         return model_unpickle, (class_id,), data

     def __getstate__(self):
         """Hook to allow choosing the attributes to pickle."""
-        return self.__dict__
+        state = self.__dict__.copy()
+        state['_state'] = copy.copy(state['_state'])
+        state['_state'].fields_cache = state['_state'].fields_cache.copy()
+        # memoryview cannot be pickled, so cast it to bytes and store
+        # separately.
+        _memoryview_attrs = []
+        for attr, value in state.items():
+            if isinstance(value, memoryview):
+                _memoryview_attrs.append((attr, bytes(value)))
+        if _memoryview_attrs:
+            state['_memoryview_attrs'] = _memoryview_attrs
+            for attr, value in _memoryview_attrs:
+                state.pop(attr)
+        return state

     def __setstate__(self, state):
-        msg = None
         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
         if pickled_version:
-            current_version = get_version()
-            if current_version != pickled_version:
-                msg = (
-                    "Pickled model instance's Django version %s does not match "
-                    "the current version %s." % (pickled_version, current_version)
+            if pickled_version != django.__version__:
+                warnings.warn(
+                    "Pickled model instance's Django version %s does not "
+                    "match the current version %s."
+                    % (pickled_version, django.__version__),
+                    RuntimeWarning,
+                    stacklevel=2,
                 )
         else:
-            msg = "Pickled model instance's Django version is not specified."
-
-        if msg:
-            warnings.warn(msg, RuntimeWarning, stacklevel=2)
-
+            warnings.warn(
+                "Pickled model instance's Django version is not specified.",
+                RuntimeWarning,
+                stacklevel=2,
+            )
+        if '_memoryview_attrs' in state:
+            for attr, value in state.pop('_memoryview_attrs'):
+                state[attr] = memoryview(value)
         self.__dict__.update(state)

     def _get_pk_val(self, meta=None):
@@ -569,6 +588,9 @@
         return getattr(self, meta.pk.attname)

     def _set_pk_val(self, value):
+        for parent_link in self._meta.parents.values():
+            if parent_link and parent_link != self._meta.pk:
+                setattr(self, parent_link.target_field.attname, value)
         return setattr(self, self._meta.pk.attname, value)

     pk = property(_get_pk_val, _set_pk_val)
@@ -670,32 +692,7 @@
         that the "save" must be an SQL insert or update (or equivalent for
         non-SQL backends), respectively. Normally, they should not be set.
         """
-        # Ensure that a model instance without a PK hasn't been assigned to
-        # a ForeignKey or OneToOneField on this model. If the field is
-        # nullable, allowing the save() would result in silent data loss.
-        for field in self._meta.concrete_fields:
-            # If the related field isn't cached, then an instance hasn't
-            # been assigned and there's no need to worry about this check.
-            if field.is_relation and field.is_cached(self):
-                obj = getattr(self, field.name, None)
-                # A pk may have been assigned manually to a model instance not
-                # saved to the database (or auto-generated in a case like
-                # UUIDField), but we allow the save to proceed and rely on the
-                # database to raise an IntegrityError if applicable. If
-                # constraints aren't supported by the database, there's the
-                # unavoidable risk of data corruption.
-                if obj and obj.pk is None:
-                    # Remove the object from a related instance cache.
-                    if not field.remote_field.multiple:
-                        field.remote_field.delete_cached_value(obj)
-                    raise ValueError(
-                        "save() prohibited to prevent data loss due to "
-                        "unsaved related object '%s'." % field.name
-                    )
-                # If the relationship's pk/to_field was changed, clear the
-                # cached relationship.
-                if obj and getattr(obj, field.target_field.attname) != getattr(self, field.attname):
-                    field.delete_cached_value(self)
+        self._prepare_related_fields_for_save(operation_name='save')

         using = using or router.db_for_write(self.__class__, instance=self)
         if force_insert and (force_update or update_fields):
@@ -712,7 +709,7 @@
             update_fields = frozenset(update_fields)
             field_names = set()

-            for field in self._meta.fields:
+            for field in self._meta.concrete_fields:
                 if not field.primary_key:
                     field_names.add(field.name)

@@ -722,12 +719,14 @@
             non_model_fields = update_fields.difference(field_names)

             if non_model_fields:
-                raise ValueError("The following fields do not exist in this "
-                                 "model or are m2m fields: %s"
-                                 % ', '.join(non_model_fields))
+                raise ValueError(
+                    'The following fields do not exist in this model, are m2m '
+                    'fields, or are non-concrete fields: %s'
+                    % ', '.join(non_model_fields)
+                )

         # If saving to the same database, and this model is deferred, then
-        # automatically do a "update_fields" save on the loaded fields.
+        # automatically do an "update_fields" save on the loaded fields.
         elif not force_insert and deferred_fields and using == self._state.db:
             field_names = set()
             for field in self._meta.concrete_fields:
@@ -841,6 +840,15 @@
         if not pk_set and (force_update or update_fields):
             raise ValueError("Cannot force an update in save() with no primary key.")
         updated = False
+        # Skip an UPDATE when adding an instance and primary key has a default.
+        if (
+            not raw and
+            not force_insert and
+            self._state.adding and
+            meta.pk.default and
+            meta.pk.default is not NOT_PROVIDED
+        ):
+            force_insert = True
         # If possible, try an UPDATE. If that doesn't update anything, do an INSERT.
         if pk_set and not force_insert:
             base_qs = cls._base_manager.using(using)
@@ -859,17 +867,21 @@
                 # autopopulate the _order field
                 field = meta.order_with_respect_to
                 filter_args = field.get_filter_kwargs_for_object(self)
-                order_value = cls._base_manager.using(using).filter(**filter_args).count()
-                self._order = order_value
-
+                self._order = cls._base_manager.using(using).filter(**filter_args).aggregate(
+                    _order__max=Coalesce(
+                        ExpressionWrapper(Max('_order') + Value(1), output_field=IntegerField()),
+                        Value(0),
+                    ),
+                )['_order__max']
             fields = meta.local_concrete_fields
             if not pk_set:
                 fields = [f for f in fields if f is not meta.auto_field]

-            update_pk = meta.auto_field and not pk_set
-            result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
-            if update_pk:
-                setattr(self, meta.pk.attname, result)
+            returning_fields = meta.db_returning_fields
+            results = self._do_insert(cls._base_manager, using, fields, returning_fields, raw)
+            if results:
+                for value, field in zip(results[0], returning_fields):
+                    setattr(self, field.attname, value)
         return updated

     def _do_update(self, base_qs, using, pk_val, values, update_fields, forced_update):
@@ -899,13 +911,49 @@
             )
         return filtered._update(values) > 0

-    def _do_insert(self, manager, using, fields, update_pk, raw):
-        """
-        Do an INSERT. If update_pk is defined then this method should return
-        the new pk for the model.
-        """
-        return manager._insert([self], fields=fields, return_id=update_pk,
-                               using=using, raw=raw)
+    def _do_insert(self, manager, using, fields, returning_fields, raw):
+        """
+        Do an INSERT. If returning_fields is defined then this method should
+        return the newly created data for the model.
+        """
+        return manager._insert(
+            [self], fields=fields, returning_fields=returning_fields,
+            using=using, raw=raw,
+        )
+
+    def _prepare_related_fields_for_save(self, operation_name):
+        # Ensure that a model instance without a PK hasn't been assigned to
+        # a ForeignKey or OneToOneField on this model. If the field is
+        # nullable, allowing the save would result in silent data loss.
+        for field in self._meta.concrete_fields:
+            # If the related field isn't cached, then an instance hasn't been
+            # assigned and there's no need to worry about this check.
+            if field.is_relation and field.is_cached(self):
+                obj = getattr(self, field.name, None)
+                if not obj:
+                    continue
+                # A pk may have been assigned manually to a model instance not
+                # saved to the database (or auto-generated in a case like
+                # UUIDField), but we allow the save to proceed and rely on the
+                # database to raise an IntegrityError if applicable. If
+                # constraints aren't supported by the database, there's the
+                # unavoidable risk of data corruption.
+                if obj.pk is None:
+                    # Remove the object from a related instance cache.
+                    if not field.remote_field.multiple:
+                        field.remote_field.delete_cached_value(obj)
+                    raise ValueError(
+                        "%s() prohibited to prevent data loss due to unsaved "
+                        "related object '%s'." % (operation_name, field.name)
+                    )
+                elif getattr(self, field.attname) in field.empty_values:
+                    # Use pk from related object if it has been saved after
+                    # an assignment.
+                    setattr(self, field.attname, obj.pk)
+                # If the relationship's pk/to_field was changed, clear the
+                # cached relationship.
+                if getattr(obj, field.target_field.attname) != getattr(self, field.attname):
+                    field.delete_cached_value(self)

     def delete(self, using=None, keep_parents=False):
         using = using or router.db_for_write(self.__class__, instance=self)
@@ -922,8 +970,9 @@

     def _get_FIELD_display(self, field):
         value = getattr(self, field.attname)
+        choices_dict = dict(make_hashable(field.flatchoices))
         # force_str() to coerce lazy strings.
-        return force_str(dict(field.flatchoices).get(value, value), strings_only=True)
+        return force_str(choices_dict.get(make_hashable(value), value), strings_only=True)

     def _get_next_or_previous_by_FIELD(self, field, is_next, **kwargs):
         if not self.pk:
@@ -999,12 +1048,14 @@
         unique_checks = []

         unique_togethers = [(self.__class__, self._meta.unique_together)]
-        constraints = [(self.__class__, self._meta.constraints)]
+        constraints = [(self.__class__, self._meta.total_unique_constraints)]
         for parent_class in self._meta.get_parent_list():
             if parent_class._meta.unique_together:
                 unique_togethers.append((parent_class, parent_class._meta.unique_together))
-            if parent_class._meta.constraints:
-                constraints.append((parent_class, parent_class._meta.constraints))
+            if parent_class._meta.total_unique_constraints:
+                constraints.append(
+                    (parent_class, parent_class._meta.total_unique_constraints)
+                )

         for model_class, unique_together in unique_togethers:
             for check in unique_together:
@@ -1014,10 +1065,7 @@

         for model_class, model_constraints in constraints:
             for constraint in model_constraints:
-                if (isinstance(constraint, UniqueConstraint) and
-                        # Partial unique constraints can't be validated.
-                        constraint.condition is None and
-                        not any(name in exclude for name in constraint.fields)):
+                if not any(name in exclude for name in constraint.fields):
                     unique_checks.append((model_class, constraint.fields))

         # These are checks for the unique_for_<date/year/month>.
@@ -1231,10 +1279,11 @@
     def check(cls, **kwargs):
         errors = [*cls._check_swappable(), *cls._check_model(), *cls._check_managers(**kwargs)]
         if not cls._meta.swapped:
+            databases = kwargs.get('databases') or []
             errors += [
                 *cls._check_fields(**kwargs),
                 *cls._check_m2m_through_same_relationship(),
-                *cls._check_long_column_names(),
+                *cls._check_long_column_names(databases),
             ]
             clash_errors = (
                 *cls._check_id_field(),
@@ -1251,12 +1300,44 @@
             errors += [
                 *cls._check_index_together(),
                 *cls._check_unique_together(),
-                *cls._check_indexes(),
+                *cls._check_indexes(databases),
                 *cls._check_ordering(),
-                *cls._check_constraints(),
+                *cls._check_constraints(databases),
+                *cls._check_default_pk(),
             ]

         return errors
+
+    @classmethod
+    def _check_default_pk(cls):
+        if (
+            not cls._meta.abstract and
+            cls._meta.pk.auto_created and
+            # Inherited PKs are checked in parents models.
+            not (
+                isinstance(cls._meta.pk, OneToOneField) and
+                cls._meta.pk.remote_field.parent_link
+            ) and
+            not settings.is_overridden('DEFAULT_AUTO_FIELD') and
+            cls._meta.app_config and
+            not cls._meta.app_config._is_default_auto_field_overridden
+        ):
+            return [
+                checks.Warning(
+                    f"Auto-created primary key used when not defining a "
+                    f"primary key type, by default "
+                    f"'{settings.DEFAULT_AUTO_FIELD}'.",
+                    hint=(
+                        f"Configure the DEFAULT_AUTO_FIELD setting or the "
+                        f"{cls._meta.app_config.__class__.__qualname__}."
+                        f"default_auto_field attribute to point to a subclass "
+                        f"of AutoField, e.g. 'django.db.models.BigAutoField'."
+                    ),
+                    obj=cls,
+                    id='models.W042',
+                ),
+            ]
+        return []

     @classmethod
     def _check_swappable(cls):
@@ -1560,10 +1641,93 @@
             return errors

     @classmethod
-    def _check_indexes(cls):
-        """Check the fields of indexes."""
+    def _check_indexes(cls, databases):
+        """Check fields, names, and conditions of indexes."""
+        errors = []
+        references = set()
+        for index in cls._meta.indexes:
+            # Index name can't start with an underscore or a number, restricted
+            # for cross-database compatibility with Oracle.
+            if index.name[0] == '_' or index.name[0].isdigit():
+                errors.append(
+                    checks.Error(
+                        "The index name '%s' cannot start with an underscore "
+                        "or a number." % index.name,
+                        obj=cls,
+                        id='models.E033',
+                    ),
+                )
+            if len(index.name) > index.max_name_length:
+                errors.append(
+                    checks.Error(
+                        "The index name '%s' cannot be longer than %d "
+                        "characters." % (index.name, index.max_name_length),
+                        obj=cls,
+                        id='models.E034',
+                    ),
+                )
+            if index.contains_expressions:
+                for expression in index.expressions:
+                    references.update(
+                        ref[0] for ref in cls._get_expr_references(expression)
+                    )
+        for db in databases:
+            if not router.allow_migrate_model(db, cls):
+                continue
+            connection = connections[db]
+            if not (
+                connection.features.supports_partial_indexes or
+                'supports_partial_indexes' in cls._meta.required_db_features
+            ) and any(index.condition is not None for index in cls._meta.indexes):
+                errors.append(
+                    checks.Warning(
+                        '%s does not support indexes with conditions.'
+                        % connection.display_name,
+                        hint=(
+                            "Conditions will be ignored. Silence this warning "
+                            "if you don't care about it."
+                        ),
+                        obj=cls,
+                        id='models.W037',
+                    )
+                )
+            if not (
+                connection.features.supports_covering_indexes or
+                'supports_covering_indexes' in cls._meta.required_db_features
+            ) and any(index.include for index in cls._meta.indexes):
+                errors.append(
+                    checks.Warning(
+                        '%s does not support indexes with non-key columns.'
+                        % connection.display_name,
+                        hint=(
+                            "Non-key columns will be ignored. Silence this "
+                            "warning if you don't care about it."
+                        ),
+                        obj=cls,
+                        id='models.W040',
+                    )
+                )
+            if not (
+                connection.features.supports_expression_indexes or
+                'supports_expression_indexes' in cls._meta.required_db_features
+            ) and any(index.contains_expressions for index in cls._meta.indexes):
+                errors.append(
+                    checks.Warning(
+                        '%s does not support indexes on expressions.'
+                        % connection.display_name,
+                        hint=(
+                            "An index won't be created. Silence this warning "
+                            "if you don't care about it."
+                        ),
+                        obj=cls,
+                        id='models.W043',
+                    )
+                )
         fields = [field for index in cls._meta.indexes for field, _ in index.fields_orders]
-        return cls._check_local_fields(fields, 'indexes')
+        fields += [include for index in cls._meta.indexes for include in index.include]
+        fields += references
+        errors.extend(cls._check_local_fields(fields, 'indexes'))
+        return errors

     @classmethod
     def _check_local_fields(cls, fields, option):
@@ -1571,9 +1735,11 @@

         # In order to avoid hitting the relation tree prematurely, we use our
         # own fields_map instead of using get_field()
-        forward_fields_map = {
-            field.name: field for field in cls._meta._get_fields(reverse=False)
-        }
+        forward_fields_map = {}
+        for field in cls._meta._get_fields(reverse=False):
+            forward_fields_map[field.name] = field
+            if hasattr(field, 'attname'):
+                forward_fields_map[field.attname] = field

         errors = []
         for field_name in fields:
@@ -1649,9 +1815,43 @@
         # Convert "-field" to "field".
         fields = ((f[1:] if f.startswith('-') else f) for f in fields)

-        # Skip ordering in the format field1__field2 (FIXME: checking
-        # this format would be nice, but it's a little fiddly).
-        fields = (f for f in fields if LOOKUP_SEP not in f)
+        # Separate related fields and non-related fields.
+        _fields = []
+        related_fields = []
+        for f in fields:
+            if LOOKUP_SEP in f:
+                related_fields.append(f)
+            else:
+                _fields.append(f)
+        fields = _fields
+
+        # Check related fields.
+        for field in related_fields:
+            _cls = cls
+            fld = None
+            for part in field.split(LOOKUP_SEP):
+                try:
+                    # pk is an alias that won't be found by opts.get_field.
+                    if part == 'pk':
+                        fld = _cls._meta.pk
+                    else:
+                        fld = _cls._meta.get_field(part)
+                    if fld.is_relation:
+                        _cls = fld.get_path_info()[-1].to_opts.model
+                    else:
+                        _cls = None
+                except (FieldDoesNotExist, AttributeError):
+                    if fld is None or (
+                        fld.get_transform(part) is None and fld.get_lookup(part) is None
+                    ):
+                        errors.append(
+                            checks.Error(
+                                "'ordering' refers to the nonexistent field, "
+                                "related field, or lookup '%s'." % field,
+                                obj=cls,
+                                id='models.E015',
+                            )
+                        )

         # Skip ordering on pk. This is always a valid order_by field
         # but is an alias and therefore won't be found by opts.get_field.
@@ -1673,7 +1873,8 @@
         for invalid_field in invalid_fields:
             errors.append(
                 checks.Error(
-                    "'ordering' refers to the nonexistent field '%s'." % invalid_field,
+                    "'ordering' refers to the nonexistent field, related "
+                    "field, or lookup '%s'." % invalid_field,
                     obj=cls,
                     id='models.E015',
                 )
@@ -1681,17 +1882,19 @@
         return errors

     @classmethod
-    def _check_long_column_names(cls):
+    def _check_long_column_names(cls, databases):
         """
         Check that any auto-generated column names are shorter than the limits
         for each database in which the model will be created.
         """
+        if not databases:
+            return []
         errors = []
         allowed_len = None
         db_alias = None

         # Find the minimum max allowed length among all specified db_aliases.
-        for db in settings.DATABASES:
+        for db in databases:
             # skip databases where the model won't be created
             if not router.allow_migrate_model(db, cls):
                 continue
@@ -1754,15 +1957,35 @@
         return errors

     @classmethod
-    def _check_constraints(cls):
+    def _get_expr_references(cls, expr):
+        if isinstance(expr, Q):
+            for child in expr.children:
+                if isinstance(child, tuple):
+                    lookup, value = child
+                    yield tuple(lookup.split(LOOKUP_SEP))
+                    yield from cls._get_expr_references(value)
+                else:
+                    yield from cls._get_expr_references(child)
+        elif isinstance(expr, F):
+            yield tuple(expr.name.split(LOOKUP_SEP))
+        elif hasattr(expr, 'get_source_expressions'):
+            for src_expr in expr.get_source_expressions():
+                yield from cls._get_expr_references(src_expr)
+
+    @classmethod
+    def _check_constraints(cls, databases):
         errors = []
-        for db in settings.DATABASES:
+        for db in databases:
             if not router.allow_migrate_model(db, cls):
                 continue
             connection = connections[db]
-            if connection.features.supports_table_check_constraints:
-                continue
-            if any(isinstance(constraint, CheckConstraint) for constraint in cls._meta.constraints):
+            if not (
+                connection.features.supports_table_check_constraints or
+                'supports_table_check_constraints' in cls._meta.required_db_features
+            ) and any(
+                isinstance(constraint, CheckConstraint)
+                for constraint in cls._meta.constraints
+            ):
                 errors.append(
                     checks.Warning(
                         '%s does not support check constraints.' % connection.display_name,
@@ -1774,6 +1997,114 @@
                         id='models.W027',
                     )
                 )
+            if not (
+                connection.features.supports_partial_indexes or
+                'supports_partial_indexes' in cls._meta.required_db_features
+            ) and any(
+                isinstance(constraint, UniqueConstraint) and constraint.condition is not None
+                for constraint in cls._meta.constraints
+            ):
+                errors.append(
+                    checks.Warning(
+                        '%s does not support unique constraints with '
+                        'conditions.' % connection.display_name,
+                        hint=(
+                            "A constraint won't be created. Silence this "
+                            "warning if you don't care about it."
+                        ),
+                        obj=cls,
+                        id='models.W036',
+                    )
+                )
+            if not (
+                connection.features.supports_deferrable_unique_constraints or
+                'supports_deferrable_unique_constraints' in cls._meta.required_db_features
+            ) and any(
+                isinstance(constraint, UniqueConstraint) and constraint.deferrable is not None
+                for constraint in cls._meta.constraints
+            ):
+                errors.append(
+                    checks.Warning(
+                        '%s does not support deferrable unique constraints.'
+                        % connection.display_name,
+                        hint=(
+                            "A constraint won't be created. Silence this "
+                            "warning if you don't care about it."
+                        ),
+                        obj=cls,
+                        id='models.W038',
+                    )
+                )
+            if not (
+                connection.features.supports_covering_indexes or
+                'supports_covering_indexes' in cls._meta.required_db_features
+            ) and any(
+                isinstance(constraint, UniqueConstraint) and constraint.include
+                for constraint in cls._meta.constraints
+            ):
+                errors.append(
+                    checks.Warning(
+                        '%s does not support unique constraints with non-key '
+                        'columns.' % connection.display_name,
+                        hint=(
+                            "A constraint won't be created. Silence this "
+                            "warning if you don't care about it."
+                        ),
+                        obj=cls,
+                        id='models.W039',
+                    )
+                )
+            fields = set(chain.from_iterable(
+                (*constraint.fields, *constraint.include)
+                for constraint in cls._meta.constraints if isinstance(constraint, UniqueConstraint)
+            ))
+            references = set()
+            for constraint in cls._meta.constraints:
+                if isinstance(constraint, UniqueConstraint):
+                    if (
+                        connection.features.supports_partial_indexes or
+                        'supports_partial_indexes' not in cls._meta.required_db_features
+                    ) and isinstance(constraint.condition, Q):
+                        references.update(cls._get_expr_references(constraint.condition))
+                elif isinstance(constraint, CheckConstraint):
+                    if (
+                        connection.features.supports_table_check_constraints or
+                        'supports_table_check_constraints' not in cls._meta.required_db_features
+                    ) and isinstance(constraint.check, Q):
+                        references.update(cls._get_expr_references(constraint.check))
+            for field_name, *lookups in references:
+                # pk is an alias that won't be found by opts.get_field.
+                if field_name != 'pk':
+                    fields.add(field_name)
+                if not lookups:
+                    # If it has no lookups it cannot result in a JOIN.
+                    continue
+                try:
+                    if field_name == 'pk':
+                        field = cls._meta.pk
+                    else:
+                        field = cls._meta.get_field(field_name)
+                    if not field.is_relation or field.many_to_many or field.one_to_many:
+                        continue
+                except FieldDoesNotExist:
+                    continue
+                # JOIN must happen at the first lookup.
+                first_lookup = lookups[0]
+                if (
+                    hasattr(field, 'get_transform') and
+                    hasattr(field, 'get_lookup') and
+                    field.get_transform(first_lookup) is None and
+                    field.get_lookup(first_lookup) is None
+                ):
+                    errors.append(
+                        checks.Error(
+                            "'constraints' refers to the joined field '%s'."
+                            % LOOKUP_SEP.join([field_name] + lookups),
+                            obj=cls,
+                            id='models.E041',
+                        )
+                    )
+            errors.extend(cls._check_local_fields(fields, 'constraints'))
         return errors


@@ -1788,11 +2119,9 @@
         using = DEFAULT_DB_ALIAS
     order_wrt = ordered_obj._meta.order_with_respect_to
     filter_args = order_wrt.get_forward_related_filter(self)
-    # FIXME: It would be nice if there was an "update many" version of update
-    # for situations like this.
-    with transaction.atomic(using=using, savepoint=False):
-        for i, j in enumerate(id_list):
-            ordered_obj.objects.filter(pk=j, **filter_args).update(_order=i)
+    ordered_obj.objects.db_manager(using).filter(**filter_args).bulk_update([
+        ordered_obj(pk=pk, _order=order) for order, pk in enumerate(id_list)
+    ], ['_order'])


 def method_get_order(self, ordered_obj):
('django/db/models/functions', 'mixins.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -32,7 +32,9 @@
         if self.output_field.get_internal_type() == 'DurationField':
             expression = self.get_source_expressions()[0]
             options = self._get_repr_options()
-            from django.db.backends.oracle.functions import IntervalToSeconds, SecondsToInterval
+            from django.db.backends.oracle.functions import (
+                IntervalToSeconds, SecondsToInterval,
+            )
             return compiler.compile(
                 SecondsToInterval(self.__class__(IntervalToSeconds(expression), **options))
             )
@@ -42,9 +44,9 @@
 class NumericOutputFieldMixin:

     def _resolve_output_field(self):
-        source_expressions = self.get_source_expressions()
-        if any(isinstance(s.output_field, DecimalField) for s in source_expressions):
+        source_fields = self.get_source_fields()
+        if any(isinstance(s, DecimalField) for s in source_fields):
             return DecimalField()
-        if any(isinstance(s.output_field, IntegerField) for s in source_expressions):
+        if any(isinstance(s, IntegerField) for s in source_fields):
             return FloatField()
-        return super()._resolve_output_field() if source_expressions else FloatField()
+        return super()._resolve_output_field() if source_fields else FloatField()
('django/db/models/functions', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,17 +1,21 @@
-from .comparison import Cast, Coalesce, Greatest, Least, NullIf
+from .comparison import (
+    Cast, Coalesce, Collate, Greatest, JSONObject, Least, NullIf,
+)
 from .datetime import (
-    Extract, ExtractDay, ExtractHour, ExtractIsoYear, ExtractMinute,
-    ExtractMonth, ExtractQuarter, ExtractSecond, ExtractWeek, ExtractWeekDay,
-    ExtractYear, Now, Trunc, TruncDate, TruncDay, TruncHour, TruncMinute,
-    TruncMonth, TruncQuarter, TruncSecond, TruncTime, TruncWeek, TruncYear,
+    Extract, ExtractDay, ExtractHour, ExtractIsoWeekDay, ExtractIsoYear,
+    ExtractMinute, ExtractMonth, ExtractQuarter, ExtractSecond, ExtractWeek,
+    ExtractWeekDay, ExtractYear, Now, Trunc, TruncDate, TruncDay, TruncHour,
+    TruncMinute, TruncMonth, TruncQuarter, TruncSecond, TruncTime, TruncWeek,
+    TruncYear,
 )
 from .math import (
     Abs, ACos, ASin, ATan, ATan2, Ceil, Cos, Cot, Degrees, Exp, Floor, Ln, Log,
-    Mod, Pi, Power, Radians, Round, Sin, Sqrt, Tan,
+    Mod, Pi, Power, Radians, Random, Round, Sign, Sin, Sqrt, Tan,
 )
 from .text import (
-    Chr, Concat, ConcatPair, Left, Length, Lower, LPad, LTrim, Ord, Repeat,
-    Replace, Reverse, Right, RPad, RTrim, StrIndex, Substr, Trim, Upper,
+    MD5, SHA1, SHA224, SHA256, SHA384, SHA512, Chr, Concat, ConcatPair, Left,
+    Length, Lower, LPad, LTrim, Ord, Repeat, Replace, Reverse, Right, RPad,
+    RTrim, StrIndex, Substr, Trim, Upper,
 )
 from .window import (
     CumeDist, DenseRank, FirstValue, Lag, LastValue, Lead, NthValue, Ntile,
@@ -20,22 +24,22 @@

 __all__ = [
     # comparison and conversion
-    'Cast', 'Coalesce', 'Greatest', 'Least', 'NullIf',
+    'Cast', 'Coalesce', 'Collate', 'Greatest', 'JSONObject', 'Least', 'NullIf',
     # datetime
     'Extract', 'ExtractDay', 'ExtractHour', 'ExtractMinute', 'ExtractMonth',
-    'ExtractQuarter', 'ExtractSecond', 'ExtractWeek', 'ExtractWeekDay',
-    'ExtractIsoYear', 'ExtractYear', 'Now', 'Trunc', 'TruncDate', 'TruncDay',
-    'TruncHour', 'TruncMinute', 'TruncMonth', 'TruncQuarter', 'TruncSecond',
-    'TruncMinute', 'TruncMonth', 'TruncQuarter', 'TruncSecond', 'TruncTime',
-    'TruncWeek', 'TruncYear',
+    'ExtractQuarter', 'ExtractSecond', 'ExtractWeek', 'ExtractIsoWeekDay',
+    'ExtractWeekDay', 'ExtractIsoYear', 'ExtractYear', 'Now', 'Trunc',
+    'TruncDate', 'TruncDay', 'TruncHour', 'TruncMinute', 'TruncMonth',
+    'TruncQuarter', 'TruncSecond', 'TruncTime', 'TruncWeek', 'TruncYear',
     # math
     'Abs', 'ACos', 'ASin', 'ATan', 'ATan2', 'Ceil', 'Cos', 'Cot', 'Degrees',
-    'Exp', 'Floor', 'Ln', 'Log', 'Mod', 'Pi', 'Power', 'Radians', 'Round',
-    'Sin', 'Sqrt', 'Tan',
+    'Exp', 'Floor', 'Ln', 'Log', 'Mod', 'Pi', 'Power', 'Radians', 'Random',
+    'Round', 'Sign', 'Sin', 'Sqrt', 'Tan',
     # text
-    'Chr', 'Concat', 'ConcatPair', 'Left', 'Length', 'Lower', 'LPad', 'LTrim',
-    'Ord', 'Repeat', 'Replace', 'Reverse', 'Right', 'RPad', 'RTrim',
-    'StrIndex', 'Substr', 'Trim', 'Upper',
+    'MD5', 'SHA1', 'SHA224', 'SHA256', 'SHA384', 'SHA512', 'Chr', 'Concat',
+    'ConcatPair', 'Left', 'Length', 'Lower', 'LPad', 'LTrim', 'Ord', 'Repeat',
+    'Replace', 'Reverse', 'Right', 'RPad', 'RTrim', 'StrIndex', 'Substr',
+    'Trim', 'Upper',
     # window
     'CumeDist', 'DenseRank', 'FirstValue', 'Lag', 'LastValue', 'Lead',
     'NthValue', 'Ntile', 'PercentRank', 'Rank', 'RowNumber',
('django/db/models/functions', 'comparison.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,5 +1,8 @@
 """Database functions that do comparisons or type conversions."""
+from django.db import NotSupportedError
 from django.db.models.expressions import Func, Value
+from django.db.models.fields.json import JSONField
+from django.utils.regex_helper import _lazy_re_compile


 class Cast(Func):
@@ -29,15 +32,22 @@
         return self.as_sql(compiler, connection, **extra_context)

     def as_mysql(self, compiler, connection, **extra_context):
+        template = None
+        output_type = self.output_field.get_internal_type()
         # MySQL doesn't support explicit cast to float.
-        template = '(%(expressions)s + 0.0)' if self.output_field.get_internal_type() == 'FloatField' else None
+        if output_type == 'FloatField':
+            template = '(%(expressions)s + 0.0)'
+        # MariaDB doesn't support explicit cast to JSON.
+        elif output_type == 'JSONField' and connection.mysql_is_mariadb:
+            template = "JSON_EXTRACT(%(expressions)s, '$')"
         return self.as_sql(compiler, connection, template=template, **extra_context)

-    def as_postgresql(self, compiler, connection, **extra_context):
-        # CAST would be valid too, but the :: shortcut syntax is more readable.
-        # 'expressions' is wrapped in parentheses in case it's a complex
-        # expression.
-        return self.as_sql(compiler, connection, template='(%(expressions)s)::%(db_type)s', **extra_context)
+    def as_oracle(self, compiler, connection, **extra_context):
+        if self.output_field.get_internal_type() == 'JSONField':
+            # Oracle doesn't support explicit cast to JSON.
+            template = "JSON_QUERY(%(expressions)s, '$')"
+            return super().as_sql(compiler, connection, template=template, **extra_context)
+        return self.as_sql(compiler, connection, **extra_context)


 class Coalesce(Func):
@@ -61,6 +71,23 @@
         return self.as_sql(compiler, connection, **extra_context)


+class Collate(Func):
+    function = 'COLLATE'
+    template = '%(expressions)s %(function)s %(collation)s'
+    # Inspired from https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS
+    collation_re = _lazy_re_compile(r'^[\w\-]+$')
+
+    def __init__(self, expression, collation):
+        if not (collation and self.collation_re.match(collation)):
+            raise ValueError('Invalid collation name: %r.' % collation)
+        self.collation = collation
+        super().__init__(expression)
+
+    def as_sql(self, compiler, connection, **extra_context):
+        extra_context.setdefault('collation', connection.ops.quote_name(self.collation))
+        return super().as_sql(compiler, connection, **extra_context)
+
+
 class Greatest(Func):
     """
     Return the maximum expression.
@@ -79,6 +106,46 @@
     def as_sqlite(self, compiler, connection, **extra_context):
         """Use the MAX function on SQLite."""
         return super().as_sqlite(compiler, connection, function='MAX', **extra_context)
+
+
+class JSONObject(Func):
+    function = 'JSON_OBJECT'
+    output_field = JSONField()
+
+    def __init__(self, **fields):
+        expressions = []
+        for key, value in fields.items():
+            expressions.extend((Value(key), value))
+        super().__init__(*expressions)
+
+    def as_sql(self, compiler, connection, **extra_context):
+        if not connection.features.has_json_object_function:
+            raise NotSupportedError(
+                'JSONObject() is not supported on this database backend.'
+            )
+        return super().as_sql(compiler, connection, **extra_context)
+
+    def as_postgresql(self, compiler, connection, **extra_context):
+        return self.as_sql(
+            compiler,
+            connection,
+            function='JSONB_BUILD_OBJECT',
+            **extra_context,
+        )
+
+    def as_oracle(self, compiler, connection, **extra_context):
+        class ArgJoiner:
+            def join(self, args):
+                args = [' VALUE '.join(arg) for arg in zip(args[::2], args[1::2])]
+                return ', '.join(args)
+
+        return self.as_sql(
+            compiler,
+            connection,
+            arg_joiner=ArgJoiner(),
+            template='%(function)s(%(expressions)s RETURNING CLOB)',
+            **extra_context,
+        )


 class Least(Func):
('django/db/models/functions', 'text.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,23 +1,42 @@
+from django.db import NotSupportedError
 from django.db.models.expressions import Func, Value
-from django.db.models.fields import IntegerField
+from django.db.models.fields import CharField, IntegerField
 from django.db.models.functions import Coalesce
 from django.db.models.lookups import Transform


-class BytesToCharFieldConversionMixin:
-    """
-    Convert CharField results from bytes to str.
-
-    MySQL returns long data types (bytes) instead of chars when it can't
-    determine the length of the result string. For example:
-        LPAD(column1, CHAR_LENGTH(column2), ' ')
-    returns the LONGTEXT (bytes) instead of VARCHAR.
-    """
-    def convert_value(self, value, expression, connection):
-        if connection.features.db_functions_convert_bytes_to_str:
-            if self.output_field.get_internal_type() == 'CharField' and isinstance(value, bytes):
-                return value.decode()
-        return super().convert_value(value, expression, connection)
+class MySQLSHA2Mixin:
+    def as_mysql(self, compiler, connection, **extra_content):
+        return super().as_sql(
+            compiler,
+            connection,
+            template='SHA2(%%(expressions)s, %s)' % self.function[3:],
+            **extra_content,
+        )
+
+
+class OracleHashMixin:
+    def as_oracle(self, compiler, connection, **extra_context):
+        return super().as_sql(
+            compiler,
+            connection,
+            template=(
+                "LOWER(RAWTOHEX(STANDARD_HASH(UTL_I18N.STRING_TO_RAW("
+                "%(expressions)s, 'AL32UTF8'), '%(function)s')))"
+            ),
+            **extra_context,
+        )
+
+
+class PostgreSQLSHAMixin:
+    def as_postgresql(self, compiler, connection, **extra_content):
+        return super().as_sql(
+            compiler,
+            connection,
+            template="ENCODE(DIGEST(%(expressions)s, '%(function)s'), 'hex')",
+            function=self.function.lower(),
+            **extra_content,
+        )


 class Chr(Transform):
@@ -100,6 +119,7 @@
 class Left(Func):
     function = 'LEFT'
     arity = 2
+    output_field = CharField()

     def __init__(self, expression, length, **extra):
         """
@@ -136,8 +156,9 @@
     lookup_name = 'lower'


-class LPad(BytesToCharFieldConversionMixin, Func):
+class LPad(Func):
     function = 'LPAD'
+    output_field = CharField()

     def __init__(self, expression, length, fill_text=Value(' '), **extra):
         if not hasattr(length, 'resolve_expression') and length is not None and length < 0:
@@ -150,6 +171,11 @@
     lookup_name = 'ltrim'


+class MD5(OracleHashMixin, Transform):
+    function = 'MD5'
+    lookup_name = 'md5'
+
+
 class Ord(Transform):
     function = 'ASCII'
     lookup_name = 'ord'
@@ -162,8 +188,9 @@
         return super().as_sql(compiler, connection, function='UNICODE', **extra_context)


-class Repeat(BytesToCharFieldConversionMixin, Func):
+class Repeat(Func):
     function = 'REPEAT'
+    output_field = CharField()

     def __init__(self, expression, number, **extra):
         if not hasattr(number, 'resolve_expression') and number is not None and number < 0:
@@ -219,6 +246,34 @@
     lookup_name = 'rtrim'


+class SHA1(OracleHashMixin, PostgreSQLSHAMixin, Transform):
+    function = 'SHA1'
+    lookup_name = 'sha1'
+
+
+class SHA224(MySQLSHA2Mixin, PostgreSQLSHAMixin, Transform):
+    function = 'SHA224'
+    lookup_name = 'sha224'
+
+    def as_oracle(self, compiler, connection, **extra_context):
+        raise NotSupportedError('SHA224 is not supported on Oracle.')
+
+
+class SHA256(MySQLSHA2Mixin, OracleHashMixin, PostgreSQLSHAMixin, Transform):
+    function = 'SHA256'
+    lookup_name = 'sha256'
+
+
+class SHA384(MySQLSHA2Mixin, OracleHashMixin, PostgreSQLSHAMixin, Transform):
+    function = 'SHA384'
+    lookup_name = 'sha384'
+
+
+class SHA512(MySQLSHA2Mixin, OracleHashMixin, PostgreSQLSHAMixin, Transform):
+    function = 'SHA512'
+    lookup_name = 'sha512'
+
+
 class StrIndex(Func):
     """
     Return a positive integer corresponding to the 1-indexed position of the
@@ -235,6 +290,7 @@

 class Substr(Func):
     function = 'SUBSTRING'
+    output_field = CharField()

     def __init__(self, expression, pos, length=None, **extra):
         """
('django/db/models/functions', 'math.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -34,12 +34,10 @@
     arity = 2

     def as_sqlite(self, compiler, connection, **extra_context):
-        if not getattr(connection.ops, 'spatialite', False) or not (
-            (4, 3, 0) <= connection.ops.spatial_version < (5, 0, 0)
-        ):
+        if not getattr(connection.ops, 'spatialite', False) or connection.ops.spatial_version >= (5, 0, 0):
             return self.as_sql(compiler, connection)
         # This function is usually ATan2(y, x), returning the inverse tangent
-        # of y / x, but it's ATan2(x, y) on SpatiaLite >= 4.3.0, < 5.0.0.
+        # of y / x, but it's ATan2(x, y) on SpatiaLite < 5.0.0.
         # Cast integers to float to avoid inconsistent/buggy behavior if the
         # arguments are mixed between integer and float or decimal.
         # https://www.gaia-gis.it/fossil/libspatialite/tktview?name=0f72cca3a2
@@ -143,9 +141,31 @@
         )


+class Random(NumericOutputFieldMixin, Func):
+    function = 'RANDOM'
+    arity = 0
+
+    def as_mysql(self, compiler, connection, **extra_context):
+        return super().as_sql(compiler, connection, function='RAND', **extra_context)
+
+    def as_oracle(self, compiler, connection, **extra_context):
+        return super().as_sql(compiler, connection, function='DBMS_RANDOM.VALUE', **extra_context)
+
+    def as_sqlite(self, compiler, connection, **extra_context):
+        return super().as_sql(compiler, connection, function='RAND', **extra_context)
+
+    def get_group_by_cols(self, alias=None):
+        return []
+
+
 class Round(Transform):
     function = 'ROUND'
     lookup_name = 'round'
+
+
+class Sign(Transform):
+    function = 'SIGN'
+    lookup_name = 'sign'


 class Sin(NumericOutputFieldMixin, Transform):
('django/db/models/functions', 'datetime.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -41,11 +41,15 @@
         super().__init__(expression, **extra)

     def as_sql(self, compiler, connection):
+        if not connection.ops.extract_trunc_lookup_pattern.fullmatch(self.lookup_name):
+            raise ValueError("Invalid lookup_name: %s" % self.lookup_name)
         sql, params = compiler.compile(self.lhs)
         lhs_output_field = self.lhs.output_field
         if isinstance(lhs_output_field, DateTimeField):
             tzname = self.get_tzname()
             sql = connection.ops.datetime_extract_sql(self.lookup_name, sql, tzname)
+        elif self.tzinfo is not None:
+            raise ValueError('tzinfo can only be used with DateTimeField.')
         elif isinstance(lhs_output_field, DateField):
             sql = connection.ops.date_extract_sql(self.lookup_name, sql)
         elif isinstance(lhs_output_field, TimeField):
@@ -73,6 +77,14 @@
             raise ValueError(
                 "Cannot extract time component '%s' from DateField '%s'. " % (copy.lookup_name, field.name)
             )
+        if (
+            isinstance(field, DurationField) and
+            copy.lookup_name in ('year', 'iso_year', 'month', 'week', 'week_day', 'iso_week_day', 'quarter')
+        ):
+            raise ValueError(
+                "Cannot extract component '%s' from DurationField '%s'."
+                % (copy.lookup_name, field.name)
+            )
         return copy


@@ -110,6 +122,11 @@
     lookup_name = 'week_day'


+class ExtractIsoWeekDay(Extract):
+    """Return Monday=1 through Sunday=7, based on ISO-8601."""
+    lookup_name = 'iso_week_day'
+
+
 class ExtractQuarter(Extract):
     lookup_name = 'quarter'

@@ -130,6 +147,7 @@
 DateField.register_lookup(ExtractMonth)
 DateField.register_lookup(ExtractDay)
 DateField.register_lookup(ExtractWeekDay)
+DateField.register_lookup(ExtractIsoWeekDay)
 DateField.register_lookup(ExtractWeek)
 DateField.register_lookup(ExtractIsoYear)
 DateField.register_lookup(ExtractQuarter)
@@ -170,19 +188,26 @@
     kind = None
     tzinfo = None

-    def __init__(self, expression, output_field=None, tzinfo=None, **extra):
+    def __init__(self, expression, output_field=None, tzinfo=None, is_dst=None, **extra):
         self.tzinfo = tzinfo
+        self.is_dst = is_dst
         super().__init__(expression, output_field=output_field, **extra)

     def as_sql(self, compiler, connection):
+        if not connection.ops.extract_trunc_lookup_pattern.fullmatch(self.kind):
+            raise ValueError("Invalid kind: %s" % self.kind)
         inner_sql, inner_params = compiler.compile(self.lhs)
+        tzname = None
+        if isinstance(self.lhs.output_field, DateTimeField):
+            tzname = self.get_tzname()
+        elif self.tzinfo is not None:
+            raise ValueError('tzinfo can only be used with DateTimeField.')
         if isinstance(self.output_field, DateTimeField):
-            tzname = self.get_tzname()
             sql = connection.ops.datetime_trunc_sql(self.kind, inner_sql, tzname)
         elif isinstance(self.output_field, DateField):
-            sql = connection.ops.date_trunc_sql(self.kind, inner_sql)
+            sql = connection.ops.date_trunc_sql(self.kind, inner_sql, tzname)
         elif isinstance(self.output_field, TimeField):
-            sql = connection.ops.time_trunc_sql(self.kind, inner_sql)
+            sql = connection.ops.time_trunc_sql(self.kind, inner_sql, tzname)
         else:
             raise ValueError('Trunc only valid on DateField, TimeField, or DateTimeField.')
         return sql, inner_params
@@ -222,7 +247,7 @@
                 pass
             elif value is not None:
                 value = value.replace(tzinfo=None)
-                value = timezone.make_aware(value, self.tzinfo)
+                value = timezone.make_aware(value, self.tzinfo, is_dst=self.is_dst)
             elif not connection.features.has_zoneinfo_database:
                 raise ValueError(
                     'Database returned an invalid datetime value. Are time '
@@ -240,9 +265,12 @@

 class Trunc(TruncBase):

-    def __init__(self, expression, kind, output_field=None, tzinfo=None, **extra):
+    def __init__(self, expression, kind, output_field=None, tzinfo=None, is_dst=None, **extra):
         self.kind = kind
-        super().__init__(expression, output_field=output_field, tzinfo=tzinfo, **extra)
+        super().__init__(
+            expression, output_field=output_field, tzinfo=tzinfo,
+            is_dst=is_dst, **extra
+        )


 class TruncYear(TruncBase):
@@ -274,7 +302,7 @@
     def as_sql(self, compiler, connection):
         # Cast to date rather than truncate to date.
         lhs, lhs_params = compiler.compile(self.lhs)
-        tzname = timezone.get_current_timezone_name() if settings.USE_TZ else None
+        tzname = self.get_tzname()
         sql = connection.ops.datetime_cast_date_sql(lhs, tzname)
         return sql, lhs_params

@@ -287,7 +315,7 @@
     def as_sql(self, compiler, connection):
         # Cast to time rather than truncate to time.
         lhs, lhs_params = compiler.compile(self.lhs)
-        tzname = timezone.get_current_timezone_name() if settings.USE_TZ else None
+        tzname = self.get_tzname()
         sql = connection.ops.datetime_cast_time_sql(lhs, tzname)
         return sql, lhs_params

('django/db/models/fields', 'related_descriptors.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -67,7 +67,16 @@
 from django.db import connections, router, transaction
 from django.db.models import Q, signals
 from django.db.models.query import QuerySet
+from django.db.models.query_utils import DeferredAttribute
+from django.db.models.utils import resolve_callables
 from django.utils.functional import cached_property
+
+
+class ForeignKeyDeferredAttribute(DeferredAttribute):
+    def __set__(self, instance, value):
+        if instance.__dict__.get(self.field.attname) != value and self.field.is_cached(instance):
+            self.field.delete_cached_value(instance)
+        instance.__dict__[self.field.attname] = value


 class ForwardManyToOneDescriptor:
@@ -572,6 +581,7 @@
             queryset._add_hints(instance=self.instance)
             if self._db:
                 queryset = queryset.using(self._db)
+            queryset._defer_next_filter = True
             queryset = queryset.filter(**self.core_filters)
             for field in self.field.foreign_related_fields:
                 val = getattr(self.instance, field.attname)
@@ -632,7 +642,6 @@

         def add(self, *objs, bulk=True):
             self._remove_prefetched_objects()
-            objs = list(objs)
             db = router.db_for_write(self.model, instance=self.instance)

             def check_and_update_obj(obj):
@@ -688,6 +697,10 @@
                 val = self.field.get_foreign_related_value(self.instance)
                 old_ids = set()
                 for obj in objs:
+                    if not isinstance(obj, self.model):
+                        raise TypeError("'%s' instance expected, got %r" % (
+                            self.model._meta.object_name, obj,
+                        ))
                     # Is obj actually part of this descriptor set?
                     if self.field.get_local_related_value(obj) == val:
                         old_ids.add(obj.pk)
@@ -875,6 +888,7 @@
             queryset._add_hints(instance=self.instance)
             if self._db:
                 queryset = queryset.using(self._db)
+            queryset._defer_next_filter = True
             return queryset._next_is_sticky().filter(**self.core_filters)

         def _remove_prefetched_objects(self):
@@ -929,33 +943,6 @@
                 False,
             )

-        @property
-        def constrained_target(self):
-            # If the through relation's target field's foreign integrity is
-            # enforced, the query can be performed solely against the through
-            # table as the INNER JOIN'ing against target table is unnecessary.
-            if not self.target_field.db_constraint:
-                return None
-            db = router.db_for_read(self.through, instance=self.instance)
-            if not connections[db].features.supports_foreign_keys:
-                return None
-            hints = {'instance': self.instance}
-            manager = self.through._base_manager.db_manager(db, hints=hints)
-            filters = {self.source_field_name: self.instance.pk}
-            # Nullable target rows must be excluded as well as they would have
-            # been filtered out from an INNER JOIN.
-            if self.target_field.null:
-                filters['%s__isnull' % self.target_field_name] = False
-            return manager.filter(**filters)
-
-        def exists(self):
-            constrained_target = self.constrained_target
-            return constrained_target.exists() if constrained_target else super().exists()
-
-        def count(self):
-            constrained_target = self.constrained_target
-            return constrained_target.count() if constrained_target else super().count()
-
         def add(self, *objs, through_defaults=None):
             self._remove_prefetched_objects()
             db = router.db_for_write(self.through, instance=self.instance)
@@ -965,11 +952,14 @@
                     through_defaults=through_defaults,
                 )
                 # If this is a symmetrical m2m relation to self, add the mirror
-                # entry in the m2m table. `through_defaults` aren't used here
-                # because of the system check error fields.E332: Many-to-many
-                # fields with intermediate tables must not be symmetrical.
+                # entry in the m2m table.
                 if self.symmetrical:
-                    self._add_items(self.target_field_name, self.source_field_name, *objs)
+                    self._add_items(
+                        self.target_field_name,
+                        self.source_field_name,
+                        *objs,
+                        through_defaults=through_defaults,
+                    )
         add.alters_data = True

         def remove(self, *objs):
@@ -1013,7 +1003,8 @@
                     for obj in objs:
                         fk_val = (
                             self.target_field.get_foreign_related_value(obj)[0]
-                            if isinstance(obj, self.model) else obj
+                            if isinstance(obj, self.model)
+                            else self.target_field.get_prep_value(obj)
                         )
                         if fk_val in old_ids:
                             old_ids.remove(fk_val)
@@ -1051,80 +1042,131 @@
             return obj, created
         update_or_create.alters_data = True

+        def _get_target_ids(self, target_field_name, objs):
+            """
+            Return the set of ids of `objs` that the target field references.
+            """
+            from django.db.models import Model
+            target_ids = set()
+            target_field = self.through._meta.get_field(target_field_name)
+            for obj in objs:
+                if isinstance(obj, self.model):
+                    if not router.allow_relation(obj, self.instance):
+                        raise ValueError(
+                            'Cannot add "%r": instance is on database "%s", '
+                            'value is on database "%s"' %
+                            (obj, self.instance._state.db, obj._state.db)
+                        )
+                    target_id = target_field.get_foreign_related_value(obj)[0]
+                    if target_id is None:
+                        raise ValueError(
+                            'Cannot add "%r": the value for field "%s" is None' %
+                            (obj, target_field_name)
+                        )
+                    target_ids.add(target_id)
+                elif isinstance(obj, Model):
+                    raise TypeError(
+                        "'%s' instance expected, got %r" %
+                        (self.model._meta.object_name, obj)
+                    )
+                else:
+                    target_ids.add(target_field.get_prep_value(obj))
+            return target_ids
+
+        def _get_missing_target_ids(self, source_field_name, target_field_name, db, target_ids):
+            """
+            Return the subset of ids of `objs` that aren't already assigned to
+            this relationship.
+            """
+            vals = self.through._default_manager.using(db).values_list(
+                target_field_name, flat=True
+            ).filter(**{
+                source_field_name: self.related_val[0],
+                '%s__in' % target_field_name: target_ids,
+            })
+            return target_ids.difference(vals)
+
+        def _get_add_plan(self, db, source_field_name):
+            """
+            Return a boolean triple of the way the add should be performed.
+
+            The first element is whether or not bulk_create(ignore_conflicts)
+            can be used, the second whether or not signals must be sent, and
+            the third element is whether or not the immediate bulk insertion
+            with conflicts ignored can be performed.
+            """
+            # Conflicts can be ignored when the intermediary model is
+            # auto-created as the only possible collision is on the
+            # (source_id, target_id) tuple. The same assertion doesn't hold for
+            # user-defined intermediary models as they could have other fields
+            # causing conflicts which must be surfaced.
+            can_ignore_conflicts = (
+                connections[db].features.supports_ignore_conflicts and
+                self.through._meta.auto_created is not False
+            )
+            # Don't send the signal when inserting duplicate data row
+            # for symmetrical reverse entries.
+            must_send_signals = (self.reverse or source_field_name == self.source_field_name) and (
+                signals.m2m_changed.has_listeners(self.through)
+            )
+            # Fast addition through bulk insertion can only be performed
+            # if no m2m_changed listeners are connected for self.through
+            # as they require the added set of ids to be provided via
+            # pk_set.
+            return can_ignore_conflicts, must_send_signals, (can_ignore_conflicts and not must_send_signals)
+
         def _add_items(self, source_field_name, target_field_name, *objs, through_defaults=None):
             # source_field_name: the PK fieldname in join table for the source object
             # target_field_name: the PK fieldname in join table for the target object
             # *objs - objects to add. Either object instances, or primary keys of object instances.
-            through_defaults = through_defaults or {}
-
-            # If there aren't any objects, there is nothing to do.
-            from django.db.models import Model
-            if objs:
-                new_ids = set()
-                for obj in objs:
-                    if isinstance(obj, self.model):
-                        if not router.allow_relation(obj, self.instance):
-                            raise ValueError(
-                                'Cannot add "%r": instance is on database "%s", value is on database "%s"' %
-                                (obj, self.instance._state.db, obj._state.db)
-                            )
-                        fk_val = self.through._meta.get_field(
-                            target_field_name).get_foreign_related_value(obj)[0]
-                        if fk_val is None:
-                            raise ValueError(
-                                'Cannot add "%r": the value for field "%s" is None' %
-                                (obj, target_field_name)
-                            )
-                        new_ids.add(fk_val)
-                    elif isinstance(obj, Model):
-                        raise TypeError(
-                            "'%s' instance expected, got %r" %
-                            (self.model._meta.object_name, obj)
-                        )
-                    else:
-                        new_ids.add(obj)
-
-                db = router.db_for_write(self.through, instance=self.instance)
-                vals = (self.through._default_manager.using(db)
-                        .values_list(target_field_name, flat=True)
-                        .filter(**{
-                            source_field_name: self.related_val[0],
-                            '%s__in' % target_field_name: new_ids,
-                        }))
-                new_ids.difference_update(vals)
-
-                with transaction.atomic(using=db, savepoint=False):
-                    if self.reverse or source_field_name == self.source_field_name:
-                        # Don't send the signal when we are inserting the
-                        # duplicate data row for symmetrical reverse entries.
-                        signals.m2m_changed.send(
-                            sender=self.through, action='pre_add',
-                            instance=self.instance, reverse=self.reverse,
-                            model=self.model, pk_set=new_ids, using=db,
-                        )
-
-                    # Add the ones that aren't there already
-                    self.through._default_manager.using(db).bulk_create([
-                        self.through(**through_defaults, **{
-                            '%s_id' % source_field_name: self.related_val[0],
-                            '%s_id' % target_field_name: obj_id,
-                        })
-                        for obj_id in new_ids
-                    ])
-
-                    if self.reverse or source_field_name == self.source_field_name:
-                        # Don't send the signal when we are inserting the
-                        # duplicate data row for symmetrical reverse entries.
-                        signals.m2m_changed.send(
-                            sender=self.through, action='post_add',
-                            instance=self.instance, reverse=self.reverse,
-                            model=self.model, pk_set=new_ids, using=db,
-                        )
+            if not objs:
+                return
+
+            through_defaults = dict(resolve_callables(through_defaults or {}))
+            target_ids = self._get_target_ids(target_field_name, objs)
+            db = router.db_for_write(self.through, instance=self.instance)
+            can_ignore_conflicts, must_send_signals, can_fast_add = self._get_add_plan(db, source_field_name)
+            if can_fast_add:
+                self.through._default_manager.using(db).bulk_create([
+                    self.through(**{
+                        '%s_id' % source_field_name: self.related_val[0],
+                        '%s_id' % target_field_name: target_id,
+                    })
+                    for target_id in target_ids
+                ], ignore_conflicts=True)
+                return
+
+            missing_target_ids = self._get_missing_target_ids(
+                source_field_name, target_field_name, db, target_ids
+            )
+            with transaction.atomic(using=db, savepoint=False):
+                if must_send_signals:
+                    signals.m2m_changed.send(
+                        sender=self.through, action='pre_add',
+                        instance=self.instance, reverse=self.reverse,
+                        model=self.model, pk_set=missing_target_ids, using=db,
+                    )
+                # Add the ones that aren't there already.
+                self.through._default_manager.using(db).bulk_create([
+                    self.through(**through_defaults, **{
+                        '%s_id' % source_field_name: self.related_val[0],
+                        '%s_id' % target_field_name: target_id,
+                    })
+                    for target_id in missing_target_ids
+                ], ignore_conflicts=can_ignore_conflicts)
+
+                if must_send_signals:
+                    signals.m2m_changed.send(
+                        sender=self.through, action='post_add',
+                        instance=self.instance, reverse=self.reverse,
+                        model=self.model, pk_set=missing_target_ids, using=db,
+                    )

         def _remove_items(self, source_field_name, target_field_name, *objs):
             # source_field_name: the PK colname in join table for the source object
             # target_field_name: the PK colname in join table for the target object
-            # *objs - objects to remove
+            # *objs - objects to remove. Either object instances, or primary
+            # keys of object instances.
             if not objs:
                 return

('django/db/models/fields', 'files.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -5,9 +5,11 @@
 from django.core import checks
 from django.core.files.base import File
 from django.core.files.images import ImageFile
-from django.core.files.storage import default_storage
+from django.core.files.storage import Storage, default_storage
+from django.core.files.utils import validate_file_name
 from django.db.models import signals
 from django.db.models.fields import Field
+from django.db.models.query_utils import DeferredAttribute
 from django.utils.translation import gettext_lazy as _


@@ -85,7 +87,7 @@
     def save(self, name, content, save=True):
         name = self.field.generate_filename(self.instance, name)
         self.name = self.storage.save(name, content, max_length=self.field.max_length)
-        setattr(self.instance, self.field.name, self.name)
+        setattr(self.instance, self.field.attname, self.name)
         self._committed = True

         # Save the object because it has changed, unless save is False
@@ -105,7 +107,7 @@
         self.storage.delete(self.name)

         self.name = None
-        setattr(self.instance, self.field.name, self.name)
+        setattr(self.instance, self.field.attname, self.name)
         self._committed = False

         if save:
@@ -123,14 +125,24 @@
             file.close()

     def __getstate__(self):
-        # FieldFile needs access to its associated model field and an instance
-        # it's attached to in order to work properly, but the only necessary
-        # data to be pickled is the file's name itself. Everything else will
-        # be restored later, by FileDescriptor below.
-        return {'name': self.name, 'closed': False, '_committed': True, '_file': None}
-
-
-class FileDescriptor:
+        # FieldFile needs access to its associated model field, an instance and
+        # the file's name. Everything else will be restored later, by
+        # FileDescriptor below.
+        return {
+            'name': self.name,
+            'closed': False,
+            '_committed': True,
+            '_file': None,
+            'instance': self.instance,
+            'field': self.field,
+        }
+
+    def __setstate__(self, state):
+        self.__dict__.update(state)
+        self.storage = self.field.storage
+
+
+class FileDescriptor(DeferredAttribute):
     """
     The descriptor for the file attribute on the model instance. Return a
     FieldFile when accessed so you can write code like::
@@ -141,12 +153,9 @@

     Assign a file object on assignment so you can do::

-        >>> with open('/path/to/hello.world', 'r') as f:
+        >>> with open('/path/to/hello.world') as f:
         ...     instance.file = File(f)
     """
-    def __init__(self, field):
-        self.field = field
-
     def __get__(self, instance, cls=None):
         if instance is None:
             return self
@@ -163,11 +172,7 @@

         # The instance dict contains whatever was originally assigned
         # in __set__.
-        if self.field.name in instance.__dict__:
-            file = instance.__dict__[self.field.name]
-        else:
-            instance.refresh_from_db(fields=[self.field.name])
-            file = getattr(instance, self.field.name)
+        file = super().__get__(instance, cls)

         # If this value is a string (instance.file = "path/to/file") or None
         # then we simply wrap it with the appropriate attribute class according
@@ -178,7 +183,7 @@
         # handle None.
         if isinstance(file, str) or file is None:
             attr = self.field.attr_class(instance, self.field, file)
-            instance.__dict__[self.field.name] = attr
+            instance.__dict__[self.field.attname] = attr

         # Other types of files may be assigned as well, but they need to have
         # the FieldFile interface added to them. Thus, we wrap any other type of
@@ -188,7 +193,7 @@
             file_copy = self.field.attr_class(instance, self.field, file.name)
             file_copy.file = file
             file_copy._committed = False
-            instance.__dict__[self.field.name] = file_copy
+            instance.__dict__[self.field.attname] = file_copy

         # Finally, because of the (some would say boneheaded) way pickle works,
         # the underlying FieldFile might not actually itself have an associated
@@ -203,10 +208,10 @@
             file.instance = instance

         # That was fun, wasn't it?
-        return instance.__dict__[self.field.name]
+        return instance.__dict__[self.field.attname]

     def __set__(self, instance, value):
-        instance.__dict__[self.field.name] = value
+        instance.__dict__[self.field.attname] = value


 class FileField(Field):
@@ -224,6 +229,15 @@
         self._primary_key_set_explicitly = 'primary_key' in kwargs

         self.storage = storage or default_storage
+        if callable(self.storage):
+            # Hold a reference to the callable for deconstruct().
+            self._storage_callable = self.storage
+            self.storage = self.storage()
+            if not isinstance(self.storage, Storage):
+                raise TypeError(
+                    "%s.storage must be a subclass/instance of %s.%s"
+                    % (self.__class__.__qualname__, Storage.__module__, Storage.__qualname__)
+                )
         self.upload_to = upload_to

         kwargs.setdefault('max_length', 100)
@@ -268,7 +282,7 @@
             del kwargs["max_length"]
         kwargs['upload_to'] = self.upload_to
         if self.storage is not default_storage:
-            kwargs['storage'] = self.storage
+            kwargs['storage'] = getattr(self, '_storage_callable', self.storage)
         return name, path, args, kwargs

     def get_internal_type(self):
@@ -290,7 +304,7 @@

     def contribute_to_class(self, cls, name, **kwargs):
         super().contribute_to_class(cls, name, **kwargs)
-        setattr(cls, self.name, self.descriptor_class(self))
+        setattr(cls, self.attname, self.descriptor_class(self))

     def generate_filename(self, instance, filename):
         """
@@ -302,8 +316,9 @@
         if callable(self.upload_to):
             filename = self.upload_to(instance, filename)
         else:
-            dirname = datetime.datetime.now().strftime(self.upload_to)
+            dirname = datetime.datetime.now().strftime(str(self.upload_to))
             filename = posixpath.join(dirname, filename)
+        filename = validate_file_name(filename, allow_relative_path=True)
         return self.storage.generate_filename(filename)

     def save_form_data(self, instance, data):
@@ -330,7 +345,7 @@
     assigning the width/height to the width_field/height_field, if appropriate.
     """
     def __set__(self, instance, value):
-        previous_file = instance.__dict__.get(self.field.name)
+        previous_file = instance.__dict__.get(self.field.attname)
         super().__set__(instance, value)

         # To prevent recalculating image dimensions when we are instantiating
@@ -377,7 +392,7 @@
                 checks.Error(
                     'Cannot use ImageField because Pillow is not installed.',
                     hint=('Get Pillow at https://pypi.org/project/Pillow/ '
-                          'or run command "pip install Pillow".'),
+                          'or run command "python -m pip install Pillow".'),
                     obj=self,
                     id='fields.E210',
                 )
('django/db/models/fields', 'mixins.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,3 +1,5 @@
+from django.core import checks
+
 NOT_PROVIDED = object()


@@ -24,3 +26,31 @@

     def delete_cached_value(self, instance):
         del instance._state.fields_cache[self.get_cache_name()]
+
+
+class CheckFieldDefaultMixin:
+    _default_hint = ('<valid default>', '<invalid default>')
+
+    def _check_default(self):
+        if self.has_default() and self.default is not None and not callable(self.default):
+            return [
+                checks.Warning(
+                    "%s default should be a callable instead of an instance "
+                    "so that it's not shared between all field instances." % (
+                        self.__class__.__name__,
+                    ),
+                    hint=(
+                        'Use a callable instead, e.g., use `%s` instead of '
+                        '`%s`.' % self._default_hint
+                    ),
+                    obj=self,
+                    id='fields.E010',
+                )
+            ]
+        else:
+            return []
+
+    def check(self, **kwargs):
+        errors = super().check(**kwargs)
+        errors.extend(self._check_default())
+        return errors
('django/db/models/fields', 'related.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -4,7 +4,7 @@

 from django import forms
 from django.apps import apps
-from django.conf import SettingsReference
+from django.conf import SettingsReference, settings
 from django.core import checks, exceptions
 from django.db import connection, router
 from django.db.backends import utils
@@ -19,9 +19,9 @@
 from . import Field
 from .mixins import FieldCacheMixin
 from .related_descriptors import (
-    ForwardManyToOneDescriptor, ForwardOneToOneDescriptor,
-    ManyToManyDescriptor, ReverseManyToOneDescriptor,
-    ReverseOneToOneDescriptor,
+    ForeignKeyDeferredAttribute, ForwardManyToOneDescriptor,
+    ForwardOneToOneDescriptor, ManyToManyDescriptor,
+    ReverseManyToOneDescriptor, ReverseOneToOneDescriptor,
 )
 from .related_lookups import (
     RelatedExact, RelatedGreaterThan, RelatedGreaterThanOrEqual, RelatedIn,
@@ -133,7 +133,7 @@
             errors.append(
                 checks.Error(
                     "Reverse query name '%s' must not end with an underscore."
-                    % (rel_query_name,),
+                    % rel_query_name,
                     hint=("Add or change a related_name or related_query_name "
                           "argument for this field."),
                     obj=self,
@@ -172,14 +172,10 @@
         if (self.remote_field.model not in self.opts.apps.get_models() and
                 not isinstance(self.remote_field.model, str) and
                 self.remote_field.model._meta.swapped):
-            model = "%s.%s" % (
-                self.remote_field.model._meta.app_label,
-                self.remote_field.model._meta.object_name
-            )
             return [
                 checks.Error(
                     "Field defines a relation with the model '%s', which has "
-                    "been swapped out." % model,
+                    "been swapped out." % self.remote_field.model._meta.label,
                     hint="Update the relation to point at 'settings.%s'." % self.remote_field.model._meta.swappable,
                     obj=self,
                     id='fields.E301',
@@ -218,14 +214,16 @@
         rel_is_hidden = self.remote_field.is_hidden()
         rel_name = self.remote_field.get_accessor_name()  # i. e. "model_set"
         rel_query_name = self.related_query_name()  # i. e. "model"
-        field_name = "%s.%s" % (opts.object_name, self.name)  # i. e. "Model.field"
+        # i.e. "app_label.Model.field".
+        field_name = '%s.%s' % (opts.label, self.name)

         # Check clashes between accessor or reverse query name of `field`
         # and any other field name -- i.e. accessor for Model.foreign is
         # model_set and it clashes with Target.model_set.
         potential_clashes = rel_opts.fields + rel_opts.many_to_many
         for clash_field in potential_clashes:
-            clash_name = "%s.%s" % (rel_opts.object_name, clash_field.name)  # i.e. "Target.model_set"
+            # i.e. "app_label.Target.model_set".
+            clash_name = '%s.%s' % (rel_opts.label, clash_field.name)
             if not rel_is_hidden and clash_field.name == rel_name:
                 errors.append(
                     checks.Error(
@@ -253,9 +251,11 @@
         # Model.m2m accessor.
         potential_clashes = (r for r in rel_opts.related_objects if r.field is not self)
         for clash_field in potential_clashes:
-            clash_name = "%s.%s" % (  # i. e. "Model.m2m"
-                clash_field.related_model._meta.object_name,
-                clash_field.field.name)
+            # i.e. "app_label.Model.m2m".
+            clash_name = '%s.%s' % (
+                clash_field.related_model._meta.label,
+                clash_field.field.name,
+            )
             if not rel_is_hidden and clash_field.get_accessor_name() == rel_name:
                 errors.append(
                     checks.Error(
@@ -528,6 +528,10 @@
             frozenset(ut)
             for ut in self.remote_field.model._meta.unique_together
         })
+        unique_foreign_fields.update({
+            frozenset(uc.fields)
+            for uc in self.remote_field.model._meta.total_unique_constraints
+        })
         foreign_fields = {f.name for f in self.foreign_related_fields}
         has_unique_constraint = any(u <= foreign_fields for u in unique_foreign_fields)

@@ -541,8 +545,10 @@
                     "No subset of the fields %s on model '%s' is unique."
                     % (field_combination, model_name),
                     hint=(
-                        "Add unique=True on any of those fields or add at "
-                        "least a subset of them to a unique_together constraint."
+                        'Mark a single field as unique=True or add a set of '
+                        'fields to a unique constraint (via unique_together '
+                        'or a UniqueConstraint (without condition) in the '
+                        'model Meta.constraints).'
                     ),
                     obj=self,
                     id='fields.E310',
@@ -553,8 +559,13 @@
             model_name = self.remote_field.model.__name__
             return [
                 checks.Error(
-                    "'%s.%s' must set unique=True because it is referenced by "
+                    "'%s.%s' must be unique because it is referenced by "
                     "a foreign key." % (model_name, field_name),
+                    hint=(
+                        'Add unique=True to this field or add a '
+                        'UniqueConstraint (without condition) in the model '
+                        'Meta.constraints.'
+                    ),
                     obj=self,
                     id='fields.E311',
                 )
@@ -570,14 +581,14 @@

         if self.remote_field.parent_link:
             kwargs['parent_link'] = self.remote_field.parent_link
-        # Work out string form of "to"
         if isinstance(self.remote_field.model, str):
-            kwargs['to'] = self.remote_field.model
+            if '.' in self.remote_field.model:
+                app_label, model_name = self.remote_field.model.split('.')
+                kwargs['to'] = '%s.%s' % (app_label, model_name.lower())
+            else:
+                kwargs['to'] = self.remote_field.model.lower()
         else:
-            kwargs['to'] = "%s.%s" % (
-                self.remote_field.model._meta.app_label,
-                self.remote_field.model._meta.object_name,
-            )
+            kwargs['to'] = self.remote_field.model._meta.label_lower
         # If swappable is True, then see if we're actually pointing to the target
         # of a swap.
         swappable_setting = self.swappable_setting
@@ -606,28 +617,29 @@
         for index in range(len(self.from_fields)):
             from_field_name = self.from_fields[index]
             to_field_name = self.to_fields[index]
-            from_field = (self if from_field_name == 'self'
-                          else self.opts.get_field(from_field_name))
+            from_field = (
+                self
+                if from_field_name == RECURSIVE_RELATIONSHIP_CONSTANT
+                else self.opts.get_field(from_field_name)
+            )
             to_field = (self.remote_field.model._meta.pk if to_field_name is None
                         else self.remote_field.model._meta.get_field(to_field_name))
             related_fields.append((from_field, to_field))
         return related_fields

-    @property
+    @cached_property
     def related_fields(self):
-        if not hasattr(self, '_related_fields'):
-            self._related_fields = self.resolve_related_fields()
-        return self._related_fields
-
-    @property
+        return self.resolve_related_fields()
+
+    @cached_property
     def reverse_related_fields(self):
         return [(rhs_field, lhs_field) for lhs_field, rhs_field in self.related_fields]

-    @property
+    @cached_property
     def local_related_fields(self):
         return tuple(lhs_field for lhs_field, rhs_field in self.related_fields)

-    @property
+    @cached_property
     def foreign_related_fields(self):
         return tuple(rhs_field for lhs_field, rhs_field in self.related_fields if rhs_field)

@@ -764,7 +776,7 @@
     By default ForeignKey will target the pk of the remote model but this
     behavior can be changed by using the ``to_field`` argument.
     """
-
+    descriptor_class = ForeignKeyDeferredAttribute
     # Field flags
     many_to_many = False
     many_to_one = True
@@ -797,6 +809,8 @@
             # the to_field during FK construction. It won't be guaranteed to
             # be correct until contribute_to_class is called. Refs #12190.
             to_field = to_field or (to._meta.pk and to._meta.pk.name)
+        if not callable(on_delete):
+            raise TypeError('on_delete must be callable.')

         kwargs['rel'] = self.rel_class(
             self, to, to_field,
@@ -808,8 +822,13 @@
         )
         kwargs.setdefault('db_index', True)

-        super().__init__(to, on_delete, from_fields=['self'], to_fields=[to_field], **kwargs)
-
+        super().__init__(
+            to,
+            on_delete,
+            from_fields=[RECURSIVE_RELATIONSHIP_CONSTANT],
+            to_fields=[to_field],
+            **kwargs,
+        )
         self.db_constraint = db_constraint

     def check(self, **kwargs):
@@ -899,7 +918,7 @@
             return

         using = router.db_for_read(self.remote_field.model, instance=model_instance)
-        qs = self.remote_field.model._default_manager.using(using).filter(
+        qs = self.remote_field.model._base_manager.using(using).filter(
             **{self.remote_field.field_name: value}
         )
         qs = qs.complex_filter(self.get_limit_choices_to())
@@ -913,6 +932,21 @@
                 },  # 'pk' is included for backwards compatibility
             )

+    def resolve_related_fields(self):
+        related_fields = super().resolve_related_fields()
+        for from_field, to_field in related_fields:
+            if to_field and to_field.model != self.remote_field.model._meta.concrete_model:
+                raise exceptions.FieldError(
+                    "'%s.%s' refers to field '%s' which is not local to model "
+                    "'%s'." % (
+                        self.model._meta.label,
+                        self.name,
+                        to_field.name,
+                        self.remote_field.model._meta.concrete_model._meta.label,
+                    )
+                )
+        return related_fields
+
     def get_attname(self):
         return '%s_id' % self.name

@@ -938,6 +972,9 @@

     def get_db_prep_value(self, value, connection, prepared=False):
         return self.target_field.get_db_prep_value(value, connection, prepared)
+
+    def get_prep_value(self, value):
+        return self.target_field.get_prep_value(value)

     def contribute_to_related_class(self, cls, related):
         super().contribute_to_related_class(cls, related)
@@ -954,6 +991,7 @@
             'queryset': self.remote_field.model._default_manager.using(using),
             'to_field_name': self.remote_field.field_name,
             **kwargs,
+            'blank': self.blank,
         })

     def db_check(self, connection):
@@ -1026,6 +1064,10 @@
             setattr(instance, self.name, data)
         else:
             setattr(instance, self.attname, data)
+            # Remote field object must be cleared otherwise Model.save()
+            # will reassign attname using the related object pk.
+            if data is None:
+                setattr(instance, self.name, data)

     def _check_unique(self, **kwargs):
         # Override ForeignKey since check isn't applicable here.
@@ -1225,18 +1267,6 @@
                 to_model_name = to_model._meta.object_name
             relationship_model_name = self.remote_field.through._meta.object_name
             self_referential = from_model == to_model
-
-            # Check symmetrical attribute.
-            if (self_referential and self.remote_field.symmetrical and
-                    not self.remote_field.through._meta.auto_created):
-                errors.append(
-                    checks.Error(
-                        'Many-to-many fields with intermediate tables must not be symmetrical.',
-                        obj=self,
-                        id='fields.E332',
-                    )
-                )
-
             # Count foreign keys in intermediate model
             if self_referential:
                 seen_self = sum(
@@ -1279,8 +1309,11 @@
                              "through_fields keyword argument.") % (self, from_model_name),
                             hint=(
                                 'If you want to create a recursive relationship, '
-                                'use ForeignKey("self", symmetrical=False, through="%s").'
-                            ) % relationship_model_name,
+                                'use ManyToManyField("%s", through="%s").'
+                            ) % (
+                                RECURSIVE_RELATIONSHIP_CONSTANT,
+                                relationship_model_name,
+                            ),
                             obj=self,
                             id='fields.E334',
                         )
@@ -1296,8 +1329,11 @@
                             "through_fields keyword argument." % (self, to_model_name),
                             hint=(
                                 'If you want to create a recursive relationship, '
-                                'use ForeignKey("self", symmetrical=False, through="%s").'
-                            ) % relationship_model_name,
+                                'use ManyToManyField("%s", through="%s").'
+                            ) % (
+                                RECURSIVE_RELATIONSHIP_CONSTANT,
+                                relationship_model_name,
+                            ),
                             obj=self,
                             id='fields.E335',
                         )
@@ -1411,12 +1447,23 @@
                 clashing_obj = '%s.%s' % (opts.label, _get_field_name(model))
             else:
                 clashing_obj = model._meta.label
+            if settings.DATABASE_ROUTERS:
+                error_class, error_id = checks.Warning, 'fields.W344'
+                error_hint = (
+                    'You have configured settings.DATABASE_ROUTERS. Verify '
+                    'that the table of %r is correctly routed to a separate '
+                    'database.' % clashing_obj
+                )
+            else:
+                error_class, error_id = checks.Error, 'fields.E340'
+                error_hint = None
             return [
-                checks.Error(
+                error_class(
                     "The field's intermediary table '%s' clashes with the "
                     "table name of '%s'." % (m2m_db_table, clashing_obj),
                     obj=self,
-                    id='fields.E340',
+                    hint=error_hint,
+                    id=error_id,
                 )
             ]
         return []
@@ -1432,18 +1479,12 @@
         if isinstance(self.remote_field.model, str):
             kwargs['to'] = self.remote_field.model
         else:
-            kwargs['to'] = "%s.%s" % (
-                self.remote_field.model._meta.app_label,
-                self.remote_field.model._meta.object_name,
-            )
+            kwargs['to'] = self.remote_field.model._meta.label
         if getattr(self.remote_field, 'through', None) is not None:
             if isinstance(self.remote_field.through, str):
                 kwargs['through'] = self.remote_field.through
             elif not self.remote_field.through._meta.auto_created:
-                kwargs['through'] = "%s.%s" % (
-                    self.remote_field.through._meta.app_label,
-                    self.remote_field.through._meta.object_name,
-                )
+                kwargs['through'] = self.remote_field.through._meta.label
         # If swappable is True, then see if we're actually pointing to the target
         # of a swap.
         swappable_setting = self.swappable_setting
@@ -1564,14 +1605,20 @@
         # automatically. The funky name reduces the chance of an accidental
         # clash.
         if self.remote_field.symmetrical and (
-                self.remote_field.model == "self" or self.remote_field.model == cls._meta.object_name):
+            self.remote_field.model == RECURSIVE_RELATIONSHIP_CONSTANT or
+            self.remote_field.model == cls._meta.object_name
+        ):
             self.remote_field.related_name = "%s_rel_+" % name
         elif self.remote_field.is_hidden():
             # If the backwards relation is disabled, replace the original
             # related_name with one generated from the m2m field name. Django
             # still uses backwards relations internally and we need to avoid
             # clashes between multiple m2m fields with related_name == '+'.
-            self.remote_field.related_name = "_%s_%s_+" % (cls.__name__.lower(), name)
+            self.remote_field.related_name = '_%s_%s_%s_+' % (
+                cls._meta.app_label,
+                cls.__name__.lower(),
+                name,
+            )

         super().contribute_to_class(cls, name, **kwargs)

('django/db/models/fields', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -12,10 +12,6 @@
 from django.apps import apps
 from django.conf import settings
 from django.core import checks, exceptions, validators
-# When the _meta object was formalized, this exception was moved to
-# django.core.exceptions. It is retained here for backwards compatibility
-# purposes.
-from django.core.exceptions import FieldDoesNotExist  # NOQA
 from django.db import connection, connections, router
 from django.db.models.constants import LOOKUP_SEP
 from django.db.models.query_utils import DeferredAttribute, RegisterLookupMixin
@@ -35,11 +31,11 @@
     'AutoField', 'BLANK_CHOICE_DASH', 'BigAutoField', 'BigIntegerField',
     'BinaryField', 'BooleanField', 'CharField', 'CommaSeparatedIntegerField',
     'DateField', 'DateTimeField', 'DecimalField', 'DurationField',
-    'EmailField', 'Empty', 'Field', 'FieldDoesNotExist', 'FilePathField',
-    'FloatField', 'GenericIPAddressField', 'IPAddressField', 'IntegerField',
-    'NOT_PROVIDED', 'NullBooleanField', 'PositiveIntegerField',
-    'PositiveSmallIntegerField', 'SlugField', 'SmallIntegerField', 'TextField',
-    'TimeField', 'URLField', 'UUIDField',
+    'EmailField', 'Empty', 'Field', 'FilePathField', 'FloatField',
+    'GenericIPAddressField', 'IPAddressField', 'IntegerField', 'NOT_PROVIDED',
+    'NullBooleanField', 'PositiveBigIntegerField', 'PositiveIntegerField',
+    'PositiveSmallIntegerField', 'SlugField', 'SmallAutoField',
+    'SmallIntegerField', 'TextField', 'TimeField', 'URLField', 'UUIDField',
 ]


@@ -122,6 +118,8 @@
     one_to_many = None
     one_to_one = None
     related_model = None
+
+    descriptor_class = DeferredAttribute

     # Generic field type description, usually overridden by subclasses
     def _description(self):
@@ -153,7 +151,7 @@
         self.unique_for_year = unique_for_year
         if isinstance(choices, collections.abc.Iterator):
             choices = list(choices)
-        self.choices = choices or []
+        self.choices = choices
         self.help_text = help_text
         self.db_index = db_index
         self.db_column = db_column
@@ -185,8 +183,7 @@
         if not hasattr(self, 'model'):
             return super().__str__()
         model = self.model
-        app = model._meta.app_label
-        return '%s.%s.%s' % (app, model._meta.object_name, self.name)
+        return '%s.%s' % (model._meta.label, self.name)

     def __repr__(self):
         """Display the module, class, and name of the field."""
@@ -223,7 +220,7 @@
         elif LOOKUP_SEP in self.name:
             return [
                 checks.Error(
-                    'Field names must not contain "%s".' % (LOOKUP_SEP,),
+                    'Field names must not contain "%s".' % LOOKUP_SEP,
                     obj=self,
                     id='fields.E002',
                 )
@@ -239,14 +236,15 @@
         else:
             return []

+    @classmethod
+    def _choices_is_value(cls, value):
+        return isinstance(value, (str, Promise)) or not is_iterable(value)
+
     def _check_choices(self):
         if not self.choices:
             return []

-        def is_value(value, accept_promise=True):
-            return isinstance(value, (str, Promise) if accept_promise else str) or not is_iterable(value)
-
-        if is_value(self.choices, accept_promise=False):
+        if not is_iterable(self.choices) or isinstance(self.choices, str):
             return [
                 checks.Error(
                     "'choices' must be an iterable (e.g., a list or tuple).",
@@ -255,6 +253,7 @@
                 )
             ]

+        choice_max_length = 0
         # Expect [group_name, [value, display]]
         for choices_group in self.choices:
             try:
@@ -264,20 +263,36 @@
                 break
             try:
                 if not all(
-                    is_value(value) and is_value(human_name)
+                    self._choices_is_value(value) and self._choices_is_value(human_name)
                     for value, human_name in group_choices
                 ):
                     break
+                if self.max_length is not None and group_choices:
+                    choice_max_length = max([
+                        choice_max_length,
+                        *(len(value) for value, _ in group_choices if isinstance(value, str)),
+                    ])
             except (TypeError, ValueError):
                 # No groups, choices in the form [value, display]
                 value, human_name = group_name, group_choices
-                if not is_value(value) or not is_value(human_name):
+                if not self._choices_is_value(value) or not self._choices_is_value(human_name):
                     break
+                if self.max_length is not None and isinstance(value, str):
+                    choice_max_length = max(choice_max_length, len(value))

             # Special case: choices=['ab']
             if isinstance(choices_group, str):
                 break
         else:
+            if self.max_length is not None and choice_max_length > self.max_length:
+                return [
+                    checks.Error(
+                        "'max_length' is too small to fit the longest value "
+                        "in 'choices' (%d characters)." % choice_max_length,
+                        obj=self,
+                        id='fields.E009',
+                    ),
+                ]
             return []

         return [
@@ -319,12 +334,15 @@
         else:
             return []

-    def _check_backend_specific_checks(self, **kwargs):
+    def _check_backend_specific_checks(self, databases=None, **kwargs):
+        if databases is None:
+            return []
         app_label = self.model._meta.app_label
-        for db in connections:
-            if router.allow_migrate(db, app_label, model_name=self.model._meta.model_name):
-                return connections[db].validation.check_field(self, **kwargs)
-        return []
+        errors = []
+        for alias in databases:
+            if router.allow_migrate(alias, app_label, model_name=self.model._meta.model_name):
+                errors.extend(connections[alias].validation.check_field(self, **kwargs))
+        return errors

     def _check_validators(self):
         errors = []
@@ -443,7 +461,7 @@
             "unique_for_date": None,
             "unique_for_month": None,
             "unique_for_year": None,
-            "choices": [],
+            "choices": None,
             "help_text": '',
             "db_column": None,
             "db_tablespace": None,
@@ -475,11 +493,13 @@
         path = "%s.%s" % (self.__class__.__module__, self.__class__.__qualname__)
         if path.startswith("django.db.models.fields.related"):
             path = path.replace("django.db.models.fields.related", "django.db.models")
-        if path.startswith("django.db.models.fields.files"):
+        elif path.startswith("django.db.models.fields.files"):
             path = path.replace("django.db.models.fields.files", "django.db.models")
-        if path.startswith("django.db.models.fields.proxy"):
+        elif path.startswith('django.db.models.fields.json'):
+            path = path.replace('django.db.models.fields.json', 'django.db.models')
+        elif path.startswith("django.db.models.fields.proxy"):
             path = path.replace("django.db.models.fields.proxy", "django.db.models")
-        if path.startswith("django.db.models.fields"):
+        elif path.startswith("django.db.models.fields"):
             path = path.replace("django.db.models.fields", "django.db.models")
         # Return basic info - other fields should override this.
         return (self.name, path, [], keywords)
@@ -495,17 +515,37 @@
     def __eq__(self, other):
         # Needed for @total_ordering
         if isinstance(other, Field):
-            return self.creation_counter == other.creation_counter
+            return (
+                self.creation_counter == other.creation_counter and
+                getattr(self, 'model', None) == getattr(other, 'model', None)
+            )
         return NotImplemented

     def __lt__(self, other):
         # This is needed because bisect does not take a comparison function.
+        # Order by creation_counter first for backward compatibility.
         if isinstance(other, Field):
-            return self.creation_counter < other.creation_counter
+            if (
+                self.creation_counter != other.creation_counter or
+                not hasattr(self, 'model') and not hasattr(other, 'model')
+            ):
+                return self.creation_counter < other.creation_counter
+            elif hasattr(self, 'model') != hasattr(other, 'model'):
+                return not hasattr(self, 'model')  # Order no-model fields first
+            else:
+                # creation_counter's are equal, compare only models.
+                return (
+                    (self.model._meta.app_label, self.model._meta.model_name) <
+                    (other.model._meta.app_label, other.model._meta.model_name)
+                )
         return NotImplemented

     def __hash__(self):
-        return hash(self.creation_counter)
+        return hash((
+            self.creation_counter,
+            self.model._meta.app_label if hasattr(self, 'model') else None,
+            self.model._meta.model_name if hasattr(self, 'model') else None,
+        ))

     def __deepcopy__(self, memodict):
         # We don't have to deepcopy very much here, since most things are not
@@ -598,7 +638,7 @@
             # Skip validation for non-editable fields.
             return

-        if self.choices and value not in self.empty_values:
+        if self.choices is not None and value not in self.empty_values:
             for option_key, option_value in self.choices:
                 if isinstance(option_value, (list, tuple)):
                     # This is an optgroup, so look inside the group for
@@ -716,6 +756,14 @@
     def db_tablespace(self):
         return self._db_tablespace or settings.DEFAULT_INDEX_TABLESPACE

+    @property
+    def db_returning(self):
+        """
+        Private API intended only to be used by Django itself. Currently only
+        the PostgreSQL backend supports returning multiple fields on a model.
+        """
+        return False
+
     def set_attributes_from_name(self, name):
         self.name = self.name or name
         self.attname, self.column = self.get_attname_column()
@@ -732,19 +780,24 @@
         """
         self.set_attributes_from_name(name)
         self.model = cls
-        if private_only:
-            cls._meta.add_field(self, private=True)
-        else:
-            cls._meta.add_field(self)
+        cls._meta.add_field(self, private=private_only)
         if self.column:
             # Don't override classmethods with the descriptor. This means that
             # if you have a classmethod and a field with the same name, then
             # such fields can't be deferred (we don't have a check for this).
             if not getattr(cls, self.attname, None):
-                setattr(cls, self.attname, DeferredAttribute(self.attname))
-        if self.choices:
-            setattr(cls, 'get_%s_display' % self.name,
-                    partialmethod(cls._get_FIELD_display, field=self))
+                setattr(cls, self.attname, self.descriptor_class(self))
+        if self.choices is not None:
+            # Don't override a get_FOO_display() method defined explicitly on
+            # this class, but don't check methods derived from inheritance, to
+            # allow overriding inherited choices. For more complex inheritance
+            # structures users should override contribute_to_class().
+            if 'get_%s_display' % self.name not in cls.__dict__:
+                setattr(
+                    cls,
+                    'get_%s_display' % self.name,
+                    partialmethod(cls._get_FIELD_display, field=self),
+                )

     def get_filter_kwargs_for_object(self, obj):
         """
@@ -812,7 +865,7 @@
         Return choices with a default blank choices included, for use
         as <select> choices for this field.
         """
-        if self.choices:
+        if self.choices is not None:
             choices = list(self.choices)
             if include_blank:
                 blank_defined = any(choice in ('', None) for choice, _ in self.flatchoices)
@@ -826,9 +879,11 @@
             if hasattr(self.remote_field, 'get_related_field')
             else 'pk'
         )
+        qs = rel_model._default_manager.complex_filter(limit_choices_to)
+        if ordering:
+            qs = qs.order_by(*ordering)
         return (blank_choice if include_blank else []) + [
-            (choice_func(x), str(x))
-            for x in rel_model._default_manager.complex_filter(limit_choices_to).order_by(*ordering)
+            (choice_func(x), str(x)) for x in qs
         ]

     def value_to_string(self, obj):
@@ -840,6 +895,8 @@

     def _get_flatchoices(self):
         """Flattened version of choices tuple."""
+        if self.choices is None:
+            return []
         flat = []
         for choice, value in self.choices:
             if isinstance(value, (list, tuple)):
@@ -865,7 +922,7 @@
                 defaults['show_hidden_initial'] = True
             else:
                 defaults['initial'] = self.get_default()
-        if self.choices:
+        if self.choices is not None:
             # Fields with choices get special treatment.
             include_blank = (self.blank or
                              not (self.has_default() or 'initial' in kwargs))
@@ -895,100 +952,11 @@
         return getattr(obj, self.attname)


-class AutoField(Field):
-    description = _("Integer")
-
-    empty_strings_allowed = False
-    default_error_messages = {
-        'invalid': _("'%(value)s' value must be an integer."),
-    }
-
-    def __init__(self, *args, **kwargs):
-        kwargs['blank'] = True
-        super().__init__(*args, **kwargs)
-
-    def check(self, **kwargs):
-        return [
-            *super().check(**kwargs),
-            *self._check_primary_key(),
-        ]
-
-    def _check_primary_key(self):
-        if not self.primary_key:
-            return [
-                checks.Error(
-                    'AutoFields must set primary_key=True.',
-                    obj=self,
-                    id='fields.E100',
-                ),
-            ]
-        else:
-            return []
-
-    def deconstruct(self):
-        name, path, args, kwargs = super().deconstruct()
-        del kwargs['blank']
-        kwargs['primary_key'] = True
-        return name, path, args, kwargs
-
-    def get_internal_type(self):
-        return "AutoField"
-
-    def to_python(self, value):
-        if value is None:
-            return value
-        try:
-            return int(value)
-        except (TypeError, ValueError):
-            raise exceptions.ValidationError(
-                self.error_messages['invalid'],
-                code='invalid',
-                params={'value': value},
-            )
-
-    def rel_db_type(self, connection):
-        return IntegerField().db_type(connection=connection)
-
-    def validate(self, value, model_instance):
-        pass
-
-    def get_db_prep_value(self, value, connection, prepared=False):
-        if not prepared:
-            value = self.get_prep_value(value)
-            value = connection.ops.validate_autopk_value(value)
-        return value
-
-    def get_prep_value(self, value):
-        from django.db.models.expressions import OuterRef
-        value = super().get_prep_value(value)
-        if value is None or isinstance(value, OuterRef):
-            return value
-        return int(value)
-
-    def contribute_to_class(self, cls, name, **kwargs):
-        assert not cls._meta.auto_field, "Model %s can't have more than one AutoField." % cls._meta.label
-        super().contribute_to_class(cls, name, **kwargs)
-        cls._meta.auto_field = self
-
-    def formfield(self, **kwargs):
-        return None
-
-
-class BigAutoField(AutoField):
-    description = _("Big (8 byte) integer")
-
-    def get_internal_type(self):
-        return "BigAutoField"
-
-    def rel_db_type(self, connection):
-        return BigIntegerField().db_type(connection=connection)
-
-
 class BooleanField(Field):
     empty_strings_allowed = False
     default_error_messages = {
-        'invalid': _("'%(value)s' value must be either True or False."),
-        'invalid_nullable': _("'%(value)s' value must be either True, False, or None."),
+        'invalid': _('“%(value)s” value must be either True or False.'),
+        'invalid_nullable': _('“%(value)s” value must be either True, False, or None.'),
     }
     description = _("Boolean (Either True or False)")

@@ -1018,7 +986,7 @@
         return self.to_python(value)

     def formfield(self, **kwargs):
-        if self.choices:
+        if self.choices is not None:
             include_blank = not (self.has_default() or 'initial' in kwargs)
             defaults = {'choices': self.get_choices(include_blank=include_blank)}
         else:
@@ -1033,13 +1001,16 @@
 class CharField(Field):
     description = _("String (up to %(max_length)s)")

-    def __init__(self, *args, **kwargs):
+    def __init__(self, *args, db_collation=None, **kwargs):
         super().__init__(*args, **kwargs)
+        self.db_collation = db_collation
         self.validators.append(validators.MaxLengthValidator(self.max_length))

     def check(self, **kwargs):
+        databases = kwargs.get('databases') or []
         return [
             *super().check(**kwargs),
+            *self._check_db_collation(databases),
             *self._check_max_length_attribute(**kwargs),
         ]

@@ -1063,6 +1034,27 @@
             ]
         else:
             return []
+
+    def _check_db_collation(self, databases):
+        errors = []
+        for db in databases:
+            if not router.allow_migrate_model(db, self.model):
+                continue
+            connection = connections[db]
+            if not (
+                self.db_collation is None or
+                'supports_collation_on_charfield' in self.model._meta.required_db_features or
+                connection.features.supports_collation_on_charfield
+            ):
+                errors.append(
+                    checks.Error(
+                        '%s does not support a database collation on '
+                        'CharFields.' % connection.display_name,
+                        obj=self,
+                        id='fields.E190',
+                    ),
+                )
+        return errors

     def cast_db_type(self, connection):
         if self.max_length is None:
@@ -1091,6 +1083,12 @@
             defaults['empty_value'] = None
         defaults.update(kwargs)
         return super().formfield(**defaults)
+
+    def deconstruct(self):
+        name, path, args, kwargs = super().deconstruct()
+        if self.db_collation:
+            kwargs['db_collation'] = self.db_collation
+        return name, path, args, kwargs


 class CommaSeparatedIntegerField(CharField):
@@ -1144,10 +1142,10 @@
 class DateField(DateTimeCheckMixin, Field):
     empty_strings_allowed = False
     default_error_messages = {
-        'invalid': _("'%(value)s' value has an invalid date format. It must be "
-                     "in YYYY-MM-DD format."),
-        'invalid_date': _("'%(value)s' value has the correct format (YYYY-MM-DD) "
-                          "but it is an invalid date."),
+        'invalid': _('“%(value)s” value has an invalid date format. It must be '
+                     'in YYYY-MM-DD format.'),
+        'invalid_date': _('“%(value)s” value has the correct format (YYYY-MM-DD) '
+                          'but it is an invalid date.'),
     }
     description = _("Date (without time)")

@@ -1287,13 +1285,13 @@
 class DateTimeField(DateField):
     empty_strings_allowed = False
     default_error_messages = {
-        'invalid': _("'%(value)s' value has an invalid format. It must be in "
-                     "YYYY-MM-DD HH:MM[:ss[.uuuuuu]][TZ] format."),
-        'invalid_date': _("'%(value)s' value has the correct format "
+        'invalid': _('“%(value)s” value has an invalid format. It must be in '
+                     'YYYY-MM-DD HH:MM[:ss[.uuuuuu]][TZ] format.'),
+        'invalid_date': _("“%(value)s” value has the correct format "
                           "(YYYY-MM-DD) but it is an invalid date."),
-        'invalid_datetime': _("'%(value)s' value has the correct format "
-                              "(YYYY-MM-DD HH:MM[:ss[.uuuuuu]][TZ]) "
-                              "but it is an invalid date/time."),
+        'invalid_datetime': _('“%(value)s” value has the correct format '
+                              '(YYYY-MM-DD HH:MM[:ss[.uuuuuu]][TZ]) '
+                              'but it is an invalid date/time.'),
     }
     description = _("Date (with time)")

@@ -1443,7 +1441,7 @@
 class DecimalField(Field):
     empty_strings_allowed = False
     default_error_messages = {
-        'invalid': _("'%(value)s' value must be a decimal number."),
+        'invalid': _('“%(value)s” value must be a decimal number.'),
     }
     description = _("Decimal number")

@@ -1552,7 +1550,7 @@
             return self.context.create_decimal_from_float(value)
         try:
             return decimal.Decimal(value)
-        except decimal.InvalidOperation:
+        except (decimal.InvalidOperation, TypeError, ValueError):
             raise exceptions.ValidationError(
                 self.error_messages['invalid'],
                 code='invalid',
@@ -1584,8 +1582,8 @@
     """
     empty_strings_allowed = False
     default_error_messages = {
-        'invalid': _("'%(value)s' value has an invalid format. It must be in "
-                     "[DD] [HH:[MM:]]ss[.uuuuuu] format.")
+        'invalid': _('“%(value)s” value has an invalid format. It must be in '
+                     '[DD] [[HH:]MM:]ss[.uuuuuu] format.')
     }
     description = _("Duration")

@@ -1710,7 +1708,7 @@

     def formfield(self, **kwargs):
         return super().formfield(**{
-            'path': self.path,
+            'path': self.path() if callable(self.path) else self.path,
             'match': self.match,
             'recursive': self.recursive,
             'form_class': forms.FilePathField,
@@ -1726,7 +1724,7 @@
 class FloatField(Field):
     empty_strings_allowed = False
     default_error_messages = {
-        'invalid': _("'%(value)s' value must be a float."),
+        'invalid': _('“%(value)s” value must be a float.'),
     }
     description = _("Floating point number")

@@ -1734,7 +1732,12 @@
         value = super().get_prep_value(value)
         if value is None:
             return None
-        return float(value)
+        try:
+            return float(value)
+        except (TypeError, ValueError) as e:
+            raise e.__class__(
+                "Field '%s' expected a number but got %r." % (self.name, value),
+            ) from e

     def get_internal_type(self):
         return "FloatField"
@@ -1761,7 +1764,7 @@
 class IntegerField(Field):
     empty_strings_allowed = False
     default_error_messages = {
-        'invalid': _("'%(value)s' value must be an integer."),
+        'invalid': _('“%(value)s” value must be an integer.'),
     }
     description = _("Integer")

@@ -1790,13 +1793,25 @@
         validators_ = super().validators
         internal_type = self.get_internal_type()
         min_value, max_value = connection.ops.integer_field_range(internal_type)
-        if (min_value is not None and not
-            any(isinstance(validator, validators.MinValueValidator) and
-                validator.limit_value >= min_value for validator in validators_)):
+        if min_value is not None and not any(
+            (
+                isinstance(validator, validators.MinValueValidator) and (
+                    validator.limit_value()
+                    if callable(validator.limit_value)
+                    else validator.limit_value
+                ) >= min_value
+            ) for validator in validators_
+        ):
             validators_.append(validators.MinValueValidator(min_value))
-        if (max_value is not None and not
-            any(isinstance(validator, validators.MaxValueValidator) and
-                validator.limit_value <= max_value for validator in validators_)):
+        if max_value is not None and not any(
+            (
+                isinstance(validator, validators.MaxValueValidator) and (
+                    validator.limit_value()
+                    if callable(validator.limit_value)
+                    else validator.limit_value
+                ) <= max_value
+            ) for validator in validators_
+        ):
             validators_.append(validators.MaxValueValidator(max_value))
         return validators_

@@ -1804,7 +1819,12 @@
         value = super().get_prep_value(value)
         if value is None:
             return None
-        return int(value)
+        try:
+            return int(value)
+        except (TypeError, ValueError) as e:
+            raise e.__class__(
+                "Field '%s' expected a number but got %r." % (self.name, value),
+            ) from e

     def get_internal_type(self):
         return "IntegerField"
@@ -1843,6 +1863,13 @@
         })


+class SmallIntegerField(IntegerField):
+    description = _('Small integer')
+
+    def get_internal_type(self):
+        return 'SmallIntegerField'
+
+
 class IPAddressField(Field):
     empty_strings_allowed = False
     description = _("IPv4 address")
@@ -1956,10 +1983,18 @@

 class NullBooleanField(BooleanField):
     default_error_messages = {
-        'invalid': _("'%(value)s' value must be either None, True or False."),
-        'invalid_nullable': _("'%(value)s' value must be either None, True or False."),
+        'invalid': _('“%(value)s” value must be either None, True or False.'),
+        'invalid_nullable': _('“%(value)s” value must be either None, True or False.'),
     }
     description = _("Boolean (Either True, False or None)")
+    system_check_deprecated_details = {
+        'msg': (
+            'NullBooleanField is deprecated. Support for it (except in '
+            'historical migrations) will be removed in Django 4.0.'
+        ),
+        'hint': 'Use BooleanField(null=True) instead.',
+        'id': 'fields.W903',
+    }

     def __init__(self, *args, **kwargs):
         kwargs['null'] = True
@@ -1977,6 +2012,17 @@


 class PositiveIntegerRelDbTypeMixin:
+    def __init_subclass__(cls, **kwargs):
+        super().__init_subclass__(**kwargs)
+        if not hasattr(cls, 'integer_field_class'):
+            cls.integer_field_class = next(
+                (
+                    parent
+                    for parent in cls.__mro__[1:]
+                    if issubclass(parent, IntegerField)
+                ),
+                None,
+            )

     def rel_db_type(self, connection):
         """
@@ -1990,14 +2036,14 @@
         if connection.features.related_fields_match_type:
             return self.db_type(connection)
         else:
-            return IntegerField().db_type(connection=connection)
-
-
-class PositiveIntegerField(PositiveIntegerRelDbTypeMixin, IntegerField):
-    description = _("Positive integer")
+            return self.integer_field_class().db_type(connection=connection)
+
+
+class PositiveBigIntegerField(PositiveIntegerRelDbTypeMixin, BigIntegerField):
+    description = _('Positive big integer')

     def get_internal_type(self):
-        return "PositiveIntegerField"
+        return 'PositiveBigIntegerField'

     def formfield(self, **kwargs):
         return super().formfield(**{
@@ -2006,7 +2052,20 @@
         })


-class PositiveSmallIntegerField(PositiveIntegerRelDbTypeMixin, IntegerField):
+class PositiveIntegerField(PositiveIntegerRelDbTypeMixin, IntegerField):
+    description = _("Positive integer")
+
+    def get_internal_type(self):
+        return "PositiveIntegerField"
+
+    def formfield(self, **kwargs):
+        return super().formfield(**{
+            'min_value': 0,
+            **kwargs,
+        })
+
+
+class PositiveSmallIntegerField(PositiveIntegerRelDbTypeMixin, SmallIntegerField):
     description = _("Positive small integer")

     def get_internal_type(self):
@@ -2052,15 +2111,40 @@
         })


-class SmallIntegerField(IntegerField):
-    description = _("Small integer")
-
-    def get_internal_type(self):
-        return "SmallIntegerField"
-
-
 class TextField(Field):
     description = _("Text")
+
+    def __init__(self, *args, db_collation=None, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.db_collation = db_collation
+
+    def check(self, **kwargs):
+        databases = kwargs.get('databases') or []
+        return [
+            *super().check(**kwargs),
+            *self._check_db_collation(databases),
+        ]
+
+    def _check_db_collation(self, databases):
+        errors = []
+        for db in databases:
+            if not router.allow_migrate_model(db, self.model):
+                continue
+            connection = connections[db]
+            if not (
+                self.db_collation is None or
+                'supports_collation_on_textfield' in self.model._meta.required_db_features or
+                connection.features.supports_collation_on_textfield
+            ):
+                errors.append(
+                    checks.Error(
+                        '%s does not support a database collation on '
+                        'TextFields.' % connection.display_name,
+                        obj=self,
+                        id='fields.E190',
+                    ),
+                )
+        return errors

     def get_internal_type(self):
         return "TextField"
@@ -2080,18 +2164,24 @@
         # the value in the form field (to pass into widget for example).
         return super().formfield(**{
             'max_length': self.max_length,
-            **({} if self.choices else {'widget': forms.Textarea}),
+            **({} if self.choices is not None else {'widget': forms.Textarea}),
             **kwargs,
         })
+
+    def deconstruct(self):
+        name, path, args, kwargs = super().deconstruct()
+        if self.db_collation:
+            kwargs['db_collation'] = self.db_collation
+        return name, path, args, kwargs


 class TimeField(DateTimeCheckMixin, Field):
     empty_strings_allowed = False
     default_error_messages = {
-        'invalid': _("'%(value)s' value has an invalid format. It must be in "
-                     "HH:MM[:ss[.uuuuuu]] format."),
-        'invalid_time': _("'%(value)s' value has the correct format "
-                          "(HH:MM[:ss[.uuuuuu]]) but it is an invalid time."),
+        'invalid': _('“%(value)s” value has an invalid format. It must be in '
+                     'HH:MM[:ss[.uuuuuu]] format.'),
+        'invalid_time': _('“%(value)s” value has the correct format '
+                          '(HH:MM[:ss[.uuuuuu]]) but it is an invalid time.'),
     }
     description = _("Time")

@@ -2250,6 +2340,21 @@
         if self.max_length is not None:
             self.validators.append(validators.MaxLengthValidator(self.max_length))

+    def check(self, **kwargs):
+        return [*super().check(**kwargs), *self._check_str_default_value()]
+
+    def _check_str_default_value(self):
+        if self.has_default() and isinstance(self.default, str):
+            return [
+                checks.Error(
+                    "BinaryField's default cannot be a string. Use bytes "
+                    "content instead.",
+                    obj=self,
+                    id='fields.E170',
+                )
+            ]
+        return []
+
     def deconstruct(self):
         name, path, args, kwargs = super().deconstruct()
         if self.editable:
@@ -2291,7 +2396,7 @@

 class UUIDField(Field):
     default_error_messages = {
-        'invalid': _("'%(value)s' is not a valid UUID."),
+        'invalid': _('“%(value)s” is not a valid UUID.'),
     }
     description = _('Universally unique identifier')
     empty_strings_allowed = False
@@ -2307,6 +2412,10 @@

     def get_internal_type(self):
         return "UUIDField"
+
+    def get_prep_value(self, value):
+        value = super().get_prep_value(value)
+        return self.to_python(value)

     def get_db_prep_value(self, value, connection, prepared=False):
         if value is None:
@@ -2336,3 +2445,110 @@
             'form_class': forms.UUIDField,
             **kwargs,
         })
+
+
+class AutoFieldMixin:
+    db_returning = True
+
+    def __init__(self, *args, **kwargs):
+        kwargs['blank'] = True
+        super().__init__(*args, **kwargs)
+
+    def check(self, **kwargs):
+        return [
+            *super().check(**kwargs),
+            *self._check_primary_key(),
+        ]
+
+    def _check_primary_key(self):
+        if not self.primary_key:
+            return [
+                checks.Error(
+                    'AutoFields must set primary_key=True.',
+                    obj=self,
+                    id='fields.E100',
+                ),
+            ]
+        else:
+            return []
+
+    def deconstruct(self):
+        name, path, args, kwargs = super().deconstruct()
+        del kwargs['blank']
+        kwargs['primary_key'] = True
+        return name, path, args, kwargs
+
+    def validate(self, value, model_instance):
+        pass
+
+    def get_db_prep_value(self, value, connection, prepared=False):
+        if not prepared:
+            value = self.get_prep_value(value)
+            value = connection.ops.validate_autopk_value(value)
+        return value
+
+    def contribute_to_class(self, cls, name, **kwargs):
+        assert not cls._meta.auto_field, (
+            "Model %s can't have more than one auto-generated field."
+            % cls._meta.label
+        )
+        super().contribute_to_class(cls, name, **kwargs)
+        cls._meta.auto_field = self
+
+    def formfield(self, **kwargs):
+        return None
+
+
+class AutoFieldMeta(type):
+    """
+    Metaclass to maintain backward inheritance compatibility for AutoField.
+
+    It is intended that AutoFieldMixin become public API when it is possible to
+    create a non-integer automatically-generated field using column defaults
+    stored in the database.
+
+    In many areas Django also relies on using isinstance() to check for an
+    automatically-generated field as a subclass of AutoField. A new flag needs
+    to be implemented on Field to be used instead.
+
+    When these issues have been addressed, this metaclass could be used to
+    deprecate inheritance from AutoField and use of isinstance() with AutoField
+    for detecting automatically-generated fields.
+    """
+
+    @property
+    def _subclasses(self):
+        return (BigAutoField, SmallAutoField)
+
+    def __instancecheck__(self, instance):
+        return isinstance(instance, self._subclasses) or super().__instancecheck__(instance)
+
+    def __subclasscheck__(self, subclass):
+        return issubclass(subclass, self._subclasses) or super().__subclasscheck__(subclass)
+
+
+class AutoField(AutoFieldMixin, IntegerField, metaclass=AutoFieldMeta):
+
+    def get_internal_type(self):
+        return 'AutoField'
+
+    def rel_db_type(self, connection):
+        return IntegerField().db_type(connection=connection)
+
+
+class BigAutoField(AutoFieldMixin, BigIntegerField):
+
+    def get_internal_type(self):
+        return 'BigAutoField'
+
+    def rel_db_type(self, connection):
+        return BigIntegerField().db_type(connection=connection)
+
+
+class SmallAutoField(AutoFieldMixin, SmallIntegerField):
+
+    def get_internal_type(self):
+        return 'SmallAutoField'
+
+    def rel_db_type(self, connection):
+        return SmallIntegerField().db_type(connection=connection)
('django/db/models/fields', 'reverse_related.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -11,6 +11,7 @@

 from django.core import exceptions
 from django.utils.functional import cached_property
+from django.utils.hashable import make_hashable

 from . import BLANK_CHOICE_DASH
 from .mixins import FieldCacheMixin
@@ -33,6 +34,7 @@
     # Reverse relations are always nullable (Django can't enforce that a
     # foreign key on the related model points to this model).
     null = True
+    empty_strings_allowed = False

     def __init__(self, field, to, related_name=None, related_query_name=None,
                  limit_choices_to=None, parent_link=False, on_delete=None):
@@ -114,7 +116,32 @@
             self.related_model._meta.model_name,
         )

-    def get_choices(self, include_blank=True, blank_choice=BLANK_CHOICE_DASH, ordering=()):
+    @property
+    def identity(self):
+        return (
+            self.field,
+            self.model,
+            self.related_name,
+            self.related_query_name,
+            make_hashable(self.limit_choices_to),
+            self.parent_link,
+            self.on_delete,
+            self.symmetrical,
+            self.multiple,
+        )
+
+    def __eq__(self, other):
+        if not isinstance(other, self.__class__):
+            return NotImplemented
+        return self.identity == other.identity
+
+    def __hash__(self):
+        return hash(self.identity)
+
+    def get_choices(
+        self, include_blank=True, blank_choice=BLANK_CHOICE_DASH,
+        limit_choices_to=None, ordering=(),
+    ):
         """
         Return choices with a default blank choices included, for use
         as <select> choices for this field.
@@ -122,8 +149,12 @@
         Analog of django.db.models.fields.Field.get_choices(), provided
         initially for utilization by RelatedFieldListFilter.
         """
+        limit_choices_to = limit_choices_to or self.limit_choices_to
+        qs = self.related_model._default_manager.complex_filter(limit_choices_to)
+        if ordering:
+            qs = qs.order_by(*ordering)
         return (blank_choice if include_blank else []) + [
-            (x.pk, str(x)) for x in self.related_model._default_manager.order_by(*ordering)
+            (x.pk, str(x)) for x in qs
         ]

     def is_hidden(self):
@@ -207,6 +238,10 @@
         state.pop('related_model', None)
         return state

+    @property
+    def identity(self):
+        return super().identity + (self.field_name,)
+
     def get_related_field(self):
         """
         Return the Field in the 'to' object to which this relationship is tied.
@@ -271,6 +306,14 @@
         self.symmetrical = symmetrical
         self.db_constraint = db_constraint

+    @property
+    def identity(self):
+        return super().identity + (
+            self.through,
+            make_hashable(self.through_fields),
+            self.db_constraint,
+        )
+
     def get_related_field(self):
         """
         Return the field in the 'to' object to which this relationship is tied.
('django/db/models/fields', 'related_lookups.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -64,7 +64,9 @@
             # For multicolumn lookups we need to build a multicolumn where clause.
             # This clause is either a SubqueryConstraint (for values that need to be compiled to
             # SQL) or an OR-combined list of (col1 = val1 AND col2 = val2 AND ...) clauses.
-            from django.db.models.sql.where import WhereNode, SubqueryConstraint, AND, OR
+            from django.db.models.sql.where import (
+                AND, OR, SubqueryConstraint, WhereNode,
+            )

             root_constraint = WhereNode(connector=OR)
             if self.rhs_is_direct_value():
@@ -101,7 +103,7 @@

 class RelatedLookupMixin:
     def get_prep_lookup(self):
-        if not isinstance(self.lhs, MultiColSource) and self.rhs_is_direct_value():
+        if not isinstance(self.lhs, MultiColSource) and not hasattr(self.rhs, 'resolve_expression'):
             # If we get here, we are dealing with single-column relations.
             self.rhs = get_normalized_value(self.rhs, self.lhs)[0]
             # We need to run the related field's get_prep_value(). Consider case
@@ -120,7 +122,7 @@
         if isinstance(self.lhs, MultiColSource):
             assert self.rhs_is_direct_value()
             self.rhs = get_normalized_value(self.rhs, self.lhs)
-            from django.db.models.sql.where import WhereNode, AND
+            from django.db.models.sql.where import AND, WhereNode
             root_constraint = WhereNode()
             for target, source, val in zip(self.lhs.targets, self.lhs.sources, self.rhs):
                 lookup_class = target.get_lookup(self.lookup_name)
('django/db/models/sql', 'compiler.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,28 +1,31 @@
 import collections
-import functools
 import re
-import warnings
+from functools import partial
 from itertools import chain

 from django.core.exceptions import EmptyResultSet, FieldError
+from django.db import DatabaseError, NotSupportedError
 from django.db.models.constants import LOOKUP_SEP
-from django.db.models.expressions import OrderBy, Random, RawSQL, Ref, Subquery
-from django.db.models.query_utils import QueryWrapper, select_related_descend
+from django.db.models.expressions import F, OrderBy, RawSQL, Ref, Value
+from django.db.models.functions import Cast, Random
+from django.db.models.query_utils import Q, select_related_descend
 from django.db.models.sql.constants import (
     CURSOR, GET_ITERATOR_CHUNK_SIZE, MULTI, NO_RESULTS, ORDER_DIR, SINGLE,
 )
 from django.db.models.sql.query import Query, get_order_dir
 from django.db.transaction import TransactionManagementError
-from django.db.utils import DatabaseError, NotSupportedError
-from django.utils.deprecation import (
-    RemovedInDjango30Warning, RemovedInDjango31Warning,
-)
-from django.utils.inspect import func_supports_parameter
-
-FORCE = object()
+from django.utils.functional import cached_property
+from django.utils.hashable import make_hashable
+from django.utils.regex_helper import _lazy_re_compile


 class SQLCompiler:
+    # Multiline ordering SQL clause may appear from RawSQL.
+    ordering_parts = _lazy_re_compile(
+        r'^(.*)\s(?:ASC|DESC).*',
+        re.MULTILINE | re.DOTALL,
+    )
+
     def __init__(self, query, connection, using):
         self.query = query
         self.connection = connection
@@ -35,7 +38,6 @@
         self.select = None
         self.annotation_col_map = None
         self.klass_info = None
-        self.ordering_parts = re.compile(r'(.*)\s(ASC|DESC)(.*)')
         self._meta_ordering = None

     def setup_query(self):
@@ -112,15 +114,22 @@
         # Note that even if the group_by is set, it is only the minimal
         # set to group by. So, we need to add cols in select, order_by, and
         # having into the select in any case.
+        ref_sources = {
+            expr.source for expr in expressions if isinstance(expr, Ref)
+        }
         for expr, _, _ in select:
+            # Skip members of the select clause that are already included
+            # by reference.
+            if expr in ref_sources:
+                continue
             cols = expr.get_group_by_cols()
             for col in cols:
                 expressions.append(col)
         for expr, (sql, params, is_ref) in order_by:
             # Skip References to the select clause, as all expressions in the
             # select clause are already part of the group by.
-            if not expr.contains_aggregate and not is_ref:
-                expressions.extend(expr.get_source_expressions())
+            if not is_ref:
+                expressions.extend(expr.get_group_by_cols())
         having_group_by = self.having.get_group_by_cols() if self.having else ()
         for expr in having_group_by:
             expressions.append(expr)
@@ -130,14 +139,11 @@

         for expr in expressions:
             sql, params = self.compile(expr)
-            if isinstance(expr, Subquery) and not sql.startswith('('):
-                # Subquery expression from HAVING clause may not contain
-                # wrapping () because they could be removed when a subquery is
-                # the "rhs" in an expression (see Subquery._prepare()).
-                sql = '(%s)' % sql
-            if (sql, tuple(params)) not in seen:
+            sql, params = expr.select_format(self, sql, params)
+            params_hash = make_hashable(params)
+            if (sql, params_hash) not in seen:
                 result.append((sql, params))
-                seen.add((sql, tuple(params)))
+                seen.add((sql, params_hash))
         return result

     def collapse_group_by(self, expressions, having):
@@ -178,7 +184,11 @@
             # database views on which the optimization might not be allowed.
             pks = {
                 expr for expr in expressions
-                if hasattr(expr, 'target') and expr.target.primary_key and expr.target.model._meta.managed
+                if (
+                    hasattr(expr, 'target') and
+                    expr.target.primary_key and
+                    self.connection.features.allows_group_by_selected_pks_on_model(expr.target.model)
+                )
             }
             aliases = {expr.alias for expr in pks}
             expressions = [
@@ -249,10 +259,12 @@
         ret = []
         for col, alias in select:
             try:
-                sql, params = self.compile(col, select_format=True)
+                sql, params = self.compile(col)
             except EmptyResultSet:
                 # Select a predicate that's always False.
                 sql, params = '0', ()
+            else:
+                sql, params = col.select_format(self, sql, params)
             ret.append((col, (sql, params), alias))
         return ret, klass_info, annotations

@@ -284,9 +296,13 @@
         order_by = []
         for field in ordering:
             if hasattr(field, 'resolve_expression'):
+                if isinstance(field, Value):
+                    # output_field must be resolved for constants.
+                    field = Cast(field, field.output_field)
                 if not isinstance(field, OrderBy):
                     field = field.asc()
                 if not self.query.standard_ordering:
+                    field = field.copy()
                     field.reverse_ordering()
                 order_by.append((field, False))
                 continue
@@ -304,10 +320,18 @@
                     True))
                 continue
             if col in self.query.annotations:
-                # References to an expression which is masked out of the SELECT clause
-                order_by.append((
-                    OrderBy(self.query.annotations[col], descending=descending),
-                    False))
+                # References to an expression which is masked out of the SELECT
+                # clause.
+                if self.query.combinator and self.select:
+                    # Don't use the resolved annotation because other
+                    # combinated queries might define it differently.
+                    expr = F(col)
+                else:
+                    expr = self.query.annotations[col]
+                    if isinstance(expr, Value):
+                        # output_field must be resolved for constants.
+                        expr = Cast(expr, expr.output_field)
+                order_by.append((OrderBy(expr, descending=descending), False))
                 continue

             if '.' in field:
@@ -321,11 +345,17 @@
                     ), False))
                 continue

-            if not self.query._extra or col not in self.query._extra:
-                # 'col' is of the form 'field' or 'field1__field2' or
-                # '-field1__field2__field', etc.
-                order_by.extend(self.find_ordering_name(
-                    field, self.query.get_meta(), default_order=asc))
+            if not self.query.extra or col not in self.query.extra:
+                if self.query.combinator and self.select:
+                    # Don't use the first model's field because other
+                    # combinated queries might define it differently.
+                    order_by.append((OrderBy(F(col), descending=descending), False))
+                else:
+                    # 'col' is of the form 'field' or 'field1__field2' or
+                    # '-field1__field2__field', etc.
+                    order_by.extend(self.find_ordering_name(
+                        field, self.query.get_meta(), default_order=asc,
+                    ))
             else:
                 if col not in self.query.extra_select:
                     order_by.append((
@@ -340,30 +370,43 @@

         for expr, is_ref in order_by:
             resolved = expr.resolve_expression(self.query, allow_joins=True, reuse=None)
-            if self.query.combinator:
+            if self.query.combinator and self.select:
                 src = resolved.get_source_expressions()[0]
+                expr_src = expr.get_source_expressions()[0]
                 # Relabel order by columns to raw numbers if this is a combined
                 # query; necessary since the columns can't be referenced by the
                 # fully qualified name and the simple column names may collide.
                 for idx, (sel_expr, _, col_alias) in enumerate(self.select):
                     if is_ref and col_alias == src.refs:
                         src = src.source
-                    elif col_alias:
+                    elif col_alias and not (
+                        isinstance(expr_src, F) and col_alias == expr_src.name
+                    ):
                         continue
                     if src == sel_expr:
                         resolved.set_source_expressions([RawSQL('%d' % (idx + 1), ())])
                         break
                 else:
-                    raise DatabaseError('ORDER BY term does not match any column in the result set.')
+                    if col_alias:
+                        raise DatabaseError('ORDER BY term does not match any column in the result set.')
+                    # Add column used in ORDER BY clause to the selected
+                    # columns and to each combined query.
+                    order_by_idx = len(self.query.select) + 1
+                    col_name = f'__orderbycol{order_by_idx}'
+                    for q in self.query.combined_queries:
+                        q.add_annotation(expr_src, col_name)
+                    self.query.add_select_col(resolved, col_name)
+                    resolved.set_source_expressions([RawSQL(f'{order_by_idx}', ())])
             sql, params = self.compile(resolved)
             # Don't add the same column twice, but the order direction is
             # not taken into account so we strip it. When this entire method
             # is refactored into expressions, then we can check each part as we
             # generate it.
-            without_ordering = self.ordering_parts.search(sql).group(1)
-            if (without_ordering, tuple(params)) in seen:
+            without_ordering = self.ordering_parts.search(sql)[1]
+            params_hash = make_hashable(params)
+            if (without_ordering, params_hash) in seen:
                 continue
-            seen.add((without_ordering, tuple(params)))
+            seen.add((without_ordering, params_hash))
             result.append((resolved, (sql, params, is_ref)))
         return result

@@ -372,7 +415,7 @@
         if self.query.distinct and not self.query.distinct_fields:
             select_sql = [t[1] for t in select]
             for expr, (sql, params, is_ref) in order_by:
-                without_ordering = self.ordering_parts.search(sql).group(1)
+                without_ordering = self.ordering_parts.search(sql)[1]
                 if not is_ref and (without_ordering, params) not in select_sql:
                     extra_select.append((expr, (without_ordering, params), None))
         return extra_select
@@ -387,21 +430,19 @@
             return self.quote_cache[name]
         if ((name in self.query.alias_map and name not in self.query.table_map) or
                 name in self.query.extra_select or (
-                    name in self.query.external_aliases and name not in self.query.table_map)):
+                    self.query.external_aliases.get(name) and name not in self.query.table_map)):
             self.quote_cache[name] = name
             return name
         r = self.connection.ops.quote_name(name)
         self.quote_cache[name] = r
         return r

-    def compile(self, node, select_format=False):
+    def compile(self, node):
         vendor_impl = getattr(node, 'as_' + self.connection.vendor, None)
         if vendor_impl:
             sql, params = vendor_impl(self, self.connection)
         else:
             sql, params = node.as_sql(self, self.connection)
-        if select_format is FORCE or (select_format and not self.query.subquery):
-            return node.output_field.select_format(self, sql, params)
         return sql, params

     def get_combinator_sql(self, combinator, all):
@@ -423,6 +464,7 @@
                 # must have the same columns list. Set the selects defined on
                 # the query on all combined queries, if not already set.
                 if not compiler.query.values_select and self.query.values_select:
+                    compiler.query = compiler.query.clone()
                     compiler.query.set_values((
                         *self.query.extra_select,
                         *self.query.values_select,
@@ -522,19 +564,26 @@
                     nowait = self.query.select_for_update_nowait
                     skip_locked = self.query.select_for_update_skip_locked
                     of = self.query.select_for_update_of
-                    # If it's a NOWAIT/SKIP LOCKED/OF query but the backend
-                    # doesn't support it, raise NotSupportedError to prevent a
-                    # possible deadlock.
+                    no_key = self.query.select_for_no_key_update
+                    # If it's a NOWAIT/SKIP LOCKED/OF/NO KEY query but the
+                    # backend doesn't support it, raise NotSupportedError to
+                    # prevent a possible deadlock.
                     if nowait and not self.connection.features.has_select_for_update_nowait:
                         raise NotSupportedError('NOWAIT is not supported on this database backend.')
                     elif skip_locked and not self.connection.features.has_select_for_update_skip_locked:
                         raise NotSupportedError('SKIP LOCKED is not supported on this database backend.')
                     elif of and not self.connection.features.has_select_for_update_of:
                         raise NotSupportedError('FOR UPDATE OF is not supported on this database backend.')
+                    elif no_key and not self.connection.features.has_select_for_no_key_update:
+                        raise NotSupportedError(
+                            'FOR NO KEY UPDATE is not supported on this '
+                            'database backend.'
+                        )
                     for_update_part = self.connection.ops.for_update_sql(
                         nowait=nowait,
                         skip_locked=skip_locked,
                         of=self.get_select_for_update_of_arguments(),
+                        no_key=no_key,
                     )

                 if for_update_part and self.connection.features.for_update_after_from:
@@ -554,17 +603,7 @@
                     order_by = order_by or self.connection.ops.force_no_ordering()
                     result.append('GROUP BY %s' % ', '.join(grouping))
                     if self._meta_ordering:
-                        # When the deprecation ends, replace with:
-                        # order_by = None
-                        warnings.warn(
-                            "%s QuerySet won't use Meta.ordering in Django 3.1. "
-                            "Add .order_by('%s') to retain the current query." % (
-                                self.query.model.__name__,
-                                "', '".join(self._meta_ordering)
-                            ),
-                            RemovedInDjango31Warning,
-                            stacklevel=4,
-                        )
+                        order_by = None
                 if having:
                     result.append('HAVING %s' % having)
                     params.extend(h_params)
@@ -701,9 +740,14 @@
         field, targets, alias, joins, path, opts, transform_function = self._setup_joins(pieces, opts, alias)

         # If we get to this point and the field is a relation to another model,
-        # append the default ordering for that model unless the attribute name
-        # of the field is specified.
-        if field.is_relation and opts.ordering and getattr(field, 'attname', None) != name:
+        # append the default ordering for that model unless it is the pk
+        # shortcut or the attribute name of the field that is specified.
+        if (
+            field.is_relation and
+            opts.ordering and
+            getattr(field, 'attname', None) != pieces[-1] and
+            name != 'pk'
+        ):
             # Firstly, avoid infinite loops.
             already_seen = already_seen or set()
             join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)
@@ -713,6 +757,11 @@

             results = []
             for item in opts.ordering:
+                if hasattr(item, 'resolve_expression') and not isinstance(item, OrderBy):
+                    item = item.desc() if descending else item.asc()
+                if isinstance(item, OrderBy):
+                    results.append((item, False))
+                    continue
                 results.extend(self.find_ordering_name(item, opts, alias,
                                                        order, already_seen))
             return results
@@ -886,6 +935,15 @@
                     select, model._meta, alias, cur_depth + 1,
                     next, restricted)
                 get_related_klass_infos(klass_info, next_klass_infos)
+
+            def local_setter(obj, from_obj):
+                # Set a reverse fk object when relation is non-empty.
+                if from_obj:
+                    f.remote_field.set_cached_value(from_obj, obj)
+
+            def remote_setter(name, obj, from_obj):
+                setattr(from_obj, name, obj)
+
             for name in list(requested):
                 # Filtered relations work only on the topmost level.
                 if cur_depth > 1:
@@ -896,18 +954,12 @@
                     model = join_opts.model
                     alias = joins[-1]
                     from_parent = issubclass(model, opts.model) and model is not opts.model
-
-                    def local_setter(obj, from_obj):
-                        f.remote_field.set_cached_value(from_obj, obj)
-
-                    def remote_setter(obj, from_obj):
-                        setattr(from_obj, name, obj)
                     klass_info = {
                         'model': model,
                         'field': f,
                         'reverse': True,
                         'local_setter': local_setter,
-                        'remote_setter': remote_setter,
+                        'remote_setter': partial(remote_setter, name),
                         'from_parent': from_parent,
                     }
                     related_klass_infos.append(klass_info)
@@ -944,6 +996,38 @@
         Return a quoted list of arguments for the SELECT FOR UPDATE OF part of
         the query.
         """
+        def _get_parent_klass_info(klass_info):
+            concrete_model = klass_info['model']._meta.concrete_model
+            for parent_model, parent_link in concrete_model._meta.parents.items():
+                parent_list = parent_model._meta.get_parent_list()
+                yield {
+                    'model': parent_model,
+                    'field': parent_link,
+                    'reverse': False,
+                    'select_fields': [
+                        select_index
+                        for select_index in klass_info['select_fields']
+                        # Selected columns from a model or its parents.
+                        if (
+                            self.select[select_index][0].target.model == parent_model or
+                            self.select[select_index][0].target.model in parent_list
+                        )
+                    ],
+                }
+
+        def _get_first_selected_col_from_model(klass_info):
+            """
+            Find the first selected column from a model. If it doesn't exist,
+            don't lock a model.
+
+            select_fields is filled recursively, so it also contains fields
+            from the parent models.
+            """
+            concrete_model = klass_info['model']._meta.concrete_model
+            for select_index in klass_info['select_fields']:
+                if self.select[select_index][0].target.model == concrete_model:
+                    return self.select[select_index][0]
+
         def _get_field_choices():
             """Yield all allowed field paths in breadth-first search order."""
             queue = collections.deque([(None, self.klass_info)])
@@ -960,33 +1044,43 @@
                     yield LOOKUP_SEP.join(path)
                 queue.extend(
                     (path, klass_info)
+                    for klass_info in _get_parent_klass_info(klass_info)
+                )
+                queue.extend(
+                    (path, klass_info)
                     for klass_info in klass_info.get('related_klass_infos', [])
                 )
         result = []
         invalid_names = []
         for name in self.query.select_for_update_of:
-            parts = [] if name == 'self' else name.split(LOOKUP_SEP)
             klass_info = self.klass_info
-            for part in parts:
-                for related_klass_info in klass_info.get('related_klass_infos', []):
-                    field = related_klass_info['field']
-                    if related_klass_info['reverse']:
-                        field = field.remote_field
-                    if field.name == part:
-                        klass_info = related_klass_info
+            if name == 'self':
+                col = _get_first_selected_col_from_model(klass_info)
+            else:
+                for part in name.split(LOOKUP_SEP):
+                    klass_infos = (
+                        *klass_info.get('related_klass_infos', []),
+                        *_get_parent_klass_info(klass_info),
+                    )
+                    for related_klass_info in klass_infos:
+                        field = related_klass_info['field']
+                        if related_klass_info['reverse']:
+                            field = field.remote_field
+                        if field.name == part:
+                            klass_info = related_klass_info
+                            break
+                    else:
+                        klass_info = None
                         break
+                if klass_info is None:
+                    invalid_names.append(name)
+                    continue
+                col = _get_first_selected_col_from_model(klass_info)
+            if col is not None:
+                if self.connection.features.select_for_update_of_column:
+                    result.append(self.compile(col)[0])
                 else:
-                    klass_info = None
-                    break
-            if klass_info is None:
-                invalid_names.append(name)
-                continue
-            select_index = klass_info['select_fields'][0]
-            col = self.select[select_index][0]
-            if self.connection.features.select_for_update_of_column:
-                result.append(self.compile(col)[0])
-            else:
-                result.append(self.quote_name_unless_alias(col.alias))
+                    result.append(self.quote_name_unless_alias(col.alias))
         if invalid_names:
             raise FieldError(
                 'Invalid field name(s) given in select_for_update(of=(...)): %s. '
@@ -1015,20 +1109,7 @@
                 backend_converters = self.connection.ops.get_db_converters(expression)
                 field_converters = expression.get_db_converters(self.connection)
                 if backend_converters or field_converters:
-                    convs = []
-                    for conv in (backend_converters + field_converters):
-                        if func_supports_parameter(conv, 'context'):
-                            warnings.warn(
-                                'Remove the context parameter from %s.%s(). Support for it '
-                                'will be removed in Django 3.0.' % (
-                                    conv.__self__.__class__.__name__,
-                                    conv.__name__,
-                                ),
-                                RemovedInDjango30Warning,
-                            )
-                            conv = functools.partial(conv, context={})
-                        convs.append(conv)
-                    converters[i] = (convs, expression)
+                    converters[i] = (backend_converters + field_converters, expression)
         return converters

     def apply_converters(self, rows, converters):
@@ -1061,9 +1142,6 @@
         Backends (e.g. NoSQL) can override this in order to use optimized
         versions of "query has any results."
         """
-        # This is always executed on a query clone, so we can modify self.query
-        self.query.add_extra({'a': 1}, None, None, None, None, None)
-        self.query.set_extra_mask(['a'])
         return bool(self.execute_sql(SINGLE))

     def execute_sql(self, result_type=MULTI, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):
@@ -1141,7 +1219,7 @@
             lhs_sql, lhs_params = self.compile(select_col)
             rhs = '%s.%s' % (qn(alias), qn2(columns[index]))
             self.query.where.add(
-                QueryWrapper('%s = %s' % (lhs_sql, rhs), lhs_params), 'AND')
+                RawSQL('%s = %s' % (lhs_sql, rhs), lhs_params), 'AND')

         sql, params = self.as_sql()
         return 'EXISTS (%s)' % sql, params
@@ -1158,7 +1236,8 @@


 class SQLInsertCompiler(SQLCompiler):
-    return_id = False
+    returning_fields = None
+    returning_params = tuple()

     def field_as_sql(self, field, val):
         """
@@ -1208,9 +1287,15 @@
                     'can only be used to update, not to insert.' % (value, field)
                 )
             if value.contains_aggregate:
-                raise FieldError("Aggregate functions are not allowed in this query")
+                raise FieldError(
+                    'Aggregate functions are not allowed in this query '
+                    '(%s=%r).' % (field.name, value)
+                )
             if value.contains_over_clause:
-                raise FieldError('Window expressions are not allowed in this query.')
+                raise FieldError(
+                    'Window expressions are not allowed in this query (%s=%r).'
+                    % (field.name, value)
+                )
         else:
             value = field.get_db_prep_save(value, connection=self.connection)
         return value
@@ -1283,15 +1368,15 @@
         # queries and generate their own placeholders. Doing that isn't
         # necessary and it should be possible to use placeholders and
         # expressions in bulk inserts too.
-        can_bulk = (not self.return_id and self.connection.features.has_bulk_insert)
+        can_bulk = (not self.returning_fields and self.connection.features.has_bulk_insert)

         placeholder_rows, param_rows = self.assemble_as_sql(fields, value_rows)

         ignore_conflicts_suffix_sql = self.connection.ops.ignore_conflicts_suffix_sql(
             ignore_conflicts=self.query.ignore_conflicts
         )
-        if self.return_id and self.connection.features.can_return_id_from_insert:
-            if self.connection.features.can_return_ids_from_bulk_insert:
+        if self.returning_fields and self.connection.features.can_return_columns_from_insert:
+            if self.connection.features.can_return_rows_from_bulk_insert:
                 result.append(self.connection.ops.bulk_insert_sql(fields, placeholder_rows))
                 params = param_rows
             else:
@@ -1299,13 +1384,12 @@
                 params = [param_rows[0]]
             if ignore_conflicts_suffix_sql:
                 result.append(ignore_conflicts_suffix_sql)
-            col = "%s.%s" % (qn(opts.db_table), qn(opts.pk.column))
-            r_fmt, r_params = self.connection.ops.return_insert_id()
-            # Skip empty r_fmt to allow subclasses to customize behavior for
+            # Skip empty r_sql to allow subclasses to customize behavior for
             # 3rd party backends. Refs #19096.
-            if r_fmt:
-                result.append(r_fmt % col)
-                params += [r_params]
+            r_sql, self.returning_params = self.connection.ops.return_insert_columns(self.returning_fields)
+            if r_sql:
+                result.append(r_sql)
+                params += [self.returning_params]
             return [(" ".join(result), tuple(chain.from_iterable(params)))]

         if can_bulk:
@@ -1321,41 +1405,84 @@
                 for p, vals in zip(placeholder_rows, param_rows)
             ]

-    def execute_sql(self, return_id=False):
+    def execute_sql(self, returning_fields=None):
         assert not (
-            return_id and len(self.query.objs) != 1 and
-            not self.connection.features.can_return_ids_from_bulk_insert
+            returning_fields and len(self.query.objs) != 1 and
+            not self.connection.features.can_return_rows_from_bulk_insert
         )
-        self.return_id = return_id
+        self.returning_fields = returning_fields
         with self.connection.cursor() as cursor:
             for sql, params in self.as_sql():
                 cursor.execute(sql, params)
-            if not return_id:
-                return
-            if self.connection.features.can_return_ids_from_bulk_insert and len(self.query.objs) > 1:
-                return self.connection.ops.fetch_returned_insert_ids(cursor)
-            if self.connection.features.can_return_id_from_insert:
+            if not self.returning_fields:
+                return []
+            if self.connection.features.can_return_rows_from_bulk_insert and len(self.query.objs) > 1:
+                return self.connection.ops.fetch_returned_insert_rows(cursor)
+            if self.connection.features.can_return_columns_from_insert:
                 assert len(self.query.objs) == 1
-                return self.connection.ops.fetch_returned_insert_id(cursor)
-            return self.connection.ops.last_insert_id(
+                return [self.connection.ops.fetch_returned_insert_columns(cursor, self.returning_params)]
+            return [(self.connection.ops.last_insert_id(
                 cursor, self.query.get_meta().db_table, self.query.get_meta().pk.column
-            )
+            ),)]


 class SQLDeleteCompiler(SQLCompiler):
-    def as_sql(self):
-        """
-        Create the SQL for this query. Return the SQL string and list of
-        parameters.
-        """
-        assert len([t for t in self.query.alias_map if self.query.alias_refcount[t] > 0]) == 1, \
-            "Can only delete from one table at a time."
-        qn = self.quote_name_unless_alias
-        result = ['DELETE FROM %s' % qn(self.query.base_table)]
-        where, params = self.compile(self.query.where)
+    @cached_property
+    def single_alias(self):
+        # Ensure base table is in aliases.
+        self.query.get_initial_alias()
+        return sum(self.query.alias_refcount[t] > 0 for t in self.query.alias_map) == 1
+
+    @classmethod
+    def _expr_refs_base_model(cls, expr, base_model):
+        if isinstance(expr, Query):
+            return expr.model == base_model
+        if not hasattr(expr, 'get_source_expressions'):
+            return False
+        return any(
+            cls._expr_refs_base_model(source_expr, base_model)
+            for source_expr in expr.get_source_expressions()
+        )
+
+    @cached_property
+    def contains_self_reference_subquery(self):
+        return any(
+            self._expr_refs_base_model(expr, self.query.model)
+            for expr in chain(self.query.annotations.values(), self.query.where.children)
+        )
+
+    def _as_sql(self, query):
+        result = [
+            'DELETE FROM %s' % self.quote_name_unless_alias(query.base_table)
+        ]
+        where, params = self.compile(query.where)
         if where:
             result.append('WHERE %s' % where)
         return ' '.join(result), tuple(params)
+
+    def as_sql(self):
+        """
+        Create the SQL for this query. Return the SQL string and list of
+        parameters.
+        """
+        if self.single_alias and not self.contains_self_reference_subquery:
+            return self._as_sql(self.query)
+        innerq = self.query.clone()
+        innerq.__class__ = Query
+        innerq.clear_select_clause()
+        pk = self.query.model._meta.pk
+        innerq.select = [
+            pk.get_col(self.query.get_initial_alias())
+        ]
+        outerq = Query(self.query.model)
+        outerq.where = self.query.where_class()
+        if not self.connection.features.update_can_self_select:
+            # Force the materialization of the inner query to allow reference
+            # to the target table on MySQL.
+            sql, params = innerq.get_compiler(connection=self.connection).as_sql()
+            innerq = RawSQL('SELECT * FROM (%s) subquery' % sql, params)
+        outerq.add_q(Q(pk__in=innerq))
+        return self._as_sql(outerq)


 class SQLUpdateCompiler(SQLCompiler):
@@ -1373,9 +1500,15 @@
             if hasattr(val, 'resolve_expression'):
                 val = val.resolve_expression(self.query, allow_joins=False, for_save=True)
                 if val.contains_aggregate:
-                    raise FieldError("Aggregate functions are not allowed in this query")
+                    raise FieldError(
+                        'Aggregate functions are not allowed in this query '
+                        '(%s=%r).' % (field.name, val)
+                    )
                 if val.contains_over_clause:
-                    raise FieldError('Window expressions are not allowed in this query.')
+                    raise FieldError(
+                        'Window expressions are not allowed in this query '
+                        '(%s=%r).' % (field.name, val)
+                    )
             elif hasattr(val, 'prepare_database_save'):
                 if field.remote_field:
                     val = field.get_db_prep_save(
@@ -1455,7 +1588,7 @@
         query = self.query.chain(klass=Query)
         query.select_related = False
         query.clear_ordering(True)
-        query._extra = {}
+        query.extra = {}
         query.select = []
         query.add_fields([query.get_meta().pk.name])
         super().pre_sql_setup()
@@ -1488,15 +1621,19 @@
         """
         sql, params = [], []
         for annotation in self.query.annotation_select.values():
-            ann_sql, ann_params = self.compile(annotation, select_format=FORCE)
+            ann_sql, ann_params = self.compile(annotation)
+            ann_sql, ann_params = annotation.select_format(self, ann_sql, ann_params)
             sql.append(ann_sql)
             params.extend(ann_params)
         self.col_count = len(self.query.annotation_select)
         sql = ', '.join(sql)
         params = tuple(params)

-        sql = 'SELECT %s FROM (%s) subquery' % (sql, self.query.subquery)
-        params = params + self.query.sub_params
+        inner_query_sql, inner_query_params = self.query.inner_query.get_compiler(
+            self.using
+        ).as_sql(with_col_aliases=True)
+        sql = 'SELECT %s FROM (%s) subquery' % (sql, inner_query_sql)
+        params = params + inner_query_params
         return sql, params


('django/db/models/sql', 'query.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -6,20 +6,24 @@
 databases). The abstraction barrier only works one way: this module has to know
 all about the internals of models in order to get the information it needs.
 """
+import copy
 import difflib
 import functools
-from collections import Counter, OrderedDict, namedtuple
+import inspect
+import sys
+import warnings
+from collections import Counter, namedtuple
 from collections.abc import Iterator, Mapping
 from itertools import chain, count, product
 from string import ascii_uppercase

-from django.core.exceptions import (
-    EmptyResultSet, FieldDoesNotExist, FieldError,
-)
+from django.core.exceptions import FieldDoesNotExist, FieldError
 from django.db import DEFAULT_DB_ALIAS, NotSupportedError, connections
 from django.db.models.aggregates import Count
 from django.db.models.constants import LOOKUP_SEP
-from django.db.models.expressions import Col, F, Ref, SimpleCol
+from django.db.models.expressions import (
+    BaseExpression, Col, Exists, F, OuterRef, Ref, ResolvedOuterRef,
+)
 from django.db.models.fields import Field
 from django.db.models.fields.related_lookups import MultiColSource
 from django.db.models.lookups import Lookup
@@ -35,10 +39,20 @@
 from django.db.models.sql.where import (
     AND, OR, ExtraWhere, NothingNode, WhereNode,
 )
+from django.utils.deprecation import RemovedInDjango40Warning
 from django.utils.functional import cached_property
+from django.utils.regex_helper import _lazy_re_compile
 from django.utils.tree import Node

 __all__ = ['Query', 'RawQuery']
+
+# Quotation marks ('"`[]), whitespace characters, semicolons, or inline
+# SQL comments are forbidden in column aliases.
+FORBIDDEN_ALIAS_PATTERN = _lazy_re_compile(r"['`\"\]\[;\s]|--|/\*|\*/")
+
+# Inspired from
+# https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS
+EXPLAIN_OPTIONS_PATTERN = _lazy_re_compile(r"[\w\-]+")


 def get_field_names_from_opts(opts):
@@ -62,17 +76,11 @@
 )


-def _get_col(target, field, alias, simple_col):
-    if simple_col:
-        return SimpleCol(target, field)
-    return target.get_col(alias, field)
-
-
 class RawQuery:
     """A single raw SQL query."""

-    def __init__(self, sql, using, params=None):
-        self.params = params or ()
+    def __init__(self, sql, using, params=()):
+        self.params = params
         self.sql = sql
         self.using = using
         self.cursor = None
@@ -113,9 +121,13 @@

     @property
     def params_type(self):
+        if self.params is None:
+            return None
         return dict if isinstance(self.params, Mapping) else tuple

     def __str__(self):
+        if self.params_type is None:
+            return self.sql
         return self.sql % self.params_type(self.params)

     def _execute_query(self):
@@ -129,6 +141,8 @@
             params = tuple(adapter(val) for val in self.params)
         elif params_type is dict:
             params = {key: adapter(val) for key, val in self.params.items()}
+        elif params_type is None:
+            params = None
         else:
             raise RuntimeError("Unexpected params type: %s" % params_type)

@@ -136,7 +150,7 @@
         self.cursor.execute(self.sql, params)


-class Query:
+class Query(BaseExpression):
     """A single SQL query."""

     alias_prefix = 'T'
@@ -144,7 +158,7 @@

     compiler = 'SQLCompiler'

-    def __init__(self, model, where=WhereNode):
+    def __init__(self, model, where=WhereNode, alias_cols=True):
         self.model = model
         self.alias_refcount = {}
         # alias_map is the most important data structure regarding joins.
@@ -152,11 +166,14 @@
         # types they are. The key is the alias of the joined table (possibly
         # the table name) and the value is a Join-like object (see
         # sql.datastructures.Join for more information).
-        self.alias_map = OrderedDict()
+        self.alias_map = {}
+        # Whether to provide alias to columns during reference resolving.
+        self.alias_cols = alias_cols
         # Sometimes the query contains references to aliases in outer queries (as
         # a result of split_exclude). Correct alias quoting needs to know these
         # aliases too.
-        self.external_aliases = set()
+        # Map external tables to whether they are aliased.
+        self.external_aliases = {}
         self.table_map = {}     # Maps table names to list of aliases.
         self.default_cols = True
         self.default_ordering = True
@@ -189,6 +206,7 @@
         self.select_for_update_nowait = False
         self.select_for_update_skip_locked = False
         self.select_for_update_of = ()
+        self.select_for_no_key_update = False

         self.select_related = False
         # Arbitrary limit for select_related to prevents infinite recursion.
@@ -199,10 +217,7 @@
         self.values_select = ()

         # SQL annotation-related attributes
-        # The _annotations will be an OrderedDict when used. Due to the cost
-        # of creating OrderedDict this attribute is created lazily (in
-        # self.annotations property).
-        self._annotations = None  # Maps alias -> Annotation Expression
+        self.annotations = {}  # Maps alias -> Annotation Expression
         self.annotation_select_mask = None
         self._annotation_select_cache = None

@@ -213,9 +228,7 @@

         # These are for extensions. The contents are more or less appended
         # verbatim to the appropriate clause.
-        # The _extra attribute is an OrderedDict, lazily created similarly to
-        # .annotations
-        self._extra = None  # Maps col_alias -> (col_sql, params).
+        self.extra = {}  # Maps col_alias -> (col_sql, params).
         self.extra_select_mask = None
         self._extra_select_cache = None

@@ -234,16 +247,12 @@
         self.explain_options = {}

     @property
-    def extra(self):
-        if self._extra is None:
-            self._extra = OrderedDict()
-        return self._extra
-
-    @property
-    def annotations(self):
-        if self._annotations is None:
-            self._annotations = OrderedDict()
-        return self._annotations
+    def output_field(self):
+        if len(self.select) == 1:
+            select = self.select[0]
+            return getattr(select, 'target', None) or select.field
+        elif len(self.annotation_select) == 1:
+            return next(iter(self.annotation_select.values())).output_field

     @property
     def has_select_fields(self):
@@ -277,9 +286,6 @@
         result = self.clone()
         memo[id(self)] = result
         return result
-
-    def _prepare(self, field):
-        return self

     def get_compiler(self, using=None, connection=None):
         if using is None and connection is None:
@@ -311,18 +317,19 @@
         obj.external_aliases = self.external_aliases.copy()
         obj.table_map = self.table_map.copy()
         obj.where = self.where.clone()
-        obj._annotations = self._annotations.copy() if self._annotations is not None else None
+        obj.annotations = self.annotations.copy()
         if self.annotation_select_mask is None:
             obj.annotation_select_mask = None
         else:
             obj.annotation_select_mask = self.annotation_select_mask.copy()
+        obj.combined_queries = tuple(query.clone() for query in self.combined_queries)
         # _annotation_select_cache cannot be copied, as doing so breaks the
         # (necessary) state in which both annotations and
         # _annotation_select_cache point to the same underlying objects.
         # It will get re-populated in the cloned queryset the next time it's
         # used.
         obj._annotation_select_cache = None
-        obj._extra = self._extra.copy() if self._extra is not None else None
+        obj.extra = self.extra.copy()
         if self.extra_select_mask is None:
             obj.extra_select_mask = None
         else:
@@ -331,6 +338,10 @@
             obj._extra_select_cache = None
         else:
             obj._extra_select_cache = self._extra_select_cache.copy()
+        if self.select_related is not False:
+            # Use deepcopy because select_related stores fields in nested
+            # dicts.
+            obj.select_related = copy.deepcopy(obj.select_related)
         if 'subq_aliases' in self.__dict__:
             obj.subq_aliases = self.subq_aliases.copy()
         obj.used_aliases = self.used_aliases.copy()
@@ -361,6 +372,11 @@
         clone = self.clone()
         clone.change_aliases(change_map)
         return clone
+
+    def _get_col(self, target, field, alias):
+        if not self.alias_cols:
+            alias = None
+        return target.get_col(alias, field)

     def rewrite_cols(self, annotation, col_cnt):
         # We must make sure the inner query has the referred columns in it.
@@ -390,18 +406,26 @@
                 # before the contains_aggregate/is_summary condition below.
                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
                 new_exprs.append(new_expr)
-            elif isinstance(expr, Col) or (expr.contains_aggregate and not expr.is_summary):
-                # Reference to column. Make sure the referenced column
-                # is selected.
-                col_cnt += 1
-                col_alias = '__col%d' % col_cnt
-                self.annotations[col_alias] = expr
-                self.append_annotation_mask([col_alias])
-                new_exprs.append(Ref(col_alias, expr))
             else:
-                # Some other expression not referencing database values
-                # directly. Its subexpression might contain Cols.
-                new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
+                # Reuse aliases of expressions already selected in subquery.
+                for col_alias, selected_annotation in self.annotation_select.items():
+                    if selected_annotation is expr:
+                        new_expr = Ref(col_alias, expr)
+                        break
+                else:
+                    # An expression that is not selected the subquery.
+                    if isinstance(expr, Col) or (expr.contains_aggregate and not expr.is_summary):
+                        # Reference column or another aggregate. Select it
+                        # under a non-conflicting alias.
+                        col_cnt += 1
+                        col_alias = '__col%d' % col_cnt
+                        self.annotations[col_alias] = expr
+                        self.append_annotation_mask([col_alias])
+                        new_expr = Ref(col_alias, expr)
+                    else:
+                        # Some other expression not referencing database values
+                        # directly. Its subexpression might contain Cols.
+                        new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
                 new_exprs.append(new_expr)
         annotation.set_source_expressions(new_exprs)
         return annotation, col_cnt
@@ -412,12 +436,11 @@
         """
         if not self.annotation_select:
             return {}
-        has_limit = self.low_mark != 0 or self.high_mark is not None
-        has_existing_annotations = any(
+        existing_annotations = [
             annotation for alias, annotation
             in self.annotations.items()
             if alias not in added_aggregate_names
-        )
+        ]
         # Decide if we need to use a subquery.
         #
         # Existing annotations would cause incorrect results as get_aggregation()
@@ -429,14 +452,16 @@
         # those operations must be done in a subquery so that the query
         # aggregates on the limit and/or distinct results instead of applying
         # the distinct and limit after the aggregation.
-        if (isinstance(self.group_by, tuple) or has_limit or has_existing_annotations or
+        if (isinstance(self.group_by, tuple) or self.is_sliced or existing_annotations or
                 self.distinct or self.combinator):
             from django.db.models.sql.subqueries import AggregateQuery
-            outer_query = AggregateQuery(self.model)
             inner_query = self.clone()
+            inner_query.subquery = True
+            outer_query = AggregateQuery(self.model, inner_query)
             inner_query.select_for_update = False
             inner_query.select_related = False
-            if not has_limit and not self.distinct_fields:
+            inner_query.set_annotation_mask(self.annotation_select)
+            if not self.is_sliced and not self.distinct_fields:
                 # Queries with distinct_fields need ordering and when a limit
                 # is applied we must take the slice from the ordered query.
                 # Otherwise no need for ordering.
@@ -447,7 +472,11 @@
                 # query is grouped by the main model's primary key. However,
                 # clearing the select clause can alter results if distinct is
                 # used.
-                if inner_query.default_cols and has_existing_annotations:
+                has_existing_aggregate_annotations = any(
+                    annotation for annotation in existing_annotations
+                    if getattr(annotation, 'contains_aggregate', True)
+                )
+                if inner_query.default_cols and has_existing_aggregate_annotations:
                     inner_query.group_by = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)
                 inner_query.default_cols = False

@@ -457,10 +486,12 @@
             # and move them to the outer AggregateQuery.
             col_cnt = 0
             for alias, expression in list(inner_query.annotation_select.items()):
+                annotation_select_mask = inner_query.annotation_select_mask
                 if expression.is_summary:
                     expression, col_cnt = inner_query.rewrite_cols(expression, col_cnt)
                     outer_query.annotations[alias] = expression.relabeled_clone(relabels)
                     del inner_query.annotations[alias]
+                    annotation_select_mask.remove(alias)
                 # Make sure the annotation_select wont use cached results.
                 inner_query.set_annotation_mask(inner_query.annotation_select_mask)
             if inner_query.select == () and not inner_query.default_cols and not inner_query.annotation_select_mask:
@@ -468,18 +499,11 @@
                 # field selected in the inner query, yet we must use a subquery.
                 # So, make sure at least one field is selected.
                 inner_query.select = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)
-            try:
-                outer_query.add_subquery(inner_query, using)
-            except EmptyResultSet:
-                return {
-                    alias: None
-                    for alias in outer_query.annotation_select
-                }
         else:
             outer_query = self
             self.select = ()
             self.default_cols = False
-            self._extra = {}
+            self.extra = {}

         outer_query.clear_ordering(True)
         outer_query.clear_limits()
@@ -509,20 +533,41 @@
     def has_filters(self):
         return self.where

-    def has_results(self, using):
+    def exists(self, using, limit=True):
         q = self.clone()
         if not q.distinct:
             if q.group_by is True:
                 q.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
-                q.set_group_by()
+                # Disable GROUP BY aliases to avoid orphaning references to the
+                # SELECT clause which is about to be cleared.
+                q.set_group_by(allow_aliases=False)
             q.clear_select_clause()
+        if q.combined_queries and q.combinator == 'union':
+            limit_combined = connections[using].features.supports_slicing_ordering_in_compound
+            q.combined_queries = tuple(
+                combined_query.exists(using, limit=limit_combined)
+                for combined_query in q.combined_queries
+            )
         q.clear_ordering(True)
-        q.set_limits(high=1)
+        if limit:
+            q.set_limits(high=1)
+        q.add_extra({'a': 1}, None, None, None, None, None)
+        q.set_extra_mask(['a'])
+        return q
+
+    def has_results(self, using):
+        q = self.exists(using)
         compiler = q.get_compiler(using=using)
         return compiler.has_results()

     def explain(self, using, format=None, **options):
         q = self.clone()
+        for option_name in options:
+            if (
+                not EXPLAIN_OPTIONS_PATTERN.fullmatch(option_name) or
+                "--" in option_name
+            ):
+                raise ValueError(f"Invalid option name: {option_name!r}.")
         q.explain_query = True
         q.explain_format = format
         q.explain_options = options
@@ -540,7 +585,7 @@
         """
         assert self.model == rhs.model, \
             "Cannot combine queries on two different base models."
-        assert self.can_filter(), \
+        assert not self.is_sliced, \
             "Cannot combine queries once a slice has been taken."
         assert self.distinct == rhs.distinct, \
             "Cannot combine a unique query with a non-unique query."
@@ -597,6 +642,10 @@
         joinpromoter.add_votes(rhs_votes)
         joinpromoter.update_join_types(self)

+        # Combine subqueries aliases to ensure aliases relabelling properly
+        # handle subqueries when combining where and select clauses.
+        self.subq_aliases |= rhs.subq_aliases
+
         # Now relabel a copy of the rhs where-clause and add it to the current
         # one.
         w = rhs.where.clone()
@@ -613,7 +662,7 @@
             # It would be nice to be able to handle this, but the queries don't
             # really make sense (or return consistent value sets). Not worth
             # the extra complexity when you can write a real query instead.
-            if self._extra and rhs._extra:
+            if self.extra and rhs.extra:
                 raise ValueError("When merging querysets using 'or', you cannot have extra(select=...) on both sides.")
         self.extra.update(rhs.extra)
         extra_select_mask = set()
@@ -825,9 +874,9 @@
         if isinstance(self.group_by, tuple):
             self.group_by = tuple([col.relabeled_clone(change_map) for col in self.group_by])
         self.select = tuple([col.relabeled_clone(change_map) for col in self.select])
-        self._annotations = self._annotations and OrderedDict(
-            (key, col.relabeled_clone(change_map)) for key, col in self._annotations.items()
-        )
+        self.annotations = self.annotations and {
+            key: col.relabeled_clone(change_map) for key, col in self.annotations.items()
+        }

         # 2. Rename the alias in the internal table/alias datastructures.
         for old_alias, new_alias in change_map.items():
@@ -844,8 +893,11 @@
                 if alias == old_alias:
                     table_aliases[pos] = new_alias
                     break
-        self.external_aliases = {change_map.get(alias, alias)
-                                 for alias in self.external_aliases}
+        self.external_aliases = {
+            # Table is aliased or it's being changed and thus is aliased.
+            change_map.get(alias, alias): (aliased or alias in change_map)
+            for alias, aliased in self.external_aliases.items()
+        }

     def bump_prefix(self, outer_query):
         """
@@ -876,22 +928,25 @@
             # No clashes between self and outer query should be possible.
             return

-        local_recursion_limit = 127  # explicitly avoid infinite loop
+        # Explicitly avoid infinite loop. The constant divider is based on how
+        # much depth recursive subquery references add to the stack. This value
+        # might need to be adjusted when adding or removing function calls from
+        # the code path in charge of performing these operations.
+        local_recursion_limit = sys.getrecursionlimit() // 16
         for pos, prefix in enumerate(prefix_gen()):
             if prefix not in self.subq_aliases:
                 self.alias_prefix = prefix
                 break
             if pos > local_recursion_limit:
-                raise RuntimeError(
+                raise RecursionError(
                     'Maximum recursion depth exceeded: too many subqueries.'
                 )
         self.subq_aliases = self.subq_aliases.union([self.alias_prefix])
         outer_query.subq_aliases = outer_query.subq_aliases.union(self.subq_aliases)
-        change_map = OrderedDict()
-        for pos, alias in enumerate(self.alias_map):
-            new_alias = '%s%d' % (self.alias_prefix, pos)
-            change_map[alias] = new_alias
-        self.change_aliases(change_map)
+        self.change_aliases({
+            alias: '%s%d' % (self.alias_prefix, pos)
+            for pos, alias in enumerate(self.alias_map)
+        })

     def get_initial_alias(self):
         """
@@ -994,11 +1049,22 @@
             alias = seen[int_model] = join_info.joins[-1]
         return alias or seen[None]

-    def add_annotation(self, annotation, alias, is_summary=False):
+    def check_alias(self, alias):
+        if FORBIDDEN_ALIAS_PATTERN.search(alias):
+            raise ValueError(
+                "Column aliases cannot contain whitespace characters, quotation marks, "
+                "semicolons, or SQL comments."
+            )
+
+    def add_annotation(self, annotation, alias, is_summary=False, select=True):
         """Add a single annotation expression to the Query."""
+        self.check_alias(alias)
         annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,
                                                    summarize=is_summary)
-        self.append_annotation_mask([alias])
+        if select:
+            self.append_annotation_mask([alias])
+        else:
+            self.set_annotation_mask(set(self.annotation_select).difference({alias}))
         self.annotations[alias] = annotation

     def resolve_expression(self, query, *args, **kwargs):
@@ -1012,29 +1078,49 @@
                 not self.distinct_fields and
                 not self.select_for_update):
             clone.clear_ordering(True)
+        clone.where.resolve_expression(query, *args, **kwargs)
+        for key, value in clone.annotations.items():
+            resolved = value.resolve_expression(query, *args, **kwargs)
+            if hasattr(resolved, 'external_aliases'):
+                resolved.external_aliases.update(clone.external_aliases)
+            clone.annotations[key] = resolved
+        # Outer query's aliases are considered external.
+        for alias, table in query.alias_map.items():
+            clone.external_aliases[alias] = (
+                (isinstance(table, Join) and table.join_field.related_model._meta.db_table != alias) or
+                (isinstance(table, BaseTable) and table.table_name != table.table_alias)
+            )
         return clone

+    def get_external_cols(self):
+        exprs = chain(self.annotations.values(), self.where.children)
+        return [
+            col for col in self._gen_cols(exprs, include_external=True)
+            if col.alias in self.external_aliases
+        ]
+
     def as_sql(self, compiler, connection):
-        return self.get_compiler(connection=connection).as_sql()
-
-    def resolve_lookup_value(self, value, can_reuse, allow_joins, simple_col):
+        sql, params = self.get_compiler(connection=connection).as_sql()
+        if self.subquery:
+            sql = '(%s)' % sql
+        return sql, params
+
+    def resolve_lookup_value(self, value, can_reuse, allow_joins):
         if hasattr(value, 'resolve_expression'):
-            kwargs = {'reuse': can_reuse, 'allow_joins': allow_joins}
-            if isinstance(value, F):
-                kwargs['simple_col'] = simple_col
-            value = value.resolve_expression(self, **kwargs)
+            value = value.resolve_expression(
+                self, reuse=can_reuse, allow_joins=allow_joins,
+            )
         elif isinstance(value, (list, tuple)):
             # The items of the iterable may be expressions and therefore need
             # to be resolved independently.
-            for sub_value in value:
-                if hasattr(sub_value, 'resolve_expression'):
-                    if isinstance(sub_value, F):
-                        sub_value.resolve_expression(
-                            self, reuse=can_reuse, allow_joins=allow_joins,
-                            simple_col=simple_col,
-                        )
-                    else:
-                        sub_value.resolve_expression(self, reuse=can_reuse, allow_joins=allow_joins)
+            values = (
+                self.resolve_lookup_value(sub_value, can_reuse, allow_joins)
+                for sub_value in value
+            )
+            type_ = type(value)
+            if hasattr(type_, '_make'):  # namedtuple
+                return type_(*values)
+            return type_(values)
         return value

     def solve_lookup_type(self, lookup):
@@ -1042,7 +1128,7 @@
         Solve the lookup type from the lookup (e.g.: 'foobar__id__icontains').
         """
         lookup_splitted = lookup.split(LOOKUP_SEP)
-        if self._annotations:
+        if self.annotations:
             expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)
             if expression:
                 return expression_lookups, (), expression
@@ -1086,6 +1172,20 @@
                 for v in value:
                     self.check_query_object_type(v, opts, field)

+    def check_filterable(self, expression):
+        """Raise an error if expression cannot be used in a WHERE clause."""
+        if (
+            hasattr(expression, 'resolve_expression') and
+            not getattr(expression, 'filterable', True)
+        ):
+            raise NotSupportedError(
+                expression.__class__.__name__ + ' is disallowed in the filter '
+                'clause.'
+            )
+        if hasattr(expression, 'get_source_expressions'):
+            for expr in expression.get_source_expressions():
+                self.check_filterable(expr)
+
     def build_lookup(self, lookups, lhs, rhs):
         """
         Try to extract transforms and lookup from given lhs.
@@ -1153,7 +1253,7 @@

     def build_filter(self, filter_expr, branch_negated=False, current_negated=False,
                      can_reuse=None, allow_joins=True, split_subq=True,
-                     reuse_with_filtered_relation=False, simple_col=False):
+                     reuse_with_filtered_relation=False, check_filterable=True):
         """
         Build a WhereNode for a single filter clause but don't add it
         to this Query. Query.add_q() will then add this filter to the where
@@ -1184,23 +1284,42 @@
         """
         if isinstance(filter_expr, dict):
             raise FieldError("Cannot parse keyword query as dict")
+        if isinstance(filter_expr, Q):
+            return self._add_q(
+                filter_expr,
+                branch_negated=branch_negated,
+                current_negated=current_negated,
+                used_aliases=can_reuse,
+                allow_joins=allow_joins,
+                split_subq=split_subq,
+                check_filterable=check_filterable,
+            )
+        if hasattr(filter_expr, 'resolve_expression'):
+            if not getattr(filter_expr, 'conditional', False):
+                raise TypeError('Cannot filter against a non-conditional expression.')
+            condition = self.build_lookup(
+                ['exact'], filter_expr.resolve_expression(self, allow_joins=allow_joins), True
+            )
+            clause = self.where_class()
+            clause.add(condition, AND)
+            return clause, []
         arg, value = filter_expr
         if not arg:
             raise FieldError("Cannot parse keyword query %r" % arg)
         lookups, parts, reffed_expression = self.solve_lookup_type(arg)

-        if not getattr(reffed_expression, 'filterable', True):
-            raise NotSupportedError(
-                reffed_expression.__class__.__name__ + ' is disallowed in '
-                'the filter clause.'
-            )
+        if check_filterable:
+            self.check_filterable(reffed_expression)

         if not allow_joins and len(parts) > 1:
             raise FieldError("Joined field references are not permitted in this query")

         pre_joins = self.alias_refcount.copy()
-        value = self.resolve_lookup_value(value, can_reuse, allow_joins, simple_col)
+        value = self.resolve_lookup_value(value, can_reuse, allow_joins)
         used_joins = {k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)}
+
+        if check_filterable:
+            self.check_filterable(value)

         clause = self.where_class()
         if reffed_expression:
@@ -1242,11 +1361,11 @@
             if num_lookups > 1:
                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
             if len(targets) == 1:
-                col = _get_col(targets[0], join_info.final_field, alias, simple_col)
+                col = self._get_col(targets[0], join_info.final_field, alias)
             else:
                 col = MultiColSource(alias, targets, join_info.targets, join_info.final_field)
         else:
-            col = _get_col(targets[0], join_info.final_field, alias, simple_col)
+            col = self._get_col(targets[0], join_info.final_field, alias)

         condition = self.build_lookup(lookups, col, value)
         lookup_type = condition.lookup_name
@@ -1255,9 +1374,7 @@
         require_outer = lookup_type == 'isnull' and condition.rhs is True and not current_negated
         if current_negated and (lookup_type != 'isnull' or condition.rhs is False) and condition.rhs is not None:
             require_outer = True
-            if (lookup_type != 'isnull' and (
-                    self.is_nullable(targets[0]) or
-                    self.alias_map[join_list[-1]].join_type == LOUTER)):
+            if lookup_type != 'isnull':
                 # The condition added here will be SQL like this:
                 # NOT (col IS NOT NULL), where the first NOT is added in
                 # upper layers of code. The reason for addition is that if col
@@ -1267,9 +1384,18 @@
                 # (col IS NULL OR col != someval)
                 #   <=>
                 # NOT (col IS NOT NULL AND col = someval).
-                lookup_class = targets[0].get_lookup('isnull')
-                col = _get_col(targets[0], join_info.targets[0], alias, simple_col)
-                clause.add(lookup_class(col, False), AND)
+                if (
+                    self.is_nullable(targets[0]) or
+                    self.alias_map[join_list[-1]].join_type == LOUTER
+                ):
+                    lookup_class = targets[0].get_lookup('isnull')
+                    col = self._get_col(targets[0], join_info.targets[0], alias)
+                    clause.add(lookup_class(col, False), AND)
+                # If someval is a nullable column, someval IS NOT NULL is
+                # added.
+                if isinstance(value, Col) and self.is_nullable(value.target):
+                    lookup_class = value.target.get_lookup('isnull')
+                    clause.add(lookup_class(value, False), AND)
         return clause, used_joins if not require_outer else ()

     def add_filter(self, filter_clause):
@@ -1292,12 +1418,12 @@
             self.where.add(clause, AND)
         self.demote_joins(existing_inner)

-    def build_where(self, q_object):
-        return self._add_q(q_object, used_aliases=set(), allow_joins=False, simple_col=True)[0]
+    def build_where(self, filter_expr):
+        return self.build_filter(filter_expr, allow_joins=False)[0]

     def _add_q(self, q_object, used_aliases, branch_negated=False,
                current_negated=False, allow_joins=True, split_subq=True,
-               simple_col=False):
+               check_filterable=True):
         """Add a Q-object to the current filter."""
         connector = q_object.connector
         current_negated = current_negated ^ q_object.negated
@@ -1306,18 +1432,12 @@
                                          negated=q_object.negated)
         joinpromoter = JoinPromoter(q_object.connector, len(q_object.children), current_negated)
         for child in q_object.children:
-            if isinstance(child, Node):
-                child_clause, needed_inner = self._add_q(
-                    child, used_aliases, branch_negated,
-                    current_negated, allow_joins, split_subq)
-                joinpromoter.add_votes(needed_inner)
-            else:
-                child_clause, needed_inner = self.build_filter(
-                    child, can_reuse=used_aliases, branch_negated=branch_negated,
-                    current_negated=current_negated, allow_joins=allow_joins,
-                    split_subq=split_subq, simple_col=simple_col,
-                )
-                joinpromoter.add_votes(needed_inner)
+            child_clause, needed_inner = self.build_filter(
+                child, can_reuse=used_aliases, branch_negated=branch_negated,
+                current_negated=current_negated, allow_joins=allow_joins,
+                split_subq=split_subq, check_filterable=check_filterable,
+            )
+            joinpromoter.add_votes(needed_inner)
             if child_clause:
                 target_clause.add(child_clause, connector)
         needed_inner = joinpromoter.update_join_types(self)
@@ -1348,14 +1468,30 @@
     def add_filtered_relation(self, filtered_relation, alias):
         filtered_relation.alias = alias
         lookups = dict(get_children_from_q(filtered_relation.condition))
-        for lookup in chain((filtered_relation.relation_name,), lookups):
-            lookup_parts, field_parts, _ = self.solve_lookup_type(lookup)
+        relation_lookup_parts, relation_field_parts, _ = self.solve_lookup_type(filtered_relation.relation_name)
+        if relation_lookup_parts:
+            raise ValueError(
+                "FilteredRelation's relation_name cannot contain lookups "
+                "(got %r)." % filtered_relation.relation_name
+            )
+        for lookup in chain(lookups):
+            lookup_parts, lookup_field_parts, _ = self.solve_lookup_type(lookup)
             shift = 2 if not lookup_parts else 1
-            if len(field_parts) > (shift + len(lookup_parts)):
-                raise ValueError(
-                    "FilteredRelation's condition doesn't support nested "
-                    "relations (got %r)." % lookup
-                )
+            lookup_field_path = lookup_field_parts[:-shift]
+            for idx, lookup_field_part in enumerate(lookup_field_path):
+                if len(relation_field_parts) > idx:
+                    if relation_field_parts[idx] != lookup_field_part:
+                        raise ValueError(
+                            "FilteredRelation's condition doesn't support "
+                            "relations outside the %r (got %r)."
+                            % (filtered_relation.relation_name, lookup)
+                        )
+                else:
+                    raise ValueError(
+                        "FilteredRelation's condition doesn't support nested "
+                        "relations deeper than the relation_name (got %r for "
+                        "%r)." % (lookup, filtered_relation.relation_name)
+                    )
         self._filtered_relations[filtered_relation.alias] = filtered_relation

     def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):
@@ -1388,7 +1524,14 @@
                     field = self.annotation_select[name].output_field
                 elif name in self._filtered_relations and pos == 0:
                     filtered_relation = self._filtered_relations[name]
-                    field = opts.get_field(filtered_relation.relation_name)
+                    if LOOKUP_SEP in filtered_relation.relation_name:
+                        parts = filtered_relation.relation_name.split(LOOKUP_SEP)
+                        filtered_relation_path, field, _, _ = self.names_to_path(
+                            parts, opts, allow_many, fail_on_missing,
+                        )
+                        path.extend(filtered_relation_path[:-1])
+                    else:
+                        field = opts.get_field(filtered_relation.relation_name)
             if field is not None:
                 # Fields that contain one-to-many relations with a generic
                 # model (like a GenericForeignKey) cannot generate reverse
@@ -1493,6 +1636,8 @@
         # fields to the appropriate wrapped version.

         def final_transformer(field, alias):
+            if not self.alias_cols:
+                alias = None
             return field.get_col(alias)

         # Try resolving all the names as fields first. If there's an error,
@@ -1584,20 +1729,53 @@
             self.unref_alias(joins.pop())
         return targets, joins[-1], joins

-    def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False, simple_col=False):
-        if not allow_joins and LOOKUP_SEP in name:
-            raise FieldError("Joined field references are not permitted in this query")
-        if name in self.annotations:
+    @classmethod
+    def _gen_cols(cls, exprs, include_external=False):
+        for expr in exprs:
+            if isinstance(expr, Col):
+                yield expr
+            elif include_external and callable(getattr(expr, 'get_external_cols', None)):
+                yield from expr.get_external_cols()
+            else:
+                yield from cls._gen_cols(
+                    expr.get_source_expressions(),
+                    include_external=include_external,
+                )
+
+    @classmethod
+    def _gen_col_aliases(cls, exprs):
+        yield from (expr.alias for expr in cls._gen_cols(exprs))
+
+    def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False):
+        annotation = self.annotations.get(name)
+        if annotation is not None:
+            if not allow_joins:
+                for alias in self._gen_col_aliases([annotation]):
+                    if isinstance(self.alias_map[alias], Join):
+                        raise FieldError(
+                            'Joined field references are not permitted in '
+                            'this query'
+                        )
             if summarize:
                 # Summarize currently means we are doing an aggregate() query
                 # which is executed as a wrapped subquery if any of the
                 # aggregate() elements reference an existing annotation. In
                 # that case we need to return a Ref to the subquery's annotation.
+                if name not in self.annotation_select:
+                    raise FieldError(
+                        "Cannot aggregate over the '%s' alias. Use annotate() "
+                        "to promote it." % name
+                    )
                 return Ref(name, self.annotation_select[name])
             else:
-                return self.annotations[name]
+                return annotation
         else:
             field_list = name.split(LOOKUP_SEP)
+            annotation = self.annotations.get(field_list[0])
+            if annotation is not None:
+                for transform in field_list[1:]:
+                    annotation = self.try_transform(annotation, transform)
+                return annotation
             join_info = self.setup_joins(field_list, self.get_meta(), self.get_initial_alias(), can_reuse=reuse)
             targets, final_alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)
             if not allow_joins and len(join_list) > 1:
@@ -1607,11 +1785,10 @@
                                  "isn't supported")
             # Verify that the last lookup in name is a field or a transform:
             # transform_function() raises FieldError if not.
-            join_info.transform_function(targets[0], final_alias)
+            transform = join_info.transform_function(targets[0], final_alias)
             if reuse is not None:
                 reuse.update(join_list)
-            col = _get_col(targets[0], join_info.targets[0], join_list[-1], simple_col)
-            return col
+            return transform

     def split_exclude(self, filter_expr, can_reuse, names_with_path):
         """
@@ -1625,32 +1802,30 @@
         filters in the original query.

         We will turn this into equivalent of:
-            WHERE NOT (pk IN (SELECT parent_id FROM thetable
-                              WHERE name = 'foo' AND parent_id IS NOT NULL))
-
-        It might be worth it to consider using WHERE NOT EXISTS as that has
-        saner null handling, and is easier for the backend's optimizer to
-        handle.
-        """
+            WHERE NOT EXISTS(
+                SELECT 1
+                FROM child
+                WHERE name = 'foo' AND child.parent_id = parent.id
+                LIMIT 1
+            )
+        """
+        filter_lhs, filter_rhs = filter_expr
+        if isinstance(filter_rhs, OuterRef):
+            filter_expr = (filter_lhs, OuterRef(filter_rhs))
+        elif isinstance(filter_rhs, F):
+            filter_expr = (filter_lhs, OuterRef(filter_rhs.name))
         # Generate the inner query.
         query = Query(self.model)
+        query._filtered_relations = self._filtered_relations
         query.add_filter(filter_expr)
         query.clear_ordering(True)
         # Try to have as simple as possible subquery -> trim leading joins from
         # the subquery.
         trimmed_prefix, contains_louter = query.trim_start(names_with_path)

-        # Add extra check to make sure the selected field will not be null
-        # since we are adding an IN <subquery> clause. This prevents the
-        # database from tripping over IN (...,NULL,...) selects and returning
-        # nothing
         col = query.select[0]
         select_field = col.target
         alias = col.alias
-        if self.is_nullable(select_field):
-            lookup_class = select_field.get_lookup('isnull')
-            lookup = lookup_class(select_field.get_col(alias), False)
-            query.where.add(lookup, AND)
         if alias in can_reuse:
             pk = select_field.model._meta.pk
             # Need to add a restriction so that outer query's filters are in effect for
@@ -1662,11 +1837,13 @@
             lookup = lookup_class(pk.get_col(query.select[0].alias),
                                   pk.get_col(alias))
             query.where.add(lookup, AND)
-            query.external_aliases.add(alias)
-
-        condition, needed_inner = self.build_filter(
-            ('%s__in' % trimmed_prefix, query),
-            current_negated=True, branch_negated=True, can_reuse=can_reuse)
+            query.external_aliases[alias] = True
+
+        lookup_class = select_field.get_lookup('exact')
+        lookup = lookup_class(col, ResolvedOuterRef(trimmed_prefix))
+        query.where.add(lookup, AND)
+        condition, needed_inner = self.build_filter(Exists(query))
+
         if contains_louter:
             or_null_condition, _ = self.build_filter(
                 ('%s__isnull' % trimmed_prefix, True),
@@ -1682,6 +1859,8 @@

     def set_empty(self):
         self.where.add(NothingNode(), AND)
+        for query in self.combined_queries:
+            query.set_empty()

     def is_empty(self):
         return any(isinstance(c, NothingNode) for c in self.where.children)
@@ -1713,6 +1892,10 @@
         """Clear any existing limits."""
         self.low_mark, self.high_mark = 0, None

+    @property
+    def is_sliced(self):
+        return self.low_mark != 0 or self.high_mark is not None
+
     def has_limit_one(self):
         return self.high_mark is not None and (self.high_mark - self.low_mark) == 1

@@ -1722,7 +1905,7 @@

         Typically, this means no limits or offsets have been put on the results.
         """
-        return not self.low_mark and self.high_mark is None
+        return not self.is_sliced

     def clear_select_clause(self):
         """Remove all fields from SELECT clause."""
@@ -1740,6 +1923,10 @@
         """
         self.select = ()
         self.values_select = ()
+
+    def add_select_col(self, col, name):
+        self.select += col,
+        self.values_select += name,

     def set_select(self, cols):
         self.default_cols = False
@@ -1782,6 +1969,11 @@
                 # For lookups spanning over relationships, show the error
                 # from the model on which the lookup failed.
                 raise
+            elif name in self.annotations:
+                raise FieldError(
+                    "Cannot select the '%s' alias. Use annotate() to promote "
+                    "it." % name
+                )
             else:
                 names = sorted([
                     *get_field_names_from_opts(opts), *self.extra,
@@ -1801,7 +1993,28 @@
         """
         errors = []
         for item in ordering:
-            if not hasattr(item, 'resolve_expression') and not ORDER_PATTERN.match(item):
+            if isinstance(item, str):
+                if '.' in item and ORDER_PATTERN.match(item):
+                    warnings.warn(
+                        'Passing column raw column aliases to order_by() is '
+                        'deprecated. Wrap %r in a RawSQL expression before '
+                        'passing it to order_by().' % item,
+                        category=RemovedInDjango40Warning,
+                        stacklevel=3,
+                    )
+                    continue
+                if item == '?':
+                    continue
+                if item.startswith('-'):
+                    item = item[1:]
+                if item in self.annotations:
+                    continue
+                if self.extra and item in self.extra:
+                    continue
+                # names_to_path() validates the lookup. A descriptive
+                # FieldError will be raise if it's not.
+                self.names_to_path(item.split(LOOKUP_SEP), self.model._meta)
+            elif not hasattr(item, 'resolve_expression'):
                 errors.append(item)
             if getattr(item, 'contains_aggregate', False):
                 raise FieldError(
@@ -1825,7 +2038,7 @@
         if force_empty:
             self.default_ordering = False

-    def set_group_by(self):
+    def set_group_by(self, allow_aliases=True):
         """
         Expand the GROUP BY clause required by the query.

@@ -1834,11 +2047,36 @@
         primary key, and the query would be equivalent, the optimization
         will be made automatically.
         """
+        # Column names from JOINs to check collisions with aliases.
+        if allow_aliases:
+            column_names = set()
+            seen_models = set()
+            for join in list(self.alias_map.values())[1:]:  # Skip base table.
+                model = join.join_field.related_model
+                if model not in seen_models:
+                    column_names.update({
+                        field.column
+                        for field in model._meta.local_concrete_fields
+                    })
+                    seen_models.add(model)
+
         group_by = list(self.select)
         if self.annotation_select:
-            for annotation in self.annotation_select.values():
-                for col in annotation.get_group_by_cols():
-                    group_by.append(col)
+            for alias, annotation in self.annotation_select.items():
+                signature = inspect.signature(annotation.get_group_by_cols)
+                if 'alias' not in signature.parameters:
+                    annotation_class = annotation.__class__
+                    msg = (
+                        '`alias=None` must be added to the signature of '
+                        '%s.%s.get_group_by_cols().'
+                    ) % (annotation_class.__module__, annotation_class.__qualname__)
+                    warnings.warn(msg, category=RemovedInDjango40Warning)
+                    group_by_cols = annotation.get_group_by_cols()
+                else:
+                    if not allow_aliases or alias in column_names:
+                        alias = None
+                    group_by_cols = annotation.get_group_by_cols(alias=alias)
+                group_by.extend(group_by_cols)
         self.group_by = tuple(group_by)

     def add_select_related(self, fields):
@@ -1867,12 +2105,13 @@
             # dictionary with their parameters in 'select_params' so that
             # subsequent updates to the select dictionary also adjust the
             # parameters appropriately.
-            select_pairs = OrderedDict()
+            select_pairs = {}
             if select_params:
                 param_iter = iter(select_params)
             else:
                 param_iter = iter([])
             for name, entry in select.items():
+                self.check_alias(name)
                 entry = str(entry)
                 entry_params = []
                 pos = entry.find("%s")
@@ -1881,7 +2120,6 @@
                         entry_params.append(next(param_iter))
                     pos = entry.find("%s", pos + 2)
                 select_pairs[name] = (entry, entry_params)
-            # This is order preserving, since self.extra_select is an OrderedDict.
             self.extra.update(select_pairs)
         if where or params:
             self.where.add(ExtraWhere(where, params), AND)
@@ -1989,16 +2227,11 @@
         self.clear_deferred_loading()
         self.clear_select_fields()

-        if self.group_by is True:
-            self.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
-            self.set_group_by()
-            self.clear_select_fields()
-
         if fields:
             field_names = []
             extra_names = []
             annotation_names = []
-            if not self._extra and not self._annotations:
+            if not self.extra and not self.annotations:
                 # Shortcut - if there are no extra or annotations, then
                 # the values() clause must be just field names.
                 field_names = list(fields)
@@ -2013,8 +2246,27 @@
                         field_names.append(f)
             self.set_extra_mask(extra_names)
             self.set_annotation_mask(annotation_names)
+            selected = frozenset(field_names + extra_names + annotation_names)
         else:
             field_names = [f.attname for f in self.model._meta.concrete_fields]
+            selected = frozenset(field_names)
+        # Selected annotations must be known before setting the GROUP BY
+        # clause.
+        if self.group_by is True:
+            self.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
+            # Disable GROUP BY aliases to avoid orphaning references to the
+            # SELECT clause which is about to be cleared.
+            self.set_group_by(allow_aliases=False)
+            self.clear_select_fields()
+        elif self.group_by:
+            # Resolve GROUP BY annotation references if they are not part of
+            # the selected fields anymore.
+            group_by = []
+            for expr in self.group_by:
+                if isinstance(expr, Ref) and expr.refs not in selected:
+                    expr = self.annotations[expr.refs]
+                group_by.append(expr)
+            self.group_by = tuple(group_by)

         self.values_select = tuple(field_names)
         self.add_fields(field_names, True)
@@ -2022,18 +2274,18 @@
     @property
     def annotation_select(self):
         """
-        Return the OrderedDict of aggregate columns that are not masked and
+        Return the dictionary of aggregate columns that are not masked and
         should be used in the SELECT clause. Cache this result for performance.
         """
         if self._annotation_select_cache is not None:
             return self._annotation_select_cache
-        elif not self._annotations:
+        elif not self.annotations:
             return {}
         elif self.annotation_select_mask is not None:
-            self._annotation_select_cache = OrderedDict(
-                (k, v) for k, v in self.annotations.items()
+            self._annotation_select_cache = {
+                k: v for k, v in self.annotations.items()
                 if k in self.annotation_select_mask
-            )
+            }
             return self._annotation_select_cache
         else:
             return self.annotations
@@ -2042,13 +2294,13 @@
     def extra_select(self):
         if self._extra_select_cache is not None:
             return self._extra_select_cache
-        if not self._extra:
+        if not self.extra:
             return {}
         elif self.extra_select_mask is not None:
-            self._extra_select_cache = OrderedDict(
-                (k, v) for k, v in self.extra.items()
+            self._extra_select_cache = {
+                k: v for k, v in self.extra.items()
                 if k in self.extra_select_mask
-            )
+            }
             return self._extra_select_cache
         else:
             return self.extra
@@ -2098,9 +2350,13 @@
             join_field.foreign_related_fields[0].name)
         trimmed_prefix = LOOKUP_SEP.join(trimmed_prefix)
         # Lets still see if we can trim the first join from the inner query
-        # (that is, self). We can't do this for LEFT JOINs because we would
-        # miss those rows that have nothing on the outer side.
-        if self.alias_map[lookup_tables[trimmed_paths + 1]].join_type != LOUTER:
+        # (that is, self). We can't do this for:
+        # - LEFT JOINs because we would miss those rows that have nothing on
+        #   the outer side,
+        # - INNER JOINs from filtered relations because we would miss their
+        #   filters.
+        first_join = self.alias_map[lookup_tables[trimmed_paths + 1]]
+        if first_join.join_type != LOUTER and not first_join.filtered_relation:
             select_fields = [r[0] for r in join_field.related_fields]
             select_alias = lookup_tables[trimmed_paths + 1]
             self.unref_alias(lookup_tables[trimmed_paths])
('django/db/models/sql', 'subqueries.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -3,7 +3,6 @@
 """

 from django.core.exceptions import FieldError
-from django.db import connections
 from django.db.models.query_utils import Q
 from django.db.models.sql.constants import (
     CURSOR, GET_ITERATOR_CHUNK_SIZE, NO_RESULTS,
@@ -22,7 +21,10 @@
         self.alias_map = {table: self.alias_map[table]}
         self.where = where
         cursor = self.get_compiler(using).execute_sql(CURSOR)
-        return cursor.rowcount if cursor else 0
+        if cursor:
+            with cursor:
+                return cursor.rowcount
+        return 0

     def delete_batch(self, pk_list, using):
         """
@@ -40,40 +42,6 @@
                 **{field.attname + '__in': pk_list[offset:offset + GET_ITERATOR_CHUNK_SIZE]}))
             num_deleted += self.do_query(self.get_meta().db_table, self.where, using=using)
         return num_deleted
-
-    def delete_qs(self, query, using):
-        """
-        Delete the queryset in one SQL query (if possible). For simple queries
-        this is done by copying the query.query.where to self.query, for
-        complex queries by using subquery.
-        """
-        innerq = query.query
-        # Make sure the inner query has at least one table in use.
-        innerq.get_initial_alias()
-        # The same for our new query.
-        self.get_initial_alias()
-        innerq_used_tables = tuple([t for t in innerq.alias_map if innerq.alias_refcount[t]])
-        if not innerq_used_tables or innerq_used_tables == tuple(self.alias_map):
-            # There is only the base table in use in the query.
-            self.where = innerq.where
-        else:
-            pk = query.model._meta.pk
-            if not connections[using].features.update_can_self_select:
-                # We can't do the delete using subquery.
-                values = list(query.values_list('pk', flat=True))
-                if not values:
-                    return 0
-                return self.delete_batch(values, using)
-            else:
-                innerq.clear_select_clause()
-                innerq.select = [
-                    pk.get_col(self.get_initial_alias())
-                ]
-                values = innerq
-            self.where = self.where_class()
-            self.add_q(Q(pk__in=values))
-        cursor = self.get_compiler(using).execute_sql(CURSOR)
-        return cursor.rowcount if cursor else 0


 class UpdateQuery(Query):
@@ -189,6 +157,6 @@

     compiler = 'SQLAggregateCompiler'

-    def add_subquery(self, query, using):
-        query.subquery = True
-        self.subquery, self.sub_params = query.get_compiler(using).as_sql(with_col_aliases=True)
+    def __init__(self, model, inner_query):
+        self.inner_query = inner_query
+        super().__init__(model)
('django/db/models/sql', 'where.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -114,7 +114,7 @@
                 sql_string = '(%s)' % sql_string
         return sql_string, result_params

-    def get_group_by_cols(self):
+    def get_group_by_cols(self, alias=None):
         cols = []
         for child in self.children:
             cols.extend(child.get_group_by_cols())
@@ -159,6 +159,9 @@
         clone.relabel_aliases(change_map)
         return clone

+    def copy(self):
+        return self.clone()
+
     @classmethod
     def _contains_aggregate(cls, obj):
         if isinstance(obj, tree.Node):
@@ -183,8 +186,25 @@
     def is_summary(self):
         return any(child.is_summary for child in self.children)

+    @staticmethod
+    def _resolve_leaf(expr, query, *args, **kwargs):
+        if hasattr(expr, 'resolve_expression'):
+            expr = expr.resolve_expression(query, *args, **kwargs)
+        return expr
+
+    @classmethod
+    def _resolve_node(cls, node, query, *args, **kwargs):
+        if hasattr(node, 'children'):
+            for child in node.children:
+                cls._resolve_node(child, query, *args, **kwargs)
+        if hasattr(node, 'lhs'):
+            node.lhs = cls._resolve_leaf(node.lhs, query, *args, **kwargs)
+        if hasattr(node, 'rhs'):
+            node.rhs = cls._resolve_leaf(node.rhs, query, *args, **kwargs)
+
     def resolve_expression(self, *args, **kwargs):
         clone = self.clone()
+        clone._resolve_node(clone, *args, **kwargs)
         clone.resolved = True
         return clone

('django/db/models/sql', 'constants.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,8 +1,7 @@
 """
 Constants specific to the SQL storage portion of the ORM.
 """
-
-import re
+from django.utils.regex_helper import _lazy_re_compile

 # Size of each "chunk" for get_iterator calls.
 # Larger values are slightly faster at the expense of more storage space.
@@ -16,11 +15,11 @@
 CURSOR = 'cursor'
 NO_RESULTS = 'no results'

-ORDER_PATTERN = re.compile(r'\?|[-+]?[.\w]+$')
 ORDER_DIR = {
     'ASC': ('ASC', 'DESC'),
     'DESC': ('DESC', 'ASC'),
 }
+ORDER_PATTERN = _lazy_re_compile(r'[-+]?[.\w]+$')

 # SQL join types.
 INNER = 'INNER JOIN'
('django/db/models/sql', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,7 +1,6 @@
-from django.core.exceptions import EmptyResultSet
 from django.db.models.sql.query import *  # NOQA
 from django.db.models.sql.query import Query
 from django.db.models.sql.subqueries import *  # NOQA
 from django.db.models.sql.where import AND, OR

-__all__ = ['Query', 'AND', 'OR', 'EmptyResultSet']
+__all__ = ['Query', 'AND', 'OR']
('django/db/models/sql', 'datastructures.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,8 +2,6 @@
 Useful auxiliary data structures for query construction. Not useful outside
 the SQL domain.
 """
-# for backwards-compatibility in Django 1.11
-from django.core.exceptions import EmptyResultSet  # NOQA: F401
 from django.db.models.sql.constants import INNER, LOUTER


@@ -116,17 +114,28 @@
             self.join_field, self.nullable, filtered_relation=filtered_relation,
         )

-    def equals(self, other, with_filtered_relation):
+    @property
+    def identity(self):
         return (
-            isinstance(other, self.__class__) and
-            self.table_name == other.table_name and
-            self.parent_alias == other.parent_alias and
-            self.join_field == other.join_field and
-            (not with_filtered_relation or self.filtered_relation == other.filtered_relation)
+            self.__class__,
+            self.table_name,
+            self.parent_alias,
+            self.join_field,
+            self.filtered_relation,
         )

     def __eq__(self, other):
-        return self.equals(other, with_filtered_relation=True)
+        if not isinstance(other, Join):
+            return NotImplemented
+        return self.identity == other.identity
+
+    def __hash__(self):
+        return hash(self.identity)
+
+    def equals(self, other, with_filtered_relation):
+        if with_filtered_relation:
+            return self == other
+        return self.identity[:-1] == other.identity[:-1]

     def demote(self):
         new = self.relabeled_clone({})
@@ -162,9 +171,17 @@
     def relabeled_clone(self, change_map):
         return self.__class__(self.table_name, change_map.get(self.table_alias, self.table_alias))

+    @property
+    def identity(self):
+        return self.__class__, self.table_name, self.table_alias
+
+    def __eq__(self, other):
+        if not isinstance(other, BaseTable):
+            return NotImplemented
+        return self.identity == other.identity
+
+    def __hash__(self):
+        return hash(self.identity)
+
     def equals(self, other, with_filtered_relation):
-        return (
-            isinstance(self, other.__class__) and
-            self.table_name == other.table_name and
-            self.table_alias == other.table_alias
-        )
+        return self.identity == other.identity
('django/views', 'csrf.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -60,14 +60,14 @@

   <p>In general, this can occur when there is a genuine Cross Site Request Forgery, or when
   <a
-  href="https://docs.djangoproject.com/en/{{ docs_version }}/ref/csrf/">Django's
+  href="https://docs.djangoproject.com/en/{{ docs_version }}/ref/csrf/">Django’s
   CSRF mechanism</a> has not been used correctly.  For POST forms, you need to
   ensure:</p>

   <ul>
     <li>Your browser is accepting cookies.</li>

-    <li>The view function passes a <code>request</code> to the template's <a
+    <li>The view function passes a <code>request</code> to the template’s <a
     href="https://docs.djangoproject.com/en/dev/topics/templates/#django.template.backends.base.Template.render"><code>render</code></a>
     method.</li>

@@ -84,7 +84,7 @@
     page with the form, because the token is rotated after a login.</li>
   </ul>

-  <p>You're seeing the help section of this page because you have <code>DEBUG =
+  <p>You’re seeing the help section of this page because you have <code>DEBUG =
   True</code> in your Django settings file. Change that to <code>False</code>,
   and only the initial error message will be displayed.  </p>

@@ -105,28 +105,28 @@
     """
     Default view used when request fails CSRF protection
     """
-    from django.middleware.csrf import REASON_NO_REFERER, REASON_NO_CSRF_COOKIE
+    from django.middleware.csrf import REASON_NO_CSRF_COOKIE, REASON_NO_REFERER
     c = {
         'title': _("Forbidden"),
         'main': _("CSRF verification failed. Request aborted."),
         'reason': reason,
         'no_referer': reason == REASON_NO_REFERER,
         'no_referer1': _(
-            "You are seeing this message because this HTTPS site requires a "
-            "'Referer header' to be sent by your Web browser, but none was "
-            "sent. This header is required for security reasons, to ensure "
-            "that your browser is not being hijacked by third parties."),
+            'You are seeing this message because this HTTPS site requires a '
+            '“Referer header” to be sent by your Web browser, but none was '
+            'sent. This header is required for security reasons, to ensure '
+            'that your browser is not being hijacked by third parties.'),
         'no_referer2': _(
-            "If you have configured your browser to disable 'Referer' headers, "
-            "please re-enable them, at least for this site, or for HTTPS "
-            "connections, or for 'same-origin' requests."),
+            'If you have configured your browser to disable “Referer” headers, '
+            'please re-enable them, at least for this site, or for HTTPS '
+            'connections, or for “same-origin” requests.'),
         'no_referer3': _(
-            "If you are using the <meta name=\"referrer\" "
-            "content=\"no-referrer\"> tag or including the 'Referrer-Policy: "
-            "no-referrer' header, please remove them. The CSRF protection "
-            "requires the 'Referer' header to do strict referer checking. If "
-            "you're concerned about privacy, use alternatives like "
-            "<a rel=\"noreferrer\" ...> for links to third-party sites."),
+            'If you are using the <meta name="referrer" '
+            'content=\"no-referrer\"> tag or including the “Referrer-Policy: '
+            'no-referrer” header, please remove them. The CSRF protection '
+            'requires the “Referer” header to do strict referer checking. If '
+            'you’re concerned about privacy, use alternatives like '
+            '<a rel=\"noreferrer\" …> for links to third-party sites.'),
         'no_cookie': reason == REASON_NO_CSRF_COOKIE,
         'no_cookie1': _(
             "You are seeing this message because this site requires a CSRF "
@@ -134,9 +134,9 @@
             "security reasons, to ensure that your browser is not being "
             "hijacked by third parties."),
         'no_cookie2': _(
-            "If you have configured your browser to disable cookies, please "
-            "re-enable them, at least for this site, or for 'same-origin' "
-            "requests."),
+            'If you have configured your browser to disable cookies, please '
+            're-enable them, at least for this site, or for “same-origin” '
+            'requests.'),
         'DEBUG': settings.DEBUG,
         'docs_version': get_docs_version(),
         'more': _("More information is available with DEBUG=True."),
('django/views', 'debug.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,17 +2,19 @@
 import re
 import sys
 import types
+import warnings
 from pathlib import Path

 from django.conf import settings
-from django.http import HttpResponse, HttpResponseNotFound
+from django.http import Http404, HttpResponse, HttpResponseNotFound
 from django.template import Context, Engine, TemplateDoesNotExist
 from django.template.defaultfilters import pprint
-from django.urls import Resolver404, resolve
+from django.urls import resolve
 from django.utils import timezone
 from django.utils.datastructures import MultiValueDict
-from django.utils.encoding import force_text
+from django.utils.encoding import force_str
 from django.utils.module_loading import import_string
+from django.utils.regex_helper import _lazy_re_compile
 from django.utils.version import get_docs_version

 # Minimal Django templates engine to render the error templates
@@ -24,11 +26,11 @@
     libraries={'i18n': 'django.templatetags.i18n'},
 )

-HIDDEN_SETTINGS = re.compile('API|TOKEN|KEY|SECRET|PASS|SIGNATURE', flags=re.IGNORECASE)
-
-CLEANSED_SUBSTITUTE = '********************'
-
 CURRENT_DIR = Path(__file__).parent
+
+
+class ExceptionCycleWarning(UserWarning):
+    pass


 class CallableSettingWrapper:
@@ -45,54 +47,18 @@
         return repr(self._wrapped)


-def cleanse_setting(key, value):
-    """
-    Cleanse an individual setting key/value of sensitive content. If the value
-    is a dictionary, recursively cleanse the keys in that dictionary.
-    """
-    try:
-        if HIDDEN_SETTINGS.search(key):
-            cleansed = CLEANSED_SUBSTITUTE
-        else:
-            if isinstance(value, dict):
-                cleansed = {k: cleanse_setting(k, v) for k, v in value.items()}
-            else:
-                cleansed = value
-    except TypeError:
-        # If the key isn't regex-able, just return as-is.
-        cleansed = value
-
-    if callable(cleansed):
-        # For fixing #21345 and #23070
-        cleansed = CallableSettingWrapper(cleansed)
-
-    return cleansed
-
-
-def get_safe_settings():
-    """
-    Return a dictionary of the settings module with values of sensitive
-    settings replaced with stars (*********).
-    """
-    settings_dict = {}
-    for k in dir(settings):
-        if k.isupper():
-            settings_dict[k] = cleanse_setting(k, getattr(settings, k))
-    return settings_dict
-
-
 def technical_500_response(request, exc_type, exc_value, tb, status_code=500):
     """
     Create a technical server error response. The last three arguments are
     the values returned from sys.exc_info() and friends.
     """
-    reporter = ExceptionReporter(request, exc_type, exc_value, tb)
-    if request.is_ajax():
+    reporter = get_exception_reporter_class(request)(request, exc_type, exc_value, tb)
+    if request.accepts('text/html'):
+        html = reporter.get_traceback_html()
+        return HttpResponse(html, status=status_code, content_type='text/html')
+    else:
         text = reporter.get_traceback_text()
         return HttpResponse(text, status=status_code, content_type='text/plain; charset=utf-8')
-    else:
-        html = reporter.get_traceback_html()
-        return HttpResponse(html, status=status_code, content_type='text/html')


 @functools.lru_cache()
@@ -106,27 +72,63 @@
     return getattr(request, 'exception_reporter_filter', default_filter)


-class ExceptionReporterFilter:
-    """
-    Base for all exception reporter filter classes. All overridable hooks
-    contain lenient default behaviors.
-    """
-
-    def get_post_parameters(self, request):
-        if request is None:
-            return {}
-        else:
-            return request.POST
-
-    def get_traceback_frame_variables(self, request, tb_frame):
-        return list(tb_frame.f_locals.items())
-
-
-class SafeExceptionReporterFilter(ExceptionReporterFilter):
+def get_exception_reporter_class(request):
+    default_exception_reporter_class = import_string(settings.DEFAULT_EXCEPTION_REPORTER)
+    return getattr(request, 'exception_reporter_class', default_exception_reporter_class)
+
+
+class SafeExceptionReporterFilter:
     """
     Use annotations made by the sensitive_post_parameters and
     sensitive_variables decorators to filter out sensitive information.
     """
+    cleansed_substitute = '********************'
+    hidden_settings = _lazy_re_compile('API|TOKEN|KEY|SECRET|PASS|SIGNATURE', flags=re.I)
+
+    def cleanse_setting(self, key, value):
+        """
+        Cleanse an individual setting key/value of sensitive content. If the
+        value is a dictionary, recursively cleanse the keys in that dictionary.
+        """
+        try:
+            is_sensitive = self.hidden_settings.search(key)
+        except TypeError:
+            is_sensitive = False
+
+        if is_sensitive:
+            cleansed = self.cleansed_substitute
+        elif isinstance(value, dict):
+            cleansed = {k: self.cleanse_setting(k, v) for k, v in value.items()}
+        elif isinstance(value, list):
+            cleansed = [self.cleanse_setting('', v) for v in value]
+        elif isinstance(value, tuple):
+            cleansed = tuple([self.cleanse_setting('', v) for v in value])
+        else:
+            cleansed = value
+
+        if callable(cleansed):
+            cleansed = CallableSettingWrapper(cleansed)
+
+        return cleansed
+
+    def get_safe_settings(self):
+        """
+        Return a dictionary of the settings module with values of sensitive
+        settings replaced with stars (*********).
+        """
+        settings_dict = {}
+        for k in dir(settings):
+            if k.isupper():
+                settings_dict[k] = self.cleanse_setting(k, getattr(settings, k))
+        return settings_dict
+
+    def get_safe_request_meta(self, request):
+        """
+        Return a dictionary of request.META with sensitive values redacted.
+        """
+        if not hasattr(request, 'META'):
+            return {}
+        return {k: self.cleanse_setting(k, v) for k, v in request.META.items()}

     def is_active(self, request):
         """
@@ -148,7 +150,7 @@
             multivaluedict = multivaluedict.copy()
             for param in sensitive_post_parameters:
                 if param in multivaluedict:
-                    multivaluedict[param] = CLEANSED_SUBSTITUTE
+                    multivaluedict[param] = self.cleansed_substitute
         return multivaluedict

     def get_post_parameters(self, request):
@@ -165,13 +167,13 @@
                 if sensitive_post_parameters == '__ALL__':
                     # Cleanse all parameters.
                     for k in cleansed:
-                        cleansed[k] = CLEANSED_SUBSTITUTE
+                        cleansed[k] = self.cleansed_substitute
                     return cleansed
                 else:
                     # Cleanse only the specified parameters.
                     for param in sensitive_post_parameters:
                         if param in cleansed:
-                            cleansed[param] = CLEANSED_SUBSTITUTE
+                            cleansed[param] = self.cleansed_substitute
                     return cleansed
             else:
                 return request.POST
@@ -214,12 +216,12 @@
             if sensitive_variables == '__ALL__':
                 # Cleanse all variables
                 for name in tb_frame.f_locals:
-                    cleansed[name] = CLEANSED_SUBSTITUTE
+                    cleansed[name] = self.cleansed_substitute
             else:
                 # Cleanse specified variables
                 for name, value in tb_frame.f_locals.items():
                     if name in sensitive_variables:
-                        value = CLEANSED_SUBSTITUTE
+                        value = self.cleansed_substitute
                     else:
                         value = self.cleanse_special_types(request, value)
                     cleansed[name] = value
@@ -235,14 +237,23 @@
             # the sensitive_variables decorator's frame, in case the variables
             # associated with those arguments were meant to be obfuscated from
             # the decorated function's frame.
-            cleansed['func_args'] = CLEANSED_SUBSTITUTE
-            cleansed['func_kwargs'] = CLEANSED_SUBSTITUTE
+            cleansed['func_args'] = self.cleansed_substitute
+            cleansed['func_kwargs'] = self.cleansed_substitute

         return cleansed.items()


 class ExceptionReporter:
     """Organize and coordinate reporting on exceptions."""
+
+    @property
+    def html_template_path(self):
+        return CURRENT_DIR / 'templates' / 'technical_500.html'
+
+    @property
+    def text_template_path(self):
+        return CURRENT_DIR / 'templates' / 'technical_500.txt'
+
     def __init__(self, request, exc_type, exc_value, tb, is_email=False):
         self.request = request
         self.filter = get_exception_reporter_filter(self.request)
@@ -280,7 +291,7 @@
             end = getattr(self.exc_value, 'end', None)
             if start is not None and end is not None:
                 unicode_str = self.exc_value.args[1]
-                unicode_hint = force_text(
+                unicode_hint = force_str(
                     unicode_str[max(start - 5, 0):min(end + 5, len(unicode_str))],
                     'ascii', errors='replace'
                 )
@@ -301,9 +312,10 @@
             'unicode_hint': unicode_hint,
             'frames': frames,
             'request': self.request,
+            'request_meta': self.filter.get_safe_request_meta(self.request),
             'user_str': user_str,
             'filtered_POST_items': list(self.filter.get_post_parameters(self.request).items()),
-            'settings': get_safe_settings(),
+            'settings': self.filter.get_safe_settings(),
             'sys_executable': sys.executable,
             'sys_version_info': '%d.%d.%d' % sys.version_info[0:3],
             'server_time': timezone.now(),
@@ -328,23 +340,19 @@

     def get_traceback_html(self):
         """Return HTML version of debug 500 HTTP error page."""
-        with Path(CURRENT_DIR, 'templates', 'technical_500.html').open() as fh:
+        with self.html_template_path.open(encoding='utf-8') as fh:
             t = DEBUG_ENGINE.from_string(fh.read())
         c = Context(self.get_traceback_data(), use_l10n=False)
         return t.render(c)

     def get_traceback_text(self):
         """Return plain text version of debug 500 HTTP error page."""
-        with Path(CURRENT_DIR, 'templates', 'technical_500.txt').open() as fh:
+        with self.text_template_path.open(encoding='utf-8') as fh:
             t = DEBUG_ENGINE.from_string(fh.read())
         c = Context(self.get_traceback_data(), autoescape=False, use_l10n=False)
         return t.render(c)

-    def _get_lines_from_file(self, filename, lineno, context_lines, loader=None, module_name=None):
-        """
-        Return context_lines before and after lineno from file.
-        Return (pre_context_lineno, pre_context, context_line, post_context).
-        """
+    def _get_source(self, filename, loader, module_name):
         source = None
         if hasattr(loader, 'get_source'):
             try:
@@ -357,8 +365,16 @@
             try:
                 with open(filename, 'rb') as fp:
                     source = fp.read().splitlines()
-            except (OSError, IOError):
+            except OSError:
                 pass
+        return source
+
+    def _get_lines_from_file(self, filename, lineno, context_lines, loader=None, module_name=None):
+        """
+        Return context_lines before and after lineno from file.
+        Return (pre_context_lineno, pre_context, context_line, post_context).
+        """
+        source = self._get_source(filename, loader, module_name)
         if source is None:
             return None, [], None, []

@@ -372,31 +388,42 @@
                 # (https://www.python.org/dev/peps/pep-0263/)
                 match = re.search(br'coding[:=]\s*([-\w.]+)', line)
                 if match:
-                    encoding = match.group(1).decode('ascii')
+                    encoding = match[1].decode('ascii')
                     break
             source = [str(sline, encoding, 'replace') for sline in source]

         lower_bound = max(0, lineno - context_lines)
         upper_bound = lineno + context_lines

-        pre_context = source[lower_bound:lineno]
-        context_line = source[lineno]
-        post_context = source[lineno + 1:upper_bound]
-
+        try:
+            pre_context = source[lower_bound:lineno]
+            context_line = source[lineno]
+            post_context = source[lineno + 1:upper_bound]
+        except IndexError:
+            return None, [], None, []
         return lower_bound, pre_context, context_line, post_context

+    def _get_explicit_or_implicit_cause(self, exc_value):
+        explicit = getattr(exc_value, '__cause__', None)
+        suppress_context = getattr(exc_value, '__suppress_context__', None)
+        implicit = getattr(exc_value, '__context__', None)
+        return explicit or (None if suppress_context else implicit)
+
     def get_traceback_frames(self):
-        def explicit_or_implicit_cause(exc_value):
-            explicit = getattr(exc_value, '__cause__', None)
-            implicit = getattr(exc_value, '__context__', None)
-            return explicit or implicit
-
         # Get the exception and all its causes
         exceptions = []
         exc_value = self.exc_value
         while exc_value:
             exceptions.append(exc_value)
-            exc_value = explicit_or_implicit_cause(exc_value)
+            exc_value = self._get_explicit_or_implicit_cause(exc_value)
+            if exc_value in exceptions:
+                warnings.warn(
+                    "Cycle in the exception chain detected: exception '%s' "
+                    "encountered again." % exc_value,
+                    ExceptionCycleWarning,
+                )
+                # Avoid infinite loop if there's a cyclic reference (#29393).
+                break

         frames = []
         # No exceptions were supplied to ExceptionReporter
@@ -406,7 +433,25 @@
         # In case there's just one exception, take the traceback from self.tb
         exc_value = exceptions.pop()
         tb = self.tb if not exceptions else exc_value.__traceback__
-
+        while True:
+            frames.extend(self.get_exception_traceback_frames(exc_value, tb))
+            try:
+                exc_value = exceptions.pop()
+            except IndexError:
+                break
+            tb = exc_value.__traceback__
+        return frames
+
+    def get_exception_traceback_frames(self, exc_value, tb):
+        exc_cause = self._get_explicit_or_implicit_cause(exc_value)
+        exc_cause_explicit = getattr(exc_value, '__cause__', True)
+        if tb is None:
+            yield {
+                'exc_cause': exc_cause,
+                'exc_cause_explicit': exc_cause_explicit,
+                'tb': None,
+                'type': 'user',
+            }
         while tb is not None:
             # Support for __traceback_hide__ which is used by a few libraries
             # to hide internal frames.
@@ -426,9 +471,9 @@
                 pre_context = []
                 context_line = '<source code not available>'
                 post_context = []
-            frames.append({
-                'exc_cause': explicit_or_implicit_cause(exc_value),
-                'exc_cause_explicit': getattr(exc_value, '__cause__', True),
+            yield {
+                'exc_cause': exc_cause,
+                'exc_cause_explicit': exc_cause_explicit,
                 'tb': tb,
                 'type': 'django' if module_name.startswith('django.') else 'user',
                 'filename': filename,
@@ -440,17 +485,8 @@
                 'context_line': context_line,
                 'post_context': post_context,
                 'pre_context_lineno': pre_context_lineno + 1,
-            })
-
-            # If the traceback for current exception is consumed, try the
-            # other exception.
-            if not tb.tb_next and exceptions:
-                exc_value = exceptions.pop()
-                tb = exc_value.__traceback__
-            else:
-                tb = tb.tb_next
-
-        return frames
+            }
+            tb = tb.tb_next


 def technical_404_response(request, exception):
@@ -463,8 +499,10 @@
     try:
         tried = exception.args[0]['tried']
     except (IndexError, TypeError, KeyError):
-        tried = []
+        resolved = True
+        tried = request.resolver_match.tried if request.resolver_match else None
     else:
+        resolved = False
         if (not tried or (                  # empty URLconf
             request.path == '/' and
             len(tried) == 1 and             # default URLconf
@@ -480,7 +518,7 @@
     caller = ''
     try:
         resolver_match = resolve(request.path)
-    except Resolver404:
+    except Http404:
         pass
     else:
         obj = resolver_match.func
@@ -494,16 +532,18 @@
             module = obj.__module__
             caller = '%s.%s' % (module, caller)

-    with Path(CURRENT_DIR, 'templates', 'technical_404.html').open() as fh:
+    with Path(CURRENT_DIR, 'templates', 'technical_404.html').open(encoding='utf-8') as fh:
         t = DEBUG_ENGINE.from_string(fh.read())
+    reporter_filter = get_default_exception_reporter_filter()
     c = Context({
         'urlconf': urlconf,
         'root_urlconf': settings.ROOT_URLCONF,
         'request_path': error_url,
         'urlpatterns': tried,
+        'resolved': resolved,
         'reason': str(exception),
         'request': request,
-        'settings': get_safe_settings(),
+        'settings': reporter_filter.get_safe_settings(),
         'raising_view_name': caller,
     })
     return HttpResponseNotFound(t.render(c), content_type='text/html')
@@ -511,7 +551,7 @@

 def default_urlconf(request):
     """Create an empty URLconf 404 error response."""
-    with Path(CURRENT_DIR, 'templates', 'default_urlconf.html').open() as fh:
+    with Path(CURRENT_DIR, 'templates', 'default_urlconf.html').open(encoding='utf-8') as fh:
         t = DEBUG_ENGINE.from_string(fh.read())
     c = Context({
         'version': get_docs_version(),
('django/views', 'static.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -24,7 +24,7 @@

         from django.views.static import serve

-        url(r'^(?P<path>.*)$', serve, {'document_root': '/path/to/my/files/'})
+        path('<path:path>', serve, {'document_root': '/path/to/my/files/'})

     in your URLconf. You must provide the ``document_root`` param. You may
     also set ``show_indexes`` to ``True`` if you'd like to serve a basic index
@@ -39,7 +39,7 @@
             return directory_index(path, fullpath)
         raise Http404(_("Directory indexes are not allowed here."))
     if not fullpath.exists():
-        raise Http404(_('"%(path)s" does not exist') % {'path': fullpath})
+        raise Http404(_('“%(path)s” does not exist') % {'path': fullpath})
     # Respect the If-Modified-Since header.
     statobj = fullpath.stat()
     if not was_modified_since(request.META.get('HTTP_IF_MODIFIED_SINCE'),
@@ -48,9 +48,9 @@
     content_type, encoding = mimetypes.guess_type(str(fullpath))
     content_type = content_type or 'application/octet-stream'
     response = FileResponse(fullpath.open('rb'), content_type=content_type)
-    response["Last-Modified"] = http_date(statobj.st_mtime)
+    response.headers["Last-Modified"] = http_date(statobj.st_mtime)
     if encoding:
-        response["Content-Encoding"] = encoding
+        response.headers["Content-Encoding"] = encoding
     return response


@@ -62,10 +62,10 @@
     <meta http-equiv="Content-type" content="text/html; charset=utf-8">
     <meta http-equiv="Content-Language" content="en-us">
     <meta name="robots" content="NONE,NOARCHIVE">
-    <title>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</title>
+    <title>{% blocktranslate %}Index of {{ directory }}{% endblocktranslate %}</title>
   </head>
   <body>
-    <h1>{% blocktrans %}Index of {{ directory }}{% endblocktrans %}</h1>
+    <h1>{% blocktranslate %}Index of {{ directory }}{% endblocktranslate %}</h1>
     <ul>
       {% if directory != "/" %}
       <li><a href="../">../</a></li>
@@ -124,8 +124,8 @@
             raise ValueError
         matches = re.match(r"^([^;]+)(; length=([0-9]+))?$", header,
                            re.IGNORECASE)
-        header_mtime = parse_http_date(matches.group(1))
-        header_len = matches.group(3)
+        header_mtime = parse_http_date(matches[1])
+        header_len = matches[3]
         if header_len and int(header_len) != size:
             raise ValueError
         if int(mtime) > header_mtime:
('django/views', 'defaults.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -11,6 +11,17 @@
 ERROR_403_TEMPLATE_NAME = '403.html'
 ERROR_400_TEMPLATE_NAME = '400.html'
 ERROR_500_TEMPLATE_NAME = '500.html'
+ERROR_PAGE_TEMPLATE = """
+<!doctype html>
+<html lang="en">
+<head>
+  <title>%(title)s</title>
+</head>
+<body>
+  <h1>%(title)s</h1><p>%(details)s</p>
+</body>
+</html>
+"""


 # This can be called when CsrfViewMiddleware.process_view has not run,
@@ -47,7 +58,7 @@
     try:
         template = loader.get_template(template_name)
         body = template.render(context, request)
-        content_type = None             # Django will use DEFAULT_CONTENT_TYPE
+        content_type = None             # Django will use 'text/html'.
     except TemplateDoesNotExist:
         if template_name != ERROR_404_TEMPLATE_NAME:
             # Reraise if it's a missing custom template.
@@ -55,8 +66,11 @@
         # Render template (even though there are no substitutions) to allow
         # inspecting the context in tests.
         template = Engine().from_string(
-            '<h1>Not Found</h1>'
-            '<p>The requested resource was not found on this server.</p>')
+            ERROR_PAGE_TEMPLATE % {
+                'title': 'Not Found',
+                'details': 'The requested resource was not found on this server.',
+            },
+        )
         body = template.render(Context(context))
         content_type = 'text/html'
     return HttpResponseNotFound(body, content_type=content_type)
@@ -76,7 +90,10 @@
         if template_name != ERROR_500_TEMPLATE_NAME:
             # Reraise if it's a missing custom template.
             raise
-        return HttpResponseServerError('<h1>Server Error (500)</h1>', content_type='text/html')
+        return HttpResponseServerError(
+            ERROR_PAGE_TEMPLATE % {'title': 'Server Error (500)', 'details': ''},
+            content_type='text/html',
+        )
     return HttpResponseServerError(template.render())


@@ -94,7 +111,10 @@
         if template_name != ERROR_400_TEMPLATE_NAME:
             # Reraise if it's a missing custom template.
             raise
-        return HttpResponseBadRequest('<h1>Bad Request (400)</h1>', content_type='text/html')
+        return HttpResponseBadRequest(
+            ERROR_PAGE_TEMPLATE % {'title': 'Bad Request (400)', 'details': ''},
+            content_type='text/html',
+        )
     # No exception content is passed to the template, to not disclose any sensitive information.
     return HttpResponseBadRequest(template.render())

@@ -119,7 +139,10 @@
         if template_name != ERROR_403_TEMPLATE_NAME:
             # Reraise if it's a missing custom template.
             raise
-        return HttpResponseForbidden('<h1>403 Forbidden</h1>', content_type='text/html')
+        return HttpResponseForbidden(
+            ERROR_PAGE_TEMPLATE % {'title': '403 Forbidden', 'details': ''},
+            content_type='text/html',
+        )
     return HttpResponseForbidden(
         template.render(request=request, context={'exception': str(exception)})
     )
('django/views', 'i18n.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,7 +2,6 @@
 import json
 import os
 import re
-from urllib.parse import unquote

 from django.apps import apps
 from django.conf import settings
@@ -10,7 +9,7 @@
 from django.template import Context, Engine
 from django.urls import translate_url
 from django.utils.formats import get_format
-from django.utils.http import is_safe_url
+from django.utils.http import url_has_allowed_host_and_scheme
 from django.utils.translation import (
     LANGUAGE_SESSION_KEY, check_for_language, get_language,
 )
@@ -31,28 +30,42 @@
     redirect to the page in the request (the 'next' parameter) without changing
     any state.
     """
-    next = request.POST.get('next', request.GET.get('next'))
-    if ((next or not request.is_ajax()) and
-            not is_safe_url(url=next, allowed_hosts={request.get_host()}, require_https=request.is_secure())):
-        next = request.META.get('HTTP_REFERER')
-        next = next and unquote(next)  # HTTP_REFERER may be encoded.
-        if not is_safe_url(url=next, allowed_hosts={request.get_host()}, require_https=request.is_secure()):
-            next = '/'
-    response = HttpResponseRedirect(next) if next else HttpResponse(status=204)
+    next_url = request.POST.get('next', request.GET.get('next'))
+    if (
+        (next_url or request.accepts('text/html')) and
+        not url_has_allowed_host_and_scheme(
+            url=next_url,
+            allowed_hosts={request.get_host()},
+            require_https=request.is_secure(),
+        )
+    ):
+        next_url = request.META.get('HTTP_REFERER')
+        if not url_has_allowed_host_and_scheme(
+            url=next_url,
+            allowed_hosts={request.get_host()},
+            require_https=request.is_secure(),
+        ):
+            next_url = '/'
+    response = HttpResponseRedirect(next_url) if next_url else HttpResponse(status=204)
     if request.method == 'POST':
         lang_code = request.POST.get(LANGUAGE_QUERY_PARAMETER)
         if lang_code and check_for_language(lang_code):
-            if next:
-                next_trans = translate_url(next, lang_code)
-                if next_trans != next:
+            if next_url:
+                next_trans = translate_url(next_url, lang_code)
+                if next_trans != next_url:
                     response = HttpResponseRedirect(next_trans)
             if hasattr(request, 'session'):
+                # Storing the language in the session is deprecated.
+                # (RemovedInDjango40Warning)
                 request.session[LANGUAGE_SESSION_KEY] = lang_code
             response.set_cookie(
                 settings.LANGUAGE_COOKIE_NAME, lang_code,
                 max_age=settings.LANGUAGE_COOKIE_AGE,
                 path=settings.LANGUAGE_COOKIE_PATH,
                 domain=settings.LANGUAGE_COOKIE_DOMAIN,
+                secure=settings.LANGUAGE_COOKIE_SECURE,
+                httponly=settings.LANGUAGE_COOKIE_HTTPONLY,
+                samesite=settings.LANGUAGE_COOKIE_SAMESITE,
             )
     return response

@@ -71,14 +84,15 @@

 js_catalog_template = r"""
 {% autoescape off %}
-(function(globals) {
-
-  var django = globals.django || (globals.django = {});
+'use strict';
+{
+  const globals = this;
+  const django = globals.django || (globals.django = {});

   {% if plural %}
   django.pluralidx = function(n) {
-    var v={{ plural }};
-    if (typeof(v) == 'boolean') {
+    const v = {{ plural }};
+    if (typeof v === 'boolean') {
       return v ? 1 : 0;
     } else {
       return v;
@@ -92,25 +106,25 @@

   django.catalog = django.catalog || {};
   {% if catalog_str %}
-  var newcatalog = {{ catalog_str }};
-  for (var key in newcatalog) {
+  const newcatalog = {{ catalog_str }};
+  for (const key in newcatalog) {
     django.catalog[key] = newcatalog[key];
   }
   {% endif %}

   if (!django.jsi18n_initialized) {
     django.gettext = function(msgid) {
-      var value = django.catalog[msgid];
-      if (typeof(value) == 'undefined') {
+      const value = django.catalog[msgid];
+      if (typeof value === 'undefined') {
         return msgid;
       } else {
-        return (typeof(value) == 'string') ? value : value[0];
+        return (typeof value === 'string') ? value : value[0];
       }
     };

     django.ngettext = function(singular, plural, count) {
-      var value = django.catalog[singular];
-      if (typeof(value) == 'undefined') {
+      const value = django.catalog[singular];
+      if (typeof value === 'undefined') {
         return (count == 1) ? singular : plural;
       } else {
         return value.constructor === Array ? value[django.pluralidx(count)] : value;
@@ -120,16 +134,16 @@
     django.gettext_noop = function(msgid) { return msgid; };

     django.pgettext = function(context, msgid) {
-      var value = django.gettext(context + '\x04' + msgid);
-      if (value.indexOf('\x04') != -1) {
+      let value = django.gettext(context + '\x04' + msgid);
+      if (value.includes('\x04')) {
         value = msgid;
       }
       return value;
     };

     django.npgettext = function(context, singular, plural, count) {
-      var value = django.ngettext(context + '\x04' + singular, context + '\x04' + plural, count);
-      if (value.indexOf('\x04') != -1) {
+      let value = django.ngettext(context + '\x04' + singular, context + '\x04' + plural, count);
+      if (value.includes('\x04')) {
         value = django.ngettext(singular, plural, count);
       }
       return value;
@@ -149,8 +163,8 @@
     django.formats = {{ formats_str }};

     django.get_format = function(format_type) {
-      var value = django.formats[format_type];
-      if (typeof(value) == 'undefined') {
+      const value = django.formats[format_type];
+      if (typeof value === 'undefined') {
         return format_type;
       } else {
         return value;
@@ -169,8 +183,7 @@

     django.jsi18n_initialized = true;
   }
-
-}(this));
+};
 {% endautoescape %}
 """

@@ -180,8 +193,8 @@
     Return the selected language catalog as a JavaScript library.

     Receive the list of packages to check for translations in the `packages`
-    kwarg either from the extra dictionary passed to the url() function or as a
-    plus-sign delimited string from the request. Default is 'django.conf'.
+    kwarg either from the extra dictionary passed to the path() function or as
+    a plus-sign delimited string from the request. Default is 'django.conf'.

     You can override the gettext domain for this view, but usually you don't
     want to do that as JavaScript messages go to the djangojs domain. This
@@ -221,7 +234,7 @@
         """
         match = re.search(r'nplurals=\s*(\d+)', self._plural_string or '')
         if match:
-            return int(match.groups()[0])
+            return int(match[1])
         return 2

     @property
('django/views/decorators', 'cache.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -20,7 +20,7 @@
     into account on caching -- just like the middleware does.
     """
     return decorator_from_middleware_with_args(CacheMiddleware)(
-        cache_timeout=timeout, cache_alias=cache, key_prefix=key_prefix
+        page_timeout=timeout, cache_alias=cache, key_prefix=key_prefix,
     )


('django/views/decorators', 'debug.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -26,6 +26,12 @@
         def my_function()
             ...
     """
+    if len(variables) == 1 and callable(variables[0]):
+        raise TypeError(
+            'sensitive_variables() must be called to use it as a decorator, '
+            'e.g., use @sensitive_variables(), not @sensitive_variables.'
+        )
+
     def decorator(func):
         @functools.wraps(func)
         def sensitive_variables_wrapper(*func_args, **func_kwargs):
@@ -61,6 +67,13 @@
         def my_view(request)
             ...
     """
+    if len(parameters) == 1 and callable(parameters[0]):
+        raise TypeError(
+            'sensitive_post_parameters() must be called to use it as a '
+            'decorator, e.g., use @sensitive_post_parameters(), not '
+            '@sensitive_post_parameters.'
+        )
+
     def decorator(view):
         @functools.wraps(view)
         def sensitive_post_parameters_wrapper(request, *args, **kwargs):
('django/views/decorators', 'http.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -102,9 +102,9 @@
             # and if the request method is safe.
             if request.method in ('GET', 'HEAD'):
                 if res_last_modified and not response.has_header('Last-Modified'):
-                    response['Last-Modified'] = http_date(res_last_modified)
+                    response.headers['Last-Modified'] = http_date(res_last_modified)
                 if res_etag:
-                    response.setdefault('ETag', res_etag)
+                    response.headers.setdefault('ETag', res_etag)

             return response

('django/views/generic', 'list.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,6 @@
 from django.core.exceptions import ImproperlyConfigured
 from django.core.paginator import InvalidPage, Paginator
-from django.db.models.query import QuerySet
+from django.db.models import QuerySet
 from django.http import Http404
 from django.utils.translation import gettext as _
 from django.views.generic.base import ContextMixin, TemplateResponseMixin, View
@@ -64,7 +64,7 @@
             if page == 'last':
                 page_number = paginator.num_pages
             else:
-                raise Http404(_("Page is not 'last', nor can it be converted to an int."))
+                raise Http404(_('Page is not “last”, nor can it be converted to an int.'))
         try:
             page = paginator.page(page_number)
             return (paginator, page, page.object_list, page.has_other_pages())
@@ -151,7 +151,7 @@
             else:
                 is_empty = not self.object_list
             if is_empty:
-                raise Http404(_("Empty list and '%(class_name)s.allow_empty' is False.") % {
+                raise Http404(_('Empty list and “%(class_name)s.allow_empty” is False.') % {
                     'class_name': self.__class__.__name__,
                 })
         context = self.get_context_data()
('django/views/generic', 'dates.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -218,7 +218,7 @@
         The first day according to the week format is 0 and the last day is 6.
         """
         week_format = self.get_week_format()
-        if week_format == '%W':                 # week starts on Monday
+        if week_format in {'%W', '%V'}:         # week starts on Monday
             return date.weekday()
         elif week_format == '%U':               # week starts on Sunday
             return (date.weekday() + 1) % 7
@@ -485,7 +485,7 @@

         date_field = self.get_date_field()
         week_format = self.get_week_format()
-        week_choices = {'%W': '1', '%U': '0'}
+        week_choices = {'%W': '1', '%U': '0', '%V': '1'}
         try:
             week_start = week_choices[week_format]
         except KeyError:
@@ -493,10 +493,15 @@
                 week_format,
                 ', '.join(sorted(week_choices)),
             ))
-        date = _date_from_string(year, self.get_year_format(),
-                                 week_start, '%w',
-                                 week, week_format)
-
+        year_format = self.get_year_format()
+        if week_format == '%V' and year_format != '%G':
+            raise ValueError(
+                "ISO week directive '%s' is incompatible with the year "
+                "directive '%s'. Use the ISO year '%%G' instead." % (
+                    week_format, year_format,
+                )
+            )
+        date = _date_from_string(year, year_format, week_start, '%w', week, week_format)
         since = self._make_date_lookup_arg(date)
         until = self._make_date_lookup_arg(self._get_next_week(date))
         lookup_kwargs = {
@@ -620,7 +625,7 @@
     try:
         return datetime.datetime.strptime(datestr, format).date()
     except ValueError:
-        raise Http404(_("Invalid date string '%(datestr)s' given format '%(format)s'") % {
+        raise Http404(_('Invalid date string “%(datestr)s” given format “%(format)s”') % {
             'datestr': datestr,
             'format': format,
         })
('django/views/generic', 'base.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -50,9 +50,10 @@
         """Main entry point for a request-response process."""
         for key in initkwargs:
             if key in cls.http_method_names:
-                raise TypeError("You tried to pass in the %s method name as a "
-                                "keyword argument to %s(). Don't do that."
-                                % (key, cls.__name__))
+                raise TypeError(
+                    'The method name %s is not accepted as a keyword argument '
+                    'to %s().' % (key, cls.__name__)
+                )
             if not hasattr(cls, key):
                 raise TypeError("%s() received an invalid keyword %r. as_view "
                                 "only accepts arguments that are already "
@@ -60,8 +61,6 @@

         def view(request, *args, **kwargs):
             self = cls(**initkwargs)
-            if hasattr(self, 'get') and not hasattr(self, 'head'):
-                self.head = self.get
             self.setup(request, *args, **kwargs)
             if not hasattr(self, 'request'):
                 raise AttributeError(
@@ -82,6 +81,8 @@

     def setup(self, request, *args, **kwargs):
         """Initialize attributes shared by all view methods."""
+        if hasattr(self, 'get') and not hasattr(self, 'head'):
+            self.head = self.get
         self.request = request
         self.args = args
         self.kwargs = kwargs
@@ -106,8 +107,8 @@
     def options(self, request, *args, **kwargs):
         """Handle responding to requests for the OPTIONS HTTP verb."""
         response = HttpResponse()
-        response['Allow'] = ', '.join(self._allowed_methods())
-        response['Content-Length'] = '0'
+        response.headers['Allow'] = ', '.join(self._allowed_methods())
+        response.headers['Content-Length'] = '0'
         return response

     def _allowed_methods(self):
('django/conf', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -16,17 +16,20 @@
 import django
 from django.conf import global_settings
 from django.core.exceptions import ImproperlyConfigured
-from django.utils.deprecation import (
-    RemovedInDjango30Warning, RemovedInDjango31Warning,
+from django.utils.deprecation import RemovedInDjango40Warning
+from django.utils.functional import LazyObject, empty
+
+ENVIRONMENT_VARIABLE = "DJANGO_SETTINGS_MODULE"
+
+PASSWORD_RESET_TIMEOUT_DAYS_DEPRECATED_MSG = (
+    'The PASSWORD_RESET_TIMEOUT_DAYS setting is deprecated. Use '
+    'PASSWORD_RESET_TIMEOUT instead.'
 )
-from django.utils.functional import LazyObject, empty
-
-ENVIRONMENT_VARIABLE = "DJANGO_SETTINGS_MODULE"
-
-DEFAULT_CONTENT_TYPE_DEPRECATED_MSG = 'The DEFAULT_CONTENT_TYPE setting is deprecated.'
-FILE_CHARSET_DEPRECATED_MSG = (
-    'The FILE_CHARSET setting is deprecated. Starting with Django 3.1, all '
-    'files read from disk must be UTF-8 encoded.'
+
+DEFAULT_HASHING_ALGORITHM_DEPRECATED_MSG = (
+    'The DEFAULT_HASHING_ALGORITHM transitional setting is deprecated. '
+    'Support for it and tokens, cookies, sessions, and signatures that use '
+    'SHA-1 hashing algorithm will be removed in Django 4.0.'
 )


@@ -78,6 +81,14 @@
         if self._wrapped is empty:
             self._setup(name)
         val = getattr(self._wrapped, name)
+
+        # Special case some settings which require further modification.
+        # This is done here for performance reasons so the modified value is cached.
+        if name in {'MEDIA_URL', 'STATIC_URL'} and val is not None:
+            val = self._add_script_prefix(val)
+        elif name == 'SECRET_KEY' and not val:
+            raise ImproperlyConfigured("The SECRET_KEY setting must not be empty.")
+
         self.__dict__[name] = val
         return val

@@ -107,8 +118,24 @@
             raise RuntimeError('Settings already configured.')
         holder = UserSettingsHolder(default_settings)
         for name, value in options.items():
+            if not name.isupper():
+                raise TypeError('Setting %r must be uppercase.' % name)
             setattr(holder, name, value)
         self._wrapped = holder
+
+    @staticmethod
+    def _add_script_prefix(value):
+        """
+        Add SCRIPT_NAME prefix to relative paths.
+
+        Useful when the app is being served at a subpath and manually prefixing
+        subpath to STATIC_URL and MEDIA_URL in settings is inconvenient.
+        """
+        # Don't apply prefix to absolute paths and URLs.
+        if value.startswith(('http://', 'https://', '/')):
+            return value
+        from django.urls import get_script_prefix
+        return '%s%s' % (get_script_prefix(), value)

     @property
     def configured(self):
@@ -116,32 +143,18 @@
         return self._wrapped is not empty

     @property
-    def DEFAULT_CONTENT_TYPE(self):
+    def PASSWORD_RESET_TIMEOUT_DAYS(self):
         stack = traceback.extract_stack()
         # Show a warning if the setting is used outside of Django.
         # Stack index: -1 this line, -2 the caller.
-        filename, _line_number, _function_name, _text = stack[-2]
+        filename, _, _, _ = stack[-2]
         if not filename.startswith(os.path.dirname(django.__file__)):
             warnings.warn(
-                DEFAULT_CONTENT_TYPE_DEPRECATED_MSG,
-                RemovedInDjango30Warning,
+                PASSWORD_RESET_TIMEOUT_DAYS_DEPRECATED_MSG,
+                RemovedInDjango40Warning,
                 stacklevel=2,
             )
-        return self.__getattr__('DEFAULT_CONTENT_TYPE')
-
-    @property
-    def FILE_CHARSET(self):
-        stack = traceback.extract_stack()
-        # Show a warning if the setting is used outside of Django.
-        # Stack index: -1 this line, -2 the caller.
-        filename, _line_number, _function_name, _text = stack[-2]
-        if not filename.startswith(os.path.dirname(django.__file__)):
-            warnings.warn(
-                FILE_CHARSET_DEPRECATED_MSG,
-                RemovedInDjango31Warning,
-                stacklevel=2,
-            )
-        return self.__getattr__('FILE_CHARSET')
+        return self.__getattr__('PASSWORD_RESET_TIMEOUT_DAYS')


 class Settings:
@@ -172,13 +185,17 @@
                 setattr(self, setting, setting_value)
                 self._explicit_settings.add(setting)

-        if not self.SECRET_KEY:
-            raise ImproperlyConfigured("The SECRET_KEY setting must not be empty.")
-
-        if self.is_overridden('DEFAULT_CONTENT_TYPE'):
-            warnings.warn(DEFAULT_CONTENT_TYPE_DEPRECATED_MSG, RemovedInDjango30Warning)
-        if self.is_overridden('FILE_CHARSET'):
-            warnings.warn(FILE_CHARSET_DEPRECATED_MSG, RemovedInDjango31Warning)
+        if self.is_overridden('PASSWORD_RESET_TIMEOUT_DAYS'):
+            if self.is_overridden('PASSWORD_RESET_TIMEOUT'):
+                raise ImproperlyConfigured(
+                    'PASSWORD_RESET_TIMEOUT_DAYS/PASSWORD_RESET_TIMEOUT are '
+                    'mutually exclusive.'
+                )
+            setattr(self, 'PASSWORD_RESET_TIMEOUT', self.PASSWORD_RESET_TIMEOUT_DAYS * 60 * 60 * 24)
+            warnings.warn(PASSWORD_RESET_TIMEOUT_DAYS_DEPRECATED_MSG, RemovedInDjango40Warning)
+
+        if self.is_overridden('DEFAULT_HASHING_ALGORITHM'):
+            warnings.warn(DEFAULT_HASHING_ALGORITHM_DEPRECATED_MSG, RemovedInDjango40Warning)

         if hasattr(time, 'tzset') and self.TIME_ZONE:
             # When we can, attempt to validate the timezone. If we can't find
@@ -217,16 +234,17 @@
         self.default_settings = default_settings

     def __getattr__(self, name):
-        if name in self._deleted:
+        if not name.isupper() or name in self._deleted:
             raise AttributeError
         return getattr(self.default_settings, name)

     def __setattr__(self, name, value):
         self._deleted.discard(name)
-        if name == 'DEFAULT_CONTENT_TYPE':
-            warnings.warn(DEFAULT_CONTENT_TYPE_DEPRECATED_MSG, RemovedInDjango30Warning)
-        elif name == 'FILE_CHARSET':
-            warnings.warn(FILE_CHARSET_DEPRECATED_MSG, RemovedInDjango31Warning)
+        if name == 'PASSWORD_RESET_TIMEOUT_DAYS':
+            setattr(self, 'PASSWORD_RESET_TIMEOUT', value * 60 * 60 * 24)
+            warnings.warn(PASSWORD_RESET_TIMEOUT_DAYS_DEPRECATED_MSG, RemovedInDjango40Warning)
+        if name == 'DEFAULT_HASHING_ALGORITHM':
+            warnings.warn(DEFAULT_HASHING_ALGORITHM_DEPRECATED_MSG, RemovedInDjango40Warning)
         super().__setattr__(name, value)

     def __delattr__(self, name):
('django/conf', 'global_settings.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -51,6 +51,7 @@
 LANGUAGES = [
     ('af', gettext_noop('Afrikaans')),
     ('ar', gettext_noop('Arabic')),
+    ('ar-dz', gettext_noop('Algerian Arabic')),
     ('ast', gettext_noop('Asturian')),
     ('az', gettext_noop('Azerbaijani')),
     ('bg', gettext_noop('Bulgarian')),
@@ -92,6 +93,7 @@
     ('hy', gettext_noop('Armenian')),
     ('ia', gettext_noop('Interlingua')),
     ('id', gettext_noop('Indonesian')),
+    ('ig', gettext_noop('Igbo')),
     ('io', gettext_noop('Ido')),
     ('is', gettext_noop('Icelandic')),
     ('it', gettext_noop('Italian')),
@@ -102,6 +104,7 @@
     ('km', gettext_noop('Khmer')),
     ('kn', gettext_noop('Kannada')),
     ('ko', gettext_noop('Korean')),
+    ('ky', gettext_noop('Kyrgyz')),
     ('lb', gettext_noop('Luxembourgish')),
     ('lt', gettext_noop('Lithuanian')),
     ('lv', gettext_noop('Latvian')),
@@ -130,19 +133,22 @@
     ('sw', gettext_noop('Swahili')),
     ('ta', gettext_noop('Tamil')),
     ('te', gettext_noop('Telugu')),
+    ('tg', gettext_noop('Tajik')),
     ('th', gettext_noop('Thai')),
+    ('tk', gettext_noop('Turkmen')),
     ('tr', gettext_noop('Turkish')),
     ('tt', gettext_noop('Tatar')),
     ('udm', gettext_noop('Udmurt')),
     ('uk', gettext_noop('Ukrainian')),
     ('ur', gettext_noop('Urdu')),
+    ('uz', gettext_noop('Uzbek')),
     ('vi', gettext_noop('Vietnamese')),
     ('zh-hans', gettext_noop('Simplified Chinese')),
     ('zh-hant', gettext_noop('Traditional Chinese')),
 ]

 # Languages using BiDi (right-to-left) layout
-LANGUAGES_BIDI = ["he", "ar", "fa", "ur"]
+LANGUAGES_BIDI = ["he", "ar", "ar-dz", "fa", "ur"]

 # If you set this to False, Django will make some optimizations so as not
 # to load the internationalization machinery.
@@ -154,6 +160,9 @@
 LANGUAGE_COOKIE_AGE = None
 LANGUAGE_COOKIE_DOMAIN = None
 LANGUAGE_COOKIE_PATH = '/'
+LANGUAGE_COOKIE_SECURE = False
+LANGUAGE_COOKIE_HTTPONLY = False
+LANGUAGE_COOKIE_SAMESITE = None


 # If you set this to True, Django will format dates, numbers and calendars
@@ -164,14 +173,9 @@
 # notifications and other various emails.
 MANAGERS = ADMINS

-# Default content type and charset to use for all HttpResponse objects, if a
-# MIME type isn't manually specified. These are used to construct the
-# Content-Type header.
-DEFAULT_CONTENT_TYPE = 'text/html'
+# Default charset to use for all HttpResponse objects, if a MIME type isn't
+# manually specified. It's used to construct the Content-Type header.
 DEFAULT_CHARSET = 'utf-8'
-
-# Encoding of files read from disk (template and initial SQL files).
-FILE_CHARSET = 'utf-8'

 # Email address that error messages come from.
 SERVER_EMAIL = 'root@localhost'
@@ -306,7 +310,7 @@

 # The numeric mode to set newly-uploaded files to. The value should be a mode
 # you'd pass directly to os.chmod; see https://docs.python.org/library/os.html#files-and-directories.
-FILE_UPLOAD_PERMISSIONS = None
+FILE_UPLOAD_PERMISSIONS = 0o644

 # The numeric mode to assign to newly-created directories, when uploading files.
 # The value should be a mode as you'd pass to os.chmod;
@@ -381,15 +385,12 @@
     '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
     '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
     '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
-    '%Y-%m-%d',              # '2006-10-25'
     '%m/%d/%Y %H:%M:%S',     # '10/25/2006 14:30:59'
     '%m/%d/%Y %H:%M:%S.%f',  # '10/25/2006 14:30:59.000200'
     '%m/%d/%Y %H:%M',        # '10/25/2006 14:30'
-    '%m/%d/%Y',              # '10/25/2006'
     '%m/%d/%y %H:%M:%S',     # '10/25/06 14:30:59'
     '%m/%d/%y %H:%M:%S.%f',  # '10/25/06 14:30:59.000200'
     '%m/%d/%y %H:%M',        # '10/25/06 14:30'
-    '%m/%d/%y',              # '10/25/06'
 ]

 # First day of week, to be used on calendars
@@ -413,8 +414,11 @@
 DEFAULT_TABLESPACE = ''
 DEFAULT_INDEX_TABLESPACE = ''

+# Default primary key field type.
+DEFAULT_AUTO_FIELD = 'django.db.models.AutoField'
+
 # Default X-Frame-Options header value
-X_FRAME_OPTIONS = 'SAMEORIGIN'
+X_FRAME_OPTIONS = 'DENY'

 USE_X_FORWARDED_HOST = False
 USE_X_FORWARDED_PORT = False
@@ -434,6 +438,12 @@
 # WARNING! Only set this if you fully understand what you're doing. Otherwise,
 # you may be opening yourself up to a security risk.
 SECURE_PROXY_SSL_HEADER = None
+
+# Default hashing algorithm to use for encoding cookies, password reset tokens
+# in the admin site, user sessions, and signatures. It's a transitional setting
+# helpful in migrating multiple instance of the same project to Django 3.1+.
+# Algorithm must be 'sha1' or 'sha256'.
+DEFAULT_HASHING_ALGORITHM = 'sha256'

 ##############
 # MIDDLEWARE #
@@ -463,7 +473,7 @@
 # Whether to use the HttpOnly flag.
 SESSION_COOKIE_HTTPONLY = True
 # Whether to set the flag restricting cookie leaks on cross-site requests.
-# This can be 'Lax', 'Strict', or None to disable the flag.
+# This can be 'Lax', 'Strict', 'None', or False to disable the flag.
 SESSION_COOKIE_SAMESITE = 'Lax'
 # Whether to save the session data on every request.
 SESSION_SAVE_EVERY_REQUEST = False
@@ -508,6 +518,9 @@
 # The number of days a password reset link is valid for
 PASSWORD_RESET_TIMEOUT_DAYS = 3

+# The number of seconds a password reset link is valid for (default: 3 days).
+PASSWORD_RESET_TIMEOUT = 60 * 60 * 24 * 3
+
 # the first hasher in this list is the preferred algorithm.  any
 # password using different algorithms will be converted automatically
 # upon login
@@ -566,6 +579,10 @@
 # Custom logging configuration.
 LOGGING = {}

+# Default exception reporter class used in case none has been
+# specifically assigned to the HttpRequest instance.
+DEFAULT_EXCEPTION_REPORTER = 'django.views.debug.ExceptionReporter'
+
 # Default exception reporter filter class used in case none has been
 # specifically assigned to the HttpRequest instance.
 DEFAULT_EXCEPTION_REPORTER_FILTER = 'django.views.debug.SafeExceptionReporterFilter'
@@ -627,10 +644,11 @@
 # SECURITY MIDDLEWARE #
 #######################
 SECURE_BROWSER_XSS_FILTER = False
-SECURE_CONTENT_TYPE_NOSNIFF = False
+SECURE_CONTENT_TYPE_NOSNIFF = True
 SECURE_HSTS_INCLUDE_SUBDOMAINS = False
 SECURE_HSTS_PRELOAD = False
 SECURE_HSTS_SECONDS = 0
 SECURE_REDIRECT_EXEMPT = []
+SECURE_REFERRER_POLICY = 'same-origin'
 SECURE_SSL_HOST = None
 SECURE_SSL_REDIRECT = False
('django/conf/locale', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -20,6 +20,12 @@
         'name': 'Arabic',
         'name_local': 'العربيّة',
     },
+    'ar-dz': {
+        'bidi': True,
+        'code': 'ar-dz',
+        'name': 'Algerian Arabic',
+        'name_local': 'العربية الجزائرية',
+    },
     'ast': {
         'bidi': False,
         'code': 'ast',
@@ -228,7 +234,7 @@
         'bidi': False,
         'code': 'hi',
         'name': 'Hindi',
-        'name_local': 'Hindi',
+        'name_local': 'हिंदी',
     },
     'hr': {
         'bidi': False,
@@ -272,6 +278,12 @@
         'name': 'Indonesian',
         'name_local': 'Bahasa Indonesia',
     },
+    'ig': {
+        'bidi': False,
+        'code': 'ig',
+        'name': 'Igbo',
+        'name_local': 'Asụsụ Ìgbò',
+    },
     'is': {
         'bidi': False,
         'code': 'is',
@@ -326,6 +338,12 @@
         'name': 'Korean',
         'name_local': '한국어',
     },
+    'ky': {
+        'bidi': False,
+        'code': 'ky',
+        'name': 'Kyrgyz',
+        'name_local': 'Кыргызча',
+    },
     'lb': {
         'bidi': False,
         'code': 'lb',
@@ -354,7 +372,7 @@
         'bidi': False,
         'code': 'ml',
         'name': 'Malayalam',
-        'name_local': 'Malayalam',
+        'name_local': 'മലയാളം',
     },
     'mn': {
         'bidi': False,
@@ -500,11 +518,23 @@
         'name': 'Telugu',
         'name_local': 'తెలుగు',
     },
+    'tg': {
+        'bidi': False,
+        'code': 'tg',
+        'name': 'Tajik',
+        'name_local': 'тоҷикӣ',
+    },
     'th': {
         'bidi': False,
         'code': 'th',
         'name': 'Thai',
         'name_local': 'ภาษาไทย',
+    },
+    'tk': {
+        'bidi': False,
+        'code': 'tk',
+        'name': 'Turkmen',
+        'name_local': 'Türkmençe',
     },
     'tr': {
         'bidi': False,
@@ -536,6 +566,12 @@
         'name': 'Urdu',
         'name_local': 'اردو',
     },
+    'uz': {
+        'bidi': False,
+        'code': 'uz',
+        'name': 'Uzbek',
+        'name_local': 'oʻzbek tili',
+    },
     'vi': {
         'bidi': False,
         'code': 'vi',
('django/conf/locale/sl', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -23,23 +23,18 @@
     '%d.%m.%Y %H:%M:%S',            # '25.10.2006 14:30:59'
     '%d.%m.%Y %H:%M:%S.%f',         # '25.10.2006 14:30:59.000200'
     '%d.%m.%Y %H:%M',               # '25.10.2006 14:30'
-    '%d.%m.%Y',                     # '25.10.2006'
     '%d.%m.%y %H:%M:%S',            # '25.10.06 14:30:59'
     '%d.%m.%y %H:%M:%S.%f',         # '25.10.06 14:30:59.000200'
     '%d.%m.%y %H:%M',                # '25.10.06 14:30'
-    '%d.%m.%y',                     # '25.10.06'
     '%d-%m-%Y %H:%M:%S',            # '25-10-2006 14:30:59'
     '%d-%m-%Y %H:%M:%S.%f',         # '25-10-2006 14:30:59.000200'
     '%d-%m-%Y %H:%M',               # '25-10-2006 14:30'
-    '%d-%m-%Y',                     # '25-10-2006'
     '%d. %m. %Y %H:%M:%S',          # '25. 10. 2006 14:30:59'
     '%d. %m. %Y %H:%M:%S.%f',       # '25. 10. 2006 14:30:59.000200'
     '%d. %m. %Y %H:%M',             # '25. 10. 2006 14:30'
-    '%d. %m. %Y',                   # '25. 10. 2006'
     '%d. %m. %y %H:%M:%S',          # '25. 10. 06 14:30:59'
     '%d. %m. %y %H:%M:%S.%f',       # '25. 10. 06 14:30:59.000200'
     '%d. %m. %y %H:%M',             # '25. 10. 06 14:30'
-    '%d. %m. %y',                   # '25. 10. 06'
 ]

 DECIMAL_SEPARATOR = ','
('django/conf/locale/sk', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -22,7 +22,6 @@
     '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
     '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
     '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
-    '%d.%m.%Y',              # '25.10.2006'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '\xa0'  # non-breaking space
('django/conf/locale/pl', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -22,7 +22,6 @@
     '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
     '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
     '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
-    '%d.%m.%Y',              # '25.10.2006'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = ' '
('django/conf/locale/de_CH', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -21,14 +21,13 @@
     '%d.%m.%Y %H:%M:%S',    # '25.10.2006 14:30:59'
     '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
     '%d.%m.%Y %H:%M',       # '25.10.2006 14:30'
-    '%d.%m.%Y',             # '25.10.2006'
 ]

 # these are the separators for non-monetary numbers. For monetary numbers,
 # the DECIMAL_SEPARATOR is a . (decimal point) and the THOUSAND_SEPARATOR is a
 # ' (single quote).
-# For details, please refer to http://www.bk.admin.ch/dokumentation/sprachen/04915/05016/index.html?lang=de
-# (in German) and the documentation
+# For details, please refer to the documentation and the following link:
+# https://www.bk.admin.ch/bk/de/home/dokumentation/sprachen/hilfsmittel-textredaktion/schreibweisungen.html
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '\xa0'  # non-breaking space
 NUMBER_GROUPING = 3
('django/conf/locale/sv', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -23,15 +23,12 @@
     '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
     '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
     '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
-    '%Y-%m-%d',              # '2006-10-25'
     '%m/%d/%Y %H:%M:%S',     # '10/25/2006 14:30:59'
     '%m/%d/%Y %H:%M:%S.%f',  # '10/25/2006 14:30:59.000200'
     '%m/%d/%Y %H:%M',        # '10/25/2006 14:30'
-    '%m/%d/%Y',              # '10/25/2006'
     '%m/%d/%y %H:%M:%S',     # '10/25/06 14:30:59'
     '%m/%d/%y %H:%M:%S.%f',  # '10/25/06 14:30:59.000200'
     '%m/%d/%y %H:%M',        # '10/25/06 14:30'
-    '%m/%d/%y',              # '10/25/06'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '\xa0'  # non-breaking space
('django/conf/locale/nn', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -24,16 +24,12 @@
     '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
     '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
     '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
-    '%Y-%m-%d',              # '2006-10-25'
-    '%Y-%m-%d',              # '2006-10-25'
     '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
     '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
     '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
-    '%d.%m.%Y',              # '25.10.2006'
     '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'
     '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'
     '%d.%m.%y %H:%M',        # '25.10.06 14:30'
-    '%d.%m.%y',              # '25.10.06'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '\xa0'  # non-breaking space
('django/conf/locale/pt_BR', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -22,11 +22,9 @@
     '%d/%m/%Y %H:%M:%S',     # '25/10/2006 14:30:59'
     '%d/%m/%Y %H:%M:%S.%f',  # '25/10/2006 14:30:59.000200'
     '%d/%m/%Y %H:%M',        # '25/10/2006 14:30'
-    '%d/%m/%Y',              # '25/10/2006'
     '%d/%m/%y %H:%M:%S',     # '25/10/06 14:30:59'
     '%d/%m/%y %H:%M:%S.%f',  # '25/10/06 14:30:59.000200'
     '%d/%m/%y %H:%M',        # '25/10/06 14:30'
-    '%d/%m/%y',              # '25/10/06'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '.'
('django/conf/locale/el', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -20,15 +20,12 @@
     '%d/%m/%Y %H:%M:%S',     # '25/10/2006 14:30:59'
     '%d/%m/%Y %H:%M:%S.%f',  # '25/10/2006 14:30:59.000200'
     '%d/%m/%Y %H:%M',        # '25/10/2006 14:30'
-    '%d/%m/%Y',              # '25/10/2006'
     '%d/%m/%y %H:%M:%S',     # '25/10/06 14:30:59'
     '%d/%m/%y %H:%M:%S.%f',  # '25/10/06 14:30:59.000200'
     '%d/%m/%y %H:%M',        # '25/10/06 14:30'
-    '%d/%m/%y',              # '25/10/06'
     '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
     '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
     '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
-    '%Y-%m-%d',              # '2006-10-25'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '.'
('django/conf/locale/lv', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -32,14 +32,12 @@
     '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
     '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
     '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
-    '%d.%m.%Y',              # '25.10.2006'
     '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'
     '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'
     '%d.%m.%y %H:%M',        # '25.10.06 14:30'
     '%d.%m.%y %H.%M.%S',     # '25.10.06 14.30.59'
     '%d.%m.%y %H.%M.%S.%f',  # '25.10.06 14.30.59.000200'
     '%d.%m.%y %H.%M',        # '25.10.06 14.30'
-    '%d.%m.%y',              # '25.10.06'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = ' '  # Non-breaking space
('django/conf/locale/it', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -22,23 +22,18 @@
     '%d/%m/%Y %H:%M:%S',     # '25/10/2006 14:30:59'
     '%d/%m/%Y %H:%M:%S.%f',  # '25/10/2006 14:30:59.000200'
     '%d/%m/%Y %H:%M',        # '25/10/2006 14:30'
-    '%d/%m/%Y',              # '25/10/2006'
     '%d/%m/%y %H:%M:%S',     # '25/10/06 14:30:59'
     '%d/%m/%y %H:%M:%S.%f',  # '25/10/06 14:30:59.000200'
     '%d/%m/%y %H:%M',        # '25/10/06 14:30'
-    '%d/%m/%y',              # '25/10/06'
     '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
     '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
     '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
-    '%Y-%m-%d',              # '2006-10-25'
     '%d-%m-%Y %H:%M:%S',     # '25-10-2006 14:30:59'
     '%d-%m-%Y %H:%M:%S.%f',  # '25-10-2006 14:30:59.000200'
     '%d-%m-%Y %H:%M',        # '25-10-2006 14:30'
-    '%d-%m-%Y',              # '25-10-2006'
     '%d-%m-%y %H:%M:%S',     # '25-10-06 14:30:59'
     '%d-%m-%y %H:%M:%S.%f',  # '25-10-06 14:30:59.000200'
     '%d-%m-%y %H:%M',        # '25-10-06 14:30'
-    '%d-%m-%y',              # '25-10-06'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '.'
('django/conf/locale/cs', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -29,12 +29,10 @@
     '%d.%m.%Y %H:%M:%S.%f',  # '05.01.2006 04:30:59.000200'
     '%d.%m.%Y %H.%M',       # '05.01.2006 04.30'
     '%d.%m.%Y %H:%M',       # '05.01.2006 04:30'
-    '%d.%m.%Y',             # '05.01.2006'
     '%d. %m. %Y %H:%M:%S',  # '05. 01. 2006 04:30:59'
     '%d. %m. %Y %H:%M:%S.%f',  # '05. 01. 2006 04:30:59.000200'
     '%d. %m. %Y %H.%M',     # '05. 01. 2006 04.30'
     '%d. %m. %Y %H:%M',     # '05. 01. 2006 04:30'
-    '%d. %m. %Y',           # '05. 01. 2006'
     '%Y-%m-%d %H.%M',       # '2006-01-05 04.30'
 ]
 DECIMAL_SEPARATOR = ','
('django/conf/locale/ru', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -21,11 +21,9 @@
     '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
     '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
     '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
-    '%d.%m.%Y',              # '25.10.2006'
     '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'
     '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'
     '%d.%m.%y %H:%M',        # '25.10.06 14:30'
-    '%d.%m.%y',              # '25.10.06'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '\xa0'  # non-breaking space
('django/conf/locale/pt', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -23,15 +23,12 @@
     '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
     '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
     '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
-    '%Y-%m-%d',              # '2006-10-25'
     '%d/%m/%Y %H:%M:%S',     # '25/10/2006 14:30:59'
     '%d/%m/%Y %H:%M:%S.%f',  # '25/10/2006 14:30:59.000200'
     '%d/%m/%Y %H:%M',        # '25/10/2006 14:30'
-    '%d/%m/%Y',              # '25/10/2006'
     '%d/%m/%y %H:%M:%S',     # '25/10/06 14:30:59'
     '%d/%m/%y %H:%M:%S.%f',  # '25/10/06 14:30:59.000200'
     '%d/%m/%y %H:%M',        # '25/10/06 14:30'
-    '%d/%m/%y',              # '25/10/06'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '.'
('django/conf/locale/uk', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -26,11 +26,9 @@
     '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
     '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
     '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
-    '%d.%m.%Y',              # '25.10.2006'
     '%d %B %Y %H:%M:%S',     # '25 October 2006 14:30:59'
     '%d %B %Y %H:%M:%S.%f',  # '25 October 2006 14:30:59.000200'
     '%d %B %Y %H:%M',        # '25 October 2006 14:30'
-    '%d %B %Y',              # '25 October 2006'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '\xa0'  # non-breaking space
('django/conf/locale/sr', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -24,19 +24,15 @@
     '%d.%m.%Y. %H:%M:%S',       # '25.10.2006. 14:30:59'
     '%d.%m.%Y. %H:%M:%S.%f',    # '25.10.2006. 14:30:59.000200'
     '%d.%m.%Y. %H:%M',          # '25.10.2006. 14:30'
-    '%d.%m.%Y.',                # '25.10.2006.'
     '%d.%m.%y. %H:%M:%S',       # '25.10.06. 14:30:59'
     '%d.%m.%y. %H:%M:%S.%f',    # '25.10.06. 14:30:59.000200'
     '%d.%m.%y. %H:%M',          # '25.10.06. 14:30'
-    '%d.%m.%y.',                # '25.10.06.'
     '%d. %m. %Y. %H:%M:%S',     # '25. 10. 2006. 14:30:59'
     '%d. %m. %Y. %H:%M:%S.%f',  # '25. 10. 2006. 14:30:59.000200'
     '%d. %m. %Y. %H:%M',        # '25. 10. 2006. 14:30'
-    '%d. %m. %Y.',              # '25. 10. 2006.'
     '%d. %m. %y. %H:%M:%S',     # '25. 10. 06. 14:30:59'
     '%d. %m. %y. %H:%M:%S.%f',  # '25. 10. 06. 14:30:59.000200'
     '%d. %m. %y. %H:%M',        # '25. 10. 06. 14:30'
-    '%d. %m. %y.',              # '25. 10. 06.'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '.'
('django/conf/locale/en_AU', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -24,15 +24,12 @@
     '%Y-%m-%d %H:%M:%S',                # '2006-10-25 14:30:59'
     '%Y-%m-%d %H:%M:%S.%f',             # '2006-10-25 14:30:59.000200'
     '%Y-%m-%d %H:%M',                   # '2006-10-25 14:30'
-    '%Y-%m-%d',                         # '2006-10-25'
     '%d/%m/%Y %H:%M:%S',                # '25/10/2006 14:30:59'
     '%d/%m/%Y %H:%M:%S.%f',             # '25/10/2006 14:30:59.000200'
     '%d/%m/%Y %H:%M',                   # '25/10/2006 14:30'
-    '%d/%m/%Y',                         # '25/10/2006'
     '%d/%m/%y %H:%M:%S',                # '25/10/06 14:30:59'
     '%d/%m/%y %H:%M:%S.%f',             # '25/10/06 14:30:59.000200'
     '%d/%m/%y %H:%M',                   # '25/10/06 14:30'
-    '%d/%m/%y',                         # '25/10/06'
 ]
 DECIMAL_SEPARATOR = '.'
 THOUSAND_SEPARATOR = ','
('django/conf/locale/en_GB', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -24,15 +24,12 @@
     '%Y-%m-%d %H:%M:%S',                # '2006-10-25 14:30:59'
     '%Y-%m-%d %H:%M:%S.%f',             # '2006-10-25 14:30:59.000200'
     '%Y-%m-%d %H:%M',                   # '2006-10-25 14:30'
-    '%Y-%m-%d',                         # '2006-10-25'
     '%d/%m/%Y %H:%M:%S',                # '25/10/2006 14:30:59'
     '%d/%m/%Y %H:%M:%S.%f',             # '25/10/2006 14:30:59.000200'
     '%d/%m/%Y %H:%M',                   # '25/10/2006 14:30'
-    '%d/%m/%Y',                         # '25/10/2006'
     '%d/%m/%y %H:%M:%S',                # '25/10/06 14:30:59'
     '%d/%m/%y %H:%M:%S.%f',             # '25/10/06 14:30:59.000200'
     '%d/%m/%y %H:%M',                   # '25/10/06 14:30'
-    '%d/%m/%y',                         # '25/10/06'
 ]
 DECIMAL_SEPARATOR = '.'
 THOUSAND_SEPARATOR = ','
('django/conf/locale/ml', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -25,15 +25,12 @@
     '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
     '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
     '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
-    '%Y-%m-%d',              # '2006-10-25'
     '%m/%d/%Y %H:%M:%S',     # '10/25/2006 14:30:59'
     '%m/%d/%Y %H:%M:%S.%f',  # '10/25/2006 14:30:59.000200'
     '%m/%d/%Y %H:%M',        # '10/25/2006 14:30'
-    '%m/%d/%Y',              # '10/25/2006'
     '%m/%d/%y %H:%M:%S',     # '10/25/06 14:30:59'
     '%m/%d/%y %H:%M:%S.%f',  # '10/25/06 14:30:59.000200'
     '%m/%d/%y %H:%M',        # '10/25/06 14:30'
-    '%m/%d/%y',              # '10/25/06'
 ]
 DECIMAL_SEPARATOR = '.'
 THOUSAND_SEPARATOR = ','
('django/conf/locale/mk', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -22,19 +22,15 @@
     '%d.%m.%Y %H:%M:%S',       # '25.10.2006 14:30:59'
     '%d.%m.%Y %H:%M:%S.%f',    # '25.10.2006 14:30:59.000200'
     '%d.%m.%Y %H:%M',          # '25.10.2006 14:30'
-    '%d.%m.%Y',                # '25.10.2006'
     '%d.%m.%y %H:%M:%S',       # '25.10.06 14:30:59'
     '%d.%m.%y %H:%M:%S.%f',    # '25.10.06 14:30:59.000200'
     '%d.%m.%y %H:%M',          # '25.10.06 14:30'
-    '%d.%m.%y',                # '25.10.06'
     '%d. %m. %Y %H:%M:%S',     # '25. 10. 2006 14:30:59'
     '%d. %m. %Y %H:%M:%S.%f',  # '25. 10. 2006 14:30:59.000200'
     '%d. %m. %Y %H:%M',        # '25. 10. 2006 14:30'
-    '%d. %m. %Y',              # '25. 10. 2006'
     '%d. %m. %y %H:%M:%S',     # '25. 10. 06 14:30:59'
     '%d. %m. %y %H:%M:%S.%f',  # '25. 10. 06 14:30:59.000200'
     '%d. %m. %y %H:%M',        # '25. 10. 06 14:30'
-    '%d. %m. %y',              # '25. 10. 06'
 ]

 DECIMAL_SEPARATOR = ','
('django/conf/locale/hr', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -23,23 +23,18 @@
     '%Y-%m-%d %H:%M:%S',        # '2006-10-25 14:30:59'
     '%Y-%m-%d %H:%M:%S.%f',     # '2006-10-25 14:30:59.000200'
     '%Y-%m-%d %H:%M',           # '2006-10-25 14:30'
-    '%Y-%m-%d',                 # '2006-10-25'
     '%d.%m.%Y. %H:%M:%S',       # '25.10.2006. 14:30:59'
     '%d.%m.%Y. %H:%M:%S.%f',    # '25.10.2006. 14:30:59.000200'
     '%d.%m.%Y. %H:%M',          # '25.10.2006. 14:30'
-    '%d.%m.%Y.',                # '25.10.2006.'
     '%d.%m.%y. %H:%M:%S',       # '25.10.06. 14:30:59'
     '%d.%m.%y. %H:%M:%S.%f',    # '25.10.06. 14:30:59.000200'
     '%d.%m.%y. %H:%M',          # '25.10.06. 14:30'
-    '%d.%m.%y.',                # '25.10.06.'
     '%d. %m. %Y. %H:%M:%S',     # '25. 10. 2006. 14:30:59'
     '%d. %m. %Y. %H:%M:%S.%f',  # '25. 10. 2006. 14:30:59.000200'
     '%d. %m. %Y. %H:%M',        # '25. 10. 2006. 14:30'
-    '%d. %m. %Y.',              # '25. 10. 2006.'
     '%d. %m. %y. %H:%M:%S',     # '25. 10. 06. 14:30:59'
     '%d. %m. %y. %H:%M:%S.%f',  # '25. 10. 06. 14:30:59.000200'
     '%d. %m. %y. %H:%M',        # '25. 10. 06. 14:30'
-    '%d. %m. %y.',              # '25. 10. 06.'
 ]

 DECIMAL_SEPARATOR = ','
('django/conf/locale/hu', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -24,7 +24,6 @@
     '%Y.%m.%d. %H:%M:%S',   # '2006.10.25. 14:30:59'
     '%Y.%m.%d. %H:%M:%S.%f',  # '2006.10.25. 14:30:59.000200'
     '%Y.%m.%d. %H:%M',      # '2006.10.25. 14:30'
-    '%Y.%m.%d.',            # '2006.10.25.'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = ' '  # Non-breaking space
('django/conf/locale/nl', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -16,6 +16,7 @@
 DATE_INPUT_FORMATS = [
     '%d-%m-%Y', '%d-%m-%y',             # '20-01-2009', '20-01-09'
     '%d/%m/%Y', '%d/%m/%y',             # '20/01/2009', '20/01/09'
+    '%Y/%m/%d',                         # '2009/01/20'
     # '%d %b %Y', '%d %b %y',           # '20 jan 2009', '20 jan 09'
     # '%d %B %Y', '%d %B %y',           # '20 januari 2009', '20 januari 09'
 ]
@@ -59,11 +60,6 @@
     '%d/%m/%Y %H.%M', '%d/%m/%y %H.%M',         # '20/01/2009 15.23', '20/01/09 15.23'
     # '%d %b %Y %H.%M', '%d %b %y %H.%M',         # '20 jan 2009 15.23', '20 jan 09 15.23'
     # '%d %B %Y %H.%M', '%d %B %y %H.%M',         # '20 januari 2009 15.23', '20 januari 2009 15.23'
-    # Without time :
-    '%d-%m-%Y', '%d-%m-%y', '%Y-%m-%d',         # '20-01-2009', '20-01-09', '2009-01-20'
-    '%d/%m/%Y', '%d/%m/%y', '%Y/%m/%d',         # '20/01/2009', '20/01/09', '2009/01/20'
-    # '%d %b %Y', '%d %b %y',                     # '20 jan 2009', '20 jan 09'
-    # '%d %B %Y', '%d %B %y',                     # '20 januari 2009', '20 januari 2009'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '.'
('django/conf/locale/nb', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -24,15 +24,12 @@
     '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
     '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
     '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
-    '%Y-%m-%d',              # '2006-10-25'
     '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
     '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
     '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
-    '%d.%m.%Y',              # '25.10.2006'
     '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'
     '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'
     '%d.%m.%y %H:%M',        # '25.10.06 14:30'
-    '%d.%m.%y',              # '25.10.06'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '\xa0'  # non-breaking space
('django/conf/locale/ka', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -16,31 +16,26 @@
 # Kept ISO formats as they are in first position
 DATE_INPUT_FORMATS = [
     '%Y-%m-%d', '%m/%d/%Y', '%m/%d/%y',     # '2006-10-25', '10/25/2006', '10/25/06'
+    '%d.%m.%Y', '%d.%m.%y',                 # '25.10.2006', '25.10.06'
     # '%d %b %Y', '%d %b, %Y', '%d %b. %Y',   # '25 Oct 2006', '25 Oct, 2006', '25 Oct. 2006'
     # '%d %B %Y', '%d %B, %Y',                # '25 October 2006', '25 October, 2006'
-    # '%d.%m.%Y', '%d.%m.%y',                 # '25.10.2006', '25.10.06'
 ]
 DATETIME_INPUT_FORMATS = [
     '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
     '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
     '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
-    '%Y-%m-%d',              # '2006-10-25'
     '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
     '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
     '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
-    '%d.%m.%Y',              # '25.10.2006'
     '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'
     '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'
     '%d.%m.%y %H:%M',        # '25.10.06 14:30'
-    '%d.%m.%y',              # '25.10.06'
     '%m/%d/%Y %H:%M:%S',     # '10/25/2006 14:30:59'
     '%m/%d/%Y %H:%M:%S.%f',  # '10/25/2006 14:30:59.000200'
     '%m/%d/%Y %H:%M',        # '10/25/2006 14:30'
-    '%m/%d/%Y',              # '10/25/2006'
     '%m/%d/%y %H:%M:%S',     # '10/25/06 14:30:59'
     '%m/%d/%y %H:%M:%S.%f',  # '10/25/06 14:30:59.000200'
     '%m/%d/%y %H:%M',        # '10/25/06 14:30'
-    '%m/%d/%y',              # '10/25/06'
 ]
 DECIMAL_SEPARATOR = '.'
 THOUSAND_SEPARATOR = " "
('django/conf/locale/de', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -21,7 +21,6 @@
     '%d.%m.%Y %H:%M:%S',    # '25.10.2006 14:30:59'
     '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
     '%d.%m.%Y %H:%M',       # '25.10.2006 14:30'
-    '%d.%m.%Y',             # '25.10.2006'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '.'
('django/conf/locale/az', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -21,11 +21,9 @@
     '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
     '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
     '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
-    '%d.%m.%Y',              # '25.10.2006'
     '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'
     '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'
     '%d.%m.%y %H:%M',        # '25.10.06 14:30'
-    '%d.%m.%y',              # '25.10.06'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '\xa0'  # non-breaking space
('django/conf/locale/ko', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -33,15 +33,12 @@
     '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
     '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
     '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
-    '%Y-%m-%d',              # '2006-10-25'
     '%m/%d/%Y %H:%M:%S',     # '10/25/2006 14:30:59'
     '%m/%d/%Y %H:%M:%S.%f',  # '10/25/2006 14:30:59.000200'
     '%m/%d/%Y %H:%M',        # '10/25/2006 14:30'
-    '%m/%d/%Y',              # '10/25/2006'
     '%m/%d/%y %H:%M:%S',     # '10/25/06 14:30:59'
     '%m/%d/%y %H:%M:%S.%f',  # '10/25/06 14:30:59.000200'
     '%m/%d/%y %H:%M',        # '10/25/06 14:30'
-    '%m/%d/%y',              # '10/25/06'

     '%Y년 %m월 %d일 %H시 %M분 %S초',  # '2006년 10월 25일 14시 30분 59초'
     '%Y년 %m월 %d일 %H시 %M분',       # '2006년 10월 25일 14시 30분'
('django/conf/locale/fi', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -21,12 +21,10 @@
     '%d.%m.%Y %H.%M.%S',     # '20.3.2014 14.30.59'
     '%d.%m.%Y %H.%M.%S.%f',  # '20.3.2014 14.30.59.000200'
     '%d.%m.%Y %H.%M',        # '20.3.2014 14.30'
-    '%d.%m.%Y',              # '20.3.2014'

     '%d.%m.%y %H.%M.%S',     # '20.3.14 14.30.59'
     '%d.%m.%y %H.%M.%S.%f',  # '20.3.14 14.30.59.000200'
     '%d.%m.%y %H.%M',        # '20.3.14 14.30'
-    '%d.%m.%y',              # '20.3.14'
 ]
 TIME_INPUT_FORMATS = [
     '%H.%M.%S',     # '14.30.59'
('django/conf/locale/sr_Latn', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -24,19 +24,15 @@
     '%d.%m.%Y. %H:%M:%S',       # '25.10.2006. 14:30:59'
     '%d.%m.%Y. %H:%M:%S.%f',    # '25.10.2006. 14:30:59.000200'
     '%d.%m.%Y. %H:%M',          # '25.10.2006. 14:30'
-    '%d.%m.%Y.',                # '25.10.2006.'
     '%d.%m.%y. %H:%M:%S',       # '25.10.06. 14:30:59'
     '%d.%m.%y. %H:%M:%S.%f',    # '25.10.06. 14:30:59.000200'
     '%d.%m.%y. %H:%M',          # '25.10.06. 14:30'
-    '%d.%m.%y.',                # '25.10.06.'
     '%d. %m. %Y. %H:%M:%S',     # '25. 10. 2006. 14:30:59'
     '%d. %m. %Y. %H:%M:%S.%f',  # '25. 10. 2006. 14:30:59.000200'
     '%d. %m. %Y. %H:%M',        # '25. 10. 2006. 14:30'
-    '%d. %m. %Y.',              # '25. 10. 2006.'
     '%d. %m. %y. %H:%M:%S',     # '25. 10. 06. 14:30:59'
     '%d. %m. %y. %H:%M:%S.%f',  # '25. 10. 06. 14:30:59.000200'
     '%d. %m. %y. %H:%M',        # '25. 10. 06. 14:30'
-    '%d. %m. %y.',              # '25. 10. 06.'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '.'
('django/conf/locale/eo', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -17,11 +17,13 @@
     '%Y-%m-%d',                         # '1887-07-26'
     '%y-%m-%d',                         # '87-07-26'
     '%Y %m %d',                         # '1887 07 26'
+    '%Y.%m.%d',                         # '1887.07.26'
     '%d-a de %b %Y',                    # '26-a de jul 1887'
     '%d %b %Y',                         # '26 jul 1887'
     '%d-a de %B %Y',                    # '26-a de julio 1887'
     '%d %B %Y',                         # '26 julio 1887'
     '%d %m %Y',                         # '26 07 1887'
+    '%d/%m/%Y',                         # '26/07/1887'
 ]
 TIME_INPUT_FORMATS = [
     '%H:%M:%S',                         # '18:59:00'
@@ -30,19 +32,15 @@
 DATETIME_INPUT_FORMATS = [
     '%Y-%m-%d %H:%M:%S',                # '1887-07-26 18:59:00'
     '%Y-%m-%d %H:%M',                   # '1887-07-26 18:59'
-    '%Y-%m-%d',                         # '1887-07-26'

     '%Y.%m.%d %H:%M:%S',                # '1887.07.26 18:59:00'
     '%Y.%m.%d %H:%M',                   # '1887.07.26 18:59'
-    '%Y.%m.%d',                         # '1887.07.26'

     '%d/%m/%Y %H:%M:%S',                # '26/07/1887 18:59:00'
     '%d/%m/%Y %H:%M',                   # '26/07/1887 18:59'
-    '%d/%m/%Y',                         # '26/07/1887'

     '%y-%m-%d %H:%M:%S',                # '87-07-26 18:59:00'
     '%y-%m-%d %H:%M',                   # '87-07-26 18:59'
-    '%y-%m-%d',                         # '87-07-26'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '\xa0'  # non-breaking space
('django/conf/locale/id', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -14,10 +14,11 @@
 # The *_INPUT_FORMATS strings use the Python strftime format syntax,
 # see https://docs.python.org/library/datetime.html#strftime-strptime-behavior
 DATE_INPUT_FORMATS = [
+    '%d-%m-%Y', '%d/%m/%Y',             # '25-10-2009', 25/10/2009'
     '%d-%m-%y', '%d/%m/%y',             # '25-10-09', 25/10/09'
-    '%d-%m-%Y', '%d/%m/%Y',             # '25-10-2009', 25/10/2009'
     '%d %b %Y',                         # '25 Oct 2006',
     '%d %B %Y',                         # '25 October 2006'
+    '%m/%d/%y', '%m/%d/%Y',             # '10/25/06', '10/25/2009'
 ]

 TIME_INPUT_FORMATS = [
@@ -29,19 +30,15 @@
     '%d-%m-%Y %H.%M.%S',                # '25-10-2009 14.30.59'
     '%d-%m-%Y %H.%M.%S.%f',             # '25-10-2009 14.30.59.000200'
     '%d-%m-%Y %H.%M',                   # '25-10-2009 14.30'
-    '%d-%m-%Y',                         # '25-10-2009'
     '%d-%m-%y %H.%M.%S',                # '25-10-09' 14.30.59'
     '%d-%m-%y %H.%M.%S.%f',             # '25-10-09' 14.30.59.000200'
     '%d-%m-%y %H.%M',                   # '25-10-09' 14.30'
-    '%d-%m-%y',                         # '25-10-09''
     '%m/%d/%y %H.%M.%S',                # '10/25/06 14.30.59'
     '%m/%d/%y %H.%M.%S.%f',             # '10/25/06 14.30.59.000200'
     '%m/%d/%y %H.%M',                   # '10/25/06 14.30'
-    '%m/%d/%y',                         # '10/25/06'
     '%m/%d/%Y %H.%M.%S',                # '25/10/2009 14.30.59'
     '%m/%d/%Y %H.%M.%S.%f',             # '25/10/2009 14.30.59.000200'
     '%m/%d/%Y %H.%M',                   # '25/10/2009 14.30'
-    '%m/%d/%Y',                         # '10/25/2009'
 ]

 DECIMAL_SEPARATOR = ','
('django/conf/locale/fr', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -22,11 +22,9 @@
     '%d/%m/%Y %H:%M:%S',     # '25/10/2006 14:30:59'
     '%d/%m/%Y %H:%M:%S.%f',  # '25/10/2006 14:30:59.000200'
     '%d/%m/%Y %H:%M',        # '25/10/2006 14:30'
-    '%d/%m/%Y',              # '25/10/2006'
     '%d.%m.%Y %H:%M:%S',     # Swiss [fr_CH), '25.10.2006 14:30:59'
     '%d.%m.%Y %H:%M:%S.%f',  # Swiss (fr_CH), '25.10.2006 14:30:59.000200'
     '%d.%m.%Y %H:%M',        # Swiss (fr_CH), '25.10.2006 14:30'
-    '%d.%m.%Y',              # Swiss (fr_CH), '25.10.2006'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '\xa0'  # non-breaking space
('django/conf/locale/en', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -25,15 +25,12 @@
     '%Y-%m-%d %H:%M:%S',     # '2006-10-25 14:30:59'
     '%Y-%m-%d %H:%M:%S.%f',  # '2006-10-25 14:30:59.000200'
     '%Y-%m-%d %H:%M',        # '2006-10-25 14:30'
-    '%Y-%m-%d',              # '2006-10-25'
     '%m/%d/%Y %H:%M:%S',     # '10/25/2006 14:30:59'
     '%m/%d/%Y %H:%M:%S.%f',  # '10/25/2006 14:30:59.000200'
     '%m/%d/%Y %H:%M',        # '10/25/2006 14:30'
-    '%m/%d/%Y',              # '10/25/2006'
     '%m/%d/%y %H:%M:%S',     # '10/25/06 14:30:59'
     '%m/%d/%y %H:%M:%S.%f',  # '10/25/06 14:30:59.000200'
     '%m/%d/%y %H:%M',        # '10/25/06 14:30'
-    '%m/%d/%y',              # '10/25/06'
 ]
 DECIMAL_SEPARATOR = '.'
 THOUSAND_SEPARATOR = ','
('django/conf/locale/lt', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -31,14 +31,12 @@
     '%d.%m.%Y %H:%M:%S',     # '25.10.2006 14:30:59'
     '%d.%m.%Y %H:%M:%S.%f',  # '25.10.2006 14:30:59.000200'
     '%d.%m.%Y %H:%M',        # '25.10.2006 14:30'
-    '%d.%m.%Y',              # '25.10.2006'
     '%d.%m.%y %H:%M:%S',     # '25.10.06 14:30:59'
     '%d.%m.%y %H:%M:%S.%f',  # '25.10.06 14:30:59.000200'
     '%d.%m.%y %H:%M',        # '25.10.06 14:30'
     '%d.%m.%y %H.%M.%S',     # '25.10.06 14.30.59'
     '%d.%m.%y %H.%M.%S.%f',  # '25.10.06 14.30.59.000200'
     '%d.%m.%y %H.%M',        # '25.10.06 14.30'
-    '%d.%m.%y',              # '25.10.06'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '.'
('django/conf/locale/cy', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -20,15 +20,12 @@
     '%Y-%m-%d %H:%M:%S',                # '2006-10-25 14:30:59'
     '%Y-%m-%d %H:%M:%S.%f',             # '2006-10-25 14:30:59.000200'
     '%Y-%m-%d %H:%M',                   # '2006-10-25 14:30'
-    '%Y-%m-%d',                         # '2006-10-25'
     '%d/%m/%Y %H:%M:%S',                # '25/10/2006 14:30:59'
     '%d/%m/%Y %H:%M:%S.%f',             # '25/10/2006 14:30:59.000200'
     '%d/%m/%Y %H:%M',                   # '25/10/2006 14:30'
-    '%d/%m/%Y',                         # '25/10/2006'
     '%d/%m/%y %H:%M:%S',                # '25/10/06 14:30:59'
     '%d/%m/%y %H:%M:%S.%f',             # '25/10/06 14:30:59.000200'
     '%d/%m/%y %H:%M',                   # '25/10/06 14:30'
-    '%d/%m/%y',                         # '25/10/06'
 ]
 DECIMAL_SEPARATOR = '.'
 THOUSAND_SEPARATOR = ','
('django/conf/locale/tr', 'formats.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -22,7 +22,6 @@
     '%d/%m/%Y %H:%M:%S',     # '25/10/2006 14:30:59'
     '%d/%m/%Y %H:%M:%S.%f',  # '25/10/2006 14:30:59.000200'
     '%d/%m/%Y %H:%M',        # '25/10/2006 14:30'
-    '%d/%m/%Y',              # '25/10/2006'
 ]
 DECIMAL_SEPARATOR = ','
 THOUSAND_SEPARATOR = '.'
('django/conf/urls', '__init__.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,4 +1,7 @@
+import warnings
+
 from django.urls import include, re_path
+from django.utils.deprecation import RemovedInDjango40Warning
 from django.views import defaults

 __all__ = ['handler400', 'handler403', 'handler404', 'handler500', 'include', 'url']
@@ -10,4 +13,10 @@


 def url(regex, view, kwargs=None, name=None):
+    warnings.warn(
+        'django.conf.urls.url() is deprecated in favor of '
+        'django.urls.re_path().',
+        RemovedInDjango40Warning,
+        stacklevel=2,
+    )
     return re_path(regex, view, kwargs, name)
('django/apps', 'config.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,9 +1,14 @@
+import inspect
 import os
+import warnings
 from importlib import import_module

 from django.core.exceptions import ImproperlyConfigured
-from django.utils.module_loading import module_has_submodule
-
+from django.utils.deprecation import RemovedInDjango41Warning
+from django.utils.functional import cached_property
+from django.utils.module_loading import import_string, module_has_submodule
+
+APPS_MODULE_NAME = 'apps'
 MODELS_MODULE_NAME = 'models'


@@ -29,6 +34,10 @@
         # This value must be unique across a Django project.
         if not hasattr(self, 'label'):
             self.label = app_name.rpartition(".")[2]
+        if not self.label.isidentifier():
+            raise ImproperlyConfigured(
+                "The app label '%s' is not a valid Python identifier." % self.label
+            )

         # Human-readable name for the application e.g. "Admin".
         if not hasattr(self, 'verbose_name'):
@@ -50,6 +59,15 @@

     def __repr__(self):
         return '<%s: %s>' % (self.__class__.__name__, self.label)
+
+    @cached_property
+    def default_auto_field(self):
+        from django.conf import settings
+        return settings.DEFAULT_AUTO_FIELD
+
+    @property
+    def _is_default_auto_field_overridden(self):
+        return self.__class__.default_auto_field is not AppConfig.default_auto_field

     def _path_from_module(self, module):
         """Attempt to determine app's filesystem path from its module."""
@@ -75,7 +93,7 @@
             raise ImproperlyConfigured(
                 "The app module %r has no filesystem location, "
                 "you must configure this app with an AppConfig subclass "
-                "with a 'path' class attribute." % (module,))
+                "with a 'path' class attribute." % module)
         return paths[0]

     @classmethod
@@ -83,73 +101,143 @@
         """
         Factory that creates an app config from an entry in INSTALLED_APPS.
         """
+        # create() eventually returns app_config_class(app_name, app_module).
+        app_config_class = None
+        app_config_name = None
+        app_name = None
+        app_module = None
+
+        # If import_module succeeds, entry points to the app module.
         try:
-            # If import_module succeeds, entry is a path to an app module,
-            # which may specify an app config class with default_app_config.
-            # Otherwise, entry is a path to an app config class or an error.
-            module = import_module(entry)
-
-        except ImportError:
-            # Track that importing as an app module failed. If importing as an
-            # app config class fails too, we'll trigger the ImportError again.
-            module = None
-
+            app_module = import_module(entry)
+        except Exception:
+            pass
+        else:
+            # If app_module has an apps submodule that defines a single
+            # AppConfig subclass, use it automatically.
+            # To prevent this, an AppConfig subclass can declare a class
+            # variable default = False.
+            # If the apps module defines more than one AppConfig subclass,
+            # the default one can declare default = True.
+            if module_has_submodule(app_module, APPS_MODULE_NAME):
+                mod_path = '%s.%s' % (entry, APPS_MODULE_NAME)
+                mod = import_module(mod_path)
+                # Check if there's exactly one AppConfig candidate,
+                # excluding those that explicitly define default = False.
+                app_configs = [
+                    (name, candidate)
+                    for name, candidate in inspect.getmembers(mod, inspect.isclass)
+                    if (
+                        issubclass(candidate, cls) and
+                        candidate is not cls and
+                        getattr(candidate, 'default', True)
+                    )
+                ]
+                if len(app_configs) == 1:
+                    app_config_class = app_configs[0][1]
+                    app_config_name = '%s.%s' % (mod_path, app_configs[0][0])
+                else:
+                    # Check if there's exactly one AppConfig subclass,
+                    # among those that explicitly define default = True.
+                    app_configs = [
+                        (name, candidate)
+                        for name, candidate in app_configs
+                        if getattr(candidate, 'default', False)
+                    ]
+                    if len(app_configs) > 1:
+                        candidates = [repr(name) for name, _ in app_configs]
+                        raise RuntimeError(
+                            '%r declares more than one default AppConfig: '
+                            '%s.' % (mod_path, ', '.join(candidates))
+                        )
+                    elif len(app_configs) == 1:
+                        app_config_class = app_configs[0][1]
+                        app_config_name = '%s.%s' % (mod_path, app_configs[0][0])
+
+            # If app_module specifies a default_app_config, follow the link.
+            # default_app_config is deprecated, but still takes over the
+            # automatic detection for backwards compatibility during the
+            # deprecation period.
+            try:
+                new_entry = app_module.default_app_config
+            except AttributeError:
+                # Use the default app config class if we didn't find anything.
+                if app_config_class is None:
+                    app_config_class = cls
+                    app_name = entry
+            else:
+                message = (
+                    '%r defines default_app_config = %r. ' % (entry, new_entry)
+                )
+                if new_entry == app_config_name:
+                    message += (
+                        'Django now detects this configuration automatically. '
+                        'You can remove default_app_config.'
+                    )
+                else:
+                    message += (
+                        "However, Django's automatic detection %s. You should "
+                        "move the default config class to the apps submodule "
+                        "of your application and, if this module defines "
+                        "several config classes, mark the default one with "
+                        "default = True." % (
+                            "picked another configuration, %r" % app_config_name
+                            if app_config_name
+                            else "did not find this configuration"
+                        )
+                    )
+                warnings.warn(message, RemovedInDjango41Warning, stacklevel=2)
+                entry = new_entry
+                app_config_class = None
+
+        # If import_string succeeds, entry is an app config class.
+        if app_config_class is None:
+            try:
+                app_config_class = import_string(entry)
+            except Exception:
+                pass
+        # If both import_module and import_string failed, it means that entry
+        # doesn't have a valid value.
+        if app_module is None and app_config_class is None:
+            # If the last component of entry starts with an uppercase letter,
+            # then it was likely intended to be an app config class; if not,
+            # an app module. Provide a nice error message in both cases.
             mod_path, _, cls_name = entry.rpartition('.')
-
-            # Raise the original exception when entry cannot be a path to an
-            # app config class.
-            if not mod_path:
-                raise
-
-        else:
-            try:
-                # If this works, the app module specifies an app config class.
-                entry = module.default_app_config
-            except AttributeError:
-                # Otherwise, it simply uses the default app config class.
-                return cls(entry, module)
+            if mod_path and cls_name[0].isupper():
+                # We could simply re-trigger the string import exception, but
+                # we're going the extra mile and providing a better error
+                # message for typos in INSTALLED_APPS.
+                # This may raise ImportError, which is the best exception
+                # possible if the module at mod_path cannot be imported.
+                mod = import_module(mod_path)
+                candidates = [
+                    repr(name)
+                    for name, candidate in inspect.getmembers(mod, inspect.isclass)
+                    if issubclass(candidate, cls) and candidate is not cls
+                ]
+                msg = "Module '%s' does not contain a '%s' class." % (mod_path, cls_name)
+                if candidates:
+                    msg += ' Choices are: %s.' % ', '.join(candidates)
+                raise ImportError(msg)
             else:
-                mod_path, _, cls_name = entry.rpartition('.')
-
-        # If we're reaching this point, we must attempt to load the app config
-        # class located at <mod_path>.<cls_name>
-        mod = import_module(mod_path)
-        try:
-            cls = getattr(mod, cls_name)
-        except AttributeError:
-            if module is None:
-                # If importing as an app module failed, check if the module
-                # contains any valid AppConfigs and show them as choices.
-                # Otherwise, that error probably contains the most informative
-                # traceback, so trigger it again.
-                candidates = sorted(
-                    repr(name) for name, candidate in mod.__dict__.items()
-                    if isinstance(candidate, type) and
-                    issubclass(candidate, AppConfig) and
-                    candidate is not AppConfig
-                )
-                if candidates:
-                    raise ImproperlyConfigured(
-                        "'%s' does not contain a class '%s'. Choices are: %s."
-                        % (mod_path, cls_name, ', '.join(candidates))
-                    )
+                # Re-trigger the module import exception.
                 import_module(entry)
-            else:
-                raise

         # Check for obvious errors. (This check prevents duck typing, but
         # it could be removed if it became a problem in practice.)
-        if not issubclass(cls, AppConfig):
+        if not issubclass(app_config_class, AppConfig):
             raise ImproperlyConfigured(
                 "'%s' isn't a subclass of AppConfig." % entry)

         # Obtain app name here rather than in AppClass.__init__ to keep
         # all error checking for entries in INSTALLED_APPS in one place.
-        try:
-            app_name = cls.name
-        except AttributeError:
-            raise ImproperlyConfigured(
-                "'%s' must supply a name attribute." % entry)
+        if app_name is None:
+            try:
+                app_name = app_config_class.name
+            except AttributeError:
+                raise ImproperlyConfigured(
+                    "'%s' must supply a name attribute." % entry
+                )

         # Ensure app_name points to a valid module.
         try:
@@ -157,12 +245,14 @@
         except ImportError:
             raise ImproperlyConfigured(
                 "Cannot import '%s'. Check that '%s.%s.name' is correct." % (
-                    app_name, mod_path, cls_name,
+                    app_name,
+                    app_config_class.__module__,
+                    app_config_class.__qualname__,
                 )
             )

         # Entry is a path to an app config class.
-        return cls(app_name, app_module)
+        return app_config_class(app_name, app_module)

     def get_model(self, model_name, require_ready=True):
         """
('django/apps', 'registry.py')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -2,7 +2,7 @@
 import sys
 import threading
 import warnings
-from collections import Counter, OrderedDict, defaultdict
+from collections import Counter, defaultdict
 from functools import partial

 from django.core.exceptions import AppRegistryNotReady, ImproperlyConfigured
@@ -31,10 +31,10 @@
         # and whether the registry has been populated. Since it isn't possible
         # to reimport a module safely (it could reexecute initialization code)
         # all_models is never overridden or reset.
-        self.all_models = defaultdict(OrderedDict)
+        self.all_models = defaultdict(dict)

         # Mapping of labels to AppConfig instances for installed apps.
-        self.app_configs = OrderedDict()
+        self.app_configs = {}

         # Stack of app_configs. Used to store the current state in
         # set_available_apps and set_installed_apps.
@@ -128,6 +128,7 @@
         """Raise an exception if all apps haven't been imported yet."""
         if not self.apps_ready:
             from django.conf import settings
+
             # If "not ready" is due to unconfigured settings, accessing
             # INSTALLED_APPS raises a more helpful ImproperlyConfigured
             # exception.
@@ -316,10 +317,11 @@
             )

         self.stored_app_configs.append(self.app_configs)
-        self.app_configs = OrderedDict(
-            (label, app_config)
+        self.app_configs = {
+            label: app_config
             for label, app_config in self.app_configs.items()
-            if app_config.name in available)
+            if app_config.name in available
+        }
         self.clear_cache()

     def unset_available_apps(self):
@@ -347,7 +349,7 @@
         if not self.ready:
             raise AppRegistryNotReady("App registry isn't ready yet.")
         self.stored_app_configs.append(self.app_configs)
-        self.app_configs = OrderedDict()
+        self.app_configs = {}
         self.apps_ready = self.models_ready = self.loading = self.ready = False
         self.clear_cache()
         self.populate(installed)
('extras', 'django_bash_completion')
--- /Users/tshi/researchProjs/django/django-2.2/
+++ /Users/tshi/researchProjs/django/django-3.2.14/
@@ -1,6 +1,5 @@
-# #########################################################################
-# This bash script adds tab-completion feature to django-admin.py and
-# manage.py.
+# #############################################################################
+# This bash script adds tab-completion feature to django-admin and manage.py.
 #
 # Testing it out without installing
 # =================================
@@ -37,19 +36,18 @@
                    COMP_CWORD=$COMP_CWORD \
                    DJANGO_AUTO_COMPLETE=1 $1 ) )
 }
+# When the django-admin.py deprecation ends, remove django-admin.py.
 complete -F _django_completion -o default django-admin.py manage.py django-admin

 _python_django_completion()
 {
     if [[ ${COMP_CWORD} -ge 2 ]]; then
         local PYTHON_EXE=${COMP_WORDS[0]##*/}
-        echo $PYTHON_EXE | egrep "python([3-9]\.[0-9])?" >/dev/null 2>&1
-        if [[ $? == 0 ]]; then
+        if echo "$PYTHON_EXE" | grep -qE "python([3-9]\.[0-9])?"; then
             local PYTHON_SCRIPT=${COMP_WORDS[1]##*/}
-            echo $PYTHON_SCRIPT | egrep "manage\.py|django-admin(\.py)?" >/dev/null 2>&1
-            if [[ $? == 0 ]]; then
-                COMPREPLY=( $( COMP_WORDS="${COMP_WORDS[*]:1}" \
-                               COMP_CWORD=$(( COMP_CWORD-1 )) \
+            if echo "$PYTHON_SCRIPT" | grep -qE "manage\.py|django-admin(\.py)?"; then
+                COMPREPLY=( $( COMP_WORDS=( "${COMP_WORDS[*]:1}" )
+                               COMP_CWORD=$(( COMP_CWORD-1 ))
                                DJANGO_AUTO_COMPLETE=1 ${COMP_WORDS[*]} ) )
             fi
         fi
@@ -64,7 +62,7 @@
         [[ $python != *-config ]] && pythons="${pythons} ${python##*/}"
     done
     unset python_interpreters
-    pythons=$(echo $pythons | tr " " "\n" | sort -u | tr "\n" " ")
+    pythons=$(echo "$pythons" | tr " " "\n" | sort -u | tr "\n" " ")
 else
     pythons=python
 fi
